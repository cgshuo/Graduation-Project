 1. Introduction
Nowadays, the available information particularly that obtained through the Internet is progressively increasing. The main way to access this information is through Information Retrieval (IR) systems. An IR system takes a user X  X  query as input and returns a set of documents sorted by their relevance to the query. IR systems are usually based on the segmentation of documents and queries into index terms, and their relevance is computed according to the index terms they have in com-between papers or bibliographic references) or some probabilistic information (e.g., the estimation of the likelihood that a shared term indicates relevance). Some well-known state-of-the-art models to compute relevance are the cosine measure in
Romano, 2004 ). These measures assume that the index terms of queries and documents are statistically independent of each not so far been proved ( Raghavan &amp; Wong, 1986; Wong, Ziarko, &amp; Raghavan, 1987 ).

In this paper, we present a proposal to include this dependency information between query terms, called LexSIR (Lexical in an IR system with mean average precision (MAP) of over 0.5, with a more inflected language like Spanish, with different corpora and with different query lengths.

The next section will analyze several proposals that sought to improve the effectiveness of IR systems. Later, we will present our proposal and finally, it will be evaluated.  X  2. Previous research
Many attempts to incorporate dependency information between index terms have been reported in the past. Some of these attempts use query expansion techniques, which usually improve precision results this section (e.g. Li, 2008 or Peat &amp; Willet, 1991 ).
 Many researchers incorporate the dependency information between index terms by means of the concept of Term
Proximity (TP) information. It means the incorporation of the following two intuitions ( Vechtomova &amp; Karamuftuoglu, are in a document, the more likely it is that the document is relevant to the query.

We consider that these previous attempts suffer from the weak points analyzed in the following three subsections. 2.1. The detection of terms that must satisfy proximity restrictions
Researchers usually calculate the TP between pairs of query terms, specifically between all possible combinations of query pairs. For example, in the query Letter Bomb for Kiesbauer , TP is calculated for the pairs  X  X  X etter  X  Bomb, Letter  X 
Kiesbauer, Bomb  X  Kiesbauer X  X  . This is a problem in long queries, in which there is not a clear dependency relation between some query terms. 2 Therefore, many researchers evaluate their proposals on short queries possible query pairs.

This problem is also overcome in Mitra, Buckley, Singhal, and Cardie (1997) by means of selecting only those query pairs that co-occur in the corpus at least 25 times. With this method, they obtained an improvement of +3.9% in MAP (from 0.3616 to 0.3758). Their experiments were replicated in Turpin and Moffat (1999) , obtaining a maximum improvement of +5.7% (from 0.3373 to 0.3579), and confirming that phrases generated as index terms are not the precision enhancing devices that they should be. Moreover, they observed that using phrases helped at low recall levels, but did not help in the top 20 doc-uments retrieved; and this method works correctly for some query phrases but is quite wrong for others.
Another example is the work by Rasolofo and Savoy (2003) that considers the instance of each pair of terms in the doc-ument if they are within a maximal distance of five terms. Nevertheless, as the authors conclude, the weakest point of this cess depends on the terms, the query and the documents. They obtain an improvement of +2.4% in MAP (from 0.2525 to 0.2586). In the same way, B X ttcher, Clarke, and Lushman (2006) reproduce Rasolofo and Savoy X  X  work, and they conclude that TP is more important with longer documents, when the size of the text collection increases and when the queries are stemmed.

Other researchers propose to overcome the problem of the selection of the set of query pairs by means of parsing. For example, Fagan (1989) states that the shortcomings of the nonsyntactic approach can be overcome by incorporating syntac-posals are analyzed, all of them with low benefits. Mittendorfer and Winiwarter (2002) present an approach for exploiting the syntactic structure of a query for an IR system that performs better than a standard vector space model in only some cases. They conclude that a categorization of the queries should be previously performed in order to decide when to use syn-tactic knowledge. Byung-Kwan, Jee-Hyub, Geunbae, and Jung Yun (2000) index exclusively Korean compound nouns with only a 0.8% of improvement in MAP. With respect to the phrases, they have to devise complex measures of similarity be-tween syntactic trees. Other authors simplify the comparison between syntactic trees by selecting head-modifier
Arampatzis, van der Weide, Koster, and van Bommel (2000) put forward some possible reasons for the lack of success when using Natural Language Processing (NLP) techniques in IR, particularly when using syntactic phrases. They state that, about whether syntactic structure is a good substitute for semantic content. They list the main problems to be solved: mor-word into different lexical categories (adjective, verb or adverb).

We propose to overcome the weak points reported in this subsection by adopting the scheme of chunking the query and documents into sets of simple phrases. Thus, our approach is more robust against errors from parsing, it does not require complex comparisons between syntactic trees or additional configuration parameters required to obtain the query pairs, and it deals with complete phrases instead of just pairs of terms. 2.2. Different ways to incorporate the TP information
With regard to the way to incorporate the TP information, many researchers re-sort a number of documents returned by a between a document d and a query q (see formula (1) , where phrase _ factor=0.5 ).
Another example, is the work by Rasolofo and Savoy (2003) , which proposes to re-sort the first 100 documents by the com-ing leads to topic deviation in concepts represented by these terms and the search results are biased towards these concepts. 5 This problem is avoided by adding a configuration parameter for the weight introduced by the TP information &amp; Ogawa, 2000 ), with the consequent increase in the complexity of the proposal.

Other researchers perform deeper modifications in the similarity measure, such as Cho, Lee, and Lee (2003) that introduces into Okapi the co-occurrence between pairs of terms within a window unit of a sentence with low MAP increases (from 0.2519 to 0.2627). Likewise, Metzler and Croft (2005) obtains similar improvements for title queries (e.g. from 0.17775 to 0.1847), concluding that dependence models may yield larger improvements for larger collections.

The approach from Liu, Liu, and Yu (2004) proposes a special treatment for the query (not for the documents), where query noun phrases are identified and classified by means of a POS-tagger into four types (proper names, dictionary phrases, simple phrases and complex phrases). Moreover, WordNet is used to disambiguate word senses of query terms (synonyms, hyponyms, words from its definition and its compound words are considered for query expansion). In this approach, a doc-ument has a phrase if all content words in the query phrase are within a window of a certain size, where the window sizes for different types of phrases are different and are learned using a decision tree: e.g. the proper names are required to be adjacent whereas the remaining phrases can have larger window sizes. They use only significant phrases: (1) proper names or dictionary phrases, or (2) simple or complex phrases whose content words are highly positively correlated in the docu-ment collection. They rank the documents firstly by a phrase-sim , and those documents with the same phrase-sim are ranked according to term-sim using the traditional Okapi similarity function. Their experimental results (in title queries without query expansion) show yields between 3% and 16% depending on the corpora.

Regarding the use of WordNet, the work by Voorhees (1993) reports the comparison between the effectiveness of sense-based vectors (formed by  X  X  X S-A X  X  relations contained within WordNet) vs. stem-based vectors. It shows the stem-based vec-tors to be superior overall, although the sense-based vectors improve the performance of some queries. The author states that the degradation is due in large part to the difficulty of disambiguating senses in short query statements.
Our proposal differs from Liu et al. (2004) and the previous ones, and contributes to the state-of-the-art in the following issues: 1. We use all the query phrases, not only a subset of those that they consider as significant, moreover whose significance depends on the corpora, which requires larger collections of documents, and furthermore this is a small number of phrases since they are using only title queries, which allows a great number of draws between documents that are ranked by term-sim (Okapi). 2. They require that the whole query phrase must appear in the document, whereas we deal with the absence of each term separately depending on its lexical type, which is a more robust approach to misspelling errors (e.g. Kiesbauer vs.
Kisbauer ), hyponyms ( Child Labor in Asia vs. Child Labor in India ) or derivational variations ( Japan vs. Japanese ). 3. They do not consider TP information between phrases as well as terms, as we do. 2.3. Inconsistent results between different corpora and queries
From approaches presented in previous subsections, similar conclusions about inconsistent results between different corpora and queries are reached, such as ( Mitra et al., 1997 ) X  X  ... this method works correctly for some query phrases but when the size of the text collection increases and when the queries are stemmed ...  X  X .

For example, in Vechtomova (2006), Vechtomova and Karamuftuoglu (2008) , the concept of lexical cohesion and TP are queries. They obtain an improvement of +9% (from 0.1947 to 0.2126) in HARD X 2005, and +2% (from 0.2348 to 0.2401) in
HARD X 2004. They justify the variation in the improvement because the more stable phrases mainly benefit from TP (e.g., clude that the improvement is not consistent across different data collections (TP is more useful for collections with long documents).

As the evaluation section will show, we will obtain consistent results in different corpora, languages, query lengths and tasks (Question Answering as well as Information Retrieval). 3. LexSIR: our proposal
Our proposal to include dependency information is called LexSIR (Lexical and Syntactic knowledge for IR). Next, we sum-marize our contributions to the state-of-the-art. The following subsection presents the intuitive model on which LexSIR is based. Afterwards, the algorithm of LexSIR is detailed. Finally, an example of the application of LexSIR is shown. 3.1. Contributions of our proposal
We propose to overcome the weak points identified in previous research in the following way between different corpora and queries (weak point number three). 1. We adopt the scheme of parsing the query to obtain the set of query terms to calculate the TP information, instead of calculating TP between all possible combinations of query pairs, but we differ from previous approaches in the following three points: (1) We do not carry out a full parsing of the query but chunking the queries into sets of simple phrases (noun, prepositional phrases and sequences of verbs). For example, the query A letter bomb from right-wing radicals sent to the black TV personality Arabella Kiesbauer in 1995 is segmented into the following five phrases: [letter bomb] [right-wing radicals] 2 [sent] 3 [black TV personality Arabella Kiesbauer] errors from parsing, and it does not require complex comparisons between syntactic trees. Consequently, TP information will be required only between the terms in a noun phrase (e.g. between  X  X  X etter X  X  and  X  X  X omb X  X ). Therefore, our proposal let us deal with long queries with no additional configuration parameters required to obtain the query pairs (e.g. the number of times that a query pair should co-occur in the corpus or the maximal distance between query pairs in the corpus, data that depends on each specific corpus). (2) We deal with complete phrases instead of just pairs of terms, because these pairs do not represent the real query concepts (e.g. in previous phrase 4, we deal with anaphoric references to the phrase, calculating TP for the whole phrase instead of for each pair such as black-Arabella or TV-Kiesbauer , which are not real concepts reported by the query). (3) Our approach does not require complex measures of similarity between syntactic trees. type of each query term. For example, in the query phrase seven wonders , if a document contains the separated phrases-fore, the distance between a number and the noun which it modifies, and the distance between two common nouns or adjective-noun should be considered in a different way. Similarly, the absence of a proper noun should not be measured should be penalized higher than the absence of the adjective black . That is because these proper nouns are close to be con-with Arabella Kiesbauer could never be considered as relevant. Moreover, TP information is incorporated in the similarity measure by means of penalizations instead of adding the weight of the pairs of terms in a phrase. In this way, no additional configuration parameters are required to avoid topic or phrase deviation. The intuitive idea behind this scheme is the aim of computing the right weight of each term depending on the remaining terms in the phrase and not adding more weight to the document if it contains these terms in the same phrase. Thus, our proposal adjusts the weight provided by any IR scheme, therefore, it can be used by any similarity measure (such as Okapi or pivoted cosine). Finally, the distance between terms is measured by the number of phrases or sentences between both terms, instead of measuring it by number of terms, because we consider phrases and sentences to be more realistic discourse units than isolated terms: phrases represent the concepts expressed in a text more accurately than single words and sentences are an important discourse unit for the interaction between phrases. 3. We apply TP measures to phrases as well as terms because phrases represent the concepts expressed in a text more accu-rately than single words. For example, in the query explosion of a letter bomb in the TV channel PRO7 , with the phrases: in a relevant document, thus we apply a TP measure between both phrases, which makes our approach more robust to syntactic variation( explosion of a letter bomb vs. letter bomb explosion ). However, phrase 3 could appear quite separate from them in the discourse structure of the document, given that the preposition in could mark the place in which the action of the document occurs. 4. We obtain consistent results in the worst conditions reported by previous research: in a state-of-the-art IR system with high MAP 7 of over 0.5, with a more inflected language like Spanish, with shorter documents and shorter collections and with different query lengths (not only the title version of the queries). 3.2. The LexSIR intuitive model
Our proposal is based on theories of discourse structure such as Discourse Representation Theory ( Kamp, 1981 ), which composed of phrases and phrases are composed of individual words. According to Grosz, Joshi, and Weinstein (1983) , partici-
Therefore, we propose to segment the documents into sentences that contain entities represented as phrases. Thus, we measure TP distances between entities and not between single terms or pairs of terms. There are several ways of interaction between these phrases, mainly through anaphora references that we consider in order to measure TP between entities.
Next, an example of the application of our intuitive model is presented for the query Assassination of Yitzhak Rabin. Who shot Yitzhak Rabin and why? Firstly, the query is chunked into phrases and sentences by means of a POS-tagger tactic chunker. Thus, there are two sentences and four phrases:
POS-TAGGING (LEMMA; WORD; TAG):
CHUNKING OF PHRASES AND SENTENCES: The two documents analyzed in this example are the following: (1) Assassin Amir does it again for the cameras Tel Aviv, Thursday WEARING a bullet-proof vest and (2) The tape is said to show confessed killer Yigal Amir standing next to a large plant then walking (3) Rabin X  X  assassination was the first of an Israeli leader in the 47-yearhistory of the security-(4) We could only remember Ziad as the man who ordered his troops to shoot Shqaqi. ... (5) The man who issued orders only a few days ago for the assassination of Palestinian struggler
The discourse focus is set at the beginning of document 1, Yitzhak Rabin , and all subsequent references to this entity are made by means of anaphoric references such as the Prime Minister or Mr. Rabin . Therefore, previous approaches would wrongly penalize the distance between the pair assassination-Yitzhak in sentence (3), whereas our approach would not be-just a reference apparition ( Rabin in sentence 3).
However, our approach penalizes sentence (5) in document 2, the assassination of Palestinian struggler Fathi Shqaqi , be-shoot requires the co-occurrence in the document with entities of its same query sentence, thus sentence (2) would not be penalized (although Yitzhak does not appear in this sentence), whereas sentence (4) would be penalized.
Similar situations occur with the entities in the intentional and attentional structure of the discourse, such as the tem-championships] ...... [the athletic qualifying heats] ... the distance to the entity 1995 could not be considered. 3.3. LexSIR algorithm
LexSIR works on the output of a traditional IR similarity measure (e.g. Okapi) that calculates the similarity value between a document d and a query q : IR w  X  d ; q  X  X  P t 2 query terms term. No query expansion is used because we want to isolate the effects of our proposal with just the same set of terms used in the traditional IR system. LexSIR has the following configuration parameters: kpenal _ verb , and it re-sorts the documents by LexSIR _ w
I POS-tagging and chunking of query and documents into simple phrases (noun, prepositional phrases and sequences of verbs) and sentences, as well as the typical removing of stop-words.

II For each term t of q , following the IR model, we calculate an estimation ( wf crimination level. For example in the tf-idf model, this estimation could be the idf of t ,orin DFR or Okapi it could be the weight of the term with all the parameters depending on the document set to 1 (such as the frequency of the term in the document, the length of the document and the average document length of all the documents of the collection).
Therefore, each query term has a single weight that is the same value for all the documents and it measures the impor-tance of each term with regard to the remaining terms of the collection.

III Let S ={ ... , q _ ph i , ... } be the set of all the query phrases in q . For each query phrase q _ ph a Calculation of its IR weight as the sum of all the IR weights of each term t of q _ ph b Calculation of the lowest penalization ( penal i that ranges from 0 to 1) for the distance between the IV Calculation of the weight of the document: LexSIR w d  X  d ; q  X  X 
V For each pair of query phrases in the pattern [ q _ ph i 1 distance between both phrases in the document. LexSIR _ w _ p VI For each query verb phrase: LexSIR _ w ( d , q )= LexSIR _ w _ p ( d , q ) penal _ verb .

With regard to step III.b, the algorithm to calculate penal
Kisbauer ). Only one level of hyponyms are obtained for each term in order to avoid excessively semantically distant terms (e.g. object vs. car ). The configuration parameter perc _ one is necessary to overcome the problems detected by Arampatzis et al. (2000) about the lack of accuracy of NLP techniques and the high syntactic variation for the same semantic content: the penalization is only applied on a percentage of the whole weight of the phrase.  X  The algorithm for the calculation of penal i for each q _ ph 1. Let D ={ ... , ph j , ... } be the set of all the document phrases in d . For each phrase ph a WF =0 . b For each t 2 q _ ph i that is also in ph j , WF  X  WF  X  P c For each t 2 q _ ph i that is not in ph j but is in other ph 2. For each t 2 q _ ph i with a lexical type t lex , and it is not in d : document that does not contain the proper noun Japan (or one of the variations previously enumerated, e.g. Japanese ) will probably be irrelevant even though it deals with fast food restaurants . Therefore, we have perc _ absence nouns, and another one for the remaining lexical types. 3. penal i  X  WF P
Step V of the main algorithm applies TP measures between phrases (contribution 3), specifically in the case that the query phrase is preceded by a preposition of or a genitive . For example, in the query phrase explosion of a letter bomb in the TV channel PRO7 , there are three phrases: [explosion] likely to be required to appear closer in a relevant document, but phrase 3 could appear quite separate from them in the discourse structure of the document, given that the preposition in could mark the place in which the action of the docu-ment occurs.

A similar example with a genitive is Women X  X  Conference . If both phrases appear in the document, the concept represented by the combination of both phrases should be checked whether it appears in the document. The penalization is calculated in a similar way as has been previously explained: penal of  X  X  1 number of phrases or sentences (depending on t lex ) between phrases i and phrases, the distance is measured by the number of phrases, otherwise it is measured by sentences. kpenal _ of is a configu-ration parameter calculated in a training phase.

Similarly, step VI of the main algorithm adjusts the weights of the query verbs with regard to the remaining query phrases it does not match the searched verb concept. The penalization is calculated as t penal accompanied by at least one query phrase of its same query sentence. 3.4. An example of the application of LexSIR algorithm
In this subsection, the application of our algorithm is analyzed on the following document sentences: (1) Ancient wonder to remain in the deep Cairo THE 2200-year-old Pharos X  one of the Seven Wonders of (2)  X  X  The Wonders of Country Music X  X : Tammy Wynette is the host of asix-part weekly series in which (3) Sept. 29 at 5 p.m. On Nov. 10, Willie Nelson takes over as host for seven weeks.

These three document sentences are POS-tagged and chunked (step 1 of LexSIR algorithm): (1) #1# Ancient wonder, #2# remain, #3# the deep Cairo, #4# THE 2200 year-old Pharos lighthouse, #5# (2) #346# The Wonders, #347# Country Music, #348# Tammy Wynette, #349# is, #350# the host, #351# a six (3) #360# Sept, #361# 29, #362# 5 p, #363# Nov, #364# 10, #365# Willie Nelson, #366# takes, #367# over,
Next, the application of LexSIR algorithm on the query Remains of the seven wonders of the ancient world is presented. Thus, step I also produces the following POS-tagging and chunking on the query: (remain; remains; NNS); (of; of; of); (seven; seven; CD); (wonder; wonders; NNS); (of; of; of); (ancient;
Ancient; JJ); (world; World; NP) #1# remains, #2# seven wonders, #3# ancient world the estimation of wf t , we have assigned a fixed value of 1 to f the term in the document, f t , d ), l d (the length of the document), and avr _ l of the collection):
Step III.a for the calculation of the IR weight of each q _ ph
Step III.b.3 in sentence (1), query phrase 2 ( seven wonders ), document phrase (number 6). Similarly, in sentence (1), phrase 3 ( ancient world ), penal appear in different document phrases (number 1 and 7), they appear in the same sentence and ancient is an adjective. There-fore, w _ q _ ph i = IR _ w _ q _ ph i for phrases 2 and 3.

However, in sentence (2), phrase 2, seven appears in phrase 369 and wonders appears in phrase 346, and given that seven is a number (POS-tagged as CD ), the distance will be measured by the number of phrases, and there are 23 phrases among both phrases, thus:
Therefore, with perc _ one = 0.75 , w _ q _ ph 2 = IR _ w _ q _ ph of sentence (1) is kept.

With regard to step III.b.1.e, the references for each entity are stored in the form of pairs (number _ of _ sentence, applied with penal _ of =0 because there is at least one reference of all these phrases in the same sentence (the number 1), and t lex  X  proper _ noun for at least one phrase in each pair of phrases linked by of .

Finally, our algorithm concludes that no penalization is applied on the document, because although the document and the query do not share the same syntactic structure, sentence (1) presents the query entities. Therefore, LexSIR _ w
IR _ w ( d , q ) . However, if the document would have been formed only by sentences (2) and (3), a penalization of 0.55764 would have been applied, because the entity seven wonders is not in the document although both terms appear. 4. Evaluation 4.1. Training of LexSIR
LexSIR has been implemented in C++ with a typical IR architecture, where there is an off-line phase in which the documents are indexed with stop-words removal and stemming of terms (Porter X  X  version), as well as the processing that we propose: tained from the Santana, Carreras, Hern X ndez, and Gonzalez (2007) tools.
We have performed the training and configuration of LexSIR on the EFE Spanish corpora used in the Cross Language Eval-uation Forum (CLEF) competition. This corpus is made up of 454,046 news stories from the EFE agency between the years 1994 and 1995. The documents have 177.4 index terms on average (excluding stop-words). The queries are those used in the CLEF X 2003 competition: 60 queries (from 141 to 200). All the queries, as shown in Fig. 1 , have three different versions tion of what makes a document relevant). The title and desc versions of each query are joined together in order to form the training queries, which have on average 8.5 phrases, 10.3 index terms (excluding stop-words) and 2 sentences.
LexSIR has been incorporated into the Deviation from Randomness (DFR) measure ( Amati et al., 2004 ) because it obtained the best results in CLEF X 2003 competition for Spanish; and because it obtained the best MAP Kaszkiel (0.4247). Our implementation of DFR baseline with stemming has obtained even better results than those reported in Amati et al. (2004) without query expansion: 0.4907 (Amati obtained 0.5510 with query expansion).
The training process has been carried out in consecutive experiments, in which the configuration parameters have been incorporated and tuned up one by one through the following experiments whose results are shown in Table 1 . (Exp.1) The TP is measured only by the number of phrases in order to check the benefits of our contribution 1 (Section 3.1 ), the detection of terms that must satisfy proximity restrictions. In this experiment, we set perc _ one to a value in {0.1, 0.25, 0.5, 0.75, 1}, and the best results were obtained with perc _ one 2.9% in R-precision (see Table 1 ). (Exp.2) The TP is measured by phrases or sentences depending on the lexical type, in order to prove contribution 2 (different TP schemes depending on the lexical type). In this experiment, we also set 0.5, 0.75, 1}, depending on the lexical type of the term that appears in a different phrase, and the best results were obtained with perc _ one =1 for proper nouns and numbers and perc _ one roborates our intuition about the differences between lexical types and makes our proposal more robust to different kinds of queries. At this point, our proposal improves the DFR baseline for 27 queries (maximum improvement of 19.8%), and wrongly for 6 queries (maximum decrease of 1.3%), whereas in (Exp.1) the relation was 26 (14.9%) vs. 12 ( 17.1%), therefore the lexical knowledge increases the improvements (27 queries vs. 26 with a maximum improvement of 19.8% vs. 14.9%) and it reduces the number of queries for which LexSIR works worse than the DFR baseline (6 vs. 12) with a lower decrease ( 1.3% vs. 17.1%). The reason for those queries for which our proposal decreases their average preci-sion is mainly due to parsing errors. For example, in the query the International Olympic Committee call for a suspension of the same phrase ( the International Olympic Committee call ), whereas it should have been parsed into two separated phrases ( the International Olympic Committee vs. call ).
 (Exp.3) Tuning of the configuration parameters for the penalization of the absence of terms, with the best results for perc _ absence = 0.2 for proper nouns; and perc _ absence = 0.1 for the remaining lexical types. Here, the damaged queries are mainly due to cases in which an abbreviation appears with its corresponding proper noun (e.g. Amnesty International cases that must be detected in future works. This experiment shows an important improvement of +5.1% in MAP and +7.7% in R-precision (see Table 1 ). (Exp.4) and (Exp.5) Tuning of the configuration parameters for the distance between query phrases (the best results were reached for kpenal _ of = 0.75 in Exp.4, and kpenal _ verb measures to phrases as well as terms).

The results in Table 1 show that our approach achieves a maximum improvement of 5.5% in MAP and 7.8% in R-precision, in comparison to our implementation of DFR baseline. This improvement is statistically significant as the Wilcoxon test ( a = 0.05) shows ( Hull, 1993 ). Moreover, it has improved the average precision of 40 queries (up to 300%) and it has only decreased the average precision of 8 queries (up to -1.9%), which proves the success of the inclusion of lexical knowledge to apply different TP schemes, obtaining a more robust approach that works well for most queries. We consider that in order or the detection of abbreviation references).

In order to compare LexSIR to other previous approaches, an adaptation of the Mitra et al. (1997) syntactic approach has been implemented and run on the same training queries with an improvement of 1.2% with regard our DFR baseline. This low improvement could be due to the complexity of the Spanish language (in English, they improved by 3.9%) and the length of the queries (they worked on short queries, although our implementation was performed only on the terms of the phrases parsed and not on all query terms). We have also implemented other well-known traditional state-of-the-art bag of words approaches (e.g. the well-known Okapi BM25), and we have also compared to other well-known state-of-the-art IR plat-4.2. Test of LexSIR
In this subsection, we will test the applicability of LexSIR with the same previously obtained configuration on different queries ), with an average of 21.4 phrases, 26 index terms and 3.8 sentences. first row shows the DFR baseline with these long queries, which obtains a higher MAP (0.5392) than the DFR with the train-ing queries (0.5054) despite the fact that they contain noisy terms that could harm the performance. For example, the narr second row shows the improvements of the application of LexSIR to these long queries, which are lower than those obtained in the training phase but statistically significant.
 Secondly, with regard to a different test corpus and different queries, an experiment with the queries used in the
CLEF X 2002 competition has been carried out. This corpus is made up of 215,738 news stories of the EFE of year 1994 and the documents have 175.3 index terms on average. There are 60 queries (from 91 to 140) and we have tested two different from the combination of the title, desc and narr version (row 5). The baseline is our implementation of DFR (row 3), which obtains similar results (0.5168) than the best system in the CLEF X 2002 competition ( Savoy, 2003 ) without query expansion: 0.5171. The improvements are again consistent, with a maximum improvement of +5% for long queries.
The corpus is the collection of the 169,477 news stories from the newspapers Los Angeles Times and Glasgow Herald, with 290.4 index terms on average. These English queries are shorter than the Spanish ones: title + desc (6.4 phrases, 9.6 index desc + narr )in Table 3 , in which LexSIR always improves the DFR baseline, with a maximum improvement of 7.6%.
We have also compared LexSIR with other well-known IR systems (JIRS, INDRI, Lucene and IRn), whose results are summed up in Table 4 , again with significant improvements. 4.3. LexSIR for Question Answering tasks
Question Answering (QA) can be defined as a task consisting in answering precise and arbitrary questions formulated by the user in a non-structured collection of documents. This task requires a deep understanding of the document by means of complex NLP tools, which are computationally expensive. Therefore, most QA systems use an IR system that filters the num-output. In this way, time of analysis is highly decreased and QA can be applied to huge corpora. However, the final QA pre-cision decreases because typical IR bag of words systems do not focus on understanding the document. As a result, many queries may never be answered because the IR tool does not return the document with the right answer. This means that, tial document retrieval and the performance of the overall QA system.
 As a proof of the importance of this problem, two editions of a Workshop focused on this issue,  X  X  X nformation Retrieval for
Question Answering (IR4QA) X  X , have been held in 2004 and 2008. Moreover, the CLEF competition celebrated a new exercise paragraph containing the answer from the document collection (neither exact answer nor multiple responses are required).
The conclusions of this CLEF exercise were that some systems did not analyze the questions at all, retrieval models, two systems used Boolean methods while the rest mainly used Okapi or a VSM-type model. Similarly, an IR4QA task has been performed at several editions of NTCIR Workshops.

In this section, we check that LexSIR also behaves better than bag of words approaches for QA tasks, in this case our DFR baseline. This experiment has been performed on the Spanish corpora used in the CLEF-QA competition: EFE 1994 and 1995; and the 800 queries are those used in CLEF competitions from 2003 to 2006. The comparison is carried out by means of the
Mean Reciprocal Rank (MRR) measure, 20 which is used for QA tasks. In this case, our implementation of DFR baseline obtains a MRR of 0.6362, whereas LexSIR obtains 0.6594, that is to say, a 3.6% of increase.

In order to analyze this increase, we present the MRR results in Table 5 for some types of queries second column presents the number of queries of each answer type within the 800 CLEF queries. The third one shows the MRR results with our DFR baseline. The following column shows LexSIR results, and the last column puts forward the percentage of improvement with regard to DFR baseline. The results are consistent between different answer types (there is only a decrease for year of a date type). 5. Conclusions
Our proposal overcomes the problems of traditional bag of word approaches (the assumption that each term occurs inde-pendently of the others) by the incorporation of the lexical and syntactic knowledge generated by a POS-tagger and a syn-tactic Chunker. It is based on theories of discourse structure, in which documents and sentences are segmented into sentences and entities. Moreover, it handles discourse references for each entity in order to measure Term Proximity (TP) measures between entities instead of between terms, as previous approaches do. It contributes to the state-of-the-art in the issues described in Section 3.1 , which are summarized next: 1. We adopt the scheme of parsing the query to obtain the set of query terms to calculate the TP information, instead of calculating TP between all possible combinations of query pairs, differing from previous approaches in: (1) Instead of full parsing, the text is chunked into sets of simple phrases in order to be more robust against errors from parsing. (2) We deal with complete phrases instead of just pairs of terms. (3) Our approach does not require complex measures of similarity between syntactic trees.
 2. We apply different TP measures depending on the lexical type of each query term. Similarly, the absence of phrase terms is also considered depending on its lexical type. Moreover, TP information is incorporated in the similarity measure by means of penalizations instead of adding the weight of the pairs of terms in a phrase. In this way, no additional config-uration parameters are required to avoid topic or phrase deviation, and it adjusts the weight provided by any IR scheme (such as Okapi or pivoted cosine). Finally, the distance between terms is measured by the number of phrases or sentences between both terms, instead of measuring it by the number of terms. 3. We apply TP measures to phrases as well as terms. with high MAP of over 0.5, with a more inflected language like Spanish, with shorter documents and shorter collections and with different query lengths (not only the title version of the queries). Moreover, it has been evaluated on Question Answering tasks obtaining significant increases too.

As future projects, we plan to introduce new Natural Language Processing tools such as an anaphora resolver (e.g. to de-tect references between an acronym and an entity: AI vs. Amnesty International ) or a Word Sense disambiguator. Moreover, we plan to apply our proposal on the query expansion phase of IR systems.
 Acknowledgement This research has been partially funded by the Valencia Government under Project PROMETEO/2009/119, and by the Spanish Government under Project Textmess 2.0 (TIN2009-13391-C04-01).
 References
