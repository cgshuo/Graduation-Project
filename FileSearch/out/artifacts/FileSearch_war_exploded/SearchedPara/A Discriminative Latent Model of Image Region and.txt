 Image understanding is a central problem in computer vision that has been extensively studied in image. As a by-product, our model also roughly localizes the image regions corresponding to the recognition, image annotation, and image segmentation. Most of the previous work uses fairly crude  X  X ag-of-words X  m odels, treating image features (ex-an image is over-segmented into a large number of segments, e ach segment typically only corre-manner. Some work [1, 3] tries to incorporate the mapping inf ormation into a generative model. modeling tools.
 and segment images by mapping image regions and textual word s to a latent meaning space using implicitly infer it during learning. variables in the model. A graphical illustration of our mode l is shown in Fig. 2. algorithm in [8], i.e. x = [ x of the four feature types, respectively. In the end, each reg ion x vector x type for this region.
 The annotation y of an image is represented as a binary vector y = ( y denote the vector y and  X  X nnotation term X  to denote each component y term y annotation terms for an image, i.e. R  X  P V annotation terms with Wordnet (see [17]).
 Given an image x and its annotation y , we assume there is an underlying unobserved many-to-one mapping to have the following conditions: (i) each image reg ion is mapped to at most one anno-inactive annotation term.
 More formally, we introduce a matrix z = { z to represent this mapping for an image with R regions: Z ( intuitions, e.g. they roughly correspond to different spor t categories in the data. annotation y using the following scoring function: model parameters have three parts  X  = {  X , X , X  } , and  X   X   X ( x , y , z ,s ) is defined as: The details of each of the terms in (4) are described in the fol lowing. Region-Annotation Matching Potential  X   X   X  ( x , z ) : This potential function measures the com-gion x ( components  X  = {  X  c } 4 size N ( written as: involve y since y is implicitly determined by z , i.e. y image x and a scene label s . Similarly, the parameters  X  consist of four parts  X  = {  X  c } 4 sponding to the four feature types, where an entry  X  c w of type c and the scene label s . This potential function is written as: corresponding to each of the scene label. Each component  X  t is a V  X  2 matrix, where  X  t compatibility of setting y for the scene label t . This potential function is written as:  X   X   X  ( y ,s ) = = The equivalence of (7a) and (7b) is due to 1 ( y which are easy to verify. for a new image x , i.e. y  X  = arg max optimization problem: (8) is the inner maximization over y and z for a fixed s , i.e.: formulate the problem as an LP, we first define the following: constant in the objective not involving y or z is omitted): we reformulate (11) as an integer linear program (ILP): max z ij  X  { 0 , 1 } to a real value in the range of [0 , 1] .
 Putting everything together, the LP relaxation of (11) can b e written as: max After solving (13) with any LP solver, we round z ( treat them as latent variables during learning.
 We adopt the latent SVM (LSVM) framework [7, 25] for learning . LSVMs extend the popular poses and actions [24], group activity recognition [9], etc .
 The latent SVM learns the model parameters  X  by solving the following optimization problem: P only involves the annotation y , because this is the only ground-truth label we have access t o. The problem in (14) can be equivalently written as an unconst rained problem: min and added to the piecewise quadratic approximation. The key of applying this algorithm to solve (15) is computing the two subgradients  X  detail below.
 First we describe how to compute  X  problem (called loss-augmented inference in the structura l SVM literature): Then it is easy to show that a subgradient  X  be re-formulated as: Using (17), it is easy to show that if we re-define b augmented inference (16) for a fixed s : Similarly, we can relax the problem to an LP using the same met hod in Sec. 3. Now we describe how to compute  X  problem: max calculated as  X  z can be solved by the following ILP: Similarly, we can solve (19) via LP relaxation by replacing t he integral constraint z a linear constraint 0  X  z used as the training set.
 We feed the training images and associated annotations (but not the ground-truth sport category P  X  by examining the co-occurrence counts of visual words and ps eudo-scene labels on the training and annotation terms with the mapping constraints ignored.
 We compare our model with a baseline method which is a set of li near SVMs separately trained information. Following [21], we use the F-measure to measur e the annotation performance. The comparison is shown in Table 1(a). Our model outperforms the baseline SVM method. We also list athlete ceiling floor grass rowboat sailboat sky sun tree wat er So we define other two baseline algorithms that use this extra information. For the second baseline algorithm (which we call pseudo-label+SVM ), we run k-means clustering this image. The predicted pseudo-labels of test images serv e as a clustering of those images. textual features. The textual features are obtained from th e pseudo-annotations. sults. Let  X  = {  X  truth categories. The NMI is defined as NMI( X  , D ) = I ( X ; D ) shown in Table 1(b). Our model outperforms other baseline me thods on the scene clustering task. scene label s , the parameter  X  s active for the scene label s . We sort the annotation terms according to  X  s  X  X osition X  visual word w  X  for this annotation term by w  X  = arg max the bottom of the image, while  X  X ky X  is preferred at the top of the image. approach like ours.
 In this work we have provided evidence that modeling these re lationships can improve image an-modifying the many-to-one correspondence assumption. (2) exploring the use of this model with tags.
