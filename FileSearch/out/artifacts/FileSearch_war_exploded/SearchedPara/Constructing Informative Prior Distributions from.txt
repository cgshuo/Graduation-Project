 Supervised learning approaches to text classification are in practice often required to work with small and unsystemat-ically collected training sets. The alternative to supervised learning is usually viewed to be building classifiers by hand, using a domain expert X  X  understanding of which features of the text are related to the class of interest. This is ex-pensive, requires a degree of sophistication about linguistics and classification, and makes it difficult to use combinations of weak predictors. We propose instead combining domain knowledge with training examples in a Bayesian framework. Domain knowledge is used to specify a prior distribution for the parameters of a logistic regression model, and labeled training data is used to produce a posterior distribution, whose mode we take as the final classifier. We show on three text categorization data sets that this approach can rescue what would otherwise be disastrously bad training situations, producing much more effective classifiers. I.2.6 [ Artificial Intelligence ]: Learning; H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing Algorithms, Experimentation knowledge-based, maximum entropy, MAP estimation Current affiliation Amazon.com Copyright 2006 ACM 1-59593-369-7/06/0008 ... $ 5.00.
Numerous studies show that effective text classifiers can be produced by supervised learning methods, including sup-port vector machines (SVMs) [12, 15, 34], regularized logis-tic regression [10, 34], and other approaches [15, 28, 34]. Most of these studies used thousands to tens of thousands of randomly selected training examples. In operational text classification settings, however, small training sets are the rule, due to the expense and inconvenience of labeling, or skepticism that efforts will be adequately repaid.
To learn from a handful of training examples, one must ei-ther use a sufficiently limited model class or some additional regularization penalty to effectively constrain the classifiers reachable with a small amount of data. Otherwise overfit-ting (learning accidental properties of the training data) will yield poor effectiveness on future data. On the other hand, strong constraints on the models limits the effectiveness of learnable classifiers.

This situation can be improved if one has advance knowl-edge of which classifiers are likely to be good for the class of interest. In text categorization, for instance, such knowledge might come from category descriptions meant for manual in-dexers, reference materials on the topics of interest, lists of features chosen by a domain expert, or many other sources.
Bayesian statistics provides a convenient framework for combining domain knowledge with training examples [3]. The approach produces a posterior distribution for the quan-tities of interest (e.g., regression coefficients). Per Bayes the-orem, the posterior distribution is proportional to the prod-uct of a prior distribution and the likelihood function .Inap-plications with large numbers of training examples, the like-lihood dominates the prior. However, with small numbers of training examples, the prior is influential and priors that reflect appropriate knowledge can provide improved predic-tive performance. In what follows we apply this approach with logistic regression as our model and text classification (in particular text categorization) as our application.
We begin by reviewing the use of logistic regression in text classification (Section 2), then discuss previous approaches to integrating domain knowledge in text classification (Sec-tion 3). Section 4 presents our Bayesian approach, which is simple and flexible. Section 5 describes our experimental methods, while Section 6 presents our results. We find on three test categorization test collections, using three diverse sources of domain knowledge, that domain-specific priors can yield large effectiveness improvements.
A logistic regression model is a linear model for the con-ditional log odds of a binary outcome, i.e. p ( y i =+1 |  X  , x i )= exp( where y i encodes the class of example i (positive = +1, negative =  X  1) and x ij is the value of feature j for example i (e.g. a within document term weight). We assume that j runs from 0 to d , the number of features, and that x i 0 for all i , i.e. the model has an intercept term.
A logistic regression training algorithm chooses a vector of model parameters  X  that optimizes some appropriate cri-terion function on a set of training examples for which y values are known. In the Bayesian MAP (maximum a pos-teriori) approach to logistic regression [10, 20], the criterion function is the sum of the log likelihood of the data and the log of the prior distribution of the regression coefficients, where p (  X  ) is the prior probability for parameter vector and we output a value  X   X  (which may or may not be unique) that maximizes l (  X  ). The prior, p (), can be any probability distribution over real-valued vectors. MAP estimation is neither necessarily superior nor inferior to other Bayesian approaches [29].

Logistic regression [8, 16, 20, 27, 34, 33] and the related probit regression [4] have been widely used in text classifi-cation. Regularization to avoid overfitting has been based on feature selection, early stopping of the fitting process, and/or a quadratic penalty on the size of regression coef-ficients. The last of these, also called ridge logistic regres-sion , can be interpreted as MAP estimation where p (  X  )isa product of univariate Gaussians with mean 0 and a shared variance [20]. Recently, Genkin et al. [10] showed MAP estimation with univariate Laplace priors, i.e. a lasso [30] version of logistic regression, was effective for text catego-rization.
Feature extraction is one use of domain knowledge (most famously in spam filtering [19]). Creating better features is good, but one would like to guide the learner to use them. Domain knowledge can also be used to choose which features to use (feature selection). An old example is stopwords [25, 31], often deleted in content-based text classification, but specifically included in authorship attribution. Another is relevance feedback, where words from the user query are usually required to appear in the learned model [2, 25]. The downside of feature selection is that it cannot reduce the impact of a term without discarding it entirely.

Relevance feedback may also treat textual queries as if they were positive examples for supervised learning (e.g. the  X  X de Regular X  algorithm [2]). Since a query (or other domain-informative text) may differ in characteristics such as length, nondomain vocabulary, and nontextual features from training documents, this approach is risky. Finally, some relevance feedback algorithms (e.g. Rocchio [2]) use a query to set initial values of some or all classifier param-eters, which are then updated by training data. This is a more flexible approach, but past algorithms have not dealt directly with negative predictors, strong predictors of uncer-tain polarity, or predictors for which we have varying degrees of domain knowledge.

Several papers have modified existing learning algorithms X  for naive Bayes [14, 17], logistic regression (fit with a boosting-style algorithm) [26], or SVMs [32] X  X o use domain knowl-edge in text categorization. All require users to convert knowledge about words into weighted training examples. Several heuristics are suggested for this weighting, but im-plicitly assume a substantial number of task documents (at least unlabeled ones) are available. A recent study [22] that upweights human-selected terms in SVM learning (by alter-ing document vectors) is similar in spirit to our work, though in an active learning context.

Closely related to using domain knowledge is mixing train-ing data from different sources in supervised learning ( do-main adaptation or transfer ). Cohen and Kudenko stud-ied the learning of propositional classifiers for email filter-ing when the bulk of the training data was from a differ-ent source than the test data [6]. They found that forbid-ding the use of negative predictors improved effectiveness. Gabrilovich and Markovitch used a combination of feature generation, feature selection, and domain adaptation from a large web directory to improve classification of diverse doc-uments [9]. Chelba and Acero [5] used out-of-task labeled examples in logistic regression training of a text capitalizer, and used the resulting MAP estimate as the mode vector of a Bayesian prior for training with in-task examples. Our work has similarities to Chelba and Acero X  X , as well as to non-textual uses of Bayesian priors to incorporate knowl-edge ([18], and citations therein).
Given the wide use in text classification of Gaussian pri-ors, and the recent success of Laplace priors, we take these as our starting point. The univariate Gaussian and Laplace distributions each have two parameters, so a product of such distributions for d features and an intercept gives 2 d +2 hy-perparameters. The Gaussian parameters are the mean  X  j and the variance  X  2 j . The Laplace parameters are the mean  X  , and the scale parameter  X  j , corresponding to a variance of 2 / X  2 j . For both distributions the mean  X  j is also the mode, and in this paper we will refer to the mode and variance as the hyperparameters for both the Gaussian and Laplace dis-tributions. The mode specifies the most likely value of  X  while the variance specifies how confident we are that  X  j near the mode.

As for domain knowledge, our interest is in a range of pos-sible clues as to which words are good predictors for a class. Focused lists of words generated specifically for classification are of interest, but so are reference materials that provide noisier evidence. We refer to all these sources as  X  X omain knowledge texts, X  and assume for simplicity there is exactly one domain knowledge text for each class (more can easily be used). We call a set of such texts a  X  X omain knowledge corpus. X  For a given class, we distinguish between two sets of words. Knowledge words (KWs) are all those that oc-cur in the domain knowledge text for the class of interest. Other words (OWs) are all words that occur in the training documents for a particular run, but are not KWs. Table 1 summarizes the methods dis cussedinthissection.
Text classification research using regularized logistic re-gression has usually set all prior modes to 0, and all prior variances to a common value (or has used the non-Bayesian equivalent). Some papers explore several values for the prior variance [16, 34], others use a single value but do not say how it was chosen [20, 33], and others choose the variance by cross-validation on the training set [10]. We used cross-validation (Section 5.1.1) to choose a common prior variance for OWs. In our  X  X o DK X  baseline (Table 1), all words are OWs.

Another simple baseline is to create X copies of the prior knowledge text for a class and add these copies to the train-ing data as additional positive examples ( X  X K examples X  in Table 1). We applied the same tokenization and term weighting (Section 5.3) to these artificial documents as to the usual training documents. We tested a range of values for X , but include results only for the best value, X =5.
Our four methods for using domain knowledge to spec-ify class-specific hyperparameters begin by giving OWs a prior with  X  j = 0 and a common variance  X  2 chosen by cross-validation. KWs are then given more ability to af-fect classification by assigning them a larger prior mode or variance than OWs. All four methods use a heuristic con-stant C DKRW , the  X  X omain knowledge relative weight X , to control how much more influence KWs have. This constant can be set manually or, as in our experiments, chosen by cross-validation on the training set (Section 5.1.1).
Two of our methods use the entire set of domain knowl-edge texts in determining how significant each word in the text for a particular class is. As a heuristic measure of sig-nificance, we use TFIDF weighting (Section 5.3) within the domain knowledge corpus: where We now describe the methods.
One view is that KWs will tend to have more weight (i.e. parameter values farther from 0) than OWs in good logistic regression models for the class, but could be either positive or negative predictors. That suggests the prior on a KW should have a larger variance than the prior on an OW. Methods Var and Var/TFIDF (Table 1) make the prior variances for KWs a multiple of the variance,  X  2 ,forOWs. This multiple is the same for all KWs in Method Var: but is proportional to our heuristic measure of term signifi-cance (Equation 1) in Method Var/TFIDF: These methods use a prior mode of 0 for both OWs and KWs.
Another view of a domain knowledge text is that it con-tains words which are likely to be positive predictors of class membership, i.e. that KWs will tend to have param-eter values greater than 0 in good logistic regression mod-els for the class. Along these lines, Methods Mode and Mode/TFIDF make the prior mode for a KW greater than 0, in contrast to the mode of 0 used for OWs. Method Mode gives the prior for every KW the same mode: while Method Mode/TFIDF makes the prior modes propor-tional to term significance: Both methods use cross-validation to choose a common vari-ance for both OWs and KWs.

While mode-setting may seem more natural than variance-setting, it carries more risks. If a term does not occur in the training data, then the MAP estimate for the correspond-ing parameter is identically the prior mode. With nonzero prior modes and a tiny training set, we risk hardwiring many untested parameter choices into the final classifier.
In this section, we describe our experimental approach to studying the use of domain knowledge in logistic regression.
As discussed in Section 3, our interest was in domain knowledge techniques that can be used with existing super-vised learning algorithms. Here we discuss the particular implementations used in our experiments.
We trained and applied all logistic regression models us-ing Version 2.04 of the BBR (Bayesian Binary Regression) package [10] 1 . BBR supports Gaussian and Laplace priors with user-specified modes and variances. With methods No DK and DK Examples we used prior modes of 0 and chose a common prior variance,  X  2 , from this set of possibilities: The BBR fitting algorithm chose the prior variance that maximized the cross-validated posterior predictive log-likeli-hood for each training set.

For methods where priors are class-specific, we used cross-validation external to BBR to choose a pair ( C DKRW , X  2 http://www.stat.rutgers.edu/  X  madigan/BBR/ a prior with mode 0 and variance  X  2 . from the cross product of a set of values for C DKRW and the above set of values for  X  2 . For Methods Var and Var/TFIDF, the C DKRW values tried were 2, 5, 10, 20, 50, 100, and 10000. For Methods Mode and Mode/TFIDF, the C DKRW values were 0.5, 1, 2, 3, 4, 5, 10, 20, 50, 100, and 10000. As with BBR X  X  cross-validation, our external cross-validation chosen the pair that maximized cross-validated posterior predictive log-likelihood on the training set.
As a baseline to ensure that logistic regression was pro-ducing reasonable classifiers without domain knowledge, we trained support vector machine (SVM) classifiers on all train-ing sets. SVMs are one of the most robust and effective ap-proaches to text categorization [12, 13, 15, 28]. In our exper-iments, we used Version 5.0 of SVM Light software [12, 13] All options were kept at their default values. Keeping the option at its default meant that SVM Light used the default choice ( C =1 . 0 for our cosine normalized examples) of the regularization parameter C . We also generated results with the regularization parameter chosen by cross-validation, but these were inferior and are not included here.
Our text classification experiments used three public text categorization datasets for which publicly available domain knowledge texts were available. We chose, as our binary classification tasks, categories with a moderate to large num-ber of positive examples. This enabled experimentation with different training set sizes.
This collection of full text biomedical articles was used in the TREC 2004 genomics track categorization experi-ments [11]. 3 The genomics track itself featured only few (and somewhat atypical) categorization tasks. However, be-cause all the articles are indexed in the National Library of Medicine X  X  MEDLINE system, they have corresponding MEDLINE records with manually assigned MeSH (Medical http://svmlight.joachims.org/ http://trec.nist.gov/data/t13 genomics.html Subject Headings) terms. We posed as our text classification tasks predicting the presence or absence of selected MeSH headings.

Documents. We split the Bio Articles documents into three 8-month segments. We used the first segment for the training and the last segment for testing. The middle seg-ment was reserved for future purposes and was not used in the experiments reported here. Training sets of various sizes were drawn from the training population of 3,742 arti-cles (period: 2002-01-01 to 2002-08-31), and classifiers were evaluated on the test set of 4,175 articles (period: 2003-05-01 to 2003-12-31).

Categories. We wanted a set of categories that were closely related to each other (to test the ability of domain knowledge to support fine distinctions) and somewhat fre-quent on the particular biomedical journal articles we had available. MeSH organizes its headings into multiple tree structures, and we choose the A11 subtree (MeSH descrip-tor:  X  X ells X ) to work with. This subtree contains 310 dis-tinct headings, and we chose to work with the 32 that were assigned to 100 or more of our documents. Note that when deciding whether a MeSH heading was assigned to a docu-ment, we stripped all subheadings from the category label.
Domain Knowledge. Each MeSH heading has a de-tailed entry provided as an aid to both NLM manual in-dexers and users of MEDLINE. Figure 1 shows a portion of one such entry. We used as our domain knowledge text for a category all words from the MeSH Heading , Scope Note , Entry Term , See Also ,and Previous Indexing fields. En-tries were taken from the 2005 MeSH keyword hierarchy [1], downloaded in November 2004.
Our second dataset was the ModApte subset of the Reuters-21578 test collection of newswire articles [15]. 4
Documents. The ModApte subset contains 9603 and 3299 Reuters news articles in the training set and test set, respectively.

Categories. Following Wu and Srihari [32] (see below) http://www.daviddlewis.com/resources/testcollections/ reuters21578/ Figure 1: A portion of MeSH entry for the MeSH heading  X  X eurons X .
 Figure 2: Keywords used as prior knowledge for the ModApte Top 10 collection [32]. we used the 10  X  X opic X  categories with the largest number of positive training examples.

Domain Knowledge. In their experiments on incor-porating prior knowledge into SVMs, Wu and Srihari [32] manually specified short lists of high value terms for the top 10 Topic categories. We used those lists (Figure 2) as our domain knowledge texts. Note that due to the small number of these texts and their highly focused nature, IDF weights within the domain knowledge corpus had almost no impact, so methods Var/TFIDF and Mode/TFIDF behaved almost identically to methods Var and Mode, respectively.
The third dataset was based on RCV1-v2, a test collection of 804 , 414 newswire articles [15]. 5
Documents. For efficiency reasons, we did not use the full set of 804 , 414 documents. Our test set was the 120 , 076 documents dated 20-December-1996 to 19-February-1997. For a large training set, we used the LYRL2004 ([15]) train-ing set of 23,149 documents from 20-August-1996 to 31-August-1996. Small training sets were drawn from a training population of 264 , 569 documents (20-August-1996 to 19-December-1996). The remaining documents was set aside for future use.

Categories. We selected a subset of the Reuters Region categories whose names exactly matched the names of geo-graphical regions with entries in the CIA World Factbook (see below) and which had one or more positive examples in http://www.ai.mit.edu/projects/jmlr/papers/volume5/ lewis04a/lyrl2004 rcv1v2 README.htm Figure 3: A portion of CIA WFB (1996 edition) entry for the category  X  X fghanistan X . our large (23 , 149 document) training set. There were 189 such matches, from which we chose the 27 with names be-ginning with the letter A or B to work with, reserving the rest for future use.

Domain Knowledge. The domain knowledge text for each Region category was the corresponding entry in the CIA World Factbook 1996. 6 Figure 3 shows a portion of the entry for  X  X fghanistan X . The HTML source code of the CIA WFB was downloaded in June 2004. The formatting of the entries did not make it easy to omit field names and boilerplate text. We instead simply deleted (in addition to HTML tags) all terms that occurred in 10% or more of the entries.
Text from each training and test document was converted to a sparse numeric vector in SVM Light format (also used by BBR). The Bio Articles documents were in XML for-mat. We concatenated the contents of the title ( &lt; atl &gt; ), subject ( &lt; docsubj &gt; ), and abstract ( &lt; abs &gt; )elementsand deleted all internal XML tags. For ModApte, we used the concatenation of text from the title ( &lt; TITLE &gt; ) and body ( &lt; BODY &gt; ) SGML elements of each article. For the RCV1 A-B Regions collection, we concatenated the contents of the headline ( &lt; HEADLINE &gt; )andtext( &lt; TEXT &gt; ) XML ele-ment of each article.
 For all datasets, text processing used the Lemur 7 utility ParseToFile. This performed case-folding, replaced punc-tuation with whitespace, and tokenized text at whitespace boundaries. The Lemur index files were then converted to document vectors in SVM Light format.

In processing text for the Bio Articles and the ModApte datasets, the Porter stemmer [21] supplied by Lemur and the SMART [23] stoplist were used in conjunction with the Lemur utility ParseToFile. 8 For the RCV1-v2 dataset we used a convenient pre-existing set of document vectors we had prepared using Lemur without stemming or stopping. Domain knowledge text corpora were processed in the same fashion as the corresponding task documents. http://www.umsl.edu/services/govdocs/wofact96/ http://www-2.cs.cmu.edu/  X  lemur ftp://ftp.cs.cornell.edu/ pub/smart/english.stop or http://jmlr.csail.mit.edu/papers/volume5/lewis04a/ lyrl2004 rcv1v2 README.htm Table 2: Type of domain knowledge texts and size of domain knowledge corpus for each of categorization datasets.

Within document weights were computed using cosine-normalized TFIDF weighting [24]. The initial weight of term t in document d i , w ij was Here N is the number of documents in the training popula-tion, f ij is the frequency of term t j in document d i ,and n is the number of training population documents containing term t j . Note, as in Section 4.2, that we use the Lookahead IDF variant of IDF weighting [7]. Cosine normalization was then applied to the TFIDF values.
We evaluated classification effectiveness using the F1 mea-sure (harmonic mean of recall and precision) [15, 31], with macroaveraging (average of per-category F1 values) across categories. Both BBR and SVM Light produce linear clas-sifiers with thresholds intended to minimize error rate, so, after training, we tuned the thresholds to maximize observed F1 on the training data, while leaving other classifier param-eters unchanged.
Our primary hypothesis was that using domain knowledge texts would greatly improve classifier effectiveness when few training examples are available, and not hurt effectiveness with large training sets. We also believed, given the di-verse and non-document-like forms of the domain knowl-edge texts, that using them to specify prior distributions in a Bayesian framework was not only more natural, but more effective, than pretending they were additional training ex-amples.

Table 2 summarizes the types of domain knowledge used, and the number of domain knowledge texts used to compute significance values for the Var/TFIDF and Mode/TFIDF methods. The number of categories used in the experiments was 32, 10 and 27 for the Bio Articles, ModApte and RCV1 collections, respectively.
This experiment trained classifiers on each collection X  X  large training set. Table 3 presents macroaveraged F1 re-sults for the three test collections. As found elsewhere [10], SVMs and lasso logistic regression show similar effectiveness, and both dominate ridge logistic regression. We note that our macroaveraged F1 value for SVMs on ModApte Top 10 (86.55) is similar to that found by Wu &amp; Srihari (ap-proximately 83.5 on a non-random sample of 1,024 training examples, from the graph in Figure 3 [32]) and Joachims (82.5 with all 9,603 training examples, computed from his Figure 2 [12]).

Method DK Examples (using domain knowledge texts as artificial positive examples) had little impact on any learn-ing algorithm with these large training sets. The four meth-ods using prior probability distributions had little impact on lasso logistic regression, but gave a substantial benefit to ridge logistic regression on the two datasets with the lowest frequency categories.
The small training sets available in practical text classifi-cation situations are produced in a variety of unsystematic ways, making it hard to define a  X  X ealistic X  small training set. We present results on three definitions that exhibit the range of properties we have seen using other definitions.
In this experiment we selected random training sets of 500 examples from the training population. The resulting training sets had 2 to 139 positive examples for categories in the Bio Articles collection, 9 to 184 positive examples for categories in the ModApte Top 10 collection, and 0 to 22 positive examples for categories in the RCV1 A-B Regions collection.

Table 4 provides the results. Effectiveness is lower than with large training sets, and the effect of the differing class frequencies is obvious. Lasso logistic regression is notably more effective on the small training sets than SVMs and ridge logistic regression. Method DK Examples gave im-provements on two of three collections, but hurt the third. The Bayesian prior based methods, in contrast, always im-proved logistic regression results. For ridge logistic regres-sion, the improvement was up to 1500%.
Operational text classification tasks often originate with a handful of known positive examples. We simulated this by randomly selecting 5 positive examples of each class from the training population, and adding 5 additional examples randomly selected from the remainder without knowledge of class labels (Table 5). Since 5 positive examples is more than occurs in random samples of 500 examples for some classes, effectiveness is sometimes be tter and sometimes worse than in Table 4. Method DK Examples has a large impact with these tiny training sets, but the impact is sometimes good and sometimes bad. The prior based methods uniformly improve ridge regression (up to 130%) and usually improve lasso regression, though the risky Mode method hurts lasso substantially in two of the conditions.
In a variation on the previous approach, we instead com-bined each of 5 random positive examples for each class with its nearest (based on highest dot product) negative neighbor. The theory was that someone attempting to quickly build a small training set might end up with positive and  X  X ear miss X  examples. It is hard to know if this is true but, surpris-ingly, effectiveness (Table 6) was lower than when positives were supplemented with random examples (Table 5). In any case, we again see DK Examples having a large but unsta-ble effect. The prior-based methods uniformly, sometimes greatly, improve ridge (up to 127%) and give small decre-ments (maximum 3.6%) to large improvements (maximum 79.7%) for lasso. test collections using large training sets.
 test collections using 500 random examples in training sets.
Domain knowledge, in any form, generally had little ef-fect with large training sets. The exception was ridge logis-tic regression, which was substantially improved on the two collections where some categories had few positives. Ridge regression performed surprisingly poorly, given its popular-ity. AcaveatisthatmanyModApteandRCV1Regions categories have a dominant single predictor, a situation that favors lasso.

Treating domain texts as artificial training examples had an erratic impact, sometimes improving and sometimes sub-stantially harming effectiveness. Converting domain texts to priors, on the other hand, almost always improved effective-ness (37 of 48 experimental conditions for lasso, and 48 of 48 for ridge from its poor baseline). As expected, mode-setting was risky, with method Mode proving either the best or, more commonly, the worst of the four prior setting meth-ods 21 of 24 times. Where we had nontrivial domain corpus TFIDF weights (Bio Articles and RCV1 A-B Regions), they proved surprisingly useful. Var/TFIDF beat Var in 14 of 16 such conditions, and Mode/TFIDF beat Mode in 16 of 16. Other source of term quality information, such as stoplists or task-document IDFs, would likely prove useful as well.
Under a view that domain knowledge should do no harm we recommend either Var/TFIDF, which reduced effective-ness vs. No DK in only 1 of 24 conditions (by 2.7%), or Mode/TFIDF, which reduced effectiveness in only 3 of 24 conditions (by a maximum of 1.7%). Both usually provided large improvements.
We have presented an initial, but highly effective, strat-egy for combining domain knowledge with supervised learn-ing for text classification using Bayesian logistic regression. On three data sets, with three diverse sources of domain knowledge, we found large improvements in effectiveness, particularly when only small training sets are available. We are continuing this work in many directions, including ex-ploring the impact of variability in the choice of both small training sets and domain knowledge texts.

Beyond that, our research program is to recast many IR heuristics (stopword lists, stemming, term weighting, etc.) as appropriate priors, with the goal of using simple binary text representations, along with priors for which a somewhat sophisticated user could have meaningful numeric intuitions. Published statements such as  X  X ating food X increases your chance of heart disease by Y% X  are often based on param-eters of fitted logistic regression models. It does not seem impossible to have intuitions of this form about words in text classification.
 The work was supported by funds provided by the KD-D group for a project at DIMACS on Monitoring Message Streams, funded through National Science Foundation grant EIA-0087022 to Rutgers University. The views expressed in this article are those of the authors, and do not necessarily represent the views of the sponsoring agency. [1] Medical Subject Headings  X  Home Page, 2005. [2] C. Buckley, G. Salton, and J. Allan. The effect of [3] B. Carlin and T. Louis. Bayes and Empirical Bayes [4] K. Chai, H. Chieu, and H. Ng. Bayesian online [5] C. Chelba and A. Acero. Adaptation of maximum [6] W. Cohen and D. Kudenko. Transferring and [7] A. Dayanik, D. Fradkin, A. Genkin, P. Kantor, [8] N. Fuhr and U. Pfeifer. Combining model-oriented [9] E. Gabrilovich and S. Markovitch. Feature generation [10] A. Genkin, D. Lewis, and D. Madigan. Large-scale [11] W. Hersh, R. Bhuptiraju, L. Ross, A. Cohen, [12] T. Joachims. Text categorization with support vector [13] T. Joachims. Learning to Classify Text Using Support [14] R. Jones, A. McCallum, K. Nigam, and E. Riloff. [15] D. Lewis, Y. Yang, T. Rose, and F. Li. RCV1: A new [16] F. Li and Y. Yang. A loss function analysis for [17] B. Liu, X. Li, W. Lee, and P. Yu. Text classification [18] D. Madigan, J. Gavrin, and A. Raftery. Eliciting prior [19] T. Meyer and B. Whateley. SpamBayes: Effective [20] K. Nigam, J. Lafferty, and A. McCallum. Using [21] M. Porter. An algorithm for suffix stripping. Program , [22] H. Raghavan, O. Madani, and R. Jones. Interactive [23] G. Salton, editor. The SMART Retrieval System: [24] G. Salton and C. Buckley. Term-weighting approaches [25] G. Salton and M. McGill. Introduction to modern [26] R. Schapire, M. Rochery, M. Rahim, and N. Gupta. [27] H. Schutze, D. Hull, and J. Pedersen. A comparison of [28] F. Sebastiani. Machine learning in automated text [29] R. Smith. Bayesian and frequentist approaches to [30] R. Tibshirani. Regression shrinkage and selection via [31] C. Van Rijsbergen. Information Retrieval .
 [32] X. Wu and R. Srihari. Incorporating prior knowledge [33] J. Zhang and Y. Yang. Robustness of regularized [34] T. Zhang and F. Oles. Text categorization based on
