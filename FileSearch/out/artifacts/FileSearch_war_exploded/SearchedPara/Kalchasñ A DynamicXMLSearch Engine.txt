 g This pap er outlines the system arc hitecture and the core data structures of Kalc has, a fulltext searc h engine for XML data with emphasis on dynamic indexing, and iden ti es fea-tures worth demonstrating. The concept of dynamic index implies that the aim is to re ect the creation of, deletion of, and up dates to relev ant les in the searc h index as early as possible. This is achiev ed by a num ber of tec hniques, in-cluding ideas dra wn from partitioned B-T rees and inverted indices. The actual rank ed retriev al of documen t is imple-men ted with XML-sp eci c query operators for lowest com-mon ancestor queries.
 A live demonstration will discuss Kalc has' beha viour in typ-ical use cases, suc h as interactiv e editing sessions and bulk loading large amoun ts of static les as well as querying the con ten ts of the indexed les; it tries to clarify both the short-comings and the adv antages of the metho d.
 H.3.3 [ Information Searc h and Retriev al ]: Searc h pro-cess Algorithms, Performance Partitioned indexes, index cascades, fulltext searc h It has been sho wn that user access patterns to resources suc h as les exhibit patterns of self-similarit y (see, e.g. , [6]). The system presen ted in this pap er puts this prop ert y to use and works towards non-in trusiv e, dynamic main tenance of fulltext indices on desktop systems. For example, the con ten ts of a documen t edited by a user in a word pro cessor are made available for querying as soon as the user saves the documen t to disk. The tec hnical challenge is now to Cop yright 2005 ACM 1 X 59593 X 140 X 6/05/0010 ... $ 5.00. up date the fulltext index to re ect the changes accurately and timely while consuming as few resources as possible; at the same time, resource consumption peaks are to be avoided since they are likely to get in the way of a user's editing exp erience by causing interruptions and by rendering the system unresp onsiv e.
 To explore to what degree this can be achiev ed, we imple-men ted the dynamic XML searc h engine Kalc has (named after a Greek prophet and seer from Homer's Iliad) whic h dra ws from a num ber of tec hniques found in the literature. We com bine the ubiquitous concept of inverted les (see, e.g. , [15]) with ideas behind the notion of partitioned B-Tree [10]; the former allo ws us to retriev e ecien tly the oc-currences of giv en keyw ords, whereas the latter are up date-ecien t (at the exp ense of query time and a merge phase for up dates). The idea is to main tain a cascade of partitions of di eren t sizes whic h corresp ond to sets of les with di eren t mo di cation probabilities. The rst partition is a transien t, memory-residen t searc h tree and considered very dynamic; this partition is supp osed to be the index for documen ts whic h have a high probabilit y of being mo di ed, i.e. , docu-men ts whic h have recen tly been mo di ed. When a user stops working on a documen ts and, consequen tly , the documen t is not mo di ed anymore, the system migrates the documen ts from the transien t index to a more permanen t index; thus, as documen ts age, they are migrated to di eren t disk-residen t indices. The conceptual justi cation for this approac h is that the power laws underlying the self-similarit y of access patterns mak e it very probable that up dates apply only to a fraction of the les in the index in a giv en time interv al. If these les are kept in an up date-friendly dynamic index par-tition as long as they are likely to be up dated, then the cost of keeping the index up-to-date can be exp ected to remain reasonable in terms of both time and memory . Furthermore, it is more cost-ecien t to index a documen t whic h is very likely to change anyw ay in an up date-ecien t index, whic h is speci cally design to re ect changes quic kly . Since data are distributed over sev eral partitions or indexes, keyw ord queries consequen tly must consult all partitions to nd all occurrences and to guaran tee perfect recall. In our system, this amoun ts to lookups being executed against three in-dexes: the aforemen tioned memory-residen t index and two disk-residen t indexes. The sizes of the disk-residen t indexes again re ect the probabilit y of documen ts being up dated. A `dynamic B-T ree' con tains documen ts whic h have aged enough not to qualify for the in-memory index anymore; it is usually small enough to be cac hed in-memory as well. The searc h cost is thus exp ected to be dominated by the lookup in the second disk-residen t index, a `large', mostly static B-Tree whic h indexes the bulk of the documen ts whic h are very unlik ely to be up dated. The setting we see our work most useful in are desktop searc h engines, indexing les on the local hard disks, and searc h engines for small intranets; we do not have exp erience in larger-scale systems. The demon-stration will thus feature typical editing tasks suc h as using word pro cessors or bro wsing the Internet as well as setting up the indexes by bulk loading. For querying, we use the concept of me et or lowest common anc estor (LCA) queries whic h has been sho wn to deliv er intuitiv e results in man y cases [11, 18] and whic h can be implemen ted ecien tly . Con tributions. This work mak es the follo wing con tri-butions: (1) Interactiv e, dynamic fulltext indices are fea-sible in small-scale systems using partitioned searc h trees. (2) Careful multi-stage index design mak es unobtrusiv e in-dexing feasible for typical workloads, in terms of CPU, I/O, and index size. (3) Lowest common ancestor queries [18] com bined with basic ranking [11] deliv er good results in our envisioned setting.
 Scenarios. Before we go into more detail of the system, we list some example scenarios with di eren t characteristics whic h should all be tak en care of by a dynamic searc h engine. Wor d Pr ocessor User. This scenario is characterised by frequen t saving and auto-sa ving of similar les. Due to the high rate of up dates, a promising strategy would be to keep the resp ectiv e indices in main-memory , and mak e them per-sisten t only when is it almost certain that the user stopp ed working on documen ts.
 News Feeds. In this scenario, Web serv ers are chec ked for up dated XML les con taining news items; the up date rates are lower than in the previous case; typically , up dates occur every ten min utes, every hour, or at an even lower frequency . Archives. Do cumen ts whic h hardly ever change are consid-ered arc hiv es. We exp ect the bulk of documen ts to fall into this category according to the power laws men tioned above. Arc hiv es should be mostly static; the few up dates we an-ticipate are mostly additions of complete les and rarely deletions.
 These scenarios should illustrate the lifecyles of les of whic h a dynamic searc h engine should tak e care. The remainder of this pap er is structured as follo ws: After intro ducing some preliminaries, we detail on the motiv ation behind building Kalc has. We describ e then Kalc has' system arc hitecture and zoom in on some tec hnical details. Before concluding, we brie y discuss some related work and outline the live demon-stration to accompan y this pap er. In this pap er, we are primarily concerned with indexing col-lections of XML documen ts. We will use the documen t in Listing 1 as a running example. Eac h XML documen t is as-sociated with a unique iden ti er suc h as a le name or, more generally , a Uniform Resource Iden ti er (URI); in terms of data structures, an XML documen t is view ed as directed graph G = ( V;E;r; oid ; el ; att ): V = V E [ V T is a set of vertices or (syn tax) nodes with a prominen t mem ber r 2 V , the documen t root. E V V is a set of edges whic h induces a spanning tree on V ; V is partitioned into two sets V E , the elemen t nodes, and V T , the text nodes. Addi-tionally , there are the follo wing functions: oid : V ! OID assigns unique iden ti ers o 2 OID to nodes; giv en an alpha-bet , el : V E ! + maps elemen t nodes to names, while att : V E ! + assigns an asso ciation list of word pairs called attributes to elemen t nodes; text : V T ! + assigns text elemen t (CD ATA or PCD ATA) to text nodes.
 For any v 2 V , let path ( v ) = [ oid ( r ) ;:::; oid ( v )] be the Dewey numb er [7] of v , where [ r;:::;v ] is the sequence of nodes found on the (unique) path from the documen t root r to v . For two Dew ey num bers d = [ d 1 ;:::;d n ] and d [ d ;:::;d 0 m ], the pre x function pref is de ned in the natural way: where l = min f i j d j = d 0 j ;j i min( n;m ). The lowest common ancestor (LCA) of two nodes d and d 0 is then the longest common pre x, calculated using pref ( d;d 0 ). For illustration, consider the follo wing (incomplete) docu-men t tree: Some relev ant paths in the tree are: The follo wing algorithm is used to compute the lowest com-mon ancestors of a sorted list of paths P . Note that the al-gorithms is a naiv e implemen tation that does not tak e into accoun t documen t statistics [23].
 Meet-Scan ( P ) 1 R ; 2 v P:PeekTop () 3 while ( P 6 = ; ) do 4 u P:PopTop () 5 d prox ( U;V ) 6 v lcp ( U;V ) 7 Rule 1: 8 if ( d = 1 ) then 9 R R ^f v g 10 v u 11 Rule 2: 12 if ( d &lt; k ) then 13 R R [f v g 14 v u 15 Rule 3: 16 if ( d k ) then 17 if ( v term \ u term = ; ) then 20 if ( v term \ u term 6 = ; ) then 22 Rule 4: 23 if ( v rank 10) then 24 R R [f v g 25 v u 26 return R This pseudo code summarises the evaluation of lowest com-mon ancestor queries once the results of the fulltext searc h are collected in the variable P . The algorithm works by it-erating through the sorted input set P until reac hing the end (lines 3{25). During eac h iteration, the next posting is read ( u ) and a temp orary LCA ( v ) is calculated. When-ever one of the rules applies to the temp orary LCA ( v ), v is added to the result set R . After adding v to the result set, we cop y the value of u into v and the iteration con-tin ues. By computing LCA in this way, we incremen tally iden tify relev ant elemen ts, by tra versing from the most spe-ci c elemen ts (those returned by the inverted index) up to more generally elemen ts (con taining multiple occurrences of terms). In summary , the follo wing rules are used to com-bine fulltext query results: (1) Output the curren t meet if the Dew ey path from the input set originates from another documen t. (2) Output the curren t meet if the meet between p and p 0 has a depth shorter than k . This happ ens when two nodes have nothing else in common but the root and the documen t ID. (3) To pro vide basic ranking we calculate how man y entities from the input set are intersected by the curren t meet ( v ), this is done by increasing the hitcounter comp onen t of v . If the term of the newly loaded Dew ey path is not already con tained in v we increase the hitcounter by 10 otherwise we increase by 1. This is done to rank nodes that con tain man y terms higher than nodes con taining few terms. (4) When we have a sucien t amoun t of elemen ts of the curren t meet ( v ) we output it and promote u to our new working meet.
 Figure 1 pro vides an overview of the mo dular structure of Kalc has. The operating system layer noti es the Kalc has kernel of interesting events suc h as le up dates; the rele-vant les are then analysed by plugins according to their le typ e. Ev entually , applications can access the query results in various ways.
 Ev entually , the concept of inverte d index is interpreted in this pap er as follo ws: For a set of documen ts, the inverted in-dex maps terms extracted from the documen ts to the Dew ey paths whic h are obtained when the documen ts are parsed. Th us eac h term is mapp ed to a list of occurrences of typ e Dew ey path. The strategy used to implemen t inverted lists is a straigh t-forw ard com bination of STL [16] multi-maps, the B-T ree data structure of Berk eley DB [20], and various compression tec hniques. Both our motiv ation and imple-men tation are based on the ideas presen ted in [9, 10], where also a performance analysis of partitioned indexes in SQL environmen ts is pro vided. This section pro vides more details on wh y Kalc has is de-signed the way it is designed. Of course, from a soft ware-engineering poin t of view it would be attractiv e to re-use existing data structure for the pro ject. Candidates would be disk-residen t data structures, suc h as B-T rees [15], whic h have been used e ectiv ely for implemen ting inverted les. They were also starting poin t for our researc h. Before going into more details about wh y they did not work as well as we exp ected, we rephrase our goals.
 Our main goal was to create an indexing system that works well for `standard' workloads on desktop mac hines. `Stan-dard' workloads, according to our informal de nition, imply that up dates are highly clustered/lo calised on the le lev el and can be mo delled using a power law [6]. By `works well' we mean that, for example, a user of a word pro cessor, when hitting the `sa ve documen t' button, does not receiv e a sev ere performance penalt y because of using the indexing service. This resulted in the litm us test of taking a 300KB documen t and being able to add it in less than one second to the index. Ev entually , we managed to achiev e this goal but not with con ventional tec hnology .
 This was for a num ber of reasons. (1) There seems to be a tension between space-compact storage formats and up date-friendly storage formats. The former tend to be more CPU-intensiv e than the latter. For frequen t and clustered up-dates, one would prefer an up date-friendly storage format, and there is only little poin t in spending resources on render-ing data to a space-ecien t represen tation if these data are very likely to become stale in the very near future anyw ay. (2) Additionally , clustered up dates imply that the ma jorit y of data are rarely up dated, and thus static. For these data, one would a compact storage format. One the one hand, this is simply to save disk space and to reduce look-up time. On the other hand, the index merge operation is dominated by the size of the disk-residen t index(es). Th us, if a compact index is if signi can tly smaller size than an up date-friendly index and the merge operation is I/O-b ound, the merge of the two indexes can be executed in signi can tly less time. (3) Our exp erimen ts sho wed that B-T rees (at least the kind we used) in an up date-in tensiv e environmen t degenerate in terms of localit y [17]. This implies that the leaf pages in a disk-residen t B-T ree should be allo cated (as far as possi-ble) on consecutiv e disk blo cks and disk trac ks so that with one mo vemen t of the disk arm man y blo cks can be read at once [17]. This is often considered a prerequisite for fast in-order scanning of index con ten ts (suc h necessary as for merging). In our case however, it is also a retriev al issue: since man y of the index entries have large data elds (one keyw ords is mapp ed to man y locations) and can span over man y disk blo cks, linearit y of data elds is also an issue if retrieving records should be ecien t.
 To illustrate the poten tial of having cascading indexes with di eren t characteristics we presen t and exp erimen t using the follo wing workload: (i) Insertion of 600 small XML les in the range 5 KB{45 (ii) Insertion of 40 XML les in the range 45 KB{500 KB. (iii) Insertion of Shak esp eare's pla ys in the range of 100 All cases have an appro ximate overall size of 10 MB. Con-ducting the above tests gave the results sho wn in Figure 2. By looking at the insertion rate, giv en in KB per second, it seems reasonable to assume that the goal of interactiv-ity has been achiev ed for the above test cases. Ev en large documen ts can be added to the index in signi can tly less than one second; this implies that, for a desktop user, per-formance is likely to be dominated by the application code that saves the documen t to a le.
 Curren tly , the system is mo di cation time-agnostic with re-spect to queries, i.e. , we do not rank or treat documen ts di eren tly in queries dep ending on when they were last in-serted. In the authors' opinion, this is a design feature. Of course, mo di cation time could be added as a parameter in the ranking function without much e ort but it is not clear not sure what the implications are, what should be the default beha viour, and in what scenario this would be appropriate. It is a topic for further researc h.
 With resp ect to the query/up date performance, query per-formance is exp ected to be dominated by the lookups in the disk-based indexes, whic h typically involv e physical I/O, and the computation of the meet. Ho wever, queries (on sin-gle user systems) are not a performance bottlenec k and are typically executed in around 100ms. Giv en that queries are read-only and merging pro cess is fully functional, queries and up dates do not a ect eac h other much apart from hav-ing to use the same physical resources, suc h as the hard disk. Generally , queries run slo wer while indexes are being merged but this is not resource con ten tion in the textb ook sense but simply due to the physical limitations of a hard driv e.
 Figure 3 illustrates the program ow when a query is is-sued. The computation of the lowest common ancestors is em bedded in the digram. Kalc has is implemen ted as a clien t-serv er system and uses Berk eley DB as a persisten t storage and logging bac kend. The primary implemen tation platform is C++ along with a num ber of libraries for XML parsing, multi-threading, and B-T rees. The curren t version of Kalc has runs on Lin ux and Windo ws operating systems.
 Figure 4 outlines the comp onen t arc hitectural structure of Kalc has. Kalc has is a daemon whic h serv es user request and listens to events. User requests can be queries or requests to index a le available at a giv en URI. The le would then be fetc hed from the location, parsed and added to the index; this is useful to bulk load the index on a newly setup sys-tem as well as indexing remote les. Additionally , Kalc has also listens to events to automatically index les once they are up dated on local hard disks. Technically , this is achiev ed through the special device /dev/inotify or through another operating system-sp eci c noti cation mec hanism. Terms coming from recen tly mo di ed les are kept in a transien t main-memory index. Once they age without becoming in-valid, they are ushed to the disk-residen t dynamic index. When no mo di cations occur for even more time (sub ject to a rule-based policy), data are migrated from the dynamic in-dex to the static index. Kalc has keeps trac k of indexed les by assigning unique documen t iden ti ers to URIs. Once a le is mo di ed, the corresp onding iden ti er is mark ed invalid and a new iden ti er is generated. Consequen tly , some index entries now become invalid and are sub ject to cleansing. In-dices are purged of invalid entries during merges when data migrate from one index to the other. There are sev eral pa-rameters whic h con tribute to the qualit y of service pro vided by Kalc has; esp ecially , care has to be tak en that indexes do not fragmen t so that scanning and, thus, cleansing and merging become exp ensiv e. This section brie y sketc hes the implemen tation asp ects of Kalc has we consider interesting and relev ant in this setting. Kalc has uses the Dewey enco ding for iden tifying locations of index terms [11]. A Dewey path is a sequence of Dewey steps , whic h are positiv e integers. Dew ey steps are assigned to XML Elemen ts in a syn tax tree; a syn tax node n is assigned a Dew ey step d n , where d n is an integer denoting that n is the d n -th child of its paren t (the unique documen t root is assigned an integer represen ting the documen t num ber). The Dew ey path of n is the concatenation of the Dew ey steps encoun tered on the path from the documen t root to n . For storing h term ; locations i tuples in the disk-residen t in-dices we use an em bedded Database System [20]. In our exp erience, signi can t impro vemen ts over na X ve Storage of Dew ey paths are possible if one tak es into accoun t that most steps are small integers. Th us, we store Dew ey paths as variable length records separated by unique mark ers. To keep the size of the index small, tec hniques tailored to-wards the data structures of em bedded databases were im-plemen ted [15]. Although we did not lev erage the full space-saving poten tial of these tec hniques due to the signi can t CPU overhead and soft ware complexit y they incur, the in-dex size is usually smaller than the original documen ts, de-spite using the Dew ey enco ding, whic h is more verb ose than traditional documen t iden ti ers whic h do not poin t to doc-umen t fragmen ts but only to entire documen ts. The notion of lowest common ancestor (LCA) [18] is used to com bine, re ne, and rank the results of fulltext queries. Thanks to the fragmen t iden ti cation inheren t in the Dew ey enco ding, it is possible to compute LCAs by only consider-ing a result set without accessing the source documen ts. On a sorted sequence of result locations, this can be done in O ( n log h ), where n is the num ber of distinct documen ts in the results set and h is the maxim um depth of these docu-men ts. A detailed algorithmic description and performance evaluation of implemen tation strategies is available [13]. In our scenarios, it turns out that the LCA computation a mi-nor con tributor to overall performance, whic h is dominated by keyw ord searc h and result construction. Since the entries of Kalc has' fulltext indices only pro vide in-formation about the syn tactic location of a result and not about the con text of a result, whic h is needed for the vi-sualisation of query results, the original input documen ts have to be scanned to reconstruct the con text. Giv en only a Dew ey path, one would have to scan the entire source documen t in the worst case to nd a result and to recon-struct the con text. For large les, suc h a pro cess would slo w down the presen tation of the query results in an un-acceptable manner. Therefore Kalc has keeps a simple but e ectiv e index with o sets into the original documen ts. The basic idea to record certain signp osts, h path ; o set i , whic h map certain Dew ey paths to byte-o sets. Whic h paths to include in the index is a matter of policy and dep ends on the actual infrastructure on whic h the engine is installed. For example, it migh t be sensible to require that there be appro x. 100 KB of source documen t between signp osts; the optimal amoun t dep ends on the pro cessor speed to parse the XML documen t and the transfer rate of the hard disk system to read the documen t. In this way, we also mak e sure that small documen ts, whic h can be easily scanned in their entiret y, do not con tribute signp osts to the index and, thus, keep the size small. To nd a giv en Dew ey path d , we look up the maxim um path d 0 d and start scanning for d from d 0 (possibly taking into accoun t that we migh t want to start scanning earlier if d and d 0 are very close to have more con text). Note that the signp ost index tends to be almost negligibly small with resp ect to the fulltext indices; in our exp erience, even on slo w and outdated systems, documen ts whic h are smaller than a few tens of kilob ytes do not need signp osts. Note that it is desirable to cac he the signp ost index since it should not con tribute additional I/O. Partitioned B-T rees have been sho wn to allo w unobtrusiv e merging, i.e. , it is not necessary to tak e the index o -line to perform main tenance [10]. Here we summarise our exp eri-ence with partitioned B-T rees in Kalc has.
 In principle, there are two avenues to merging two index partitions. First, we can merge the rst partition into the second, thus re-using the storage structure of the second one. Second, we can merge the two indexes and pro duce a new, third index. Both metho ds have pros and cons. The main adv antage of the rst approac h, as we see it, is that we can choose how much of the rst index to merge into the second at a time. Merging too large chunks ma y slo w down the system in an unacceptable manner. The disadv antage of this approac h is that the second index ma y fragmen t so that scanning the index ma y gradually become exp ensiv e. The second approac h avoids this dilemma: when pro duc-ing a new index it is much easier to achiev e `linear' allo ca-tion of leaf nodes on disk blo cks and thus mak e scanning cheap er. Kalc has emplo ys a hybrid strategy , i.e. , after a num ber of in-place merges of the rst kind, it pro duces a new index. For completeness, we remark that merging in-dexes alw ays includes ltering out terms originating from stale documen ts. In the small-scale settings we use Kalc has in suc h ltering, although computationally exp ensiv e, is fea-sible using in-memory hash tables. It is clearly desirable to be able to merge the in-memory index in as little time as possible to achiev e persistence. For example, in case of a system shutdo wn, there should be an upp er limit of a few seconds to write the in-memory index to disk. Th us, we limit both the num ber of valid entries in the in-memory as well as the size of the disk-residen t index. In our settings, setting the threshold of the in-memory index to 100,000 entries and the size of the disk-residen t index to 4 megab ytes enables both a satisfying user exp erience and a merge time of about 1.5 seconds on a 1.5 GHz Pentium PC with an IDE hard disk. When merging the possibly very large persisten t indices care has to be tak en that no data are lost due to errors or sys-tem failure since rebuilding the index migh t be prohibitiv ely exp ensiv e and would e ectiv ely tak e the system o -line. If we pro duce a new index, we can do this without mo dify-ing the original indexes by ushing the new index to disk at certain chec kp oin ts; this ensures that not all work is lost in case of failure. Additionally , we tend to end up with an index whic h exhibits little fragmen tation at the exp ense of additional storage space for the newly generated index. The downside of this strategy is that the merge pro cess should be nished as soon as possible since we cannot mak e the in-memory index persisten t as long as the merge is carried out. The alternativ e is to merge the smaller index into the larger one without requiring additional memory . This can be executed in small pieces at the exp ense of higher frag-men tation and overall execution time. Curren tly , Kalc has is able to merge persisten t indexes at a rate of 4{6 MB per second on the above-men tioned hardw are. We remark that the data structures used to implemen ted the three indexes di er sligh tly from eac h other in details due to performance optimisation considerations. For exam-ple, the transien t index is mainly a straigh t-forw ard C++ multimap&lt;&gt; coupled with a logging facilit y to keep trac k of up dates. The disk-based dynamic and static index di er in the compression schemes used to keep the compact while be-ing mo dest on resources. Th us, the dynamic index only uses the variable length records while the static index uses com-pression as well as the tec hniques men tioned in Section 5.1. Since ranking is not a primary researc h issue in this pro ject, we resort to well-kno wn tec hniques [11]. In particular, the ranking function mak es use of the follo wing building blo cks: (1) distance of hits in the syn tax tree, (2) appro ximate tex-tual distance of hits, (3) num ber of hits con tained in result node, and (4) coverage of hits. This information can be gathered while computing the LCAs. It is Kac has's goal to keep ranking as simple and transparen t as possible so that users nd it possible to comprehend wh y query results are rank ed the way they are rank ed. For performance reasons, it turns out not to be feasible to protect all database operations with transactions. Fortu-nately , this is not necessary as long as we focus on the crit-ical poin ts. Since all of the indices ma y con tain stale data, we only have to mak e sure that we protect the generation of documen t iden ti ers and that we ush all bu ers after merge operations. If all ush operations succeed, we regard the new index as persisten t. Th us, only operations on the meta-data of the index are fully protected by transactions; the list of indexed les is reco verable but not the individual index entries, whic h would be problematic anyw ay due to the optimised storage structures used. Ho oking into applic ations and taking adv antage of their in-ternal XML data structures can further lower parsing and extraction costs. Th us, the Kalc has serv er does not only digest XML documen ts but also pre-pro cessed index data. Wr apping data sour ces. Because LCA queries work only on XML documen ts, other typ es of sources should be wrapp ed in XML. Often meta-information about les can be easily arranged in a hierarc hical manner, or solutions are read-ily available. Kalc has works on a variet y of le typ es that are automatically con verted to XML for indexing purp oses. Sometimes it is just the meta-information that is indexed, suc h as in the case of images or music les; sometimes the le con ten ts are annotated in a simple manner, suc h as with plain-text les, or appro ximated.
 Index sharing. Kalc has can be con gured to accept queries originating from mac hines on a local area net work. Th us, it is possible to include shared les from other mac hines into the searc h results. The demonstration will consist of di eren t scenarios that il-lustrate both the strengths of the system as well as the limi-tations and ways to get around them. The scenarios will be similar to those men tioned in the intro duction. We will start out by opening a typical working documen t of appro x. 200 KB in a simple word pro cessing application; the documen t is edited in the word pro cessor, saved to disk, and the up dated documen t is almost immediately available for being queried { in our curren t implemen tation with a dela y of appro x. 100 ms. After making sev eral more changes, we will see how, after a perio d idle time, the recen tly edited documen t sifts from the transien t index to the disk-based indexes. Previ-ously , we argued that typical user patterns tend to focus on up dating a comparativ ely small set of les. There are, how-ever, at least two common exceptions to this observ ation, whic h should be tak en care of. First, it is quite common to, for example, setup a new hard disk and thus create a new index from a poten tially large num ber of les. Second, it of-ten happ ens that users cop y a num ber of les from external sources, suc h as the Internet or a CDR OM, to their hard disk; these les should, of course, be ecien tly added to an existing index. The demonstration will sho w how the index cascade performs in these situations and con trast it to bulk-orien ted sort-merge metho ds whic h have been optimised to deliv er better performance in certain con texts. Th us, rel-ativ e performance of the index cascade as well as the user exp erience when up dating and querying will be exempli ed. Ev entually , di eren t user interfaces, namely a nativ e GUI as well as an HTML-based one, will be sho wn o . Sp eci cally , we will try to demonstrate that the generic meet operator is a sensible tool when querying documen ts about whose seman tics we have no prior kno wledge.
 The demonstration will also sho w that a mo dest implemen-tation platform, namely a 1.4 MHz commo dit y laptop, is sucien t to achiev e performance whic h is usually unobtru-siv e to a user's exp erience. Fulltext indexes have been used for Information Retriev al purp oses for a long time. Tw o well kno wn data structures for implemen ting fulltext indexes are signature les and in-verted les [3, 8]. According to Zob el et al. [24], inverted les, as used in this pap er, \[. . . ] are distinctly sup erior to signature les" in terms of query performance, space util-isation and functionalit y. Originally , fulltext indexes were mostly used for searc hing static arc hiv es. Some examples of how to up date indexes and at the same time to avoid a complete rebuild of the index are the follo wing. In the con text of object stores, incremen tal and batc hed incremen-tal up dates to fulltext can be optimised by manipulating link ed-lists of term occurrences [4]. A two-tier approac h, including a main-memory and a disk-residen t index, to full-text indexing is presen ted and discussed in [21] as welll as the optimisation of adding les to the index.
 So far, the authors have failed to nd mo dels for sim ulat-ing and analysing user beha viour whic h tak e into accoun t all necessary parameters suc h as clustering, text statistics, etc. Although there exist attempts to mo del workloads for indexing mac hines [19, 24], they fail to tak e into accoun t the complex time-, documen t-, and user-dep enden t assumptions whic h underly our observ ations. We thus consider workload mo delling as an imp ortan t sub ject of future researc h. In this sense, the tec hniques used in this pap er could be classi ed as follo ws: We exploit the localit y of up dates by using a workload-adaptiv e ageing mec hanism whic h deter-mines when a giv en curren t documen t should be merged into whic h index. Ho wever, although the notion of time pla ys an imp ortan t role in the presen t work, it does not attempt to pro vide temp oral database functionalit y for fulltext in-dexes [1].
 The concept of lowest common ancestor (LCA) queries on XML databases and its ecien t implemen tation giv en doc-umen t statistics is investigated in [23]. It would be very in-teresting to investigate how to adapt these tec hniques to our settings as they can be exp ect to yield a signi can t perfor-mance boost. A detailed investigation on how and when to merge indexes is found in [14]. Kalc has naiv ely treats every version pro duced by an up date to a documen t equally; [5] and [22] presen t approac hes whic h compute the di erences between two successiv e versions. We outlined the system arc hitecture of Kalc has, a dynamic XML searc h engine. Kalc has com bines ideas from parti-tioned B-T rees, fulltext indexes, and lowest common ances-tor queries to index XML documen ts and mak e them avail-able for querying as early as possible. The demonstration includes sho wing the beha viour of the system and its limita-tions in man y interesting every-da y situations exp erienced by users; the general schema of the demonstration follo ws the usage scenarios presen ted in the intro duction and addi-tionally covers bulk up dates. Future Work. We plan to con tin ue investigating the scal-ing beha viour in larger scale settings and integrate standard tec hniques to impro ve searc h term qualit y. Additionally , we are interested in further optimisation of storage formats and con version algorithms to reduce CPU and I/O loads. Cur-ren tly the query results are presen ted mostly as XML text whic h is stripp ed o the tags, i.e. , in a searc h engine style; it would desirable to switc h to visually more app ealing render-ings, for example, by using stylesheets whic h are asso ciated with man y typ es of les or by generating thum bnails where this is feasible.
 The authors would like to thank Christian S. Jensen for stim ulating discussions. This researc h was funded (in part) by the Danish Researc h Council for Technology and Pro-duction Sciences pro ject No. 26-04-0092 Intelligen t Sound ( http://www.intelligentsound. org ). [1] P. Anic k and R. Flynn. Versioning a Full-text [2] R. Ba yer and E. McCreigh t. Organization and [3] E. Bro wn. Execution Performance Issues in Full-text [4] E. Bro wn, J Callan, and W. Croft. Fast Incremen tal [5] G. Cob ena, S. Abiteb oul, and A. Marian. Detecting [6] M. Cro vella and A. Besta vros. Self-Similarit y in World [7] M. Dew ey. A Classi cation and Sub ject Index for [8] C. Faloutsos. Access metho ds for text. ACM [9] G. Graefe. Partitioned B-T rees -a User's Guide. [10] G. Graefe. Sorting and Indexing with Partitioned [11] L. Guo, F. Shao, C. Botev, and [12] R. Kaae, T. Nguy en, and D. Nrgaard. Kalc has { [13] R. Kaae and D. Nrgaard. Engineering an XML [14] N. Lester, J. Zob el, and H. Williams. In-Place versus [15] S. Melnik, S. Ragha van, B. Yang, and [16] D. Musser and A. Saini. STL Tutorial and Reference [17] P. O'Neil. The SB-T ree: An Index-Sequen tial [18] A. Schmidt, M. Kersten, and M. Windhou wer.
 [19] K. Sho ens, A. Tomasic, and H. Garc a-Molina. [20] Sleep ycat Soft ware. Berk eley DB. Av ailable at [21] A. Tomasic, H. Garc a-Molina, and K. Sho ens. [22] Y. Wang, D. DeWitt, and J. Cai. X-Di : An E ectiv e [23] Y. Xu and Y. Papak onstan tinou. Ecien t Keyw ord [24] J. Zob el, A. Mo at, and K Ramamohanarao. Inverted
