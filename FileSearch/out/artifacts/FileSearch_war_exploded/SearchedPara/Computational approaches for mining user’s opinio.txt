 1. Introduction
Opinion mining deals with analyzing people X  X  opinions, attitudes and emotions towards different brands, companies, ing such as natural language processing (NLP), information extraction and information retrieval have quite a considerable history, the research on mining people X  X  opinions has become quite popular in the last couple of years with the rise of the Web 2.0. User generated content on the Social Web can contain a variety of relevant market research information and deeply analyzing and exploiting it leads to more targeted business decisions ( Guozheng, Faming, Fang, &amp; Jian, 2008; Liu, 2008 ).

Analyzing opinions on the Social Web is met with a variety of challenges: (i) the  X  X  X sual X  X  challenges known from natural language processing (such as word sense disambiguation, topic recognition and co-reference resolutions) and (ii) challenges arising from user generated content:  X 
Noisy texts, language variations : User generated texts tend to be less grammatically correct and often use specific charac-
Haque, 2009 ). Moreover, social media texts typically assume a higher level of knowledge about the context by the reader than more formal texts ( Maynard, Bontcheva, &amp; Rout, 2012 ).

Relevance and boilerplate : When web texts and social media texts are gathered using a web crawler, the gained texts usu-ally contain irrelevant content like advertisements, navigational elements or previews of other articles ( Maynard et al., 2012; Petz et al., 2012; Yi &amp; Liu, 2003 ).

Target identification : Search-based approaches have to deal with the problem, that topics of retrieved documents do not necessarily match the mentioned sentiment object ( Maynard et al., 2012 ).

Big data challenges : That can be broken into several contexts such as temporal, spatial and spatio-temporal contexts ( Derczynski, Yang, et al., 2013; Maynard, Dupplaw, &amp; Hare, 2013 ).

Due to these challenges, research papers usually deal with assumptions and constraints: Many of the approaches to ana-ysis approaches is quite important in order to achieve good results.

The objectives of this paper are (i) to investigate the differences between social media channels regarding opinion mining media channels. To attain these objectives, we set up the research methodology as follows: (1) Identification of popular approaches and algorithms to carry out text preprocessing as a prior step to sentiment (2) Identification of differences between social media channels and deduction of impacts on opinion mining and text (3) Evaluation of the effectiveness and properness of several algorithms in order to determine their applicability.
The rest of the paper is organized as follows: in the next section we discuss some related work in the field of opinion min-discusses the impacts of these characteristics on some frequently used algorithms and evaluates their performance regarding noisy text. 2. Related work, background 2.1. Sentiment analysis and opinion mining
Pang and Lee (2008) and Liu (2012) present a detailed review of opinion mining. Liu defines an opinion as a quintuple ( e a , s ijkl , h k , t l ), where e i is the name of an entity, a items in the definition must correspond to one another. ( Liu, 2012; Wilson, Wiebe, &amp; Hoffmann, 2009 ) cross-domain sentiment analysis (e.g. Bollegala, Weir, &amp; Carroll, 2011; Pan, Ni, Sun, Yang, &amp; Chen, 2010 ).
The classification of texts regarding sentiment polarity can be done at three different levels: (1) document level, (2) sen-tence level and (3) entity and aspect-level. There are several approaches to analyze opinions: (1) corpus-based approaches (e.g. Hatzivassiloglou &amp; Wiebe, 2000; Turney, 2002; Wiebe &amp; Mihalcea, 2006 ) and dictionary-based/lexicon-based 2012 ), (2) machine learning approaches. These approaches can be categorized as follows: (1) Supervised learning : Supervised learning ( X  X  X lassification X  X ) is a machine learning task of inferring a function from (2) Unsupervised learning : Unsupervised learning is often used when the user wants to find hidden structures in unlabeled data and is often called  X  X  X lustering X  X . A variety of algorithms exist in this subject: k-means, mixture models, hierarchi-cal clustering, etc. ( Liu, 2008 ). (3) Other approaches/algorithms : Since supervised learning needs a large number of labeled data for training and this task is often done manually and therefore is particularly time consuming, some researchers developed partially supervised learning approaches. The tasks include learning from labeled and unlabeled examples ( X  X  X U learning X  X ) and learning from positive and unlabeled examples ( X  X  X U learning X  X ). Exemplary algorithms include Expectation X  X aximization (EM) algorithms, co-training and transductive support vector machines ( Liu, 2008 ). One can identify several other algorithms used in opinion mining. E.g. aspect based opinion mining focuses on feature based approaches (e.g. Hu &amp; Liu, 2004a; Liu et al., 2005; Moghaddam &amp; Ester, 2010 ), while others apply latent variable models like the hidden Markov model (HMM, e.g. Jin, Ho, &amp; Srihari, 2009; Wong, Bing, &amp; Lam, 2011 ), conditional random fields (CRF, e.g. Choi &amp; Cardie, 2010; Li et al., 2010 ), latent semantic association (e.g. Guo, Zhu, Guo, &amp; Su, 2011; Guo, Zhu, Guo, Zahng, &amp; Su, 2009; Hofmann, 2001 ) up to combinations and variations of existing algorithms ( Airoldi, Bai, &amp;
Padman, 2006; Choi, Cardie, Riloff, &amp; Patwardhan, 2005; Jakob &amp; Gurevych, 2010; Nakagawa, Inui, &amp; Kurohashi, 2010 ). One important part of the preprocessing steps in opinion mining is Part-of-Speech (POS) tagging; a variety of algorithms can be applied to implement this task: rule based approaches, Markov model approaches, maximum entropy approaches, etc. The majority of research work has focused on POS tagging in English, although there are research efforts under way to extend POS tagging to other languages as well ( G X ng X r, 2010 ).

Due to the number of different techniques, several researchers experimented with different algorithms and drew com-experiment with adaption of machine learning approaches to other languages and domains, e.g. ( Boyd-Graber &amp; Resnik, 2010 ), ( Balahur &amp; Turchi, 2013 ). 2.2. Preprocessing noisy text
Only little research work can be identified on preprocessing of noisy texts. The objective of preprocessing is to produce clean texts for further analysis processes. The tasks usually include identifying and correcting spelling errors, eliminating arbitrary sequences of whitespaces between words, detecting sentence boundaries, eliminating arbitrary use of punctuation ghan et al.; the authors describe a software program that corrects spelling mistakes and typos based on a noisy channel tence boundary disambiguation, word disambiguation in terms of capitalization and identification of abbreviations ( Mikheev, 2002 ). A couple of authors used machine learning approaches to deal with noisy texts (e.g. Clark, 2003; Gotoh texts in social media. Dey/Haque propose a framework for opinion extraction and mining from noisy text; the framework includes sentence boundary detection, improper case correction and context-dependent spelling correction. In order to determine opinions, the framework uses the cleaned text as input to POS tagging, dependency trees and other algorithms the authors report a weak performance of these taggers when applied to tweets. The most frequent POS tagging errors result from internet slang words, common misspellings, genre-oriented phrases and unknown words. Based on this error analysis, tweets on several text preprocessing steps: language identification, tokenization, POS tagging and named entity recognition.
All in all, the performance of machine learning methods suffers from noisy texts. Normalization approaches (basic resp. strong normalization) of noisy texts offer only small improvements, but are helpful and have to be further developed. 3. Comparison of social media channels
Kessler/Nicolov collected 194 blog entries about cars and digital cameras and calculated some opinion mining relevant statistics (e.g. number of tokens between a sentiment expression and its target; target in the same sentence as their senti-media channels in order to derive impacts on the opinion mining process. 3.1. Methodology
We carried out an empirical analysis of social media texts; for this purpose we collected data from social network services (Facebook; 410 postings), microblogs (Twitter; 287 tweets), blogs (387 blog posts), discussion forums (417 posts from 4 different forums) and product review portals (433 reviews from Amazon, and two product review pages) in four different languages (English, German, Czech, Polish). In order to conduct a representative survey, we drew a quota sample and there-fore focused on one specific brand (Samsung) in a specific time period (between 15 June 2011 and 28 January 2013) in the different social media channels. The collection was performed both manually (discussion forums) and automatically using
APIs and Web crawlers. The data sets were labeled manually by four different human labelers. Rules for labeling have been discussed and defined to make the labeling as consistent as possible. We defined several measure criteria (e.g. number of words per posting, number of sentences per posting, etc.) along with their scale type. All in all 1934 postings have been analyzed; the statistical calculations were carried out using SPSS, correlations between variables were tested with Post-
Hoc-tests/Bonferroni ( Petz et al., 2013 ). 3.2. Results In the following section we give a short overview on some key findings
Basic figures : The survey revealed some findings that were to be expected. E.g. the length of the postings differs between the social media channels: the average number of words in a posting is highest in product reviews, and lowest in microb-logs (see Table 1 ).

Subjectivity : Many papers assume that content in social media is highly subjective and therefore a perfect foundation for sentiment analysis tasks. Our analysis revealed that microblogs contain the most subjective information (82.9% of the postings). Product reviews consist of subjective and objective texts; e.g. consumers write both about feelings and impres-sions about a product and usually summarize facts as well. Surprisingly only about 50% of discussion forums postings contain subjective texts. A closer glance at the discussions forums showed that users often write about hints, tips and instructions of how to deal with a specific problem. The detailed correlations between the variables were tested with
Post-Hoc-tests/Bonferroni: Facebook/discussion forum ( p = 0.001), Twitter/product review ( p = 0.0), Twitter/blog ( p = 0.033), Twitter/discussion forum ( p = 0.0) (see Table 2 ).

Grammatical correctness : Some authors have taken grammatically false texts into consideration, but the vast majority of research work assumes grammatically correct texts. As the following chart exhibits, all social media channels contain many grammatical and orthographical errors. We calculated an error ratio as follows: the number of incorrect sentences divided by the number of sentences. Fig. 1 exhibits the error ratio in more detail; the correlations between the variables were tested with Post-Hoc-tests/Bonferroni: product review/Twitter ( p = 0.002), Twitter/blog ( p = 0.0).
The evaluation of grammatical correctness was not easy: people sometimes do not use punctuation marks, which leads to discussions among the labelers of how many sentences are expressed in that posting; in German texts one can find com-plete texts without capitalization. Some of the tweets in Twitter are nearly impossible to understand without context knowledge (e.g.  X  X  X wagger Shout Out ! @TWAMBIT @MixtapeOficial @SamsungAT @greentravel @__jitesh__ #Sunglasses #ShoutOut X  X ). Additionally, a considerable number of authors lengthen words in order to emphasize specific aspects or feelings (e.g.  X  X  X ood phone goooooooooooooood X  X ).
 and abbreviations (e.g.  X  X  X OL X  X ,  X  X  X MHO X  X ). The following tables provide an overview about the usage of emoticons and abbreviations: Table 3 shows that about 23% of postings in social networks contain one emoticon; interestingly quite a low number of postings contain more than two emoticons. The situation regarding abbreviations is quite similar. Because of the limited amount of characters per tweet, Twitter contains the most abbreviations (see Table 4 ). Some research papers discuss the possibility of using  X  X  X entiment hashtags X  X  (like #sarcasm, #fear, #anger) as indicators of emotions or sarcastic texts in Twitter; however, our sample did not contain these patterns.

Opinion holder : In most cases the opinion holder is the author of the posting; with the exception of discussion forum entries, between 95% and 97.6% of the postings reveal the author as the opinion holder. In 90.7% of the postings in dis-cussions forums the author is the opinion holder, 6.2% of the entries in discussion forums have several opinion holders, and 3.1% depict the opinion of another person.

Differences between languages : Clearly, there are differences between the languages regarding the length of the postings (when they are not limited). Although there are differences in terms of usage of emoticons and abbreviations as well, there are no figures that have led to completely new findings other than the ones presented above.

Since the sample has focused only on one brand in the consumer goods sector, the question arises concerning the extent to which generalizations can be made based on these results and then applied to other areas. We assume that these results can be generalized to other products and brands, but possibly not to political discussions or discussions about arts. 4. Evaluation of Impacts
The text preprocessing is an important step in the opinion mining process. As mentioned above, only few papers deal with noisy texts derived from different social media channels. The following section focuses on the factual impact of the findings above.
 4.1. Methodology
We evaluated the factual impact of user generated content on selected algorithms that are frequently used for prepro-cessing texts during the opinion mining process. The following algorithms were evaluated:
Sentence splitting : Since many other preprocessing steps require text fragments based on sentences, one of the first steps is usually sentence boundary detection. Having little or no language-specific implementations for the Czech and Polish language, only texts in German (422 postings) and English (411 postings) were used. The following algorithms were com-pared: (i)  X  X  X NNIE sentence splitter X  X  and (ii)  X  X  X egex splitter X  X  from the software package  X  X  X ATE X  X  (Version 7.1 build 4485
GATE Cunningham, Maynard, Bontcheva, &amp; Tablan, 2002 ) and the (iii)  X  X  X egex splitter X  X  and from the library  X  X  X penNLP X  X  (version 1.5, model  X  X  X n-sent.bin X  X  The Apache Software Foundation ).

Stemming : Stemming is the process of reducing words to their base form; we used the Snowball stemmer ( Porter, 2001 ) implemented in the Lucene.NET library. Lucene is an open source information retrieval library; Lucene.NET is a port of the library written in C# ( The Apache Software Foundation ; McCandless, Hatcher, &amp; Gospodnetic  X  , 2010 ). The German texts were stemmed with the algorithm of Caumanns ( Caumanns, 1999 ) implemented in RapidMiner ( Rapid-i ).

Part of speech tagger : POS taggers label tokens with their corresponding word type. The implementation of OpenNLP with the English POS model and GATE ANNIE POS tagger (version 7.1 build 4485) were used. These taggers use the Penn
Treebank tag set. ( The Apache Software Foundation ) In order to evaluate performance of POS tagger we used a sample ( n = 150) of English texts from the above sample set. The texts have been corrected manually by the researchers. Subsequently two taggers have been applied to the texts.

Parser : A parser reveals the structure of sentences, e.g. which words can be grouped into phrases, which words are the subject or the object of a verb. We used the parser from OpenNLP with the model  X  X  X n-parser-chunking.bin X  X  for the English texts ( The Apache Software Foundation ).

The evaluation was carried out as follows: a quota sample of the previously collected data was drawn in order to cover each social media channel. Depending on the availability of algorithms for specific languages, we took postings in other lan-meaningful texts). Then the algorithms were compared against these manually corrected postings. 4.2. Results In the following section we give a short overview on some key findings:
Sentence splitting : Tables 5 and 6 show the percentage of the correct sentence splitting. The performances of the sentence works and product reviews, but comparatively low in blogs and discussions forums. The performance for German texts is worse. A detailed glance at the data reveals, that the sample contains simple sentences for Twitter and Facebook (e.g.  X  X  X ool X  X ,  X  X  X  so want this camera X  X ), while product reviews, blogs and discussion forums contain a larger number of sentences and higher complexity of the writing. Wrong sentence boundary detection often also arises in that bulleted lists are not interpreted as sentences, while human labelers tend to interpret a bullet item as one (or more) sentences.
Stemming : Due to the simple rules implemented in stemming algorithms, the results of the stemmed texts are compara-ble. We suspected that typos and strange abbreviations could lead to different stemmed words, but that was not the case in our sample.

Part of speech tagging : The part of speech tagging algorithms seem to be relatively robust regarding noisy texts; the major-(your PRP$) (firm NN) X  X . False sentence boundary detection has surprisingly little impact. The high POS output equivalent on the microblogs can be explained in such a way that it is difficult to correct these posts, as there are often many hash-tags ( X  X # X  X ) and involve direct addressing ( X  X  X  X  X ) and contain only a few words. The following table depicts, whether the output of the POS tagging of the original posting is equivalent to the corrected posting (see Table 7 ).
Parsing : As expected, parsing is more sensitive to grammar mistakes than is POS tagging. The following table shows, whether the output of the parser of the original posting is grammatically equivalent to the corrected posting. Because of the short sentences in Twitter it is not surprising that the parser output matches best (see Table 8 ). 4.3. Improvements to the opinion mining process
Some authors suggest stripping symbols in order to retrieve clean texts; as the above figures show, user generated texts contain a lot of emoticons that can express feelings too. If these symbols were removed, additional context information that could be used to improve sentiment analysis would be lost. Dey/Haque propose in their framework several steps for cleaning texts and then use PoS tagging and dependency trees among others for determining the sentiment orientation. The proposed steps  X  sentence boundary detection, improper case correction and context-dependent spelling corrections  X  are important heavily on parsing to execute preprocessing steps, because the output of parsers may vary with the noise included in the input texts.
 The characteristics of the different social media channels should be taken more into consideration:
Twitter : Although postings on Twitter have a limited length, the hashtags can deliver some kind of contextual informa-tion. Several researchers have already exploited Twitter characteristics, e.g. Davidov et al. (2010) treat hashtags and lan-guage conventions as features, Zhang, Ghosh, Dekhil, Hsu, and Liu (2011) combine lexicon-based and learning-based because these words are often used as subjective words to emphasize the sentiment they convey. Balahur (2013) suggests a variety of preprocessing steps, including slang replacement, emoticon replacement with their polarity, word normali-zation, user and topic labeling, and affect word matching. Some researchers query the use of part of speech features in the microblogging domain (e.g. Kouloumpis, Wilson, &amp; Moore, 2011 ). However, the POS tagging of a grammatically corrected tweet is comparable to the POS tagging of the original tweet.
 viations and emoticons are contained not only in tweets, but in other social media channels as well (with different fre-quencies). Hence, it seems to be reasonable to adapt the above mentioned techniques to the other channels. Our survey focused on social media channels in four different languages; however, our sample contained several other languages as well. Hence, language detection seems to be reasonable to improve sentiment analysis ( Bergsma, McNamee, Bagdouri,
Fink, &amp; Wilson, 2012; Petz et al., 2012 ). 5. Conclusion and further research
Computational approaches on opinion mining are an emerging research topic and there are many challenging future research avenues. In this article we investigated the differences of several social media channels concerning text quality and its factual impacts to a couple of frequently used algorithms in the opinion mining process. In the first section of this work we reported on a survey, which was carried out in order to find the characteristics of social network services, microb-logs, blogs, product reviews and discussions forums. The survey focused on the viewpoint of a company; hence the survey covered a specific brand of a manufacturer of electronic devices. In the second section we presented the evaluation of the performance in terms of correctness of sentence splitting, stemming, POS tagging and parsing. The results demonstrated that both extensive text preprocessing prior to sentiment analysis tasks and improved algorithms that take noisy text into account seem to be reasonable in order to cope with texts published by users on social media platforms.

In further research work we shall address: (i) measure and evaluate implications of noisy texts on more algorithms in order to find a set of useful preprocessing techniques that improve sentiment analysis; (ii) increase sample size and expand sample to other areas than brands in consumer electronics in order to retrieve more generalizable results; (iii) experiment and develop machine learning models that take noisy text into consideration.
 Acknowledgements This work emerged from the research projects OPMIN 2.0 and SENOMWEB. The project SENOMWEB is funded by the
European Regional Development fund (EFRE, Regio 13). OPMIN 2.0 is funded under the program COIN  X  Cooperation &amp; Inno-vation. COIN is a joint initiative launched by the Austrian Federal Ministry for Transport, Innovation and Technology (BMVIT) and the Austrian Federal Ministry of Economy, Family and Youth (BMWFJ).
 References
