 Past work showed that significant inconsistencies between retrieval results occurred on different test collections, even when one of the test collections contained only a subset of the documents in the other. However, the experimental methodologies in that paper made it hard to determine the cause of the inconsistencies. Us-ing a novel methodology that eliminates the problems with uneven distribution of relevant documents, we confirm that observing a sta-tistically significant improvement between two IR systems can be strongly influenced by the choice of documents in the test collec-tion. We investigate two possible causes of this problem of test collections. Our results show that collection size and document source have a strong influence in the way that a test collection will rank one retrieval system relative to another. This is of particular interest when constructing test collections, as we show that using different subsets of a collection produces differing evaluation re-sults.
 H.3.4 [ Information Storage and Retrieval ]: Systems and Soft-ware X  Performance evaluation Measurement; Reliability Information retrieval; evaluation; subcollections; TREC
If a significant difference is measured between two IR systems using a test collection, is the difference real or an artifact of the collection? This question has been asked ever since test collections were first described [2]. While the reliability of relevance asses-sors [3], evaluation measures [4], and topics [9, 10] has been inves-tigated, there has been little examination on whether the properties of a particular collection of documents influences search results.
While there is a general feeling in the IR community that it is preferable to demonstrate the superiority of one algorithm over an-other using multiple test collections, and this is often instantiated as using multiple metrics, topic sets, and relevance judgments, it is not uncommon to see published work that uses just one collection. Is this a safe experimental practice or not? There is little or no empir-ical evidence to show how many test collections are needed, what different types of collections are required, how one might measure the difference between collections, or indeed if it is necessary to test widely in the first place.

Sanderson et al. [6] presented evidence that the relative differ-ence in effectiveness between retrieval systems varies across sub-sets (i.e. subcollections ) of a single document collection, a result that had not been shown before. However, in that work, there were multiple differences between the subcollections being compared, making it difficult to know what the cause of this effect was. The work was also tested on relatively old retrieval systems.
In this paper, the experiments of Sanderson et al. are expanded using stricter controls on the differences between the subcollec-tions, and a state-of-the-art retrieval system. The paper asks the following research questions: 1. What are the causes of the measured differences between 2. Do the measured differences still occur when relevant docu-
The work described here follows on from Sanderson et al. [6] who described a series of experiments examining TREC run data ranked using standard effectiveness measures based on different subcollections of a TREC test collection. They found that the or-dering of the runs was substantially different between the subcol-lections and that these differences were much greater than would be expected by random chance.

Different ways of forming subcollections were investigated, such as splitting by document source (e.g. Financial Times, Congres-sional Record, etc.) or topical similarity (using k-means cluster-ing). However, when the subcollections were formed based on any of the tested criteria, other factors also varied, which may have af-fected the comparisons. The most important of these factors were that the subcollections varied in size, sometimes substantially, and the number of relevant documents in each subcollection was dif-ferent. In this paper, we mitigate these factors, in order to better understand what properties of subcollections may play a role in leading to inconsistent retrieval results.

Additionally Sanderson et al. used IR systems that were 10  X  20 years old: their experiments may have identified old problems that were resolved in newer ranking algorithms. In this paper, we use a state-of-the-art retrieval system. Finally, their method for com-paring run ordering was Kendall X  X   X  . This measure quantifies the degree of correlation between the rank ordering of two lists. How-ever,  X  may be affected by statistically insignificant swaps between systems. Here, we investigate a new agreement-based approach, which focuses only on statistically significant differences.
There is little past work on the impact of collection choice when measuring retrieval effectiveness. A recent tutorial on test collec-tion construction [8] did not mention collection choice or the prop-erties of the documents that should make up a collection. There has perhaps been an assumption, that an IR system will be tested on a representative sample of the documents the system will be de-ployed on. However, if advances in IR systems are to generalize to any document collection, rigourous evaluation is necessary.
Apart from the work of Sanderson et al., Azzopardi and col-leagues have investigated bias in ranking functions. They showed evidence that ranking algorithms appeared to be influenced by the length of the documents they retrieved (most recently in Wilkie and Azzopardi [11]). Further, they showed that different algorithms were affected by length in different ways. Although work in the past has attempted to eliminate such biases [7], it would appear, they are still present.
Here, we describe the collections, system and measures used to investigate Sanderson et al X  X   X  X ubcollection effect X .
The TREC 4 X 8 ad hoc collections are used for our experiments, in-keeping with the analysis of Sanderson et al. These collections suit splitting by document source, as all five collections are com-posed of texts from multiple sources.
When Sanderson et al. split subcollections by source, the num-ber of judged relevant documents available in each subcollection varied substantially. To address this potential problem, we used the following methodology. We separated each test collection into two subsets: one containing all documents that were judged as rel-evant ( all-rel ); the other containing all the other documents in the collection, including documents judged as irrelevant ( not-rel ). All subcollection splits were then carried out by partitioning the not-rel subset of the collection based on document source. All the docu-ments from all-rel were then added to each subcollection.
By forming subcollections in this way, we ask the question: is the ability of an IR system to retrieve the same relevant documents af-fected by distinct sets of non-relevant documents? Further, if there is an effect, are different IR systems affected in different ways?
Evaluating the effectiveness of a new ranking approach using a test collection typically compares two variants of an IR system X  a new configuration of the system, and the previous configuration (the baseline) X  X ver a fixed set of queries, documents, and rele-vance judgements. The outcome of such an experiment is typically quantified by reporting the mean effectiveness of each system ac-companied by a statistical significance test. Having run such an experiment on one collection, one might ask whether the same out-come would be observed when comparing the same systems on an-other collection. To investigate this, we adapted an approach used by Moffat et al. [4] to investigate the agreement on IR experimen-tal results when using different effectiveness metrics. However, here we examine the agreement on significance when using differ-ent collections. Consider the process of comparing systems, S1 and S2, on two subcollections, C1 and C2. There are a number of possible outcomes as follows.
As per Moffat et al. [4], for a given collection pair, the proportion of significant differences between systems observed for which both collections agree on which is the better system is given by
In order to simulate multiple research experiments of IR sys-tems, we used the Terrier search engine [5] configured to use six-teen different ranking models: BB2, BM25, DFR_BM25, DFRee, DLH13, DLH, DPH, Hiemstra_LM, IFB2, In_expB2, In_expC2, InL2, Lemur TF_IDF, LGD, PL2, and TF_IDF. The search engine was in default TREC mode. The models were run on each subcol-lection, and evaluated using MAP, producing 120 pairs of system comparisons for each subcollection.
The results of the experiments to test potential confounding fac-tors in Sanderson et al X  X  methodology are described here.
The first seven columns of Table 1 shows agree -SS a for each source-based subcollection for the TREC 4 X 8 ad hoc collections, with the mean (  X  ) and standard error (s.e.) for each subcollection reported in columns eight and nine. Recall that the original collec-tions are split by document source, but the number of relevant doc-uments is held constant in each subcollection. Comparisons also included comparing the whole TREC collection with each subcol-lection. The minimum, median, and maximum of  X  are 8%, 62%, and 74% respectively. The central 50% of the data are in the range from 40% to 67%. Broadly speaking, running experiments on dif-ferent subcollections may lead to inconsistent conclusions about system superiority a substantial proportion of the time.
Kendall X  X  Tau (  X  ) is shown to facilitate comparison to Sanderson et al. [6], and the values are in the same range as those obtained by Sanderson et al., and rather low. The  X  values generally correlate with agree -SS a , although differences exist.

For further context, the agree -SS a values obtained when mea-suring agreement when comparing each subcollection against itself using two different effectiveness metrics X  X AP and NDCG X  X re shown in the final column of Table 1. Both measures were calcu-lated over the top-1000 retrieved documents. In all but three cases, when comparing systems across subcollections (labelled in the rows of each section and ranked by increasing size). Means ( are across all pairings for each subcollection, and the standard error of  X  (s.e.) was derived using bootstrapping. For com-MAP is shown, and mean  X  is in the last column. the agree -SS a on different evaluation measures was higher than the mean subcollection agree -SS a .

Subcollections containing identical sets of relevant documents appear to inconsistently identify many significant differences in variations of the same retrieval system.
While relevant documents were kept constant across subcollec-tions in the previous experiment, each compared collection was a different size, ranging from 12,994 to 223,751 documents. It is therefore plausible that size X  X hich represents the number of non-relevant documents in a subcollection X  X ffected the agree -To further investigate the effect of size on the results, a simulation experiment was run measuring agree -SS a across subcollections of different size. We created several new subcollections of similar sizes to the TREC subcollections, but using documents selected at random from the whole TREC-8 collection rather than split by source.

Figure 1 shows the inter-size agreements. The points on the diag-Figure 1: Agreement between collections as a function of col-lection size. Note the gradient of lighter points (less agreement) towards the bottom right of the figure. Eight distinct pairs of random subcollections were tested for each comparison, plot-ted as a square centered on the actual sizes. onal compare random subcollections of the same size, and agree -between these collections is high. Towards the bottom right of the figure, where subcollection sizes differ by up to a factor of four, agree -SS a reduces.

However, the low agree -SS a values in Figure 1 X  X ven when subcollections were substantially different in size X  X re generally higher than those recorded in Table 1. While this result indicates that the relative size of subcollections could impact on agree -it appears that this is not the main cause of the results shown in Ta-ble 1. Additionally, it was noted that two subcollections, fbis and latimes, were of similar size, but across TRECs-6,7,8 the agree -was low (28%, 68%, 71%), which we further examine. To investigate why fbis and latimes had such a low agree -but similar sizes, a series of comparisons were made between the subcollections by varying the amount of random documents that were added to each subcollection. To facilitate exposition, we la-bel the new subcollections fbis-20, fbis-40, fbis-60 and fbis-80, where fbis-N is the same size as fbis, but contains N% documents pulled at random from the fbis subcollection, and 100-N% of doc-uments pulled from the rest of the TREC-8 collection. As in the other collections used in this paper, these collections all follow the construction methodology described in Section 3.2. For example, an instance of the fbis-20 collection includes a portion the same size as the non-relevant documents from fbis, but that portion is instead comprised of 20% documents selected randomly from the non-relevant documents in fbis, and 80% documents from the non-relevant documents that are not in fbis. Then, the all-rel collection is added, for a total size equal to the non-relevant portion of fbis, plus the size of the all-rel collection. Using this methodology, we also constructed latimes-20, latimes-40, latimes-60 and latimes-80. Figure 2: Agreement between collections for various percent-ages of source restricted splits using fbis and latimes from TREC8. The horizontal line indicates the agreement for a fully source based split.

For each value of N  X  X  20 , 40 , 60 , 80 } , 50 fbis and 50 latimes subcollections were formed and agree -SS a was measured (Figure 2). As collections become more source-based, there is a corre-sponding decrease in agreement, indicating that the source-based split is indeed contributing to the disagreement observed above.
This paper addresses the following two research questions. What are the causes of the measured differences between subcollections described in past papers? Do the measured differences still occur when assessing a state-of-the-art retrieval system?
Using a novel methodology, this work has confirmed what Sander-son et al. reported in an earlier paper [6]. There is good evidence that the choice of collection impacts on whether one retrieval sys-tem will be measured to be significantly better than another re-trieval system. Note also, this effect is visible even when a sub-set of a larger collection is used. The focus of work here has been to eliminate confounding factors introduced in Sanderson et al X  X  methodology, and to begin to understand the causes of the effect.
To that end, the work here has established that the measured in-consistency in evaluation across subcollections is not an effect of the number of relevant documents in that collection or the rela-tive difference in collection size. Additionally, the measured dif-ferences between subcollections also appear to still be occurring when using a state-of-the-art retrieval system and comparing be-tween variants of the same collection.

While it has long been understood that one should test on multi-ple collections in order to cover for unknown variations in test col-lections, there has been little or no work understanding what those variations are. Such an understanding could reduce the number of collections one needs to test on.

We contend, as Sanderson et al did, that the results here are quite striking. Many of the subcollections in the early years of TREC were on the surface, quite similar to each other. In TREC-7 for example, three of the four subcollections, fbis, latimes, and ft, are collections of news, just from different sources. We view this work as a starting point: the most important issue to address now is what is causing the inconsistency in measurement of effectiveness?
In addition, this work has implications when creating collections, since test collections are typically subcollections of the available documents appropriate for the collection. It is important to create test collections that agree with the real world of all documents, and therefore it is important to understand the factors that cause evalu-ations on subcollections to disagree with the original collection.
Towards understanding the inconsistency in evaluation, this work has demonstrated that both collection size and source of documents contribute to the reliability of experiments. Although document source is a meta-feature that does not describe specifically how the document is different, and is not typically used as a feature by rank-ing algorithms, document source has been used at collection cre-ation time, such as in the UK web spam collections X  X here only URLs ending in .uk were included [1].

Although the collection size and document source both strongly affect the consistency of evaluation, these features do not explain the whole effect. Some further underlying cause(s) must be affect-ing the results measured. Finding these will be our next focus.
Once we understand the effect well enough to predict whether particular subcollections will agree with each other (and with the whole collection that they are sampled from), many interesting ap-plications are possible. For example, these results will allow the creations of test collections that are confidently generalizable, i.e., subcollections that agree with the larger set of available documents. Additionally, by splitting existing collections into subcollections where different retrieval approaches perform best, increases in re-trieval performance may be possible.
 This work was supported in part by the Australian Research Coun-cil (DP130104007) and also by a Google Faculty Research Award. [1] C. Castillo, D. Donato, L. Becchetti, P. Boldi, S. Leonardi, [2] C. W. Cleverdon. Report on the Testing and Analysis of an [3] C. W. Cleverdon. The effect of variations in relevance [4] A. Moffat, F. Scholer, and P. Thomas. Models and metrics: [5] I. Ounis, G. Amati, V. Plachouras, B. He, C. Macdonald, and [6] M. Sanderson, A. Turpin, Y. Zhang, and F. Scholer.
 [7] A. Singhal, C. Buckley, and M. Mitra. Pivoted document [8] I. Soboroff. Test collection diagnosis and treatment. Proc. [9] J. Urbano, M. Marrero, and D. Mart X n. On the measurement [10] E. M. Voorhees. Topic set size redux. In Proc. SIGIR , SIGIR [11] C. Wilkie and L. Azzopardi. Best and fairest: An empirical
