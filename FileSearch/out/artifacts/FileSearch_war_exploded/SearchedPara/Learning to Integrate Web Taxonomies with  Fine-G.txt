 A web taxonomy, or directory, is a hierarchical collection of categories and docu-ments [2]. In the last decade, thousands of such taxonomies have been developed for various services, such as electronic auction markets, online book stores, electronic li-braries, and search engines. Yahoo! and Google Directories are two good examples. The benefits of taxonomies include encouraging the serendipitous discovery of infor-mation, improving navigation among related topics, and enhancing full-text search-ing. In a web taxonomy, a category X  X  concept is its parent X  X  sub-concept [9]. 
Many of these taxonomies cover similar topics and knowledge. In recent years, in-tegrating these taxonomies, which enables the reuse of information more efficiently and correctly, has become increasingly popular. For instance, Google News [1] col-omy, which is a typical example of assigning data from an existing taxonomy to an-other taxonomy. In B2B systems, millions of items need to be exchanged among thousands of taxonomies [8]. 
Given the enormous scale of the Web, manually integrating taxonomies is labor-intensive and time consuming. In recent years, various machine learning approaches, such as enhanced Na X ve Bayes [3], Co-Bootstrapping [18], and SVM-based ap-proaches [19] have been proposed. It is straightforward to formulate taxonomy inte-gration as a classification task [3]. Suppose we want to integrate the BBC News web site with the Google News web site. The simplest way would be to assign news arti-cles from BBC news to Google news based on the information contained in those ar-ticles. However, the relations between the categories in these two web sites could provide valuable information for assigning the articles. For example, if an article be-longs to the Sports category of BBC news, it is likely that the article also belongs to the Sports category of Google news. Unfortunately, the relations between two catego-standards for constructing taxonomies. In addition, taxonomies often overlap par-tially, as in Software and Open source_software , which could undermine the accu-racy of taxonomy integration. 
Our taxonomy integration approach exploits the relations between a category in the source taxonomy and a category in the target taxonomy to improve the classification performance. We also consider the issue of partial concept overlap. 
The remainder of this paper is organized as follows: In Section 2, we define the taxonomy integration task. In Section 3, state-of-the-art taxonomy technologies are briefly introduced. The features used in our taxonomy integration approach are pre-sented in Section 4. In Section 5, we describe our experiments, including the dataset, also indicate possible future research directions in Section 6. The web taxonomy integration task in Fig.1 was originally defined by Agrawal and omy to a target taxonomy. The terms used in this task include:  X  contains a set of documents.  X  contains a set of documents. paper, we follow the settings of Agrawal and Srikant [3] and Zhang and Lee [19] [18], which simply consider level-one categories in T as the target categories. ment to one or more target categories. However, in taxonomy integration, we have the tions between S and T can be used to enhance the accuracy of integration. For exam-that these two categories are similar. Thus, any document x in s i is likely to be catego-target categories can be measured by estimating the degree of overlap between them. However, we have to resolve the following problems: (1) how to estimate the degree of overlap, and (2) how to use this information. 
We now briefly introduce some state-of-the-art approaches that use the information about S, or the relations between S and T . The source taxonomy provides information about the relations between corresponding categories, including the documents in them. For taxonomy integration, these relations can be used to augment inadequate in-formation about the documents themselves. Zhang and Lee developed the cluster shrinkage algorithm (CS) [19], which combines information about documents in cate-gories of the same category. The authors estimated that CS can achieve a 15% im-provement over traditional SVM methods. 
The Enhanced Na X ve Bayes (ENB) algorithm [3] and Co-Bootstrapping (CB) algo-rithm [18] are the two main approaches that use the relations between the source and target taxonomies. The ENB algorithm, proposed by Agrawal and Srikant, initially used a Na X ve Bayes (NB) classifier [13] to estimate the degree of overlap between the source and target categories. The estimated scores were then combined with the prob-abilities calculated by a second NB classifier. According to Agrawal and Srikant [3], ENB is 15% more accurate than NB. 
Similarly, Co-Bootstrapping (CB) [18] exploits inter-taxonomy relationships by providing category indicator functions as additional features of documents. According to Zhang and Lee [18], CB achieves close to a 15% improvement over NB. We dis-cuss the above approaches in more detail in Section 4.2. We also use the relations between corresponding categories in S and T to enhance the integration process; however, unlike previous approaches, we do not consider a flat-tened taxonomy only, i.e., a taxonomy reduced to a single level [3].The relations be-tween the level-one categories in S and T could be noisier than the relations between lower-levels, since the concept space of higher-level categories in taxonomies is more general. Therefore, we employ some features used in machine learning models to ex-tract finer relations between lower-level categories in S and T .

In this section, we introduce five features used in our taxonomy integration ap-proach. One feature is commonly used in text classification, two are derived from other taxonomy integration systems, and the remaining two are our own. We then in-troduce the Maximum Entropy (ME) model, a well-known classifier used in many applications. The section concludes with a discussion of ME X  X  advantages and the process of our approach. 4.1 Features Feature selection is critical to the success of machine learning approaches. In this sec-tion, we describe the features used in our system and discuss the effectiveness of each feature. Word-TargetCat Features (WT) specifically, a distinct feature is initiated for each word-category combination. In ad-dition, if a word occurs often in one class, we would expect the weight for that word-category pair to be higher than if the word were paired with other categories. In text classification, features accounted for the number of times a word appears should im-prove classification. For example, Na X ve Bayes implementations that use word counts outperform implementations that do not [14]. Since taxonomy integration is an exten-sion of text classification, we adopt these features in our approach. For each word, w , feature as: the number of words in x .
 Normalized Word-TargetCat Features (NWT) small compared to its vocabulary size in the document. According to the definition in Equation 1, most Word-TargetCat features will be zero. Therefore, it would be diffi-tion, solving this problem is difficult, since no more information can be used. In tax-onomy classification, however, information about a document X  X  category in both the source taxonomy and the target taxonomy is available. For each word, w , we can add count. We regard this step as a kind of normalization, after which many zero Word-TargetCat features become non-zero values. Zhang and Lee [19] developed the cluster shrinkage (CS) algorithm to perform this normalization, which conceptually moves each document to the center of its level-one parent category. Zhang and Lee showed that this normalization significantly boosts the accuracy of taxonomy integration. Here, NWT is calculated by a modified version of CS. For each word w and category t  X  in the target taxonomy, T , we define the NWT feature as: number of words in x ; N ( c T ,, w ) is the number of times word w appears in x  X  X  level-strength of normalization effect. Normalized Fine-Grained Word-TargetCat Features (NFWT) As described in the last section, CS effectively helps Word-TargetCat features be-come non-zero values. That is, after applying CS, each document moves closer to its level-one parent category X  X  center. In some cases, this can augment the correct infor-mation for classifying documents. However, the level-one parent category usually contains words that are too general or belong to cross-domains. Therefore, the accu-racy improvement of taxonomy integration achieved by CS is reduced. In our ap-proach, we consider the hierarchy X  X  structure and regard the lowest-level parent cate-gory of the document as its category. Compared to the level-one parent category, the lowest-parent category contains more coherent information. As shown in Fig. 3, x is a lowest-level as the document X  X  category would avoid this potential error of Word-TargetCat features. 
For each word w and category t in the target taxonomy, T , we define NFWT as: number of words in x ; N ( c B , w ) is the number of times word w appears in x  X  X  lowest strength of normalization effect.. Normalized TargetCat-SourceCat Features (NTS) For taxonomy integration, there is another type of information that can be used to de-cide a document X  X  target category, namely, the relations between corresponding source categories and the target categories. Zhang and Lee [18] initiated a distinct fea-ture for each target-source category combination. In the training phase, documents in S are used to train a multi-class classifier. Then, for each document y in T , we use the classifier to decide y  X  X  category in S , denoted as s  X  . The feature corresponding to the or score for each document x in S , the feature corresponding to the combination of t  X  this feature boosts the classification accuracy. We implement such features as Nor-malized TargetCat-SourceCat features (NTS). For each category t  X  in the target tax-onomy T , and each category s  X  in the target taxonomy S , we define the NTS features as: Normalized Word-TargetCat-SourceCatSourceCat Features (NWTS) Although NTS improves the integration accuracy by relating each source and target not precise enough to achieve a significant improvement. Since different words can source combinations should be further divided by each distinct word. For each word w and category t  X  in the target taxonomy T , and category s  X  in the source taxonomy S , we define the normalized word count feature as: of normalization effect. 
We illustrate the use of the Word-TargetCat-SourceCat feature by the following tially in the conceptual space. However, we still do not know which documents of s 1 We need other information to know the relations between the lower-level categories. Therefore, we add the word dimension to the original NTS, to combine the dimen-sions of the target and source categories. 4.2 Using Features in Maximum Entropy We select the Maximum Entropy (ME) model [4] to implement our approach. ME is a statistical modeling technique used for estimating the conditional probability of a tar-get label by the given information. ME computes the probability, p ( o | h ), where o de-notes all possible outcomes from the space, and h denoted all possible histories from the space. A history is all the conditioning data that enables one to assign probabilities to the space of outcomes. In the taxonomy integration task, the history can be viewed as all information derivable from the documents of the taxonomy relative to the cur-rent document, and the outcome can be viewed as the target category label. The com-predictions about the outcome. 
Given a set of features and a training set, the ME estimation process produces a model where every feature i f has a weight  X  i . From Berger [4], we can compute the conditional probability as: The probability is derived by multiplying the weights of the active features (i.e., those f ( h,o ) = 1). The weight,  X  i, is estimated by a procedure called improved iterative scal-ing (IIS) [7], which improves the estimation of weights iteratively. The ME estima-tion technique guarantees that for every f i , the expected value of  X  i will equal the em-duced in Section 4.1. The process of our approach is shown in Fig. 3. Advantages of Maximum Entropy competitive performance in various tasks, including part-of-speech tagging [15], named entity recognition [5], English parser [6], prepositional phrase attachment [16], and text classification [14]. 
As noted in [5], ME allows users to focus on finding features that characterize the problem, while leaving feature weight assignment to the ME estimation routine. When new features are discovered, users do not need to reformulate the model as in other machine-based approaches, because the ME estimation routine automatically calculates new weight assignments. 
Although using the ME model is a good choice, other machine learning algorithms, such as Support Vector Machine [10], Conditional Random Field [11], or Boosting [17], could also be adopted in our approach to improve taxonomy integration. 4.3 The Algorithm of Our Approach: NFWT+NWTS Our proposed approach consists of NFWT and NWTS, which were introduced in Sec-tion 4.1. The procedure of NFWT+NWTS is shown in Fig.3. 
There are two steps in NFWT+NWTS. The first step uses labeled documents in S used to generate feature values representing the source-target relations, which are nec-transfer documents from S to T with both NWTS and NFWT features. Therefore, the shrinkage algorithm as well as NWTS, which considers word dimensions. 5.1 Datasets We collected five datasets from the Google and Yahoo! Directories to evaluate our approach. Each dataset included a category in Google Directory, the corresponding category with a similar topic in Yahoo! Directory, and vice versa. Hyperlinks and web five dataset names and their paths in the directories. 
In the Google and Yahoo! Directories, each link/document includes the web page X  X  title, URL, and description. For example: Title : BBC News URL : http://news.bbc.co.uk/
Description : offers U.K., world, business, science, and entertainment news. T: target taxonomy S: source taxonomy
Taxonomy_Integration_Main ( T, S ) 1: Use labeled documents in S to induce a ME-based classifier with NFWT features for transferring the document from T to S and then use these results to measure the similarity between those corresponding categories in S and T . 2: Use labeled documents in T to induce a ME-based classifier with NWTS and NFWT features for transfer-ring the document in S to T . 3: Return classification result ;
In the experiment, we used the information from the title, description, and content of the web page as the information for training and testing. All documents were pre-processed by removing the stop words, and stemming. Disease Top/Health/ 
Book Top/Shopping/Publications 
Movies Top/Arts/Movies/Genres/ Entertainment/ Garden Top/Shopping/ Outdoor Top/Recreation/Outdoors/ Recreation/Outdoors/ 
In Table 2, each row shows the dataset name, the number of links within each di-rectory, and the number of shared links between the two directories. In each dataset, the shared links, identified by their URLs, are used as the testing data, while the rest of the links are used as the training data. Only a small proportion of links are shared by the two web taxonomies, which shows the benefit of integrating them. 
The number of categories is shown in Table 3. As mentioned earlier, we use the level-one categories as the target classes in our classification task.
 Total # 80364 50349 9861 Total # 743 841 5.2 Experimental Design Our task is to classify documents from a source taxonomy into a target taxonomy. The experiment of each dataset consists of classifying a document from Google to Yahoo and vice versa. We use the documents of Yahoo (excluding the shared links) for train-ing and classifying documents from Google into Yahoo. Similarly, we use documents from Google (excluding the shared documents) for training and classifying documents from Yahoo into Google. The shared documents are used as testing data. 
To measure the correctness of all approaches, we defined the following classifica-tion accuracy: 5.3 Settings In the NB and ENB experiments, we implement the NB and ENB modules. The pa-rameter w of ENB is selected from a series of numbers: {0, 1, 3, 10, 30, 100, 300, and 1,000} that have the best performance. The smoothing parameter [3] of the NB and ENB classifier is set to 0.1. We use Maximum Entropy Toolkit [12] to implement the ME-based approaches. To compare our approach with normal text classification methods, we implement the ME-based text classification algorithm proposed by Kamal Nigam, John Lafferty, and Andrew McCallum [14]. We denote it as MEtext, which simply uses the Word-TargetCat features (WT). 
We compare the features of our approach with the features used in previous ap-proaches (NTS for [18] and NWT for [19] as discussed in Section 4). Although in previous works [18, 19], the features were implemented with other machine learning models, they can also be easily implemented with ME. The parameter  X  used in NWT, NFWT, and NWTS is set to 0.5. 5.4 Experimental Results In Table 4, we denote the ME-based text classification that only uses the WT feature as MEtext, and our proposed approach as NFWT+NWTS. To make a distinction be-NWTS . One can see that NFWT + NWTS performs significantly better than normal text classification approaches [14] in all five topics. These results suggest that our ap-proach can effectively exploit the relations between corresponding categories in the target and source taxonomies to enhance the classification accuracy. 
Next, we compare our approach ( NFWT + NWTS ) with NB and ENB. In Table 5, one can see that, as previous works showed, ENB performs slightly better than NB. However, our proposed approach, NFWT + NWTS , outperforms NB and ENB by 17% and 11%, respectively. These results show that referring to the relationships between taxonomies and replacing NB with ME can improve the accuracy of taxonomy inte-gration. The former is due to the high degree of relevance between the two taxono-mies. The latter is because ME can catch more dependencies among different features that commonly exist in text categorization and taxonomy integration problems. G to Y Y to G NB ENB NFWT G to Y Y to G 
Unlike other approaches, our approach retains the hierarchical structure of taxono-mies, and estimates the relationship between lower-level categories. Since the number of words on a web page may be significantly fewer than in a normal news article, the results of web page classification are more likely to be affected by the sparseness of words. The information in the source and target taxonomies can provide a great deal of help in smoothing the word frequency vectors of web pages, or measuring the simi-larity between source and target categories. 
One may further ask: How can information in the source and target taxonomies be used to achieve better performance? Next, we will compare our approach and previ-ous approaches on taxonomy integration. 
In Table 6, the major difference between NWT and NFWT is that NWT uses level-one categories in the cluster shrinkage algorithm, while NFWT uses the lowest-level categories. The experimental results suggest that using lower-level categories yields a better performance than level-one categories. This supports our observation that level-one categories usually contain words that are too general or belong to cross-domains, which could undermine the performance. Even though the NWT is not as efficient as NFWT , its performance is still better than MEtext, as shown in Table 4. 
Considering document dimension 
Considering word dimension 
Now, we compare the effects of two factors: (1) using level-one or lowest-level categories in the cluster shrinkage algorithm, and (2) using document or word dimen-sions to represent the source-target relations. Table 7 shows all combinations of these two factors: NWT+NTS, NWT+ NWTS , NFWT +NTS, and NFWT + NWTS . The con-figuration name is composed of the features it uses. For example, NWT +NTS means measure the source-target relations. In Table 8, we can see that NFWT +NTS outper-forms NWT+NTS, and NFWT + NWTS outperforms NWT+ NWTS . These results es-tablish that lowest-level categories contain more precise information for categoriza-tion than level-one. Therefore, configurations using lowest-level categories in the cluster shrinkage algorithm ( NFWT ) outperform those using level-one categories (NWT). In addition, we can see that NWT+ NWTS outperforms NWT+NTS, and NFWT + NWTS outperforms NFWT +NTS. These results demonstrate that using word-dimensions ( NWTS ) rather than document dimensions (NTS) to represent source-target relations could further alleviate the partial overlap problem. NWT + NTS NWT + NWTS NFWT + NTS NFWT + NWTS G to Y Y to G 
Generally speaking, using information of source category improves the categoriza-tion accuracy. We can see that NWT+NTS and NWT+ NWTS outperform NWT, and NFWT +NTS and NFWT + NWTS outperform NFWT . Among these four configura-tions, the performance of NWT+NTS is the worst, such as in the Garden category. We believe this is because the classification criteria of Google X  X  Garden directory is much different with that of Yahoo! X  X  Garden directory. As a result, the partial overlap prob-lem becomes very serious in Garden category. To further justify this argument, we compare the name of level-one categories of Garden in Yahoo! and Google. It is found that there is no common name between those categories in Google and Ya-hoo! X  X  Garden directory. From this observation, we conclude that, NWT+NTS, which uses level-one categories in the cluster shrinkage algorithm and considers only docu-ment dimension in measuring the source-target relations, is influenced most deeply by the different classification criteria between the source and target taxonomy. 
The experimental results of each taxonomy integration approach are shown in Fig. 4 and Fig. 5 respectively. In this paper, we have proposed an approach that effectively uses the relations be-tween corresponding categories in source and target taxonomies to improve taxonomy source, we utilize hierarchical information to extract fine-grained relations, which al-leviates the partial concept overlap problem in taxonomy integration. 
The proposed approach was tested using real Internet directories. Its performance works. The experimental results also support the assumption that using a flattened hi-erarchy could cause the loss of valuable information about relations between corre-sponding categories. 
In the future, more information, such as web resources, a third taxonomy, or exist-ing knowledge ontology could be incorporated into our approach. It would also be in-teresting to see how our approach can be applied to other applications. We are grateful for the support of National Science Council under grant NSC94-2752-E-001-001, and the support of the thematic program of Academia Sinica under grant AS91IIS1PP and 94B003. To facilitate both the understanding and the discovery of information, we need to utilize multiple sources of evidence, integrate a variety of methodologies, and combine human capabilities with those of the machine. The Web Information Discovery Integrated Tool (WIDIT) Laboratory at the School of Library and Information Science, Indiana University-Bloomington, houses several projects that employ this idea of multi-level fusion in the areas of information retrieval and knowledge discovery. This paper describes a Web search optimization study by the TREC research group of WIDIT, who explores a fusion-based approach to enhancing retrieval performance on the Web. In the study, we employed both static and dynamic tuning methods to optimize the fusion formula that combines multiple sources of evidence. By static tuning, we refer to the typical stepwise tuning of system parameters based on training data.  X  X ynamic tuning X , the key idea of which is to combine the human intelligence, especially pattern recognition ability, with the computational power of the machine, involves an interactive system tuning process that facilitates fine-tuning of the system parameters based on the cognitive analysis of immediate system feedback. The rest of the paper is organized as follows. The next section discusses related work in Web information retrieval (IR). Section 3 details the WIDIT approach to Web IR, followed by the description of our experiment using the TREC .gov data in section 4 and the discussion of results in section 5. Information discovery on the Web is challenging. The complexity and richness of the Web search environment call for approaches that extend conventional IR methods to leverage rich sources of information on the Web. Text Retrieval Conference (TREC) has been a fertile ground for cutting-edge Web information retrieval (IR) research in a standardized environment. The Web IR experiment of TREC, otherwise known as retrieval task as was done previously with plain text documents. Although many TREC participants explored methods of leveraging non-textual sources of information such as hyperlinks and document structure, the general consensus among the early perform as well as the content-based retrieval methods fine-tuned over the years Rasolofo, 2001). 
There have been many speculations as to why link analysis, which showed much promise in previous research and has been so readily embraced by commercial Web speculations point to potential problems with Web track X  X  earlier test collections, Singhal &amp; Kazkiel, 2001), and relevance judgments that penalize the link analysis by (Singhal &amp; Kazkiel, 2001), to unrealistic queri es that are too detailed and specific to be representative of real world Web searches (Singhal &amp; Kaszkiel, 2001). 
In an effort to address the criticism and problems associated with the early Web track experiments, TREC abandoned the ad-hoc Web retrieval task in 2002 in favor of topic distillation and named page finding task and replaced its earlier Web test collection of randomly selected Web pages with a larger and potentially higher quality domain-specific collection 1 . The topic distillation task in TREC-2002 is whose name is described by the query (Hawking &amp; Craswell, 2002; Craswell &amp; Hawking, 2003). Adjustment of the Web track environment brought forth renewed interest in retrieval approaches that leverage Web-specific sources of evidences such as link structure and document structure. 
For the home page finding task, where the objective is to find the entry page of a specific site described by the query, Web page X  X  URL characteristics, such as its type specific page on the Web, the use of anchor text still proved to be an effective strategy still seemed to be a useful resource, especially as a mean to boost the performance of content-based methods via fusion (i.e. result merging), although the level of its Craswell, 2002; Craswell &amp; Hawking, 2003). Various site compression strategies, 2003). It is interesting to note that link analysis (e.g. PageRank, HITS variations) has not yet proven itself to be an effective strategy and the content-based method seems to TREC-2002 topic distillation task were achieved by the baseline systems that used only the content-based methods (MacFarlane, 2003; Zhang et al., 2003). 
In our earlier studies (Yang, 2002a; Yang, 2002b), where we investigated various fusion approaches for ad-hoc retrieval using the WT10g collection, we found that did not enhance retrieval performance in general. TREC participants in recent Web track environment, however, found that use of non-textual information such as hyperlinks, document structure, and URL could be beneficial for specific tasks such only due to the change in the retrieval environment (i.e. test collection, retrieval task) but also the result of more dynamic approach to combining multiple sources of evidence than straightforward result merging. Based on the assumption that the key to effective Web IR lies in exploiting the richness of Web search environment by combining multiple sources of evidence, we focused our efforts on extending and optimizing the fusion methods. First, we combined multiple sets of retrieval results generated from multiple sources of evidence (e.g. body text, anchor text, header text) and multiple query formulations using a weighted sum fusion formula, whose parameters were tuned via a static tuning process using training data. The ranking of the fusion result was then  X  X ptimized X  via a dynamic tuning process that involved iterative refining of fusion formula that combines the contributions of diverse Web-based evidence (e.g. hyperlinks, URL, document structure). The dynamic tuning process is implemented as a Web application; where interactive system parameter tuning by the user produces in real time the display of system performance changes as well as the new search results annotated with metadata of fusion parameter values (e.g. link counts, URL type, etc.). The key idea of dynamic tuning, which is to combine the human intelligence, especially pattern recognition ability, with the computational power of the machine, is implemented in this Web application that allows human to examine not only the immediate effect of his/her system tuning but also the possible explanation of the tuning effect in the form of data patterns. 3.1 WIDIT Web IR System Architecture WIDIT Web IR system consists of five main modules: indexing, retrieval, fusion (i.e. result merging), query classification, and reranking modules. The indexing module processes various sources of evidence to generate multiple indexes. The retrieval module produces multiple result sets from using different query formulations against multiple indexes. The fusion module, which is optimized via the static tuning process, combines result sets using weighted sum formula. The query classification module uses a combination of statistical and linguistic classification methods to determine query types. The reranking module uses query type-specific reranking formulas optimized via dynamic tuning process to rerank the merged results. Figure 1 shows an overview of WIDIT Web IR system architecture. 3.2 Indexing Module WIDIT preprocesses documents by removing HTML tags and stopwords and consist of non-meaningful words such as words in a standard stopword list, non-words that contain 3 or more repeated characters. Hyphenated words are split into parts before applying the stopword exclusion, and acronyms and abbreviations are kept as index terms 3 . 
In addition to extracting body text terms (i.e. terms between &lt;body&gt; and &lt;/body&gt; tags), WIDIT extracts terms from document title, meta keywords and descriptions, extracting terms from the anchor texts of incoming links. Thus, WIDIT creates three sets of term indexes: first based on document content (i.e. body index), second based on document structure (header index), and third based on link structure (anchor index). 
In order to enable incremental indexing as well as to scale up to larger collections, each of the indexes consists of smaller subcolllections, which are created and searched in parallel. The whole collection term statistics, which are derived after the creation of the subcollections, are used to compute the term weights so that subcollection retrieval results can simply be merged by retrieval scores. 3.3 Query Classification Module The task of our query classification module is to categorize a Web query into one of three query types: topic distillation (TD), homepage finding (HP), and named page finding (NP). We suspected that machine learning approaches may not be very effective in classifying Web queries, which tend to be short (Silverstein et al., 1998; H X lscher &amp; Strube, 2000). Consequently, we decided to combine the statistical training data 4 . For example, we noticed that queries that end in all uppercase letters tended to be HP, queries containing 4-digit year were more likely to be NP, and TD queries were shorter in general than HP or NP queries. We also identified some word bureau, etc.) query types. After constructing the linguistic classifier, we combined the arrived at the query classification in the following manner: 3.4 Retrieval Module The retrieval component of WIDIT implements both Vector Space Model (VSM) using the SMART length-normalized term weights and the probabilistic model using the Okapi BM25 formula. Documents are ranked in decreasing order of the inner product of document and query vectors, and t is the number of terms in the index. 
For the VSM implementation, SMART Lnu weights with the slope of 0.3 are used retrieval given a document length with the probability of relevance given that length (Singhal et al., 1996). Equation (2) describes the SMART formula, where d ik is the document term weight document frequency of term k , and t is the number of terms in document or query.
The simplified version of the Okapi BM25 relevance scoring formula (Robertson &amp; Walker, 1994), which is used to implement the probabilistic model, is described in document frequency, dl is the document length, avdl is the average document length, and k1, b, k3 are parameters (1.2, 0.75, 7 to 1000, respectively). 
Multiple sets of queries, resulting from various query formulation methods (e.g. noun extraction, phrase identification, synonym expansion) are applied against the merged by the fusion module 6 . 3.5 Fusion Module The fusion module combines the multiple sets of search results after retrieval time. In addition to two of the most common fusion formulas, Similarity Merge (Fox &amp; Shaw, 1995; Lee, 1997) and Weighted Sum (Bartell et al., 1994; Thompson, 1990), WIDIT employs variations of the weighted sum formula. The similarity merge formula multiplies the sum of fusion component scores for a document by the number of fusion components that retrieved the docume nt (i.e. overlap), based on the assumption that documents with higher overlap are more likely to be relevant. Instead of relying on overlap, the weighted sum formula sums fusion component scores weighted with the relative contributions of the fusion components that retrieved them, which is typically estimated based on training data. Both formulas compute the fusion score of a document by a linear combination of fusion component scores. 
In our earlier study (Yang, 2002b), similarity merge approach proved ineffective when combining content-and link-based results, so we devised three variations of the weighted sum fusion formula, which were shown to be more effective in combining fusion components that are dissimilar (Yang, 2002a). Equation (4) describes the simple Weight Sum (WS) formula, which sums the normalized system scores multiplied by system contribution weights. Equation (5) describes the Overlap Weight Sum (OWS) formula, which multiplies the WS score by overlap. Equation (6) describes the Weighted Overlap Weighted Sum (WOWS) formula, which multiplies the WS score by overlap weighted by system contributions: where: FS = fusion score of a document, The normalized document score, NS i , is computed by Lee X  X  min-max formula (1997), maximum and minimum document scores by method i . determination of the optimum weights for each system ( w i ). In order to optimize the fusion weights, WIDIT engages in a static tuning process, where various weight evaluated with the training data of past Web track results in a stepwise fashion. 3.6 Reranking Module based on the content-and link-based evidences (e.g. hyperlinks, URL, document boosting rules arrived at by dynamic tuning process involving interactive retrieval and manual system tuning in real time. The dynamic tuning process is applied to the best single and best fusion systems to  X  X une X  the ranking heuristic. 
The dynamic tuning component of WIDIT produces retrieval results that display individual scores for each source of evidence such as inter/intrasite in/outdegree, phrase/proximity match counts in body/header/anchor texts, and query term matches in URL as well as ranking and retrieval scores before/after the adjustment of reranking parameters by dynamic tuning. 3.6.1 Reranking Factors TREC participants found various sources of evidence such as anchor text (Craswell, Hawking &amp; Robertson, 2001; Hawking &amp; Craswell, 2002; Craswell &amp; Hawking, analysis of our previous Web IR studies, we decided to focus on four categories of the reranking factors. The first category is the field-specific match, where we score each document by counting the occurrences of query words (keyword, acronym, phrase) in URL, title, header, and anchor texts. The second category of reranking factors we use is the exact match, where we look for exact match of query text in title, header, and anchor texts (exact), or in the body text (exact2) of documents. The third category is link-based, where we count documents X  inlinks (indegree) and outlinks (outdegree). The last category is the document type, which is derived based on its URL to the one used in query classification. 3.6.2 Dynamic Tuning The dynamic tuning interface is implemented as a Web application (Figure 2); where system performance changes as well as the new search results annotated with metadata of fusion parameter values (e.g. link counts, URL type, etc.). 
The key idea of dynamic tuning, which is to combine the human intelligence, especially pattern recognition ability, with the computational power of the machine, is implemented in this Web application that allows human to examine not only the immediate effect of his/her system tuning but also the possible explanation of the process that successively fine-tune the fusion parameters based on the cognitive analysis of immediate system feedback, WIDIT can increase system performance without resorting to an exhaustive evaluation of parameter combinations, which can not only be prohibitively resource intensive with numerous parameters but also fail to produce the optimal outcome due to its linear approach to fusion components combination. 
The dynamic tuning interface, as can be seen in Figure 1, has a navigation pane on the left with query numbers 7 , a click of which will populate the main display pane on the right. The main display pane has three horizontal components: side-by-side performance scores for original and reranked retrieval results at the top, weight the ranked list of retrieved documents with individual fusion component scores at the bottom. The main idea is to discover patterns in fusion component scores across ranks that can be leveraged into improving retrieval performance by fine-tuning the fusion into an effective weighting function is a trial-and-error process guided by a real-time analysis of identified patter+ns suggests reranking heuristic that goes beyond a simple ranked fixed). In such cases, one must update the fusion formula component of the main display pane to accommodate the devised reranking heuristic. The dynamic tuning process as a whole is iterative because new patterns emerge with each refinement of the fusion formula until the performance stabilizes. In order to evaluate the effectiveness of the WIDIT approach to Web IR, we conducted a series of retrieval experiments using the TREC-2004 .gov test collection, which consists of 1.25 million Web pages (18 GB) in .gov domain and 225 queries of mixed type (75 TD, 75 HP, 75 NP) and associated relevance judgments. First, we tested our query classification strategy, followed by the evaluation of fusion and reranking strategies. 4.1 Query Classification We used TREC-2004 Web queries to evaluate our query classification approach after training the query classification module on TREC-2003 Web queries. The main challenge of the query classification task stemmed from the short length of the queries, which contained only three words on the average (Table 1). We suspected that machine learning approaches may not be very effective in classifying texts with only a few words. Furthermore, the quality and the quantity of the training data available from previous years also seemed suboptimal for machine learning. There were 100 TD training queries compared to 300 HP and 295 NP queries, which were also short in length (Table 2) and appeared to be often ambiguous upon manual examination. 
Query Length 
To supplement the training data for automatic classifiers, which had three times as many HP and NP than TD queries, we created a lexicon of US government topics by manually selecting keywords from the crawl of the Yahoo! X  X  U.S. Government category. We tested Na X ve Bayes and SVM classifiers with the Yahoo-enriched performance by the combination classifier. 4.2 Static and Dynamic Tuning for Fusion Having engaged in the query classification, our approach to handling the mixed query types was based on optimizing retrieval strategy for each of the query types. To leverage the multiple sources of evidence, we created separate document indexes for optimized via a static tuning process, where search results were combined using weighted sum with various weights without regards to query types. 
After the fusion optimization by static tuning, we employed a post-retrieval rank-boosting strategy to rerank the merged results for each query type using the devised a static reranking approach based on previous TREC research. Our static approach to query type-specific reranking was as follows: boost the rank of potential homepages if the query is topic-distillation or homepage finding type; boost the rank of pages with keyword matches if the query is hompage or named page finding type. More specifically, our rank boosting heuristic kept top 5 ranks static, while boosting the ranks of potential homepages (identified by URL type determination) as well as pages with keyword matches in document titles and URLs. 
We performed a series of dynamic tuning sessions using TREC-2003 training data, on real time evaluation of retrieval results. In contrast to static tuning, dynamic tuning process allows tuning of systems with numerous parameters by leveraging human intelligence. The main components of reranking heuristic we used were inter/intrasite large indegree for home/named page finding), phrase/proximity match (e.g. boost boost to top 10 rank if acronym match in URL). The reranking heuristics for home/named page finding task also involved the query classification component, which assigned different emphasis on evidence sources according to the query type. 
The effective reranking factors observed from the iterations of dynamic reranking were: indegree, outdegree, exact match, and URL/Pagetype with the minimum number of outdegree of 1 for HP queries; indegree, outdegree, and URLtype for NP queries (1/3 impact of HP factors); acronym, outdegree, and URLtype with the minimum number of outdegree of 10 for TD queries. In addition to harnessing both the human intelligence and machine processing power to facilitate the process of system tuning with many parameters, dynamic tuning turned out to be a good tool for failure analysis. We examined severe search failure instances by WIDIT using the dynamic tuning interface and observed the following: Table 3 shows the retrieval results using the TREC-2004 mixed Web queries. The best fusion run (F3) combined the best individual result, which used anchor text index anchor, and header index results. The fusion improved the baseline performance by 31% for TD, 13% for NP, and 42% for HP queries. The static reranking run (SR_o) shows only a slight improvement over the fusion run, but the dynamic reranking run shows improvements of 39% for TD, 7% for NP, and 47% for NP over the fusion result. It is clear from the table that both static tuning for fusion and dynamic tuning for post-retrieval reranking are effective system performance optimization methods for leveraging diverse sources of evidence in Web IR. B1: Best individual run F3: Best fusion run SR_o: Static reranking run using the official query type DR_o: Dynamic reranking run using the official query type 
In order to assess the effect of query classification error, we generated random assignment of query types (DR_r) and worst possible assignment of query types (DR_b). Table 4.1 compares the classification error of WIDIT query classification algorithm with random and worst classification. Because TD task is biased towards both known-item search task, HP-NP error is less severe than NP-TD, which is the query classification, we can see that random or poor query classification will adversely affect the retrieval performance. Table 4.2 also shows the random query type results to be comparable with TREC median performance for TD and HP queries. DR_o: Dynamic reranking run using the official query type DR_g: Dynamic reranking run using the guessed query type DR_r: Dynamic reranking run using the random query type 
DR_b: Dynamic reranking run using the bad query type We believe fusion is a promising area of investigation for Web IR. Our results show that exploiting the richness of Web search environment by combining multiple sources of evidence is an effective strategy. We extended the conventional fusion approach by introducing the  X  X ynamic tuning X  process with which to optimize the fusion formula that combines the contributions of diverse sources of evidence on the Web. By engaging in iterative dynamic tuning process, where we successively fine-tuned the fusion parameters based on the cognitive analysis of immediate system feedback, we were able to significantly increase the retrieval performance. 
