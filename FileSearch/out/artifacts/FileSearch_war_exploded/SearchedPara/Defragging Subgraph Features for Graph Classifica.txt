 Graph classification is an important tool for analysing struc-tured and semi-structured data, where subgraphs are com-monly used as the feature representation. However, the number and size of subgraph features crucially depend on the threshold parameters of frequent subgraph mining algo-rithms. Any improper setting of the parameters will gen-erate many trivial short-pattern subgraph fragments which dominate the feature space, distort graph classifiers and bury interesting long-pattern subgraphs. In this paper, we propose a new Subgraph Join Feature Selection ( SJFS ) al-gorithm. The SJFS algorithm, by forcing graph classifiers to join short-pattern subgraph fragments, can defrag trivial subgraph features and deliver long-pattern interesting sub-graphs. Experimental results on both synthetic and real-world social network graph data demonstrate the perfor-mance of the proposed method.
 H.2.8 [ Data Management ]: Database Applications X  Data Mining Graph Classification; Subgraph Join; Feature Selection
Graph mining is an important branch of the data min-ing research. Compared with vectorial data, graph data do not have features readily available to fit into popularly used classifiers such as SVMs and decision trees. Therefore, many research efforts [1, 2, 4] have been focused on selecting dis-criminative subgraphs as vectorial features for graph classi-fication.

However, generating subgraph features using existing fre-quent subgraph mining algorithms brings new challenges. The number and size of subgraph features crucially depend on the threshold parameters in frequent pattern mining al-gorithms. A common situation is that we don X  X  have the c  X  Fi gure 1: An illustration of a long-pattern subgraph feature buried under two short-pattern subgraph features in the information cascade data. Consider four graphs g 1 ; : : : ; g 4 . g 1 and g 2 from class X +1 X  X hile g 3 and g 4 from  X -1 X . Assume we have two short-pattern subgraphs f 1 : U 1  X  U 2 and f 2 : U 2  X  U 3 , and a long-pattern subgraph f 3 : U 1  X  U 2  X  U 3 by join-ing f 1 and f 2 . If one feature is allowed to select for classification, then f 1 or f 2 is likely to be selected, instead of the more interesting f 3 . prior knowledge of the best threshold parameters and usu-ally set it to be a small number such that many possible candidate subgraph features can be preserved, and we ex-pect the following steps of graph classification and feature selection can remove unnecessary subgraph features. How-ever, an unpleasant situation is that many algorithms will be trapped into short-pattern subgraph fragments, and the interesting long subgraph patterns are buried behind the fragments.

This research is motivated by a recent work on extract-ing subgraphs from a real-world social network cascade data set for network outbreak prediction. The data set is down-loaded from http://snap.stanford.edu/infopath/ . In the ex-periments, we observed that the feature space is dominated by subgraph fragments due to the downward closure prop-erty of the frequent subgraph mining algorithms [3], i.e. , the short-pattern subgraph fragments are always more frequent than long-pattern interesting subgraphs. Fig. 1 shows the example that a long-pattern subgraph feature f 3 is buried under two subgraph fragments f 1 and f 2 and a classifier is likely to choose the two subgraph fragments instead of the more interesting long-pattern subgraph feature which reflects a latent social group.

To discover long-pattern subgraphs buried under a huge number of subgraph fragments, we need to systematically design a new feature selection method. Generally, we set a small value of the frequency support threshold, such that all candidate subgraphs can be preserved in the feature space. However, this method will unavoidably generate an expo-nential number of short-pattern subgraph fragments which bury interesting long-pattern subgraph features. Therefore, we need to design a new subgraph join approach that can select long-pattern subgraphs. The challenges of designing such a subgraph join approach are as follows: To solve the challenges, we propose in this paper a new Subgraph Join Feature Selection algorithm (SJFS for short). To solve Challenge 1 , SJFS uses a correlated subgraph join method to defrag short-pattern subgraph fragments (Defini-tion 3 in Section 2). To solve Challenge 2 , new constraints of joining subgraphs are added to the max-margin graph classifier (Eq.(1) in Section 3) so that the classifier prefers long-pattern subgraphs. To solve Challenge 3 , we evaluate the proposed algorithm using both synthetic and real-world social network cascade data (Section 4).
We consider directed and acyclic graphs (DAG). Given a directed acyclic graph G = ( V; E ), g i = ( V  X  ; E  X  ) is a sub-graph of G , i.e. , g i  X  G , iff V  X   X  V , and E  X   X  E . If g subgraph of G , then G is a supergraph of g i . We use lower-case bold letters to represent vectors and upper-case bold letters to represent matrices. For example, the symbol e rep-resents a unit vector with all entries being 1. For a binary classification problem, the purpose is to learn a classification boundary from a training graph set { ( G k ; y k ) } , 1  X  where each G k is a training graph with class label denoted by y k  X  X  X  1 ; +1 } .

Definition 1. ( Subgraph Features ) Let S = { g 1 ;  X  X  X  ; g be a subgraph feature space. Then, each graph G k can be encoded as an m -dimensional vector x k with each element x (1  X  u  X  m ) denoted by x u k = I ( g u  X  G k ) ;  X  g u  X  where I (  X  ) equals to 1 if the condition satis es, otherwise, 0. Fig. 2 explains the generation of a subgraph feature space. Consider a graph A  X  B  X  X  X  X  E . The graph contains subgraphs B  X  C and C  X  D  X  E . Then, the elements in the binary subgraph feature vector are set to 1. Fi gure 2: The subgraph feature space. The graph (left) is converted to a binary feature vector (right) by examining the existence of all subgraphs based on Definition 1.
 Fi gure 3: The join of correlated subgraph fragments.
Note the concatenation of two subgraphs is uncertain be-cause of graph isomorphic [5]. In this paper, we only con-sider to join correlated subgraphs given in Definition 2, be-cause correlated subgraphs have a high probability to gener-ate interesting long-pattern subgraphs. The join result given in Definition 3 is unique, as shown in Fig. 3.

Definition 2. ( Correlated Subgraphs ) Consider two di-rected acyclic subgraphs g i = { V  X  ;  X  X  X  ; V  X  } and g j V  X  } , if V lated subgraphs.

Definition 3. ( Correlated Subgraphs Join (CSJ) ) If g = { V  X  ;  X  X  X  ; V  X  } and g j = { V  X   X  ; V  X   X   X  X  X  ; V  X  lated subgraphs, then g i is a new generated subgraph after CSJ, where catenation operation.

Given a set of graphs, the dimension of subgraph feature space exponentially increases w.r.t. graph size. Therefore, it is impractical to use all derived subgraphs as features. In-spired by the frequent substructures popularly used in graph classification, we use frequent subgraphs to prune trivial subgraph patterns.
We present a Subgraph Join Feature Selection algorithm (SJFS) based on a max-margin graph classifier and the CSJ definition. The proposed SJFS algorithm can handle ex-tremely high dimensional subgraph features. Subgraph join generates long-pattern subgraphs without actually generat-ing all candidate subgraphs. After subgraph concatenation, discriminative subgraph features can be selected.
We introduce a feature scaling vector = [ 1 ;  X  X  X  ; m ] T  X  { 0 ; 1 } m with || || 1 = feature is selected ( j = 1) or not ( j = 0). Thus, at most B subgraphs are selected. Given a graph G i , we impose = [ 1 ;  X  X  X  ; m ] T to a re-scaled example  X  x i = x i x  X  represents the element-wise product between vectors x i and . Therefore, the decision function is defined as, f ( x i ) = w T ( x i  X  ), where w = [ w 1 ;  X  X  X  ; w m ] T vector.

Formally, we add extra constraints in the graph classifier to select long-pattern subgraphs by joining short-pattern fragments. The new constrain is stated as: if two short-pattern subgraph fragments p a and p b have been selected, denoted by p a = 1 and p b = 1, then the derived pattern p  X  p b is likely to be selected, denoted by p on the constraint, we obtain a new long-pattern driven max-margin graph classifier as follows, where w  X  R m and b determine the classification boundary,  X  0 is the empirical error of example x i ,  X  = {  X  R m | || || 1  X  B; j  X  X  0 ; 1 } ; j = 1 ;  X  X  X  ; m } be the domain of , and C is a trade-off parameter. The problem is non-convex with respect to w and simultaneously.

The new constraint shows that if two subgraph fragments can be concatenated according to Definition 3, then the new subgraph will be set to 1. When is fixed, Eq. (1) degener-ates to a standard SVM.

By introducing the Lagrangian multiplier i  X  0 to each constraint y i ( w T ( x i  X  + b )  X  1  X  i and setting the deriva-tives of the Lagrange function to 0 with respect to parame-ters w ; and b , respectively, we obtain, w = ) ; 1
Plugging the above results back into Eq.(1), and inter-changing the order of min  X  D and max  X  X  based on the minimax saddle-point theorem, we obtain the dual form of the original problem as follows, where A = { |
Apparently, the primal problem in Eq.(1) can be equiva-lently formulated as its dual given in Eq.(2). Eq.(2) has only two variables and , and it is linear with respect to and convex w.r.t. , which can be solved by a block coordinate descent approach that alternates between and .

Given a fixed d , the optimization problem in Eq. (2) is reduced as follows: ( Optimization 1 : fix and solve ) where  X  x i is re-scaled by the given , i.e. ,  X  x i = x i to the sparsity of scaler , the above problem can be solved by using the standard quadratic programming with a small set of features  X  x i .

When the variable is determined, we select at most B features as in Eq. (4). ( Optimization 2 : fix and solve )
To solve Eq. (4), we define a score function to denote the weight of each feature, i.e., c ( ) = Based on the above definition of feature weight, we have  X   X  mization problem in Eq. (4) can be further converted to a linear programming problem with respect to as follows:
Eq. (5) can be solved analytically. First, we construct a feasible solution by finding the largest scores [ c j ( )] we set the scaler j to 1 and the remaining to 0.
Note that Eq. (5) can be solved efficiently by using online linear programming where the constraint matrix is revealed column by column along with the objective function, i.e. , the features are processed column by column in a one-scan manner.

Table 1 shows the three different types of results when gen-erating a long-pattern subgraph p ab from two short-pattern subgraph fragments p a and p b . For example, consider the four training graphs g 1 ;  X  X  X  ; g 4 , where g 1 and g 2 belong to the same group while g 3 and g 4 fall into another group. Assume we have obtained two subgraph fragments P ( a ) = A  X  B and P ( b ) = B  X  C . Then, we can generate a long-pattern subgraph A  X  B  X  C based on Definition 3. The classifier may have three different types of results, Equal, Improve and Reduce. Therefore, we have the following in-tuitive conclusion.

Observation 1. Consider two subgraph fragments p a = 1 and p b = 1, if p ab = p a  X  p b , then the generated long-pattern subgraphs p ab can be used to replace the original two patterns p a and p b .
To validate the performance, we use four synthetic datasets and one real-world social network information cascade data for testing. The real world data contains about 27.6 million news articles and blog posts from 3.3 million online sources. We select cascades having more than 300 nodes as the pos-itive class (877 cascades in total), and cascades with nodes less than 100 as the negative class (27,515,721 cascades in total). Then, we randomly select 877 negative cascades com-bined with the 877 positive cascades for experiments. The purpose is to extract interesting long-pattern subgraph fea-tures for cascade data classification and prediction.
We compare our algorithm with two methods: 1) sub-graph feature selection without subgraph join, denoted by SFS , and 2) random subgraph feature selection, denoted by RSFS . The accuracy, F1 score, precision and recall are used as measures under support thresholds of 30, 50, 70 and 90. The RSFS algorithm selects B features randomly under the standard SVM classifier. h ttp://snap.stanford.edu/infopath/ Figure 4: Parameter study w.r.t. the number of selected features and subgraph features. Figure 5: Percentage of patterns w.r.t. support threshold and pattern length.

Parameter study: We test SJFS w.r.t. the number of selected features B and the number of subgraph features. From Fig. 4 (i), we observe that when B equals to 30% of all the generated features, SJFS achieves the best performance. Fig. 4 (ii) shows the number of subgraph features decreases when the support threshold increases.

Experimental results : Fig. 5(i) shows the proportion of short-pattern subgraph features (length &lt; 3) and long-pattern subgraph features (length  X  3) when support equals to 30. Fig. 5 (ii) shows the comparisons of long-pattern subgraphs. The performance w.r.t. the support thresholds on the real world data is given in Figs. 6 and 7. Table 2 gives the result on the synthetic data set. From these results, we have four interesting observations: 1) SJFS selects more long-pattern subgraphs and has more long-pattern subgraphs under dif-ferent support thresholds than SFS. 2) The classification Fi gure 6: Accuracy and F1 score w.r.t. support threshold.
 Fi gure 7: Precision and Recall w.r.t. support threshold. precision, accuracy and F1 score decrease when the support threshold increases. This is because more information will be available when the support threshold increases. 3) The results of precision, accuracy, F1 score and recall show that SJFS and SFS mostly outperform RSFS. SJFS and SFS have similar results. 4) The results in 3) validate our discussion in Table 1. The column  X  X mprove X  indicates that SJFS has higher accuracy than SFS, while columns  X  X qual X  and  X  X e-duce X  explain why SJFS and SFS have similar results.
We have studied a new problem of defragging subgraph features for graph classification. The problem is motivated by our recent study on extracting subgraph features from so-cial network cascade data for network outbreak prediction. We have observed that short-pattern subgraph fragments al-ways dominate feature space due to the downward closure property of the frequent subgraph mining algorithms [3] and long-pattern interesting subgraphs are often buried under subgraph fragments. In this paper we have proposed a new Subgraph Join Feature Selection ( SJFS ) algorithm as the solution. SJFS joins subgraph fragments during graph clas-sification and delivers long-pattern subgraphs. Experiments have validated the performance of the proposed method.
This work was supported by the Australian Research Coun-cil (ARC) Discovery Project under Grant No. DP140100545 and DP140102206, and ARC Future Fellowship FT130100746. [1] W. Zheng, L. Zou, et al. Efficient Subgraph Skyline [2] X. Kong, P.S. Yu. Semi-Supervised Feature Selection [3] X. Yan, X. Zhou, J. Han, Mining closed relational [4] X. Yan, and J. Han. gSpan: graph-based substructure [5] Y. Zhu, L. Qin, et al. High efficiency and quality: large
