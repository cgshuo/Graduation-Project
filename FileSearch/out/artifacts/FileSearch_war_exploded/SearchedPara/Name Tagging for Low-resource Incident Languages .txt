 In many emergent situations such as disease out-breaks and natural disasters, there is great demand to rapidly develop a Natural Language Processing (NLP) system, such as name tagger, for a  X  X urprise X  Incident Language (IL) with very few resources. Traditional supervised learning methods that rely on large-scale manual annotations would be too costly.
Let X  X  start by investigating how a human would discover information in a foreign IL environment. When we are in a foreign country, even if we don X  X  know the language, we would still be able to guess the word  X  gate  X  from the airport broadcast based on its frequency and position in a sentence; guess the word  X  station  X  by pattern mining of many subway station labels; and guess the word  X  left  X  or  X  right  X  from a taxi driver X  X  GPS speaker by matching move-ment actions. We designed a  X  Tibetan Room  X  game, similar to  X  Chinese Room  X  (Searle, 1980), by ask-ing a human user who doesn X  X  know Tibetan to find persons, locations and organizations from some Ti-betan documents. We designed an interface where test sentences are presented to the player one by one. When the player clicks token, the interface will dis-play up to 100 manually labeled Tibetan sentences that include this token. The player can also see trans-lations of some common words and a small gazetteer of common names (800 entries) in the interface. 14 players who don X  X  know Tibetan joined the game. Their name tagging F-scores ranged from 0% to 94%. We found that good players usually bring in some kind of  X  expectations  X  derived from their own native languages, or general linguistic knowl-edge, or background knowledge about the scenario. Then they actively search, confirm, adjust and up-date these expectations during tagging. For exam-ple, they know from English that location names are often ended with suffix words such as  X  city  X  and  X  country  X , so they search for phrases starting or end-ing with the translations of these suffix words. After they successfully tag some seeds, they will continue to discover more names based on more expectations. For example, if they already tagged an organization name A , and now observe a sequence matching a common English pattern  X  [A(Organization)]  X  s[Ti-tle] [B (Person)]  X , they will tag B as a person name. And if they know the scenario is about Ebola, they will be looking for a phrase with translation simi-lar to  X  West Africa  X  and tag it as a location. Sim-ilarly, based on the knowledge that names appear in a conjunction structure often have the same type, they propagate high-confidence types across multi-ple names. They also keep gathering and synthe-sizing common contextual patterns and rules (such as position, frequency and length information) about names and non-names to expand their expectations. For example, after observing a token frequently ap-pearing between a subsidiary and a parent organiza-tion, they will predict it as a preposition similar to  X  of  X  in English, and tag the entire string as a nested organization.

Based on these lessons learned from this game, we propose to automatically acquire and encode expec-tations about what will appear in IL data (names, pat-terns, rules), and encode those expectations to drive IL name tagging. We explored various ways of sys-tematically discovering and unifying latent and ex-pressed expectations from nontraditional resources:
Furthermore, in emergent situations these expec-tations might not be available at once, and they may have different costs, so we need to organize and prioritize them to yield optimal performance within given time bounds. Therefore we also experimented with various cost-aware composition methods with the input of acquired expectations, plus a time bound for development (1 hour, 2 hours), and the output as a wall-time schedule that determines the best se-quence of applying modules and maximizes the use of all available resources. Experiments on seven low-resource languages demonstrate that our frame-work can create an effective name tagger for an IL within a couple of hours using very few resources. First we use some language universal rules, gazetteers and patterns to generate a binary feature vector F = f f 1 ; f 2 ; ::: g for each token. Table 1 shows these features along with examples. An identification rule is r I =  X  T I ; f = f f a ; f b ; ::: g X  where T I is a  X  X /I/O X  tag to indicate the beginning, inside or outside of a name, and f f a ; f b ; ::: g is a set of selected features. If the features are all matched, the token will be tagged as T I . Similarly, a classifi-cation rule is r C =  X  T C ; f = f f a ; f b ; ::: g X  , where T
C is  X  X erson/Organization/Location X . These rules are triggered in order, and some examples are as fol-lows:  X  B, {AllUppercased}  X  ,  X  PER, {PersonGaz}  X  ,  X  ORG, {Capitalized, LongLength}  X  , etc. 3.1 Approach Overview Figure 1 illustrates our overall approach of acquiring various expectations, by simulating the strategies hu-man players adopted during the Tibetan Room game. Next we will present details about discovering ex-pectations from each source.
 Figure 1: Expectation Driven Name Tagger Overview 3.2 Survey with Native Speaker The best way to understand a language is to con-sult people who speak it. We introduce a human-in-the-loop process to acquire knowledge from native speakers. To meet the needs in the emergent set-ting, we design a comprehensive survey that aims to acquire a wide-range of IL-specific knowledge from native speakers in an efficient way. The sur-vey categorizes questions and organizes them into a tree structure, so that the order of questions is cho-sen based on the answers of previous questions. The survey answers are then automatically translated into rules, patterns or gazetteers in the tagger. Some ex-ample questions are shown in Table 2. 3.3 Mono-lingual Expectation Mining We use a bootstrapping method to acquire IL pat-terns from unlabeled mono-lingual IL documents. Following the same idea in (Agichtein and Gravano, 2000; Collins and Singer, 1999), we first use names identified by high-confident rules as seeds, and gen-eralize patterns from the contexts of these seeds. Then we evaluate the patterns and apply high-quality ones to find more names as new seeds. This process is repeated iteratively 2 .

We define a pattern as a triple  X  lef t; name; right  X  , where name is a name, left and right 3 are context vectors with weighted terms (the weight is computed based on each token X  X  tf-idf score). For example, from a Hausa sentence  X  gwamnatin kasar Sin ta samar wa kasashen yammacin Afirka ... (the Government of China has given ... products to the West African countries)  X , we can discover a pattern: This pattern matches strings like  X  gwamnatin kasar Fiji ta (by the government of Fiji)  X .

For any two triples t i =  X  l i ; name i ; r i  X  and t j =  X  l ; name j ; r j  X  , we comput e their similarity by: We use this similarity measurement to cluster all triples and select the centroid triples in each cluster as candidate patterns.

Similar to (Agichtein and Gravano, 2000), we evaluate the quality of a candidate pattern P by: ,where P positive is the number of positive matches for P and P negative is the number of negative matches. Due to the lack of syntactic and seman-tic resources to refine these lexical patterns, we set a conservative confidence threshold 0.9. 3.4 Cross-lingual Expectation Projection Name tagging research has been done for high-resource languages such as English for over twenty years, so we have learned a lot about them. We col-lected 1,362 patterns from English name tagging lit-erature. Some examples are listed below:
Besides the static knowledge like patterns, we can also dynamically acquire expected names from topically-related English documents for a given IL document. We apply the Stanford name tag-ger (Finkel et al., 2005) to the English documents to obtain a list of expected names. Then we translate the English patterns and expected names to IL. When there is no human constructed English-to-IL lexicon available, we derive a word-for-word translation ta-ble from a small parallel data set using the GIZA++ word alignment tool (Och and Ney, 2003). We also convert IL text to Latin characters based on Unicode mapping 4 , and then apply Soundex code (Mortimer and Salathiel, 1995; Raghavan and Allan, 2004) to find the IL name equivalent that shares the most sim-ilar pronunciation as each English name. For exam-ple, the Bengali name  X   X  X  X  X   X  X  X  X  X  X   X  and  X  Tony Blair  X  have the same Soundex code  X  T500 B460  X . 3.5 Mining Expectations from KB In addition to unstructured documents, we also try to leverage structured English knowledge bases (KBs) such as DBpedia 5 . Each entry is associated with a set of types such as Company , Actor and Agent . We utilize the Abstract Meaning Representation cor-pus (Banarescu et al., 2013) which contains both en-tity type and linked KB title annotations, to automat-ically map 9 ; 514 entity types in DBPedia to three main entity types of interest: Person (PER), Loca-tion (LOC) and Organization (ORG).

Then we adopt a language-independent cross-lingual entity linking system (Wang et al., 2015) to link each IL name mention to English DBPe-dia. This linker is based on an unsupervised quan-tified collective inference approach. It constructs knowledge networks from the IL source documents based on entity mention co-occurrence, and knowl-edge networks from KB. Each IL name is matched with candidate entities in English KB using name translation pairs derived from inter-lingual KB links in Wikipedia and DBPedia. We also apply the word-for-word translation tables constructed from paral-lel data as described in Section 3.4 to translate some uncommon names. Then it performs semantic com-parison between two knowledge networks based on three criteria: salience, similarity and coherence. Fi-nally we map the DBPedia types associated with the linked entity candidates to obtain the entity type for each IL name. We anticipated that not all expectations can be en-coded as explicit rules and patterns, or covered by projected names, therefore for comparison we in-troduce a supervised method with pool-based ac-tive learning to learn implicit expectations (features, new names, etc.) directly from human data annota-tion. We exploited basic lexical features including ngrams, adjacent tokens, casing information, punc-tuations and frequency to train a Conditional Ran-dom Fields (CRFs) (Lafferty et al., 2001) based model through active learning (Settles, 2010).
We segment documents into sentences and use each sentence as a training unit. Let x b be the most informative instance according to a query strategy  X  ( x ) , which is a function used to evaluate each in-stance x in the unlabeled pool U . Algorithm 1 illus-trates the procedure.
 Algorithm 1 Pool-based Active Learning
Jing et al. (2004) proposed an entropy measure for active learning for image retrieval task. We compared it with other measures proposed by (Set-tles and Craven, 2008) and found that sequence entropy (SE) is most effective for our name tagging task. We use  X  SE to represent how informative a sentence is: , where T is the length of x , m ranges over all pos-sible token labels and P  X  ( y t = m ) is the probability when y t is tagged as m . A new requirement for IL name tagging is a Linguis-tic Workflow Generator , which can generate an activity schedule to organize and maximize the use of acquired expectations to yield optimal F-scores within given time bounds. Therefore, the input to the IL name tagger is not only the test data, but also a time bound for development (1 hour, 2 hours, 24 hours, 1 week, 1 month, etc.).

Figure 2 illustrates our cost-aware expectation composition approach. Given some IL documents as input, as the clock ticks, the system delivers name tagging results at time 0 (immediately), time 1 (e.g., in one hour) and time 2 (e.g., in two hours). At time 0, name tagging results are provided by the universal tagger described in Section 2. During the first hour, we can either ask the native speaker to annotate a small amount of data for supervised active learning of a CRFs model, or fill in the survey to build a rule-based tagger. We estimate the confidence value of each expectation-driven rule based on its precision score on a small development set of ten documents. Then we apply these rules in the priority order of their confidence values. When the results of two tag-gers are conflicting on either mention boundary or type, if the applied rule has high confidence we will trust its output, otherwise adopt the CRFs model X  X  output. In this section we will present our experimental de-tails, results and observations. 6.1 Data We evaluate our framework on seven low-resource incidentlanguages: Bengali, Hausa, Tagalog, Tamil, Thai, Turkish and Yoruba, using the ground-truth name tagging annotations from the DARPA LORELEI program 6 . Table 3 shows data statistics. 6.2 Cost-aware Overall Performance We test with three checking points: starting time, within one hour, and within two hours. Based on the combination approach described in Section 5, we can have three possible combinations of the expectation-driven learning and supervised active learning meth-ods during two hours: (1) expectation-driven learn-ing + supervised active learning; (2) supervised ac-tive learning + expectation-driven learning; and (3) supervised active learning for two hours. Figure 3 compares the overall performance of these combi-nations for each language.

We can see that our approach is able to rapidly set up a name tagger for an IL and achieves promis-ing performance. During the first hour, there is no clear winner between expectation-driven learning or supervised active learning. But it X  X  clear that super-vised active learning for two hours is generally not the optimal solution. Using Hausa as a case study, we take a closer look at the supervised active learn-ing curve as shown in Figure 4. We can see that su-pervised active learning based on simple lexical fea-tures tends to converge quickly. As time goes by it will reach its own upper-bound of learning and gen-eralizing linguistic features. In these cases our pro-posed expectation-driven learning method can com-pensate by providing more explicit and deeper IL-specific linguistic knowledge. 6.3 Comparison of Expectation Discovery Table 4 shows the performance gain of each type of expectation acquisition method. IL gazetteers cov-ered some common names, especially when the uni-versal case-based rules failed at identifying names from non-Latin languages. IL name patterns were mainly effective for classification. For example, the Tamil name  X   X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X   X  X  X  X  X  X  X  X   X  X  X  X  X  X  X  X  X  X  (Catholic Syrian Bank) X  was classified as an orga-nization because it ends with an organization suf-fix word  X   X  X  X  X  X  X  X  X  X  X  (bank) X . The patterns projected from English were proven very effective at identi-fying name boundaries. For example, some non-names such as titles are also capitalized in Turkish, so simple case-based patterns produced many spu-rious names. But projected patterns can fix many of them. In the following Turkish sentence,  X  An-cakAvrupaBirli X iD X  X  X li X kilerSorumlusuCatherine Ashton,...(But European Union foreign policy chief Catherine Ashton,...)  X , among all these capitalized tokens, after we confirmed  X  Avrupa Birli X i (Euro-pean Union)  X  as an organization and  X  D X  X   X li X kiler Sorumlusu (foreign policy chief)  X  as a title, we ap-plied a pattern projected from English  X  [Organiza-tion] [Title] [Person]  X  and successfully identified  X  Catherine Ashton  X  as a person. Cross-lingual en-tity linking based typing successfully enhanced clas-sification accuracy, especially for languages where names often appear the same as their English forms and so entity linking achieved high accuracy. For example,  X  George Bush  X  keeps the same in Hausa, Tagalog and Yoruba as English. 6.4 Impact of Supervised Active Learning Figure 5 shows the comparison of supervised active learning and passive learning (random sampling in training data selection). We asked a native speaker to annotate Chinese news documents in one hour, and estimated the human annotation speed approxi-mately as 7,000 tokens per hour. Therefore we set the number of tokens as 7,000 for one hour, and 14,000 for two hours. We can clearly see that super-vised active learning significantly outperforms pas-sive learning for all languages, especially for Tamil, Tagalog and Yoruba. Because of the rich morphol-ogy in Turkish, the gain of supervised active learn-ing is relatively small because simple lexical fea-tures cannot capture name-specific characteristics regardless of the size of labeled data. For example, some prepositions (e.g.,  X  nin (in)  X ) can be part of the names, so it X  X  difficult to determine name bound-aries, such as  X  &lt;ORGLudianb X lgesi hastanesi &gt; nin (in &lt;ORG Ludian Hospital&gt;)  X  given various time bounds Figure 4: Hausa Supervised Active Learning Curve Figure 5: Active Learning vs. Passive Learning (%) 6.5 Remaining Error Analysis
Table 5 presents the detailed break-down scores for all languages. We can see that name identifi-cation, especially organization identification is the main bottleneck for all languages. For example, many organization names in Hausa are often very long, nested or all low-cased, such as  X  makaran-tar horas da Malaman makaranta ta Bawa Jan Gwarzo(BawaJanGwarzoMemorialTeachersCol-lege)  X  and  X  kungiyar masana X  X ntu da tattalin arziki takasarSin(China X  X AssociationofBusinessandIn-dustry)  X . Our name tagger will further benefit from more robust universal word segmentation, rich mor-phology analysis and IL-specific knowledge. For ex-ample, in Tamil  X   X   X  is a visarga used as a diacritic to write foreign sounds, so we can infer a phrase in-cluding it (e.g.,  X   X  X  X  X  X  X  X  X  X  X  X  X  X  X  (Haifa) X ) is likely to be a foreign name. Therefore our survey should be enriched by exercising with many languages to cap-ture more categories of linguistic phenomena. Name Tagging is a well-studied problem. Many types of frameworks have been used, including rules (Farmakiotou et al., 2000; Nadeau and Sekine, 2007), supervised models using monolingual labeled data (Zhou and Su, 2002; Chieu and Ng, 2002; Rizzo and Troncy, 2012; McCallum and Li, 2003; Li and McCallum, 2003), bilingual labeled data (Li et al., 2012; Kim et al., 2012; Che et al., 2013; Wang et al., 2013) or naturally partially annotated data such as Wikipedia (Nothman et al., 2013), bootstrap-ping (Agichtein and Gravano, 2000; Niu et al., 2003; Becker et al., 2005; Wu et al., 2009; Chiticariu et al., 2010), and unsupervised learning (Mikheev et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005; Nadeau et al., 2006; Nadeau and Sekine, 2007; Ji and Lin, 2009).
 Name tagging has been explored for many non-English languages such as in Chinese (Ji and Gr-ishman, 2005; Li et al., 2014), Japanese (Asahara and Matsumoto, 2003; Li et al., 2014), Arabic (Mal-oney and Niv, 1998), Catalan (Carreras et al., 2003), Bulgarian (Osenova and Kolkovska, 2002), Dutch (De Meulder et al., 2002), French (B X chet et al., 2000), German (Thielen, 1995), Ital-ian (Cucchiarelli et al., 1998), Greek (Karkaletsis et al., 1999), Spanish (Ar X valo et al., 2002), Por-tuguese (Hana et al., 2006), Serbo-croatian (Nenadi X  and Spasi X , 2000), Swedish (Dalianis and  X str X m, 2001) and Turkish (T X r et al., 2003). However, most of previous work relied on substantial amount of re-sources such as language-specific rules, basic tools such as part-of-speech taggers, a large amount of la-beled data, or a huge amount of Web ngram data, which are usually unavailable for low-resource ILs. In contrast, in this paper we put the name tagging task in a new emergent setting where we need to pro-cess a surprise IL within very short time using very few resources.
 The TIDES 2003 Surprise Language Hindi Named Entity Recognition task (Li and McCallum, 2003) had a similar setting. A name tagger was re-quired to be finished within a time bound (five days). However, 628 labeled documents were provided in the TIDES task, while in our setting no labeled doc-uments are available at the starting point. There-fore we applied active learning to efficiently anno-tate about 40 documents for each language and pro-posed new methods to learn expectations. The re-sults of the tested ILs are still far from perfect, but we hope our detailed comparison and result analysis can introduce new ideas to balance the quality and cost of name tagging. Name tagging for a new IL is a very important but also challenging task. We conducted a thor-ough study on various ways of acquiring, encod-ing and composing expectations from multiple non-traditional sources. Experiments demonstrate that this framework can be used to build a promising name tagger for a new IL within a few hours. In the future we will exploit broader and deeper entity prior knowledge to improve name identification. We will aim to make the framework more transparent for native speakers so the survey can be done in an au-tomatic interactive question-answering fashion. We willalsodevelopmethodstomakethetaggercapable of active self-assessment to produce the best work-flow within time bounds.
 This work was supported by the U.S. DARPA LORELEI Program No. HR0011-15-C-0115 and ARL/ARO MURI W911NF-10-1-0533. The views and conclusions contained in this document are those of the authors and should not be interpreted as rep-resenting the official policies, either expressed or implied, of the U.S. Government. The U.S. Gov-ernment is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on.

