 Textual definitions constitute a fundamental source to look up when the meaning of a term is sought. Definitions are usually collected in dictio-naries and domain glossaries for consultation pur-poses. However, manually constructing and up-dating glossaries requires the cooperative effort of a team of domain experts. Further, in the presence of new words or usages, and  X  even worse  X  new domains, such resources are of no help. Nonethe-less, terms are attested in texts and some (usually few) of the sentences in which a term occurs are typically definitional, that is they provide a formal explanation for the term of interest. While it is not feasible to manually search texts for definitions, this task can be automatized by means of Machine Learning (ML) and Natural Language Processing (NLP) techniques.

Automatic definition extraction is useful not only in the construction of glossaries, but also in many other NLP tasks. In ontology learning, definitions are used to create and enrich concepts with textual information (Gangemi et al., 2003), and extract taxonomic and non-taxonomic rela-tions (Snow et al., 2004; Navigli and Velardi, 2006; Navigli, 2009a). Definitions are also har-vested in Question Answering to deal with  X  X hat is X  questions (Cui et al., 2007; Saggion, 2004). In eLearning, they are used to help students as-similate knowledge (Westerhout and Monachesi, 2007), etc.

Much of the current literature focuses on the use of lexico-syntactic patterns, inspired by Hearst X  X  (1992) seminal work. However, these methods suffer both from low recall and precision, as defi-nitional sentences occur in highly variable syntac-tic structures, and because the most frequent def-initional pattern  X  X is a Y  X  is inherently very noisy.

In this paper we propose a generalized form of word lattices, called Word-Class Lattices (WCLs), as an alternative to lexico-syntactic pattern learn-ing. A lattice is a directed acyclic graph (DAG), a subclass of non-deterministic finite state automata (NFA). The lattice structure has the purpose of preserving the salient differences among distinct sequences, while eliminating redundant informa-tion. In computational linguistics, lattices have been used to model in a compact way many se-quences of symbols, each representing an alter-native hypothesis. Lattice-based methods differ in the types of nodes (words, phonemes, con-cepts), the interpretation of links (representing ei-ther a sequential or hierarchical ordering between nodes), their means of creation, and the scor-ing method used to extract the best consensus output from the lattice (Schroeder et al., 2009). In speech processing, phoneme or word lattices (Campbell et al., 2007; Mathias and Byrne, 2006; Collins et al., 2004) are used as an interface be-tween speech recognition and understanding. Lat-tices are adopted also in Chinese word segmenta-tion (Jiang et al., 2008), decompounding in Ger-man (Dyer, 2009), and to represent classes of translation models in machine translation (Dyer et al., 2008; Schroeder et al., 2009). In more com-plex text processing tasks, such as information re-trieval, information extraction and summarization, the use of word lattices has been postulated but is considered unrealistic because of the dimension of the hypothesis space.

To reduce this problem, concept lattices have been proposed (Carpineto and Romano, 2005; Klein, 2008; Zhong et al., 2008). Here links repre-sent hierarchical relations, rather than the sequen-tial order of symbols like in word/phoneme lat-tices, and nodes are clusters of salient words ag-gregated using synonymy, similarity, or subtrees of a thesaurus. However, salient word selection and aggregation is non-obvious and furthermore it falls into word sense disambiguation, a notori-ously AI-hard problem (Navigli, 2009b).

In definition extraction, the variability of pat-terns is higher than for  X  X raditional X  applications of lattices, such as translation and speech, how-ever not as high as in unconstrained sentences. The methodology that we propose to align patterns is based on the use of star (wildcard *) charac-ters to facilitate sentence clustering. Each clus-ter of sentences is then generalized to a lattice of word classes (each class being either a frequent word or a part of speech). A key feature of our approach is its inherent ability to both identify def-initions and extract hypernyms. The method is tested on an annotated corpus of Wikipedia sen-tences and a large Web corpus, in order to demon-strate the independence of the method from the annotated dataset. WCLs are shown to general-ize over lexico-syntactic patterns, and outperform well-known approaches to definition and hyper-nym extraction.

The paper is organized as follows: Section 2 discusses related work, WCLs are introduced in Section 3 and illustrated by means of an example in Section 4, experiments are presented in Section 5. We conclude the paper in Section 6. Definition Extraction. A great deal of work is concerned with definition extraction in several languages (Klavans and Muresan, 2001; Storrer and Wellinghoff, 2006; Gaudio and Branco, 2007; Iftene et al., 2007; Westerhout and Monachesi, 2007; Przepi  X  orkowski et al., 2007; Deg  X  orski et al., 2008). The majority of these approaches use symbolic methods that depend on lexico-syntactic patterns or features, which are manually crafted or semi-automatically learned (Zhang and Jiang, 2009; Hovy et al., 2003; Fahmi and Bouma, 2006; Westerhout, 2009). Patterns are either very sim-ple sequences of words (e.g.  X  X efers to X ,  X  X s de-fined as X ,  X  X s a X ) or more complex sequences of words, parts of speech and chunks. A fully au-tomated method is instead proposed by Borg et al. (2009): they use genetic programming to learn simple features to distinguish between definitions and non-definitions, and then they apply a genetic algorithm to learn individual weights of features. However, rules are learned for only one category of patterns, namely  X  X s X  patterns. As we already remarked, most methods suffer from both low re-call and precision, because definitional sentences occur in highly variable and potentially noisy syn-tactic structures. Higher performance (around 60-70% F 1 -measure) is obtained only for specific do-mains (e.g., an ICT corpus) and patterns (Borg et al., 2009).

Only few papers try to cope with the general-ity of patterns and domains in real-world corpora (like the Web). In the GlossExtractor web-based system (Velardi et al., 2008), to improve precision while keeping pattern generality, candidates are pruned using more refined stylistic patterns and lexical filters. Cui et al. (2007) propose the use of probabilistic lexico-semantic patterns, called soft patterns , for definitional question answering in the TREC contest 1 . The authors describe two soft matching models: one is based on an n -gram language model (with the Expectation Maximiza-tion algorithm used to estimate the model param-eter), the other on Profile Hidden Markov Mod-els (PHMM). Soft patterns generalize over lexico-syntactic  X  X ard X  patterns in that they allow a par-tial matching by calculating a generative degree of match probability between the test instance and the set of training instances. Thanks to its gen-eralization power, this method is the most closely related to our work, however the task of defini-tional question answering to which it is applied is slightly different from that of definition extraction, so a direct performance comparison is not possi-ble 2 . In fact, the TREC evaluation datasets cannot be considered true definitions, but rather text frag-ments providing some relevant fact about a target term. For example, sentences like:  X  X ollywood is a Bombay-based film industry X  and  X 700 or more films produced by India with 200 or more from Bollywood X  are both  X  X ital X  answers for the ques-tion  X  X ollywood X , according to TREC classifica-tion, but the second sentence is not a definition. Hypernym Extraction. The literature on hy-pernym extraction offers a higher variability of methods, from simple lexical patterns (Hearst, 1992; Oakes, 2005) to statistical and machine learning techniques (Agirre et al., 2000; Cara-ballo, 1999; Dolan et al., 1993; Sanfilippo and Pozna  X  nski, 1992; Ritter et al., 2009). One of the highest-coverage methods is proposed by Snow et al. (2004). They first search sentences that con-tain two terms which are known to be in a taxo-nomic relation (term pairs are taken from Word-Net (Miller et al., 1990)); then they parse the sen-tences, and automatically extract patterns from the parse trees. Finally, they train a hypernym clas-sifer based on these features. Lexico-syntactic pat-terns are generated for each sentence relating a term to its hypernym, and a dependency parser is used to represent them. 3.1 Preliminaries Notion of definition. In our work, we rely on a formal notion of textual definition. Specifically, given a definition, e.g.:  X  X n computer science, a closure is a first-class function with free variables that are bound in the lexical environment X , we as-sume that it contains the following fields (Storrer and Wellinghoff, 2006):  X  The D EFINIENDUM field (DF): this part of  X  The D EFINITOR field (VF): it includes the  X  The D EFINIENS field (GF): it includes the  X  The R EST field (RF): it includes additional
Further examples of definitional sentences an-notated with the above fields are shown in Table 1. For each sentence, the definiendum (that is, the word being defined) and its hypernym are marked in bold and italic, respectively. Given the lexico-syntactic nature of the definition extraction mod-els we experiment with, training and test sentences are part-of-speech tagged with the TreeTagger sys-tem, a part-of-speech tagger available for many languages (Schmid, 1995).
 Word Classes and Generalized Sentences. We now introduce our notion of word class, on which our learning model is based. Let T be the set of training sentences, manually bracketed with the DF, VF, GF and RF fields. We first determine the set F of words in T whose frequency is above a threshold  X  (e.g., the , a , is , of , refer , etc.). In our training sentences, we replace the term being de-fined with  X  TARGET  X  , thus this frequent token is also included in F .

We use the set of frequent words F to generalize words to  X  X ord classes X . We define a word class as either a word itself or its part of speech. Given a sentence s = w 1 ,w 2 ,...,w | s | , where w i is the i -th word of s , we generalize its words w i to word classes  X  i as follows: that is, a word w i is left unchanged if it occurs frequently in the training corpus (i.e., w i  X  F ) or is transformed to its part of speech ( POS ( w i ) ) otherwise. As a result, we obtain a general-ized sentence s 0 =  X  1 , X  2 ,..., X  | s | . For instance, given the first sentence in Table 1, we obtain the corresponding generalized sentence:  X  X n NN, a  X  TARGET  X  is a JJ NN X , where NN and JJ indicate the noun and adjective classes, respectively. 3.2 Algorithm We now describe our learning algorithm based on Word-Class Lattices. The algorithm consists of three steps: GF [that is part of a computer image] R EST .  X  Star patterns: each sentence in the training  X  Sentence clustering: the training sentences  X  Word-Class Lattice construction: for each
We present two variants of our WCL model, dealing either globally with the entire sentence or separately with its definition fields (Section 3.2.4). The WCL models can then be used to classify any input sentence of interest (Section 3.2.5). 3.2.1 Star Patterns Let T be the set of training sentences. In this step, we associate a star pattern  X  ( s ) with each sentence s  X  X  . To do so, let s  X  X  be a sentence such that s = w 1 ,w 2 ,...,w | s | , where w i is its i -th word. Given the set F of most frequent words in T (cf. Section 3.1), the star pattern  X  ( s ) associated with s is obtained by replacing with * all the words w i 6 X  F , that is all the tokens that are non-frequent words. For instance, given the sentence  X  X n arts, a chiaroscuro is a monochrome picture X , the cor-responding star pattern is  X  X n *, a  X  TARGET  X  is a * X , where  X  TARGET  X  is the defined term.

Note that, here and in what follows, we discard the sentence fragments tagged with the R EST field, which is used only to delimit the core part of defi-nitional sentences. 3.2.2 Sentence Clustering In the second step, we cluster the sentences in our training set T based on their star patterns. For-mally, let  X  = (  X  1 ,..., X  m ) be the set of star patterns associated with the sentences in T . We create a clustering C = ( C 1 ,...,C m ) such that C i = { s  X  X  :  X  ( s ) =  X  i } , that is C i contains all the sentences whose star pattern is  X  i .

As an example, assume  X  3 =  X  X n *, a  X  TARGET  X  is a * X . The sentences reported in Ta-ble 1 are all grouped into cluster C 3 . We note that each cluster C i contains sentences whose degree of variability is generally much lower than for any pair of sentences in T belonging to two different clusters. 3.2.3 Word-Class Lattice Construction Finally, the third step consists of the construction of a Word-Class Lattice for each sentence cluster. Given such a cluster C i  X  C , we apply a greedy algorithm that iteratively constructs the WCL.
Let C i = { s 1 ,s 2 ,...,s | C its first sentence s 1 = w 1 1 ,w 1 2 ,...,w 1 | s denotes the i -th token of the j -th sentence). We first produce the corresponding general-ized sentence s 0 1 =  X  1 1 , X  1 2 ,..., X  1 | s tion 3.1). We then create a directed graph G = ( V,E ) such that V = {  X  1 1 ,..., X  1 | s E = { (  X  1 1 , X  1 2 ) , (  X  1 2 , X  1 3 ) ,..., (  X  1 | s Next, for the subsequent sentences in C i , that is, for each j = 2 ,..., | C i | , we determine the alignment between the sentence s j and each sentence s k  X  C i such that k &lt; j based on the following dynamic programming formulation (Cormen et al., 1990, pp. 314 X 319): M where a  X  { 1 ,..., | s k |} and b  X  { 1 ,..., | s j |} , S a,b is a score of the matching between the a -th token of s k and the b -th token of s j , and M 0 , 0 M 0 ,b and M a, 0 are initially set to 0 for all a and b .
The matching score S a,b is calculated on the generalized sentences s 0 k of s k and s 0 j of s j as fol-lows: where  X  k a and  X  j b are the a -th and b -th word classes of s 0 k and s 0 j , respectively. In other words, the matching score equals 1 if the a -th and the b -th tokens of the two original sentences have the same word class.

Finally, the alignment score between s k and s j is given by M | s beside the corresponding node. mal number of misalignments between the two to-ken sequences. We repeat this calculation for each sentence s k ( k = 1 ,...,j  X  1 ) and choose the one that maximizes its alignment score with s j . We then use the best alignment to add s j to the graph G . Such alignment is obtained by means of backtracking from M | s to the set of vertices V the tokens of the gen-eralized sentence s 0 j for which there is no align-ment to s 0 k and we add to E the edges (  X  j 1 , X  j 2 ) , tice, nodes associated with the hypernym words in the learning sentences are marked as hypernyms in order to be able to determine the hypernym of a test sentence at classification time. 3.2.4 Variants of the WCL Model So far, we have assumed that our WCL model learns lattices from the training sentences in their entirety (we call this model WCL-1). We now propose a second model that learns separate WCLs for each field of the definition, namely: D
EFINIENS (GF) fields (see Section 3.1). We re-fer to this latter model as WCL-3. Rather than ap-plying the WCL algorithm to the entire sentence, the very same method is applied to the sentence fragments tagged with one of the three definition fields. The reason for introducing the WCL-3 model is that, while definitional patterns are highly variable, DF, VF and GF individually exhibit a lower variability, thus WCL-3 should improve the generalization power. 3.2.5 Classification Once the learning process is over, a set of WCLs is produced. Given a test sentence s , the classifica-tion phase for the WCL-1 model consists of deter-mining whether it exists a lattice that matches s . In the case of WCL-3, we consider any combination lattices. While WCL-1 is applied as a yes-no clas-sifier as there is a single WCL that can possibly match the input sentence, WCL-3 selects, if any, the combination of the three WCLs that best fits the sentence. In fact, choosing the most appro-priate combination of lattices impacts the perfor-mance of hypernym extraction. The best combi-nation of WCLs is selected by maximizing the fol-lowing confidence score: score ( s,l DF ,l VF ,l GF ) = coverage  X  log ( support ) where s is the candidate sentence, l DF , l VF and l GF are three lattices one for each definition field, cov-erage is the fraction of words of the input sentence covered by the three lattices, and support is the sum of the number of sentences in the star patterns corresponding to the three lattices.

Finally, when a sentence is classified as a def-inition, its hypernym is extracted by selecting the words in the input sentence that are marked as  X  X y-pernyms X  in the WCL-1 lattice (or in the WCL-3 GF lattice). As an example, consider the definitions in Table 1. As illustrated in Section 3.2.2, their star pat-tern is  X  X n *, a  X  TARGET  X  is a * X . The corre-sponding WCL is built as follows: the first part-of-speech tagged sentence,  X  X n/IN arts/NN , a/DT  X  TARGET  X  /NN is/VBZ a/DT monochrome/JJ pic-ture /NN X , is considered. The corresponding gen-eralized sentence is  X  X n NN , a  X  TARGET  X  is a JJ NN X . The initially empty graph is thus popu-lated with one node for each word class and one edge for each pair of consecutive tokens, as shown in Figure 1 (the central sequence of nodes in the graph). Note that we draw the hypernym token
NN 2 with a rectangle shape. We also add to the graph a start node  X  and an end node  X   X  , and con-nect them to the corresponding initial and final sentence tokens. Next, the second sentence,  X  X n mathematics, a graph is a data structure that con-sists of... X , is aligned to the first sentence. The alignment of the generalized sentence is perfect, apart from the NN 3 node corresponding to  X  X ata X . The node is added to the graph together with the edges a  X  NN 3 and NN 3  X  NN 2 . Finally, the third sentence in Table 1,  X  X n computer science, a pixel is a dot that is part of a computer image X , is generalized as  X  X n NN NN , a  X  TARGET  X  is a NN X . Thus, a new node NN 4 is added, corre-sponding to  X  X omputer X  and new edges are added: In  X  NN 4 and NN 4  X  NN 1 . Figure 1 shows the re-sulting WCL-1 lattice. 5.1 Experimental Setup Datasets. We conducted experiments on two different datasets:  X  A corpus of 4,619 Wikipedia sentences, that  X  A subset of the ukWaC Web corpus (Fer-The reason for using the ukWaC corpus is that, un-like the  X  X lean X  Wikipedia dataset, in which rel-atively simple patterns can achieve good results, ukWaC represents a real-world test, with many complex cases. For example, there are sentences that should be classified as definitional according to Section 3.1 but are rather uninformative, like  X  dynamic programming was the brainchild of an american mathematician X , as well as informative sentences that are not definitional (e.g., they do not have a hypernym), like  X  cubism was characterised by muted colours and fragmented images X . Even more frequently, the dataset includes sentences which are not definitions but have a definitional pattern ( X  X  Pacific Northwest tribe X  X  saga refers to a young woman who [..] X ), or sentences with very complex definitional patterns ( X  white body cells are the body X  X  clean up squad  X  and  X  joule is also an expression of electric energy  X ). These cases can be correctly handled only with fine-grained pat-terns. Additional details on the corpus and a more thorough linguistic analysis of complex cases can be found in Navigli et al. (2010).
 Systems. For definition extraction, we experi-ment with the following systems:  X  WCL-1 and WCL-3 : these two classifiers  X  Star patterns : a simple classifier based on  X  Bigrams : an implementation of the bigram Algorithm P R F 1 A WCL-1 99.88 42.09 59.22 76.06 WCL-3 98.81 60.74 75.23 83.48 Star patterns 86.74 66.14 75.05 81.84 Bigrams 66.70 82.70 73.84 75.80 Random BL 50.00 50.00 50.00 50.00
Table 2: Performance on the Wikipedia dataset.
For hypernym extraction, we compared WCL-1 and WCL-3 with Hearst X  X  patterns , a system that extracts hypernyms from sentences based on the lexico-syntactic patterns specified in Hearst X  X  seminal work (1992). These include (hypernym in italic):  X  X uch NP as { NP , } { (or | and) } NP X ,  X  X P { , NP } { , } or other NP  X ,  X  NP { , } includ-ing { NP , }{ or | and } NP X ,  X  NP { , } especially { NP , }{ or | and } NP X , and variants thereof. How-ever, it should be noted that hypernym extraction methods in the literature do not extract hypernyms from definitional sentences, like we do, but rather from specific patterns like  X  X  such as Y X . There-fore a direct comparison with these methods is not possible. Nonetheless, we decided to implement Hearst X  X  patterns for the sake of completeness. We could not replicate the more refined approach by Snow et al. (2004) because it requires the annota-tion of a possibly very large dataset of sentence fragments. In any case Snow et al. (2004) re-ported the following performance figures on a cor-pus of dimension and complexity comparable with ukWaC: the recall-precision graph indicates preci-sion 85% at recall 10% and precision 25% at re-call of 30% for the hypernym classifier. A variant of the classifier that includes evidence from coor-dinate terms (terms with a common ancestor in a taxonomy) obtains an increased precision of 35% at recall 30%. We see no reasons why these figures should vary dramatically on the ukWaC.

Finally, we compare all systems with the ran-dom baseline , that classifies a sentence as a defi-nition with probability 1 2 .
 Table 3: Performance on the ukWaC dataset (  X  Re-call is estimated).
 Measures. To assess the performance of our systems, we calculated the following measures:  X  precision  X  the number of definitional sen- X  recall  X  the number of definitional sen- X  the F 1 -measure  X  a harmonic mean of preci- X  accuracy  X  the number of correctly classi-5.2 Results and Discussion Definition Extraction. In Table 2 we report the results of definition extraction systems on the Wikipedia dataset. Given this dataset is also used for training, experiments are performed with 10-fold cross validation. The results show very high precision for WCL-1, WCL-3 (around 99%) and star patterns (86%). As expected, bigrams and star patterns exhibit a higher recall (82% and 66%, re-spectively). The lower recall of WCL-1 is due to its limited ability to generalize compared to WCL-3 and the other methods. In terms of F 1 -measure, star patterns and WCL-3 achieve 75%, and are thus the best systems. Similar performance is ob-served when we also account for negative sen-tences  X  that is we calculate accuracy (with WCL-3 performing better). All the systems perform sig-nificantly better than the random baseline.

From our Wikipedia corpus, we learned over 1,000 lattices (and star patterns). Using WCL-3, we learned 381 DF, 252 VF and 395 GF lat-tices, that then we used to extract definitions from Table 4: Precision in hypernym extraction on the Wikipedia dataset the ukWaC dataset. To calculate precision on this dataset, we manually validated the definitions out-put by each system. However, given the large size of the test set, recall could only be estimated. To this end, we manually analyzed 50,000 sentences and identified 99 definitions, against which recall was calculated. The results are shown in Table 3. On the ukWaC dataset, WCL-3 performs best, ob-taining 94.87% precision and 56.57% recall (we did not calculate F 1 , as recall is estimated). In-terestingly, star patterns obtain only 44% preci-sion and around 63% recall. Bigrams achieve even lower performance, namely 46.60% preci-sion, 45.45% recall. The reason for such bad performance on ukWaC is due to the very dif-ferent nature of the two datasets: for example, in Wikipedia most  X  X s a X  sentences are definitional, whereas this property is not verified in the real world (that is, on the Web, of which ukWaC is a sample). Also, while WCL does not need any parameter tuning 5 , the same does not hold for bi-grams 6 , whose probability threshold and mixture weights need to be best tuned on the task at hand. Hypernym Extraction. For hypernym extrac-tion, we tested WCL-1, WCL-3 and Hearst X  X  pat-terns. Precision results are reported in Tables 4 and 5 for the two datasets, respectively. The Sub-string column refers to the case in which the cap-tured hypernym is a substring of what the annota-tor considered to be the correct hypernym. Notice that this is a complex matter, because often the se-lection of a hypernym depends on semantic and contextual issues. For example,  X  Fluoroscopy is an imaging method  X  and  X  X he Mosaic was an in-teresting project  X  have precisely the same genus pattern, but (probably depending on the vagueness of the noun in the first sentence, and of the adjec-tive in the second) the annotator selected respec-Table 5: Precision in hypernym extraction on the ukWaC dataset (number of hypernyms in paren-theses). tively imaging method and project as hypernyms. For the above reasons it is difficult to achieve high performance in capturing the correct hypernym (e.g. 40.73% with WCL-3 on Wikipedia). How-ever, our performance of identifying a substring of the correct hypernym is much higher (around 78.58%). In Table 4 we do not report the preci-sion of Hearst X  X  patterns, as only one hypernym was found, due to the inherently low coverage of the method.

On the ukWaC dataset, the hypernyms returned by the three systems were manually validated and precision was calculated. Both WCL-1 and WCL-3 obtained a very high precision (86-89% and 96% in identifying the exact hypernym and a substring of it, respectively). Both WCL models are thus equally robust in identifying hypernyms, whereas WCL-1 suffers from a lack of generalization in definition extraction (cf. Tables 2 and 3). Also, given that the ukWaC dataset contains sentences in which any of 239 domain terms occur, WCL-3 extracts on average 1.6 and 1.7 full and substring hypernyms per term, respectively. Hearst X  X  pat-terns also obtain high precision, especially when substrings are taken into account. However, the number of hypernyms returned by this method is much lower, due to the specificity of the patterns (62 vs. 383 hypernyms returned by WCL-3). In this paper, we have presented a lattice-based ap-proach to definition and hypernym extraction. The novelty of our approach is: 1. The use of a lattice structure to generalize 2. The ability of the system to jointly identify 3. The generality of the method, which applies 4. The high performance as compared with the
Even though definitional patterns are learned from a manually annotated dataset, the dimension and heterogeneity of the training dataset ensures that training needs not to be repeated for specific domains 7 , as demonstrated by the cross-domain evaluation on the ukWaC corpus.

The datasets used in our experiments are avail-able from http://lcl.uniroma1.it/wcl .
 We also plan to release our system to the research community. In the near future, we aim to apply the output of our classifiers to the task of automated taxonomy building, and to test the WCL approach on other information extraction tasks, like hyper-nym extraction from generic sentence fragments, as in Snow et al. (2004).

