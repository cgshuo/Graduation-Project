 David Mimno mimno@cs.princeton.edu Matthew D. Hoffman mdhoffma@cs.princeton.edu David M. Blei blei@cs.princeton.edu Topic models are hierarchical Bayesian models of doc-ument collections (Blei et al., 2003). They can uncover the main themes that pervade a corpus and then use those themes to help organize, search, and explore the documents. In topic modeling, a  X  X opic X  is a distri-bution over a fixed vocabulary and each document ex-hibits the topics with different proportions. Both the topics and the topic proportions of documents are hid-den variables. Inferring the conditional distribution of these variables given an observed set of documents is the central computational problem.
 In this paper, we develop a posterior inference method for topic modeling that can find large numbers of top-ics in massive collections of documents. We demon-strate our approach by analyzing a collection of 1.2 million out-of-copyright books, comprising 33 billion observed words. Using our algorithm, we fit a topic model to this corpus with thousands of topics. We il-lustrate the most frequent words from several of these topics in Table 1.
 Our algorithm builds on variational inference (Jor-dan et al., 1999). In variational inference, we define a parameterized family of distributions over the hid-den structure X  X n this case topics and document-topic proportions X  X nd then optimize the parameters to find a member of the family that is close to the poste-rior. Traditional variational inference for topic model-ing uses coordinate ascent. The algorithm alternates between estimating document-topic proportions under the current settings of the topics and re-estimating the topics based on the estimated document proportions. This requires multiple passes through an entire collec-tion, which is not practical when working with very large corpora.
 Recently, Hoffman et al. (2010) introduced Online LDA, a stochastic gradient optimization algorithm for topic modeling. The algorithm repeatedly subsamples a small set of documents from the collection and then updates the topics from an analysis of the subsam-ple. This method uses less memory than the standard approach because we do not need to store topic pro-portions for the full corpus. It also converges faster because we update topics more frequently. However, while it handles large corpora it does not scale to large numbers of topics.
 Our algorithm builds on this method by using sam-pling to introduce a second source of stochasticity into the gradient. This approach lets us take advantage of sparse computation, scaling sublinearly with the num-ber of topics. Using this algorithm, we can fit topic models to large collections with many topics. We model each of the D documents in a corpus as a mixture of K topics. This topic model can be divided into corpus-level global variables and document-level local variables. The global variables are K topic-word distributions  X  1 ,...,  X  K over the V -dimensional vocab-ulary, each drawn from a Dirichlet prior with param-eter  X  . For a document d of length N d , the local vari-ables are (a) a distribution over topics  X  d drawn from a Dirichlet prior with parameter  X  and (b) N d token-topic indicator variables z d 1 ,...,z dN Our goal is to estimate the posterior distribution of the hidden variables given an observed corpus. We will use variational inference. Unlike standard mean-field vari-ational inference, but similar to Griffiths &amp; Steyvers (2004) and Teh et al. (2006), we will marginalize out the topic proportions  X  d . Thus we need to approxi-mate the posterior over the topic assignments z d and the topics  X  1: K .
 We will use a variational distribution of the form This factorization differs from the usual mean-field family for topic models. Rather than defining a distri-bution that factorizes over individual tokens, we treat each document X  X  sequence of topic indicator variables z rather than a product of N d distributions, each over K possible values.
 We now derive an algorithm that uses Gibbs sampling to estimate variational expectations of the local vari-ables and a stochastic natural gradient step to update the variational distribution of global variables. A lower bound on the marginal log probability of the observed words given the hyperparameters is log p ( w |  X , X  )  X  X where H ( q ) denotes the entropy of q .
 Following Bishop (2006), the optimal variational dis-tribution over topic configurations for a document, holding all other variational distributions fixed, is where I a = b is 1 if a = b and 0 otherwise, and  X  z denotes the set of all unobserved variables besides z d . We can compute Eq. 4 for a specific topic configura-tion z d , but we cannot tractably normalize it to get The optimal variational distribution over topic-word distributions, holding the other distributions fixed, is the kernel of a Dirichlet distribution with parameters This expression includes the expectation under q of the number of tokens of type w assigned to topic k . Computing this expectation would require evaluating the intractable distribution q ? ( z d ). 2.1. Online stochastic inference We optimize the variational topic-word parameters  X  kw using stochastic gradient ascent. Stochastic gra-dient ascent iteratively updates parameters with noisy estimates of the gradient. We obtain these noisy esti-mates by subsampling the data (Sato, 2001; Hoffman et al., 2010).
 We first recast the variational objective in Eq. 2 as a summation over per-document terms ` d , so that the full gradient with respect to  X  k is the sum P d  X   X   X  We can then generate a noisy approximation to this full gradient by sampling a minibatch of documents B and then scaling the sum of the document-specific gradients to match the total size of the corpus, (The expectation is with respect to the random sample B .) Pushing the per-topic terms in Eq. 2 inside the summation over documents and removing terms not involving  X  kw we obtain Algorithm 1 Algorithm for hybrid stochastic variational-Gibbs inference. for t  X  1 ,...,  X  do end for where E q [ N dkw ] = P i E q [ I z of Eq. 7 with respect to the parameters  X  k 1 ,..., X  can be factored into the product of a matrix and a vector. The matrix, which contains derivatives of the digamma function, is the Fisher information matrix for the topic parameters. Element w of the vector is Premultiplying the gradient of an objective function by the inverse Fisher information matrix of the distribu-tion being optimized (in our case the variational distri-bution q ) results in the natural gradient (Sato, 2001). Since our gradient is the product of the Fisher informa-tion matrix and a vector, the natural gradient is there-fore simply Eq. 8 (Hoffman et al., 2010). Compared to the standard Euclidean gradient, the natural gradient offers both faster convergence (because it takes into account the information geometry of the variational distribution) and cheaper computation (because the vector in Eq. 8 is a simple linear function). 2.2. MCMC within stochastic inference We cannot evaluate the expectation in Eq. 8 because we would have to consider a combinatorial number of topic configurations z d . To use stochastic gradient as-cent, however, we only need an approximation to this expectation. We use Markov chain Monte Carlo to sample topic configurations from q ? ( z d ). We then use the empirical average of these samples to estimate the expectations needed for Eq. 8.
 Gibbs sampling for a document starts with a random initialization of the topic indicator variables z d . We then iteratively resample the topic indicator at each position from the conditional distribution over that position given the remaining topic indicator variables: q where the expectation of the log probability of word w given a topic k is  X (  X  kw )  X   X ( P w 0  X  kw 0 ). After B burn-in sweeps, we begin saving sampled topic config-urations. Once we have saved S samples { z } 1 ,...,S , we can define approximate sufficient statistics Using MCMC estimates adds noise to our gradient, but allows us to use a collapsed objective function that does not represent document-topic proportions  X  d . In addition, an average over a finite set of samples pro-vides a sparse estimate of the gradient: for many words and topics, our estimate of E q [ N dkw ] will be zero. 2.3. Algorithm We have defined a natural gradient and a method for approximating the sufficient statistics of that gradient. For a sequence of learning rates  X  t = ( t 0 + t )  X   X  , the following update will lead to a stationary point: This update results in Algorithm 1. Two implementa-tion details that result in sparse computations can be found in Appendix A. This online algorithm has the important advantage over Online LDA of preserving sparsity in the topic-word parameters, so that  X  kw =  X  for most values of k and w . Sparsity increases the effi-ciency of updates to  X  k and of Gibbs sampling for z d . Previous variational methods lead to dense updates to KV topic parameters, making them expensive to ap-ply to large vocabularies and large numbers of topics. Our method, in contrast, is able to exploit the sparsity exhibited by samples from the variational distribution q , resulting in much more efficient updates. This paper combines two sources of zero-mean noise in constructing an approximate gradient for a variational inference algorithm: subsampling of data, and Monte Carlo inference. These sources of variance have been used individually in previous work. Stochastic approx-imation EM (SAEM, Delyon et al., 1999) combines an EM algorithm with a stochastic online inference proce-dure. SAEM does not subsample data, but rather in-terpolates between Monte Carlo estimates of the com-plete data. Kuhn &amp; Lavielle (2004) extend SAEM to use MCMC estimates. Similarly, online EM (Capp  X e &amp; Moulines, 2009) sub-samples data but preserves stan-dard inference procedures for local variables. Standard collapsed Gibbs sampling uses multiple sweeps over the entire corpus, representing topic-word distributions using the topic-word assignment vari-ables of the entire corpus except for the current token. As a result, topic assignment variables must in theory be sampled sequentially, although parallel approxima-tions work well empirically (Asuncion et al., 2008). In contrast, Algorithm 1 treats topic-word distributions as a global variable distinct from the local token-topic assignment variables, and so can parallelize trivially. In this work we integrate over document-topic pro-portions  X  d within a variational algorithm. Collapsed variational inference (Teh et al., 2006) also analytically marginalizes over the topic proportions, but still main-tains a fully factorized distribution over topic assign-ments at each position. The method described here does not restrict itself to such factored distributions, and therefore reduces bias, but this reduction may be offset by the bias we introduce when we initialize the Gibbs chain. In this section we compare the sampled online algo-rithm to two related online methods and measure the effect of model parameters. We use a selection of met-rics to evaluate models. 4.1. Evaluation Held-out probability. A model that characterizes the semantic structure of a corpus should place more of its probability mass on sensible documents than on random sequences of words. We can use this as-sumption to compare different models by asking each model to estimate the probability of a previously un-seen document. A better model should, on average, assign higher probability to real documents than a lower-quality model. We evaluate held-out probabil-ity using the left-to-right sequential sampling method (Wallach et al., 2009; Buntine, 2009). For each trained model we generate point estimates of the topic-word probabilities  X  p ( w | k ). We then process each document by iterating through the tokens w 1 ,...,w N position i we calculate the marginal probability  X  p ( w i | w &lt;i ) = X We then sample z i proportional to the terms of that summation and continue to the next token. 1 In order to normalize for document lengths, we divide the sum of the logs of the marginal probabilities by N d . Coherence. This metric measures the semantic quality of a topic by approximating the experience of a user viewing the W most probable words for the topic (Mimno et al., 2011). It is related to point-wise mutual information (Newman et al., 2010). Let D ( w ) be the document frequencies for each word w , that is, the number of documents containing one or more to-kens of type w , and let D ( w 1 ,w 2 ) be the number of documents containing at least one token of w 1 and of w . For each pair of words w 1 ,w 2 in the top W list, we calculate the number of documents that contain at least one token of the higher ranked word w 1 that also contain at least one token of the lower ranked word w : where is a small constant used to avoid log zero. Values closer to zero indicate greater co-occurrence. Unlike held-out probability, which reports scores for held-out documents , coherence reports scores for indi-vidual topics .
 Wallclock time. Our goal is to train useful models as efficiently as possible. In addition to model qual-ity metrics, we are therefore also interested in total computation time. 4.2. Comparison to Online VB Our first corpus consists of 350,000 research articles from three major journals: Science, Nature, and the Proceedings of the National Academy of Sciences of the USA. We use a vocabulary with 19,000 distinct words, including selected multi-word terms. We train models on 90% of the Science/Nature/PNAS corpus, holding out the remaining documents for testing pur-poses. We save topic-word parameters  X  N kw after epochs consisting of 500,000 documents.
 Sampled online variational Bayesian inference com-pares well in terms of wallclock time to standard online VB inference, particularly with respect to the number of topics K . Figure 1 shows results comparing stan-dard online VB inference to sampled online inference
Sec 10 for K up to 1000. Each iteration consists of a mini-batch of 100 documents. Standard online inference takes time linear in K , while wallclock time for sam-pled online inference grows more slowly.
 We would like to know if there is a difference in the quality of models trained through the hybrid sampled variational algorithm and the online LDA algorithm. We compare an implementation of Online LDA that tries to be as close as possible to the sampled online implementation, but using a dense VB update instead of a sparse sampled update for the local variables. In particular, the number of coordinate ascent steps in standard VB is equal to the number of Gibbs sweeps in the sampled algorithm.
 Per-topic coherence for K = 200 is shown in Fig-ure 2. Sampled online inference produces fewer very poor topics. This difference is significant under a two-sample t -test ( p &lt; 0 . 001) and does not decrease with additional training epochs. Sampled online inference also assigns greater held-out probability than Online LDA for every test document, by a wide margin. We evaluated several possible reasons for this difference in performance. Held-out probability estimation can be affected by evaluation-time smoothing parameter set-tings, but we found both models were affected equally. The log probability of a document is the sum of the log probabilities of its words. It is possible that if one model assigned very small probability to a hand-ful of tokens, those words could significantly affect the overall score, but the difference in log probability was consistent across many tokens. The scale of parame-ters might not be comparable, but as both methods use the same learning schedule, the sum of the trained parameters  X  kw is nearly identical.
 The main difference appears to be the entropy of the topic distributions: the sampled-online algorithm pro-duces less concentrated distributions (mean entropy 6 . 8  X  0 . 46) than standard online LDA (mean entropy 6 . 0  X  0 . 58). This result could indicate that coordinate ascent over the local variables for Online LDA is not converging. 4.3. Comparison to Sequential Monte Carlo Sequential Monte Carlo is an online algorithm similar to Gibbs sampling in that it represents topics using sums over assignment variables (Ahmed et al., 2012). A Gibbs sampler starts with a random initialization for all hidden variables and sweeps repeatedly over the entire data set, updating each variable given the cur-rent value of all other variables. SMC samples values for hidden variables in sequential order, conditioning only on previously-seen variables. It is common to keep multiple sampling states or  X  X articles X , but this process adds both computation and significant book-keeping complexity. Ahmed et al. (2012) use a single SMC state.
 In order to compare SMC to the sampled online al-gorithm, we ran 10 independent SMC samplers over the Science/Nature/PNAS dataset, with documents ordered randomly. We also ran 10 independent sam-pled trainers, stopping after a number of documents had been sampled equivalent to the size of the corpus. In order to make the comparison more fair, we allowed the SMC sampler to sweep through each document the same number of times as the sampled online algorithm, but only the final topic configuration of a document was available to the subsequent documents. 2 Results for K = 200 are shown in Figure 3. SMC has con-sistently worse per-topic coherence and per-document held-out log probability. The sampled online algo-rithm in this paper differs from SMC in that the contri-bution of local token-topic assignment variables decays according to the learning rate schedule, so that more recently sampled documents can have greater weight than earlier documents. This decay allows sampled on-line inference to  X  X orget X  its initial topics, unlike SMC, which weights all documents equally. 4.4. Effect of parameter settings Number of samples. In the inner loop of our algo-rithm we initialize 3 the topic indicator variables z for a document and then perform several Gibbs sweeps. In each sweep we resample the value of each topic in-dicator variable in turn. We introduce bias when we initialize, so we discard B  X  X urn-in X  sweeps and use values of z saved after S additional sweeps to calculate the gradient. Since performance is linear in the total number of sweeps B + S , we want to find the smallest number of sweeps that does not sacrifice performance. We consider nine settings of the pair ( B,S ). Under the first three settings we save one sweep and vary the number of burn-in sweeps: (1,1), (2,1), (3,1). For the second three settings we perform five sweeps, vary-ing how many we discard: (2,3), (3,2), (4,1). The final three settings fix B = S and consider larger total num-bers of sweeps: (5,5), (10,10), (20,20). We evaluate each setting after processing 500,000 documents. Performance was similar across settings with the fol-lowing exceptions, which were significant at p &lt; 0 . 001 under a two-sample t -test. The two-sweep setting (1,1) had better topic coherence but worse held-out proba-bility than the all other settings. The (5,5) setting had the best mean held-out probability, but it was not significantly better than (10,10) and (20,20). The many-sweep settings (5,5), (10,10), (20,20) had worse topic coherence than the other settings, with many vis-ibly low-quality topics. These results suggest that 3 X 5 sweeps is sufficient.
 Topic-word smoothing. Eq. 9 involves the func-tion e  X ( x ) . This function approaches x  X  1 2 as x gets large, but for values of x near 0, it is non-linear. For the values of topic parameters are in this range, a mi-nuscule increase in the parameter for word w in topic k can cause a profound change in the sampling distri-bution for that word: all subsequent tokens of type w will be assigned to topic k with probability near 1.0. In general, the randomness introduced by sampling topic assignments helps to avoid becoming trapped in local maxima. When parameters are near zero, how-ever, random decisions early in the inference process risk becoming permanent. The topic-word smooth-ing parameter  X  can push parameter values away from this explosive region. We measured coherence for six settings of the topic-word hyperparameter  X  , for batch variational inference, many topics are visi-bly nonsensical. Average coherence improves signifi-( p &lt; 0 . 001). There is no significant difference in aver-age coherence for  X   X  X  0 . 4 , 0 . 5 , 0 . 6 } . Forgetting factors. We now consider the learning rate  X  t = ( t 0 + t )  X   X  and its relation to the corpus size D . We fix  X  = 0 . 6 and vary the offset param-eter t 0  X  { 3000 , 15000 , 30000 , 150000 , 300000 } , saving topic parameters after five training epochs of 500,000 documents each. There was no significant difference in average topic coherence.
 The learning rate, however, is not the only factor that determines the magnitude of parameter updates. Eq. 11 also includes the size of the corpus D . If the cor-pus is larger, we will take larger steps, regardless of the contents of the mini-batch. The offset parameter t 0 had no significant effect on coherence for the full corpus, but it may have an effect if we also vary the corpus size.
 We simulate different size corpora by subsampling the full data set. Results are shown in Figure 4 for models trained on one half, one quarter, and one eighth of the corpus. Each corpus is a subset of the next larger cor-pus. In the smallest corpus (12.5%), the model with t = 300000 is significantly worse than other settings ( p &lt; 0 . 001). Otherwise, there is no significant differ-ence in average topic coherence.
 4.5. Scalability Pre-1922 books. To demonstrate the scalability of the method, we modeled a collection of 1.2 million out-of-copyright books. Topic models are useful in char-acterizing the contents of the corpus and supporting browsing applications: even scanning titles for a col-lection of this size is impossible for one person. Pre-vious approaches to million-book digital libraries have focused on keyword search and word frequency his-tograms (Michel et al., 2011). Such methods do not account for variability in meaning or context. There is no guarantee that the words being counted match the meaning assumed by the user. In contrast, an interface based on a topic model could, for example, distinguish uses of the word  X  X train X  in immunology, mechanical engineering, and cookery.
 We divide each book into 10-page sections, resulting in 44 million  X  X ocuments X  with a vocabulary size of 2 16 . We trained models with K  X  { 100 , 500 , 1000 , 2000 } . Randomly selected example topics are shown in Ta-ble 1, illustrating the average level of topic quality. Models are sparse: at K = 2000, less than 1% of the 2000  X  2 16 possible topic-word parameters are non-zero. The algorithm scales well as K increases. The number of milliseconds taken to process a sequence of 10,000 documents was similar for K = 1000 and 2000, despite doubling the number of topics. Stochastic online inference allows us to scale topic modeling to large document sets. Sparse Gibbs sam-pling allows us to scale to large numbers of topics. The algorithm presented in this paper combines the advantages of these two methods. As a result, models can be trained on vast, open-ended corpora without requiring access to vast computer clusters. If parallel architectures are available, we can trivially parallelize computation within each mini-batch. As this work is related to the Online LDA algorithm of Hoffman et al. (2010), extensions to that model are also applicable, such as adaptive scheduling algorithms (Wahabzada &amp; Kersting, 2011). The use of MCMC within stochastic variational inference reduces one source of bias in es-timating local variables. Although we have focused on text analysis applications, this hybrid method gener-alizes to a broad class of Bayesian models.
 John Langford, Iain Murray, Charles Sutton provided helpful comments. Yahoo! and PICSciE provided computational resources. DM is supported by a CRA CI fellowship. MDH is supported by NSF ATM-0934516, DOE DE-SC0002099, and IES R305D100017. DMB is supported by ONR N00014-11-1-0651, NSF CAREER 0745520, AFOSR FA9550-09-1-0668, the Alfred P. Sloan foundation, and a grant from Google. Sparse sampling over topics. Sampling z s di  X  (  X  + N dk ) e E q [log  X  kw ] requires calculating the normal-izing constant Z = P k (  X  + N dk ) e E q [log  X  kw ] . This cal-culation can be accomplished in time much less than O ( k ) if we can represent the topic-word parameters  X  kw sparsely. The smoothing parameter  X  can be fac-tored out of Equation 11 as long as we assume that all initial values  X  0 kw  X   X  . Rearranging this equation to separate the Dirichlet hyperparameter  X  shows that we can define an alternative parameter  X  N portion of the variational Dirichlet parameter, and ig-nore the contribution of the smoothing parameter until it is time to calculate expectations.
 For any given w , it is likely that most values of  X  N kw will be zero. We can therefore rewrite the normalizing constant as
Z = X The second summation does not depend on any word-specific variables, and can therefore be calculated and then updated incrementally as N dk changes. The first summation is non-zero only for k such that  X  N kw &gt; 0. Sparse updates in the vocabulary. We expect that a typical mini-batch will contain a small fraction of the words in the vocabulary. Eq. 11, however, up-dates  X  N kw for all words, even words that do not occur in the current mini-batch. Expanding the recursive definition of  X  N t kw , and letting  X  N t kw = D |B| N S  X  N Dividing both sides by Q t i =1 (1  X   X  i ), Defining a variable  X  t = Q t i =1 (1  X   X  i ), the update be-comes This update is sparse: only elements with non-zero n dw will be modified. To calculate the ex-pectation of p ( w | k ), we compute  X   X  +  X  t N t kw The scale factor  X  t can become small after several hun-dred mini-batches. We periodically  X  X eset X  this pa-rameter by setting all stored values to  X  N t kw =  X  t  X 
