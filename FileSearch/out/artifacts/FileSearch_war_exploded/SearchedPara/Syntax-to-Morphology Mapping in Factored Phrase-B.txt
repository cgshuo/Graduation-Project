 Statistical machine translation into a morphologi-cally complex language such as Turkish, Finnish or Arabic, involves the generation of target words with the proper morphology, in addition to prop-erly ordering the target words. Earlier work on translation from English to Turkish (Oflazer and Durgar-El-Kahlout, 2007; Oflazer, 2008; Durgar-El-Kahlout and Oflazer, 2010) has used an ap-proach which relied on identifying the contextu-ally correct parts-of-speech, roots and any mor-phemes on the English side, and the complete se-quence of roots and overt derivational and inflec-tional morphemes for each word on the Turkish side. Once these were identified as separate to-kens, they were then used as  X  X ords X  in a stan-dard phrase-based framework (Koehn et al., 2003). They have reported that, given the typical com-plexity of Turkish words, there was a substantial percentage of words whose morphological struc-ture was incorrect: either the morphemes were not applicable for the part-of-speech category of the root word selected, or the morphemes were in the wrong order. The main reason given for these problems was that the same statistical trans-lation, reordering and language modeling mecha-nisms were being employed to both determine the morphological structure of the words and , at the same time, get the global order of the words cor-rect. Even though a significant improvement of a standard word-based baseline was achieved, fur-ther analysis hinted at a direction where morphol-ogy and syntax on the Turkish side had to be dealt with using separate mechanisms.

Motivated by the observation that many lo-cal and some nonlocal syntactic structures in En-glish essentially map to morphologically complex words in Turkish, we present a radically different approach which does not segment Turkish words into morphemes, but uses a representation equiv-alent to the full word form. On the English side, we rely on a full syntactic analysis using a depen-dency parser. This analysis then lets us abstract and encode many local and some nonlocal syn-tactic structures as complex tags (dynamically, as opposed to the static complex tags as proposed by Birch et al. (2007) and Hassan et al. (2007)). Thus we can bring the representation of English syntax closer to the Turkish morphosyntax.
 Such an approach enables the following: (i) Driven by the pattern of morphological structures of full word forms on the Turkish side represented as root words and complex tags, we can iden-tify and reorganize phrases on the English side, to  X  X lign X  English syntax to Turkish morphology wherever possible. (ii) Continuous and discontin-uous variants of certain (syntactic) phrases can be conflated during the SMT phrase extraction pro-cess. (iii) The length of the English sentences can be dramatically reduced, as most function words encoding syntax are now abstracted into complex tags. (iv) The representation of both the source and the target sides of the parallel corpus can now be mostly normalized. This facilitates the use of fac-tored phrase-based translation that was not pre-viously applicable due to the morphological com-plexity on the target side and mismatch between source and target morphologies.

We find that with the full set of syntax-to-morphology transformations and some additional techniques we can get about 39% relative im-provement in BLEU scores over a word-based baseline and about 28% improvement of a factored baseline, all experiments being done over 10 train-ing and test sets. We also find that further con-stituent reordering taking advantage of the syntac-tic analysis of the source side, does not provide tangible improvements when averaged over the 10 data sets.

This paper is organized as follows: Sec-tion 2 presents the basic idea behind syntax-to-morphology alignment. Section 3 describes our experimental set-up and presents results from a sequence of incremental syntax-to-morphology transformations, and additional techniques. Sec-tion 4 summarizes our constituent reordering ex-periments and their results. Section 5 presents a review of related work and situates our approach.
We assume that the reader is familiar with the basics of phrase-based statistical machine transla-tion (Koehn et al., 2003) and factored statistical machine translation (Koehn and Hoang, 2007). In this section, we describe how we map between certain source language syntactic structures and target words with complex morphological struc-tures. At the top of Figure 1, we see a pair of (syntactic) phrases, where we have (positionally) aligned the words that should be translated to each other. We can note that the function words on and Figure 1: Transformation of an English preposi-tional phrase their are not really aligned to any of the Turkish words as they really correspond to two of the mor-phemes of the last Turkish word.

When we tag and syntactically analyze the En-glish side into dependency relations, and morpho-logically analyze and disambiguate the Turkish phrase, we get the representation in the middle of Figure 1, where we have co-indexed components that should map to each other, and some of the syntactic relations that the function words are in-volved in are marked with dependency links. 1
The basic idea in our approach is to take various function words on the English side, whose syntac-tic relationships are identified by the parser, and then package them as complex tags on the related content words. So, in this example, if we move the first two function words from the English side and attach as syntactic tags to the word they are in dependency relation with, we get the aligned rep-resentation at the bottom of Figure 1. 2 , 3 Here we can note that all root words and tags that corre-spond to each other are nicely structured and are in the same relative order. In fact, we can treat each token as being composed of two factors: the roots and the accompanying tags. The tags on the Turkish side encode morphosyntactic information encoded in the morphology of the words, while the (complex) tags on the English side encode local (and sometimes, non-local) syntactic information. Furthermore, we can see that before the transfor-mations, the English side has 4 words, while af-terwards it has only 2 words. We find (and elab-orate later) that this reduction in the English side of the training corpus, in general, is about 30%, and is correlated with improved BLEU scores. We believe the removal of many function words and their folding into complex tags (which do not get involved in GIZA++ alignment  X  we only align the root words) seems to improve alignment as there are less number of  X  X ords X  to worry about during that process. 4
Another interesting side effect of this represen-tation is the following. As the complex syntac-tic tags on the English side are based on syntactic relations and not necessarily positional proximity, the tag for relations in a phrase like in their cul-tural, historical and economic relations would be exactly the same as above. Thus phrase extrac-tion algorithms can conflate all constructs like in their . . . economic relations as one phrase, regard-less of the intervening modifiers, assuming that parser does its job properly.

Not all cases can be captured as cleanly as the example above, but most transformations capture local and nonlocal syntax involving many function words and then encode syntax with complex tags resembling full morphological tags on the Turk-ish side. These transformations, however, are not meant to perform sentence level constituent re-ordering on the English side. We explore these later.

We developed set of about 20 linguistically-motivated syntax-to-morphology transformations which had variants parameterized depending on what, for instance, the preposition or the adverbial was, and how they map to morphological struc-ture on the Turkish side. For instance, one general rule handles cases like while . . . verb and if . . . verb etc., mapping these to appropriate complex tags. It is also possible that multiple transformations can apply to generate a single English complex tag: a portion of the tag can come from a verb complex transformation, and another from an ad-verbial phrase transformation involving a marked such as while . Our transformations handle the fol-lowing cases:  X  Prepositions attach to the head-word of their  X  Possessive pronouns attach to the head-word  X  The possessive markers following a noun  X  Auxiliary verbs and negation markers attach  X  Modals attach to the lexical verb they modify.  X  Forms of be used as predicates with adjecti- X  Forms of be or have used to form passive  X  Various adverbial clauses formed with if , As stated earlier, these rules are linguistically mo-tivated and are based on the morphological struc-ture of the target language words. Hence for dif-ferent target languages these rules will be differ-ent. The rules recognize various local and nonlo-cal syntactic structures in the source side parse tree that correspond to complex morphological of tar-get words and then remove source function words folding them into complex tags. For instance, the transformations in Figure 1 are handled by scripts that process Malt Parser X  X  dependency structure output and that essentially implement the follow-ing sequence of rules expressed as pseudo code: log like-variables that bind to patterns (mostly root words), and the conditions check for specified de-pendency relations (e.g., PMOD ) between the left and the right sides. When the condition is satis-fied, then the part matching the function word is removed and its syntactic information is appended to form the complex tag on the noun ( &lt;TAG&gt; would either match null string or any previously ap-pended function word markers.) 5
There are several other rules that handle more mundane cases of date and time constructions (for which, the part of the date construct which the parser attaches a preposition, is usually different than the part on the Turkish side that gets inflected with case markers, and these have to be reconciled by overriding the parser output.)
The next section presents an example of a sen-tence with multiple transformations applied, after discussing the preprocessing steps. 3.1 Data Preparation We worked on an English-Turkish parallel corpus which consists of approximately 50K sentences with an average of 23 words in English sentences and 18 words in Turkish sentences. This is the same parallel data that has been used in earlier SMT work on Turkish (Durgar-El-Kahlout and Oflazer, 2010). Let X  X  assume we have the follow-ing pair of parallel sentences:
On the English side of the data, we use the Stan-ford Log-Linear Tagger (Toutanova et al., 2003), to tag the text with Penn Treebank Tagset. On the Turkish side, we perform a full morphological analysis, (Oflazer, 1994), and morphological dis-ambiguation (Yuret and T  X  ure, 2006) to select the contextually salient interpretation of words. We then remove any morphological features that are not explicitly marked by an overt morpheme . 6 So for both sides we get, Finally we parse the English sentences using MaltParser (Nivre et al., 2007), which gives us labeled dependency parses. On the output of the parser, we make one more transformation. We re-place each word with its root, and possibly add an additional tag for any inflectional information con-veyed by overt morphemes or exceptional forms. This is done by running the TreeTagger (Schmid, 1994) on the English side which provides the roots in addition to the tags, and then carrying over this information to the parser output. For example, is is tagged as be+VB VBZ , made is tagged as make+VB VBN , and a word like books is tagged as book+NN NNS (and not as books+NNS ). On the Turkish side, each marker with a preceding + is a morphological feature. The first marker is the part-of-speech tag of the root and the re-mainder are the overt inflectional and derivational markers of the word. For example, the analy-sis kitap+Noun+P2pl+A3pl+Gen for a word like kitap-lar- X n X z- X n 7 ( of your books ) represents the root kitap ( book ), a Noun , with third person plural agreement A3pl , second per-son plural possessive agreement, P2pl and geni-tive case Gen .

The sentence representations in the middle part of Figure 2 show these sentences with some of the dependency relations (relevant to our transforma-tions) extracted by the parser, explicitly marked as labeled links. The representation at the bottom of this figure (except for the co-indexation markers) corresponds to the final transformed form of the parallel training and test data. The co-indexation is meant to show which root words on one side map to which on the other side. Ultimately we would want the alignment process to uncover the root word alignments indicated here. We can also note that the initial form of the English sentence has 14 words and the final form after transforma-tions, has 7 words (with complex tags). 8 3.2 Experiments We evaluated the impact of the transformations in factored phrase-based SMT with an English-Turkish data set which consists of 52712 parallel sentences. In order to have more confidence in the impact of our transformations, we randomly gen-erated 10 training, test and tune set combinations. For each combination, the latter two were 1000 sentences each and the remaining 50712 sentences were used as training sets. 9 , 10
We performed our experiments with the Moses toolkit (Koehn et al., 2007). In order to encourage long distance reordering in the decoder, we used a distortion limit of -1 and a distortion weight of 0.1. 11 We did not use MERT to further optimize our model. 12
For evaluation, we used the BLEU metric (Pap-ineni et al., 2001). Each experiment was repeated over the 10 data sets. Wherever meaningful, we report the average BLEU scores over 10 data sets along with the maximum and minimum values and the standard deviation. 3.2.1 The Baseline Systems As a baseline system, we built a standard phrase-based system, using the surface forms of the words without any transformations, and with a 3-gram LM in the decoder. We also built a second baseline system with a factored model. Instead of using just the surface form of the word, we included the root, part-of-speech and morphological tag information into the corpus as additional factors alongside the surface form. 13 Thus, a token is represented with three factors as Surface|Root|Tags where Tags are complex tags on the English side, and morphological tags on the Turkish side. 14
Moses lets word alignment to align over any of the factors. We aligned our training sets using only the root factor to conflate statistics from different forms of the same root. The rest of the factors are then automatically assumed to be aligned, based on the root alignment. Furthermore, in factored models, we can employ different language models for different factors. For the initial set of experi-ments we used 3-gram LMs for all the factors.
For factored decoding, we employed a model whereby we let the decoder translate a surface form directly, but if/when that fails, the decoder can back-off with a generation model that builds a target word from independent translations of the root and tags.
The results of our baseline models are given in top two rows of Table 1. As expected, the word-based baseline performs worse than the factored baseline. We believe that the use of multiple lan-guage models (some much less sparse than the sur-face LM) in the factored baseline is the main rea-son for the improvement. 3.2.2 Applying Syntax-to-Morphology To gauge the effects of transformations separately, we first performed them in batches on the En-glish side. These batches were (i) transforma-tions involving nouns and adjectives ( Noun+Adj ), (ii) transformations involving verbs ( Verb ), (iii) transformations involving adverbs ( Adv ), and (iv) transformations involving verbs and adverbs ( Verb+Adv ).

We also performed one set of transformations on the Turkish side. In general, English preposi-tions translate as case markers on Turkish nouns. However, there are quite a number of lexical post-positions in Turkish which also correspond to En-glish prepositions. To normalize these with the handling of case-markers, we treated these postpo-sitions as if they were case-markers and attached them to the immediately preceding noun, and then aligned the resulting training data ( PostP ). 15
The results of these experiments are presented in Table 1. We can observe that the com-bined syntax-to-morphology transformations on the source side provide a substantial improvement by themselves and a simple target side transfor-mation on top of those provides a further boost to 21.96 BLEU which represents a 28.57% rel-ative improvement over the word-based baseline and a 18.00% relative improvement over the fac-tored baseline.
 Table 1: BLEU scores for a variety of transforma-tion combinations
We can see that every transformation improves the baseline system and the highest performance is attained when all transformations are performed. However when we take a closer look at the indi-vidual transformations performed on English side, we observe that not all of them have the same ef-fect. While Noun+Adj transformations give us an increase of 2.73 BLEU points, Verbs improve the result by only 0.8 points and improvement with Adverbs is even lower. To understand why we get such a difference, we investigated the corre-lation of the decrease in the number of tokens on both sides of the parallel data, with the change in BLEU scores. The graph in Figure 3 plots the BLEU scores and the number of tokens in the two sides of the training data as the data is modified with transformations. We can see that as the num-ber of tokens in English decrease, the BLEU score increases. In order to measure the relationship between these two variables statistically, we per-formed a correlation analysis and found that there is a strong negative correlation of -0.99 between the BLEU score and the number of English tokens. We can also note that the largest reduction in the number of tokens comes with the application of the Noun+Adj transformations, which correlates with the largest increase in BLEU score.

It is also interesting to look at the n -gram pre-cision components of the BLEU scores (again av-eraged). In Table 2, we list these for words (ac-tual BLEU), roots (BLEU-R) to see how effective we are in getting the root words right, and mor-phological tags, (BLEU-M), to see how effective we are in getting just the morphosyntax right. It Table 2: Details of Word, Root and Morphology BLEU Scores seems we are getting almost 69% of the root words and 68% of the morphological tags correct, but not necessarily getting the combination equally as good, since only about 56% of the full word forms are correct. One possible way to address is to use longer distance constraints on the morphological tag factors, to see if we can select them better. 3.2.3 Experiments with higher-order Factored phrase-based SMT allows the use of mul-tiple language models for the target side, for dif-ferent factors during decoding. Since the number of possible distinct morphological tags (the mor-phological tag vocabulary size) in our training data (about 3700) is small compared to distinct num-ber of surface forms (about 52K) and distinct roots (about 15K including numbers), it makes sense to investigate the contribution of higher order n -gram language models for the morphological tag factor on the target side, to see if we can address the ob-servation in the previous section.

Using the data transformed with Noun+Adj-+Verb+Adv+PostP transformations which previ-ously gave us the best results overall, we experi-mented with using higher order models (4-grams to 9-grams) during decoding, for the morphologi-cal tag factor models, keeping the surface and root models at 3-gram. We observed that for all the 10 data sets, the improvements were consistent for up to 8-gram. The BLEU with the 8-gram for only the morphological tag factor averaged over the 10 data sets was 22.61 (max: 23.66, min: 21.37, std: 0.72) compared to the 21.96 in Table 1. Using a 4-gram root LM, considerably less sparse than word forms but more sparse that tags, we get a BLEU score of 22.80 (max: 24.07, min: 21.57, std: 0.85). The details of the various BLEU scores are shown in the two halves of Table 3. It seems that larger n -gram LMs contribute to the larger n -gram preci-sions contributing to the BLEU but not to the uni-gram precision. Table 3: Details of Word, Root and Morphology BLEU Scores, with 8-gram tag LM and 3/4-gram root LMs 3.2.4 Augmenting the Training Data In order to alleviate the lack of large scale parallel corpora for the English X  X urkish language pair, we experimented with augmenting the training data with reliable phrase pairs obtained from a previous alignment. Phrase table entries for the surface fac-tors produced by Moses after it does an alignment on the roots, contain the English ( e ) and Turkish ( t ) parts of a pair of aligned phrases, and the proba-bilities, p ( e | t ) , the conditional probability that the English phrase is e given that the Turkish phrase is t , and p ( t | e ) , the conditional probability that the Turkish phrase is t given the English phrase is e . Among these phrase table entries, those with some threshold, can be considered as reliable mu-tual translations, in that they mostly translate to each other and not much to others. We extracted from the phrase table those phrases with 0 . 9  X  and added them to the training data to further bias the alignment process. The resulting BLEU score was 23.78 averaged over 10 data sets (max: 24.52, min: 22.25, std: 0.71). 16 The transformations in the previous section do not perform any constituent level reordering , but rather eliminate certain English function words as tokens in the text and fold them into complex syn-tactic tags. That is, no transformations reorder the English SVO order to Turkish SOV, 17 for in-stance, or move postnominal prepositional phrase modifiers in English, to prenominal phrasal mod-ifiers in Turkish. Now that we have the parses of the English side, we have also investigated a more comprehensive set of reordering transforma-tions which perform the following constituent re-orderings to bring English constituent order more in line with the Turkish constitent order at the top and embedded phrase levels:  X  Object reordering ( ObjR ), in which the ob- X  Adverbial phrase reordering ( AdvR ), which  X  Passive sentence agent reordering ( PassAgR ),  X  Subordinate clause reordering ( SubCR )
We performed these reorderings on top of the data obtained with the Noun+Adj+Verb+Adv+PostP transformations earlier in Section 3.2.2 and used the same decoder parameters. Table 4 shows the performance obtained after various combination of reordering operations over the 10 data sets. Although there were some improvements for certain cases, none of reordering gave consistent improvements for all the data sets. A cursory examinations of the alignments produced after these reordering transformations indicated that the resulting root alignments were not necessarily that close to being monotonic as we would have expected.
 Table 4: BLEU scores of after reordering transfor-mations Statistical Machine Translation into a morpholog-ically rich language is a challenging problem in that, on the target side, the decoder needs to gen-erate both the right sequence of constituents and the right sequence of morphemes for each word. Furthermore, since for such languages one can generate tens of hundreds of inflected variants, standard word-based alignment approaches suf-fer from sparseness issues. Koehn (2005) applied standard phrase-based SMT to Finnish using the Europarl corpus and reported that translation to Finnish had the worst BLEU scores.

Using morphology in statistical machine trans-lation has been addressed by many researchers for translation from or into morphologically rich(er) languages. Niessen and Ney (2004) used mor-phological decomposition to get better alignments. Yang and Kirchhoff (2006) have used phrase-based backoff models to translate unknown words by morphologically decomposing the unknown source words. Lee (2004) and Zolmann et al. (2006) have exploited morphology in Arabic-English SMT. Popovic and Ney (2004) investi-gated improving translation quality from inflected languages by using stems, suffixes and part-of-speech tags. Goldwater and McClosky (2005) use morphological analysis on the Czech side to get improvements in Czech-to-English statistical machine translation. Minkov et al. (2007) have used morphological postprocessing on the target side, to improve translation quality. Avramidis and Koehn (2008) have annotated English with addi-tional morphological information extracted from a syntactic tree, and have used this in translation to Greek and Czech. Recently, Bisazza and Federico (2009) have applied morphological segmentation in Turkish-to-English statistical machine transla-tion and found that it provides nontrivial BLEU score improvements.
 In the context of translation from English to Turkish, Durgar-El Kahlout and Oflazer (2010) have explored different representational units of the lexical morphemes and found that selectively splitting morphemes on the target side provided nontrivial improvement in the BLEU score. Their approach was based on splitting the target Turk-ish side, into constituent morphemes while our ap-proach in this paper is the polar opposite: we do not segment morphemes on the Turkish side but rather join function words on the English side to the related content words. Our approach is some-what similar to recent approaches that use com-plex syntactically-motivated complex tags. Birch et al. (2007) have integrated more syntax in a factored translation approach by using CCG su-pertags as a separate factor and have reported a 0.46 BLEU point improvement in Dutch-to-English translations. Although they used su-pertags, these were obtained not via syntactic anal-ysis but by supertagging, while we determine, on the fly, the appropriate syntactic tags based on syn-tactic structure. A similar approach based on su-pertagging was proposed by Hassan et al. (2007). They used both CCG supertags and LTAG su-pertags in Arabic-to-English phrase-based transla-tion and have reported about 6% relative improve-ment in BLEU scores. In the context of reorder-ing, one recent work (Xu et al., 2009), was able to get an improvement of 0.6 BLEU points by us-ing source syntactic analysis and a constituent re-ordering scheme like ours for English-to-Turkish translation, but without using any morphology. We have presented a novel way to incorporate source syntactic structure in English-to-Turkish phrase-based machine translation by parsing the source sentences and then encoding many local and nonlocal source syntactic structures as addi-tional complex tag factors. Our goal was to ob-tain representations of source syntactic structures that parallel target morphological structures, and enable us to extend factored translation, in appli-cability, to languages with very disparate morpho-logical structures.

In our experiments over a limited amount train-ing data, but repeated with 10 different training and test sets, we found that syntax-to-morphology mapping transformations on the source side sen-tences, along with a very small set of transforma-tions on the target side, coupled with some ad-ditional techniques provided about 39% relative improvement in BLEU scores over a word-based baseline and about 28% improvement of a factored baseline. We also experimented with numerous additional syntactic reordering transformation on the source to further bring the constituent order in line with the target order but found that these did not provide any tangible improvements when av-eraged over the 10 different data sets.

It is possible that the techniques presented in this paper may be less effective if the available data is much larger, but we have reasons to be-lieve that they will still be effective then also. The reduction in size of the source language side of the training corpus seems to be definitely effective and there no reason why such a reduction (if not more) will not be observed in larger data. Also, the preprocessing of English prepositional phrases and many adverbial phrases usually involve rather long distance relations in the source side syntactic structure 18 and when such structures are coded as complex tags on the nominal or verbal heads, such long distance syntax is effectively  X  X ocalized X  and thus can be better captured with the limited win-dow size used for phrase extraction.

One limitation of the approach presented here is that it is not directly applicable in the reverse direction. The data encoding and set-up can di-rectly be employed to generate English  X  X ransla-tion X  expressed as a sequence of root and complex tag combinations, but then some of the complex tags could encode various syntactic constructs. To finalize the translation after the decoding step, the function words/tags in the complex tag would then have to be unattached and their proper positions in the sentence would have to be located. The problem is essentially one of generating multiple candidate sentences with the unattached function words ambiguously positioned (say in a lattice) and then use a second language model to rerank these sentences to select the target sentence. This is an avenue of research that we intend to look at in the very near future.
 We thank Joakim Nivre for providing us with the parser. This publication was made possible by the generous support of the Qatar Foundation through Carnegie Mellon University X  X  Seed Research pro-gram. The statements made herein are solely the responsibility of the authors.
