 The prevalent use of social media produces mountains of un-labeled, high-dimensional data. Feature selection has been shown effective in dealing with high-dimensional data for efficient data mining. Feature selection for unlabeled data remains a challenging task due to the absence of label infor-mation by which the feature relevance can be assessed. The unique characteristics of social media data further compli-cate the already challenging problem of unsupervised feature selection, (e.g., part of social media data is linked, which makes invalid the independent and identically distributed assumption), bringing about new challenges to traditional unsupervised feature selection algorithms. In this paper, we study the differences between social media data and tradi-tional attribute-value data, investigate if the relations re-vealed in linked data can be used to help select relevant features, and propose a novel unsupervised feature selection framework, LUFS, for linked social media data. We perform experiments with real-world social media datasets to evalu-ate the effectiveness of the proposed framework and probe the working of its key components.
 1.5.2 [ Pattern Recognition ]: Design Methodology X  Fea-ture evaluation and Selection Algorithms, Theory Unsupervised Feature Selection, Linked Social Media Data, Pseudo-class Label
The myriads of social media services such as Facebook and Twitter are available that allow people to communicate and express themselves conveniently and effortlessly. The pervasive use of social media produces massive data at an unprecedented speed. For example, 200 million tweets are posted per day 1 ; 3,000 photos are uploaded per minute to Flickr 2 ; and the number of Facebook users has increased from 100 million in 2008 to 800 million in 2011 3 . The massive and high-dimensional social media data poses new challenges to data mining tasks such as classification and clustering. One traditional and effective approach to han-dle high-dimensional data is feature selection [7, 18]. Fea-ture selection aims to select relevant features from the high-dimensional data for a compact and accurate data represen-tation. It can alleviate the curse of dimensionality, speed up the learning process, and improve the generalization capa-bility of a learning model [16, 19].

According to whether the training data is labeled or unla-beled, feature selection algorithms can be roughly divided into supervised and unsupervised feature selection. It is time-consuming and costly to obtain labeled data. Given the scale of social media data, we propose to study unsu-pervised feature selection. Unsupervised feature selection is particularly difficult due to the absence of class labels for fea-ture relevance assessment [3]. Social media data adds further challenges to feature selection. Most existing feature selec-tion algorithms work with  X  X lat X  attribute-value data which is typically assumed to be independent and identically dis-tributed ( i.i.d. ). However, the i.i.d. assumption does not hold for social media data since it is inherently linked. Fig-ure 1 shows a simple example of linked data in social media and its two data representations. Figure 1(a) shows 8 linked instances ( u 1 to u 8 ). These instances usually form groups and instances within groups have more connections than in-stances between groups [31]. For example, u 1 has more con-nections to { u 2 , u 3 , u 4 } than { u 5 , u 6 , u 7 , u a conventional representation of attribute-value data: rows are instances and columns are features. For linked data, except for the conventional representation, there is link in-formation between instances as shown in Figure 1(c).
Linked data in social media presents both challenges and opportunities for unsupervised feature selection. In this work, we investigate: (1) how to exploit and model the rela-tions among data instances , and (2) how to take advantage of these relations for feature selection using unlabeled data . One key difference between supervised and unsupervised fea-http://techcrunch.com/2011/06/30/twitter-3200-million-tweets/ http://www.flickr.com/ http://en.wikipedia.org/wiki/Facebook ture selection is the availability of label information. If we change our perspective and consider label information as a sort of constraints in the learning space, we can turn both types of feature selection to a problem with the same intrin-sic property: selecting features to be consistent with some constraints [35]. In supervised learning, label information plays the role of constraint. Without labels, other alter-native constraints are proposed, such as data variance and separability, for unsupervised learning [27, 7, 4, 14, 35, 3]. However, most of them evaluate the importance of feature individually and neglect the feature correlation [36, 34].
In our attempt to address the challenges of unsupervised feature selection for linked social media data, we propose a novel framework of Linked Unsupervised Feature Selection (LUFS). Our main contributions are summarized below:
The rest of this paper is organized as follows. We for-mally define the problem of unsupervised feature selection for linked data in social media in Section 2; introduce our new framework of unsupervised feature selection, LUFS in Section 3, including social dimension regularization, opti-mization, and convergence analysis; present empirical eval-uation with discussion in Section 4 and the related work in Section 5; and conclude this work in Section 6.
In this paper, scalars are denoted by lower-case letters letters ( a , b , . . . ), and matrices correspond to boldfaced upper-case letters ( A , B , . . . ).

Let u = { u 1 , u 2 , . . . , u n } be the set of linked data (e.g., u to u 8 in Figure 1) where n is the number of data instances and f = { f 1 , f 2 , . . . , f m } be the set of features where m is the number of features. Let X = ( x 1 , x 2 , . . . , x n be the conventional representation of u w.r.t . f where x is the frequency of f j used by u i (e.g., Figure 1(b)). We assume that the data, X , is centered, that is, P n i =1 x which can be realized as X = XP where P = I n  X  1 n 1 n 1
F or social media data, there also exist connections among its data instances. Let R  X  R n  X  n denote their connections where R ( i, j ) = 1 if u i and u j are linked, otherwise zero (e.g., the right subgraph in Figure 1(c)). In our work, we assume that the relationships among data instances form an undirected graph, i.e., R = R  X  .

With the above notations defined, the problem of unsuper-vised feature selection for linked data can be stated as: given n linked data instances, its conventional representation X , its link representation R , develop a method f , which can se-lect a subset of relevant features f  X  from f by utilizing both X and R for these n linked instances, formally stated as, The above is substantially different from the traditional task of unsupervised feature selection, formally defined as,
Set s =  X  ( function and k is the number of features to select where s = 1 indicates that the i -th feature is selected. The orig-inal data can be represented as diag( s ) X with k selected features, where diag( s ) is a diagonal matrix. The difficulty faced with unsupervised feature selection is due to lack of class labels. Hence, we introduce the concept of pseudo-class label to guide unsupervised learning. We assume that there is a mapping matrix W  X  R m  X  c , which assigns each data point with a pseudo-class label where c is the number of pseudo-class labels. The pseudo-class label indicator matrix is Y = W  X  diag( s ) X  X  R c  X  n . Each column of Y has only one nonzero entity, i.e., k Y (: , i ) k 0 = 1 where k  X  k vector zero norm, counting the number of nonzero elements in the vector. Since X is centered, it is easy to verify that Y is also centered,
X
As shown above, both supervised and unsupervised learn-ing tasks aim to solve the same problem: selecting features consistent with given constraints. In the supervised set-t ing, pseudo-class label information is tantamount to the provided label information; in the unsupervised setting, we seek pseudo-class label information from linked social media data. We discuss technical details of the proposed frame-work LUFS in the following subsection.
First, we embark on dealing with linked data. In [30], so-cial dimension is introduced as a means to integrate the in-terdependency among linked data and attribute-value data. Instances from different social dimensions are dissimilar while instances in the same social dimension are similar. Flatten-ing linked data with social dimensions, traditional classifi-cation methods such as SVM obtain better performance for relational learning [30].

Social dimension extraction is a well-studied problem by social network analysis community [24, 11, 30]. In our work, we adopt a widely used algorithm, i.e., Modularity Maxi-mization [24], which can formulated as: where H  X  R K  X  n is the social dimension indicator matrix, K is the number of social dimensions, and M is the modularity matrix defined as: where d is a degree vector, and d i is the degree of u i .
The standard K-means algorithm is performed to obtain the discrete-valued social dimension assignment, H ( i, j ) = 1, if u j is in the i -th dimension, and H ( i, j ) = 0, otherwise.
Recall the simple example of linked data in Figure 1, we can extract two social dimensions, i.e, { u 1 , u 2 , u 3 { u 5 , u 6 , u 7 , u 8 } . From Figure 1, we can observe that in-stances in the same social dimension have more connections. The discrete-valued social dimension indicator matrix, H , is:
Since social dimensions represent a type of user affilia-tions, inspired by Linear Discriminant Analysis, we define three matrices: within, between, and total social dimension scatter matrixes S w , S b , and S t as follows: where F is the weighted social dimension indicator matrix, which can be obtained from H as below, where the j -th column of F is given by where h j is the number of instances in the j -th social di-mension.

Instances in the same social dimension are similar and in-stances from different social dimensions are dissimilar. We c an obtain a constraint from link information, social di-mension regularization , to model the relations among linked instances, via the following maximization problem,
To take advantage of information from attribute-value part, i.e., X , similar data instances should have similar la-bels. According to spectral analysis [22], the constraint from attribute-value part can be formulated as the following min-imization problem, where L = D  X  S is a laplacian matrix and D is a diagonal matrix with its elements defined as D ( i, i ) = P n j =1 S  X  R n  X  n denotes the similarity matrix based on X , ob-tained through a RBF kernel as,
By introducing the concept of pseudo-class labels , con-straints from both link information and unlabeled attribute-value data are ready for unsupervised feature selection. Our framework of linked unsupervised feature selection, LUFS, is conceptually shown in Figure 2 with our solutions to the two challenges (need to take into account of linked data and lack of labels): extracting constraints from both linked and attribute-value data, and then constructing pseudo-class la-bels through social dimension extraction and spectral anal-ysis. Thus, our proposed framework, LUFS, is equivalent to solving the following optimization problem,
The problem in Eq. (12) is difficult to solve due to the two constraints. To make the problem solvable, we use some widely used relaxation methods. First, according to common relaxation for label indicator matrix [22], the constraint on Y is relaxed to orthogonality, i.e., YY  X  = I c .
With this relaxation, S t in social dimension regularization is an identity matrix and T r ( YY  X  ) = c is a constant. Social dimension regularization can be rewritten as, the reason for adding the constant in social dimension reg-u larization will be explained later. With this relaxation, Eq. (12) can be relaxed to,
The constraint on s makes Eq. (14) mixed integer pro-gramming [2], which is still difficult to solve. We observe that that diag( s ) and W is as the form of diag( s ) W in Eq. (14). Since s is a binary vector and m  X  k rows of the diag( s ) are all zeros, diag( s ) W is a matrix where the elements of many rows are all zeros. This motivates us to absorb the diag( s ) into W , W = diag( s ) W , and add  X  norm on W to achieve feature selection as, where  X  I is added to make ( XX  X  +  X  I ) nonsingular. k W k controls the capacity of k W k and also ensures that k W k is sparse in rows, making it particularly suitable for feature selection. k W k 2 , 1 is the  X  2 , 1 -norm of k W k defined as [5],
Lemma 1. Assume E  X  R n  X  m is a matrix with orthonor-mal columns, that is, E  X  E = I m , then I n  X  EE  X  is a sym-metric and positive semidefinite matrix.

Proof. It is easy to verify that I n  X  EE  X  is a symmetric matrix since ( I n  X  EE  X  )  X  = I n  X  EE  X  .

Let E = U  X  V  X  be the Singular Value Decomposition (SVD) of E , where U  X  R n  X  n and V  X  R m  X  m are or- X   X   X  2  X  . . .  X   X  m  X  0. Then we have, Then, Since ( I n  X   X  X   X  ) is a diagonal matrix and the diagonal ele-ments are all nonnegative, ( I n  X   X  X   X  ) is a positive semidefi-nite matrix. Thus U ( I n  X   X  X   X  ) U  X  is a semidefinite matrix, which completes the proof.

The importance of Lemma 1 is two-fold. First, it explains w hy we need a constant on the social dimension regulariza-tion. Adding the constant can guarantee the convexity of the objective function in Eq. (15). Second, it paves the way for the following theorem for LUFS.

Theorem 1. Eq. (15) can be converted into the following optimization problem, where A is a symmetric and positive semidefinite matrix and B is a symmetric and positive matrix.

Proof. It suffices to show how to construct a symmetric and positive semidefinite matrix A and a symmetric and positive matrix B from Eq. (15).

We construct B = XX  X  +  X  I . It is easy to check that B is symmetric and positive when  X  6 = 0 and then the constraint in Eq. (15) is converted into the one in Eq. (19).
We construct A = XLX  X  +  X  X ( I n  X  FF  X  ) X  X  and then the objective function in Eq. (15) is converted into the one in Eq. (19). According to Lemma 1, ( I n  X  FF  X  ) is symmetric and positive semidefinite; and according to the definition of Laplacian matrix, XLX  X  is symmetric and positive semidef-inite. Thus, A is symmetric and positive semidefinite, which completes the proof.
I n recent years, many methods have been proposed to solve the  X  2 , 1 -norm minimization problem [21, 25, 34]. How-ever, our problem is different from these existing ones due to the orthogonal constraint in Eq. (19). Hence, we pro-pose the following algorithm, as shown in Algorithm 1, to optimize the problem in Eq. (19).
 Algorithm 1 L UFS Input: { X , R ,  X ,  X ,  X , c, K, k } Output: k most relevant features 1: Obtain the social dimension indicator matrix H 2: Set F = H ( H  X  H )  X  1 2 3 : Construct S through Eq. (11) 4: Set L = D  X  S 5: Set A = XLX  X  +  X  X ( I n  X  FF  X  ) X  X  6: Set B = XX  X  +  X  I 7: Set t = 0 and initialize D 0 as an identity matrix 8: while Not convergent do 9: Set C t = B  X  1 ( A +  X  D t ) 10: Set W t = [ q 1 , . . . , q c ] where q 1 , . . . , q 11: Update the diagonal matrix D t +1 , where the i -th di-12: Set t = t + 1 13: end while 14: Sort each feature according to k W ( i, :) k 2 in descend-
In Algorithm 1, social dimension extraction and weighted s ocial dimension indicator construction are from line 1 to line 2. The iterative algorithm to optimize Eq. (19) is pre-sented from line 8 to line 13. The convergence analysis of the algorithm starts with the following two lemmas.
Lemma 2. A  X  R m  X  m is symmetric and positive semidef-inite and B  X  R m  X  m is symmetric and positive. If W  X  R m  X  c solves the following minimization problem: then W consists of the eigenvectors of B  X  1 A corresponding to the c smallest eigenvalues.
 Proof. The lemma can be easily obtained through Ky Fan Theorem [15].
Lemma 3. T he following inequality holds if w i t | r i =1 non-zero vectors, where r is an arbitrary number.
Proof. The detailed proof can be found in our previous work [29].

With the above two lemmas, we develop the following the-o rem regarding the convergence of Algorithm 1.

Theorem 2. At each iteration of Algorithm 1, the value of the objective function in Eq. (19) monotonically decreases.
Proof. According to Lemma 2, W t +1 in line 9 of Algo-rithm 1 is the solution to the following problem, which indicates that,
T r W  X  t +1 ( A +  X  D t ) W t +1  X  T r W  X  t ( A +  X  D t Then we have the following inequality, According to the lemma 3, we can obtain, which completes the proof.

According to Theorem 2, Algorithm 1 converges to the o ptimal W for the problem in Eq. (19).
In this section, we present experiment details to verify the effectiveness of the proposed framework, LUFS. After intro-ducing real-world social media datasets, we first evaluate the quality of selected features in terms of clustering perfor-mance, then study the effects of parameters on performance and finally further verify the constraint extracted from link information by social dimension.
We collect two datasets from real-world social media web-sites, i.e., BlogCatalog 4 and Flickr 5 , which are the subsets of two public available datasets used in [31] to uncover overlap-ping groups in social media. Some statistics of the datasets are shown in Table 1.

LUFS is compared with the following three unsupervised feature selection algorithms, h ttp://www.blogcatalog.com http://www.flickr.com/ Following the existing evaluation practice for unsupervised feature selection, we assess LUFS in terms of clustering performance. We vary the numbers of selected features selection algorithm is first performed to select features, then K-means clustering is performed based on the selected fea-tures. Since K-means often converges to local minima, we repeat each experiment 20 times and report the average per-formance.

The clustering quality is evaluated by two commonly used metrics, accuracy and normalized mutual information (NMI) label of the j -th document, the accuracy is defined as: where  X  ( x, y ) is the delta function that its value is 1 if x = y and 0 otherwise.
 Given two clusterings C and C  X  , the mutual information M I ( C, C  X  ) is defined as: and NMI is defined by where H ( C ) and H ( C  X  ) represent the entropies of clusterings C and C  X  , respectively. Larger NMI values represent better clustering qualities.
In this subsection, we compare the quality of features selected by different algorithms using performance metrics given above. Conventionally, the parameters in feature se-lection algorithms are tuned via cross-validation. More dis-cussion is given in Section 4.3. The resulting parameter values for LUFS are: {  X  = 0 . 1 ,  X  = 0 . 1 , K = 70 , c = 9 } for Flickr while {  X  = 0 . 1 ,  X  = 0 . 1 , K = 10 , c = 6 } for Blog-Catalog.  X  is used to make ( XX  X  +  X  I ) nonsingular and according to empirical experience, we set  X  to 0 . 01 in both
W e use the source code from http://www.zjucadcg.cn/dengcai/Data/Clustering.html. datasets. The comparison results of Flickr and BlogCata-l og are shown in Tables 2 and 3, respectively. Note that the clustering performance with all features (i.e., without feature selection) is reported in the last columns.
We observe the performance change with the numbers of selected features: it increases, reaches the peak, and then de-creases. For example, LUFS achieves its peak values when the number of selected features are 500 and 300 for Flickr and BlogCatalog, respectively. The clustering performance with as few as 200 features is better than that with all fea-tures. For instances, LUFS obtains 10 . 51% and 18 . 68% relative improvement in terms of accuracy for Flickr and BlogCatalog, respectively. These results demonstrate that the number of features can be significantly reduced without performance deterioration.

LapScore obtains comparable results with SPEC on both datasets. Most of time, UDFS outperforms both LapScore and SPEC, which is consistent with the results reported in [34]. LapScore and SPEC analyze features separately and select features one after another, which may omit the pos-sible correlation between different features. While UDFS selects features in a batch mode and considers feature corre-lation. These observations support the conclusion in [3, 36, 34]: it is recommended to analyze data features jointly for feature selection.

We observe that LUFS consistently outperforms these base-line methods on both datasets. For example, LUFS gains up to 19 . 82% and 11 . 44% relative improvement in terms of ac-curacy in Flickr and BlogCatalog, respectively. All baseline methods are based on the data i.i.d. assumption, which is not valid due to linked data in social media; and LUFS ex-plicitly takes advantage of link information through social dimension regularization. We also note that with the in-creasing number of features, the improvement progressively decreases. Usually, LUFS achieves its best results sooner than baseline methods do. For example, LUFS achieves its best results in terms of accuracy in Flickr when 500 features are selected compared to 900 features for baseline methods.
In addition to determining the number of selected features (which remains an open problem [34]), LUFS has four im-portant parameters: the number of pseudo-class labels ( c ), the number of social dimensions ( K ),  X  (controlling social dimension regularization) and  X  (controlling  X  2 , 1 -norm reg-ularization). Hence, we study the effect of each of the four parameters ( c , K ,  X  , or  X  ) by fixing the other 3 to see how the performance of LUFS varies with the number of selected features. The processes of parameter selection for Flickr and for BlogCatalog are similar and we present the details for Flickr to save space. Examples of experimental results are presented next.

The number of pseudo-class labels c is varied from 2 to 20 with an incremental step of 1 while setting {  X  = 0 . 1 ,  X  = 0 . 1 , K = 70 } . The performance variation w.r.t. c and the number of features is depicted in Figure 3. Most of the time, the clustering performance first increases rapidly, reaches its best performance and decreases as c increases. This obser-vation can be used to guide the determination of the optimal number of c for LUFS. Note that when c varies from 5 to 11, the clustering performance is not sensitive to c , especially when the number of selected feature is large.

Fixing c = 9,  X  = 0 . 1 and  X  = 0 . 1, we vary the number of F igure 3: Number of Pseudo-class Labels vs Number of Features F igure 4: Number of Social Dimension vs Number of Features social dimensions from 10 to 100 with an incremental step of 10 and the performance variation w.r.t. the number of so-cial dimensions and the number of features is demonstrated in Figure 4. Most of the time, with the increasing number of social dimensions, the performance first increases, reaches its peak value and degrades. This pattern can be used to de-termine the optimal number of social dimensions for LUFS.
Fixing c = 9, K = 70 and  X  = 0 . 1, we vary  X  as { 1 e  X  6 , w.r.t.  X  and the number of features is depicted in Fig-ure 5. The performance first increases and most of time, the peak values for both accuracy and NMI are achieved when  X  = 0 . 1, indicating the importance of social dimension regularization for LUFS. After  X  = 0 . 5, the performance is dramatically degraded, suggesting that only link informa-tion is not enough for LUFS.

To study how  X  and the number of features affect the 0 . 7 , 1 } and set c = 9, K = 70 and  X  = 0 . 1. The results are shown in Figure 6. We observe that the performance improves as  X  changes from 1 e  X  3 to 1 e  X  2 and from 1 e  X  2 to 0 . 1. These results demonstrate the capability of the  X  norm for feature selection.
Among the these four parameters of LUFS,  X  is most sen-sitive, the number of pseudo-class labels, the number of so-cial dimensions and  X  are not so.
A key contribution of LUFS to the performance improve-ment is to employ social dimensions extracted from linked data. Hence, we would like to probe further why the use of social dimensions works. One way is to investigate whether instances in the same social dimension are similar and in-stances from different social dimensions are dissimilar.
Let z = { z 1 , z 2 , . . . , z K } be the K social dimensions given by the social dimension extraction algorithm, i.e., Modular-ity Maximization [24] here, with sizes of { n 1 , n 2 , . . . , n where n i is number of instances in the i -th social dimension and P i n i = n . To create reference groups in comparison with social dimensions, we also randomly divide these n in-stances into K groups with sizes of { n 1 , n 2 , . . . , n z groups with sizes of { n 1 , n 2 , . . . , n K } and then each group in z  X  (e.g., z  X  i ) corresponds to a social dimension in z (e.g., z ). The label information is used earlier in assessing clus-tering performance. We use it again here. Let Y  X  be the class label indicator matrix. We center Y  X  as: Y  X  = Y  X  Two distance metrics are defined: D within and D between for within-and between-social dimension distance. D within and D between can be obtained from within social dimension scat-ter matrix S w and between social dimension scatter matrix S , respectively, For each specific number of social dimensions, K , we calcu-late D within and D between for z and z  X  . Varying K from 2 to 100 with an incremental step of 1, we obtain 99 pairs of D within and 99 pairs of D between . The results for Flickr and BlogCatalog are shown in Figure 7.

D within and D between change much faster for social dimen-sions, comparing with groups of random assignment. More-over, D within of a social dimension is much smaller than that of a random assignment group, thus, instances in the same social dimension are of similar labels. D between of a social dimension is much larger than that of a random assignment group, indicating that instances from different social dimen-sions are dissimilar.
 We also perform a two-sample t -test on these pairs of D within of z and z  X  at significant level 0 . 001. The null hypothesis, H 0 , is that there is no different between these pairs; the alternative hypothesis, H 1 , is that D within social dimension is less than that of the corresponding ran-dom assignment group. The t -test results, p -values, are 6 . 0397 e  X  060 and 8 . 2406 e  X  083 on Flicker and BlogCatalog, (c) D w ithin for BlogCatalog Figure 7: Within and Between Distance Achieved by Social Dimension and Random Assignment respectively. Hence, there is strong evidence to reject the null hypothesis. We conduct a similar test for the pairs of D between , and there is strong evidence to support that D between of a social dimension is significantly larger than that of its counterpart, a random assignment group. The evidence from both figures and t-test confirms the positive impact of the constraint from link information via social di-mensions.
Traditionally, feature selection algorithms can be either supervised or unsupervised [7, 20] based on the training data being labeled or unlabeled.

Supervised feature selection methods [20] can be broadly categorized into the wrapper models [8, 17] and the filter models [13, 26]. The wrapper model uses the predictive ac-curacy of a predetermined learning algorithm to determine the quality of selected features. These methods can be egre-giously expensive to run for data with a large number of features [6, 12]. The filter model separates feature selection from classifier learning so that the bias of a learning algo-rithm does not interact with the bias of a feature selection algorithm. It relies on measures of the general characteris-tics of the training data such as distance, consistency, depen-dency, information, and correlation [13]. Many researchers paid great attention to developing unsupervised feature se-lection [32, 14, 4]. Unsupervised feature selection [14, 7, 35] is a less constrained search problem without class labels, depending on clustering quality measures [10, 9], and can eventuate many equally valid feature subsets. With high-dimensional data, it is likely to find many sets of features that seem equally good without considering additional con-straints. Another key difficulty is how to objectively mea-sure the results of feature selection. A wrapper model is proposed in [7] to use a clustering algorithm in evaluating the quality of feature selection.

Recently, sparsity regularization, such as the  X  2 , 1 -norm of a matrix [5], in dimensionality reduction has been widely in-vestigated and applied to feature selection including multi-task feature selection [1, 21], robust joint  X  2 , 1 -Norms [25], spectral feature selection [35], discriminative unsupervised feature selection [34]. Through sparsity regularization, fea-ture selection can be embedded in the learning process.
The first attempt to select features on social media data is LinkedFS [29], a supervised algorithm. Various relations (coPost, coFollowing, coFollowed and Following) are extracted following social correlation theories [23]. LinkedFS signif-icantly improves the performance of feature selection by incorporating these relations into feature selection. How-ever, LinkedFS and LUFS are distinctively different. First, LinkedFS is formally stated as: f : { f ; X , Y }  X  { f  X  Y contains the label information, while LUFS is a unsu-pervised feature selection algorithm. Second, LinkedFS ex-ploits relations individually while LUFS employs relations as groups via social dimensions.
Linked data in social media presents new challenges to tra-ditional feature selection algorithms, which assume the data instances to be independent and identically distributed. In this paper, we propose a novel unsupervised feature selection framework, LUFS, for linked data in social media. We utilize a recent developed concept of social dimensions from social network analysis to extract relations among linked data as groups, and define social dimension regularization inspired by Linear Discriminant Analysis to mathematically model these relations. We then propose the concept of pseudo-class labels to develop a new unsupervised feature selection framework by ensuring that instances in a social dimension are similar, and otherwise dissimilar. Experimental results on two datasets from real-world social media websites show that the proposed method can effectively exploit link infor-mation in comparison with the state-of-the-art unsupervised feature selection methods.

In social media networks, the availability of various link formation can lead to networks with relationships of different strengths [33, 28], which means that weak links and strong links are often mixed together. We plan to incorporate tie strength prediction into LUFS to further exploit link infor-mation. Also we believe that the concept of pseudo-class label introduced in the paper is a powerful means to effec-tively constrain the learning space of unsupervised feature selection and can be extended to different applications with-out labeled data but additional information.
 The work is, in part, supported by ARO (#025071) and NSF (#0812551). [1] A. Argyriou, T. Evgeniou, and M. Pontil. Multi-task [2] S. Boyd and L. Vandenberghe. Convex optimization . [3] D. Cai, C. Zhang, and X. He. Unsupervised feature [4] C. Constantinopoulos, M. Titsias, and A. Likas. [5] C. Ding, D. Zhou, X. He, and H. Zha. R 1-pca: [6] R. Duda, P. Hart, D. Stork, et al. Pattern [7] J. Dy and C. Brodley. Feature selection for [8] J. G. Dy and C. E. Brodley. Feature subset selection [9] J. G. Dy and C. E. Brodley. Visualization and [10] J. G. Dy, C. E. Brodley, A. C. Kak, L. S. Broderick, [11] E. Erosheva, S. Fienberg, and J. Lafferty.
 [12] I. Guyon, J. Weston, S. Barnhill, and V. Vapnik. Gene [13] M. Hall. Correlation-based feature selection for [14] X. He, D. Cai, and P. Niyogi. Laplacian score for [15] R. Horn and C. Johnson. Matrix analysis . Cambridge [16] G. John, R. Kohavi, and K. Pfleger. Irrelevant feature [17] Y. Kim, W. Street, and F. Menczer. Feature selection [18] H. Liu and H. Motoda. Computational methods of [19] H. Liu and L. Yu. Toward integrating feature selection [20] H. Liu and L. Yu. Toward integrating feature selection [21] J. Liu, S. Ji, and J. Ye. Multi-task feature learning via [22] U. Luxburg. A tutorial on spectral clustering. [23] P. Marsden and N. Friedkin. Network studies of social [24] M. E. J. Newman and M. Girvan. Finding and [25] F. Nie, H. Huang, X. Cai, and C. Ding. Efficient and [26] H. Peng, F. Long, and C. Ding. Feature selection [27] V. Roth and T. Lange. Feature selection in clustering [28] J. Tang, H. Gao, and H. Liu. mtrust: Discerning [29] J. Tang and H. Liu. Feature selection with linked data [30] L. Tang and H. Liu. Relational learning via latent [31] X. Wang, L. Tang, H. Gao, and H. Liu. Discovering [32] L. Wolf and A. Shashua. Feature selection for [33] R. Xiang, J. Neville, and M. Rogati. Modeling [34] Y. Yang, H. Shen, Z. Ma, Z. Huang, and X. Zhou. [35] Z. Zhao and H. Liu. Spectral feature selection for [36] Z. Zhao, L. Wang, and H. Liu. Efficient spectral
