 In many practical machine learning applications, obtaining a fully labeled data set is usually difficult. The requirement of lots of human expertise makes the labeling process fairly expensive. A more feasible way is to label just a small part of data set, leaving a huge amount of examples in data set unlabeled. The learner itself should find a way to exploit the merit of unlabeled data. in both labeled and unlabeled data. Self-training [10] is a well-known semi-supervised algorithm. In self-training process, a base learner is firstly trained on labeled set. Then, iteratively, it attempts to choose to label several examples that it is most confident of in the unlabeled set. After that it enlarges its labeled training set with these self-labeled examples. Since the labeled set is usually insufficient for learning, misclassifying a certain amount of unlabeled data is unavoidable. Thus, the enlarged labeled set for the learner to learn in the next iteration could contain much noise. Once those noisy examples are added into the learner X  X  training set, there is no chance for the self-trained learner to reconsider the validity of those incorrect labels, and the mislabeled examples will keep on affecting the learner in the following iterations. If the distribution the learner has caught is badly distorted by those mislabeled examples, the generalization ability degrades as the self-training process goes on. Therefore, it is obvious that identifying and removing the mislabeled examples in each iteration might help improve the generalization ability of the learned hypothesis.
 TRaining with EDiting) is proposed. Setred introduces a data editing technique to the self-training process to filter out the noise in the self-labeled examples. Specifically, after labeling some examples chosen from the unlabeled set, Se-tred actively identifies the possibly mislabeled examples with the help of some local information in a neighborhood graph, and keeps those mislabeled examples from being added to the learner X  X  training set, hence a less noisy training set is obtained. Actually, Setred could be considered as a semi-supervised algorithm that utilizes an active-learning-like technique to improve its performance. Exper-iments on ten UCI data sets show that Setred is more robust than the standard self-training algorithm, and the generalization ability of its learned hypotheses outperform those learned by standard self-training.
 works on learning from labeled and unlabeled data. Section 3 presents Setred . Section 4 reports the experiment result. Finally, Section 5 concludes and issues some future work. An effective way to utilize unlabeled data in assistance of supervised learning is known as semi-supervised learning [13], where an initial hypothesis is learned from the labeled set and then refined through information derived from the unlabeled set.
 examples iteratively according its own knowledge. Such methods include using Estimation-Maximization approach to estimate posterior parameters of a gen-erative model, such as Naive Bayes, by assigning each unlabeled example a soft label, i.e. a probability for each class [11]; using the unlabeled data to search for a better structure of Bayesian Network [2]; using a transductive inference for support vector machines on a special test set [4]. The self-training algorithm [10] is of this kind, where in each iteration the learner converts the most confidently predicted unlabeled example of each class into a labeled training example. representative is the co-training paradigm proposed by Blum and Mitchell [1]. In co-training, two base learners are trained within the multi-view framework, i.e. two sets of independent attributes, each of which is sufficient for classification. One base learner iteratively labels several examples which it is most confident of from its point of view, and feeds them to the other learner. The co-training paradigm has already been successfully applied to many areas such as nature language processing [12].
 unlabeled data, that is, active learning . Different from semi-supervised learning choosing confident examples to label by itself, active learning actively chooses most problematic examples from the unlabeled set and asks a teacher for the labels. Two major techniques in active learning are uncertainty-base sampling [5] and committee-base sampling [14].
 different way, their merits can be combined through some specifically designs. McCallum and Nigam [6] combined semi-supervised EM with committee-based sampling in text classification. Muslea et al.[9] employed co-testing [8] to choose unlabeled examples to query, and used co-EM [10] to boost the accuracy of the hypotheses. Zhou et al. [16] combined co-training with co-testing in content-based image retrieval. Let L and U denote the labeled and unlabeled set drawn from the same distri-bution D ( X ,Y ), respectively, where X is a p -dimensional feature vector while Y is a class label. In standard self-training process, a learner keeps on choosing to label a small set of its most confident examples, say L ,from U and retraining it-self on L  X  L . Self-training requires neither estimation and maximization of some posterior probability nor a sufficient and conditional independent attributes, so it is much easier to use than semi-supervised EM and standard co-training. How-ever, due to the small size of L , the generalization ability of the initial hypothesis may be poor. Consequently, L may contain much noise because the learner may incorrectly assign labels to some unlabeled examples, and the generalization abil-ity of the final hypothesis will be hurt by the accumulation of such noise in each iteration of the training process. Therefore, it is obvious that if the mislabeled examples in L could be identified in the self-training process, especially in the early iterations, the learned hypothesis is expected to perform better. training set through identifying and eliminating the training examples wrongly generated in the human labeling process. Some useful data editing methods have been studied in [3][15]. In those works, another learner is used to improve the quality of the training set before the wanted learner are trained. In a recent work, Muhlenbach et al. [7] proposed a method based on a statistical method called cut edge weight statistic [17] to identify mislabeled examples in the training set. Here this data editing method is employed to identify the examples possibly mislabeled by the learner in the self-training process.
 pothesis form the labeled set. In each self-training iteration, the base learner detects unlabeled examples on which it makes most confident prediction and labels those examples according to the prediction. Then, for each possible label y j (where j ranges from 1 to the number of possible labels), k j examples are selected and added to L according to the prediction confidence of their labels, keeping the class distribution in L similar to that in L . For instance, if there are 4 positive and 16 negative examples in L , then L contains 1 positive and 4 negative examples. the learner X  X  potential training set L  X  L . Firstly, Setred constructs a neighbor-hood graph [17] that expresses certain local information from all the examples in
L  X  L . A neighborhood graph is a graph in p -dimensional feature space where a distance metric could be defined. Each example in the graph is a vertex and there exists an edge between two vertices a and b if the distance between a and b satisfies Eq. 1. An edge connecting two vertices that have different labels is called cut edge .
 in the graph. The neighborhood of an example is a set of examples it connected to with edges in graph. Intuitively, most examples possess the same label in a neighborhood. So if an example locates in a neighborhood with too many cut edges, this example should be considered problematic . Thus, cut edge plays an important role for identifying mislabeled examples. To explore the information of cut edges, Setred associates every ( x i ,  X  y i )in L with a local cut edge weight statistic J i defined in Eq. 2.
 where N i is the neighborhood of x i , w ij is the weight on the edge between x i and x j and I ij are i.i.d random variables according to the Bernouilli law of parameter P ( y = X  y i ) .
 other vertices in its neighborhood and the fact that they have the same label [17]. Similarly, a null hypothesis H 0 that can be tested with J i is defined as every examples in L  X  L is independently labeled according to the marginal distribution D ( Y ). H 0 specifies a case that the label y i is assigned to each example x i without considering any information from x i , i.e. for any example ( x i ,y i ), the probability of examples in its neighborhood possessing labels other than y i is expected to be no more than 1  X  P ( y = X  y i ) under H 0 . Hence, a good example will be incompatible with H 0 . To test H 0 with J i , the distribution of J i under H 0 is need. The distribution of J i can be approximated to a normal distribution with mean  X  i and variance  X  2 i estimated by Eq. 3 and Eq. 4, if the size of neighborhood is big and the weights are not too unbalanced, otherwise a simulation must be proceed [7]. ( x i ,  X  y i )in L locates in the left rejection region, then there are significantly less cut edges than expected under H 0 , hence it is a good example. In contrast, if the observation value locates in places other than the left rejection region, then lots of examples in the neighborhood disagree with its label, hence it could be regarded as a mislabeled example. The left rejection region is specified by a pre-set parameter  X  .
 discards those examples, keeping the good ones intact. Consequently, a filtered set L is obtained. Note that Setred does not try to relabel the identified mislabeled examples in order to avoid introducing new noise to the data set. Finally, Setred finishes the current iteration by relearning a hypothesis on L  X  L . Setred stops self-training process after the pre-set maximum times of iteration M is reached. The pseudo-code of Setred is shown in Table 1. rithm. Blum and Mitchell [1] suggested to choose examples from a smaller pool in stead of the whole unlabeled set. For convenience, we adopt this strategy di-rectly without verification. Furthermore, it is worth noticing that the Setred could be regarded as a type of active semi-supervised learning algorithm that ac-tively identify the bad examples from the self-labeled set. Absence of the teacher, Setred just discards the problematic data after identification instead of asking the teacher for labels as in the standard active learning scenario. In order to test the performance of Setred , ten UCI data sets are used. Infor-mation on these data sets are tabulated in Table 2.
 learned hypothesis, while the remaining 75% data are partitioned into labeled set and unlabeled set under the unlabel rate 90%, i.e. just 10% (of the 75%) data are used as labeled examples while the remaining 90% (of the 75%) data are used as unlabeled examples. Note that the class distributions in these splits are similar to that in the original data set.
 the learner that utilizes local information is expected to benefit a lot from this method. Therefore in the experiments, the Nearest Neighbor classifier is used as the base learner. Unlike those probabilistic model such as Naive Bayes, whose confidence for an example belonging to a certain class can be measured by the output probability in prediction, the Nearest Neighbor classifier has no explicitly measured confidence for an example. Here for a Nearest Neighbor classifier, the most confidently predicted unlabeled example with label y j is defined as the unlabeled example which is the nearest to labeled examples with label y j while far away from those with labels other than y j . The pre-set parameter  X  that specifies the left rejection region of the distribution of J i is fixed on 0.1, the same as that in [7]. The self-training process stops when either there are no unlabeled examples available or 40 iterations have been done.
 same labeled/unlabeled/test splits as those used for evaluating Setred . Same as
Setred , the maximum iteration is also 40. Moreover, two base lines, denoted by NN-L and NN-A respectively, are used for comparison. One is a Nearest Neighbor trained only from the labeled set L , and the other is the one that trained from L  X  U provided the true label of all the examples in U . Note that NN-L is the initial state of both Setred and Self-training before they utilize any information from the unlabeled examples. NN-A is the ideal state of Setred and Self-training since every examples chosen in self-training process are given the correct label and all the examples available in the unlabeled set are used. the four learners are trained and evaluated on the randomly partitioned la-beled/unlabeled/test splits. In Table 3, the first four columns are the average error rates of NN-A, NN-L, Setred and Self-training respectively over 50 runs on each data set. The last two columns denoted by  X  Setred -imprv. X  and  X  X elf-imprv. X  respectively show the performance improvements of Setred and Self-training over NN-L, which is computed by the error rate of learned hypothesis of Setred and Self-training over the error rate of the learned hypothesis of NN-L. the performance improvements are evident in 8 data sets, except that it goes worse on german and hepatitis . The two-tailed paired t -test under the significant level of 95% shows that all the improvement of performance are significant. Note that on 4 data sets Setred performs even better than NN-A which is able to access all the information of the unlabeled examples. In contrast, although the performance of the learned hypothesis of Self-training improves on 6 data sets, only on five the improvements are significant, including australian , breast-w , colic , heart-statlog and hepatitis . Furthermore, Table 3 also shows that Setred outperforms Self-training on 9 data sets, among which significance is evident in 6 data sets under a two-tailed pair-wise t -test with the significance level of 95%. This evidence supports our claim that Setred is robust to noise in the self-labled examples hence achieves better performance than Self-training. atitis , while the performance of Setred degrades. One possible explanation is that Setred suffers imbalance of the data set. In hepatitis data set, there are only 32 positive examples out of 155 examples in all, which is only 20.6% of the total. Recall the method we used for identifying mislabeled examples, one can only be regarded as a good example only if there exists a significantly large num-ber of examples having the same label in its neighborhood. Since the data set is unbalanced, a correctly labeled positive examples could be easily mis-identified as mislabeled examples and rejected to be added to the labeled set for further training, due to the lack of neighbors possessing the same label. The percentage of the negative examples in the labeled set increases as the self-training process goes on, hence less chance for a correctly labeled positive examples available for further training. The more the distribution of the training set is distorted, the easier for the learner to be misled. Consequently, the performance degrades. Similarly, the error rate of hypothesis learned via Setred climbs up to 0.349 from the initial error rate of 0.339 on german , in which the negative examples are only 30%. The imbalance of this data set might also account for the performance degradation of Setred .
 data set is investigated carefully. Fig. 1 gives plot of error rate versus number of iterations of the median performance on each data set respectively. Note that Setred and Self-training stop before the maximum number of iterations is reached on several data set such as heart-statlog and hepatitis , due to no more unlabeled examples available for further training. In most cases except for german and hepatitis , Setred outperforms Self-training, and the error rates of the learned hypothesis by Setred usually go lower than or converge to the error rates of NN-A. These are consistent with the average performance of the 50 runs on the experimental data sets.
 a few iterations and remains unchanged. By contrast, the curve of Self-training drops when many unlabeled examples have been self-labeled and used for further training. This supports the explanation above for the Setred  X  X  failure that Setred suffers the imbalance data. Once the correctly labeled positive examples are rejected by Setred , the misclassified positive examples in the test set, which are probably be correctly classified after more positive examples are learned, will remain being misclassified in the following iterations.
 beled examples. Setred is robust to the noise introduced in self-labeling process and its learned hypothesis outperforms that learned via standard self-training. In this paper, a novel self-training style algorithm named Setred , which incor-porates data editing technique to learn actively from the self-labeled examples, is proposed. In detail, Setred firstly learns from labeled examples and then it-eratively chooses to label a few unlabeled examples, on which the learner is most confident in prediction, and adds those self-labeled examples to its labeled set for further training. In each iteration, Setred does not completely accept all the self-labeled examples that might be highly noisy. Instead, it actively identifies the possibly mislabeled examples from those self-labeled examples by testing a predefined null hypothesis with the local cut edge weight statistic associated with each self-labeled example. If the test indicates a left rejection, the example is regarded as a good example, otherwise it is a possible mislabeled example which should be kept from adding to the learner X  X  training set. The experiment results on 10 UCI data sets show that Setred is able to benefit from the information provided by unlabeled examples, and it is robust to the noise introduced in the self-labeling process hence the generalization ability of its learned hypothesis is better than that learned via standard self-training, which is easily affected a lot by those noise. Since Setred is sensitive to imbalance data, exploring a way to solve this problem will be done in future. Since Setred uses a Nearest Neighbor as base learner, extending this idea to other base leaners will also be future work. style algorithm but shedding a light on a possible way to handle the noise intro-duced in the learning process by incorporating an active-learning-like technique to refine the self-labeled examples in semi-supervised learning scenario, hence obtaining better performance of the learned hypothesis. Different from others, the work is done when the teacher to assign labels to the problematic examples is absent. In the future work, theoretical verification of this method will be done, which might help to understand the functionality of this method. Moreover, extending this method to classic semi-supervised learning algorithms, such as co-training [1], or searching for more suitable active learning methods for those algorithms to improve their performance will also be the future work. This work was supported by the National Science Foundation of China under the Grant No. 60473046, the Jiangsu Science Foundation Key Project under the Grant No. BK2004001, the Foundation for the Author of National Excellent Doctoral Dissertation of China under the Grant No. 200343, and the Graduate Innovation Program of Jiangsu Province.

