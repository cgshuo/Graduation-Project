 Result merging is an important step in federated search to merge the documents returned from multiple source-specific ranked lists for a user query. Previous result merging meth-ods such as Semi-Supervised Learning (SSL) and Sample-Agglomerate Fitting Estimate (SAFE) use regression meth-ods to estimate global document scores from document ranks in individual ranked lists. SSL relies on overlapping docu-ments that exist in both individual ranked lists and a cen-tralized sample database. SAFE goes a step further by us-ing both overlapping documents with accurate rank infor-mation and documents with estimated rank information for regression. However, existing methods do not distinguish the accurate rank information from the estimated informa-tion. Furthermore, all documents are assigned equal weights in regression while intuitively, documents in the top should carry higher weights. This paper proposes a weighted curve fitting method for result merging in federated search. The new method explicitly models the importance of informa-tion from overlapping documents over non-overlapping ones. It also weights documents at di ff erent positions di ff erently. Empirically results on two datasets clearly demonstrate the advantage of the proposed algorithm.
 H.3 [ Information Storage and Retrieval ]: H3.3 Infor-mation Search and Retrieval; H.3.4 Systems and Software Algorithms, Design, Performance Federated Search, Result Merging, Curve Fitting
Text information behind individual search engines of dis-tributed information sources may not be easily crawled for building a centralized index because of data protection, copy-rights or security. Federated text search has been proposed to search the distributed information in these environments. There are three basic problems of federated search: resource representation, resource selection and result merging. This paper focuses on result merging for generating a single ranked list of documents in di ff erent sources for a user query.
Existing result merging algorithms can be categorized into two groups. The first group assumes some level of collabora-tion of distributed information sources and use some statis-tics provided by those sources for merging [1, 3]. In non-cooperative environment, a semi-supervised learning (SSL) [5] method is a more practical approach. Basically, after the first step of query-based sampling [1], each informa-tion source is represented by a set of sample documents. The set of all sample documents is called centralized sample database . SSL utilizes overlapping documents in both indi-vidual ranked lists and centralized sample database to build a regression. Once regression is done, SSL can convert the rank of any document returned from an individual source to that document X  X  centralized score. Those centralized scores are used as global scores to merge all documents.
The more recent Sample-Agglomerate Fitting Estimate (SAFE) method [4] relaxes the requirement of SSL for over-lapping documents. It estimates document ranks based on the uniform sampling assumption, and uses those estimated ranks for regression. SAFE algorithm however, does not distinguish the contribution of overlapping documents with accurate ranks (i.e., existing in the source X  X  returned list) and sample documents with estimated ranks for regression. Moreover, top ranked documents (in source-specific list) are probably more important for curve fitting because of the goal of high-precision, which is not considered in SAFE.
Based on the observation, we propose a novel result merg-ing method called Weighted Curve Fitting (WCF) that com-bines the features of SSL and SAFE for result merging. The new method distinguishes accurate and estimated rank in-formation. It also considers the importance of documents in di ff erent positions for regression. Section 2 proposes the algorithm and Section 3 presents the empirical results.
We describe the method of estimating document ranks in SAFE, which is now reused in WCF. Suppose that the sampling process is uniform, a centralized document of rank r in the individual sample source is estimated to have rank P in the corresponding remote source, where we define P = source c k and its sample  X  k ). P is then used for regression if the document is unseen in the source X  X  ranked list.
As mentioned before, SAFE does not di ff erentiate the contribution of overlapping documents and non-overlapping ones in making regression. It is better to put more weights on overlapping documents, since their ranks are more accu-rate than those estimated. Moreover, by assuming that top documents are more representative than the others, docu-ments on top of the returned ranked list should contribute to the regression more than documents at the bottom. For-mally, given a query and a ranked list of documents that match the query in the centralized database, let S j be the centralized relevance score of document d j , and let r j rank of d j in the source X  X  individual ranked list. Note that r could either be real or estimated, depending on whether d appears in the individual ranked list. Then both SSL and SAFE attempt to fit the curve S j = a  X  f ( r j )+ b , where f is any function, and a , b are the learning parameters. We fit the curve by minimizing the quadratic loss function: Now we add two more parameters for the new model: where  X  e is a function that gives larger value for the docu-ment with an estimated rank than a document with a true rank.  X  p is a function depending on the position of doc-ument d j in the centralized ranked list. In this paper, we choose a linear function for  X  p ,i.e.  X  p ( d j )= r j .Wealso choose a two-level function for  X  e : c is set to 16 in all the experiments, to indicate the penal-ized term for estimated ranks. We call this method Weighted Curve Fitting (WCF). Parameter estimation can be done by rewriting the objective function and solving the normal lin-ear regression. Let w j =  X  e ( d j )  X   X  p ( d j ), then
We conducted our experiments with two standard TREC datasets: trec123, which contains 100 collections of TREC CDs 1, 2 and 3 organized by publication sources; and trec4-kmeans, which contains 100 collections created from TREC4 by k-means clustering [1]. Each information source uses a di ff erent retrieval model among INQUERY [2], language modeling and vector space tf-idf. We sample 300 documents for each source, use CORI [1] to select the top 5 sources for each query and INQUERY for querying the centralized sample database. As for the function f , SAFE uses linear, log, square, power and hybrid functions. We follow the same approach, but only report and compare results of the hybrid choice, which was shown to obtain best results in [4].
Table 1 shows the precision results when top 20 docu-ments returned from each source are merged. We also con-duct experiments with top 50 and 100 documents selected from each source. The full result is shown in Figure 1. Table 1: Document Precision on TREC123, TREC4 with Top 20 Documents of each Source P@n @5 @10 @15 @20 @5 @10 @15 @20 SAFE .32 .29 .27 .25 .27 .22 .19 .17
WCF .34 .31 .29 .27 .31 .26 .24 .24 Figure 1: Document Precision with Top 20, 50, 100 Documents of each Source In both datasets, the curves of the same color denote the performance of SAFE and WCF of the same setting. In general, WCF consistently outperforms SAFE. For trec4-kmeans, the precision is increasing when more documents from each source are merged. However, this is not the case with trec123. It may be argued that the algorithm has to deal with more noisy data when merging more documents, so a possible solution is to assign even smaller weight to doc-uments along the tail of the returned list. The exploration of using other functions for  X  p is reserved for the future work. In general, WCF works well without too much overhead.
This paper proposes a result merging algorithm for fed-erated search. Similar to SAFE, the new method does not rely on overlapping documents. More importantly, the new method can accurately estimate global document scores for result merging by distinguishing accurate rank information and estimated rank information in regression. It also meets the high-precision criterion by putting more weights for doc-uments in the top part of the ranked lists for curve-fitting. Empirical results on two datasets have shown the e ff ective-ness of the proposed results merging algorithm.
