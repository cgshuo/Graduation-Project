 After a decade of TREC evaluations based on binary relevance assessments, the importance of Information Retrieval (IR) evaluation based on multigrade rele-vance assessments is receiving more attention than ever [1, 2, 3, 4]. NTCIR has used multigrade relevance from the very beginning, but has not fully utilised the richness of the relevance data: Following the TREC evaluation method-ology, noninterpolated Average Precision (AveP) is used as the official met-ric for ranking the systems submitted to NTCIR. Since AveP cannot handle multigrade relevance, two AveP values are reported for each system using the trec eval program: Relaxed AveP, which treats S-relevant (highly relevant), A-relevant (relevant), B-relevant (partially relevant) documents as just  X  X elevant X , and Rigid AveP, which ignores the B-relevant ones. Thus, Relaxed AveP wastes the relevance levels, while Rigid AveP wastes the partially relevant documents in addition. This situation is clearly undesirable.
 measure were proposed for evaluation based on multigrade relevance [6]. This paper shows that Q-measure inherits both the reliability of AveP and the multi-grade relevance capability of Average Weighted Precision (AWP) [4, 6] through a theoretical analysis, and then verify the above claim through experiments by actually ranking the systems submitted to the NTCIR-3 CLIR Task. Our exper-iments confirm that the Q-measure ranking is very highly correlated with the Average Precision ranking and that it is more reliable than Average Weighted Precision.
 measure and R-measure, and discusses how they are theoretically related to existing retrieval effectiveness metrics. Section 3 reports on our experiments using the systems submitted to the NTCIR-3 CLIR task for demonstrating the practicality and the reliability of Q-measure. Section 4 discusses previous work, and Section 5 concludes this paper. 2.1 Average Precision Let R denote the total number of known relevant documents for a particular search request (or a topic ), and let cou n t ( r ) denote the number of relevant doc-uments within the top r documents of the ranked output. Clearly, the Precision ifthedocumentatRank r is relevant and isre l ( r ) = 0 otherwise. Then, Average Precision (AveP) is defined as: where L is the ranked output size.
 2.2 Average Weighted Precision We now look at Average Weighted Precision (AWP) proposed by Kando et al. [4], which was intended for evaluation based on multigrade relevance.
 successfully retrieving an X -relevant document. For the NTCIR CLIR test col-lections, X  X  X  S, A, B } [4], and a typical gain value assignment would be gai n ( S )=3, gai n ( A )=2and gai n ( B ) = 1. Let X ( r ) denote the relevance level of the document at Rank r (  X  L ). Then, the gain at Rank r is given by g ( r )= gai n ( X ( r )) if the document at Rank r is relevant, and g ( r )=0ifitis nonrelevant. The cumulative gain at Rank r is given by cg ( r )= g ( r )+ cg ( r  X  1) for r&gt; 1and cg (1) = g (1).
 put. (An ideal ranked output for NTCIR can be obtained by listing up all S-relevant documents, then all A-relevant documents, then all B-relevant docu-ments.) Then, AWP is given by: an equivalent concept already existed in the 1960s, when Pollack proposed the sliding ratio measure.
 a gain value of 1, then, by definition, both and hold for r  X  R . Thus, with binary relevance, holds for r  X  R . Therefore, from Equations (1) and (3), if the relevance assess-ments are binary and if the system output does not have any relevant documents below Rank R ,then holds. Similarly, from Equations (2) and (4), with binary relevance, serious problem. Since there are no more than R relevant documents, holds for r&gt; R . That is, after Rank R , cig ( r ) becomes a constant ,which implies, from Equation (3), that AWP cannot distinguish between System A that has a relevant document at Rank R and System B that has a relevant document at Rank L (i.e. at the very bottom of the ranked list). For exam-ple, suppose that R = R ( B )=5foratopic,where R ( X ) represents the number of X -relevant documents. Given that gai n ( B ) = 1, the sequence of System B retrieved only one relevant document, but that System A has it at Rank 5 and that System B has it at Rank 1000. Then, for System A, the se-0 . 04. For System B, the sequence of cg ( r )is(0 , 0 , 0 , 0 , 0 ,..., 1 )and AW P = ( cg (1000) / cig (1000)) / 5=( 1 / 5) / 5=0 . 04. Thus the two systems would be con-sidered as identical in performance.  X  X reezes X  after Rank r . In contrast, AveP is free from this problem because its denominator r is guaranteed to increase steadily. R-Precision and R-WP are also free from the problem because they only look at the top R documents. 2.3 Q-Measure Q-measure, proposed by Sakai [6] at NTCIR-4, is designed to solve the afore-mentioned problem of AWP.
 bg ( r )= g ( r )+1 if g ( r ) &gt; 0and bg ( r )=0if g ( r )=0.Then,the cumulative bonusedgainatRank r is given by cbg ( r )= bg ( r )+ cbg ( r  X  1) for r&gt; 1and cbg (1) = bg (1). That is, the system receives an extra reward for each retrieved relevant document. Q-measure is defined as: not to  X  X reeze X , so that relevant documents found below Rank R can be handled properly. For the example given in Section 2.2, for System A, the sequence of Q -measure =( cbg (1000) / ( cig (1000) + 1000)) / 5=( 2 / (5 + 1000)) / 5=0 . 0004. contrast, both R-measure and R-WP are equal to one iff all the top R documents are (at least partially) relevant: thus, for example, B-relevant documents may be ranked above the A-relevant ones. In this respect, Q-measure is superior to R-measure [6].
 holds for r  X  1. Therefore, Q-measure and R-measure can alternatively be ex-pressed as: with Equations (2) and (4), it can be observed that Q-measure and R-measure are  X  X lended X  metrics: Q-measure inherits the properties of both AWP and AveP, and R-measure inherits the properties of both R-WP and R-Precision. More-over, it is clear that using large gain values would emphasise the AWP aspect of Q-measure, while using small gain values would emphasise its AveP aspect. Sim-ilarly, using large gain values would emphasize the R-WP aspect of R-measure, while using small gain values would emphasise its R-Precision aspect. For ex-ample, letting gai n ( S ) = 30, gai n ( A ) = 20, and gai n ( B ) = 10 (or conversely gai n ( S )=0 . 3, gai n ( A )=0 . 2, and gai n ( B )=0 . 1) instead of gai n ( S )=3, gai n ( A ) = 2, and gai n ( B ) = 1 is equivalent to using the following generalised equations and letting  X  = 10 (or conversely  X  =0 . 1): holds for r  X  R . Therefore, From Equations (1) and (13), if the relevance assess-ments are binary and if the system output does not have any relevant documents below Rank R , then Equation (8) can be generalised as: Similarly, from Equations (2) and(14), with binary relevance, Equation (9) can be generalised as: 3.1 Data The following files, provided by National Institute of Informatics, Japan, were used for the analyses reported in this paper.  X  ntc3clir-allCruns.20040511.zip  X  ntc3clir-allJruns.20040511.zip  X  ntc3clir-allEruns.20040511.zip  X  ntc3clir-allKruns.20040511.zip The above files contain runs submitted by 14 different participants, and include both monolingual and cross-language runs, as well as runs using different topic fields, e.g. TITLE, DESCRIPTION etc. 3.2 Q-Measure Versus Other Metrics Tables 1-3 show the Spearman and Kendall Rank Correlations for Q-measure and its related metrics based on the NTCIR-4 CLIR C-runs, J-runs and E-runs, respectively 1 . The correlation coefficients are equal to 1 when two rankings are identical, and are equal to  X  1 when two rankings are completely reversed. It is known that the Spearman X  X  coefficient is usually higher than the Kendall X  X . Values higher than 0.99 (i.e. extremely high correlations) are indicated in ital-ics.  X  X elaxed X  represents Relaxed AveP,  X  X igid X  represents Rigid AveP.  X  X -measure X  and  X  X WP X  use the default gain values: gai n ( S )=3, gai n ( A )=2 and gai n ( B ) = 1. Moreover, the columns in Part (b) of the table represent Q-measure with different gain values: For example,  X  X 30:20:10 X  means Q-measure using gai n ( S ) = 30, gai n ( A ) = 20 and gai n ( B ) = 10. Thus,  X  X 1:1:1 X  implies binary relevance, and  X  X 10:5:1 X  implies stronger emphasis on highly relevant documents. Table 4 condenses the four tables (C, J, E, K) into one by taking averages over the four sets of data.
 in decreasing order of Relaxed AveP and then renaming each system as System No. 1, System No. 2, and so on 2 . Thus, the Relaxed AveP curves are guaranteed to decrease monotonically, and the other curves (representing system rankings based on other metrics) would also decrease monotonically only if their rank-ings agree perfectly with that of Relaxed AveP. That is, an increase in a curve represents a swop .
 1. While it is theoretically clear that AWP is unreliable when relevant docu-2. Compared to AWP, the Q-measure curves are clearly more stable. Moreover, 3. From Part (a) of each table, it can be observed that Q-measure is more 4. It can be observed that the behaviour of Q-measure is relatively stable with 5. From Part (b) of each table, it can observed that  X  X 1:1:1 X  (i.e. Q-measure 3.3 R-Measure Versus Other Metrics Tables 5-7 show the Spearman and Kendall Rank Correlations for R-measure and its related metrics based on the NTCIR-4 CLIR C-runs, J-runs and E-runs, respectively 4 . Table 8 condenses the four tables into one by taking averages over the four sets of data. Again,  X  X -measure X ,  X  X -measure X  and  X  X -WP X  use the default gain values,  X  X 30:20:10 X  represents R-measure using gai n ( S ) = 30, gai n ( A ) = 20 and gai n ( B ) = 10, and so on. As  X  X 1:1:1 X  (R-measure with binary relevance) is identical to R-Precision (and R-WP), it is not included in the tables (Recall Equation (19)).
 1. From Part (a) of each table, it can be observed that R-measure, R-WP and R-2. From the tables, it can be observed that R-measure is relatively stable with The original (Discounted) Cumulative Gain ((D)CG) proposed by J  X  arvelin and well. Thus, they later proposed normalisation as well as averaging across a range of document ranks for (D)CG [3]. Work is underway for comparing the above metrics with Q-measure and R-measure from the viewpoint of reliability and stability. The disadvantages of Average Distance Measure (ADM) [1] compared to Q-measure and Average Precision have been discussed in [6]. This paper showed that Q-measure inherits both the reliability of noninterpo-lated AveP and the multigrade relevance capability of AWP through a theoreti-cal analysis, and then verified the above claim through experiments by actually ranking the systems submitted to the NTCIR-3 CLIR Task. Our experiments confirm that the Q-measure ranking is very highly correlated with the AveP ranking and that it is more reliable than AWP.

