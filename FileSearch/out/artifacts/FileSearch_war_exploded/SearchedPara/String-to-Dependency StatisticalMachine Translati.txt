 Raytheon BBN Technologies Raytheon BBN Technologies Raytheon BBN Technologies 1. Introduction n -gram Language Models (LMs )have been widely used in current Statistical Machine
Translation (SMT )systems. Because they treat a sentence as a flat string of tokens, a drawback of traditional n -gram LMs is that they cannot model long range word rela-tions, such as predicate X  X rgument attachments, that are critical to translation quality. a dependency LM while decoding (as opposed to during reranking n -best output )to score alternative translations based on their structural soundness. In order to generate the structured output (dependency trees )required for dependency LM scoring, transla-tion rules in our system represent the target side as dependency structures. We restrict the target side of the rules to well-formed dependency structures to weed out bad translation rules and enable efficient decoding through dynamic programming. Due to the flexibility of well-formed dependency structures, such structures can cover a large set of non-constituent transfer rules (Marcu et al. 2006 )that have been shown useful for MT.
 2005), a state-of-the-art hierarchical string-to-string model. Our experiments show that the string-to-dependency decoder significantly improves MT performance. Overall, the improvement in BLEU score is around 2 BLEU points on NIST Arabic-to-English and Chinese-to-English newswire test sets.
 work. Section 3 provides an overview of our string-to-dependency translation system.
Section 4 provides a complete description of our system, including formal definitions of well-formed dependency structures and their operations, as well as proofs about their key properties. Section 5 describes the implementation details, which include rule extraction, decoding, using dependency LM scores, and using labels in translation rules. We discuss experimental results in Section 6, compare our work with related work in Section 7, and draw conclusions in Section 8. 2. Previous Approaches to SMT
Phrase-based systems (Koehn, Och, and Marcu 2003; Och 2003 )had dominated SMT until recently. Such systems typically treat the input as a sequence of phrases (word n -grams), reorder them, and produce a translation for the reordered sentence based on translation options of each source phrase. A prominent feature of such systems is the use of an n -gram LM to measure the quality of translation hypotheses. A drawback of to score translation hypotheses based on their structural soundness.
 rules in Hiero contain non-terminals (NTs), as well as words, which allow the input to be translated in a hierarchical manner. Because both the source and target sides of measure translation quality based on structural relations such as predicate X  X rgument agreement.
 a source parse tree into a target string. This method depends on the quality of source side parsing, and ignores target information during source side analysis. Mi, Huang, and Liu (2008 )later proposed a translation model that takes the source parse forest as MT input to reduce translation errors due to imperfect source side analysis. string inputs in order to exploit the syntactic structure of the target language. Galley et al. (2006 )formalized this approach with tree transducers (Graehl and Knight 2004 ) by using context-free parse trees to represent the target side. However, it was later shown by Marcu et al. (2006 )and Wang, Knight, and Marcu (2007 )that coverage could be a big issue for the constituent based rules, even though the translation rule set was already very large.

Tree Adjoining Grammar (TAG )(Joshi and Schabes 1997; Shen, Champollion, and Joshi 2008). In this model, a translation rule is composed of a source string and a target elementary tree. Target hypothesis trees are combined with the adjoining operation, and there are no NT slots for substitution as in LTAG-spinal parsing (Shen and Joshi 2005, 2008; Carreras, Collins, and Koo 2008). Without the constraint of NT slots, the adjoining operation allows very flexible composition, so that the search space becomes much larger. One has to carefully prune the search space.
 implementation, a TAG grammar was transformed to an equivalent Tree Insertion 650
Grammar (TIG). In this way, they do not have an explicit adjoining operation in their system, and as such reduce the search space in decoding. Sub-trees are combined with NT substitution.
 take advantage of structural knowledge on both sides X  X or example, as in the papers
Cherry (2005). Although tree-to-tree models can represent rich structural information of the input and the output, they have not significantly improved MT performance, possibly due to a much larger grammar and search space. On the other hand, Smith and
Eisner (2006 )showed the necessity of allowing loose transformations between the trees, which made tree-to-tree models even more complicated. 3. Overview o fString-to-Dependency Translation
Our system is designed to address problems with existing SMT approaches. It is novel in two respects. First, it uses a dependency LM to model long-distance relations. Sec-ond, it uses well-formed dependency structures to represent translation hypotheses to achieve an effective trade-off between model coverage and decoding complexity. 3.1 Dependency-Based Translation and Language Models
Our system generates target dependency trees as output and exploits a dependency LM in scoring translation hypotheses. As described before, the goal of using a dependency
LM is to exploit long-distance word dependencies and as such model the quality of the output more accurately.
 parent. In this example, the word find is the root.
 n -gram LM to score translation hypotheses: where w 1 , w 2 ,and w 3 are feature weights. S f is the input and S is the probability of the target string given the source, and P ( S the source given the target. P ( S e )is the prior probability of the target string S n -gram LM.
 where P ( D )is the dependency LM score of target dependency tree D . We will show how to compute P ( D )in Section 5.4.
 where n = 3, F 1 = log P ( D | S f ), F 2 = log P ( S f | both a dependency LM and a traditional n -gram LM (also known as a string LM), as our decoder. 3.2 Well-Formed Dependency Structures
A central question in our system design is: What kinds of dependency structures are allowed in translation rules? One extreme would be to allow any arbitrary multiple level treelets, as in Ding and Palmer (2005 )and Quirk, Menezes, and Cherry (2005 ).
One can define translation rules on any fragment of a parse/dependency tree. It offers maximum coverage of translation patterns, but suffers from data sparseness and a large search space.
 a more robust model and a small search space, but excludes many useful transfer rules. structures (see Section 4 for formal definitions )for a trade-off between rule coverage, model robustness, and decoding complexity. In short, a well-formed dependency struc-sequence of siblings, each being a complete sub-tree.
 non-constituent rules in addition to rules that are complete constituents. For example, the following translation is obviously useful for Chinese-to-English MT, but cannot be represented in some tree-based translation systems since the red is a partial constituent. However, it is a valid dependency structure in our system. 4. Formalism
We first formally define the well-formed dependency structures, which are used to represent target hypotheses. Then, we define the operations to build well-formed de-pendency structures from the bottom up in decoding. 652 4.1 Well-Formed Dependency Structures and Categories In order to exclude undesirable structures and reduce the search space, we only allow
S e whose dependency structure D is well formed, which we will define subsequently. The well-formedness requirement will be applied to partial decoding results. dependency structures, fixed and floating. Fixed structures consist of a sub-root with children, each of which must be a complete constituent. We call them fixed dependency structures because the head is known or fixed. Floating structures consist of a number of consecutive sibling nodes of a common head, but the head itself is unspecified. Each of the siblings must be a complete constituent. Floating structures can represent many of a noun. Only those two kinds of dependency structures are well-formed structures in our system.
 structures and combinatory operations over them, so that we can easily manipulate them in decoding. Examples will be provided along with the formal definitions to aid understanding.
 each word. For example, d 4 = 2 means that w 4 depends on w d = 0.
 Definition 1 or fixed for short, if and only if it meets the following conditions 1. d h /  X  [ i , j ] 2.  X  k  X  [ i , j ]and k = h , d k  X  [ i , j ] 3.  X  k /  X  [ i , j ], d k = h or d k /  X  [ i , j ]
We say the category of d i .. j is (  X  , h ,  X  ), where  X  Definition 2
A dependency structure d i ... d j is floating with children C , for a non-empty set C { i , ... , j } ,or floating for short, if and only if it meets the following conditions 1.  X  h /  X  [ i , j ], s . t .  X  k  X  C , d k = h 2.  X  k  X  [ i , j ]and k /  X  C , d k  X  [ i , j ] 3.  X  k /  X  [ i , j ], d k /  X  [ i , j ] side of the head, or (  X  ,  X  , C )otherwise.
 head, and fields A and B represent left and right dependents of the head, respectively.
A dependency structure is well-formed if and only if it is either fixed or floating. Examples
We represent dependency structures with graphs. Figure 2 shows examples of fixed structures, Figure 3 shows examples of floating structures, and Figure 4 shows ill-formed dependency structures.
 a continuous segment due to the missing it .
 structure.
 meta category operations later. However, for the sake of convenience, we would like to assign a single category to each well-formed structure.
 654 Definition 3
Let structure T be well formed. Category cat of T is defined as follows cat( T ) = cat is well-defined according to Definitions 1 and 2. 4.2 Operations
One of the purposes of introducing floating dependency structures is that siblings hav-ing a common parent will become a well-defined entity, although they are not con-sidered a constituent. We always build well-formed partial structures on the target side in decoding. Furthermore, we combine partial dependency structures in a way such that we can obtain all possible well-formed dependency structures (but no ill-formed ones )during bottom X  X p decoding.
 dency structure has a category. We can apply four combinatory operations over the categories. If we can combine two categories with a certain category operation, we can use a corresponding tree operation to combine two dependency structures. The cate-gory of the combined dependency structure is the result of the combinatory category operations.
 Operations on Well-Formed Dependency Structures
There are four types of operations on well-formed dependency structures. Instead of providing formal definitions, we use figures to illustrate these operations to make them easy to understand. Figure 1 shows a traditional dependency tree. Figure 5 shows the four operations for combining partial dependency structures, which are left adjoining (LA), right adjoining (RA), left concatenation (LC), and right concatenation (RC). We always combine two well-formed structures in one of the four ways, and obtain a larger well-formed structure.
 dependency formalism. We can adjoin either a fixed structure or a floating structure to the head of a fixed structure.
 tures in the same direction.
 translation fragments encoded in transfer rules. Figure 6 shows alternative ways of applying operations on well-formed structures to build larger structures in a bottom X  up style. Numbers represent the order of operation. The fact that the same dependency structure can have multiple derivations means that we can utilize various rules learned from different training samples. Such flexibility is important for MT. Meta Operations on Categories
We first introduce three meta category operations, which will later be used to define category operations. Two of the meta operations are unary operations, left raising (LR) and right raising (RR), and one is the binary operation unification (UF). Definition 4 Meta Category Operations floating structure, according to Theorem 1.
 Theorem 1 children { h } if there are no outside words depending on word h , which means that Proof and 2 immediately follow from conditions 1 and 2 of the fixed structure, respectively.
Condition 3 is met according to Equation (4 )and condition 3 of the fixed structure. 656 Therefore, we can always raise a fixed structure if we assume it is complete, that is, Equation (4 )holds.
 unify two fixed structures. Operations on Categories
Now we define category operations. For the sake of convenience, we use the same names for category operations and dependency structure operations. We can easily use the meta category operations to define the four combinatory category operations. The definition of the operations is as follows.
 Definition 5 Combinatory category operations erations, one can verify the one-to-one correspondence. This correspondence can be formally stated in the following theorem.
 Theorem 2 defined. We have Proof
The proof of the theorem is rather routine, so we just give a sketch here. One can show it by induction on the number of nodes in dependency structures. It suffices to show that Equation (5 )holds for all the operations. Actually, the category operations are designed to meet this requirement; the three fields of a category represent the head and the children on both sides.
 and constrain operations in decoding.
 Soundness and Completeness
Now we show the soundness and completeness of the operations on dependency structures and only the well-formed structures.
 Theorem 3 (Soundness)
Let X and Y be two well-defined dependency structures, and OP an operation over X and Y . It can be shown that OP( X , Y )is also a well-defined dependency structure. Proof Theorem 3 immediately follows Theorem 2.
 Theorem 4 (Completeness) shown that there exist well-formed structures X , Y and an operation OP, such that Z = OP( X , Y ).
 Proof
If Z is fixed on h , without losing generality, we assume g is the leftmost child (or
X and Y which are rooted on g and h respectively. It can be verified that X and Y are well-formed, and Z = LA( X , Y ).
 floating structures with children { c 1 } and { c 2 , ... , c that they are the sub-structures X and Y that we are looking for. 658 5. Implementation 5.1 Translation Rules
Translation rules are central to an MT system. In our system, each rule translates a source sub-string into a target dependency structure. The target side of the translation rules constitutes a tree grammar.
 formed structures can be combined into a larger one with adjoining or concatenation, and there is no non-terminal slot for substitution. This is similar to tree grammars without substitution, such as the original TAG (Joshi, Levy, and Takahashi 1975 )and
LTAG-spinal (Shen, Champollion, and Joshi 2008). A corresponding MT model was proposed in Carreras and Collins (2009). Search space is a major problem for such an approach, as we described earlier.

NT slots for substitution come from what we have observed in training data. Combina-tion of well-formed dependency structures can only happen on NT slots. By replacing
NT slots with well-formed structures, we implicitly adjoin or concatenate sub-structures based on the dependency information stored in rules. We extract the translation rules from the training data containing word-to-word alignment and target parse trees, which we will explain in the next section. A similar strategy was employed by DeNeefe and Knight (2009). They turned a TAG into an equivalent TIG.
 nate two neighboring hypotheses. Each of the special rules has two NT slots, but they vary on target dependency structures. They are comparable to the glue rules in Chiang (2005).
 type . 1 T f is a set of terminals (words )in the source language, and T in the target language.

S  X  ( T dependency structure for S e ,and A is the alignment between S alignments in A must be one-to-one. We ignore the left hand side for both source and target, since there is only one NT type. 5.2 Rule Extraction Now we explain how we extract string-to-dependency rules from parallel training data.
The procedure is similar to Chiang (2007 )except that we maintain tree structures on the target side, instead of strings.

Ney 2003 )to generate word level alignment. We use a statistical CFG parser to parse the English side of the training data, and extract dependency trees with Magerman X  X  rules (1995). Then we use heuristic rules to extract transfer rules recursively based on word alignments and the target dependency trees. The rule extraction procedure is as follows. 1. Initialization: 2. Inference: aligned NT slots if there are enough words and alignments. In order to make the size of the grammar manageable, we keep only rules with at most two NT slots and at most seven source elements.
 probability given by GIZA. The two conditional probabilities are simply estimated by counting in all the extracted rules. 660 5.3 Decoding
Following previous work on hierarchical MT (Chiang 2005; Galley et al. 2006), we solve the decoding problem with chart parsing. We view the target dependency trees as hidden structures in the input. The task of decoding is then to find the best hidden structure for the input given the transfer grammar and the language models (a string n -gram LM and a dependency LM).
 pendency structure by substituting component dependency structures for correspond-ing NTs in the target dependency structure of rules.
 forest, or AND X  X R structures. An AND-structure represents an application of a rule over component OR-structures, and an OR-structure represents a set of alternative
AND-structures with the same state. A state keeps the necessary information about hypotheses under it, which is needed for computing scores for higher level hypotheses for dynamic programming. For example, with an n -gram string LM in decoding, a state keeps the leftmost n  X  1 words and the rightmost n  X  1 words shared by hypotheses in that state. Because of the use of a dependency LM in decoding, the state information also includes boundary information about dependency structures for the purpose of computing dependency LM scores for larger structures.
 dependency language model during decoding. 5.4 Using Dependency LM Scores For the dependency tree in Figure 1, we calculate the probability of the tree as follows Here P T ( x )is the probability that word x is the root of a dependency tree. P are left and right side generative probabilities respectively. Let w w use a tri-gram dependency LM, represents the event that w L i is a sibling word. The computation of STOP probabilities greatly complicates the implementation of inside dependency LM probabilities, so we ignored it in practice. Right side probability P R is defined in a similar way. be used by changing the independence assumptions in the above formulas. The choice of using a tri-gram model in our experiments is a trade-off between model robustness and sharpness given the training data available.
 short, on the fly for partial hypotheses in a bottom X  X p decoding, we need to save more information in categories and states.
 structure. h represents the head. Relative to the head, LF is the farthest children on theleftsideand RF the farthest children on the right side. Similarly, LN is the nearest children on the left side and RN the nearest children on the right. The three types of categories are as follows.
Furthermore, operations similar to those described in Section 4.2 are used to keep track of the head and boundary child nodes, which are then used to compute depLM scores in decoding. 5.5 Using Labels in Transfer Rules
In the formalism introduced in the previous section, there is only a single non-terminal a rule whose target dependency structure is X 1  X  says  X  X on says . In the training data, X 1 comes from a tree rooted on a noun and X in either of these two slots in the decoding phase.
 a label. When we replace an NT with a sub-structure, we check if the label of the sub-structure is the same as the NT label. If they do not match, we assign a penalty to this replacement.
 the previous example, the target structure would generate X where NN means noun (singular or mass )and VBP means verb (non-3rd person sin-gular present), and the whole target structure has a label of VBZ , which means verb (3rd person singular present). If we replace NN with a sub-tree rooted at, for example, a preposition, there will be a penalty.

We always use the generic label for floating structures. Any NT substitution with this any floating structure during decoding.
 scribed in the previous section. Instead, we modify the representation of translation type, the whole structure has a label which is the POS tag of the head word. Otherwise, the label is X . Similarly, each NT slot has a label which is defined in the same way, 662 based on the dependency structure from which the rule is extracted. In decoding, each state has an extra field representing the label for the dependency structure of the hypothesis. 5.6 Other Details
We have nine features in our system. 1. Log probability of the source side given the target side of a rule 2. Log probability of the target side given the source side of a rule 3. Log probability of word alignment 4. Number of target words 5. Number of special rules (see Section 5.1 )used 6. Log probability of string LM 7. Log probability of dependency LM 8. Discount on ill-formed dependency structures 9. Discount on unmatched labels tion. The fifth feature counts the number of times the adjoining and concatenation rules are used in a translation. The string LM score and dependency LM score are the next two features.
 derivation, but they are penalized. For this purpose, we introduce the null dependency structure e . For any operation OP and dependency structure X , Y , we have
Because part of a hypothesis may have a null dependency structure, we cannot calculate dependency LM scores on some of the related words. Therefore, we give a discount for each of these words. This is the eighth feature.
 that we used may fail to generate parse trees for short segments X  X or example, dictio-structures. We limited phrasal rules to at most three lexical items for each side. ficient statistics for feature calculation. For example, each state should memorize the leftmost two words and rightmost two words for LM score calculation. Similar exten-sions are required for dependency LM score and NT labels. Therefore, we use beam search with cube pruning as in Chiang (2005 )for speedup. Like chart parsing, the computational complexity of decoding time is O ( n 3  X  B  X | applicable to a span with translation grammar G . This number agrees with the empirical results.

Och (2003), the k -best results are accumulated as the input to the optimizer. Powell X  X  method is used for optimization with 20 random starting points around the weight vector of the last iteration. For improved results, we rescore 1,000-best translations, generated using the technique described by Huang and Chiang (2005), by replacing tri-gram string LM scores in the output with 5-gram string LM scores. The algorithm to tune the rescoring weights is similar to the one to tune the decoder weights. 6. Experiments
We experimented with four models: dependency model. They use similar rule extraction and decoding algorithms. The structures instead of strings; thus, the comparison will show the contribution of using dependency information in decoding.
 on BLEU, TER (Snover et al. 2006), and METEOR (Banerjee and Lavie 2005). It is well uncommon for a technique to improve the metric that is used for tuning but hurt other metrics. The use of multiple metrics helps us avoid drawing false conclusions based on metric-specific improvements. For both Arabic-to-English and Chinese-to-English MT, we tuned on NIST MT02-05 and tested on MT06 and MT08 newswire sets.
 bi-lingual sentences from ten corpora: LDC2004T17, LDC2004T18, LDC2005E46, LDC-2006E25, LDC2006G05, LDC2005E85, LDC2006E36, LDC2006E82, LDC2006E95, and SSUSAC27 (Sakhr Arabic-English Parallel Corpus). The training data for Chinese-to-English MT contains around 1.0 million pairs of bi-lingual sentences from eight corpora: LDC2002E18, LDC2005T06, LDC2005T10, LDC2006E26, LDC2006G05, LDC2002L27, LDC2005T34, and LDC2003E07.
 pose, we parsed the English side of the parallel data. Two separate models were trained: one for Arabic from the Arabic training data and the other for Chinese from the Chinese training data. Traditional tri-gram and 5-gram string LMs were trained on 664 the English side of the parallel data as well as the English Gigaword corpus V3.0 in a way described by Bulyko et al. (2007).
 the tuning and test sets. The constraint of well-formed dependency structures greatly rating dependency structures and labels in rules, the size of string-to-dependency rule set is about 10% to 20% of the baseline.
 Arabic-to-English MT. Tables 4 and 5 show the scores for Chinese-to-English MT. decoding output because it is the metric on which all systems were tuned. We measured the significance of BLEU, TER, and METEOR with paired bootstrap resampling as proposed by Koehn (2004). In Tables 2 through 5, ( +/-)represent being better/worse difference from the baseline.

MT06 and 1.2 on MT08 before 5-gram rescoring. For Chinese-to-English MT, the im-provements in BLEU were 1.0 on MT06 and 1.4 on MT08. After rescoring, the improve-ments became smaller, ranging from 0.8 to 1.3. All the BLEU improvements on 5-gram scores are statistically significant.
 0.7 points on average. The overall BLEU improvement on lower-cased decoding output is 1.8 points on MT06 and 2.1 points on MT08 for Arabic-to-English translation, and 2.0 points on MT06 and 1.6 points on MT08 for Chinese-to-English translation. nificantly for Arabic-to-English but marginally on Chinese-to-English tasks. The results on METEOR and TER suggested that the new model did improve translation accuracy. to-dependency rules. It shows the performance of using dependency structure for rule was significantly worse, which means that many useful rules were lost due to the structural constraints. On Chinese-to-English, the tri-gram scores of the filtered model were a little bit worse. However, after 5-gram rescoring, the BLEU scores became higher than the baseline, and METEOR scores were even significantly better. We suspect that the different performance that we observed is due to the difference in source languages and their tokenization methods.
 666 constraints for rule filtering, although it greatly reduced the rule size and allowed the use of more useful training data potentially. The use of structural constraints is compulsory for the introduction of dependency LMs and non-terminal labels, which compensated for the loss of rule filtering, and led to significant overall improvement. 7. Comparison to Related Work
Fox (2002), Ding and Palmer (2005), and Quirk, Menezes, and Cherry (2005) showed that, for the purpose of representing word relations, dependency structures are ad-vantageous over CFG structures because they do not require complete constituents. A number of techniques have been proposed to improve rule coverage. Marcu et al. (2006) interest. The binarization method used by Wang, Knight, and Marcu (2007 )can cover many non-constituent rules also, but not all of them. DeNeefe et al. (2007 )showed that the best results were obtained by combining these methods.
 translation model which employed a syntax-based language model to select the best work is that they only used the tree-based LM in rescoring, possibly due to the com-plexity of the syntax-based LM. In contrast, our system uses a dependency LM directly in decoding and as such can prune out unpromising hypotheses as soon as possible.
ASR (Chelba and Jelinek 2000; Xu, Chelba, and Jelinek 2002), with the same motivation of exploiting long-distance relations. A difference is that the dependency LM is used bottom X  X p in our MT system, whereas the structured LM is used left-to-right in ASR.
Another difference is that long-distance relations are more important in MT due to word re-orderings.
 tures in previous work on monolingual parsing (Eisner and Satta 1999; McDonald,
Crammer, and Pereira 2005), which allowed floating structures as well defined states in derivation, too. However, as for monolingual parsing, one usually wants exactly one derivation for each parse tree, so as to avoid spurious ambiguity of derivations for the same parse. The derivation model proposed by Eisner and Satta (1999 )satisfied was O ( n 4 )in many other derivation models. In our MT model, the motivation is to exploit various translation fragments learned from the training data, and the opera-tions in monolingual parsing were designed to avoid artificial ambiguity of derivation.
Another difference is that we have fixed structures growing on both sides, whereas fixed structures in (Eisner and Satta 1999 )can only grow in one direction. inspired by the well-known approach of Combinatory Categorial Grammar (CCG) (Steedman 2000). In fact, the names of left raising and right raising stem from the raising operation in CCG.
 nous Tree Adjoining Grammar (STAG )(Shieber and Schabes 1990 ). Trees on the source side are weakened to strings, and multi-rooted structures are employed on the tar-get side. The adjoining operation in our model is similar to attachment in LTAG-spinal (Shen, Champollion, and Joshi 2008 )and sister adjunction in variants (Rambow, Shanker, and Weir 1995; Chiang 2000; Carreras, Collins, and Koo 2008 )of TAG (Joshi and
Schabes 1997). Translation rules can be viewed as constraints on the tree operations. 8. Conclusions and Future Work
In this article, we propose a novel string-to-dependency algorithm for statistical ma-chine translation. It employs a target dependency language model to exploit long dis-tance word relations in decoding, which cannot be captured with a traditional n -gram language model.
 dependency system generates about 80% fewer rules. The overall gain in BLEU score on lower-cased decoding output is about two points.
 knowledge in MT. We will extend our approach with deeper linguistic features such as propositional structures (Palmer, Gildea, and Kingsbury 2005). The fixed and floating structures proposed in this article can be extended to model predicates and arguments. Acknowledgments 668 670
