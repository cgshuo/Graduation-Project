 Beijing Institute of Technology Beijing Institute of Technology Beijing Institute of Technology Beijing Institute of Technology that is usually time-consuming. 1. Introduction
Word segmentation is an important task in natural language processing (NLP) for to Chinese word segmentation (CWS) are supervised. Although supervised approaches reach higher accuracy than unsupervised ones in many cases, they involve much more human effort. Furthermore, unsupervised approaches are more adaptive to relatively unfamiliar languages for which we do not have enough linguistic knowledge. In ad-dition, unsupervised approaches can cooperate with supervised ones to overcome drawbacks of both.
 mentation, some researchers have conducted research on unsupervised approaches to word segmentation (Chang and Su 1997). Peng and Schuurmans (2001) proposed an unsupervised approach based on an improved expectation maximum (EM) learning algorithm and a pruning algorithm based on MI. Their approach outperforms soft-and hierarchical HPYP (HHPYP) (Wood and Teh 2008) X  X ave been introduced to word segmentation. Mochihashi, Yamada, and Ueda (2009) proposed a novel unsupervised approach based on HPYP, and evaluated it on a part of SIGHAN Bakeoff-2 data set (Emerson 2005). Their evaluation results suggested that their approach outperformed the previous ones.
 (Cohen, Heeringa, and Adams 2002; Cohen, Adams, and Heeringa 2007), are based on relatively simple ideas. In most cases, an unsupervised approach can be viewed as a kind of goodness measurement to find boundaries between words or filter words from candidates or both. There are four goodness algorithms reviewed by Zhao and
Kit (2008a). The algorithms, including Description Length Gain (DLG) (Kit and Wilks 2006).
 vated by the following considerations: 1. In contrast to the semi-supervised or supervised approaches, we want 2. In contrast to existing unsupervised approaches, we want to explore the
To avoid manual interference, we need to consider the following issues: 1. Unsupervised approaches usually rely on a maximization strategy or 422 2. Many approaches have constraints on maximum word length.
 3. Many approaches process characters with different strategies according
ESA is based on a new goodness algorithm that adopts a local maximum strategy and avoids thresholds. ESA has no constraints on maximum word length. In practice, this kind of constraint can have a negative impact on ESA X  X  segmentation. A simple
Moreover, ESA uses a self-revision mechanism to improve segmentation accuracy and formulae proposed in this article.
 2. ESA based on two simple ideas: 1. A better result can be produced by combining certainty and uncertainty. 2. A better result can be produced by adopting the self-revised pattern based
The input text can be viewed as a character sequence. A character sequence can be divided into two adjacent subsequences. The certainty mentioned previously means certainty of co-occurrence of adjacent subsequences. And the uncertainty means un-certainty of co-occurrence of adjacent subsequences. For example, suppose there are acter sequence into two adjacent subsequences or not depends on both certainty and adjust the next one.
 utterances (Seidl and Johnson 2006), meaning maps (Estes et al. 2007), and auditory forms of words (Swingley 2008). There are a few notable issues: 1. Familiarity (Bortfeld et al. 2005) can be represented by high frequency. 2. The edges of utterances (Seidl and Johnson 2006) can be viewed as natural 3. Both word lists and statistical data as prior knowledge enable human 4. The early vocabularies of human infants are based on the sounds of words machines can understand symbol sequences with simple logic. 2.1 Evaluation to be settled: 1. What are the character sequence and the pair of adjacent subsequences 2. What is the necessary statistical information and how do we get it? 3. How do we calculate the goodness? 424 2.1.1 The Target of Evaluation. A character sequence contains where N is the number of characters in the character sequence. These subsequences are the targets to be evaluated.
 boundary between the two subsequences. Every character sequence has an individual goodness value (IV). Every pair of adjacent subsequences has a combined goodness value (CV) based on the IV of each subsequence and the goodness value of the gap
LRV serves as the modifier of the base. measured: 1. The frequency of a character sequence. For example, if the character 2. The number of character sequences of the same length. For example, if the just mentioned: 1. The average frequency of character sequences of the same length. For 2. The entropy of Sequence Plus One (SP1) of a character sequence. For 3. The average entropies of SP1s of character sequences of the same length. data structures can also be used (Morrison 1968; McCreight 1976; Manber and Myers 1990). 2.1.3 The Calculation of Goodness. The IV of a character sequence is formulated as length of x . F denotes the frequency of a character sequence, and therefore F of x ; FM denotes the average frequency of character sequences of the same length, called Balancing, which means keeping balance between the local and global effects. Furthermore, FM is formulated as 426 and F
The average entropies of SP1Ls and SP1Rs of character sequences of the same length are formulated as and respectively. HLM and HRM denote the average entropies of SP1Ls and SP1Rs of of HL or HR ; L is the length of character sequences.
 where the LRV is formulated as
The superscript x is the exponent; The symbol  X  denotes that S adjacent sequences; L 1and L 2 denote the lengths of character sequences in SP1R( S
S the entropies of SP1s of character sequences of different lengths become comparable with each other by using Balancing.
 sents the weight of LRV in CV . 2.2 Selection segmentation of a character sequence.
 and ABC. Therefore, ABC can be divided into various pairs of adjacent subsequences, as shown in Figure 3.
 with the pair of adjacent subsequences B  X  C. If the CV of B BC, BC will be segmented into B and C.
 has two adjacent subsequence pairs: A  X  BC and AB  X  C. Both BC and AB have their process BC and AB before processing ABC. The IV of ABC, the CV of A and AB. The one with the highest goodness value is the final choice. length of the character sequence being processed. goodness value, which can be defined as finally selected segmentation, and E denotes an evaluation.
 428 formulated as where
S 0 is the character sequence and N is its length; S i (1 The combination of IV and CV is the primary criterion in Selection. secondary criterion in Selection: practice. 2.3 Adjustment
There are two issues to be settled: 1. What data can be updated after the previous Selection? 2. How can we use the updated data for the next Evaluation?
X X  X  proper subsequences are considered as being overestimated because all of them
Therefore, the frequencies of these subsequences are reduced, and Adjustment can be viewed as the corrections to the overestimated frequencies.
 by 1 and those of the other ABC are not changed, as shown in Figure 7. implies that the existence of a character sequence usually has a negative impact on of a subsequence is near to that of its supersequence, the subsequence will be re-moved. Frequency of Substring with Reduction (FSR) (Zhao and Kit 2008a) is derived from SSR. quantity to be changed. FM in IV , LRV , and others are not changed. 430 are A(1), B(3), C(2), AB(1), BB(1), BC(2), ABC(1), and BBC(1), where the number in
BB C, respectively. The subsequences of AB are A and B, and the subsequences of BB reduced. Therefore, the records become A(1  X  1 = 0), B(3  X 
BB(1), BC(1), ABC(1), and BBC(1) after Adjustment. 2. 4Preprocessing Input Data
Sentences are the input data directly accepted by many approaches. The difference not punctuation is used as prior knowledge to segment text. Some approaches further processed.
 the limitation has a negative impact on the segmentation accuracy of ESA. 2.4.1 Maximum Sequence Length. The time complexity of ESA is polynomial over the length of the character sequence being processed. The preprocessing segments the segmentation ( LoS ) is determined by the formula exceeds the limit, s will be further divided. ness of the main algorithm.
 the maximum word length usually has a positive impact on many approaches. to improve the results: 1. Punctuation can be used to divide a character sequence into natural 2. Different types of characters can be separately processed. 2.5 The Result Form
ESA can output the segmentation results in a hierarchical format. Figure 8 shows an example.
 like A B C, where the symbol denotes a space character. 2.6 Summary
In this section, we briefly describe the whole algorithm of ESA and provide some thoughts about it. Step 1: Preprocessing.
 Step 2: Evaluation and Selection.
 432 Step 3: Adjustment.
 the process from Step 1.

Step 1: Segment a corpus into multiple character sequences. No character sequence exceeds the limit of maximum length.
 calculated.
 Algorithm: Segment.
 Input: An entry of data structure. The entry represents a character sequence S . Output: None.
 character sequences, respectively. 01: If S  X  X  FV = 0 then 02: If L = 1 then 03: S  X  X  FV  X  IV ( S ) 04: S  X  X  FS  X  s [1][ L ] 05: Else 06: Max  X  IV ( S ) 07: Seg  X  s [1][ L ] 08: For i  X  1to L 09: Segment ( s [1][ i ]) 10: Segment ( s [ i ][ L ]) 11: CV  X  s [1][ i ] X  X  FV  X  s [ i ][ L ] X  X  FV  X  LRV ( s [1][ i ] 12: If Max &lt; CV then 13: Max  X  CV 14: Seg  X  s [1][ i ] X  X  FS  X   X  s [ i ][ L ] X  X  FS 15: End if 16: End for 17: S  X  X  FV  X  Max 18: S  X  X  FS  X  Seg 19: End if 20: End if the goodness. LRV in CV .
 be viewed as nested CVs . For example, the character sequence ABC is segmented into A B C. The whole goodness is equivalent to or
Although the character sequence can possibly be segmented in a different order, the whole goodness is equivalent to
Therefore, the design of CV ensures the consistency of the goodness measurement across different orders of segmentation.
 character sequence ABC can be segmented into A B C, AB C, A BC, and ABC. The products of IVs in the nested CVs are unannotated corpora alone. The only parameter in ESA is the exponent in LRV , which can be predicted with the empirical formulae.
 the approach should not depend on punctuation to segment sentences: 1. Although punctuation is easily identified with encoding information, it 434 2. Segmenting sentences with punctuation is based on the assumption that a 3. The completely unsupervised approach should have more abilities to 3. Experiment The experiment uses the SIGHAN Bakeoff-2 data set that is publicly available on the SIGHAN Web site (www.sighan.org). We tested ESA with various settings. those results. According to the experiment, we establish the empirical formulae to predict the exponent in LRV . 3.1 Settings
We used four different settings to test ESA: 1. Punctuation and other encoding information are not used, and the 2. Punctuation and other encoding information are not used, and the 3. Punctuation is used to segment character sequences into sentences, and 4. Both punctuation and other encoding information are used, and the the baseline in the paper of Zhao and Kit (2008a). We believe there are two rea-sons why CAW might be viewed as evidence of the effectiveness of unsupervised approaches: 1. The unsupervised approaches are not comparable with supervised 2. The unsupervised approaches are not comparable with each other to analysis for Chinese. However, we think that making comparisons based on similar corpora and settings with other approaches is necessary when regarding word segmen-tation as an independent task. 3.2 Targets experiment, the number of character sequences (N1) and nodes in trie (N2) produced by ESA are also different, as shown in Table 1. 3.3 Results
The experimental results produced by ESA with the four different settings are shown almost the best; the asterisked number is the proper exponent.
 436 less the exponent considerably diverges from the proper value. The larger exponent leads to more insertion errors, whereas the smaller one leads to more deletion er-rors. On one hand, ESA segments a character sequence into more parts when we increase the exponent in LRV (the weight of LRV in CV ), which can produce fewer after a number of iterations, which can produce more deletion errors. ESA produced too many deletion errors with an excessively low weight of LRV (such as 0.5 in the result.
 tice, the results are improved when increasing the maximum length from 10 to 30 438 as shown in Figure 9. Therefore, the limitation on the maximum length really makes a negative impact on ESA. However, when the maximum length of input character Table 6.
 encoding information, punctuation and other encoding information are effective in improving segmentation results, as shown in Figure 9.
LRV and the scale of the corpus (N1 and N2), as shown in Figure 10. According to not perfectly fit the proper exponents, but they can be used to produce acceptable results. 440 3. 4Discussion complexity of ESA. 3.4.1 Convergence. In this section, we discuss the convergence of ESA. Definition 1 each item s i is a subset of I : I = N
Therefore,
ABC, namely I = { A B C, AB C, A BC, ABC } , where is a delimiter. There are three items s 0 , s 1 ,and s 2 in the sequence S . Specifically, s 442 integers (i.e., the subscripts 0, 1, 2).
 Definition 2 to the next one, as shown in Figure 11: Theorem 1 The main algorithm of ESA is a monotone increasing function of F (the F in IV ). Proof In ESA, the goodness evaluation E of segmentations of a character sequence X is where
M 1and M 2 are the numbers of segments and delimiters in the segmentation, respec-be viewed as a function of F ,thatis, E is a monotone increasing function of F .
 Theorem 2 ESA converge.
 Proof tation of X respectively, the goodness value of s i is larger than that of s
E ( s i +1 ). Additionally, the number of delimiters in s i +1 is larger than that in s Definition 2 cannot exist.
 frequencies of subsequences A and B are reduced by 1, whereas those of segments be A B C.
 same segmentation, which means that the results with changes 3a or 3b are viewed as unchanged results by ESA. Therefore, the results produced by ESA are monotone changes 1 and 3 in ESA. Additionally, S can be viewed as a finite sequence of non-numbers with a lower bound. According to the monotone convergence theorem, the successive results produced by ESA converge.
 cannot prove that the results always converge to the optimum F-measure, ESA ensures that the F-measure is monotone increasing in most cases.
 444 by ESA, which means that ESA theoretically can converge (Dempster, Laird, and Rubin 1977; Wu 1983). 3.4.2 Complexity. The core algorithm, Segment, is implemented with dynamic pro-processes is N including 1 IV and N  X  1 CVs , which means that the growth rate is O( N analyzing the algorithm, the total number of comparisons is and the total number of calculations is Therefore, the time complexity is O( N 3 ) in the worst case.

Yor Language Model (NPYLM) (Mochihashi, Yamada, and Ueda 2009) is O( N bigrams, where N is the length of a character sequence and L is the maximum word length accepted. Therefore, the time complexity of NPYLM is O( N of the character sequence and the maximum word length are equal to each other. In addition, NPYLM approximately converges around 50 iterations in the experiment ESA seems to be faster.

Figure 13, where N is the maximum length of input character sequences. The ESA implementation is a single thread program, and we run it on an AMD Athlon64 system at 2.61GHz. 4. Comparison them with ESA. We mainly concentrate on unsupervised approaches because of the motivation for our study. 4.1 Descriptive Comparison
A statistical approach was proposed by Teahan et al. (2000) (TH), which is based on the partial matching (PPM) symbol-wise compression scheme. The approach consists of segmented. Therefore, TH is a supervised approach.
 (MI and entropy, respectively). The approach segments words according to an aug-mented dictionary and adds potential words to the dictionary. The program consists of a segmentation module and a filtering module. The augmented dictionary consists words given as prior knowledge. The potential words are produced by merging and
Chang and Su (1997), the system dictionary was the combination of two dictionaries. 446 In addition, their approach extracted the potential words from unannotated corpora. Therefore, IWSLRR is semi-supervised. Whereas ESA is completely unsupervised, both
IWSLRR and ESA use the combination of certainty and uncertainty. MI and entropy are the measures of two kinds of information in IWSLRR, whereas ESA uses IV and LRV. ESA directly multiplies IV and LRV to combine them, unlike IWSLRR, which uses Gaussian mixture.
 when replacing the one-character sequence. Therefore DLG cannot uniformly process one-character words, whereas ESA uniformly processes words of different lengths. of goodness values produced from the comparisons is called the  X  X otal vote. X  If the will be determined. The algorithm uses held-out data sets to estimate the maximum order of n -grams and the threshold. The held-out data sets are manually annotated.
Therefore, TONGO is semi-supervised. The non-straddling and straddling strings can differences between ESA and TONGO are similar to those between ESA and IWSLRR. 1. Use EM to establish a core lexicon and a candidate lexicon. The selected 2. Use MI to split long words in the lexicon.

The pruning algorithm uses two thresholds that are manually assigned, and the vali-information are independently considered by SS, whereas ESA uses a different method to combine them.

Heeringa 2007) uses logarithm frequency and boundary entropy. Two independent maximum strategy in a window. The maximum window size limits the word length.
To process words of different lengths, VE standardizes frequencies and boundary en-dently processes the two kinds of information, whereas ESA combines them in CV. words. Several functions are provided to balance words of different lengths. The AV value is similar to H(SP1) in ESA.
 1. The entropy of the location is the local maxima. 2. The entropy of the location is greater than that of the previous location in 3. The entropy of the location is larger than a given threshold.

BE only uses uncertainty information. 448 2009) is a Hierarchical Pitman-Yor Language Model (HPYLM) (Teh 2006a, 2006b). The base measure of the HPYLM is also a HPYLM. Specifically, NPYLM uses a character HPYLM as the base measure of a word HPYLM. To process words of different lengths, NPYLM uses Poisson distribution to correct the base measure of the character HPYLM. guage and word types. In detail, the  X  is estimated by a Gamma distribution with two hyperparameters assigned manually. It is noteworthy that the F-measures of this approach were higher than 0.8 on two corpora of Bakeoff-2.
 of them, such as AV and BE. Others use both of them, such as IWSLRR, TONGO, SS,
Using thresholds involves more human effort and introduces random factors because different corpora without much adjustment.
 1. The Poisson distribution in NPYLM. 2. Pruning in SS. 3. The restriction on the order of n -gram in TONGO. 4. Standardizing in VE. 5. Several functions and thresholds in AV. 6. Ignoring one-character words in DLG. 7. Merging and filtering in IWSLRR. 8. Balancing in ESA.
 (Chang and Su 1997). In practice, ESA greatly improves its segmentation results by using an iterative process. 4.2 Quantitative Comparison
In this section, we compare the performance of ESA with that of other approaches, which we can divide into two categories: 1. Unsupervised approaches. NPYLM, AV, BE, and DLG were tested on 2. Semi-supervised and supervised approaches. We compare ESA with merged corpora to compare with NPYLM. Specifically, we merge the test corpora with of the CITYU training corpus as a single corpus. NPYLM considered specific language test (Emerson 2005) for comparison in Table 8.
 Bakeoff-3 to extract word candidates. Therefore, we evaluate ESA on merged corpora. Zhao and Kit claimed that the approaches were tested without any prior knowledge. and worst F-measures of supervised approaches in the Bakeoff-3 closed test (Levow 2006) for comparison in Table 9.
 merged corpora in both of them. The part of a segmentation result belonging to the result is evaluated in E2.
 and Adams 2002; Cohen, Adams, and Heeringa 2007). When the average length of 450 words (VE1) was given, the F-measure of VE was 0.77. However, the F-measure was of VE was six and the punctuation of the corpus was removed in their experiment. Therefore, we use the result of setting 3 to compare with VE, as shown in Table 10. proaches: IWSLRR (Chang and Su 1997), SS (Peng and Schuurmans 2001), and TONGO use the arithmetic mean of F-measures of four Bakeoff-2 training corpora to compare with them, as shown in Table 11.

Sinica dictionary (CKIP 90) and the BDC electronic dictionary (BDC 93). The unanno-tated Chinese corpus used by IWSLRR contained 311,591 sentences (about 1,670,000 respectively. The result of IWSLRR was achieved after 21 iterations. Xinhua newswire. The maximum length of words in the test of SS was four. acters, which came from the 1993 Nikkei Japanese newswire. The maximum order of n -grams in the test of TONGO was six. However, Ando and Lee (2000, 2003) did not directly present specific F-measures. The F-measure of TONGO was approximately 0.816 in their charts.
 sion analysis. The corpora used by TH were Guo Jin X  X  Mandarin Chinese PH corpus containing about 1M words and the Rocling Standard Segmentation Corpus containing about 2M words. L and P in the table denote the estimations of a linear regression model (R 2 = 0.905) and a second order polynomial model (R
T1 denotes training with the PH corpus and testing with the Rocling corpus, and T2 denotes training with the Rocling corpus and testing with the PH corpus. 5. Conclusion estimated. The only parameter (the exponent in LRV) can be predicted via empirical formulae. ESA can produce acceptable results without any encoding information ex-cept line breaks. When ESA utilizes prior knowledge such as punctuation and other encoding information, it performs much better.
 including: 1. Combine with supervised approaches (Zhao and Kit 2008b) or become 2. Find more suitable applications for unsupervised approaches (Bod 2006). Acknowledgments References 452
