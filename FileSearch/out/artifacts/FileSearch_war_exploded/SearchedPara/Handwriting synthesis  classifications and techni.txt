 ORIGINAL PAPER Yousef Elarian  X  Radwan Abdel-Aal  X  Irfan Ahmad  X  Mohammad Tanvir Parvez  X  Abdelmalek Zidouri Abstract Handwriting synthesis is the automatic gener-ation of data that resemble natural handwriting. Although handwriting synthesis has recently gained increasing inter-est, the area still lacks a stand-alone review. This paper pro-vides classifications for the different aspects of handwriting synthesis. It presents the applications, techniques, and evalu-ation methods for handwriting synthesis based on the several aspects that we identify. Then, it discusses various synthesis techniques. To the best of our knowledge, this paper is the only stand-alone survey on this topic, and we believe it can serve as a useful reference for the researchers in the field of handwriting synthesis. 1 Introduction Handwriting synthesis, also referred to as synthesis hereafter, refers to the artificial generation of data that resemble human writing. Synthesis has applications such as the improvement of text recognition systems, PC-personalization, forgery detection, and Completely Automated Public Turing test to tell Computers and Humans Apart (CAPTCHA). These applications may require certain specifications on the syn-thesized data or on the synthesis technique, such as being of a specific writer X  X  style or a specific script. These applications also suggest methods to evaluate the adequacy of synthesized data.

Handwriting can be modeled via the simulation of the human writing process ( top-down approach) or of its out-come ( bottom-up approach). In the top-down approach, the neuromuscular acts of writing are simulated in what is com-monly termed movement simulation . When the data itself is regenerated without imitating human movements, synthesis is termed shape simulation [ 1 ]. Moreover, there are online and offline handwriting synthesis scenarios. In online syn-thesis, text is generated with temporal information. On the other hand, text is generated without temporal information but with stroke thickness and inking information in offline synthesis.

Synthesis can be seen as the reverse of more well-known problems. For example, when synthesis aims at the genera-tion of individual characters from their ASCII codes, it can be regarded as the reverse of character recognition. Similarly, when synthesis aims at the generation of words through the concatenation of characters, it can be regarded as the inverse of the character segmentation problem.

Handwriting synthesis has recently become a hot topic withincreasinginterestfromtheresearchcommunity.Among the refereed journals that contribute to the dissemination of established knowledge in the area are the International Jour-nal of Document Analysis and Recognition (IJDAR) (e.g., [ 2  X  4 ]), Pattern Recognition (e.g., [ 5  X  8 ]), Pattern Recogni-tion Letters (e.g., [ 9 ]), Machine Learning (e.g., [ 10 ]), and others. Besides, some prestigious conferences such as the International Conference on Document Analysis and Recog-nition (ICDAR) (e.g., [ 11  X  14 ]), the International Workshop on Document Analysis Systems (DAS) (e.g., [ 15 ]), the Inter-national Conference on Pattern Recognition (ICPR) (e.g., [ 16  X  19 ]), and the International Conference on Frontiers in Handwriting Recognition (ICFHR) (e.g., [ 20  X  23 ]) help in spreading the advances in the field. However, there is a lack of a stand-alone paper that surveys the field. Hence, we con-tribute in this paper by classifying the applications of synthe-sis and linking them to certain aspects of synthesis. We also survey the synthesis techniques with more focus on shape-simulation approaches.

The rest of this paper is organized as follows: In Sect. 2 , we link the applications, aspects, and evaluation methods of synthesis. Then, we present a review of shape-simulation approaches in Sect. 3 . In Sect. 4 , we present other techniques. Finally, we summarize and conclude in Sect. 5 . 2 Synthesis applications, aspects, and evaluation methods The applications of synthesized data guide the aspects of synthesis and dictate methods to evaluate their results. In this section, we identify some handwriting synthesis applications and link them to the aspects and evaluation methods that may suit them. 2.1 Synthesis applications Handwriting synthesis has a wide range of applications. It can be used to generate desired and inexpensive ground-truth data for the development of text segmentation and recogni-tion systems [ 24 ]. Writer-specific synthesis can also be a means for font personalization [ 25 , 26 ], calligraphy gener-ation, word spotting, forgery detection, and writer identifi-cation. In addition, a recent application for synthesis is the production of CAPTCHAs.

Synthesized handwriting might target humans, machines, or both. It may be intended to imitate a particular writer X  X  style, to generate writer-independent handwriting, or to tell humans and machines apart. Synthesized calligraphy, for example,targetshumansubjects[ 21 , 27 , 28 ],whereasgeneric training data may target text recognition systems [ 9 , 15 , 29 ]. Word spotting systems may benefit from writer-specific syn-thesis to find words written by a particular scribe [ 30 , 31 ] and from generic synthesis to find words regardless of scribes. CAPTCHA requires reasonable human legibility but low machine readability [ 32 ].

Figure 1 shows some applications on a machine/human readability plane. Handwritten CAPTCHAs exploit the gap between humans and machines in reading handwriting [ 5 ]. Calligraphic and personalized fonts aim at the aesthetic aspects of writing but may be confusing to machines. On the other hand, some perturbed and noisy text which might not be pleasant to humans can be useful for training recognition systems [ 13 , 17 , 29 ]. Steganography, the art of hiding data, is a possible application for synthesized handwriting where secret messages can be communicated by certain choices of the optional features in a script [ 33 , 34 ]. 2.2 Aspects of handwriting synthesis The following aspects of handwriting synthesis can be spec-ified based on the needed applications into: 1. Level of granularity: Strokes, characters, character 2. Techniques: Generation or concatenation. 3. Online versus offline data types 4. Scripts and languages: Arabic, Chinese, Indian, Latin, 5. Writing variability: Writer-specific vs. writer-6. Parameterization: parametric versus non-parametric sys-Most of the aspects above specify the outputs of a synthe-sis system. The techniques and the parameterization aspects specify synthesis systems, rather than their outputs. The level of granularity can qualify input or output data, but the aspect, here, is restricted to the synthesis output. 2.2.1 Level of granularity The  X  X evel of granularity X  refers to the unit (whether as strokes, characters, character groups, words, lines, or para-graphs) that is output by a synthesis system. 2.2.2 Techniques Handwriting synthesis receives images of handwritten sam-ples and generates new handwriting images. The input and output images can be of similar or different levels of granular-ity. Based on the relationship between the granularity levels of the input units and the output units, we classify synthesis techniques into two types: generation techniques and con-catenation techniques. Generation techniques produce new synthesized images at the same level of the input samples they receive. Concatenation techniques, in contrast, produce output images at higher levels than their inputs, often guided by ASCII or Unicode text specifying the required synthesis string. 2.2.3 Online versus offline data Online data, such as coordinate time-stamps and pressure, are captured as writing occurs on special devices called tablets. Figure 2 a, c shows online data. Offline data are taken as sta-tic images of script that are usually written on paper. Offline data lack temporal information but contain inking and stroke-thickness information (e.g., Fig. 2 d). The data types of the inputs and the outputs of synthesis systems are usually the same. Sometimes, however, online data might be used to generate offline-like outputs, often by the addition of inking effects [ 15 , 20 , 27 ]. In addition, some systems utilize a mix-ture of online and offline data in their inputs (e.g., Fig. 2 b), for example, when a printed character is used as a standard reference for handwritten samples [ 35 ]. 2.2.4 Scripts and languages A script can be used to write several languages. The Latin script, for example, is used in English and Spanish languages. A script can be inherently cursive as in Arabic, inherently discrete as in Hiragana [ 36 ] and Katakana [ 37 ], or mixed as in modern Latin. Researchers have worked on synthesis of Latin [ 20 , 21 ], Arabic [ 4 , 38 ], Cyrillic [ 9 ], Chinese [ 11 , 39 ], Korean (Hangul) [ 15 ], Japanese ([ 36 , 40 ] and [ 37 ]), and Indian (Hindi, Tamil, Malayalam, and Telugu) scripts [ 23 ]. Occasionally, systems are implemented and tested on multi-scripts [ 2 , 9 , 30 ]. 2.2.5 Writing variability Synthesis may or may not aim at the imitation of a spe-cific writer X  X  style, depending on the applications. Synthe-sis for character recognition improvement [ 15 , 36 , 41 , 42 ], as well as for CAPTCHA generation, usually lacks writer-specific features [ 5 , 43 ]. On the other side, applications such as PC personalization [ 20 , 21 , 28 , 44 ] and writer identifica-tion [ 28 , 45 , 46 ] call for writer-specific synthesis. In Table 1 , we classify the applications of handwriting synthesis by their writer imitation and target aspects. In some cases (e.g., [ 16 ]), large databases of handwriting can be synthesized to generate setup. Some researchers have developed systems that can function in either a writer-independent or a writer-specific modes [ 22 , 47 ]. 2.2.6 Parameterization Any synthesis technique uses a number of parameters which need to be learned or calibrated. Parameterization is an important aspect to study when comparing different syn-thesis techniques. Systems with fewer parameters are gen-erally preferable. Parameters may also affect the computa-tional efficiency of a technique. Another important aspect of parameters is their estimation/training. Some techniques may involve parameters which require expert knowledge for calibration while other parameters may be trained from the data available. Moreover, the number of parameters that need to be trained also places some constraint on the min-imum data required to robustly train the model [ 54 ]. But sometimes, more parameters provide increased flexibility in deciding the desired quality and property of the synthesized text.

Parameters are in general closely tied to the underlying techniques. For example, Sigma-lognormal models [ 16 , 55 ], signal-based models [ 48 ], and spline-based models [ 21 , 42 ] depend on parameters for the definition of character shapes. Parameterization may be used to smooth joining ligatures between characters in concatenation systems [ 5 , 25 ]. In gen-erative systems, changes to samples are controlled via para-meters. For example, perturbation is added to samples, as in [ 14 , 29 , 56 ]. Naturalness can be parameterized, as in [ 44 ], where the relative distance from the printed sam-ple and the nearness to handwritten sample is considered naturalness .

Figure 2 shows examples of different synthesis based on the aspects previously described. Figure 2 a[ 47 ] and Fig. 2 b [ 40 ] exemplifies the generation of Hangul and Hiragana char-acters, respectively, whereas Fig. 2 c[ 38 ], Fig. 2 d[ 20 ], Fig. 2 e [ 38 ] and Fig. 2 f[ 20 ] exemplifies the concatenation of a char-acter, a word, a text-line, and a paragraph, respectively. Figure 2 a, c, f uses and generates online data (where dotted lines indicate pen-rises); Fig. 2 d, e shows samples of offline Latin and Arabic words synthesis; Fig. 2 b matches online and printed (offline) samples to synthesize offline characters. Figure 2 b shows an example of a parameterizable system where naturalness is parameterized via the distance from the printed font of the ka character (the origins of the small arrows) to the online handwritten version (pointed to by the arrows), and Fig. 2 c shows four numbered shape vectors that define the splines to form a synthesized  X  X  X  character. Fig-ure 2 f is a clear example of writer-specific synthesis. Each shape vector has a direction indicated by its prefix (origin) and suffix (destination). 2.3 Evaluation methods The choice of evaluation methods for synthesized data depends on the application domains for which the synthesis system is designed. Evaluations are used for following two broad purposes: to assess the quality (e.g., syntactic correct-ness, naturalness) of synthesis itself and to assess how well synthesis fulfills an application X  X  end goal. For example, in the case of text synthesis for OCR improvement, there can be some evaluation methods for the assessment of the quality of synthesized text and how well it imitates the training data in a quality like its natural appearance. On the other hand, there can be evaluation methods for the assessment of the goal of the OCR application, say by measuring the increase in text recognition results due to the addition of synthesized training data.

Commonly used evaluation methods fall into two main categories: subjective and objective. S ubjective evaluation methodsmainlyrelyontheopinionofhumansubjects.Some-times, trained experts may be solicited to decide whether some handwriting belongs to a specific writer, whereas in other cases, evaluators only need to decide whether the writ-ing looks natural or not.

Several researchers have used subjective methods for eval-uating the synthesized handwriting. Subjective opinions of 21 English native speakers, that were not among the 15 writ-ers of the database of [ 52 ], were used to evaluate the perfor-mance of their parameter calibration. For example, Guyon mentioned that in subjective evaluation, the trained eye can find exaggerated regularities in letter shapes and probable inconsistencies in inking [ 20 ]. Other works that rely on sub-jective evaluation include [ 21 , 28 , 47 ].

Objective methods rely on quantitative measures for the evaluation of synthesized handwriting. Text and writer recog-nition systems give success rates which can be used as mea-sures of the machine readability or writer resemblance of some handwriting [ 46 , 57 ]. In order to assess data that is syn-thesized for OCR improvements, the data can be injected to the training set. Injecting more synthesized data to training data is expected to improve the performance of the recog-nizer under the condition that the synthesized data captures the variability of natural writing. The premise is taken from a rule of thumb with real data: The more training data the better the recognition [ 58 ].

Figure 3 shows the most common evaluation methods grouped into the subjective and objective criteria.
Improvements in HMM-OCR performance on the IAM database were reported after the injection of synthetic train-ing data in [ 13 ] and [ 57 ]. Support vector machine OCR that runs on a database of 10 Hiragana characters (from the HANDS-nakayosi t-98-09 database) was used in [ 15 ], with reported improvements on the OCR performance. Similar efforts for improving OCRs using synthesized data include [ 41 , 46 , 50 , 59 ]. A script recognizer was used to classify syn-thesized text into Arabic, Latin, or Russian by Vincent et al. [ 9 ]. Although all of their synthesized data were perfectly labeled with its correct script type, the authors commented that the differences between correlation coefficients were quite small and not very reliable. In [ 12 ], normal OCR Turing test is used for the evaluation of synthesized Arabic handwrit-ing. The models derived in [ 48 ] achieve 99.4% success rate when tested as recognizers.

Analysis by synthesis is an objective evaluation method that judges synthesizers by the quality of their recognition models. This evaluationmethodis especiallyuseful withgen-erative model-based synthesizers. An analysis by synthesis scenario was used in [ 48 ], where the authors performed a test of completeness on their statistical model to demonstrate the ability to recognize data not in the training set.
Another objective evaluation method for synthesis com-pares synthesized handwriting to some reference model . Dolinsky and Takagi consider printed Hiragana characters as reference models that are deformed by personal handwrit-ing styles [ 60 ]. Correlations and regression analysis are used to quantify the difference between the synthesized and ref-erence model. Zheng et al. [ 14 ] also quantify the amount of deformation needed for their fusion-based algorithm.
A combination of subjective and objective evaluations was performed by Rao [ 42 ]. He used his synthesis model to implement a recognition scheme, in an analysis by synthesis scenario. He also demonstrated the distances between some original and synthesized sample characters and reported the natural and legible appearance of the results. The results of character synthesis are reported to be similar to their corre-sponding natural characters. The shape vectors used in that work achieve 94% success rate as recognition models.
The performance of CAPTCHAs is evaluated by low OCR recognition rates while preserving reasonable human legibil-ity. Hence, both OCR and subjective evaluation methods are needed to evaluate CAPTCHAs [ 5 , 43 ]. 2.4 Linking applications, aspects, and evaluation methods Applications may drive specifications related to the aspects of synthesis systems. Table 2 suggests specifications of the outputs of synthesis systems for some common applications of synthesized handwriting along with some suitable eval-uation methods. The script aspect is not shown because it directly follows from the application script.

Some applications require synthesized data that is con-strained to the style of a specific writer, whereas other applications can accept (and sometimes require) writer-independent style. Examples of writer-specific applications include forgery detection and font personalization. Synthe-sized handwritten CAPTCHAs typically allow for mixed writing styles. For character recognition systems, it is impor-tant to synthesize texts having realistic variability of multiple writers. Artistic and calligraphic applications may be some-times constrained to writer-specific styles and may some-times benefit from several styles together.

Applications that target end users usually call for offline synthesis,forthisisthenaturalrepresentationforhumans.On the other side, most applications that target computers, such asOCRandwriteridentification,maycallforofflineoronline synthesis, based on their internal representation of data. The performance of these latter applications can be evaluated by objective measures that comply with their targets. 3 Review of shape-simulation approaches Shape-simulation approaches for handwriting synthesis model the shapes of handwriting units rather than the move-ments that produce them. Shape-simulation techniques are practical when online data are not available, i.e., when data acquisition means are not restricted to PC tablets.
There are generation and concatenation techniques for shape simulation. Generation techniques synthesize new instances for a given writing unit, while concatenation tech-niques connect smaller scripting units into larger ones. Figure 4 shows a classification of shape-simulation tech-niquesunderthegenerationandtheconcatenationapproaches.
Generationtechniquescanbesubdividedintoperturbation-based, fusion-based, and model-based techniques. Perturba-tion-based techniques alter one input sample to obtain new samples from it. Fusion-based techniques take two-to-few input samples and fuse parts of each into novel samples. Model-based techniques capture the variations in writing from many samples of a desired unit into models.

Concatenation techniques can be subdivided, according to the connection means they adopt, into no-connection, direct-connection, and modeled-connection. No-connection techniques juxtapose writing units into text lines. Direct-connection techniques take writing units and position them such that the ending ligature from one unit (also referred to as tail [ 21 , 61 , 62 ] or prefix segment [ 42 ]) directly connects to the starting ligature of the next unit (also referred to as head or suffix segment) to form a text line. Modeled-connection techniquesaddnewconnectionligaturessynthesizedbypara-metric curves. The tail segment of a letter and the head seg-ment of the subsequent letter can form control points to the synthesis of a connecting ligature.

In several scripts, there are parts that connect and others that do not. Connecting parts usually use direct-connection and modeled-connection techniques, whereas the non-connecting parts would use no-connection technique.
Table 3 classifies common shape-simulation works, with the type of data and scripting units used. For character synthesis, generation techniques are more popular although concatenation was used to synthesize characters from sub-characters [ 23 , 42 ]. On the other hand, cursive sub-words are mainly concatenated except when they are part of complete lines which are generated using perturbation [ 13 ]. For text line synthesis, both concatenation as well as generation tech-niques are commonly used although no work is reported on online synthesis of text lines using generation techniques. In Sects. 3.1 and 3.2 , we discuss generation and concatenation techniques, respectively. 3.1 Generation techniques As mentioned before, generation can be performed by per-turbation, fusion, or modeling, requiring one, two, or more input samples, respectively. Except for perturbation-based techniques, the two other techniques require shape-matching operations [ 14 , 50 ]. Table 4 presents different works classi-fied by the three generation-based techniques along with the various output data types used. In the following subsections, each of the three generation techniques is discussed in detail. 3.1.1 Perturbation-based generation Perturbation-based techniques generate new samples by altering geometric features such as the size, thickness, and slant of a given sample. Perturbation-based operations can be seen as the inverse of the preprocessing steps employed in text recognition. Perturbation-based techniques are easy to apply, but the results may be unnatural due to random and non-calibrated parameter settings [ 29 , 46 , 52 ].
Stroke-wise rotation and scaling perturbations are applied to online strokes with high curvature points in [ 28 ]. Pertur-bations are added to text lines in [ 21 ] in order to generate additional training data to increase the variability within the dataset. Varga and Bunke [ 13 , 17 , 29 ] apply nonlinear geo-metric perturbations on complete text lines and connected components of offline images. They choose the parameters of their perturbation models randomly from predefined ranges. Their results show that this approach can be useful in improv-ing OCR recognition performance by adding synthesized data to otherwise small training sets. Cheng and Lopresti [ 52 ] calibrate the parameters of the perturbation-based model of the work of Varga and Bunke [ 13 ]. Chen et al. use those perturbation models for writer identification on Arabic hand-written data [ 46 ]. 3.1.2 Fusion-based generation Fusion-basedtechniquestakefewinputsamplesandcombine them into new synthesized outputs. They differ from concate-nation techniques in that they generate scripting units at the same level as their inputs; e.g., characters generate new char-acters. Shape-matching algorithms are necessary for fusion-based techniques to make sure that segments are properly aligned. The number of unique outputs is limited in fusion-based techniques as compared to that of other generation techniques.

Zheng et al. [ 14 ] present a point-matching algorithm and apply it to generate online Latin letters by displacing the points in the range between two samples. Viswanath et al. [ 18 , 50 , 51 ] implicitly combine different partitions of sam-ples of offline images into hybrid images while fixing their shared components. Fusion-based handwriting synthesis is not very common in the literature, probably because it is not as established as model-based techniques. 3.1.3 Model-based generation Model-based techniques capture the statistics of natural handwriting variations into models. Although model-based techniques are profoundly established in theory, they may often be challenging to implement due to the large num-ber of samples they require [ 14 ]. Models resulting from these techniques can also be utilized in recognition systems [ 42 , 63 ].

Model-based generation may process sampled points of data often chosen for their structural features, e.g., maximum curvature [ 21 ] or zero-velocity [ 48 ], by spatial sampling, e.g., equidistance [ 40 ] or by drawing them from a generative sta-tistical recognizer, e.g., a Bayesian network [ 47 ]. A common modeling scenario is that statistics on displacements of the sample points from a template sample are captured. New sample points are then drawn from the statistical model to generate shapes.

Techniques adopted for model-based generation depend, again, on the target applications and data types. In the fol-lowing, we discuss the various techniques for model-based generation under online and offline categories.

Techniques that use online data. Different techniques are used to sample the drawn coordinates of online data. One can extractstraightgraphemeswithinonlinecharactersandselect them to be control points [ 42 ]. From these control points, more significant ones can be selected using Gabor filters [ 21 ] or Principle Component Analysis (PCA) [ 1 ]. Some works avoid the sampling of points and generate the coordinates directly [ 47 , 48 ].

Oncecontrolpointsareselectedfromtheonlinedata,char-acters can be synthesized by using polynomial splines by connecting the control points [ 42 ]. One approach [ 21 ]isto match the control points to a template that is computed from all the sample characters. Synthesis is done by drawing the control points according to a generative model of their dis-placements from the template and then using curves (splines) to connect them into a character shape. Some authors have used eigen vectors instead of splines [ 1 ].

Techniques that do not directly rely on the extraction of control points from sample characters define genera-tive models from which new samples can be synthesized. Some authors use generative statistical systems to synthesize handwriting through sampling from estimated joint distrib-utions [ 47 ]. Others consider the online x and y sequences of single-stroke character shapes as the impulse response of an online signal [ 48 ]. The authors sample characters into fixed sized vectors and match the points by using the modified Newton method. They find the character syn-thesizing filters by solving the optimization problems of the transfer functions for each pair of inputs and matched outputs.

Techniques that use offline data. These techniques work on the images of handwritten texts. A natural idea is to derive some template patterns from the offline data and then gen-erate new samples from the templates. In [ 41 ], all the points from a sample of training data are matched with its class tem-plate (taken as the average of all available samples for that class) and their displacements are recorded. Then, generation of new samples is done by selecting new points within the pre-calculated displacements. A similar approach of generat-ing samples from templates using the displacements is used in [ 40 ]. However, the authors used characters from standard fonts as templates. To calculate the displacements, the out-lines of font templates are sampled equidistantly to match it with the offline images.

In another approach, Vincent et al. [ 9 ] applied fractal decomposition and synthesis as a lossy encoding X  X ecoding process to offline character images. They defined reference bases that are repeated in an alphabet and then used these to model characters of the alphabet.
Techniques that use mixed online and offline data. There are several works that try to take benefits of both online and offline data. In [ 36 ], online strokes help in defining affine perturbations to offline data. Online samples of some Hira-gana characters were matched to a selected template sam-ple by dynamic programming. Using PCA on the differences between the template and the samples, the patterns with most effective impact were chosen to improve the variety of pat-terns in [ 15 ].
 Liu et al. [ 27 ] patented the idea of using trained Hidden Markov Models (HMMs) as generative statistical models to synthesize handwritten samples. The HMMs were trained as handwriting recognizers using handwritten and calligraphic-font samples. Pressure and ink data provided online and offline flavored outputs. 3.2 Concatenation techniques Concatenation refers to any synthesis approach that com-bines input samples into outputs of higher semantic lev-els. One common example is the concatenation of character shapes into words or text lines. Concatenation can be seen as the reverse of character segmentation in a text recogni-tion system. It encompasses tasks such as baseline detection, horizontalspacemodeling,connectionpartsegmentationand modeling, and segment joining and trimming. The input units for concatenation techniques are usually characters [ 53 ]but can also be sub-characters [ 42 ], character groups [ 20 ], or connected components [ 38 ].

Concatenation techniques depend on the knowledge of the rules of a writing script. Some scripts, such as Arabic, enforce most characters to be joined in a continuous flow [ 64 ], whereas the composite style of Latin allows the writer to connect or disconnect letters, and the Chinese script does not connect characters together but is often composed of inter-connected components/strokes.

The shape of the segments connecting letters, referred to as ligatures in [ 5 ], also depends on the script. In Latin, they often ascend in a curvy line to connect the suffix segment of a letter to the prefix segment of the subsequent letter [ 42 ]. The Arabic connection (Kashidah) is usually horizontal with occasional vertical ligatures [ 38 ].

Table 5 summarizes the different works for online and offline no-connection, direct-connection, and modeled-connection categories. 3.2.1 No-connection concatenation No-connection techniques concatenate scripting units by aligning and juxtaposing them without connection. Many techniques can be employed for the alignment of the text components such as projection profiles, baseline estimation, and connected component analysis. Usually, a number of techniques are used together for robust alignment [ 38 , 43 ].
Guyon [ 20 ] suggests simple juxtaposition of selected let-ter strings to synthesize semi-cursive text. Letter groups are selected based on their frequency in a linguistic corpus. In the training phase, a sample of each of the letter strings is collected from the writer whose handwriting is to be imi-tated on an online tablet. In the synthesis phase, the text to be synthesized is parsed into a sequence of available letter strings and the corresponding letter string images are placed as text lines and paragraphs. This approach works well in subjective tests at the first glance. However, the trained eye may soon notice abrupt pen lifts between glyphs, repetitions of glyph appearance, and too regular pressure or inking. Geo-metric transformations are introduced to reduce such effects. In [ 38 ], non-connecting PAWs (Parts of Arabic Words) are aligned without any connection. 3.2.2 Direct-connection concatenation Direct-connection techniques take writing units and position them such that the ending ligature from one unit directly con-nects to the starting ligature of the next unit to form text lines. These techniques are suitable for inherently cursive scripts like Arabic. Arabic online handwritten samples have been segmented and later concatenated to produce new samples in [ 12 ]. Similar ideas for segmenting, sampling, and concate-nating Latin letters were proposed in [ 1 , 21 , 28 ] and patented in [ 53 ]. In [ 38 ], samples of offline Arabic segmented letters are conditionally selected and later connected directly using the horizontal connection stroke (Kashidah). 3.2.3 Modeled-connection concatenation Modeled-connection techniques add new connection liga-tures synthesized by parametric curves. Rao [ 42 ] modeled the connection between the suffix segment of a letter to the prefix segment of the subsequent letter using polynomial and Bezier curves. His results of letter to letter concatenation are reported to appear natural, provided the segments of letters are adequately extracted.

Wang et al. [ 21 ] and Xu et al. [ 49 ] developed a letter concatenation model in addition to a letter generation model. Their letter concatenation technique is similar to that of [ 42 ]: They concatenate the tail segment of a letter to the head segment of the subsequent letter (corresponding to the suffix and prefix segment in Rao X  X  work, respectively) to minimize energy in a deformable model.
 Style preserving concatenation [ 28 ] suggests connecting Latin letters according to some probabilities that reflect the writer X  X  style. Whenever it is decided that letters should be connected, the extensions (probably trimmed) are connected with interpolation. If it is decided that letters should not be connected, an ending-position, rather than a middle position, sample of the letter is used (i.e., a no-connection technique).
Cursive handwritten CAPTCHAs are produced by the concatenation of skeletonized letters at the level of the base-line [ 43 ]. They define their connection ligatures by looking at the derivative of the vertical projection. They parameter-ize ligatures and join them from the end of a letter to the body of the next letter. Table 6 summarizes some key shape-simulation works. 4 Overview of some other synthesis approaches In this section, we present techniques for handwriting synthe-sis which are non-shape simulation approaches. Movement-simulation approaches are the most common non-shape sim-ulation techniques. Movement simulation is a top-down approach to handwriting synthesis where the neuromuscular acts of writing are simulated. One approach to synthesizing handwritten data is to model strokes as oscillatory compo-nents, where the letter formation is a result of horizontal and vertical oscillations (i.e., constrained modulation). The hori-zontal oscillation and its modulation control the stroke/letter shape, and the vertical oscillation and its modulation control the letter height [ 62 ]. Motivated by this idea, Gangadhar et al. proposed a neural network model of handwriting strokes, where the stroke velocities are expressed as oscillatory neural activities. The architecture has stroke selection as the input layer and the estimated stroke velocities are represented by the output layer [ 3 ].

Oneofthemostnotablecontributionsformodelingstrokes is by Plamondon and his group [ 6  X  8 , 55 , 65  X  67 ]. The strokes are defined from the context of kinematic theory of rapid human movement as primitive movement units which can be superimposed to construct word patterns [ 16 ]. A stroke model describes the essential characteristics of the pen-tip trajectory [ 68 ]. The main idea behind the kinematic the-ory is that a neuromuscular systems involved in the pro-duction of a rapid movement can be considered as a linear system made up of a large number of coupled subsystems, and the impulse response of such system converges toward a lognormal function under certain conditions [ 66 , 67 , 69 ]. There are many models derived from this lognormal para-digm. These models can be broadly categorized into two: (i) Delta-lognormal model, which involves two neuromuscular systems (each described by a lognormal impulse response and timing properties), one agonist to, and the other antago-nist to, the direction of the movement. This model generates straight strokes and predicts all the velocity patterns observ-able in a set of strokes; and (ii) sigma-lognormal model, where the assumption is that the two neuromuscular sys-tems do not work in exactly opposite directions, and thus, the resultant velocity is described by the vectorial summa-tion of the contribution of each of the neuromuscular systems involved.

All the different models differ in their stroke generation quality depending on the number of parameters used in a given model (the simple one with three parameters to the more complex ones having up to 11 parameters) [ 55 ]. Esti-mating the parameters robustly is one of the issues in using these stroke models for handwriting synthesis. Moreover, the variabilityofhandwriting,asaresultofvaryingtheparameter values, to generate realistic text needs further investigation. There are many methods proposed to estimate the initial para-meters of thelognormal strokemodels likeINFLEX, INITRI, and the XZERO [ 68 , 70 , 71 ]. Each of the algorithms has its advantages and limitations, and the authors have proposed using hybrid versions of them as they seem complimentary to each other.

In [ 16 ], the authors presented a system for synthesizing a large database of handwriting from few specimens using the sigma-lognormal model. The system can be used to gener-ate writing samples for a single writer, as well as in multi-writer setup. The variability observed in handwriting data can be regenerated by varying the sigma-lognormal parameters around their mean values within the limits fixed by their stan-dard deviations. The factor of variability needs to be carefully fixed so as to get intelligible samples.

Inanotherapproach,timetrajectoriesoftheEnglishalpha-bet were modeled using an oversampled reverse time delay neural network (TDNN) architecture to generate outputs that can control the writing of characters with a pen [ 72 ]. The network was trained on character glyphs as a sequence of successive points in time. Three outputs provided the time sequences of signals that controlled the X and Y positions of the pen and up/down pen control.

Bayoudh et al. [ 10 ] propose using the principle of analog-ical proportion to synthesize new examples from an existing limited set of real examples. Each character is represented as a sequence of Freeman chain codes including a set of anchorage points. Experiments evaluated the improvement in the training of a set of classifiers on character recogni-tion rate as a result of increasing the size of the dataset. The results confirmed that the proposed approach is as effective as character synthesis through knowledge-based approaches in the form of image-based (scant and slat) distortions and online (speed and curvature) distortions.

Slim and Benrejeb [ 73 ] modeled the handwriting process of few Arabic letters using electromyographic signals (EMG) generated by muscles in the forearm. An RBF neural net-work with feedback and time delay learns to associate the EMG signals generated, as a character is drawn, with the sequence of pen displacements recorded in the X and Y directions. Inverse models are also described for generating the EMG signals from the recorded position signals. 5 Summary and conclusions Handwriting synthesis is the artificial generation of images that resemble human writing. Synthesis has several appli-cations such as the improvement of text recognition sys-tems, font personalization, and CAPTCHAs. The most pre-dominant applications of synthesis are those related to recognition. The most common approaches to handwrit-ing synthesis are shape simulation and movement simula-tion. Shape-simulation approaches for handwriting synthe-sis model handwritten shapes, while movement-simulation approaches model neuromuscular movements. Important aspects of handwriting synthesis, such as input/output levels, data types, and script, were discussed. Different evaluation methods were reported along with the applications that they suit. We noticed that subjective evaluation methods are used more frequently in the literature than objective ones.
The works on shape-simulation generation and concatena-tion techniques were classified and surveyed. The most com-mon unit for handwriting generation is the character. Sub-words are the most prevailing output units for handwriting concatenation. We noticed that among the techniques of syn-thesis, fusion-based generation is probably the most under-explored. Some important non-shape simulation techniques were also presented.

Some challenges and possibilities for future works were identified: Offline handwriting lacks temporal information which makes the identification of its segments more chal-lenging as compared to online handwriting. Besides, offline data have inking effects which need special treatment when processed. On the other hand, the synthesis of online data requires velocity and pressure estimation. Moreover, there is a lack of standard benchmarks for objective comparisons of different techniques. In addition, the challenging problem of segmentation is necessitated by some techniques. Not much work has been reported addressing the issue of sample selec-tion of consequent units for concatenation.
 References
