 , Yiyu Yao  X  X  and Jue Wang  X 
Useful knowledge hidden in a dataset is crucial to the suc-cess of machine learning and data mining. The knowledge may be expressed in many forms, such as association rules, classification rules, clusters, frequent patterns, etc. [1 ], [2]. Each of them has been studied deeply in the correspond-ing research field and many methods have been proposed to learn or discover them. Recently, peculiarity rules are considered as a new type of knowledge, which may be hidden in a relatively small subset of a large dataset [3], [4]. Peculiarity-oriented mining (POM) is a data mining method aiming to find these interesting peculiarity rules in this small subset [4]. It includes two tasks: identifying peculiar data and analyzing peculiar data. Peculiar data identification is to measure the peculiarity of each data by a score and select these with higher scores as peculiar data. F or the analysis of peculiar data, existing mining methods are adapted to discover interesting rules hidden in peculiar da ta by considering the problem of a small number of samples. Peculiarity factor (PF) and local peculiarity factor (LPF) are two kinds of peculiarity measure. Roughly speaking, PF and LPF of a data point are defined as some weighted sum of distances to its nearest neighbors [2], [3], [5]. In the one-dimensional case, they are called attribute PF and attribut e LPF. In the multi-dimensional case, they are called record P F and record LPF. The focus of this paper is on the theoretical analysis of record LPF and its application for classificatio n problems.

PF and LPF are two key concepts in identifying peculiar data. Studies on them have produced useful theoretical results and applications. A preliminary analysis of attrib ute PF demonstrates that it indeed reflects our intuitive un-derstanding of two properties of peculiar data [6]: they describe a relatively small number of objects; and they are very different from other objects in the dataset [2]. With attribute PF, Zhong et al. used RVER (reverse variant entity -relationship) to represent peculiar data and their concept ual relationships discovered from multiple databases [2], [4] , [5]. With record PF and using techniques from inductive logic programming [7], relational POM was proposed to combine results from relational mining and POM at record level [8]. Ohshima et al. discussed two basic tasks of relational POM, description and explanation, and developed a prototype sys -tem based on relational POM [2]. Further theoretical analys is on attribute PF was given in [3]. It was proved to be capable of characterizing the peculiarity of a one-dimensional nor mal distribution, i.e., data points with lower probability den sity function (PDF) have larger attribute PF values, but incapab le of describing the peculiarity of some other continuous one-dimensional distributions. To resolve this difficulty, a lo cal version of attribute PF called attribute LPF was introduced , and it was shown that attribute LPF can characterize the probability density function (PDF) of any continuous one-dimensional distribution at any precision. Based on attrib ute LPF, a definition of record LPF was proposed and it was used to design an effective outlier detection algorithm [3] .
All existing theoretical results on PF and LPF are at attribute level [3], [6]. It is difficult to theoretically an alyze them at record level, since the existing definitions of recor d PF and record LPF are the weighted sum of attribute ones on all attributes. In this paper, we redefine record LPF and study its property deeply. Since LPF is a local version of PF, similar improvement and property can be obtained for PF. We simply define record LPF as the sum of distances from a point to its nearest neighbors and call it distance-based record LPF (D-record LPF). D-record LPF is directly founded on a distance measure and fits for more real world problems than existing ones [2], [3]. In fact, the sum of distances from a point to its k -nearest neighbors has been adopted in some anomaly (outlier) detection techniques wit h different names such as weight [9], outlying degree [10] and so on. Though several strategies have been proposed to make the related detection techniques more efficient and effecti ve [9], [10], [11], [12], there is still a lack of rationality an alysis on using the sum as a measure of anomaly.

One contribution of this paper is to propose and prove two properties of D-record LPF. We prove that for any dataset sampled from a continuous m -dimensional distribution, data points with larger D-record LPF have lower PDF values. Furthermore, D-record LPF is proved to be capable of describing the probability density function (PDF) of any continuous m -dimensional distribution at any precision. The significance of the properties can be seen from the following three aspects. Firstly, they are the generalization of exis ting properties of attribute LPF from one-dimensional case to m -dimensional case. Secondly, they can be taken as the theoretical basis on which the sum of distances from a point to its nearest neighbors is used to detect outliers. Thirdly , the results are valuable for some machine learning and data mining problems, where the estimation or characterization of the related PDFs is needed. As a case study, we apply D-record LPF to solve classification problems.
 Another contribution of the paper is to apply D-record LPF to classification problems and propose two classifiers. By the proved properties of D-record LPF, it can be used to describe class-conditional PDFs in the Bayesian classifier . By the relationship between D-record LPF and PDF, data points can be classified by a decision rule similar to the Bayesian decision rule, and the associated algorithm is cal led LPF-Bayes classifier. It has some intrinsic connection to the Bayes classifier. We also give its kernelized version and call it kernel LPF-Bayes classifier. The validity of the proposed algorithms is demonstrated by their results on several benchmark datasets.

The rest of the paper is organized as follows. In Section 2, we recall some concepts and results on PF and LPF. The concept of distance based record LPF is proposed in Section 3 and its two desirable properties are shown. In Section 4, based on the Bayesian classifier, the proposed record LPF is applied for binary classification problems, and an LPF-Bayes classifier and its kernelized version are presented. I n Section 5, we report some results of the proposed classifiers on several benchmark datasets. Conclusions and discussion s are given in Section 6.

Denote a one-dimensional variable by a small letter, and an m -dimensional variable by a small letter in bold face. We suppose that p ( ) is the probability density function (PDF) of a continuous random variable,  X  ( x , z ) = k x  X  z k  X  0 ) is a  X  X istance X  measure (not necessarily satisfying the triangle inequality) on R m , and X = { x 1 , x 2 , , x n dataset with x i = ( x i 1 , x i 2 , , x im ) described by attributes A 1 , A 2 , , A m .

Peculiarity factor (PF) is a concept used to describe the peculiarity of a point in peculiarity oriented mining (POM) . Two levels of PF can be identified, representing attribute PF (denoted by PF a ) for one-dimensional data and record PF (denoted by PF r ) for multi-dimensional data. Zhong et al. defined the attribute PF of a attribute value x il as the sum of distances from other attribute values [4], i.e., In order to gain more insights on attribute PF, it was generalized to a continuous one-dimensional distribution by using the integral to replace the sum [3], i.e., For a data sampled from a one-dimensional normal distribu-tion, its attribute PF was proved to strictly increase with i ts distance to the mean of the distribution [3]. It follows that attribute PF correctly reflects our intuitive interpretati on of the peculiarity for the distribution.

After attribute PFs on all attributes are known, one way to calculate the record PF of a multi-dimensional data point is the weighted q -norm of its attribute PFs [2], [3], i.e., PF r ( x i ) = where x i  X  X ,  X  l is the weight of A l , and PF a ( x il by Eq. (1). But the weighted q -norm makes the theoretical analysis on the record LPF quite difficult, and there is still few theoretical result on it.

Attribute PF can describe the PDF of a one-dimensional normal distribution accurately. However, it cannot do so fo r some other continuous one-dimensional distributions (e.g ., Gaussian mixture distributions) [3]. Fortunately, it can b e improved as attribute LPF (denoted by LPF a ) by using a local mean [3], i.e., where c  X  (0 , 1] is a constant and a satisfies R x + a x  X  a c . The constant c is the percentage of data used to describe the peculiarity of a point. If c = 1 , we have a = +  X  and attribute LPF is equivalent to attribute PF. It has been prov ed that, for a one-dimensional normal distribution the attrib ute LPF with any c  X  (0 , 1] strictly increases with the distance to its mean [3]. Therefore it has the same power as attribute PF. For any continuous one-dimensional distribution, attribu te LPF can describe its PDF at any precision, i.e., for any  X  &gt; 0 and x, z with p ( x )  X  p ( z ) &gt;  X  , there exists a constant The constant c is determined by  X  . Generally speaking, the smaller  X  is, the smaller the constant c , and hence the more accurate the description. To approximate attribute LPF for a dataset, the integral in Eq. (4) has been replaced by a sum in k -nearest neighbors [3], i.e., where N k ( x il ) is the set of k nearest neighbors of x attribute A l .

Record LPF can be given by a manner easier than the one in Eq. (3). If attribute LPF can accurately describe the peculiarity of a data point on each attribute and its peculia rity has linear relationships to its peculiarity on each attribu te, the record LPF of the data point can be simply defined by the weighted sum of these attribute LPFs [3], i.e., where  X  l is the weight of A l , and LPF k l a ( x il ) is given by Eq. (5). To differentiate the record LPF given by Eq. (6) with the one we will define in the next section, we call it attribute based record LPF (A-record LPF). A-record LPF has be used to detect outliers and an algorithm called LPF-Outlier was proposed [3], whose performance on some benchmark datasets is better than many popular outlier detection methods, such as Active-Outlier [13], LOF [14], FindFPOF [15], Feature Bagging [16], KNN [17].
 Since there are attribute LPFs between data and A-record LPF (including record PF given by Eq. (3)), it is hard to analyze the property of A-record LPF theoretically and it has not been studied extensively.

More importantly, A-record LPF is not effective when some attributes have quite complex relationships. For exam -ple, we consider a 2-dimensional Gaussian mixture distribu -tion P = 1 4 ( N ( u 1 ,  X  ) + N ( u 2 ,  X  ) + N ( u 3 ,  X  ) + N ( u where u 1 = (0 , 0) , u 2 = (6 , 6) , u 3 = (17 , 0) , u 4 and  X  = (8 , 2; 2 , 9) . We plotted 2000 data points sampled from the distribution and isoclines of the distribution in Fig. 1, where the four Gaussian components are denoted by A , B , C and D , respectively. It can be seen that the attribute LPF for A is mainly determined by data points from A and D on attribute A 1 , as well as A and C for attribute LPFs on A 2 . This shows that the A-record LPFs of points sampled from component A depend to a higher degree on points from C and D than from B . But from Fig. 1, it can be seen that nearest neighbors of data points from A are mainly in A and B . Hence their record LPFs should depend more on points from B rather than C or D . That is to say, A-record LPF is incapable in describing the peculiarity of the distribution. The reason is that the neighborhood of the se data points cannot be simply embodied by neighborhoods on each attribute.

To solve this problem, we define record LPF directly on a distance measure and call it distance-based record LPF (D-record LPF). Simply speaking, D-record LPF is given by the sum of distances from a point to its nearest neighbors. If the distance measure can accurately indicate the relationship of data points, D-record LPF is able to describe the peculiarit y of the distribution effectively. Furthermore, theoretica l anal-ysis on D-record LPF is easier than A-record LPF, which has been studied and applied in outlier detection problems in [3 ]. In the following discussion D-record LPF is also denoted by LPF r .

Concretely, D-record LPF for a continuous m -dimensional distribution is defined as follows.
 Definition 1. Suppose that x is a data point sampled from an m -dimensional distribution with continuous PDF p ( x ) and c  X  (0 , 1] is a constant, then the D-record LPF of x is given by where U ( x ,  X  ) = { t |  X  ( x , t )  X   X  } and  X  satisfies R
The constant c controls the size of the neighborhood used to describe the peculiarity of a point. If c = 1 , the neighborhood would be the whole R m . D-record LPF can be seen as the generalization of attribute LPF from one-dimensional distributions to m -dimensional distributions. Furthermore, D-record LPF has good properties similar to attribute LPF.

Roughly speaking, for two points sampled from an m -dimensional distribution, the one with lower PDF has larger D-record LPF values. The accurate description of the prop-erty is given by the following theorem.
 Theorem 1. Suppose that x 0 and z 0 are sampled from an m -dimensional distribution with continuous PDF p ( x ) . If the inequality p ( x 0 ) &gt; p ( z 0 ) holds, there exists c  X  (0 , 1] such that for any  X  c  X  (0 , c ) , the inequality LPF  X  c there exist  X  1 ,  X  2  X  R satisfying | p ( x 0 )  X  p ( t ) | &lt; respectively. Hence for all t  X  U ( x 0 ,  X  1 ) and s  X  U ( z the inequality p ( t ) &gt; p ( s ) holds.

Let c = min { R U ( x  X  c  X  (0 , c ) , let &gt; 0 ,  X  &gt; 0 satisfy R R
U ( z 0 , X  ) p ( t ) d t =  X  c , then we have  X   X  1 ,  X   X   X  and  X  &gt; . Denote LPF  X  c r ( x 0 ) = R U ( x invariant, by letting h = t + x 0  X  z 0 we can get LPF  X  c R R all t  X  U ( x 0 , ) , by the First Integral Mean Value Theorem, we know that there exists a b  X  U ( x 0 , ) such that Since  X  ( x 0 , b ) &gt;  X  ( x 0 , t ) holds for all t  X  U ( x U ( x 0 , ) , we have This shows that for any  X  c  X  (0 , c ) the inequality
Theorem 1 can be easily generalized to describe n data points. For a dataset X = { x 1 , x 2 , , x n } , without loss of generality, we suppose that p ( x 1 ) &gt; p ( x 2 ) &gt; &gt; p ( x Then there exists c  X  (0 , 1] such that for any  X  c  X  (0 , c ) , the inequalities LPF  X  c r ( x 1 ) &lt; LPF  X  c r ( x 2 hold. Hence for any dataset, data points with smaller PDF values have larger D-record LPF. It follows that D-record LPF correctly reflects our intuitive interpretation of pecu -liarity.
 Similar to attribute LPF, by selecting a proper c D-record LPF can describe the PDF of a continuous m -dimensional distribution at any precision, which is quantitatively cha r-acterized by the notion of  X   X  -sensitive description X  [3]. Here the definition of  X   X  -sensitive description X  is extended from a one-dimensional distribution to an m -dimensional distribution.
 Definition 2. Suppose that P is an m -dimensional distri-bution with continuous PDF p ( x ) , h is a description of P and  X  &gt; 0 is a given constant. If for all x , z sampled from holds, h is called an  X  -sensitive description of P .
The parameter  X  should be adjusted according to the problem requirement or prior knowledge. The smaller  X  is, the more accurate the description, which implies that more data is needed. The effect of all data points with the PDF value smaller than  X  is ignored in an  X  -sensitive description.
With the notion of  X  -sensitive description, we have the following property of D-record LPF.
 Theorem 2. For an m -dimensional distribution P with continuous PDF p ( x ) and any given  X  &gt; 0 , there exists a constant c  X  (0 , 1] such that for any  X  c  X  (0 , c ) , LPF an  X  -sensitive description of P .
 we know that the maximum value of p ( t ) on R m exists and it is denoted by z . Without loss of generality, we may suppose that the given  X  satisfies  X  &lt; z . Denote the inverse image of [  X , z ] under p by E , i.e., E = { t |  X   X  p ( t )  X  z, t  X  R m } . Obviously, it is closed and bounded. Hence there exists a  X  &gt; 0 such that for all t ,  X  t  X  E with k t  X   X  t k  X   X  , the R U ( x , X  ) p ( t ) d t , then  X  ( x ) can reach its minimum value on E . Denote the minimum value by c = min x  X  B  X  ( x ) , then it is easy to see that c &gt; 0 .

For any x 1 , x 2 with p ( x 1 )  X  p ( x 2 ) &gt;  X  , we have of Theorem 1, we know that for any  X  c  X  (0 , c ) , the inequality defined above and any  X  c  X  (0 , c ) , LPF  X  c r is an  X  -sensitive description of P .

Theorem 2 generalizes the property of attribute LPF from a one-dimensional distribution to the m -dimensional case. From Theorem 1 and Theorem 2, it can be seen that D-record LPF can correctly reflect our intuitive interpretati on of peculiarity, as well as giving a characterization of the P DF of an m -dimensional distribution. The characterization is valuable in many problems, where there are only some data points sampled from an unknown distribution and the hidden PDF is needed, e.g., in the Bayesian classifier. Detailed discussion on applying D-record LPF to solve classification problems will be given in the next section.

For a finite dataset, a neighborhood sum can again be used to replace the integral in Eq. (7). Then the D-record LPF in Definition 1 can be approximated as follows.
 Definition 3. Suppose that k is a given natural number and X = { x 1 , x 2 , , x n } is a dataset of n points sampled from a continuous m -dimensional distribution. For the dataset, the D-record LPF of x i is given by where N k ( x i ) is the set of k -nearest neighbors of x set X .

D-record LPF for a dataset is just the sum of distances between a point and its k -nearest neighbors. It is quite simple, but it is the discrete form of D-record LPF for continuous distributions and has theoretical basis given b y Theorem 1 and Theorem 2.

In fact, the sum of distances from k -nearest neighbors has been used as an anomaly score determining the degree on which a point is considered as an anomaly. Anomaly (outlier) detection techniques based on the anomaly score are called distance-based techniques or nearest neighbors based techniques, which is one of the four categories of major anomaly detection methods as characterized in [18]. Several algorithms have been proposed to reduce the time complexity of finding the k -nearest neighbors by using the k -NN score to detect outliers. Eskin et al. used canopy clustering as a means of breaking down the space into smaller subsets so as to remove the necessity of checking every data point [11], [12]. Angiulli and Pizzuti termed the k -NN score the weight of a point and linearized the search space through the Hilbert space filling curve [9]. Zhang and Wang called the k -NN score the outlying degree (OD in short) of a point and used two heuristic pruning strategies t o speed up finding k -nearest neighbors in a subspace (subsets of features) [10]. In addition, He and Wang adopted the k -NN score to detect fault in semiconductor manufacturing processes [19].
 Though these anomaly detection algorithms based the k -NN score have been studied quite deeply and performed well on many synthetic and real world datasets, there is stil l a lack of theoretical analysis on the rationality of using th e k -NN sum as an anomaly score. We take the k -NN score (D-record LPF for a dataset) as a discrete approximation of the D-record LPF for continuous distributions. It is then connected to the theoretical analysis on D-record LPF for a continuous distribution. Roughly speaking, points with lo wer PDF values have larger k -NN scores. Points with lower PDF values should be considered more anomalistic than that with larger ones. From the view point of the statistical analysis , the k -NN score is an effective anomaly score. That is to say, D-record LPF for a continuous distribution and its properti es in Theorem 1 can be seen as the theoretical basis of distance-based anomaly detection techniques with the k -NN score. The property also reveals that there exists close relation between distance based anomaly detection techniques and statistical anomaly detection techniques. As far as we know , this is the first study on the theoretical basis of these dista nce based techniques.

Since D-record LPF can characterize the PDF of a contin-uous distribution quite accurately, the relationship betw een the D-record LPF and PDF value of a data point can be used to solve problems where the PDF of the hidden distribution is needed. As a case study, we will apply D-record LPF to classification problems.

For classification problems, if the prior probability of each category and all class-conditional PDFs are known, one can classify a point by the Bayesian classifier, the theoretically optimal classifier. However, the main proble m to apply Bayesian classifier is that they need the class-conditional PDFs, which are very difficult to be estimated only from the training set, especially for multi-dimension al distributions. Therefore, many advanced classifiers, such as neural networks [20], boosting [21], support vector machin e (SVM) [22] etc., solved the classification problem directly and advise against estimating the PDF of a distribution.
Peculiarity analysis is used to solve the problem. We only consider binary classifications and denote class labels by  X  1 = 1 and  X  2 =  X  1 . Suppose that x is a data point sampled from a m -dimensional distribution with continuous PDF p ( x ) . Denote prior probabilities by P (  X  1 ) and P (  X  and class-conditional PDFs by p ( x |  X  1 ) and p ( x |  X  tively.
 By Bayes X  formula, a posterior probability is given by The Bayesian decision rule is based on minimizing the probability of error is: If P (  X  s | x ) = max l =1 , 2 P (  X  the label of x is  X  s . From Eq. (9), it can be seen that for a data point, its posterior probabilities for each class is mainly determined by the numerator p ( x |  X  i ) P (  X  i ) , since the denominator for both classes are same. Prior probabilities are always estimated by proportions of each class in the trainin g set. So the key point in designing a Bayesian classifier is to estimate class-conditional probabilities p ( x |  X  l ) , l = 1 , 2 . A. LPF-Bayes Classifiers
Using the relationship between D-record LPFs and the class-conditional PDF values, a classification algorithm called LPF-Bayes classifier is designed in the light of the Bayesian classifier.

For each data point, its two D-record LPFs of each category are calculated. From Theorem 1 and Theorem 2, D-record LPF can characterize the relation between D-record LPFs and class-conditional PDF values for one category, but the peculiarity characterization may not be comparable among different classes. To neutralize this effect, we mult i-ply a positive factor to D-record LPFs for one category. The factor can be learned by minimizing the training error. The normalized D-record LPFs for different categories would have comparable values. For a data point, the smaller the normalized D-record LPF, the larger the corresponding clas s-conditional PDF value. Then a decision rule similar to the Bayesian rule can be adopted to classify it.

Suppose that the training dataset is { ( x 1 , y 1 ) , ( x { x i  X  X | y i =  X  l } . For a data point x 0 , its D-record LPF on X l is calculated by where N l k ( x 0 ) is the set of x 0  X  X  k -nearest neighbors in X And LPF k r ( x 0 |  X  l ) is called x 0  X  X  class-conditional D-record LPF for class  X  l .

A scale factor w &gt; 0 is introduced on the class-conditional D-record LPFs for category  X  2 . After the normalization, the class-conditional D-record LPFs are mula, x 0  X  X  posterior D-record LPFs are given by
Since both w and the prior probabilities P (  X  l ) need to be learned from training data, the prior probabilities can b e absorbed into w . Therefore the posterior D-record LPFs can be simplified as LPF k r (  X  1 | x 0 ) = and LPF k r (  X  2 | x 0 ) = Once w is learned from training data, without considering the impact of the same denominator in posterior D-record LPFs, the LPF-Bayes decision rule based on the minimum probability of error is: If LPF k r ( x 0 |  X  1 ) &lt; LPF label x 0 as  X  1 , otherwise as  X  2 .

To learn the optimal w , we minimize the LPF based classifier X  X  error on training data. For a training point x with  X  1 = 1 ,  X  2 =  X  1 in mind we can define a loss function g ( x i ) = (LPF k r ( x i |  X  2 ) w  X  LPF k r ( x i |  X  1 Bayes classifier labels x i correctively, we have g ( x i otherwise g ( x i ) &lt; 0 . To minimize the training error is to minimize the sum of | g ( x i ) | on misclassified training data, i.e., In order to transform it to a form easier to solve, slack variables  X  i are introduced into optimization problem (11) to obtain, min s . t . (LPF k r ( x i |  X  2 ) w  X  LPF k r ( x i |  X  1 )) y Optimization problem (12) is a linear programming problem and can be easily solved by standard methods.
 To avoid overfitting on training data, we penalize on the L 1 norm of w in eq. (12). In addition, if more than k training items on a point x i belongs to the class  X  2 , we remove the training items on that point with label  X  1 and the corresponding constraint from optimization problem (12). Also, it is clear that for training data with LPF k r ( x the constraint on x i will have nothing to do with w , and hence can be removed. Therefore, we can solve optimal problem (12) only under constraint conditions on training data with LPF k r ( x i |  X  2 ) &gt; 0 . Denote the number of such training data by r , we obtain min s . t . (LPF k r ( x i |  X  2 ) w  X  LPF k r ( x i |  X  1 )) y where  X  is a parameter controlling the weight of the L 1 norm. The constant r normalizes  X  to be equivalent with the mean loss of misclassified training data. Its advantage i s that for datasets with different size, the range of the optim al Algorithm 1 LPF-Bayes classifier (LPFBC) 1: Input dataset { ( x 1 , y 1 ) , ( x 2 , y 2 ) , , ( x n 2: Output LPF-Bayes classifier f . 3: For i = 1 to n do 4: Calculate the class-conditional D-record LPF values 5: End for 6: Solve linear programming problem (13) for w . 7: For a test point x 0 , calculate its normalized class-8: Classify x 0 by the LPF-Bayes classifier f . f ( x 0 ) =  X  parameter  X  is relatively fixed and its selection is relatively easy.

After the optimal factor w is obtained from (13), we can classify test data by the LPF-Bayes classifier. For a data point x 0 , its normalized class-conditional D-record LPFs, it is labeled as the class with the smaller one. The detailed LPF-Bayes classifier algorithm is given in Algorithm 1.
The computational complexity of a LPF-Bayes classifier depends mostly on the speed of computing k -nearest neigh-bors and solving the linear programming problem, which is a well-researched topic and several fast algorithms exist [ 9], [10], [12], [23], [24], [25], [26].
 B. Kernel LPF-Bayes Classifiers
The kernel trick is widely used in statistical machine learning. It has been proved that with a proper kernel func-tion, any dataset can be mapped to be linearly separable in a high-dimensional kernel space. Many algorithms become more powerful by applying this trick. Examples are kernel SVM [22], kernel principal component analysis (PCA) [27], kernel Fisher discriminant analysis (FDA) [28] and so on. In order to improve the performance of LPF-Bayes classifiers, we give its kernelized implementation and call it kernel LPF -Bayes classifier. Similar to other learning algorithms with the kernel trick, in a kernel LPF-Bayes classifier, data poin ts are mapped from the original m -dimensional space Z into a high-dimensional space  X  ( Z ) by a kernel function  X  , then D-record LPF are calculated in  X  ( Z ) and test data are classified by the LPF-Bayes classifier. For a data point x 0 , its D-record LPF for the dataset  X  ( X ) = {  X  ( x 1 ) ,  X  ( x 2 ) , ,  X  ( x given by LPF k r (  X  ( x 0 )) = X where N k (  X  ( x 0 )) consists of k -nearest neighbors of  X  ( x in  X  ( X ) .
 Algorithm 2 Kernel LPF-Bayes classifier (Kernel LPFBC) 1: Input dataset { ( x 1 , y 1 ) , ( x 2 , y 2 ) , , ( x n 2: Output kernel LPF-Bayes classifier f . 3: For i = 1 to n do 4: Calculate kernel class-conditional D-record LPF val-5: End for 6: Solve linear programming problem (15) for w . 7: For a test point x 0 , calculate its normalized kernel 8: Classify x 0 by the kernel D-record LPF-Bayes clas-
To simplify notations, we use the common Euclidean distance  X  ( x , z ) = k x  X  z k 2 as the distance measure and Radial basis function (RBF) as the kernel function. Then for any x , z  X  X  , we have Distances involved in kernel D-record LPF LPF k r (  X  ( x can be calculated via the corresponding distances in Z . Since  X  is strictly increasing with k x  X  z k , we have that x is a k -nearest neighbor of z in Z if and only if  X  ( x ) is a k -nearest neighbor of  X  ( z ) in  X  ( Z ) . Hence x 0  X  X  kernel D-record LPF on the dataset  X  ( X ) can be calculated by
LPF k r (  X  ( x 0 )) = X
Similar to Eq. (10), kernel class-conditional D-record LPF can be calculated by LPF k r (  X  ( x 0 ) |  X  l ) = X And similar to optimization problem (13), a kernel LPF based classifier learns the optimal w by solving
Once the optimal w is obtained, we classify a test data x 0 to the class with a smaller normalized ker-nel class-conditional D-record LPF, LPF k r (  X  ( x 0 ) |  X  LPF k r (  X  ( x 0 ) |  X  2 ) w . The detailed kernel LPF-Bayes classi-fier is given in Algorithm 2.
In the following experiments, we selected  X  l = 1 and k 1 = k 2 = = k m to calculate A-record LPF, and the common Euclidean distance as the distance measure to cal-culate attribute LPF and D-record LPF. We also normalized all A-record LPFs and D-record LPFs into range (0,1). A. A Synthetic Dataset
To accurately demonstrate the fact that D-record LPF can describe the PDF of some continuous distributions more effectively than A-record LPF, we sampled 2000 data points from the distribution plotted in Fig. 1, and calculated thei r record LPFs with different nearest neighbors parameters k . Then we sorted these PDF values in ascending order and sort two kinds of record LPFs in descending order, and calculated Kendall X  X  Tau distances between the ordered PDF values and D-record LPFs and A-record LPFs, respectively. The Kendall X  X  Tau distance is a commonly used measure to evaluate the consistency of two rankings in learning to rank problems of machine learning [3], [29]. The range of the measure is [-1, 1]. The more consistent the indexes of the two sequences are, the larger the measure is.

For each selected nearest neighbors parameter k , we did the experiment 30 times. The means and variances of the two Kendall X  X  Tau distances with different k are plotted in Fig. 2. From the figure, it can be seen that for any nearest neighbors parameter k , the Kendall X  X  Tau distance between the ordered PDF values and ordered D-record LPFs is always larger than that between the ordered PDF values and ordered A-record LPFs. This shows that the peculiarity given by D-record LPF is more consistent with the PDF of the distribution than A-record LPF. It follows that D-record LPF can describe the PDF more accurately than A-record LPF.
 B. Benchmark Datasets
We performed the proposed LPF-Bayes classifier and ker-nel LPF-Bayes classifier on benchmark datasets composed by R  X atsch et al. [21]. Their results are compared with some popular classification algorithms, including the single RB F classifier, AdaBoost with RBF-Network, LP Reg-AdaBoost, QP Reg-AdaBoost, AdaBoost Reg, SVM with RBF-Kernel and Kernel Fisher Discriminant [21], [28], [30].

The benchmark datasets consist of 13 artificial and real world datasets from the UCI, DELVE and STATLOG bench-mark repositories: banana, breast cancer (breast), diabet es, german, heart, image segment, ringnorm, flare solar, splice , thyroid, titanic, twonorm, waveform. Some of the problems are not binary classification problems originally, hence a r an-dom partition into two classes was used. 100 partitions (20 partitions for image and splice) were generated into traini ng set (about 60%) and test set (about 40%). Some information about these datasets are given in Table I, including numbers of attributes, training data, test data and realizations. T he processed data, a short description, the splits into the rea l-izations and experiment results of some popular classifiers are available at http://ida.first.fraunhofer.de/project s/bench/.
On each of these datasets, we implemented LPF-Bayes classifier and kernel LPF-Bayes classifier. The average test errors over these 100 (or 20) runs and the corresponding standard deviations are given in Table II. Results of other algorithms given in Table II directly come from the infor-mation of the benchmark datasets available together with datasets. For each dataset, the best results are in bold face . The last row of Table II illustrates the numbers of datasets o n which a classification algorithm performed best. The values of parameters  X  ,  X  2 and k , under which the LPF-Bayes classifier and kernel LPF-Bayes classifier output results in Table II, are given in Table III. From Table II, it can be seen that in most cases the LPF-Bayes classifier and kernel LPF-Bayes classifier perform better than the single RBF classifier, AdaBoost with RBF-Network, LP Reg-AdaBoost and QP Reg-AdaBoost, and competitively to AdaBoost Reg, SVM with RBF-Kernel and Kernel Fisher Discriminant. In summary, evaluated from the number of datasets with best performance, kernel LPF-Bayes classifier did best, and LPF-Bayes classifier, AdaBoost Reg and SVM with RBF-Kernel did second best.

We have studied a new definition of record LPF from theoretical analysis and experimental research. It has bee n proved that for data point sampled from a continuous m -dimensional distribution, the smaller its PDF value the lar ger its record LPF. It follows that the record LPF can describe the peculiarity and PDF of these distributions accurately. Based on these properties, the record LPF is used to solve classification problems. Motivated by the Bayesian decisio n rule, we propose an LPF-Bayes classifier and its kernel version, kernel LPF-Bayes classifier. They are similar to Bayesian classifier but do not estimate class-conditional PDFs. Experiment results on a synthetic dataset indicate that the proposed record LPF can describe the PDF of a continuous m -dimensional distribution more accurately than the existing one, and results on 13 benchmark datasets demonstrate the effectiveness of the proposed classifiers.
As future work for the proposed classifiers, we will validate their effectiveness on more datasets and show how the nearest neighbor parameter in D-record LPF affects their classification accuracy. Peculiarity analysis for mu lti-class classifications is not considered in this paper. In the case, there may not exist a simple loss function which can translate the idea of the LPF-Bayes classifier into a linear programming problem. Hence the proposed record LPF cannot be directly applied to solve multi-class classificat ions. But similar to SVM for multi-class problems, multi-class LPF-Bayes classifiers might be implemented by several two-class LPF-Bayes classifiers with a one-versus-all strategy or one-versus-one strategy. These strategies still need to be studied further.

The first author would like to thank Dr. Fuxin Li for his many insightful discussions and suggestions. This work was partially supported by the National Science Foundation of China (No. 60673015 and No. 60905027), the Startup Foundation for Doctors of Beijing University of Technology (No. X0007011200902) and the grant-in-aid for scientific research (No. 18300053) from the Japanese Ministry of Education, Culture, Sports, Science and Technology.
