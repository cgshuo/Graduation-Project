 Multi-task feature learning has been proposed to improve the generalization performance by learning the shared fea-tures among multiple related tasks and it has been suc-cessfully applied to many real-world problems in machine learning, data mining, computer vision and bioinformatics. Most existing multi-task feature learning models simply as-sume a common noise level for all tasks, which may not be the case in real applications. Recently, a Calibrated Multi-variate Regression (CMR) model has been proposed, which calibrates different tasks with respect to their noise levels and achieves superior prediction performance over the non-calibrated one. A major challenge is how to solve the CMR model efficiently as it is formulated as a composite opti-mization problem consisting of two non-smooth terms. In this paper, we propose a variant of the calibrated multi-task feature learning formulation by including a squared norm regularizer. We show that the dual problem of the proposed formulation is a smooth optimization problem with a piece-wise sphere constraint. The simplicity of the dual problem enables us to develop fast dual optimization algorithms with low per-iteration cost. We also provide a detailed conver-gence analysis for the proposed dual optimization algorithm. Empirical studies demonstrate that, the dual optimization algorithm quickly converges and it is much more efficient than the primal optimization algorithm. Moreover, the cali-brated multi-task feature learning algorithms with and with-out the squared norm regularizer achieve similar prediction performance and both outperform the non-calibrated ones. Thus, the proposed variant not only enables us to develop fast optimization algorithms, but also keeps the superior prediction performance of the calibrated multi-task feature learning over the non-calibrated one.
 H.2.8 [ Database Management ]: Database Applications X  Data Mining Multi-task learning; feature selection; calibration; dual prob-lem; accelerated gradient descent
Multi-task feature learning aims to improve the general-ization performance by learning multiple related tasks to-gether and exploring the shared features among tasks. It has received a lot of interests and has been successfully ap-plied to a wide range of applications including gene data analysis [18], breast cancer classification [34], neural seman-tic basis discovery [21] and disease progression prediction [36, 37]. Most existing multi-task feature learning models can be formulated as a regularized optimization problem and they usually focus on how to design a good regularizer to capture the underlying shared features among tasks; ex-amples include group lasso multi-task feature learning [1, 19, 23, 25, 29, 33, 34], tree-guided group lasso multi-task feature learning [18], composite regularized multi-task fea-ture learning [14, 16] and non-convex regularized multi-task feature learning [13, 15].

However, most existing multi-task feature learning models simply assume a common noise level for all tasks, which may not hold in real applications. Moreover, theoretical analysis in Lounici et al. [26] shows that, to achieve the optimal pa-rameter estimation error bounds, the regularized parameter should be chosen in proportion to the maximum standard deviations of the noise for all tasks. In practice, the standard deviations of the noise are unknown or very difficult to esti-mate, which makes the parameter tuning quite challenging. To this end, Liu et al. [22] propose a Calibrated Multivari-ate Regression (CMR) model which calibrates each task by employing different noise levels for all tasks and achieve su-perior prediction performance over the non-calibrated one. Moreover, their theoretical analysis shows that the CMR model can achieve the optimal parameter estimation error bounds by properly tuning the regularized parameter which does not depend on the standard deviations of the noise for all tasks. This makes the parameter tuning of the CMR model much more insensitive to the noise levels. However, one major challenge in the practical use of the CMR model is that it is formulated as a composite optimization prob-lem consisting of two non-smooth terms, which makes the optimization problem challenging to solve.

In this paper, inspired by the great success of the elastic net [38], we propose a variant of the calibrated multi-task feature learning formulation by including a squared norm regularizer. The major contributions of this paper include: (1) We show that the dual problem of the proposed for-m ulation is a smooth optimization problem with a piece-wise sphere constraint. (2) The simplicity of the dual prob-lem enables us to develop fast dual optimization algorithms with low per-iteration cost. (3) We provide a detailed con-vergence analysis for the proposed algorithm. (4) Experi-mental results demonstrate that, the dual optimization al-gorithm quickly converges and it is much more efficient than the primal optimization algorithm. Moreover, the calibrated multi-task feature learning algorithms with and without the squared norm regularizer achieve similar prediction perfor-mance and both outperform the non-calibrated ones. These results demonstrate that the proposed variant not only en-ables us to develop fast optimization algorithms, but also keeps the superior prediction performance of the calibrated multi-task feature learning over the non-calibrated one.
The rest of the paper is organized as follows: We intro-duce preliminaries in Section 2. We propose a variant of the multi-task feature formulation and develop fast optimization algorithms in Section 3. We report experimental results in Section 4 and we conclude in Section 5.
Assume that we are given m learning tasks associated with where X i  X  R n i  X  d is the data matrix and y i  X  R n i is the re-sponse vector for the i -th task, respectively; w  X  i  X  R the i -th task, respectively; each entry of  X  i is sampled from the normal distribution with mean zero and standard devi-ation  X  i . The ordinary (non-calibrated) multi-task feature learning model is formulated as the following problem: where k  X  k is the Euclidean norm; W = [ w 1 ,  X  X  X  , w m ]  X  R d  X  m is the weight matrix with the i -th column w i  X  R d being the weight vector for the i -th task;  X  &gt; 0 is a regu-larized parameter; r ( W ) is a model-specific regularizer (such as group lasso, tree-guided group lasso, a composite regular-izer and a non-convex regularizer). To calibrate each task by considering the different noise levels of all tasks, Liu et al. [22] propose to use the square-root function [6, 11] in-stead of the least squares function, resulting in the following Calibrated Multivariate Regression (CMR) model: Liu et al. [22] interpret the CMR model in Eq. (2) as the following weighted regularized least squares problem: where 1  X  we do not have any prior knowledge on  X  i  X  X ,  X  i is chosen as Thus, using the weight defined in Eq. (4), the CMR model in Eq. (2) calibrates different tasks via solving the weighted regularized least squares model in Eq. (3). Although the Robust Feature Selection (RFS) algorithm in [28] also uses a similar square-root loss, it is quite different from the CMR model in Eq. (2) that RFS  X  X alibrates X  different samples but not different tasks via the square-root loss. Notice that the regularizer and the loss function in Eq. (2) are usually both non-smooth, which makes the optimization problem chal-lenging to solve.
Inspired by the great success of the elastic net [38], we pro-pose a variant of the calibrated multi-task feature learning model by adding a squared norm regularizer as follows: where k W k 1 , 2 = of W ; k W k 2 F = entry of W ;  X  1 ,  X  2 &gt; 0 are regularized parameters. The major benefit of the proposed formulation is that we can derive a smooth dual optimization problem with a piece-wise sphere constraint. The simplicity of the dual problem enables us to develop fast optimization algorithms with low per-iteration cost. Moreover, the proposed variant keeps the superior prediction performance of the calibrated multi-task feature learning over the non-calibrated one.
We first derive the dual problem of the optimization prob-lem in Eq. (5) and then present key properties of the dual problem. Finally, we propose fast algorithms to solve the dual problem and provide detailed convergence analysis ac-cordingly. Let z i = X i w i  X  y i . Then Eq. (5) is equivalent to the following constrained optimization problem: the strong duality [9] holds for the optimization problem in Eq. (6). We can write down the Lagrange function of the above problem as follows: the Lagrange multiplier corresponding to the constraint z X w i  X  y i . Minimizing L ( W, z ,  X  ) with respect to W and z , we obtain the objective function of the dual problem of Eq. (6) as follows:  X  D (  X  ) = min where Thus, we obtain the dual problem of Eq. (6) as follows: where the objective function is D (  X  ) = min
We show that the optimization problem in Eq. (11) has nice properties which enable us to develop fast algorithms for solving the dual problem in Eq. (10).

Theorem 1. The optimization problem in Eq. (11) has a unique solution W (  X  ) . Moreover, D (  X  ) is continuously dif-ferentiable and L -Lipschitz continuous gradient. Specifically, the gradient of D (  X  ) is  X  X  (  X  ) = [( X 1 w 1 (  X  )  X  y 1 ) T ,  X  X  X  , ( X m w m (  X  )  X  y where w i (  X  ) is the i -th column of W (  X  ) . The Lipschitz con-stant of  X  X  (  X  ) is where  X  max ( X i ) is the largest singular value of X i .
Proof. Denote the objective function of the optimization problem in Eq. (11) as f ( W,  X  ) =  X  1 k W k 1 , 2 +  X  2 Obviously, f ( W,  X  ) is strongly convex and coercive with re-spect to W . Hence, the optimization problem in Eq. (11) has a unique solution W (  X  ).

To prove the remaining part of Theorem 1, we rewrite f ( W,  X  ) in the following compact form: f ( W,  X  ) =  X  1 k W k 1 , 2 +  X  2 where Tr(  X  ) denotes the trace norm and Denote Then we have D (  X  ) = min where g  X  (  X  ) is the conjugate function of g (  X  ). Recalling the definition of g (  X  ) in Eq. (15), we know that g (  X  ) is a proper, lower semi-continuous and convex function. Thus, continuous and convex function. Moreover, g (  X  ) is strongly convex with the parameter  X  2 . Thus, by Proposition 12.60  X  g  X  (  X  ) is Lipschitz continuous with constant 1 / X  2 . Hence, D (  X  ) is continuously differentiable. Noting that W (  X  ) is the minimizer of the optimization problem in Eq. (11), we have and hence According to Corollary 23.5.1 in [30], we have  X  U (  X  )  X  gether with Eq. (16) and the definition of U (  X  ) implies that where with  X  i g  X  (  X  U (  X  )) being the partial derivative of g with respect to the i -th column of  X  U (  X  ). Thus, Eq. (12) immediately follows from Eq. (17). Moreover, for any  X  ,  X   X  R Recalling that  X  g  X  (  X  ) is Lipschitz continuous with constant 1 / X  2 , we have k X  X  (  X  )  X  X  X D (  X  ) k  X   X  max ( X ) k s (  X  )  X  s (  X  ) k w here  X  max ( X ) is the largest singular value of X . By notic-ing that X is a block diagonal matrix, we have k X  X  (  X  )  X  X  X D (  X  ) k  X  T hat is,  X  X  (  X  ) is Lipschitz continuous with constant L de-fined in Eq. (13).
 Remark 1. The squared norm regularizer  X  2 2 k W k 2 F i n Eq. (5) is critical for the establishment of Theorem 1 and hence is crucial to develop fast optimization algorithms on solving the dual problem in Eq. (10). Based on Theorem 1, we know that the objective function of t he dual problem in Eq. (10) is smooth (Lipschitz continuous gradient) and the constraint is a piecewise sphere. Thus, we can use the framework of FISTA [4, 24, 12, 2] to solve the dual problem in Eq. (10). We present the pseudo codes in Algorithm 1. Notice that the dual problem in Eq. (10) is a maximization problem. Therefore, the gradient projection step in Eq. (20) and the line search criterion in Eq. (21) are modified accordingly.

In Algorithm 1, there are two critical steps. In the fol-lowing, we show that both steps have closed-form solutions with low cost. The first critical step is how to efficiently com-pute the dual objective function value D (  X  ) and the gradient  X  X  (  X  ), which depends on how to efficiently obtain the op-timal solution W (  X  ) for the problem in Eq. (11). Next, we show how to efficiently solve the optimization problem in Eq. (11). By exploring the special structure of Eq. (11), we have for all j  X  { 1 ,  X  X  X  , d } : w j (  X  ) = arg min following closed-form solution [14, 23]: The second critical step is how to efficiently solve the gradi-ent projection subproblem. Due to the simplicity of the piecewise sphere constraint, the gradient projection sub-problem has a closed-form solution in Eq. (20).
 By Theorem 4.4 in [4], we have the following result.
Theorem 2. (Theorem 4.4 [4]) Let {  X  k } be the sequence generated by Algorithm 1 and  X   X  be an optimal solution for the problem in Eq. (10). Then for all k  X  1 , we have where  X  0 is the initial point;  X  &gt; 1 is the factor used in the line search (  X  is defined in Algorithm 1); L is the Lipschitz constant defined in Eq. (13).

Based on Theorem 2, we establish a convergence rate for the sequence { W k } generated by Algorithm 1 in the follow-ing theorem:
Theorem 3. Let { W k } and {  X  k } be the sequences gen-erated by Algorithm 1, and let W  X  and  X   X  be any optimal solutions for the problems in Eq. (5) and Eq. (10), respec-tively. Then for all k  X  1 , we have Algorithm 1: A ccelerated Dual Ascent Algorithm 1 Initialize  X  1  X   X  0 ; t 0  X  1; t 1  X  1; 2 for k = 1 , 2 ,  X  X  X  do 5 for j = 0 , 1 , 2 ,  X  X  X  do 7 C ompute  X  k via gradient projection: 8 b reak; 9 end 10 end 11 Compute W k  X  W (  X  k ) via Eq. (19); 12 if some convergence criterion is satisfied then 13 L et  X   X   X   X  k and W  X   X  W k ; 14 break; 15 end 1 7 end Output :  X   X  , W  X 
Proof. W e follow the proof of Theorem 4.1 in [5]. Denote where {  X  k } is the sequence generated by Algorithm 1. Con-sidering Eqs. (7), (14), (22) together, we have From Algorithm 1, we have By the optimality condition of Eq. (25), we have where O denotes the zero matrix and  X  W k f ( W k ,  X  k ) denotes the sub-differential of f ( W,  X  k ) with respect to W at the point W = W k . Recalling that f ( W,  X  k ) is strongly convex with respect to W with the parameter  X  2 , which together with Eq. (26) implies that for all W  X  R d  X  m By Eq. (23), we have for all z  X  R P m i = 1 n i Adding Eqs. (27), (28) together and considering Eq. (24), From Eqs. (11), (25), we know that Recalling that the sequence {  X  k } = { [(  X  k 1 ) T ,  X  X  X  , (  X  satisfies the constraint in Eq. (10) and considering Eqs. (9), (23), we have for all k  X  1: which together with Eqs. (24), (30) implies that Denote by ( W  X  , z  X  ) as an optimal solution for the optimiza-tion problem in Eq. (6). Then by the equality constraint in Eq. (6), we have for all i  X  { 1 ,  X  X  X  , m } which together with Eq. (7) implies that
L ( W  X  , z  X  ,  X  k ) = Noticing that Eq. (32) attains the minimum objective func-tion value in Eq. (6) and strong duality holds for the opti-mization problem in Eq. (6), we have where  X   X  is an optimal solution of the dual problem in Eq. (10). Substituting W = W  X  , z = z  X  into Eq. (29) and considering Eqs. (31), (33), we obtain w hich together with Theorem 2 immediately implies that the inequality in Theorem 3 holds.

Remark 2. In additional to FISTA [4], we can use many other optimization algorithms to solve the dual problem in Eq. (10). For example, the SpaRSA framework [32] can be adopted to efficiently solve the dual problem. The the-oretical convergence rate of SpaRSA is no better than that of FISTA. However, due to the utilization of the Barzilai-Borwein (BB) rule [3, 32], the empirical convergence per-formance of SpaRSA is much better than that of FISTA for solving the dual problem in Eq. (10) [see Section 4.2].
Notice that the objective function in Eq. (5) consists of P the framework of FISTA [4] to directly solve the optimiza-tion problem in Eq. (5), achieving a convergence rate of O (1 /k 2 ). If we additionally consider the strong convexity of the objective function in Eq. (5), the proximal gradient method (PGM) can achieve a geometrically linear conver-gence rate. However, both FISTA and PGM are not practi-cal for directly solving the optimization problem in Eq. (5). Specifically, at each iteration of both FISTA and PGM, we need to solve the following proximal operator problem: min where B  X  R d  X  m and  X  &gt; 0 are both constant with re-spect to W . Obviously, solving the above proximal operator problem is as difficult as solving the original optimization problem in Eq. (5).

We can also use a similar primal smoothed optimization algorithm (more details are provided in the Appendix A) proposed in [22] to solve the primal problem in Eq. (5) di-rectly. One problem is that the smoothing parameter is quite challenging to tune while this parameter is crucial to the convergence performance of the SPG algorithm.
Another approach which can solve the optimization prob-lem in Eq. (5) is the Alternating Direction Method of Multi-pliers (ADMM) method [8] (more details are provided in the Appendix B). One problem is that, at each step, we need to solve a Lasso problem which does not admit a closed-form solution. Moreover, the penalty parameter is very hard to tune while this parameter is critical for the efficiency of the ADMM method.

A very straightforward approach to solve the optimiza-tion problem in Eq. (5) is the sub-gradient method [10]. In the non-smooth case, the objective function value sequence { l ( W k ) } generated by the sub-gradient method converges to the optimal value l  X  at a rate of O (1 / conditions [10], where l ( W ) denotes the objective function in Eq. (5). However, this convergence rate is established with-out exploiting the strong convexity of the objective function l ( W ). By considering the strong convexity of l ( W ), Beck and Teboulle [5] show that the convergence rate of the ob-jective function value has been improved from O (1 / O (ln( k ) /k ) using the sub-gradient method. However, the sub-gradient method still converges slowly because the sub-gradient method is a very general method for solving non-smooth optimization problems without exploring the special structure of Eq. (5) and the step size at each step is shrinking to zero (or the step size is a very small constant).
The cutting plane method [7] and the level set method [20] are another two approaches which can solve the optimization problem in Eq. (5). They can be viewed as improved meth-ods of the sub-gradient method. Similar to the sub-gradient method, the cutting plane and the level set methods are gen-eral methods for solving non-smooth optimization problems and do not exploit the the special structure of Eq. (5).
In this section, we first evaluate the prediction perfor-mance of the calibrated multi-task feature algorithms with and without the squared norm regularizer. Then, we present computational efficiency studies for our proposed algorithms. Matlab codes are included at MALSAR package [35]. We include the following algorithms for comparison: (1) Calibration : the calibrated multi-task feature learning for-mulation in Eq. (5) by setting  X  2 = 0; (2) Calibration-function value in Eq. (5) and regularized parameters are set as  X  of samples from the i -th task.
 L2 : the calibrated multi-task feature learning formulation in Eq. (5) by setting  X  2 6 = 0; (3) LeastSquares : the non-calibrated multi-task feature learning formulation in Eq. (1) by setting r ( W ) = k W k 1 , 2 ; (4) LeastSquares-L2 : the non-calibrated multi-task feature learning formulation in Eq. (1) by setting r ( W ) = k W k 1 , 2 and adding a squared norm regu-larizer  X  2 2 k W k 2 F . We conduct experiments on both synthetic and real-world data sets which are described as follows:
Synthetic Data : We adopt the similar procedure in [22] to generate the synthetic data as follows: we set the num-ber of tasks as m = 10 and each task has n i = 100 sam-ples which have d = 200 features (i.e., the dimensionality is d = 200); each row of the data matrix X i  X  R n i  X  d of the i -th task is independently sampled from the multivari-ate normal distribution N ( 0 ,  X ) with  X  jj = 1 and  X  jk for all j 6 = k , and it is normalized such that the length of each column of X i is 1; each entry of the underlying ground truth matrix W  X  R d  X  m is independently sampled from the uniform distribution U (  X  10 , 10) and then we randomly set 95% rows of W as zero vectors; each entry of the noise vector  X  response vector y i  X  R n i of the i -th task is computed by y
R eal-World Data : We conduct experiments on two real-world data sets which are summarized as follows: (I) The Alzheimer X  X  Disease Neuroimaging Initiative (ADNI) 1 is a longitudinal study aiming at identifying important neuroimag-ing biomarkers that are predictive of the progression of the Alzheimer X  X  disease and building predictive models for prog-nosis of the disease. In our experiments, we use 310 MRI features from 648 patients to predict the MMSE value, which is a cognitive score that indicates the cognitive functionality of the patients. According to the medical diagnosis, there w ww.loni.ucla.edu/ADNI/ number of samples from the i -th task. are 191 normal control (NC), 319 mild cognitive impairment (MCI), and 138 patients with probably Alzheimer X  X  disease (AD) [please refer to [36, 37] for more details about the data]. We thus construct three tasks, and in each medical group we build one regression model to predict the corresponding MMSE scores. (II) We also use the ADNI data set and build multi-task regression models for predicting the ADAS-Cog score, which is another important cognitive score. We con-struct the task in the same way as in (I).

In the prediction performance experiments, we terminate the comparative algorithms when the relative change of the two consecutive objective function values is less than 10 the number of iterations exceeds 10000. We randomly split the samples from each task into training and test samples with different training ratios (60%, 70% and 80%). All pa-rameters of the comparative algorithms are tuned via 5-fold cross validation. For each training ratio, we report the aver-aged mean squared error (MSE) and the standard deviation over 10 random splittings of training and test samples as shown in Table 1. From these results, we have the following observations: (a) The calibrated multi-task feature learn-ing algorithms (Calibration and Calibration-L2) outperform the non-calibrated ones (LeastSquares and LeastSquares-L2), which shows the superior prediction performance of the calibrated algorithms over the non-calibrated ones. (b) Cal-ibration and Calibration-L2 achieve very similar prediction performance, which demonstrates that the proposed variant keeps the superior prediction performance of the calibrated multi-task feature learning over the non-calibrated one. In the next subsection, we will show that solving the smooth dual problem using a proper optimization algorithm is much more efficient than solving the primal problem directly.
We study the computational efficiency of solving the op-timization problem in Eq. (5) by comparing the following algorithms: (1) D-PG : solve the primal optimization prob-lem in Eq. (5) via using SpaRSA [32] to solve the dual opti-mization problem in Eq. (10); (2) D-FISTA : solve the pri-mal optimization problem in Eq. (5) via using FISTA [4] to solve the dual optimization problem in Eq. (10); (3) P-SPG : solve the primal optimization problem in Eq. (5) via using SpaRSA [32] to solve the smoothed optimization problem in Eq. (37); (4) P-SFISTA : solve the primal optimization problem in Eq. (5) via using FISTA [4] to solve the smoothed optimization problem in Eq. (37); (5) P-ADMM : solve the primal optimization problem in Eq. (5) by using ADMM [8]. We conduct the experiments on the same data sets (both synthetic and real-world) described in the last subsection. We initialize D-PG and D-FISTA with  X  0 whose entries are independently sampled from the standard normal distribu-tion, and initialize P-SPG, P-FISTA and P-ADMM with W 0 which is computed by Eq. (19) using  X  0 . We terminate the comparative algorithms when the relative change of the two consecutive objective function values 2 is less than 10 or the number of iterations exceeds 10000. All algorithms are implemented in Matlab and executed on an Intel Core i7-3770 CPU (@3.4GHz) with 32GB memory.
T he objective function value indicates the dual objective function value in Eq. (10) and the primal objective function value in Eq. (5) for dual optimization algorithms (D-PG and D-FISTA) and for primal optimization algorithms (P-SPG, P-FISTA and P-ADMM), respectively. function value in Eq. (5) and regularized parameters are set as  X  of samples from the i -th task.

To show detailed convergence behaviors of the compara-tive algorithms and compare their computational efficiency, we report the primal objective function value (for P-SPG, P-FISTA and P-ADMM) and the dual objective function value (for D-PG and D-FISTA) vs. CPU time plots as shown in Figure 1, Figure 2 and Figure 3. To better compare the efficiency of dual and primal optimization algorithms, in each figure, we also draw a horizontal line indicating the optimal primal objective function value in Eq. (5). From these results, we have the following observations: (a) D-PG and D-FISTA increase the dual objective function value which finally converges to the optimal primal objective func-tion value. This validates the fact that the strong duality holds for the optimization problem in Eq. (5). (b) D-PG is the most efficient among all algorithms on all data sets. Specifically, D-PG always quickly increases the dual objec-tive function and rapidly terminates by approaching the op-timal primal objective function value. (c) D-PG converges faster than D-FISTA and D-SPG converges faster than P-SFISTA, which demonstrates that SpaRSA indeed has very good empirical convergence performance. Although there is no rigorous proof that D-PG is the most efficient, empirical studies demonstrate that using SpaRSA to solve the dual problem is much more efficient than that by solving the pri-mal problem directly. This may be due to the nice properties of the dual problem and the empirically fast convergence of SpaRSA.
In this paper, we study a variant of the calibrated multi-task feature learning by adding a squared norm regularizer. We derive a smooth dual optimization problem with a piece-wise sphere constraint, which enables us to develop fast dual optimization algorithms. We also provide a detailed conver-gence analysis for the proposed dual optimization algorithm. Empirical studies demonstrate that, the dual optimization algorithm quickly converges and it is much more efficient than the primal optimization algorithm. Moreover, the cali-brated multi-task feature learning algorithms with and with-out the squared norm regularizer achieve similar prediction performance and both outperform the non-calibrated ones. In our future work, we will analyze statistical properties of the proposed formulation and apply it to other applications such as Drosophila image annotation [17].
 This work is supported in part by China 973 Fundamental R&amp;D Program (No.2014CB340304), NIH (R01 LM010730), and NSF (IIS-0953662, CCF-1025177). We use the smoothing technique [27] to derive a primal smoothed optimization algorithm to solve Eq. (5). Based on the definition of the dual norm, we have Define a function as where  X  &gt; 0 is a smoothing parameter. We show in the following proposition that
Theorem 4. The optimization problem in Eq. (35) has a unique solution in the following closed-form: Moreover, and Lipschitz continuous gradient with respect to W . Specif-ically, the gradient G  X  ( W ) of and the Lipschitz constant of G  X  ( W ) is Based on Theorem 4 (the proof of Theorem 4 is very similar to that of Theorem 1 and is thus omitted here), we know that smooth term the following smoothed version of the optimization problem in Eq. (5): c W = arg min The primal smoothed algorithm in fact solves the optimiza-t ion problem in Eq. (5) via solving the smoothed optimiza-tion problem in Eq. (37). To be more specific, we present a primal smoothed optimization algorithm called accelerated primal smoothed algorithm given in Algorithm 2, in which FISTA [4] is used to solve the smoothed optimization prob-lem in Eq. (37) and some notations are defined as follows:
T he convergence analysis of Algorithm 2 is similar to The-orem 2.2 in [22] and is thus omitted here.
 Algorithm 2: A ccelerated Primal Smoothed Algorithm
Input : W 0  X  R d  X  m ,  X  0 &gt; 0 ,  X  &gt; 1; 1 Initialize W 1  X  W 0 ; t 0  X  1; t 1  X  1; 2 for k = 1 , 2 ,  X  X  X  do 5 for j = 0 , 1 , 2 ,  X  X  X  do 7 C ompute W k via proximal gradient: 8 b reak; 9 end 10 end 11 if some convergence criterion is satisfied then 12 L et W  X   X  W k ; 13 break; 14 end 1 6 end Output : W  X 
Remark 3 . Similar to the dual optimization algorithm, we can also use SpaRSA [32] to solve the smoothed opti-mization problem in Eq. (37). Due to the use of the Barzilai-Borwein (BB) rule [3, 32], SpaRSA achieves very good em-pirical convergence performance.
 We know that the optimization problem in Eq. (5) is equiv-alent to the constrained optimization problem in Eq. (6) and we can write down the augmented Lagrange function of Eq. (6) as follows: w here z = [ z T 1 ,  X  X  X  , z T m ] T  X  R P m i =1 n i ;  X  = [  X  R responding to the constraint z i = y i  X  X i w i ;  X  &gt; 0 is a penalty parameter.

The ADMM solves the optimization problem in Eq. (5) by alternatively updating the variables as follows: (1) Update W : W k = arg min
W e can use FISTA [4] or SpaSRA [32] to efficiently solve the above problem, where the gradient of the smooth part of the objective function is given by: ( 2) Update z : z k = arg min F or the above problem, the optimal solution is given by the following closed-form: z i = arg min w here v i =  X  ( 3) Update  X  :
