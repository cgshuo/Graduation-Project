 The revenue of search-engine providers strongly depends on targeted advertisement. Targeted advertisement is becom-ing more reliant on personal data. This puts user privacy at risk. One way to improve privacy is to anonymize search logs, but this reduces usefulness for ad placement. Further , the usefulness depends on the target function used for the anonymization. This paper is the first to study this tradeoff systematically. We quantify the usefulness of an anonymize d search log for advertisement purposes, by estimating out-comes such as the number of clicks on ads or the number of ad impressions possible after anonymization. A main re-sult is that anonymized search logs are still useful for adve r-tisement purposes, but the extent strongly depends on the target function.
 H.2.0 [ Database Management ]: General X  Security, in-tegrity, and protection ; H.3.3 [ Information Systems ]: In-formation Search and Retrieval X  Search process Security privacy, search engine, advertisement, anonymity
Advertisement is the main source of income of search-engine providers (referred to as  X  X rovider X ). To place ads that are of interest to users, providers try to predict their interests. To improve these predictions, providers take th e search histories of the users into account, as stated in pro-http://www.google.co.uk/intl/en/privacy_cookies.

However, as AOL discovered when releasing supposedly de-identified search logs [1], query terms alone are sufficien t for re-identification [2]. This raises significant privacy i ssues in the use of search logs  X  The European Community re-gards search logs as personally identifiable data whose use is restricted by privacy laws [3].

Storing and processing personally identifiable data places user privacy at risk, requiring additional care in the manag e-ment and use of that data. Further, providers are subject to data-protection law. For example, in the European Commu-nity this obligates providers to obtain user X  X  permission t o process the data, provide processes to delete data, etc. [3, 4]. Most of these requirements are difficult to comply with (and compliance is expensive). Further, situations where provi -ders have failed to comply can lead to fines, restrictions on products, negative media coverage, etc.

One way to protect privacy is anonymization. Working with anonymized search logs would not only improve user privacy, it would also relieve providers from having to com-ply with those legal obligations. ( k, m ) -Anonymity [5] has defined anonymity for set-valued data. In this article, we make use of their definition. With slight adaptations for search logs, it is as follows:
Definition 1 ( ( k, m ) -Anonymity). A search log is ( k, m ) -anonymous if any combination of m terms out of the history of a user appears in at least k  X  1 histories of other users. ( k, m ) is the level of privacy of the search log. The higher m , the greater the background knowledge that must be known to identify a log as belonging to an individ-ual. The higher k , the higher the number of individuals who are indistinguishable to an adversary who knows at most m attributes about an individual.
 Challenges when anonymizing search logs. We deem this definition of anonymity useful, for the following reaso ns:
First , anonymization poses a tradeoff between privacy and the usefulness of the anonymized data [6], in our case for ad-placement. It is important to note that there are several ways to reach the same degree of privacy. Existing pro-posals [5 X 7] feature anonymization algorithms and study their effectiveness. Each does an optimization with regard to one generic target function, e.g., the number of term generalizations necessary or the size of the log after ano-nymization. However, such an evaluation is undifferenti-ated: For instance, with the same degree of anonymity, op-timality of the log depends on the business model of the html , November 2009; http://privacy.microsoft.com , January 2010; http://privacy.yahoo.com , January 2010. provider and the advertisers. Relevant optimization crite ria include the revenue, the number of ad impressions or clicks on ads, costs for placing ads, the frequency of a term, etc. Thus, an anonymization algorithm should be flexible regard-ing its target function. It is important to study the effect of the various target functions systematically. Second , the organization carrying out the anonymization has no apriori knowledge which search terms or combinations of terms are sensible or may lead to the identification of a user ( quasi-identifier ). This means that some well-known anonymiz-ation techniques, e.g., any technique that divides data int o identifying attributes and sensitive ones [8 X 10], is not ap pli-cable. Third , using generalization hierarchies [5, 7] is simply not feasible. Such hierarchies are not available for the set of terms occurring in generic search histories. Fourth , insert-ing queries into search logs seems to be a way to disguise the user interests/intentions [11]. This, however, render s the personalized ad placement much more difficult or even impossible. Fifth , finding an optimal anonymization (based on k -Anonymity) is NP-hard [12]. This holds true in gen-eral for ( k, m )-Anonymity, since k -Anonymity is equivalent to ( k, | H | )-Anonymity (where | H | is the maximum number of distinct terms in a search history).

We use suppression to achieve ( k, m )-Anonymity: we delete terms from individual user logs until the logs comply with our definition of anonymity.
 Contributions Based on ( k, m )-Anonymity, we evaluate the influence of the target function of the anonymization on the usefulness of the resulting data for ad-placement. To do so, we make use of a heuristic anonymization algorithm that is flexible regarding the target function. The target functions studied are on the one hand generic characteristi cs such as  X  X ize of the anonymized search log X . On the other hand, we also take target functions into account that are specific for advertisement, e.g., target functions highlig ht-ing terms with a high marketing value. We compare the anonymized search logs obtained with these different target functions and for varying degrees of privacy.

Our main result is that ( k, m )-Anonymity shields users from the type of re-identification in the AOL case, while at the same time is keeping information in the anonymized log that is helpful for ad placement. To illustrate, a real-world log that we have anonymized for m = 3, k = 100 still contains the terms that are responsible for 61% of the ad clicks, according to our estimation. Furthermore, we show the influence of the target function on the usefulness for ad placement in the context of anonymization. For instance, with the same degree of privacy ( m = 3, k = 100), a different target function may result in 40% fewer ad clicks.
We now formalize our problem statement, describe the target functions we optimize for, and sketch our algorithm.
It is an open question if anonymized search logs have value for marketing and ad placement. To obtain a ( k, m )-anonymous log S  X  from the initial log S , we delete terms from the search histories of the users. The log S is a set of search histories of individual users. Here, a history is the set of terms a user has used in queries. The crucial ques-tion in the context of anonymization is which term to delete from the history ( H i ) of a user n i . Clearly, this depends on the target function ta . A target function obtains a set of terms as parameter and returns the term with the smallest utility according to the target function. Since different ta r-get functions are conceivable, there are different anonymiz ed variants S  X  of a log S . They have the same degree of pri-vacy, but different utility. In the following, U ta ( S  X  ) is the usefulness of an anonymized log S  X  with respect to the char-acteristic of the log on which ta focuses. The usefulness of an anonymized log is the sum of the individual utilities of the terms of all histories, given the current target functio n. Problem statement: Given different degrees of privacy and different target functions ta and ta  X  , is there a signif-icant difference between the usefulness of an anonymized search log generated with ta and the one generated with ta  X  ? In other words, we wonder how strongly different target functions affect the usefulness of the anonymization result .
While formally answering this problem would require an optimal algorithm for each target function, as we shall see, the greedy heuristic we present in Section 2.3 shows sufficien t variation between target functions to give confidence that the evaluation we will show answers this question.
We now describe the various target functions. We have de-rived the advertisement-specific ones from the Yahoo! mar-keting platform. It contains information Yahoo! offers to po -tential customers so that they can plan and optimize their advertisement campaigns. log size. Providers might be interested in search logs of maximal size. This may be the case when frequent occur-rences of a term are more important than different, rare terms. This might be relevant if several customers bid on the same term to have their ads displayed. # users. Providers may have a preference for a large number of user histories. This is because they can match ads with many users, even though the information available might be vague. bid. In most cases, advertisers pay providers per click on ads. In consequence, providers might want to keep exactly those terms advertisers bid highly for. U bid in this case is P term bid(term) . (The following target functions are derived similarly.) estimated clicks. In line with the business model (pay-per click), advertisers might want to keep those terms for which ads are clicked frequently, in contrast to terms with high bids, but few clicks. ad impressions. An alternative business model of provi-ders is payment contingent on the number of ad impressions, i.e., how often the ad is displayed, irrespective of the clic ks. revenue. The rationale behind this criterion is to keep terms in the log where clicks(term)  X  bid(term) is maximal.  X  X id X  and  X  X evenue X , as introduced here, do not represent the actual revenue to a provider. This is because providers typically offer a generalized and possibly modified second-price auction, display several different kinds of ads etc. Ho w-ever, these do enable us to show relative impact of differ-ent forms of anonymization on revenue. To the best of our knowledge, this is the first study quantifying the usefulnes s of anonymized data for advertisement purposes.
This section describes our anonymization algorithm (Al-gorithm 1). It is heuristic in nature. S is the search log, Algorithm 1 ( k, m )-Anonymity  X  Greedy Heuristic N the number of users. H i is the history of user n i and S = S H i . fis is the set of all frequent term combinations, i.e., combinations that at least k users have used. To gener-ate fis , we make use of an FP-growth implementation [13]. C i contains all combinations of terms of size m consisting of terms in H i . c ij is the j -th combination in C i . We use the symbol  X  to denote an assignment of a set to a variable.
The first step is to generate all possible combinations of terms in H i of size m and all subsets of it, for any user n i  X  N . For each of these combinations, we then check if it is a subset of fis . Whenever this is the case for a term combination, it is ( k, m )-anonymous. If the frequency of a term combination is less than k , i.e., support ( c ij ) &lt; k , we remove a term from c ij . Our heuristic is greedy in the sense that it removes the term that is a best fit, given the target function and the term combination currently under inspection. Function target fct in Line 10 computes the term to be removed. target fct can behave in two ways. One is  X  X andom X , the other is according to the generic or specific target functions just described. For  X  X andom X  the term to be removed from c ij is chosen uniformly. We use this scheme as a reference point, to assess the usefulness of the anonymize d data sets generated based on our various target functions. Algorithm 2 target fct(c,ta)
Algorithm 2 shows how we make use of different target functions. The usefulness of a term out of a term combi-nation c depends on the target function ta . The algorithm yields the term whose usefulness with regard to ta is minimal (Line 5). If two terms are equally useful, we pick one ran-domly. Our implementation evaluates ta  X  X  random, logsize, users, bid, clicks, impressions revenue } .
 Once a term is removed, we must update fis (Algorithm 1, Line 12). We reduce the support of all term combinations containing this term. It is important to only look at term combinations that we can form from terms t  X  H i . We remove the combinations that have a new support value  X  k after maintaining fis . This heuristic is applied iteratively until no more term has to be deleted. We first describe the data used in our evaluation. Altavista Log. The Altavista search log 2 contains 3.5 mil-lion queries from one day, issued by 370,585 users. We have preprocessed the data, i.e., we have removed queries issued by bots according to [14] and removed special characters. As a result, there are 1,846,134 queries remaining, issued by 367,803 users with 251,115 distinct terms and 5,501,825 occurrences of these terms.
 Yahoo! Search Marketing Data. To quantify the usefulness of an anonymized search log in a realistic way, we assign a value to each term. Given a term, the Yahoo! impressions, (ii) of the number of ad clicks, and (iii) of the maximal bid Yahoo! recommends to its customers. We have collected the data from this page systematically, and have gathered the marketing information for each of our roughly 250,000 terms from the Altavista log.
We first present the generic characteristics of anonymized search logs, then the ones specific to advertisement. We compute the anonymized log for each target function, for values of m = 2 , 3, in combination with all values of k from 2 to 100. In all cases, using a particular target function has lead to the highest utility according to that target. Table 1 contains characteristics of the results. log size. For m = 2, k = 2, we observe a significant reduc-tion in size of the anonymized log, by 55%. This appears to be due to the long-tail characteristic of search logs. For m = 2, k = 20, the size of the anonymized search log is 26% of the original log size, for m = 3, k = 100, it is 18%. # users. Independent of m , the anonymization result con-tains 95% of the users, for small values of k . For larger values of k , e.g., k = 100, it is possible to retain information from 70% of all users. Our interpretation is that these numbers are promising, in order to get at least a rough idea regarding the user interests.

Having looked at the generic characteristics, we now turn to measures that are specific for advertisement. bid. Column  X  X id X  reveals that, for m = 2, k = 2, the sum of bids for terms contained in the anonymization result still i s 51% of the sum computed on the original data. For m = 3, k = 100, it is still 20%. clicks. For large values of k , e.g., k = 100, and for m = 3, the terms still contained in the anonymized log retain 61% of the clicks, according to our estimation. For m = 2, k = 2, the share is 85%. http://sem.smallbusiness.yahoo.com/ searchenginemarketing/marketingcost.php impressions. The respective results are very similar to the ones for clicks. We can retain terms in the search log that lead to 57% to 85% of all ad impressions, contingent on the degree of privacy. revenue. Revenue both depends on the bids issued by ad-vertisers as well as on the prospective number of clicks and the term frequency. We observe that, again depending on the degree of privacy, revenue varies between 30% and 61% of the original value.
We now compare the effect of applying different target functions while keeping the utility measure constant. For example, we measure the number of clicks on ads when ap-plying the various target functions available. Table 2 con-tains our results. For each measure, the table contains the difference of the best result and the one obtained with  X  X an-dom X  (R) as well as the difference of the best and the worst result (W).  X  X andom X  is the mean value of two runs.
Our first result is that the difference between the best target function and the worst one is less than 4.2% in all combinations, for the generic characteristics. Comparing the best target function to  X  X andom X  in contrast, the difference of up to 28% is relatively high. In consequence, we see that, in spite of the small difference between the various generic target functions, carefully selecting the target function may make a significant difference.

Our second result relates to the characteristics specific to advertisement. Here, the total usefulness regarding a char -acteristic strongly depends on the target function, i.e., t he difference between the best and the worst result is signifi-cant. To illustrate, consider the estimated number of click s on an ad (Figure 1). It shows that the difference between the best target function (# clicks) and the worst one (bid) is more than 40%.
Improvements of personalized ad placement will depend on building comprehensive user profiles. This puts user privacy at risk and requires provider to comply with the complex requirements of data-protection laws. We have ex-plained that anonymization of search logs lets search-engi ne providers avoid the processing of person-related data. At the same time, effective ad placement is still possible. Re-lying on the notion of ( k, m )-Anonymity, an anonymization algorithm that is flexible regarding the target function can produce anonymized search logs that contain information that is valuable for providers and advertisers. To illustra te, anonymization can retain 70% and 95% of all users and 61% to 85% of the expected ad clicks. Further, the target func-tion is important. If poorly chosen, the number of ad clicks may be 40% under the value that is achievable. Overall, we conclude that anonymization to protect privacy and to free providers from legal obligations appears to be feasible. Acknowledgements This work was partially supported by DFG BO2129/8-1, AFOSR MURI FA9550-08-1-0265, and Yahoo! Inc. [1] Greg Pass, Abdur Chowdhury, and Cayley Torgeson. [2] Michael Barbaro and Jr. Tom Zeller. A face is exposed [3] Article 29 Data Protection Working Party. Opinion [4] Directive 95/46/EC of the European Parliament and [5] Manolis Terrovitis et al. Privacy-preserving [6] E. Adar. User 4xxxxx9: Anonymizing query logs. In [7] Yeye He and Jeff Naughton. Anonymization of [8] Latanya Sweeney. k-anonymity: a model for [9] A. Machanavajjhala et al. l-diversity: Privacy beyond [10] Ninghui Li et al. t-closeness: Privacy beyond [11] Mummoorthy Murugesan and Chris Clifton. Providing [12] Adam Meyerson and Ryan Williams. On the [13] Christian Borgelt. An implementation of the [14] Bernard J. Jansen et al. A temporal comparison of
