 The detection of repeated subsequences, time series motifs , is a problem which has been shown to have great utility for several higherlevel data mining algorithms, including clas sification, clustering, segmentation, forecasting, and rule dis covery. In recent years there has been significant research effort sp ent on efficiently discovering these motifs in static offline databases. However, for many domains, the inherent streaming nature of time series demands online discovery and maintenance of time series motifs. In this paper, we develop the first online motif di scovery algorithm which monitors and maintains motifs exactly in real time over the most recent history of a stream. Our algorithm has a worstcase update time which is linear to the windo w size and is extendible to maintain more complex pattern structu res. In contrast, the current offline algorithms either nee d significant update time or require very costly preprocessing s teps which online algorithms simply cannot afford. Our core ideas allow useful extensions of our algor ithm to deal with arbitrary data rates and discovering multidime nsional motifs. We demonstrate the utility of our algorithms with a variety of case studies in the domains of robotics, acoustic monito ring and online compression. H.3.3 [ Information Systems ]: Information Search and Retrieval Algorithms, Performance Time Series, Motifs, Online Algorithms Time series motifs are approximately repeated subsequences of a longer time series stream. Figure 1 shows an exampl e of a ten minute long motif discovered in telemetry from a sh uttle mission. 
Figure 1: Forty-five minutes of Space Shuttle telem etry from an accelerometer. The two occurrences of the b est ten-minute long motif are highlighted. Whenever a repeated structure is discovered, it imm ediately suggests some underlying reason for the conservatio n of the pattern. In this case a little investigation tells us that this pattern is indicative of a  X  X orrection burn X  subroutine to com pensate for random drift in the orbiter. The utility of automat ic algorithms for finding such motifs has been demonstrated in many d omains. For example, [24] recently investigated a motifbased a lgorithm for controlling the performance of data center chillers , and reported  X  switching from motif 8 to motif 5 gives us a nearly $40,000 in annual savings! X  . Motif discovery is also a core subroutine in at least a dozen research projects on activity discovery for humans and animals, with applications in elder care [27], surveillance and sports training. In addition, there has been a rece nt explosion of interest in motifs from the graphics and animation communities, where they are used for a variety of tasks, includi ng finding transition sequences to allow just a few motion cap ture sequences to be stitched together in an endless cycle [2]. Given the ubiquity of time series motifs it is hard ly surprising that many researchers have introduced techniques to find them efficiently. Until recently, all scalable algorithm s were approximate [6] [27][2][16], but in [17] a scalable exact algorithm was introduced, and it was shown that exact motif d iscovery is tenable for a database with tens of millions of tim e series objects. However, as others have observed in many other sett ings, most data sources are not static but dynamic, and data m ay stream in effectively forever. This suggests two obvious ques tions: is it possible to discover and maintain motifs on streami ng data, and is it meaningful and useful to do so? In this work we answer both questions in the affirmative. We develop the first online motif discovery algorithm which monitors and maintains exact motifs in real time over the most recent history of a stream. While we defer a formal definition of the problem until later, Fig ure 2 gives a visual intuition of the problem 1 . 
Figure 2: Maintaining motifs on a forty-five minute long sliding window. top ) Initially A and B are the motif pair. bottom ) but at time 790, two new subsequences C and D become the new motif pair of subsequences. Our algorithm has a worstcase update time which is linear to the window size, allowing deployment in realistic setti ngs with current offtheshelf hardware. As to the utility o f streaming motif discovery, we show empirically its usefulness on se veral real world problems in the domains of robotics, wildlife monitoring, and online compression. In addition, we show that o ur core ideas allow useful extensions of our algorithm to deal wi th data sources that produce data at changing rates and discovering motifs in multidimensional streams. The rest of this paper is organized as follows. In Section 2 we introduce necessary background materials and notati on and in Section 3 we discuss related work. In Section 4 we introduce our algorithm, and in Section 5 evaluate its performanc e. Section 6 we consider some extensions to allow us to solve relat ed problems. Section 7 sees an extensive testing of our ideas in several diverse domains, and we offer conclusions and directions in Section 8. Our algorithm considers real time streaming environ ments. In this section we define the environment in which our algo rithm works and the notion of online motif . We begin by defining the data type of interest, a time series: 
Definition 1: A time series is a continuous sequence x = ( x 1 ,x 2 ,...x t ) of realvalued numbers where x t value. The numbers are generated at a rate of  X  , which can be constant or variable within a range. We are not interested in the entire history of the time series, but rather the recent history, which we capture in a sl iding window: 
Definition 2: A sliding window ( W ) is the latest w numbers ( x w+1 ,x t*w+2 ,...x t ) in the sequence x . Within a sliding window, we are interested in motif s, which informally are repeated subsequences: 
Definition 3: A subsequence of length m of a time series x = ( x 1 ,x 2 ,...x t ) is a time series x i,m = ( x i m+1.
 We are now in a position to define the online motif . We define the real time motif of length m in the most recent sliding window as the most similar nonoverlapping pair of subsequenc es. 
Definition 4: The online motif of length m of a time series x = ( x 1 ,x 2 ,...x t ) is a pair of subsequences ( x i,m i+m  X  j  X  t*m+1 such that distance ( x i,m , x j,m ) is the smallest among all such pairs. The reason for considering only the nonoverlapping sequences is to avoid trivial matches that are inherently similar because they share most of their values [6]. Glancing back at Fi gure 2, we can see the examples of online motifs, which changed as time passed. Now we can define the class of algorithms our metho d belongs to. 
Definition 5: The exact search for the online motif of length m of a time series x = ( x 1 ,x 2 ,...x t ) finds the pair of subsequences ( x i,m , x j,m ) for 1  X  i &lt; i+m  X  j  X  t*m+1 such that Euclidean distance between x i,m and x j,m is the smallest among all such pairs. Note that there is always a motif pair under the de finition of an exact search. We denote the output produced by an e xact search as the exact motif , as opposed to the approximate motifs, which may not be the most similar pair under Euclidean distan ce. For ease of presentation we only discuss the case o f maintaining a single motif pair. However, it is simple to modify our algorithm to maintain a pattern that appears k times or to maintain all pairs having distances smaller than a threshold. Moreover , motifs from different windows can be collectively useful in hig h level data mining for collaborative structuring [29]. We think these high level modifications are out of this paper X  X  scope. To measure the distance between subsequences we us e the ubiquitous Euclidean distance in Definition 5. Rece nt work has shown that in terms of time series classification a ccuracy, the Euclidean distance is surprisingly competitive [10] . Furthermore, Euclidean distance is a metric and allows the classic early abandoning optimization [1] and we exploit both facts in this work. To make the algorithm invariant to baseline a nd amplitude scaling, we z normalize every subsequence and store it to avoid renormalizing every time it is compared. It is know n that z normalization improves the accuracy of time series classification for virtually every problem (i.e. on 37 out of 39 p roblems considered in [10]), but if appropriate we can work with non normalized data. Therefore, we are now obliged to t hink of a subsequence as an independent object or a point in a high dimensional space unrelated to other objects/points . So the motif becomes the closest pair of points in this space. At every time tick, a new subsequence x t*m+2,m of length m is generated in W and the oldest subsequence x t*w,m is deleted from W . Therefore, in our model of online motif discovery we assume that at every time tick a new object/point (i.e. su bsequence) is generated and the oldest object/point is deleted. O bjects may have an exclusion condition (for example, to avoid trivi al matches) specifying the objects with which it should not be compared. This model is general enough to discover the online moti f in streams of independent objects like individual images, video f rames, transactions, motion poses, etc. Our algorithm requires O( wm ) arithmetic operations to compute all distances in one update. The most costly part o f this is floating point multiplication. Let's assume a pessimistic co nstant of b which roughly denotes the amount of time each float ing point operation takes. In current computers, b can be close to 10 seconds. Given this and a usergiven ( m , w ) pair, we can easily compute the maximum rate (1/ bmw ) at which our algorithm guarantees to operate. We assume that  X  is below this maximum rate until section 6.1, where we remove the restric tion. Here we explicitly state why this problem does not lend itself to simple or  X  X fftheshelf X  solutions. The issues are well known in the general context of dynamic closest pair [12], b ut are worth restating here. Assume that we have identified the motif pair in a window W . We know the exact locations of the two occurrences of the motif, and their exact Euclidean distance D . If we now insert a single data point at the head of the queue, what do we now know ? The answer is very little; the motif pair may have chan ged, and if it has, then all we know is that the new motif pair ha s a distance of at most D . The locations of the new motif pair can be anywhe re. Suppose instead that we delete a single data point from the tail of the queue, what do we now know? The answer is again , very little, as the new locations of the motif pair can be anywhere. All we know is the (now) lower bound D on the motif distance. However, in the case we are considering, we both in sert (enqueue) and delete (dequeue) at each time tick, so we have neither an upper nor a lower bound on the motif distance, nor any constraint on where they might be. So in principle, we can be forced to completely  X  X esolve X  the motif discovery at each ti me step, with a O( w 2 m ) cost, even though our data has changed only a tin y amount, say, 0.0001%. However, as we shall show in the next section, by caching some computations we can guaran tee that we can maintain the motifs in just O( wm ) time. Eppstein describes an algorithm for maintaining the closest pair of points under any distance measure [12]. This algori thm solves a slightly more general problem than the one we consi der, in that it can have any arbitrary order of insertion and delet ion, and it does not require metric properties in the distance measu re. It has found use mainly in speeding up agglomerative clustering and in some other offline applications. There is a subtle but c ritical difference between the dynamic closest pair maintained in [12] [19] and the online motif discovery we consider here. In our ca se we are in a streaming environment where for every update we hav e a fixed time until the next value comes in. As such we must optimize the worstcase time at each insertion for an application that runs forever . In contrast, the method in [12] optimizes the total running time after all of the updates (i.e. the clustering) for an application that runs for a finite time . These distinctions are critical, and cannot be removed by assuming a buffe r in which we temporarily cache difficult cases, since an arbitra ry number of difficult cases may arrive one after another. Another approach in dealing with dynamic closest pa ir maintenance is commonly known as the  X  X azy approach  X  [5]. Here the data structure is not updated until the closest pair changes. In a streaming scenario we are interested in, this idea reduces the amortized time costs, but does not allow us to tigh tly bound the time per individual object arrival on arbitrary str eams. In [3] an optimal algorithm for maintaining the clo sest pair of points is described. It runs in logarithmic time wi th linear space. This algorithm works by hierarchically dividing the space into subspaces and has a problematic exponential consta nt (2 d d is the dimensionality of the objects. It is well u nderstood that space/data partitioning methods do not work well be yond dimensionality on the order of eight to ten [28]. H owever, the time series motifs that we are trying to maintain c an be of any length from hundreds to thousands. Eppstein actually introduces two different types of data structures in [12], a quadratic spacelinear update time and a linear space O( w log 2 w ) update time considering constant m . We believe that for the general dynamic closest pair problem these are currently the best two choices. Our algorithm falls in the fi rst category and utilizes the temporal ordering of updates to have a n amortized O( w 3/2 ) space complexity. In [8], statistics such as average , sum , minimum , maximum , etc. are maintained over a sliding window. Their objecti ve is to approximate these statistics in bounded space and t ime, whereas we are dealing with higher level statistics ,i.e. t he closest pair. Our work can be seen as an attempt to add motif to the set of statistics that can be maintained; however none of the techniq ues in [8] are of direct help to us. In summary, to the best of our knowledge, none of t his work, nor the rest of the literature on maintaining the close st pair of points has direct bearing on the exact search problem. In this section we describe our algorithm with a ru nning example. Assume that we are given a set of eight points in 2 D as shown in Figure 3(a) (for now ignore the connecting arrows). Every point is numbered by the timestamp of their time of arrival. Recall that our task is to find the closest pair of points (cur rently 4 and 1), and maintain the closest pair as we simultaneously delete 1 and insert 9, then delete 2/insert 10, then delete 3/in sert 11, etc. We will begin with a naive version and revise it to de fine our algorithm. First note that the closest pair in Figure 3(a) can be changed by one or both of the following two events (see Figure 3(b)): there must be a new closest pair having a distance not less than that of the departing closest pair. For example, af ter 1 is deleted (8,2) is the new closest pair. current closest pair, the motif pair must be update d to reflect that. For example, (6,9) is the new closest pair after th e insertion of 9. Note that in our example, the closest pair has been changed by both the insertion and deletion. 
Figure 3: a) A set of 8 points. b) At a certain tim e tick 1 is deleted and 9 is inserted. c) The data structure of points. d) The data structure after the update. Now, the arrows connecting the points in Figure 3(a ,b) represent the nearest neighbor relation. For example, the arr ow from 5 to 2 denotes that 2 is the nearest neighbor of 5. To mai ntain the closest pair online, our first choice is to track the neare st neighbors of all of the objects. We use the data structure shown in Figure 3(c) for this purpose. Here the horizontal arrows show the d irection of insertion and deletion of points representing norma lized subsequences. Each data point is associated with a list of pointers to the reverse nearest neighbors, the RNN-list. RNNlist is not ordered therefore insertion to it is a constant tim e operation. A data point also has a pointer to its nearest neighb or, NN. With each pointer the distance associated with the pair is also stored. If we can maintain such a data structure, we can answe r the closest pair query for this sliding window efficiently simp ly by finding the minimum of the NN distances. Next we show how we update this data structure. Update upon insertion: When a new point 9 is inserted, the distances to all of the existing points (18) from 9 are computed to find its NN (6). While computing the distances we m ay find that the new point is nearest to an older point. Therefo re, we may need to reset an older point X  X  NN as well as the new poi nt's RNNlist. For example, after 9 is inserted, the NN of 6 is ch anged to 9 from 8 (Figure 3(b)), and also, 6 is inserted in the RNN list of 9. After the nearest neighbor x of the new object is found we need to update the RNNlist of x . For example, the NN of 9 is 6 and therefore, 9 is added in the RNNlist of 6 (Figure 3(d)). The update upon insertion is O( wm ), as we have no way to avoid those distance computations. Update upon deletion: To handle deletion we need to look at the RNNlist of the departing point. For each of those reverse nearest neighbors, we need to find their new nearest neighb ors. For example, after 1 is deleted, both 4 and 7 have been assigned new nearest neighbors (Figure 3(d)). In the worst case, a point can have O( w ) reverse nearest neighbor and thus the naive appro ach to handle the deletion would take O( w 2 m ) time. Counting both insertion and deletion, the naive alg orithm needs O( w 2 m ) update time. The space complexity is O( w ) since each point appears exactly once in all of the RNNlists. In the next version of our algorithm we reduce the update time complexity to O( w 2 ). As visually hinted at in Figure 4(a), we create a huge space overhead in addressing the problem, which we will m itigate later. The squared space version: In this version we change the data structure to store a complete neighbor list ( N-list ) instead of just the nearest neighbor (NN). The Nlist entries are s orted by the distances from the owner of the list (Figure 4(a)). Here also the closest pair is the minimum of the first points of the Nlists. Update upon insertion: The new object needs to be compared with every old object and be inserted in every old object's Nlist in distance order. If we implement Nlist by minheap then insertion in an Nlist is O(log w ). As a whole, the insertion cost can be as low as O( wm ). If Nlists are simple linkedlists, the insertio n cost would be O( w 2 ) since ordered insertion is O( w ) and w &gt; m . Update upon deletion: For every reverse nearest neighbor x of the departing point p , we delete the first few entries (including the departing one) from the Nlist of x to get the next nearest neighbor y within the sliding window. We also insert x in the RNNlist of y . For example, when 1 goes out of the sliding window (Figure 4(a)), 1 is deleted from the heads of the Nlists o f all of its RNNs (7 and 4). Then 7 and 4 are inserted in the RNNlis ts of 3 and 7 respectively. Similarly, when 2 departs, 2 is delet ed from 5 X  X  N list leaving 1 in the head of 5 X  X  Nlist. Since 1 w ould be an invalid entry as it is already out of the sliding window, i t is also deleted for consistency. 2 is then deleted from 8 X  X  Nlist leaving 6 in the head which is a valid entry as 6 is not yet departe d. If we use minheap we may need to heapify after eve ry deletion to get the next minimum distance. Therefore, minheap increases the deletion cost to O( w 2 log w ). For simple linkedlist, the worst case is O( w 2 ) as we may need to delete w 2 entries from an overgrown 2 w 2 sized data structure. Altogether, we opt for simple linkedlist as the da ta structure for the Nlist and can perform an update in O( w 2 ) time. 
Figure 4: a) The squared space structure. Each poin t has one RNN-list (upper part) and one N-list (lower par t). N-lists are in order of the distances from the owner. b) The reduction of space using observation 1. We use two observations stated below for further re finement. 
Observation 1: Every pair of points appears twice in the data structure. If we keep just one copy of each, it is still possible to retrieve the closest pair from this data structu re. To exploit the above observation, we can skip updat ing the old N lists during insertion even if the new point become s the nearest neighbor of an older point. That way the insertion involves only building the Nlist of the new point and inserting into exactly one RNN list. This is clearly O( wm ) as we can sort the Nlist after inserting all of the old objects. Figure 4(b) shows the data structure after applying observation 1. Note that t he Nlist of a point now only holds points that arrived earlier than it. Also note that the RNNlists contain only later points. For example, the RNNlist of 7 does not have 3 although 7 is the NN of 3. The RNNlist of a point is built when subsequent points are added and we will denote it as R-list (Reverse list) from this point on. The reason is that Rlist points to the opposite direct ion of Nlist and stores the pointer to the later/newer Nlists where its owner is in the head. Deletion is still O( w 2 ). Since the Nlists are always kept sorted and valid, the motif pair is guaranteed to be among the first points of the Nlists as before. 
Observation 2: A point x can never make a motif pair ( x , y )  X  d ( z , y ). This is because ( z,y ) would remain the closest pair when x goes out of the sliding window. The direct implication o f the above is that the points in an Nlist can be stored in the s trict increasing order of their timestamps starting with the nearest neighbor. Obviously the distance ordering must be preserved. For example, (6,4) will never get a chance to be th e motif because (6,5) has smaller distance than (6,4) (Figure 4(b)) and we can safely skip (6,4) when the Nlist of 6 is created i n the newer version (Figure 5(a)). Note that &lt;2, 5&gt; is a strict ly increasing sub list of the Nlist of 6, but it does not start with the nearest a) neighbor (3) and so it would be an erroneous Nlist . The correct Nlist for 6 is &lt;3, 5&gt; as shown in Figure 5(a). After building the Nlist, we can use observation 2 to delete some of the older points safely and build a strictly tim e ordered list by only one pass over the Nlist. Therefore, it does n ot increase the insertion cost. As a benefit of the strict temporal ordering, now a departing point can only occur in the head of the N lists of the points in its Rlists and nowhere else. This remove s the burden of deleting extraneous pointers after the heads at del etion time and reduces the deletion cost to O( w ). The update cost is dominated by the distance computations upon insertion which is O ( wm ). The space complexity still appears as worstcasequ adratic with the above two observations. In the worstcase, the Nlist of every point could contain all of the previous points exac tly in the order of their arrival. However, we argue that such a pat hological worst case can never occur. In terms of amortized space c ost, we can prove that our algorithm needs O( w 3/2 ) amortized space. The proof is the following. The Nlist of a point arriving at time t can be any of the random permutations of all of the objects preceding it i.e . t*w+1, t*w+2,... ,t*1 . There are O( w ) preceding objects and w ! possible permutations. Now, we are storing the neighbors in the ascending order of their arrivals in the NNlist. Therefore, the average length of an NNlist is at most as large as the average le ngth ( L longest increasing subsequence of a random permutation of length w . There have been many conjectures about the exact distribution of L n but all agree that the expected value is O( n Therefore, the expected space needed for the data s tructure is Reducing Time to create N-list: To further reduce the update time we need to reduce the number of distance compu tations upon insertion. We can use an order line [17] to order the points on a 1D line. The order line is just a circular projecti on of all of the points around a reference/ pivot point [11]. The relative distance between a pair of points on the order line is a low er bound on their true distance in the original space. Thus, for ever y pair of points we now know a lower bound on their true distance, w hich can be used to decide if we will compare and insert a poin t into the Nlist of the newly added point. To facilitate this, we fi rst find an allowable upper limit of the distance between an ol der point and the new point and then check if the lower bound for this pair is larger than this upper limit. Given any growing Nl ist, the allowable upper limit of the distance between a poi nt x and the new point n is the minimum of d ( n , y ) for y &gt; x . 
Figure 5: a) The space reduction using the temporal ordering of the neighbors. b) In the next time tick 1 is deleted from all of the lists and 9 is inserted. To illustrate this idea, in Figure 6 the evolving N list of point 6 is shown. On the left the order line is shown with poi nts 1 through 6 and their positions/referenced distances illustrate d. Starting from 6 the algorithm walks both directions on the order li ne and compares every new point encountered with point 6. Thus the order line provides a specific order of the points within the sliding window to be compared with the new point. In this e xample the order can be 3,2,1,4 and 5. The state of the Nlist after each of the points is considered is shown by Figure 6. First of all, 3 is inserted as UL (3,6)= X . Now, 2 has a lower bound LB (2,6)=1, which is smaller than the upper limit UL (2,6)= d (3,6)=1.5. Therefore, we compute d (2,6)=3, which is larger than the UL , and so 2 is not inserted. Similarly, 1 has a lower bound LB (1,6)=2 which is larger than UL (1,6)= d (3,6)=1.5 and therefore, 1 is not inserted. After that, 4 is inserted, as it has LB (4,6)=2 smaller than UL (4,6)= X . Finally, 5 is inserted for the same reason in the l ist. The last step is to sort the list and remove outoforder points. Fo r example, 4 is knocked out of the Nlist at this step. 
Figure 6: Building the Neighbor list of point 6. ( left ) The order line while 6 is being inserted. ( middle ) The states of the 
N-lists after each insertion. ( right ) The distance values assumed in this example. With the above example elucidated, we can complete the description of the subsequent modifications made to the naive algorithm to produce our final algorithm. Table 1 through Table 3 show the pseudocode of our algorithm. There are two subroutines for insertion and deletion made to the sliding wind ow. Each of them takes in a point as the argument and performs the necessary operations on the data structure. At every time tic k, insertPoint(latest point) and deletePoint(oldest point) are called to keep the data structure updated. The locations and the distance of the motif pair are always available after these two operations. The data structure is assumed to be accessible by every subroutine. When insertPoint(p) (Table 1) is called with the new point, p , p is compared with the reference point (randomly generat ed or chosen from the database [17]). By projecting p on the ord er line (line 1) we mean computing the referenced distance (i.e. d ( p , r )) and inserting it in the sorted orderline (which is sim ply a doubly linked list of pointers). After that, the buildNeighborList(p) (Table 2) is called to insert p and create its Nlist in the data structure. As des cribed earlier, the points are considered in the order of the distance from p on the order line (line 2 in Table 2). Before inserting a point n in the N list, the algorithm finds the allowable upper limit u by looking at the current Nlist (line 3) and compares it with th e lower bound which is the same as the difference between the ref erenced bound is smaller than the upper limit, the algorith m computes the distance d(p, n) and again compares this with u . In case of d(p, n) finishes when the immediately previous point in the time order of p is already inserted and the lower bound of a point is larger than would be considered if the loop were not broken mus t have u &lt; d(prev time (p), p) and therefore would never succeed in the if statement at line 4. 
Procedure insertPoint(p) When buildNeighborList(p) returns, the Nlist is sorted according to the distances from p (line 3 of Table 1) and all the points that meet observation 2 are removed from the Nlist (lin e 4). Then, p is inserted in the Rlist of the first point of its ow n Nlist (line 5). At line 6 the algorithm checks if the new point forms a motif and updates the motif pair if it is so. Note that the c omputation of upper limit should be efficient enough to preserve the benefit of reduction in distance computation. We leave it as a design choice for the practitioners for brevity and lack of space . 
Procedure buildNeighborList(p) 8 b reak
Procedure deletePoint(p) 1 for all points q in Rlist of p 2 Remove q .Nlist.head 3 Insert q into Rlist of q .Nlist.head 4 Remove p from the order line 5 if p is one of the motif pair 6 Find x for which 7 Update motif pair with ( x,x. Nlist.head) When the deletePoint(p) (Table 3) is called with the oldest point p , all of its reverse neighbors ( q ) will lose their nearest neighbor which is p itself (line 2). Since q is a later point than the new q .N list.head, the algorithm inserts q into the Rlist of q .Nlist.head. If p is one of the motif pair, the algorithm finds a ne w motif by finding the minimum of all of the nearest neighbor distances (lines 67). We have used four very different datasets in our ex periments, EEG trace [17], EOG trace, insect behavior trace [1 7] and a synthetic random walk (RW). All datasets, codes, vi deos and numbers used to generate the figures in this sectio n are available to be downloaded from the supporting webpage [31]. We use a 2.67 GHz Intel quad core processor with 6GB RAM. To the best of our knowledge there is no other algo rithm that discovers time series motifs online 2 , although there are works on dynamic maintenance of the closest pair in high dim ensionality. It is possible to trivially modify any of these algori thms to perform the online closest pair problem. We have selected t he highly optimized implementation of the well referenced wor k [12] for this purpose. To be fair to the author of [12], we note that we made changes to the implementation to specialize it for time series motif discovery, and the original code is more gene ral than our problem requires, as it allows arbitrary insertions and deletions, whereas we only need to be able to support insertio ns at the  X  X ead X  and deletions at the  X  X ails X . We have used the implementation of the FastPair data structure as it performs best in most of the applications [12 ]. Figure 7( top ) shows that our algorithm grows a lot more slowly th an FastPair if we change both of the parameters w and m while fixing the other at a specific value. For different datasets FastPair performs almost identically, so we show only the best one. T he speedup in average update time is guaranteed as we compute O( w ) distances per update while FastPair computes O( w log 2 w ) distances. Although we cache more statistics and thus use more space per point, in Figure 7( bottom ) we can see an almost flat average space usage per point over a large range of window sizes and motif lengths. This is significantly less than the worst case space needed per point, which is O( w ). 
Figure 7: Empirical demonstration of the slow growt h of ( top ) avg. update time and ( bottom ) avg. length of N-list. The parameters varied are ( left ) the window size with m =256 and ( right ) the motif length with w =40,000. Labels are in order of the heights of the right-most points of the curves.
 Note that random walk needs significantly larger N lists to accommodate more neighbors. The reason for this is the prominent lowvarying trends of random walk. For an y m , a new subsequence becomes neighbor to a relatively larger set of subsequences that just show the same trend after no rmalization even if they have different slopes and variances. We have two parameters to be set by the users, w and m. Optimum values of ( w, m ) significantly depend on the domain and are very easy for the practitioners to interpret as both can be measured in seconds or in the number of samples. In Figure 8( left ) we show the average update time per point for every combination of two sets of possible values of w and m (Figure 8). Although the figure shows values for the EEG dataset, other data sets exhibit a similar shape. Figure 8( right ) shows the space used per point for the EOG dataset. Note that there are three zero val ues showing the invalid combinations where a motif cannot be define d such as w =1000, m = 512. 
F igure 8: ( left ) Time usage per point in EEG dataset with varying w and m . ( left ) Space usage per point in EOG dataset with varying w and m . As impressive as these results are, the following o bservation allows us to further improve them. In most applicat ions, we can define the maximum distance ( d m ) beyond which no pair can be meaningfully called a motif simply because they are not similar enough to be of interest in that domain. As a concr ete example, in the wildlife monitoring application discussed in Se ction 7.2, we found that motifs that had a value greater than abo ut 12.0 did not correspond to sounds that people found to be subjec tively similar. Therefore, we can ask the algorithm not to even bot her finding the motif pair, if they would have a distance of more t han d To incorporate d m in our algorithm, only line 8 in Table 2 needs to be changed, to test if LB ( n , p )  X  min ( d ( prev time can obtain a reasonable d m from domain experts, it can reduce the number of distance computations performed per point with the help of the order line. The reason for this is that our algorithm can prune off all of the pairs having distances &gt; d m without computing the true distances. Consequently, it makes our alg orithm faster. Figure 9( left ) shows that when we use d m =0.4 m (equivalent to 80% correlation) and 0.2 m (equivalent to 90% correlation) then the average number of distance computation in the E EG dataset has been reduced for every window size. The speedup is generic for all of the datasets, as shown in Figure 9( right ). The basic online motif discovery algorithm describe d above can be extended, augmented and modified in numerous way s. We shared a very early draft of this work with domain experts in motion capture, medicine, robotics and agricultural monitoring, and asked them to suggest a  X  X ish list X  of extensio ns. The top two requests were adapting to variable data rates (robo tics and agricultural monitoring) and handling multidimensio nal motifs (motion capture, robotics). In the next two subsect ions we show how this can be accomplished. 
Figure 9: ( left ) The average amount of distance computation is much less in our algorithm than FastPair for EEG and further decreases with decreasing d m . ( right ) Speedup is consistent over all of the datasets for m=256 and w = 40,000 . Recall that our framework allows a guaranteed performance rate. That is to say, given values for m , w and a time to compute one distance calculation, we can compute the fastest ar rival rate  X  that we can a guarantee to handle (cf. Section 3). Howev er, even if asked to perform at exactly this rate, we can generally expect to have idle CPU cycles, simply because there is a gap between the pathological worst case we must consider and the av erage performance on real datasets. An obvious question i s whether we can we bridge this gap between our average performance, and the worst*case situation we must guarantee to handle, but expect to rarely if ever encounter in the real world. The pro blem is exasperated by the fact that up to this point we ar e assuming constant arrival rates . For example, suppose that a stream produces data at 100Hz 99.999999% of the time, but very occasionally produces a burst at 120Hz. If we can just handle 100Hz with an offtheshelf processor, must we real ly spend $300 for a faster processor that can handle the rarely s een faster rate? Much of the literature on monitoring streams for va rious events makes the constant arrival rate assumption. However , variable arrival rates are common in many domains. Previousl y, similar problems have been dealt with by load shedding in D ata Stream Management systems with techniques that allow dropp ing operators [18], while still maintaining the quality of the results. We believe that skipping points is also the best solution in the current context. Concretely, we skip every point that arrives within the current update operation (one insertion and one deletion). For example, for a 100Hz stream, if the update for x i,m takes 30 ms then our algorithm would skip two immediate points ( x i+1,m and x would start updating from the third point ( x i+3,m ) on. However, if an update takes less than 10 ms then we would not s kip the following point. Therefore, for a smaller average u pdate time (i.e. 6 ms in a 100Hz stream) a whole range of data usage (amount of data not skipped) is possible. For example, if all of the updates take 6 ms then 100% data points are used and nothin g is skipped. In contrast, about 50% of the data will be skipped if there are oscillating update times of 1ms, 11ms, 1ms, 11ms, e tc. Figure different data rates with of w =32,000 and m =256.. For most of our datasets, our algorithm can process at 200Hz while skipping every alternate point. Most real time sensors work on les s than 100Hz, a rate at which we process more than 60% of the data.
 Obviously there is a chance that one of the skipped points is a potential motif. There is no way to predict if a sk ipped point would be a motif with some future subsequence. Ther efore, we accept this potential loss for the sake of an infin itely running system. In Figure 10( right ) we show that although we skipped 30 40% of the points in high data rates (i.e. over 100 Hz), we did not miss many of the motifs. The drop rate of the numbe r of motifs discovered is slower than the drop in data usage. 
Figure 10: ( left ) Fraction of Data Used (the amount of subsequences considered) plotted against the varyin g data rate for w =32,000 and m =256. Our algorithm can operate at 200Hz while skipping roughly every other point. ( right ) The fraction of the motifs discovered drops more slowly than the fraction of data used. If we considered the unique motifs (nonoverlapping) only, our algorithm would rarely miss any of them. The reason for this is the following: A skipped subsequence is very simila r to the previous and following nonskipped subsequences (i. e. they are  X  trivial matches  X  [6]). Thus, even if we skipped a subsequence, its trivial mates would get a chance to form a motif th at is almost identical to the nonskip version. Our algorithm is trivially extendible to multidimen sional time series motifs. For simplicity, let's consider the t wodimensional case of online motif discovery. At every time tick here we have exactly two points arriving and two points departin g. For the two time series we keep two separate data structures, e ach similar to Figure 5(a). Depending on the application we can ig nore/allow a motif within/across the same/different series. The primary change is to redefine the set of subsequences that are com pared with the latest subsequence at the time of insertion. Thus, in the Nlist and Rlist nodes can point to points in both of the seq uences. Both of the observations of Section 4.2 hold for such Nlis ts, and the size of an Nlist on average is still O( w 1/2 ). The update cost is now O( wd ), where d is the number of simultaneous time series. The space needed for the whole data structure is O( w 3/2 d ). The closest pair can be found as before by checking the heads of the Nlists in both of the da ta structure. Online motif discovery is appropriate for settings where real valued numbers are generated at a high rate and the re is a necessity for tracking a particular behavior that c reates similar subsequences in the stream. We have tested our algo rithm on several datasets that fit this model, and use onlin e motif discovery as a subroutine. We note that these case studies a re really demonstrations rather than experiments (recall our classic experiments are in Section 5). In particular, space limitations prohibit us from providing pseudocode and some mino r details. However, this section is useful to motivate some ap plications made possible by online motif discovery. Note that, as before, all data and code is freely available at [31]. Online summarization/compression of time series dat a is an important problem that has attracted considerable r esearch. Existing approaches use various time series represe ntations, including piecewise linear approximations (PLA) (as shown in Figure 11. middle ), piecewise constant approximations [15], Fourier approximations [15] and wavelets [4]. Howev er, the obvious idea of summarizing a real*valued stream by dynamically finding reoccurring patterns in the data, placing o ne occurrence in a dictionary and assigning future occurrences to po inters to the dictionary entry, does not appear in the literature . We believe that this omission is due to the fact that until now the re was no practical method to find the necessary reoccurring patterns in real time. Clearly the results in this paper repair this omission. Figure 11: top ) An excerpt of record sddb/51 Sudden 
Cardiac Death Holter Data. middle ) A PLA of the data with an approximately 0.06 compression rate bottom ) A Motif-
Representation approximation of the data at twice t he compression rate still has a lower error. Plugging motifs into virtually any online compressi on algorithm is very simple. Most of the algorithms keep a small bu ffer of raw data similar to our sliding window (c.f. Section 3) . Within that buffer they run a simple search algorithm, deciding , for example, whether to approximate a heartbeat with 6 or 5 line ar segments (See Table 6 of [14] as a concrete example) All we have to do is add a new search operator that asks  X  would it be better to approximate this section with linear segments, or o ne of the motifs in the current motif dictionar y? X . Given this idea, all we need to do is set two parameters; how many motifs and of wh at length we should keep in the dictionary. In Figure 11 we show an excerpt where we chose (after seeing the first five minutes of the data) to maintain just two motifs, one of length 250 and one of length 200 In this example we compare our approach to the most referenced method [22], which uses PLA. We found that even if we force the motifrepresentation based method to use half the s pace of PLA, it can still approximate the data with a residual erro r that is approximately oneninth that of PLA. The approximat ions achieved are not only of a higher fidelity than oth er methods, but have the advantage of being highly interpretable in some circumstances. Note that the improvements achieved by the motif based algorithms are highly variable. On stock mark et data, with little or no repeated structure, there is no improv ement; but on normal heartbeats, which are of course highly repet itive, the reduction in size (for the same residual error as P LA) can be two or three orders of magnitude for larger datasets. Acoustic wildlife management is a useful tool for m easuring the health of an ecosystem, and several projects are c urrently monitoring the calls of various birds, frog and ins ects [26]. A key issue is that while sensors typically monitor twent yfour hours a day, memory limits for storage, or bandwidth limits for transmission, form a bottleneck on how much data ca n be retained in fielddeployed sensors. For example, [26] report s that when using a simple thresholding algorithm,  X  we have been able to reduce half an hour of raw recording to only 13 sec onds of audio,  X  however, they acknowledge that this data comes wi th some false positives. However, as [9] notes,  X  Animals of many species sing or call in a repetitive and species specific fashion  X  (our emphasis). We can exploit the repetitive natur e of certain bird calls to reduce the amount of data retained wh ile also reducing the false positive rate. For example, cons ider our efforts to monitor a sensor from woods in Monterrey, Califo rnia. The sound data is converted into melfrequency cepstrum coefficients, and only the first coefficient is examined. In this project, only Strigiformes (owls) are of interest, and domain exp erts have noted that most owls repeat their calls in a window of ei ght to ten seconds and that the calls last from one to three s econds [25]. Given this, we set w = 12 seconds, and m = 3 seconds, erring a little on the long side of those values. On a thirt y second trace that we manually confirmed had only ambient noise, we found that the mean motif value was 42.3, with an STD of 7.1. Give n that we only record sounds that have corresponding motifs w ith a value less than 10.0, such a value is very unlikely to ha ppen by chance. In Figure 12 we show an example of a detected motif with a value of 4.57, which corresponds to the call of a Great H orned Owl. 
Figure 12: top ) A stream is examined for motifs 3 seconds long, with a history of 12 seconds. bottom ) The discovered motif is the cry of a Great Horned Owl, whose cry i s phonetically written as hoo, hooo,---,hooo, hooo. Audio is available at [31]. Closing the loop is a classic problem in robotics i n which we task the robot with recognizing when it has returned to a previously visited place. The robot may sense its environment with a multitude of sensors, including cameras and ultraso nic transceivers, all of whose output can be represente d as  X  X ime series. X  The problem is challenging in two aspects: first, the robot must be able to recognize that it has returned to a previously visited place. This is a significant challenge, but assuming we can solve it, there is the second challenge of mitigati ng time and space complexity on resource limited robots. Naturally, w e can see our algorithm as a tool for continuously maintaining th e most likely candidate locations for loop closure. In an effort to verify this utility, we use the "Ne w College" dataset [7], where a set of 2,146 images have been collecte d by a moving robot. The images are taken from both sides of the robot. We convert the images to "time series" by taking their color histogram and group the images from both sides to form a sequ ence of imagepairs. We feed our algorithm with this data a nd w = 200. We also provide a separation window of 90 images fo r excluding trivial similarities. Our algorithm found 89 unique motifs, 46 of them being loopclosures. One of the motifs and its location are shown in Figure 13. 
Figure 13: ( left ) The map of the "New College." A segment of robot motion is shown. ( right ) Motif: The most similar pair of image-pairs that are 90 samples apart and t heir color histograms. The image-pairs are from the same locat ion and thus our algorithm detected the loop-closure. In this work we introduced the first practical algo rithm for finding and maintaining time series motifs on fast moving s treams. Our algorithm performs updates in O( w ) time and O( w 3/2 ) amortized space where w is the size of the most recent window. We showed applications of our ideas in robotics, online compr ession and wildlife management. Future work includes reducing the worst case space complexity and an extensive field testin g of the wildlife monitoring scenario. [1] Agrawal, R., Faloutsos, C. and Swami, A.N. Efficien t [2] Beaudoin, P., Panne, M., Poulin, P. and Coros, S. M otion [3] Bespamyatnikh, S. N. An Optimal Algorithm for Close st Pair [4] Bulut, A. and Singh, A.: SWAT: Hierarchical Stream [5] Cardinal, J. and Eppstein, D. Lazy Algorithms for D ynamic [6] Chiu, B., Keogh, E. and Lonardi, S. Probabilistic D iscovery [7] Cummins, M. and Newman, P. FAB*MAP: Probabilistic [8] Datar, M., Gionis, A., Indyk, P., and Motwani, R. [9] Dawson, D. K. and Efford, M. G . Bird Population Density [10] Ding, H., Trajcevski, G., Scheuermann, P., Wang, X. and [11] Dohnal, V., Gennaro C. and Zezula, P. Similarity Join in [12] Eppstein, D. Fast Hierarchical Clustering and Other [13] Fuchs, E., Gruber, T., Nitschke, J. and Sick, B. On*line Motif [14] Keogh, E., Chu, S., Hart, D. and Pazzani, M. An Online [15] Lazaridis, I. and Mehrotra, S. Capturing Sensor*Generated [16] Lin, J., Keogh, E., Lonardi, S. and Patel, P. Finding Motifs in [17] Mueen, A., Keogh, E., Zhu, Q., Cash, S. and Westove r, B. [18] N. Tatbul, N.,  X etintemel, U., Zdonik, S., Cherniac k, M. and [19] Nanopoulos A., Theodoridis Y. and Manolopoulos, Y. [20] Odlyzko, A.M. and Rains, E.M. On Longest Increasing [21] Ogras, Y. and Ferhatosmanoglu, H. Online Summarization of [22] Palpanas, T., Vlachos, M., Keogh, E., Gunopulos, D. and [23] Patel, P., Keogh, E., Lin, J. and Lonardi, S. Mining Motifs in [24] Patnaik, D., Marwah, M., Sharma, R.K. and Ramakrish nan, [25] Penteriani, V. Variation in the Function of Eagle Owl Vocal [26] Trifa, V.M., Girod, L., Collier, T., Blumstein, D.T . and [27] Vahdatpour, A., Amini, N. and Sarrafzadeh, M. Towards [28] Weber R., Schek, HJ. and Blott, S. A Quantitative Analysis [29] Wurst, M., Morik, K. and Mierswa, I. Localized Alternative [30] Yankov, D., Keogh, E., Medina, J., Chiu, B. and Zor dan V. [31] Supporting webpage containing Data, Code, Videos, E xcel 
