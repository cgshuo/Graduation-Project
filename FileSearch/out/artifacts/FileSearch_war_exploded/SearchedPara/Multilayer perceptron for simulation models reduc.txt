 1. Introduction
Simulation is used in many goals. One of them is to evaluate supply chain or workshop performance. There are three different ways of measuring this performance: analytical models (queuing theory, etc.), physical experimentation (lab platforms, industrial pilot implementation, etc.), and Monte Carlo methods (simulation or emulation) ( Thierry et al., 2008 ). Analytical methods are generally impracticable because the mathematical model corre-sponding to a realistic case is often too complex to be solved, and physical experiments suffer from technical and cost-related limitations. Simulation is the better approach to model and analyze performance for large-scale cases. In the simulation model, the number of  X  X bjects X  of the model and the number of events can be very large. Consequently, the first problem could be the time needed to build the model and the simulation duration on a computer can be unacceptable for operational use. Thus, it is necessary to reduce the model size ( Thierry et al., 2008 ).
On the one hand, constructing a simulation model is a complex task that can take modelers a lot of time. Effectively, simulation models of actual industrial cases are often very complex and the modelers encounter problems of scale ( Page et al., 1999 ). Thus, numerous authors have expressed interest in using simplest (reduced/aggregated) models of simulation ( Ward, 1989; Musselman, 1993; Pidd, 1996; Brooks and Tobias, 2000; Chwif et al., 2006 ).

On the other hand, to establish and to initialize  X  X redictive schedule X  or  X  X eactive schedule X , the knowledge of the evolution of resources states (WIP (work in process) and queues) are needed. This knowledge can be obtained by using a simulation model. Reduced models can be very useful, because they are quickly parameterized and simulated.

Furthermore, at this level of planning (master production schedule), load/capacity balancing is obtained via the  X  X anage-ment of critical resource capacity X  function or  X  X ough-cut capacity planning X  (RCCP), which essentially deals with bottlenecks ( Vollmann et al., 1992 ). Goldratt and Cox (1992) in  X  X he Goal X  put forward the  X  X heory of constraints X  (TOC), which proposes to manage all the workshops by bottlenecks control. Thomas and Charpentier (2005) have shown that a good method to build a simulation model would be to reduce the model according to the TOC.

Moreover, neural networks have been used in all application areas of the manufacturing: scheduling ( Akyol and Bayhan, 2007 ), design of manufacturing process ( Cakar and Cil, 2004 ), etc.
Therefore, the main goal of this work is to propose a design approach for simulation models, which would be less time consuming and simpler for the modelers, and which could be partially automated. This approach is based on the learning capabilities of neural networks and on the TOC.

The rest of the paper is structured as follows. Section 2 contains a brief bibliography overview and Section 3 presents the proposed approach of the reduction model and multilayer perceptron. Section 4 is devoted to the validation of the proposed approach in an industrial application, which is a sawmill flow shop case. 2. A bibliography overview 2.1. On supply chain simulations
One main goal of the supply chain simulation is to evaluate the performance of supply chain management in order to support decision-making at three levels:  X  strategic level (designing or redesigning a supply chain, loca-lization of factories and warehouses, partners selection, etc.),  X  tactical level (validation of the global forecasted production capacities according to forecasted demand), and  X  operational level (control policies, scheduling, cooperation policies on the shop floor, etc.).

The simulation model must be constructed according to its use and the supply chain to be modeled.

Kleijnen and Smits (2003) distinguish four simulation types for supply chain management:  X  spreadsheet simulation (may be part of production control software),  X  system dynamic (may explain the bullwhip effect),  X  discrete-event dynamic systems (DEDS) simulation (may pre-dict fill rate values), and  X  business game (may educate and train users).

Spreadsheets have been used to implement manufacturing resource planning (MRP), but this type of simulation is often too simple and unrealistic ( Kleijnen, 2005 ).

System dynamic is based on the work of Forrester (1961) .In this approach, companies are seen as systems with six types of flows (materials, goods, personnel, money, orders, and informa-tion) and different stocks. Managerial control is realized through the changing of rate variables. The feedback principle plays a crucial role in this approach ( Kleijnen, 2005 ).
 A DEDS simulation is more detailed than the preceding ones.
DEDS concerns the modeling of a system by a representation in which the state variables change instantaneously according to event occurring. Moreover, it takes into account uncertainties ( Law and Kelton, 2000 ).

A business game is a simulated world that may represent a supply chain and its environment. It is used for educational and research goals ( Kleijnen, 2005 ).

The two main difficulties encountered during the design step of a supply chain simulation model are related to the size of the system and the complexity of the control system. A supply chain is composed of a group of enterprises, composed in turn of a group of factories, composed of a group of workshops, etc. Moreover, modeling the behavior of the leading policies of each enterprise and the relationships between them is needed ( Thierry et al., 2008 ). This fact implies that the duration of one simulation may become unacceptably long to be usable. The same difficulty has been highlighted by Thomas and Charpentier (2005) concerning workshop. Therefore, it may be useful to reduce the size of the model. Different ways can be used to perform the model reduction:  X  abstraction, which allows the complexity of the model to be reduced and preserves the validity of the results ( Frantz, 1995 ), 2.2. On model reduction ques for general modeling. Their approach is composed of four steps: hypotheses (identify the important parts of the system), formulation (specify the model), coding (build the model), and experiments. Based on these works, different approaches have been proposed.
 approach for cases where the indicators to be followed are the average throughput rates. They suggest an eight-stage procedure.
The reduced model can be very simple and then an analytical solution becomes feasible and the dynamic simulation redundant.
Their work is interesting, but is valid in cases where the required results are averaged and where the aim is to measure throughput.
It is not interesting to follow the various events taking place in the work center (WC).
 semiconductor industry, which uses cycle time as an indicator. This model has been improved by Hung and Leachman (1999) .
They propose a technique for model reduction to be applied in large wafer fabrication facilities. They use  X  X otal cycle time X  and  X  X quipment utilization X  as decision-making indicators to do away with the WC. In their case, these WCs have a low utilization rate and a fixed service level (they use the standard deviation of batch waiting time as a decision-making criterion).
 to an  X  X ggregate model X  (macro) by using the  X  X low time X  indicator.
They suggest reducing the model by mixing the  X  X acro X  and  X  X icro X  approaches, so as to minimize errors in complex models.
Here again, for the  X  X acro X  view, they deal only with the estima-tion of flow time as a whole. For the  X  X icro X  approach, they construct an individual regression model for each stage of the operation to estimate its individual flow time. The cumulative order of flow time estimates is then the sum of the individual flow times. They, then, try to mix the macro and micro approaches.
These different approaches simplify the model by using a macro-scopic view of the system and by optimizing a macroscopic indicator (total cycle time, flow time, etc.) the aggregation of machines on the production line. They build a complete model of the production line and, if the last two machines correspond to a serial line, they aggregate them. The same is performed with the first two machines if they correspond to a serial line. These aggregation steps may be performed recursively and they denote backward and forward aggregation.
If the two machines to be aggregated follow a Bernoulli model or an exponential model, an analytical investigation allows the production rate of the new aggregated machine to be determined.
If not, a simulation phase must be performed to determine an empirical formula for the production rate.

Petri nets as tool in order to simplify network structures by using macro-places, which represent complex activities associated with function groups.
 continuous flow model based on gradient estimation for stochastic systems in order to approximate discrete manufacturing environ-ments ( Ho, 1987; Suri and Fu, 1994 ). Other authors use metamodels (linear regression, splines, Kriging, etc.) to perform a simulation model ( Kleijnen and Sargent, 2000 ). Neural networks can be viewed as a type of metamodel ( Barton, 1994; Pierreval, 1996; Kleijnen and
Sargent, 2000 ). In addition, neural networks have proved their abilities to extract models from experimental data ( Thomas et al., 1999 ). Therefore, the use of neural networks has emerged recently as an interesting approach within the framework of the supply chain or workshop management ( Shervais et al., 2003; Chiu and
Lin, 2004 ). 2.3. On neural network in manufacturing
Neural network approaches have been used in all application areas of the manufacturing. Zhang and Huang (1995) have noticed that neural networks are used in monitoring and diagnosis, process modeling and control, group technology, engineering design, qual-ity assurance, robotics, scheduling, or process planning areas.
Different typologies of neural network have been used for dealing with scheduling problem ( Akyol and Bayhan, 2007 ).
Hopfield network and its extensions are used to solve optimiza-tion problems. So, many works use Hopfield network to deter-mine static scheduling by minimizing the sum of all the starting times of each job X  X  last operation ( Foo and Takefuji, 1988; Foo et al., 1995 ), by minimizing makespan ( Willems and Brandts, 1995 ), or by minimizing the weighted sum of the earliness and tardiness penalties ( Akyol and Bayhan, 2007 ). These approaches are generally infeasible for large size problems and may generate constraints-violating solutions. Competitive network and self-organizing map have also been used to deal with the same problem. Fang and Li (1990) use competitive networks in order to minimize total tardiness. The use of competitive network needs the definition of equations of motion for the problem constraints and an energy function that converges to stable state. Chen and
Wang (2009) develop a self-organizing map-back propagation network to estimate the remaining cycle time of every job in a semiconductor manufacturing factory. Multilayer perceptrons are also used in scheduling applications in order to select a suitable scheduling strategy ( Geneste and Grabot, 1997 ), or designing a scheduling software ( Feng et al., 2003 ).

Some works focus on one aspect of scheduling problems. Lin and Hwang (1999) study dynamic task allocation and use two multilayer perceptrons to allocate the task between human and computer. Dispatching rules selection in a job shop has been investigated by El Bouri and Shah (2006) , which minimize the makespan and the mean flow time with two different neural networks. Kuo et al. (2007) address the same problem by focusing on the construction of the learning data set. Mouelhi-Chibani and
Pierreval (2010) determine the parameters through simulation optimization to perform the dispatching rules selection.
The design of manufacturing process is another important area of neural network applications. Cakar and Cil (2004) determine the number of machines in a work center in function of priority rules. The inputs of the multilayer perceptron used are machine utilization rate, percentage of the late parts, and mean values of flow time, tardiness, and completion time. Vosniakos et al. (2006) associate multilayer erceptron and genetic algorithm to analyse and design manufacturing cells. Araz et al. (2008) focus on the determination of the optimum kanban parameters by using a multilayer perceptron for generating simulation metamodels.
This area is related to the process metamodeling, which is the core of many works. Chambers and Mount-Campbell (2002) propose to model each component of a process by a neural model and to associate them in order to optimize the complete process.
In order to estimate important parameters, some authors pro-pose to use neural networks as metamodels. Fonseca and Navarese (2002) determine the manufacturing lead times for orders simulta-neously processed in a job shop. The time-throughput is estimated by using multilayer perceptron for single/multi product manufac-turing environments ( Yang, 2010 ). Kutschenreiter-Praszkiewicz (2008) uses Radial Basis Function to estimate time consumption in machining.

In all these applications, the choice of the structure of the neural network is always a complex task and determining a suitable or near-optimal structure for a neural network has been called a  X  X  X lack art X  X  ( Branke, 1995 ). However, some authors try to respond to this question. Sukthomya and Tannock (2005) inves-tigate the selection of inputs, and they attempt to provide guide-lines for the training of neural networks to model complex industrial processes. Khosravi et al. (2010) try to construct neural metamodels with optimal structure. Moreover, they build predic-tion intervals for point predictions of neural metamodels.
For multilayer perceptron, the determination of the near-optimal structure is a well-known problem investigated by many authors. Two main approaches can be used: constructive approaches ( Chentouf and Jutten, 1996; Rivals and Personnaz, 2003 ) and pruning approaches ( Hassibi and Stork, 1993; Drucker, 2002 ).

Akyol and Bayhan (2007) recall the main advantages and disadvantages of the Hopfield networks, competitive networks and multilayer perceptrons.

The main advantage of Hopfield networks is their massive parallelism architecture when their main disadvantages are that they may converge to local optimum, the ways of incorporating constraints into the energy function and the termination criteria affect the quality of the results, and the translation of the problem into the energy function is difficult.

Competitive networks are best applicable to optimization and classification problem and by using competitive learning rule, the penalty terms are handled explicitly therefore the energy function is simplified and the time required in obtaining coefficient is reduced. However, equations of motions need to be derived before solving the problem, competitive networks cannot be applied to simplify the energy function of all scheduling problems and their convergence should be analyzed carefully.

Multilayer perceptrons are universal approximators, which have better generalization capabilities to capture complex rela-tionship between inputs and outputs. However, their main drawbacks are that gradient-based training techniques may be trapped into local minima, the generation of training set is time consuming, and overlearning degrades the performance of the network. The problem of local optimum may be attenuated by using an adapted initialization algorithm and the determination of the optimal structure allows avoiding the overlearning problem. 3. Model reduction process 3.1. Proposed approach
The proposed approach is based on the association of discrete-event models and continuous models (neural network) in order to design a simulation model. Our objective is to maximize the bottleneck utilization rate and, at the same time, simplify simula-tion model construction for modelers.

The reduction algorithm proposed is an extension of those presented by Thomas and Charpentier (2005) . The main goal of this algorithm is to reduce the number of simulation blocks. It is based on the  X  X heory of constraints X , which uses the concept of bottleneck. Many definitions of bottleneck are available in the literature and still more are used in practice. Most of them view the bottleneck as the worst machine, e.g., the machine with the smallest efficiency. Other authors consider the machine with the largest effect on the throughput as bottleneck ( Li et al., 2009 ).
Here, two particular types of work centers (WC) are defined:  X  X onjunctural bottleneck X  (current bottleneck) is a WC that is saturated for the master production schedule (MPS) in the predictive scheduling in question. This means that it uses all of its available capacity.  X  X tructural bottleneck X  means WC that has often been or is in such a condition. These  X  X tructural bottlenecks X  are determined by experience feedback.

The proposed algorithm is presented in Fig. 1 and its main steps are recalled and explained below: 1. Identify the WC which is the structural bottleneck. As said before, this one has been the main capacity constraint for 2. Identify the conjunctural bottleneck for the bundle of MOs of 3. Among the WCs not listed in 1 and 2, identify the one 4. If all MOs have been considered, go to 5; if not, go to 3. 5. Model all the WCs that have not been found during the tural or structural bottlenecks, or are WCs that are vital to the synchronization of the MOs. All other WCs are incorporated in  X  X ggregated blocks X  upstream or downstream of the bottlenecks.
These  X  X ggregated blocks X  are modeled by neural networks, which estimate the throughput times between two bottlenecks. These Is a structural bottle neck? First work center (WC) Is a conjun ctural bottle neck? NextWC
Considering whole MO? Is the last? models permit to simulate the alimentation of the bottlenecks and so, to control the bottlenecks.

The main benefits of this algorithm are:  X  modelers can focus on the description of the bottlenecks,  X  noncrucial parts of the system are modeled with a learning approach (automatization of this modeling step),  X  the resulting model is less complex than a complete one, and  X  simulation time is shorter than with a complete model.
This paper focuses on step 5 of the reduction algorithm. The bottlenecks are considered here as known. 3.2. The multilayer perceptron (MLP)
The works of Cybenko (1989) and Funahashi (1989) have proved that a multilayer neural network with only one hidden layer using a sigmoidal activation function and an output layer using a linear activation function can approximat e all nonlinear functions with the desired accuracy. This result explains the great interest of this type of neural network, which is called  X  X ultilayer perceptron. X  In this work, the objective is to model the throughput times of parts between two bottlenecks by using information given by the system.
It is assumed that this throughput time could be approximate with a nonlinear function obtained with a MLP.

The structure of the multilayer perceptron is recalled here. Its structure is shown in Fig. 2 . The neurons of the first (or input) layer distribute just the n 0 inputs f x 0 1 , , x 0 n neurons of the next (hidden) layer. A special input neuron (depicted by a square in Fig. 2 ) represents a constant input equal to 1, and it is used to represent the biases or thresholds of the hidden layer. The output of the neurons of the hidden and of the output layers is given by a so-called  X  X ctivation function X  of the weighted sum of its inputs. The activation function of the hidden neurons is the hyperbolic tangent when the activation function of the output neuron is a linear one. The form of this neural network is given, for single output, by z  X  where x 0 h , h  X  1, y , n 0 , are the inputs of the network, w 1 i  X  1, y , n 1 , h  X  1, y , n 0 , are the weights and biases of the hidden and b are the weights and bias of the output neuron.

Kleijnen and Sargent (2000) have proposed a modeling process that can be subdivided into 10 steps:  X  determine the goal of the model,  X  identify the inputs and their characteristics,  X  specify the domain of applicability,  X  identify the output variable and its characteristics,  X  specify the accuracy required of the model,  X  specify the model X  X  validity measures and their required values,  X  specify the model and review this specification,  X  specify a design,  X  fit the model, and  X  determine the validity of the model.

In this work, these different steps are used to construct the neural network. The four first steps are related to the design of the input and output layers. The output neurons represent the information to model when the input neurons correspond to the data available in order to model the considered system.
The three last steps are related to the design of the hidden layer and to the learning of the parameters. The determination of the hidden layer and the learning of the parameters are per-formed simultaneously. For this, the learning starts from an overparameterized structure and it is performed in three steps:  X  initialization of the weights and biases of the oversized structure,  X  learning of the parameters, and  X  pruning of the spurious parameters.

The initialization of the weights and biases is performed by using an evolution of the Nguyen-Widrow (1990) algorithm proposed by Thomas and Bloch (1997) . This algorithm permits to associate a random initialization of weights and biases to an optimal placement in the input and output spaces. This method is similar to the slice linearization and permits to avoid the initial saturation of hidden neurons.

The learning algorithm used is the Levenberg X  X arquard algo-rithm with a robust criterion ( Thomas and Bloch, 1996 ). The Levenberg X  X arquard algorithm permits to associate the speed of the Hessian methods to the stability of the gradient methods. This is performed by adding a parameter multiplied by the identity matrix in order to permit the inversion of the Hessian matrix even if it is singular. The tuning of this parameter during the learning permits the Levenberg X  X arquard algorithm to work as a gradient descent algorithm when this parameter is large and as a Gauss X  Newton algorithm when this parameter is small. The use of a robust criterion permits to avoid the influence of outliers and, has a regularization effect in order to prevent overfitting. The pruning algorithm used is the Neural Network Pruning for Function Approximation (N2PFA) algorithm ( Setiono and Leow, 2000 ). This algorithm uses the mean absolute deviation (MAD) to measure the performance of the neural network. It is performed into two main steps. In the first one, the spurious hidden neurons are pruned, and in the second one, the feature selection is performed. The strategy for eliminating a hidden neuron (first step) or an input (second step) is the same and is very simple and fast. During the first step, the hidden neuron i ( i  X  1, deleted (by vanish the weight w 2 i ) and the resulting structure is evaluated by calculating the MAD values for the learning and validation data sets. The best resulting structure is compared with the initial one, and, if its MAD values are not so degraded, the considered hidden neurons is removed and the procedure is repeated until no hidden neurons can be removed. Else, the initial structure is kept. x input layer x x
The same work is performed in a second step on the input neurons. 4. Illustration of the proposition
For illustration, we use the proposed approach to build a simulation model of a sawmill workshop. The main objective of sawmill is to cut tree trunks into planks of different sizes. In this actual case, managers need a tool to help them in their weekly decision-making Master Production schedule (MPS) process. This work is resulted from collaboration with the sawmill which want:  X  to evaluate the effectiveness of its MPS,  X  to maximize its load rate, and so, its global productivity,  X  to explain some unexplained congestion phenomena of the trimmer WC.

A first work ( Thomas and Charpentier, 2005 ) with a complete model has permit to represent the congestion phenomena and to use this representation in order to improve the load rate. This model, which is recalled in Section 4.2.1, has permit to show that a load rate of the bottleneck too high (higher than 60%) degrades the productivity of this bottleneck, and so, the productivity of the sawmill. The difficulty is that the bottleneck is the last work center of the sawmill but all the influent factors on the produc-tivity of the bottleneck depends of the first work center.
However, this complete model is unusable on a real case for the dynamic evaluation of the MPS because of the time needed to modified it. 4.1. Overview of the sawmill
At the time of the study, the sawmill has a capacity of 270,000 m 3 /year, a turnover of h 52 million and 300 employees. This workshop can be described from a process point of view.
This sawmill can be represented by two linear parallel flows for the main and secondary products. This fact, associated to the variation of log dimensions lead the process to be nonlinear.
Therefore, the physical industrial production system can be divided into three main parts. To understand the functioning of the process, the course of a log from its admission into the process to its exit in planks form will be described.
 which is presented in Fig. 3 . Dashed arrows indicate the products flow. The considered log is taken into the process by using conveyors RQM1, RQM2, and RQM3. According to its character-istics (determined by scanner MS), the log is driven to conveyors
RQM4 or RQM5, which are used as input inventory for the Canter line. Only RQM5 is used here in order to simplify the presentation. After that, logs go on the first canter X  X  machine and later on the
CSMK saw, which transforms logs into square-shaped parallele-pipeds ( Fig. 4 ).
 lepipeds, produces two planks (called secondary products), which are taken out of the Canter line by the BT4 and BT5 conveyors. The log is then driven on the RQM6 conveyors, then rotated 90 stored in RQM7 awaiting its second passage on the CSMK saw.
After the second passage, the squared is completed, and two other secondary products are taken out of the Canter line by the BT4 and BT5 conveyors toward the second part of the process, the
Kockums line. The squared log is cut on the MKV saw into three planks (called main products). These main products are driven to the third part of the process, the trimmer line. The cutting of the log into main and secondary products is described in the cutting plan ( Fig. 4 ).
 machine is the Kockums saw. Only secondary products are driven on this part. The secondary products are taken in the line by the
BT4 and BT5 conveyors. They are cut by the QM11 saw, after which they reach the Kockums saw, which optimizes the planks according to the products needed. The alignment table is used as the input inventory of the Kockums saw. The secondary products (first and second passage) are finally sent to the third part of the process by the exit conveyor.

The third part of the process is the trimmer line, which is presented in Fig. 6 . This line performs the final operation of cross cutting. This operation consists in cutting up products to length.
The inputs of the line are from collectors 1 and 2, which collect the secondary and main products from Kockums and Canter lines, respectively. Saw 1 is used to perform default bleeding and Saw 2 cuts up products to length.

A previous work ( Thomas and Charpentier, 2005 ) has shown that this last machine, the trimmer saw, is the bottleneck of the entire process, and, as said previously, the productivity of the trimmer depends to the decisions taken on the canter work center. So, the impact of bad decisions is seen too much late to be corrected. So in order to evaluate decisions, managers need a simulation tool. 4.2. The simulation models 4.2.1. The complete model
The complete model of the sawmill process ( Fig. 7 ) is con-structed with the Arena s software and consists of different modules. The first module is used to model the log arrival, which follows a homogeneous Poisson process with a mean of 20. In this module, the characteristics of the log, which are measured by the scanner ( Fig. 3 ), are associated with it. In the simulation model, 2000 logs are presented at the entry of the process. The dimen-sions of these logs follow uniform distributions. At starting time, the process is empty.
 A second module, the  X  X nput sorter X , directs the logs to either
RQM4 or RQM5, according to their characteristics. It may also eject the log out of the process if it is machine-gunned or if its dimensions are out of range. The logs go to the next module, which models the RQM4 and RQM5 queues. Conveyors RQM4,
RQM5, and even RQM7, are used as input inventory for the Canter line. Two other modules are used for the simulation of the Canter line and the passage of the squared log in RQM7. The Canter line model uses two submodels for the management of main and secondary products. The Canter line has three outputs, which lead to the Kockums line for the secondary products and to the trimmer line for the main products.

The other modules, which correspond to the core of the process, are the simplest. They are used to model the Kockums and the trimmer lines and the last module is used to model the sorter of products into different racks.

The different submodels make the model more complex as shown in Fig. 7 . In particular, construction of the submodel used to manage the priority rules for choosing the input inventory that supplies the Canter line is a very complex task. The simulation ended when all the logs (with the exception of ejected ones) are cut into planks. The simulation results of this model will be used in Section 4.3.

This model has permit to explain some phenomena of conges-tion of the trimmer and so, to improve the productivity of the sawmill. However, it is not useful for the dynamical evaluation of the MPS. In fact, the construction of the complete model needs one day for an experimented people. But, the time of readjust-ment of the initial model each time an event forces to re-use the model is prohibitive. An event may force to change attributes, distribution laws, times, and even, may need to modify some flow or work centers, etc. And, this work must be performed in a very short time because we need to react quickly to the event. That is why a complete model is not pertinent and the use of a reduced model, which is quicker to construct is needed. 4.2.2. The reduced model As said previously, the bottleneck of this line is the trimmer. Consequently, modeling the function of the inventories RQM4,
RQM5, and RQM7, and of the Canter and Kockums lines is unnecessary. Furthermore, the part surrounded by the gray dashed line in Fig. 7 gave no direct and useful information for the evaluation of the MPS. In fact, only the arrival times of the products in the trimmer queue are useful for simulating the load of this bottleneck, and this is the reason for using a multilayer perceptron.
 specific sawmill neural model could be constructed. To build a neural network, we need to identify the input variables. Thomas et al. (2008) collected the available input data which can be classified into three categories: data related to the products (here the logs), data related to the process, and data related to the bill of material or routing (here the cutting plan).
 as length (lg) and three values for timber diameters (diaPB, diaGB, and diaMOY). The thickness of the finished product, may also be used. However, in a previous work ( Thomas et al., 2008 ), it was shown that thickness has no impact on the result and so it is not taken into account in this work.
 collected at the time of log arrival. In particular, we require the input stock and the utilization rate of the bottleneck, here the trimmer (Q_trim, and U_trim, respectively). The number of logs present in the process between the inputs of RQM5 and the exit of the Canter line (Q_RQM) is needed.
 information related to the cutting plan of the logs, which must be cut into main and secondary products. Here the cutting plan ( Fig. 4 ) divides the log into seven products: products can be classified into three categories, according to the location (CSMK or MKV) and the time during the cutting process (first or second cutting). This information is given by the two variables (prod and Step). The  X  X  X rod X  X  variable indicates whether products are main or secondary ones. The  X  X  X tep X  X  variable indi-cates whether the secondary products are performed during the first or second step (before or after the logs went along the RQM7 queue).
 diaGB, diaMOY, diaPB, Prod, Step, Q_trim, U_trim, and Q_RQM. In our application, 12,775 products are simulated with the complete model. These data are used to fit the behavior of the reduced model to the complete one.

The next step is to identify the output variable. Our objective is to estimate the delay ( D T ) corresponding to the duration of the throughput time for the 12,775 products. D T is measured between the process input time and the trimmer queue input time. Hence, D T is the output variable of the neural network: D T  X 
To specify the model, the number of hidden neurons needs to be determined. Therefore, a weight elimination method, N2PFA, is used to remove spurious parameters ( Setiono and Leow, 2000 ). As explained in Section 3.2, the N2PFA algorithm uses the mean absolute deviation (MAD) to determine the effectiveness of the network. This algorithm works in two steps. In the first step, it prunes the spurious hidden neurons. During the second step, the spurious inputs are pruned. In order to avoid an early stopping of the algorithm which drives to an overparametrized structure, a parameter must be tuned which permits a slightly degradation of the MAD values in exchange of the suppression of one neuron.
This parameter is tuned to 0.025. With this choice, the deletion of one neuron which degrades the MAD values of 2.5% is accepted.
Therefore, the learning begins with a structure using 15 hidden neurons (2), which correspond to 166 parameters.
The learning of the network is supervised. Hence, it is neces-sary to divide the database into two data sets, namely, learning and validation. The database is constructed with the complete model, which is used as reference. To fit the model, we use the learning algorithm called the  X  X evenberg Marquard algorithm with robust criterion X  ( Thomas and Bloch, 1996 ). The learning approach corresponds to a local search of a minimum and the results may differ according to the initial weights. To evaluate the dispersion of the results, 50 different sets of initials weights are used. The pruning procedure led to the preservation of the nine inputs in 86% of the cases. In the other cases, only the input U_trim (utilization rate of the trimmer) is removed. The number of hidden neurons after pruning varied from 4 to 14. Fig. 8 presents the distribution of the hidden neurons number during the 50 trials.
This figure shows that, in 88% of the cases, the number of hidden neurons remaining after pruning ranged from four to seven. Table 1 presents the means and the standard deviations of the residuals for the 50 trials on the learning and the validation data sets.

Table 1 shows that, even though the network structure may vary, the results obtained on the learning and validation data sets are very close to each other. Moreover, when the obtained residuals in these different cases are studied, we notice that the worst results are generally obtained when few hidden neurons are pruned. However, the best results are obtained when all the nine inputs are preserved and only four to seven hidden neurons are retained. The selected structure used nine input neurons and seven hidden neurons, corresponding to 78 parameters. With this structure, the means of the obtained residuals on the learning and the validation data sets are very close to 0 (0.0012 and 0.0865, respectively). The standard deviations of the residuals are 53.075 and 53.826 on the learning and validation data sets, respectively.
To determine if some dynamics existing in the data are not taken into account by the learning process (i.e. the learning process has failed), the correlation between the different inputs and the residuals is performed on the learning and validation data sets ( Table 2 ).

Table 2 presents the mean, the standard deviation, the mini-mum, and the maximum values of the correlation coefficient absolute value between the nine inputs and the residuals obtained with the 50 different trials (50 different neural models) (for the learning and validation data sets). These results show the variables  X  X  X g X  X  (length),  X  X  X iaGB X  X  (great diameter of the log),  X  X  X iaMOY X  X  (medium diameter of the log),  X  X  X tep X  X  (time of produc-tion of the secondary products),  X  X  X _trim X  X  (input queue of the trimmer), and  X  X  X _trim X  X  (utilization rate of the trimmer) are always not correlated with the residuals.

The variables  X  X  X rod X  X  (main or secondary products) and even-tually  X  X  X iaPB X  X  (smallest diameter of the log) and  X  X  X _RQM X  X  (number 0 5 10 15 20 25 30 distribution (%) of logs present in the process between the input of RQM5 and the exit of the Canter line) are also generally not correlated with the residuals. In some rare cases, these variables and the residuals are correlated. However, these cases correspond to the network struc-tures where too many hidden neurons are kept (more than seven hidden neurons). It can be noticed that the minimal value for the correlation between  X  X  X _trim X  X  (utilization rate of the trimmer) and residuals is 0. This could be attributed to the pruning of the input in some cases. For the selected structure, the coefficient of correlation between inputs and residuals is never more than 0.022505.
Now, we consider the residuals obtained with the selected structure. For the learning data set, the mean of the residual is very close to 0 (0.0012) and it represents an error lesser than 2.7% of the throughput time D T . This result shows that the structure of the neural network used is sufficient for learning D T . Those obtained on the validation data set corroborate these results. On the validation data set, the mean of the residuals is also close to 0 (0.0865) and the residual represents an error lesser than 2.3% of
D T . Therefore, we conclude that no overfitting problem occurs and the neural network can estimate the delay for data sets other than the learning one.

Based on these results, we can conclude that our neural network is a good representation of this part of the process.
The neural network model obtained is included into the reduced model shown in Fig. 9 . The modules  X  X og arrival, X   X  X nput sorter X , and  X  X rimmer X  in the reduced model are identical to those used in the complete one. Only the part surrounded by a gray dashed line is replaced. A comparison between the complete ( Fig. 7 ) and reduced ( Fig. 9 ) models shows that the model complexity is greatly reduced. In particular, the different sub-models are removed from the reduced model. The reduced model can be so constructed in one hour with an automated procedure. 4.3. Evaluation of the reduced model
In this section, we compare the results obtained with the reduced and the complete models.
 trimmer as a function of time (s). This comparison is performed with two different data sets obtained under the same condi-tions. Fig. 10 a shows that the two models present the same type of queue evolution. However, one difference can be noticed: the trimmer queue value of the complete model between 0 and 500 s increases to 90 before decreasing to values very similar to those obtained with the reduced model. The observed difference between the two values curves are due to the models initializa-tion. The log arrival in the two models follows a homogeneous
Poisson distribution with a mean of 20 s. In addition, many parameters of the models follow a stochastic process. The initi-alizations of the models produce an edge effect, which could explain the differences in behavior between the two models ( Thierry et al., 2008; El Haouzi et al., 2008 ).

Fig. 10 b shows the utilization rate of the trimmer as a function of time (s) for the two models (complete in black and reduced in gray). The two models present a similar evolution of the utiliza-tion rate and converge to the same value after 1000 s.
To confirm that the neural network included in the reduced model has correctly learned the process, we investigate the results obtained by using a homogeneous Poisson process with different means as log arrival rule. As an example, Fig. 11 presents the utilization rate of the trimmer when the mean of the Poisson process is 30 s (compared with the 20 s used previously). Fig. 11 shows that the two models gave similar results and that the utilization rates converge to the same value of 33% for the two models (compared with the value of 49% obtained previously).
Similar results are obtained when other Poisson processes are used as log arrival rule ( Fig. 12 ).

These results show that a reduced model gives similar results than a complete one. Therefore, it is relevant to use reduced model instead of complete one because it is quicker to construct and use without loss of precision. 5. Conclusion
A new approach for simulation model reduction has been presented here. This approach uses a neural network and, more particularly, a multilayer perceptron to model the functioning of a part of the process that is not constrained in capacity. This approach has been applied to the modelisation of a sawmill workshop. The results show that:  X  the two data sets present similar results,  X  the average of the error is small relative to the process time scale, and  X  the complete and reduced models gave similar results even if the log arrival rule is changed.

This means that it seems efficient to use a neural network to model a part of a process instead of c onstructing the complete model.
Assuming that the construction of a neural network is a quasi-automated task, in which the modeler only collects and selects the input data set. It is faster and easier to construct this kind of reduced model. This approach allows the modeler to focus on the management of bottlenecks.

Our intentions for future work are to investigate the structure determination of the neural network, particularly the choice of its inputs, and the validation of this approach on different applica-tions, particularly on several external supply chains, such that at least one particular enterprise belongs to different supply chains. References 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5
U 0.05 0.1 0.15 0.2 0.25 0.3 0.35
U
