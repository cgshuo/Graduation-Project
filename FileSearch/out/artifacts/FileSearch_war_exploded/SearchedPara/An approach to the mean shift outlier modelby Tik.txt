
Department of Mathematics, Dicle University, Diyarbak X r, Turkey Institute of Applied Mathematics, Middle East Technical University, Ankara, Turkey 1. Introduction 1.1. Foundations generated by some different mechanism. Hawkins (1980) called such an observation an outlier variable . to deviate markedly from other members of the sample in which it occurs [33]. A geometrical idea of outliers is shown in Fig. 1.
 outliers become s rather difficult because of the eff ects of masking and swamping [5]. Masking occurs when one ou tlier is not det ected because of the presence of other s, while swamping occurs whenever a non-outlier is wrongly identified, caused by t he effect of some hi dden outliers [7].
In such cases, observations such as these will cause biased parameter estimations; then, a statisti-regression coefficients. Therefore, it is very important to identify these observations and bound their methods is provided by constructing a Tikhonov regularization problem; however, the problem is solved using Conic Quadratic Programming (CQP), with its very efficient interior point methods [24,34]. A problem that is closely related to quadratic programming is the Second-Order Cone Program (SOCP). The standard form for SOCP problem is or, partitioning the data matrices [ A i ; b i ] by with D i being of the type ( n i  X  1)  X  (dim x ) , the problem can be written as Here,  X  2 is the Euclidean norm, and L n i is the n i -dimensional ice-cream second-order (or Lorentz ) cone defined by
In this form, D i represents matrices of the same row dimension as x . Furthermore, the lengths of problem can be stated as follows: can be expressed in the following form: The design variables in this latest problem are the column vectors  X  i , which have the same dimensions as the vectors d i , and the reals  X  i ( i =1 , 2 ,...,k ) [1,8,19].
 In Section 2, the linear regression and some of the direct outlier identification methods are reviewed. In Section 3, the Mean-Shift Outlier model is considered, and three types of Tikhonov regularization problems for this model are presented. In Section 3, this problem is solved by continuous optimization methods. Finally, in Section 4, some applications to compute, investigate, and conduct numerical com-parisons of the performance for LM, MSOM, and CTMSOM are provided and are applied on data sets that possess different characteristics . 1.2. Notation where  X  T  X  represents transposition. A matrix of variables is written in boldface and uppercase, such model), MSOM (Mean Shift Outlier Model), and CTMSOM (Mean Shift Outlier Model based the Conic used in this report. 2. Methods for outlier identification
The standard linear model (LM) [3], with n observations (response data) and p independent variables, is given by parameters, and  X  is an n  X  1 vector of independent, identically distributed random errors whose con-is an unknown parameter, and I n is the identity matrix of order n . Assuming n p, the least-squares estimates of  X  and  X  2 are given by  X   X  = X T X H is the hat operator H = X X T X we do not need the assumption n p. For LM, there are different approaches for outlier identification. These approaches are separated into two categories  X  direct approaches and indirect approaches  X  X sing residuals from the robust fit.
 separate the data set into a clean subset without outliers and a complementary subset that contains all Here, k is the number of observations that are possible outliers. Then, they compute scale d i based on the clean subset U as follows: mean square, respectively, indexed by U . As a third step, they arrange the observations in ascending | d a new subset, U ,takingthefirst s +1 ordered observations. If n = s +1 , then they declare no outliers in the data and stop computation; otherwise, they continue to the second step. Their second approach is to [7], the success of the procedure is based on the initial clean subset of data. This procedure works fail when the sample contains a set of several high-leverage outliers. This type of deletion technique reveals a strong leverage and influence on the regression coefficients.
 An indirect approach to outlier identification can be achieved through a robust regression estimate. bust regression, however, the methods that are most commonly used today are M estimation (this class of estimators can be regarded as a generalization of Maximum-Likelihood estimation) [20] and Least Trimmed Square (LTS) estimation [22]. However, in this report, an outlier identification method called Mean Shift Outlier Model [27,29] (see. SubSection 3.1) is considered, and it is a method different from both M estimation and LTS estimation. 2.1. Mean shift outlier model
In evaluating the given outliers detection methods, the Mean-Shift Outlier Model (MSOM) should not be omitted, which is an indirect method. In fact, because the MSOM gives the same residual sum of the regression model in the presence of outliers. This model is given by ( y i , x i  X  ) would hence be an outlier. To check this fact, we test the hypothesis against the alternative using the likelihood-ratio test statistic [6] where SSE ( H 0 ) is the residual sum of the squares in the model y = X X  +  X  , containing all the n observations, and SSE ( H 1 ) is the residual sum of squares in the model y = X X  + e i  X  +  X  , i.e., and SSE ( H 1 ) can be written as estimator of the  X  2 i is defined by the test statistic Eq. (3) may be written as where r  X  i is called the i th externally Studentized residuals [6].

According to the Theorem of Beckman and Trussel [6], if rank ( X  X  i )= p and  X   X  N (0 , X  2 I n ) ,then zero as h ii increases. Therefore, the detection of outliers becomes difficult when h ii is large [6]. 3. MSOM revisited by tikhonov regularization
As we have mentioned, the goal of robust regression is to provide resistant (stable) results in the larization [4] method for the MSOM because the Tikhonov regularization belongs to the most commonly used methods of making ill-posed problems well-pose d (regular or stable) [26].

There are different types of basic formulation of the Tikhonov regularization problems, all of which are represented by minimization problems. A Tikhonov solution can be expressed quite easily in terms of the singular value decomposition ( SVD )ofthecoefficientmatrix X of the general linear regression model Eq. (1) using the MATLAB Regularization Toolbox [26]. In this report, a Tikhonov regularization problem is constructed for the MSOM, and it can be solved using Conic Quadratic Programming [1]; this problem will be called CTMSOM. 3.1. Tikhonov regularization problems for MSOM
For CTMSOM, we try to construct minimization problems mentioned by the Tikhonov regularization for the MSOM. Let us assume that m observations y i ,where m&lt;n , deviate systematically from the corresponding model y i = x i  X  +  X  i by  X  i using one of the direct methods such as the Hadi and is, m observations are outliers. Then, the MSOM can be written as where X is a full rank n  X  p matrix of independent or explanatory variables, E is an n  X  m matrix with variables. Then, the MSOM can be stated as where X  X  =( X | E ) is an n  X  ( p + m ) block matrix constructed by the matrices X and E and  X   X  = (  X  ple [31], the Tikhonov regularization problems for the regression model Eq. (7) can be written as or as where  X  1 is an upper bound for the noise level Eq. (7) of the observation and is regarded as known a-priori. As  X  1 increases, the set of feasible models expands and the minimum value of minimize  X   X  2 decreases. We can thus trace a curve of minimum values of minimize  X   X  2 versus  X  1 . Additionally, it is possible to trace this curve by considering problems of the form Eq. (13) based on the upper bound  X  2 .As  X  2 decreases, the set of feasible solutions becomes smaller and the minimum value of y  X  X  X   X   X  2 [26]. One interpretation of a Tikhonov regularization problem is that we consider model problem arising when we apply the method of Lagrange multipliers to Eq. (8) [26]. Here,  X  2 can be considered a penalty parameter. This parameter establishes a tradeoff between both accuracy, i.e., a small sum of when plotted on a log-log scale, the curve of the optimal values of  X   X  2 and y  X  X  X   X   X  2 often take value configuration point at the L-curve is the corner point, which is marked by a star. The L-curve, of an upper bound; in case of several such parameters, we obtain an efficiency surface. All these types In addition to the discrepancy principle, another popular criterion for picking the value of  X  is the of the L-curve [26]. In fact, the use of these IPMs is one of the main advantages that is prepared by our model-based mathematical approach. For this reason, problem formulation Eq. (9) is our preferred realization of the Tikhonov regularization. 3.2. An alternative solution for tikhonov regularization problem with conic quadratic programming
The Tikhonov regularization problem Eq. (14) can be addressed through CQP . Formulation Eq. (14) can be easily formulated as a CQP problem. Based on an appropriate choice of a bound M ,thefollowing optimization problem can be stated: Now, the optimization problem can be written as follows: or, equivalently, Using modern methods of continuous optimization techniques, especially those from CQP , we refer to the basic notation [1] where k is the number of constraint functions that are defined according to the optimization problem at hand. If optimization problem Eq. (10) is considered, the objective function is moved to the list of is a CQP program with The dual problem to the latter primal problem is given by Moreover, ( t,  X   X  ,  X  ,  X  ,  X  1 ,  X  2 ) is a primal dual optimal solution if and only if
To solve  X  X ell-structured X  convex problems such as CQP problems, IPMs can be used [15,34], which were initially introduced by Karmarkar (1984). These al gorithms have the advantage of employing the structure of the problem, allowing better complexity bounds and exhibiting a much better practical per-formance. 4. Numerical comparison of the perf ormance for LM, MSOM, and CTMSOM In this section, the performances of three methods  X  X M, MSOM, and the MSOM alternative CTMSOM X  are compared according to some general measures. Initially, the data sets and the perfor-mance measures are described. Then, the outcomes of the comparison studies are presented. 4.1. Data sets
To compare the performance of the LM, MSOM, and CTMSOM, seven data sets, with different sam-ple sizes ( n ) and numbers of independent variables ( p ), are selected from regression test problems. Information on the data sets used in this study is presented in Table 1.
 Data 1: Thefirstdatasetisthe delivery time data taken from Rousseeuw and Leroy [21, p. 155], Table 23 [21]. In this data set, which contains 25 observations collected for the service of a vending by the route driver.
 Data 2: As the second data set for comparison, the stack data set is considered. This data set is a well-known stack-loss data set presented by Brownlee (1965) and was taken from SAS Customer Sup-port [28]. The data describe the operation of a plant for the oxidation of ammonia to nitric acid and Data 3: The third data set, a simulation data generated by using MATLAB, has 100 observations. To determine which methods perform better, a test problem is taken from [34]. In this problem, data are artificially created from the following function: obtain the response, normally distributed noise with a mean of zero is added to the function f ( X ) . The data set includes 103 observations. There are 7 input variables, which are cement, slag, fly ash, water, SP, coarse aggregate, and fine aggregate, and an output variable known as 28-day compressive strength (MPa).
 Data 5: Our fifth data set is a synthetic data set known as the Hawkins data . This data set has 128 observations ( n = 128), 8 independent variables ( p = 8), and one outcome. More information about this data set can be found in [16].
 Data 6: The sixth data set is the Concrete Compressive Strength data [32]. Based on the sample size, aggregate.
 Data 7: The last data set, which is taken from UCI Machine Learning Repository, is a housing data observations and 13 explanatory variables. The response variable is a median value of owner-occupied houses with values of 1000 s of USDs. 4.2. Performance measures used in comparison
The goal of these applications is to find the best method among the LM, MSOM, and CTMSOM in the the performance measures applied on the LM, MSOM, and CTMSOM are compared. The performance measures with their general notations are presented as follows: Mean Absolute Error (MAE): MAE is the average magnitude of the error; it is defined by The smaller value for this measure gives the better model.
 Root Mean Square Error (RMSE): RMSE is the square root of the mean square error (MSE), defined as The RMSE represents an unbiased estimate of the error variance, and it depends on both the residual and number of predictive variables. Similar to the MAE, smaller values for the RMSE indicate a better model.
 come higher, a better fit is obtained for the model. The formulation of R 2 is as follows: Percentage of Residuals within Three Standard Deviation of Mean (PWI): PWI is obtained by the sum of indicator variables over all observations divided by the total number of observations. The indicator variables take the value of 1 if the absolute value of the difference between the actual and predicted response is within some user-specified thresholds [10]. In this study, 3 standard deviations of mean is used as a threshold.
 for comparing models with different numbers of independent variables. The higher the Adj-R 2 value is, the better the model fits the data. The formula is: where n  X  p  X  1 =0 [14]. Here, p is the number of parameters in the regression equation for both the RMSE and Adj-R 2 . 4.3. Applications and results
In these applications, a special code is written using MATLAB [16] for CTMSOM; to solve the CQP problem, MOSEK software is preferred [17]. While implementing the LM, MSOM, and CTMSOM, MATLAB is used.

To find the potential outliers for each data set, we apply the following outlier detection procedure: 1. The multivariate linear model is constructed to fit the data. 2. The fit values and ordinary residuals are computed using the model from step 1. 3. Studentized residual and Cook X  X  distance values are calculated. After applying these steps to all the data sets, Table 2 provides the number of potential outliers. To all possible subset regressions of MSOM are constructed for this data set.
 large. The Studentized residual value at point 9 is considerably larger than the standardized residual. deletion of observation 9 would reduce the error and provide a good estimate. After deletion of the 9th observation, subset regressions of MSOM as Model 2 are built. When comparing these two models, Model 2 has better performance than Model 1 according to all performance measures, as expected (see Table 3). The next largest value of ordinary residuals, the Studentized residual, and Cook X  X  distance gives better performance than Model 1 (see Table 3). On the other hand, the best estimation performance is obtained when potential outliers at point 9 and 22 are simultaneously deleted. Model 4, the subset regressions of MSOM, is constructed after deleting observations 9 and 22. It has better performance according to the all performance measures than Model 1, Model 2, and Model 3 (see Table 3). Model 1 : The model has variables X 1 and X 2 ; Model 2 : The model has variables X 1 , X 2 ,and  X  9 ; Here,  X  9 represents a potential outlier for the 9th observation.
 Model 3 : The model has variables X 1 , X 2 ,and  X  22 ; Here,  X  22 represents a potential outlier for the 22nd observation.
 Model 4 : The model has variables X 1 , X 2 ,and  X  9 ,  X  22 ; After construction of the LM, MSOM, and CTMSOM, the performance measures that are described in Subsection 4.2 are computed. The results are presented in Table 3. As can be seen from Table 3, the LM is not accurate compared to the MSOM and CTMSOM, according to all performance measures. However, the MSOM and CTMSOM, fitted with X 1 and X 2 and with observation 9 removed, have better values in terms of the Adj-R 2 , MSE, and RMSE. On the other hand, the MSOM and CTMSOM, fitted with X 1 and X 2 and with observation 22 removed, do not give a promising performance compared with the previous model. This means that removing observation 22 does not provide a better estimate than removing observation 9. MSOM and CTMSOM, fitted with X 1 and X 2 and with observations 9 and 22 removed, show good fits according to MAE and R 2 because these models do not include X by Rousseeuw and Leroy [21]. In conclusion, according to all performance measures, the CTMSOM produces as good results as the MSOM.

The same procedure is applied to the other selected data sets. For the second data set, the model with representation of the outlier observations for the stack data can be seen in Fig. 3.

The performance measures of the LM, MSOM, and CTMSOM are calculated for all data sets, and the the LM and MSOM, according to all the chosen measures. For the other data sets, the CTMSOM and MSOM produce the same performance with respect to all the performance measures. When considering outliers, the results for the LM show a weak performance compared to the CTMSOM and MSOM. The CTMSOM can be successfully applied in a data set containing outlier observations as an alternative for the MSOM.

To compare the efficiencies of the LM, MSOM, and CTMSOM, the elapsed times (in seconds) for each data sets are recorded on a computer with an Intel (R) Pentium (R) CPU 3.40 GHz processor and 32-bit Windows operating system. The maximum elapsed times obtained for all data sets are given in Table 5. Depending on the results, the LM and MSOM have almost similar efficiencies for all data sets. On the other hand, the run time for the CTMSOM is less than that of the MSOM and LM. When the the LM and MSOM. However, the efficiency of the CTMSOM is not affected by the size of the data set because the CTMSOM uses the CQP based on interior point methods. 5. Concluding remarks
This report provides a new contribution to the challenges of the mean shift outlier regression problem by enabling the accessibility and usability of modern methods of continuous optimization, especially CQP. Herewith, a bridge has been offered between statistical learning, inverse problems and the pow-erful tools prepared for well-structured convex optimization problems. In this current report, the Mean Shift Outlier regression problem was represented as a conic quadratic problem and achieved excellent this bound. With this report, the door has been opened to a broad area of future research challenges in theory, methods and applications, in which we invite the reader to participate. These studies can range from the inclusion of interactions, e.g., by CMARS [9,11,25] and its new robust version, RCMARS [2], analysis and control in manufacturing, early-warning systems in meteorology, ecology and the financial sector, especially for a new view on the analysis and control of financial  X  X ubbles X .
This study on the detection and following treatment of outliers benefits from prior data and, hence, model regularization, which we achieve with the help of CQP. Furthermore, the handling of outliers in the same direction of  X  X moothening X  the data via a model that represents core information and are orchestrated in this type of report.
 whichiscalleda spike . As time proceeds, it can be regarded as a double jump: a jump up (or down) followed by a jump down (or up). Here, the second jump can be regarded as a type of (maybe partial)  X  X ithdrawal X  of the first jump, which explains the outlier property of the spike. On the other hand, in finite (double) jump X , and through infiniteness, it becomes very essential and systematically employed. Acknowledgements
Fatma Yerlikaya- X zkurt is supported by the Turkish Scientific and Technological Research Council of Turkey (TUBITAK) Domestic Doctoral Scholarship Program.
 References
