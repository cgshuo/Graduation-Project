 We study the problem of domain adaptation, which aims to adapt the classifiers trained on a labeled source domain to an unlabeled target domain. We propose a novel method to solve domain adaptation task in a transductive setting. The proposed method bridges the distribution gap between source domain and target domain through affinity learning. It exploits the existence of a subset of data points in tar-get domain which distribute similarly to the data points in the source domain. These data points act as the bridge that facilitates the data similarities propagation across do-mains. We also propose to control the relative importance of intra-and inter-domain similarities to boost the similarity propagation. In our approach, we first construct the similar-ity matrix which encodes both the intra-and inter-domain similarities. We then learn the true similarities among data points in joint manifold using graph diffusion. We demon-strate that with improved similarities between source and target data, spectral embedding provides a better data rep-resentation, which boosts the prediction accuracy. The effec-tiveness of our method is validated on standard benchmark datasets for visual object recognition (multi-category). H.4 [ Information Systems Applications ]: Miscellaneous; D.2.8 [ Software Engineering ]: Metrics X  complexity mea-sures, performance measures Domain Adaptation, Affinity Learning
In recent years, domain adaptation has gained significant attention in many areas of applied machine learning, includ-ing bioinformatics, speech and language processing, com-puter vision etc. In these practical problems, given that the instances in the training and testing domains may be drawn c  X  from different distributions, traditional learning method can not achieve good performance on the new domain. Domain adaptation algorithms are therefore designed to bridge the distribution gap between training (source) data and testing (target) data. Domain adaptation methods seek to eliminate the difference between source and target distributions.
In this paper, we propose a transductive method to ex-plicitly improve intra-and inter-domain similarities. Our contribution is two-fold: first, we perform affinity learning via graph diffusion to bridge the distribution gap of source and target domain. The key idea is to exploit the existence of a subset of data points in the target domain which dis-tributes similarly to the data points in the source domain. We denote this subset of data points as Bridge Points (BP). Through graph diffusion, we propagate the similarities be-tween BP and other data points in the target domain, as well as the similarities between BP and data points in the source domain. Affinity learning is able to give robust pair-wise similarities of data points, since all paths between all pairs of data points are considered. In this way, affinity learning can bridge distribution gap of source and target domain. As our experimental results clearly demonstrate, our assumption that part of the target data is similar to part of the source data is often satisfied by real world data sets. Figure 1 illustrates our motivation. Our second contri-bution is to adjust the intra-and inter-domain similarities. The intuition is that data points in the same domain are often more similar to each other than to those in different domain. In graph diffusion process, this makes the similar-ity propagation from data points in source domain to data points in target domain ineffective. Therefore, the proposed adjustment of the intra-and inter-domain similarities is a key step in making the affinity propagation successful. We balance the intra-and inter-domain edges by picking equal number of nearest neighbors in source and target domain for each data point and also re-weight intra-and inter-domain edges.

In summary, given the similarity matrix of source and target data, the procedure of our framework includes the following key steps: 1) Similarity Adjustment: re-weight intra-and inter-domain similarities. 2) Affinity Learning: iteratively learn similarities in joint geometric structure via Tensor Product Graph Diffusion (TPGD)[18]. 3) Spectral Embedding: apply spectral embedding on diffusion matrix to get a low-dimensional representation. In this paper, we use Tensor Product Graph Diffusion(TPGD) [18] to capture the joint manifold structure for the source and target do-main. As demonstrated in [18], TPGD can robustly discover the true, underlying manifold structure in image retrieval. We utilize TPGD to learn joint geometric structure in the context of domain adaptation when training and testing are drawn from different distributions.

The rest of the paper is organized as follows. We first give a brief review of related works in Section 2. In Sec-tion 3, we describe the proposed affinity learning for domain adaptation task. In particular, we describe how to construct the transition probability matrix with similarity adjustment. We also show how to perform graph diffusion on a tensor product graph to obtain robust similarities. In Section 4, we present our experimental results on benchmark datasets and compare it to several state-of-the-art methods. Finally, we come to the conclusion is in Section 5.
Domain adaptation has been extensively studied in many research areas [13, 8, 2, 19]. Domain adaptation can be cat-egorized into three types. The first type are self-labeling approaches, which include self-training [14] and co-training [2]. The second type of algorithms proposes to weight or se-lect training instances to minimize the discrepancy distance [7, 9]. Our work belongs to the third type, which aims at finding  X  X ood X  feature representations to minimize domain divergence and classification error, such as [1, 5, 12]. In particular, for object recognition application in computer vision, many works have been proposed to learn new feature representation, such as [11, 16, 4]. Compared to existing approaches, our method focus on affinity learning to bridge the distribution gap between source and target domain.
While our work share some common components com-pared to graph-based semi-supervised method, such as [20] where graph is used to propagate labels, the key difference is that we aim to solve the domain adaptation problem and our goal is to use affinity learning to improve the noisy pair-wise similarities due to domain shift. That motivates us to reweight the inter-and intra-domain edges, and use spectral embedding to obtain the low dimensional domain-invariant data representation.

There are also several works attempting to solve transfer learning in a transductive setting [15, 10, 3]. They apply la-bel propagation to zero-shot and few-shot learning based on attribute graph or semantic graph. [17] exploits the mixture distribution to refine the classification labels. These work did not try to improve pair-wise similarities. We assume that our data originate from two domains, Source (S) and Target (T). Source data D S = { ( x 1 S ,y  X  X  X  , ( x N S S ,y N S S ) } is fully labeled, each pair ( x R d  X  y space. The source data are sampled from some distribution P S ( X,Y ). The target data has equal dimen-sion d as the source data but is sampled from a differ-ent distribution P T ( X,Y ). We denote the target data as D known. Given D S and D T , our goal is to infer the class labels of data points in D T .

In the rest of this section, we first describe how to con-struct the transition matrix P for source and target data jointly. We then iteratively learn the joint geometric struc-ture and capture true similarities among data points in source and target domain. After we get the diffusion matrix, we compute the Laplacian graph and solve the smallest K eigen-vectors to obtain a new feature representation of data points in source domain and target domain. After that, any classi-fication approach can be adopted to predict labels for target data. In this work, we choose SVM classifier with linear kernel.
The goal of this section is to construct the transition ma-trix of a graph G whose nodes consist of data points in both source and target domain. We use P SS and P TT to de-note the transition probability matrices of data points in the source and target domains respectively. P ST and P TS de-note the transition probability matrix of data points across domains. We construct the overall transition matrix as fol-lows: where  X  controls the relative importance of the intra-and the inter-domain transition probabilities and  X   X  [0 , 1]. Em-pirically,  X  can be set by solving  X  1  X   X  = 2 N S N T N 2 brates the average inter-and intra-domain edge weights to be close.

P SS and P TT are row-wise normalized similarity matrices which are computed as: where A SS is the similarity matrix of data points in source domain and A TT encodes the similarities between data points in target domain. D SS and D TT are the diagonal matrices of the row sums of A SS and A TT .

To compute A SS , we take advantage of the available labels of source data, and define  X  X loseness X  in a supervised man-ner, i.e., nodes i and j are connected if x i and x j share the same label. The similarity matrix A SS is defined as follows: A
SS ( i,j ) = Here, we use N p to denote the p nearest neighbors. In the source domain, the label information is embedded into the similarity matrix. While we compute the similarity ma-trix A SS with the cosine similarity measure, other similarity measures may also be applicable.

To compute A TT , since the labels of target data are not available, we define  X  X loseness X  in an unsupervised manner, i.e., nodes i and j are connected if i is among p nearest neigh-bors of j or j is among p nearest neighbors of i . Formally, we have: A
TT ( i,j ) = where N p ( x i T ) is the set of p nearest neighbors of x
Similarly cross-domain transition probability matrices P ST and P TS are computed as follows: where A ST denotes the cross-domain similarities. D ST and D TS are the diagonal matrices of the row sums of A ST and A TS . A ST is computed as follows: A
ST ( i,j ) = To summarize, we introduce two major differences in the transition matrix construction process which are tailored for unsupervised domain adaptation task: First, we add super-vised information to the similarity matrix A SS to remove noisy entries in A SS . Second, we control the relative impor-tance of intra-domain and inter-domain transition probabil-ities. When building KNN connected graph, we balance the intra-and inter-domain edges by picking equal number of nearest neighbors in source and target domain for each data point. We also perform a reweighting on the intra-and inter-domain similarities. As demonstrated by our experiments in Section 4.1, these two steps greatly boost the performance of affinity learning which results in a better data represen-tation, and therefore a higher prediction accuracy.
In this section we review tensor product graph diffusion process introduced in [18]. Given the edge (transition prob-ability matrix) P , we define Q (1) = P and where I is the identity matrix. We iterate (7) until conver-gence. Let us denote the limit matrix by Q  X  = lim t  X  X  X  Q A closed form expression for Q  X  is as follows: The proof of the convergence of (7) and closed form equation can be found in [18], where P is the tensor product of P with itself. Since Q  X  = P  X  , we obtain that the iterative algorithm on Q defined by (7) yields the same similarities as the TPG diffusion process on P for a sufficient number of iterations.
In this section, we present our experimental results on vi-sual object recognition tasks. We set K and p in our method through cross-validation based on classification error of data samples in source domains. We first compare to the base-line approach and evaluate the performance gain at each step and give detailed analysis. This provides clear insight about the merits of the proposed method. Our results on benchmark datasets are also favorable when compared to several state-of-the-art domain adaptation methods.
We perform experiments using 4 object recognition datasets, which includes: Amazon, Webcam, DSLR. These three datasets are first introduced in [16]. Additionally, we use Caltech-256 in [6] as the fourth dataset to further evaluate the proposed methods. Each dataset is treated as a domain and 10 com-mon object categories are extracted. We downloaded the processed datasets with SURF features from [5]. We con-duct each experiment using every pair of source and target dataset. We report the recognition accuracy on every pair of source and target dataset.
As the baseline approach, we adopt the original features and train a linear SVM model on source domain. To il-lustrate the significance of performance gain using affinity learning to facilitate domain adaptation, we study four vari-ants of our method. In the first variant, we apply spectral embedding directly to the original similarity matrix. In the second variant, we add intra-and inter-domain similari-ties reweighting before applying spectral embedding. In the third variant, we apply graph diffusion to the original sim-ilarity matrix before applying spectral embedding. In the final variant, we put all components together which is the proposed approach.

We compare the recognition accuracy of the baseline ap-proach and the 4 variants in Tabel 1. We can see that low-dimensional feature representation obtained by spectral embedding can preserve most of the information for each dataset, whose accuracy is comparable to the baseline, but with no improvement. If we adjust intra-and inter-do-main similarities and apply spectral embedding, the aver-age recognition accuracy improves 7 . 5% compared to that of the baseline. If we apply affinity learning and spectral embedding together, the average recognition accuracy im-proves 4 . 6% compared to that of the baseline. If we combine adjusting intra-and inter-domain similarities and affinity learning through graph diffusion, the performance improves 10 . 0% compared to that of the baseline method. Overall, these results demonstrate that adjusting intra-and inter-domain similarities can facilitate the affinity learning, and affinity learning can provide more reliable affinities for data points in joint manifold.
We compare the proposed method to several state-of-the-art methods: KMM [7], TCA [12], GFK [5], LandMark [4]. Table 2 summarizes accuracy of object recognition on 8 pairs of source and target domains obtained from the four datasets. For the compared methods, most results are quoted from [4], except for D-A and D-W which we generated using the code downloaded from authors X  websites. The average recognition accuracy of our method improves 3 . 7% when compared to that of the second best method  X  X andMark X . Our method performs the best on 6 out of 8 pairs of do-mains. We propose a novel transductive domain adaptation method. Empirical results clearly demonstrate that it outperforms state-of-the-art methods. This work was in part supported by NSF under Grants OIA-1027897 and IIS-1302164. [1] John Blitzer, Ryan T. McDonald, and Fernando [2] Minmin Chen, Kilian Q. Weinberger, and John [3] Yanwei Fu, Timothy M. Hospedales, Tao Xiang, [4] Boqing Gong, Kristen Grauman, and Fei Sha.
 [5] Boqing Gong, Yuan Shi, Fei Sha, and Kristen [6] G. Griffin, A. Holub, and P. Perona. Caltech-256 [7] Jiayuan Huang, Alexander J. Smola, Arthur Gretton, [8] Hal Daum  X e III. Frustratingly easy domain adaptation. [9] Jing Jiang and ChengXiang Zhai. Instance weighting [10] Yu-Gang Jiang, Jun Wang, Shih-Fu Chang, and [11] Brian Kulis, Kate Saenko, and Trevor Darrell. What [12] Sinno Jialin Pan, Ivor W. Tsang, James T. Kwok, and [13] Sinno Jialin Pan and Qiang Yang. A survey on [14] Rajat Raina, Alexis Battle, Honglak Lee, Benjamin [15] Marcus Rohrbach, Sandra Ebert, and Bernt Schiele. [16] Kate Saenko, Brian Kulis, Mario Fritz, and Trevor [17] Dikan Xing, Wenyuan Dai, Gui-Rong Xue, and Yong [18] Xingwei Yang, Lakshman Prasad, and Longin Jan [19] Chao Zhang, Lei Zhang, Wei Fan, and Jieping Ye. [20] Xiaojin Zhu, Zoubin Ghahramani, and John D.

