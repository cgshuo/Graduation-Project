 Collaborative filtering (CF) has been very successful in both research and application domains such as information filtering and E-commerce. The k-Nearest Neighbor (KNN) method is a popular memory-based CF algorithm. Its key technique is to find k nearest neighbors for a given user to predict his interest using the information of his neighbors. However, this method suffers from a fundamental problem: sparsity. Spar-sity problem occurs because most users only visit a few among the whole Web pages and hence the user-page rating matrix that is generated from such user navigation actions is very sparse, which accordingly af fects the CF accuracy of the KNN method. 
To solve the sparsity problem, clustering method is proposed, which works by identifying groups of users who appear to have similar preferences [2][9][11][18]. potential weakness of the clustering is that any individual user with strong personality will lose its personal properties, since the prediction is an averag e across the clusters, weighted by degree of participation. And in some cases, the clustering-based method performed even worse than basic KNN algorithm [2] on a sparse data set. Despite the conclusive findings on whether clustering methods can be used to improve the CF performance consistently. In this paper we propose a novel method for cluster-based collaborative filtering, Cluster-based KNN, which can perform consistently on the EachMovie dataset. It makes significant improvements over the KNN method while clusters are used auto-matically without user-provided relevance in formation. We conjecture that there are two main reasons for the success of our method. First, our proposed clustering method can fully exploit the information between user and items. By iterative clustering, the be improved. Second, the sophisticated smoothi ng techniques that we used to interpo-late the KNN method and the clustering method, may better capture the characteristics of the group and individuals than previously used methods. collaborative filtering and the novel cluster-based KNN algorithm in Section 3. In cludes the paper and points out possible directions for future work. In this section we briefly present some of the research literatures related to collabora-tive filtering methods and recommender systems. recommender systems. This system relied on the explicit opinions of people from a close-knit community, such as an office workgroup. However, recommender systems for large communities cannot depend on the assumption that people know each other. Later, several ratings-based recommender systems were developed. The GroupLens system [13] provides a pseudonymous collaborative filtering solution for Usenet news and movies. Ringo [24] and Video Recommender [8] are email and web-based sys-tems that generate recommendations on music and movies, respectively. Other technologies [2] have also been a pplied to recommender systems, including Bayesian networks, clustering [18][24]. Bayesian networks create a model based on a training set with a decision tree, and edges representing user information. The model accurate as KNN methods [2]. Bayesian netw orks are practical for environments in which knowledge of user preferences changes slowly with respect to the time needed models need to be updated rapidly or frequently. 
Clustering techniques work [18][24] by identifying groups of users who appear to clustering techniques represent each user with partial participation in several clusters. The prediction is then an average across the clusters, weighted by degree of participa-tion. Clustering techniques usually produce less-personal recommendations than other methods. And in some cases, the clustering methods have worse accuracy than nearest neighbor algorithms [6]. Once the clustering is complete, however, the performance techniques can also be applied as a "first step" in reducing the candidate set for KNN algorithm or for distributing nearest-neighbor computation across several recom-mender engines. Note that dividing the population into clusters may hurt the accuracy or recommendations to users. Most of the clustering methods group users with the method to partition the user set. And an EM algorithm was proposed in [2] for classi-fying the rating data. 
Probabilistic Latent Semantic Analsys (P LSA) [9][11] was proposed to learn the latent information. The method models individual preferences as a convex combina-with each observation pair of a user and an item. PLSA assumes that users and items are independent from each other given the latent class variable. Thus, the probability for each observed pair ( x , y , r ) is calculated as follows: with zero mean and one variance. A Gaussian distribution is used for ) , | ( x z r P , and a multinomial distribution for ) | ( y z P . In this section, we first describe the KNN algorithm for collaborative filtering. Then, by analyzing the clustering method, we propose the clustering based KNN collabora-tive filtering algorithm to optimize the quality of recommendation. First, we briefly describe the notations that are used throughout this paper. Let denotes the rating of item i by user u , and 3.1 KNN Method Memory-based CF algorithm calculates the pr ediction as a weighted average of other users X  votes on that item. Most memory-based algorithms differ in the way they calculate the weights. The popular weights calculation method is Pearson Correlation Coefficient [13]. 
In the neighborhood-based algorithm [13], a subset of users is first chosen based on used to produce predictions for the active user. The algorithm we use can be summa-rized in the following steps: 
Step 1. Weight all users with respect to the similarity to the active user. This simi-larity between users is measured as th e Pearson correlation coefficient between their rating vectors. users form the neighborhood. the neighbor X  X  ratings. In Step 1, similarity between users a and b is computed using the Pearson correlation coefficient defined below: mean rating given by users a , w a,b is the similarity between user a and user b . 
In Step 2, i.e., neighborhood-based methods, a subset of appropriate users is cho-sen based on their similarity to the active us er, and a weighted aggregate of their rat-ings is used to generate predictions for the active user in the step 3. 
In Step 3, predictions are co mputed as the weighted average of deviations from the neighbor X  X  mean: neighborhood. 
The KNN algorithm has the advantage of being able to rapidly incorporate the most up-to-date information and make relatively accurate prediction. Furthermore, the algorithm is to find the most similar individual users to help prediction and the similar kind of prediction is more personalized. 
However, given an active user, the rating of his neighbors is too sparse. As statis-tics from the whole EachMovie data, the average rating number of the movies per rated by the users. Thus, the item to be predicted may not be rated by the neighbors. On such sparse data, the performance of the KNN algorithm would be limited. 3.2 Cluster-Based Collaborative Filtering The main idea of clustering techniques is to work by identifying groups of users who individual can be made by averaging the opinions of the other users in that cluster. In such environment, the sparsity of the rating problem could be alleviated by using the rating information of all users in the group. with each other. Then, a prediction method is given based on the clustering results to solve the sparsity problem. 3.2.1 Iterative Clustering items on either side. Furthermore, the grap h is a weighted graph and the edge weights is used to represent rating score. 
As we discussed in section 1, traditional clustering algorithms could not exploit the inter-relationship between users and items. For example, when clustering the users, the traditional algorithms just take the items the user rated as the features of the corre-sponding user. While clustering the items, the algorithms just take the users that rate the item as the features of the item. Two challenges exist in such methods. First is the sparsity problem on the large feature space, which will affect the performance of clustering. Second is that the methods could not exploit the inter-relationship between items and users. 
Based on the analysis, here we propose an iterative reinforcement clustering framework, the clustering results between the users and items could be reinforced another by their relationship until converge. The sparsity problem could be alleviated through the iterative process. The details of the iterative clustering algorithm are described in Table 1. The running time of this algorithm for each pass is linear in total number ( N ) of nodes O( Z *( mM + nN )). 
In order to consider the users with similar preferences on items may still rate items differently, we take the Pearson correlation coefficient function as the similarity (2). While the similarity between items i and j can be defined similarly. 
Since the edge will be updated during the iteration process, the similarity function will be modified according to the updated edge. tively. Then, the rating over user class and item class can be written as: 3.2.2 Select Similar Cluster Before predicting the active user X  X  rating on an item, we first identify the user X  X  group represent the vector of the cluster and us e the Pearson correlation coefficient function as the similarity measure function between the cluster and the active user. 3.2.3 Predict Function Once we isolate the cluster of the user and similar item, the next step is to look into the target users X  ratings and use a technique to obtain predictions. 
The method for applying the clustering results is to use the average rating that the cluster C a of the active a to the cluster C i of the rated item i . That is: 3.3 Cluster-Based KNN fo r Collaborative Filtering As we known that KNN algorithm faces the sparse data problem. Since no explicit global model is constructed, nothing is really learned form the available user profiles. Thus, the method generally cannot provide explanations of predictions or further insights into the data. However, the KNN algorithm could provide more personal prediction, which is predicted based on the most similar individual users. 
On the other hand, as we mention in Section 1, the cluster based method has been proven to be non-consistent performance in CF prediction because of the less-personal recommendations. That is, the prediction for an individual is made by recommendations, or simply makes assumptions more explicit, which could solve the sparsity problem by grouping the similar users together. 
Based on advantages and disadvantages of the two algorithms, we propose cluster-based KNN method to alleviate the sparseness and produce a more personalized pre-collaborative filtering. 
The new method of cluster-based KNN is one that smoothes representation of indi-vidual users using models of the clusters th at they are most similar. We formulate our model as where  X  is the parameter for smoothing which take different values in different smoothing methods. In this section, we describe the dataset, metrics and methodology for the comparison between different prediction algorithms, and present the results of our experiments. 4.1 Dataset We use EachMovie data set (http://research.compaq.com/SRC/eachmovie) to evaluate the performance of our proposed algorithm. The EachMovie data set is provided by the Compaq System Research Center, which ran the EachMovie recommendation service they gathered during that period consists of 72,916 users, 1,628 movies, and 2,811,983 the EachMovie data set. We randomly select 20,000 users who rated 30 or more items and then divide them into a training set (75% users) and a test set (25% users). Finally, define the data set as 20000-users dataset. Moreover, in order to tune the parameter for the algorithm, we generate a small training and testing data set. In the small data set, the which is called as the parameters-tuning dataset in the following experiment. 4.2 Metrics and Methodology We use the Mean Absolute Error (MAE), a statistical accuracy metrics, to measure the prediction quality metric: where ) ( j u t R is the rating given to item t j by user u , user u on item t j , T is the test set, and | T | is the size of the test set. user named from Given5 , Given10 , Given20 to AllBut1 [2]. 4.3 Experiment Results current well-know algorithms for collaborative filtering: KNN, Clustering and PLSA. periments on the parameter-tuning data set mentioned in Section 5.1. The effect of  X  on the performance is shown in Figure 3. The neighbors of the KNN are set to 20. The data set we evaluate is 20000-users dataset. 
The results are shown in Table 2. From the Table 2, we can see that our proposed method outperforms all the three baseline methods substantially and consistently over the EachMovie dataset. 
In order to show the performance of our proposed algorithm on the sparse data, we also conduct compared experiment with KNN and Clustering methods on the 20000-users dataset. In Fig. 2 we empirically analyze how prediction precision evolves when the rating data is changed from sparse to dense. In this experiment, we randomly select 10%, 20%, ..., and 100% of the training data to represent the different degree of how dense of the data. We employ the protocol Given20 for the experiment. impact on the performance of prediction. As we discussed above, the KNN algorithm has very low performance when the rating data is too sparse. Clustering based method has the ability to achieve higher performance than the KNN algorithm. Our proposed algorithm has been adversely affected than two other algorithms. When the rating data become dense, the clustering method and our proposed method may improve in MAE. rating data is larger than 70%, the performance is very close to that of 100%. That is, the clustering technique could solve the sparsity problem to some extent. 
In order to measure how the interpolating parameter is to affect the performance of the cluster-based KNN CF, we do the experiment that the parameter  X  is ranged from shown in Fig. 3. As shown in the Fig 3, when the interpolating parameter is equal to 0.5, our algorithm could achieve the highest performance. filtering could not provide the personal recommendation, we propose a unified algo-rithm that integrates the advantages of both the clustering methods and KNN algo-rithm. Our proposed algorithm could leverage the information from both the individ-ual users and the clusters/groups they belong to. Furthermore, we propose an iterative clustering algorithm to fully exploit the relationship between the users and the items. Experimental results show that our proposed collaborative filtering algorithm can significantly outperform the traditional collaborative filtering algorithms. 
