 Conventional Language Models (LMs) are based on n-grams, and thus rely upon a limited number of preceding words to assign a probability to the next word in a document. Recently, Mikolov et al. (2010) proposed a Recurrent Neural Network (RNN) LM which uses a vector representation of all the pre-ceding words in a sentence as the context for lan-guage modeling. This model, which theoretically can utilize an infinite context window within a sen-tence, yields an LM with lower perplexity than that of n-gram-based LMs. However, the model does not leverage the wider contextual information provided by words in other sentences in a document or in re-lated documents.

Several researchers have explored extending the contextual information of an RNN-based LM. Mikolov and Zweig (2012) proposed a context-dependent RNN LM that employs Latent Dirich-let Allocation for modeling a long span of context. Wang and Cho (2015) offered a bag-of-words repre-sentation of preceding sentences as the context for the RNN LM. Ji et al. (2015) used a Document-Context LM (DCLM) to leverage both intra-and inter-sentence context.

These works focused on contextual information at the document level for LM, but did not con-sider information at the inter-document level. Many document sets on the Internet are structured, which means there are connections between different docu-ments. This phenomenon is prominent in social me-dia, where all the posts are directly linked to several other posts. We posit that these related documents could hold important information about a particu-lar post, including the topic and language use, and propose an RNN-based LM architecture that utilizes both intra-and inter-document contextual informa-tion. Our approach, which was tested on the social media dataset reddit , yielded promising results, which significantly improve on the state of the art. We used pre-collected reddit data, 1 which as of December, 2015, consists of approximately 1.7 bil-lion comments in JSON format. A comment thread starts with a  X  X opic X , which might be a link or an im-age. The users then begin to comment on the topic, or reply to previous comments. Over time, this pro-cess creates a tree-structured document repository (Figure 1), where a level indicator is assigned to each comment, e.g., a response to the root topic is assigned level 1, and the reply to a level n com-ment is assigned level n + 1 . We parsed the raw data in JSON format into a tree structure, removing threads that have less than three comments, contain deleted comments, or do not have comments above training 1500 14592 40709 648624 testing 500 5007 13612 217164 validation 100 968 2762 44575 level 2. We randomly selected 2100 threads that fit these criteria. The data were then split into train-ing/testing/validation sets. Table 1 displays some statistics of our dataset. Our inter-document contextual language model scaffolds on the RNN LM (Mikolov et al., 2010) and DCLMs (Ji et al., 2015), as described below. RNN-LSTM. Given a sentence { x t } t  X  [1 ,...,N ] , where x t is the vector representation of the t -th word in the sentence, and N is the length of the sentence, Mikolov et al.  X  X  (2010) RNN LM can be defined as: where h t is the hidden unit at word t , and y t is the prediction of the t -th word given the previous hidden unit h t  X  1 . The function f in Equation 1 can be any non-linear function. Following the ap-proach in (Sundermeyer et al., 2012) and (Ji et al., 2015), we make use of Long Short-Term Memory (LSTM) units (Hochreiter and Schmidhuber, 1997) rather than the simple hidden units used in the orig-inal RNN LM. In our work, the word representation x t is obtained from the one-hot representation using an affine transformation, as follows: where o t is the one-hot representation, W p is the projection matrix, and b p is a bias term.
 Document Context LMs (DCLMs). We re-implemented two of Ji et al.  X  X  (2015) DCLMs as our Context-to-output (Figure 2b). These models extend the RNN-LSTM model by leveraging information from preceding sentences.

The context-to-context model (ccDCLM) con-catenates the final hidden unit of the previous sen-tence with the word vectors of the current sentence. Thus, Equation 1 becomes: where N i  X  1 is the length of the previous sentence in the document, x i,t is the vector representation of the t -th word in the i -th sentence, x 0 i,t is the con-catenation of the vector representation x i,t and the previous sentence X  X  final hidden unit h i  X  1
The context-to-output model (coDCLM) applies the additional information directly to the word-decoding phase. Thus, Equation 2 becomes: We now extend the DCLM by leveraging the in-formation at the inter-document level, taking ad-vantage of the structure of the repository  X  a tree in reddit . Specifically, by harnessing the infor-mation in documents related to a target document, i.e., its siblings and parent, the LM is expected to contain additional relevant information, and hence lower perplexity. Formally, let X  X  call the sentence-level context vector h s , the parent document context vector h p , the sibling context vector h l , and the over-all context vector h c . Our framework is defined as:
We use the last hidden vector of the RNNs as the representation of the parent post, the older-sibling, and the previous sentence. The definition of the con-text function ( g h ), the input function ( g i ), and the word-decoding function ( g o ) yields different config-urations.

We also explored two strategies of training the models: Disconnected (disC) and Fully Connected (fulC) . In the disC-trained models, the error signal within a time step (i.e. a post or sentence) only af-fects the parameters in that time step. This is in con-trast to the fulC-trained models, where the error sig-nal is propagated to the previous time steps, hence influencing parameters in those time steps too. 4.1 Analysis of our modelling approach In this section, we empirically analyze different training and modelling decisions within our frame-work, namely DC vs FC training, as well as contex-tual information from parent vs sibling.
 The Setup. For our analysis, we employed a sub-set of the data described in Table 1 which contains 450 threads split into training/testing/validation sets with 300/100/50 threads respectively. The hidden-vector and word-vector dimensions were set to 50 and 70, respectively. The models were implemented in Theano (Bastien et al., 2012; Bergstra et al., 2010), and trained with RMSProp (Tieleman and Hinton, 2012).
 Table 2: disC/fulC-trained models vs the baselines. disC vs fulC. We first compared the disC and fulC strategies, at the sentence level only, in order to se-lect the best strategy in a known setting. To this ef-fect, we re-implemented Ji et al.  X  X  (2015) DCLMs with the disC strategy, noting that Ji et al.  X  X  original sentence-based models are fulC-trained. The results of this experiment appear in Table 2 which further compares these models with the following baselines: (1) vanila RNN-LSTM, and (2) a 6-gram LM with The disC-trained models showed no improvement over the RNN-LSTM, and lagged behind their fulC-trained counterparts. The lower performance of the disC-trained models may be due to not fully lever-aging the contextual information; disC-training lose information, as the error signal from the current time step is not used to calibrate the parameters of pre-vious time steps. Therefore, we make use of fulC strategy to train our models in the rest of this paper. Parent vs Sibling Context. The inter-document information in reddit  X  X  case may come from a par-ent post, sibling posts or both. We tested our models with different combinations of inter-document con-text information to reflect these options. At present, we consider only the closest older-sibling of a post, as it is deemed the most related; different combina-tions of sibling posts are left for future work. We tested the following three context-to-context config-urations: parent only (P-ccDCLM), sibling only (S-ccDCLM), and parent and sibling (PS-ccDCLM), which define the context function as Equation 11, 12 and 13 respectively. The three configurations use the same word-decoding function (Equation 15) and the same input function (Equation 14).
The results of this experiment appear in the first three rows of Table 3, which shows that the best-performing model is PS-ccDCLM.

As discussed by Ji et al. (2015), the coDCLM makes the hidden units of the previous sentence have no effect on the hidden units of the current sen-tence. While this configuration might have some advantages (Ji et al., 2015), applying it directly to a larger context may lead to complications. Suppose we use the last hidden unit of the previous docu-ment as the context for the next document. With the context-to-output approach, the last hidden unit summarizes only the information in the last sentence of the previous document, and doesn X  X  reflect the en-tire document. We address this problem by not using the context-to-output approach in isolation. Instead, we use the context-to-output approach in tandem with the context-to-context approach of ccDCLM. This approach was tested in an additional parent-sibling configuration (PS-ccoDCLM), as an alterna-tive to the best performing context-to-context con-figuration. The PS-ccoDCLM is similar to the PS-ccDCLM except for the decoding equation, which is changed into Equation 16. Based on the results of these trials, we chose the best-performing PS-ccDCLM (Figure 2c) as our fi-nal system.
 Table 3: Comparing models incorporating parent (P) and/or sibling (S) contextual information.
 4.2 Results The model perplexity obtained by the baselines and our best-performing model for the test set (Table 1) is shown in Table 4  X  our system (PS-ccDCLM) statistically significantly outperforms the best base-line (ccDCLM), with  X  = 0 . 01 , using the Fried-man test. The inter-sentence contextual informa-tion under the context-to-context regime (ccDCLM) decreases model perplexity by 9% compared to the original RNN-LSTM, while the inter-document con-textual information (PS-ccDCLM) reduces perplex-ity by a further 5% compared to ccDCLM. Our results show that including inter-document con-textual information yields additional improvements to those obtained from inter-sentence information. However, as expected, the former are smaller than the latter, as sentences in the same post are more re-lated than sentences in different posts. At present, we rely on the final hidden-vector of the sentences and the posts for contextual information. In the fu-ture, we propose to explore other options, such as additional models to combine the contextual infor-mation from all siblings in the tree structure, and ex-tending our model to structures beyond trees.
