 Common approaches to multi-label classification learn independent classifiers for each category, and employ ranking or thresholding schemes for classification. Because they do not exploit dependen-cies between labels, such techniques are only well-suited to prob-lems in which categories are independent. However, in many do-mains labels are highly interdependent. This paper explores multi-label conditional random field (CRF) classification models that di-rectly parameterize label co-occurrences in multi-label classifica-tion. Experiments show that the models outperform their single-label counterparts on standard text corpora. Even when multi-labels are sparse, the models improve subset classification error by as much as 40%.
 I.5.1 [ Pattern Recognition ]: Models X  statistical,structural Design,Experimentation,Performance Classification, machine learning, multi-label, statistical learning, uncertainty
Single-label classification assigns an object to exactly one class, when there are two or more classes. Multi-label classification is the task of assigning an object simultaneously to one or multiple classes.

The most common approach independently learns a binary clas-sifier for each class, and then assigns to a test instance all of the class labels for which the corresponding classifier says  X  X es. X  Ex-periments have shown that the classifiers such as Widrow-Hoff, k-nearest-neighbor, neural networks and linear least squares fit map-ping are viable techniques for this approach [17], as are support vector machines [8]. Although some binary classifiers provide pos-terior probability over their binary answers, they need only have binary valued output.

Another approach requires a real-valued score for each class, suitable for ranking class labels, and then classifies an object into the classes that rank above a threshold. Schapire Singer Schapire99 [14] develop a boosting algorithm that gives rise to such a ranking. The model described by Crammer Singer Crammer02 [5] learns a prototype feature vector for each class, and a class rank is derived from the angle between its prototype and the document. The model in Gao et al. Gao04 [6] trains independent classifiers for each cate-gory that may share some parameters, and ranks each classification according to a confidence measure.
 The above methods learn independent classifiers for each class. However, it is often the case that there are strong co-occurrence patterns and dependencies among the class labels. Explicitly lever-aging these patterns may be advantageous. For example, the belief that a research article having the word sodium is likely to be labeled HEART DISEASE supports the belief that the document should also be given the label HYPERTENSION . A method that captures de-pendencies between class labels is likely to provide improved clas-sification performance, particularly for more richly multi-labeled corpora than those used in experiments.

This paper presents two multi-label graphical models for classi-fication that parameterize label co-occurrences. As in traditional classifiers, both models learn parameters associated with feature-label pairs. The Collective Multi-Label classifier (CML) also, jointly, learns parameters for each pair of labels. The Collective Multi-Label with Features classifier (CMLF) learns parameters for feature-label-label triples X  X apturing the impact that an individual feature has on the co-occurrence probability of a pair of labels.
We present experiments using two data sets that, although sparsely multi-labeled, have become standard for multi-label classification experiments: the Reuters-21578 and OHSU-Med text corpora. CML and CMLF outperformed the binary models: they reduced error in subset accuracy by as much as 27%, reduced error in macro-and micro-averages by up to 9%, and had consistently better perfor-mance than their binary counterparts. work for parameterizing relationships between class labels and fea-tures, or characteristics, of objects. Furthermore, such models often outperform their generative counterparts.

Conditionally trained undirected graphical models, or conditional random fields (CRFs) [9], can naturally model arbitrary dependen-cies between features and labels, as well as among multiple labels. These dependencies are represented in the form of new (larger) cliques, which allow various clique parameterizations to express preferences for arbitrary types of co-occurrences.
 Traditional maximum entropy classifiers, e.g. [13], are trivial CRFs in which there is one output random variable. We begin by describing this traditional classifier, then we describe its com-mon extension to the multi-label case (with independently-trained binary classifiers), and then we present our two new models that represent dependencies among class labels.
In single-label classification, any real-valued function f of the object x and class y can be treated as a feature . For example, this may be the frequency of a word w k in a text document, or a property of a region r k of an image. Let V be a vocabulary of characteristics. The constraints are the expected values of these features, computed using training data. Suppose that Y is a set of classes and  X  k are parameters to be estimated, which correspond to features f k , where k enumerates the following features: That is, k is an index over features, and each feature corresponds to a pair consisting of a label and a characteristic (such as a word). Then the learned distribution p ( Y | x ) is of the parametric exponen-tial form [1]: Z ( x ) is the normalizing factor over the labels: Given training data the penalized log likelihood of parameters  X  is where the last term is due to the Gaussian prior used to reduce overfitting. The trainer attempts to find a  X  that maximizes iteratively. The gradient of the log likelihood at k is  X  ( X  | D ) Since this cannot be solved analytically in closed form, the optimal  X  is found by convex optimization. BFGS [3] is a fast optimization method that finds the global maximum of the likelihood function given the value and gradient. The single-label model above learns a distribution over labels. In a multi-label task, the model should learn a distribution over subsets of the set of labels Y , which are represented as bit vectors y of length |Y| .

In the most general form, given instance x and features f where Z ( x ) is the normalizing constant. All three CRF models capture the following enumeration over features in the learned dis-tribution: That is, all three models capture the dependency between each ob-ject feature and each label.
A common way to perform multi-label classification is with a binary classifier for each class. For each label y b , the binary model trains an independent binary classifier c b , partitioning training in-stances into positive ( + ) and negative (  X  ) classes (Figure 1(a)). The learned distribution p b is as in Equation 1, except that since r j  X  X  + ,  X  X  . However, the distribution over multi-labelings, p ( y | x ) is as follows: This scheme attributes an object x to category labeled y b sifies x positively. However,the classifications are treated indepen-dently.

Figure 1(a) depicts this model as a factor graph. The black squares (factors) represent the model parameters. For example, in Figure 1(a), the binary model maintains a parameter for each pair consisting of a label and a feature. Factor graphs are graphical models that depict the clique parameterizations. Inference in factor graphs is done in a way similar to inference in graphical models [10].
In order to capture co-occurrence patterns among labels, this pa-per presents a conditional random field representing dependencies among the output variables.

In addition to having feature for each label-term pair, CML main-tains features accounting for label co-occurrences. This model is depicted in Figure 1(b). For object e and labels y and y  X  four features: For k = WHEAT , GRAIN , 2 and training document x , y , f is 1 if x , y is labeled GRAIN but not WHEAT , and 0 otherwise. A document has 4 | Y | 2 such features.

The distribution p ( y | x ) thus becomes where Z  X  ( x ) is the normalizing constant and Figure 1: Factor graphs representing the multi-label models, where y is a label and x i is a feature, and the black squares represent each pair of labels and each feature. The log likelihood l ( X  | D ) is similar to Equation 3: The computation of the gradient is analogous to Equation 4. CML captures the label co-occurrences in the corpus independent of the object X  X  feature values. Effectively, for each label set, it adds a bias that varies proportionally to the label set frequency in training data. The factor graph for this model is depicted in Figure 1(b).
While CML parameterizes the dependencies between labels in general, these dependencies do not account for the presence of par-ticular observational features (e.g., words). The tendency of labels to occur together in a multi-labeling is not independent of the ap-pearance of the observational features. For instance, a text docu-ment belonging to the categories RICE and SOYBEAN might have increased likelihood of being correctly classified if the document has the word cooking , but decreased likelihood of belonging to ALTERNATIVE FUELS . The factor graph in Figure 1(c) reflects this dependency. The CMLF model maintains parameters that corre-spond to features for each term, label 1 ,label 2 triplet, capturing parameter values for cooking, RICE , SOYBEAN , for example.
As with CML, CMLF defines feature parameters over the labels and words, but also defines parameters over pairs of labels and words, for a total of O ( n 2 | V | ) parameters for n labels. Note that CMLF maintains overlap in term occurrences: it has a feature for each pair consisting of a term and a label, as well as a feature for each triplet consisting of a term and two labels. The features enumerated by provide some shrinkage, and thus protection from overfitting [4].
The corresponding distribution that CMLF learns is The gradients of the log likelihood at k and at k are the same as those of CML, except that k enumerates different features. CML has four features for each pair of labels, while CMLF has | tures for each pair of labels. The factor graph for this model is depicted in Figure 1(c); note that for each observational feature, there is a parameter for each label, and also a parameter for each pair of labels.

Parameter estimation in these models is the same as for the single-label model: calculation of the value and gradient is straight-forward, and BFGS is used to find the optimal parameters given the gradient of the log-likelihood. Note that neither multi-label model assumes that the label taxonomy has a complex structure, although extra pa-rameters accounting for this could easily be added.
 Table 1 shows the asymptotic complexity of training an instance. The binary technique is faster than the multi-label models in most cases, but performance of binary pruning depends on selection of the threshold, which determines the number of classes. In large datasets with many rarely occurring multi-labelings, binary prun-ing requires considerably less training time than supported infer-ence, for comparable classification performance. Experiments sug-gest that the binary pruned inference technique is faster than sup-ported inference. CMLF is linear with respect to CML, which is asymptotically simpler than the binary classifier method only if the multi-labelings are sparse. However, in practice binary classifiers are faster to train because they use fewer parameters in optimiza-tion. Table 1: Asymptotic per-instance training complexity, given |
V | = v , k labels, s total label combinations of average size a and r labels ranking above threshold on average.
Rather than providing a probability estimate for each label, exact inference using the collective models requires learning a probabil-ity distribution over all possible multi-labelings  X  that is, over all subsets of Y . This method is intuitively appealing: it is easy to explain, and it is informative, since it offers a probability score for each combination of labels, regardless of the combination presence in the training data. However, since the number of subsets is expo-nential in the number of class labels, the problem is tractable only for about 3-12 classes. When the number of classes is larger, ap-proximate inference methods may prune certain combinations of labels, and calculate the conditional distribution over the pruned set.

One method of pruning is to include only the label combina-tions that that occur in training data X  X hich we term the supported combinations. This method can sometimes be surprisingly effec-tive. For the top 10 classes in Reuters-21578, only 0.6% of test instances belong to combinations of categories that do not occur in training data. For the entire ModApte split, the error due to sup-ported inference is more significant: 4% of test instances have label combinations that do not occur in training data. When there are few classes and few such outliers, or when such rare combinations can be excluded, then supported inference is a very good solution.
An alternative approximate inference method is termed binary pruned inference , and represents a compromise between supported and exact inference. The model trains an independent binary classi-fier for each label. Then when classifying an object, exact inference considers only the labels having binary classifier probability scores above a certain threshold ( t ). Cross validation on training data is used to choose the threshold.

Binary pruned inference makes it possible to correctly classify test documents whose actual combinations do not occur in the train-ing data. Furthermore, the method requires less training time than supported inference.
We present experiments with these multi-label classifiers on two standard multi-label data sets: Reuters-21578 and the  X  X eart Dis-ease X  ( HD ) documents of OHSU-Med. The corpora differ in the noise level and length of documents. Both have simple label tax-onomies: labels are not hierarchical, and each document has at least one label from the entire label set.

Except in the case of the k features of CML, features f i resented by count of occurrences, in experiments presented here. Alternate representations include frequency of occurrences, for ex-ample.
The ModApte split of Reuters-21578, in which all labeled docu-ments that occur before April 8, 1987 are used in training and other labeled documents are used in testing, is a popular benchmark for classifier incorrectly assigns one of the labels to an object, or fails to assign the correct label to an object. experiments. The ModApte documents consist of those documents labeled by the 90 classes which have at least one training and one testing instance, accounting for 94% of the corpus. Roughly 8.7% of these documents have multiple topic labels.

Experiments using corpus Reuters10 use only documents be-longing to the 10 largest classes, which label 84% of the docu-ments and form 39 distinct combinations of labels in the training data. Table 2 depicts the distribution of multi-label cardinalities in the ReutersAll test set, together with the label classification error rate of the binary classifiers.

The OHSU-Med [7] HD corpus, a popular dataset for text clas-sification, is a collection of titles and abstracts of medical research journal articles from 1989-1991 corresponding to characterizations of the relevant heart conditions, such as  X  X eart Aneyurism X  and  X  X yocarditis X . The HD-small documents belong to the 40 cate-gories which label between 15 and 74 training documents, forming 106 combinations of labels in the training data. HD-big consists of documents belonging to the remaining 16 categories that each label 75 or more training documents.
Features are ranked according to their mutual information, so that the classifiers may select a proportion of features having the highest rank. Parameters that influence performance of the clas-sifiers include proportion of features selected, Gaussian prior vari-ance of the parameters, and in the case of binary pruning, the thresh-old for the binary classifiers. The classifiers are least sensitive to the Gaussian prior, and binary pruning is most sensitive to the thresh-old. Lower thresholds have higher classification cost but higher thresholds limit the performance of CML and CMLF to the perfor-mance of the binary classifiers.

In experiments presented in this paper, words occurring fewer than 5 times in all training documents are excluded from the vo-cabulary, and all classifiers assume a Gaussian prior variance of 1.0. Thresholds and feature proportions are learned using cross validation on training data. That is, the parameters that a given classifier uses are those which yield the best average performance, of the binary model and its multi-label counterpart, using a random partition of the training data into training and validation instances. The results are compared using three metrics: F1 micro-average, F1 macro-average [17], and subset accuracy. The macro-average is the mean of the F1-scores of all the labels, thus attributing equal weights to each F1-score. The micro-average is the F1-score ob-tained from the summation of contingency matrices for all binary classifiers. The micro-average metric gives equal weight to all clas-sifications, so that F1 scores of larger classes influence the metric more than F1 scores of smaller classes. F1-score reflects the har-monic mean of precision and recall. Subset accuracy is the propor-tion of documents with entirely correct bit vectors y .
Even for the sparsely multi-labeled ReutersAll , CMLF reduces error in F1 averages by as much as 5%, and reduces error in subset classification by 16%. Table 3 depicts the results of experiments on Table 3: Performance of the three inference techniques. Fea-ture proportions, and threshold parameters for binary pruning ( t ), are learned using cross-validation on training data. Even for this sparsely multi-labeled corpus, the multi-label models always outperform their binary counterparts, reducing error in subset accuracy by as much as 8% and in F1 scores by 5-8%.
 ReutersAll using the ModApte split, as well as a comparison of the two inference methods. Supported inference experiments are more costly in time and space than binary pruning.

With ReutersAll , binary pruning generally performs better than supported inference. Furthermore CML and CMLF perform better than the best reported results.

The binary pruning technique resulted in 3% higher F1 micro-average and 23% higher macro-average than supported inference. The significant gain in macro-average suggests that binary pruning improves performance of smaller classes.

Collective classifiers perform better than the traditional binary model, supporting our contention that the classes are not indepen-dent, and that directly parameterizing these dependencies is advan-tageous.
HD is a noisier corpus than Reuters-21578, having topics that span a narrower semantic scope. As with Reuters-21578, CML and CMLF trump the traditional binary models. With thresholds chosen using cross validation on training data, CML and CMLF achieve better performance with supported inference than binary pruning.
In HD , typically more than half of the misclassifications in bi-nary pruning are due to the pruning of positive classes. Thus on pruned instances, the F1 averages that the collective models achieve with supported inference are higher than the averages achieved us-ing binary pruning.

Table 4 depicts performance of the five techniques on HD-small and HD-big . Compared to the traditional binary model, using sup-ported inference, the collective classifiers improve subset accuracy by 20-40%, whereas with ReutersAll , this improvement is about 4%. (The collective models increase F1 averages by 5-9% for both HD corpora.) It is gratifying to see that on tasks with larger, more complex multi-labeled sets, our method provides even greater im-provement.

The average improvement of CML and CMLF over binary clas-sifiers is even greater across several trials using random test-train splits (of comparable proportions to those of Table 4 experiments) of the corpus. Experiments suggest that more innovative binary pruning models could improve performance considerably.
 Table 4: Results of experiments on HD , trained on documents from 1991 and tested on documents from 1990. Multi-label models reduce F1 macro and micro-average error by 8%.
Some existing models indirectly leverage the multi-label depen-dencies that traditional methods do not. semantic scene classifi-cation, Boutell et al. Boutell03 [2] train a single-label classifier for each label, using all single-label documents and only the multi-label documents with that label. This approach indirectly leverages label co-occurrences, but it does not directly parameterize multi-label dependencies.

Expectation Maximization has been used to train a mixture model [11] for which the features of each document are produced by a mixture of word distributions for each class. [16] take a similar approach in that each word in each category is generated from a multinomial distribution over vocabulary words. Both of these ap-proaches are generative, and both leverage information about multi-ple class memberships for a given document implicitly by learning which classes generate which features.

Relational Markov Network models (RMNs) [15], are undirected graphical models like CML and CMLF. However, they perform single-label classification simultaneously of multiple documents, whereas CML and CMLF address the issue of multi-label classi-fication of a single document. Furthermore, RMNs use the hy-perlinks linking separate documents to capture dependencies be-tween documents, but the model relies on the inherent sparseness of those dependencies, while CML and CMLF prove advantageous for densely multi-labeled corpora. RMNs use loopy belief propa-gation is used for estimating the gradient.
Multi-label classification is an important task in domains be-yond text. In many real-world tasks, classes are not independent. CML and CMLF offer a framework for leveraging the dependen-cies between categories by including factors that capture label co-occurrences, whereas previous methods leverage category depen-dencies only indirectly, at best.

The success of conventional classification approaches depends on properties such as independence of classes and sparsity of multi-labelings. On varying corpora, over several metrics, the collective models outperform these methods.

Research related to multi-label classification involves automat-ically annotating biomedical abstracts with lists of genes that are mentioned in the documents. This is related to multi-label classifi-cation because each gene may have several synonyms, and a syn-onym may refer to several genes. More generally, in any domain in which subsets of unstructured interdependent outcomes are to be assigned, the CML and CMLF framework suggests a viable solu-tion.

Future experiments may test the models in different domains and use corpora with varying noise characteristics, as well as domains in which features do not have uniform weight and type, including semantic scene classification.

Improved inference and pruning methods may be more tractable than exact and supported inference and allow greater flexibility than binary pruning.

A more general extension of CML and CMLF would parame-terize larger factors, rather than pairs of labels, and incorporate schemes for learning which factors to include [12]. Enhanced mod-els could also handle unlabeled data. This work was supported in part by the Center for Intelligent Information Retrieval, in part by Air Force Office of Scientific Re-search contract #FA9550-04-C-0053 through subcontract #0250-1181 from Aptima, Inc., in part by The Central Intelligence Agency, the National Security Agency and National Science Foundation un-der NSF grant #IIS-0326249, and in part by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division, under contract number NBCHD030010. Any opinions, findings and conclusions or recommendations expressed in this material are the author(s) and do not necessarily reflect those of the sponsor. [1] A. L. Berger, V. J. D. Pietra, and S. A. D. Pietra. A maximum [2] M. Boutell, X. Shen, J. Luo, and C. Brown. Multi-label [3] R. H. Byrd, J. Nocedal, and R. B. Schnabel. Representations [4] S. F. Chen and R. Rosenfeld. A gaussian prior for smoothing [5] K. Crammer and Y. Singer. A new family of online [6] S. Gao, W. Wu, C.-H. Lee, and T.-S. Chua. A mfom learning [7] W. R. Hersh, C. Buckley, T. J. Leone, and D. H. Hickam. [8] T. Joachims. Text categorization with suport vector [9] J. D. Lafferty, A. McCallum, and F. C. N. Pereira.
 [10] H. A. Loeliger. An introduction to factor graphs. In IEEE [11] A. McCallum. Multi-label text classification with a mixture [12] A. McCallum. Efficiently inducing features of conditional [13] K. Nigam, J. Lafferty, and A. McCallum. Using maximum [14] R. E. Schapire and Y. Singer. Boostexter: A boosting-based [15] B. Taskar, P. Abbeel, and D. Koller. Discriminative [16] N. Ueda and K. Saito. Parametric mixture models for [17] Y. Yang. An evaluation of statistical approaches to text
