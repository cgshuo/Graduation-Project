 1. Introduction
Patent document retrieval is a particularly difficult task in the text search domains: one aspect of the dif-ogies. The NTCIR-3 patent task is intended to study this aspect, simulating a technology survey search motivated by newspaper articles. The other aspect arises from the nature of information-seeking activities when patent documents are searched: information is retrieved as evidential material for legal proceedings. The NTCIR-4 patent task is a laboratory environment to simulate text search aspects of invalidity investiga-tion tasks. We examined these two different search tasks comparatively: a technology survey search against Japanese patent collections as experiments using the NTCIR-3 patent test collection and an invalidity search as the main task in the NTCIR-4 patent task. Presumably different techniques dominate in each task because of the differences in task characteristics.

In particular, pseudo-feedback effectiveness is sensitive to the task characteristics, because the technique assumes that relevant documents are similar to each other. Characteristics of sets of relevant documents against a topic depend greatly on judgment and consequently on the task definition. In order to distinguish feedback effectiveness from retrieval model effectiveness, two different retrieval models were applied to these two tasks with pseudo-relevance feedback strategies: a TF*IDF approach using BM25 TF; and the Kullback X 
Leibler divergence (KL-divergence) approach, which is one of the probabilistic language modeling approaches recently introduced by some information retrieval researchers ( Lafferty and Zhai, 2001; Zhai &amp; Lafferty, 2001 ).

The remainder of the paper is organized as follows: Section 2 analyzes the characteristics of the patent test collections and patent search tasks. Section 3 briefly explains the NTCIR-3 and -4 test collections and other test collections. Section 4 describes the retrieval models used. Section 5 describes patent document retrieval experiments using the NTCIR-3 and -4 test collections, and Section 6 contains a discussion of the character-istics of different collections, examining five test collections recently utilized in NTCIR and TREC workshops.
Section 7 concludes the work. 2. Characteristics of patent search and patent collections
Patent documents are characterized by their special stylistic features as well as highly structured and attri-buted bibliographic information. In some respects, patent documents can be considered as a form of techno-scientific writing that describes technological inventions. The NTCIR-3 patent task addressed these features of patent documents: simulated information needs motivated by newspaper articles and simulated relevance assessments by a group of corporate intellectual property administrators from various industry domains.
On the other hand, an invalidation investigation is not limited to a traditional database retrieval against diverse forms of documentation looking for prior art that might invalidate the claim in question; it may be expanded to a sort of  X  X  X now-who X  X  search, where one seeks a specialist in the domain who may possibly know disclosure, displays, publications or uses of the invention by products/processes. 2.1. Technology survey search and invalidity search
A technology survey search task can be understood as a subject topic search of technological documenta-of an invalidation investigation may be applicable to patentability, novelty, validity and infringement inves-tigations when adapted to different search environments. Such search tasks require more rigid standards of relevance, i.e., adequacy as evidential material, than an ordinary subject topic search of technological docu-mentation. This leads to a smaller number of relevant documents for each query.

Naturally, the patent application in question is a good starting point for seeking for proof items. Claim sentences are the units for legal protection and invalidation; consequently, proofs of prior art are to be pre-documents in the two different task settings. In fact, some empirical studies have suggested that in patent ogy survey task, the document length does not affect relevance, whereas in an invalidity search task, it does, the document length affects the relevance more clearly. These two different behaviors are explained by two hypotheses on document length, namely the  X  X  X erbosity hypothesis X  X  and the  X  X  X cope hypothesis X  X ; by the first hypothesis, the document length affects the relevance, whereas by the second hypothesis, it does not. A longer patent document is not only verbose but strong when claiming rights. This is why longer documents are more likely to be relevant in invalidation searches. The two document length hypotheses distinguish whether a single document is topically cohesive or not through its whole extent. Let us now consider a set of documents rel-evant to a topic. The questions that naturally follow include: do relevant documents resemble each other?
Do relevant documents look similar to each other? These questions are referred to as the  X  X  X lustering hypoth-same requests  X  X  ( van Rijsbergen, 1979, Chapter 3 ).
 2.2. Cluster hypothesis ter hypothesis that relevant documents look similar as long as the hypothesis is applicable to the collection makes some sense in retrieval strategies. For example, relevance feedback and pseudo-relevance feedback strategies assume that relevant documents share some terms apart from the original query terms, and conse-quently they should resemble each other in some degree when the strategies are effective.

A test for how well the cluster hypothesis characterizes a collection was proposed by Voorhees (1985) , but evant documents are frequently more similar to nonrelevant documents than to relevant ones, and such inter-mingling is deleterious in a cluster-based search. If relevant documents are more similar to nonrelevant documents, this presumably hurts gains from relevance (especially pseudo-relevance) feedback as well. There-fore the extent to which the cluster hypothesis holds true affects the retrieval strategy when higher recall is ter hypothesis X  X  holds true, some terminology is shared by relevant documents. Some relevant documents are retrieved by the pilot search and their terms are utilized for feedback. Consequently the measure of such a weak cluster hypothesis may affect the feedback effectiveness by indicating the presence or absence of useful feedback terms in candidate documents. 3. NTCIR and other test collections The NTCIR (NACSIS-NII Test Collection for Information Retrieval Systems; for details see, for example, Kando, 2004 ) project group has organized a series of  X  X  X istributed experiments/centralized evaluation X  X -style workshops. They provide reusable test collections to workshop participants (and also other researchers) for experimental research on diverse information access technologies. These test collections consist of several forms of document collections including scientific paper abstracts, newspapers, Web and patent documents, mainly in Japanese but with some in English, Chinese and Korean.

We report studies using the NTCIR-3 and -4 patent task test collections, while feedback effectiveness is comparatively evaluated through diverse test collections such as the NTCIR-3 and -4 CLIR J-J collections, which are Japanese newspaper collections, and the TREC-2004 MEDLINE collection, which consists of Eng-lish abstracts of medico-scientific research papers. The NTCIR-3 patent collection was considered to exem-plify the  X  X  X erbosity hypothesis X  X , whereas the NTCIR-4 patent collection showed slightly different characteristics in a previous document length study ( Fujita, 2004a ). We examine these collections from a dif-ferent perspective, namely feedback effectiveness. Newspaper test collections are utilized because they repre-sent a typical case of  X  X  X cope hypothesis X  X  in the document length study, and we test whether such a characteristic affects the feedback effectiveness. The TREC 2004 MEDLINE collection was a sort of neutral case in terms of document length hypotheses according to another study ( Fujita, 2004b ).
 relevance levels, two types of relevance judgment files are provided: S or A documents are marked relevant in  X  X  X igid X  X  relevance judgment files; and S, A or B documents are marked relevant in  X  X  X elaxed X  X  relevance judg-ment files. In this paper,  X  X  X igid X  X  relevance judgment files are used unless otherwise noted. 3.1. Patent test collections
The NTCIR-3 patent test collection consists of the unexamined patent application documents of 1998 X 99 (full text, SGML formatted, 697330 documents) released from the Japanese Patent Office (JPO), plus 31 topics using the collection, which was intended to simulate a  X  X  X echnology survey search X  X , was designed to retrieve patent documents by creating queries from the newspaper articles. As shown in Fig. 1 , topic descriptions con-tain &lt;SUPPLEMENT&gt;, &lt;DESCRIPTION&gt;, &lt;NARRATIVE&gt; and &lt;CONCEPT&gt; fields in addition to the newspaper article information, and these fields both provide assessors with criteria for relevance and act as bases for queries. Patent documents that mention the technologies stipulated in the topic description are con-sidered relevant, although the topic may not relate to the essence of the invention. In this study, the &lt;DESCRIPTION&gt; fields are utilized as queries.

The NTCIR-4 patent test collection consists of the unexamined patent application documents of 1993 X 97 (the same format as NTCIR-3; 1707184 documents), plus 34 main topics for which pooling and relevance assessments were carried out, and 69 additional topics for which no relevance assessment was done but
JPO citations were used as relevant documents ( Fujii, Iwayama, &amp; Kando, 2004 ). JPO citations are patent document references used to justify the rejection of the original patent application. Both topic sets are inde-pendent claim sentences extracted from patent application documents. No additional topic description is pro-vided apart from the full text of the original application. Relevance assessments were carried out on the basis of adequacy as an evidence item to support an invalidation application against the patent application of the application used as one of the formal run topics; task participants were asked to submit at least one run uti-simulate a real invalidation investigation, relevant documents should have been published before the date stip-the priority date in the case of a  X  X  X onvention application via Paris route X  X . 3.2. NTCIR CLIR J-J test collections
The NTCIR-3 CLIR J-J test collection consists of 1998 X 99 Mainichi newspaper documents (220078 doc-uments) and 42 search topics with assessed document lists ( Chen et al., 2002 ). For the NTCIR-4 CLIR J-J test collection, Yomiuri newspaper documents (375980 documents) were added to the NTCIR-3 CLIR J-J collec-tion, and 55 search topics with assessed document lists were provided.

Each topic has four fields, namely TITLE, DESC, NARR and CONC ( Kishida et al., 2004 ). The TITLE field consists of a few words, and the DESC field consists of one sentence describing the information needs. The NARR field contains two or three sentences describing the conditions for documents to be relevant, and the CONC field contains some words or phrases describing principal concepts of the information needs.
Newspaper ad hoc retrieval has been studied much more than patent retrieval in previous test collections such as TRECs ( Harman, 1995 ) and NTCIRs ( Chen et al., 2002 ), and general techniques have been found to be more effective than collection-specific techniques. Such techniques might be very good baselines against more specialized techniques for the patent retrieval tasks. 3.3. TREC 2004 genomics track: MEDLINE test collection
The ad hoc retrieval task of the TREC 2004 Genomics track ( Hersh et al., 2004 ) was designed to simulate the subject topic retrieval against a 10-year subset (4591008 records) of the MEDLINE bibliographic data-ple) search topics are derived from interviews with real biology researchers written using technical English terminologies. Relevance assessments were carried out using the conventional pooling method, and each pooled document is judged as definitely relevant (DR), possibly relevant (PR) or not relevant (NR) against the search requests. Documents rated DR or PR are considered as relevant in official evaluations.
The MEDLINE search task is another typical example of a subject topic search, where the search target is to the NTCIR-3 patent task. 4. Retrieval models Our evaluation environment was developed from the Lemur toolkit 2.0.1 for indexing systems ( Ogilvie &amp;
Callan, 2002 ) with the PostgreSQL RDB system integrated for treating bibliographic information. The system is operated on a PC running the Linux operating system. 4.1. Indexing language and indexing strategies
The ChaSen version 2.2.9 Japanese morphological analyzer with the IPADIC dictionary version 2.5.1 was utilized for Japanese text segmentation. The single-word outputs are indexed, excluding stop words. Stop word lists for patent documents and for newspaper documents were prepared separately. As the system is not sufficiently scalable to index the entire textual contents of the whole patent collection at once, the
NTCIR-3 and NTCIR-4 patent collections were partitioned into subcollections according to the published abstract and claim fields, was also prepared. 4.2. TF*IDF retrieval model The first retrieval model, which was used as a baseline for any other new methods, is TF*IDF with Okapi
BM25 TF (TF*IDF), which we have used extensively in the past. Retrieval status value (RSV) between a doc-ument d and a query q is calculated as the dot product between the document term vector and the query term vector, where each term is weighted by either TF( d , t )IDF( t ) or TF( q , t )IDF( t ).

As the following formula shows, BM25 TF ( Robertson &amp; Walker, 1994; Robertson, Walker, Jones, Han-or b , must be adjusted empirically ( Fujita, 2000 ) d document or query t term
N total number of documents in the collection df ( t ) number of documents where t appears freq( d , t ) number of occurrences of t in d dld document length of d avdl average document length in the collection large, we adopted a standard IDF adjusted by the k 4 parameter as described in Robertson and Walker (1997) . The same weighting is applied to the query but with a different value for k 1 and without length normalization, 4.3. KL-divergence retrieval model
The second retrieval model applied, the KL-divergence of probabilistic language models with Dirichlet prior smoothing (KL-Dir), is a comparatively new method ( Lafferty and Zhai, 2001; Zhai &amp; Lafferty, 2001 ). For the KL-divergence model, Jelinek X  X ercer smoothing was also tried as an alternative smoothing method (KL X  X M).

The adopted model is simple: estimate a language model for each document, and rank documents by the which estimates a language model for each class and ranks the classes by the likelihood of generating the doc-eliminating the document-independent part, we have Assuming a simple unigram model of documents, p ( q j d ) is computed as a product of the probabilities of each query term q i given document d as follows: Taking the logarithm, the retrieval function becomes A document-dependent prior probability p ( d ) can be either a uniform probability or any document-dependent factors that may affect the relevance, such as document length or hyperlink-related information. Assuming a uniform prior probability and dropping the first term, transforming the summation over query term positions into a summation over words in the vocabulary, and dividing by the query length, we have This is exactly the negative cross-entropy of a query language model with a document language model, which measures the difference between the two probability distributions. This is equivalent to the KL-divergence of a query language model from a document language model when ranking documents against the given query. 4.4. Smoothing methods Zhai and Lafferty suggest that a smoothing method plays a crucial role in language-modeling IR ( Zhai &amp; accommodate generation of common words in a query X  X . In this respect, smoothing plays a role similar to IDF in TF*IDF weighting. They propose three types of smoothing strategies including: the Jelinek X  X ercer method, which is a simple linear combination of an estimated document model and a background model p ( w j C ); Bayesian smoothing using the Dirichlet priors method, which computes maximum a posteriori param-eter values with a Dirichlet prior (i.e., generalization of Laplace smoothing); and the absolute discount method, which we do not consider further in this paper. C in p ( w j C ) stands for  X  X  X ollection X  X  and denotes the collection language model.

The Jelinek X  X ercer method is as follows:
The Dirichlet prior method is as follows: consideration in Dirichlet prior smoothing. 4.5. Feedback strategies
A so-called pseudo-relevance feedback strategy was applied in both tasks. The top k documents from the result list of a pilot search were used for feedback term extraction, and the expanded queries were submitted against the target collection to obtain the final results.
 We adopted Rocchio feedback ( Rocchio, 1971 ) for TF*IDF, a Markov chain query update method for the
Lafferty, 2001a ) for some comparative experiments of KL-divergence runs; salient terms from these pseudo-relevant documents are extracted and added to the original query. For example, in Rocchio feedback, relevant documents. Finally, an updated query vector Q 0 was computed from the original query vector Q and a set of (pseudo) relevant document vectors R
The parameters, such as the number of documents for the pseudo-relevant set, the number of terms to feed back, some score cutoff threshold values and mixture coefficients of feedback terms  X  X  X osCoeff X  X  against origi-nal terms, are decided by empirical examination.

For the KL-divergence retrieval model, the first choice for the feedback strategy is the so-called  X  X  X ixture model query update method X  X , where language models for pseudo-relevant documents are distilled by eliminat-document model p ( F j h ) is estimated given a fixed mixture parameter k
Another feedback strategy performed in the NTCIR-4 patent task is the so-called  X  X  X arkov chain method X  X  of the probability of word w given the original query q and a set of relevant or pseudo-relevant documents
R ( q ), as follows: feedback coefficient a
The parameters introduced in Section 4.4 , such as the number of documents for the pseudo-relevant set, the number of terms to feed back and the score cutoff threshold values, are applicable here, but the feedback coef-model, where a positive coefficient is a multiplier for the weights of the terms from positive documents. 5. Patent document retrieval experiments
Comparative experiments using the NTCIR-3 and -4 patent test collections are presented. As various issues subcollections, but centralized searches are simulated.

As shown in Table 1 , a stable improvement of mean average precision (MAP) from 15.03% to 19.84% is observed in the NTCIR-3 patent test collection, i.e., a simulated technology survey task. The mixture model feedback is applied for KL-Dir and KL X  X M runs. The improvements are statistically significant ( p &lt; 0.05) in experience.

Fig. 3 compares feedback weighting parameters (feedback coefficient parameters) for NTCIR-3 with those for NTCIR-4 when KL-Dir retrieval models are applied. The feedback coefficient parameters of the NTCIR-3 patent runs show that the maximum effectiveness is achieved when the feedback coefficient is 0.5, which means moderately heavy weighting on expansion terms. Fig. 4 compares the feedback weighting parameters, or feed-back positive coefficient parameters, of TF*IDF runs. A positive weighting on positive feedback terms clearly improves effectiveness in the NTCIR-3 patent runs. An alternative query update method in KL runs, i.e., the Markov chain update method, did not improve the results, as the lowest row of Table 1 shows. As shown in Table 2 , the best KL-Dir runs improved by 9.5 X 9.7% with pseudo-feedback, whereas the best runs were degraded by pseudo-feedback. For TF*IDF, no feedback run achieved the best MAP; this indicates that a feedback strategy does not really help in this task setting. Simply, poor baseline runs of KL-Dir and KL X  X M emphasized the effects of feedback.

In the middle row of Table 2 , where the pseudo-feedback results of NTCIR-4 patents are shown, we adopted a Markov chain query update method for KL runs, as it tended to be better than the mixture model feedback, although they are less effective than the best TF*IDF no-feedback runs. It turned out that a Markov chain query update method returns sometimes only a few words to expand, so feedback queries are much shorter than those of a mixture model method, as Table 3 shows. In patent retrieval situations, even more than 200 documents are not enough to extract about 100 terms. Instead, the threshold of the probability of gener-ating an expansion term is decreased to extract more terms. However, using the same number of expansion terms as a mixture model feedback does not improve the results.

For the NTCIR-4 patent runs, only a small number of feedback terms improved the effectiveness, whereas 100 terms degraded it. The lowest row of Table 2 shows the best KL-Dir and KL X  X M runs with pseudo-feedback using the mixture model query update method. Both the KL-Dir and KL X  X M runs were degraded. The best performance was achieved when the feedback coefficient was 0.0, whereas it was 0.5 in the NTCIR-3 patent runs. This indicates that the feedback document models are not a good representation of relevance, so that feedback only increased effectiveness when the baseline performance was poor.

Fig. 3 shows that some improvements with feedback are observed in KL-Dir runs, mainly because of the of TF*IDF runs in the NTCIR-4 patent setting, whereas it improves the NTCIR-3 patent runs.

Fig. 5 shows the sensitivity of MAP to Dirichlet prior l for KL-Dir runs, where our parameter settings for tendency of document length of relevant documents in the two test collections; these issues were extensively discussed in Fujita (2004a) .

In short, the situation in the NTCIR-4 patent runs is very different from that in the NTCIR-3 patent runs, where all the TF-IDF, KL-Dir and KL X  X M runs achieved some improvement with pseudo-feedback. The NTCIR-4 patent runs, which simulate invalidity search, characterize a situation where it is very difficult for pseudo-feedback to improve the results. 6. A comparative study of different collections in view of pseudo-feedback effectiveness
An  X  X  X utomatic feedback from the top-k documents X  X  strategy, which is referred to as  X  X  X seudo-relevance feedback X  X  or  X  X  X lind feedback X  X , gains as much as 20% improvement of MAP, or even more, over the baseline runs in test collection-based evaluations in our experience. No other single method achieves such big gains against reasonably conducted baseline runs. One question that naturally follows is: for what search topics are the feedback strategies effective? If the original query is too poor to retrieve pseudo-relevant documents to improve the results further, pseudo-feedback might again not be successful. Too few relevant documents, where promoting documents does not improve the effectiveness, may result in unsuccessful feedback. None of these conditions is easily predictable when considering search requests in operating search systems. An alter-native question is: to what types of search tasks or collections can the feedback strategy be successfully applied? 6.1. Pseudo-feedback effectiveness As shown in Table 4 , the pseudo-feedback strategy is more successful in the newspaper search tasks of NTCIR-3 or -4 CLIR J-J than in other test collections. It tended to gain more in BM25 TF*IDF runs, where a version of Rocchio feedback is applied, than in KL-Dir runs. Remarkable exceptions are NTCIR-4 patent task runs, where pseudo-feedback degraded the effectiveness even when feedback terms were given low weights.

A pseudo-feedback KL-Dir run achieved +9.38% gain in the NTCIR-4 patent, but the no-pseudo-feedback baseline run is much worse than that of TF*IDF, which suggests that the effect of the pseudo-feedback is over-estimated by the poor baseline performance. Even the best KL-Dir feedback run (0.2508) is worse than the best TF*IDF no-feedback run (0.2625).

The case of TREC 2004 MEDLINE is a sort of  X  X  X aturated original query case X  X , where the long queries contain enough good terms so that further improvement was difficult to achieve. This is examined by  X  X  X ITLE only query X  X  runs as described by Fujita (2004b) where the maximum 13% of pseudo-feedback gains are observed.

The NTCIR-4 patent task is the case for which pseudo-feedback is very difficult, because there are too few relevant documents and comparatively long original queries. Even in this situation, evaluating by relaxed rel-evance, and consequently with more relevant documents, 0.6% of gain is observed. Using relaxed relevance criteria, feedback improvement seems to be more modest than using rigid relevance. A larger number of rel-evant documents seems to make evaluation measures comparatively stable, and overestimated improvements are weakened. Table 5 shows the numbers of relevant, partially relevant, pooled, and all documents in each examined test collection. The density of relevant documents is extremely low in the NTCIR-4 patent collection.

The question arising here is whether the NTCIR-3 patent and NTCIR-4 patent tasks are different in view of search techniques. Fujita (2004a) pointed out that the NTCIR-4 patent task requires more document-length-sensitive techniques, where longer documents are more likely to be relevant. We examine the question from a different aspect, i.e., the cluster hypothesis. 6.2. Sensitivity of feedback effectiveness to test collection characteristics
From these observations, we conclude that the top-k document feedback strategy is exceptionally successful in some test collections, such as NTCIR-3 and 4 CLIR J-J, where about 20% improvement is achieved. Typ-with such test collections. The common characteristics of such test collections include the following items. (1) Short queries : (2) Sufficient relevant documents : (3) Terminologically controlled and  X  X  X lean X  X  document collections such as newspapers or newswires : (4) Topical cohesion through relevant documents :
We examine the last characteristic in this section. Cohesiveness is understood as groups of shared terminol-ogy by documents relevant to the same search topics. The notion is studied as the  X  X  X luster hypothesis X  X  in the history of IR studies, and some measures indicating how well the  X  X  X luster hypothesis X  X  holds true have been proposed, but no clear correlation between the measure and retrieval effectiveness has been observed ( Voorh-uments, among which some are possibly relevant to the same search topics, in the collection, while it indicates relevant documents are topically more cohesive than the  X  X  X elaxed X  X  relevant documents. To measure such top-ical cohesiveness, the following tests are introduced. 6.3. Five-nearest-neighbor test
Voorhees (1985) proposed a test for measuring how well the cluster hypothesis characterizes a collection. It neighbor documents of a relevant document.

Fig. 6 shows the five-nearest-neighbor test against four test collections as well as the TREC-4 ad hoc test collection ( Harman, 1995 ) and some other collections analyzed by Voorhees (1985) . For the NTCIR-3 and -4 Section 4.1 to reduce computational costs. TREC-4 data is added because it was an interesting collection where top-k document feedback strategies were exceptionally successful. MED, CACM, CISI and INSPEC are all test collections from the pre-TREC era, and the numbers of documents are limited. Six graded color tones (from white to dark) indicate percentages of relevant documents with 0, 1, 2, 3, 4 and 5 relevant docu-ments respectively found in five nearest neighbors, against all the relevant documents.

For the four test collections examined, the hypothesis seems to hold true: in test collections that strongly satisfy the nearest neighbor test, top-k document feedback strategies tend to be effective. In the NTCIR-3 and 4 CLIR J-J test collections, for about 86 X 88% of relevant documents, at least one relevant document is found slightly lower, around 76%, and the feedback effectiveness is lower than for the CLIR J-J collections. The NTCIR-4 patent collection is an extremely case where only 45% of relevant documents have more than one relevant documents in their five nearest neighbors. It is partially because the NTCIR-4 patent collection test. Among other collections, even the second smallest one, CACM, has 15.3 relevant documents per topic. In any case, only 45% of relevant documents have at least one relevant neighbor in the five nearest documents, which explains why no feedback strategy is successful in this test collection. 6.4. IPC test
For each patent application document, more than one IPC code is attributed according to the Strasbourg pact. For example, for topic 008 of the NTCIR-4 patent task, only one relevant document was found, and for this relevant document, four different IPC codes were assigned. We consider an IPC code as a species and an assignment of an IPC code to a document as an individual for a biological diversity index: Simpson X  X  D given as follows, is applied in view of measuring the diversity of IPC assignment in a document set ( Simpson, 1949 ) S total number of species p i proportion of S made up of the i th species
S denotes the number or species, i.e., unique number of IPCs, and it signifies the richness of species. Simp-son X  X  D indicates the probability that two randomly selected individuals in the community are of the same spe-cies. Consequently 1 D measures the diversity. Equitability or Evenness ( E ) indicates how evenly individuals are distributed among many species.
 As Table 6 shows, NTCIR-3 patent sets have more diversity, and AB-relevant sets are more diverse than A-relevant sets. On the other hand, NTCIR-4 patent sets have much higher equitability than NTCIR-3 sets. Because the number of relevant documents differs considerably between the NTCIR-3 and -4 patent collec-tions, it is difficult to say which sets are more diverse in view of IPC code assignment. Although Simposon X  X  D is less sensitive to the number of species than Shanon X  X  H , complete rank order concordance is observed between S and D , as well as complete inversed rank order concordance between S and E . 6.5. Query clarity test
Cronen-Townsend, Zhou, and Croft (2002) proposed a query clarity measure, KL-divergence between the query and collection language models, to predict the performance of a given query. We computed a KL-diver-gence between the feedback and the collection language models in four test collections as follows, but we were not able to find any strong correlation between the measure and the effectiveness on a topic-by-topic basis p ( w j F ) feedback document model p coll ( w ) collection language model
The query clarity measure gives a large value for a term with low DF, i.e., higher discriminative power through the collection and with higher TF in the subset, which is feedback documents in our case, compared with documents that contain at least one query term in the Cronen-Townsend case, where a query language model of Lavrenko and Croft (2001) was adopted. In this sense, the measure is similar to the Robertson X 
Sparck-Jones probabilistic weighting or the mutual information between relevance and the collections exam-ined by Fujita (2005) to explain the search effectiveness from query discriminative power. The problem with the score is that even when the query contains many poor terms with lower clarity, the clarities for some query terms, which characterize the subset, become higher. Consequently the query clarity measure does not corre-among all the query terms but is not adequate to show how good the total query is.

Figs. 7 and 8 show the NTCIR-3 and -4 patent initial APs, feedback APs and query clarity measures on a topic-by-topic basis in decreasing order of feedback APs. In the NTCIR-4 patent task, the feedback gains depend on only a few topics, mainly topic 10: the effect is presumably unstable.

Pearson X  X  correlations of query clarity measures with feedback APs or feedback gains are shown in Table 7 . 6.6. A methodological question for evaluation Small numbers of relevant documents and uncertain judgment cause unstable intersystem rankings of uations, not only intersystem rankings across participating groups but also intersystem rankings in the same group are very important in claiming any significant findings. One way to stabilize the evaluation is to use many more topics using JPO data, as has been widely practiced in the evaluation of industrial search engines. essarily limited to searching JPO citations like documents (except for the patent examination officer) but should use much more diverse information, including who is the authority of the domain and what is the prod-uct using the technology. A citation search is much narrower than the information that a patent  X  X  X nvalidator X  X  must retrieve. Furthermore, hundreds of topics make evaluation of manual query construction, which is inten-sively used by professional patent searchers, practically impossible.

On the other hand, the NTCIR-3 patent collection gathers many more relevant documents for each topic; such as the NTCIR-3 patent collection, simulating the narrower information needs than a technology survey search, but enhancing relevant documents by considering diverse aspects of the invalidity search, where in fact document feedback strategy is also observed in the NTCIR-3 collection. 7. Conclusions
A comparative study of patent search tasks using the NTCIR-3 and -4 patent collections has been reported ferent search tasks. The TF*IDF approach and the KL-divergence language modeling approach were applied to two test collections with different search tasks.

A comparative evaluation of results suggests that in the NTCIR-4 patent task, it is difficult to improve the results using top-k document feedback strategies, because of the small number of relevant documents and comparatively long original queries. On the other hand, stable and statistically significant improvements are observed in the NTCIR-3 patent task. The five-nearest-neighbor test suggests that the NTCIR-4 patent col-of relevant documents are available and that they are not very similar to each other, whereas the NTCIR-3
On the other hand, a diversity analysis on IPC code distributions in relevant documents suggests that rel-evant documents of the NTCIR-4 patent collection are more evenly distributed in terms of IPC code assign-ments, whereas those of the NTCIR-3 patent collection are assigned more IPC codes and consequently are more diverse in view of category richness.
 Acknowledgements
We extend thanks to the NII-NTCIR projects for providing the NTCIR-3/4 CLIR J-J/patent test collections.
 We are grateful to the CMU/Umass Lemur Project for making the Lemur toolkit available.
 References
