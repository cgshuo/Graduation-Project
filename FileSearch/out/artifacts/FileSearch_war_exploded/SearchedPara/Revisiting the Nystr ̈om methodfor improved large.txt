 Alex Gittens gittens@caltech.edu Michael W. Mahoney mmahoney@cs.stanford.edu Dept. of Mathematics, Stanford University, Stanford, CA 9430 USA We reconsider randomized algorithms for the low-rank approximation of SPSD matrices such as Laplacian and kernel matrices that arise in data analysis and machine learning applications. Of particular interest are sampling-based versus projection-based methods as well as the use of uniform sampling versus nonuni-form sampling based on the leverage score probabili-ties. Our main contributions are fourfold.
 First, we provide an empirical evaluation of the complementary strengths and weaknesses of data-independent random projection methods and data-dependent random sampling methods when applied to SPSD matrices. We do so for a diverse class of SPSD matrices drawn from machine learning and more gen-eral data analysis applications, and we consider recon-struction error with respect to the spectral, Frobenius, as well as trace norms.
 Second, we consider the running time of high-quality sampling and projection algorithms. By exploiting and extending recent work on  X  X ast X  random projec-tions and related recent work on  X  X ast X  approxima-tion of the statistical leverage scores, we illustrate that high-quality leverage-based random sampling and high-quality random projection algorithms have com-parable running times.
 Third, our main technical contribution is a set of deter-ministic structural results that hold for any  X  X ketching matrix X  applied to an SPSD matrix. (A precise state-ment of these results is given in Theorems 1, 2, and 3 in Section 4.1.) We call these  X  X eterministic structural results X  since there is no randomness involved in their statement or analysis and since they depend on struc-tural properties of the input data matrix and the way the sketching matrix interacts with the input data. Fourth, our main algorithmic contribution is to show that when the low-rank sketching matrix represents certain random projections or random sampling oper-ations, then (by using our deterministic structural con-ditions) we obtain worst-case quality-of-approximation bounds that hold with high probability. (A precise statement of these results is given in Lemmas 1, 2, 3, and 4 in Section 4.2.) These bounds are qualita-tively better than existing bounds (when nontrivial prior bounds even exist).
 Our analysis is timely for at least two reasons. First, existing theory for the Nystr  X om method is quite mod-est. For example, existing worst-case bounds such as those of (Drineas &amp; Mahoney, 2005) are very weak, especially compared with existing bounds for least-squares regression and general low-rank matrix ap-proximation problems (Drineas et al., 2008; 2010; Mahoney, 2011). Moreover, many other worst-case bounds make strong assumptions about the coher-ence properties of the input data (Kumar et al., 2012; Gittens, 2011). Second, there have been conflicting views about the usefulness of uniform sampling versus nonuniform sampling based on the empirical statisti-cal leverage scores of the data in realistic data analysis and machine learning applications. For example, some work has concluded that the statistical leverage scores of realistic data matrices are fairly uniform, meaning that the coherence is small and thus uniform sampling is appropriate (Williams &amp; Seeger, 2001; Kumar et al., 2012), while other work has demonstrated that lever-age scores are often very nonuniform in ways that ren-der uniform sampling inappropriate and that can be essential to highlight properties of downstream inter-est (Paschou et al., 2007; Mahoney &amp; Drineas, 2009). Remark. Space limitations prevent us from present-ing more details from our empirical analysis (including results on additional data, results when the low-rank approximation is regularized to be better conditioned, detailed results on running times for different sketch-ing methods, etc.) as well as additional theoretical analysis. These results, as well as additional discus-sion, are available in the technical report version of this paper (Gittens &amp; Mahoney, 2013). Let A  X  R n  X  n be an arbitrary SPSD matrix with eigenvalue decomposition A = U X U T , where we par-tition U and  X  as Here, U 1 comprises k orthonormal columns spanning the top k -dimensional eigenspace of A ; likewise, U 2 is an orthonormal basis for the bottom n  X  k dimensional eigenspace of A . The diagonal matrix  X  1 contains the largest k eigenvalues of A ; likewise,  X  2 is a diagonal matrix containing the smallest n  X  k eigenvalues of A . We assume  X  1 is full-rank. A k = U 1  X  1 U T 1 is the optimal rank-k approximation to A in any unitarily invariant norm.
 The statistical leverage scores of A relative to the best rank-k approximation to A are the squared Euclidean norms of the rows of the n  X  k matrix U 1 : We denote by S an arbitrary n  X  ` sketching matrix . The matrices capture the interaction of S with the top and bottom eigenspaces of A , respectively. The orthogonal projec-tion onto the range space of a matrix M is written P M . For a vector x  X  R n , let k x k  X  , for  X  = 1 , 2 ,  X  , denote the 1-norm, the Euclidean norm, and the  X  -norm, re-spectively. Then, k A k 2 = k Diag(  X  ) k  X  denotes the spectral norm of A ; k A k F = k Diag(  X  ) k 2 denotes the Frobenius norm of A ; and k A k ? = k Diag(  X  ) k 1 de-notes the trace norm (or nuclear norm) of A .
 The following model for the low-rank approximation of SPSD matrices subsumes sketches based on column-sampling (also known as Nystr  X om extensions) as well as those based on mixtures of columns (also known as projection-based sketches, since the mixtures are often accomplished using Johnson X  X indenstrauss-type dimensionality reducing  X  X rojections X ).  X  SPSD Sketching Model. Let A be an n  X  n pos-The distribution of the (random) matrix S leads to different classes of low-rank approximations.
 We point out that sketches formed using the so-called power method (Halko et al., 2011), for which C = A q S 0 and W = S T and sketching matrix S 0 , fit this model if one consid-ers the sketching matrix to be A q  X  1 S 0 . In (Gittens &amp; Mahoney, 2013), we provide theoretical guarantees on the efficacy of the power method. (Halko et al., 2011) considers SPSD sketches that can be written in the forms P AS AP AS and A ( P AS AP AS )  X  A and finds that the second scheme is empirically more effective, but it provides guaran-tees only for the performance of the first scheme. We note that both schemes fit into our SPSD Sketching Model; we provide error bounds for the second sketch-ing scheme by establishing that it is an instance of the power method, with q = 2 . See (Gittens &amp; Mahoney, 2013) for details.
 A large part of the recent body of work on randomized matrix algorithms has been summarized in the recent monograph of Mahoney (Mahoney, 2011) and the re-cent review of Halko, Martinsson, and Tropp (Halko et al., 2011). Much of the work in machine learning on the Nystr  X om method has focused on new proposals for selecting columns ( e.g. , (Zhang et al., 2008; Zhang &amp; Kwok, 2009; Liu et al., 2010; Arcolano &amp; Wolfe, 2010)) and/or coupling the method with downstream applica-tions. Ensemble Nystr  X om methods, which mix several simpler Nystr  X om extensions, and related schemes for improving the accuracy of Nystr  X om extensions have also been investigated (Kumar et al., 2009; Li et al., 2010; Kumar et al., 2012).
 On the theoretical side, much of the work has followed that of Drineas and Mahoney (Drineas &amp; Mahoney, 2005), who provided the first rigorous bounds for the Nystr  X om extension of a general SPSD matrix. Rather than summarize this work in detail, we simply refer to Table 1 (our results are from Lemma 4; we have similar improvements for Lemmas 1, 2 and 3) to provide an example of our improvement relative to related work. Here, we present our main empirical results, which consist of evaluating sampling and projection algo-rithms applied to a diverse set of SPSD matrices. We don X  X  intend these results to be  X  X omprehensive X  but instead to be  X  X llustrative X  case-studies. That is, we illustrate the tradeoffs between these methods in dif-ferent realistic applications. 3.1. SPSD Sketching Algorithms We provide empirical results for four sketches, based on sampling columns uniformly at random, sampling columns using leverage scores, mixing columns using a subsampled randomized Fourier transform (SRFT), and taking Gaussian mixtures of columns.
 In the case of Gaussian mixtures, S is a matrix of i.i.d. N (0 , 1) random variables. In the case of SRFT mix-tures, S = p n ` DFR , where D is a diagonal matrix of Rademacher random variables ( i.e. , random  X  1s with equal probability), F is the real Fourier transform ma-trix, and R restricts to ` columns.
 Observe that { ` j /k } j  X  X  1 ,...,n } is a probability distribu-tion over the columns of A . Sketches based on lever-age scores take S = RD where R  X  R n  X  ` is a column selection matrix that samples columns of A from the given distribution and D is a diagonal rescaling matrix satisfying D jj = 1  X  Exact computation of leverage scores is expensive, so we also consider two sketches that use approximate leverage scores: the  X  X ower X  scheme iteratively ap-proximates the leverage scores of A and uses the ap-proximate leverage scores obtained once a specified convergence condition has been met; and the  X  X rob lev X  scheme uses the leverage scores of A X  , where  X  is a fast Johnson-Lindenstrauss transform (Drineas et al., 2012). See (Gittens &amp; Mahoney, 2013) for the full details. 3.2. Data Sets We consider four classes of matrices: normalized Laplacians of very sparse graphs drawn from  X  X n-formatics graph X  applications; dense matrices corre-sponding to Linear Kernels from machine learning ap-plications; dense matrices constructed from a Gaus-sian Radial Basis Function Kernel (RBFK); and sparse RBFK matrices constructed using Gaussian radial ba-sis functions, truncated to be nonzero only for nearest neighbors.
 Recall that, given a graph with weighted adjacency matrix W , the normalized graph Laplacian is A = I  X  D  X  1 / 2 WD  X  1 / 2 , where D is the diagonal matrix of weighted degrees of the nodes of the graph, i.e. , D ii = P j 6 = i W ij . Given a set of data points x 1 ,..., x n  X  the Linear Kernel matrix A corresponding to those points is given by A ij =  X  x i , x j  X  , and a Gaussian where  X  is a nonnegative number. One can spar-sify RBF Kernels while preserving their SPSD nature. See (Gittens &amp; Mahoney, 2013) for the details of the method employed.
 Table 2 illustrates the diverse range of properties ex-hibited by these four classes of data sets. Several ob-servations are particularly relevant to our discussion below. First, the Laplacian Kernels drawn from infor-matics graph applications are extremely sparse, and tend to have very slow spectral decay. Second, both the Linear Kernels and the Dense RBF Kernels are much denser and are much more well-approximated by moderately to very low-rank matrices. In addition, both the Linear Kernels and the Dense RBF Kernels have more uniform leverage scores. Third, we con-sider two values of the  X  parameter for the RBF Ker-nels, chosen (somewhat) arbitrarily. For AbaloneD, we see that decreasing  X  from 1 to 0 . 15, i.e. , letting data points  X  X ee X  fewer nearby points, has two important effects: first, it results in matrices that are much less well-approximated by low-rank matrices; and second, it results in matrices with much more heterogeneous leverage scores. Fourth, for the Sparse RBF Kernels, there are a range of sparsities, ranging from above the sparsity of the sparsest Linear Kernel, but all are much denser than the Laplacian Kernels. In addition, spar-sifying a Dense RBF Kernel has the effects of making the matrix less well approximated by a low-rank ma-trix and making the leverage scores more nonuniform. 3.3. Reconstruction Accuracy of Sampling and Here, we describe the performances of the SPSD sketches in terms of reconstruction accuracy for the data sets described in Section 3.2.
 Abridged Empirical Evaluation. Figure 1 shows the Frobenius (top panel) and trace (bottom panel) norm errors of several Nystr  X om schemes, as a func-tion of the number of column samples ` , for datasets from Table 2.  X  X nif X  denotes uniform column sam-pling;  X  X rft X  denotes SRFT mixtures;  X  X aussian X  de-notes Gaussian mixtures;  X  X evscore X  denotes column sampling using the exact leverage scores; and  X  X ower X  and  X  X rob lev X  are as described in Section 3.1. Figure 2 shows the times required to compute these sketches. Summary of Comparison of Sampling and Pro-jection Algorithms. Linear Kernels and to a lesser extent Dense RBF Kernels with larger  X  parameter have relatively low-rank and relatively uniform lever-age scores. In these circumstances uniform sampling does quite well. These data sets correspond most closely with those that have been studied previously in the machine learning literature; for these data sets our results are in agreement with those of prior work. Sparsifying RBF Kernels and/or choosing a smaller  X  parameter tends to make these kernels less well-approximated by low-rank matrices and to have more heterogeneous leverage scores. In general, these two properties are not directly related X  X he spectrum is a property of eigenvalues, while the leverage scores are determined by the eigenvectors X  X ut for the data we examined they are related, in that matrices with more slowly decaying spectra also often have more hetero-geneous leverage scores.
 For Dense RBF Kernels with smaller  X  and Sparse RBF Kernels, leverage score sampling tends to out-perform other methods. Interestingly, the Sparse RBF Kernels have many properties of very sparse Lapla-cian Kernels corresponding to relatively-unstructured informatics graphs, an observation which should be of interest for researchers who construct sparse graphs from data using, e.g. ,  X  X ocally linear X  methods to try to reconstruct hypothesized low-dimensional manifolds. Reconstruction quality under exact leverage score sam-pling saturates, as a function of choosing more sam-ples ` . As a consequence, the value of ` used deter-mines whether leverage score sampling or other sketch-ing methods result in lower errors.
 Summary of Leverage Score Approximation Algorithms. The running time of computing the exact leverage scores is generally much worse than that of uniform sampling and both SRFT-based and Gaussian-based random projection methods. The run-ning time of computing approximations to the lever-age scores can, with appropriate choice of parameters, be much faster than the exact computation; and, es-pecially for  X  X  X rob lev, X  X  it can be comparable to the time needed to execute the random projection used in the leverage score approximation algorithm (Drineas et al., 2012). For methods that involve q &gt; 1 it-erations to compute stronger approximations to the leverage scores, the running time can vary consider-ably depending on the stopping condition.
 The leverage scores computed by the  X  X rob lev X  pro-cedure are typically very different than the  X  X xact X  leverage scores, but they are leverage scores for a low-rank space that is near the best rank-k approximation to the matrix. This is often sufficient for good low-rank approximation, although the reconstruction accu-racy can degrade in the rank-restricted cases (not pre-sented here) as ` is increased. The approximate lever-age scores computed from  X  X ower X  approach those of the exact leverage scores, as q is increased; and they obtain reconstruction accuracy that is no worse, and in many cases is better, than those obtained using the exact leverage scores. This suggests that, by not fit-ting exactly to the empirical statistical leverage scores, we are observing a form of regularization. In this section, we present our main theoretical re-sults, which consist of a suite of bounds on the qual-ity of low-rank approximation under several different sketching methods. These were motivated by our em-pirical observation that all of the sampling and projec-tion methods we considered perform much better on the SPSD matrices we considered than previous worst-case bounds ( e.g. , (Drineas &amp; Mahoney, 2005; Kumar et al., 2012; Gittens, 2011)) would suggest.
 Our results are based on the fact, established in (Git-tens, 2011), that approximations which satisfy our SPSD Sketching Model satisfy 4.1. Deterministic Error Bounds Here, we present theorems that bound the spec-tral, Frobenius, and trace norm approximation errors. Throughout, A is an n  X  n SPSD matrix with eigen-value decomposition partitioned as in Equation (1), S is a sketching matrix of size n  X  ` , C = AS and W = S T AS , and  X  1 and  X  2 are defined in Equa-tion (3).
 Spectral Norm Bounds. We start with a bound on the spectral norm of the residual error.
 Theorem 1. If  X  1 has full row-rank, then Proof. It follows from Equation (4) that Next, recall that  X  i = U T i S , and that  X  1 has full-row rank. It can be shown that for any matrix X (Boutsidis et al., 2009; Halko et al., 2011).
 Frobenius Norm Bounds. Next, we state the bound on the Frobenius norm of the residual error. Theorem 2. If  X  1 has full row-rank, then A  X  CW  X  C T Proof. It follows from Equation (4) that E := A  X  CW  X  C T To bound this, we first use the unitary invariance of the Frobenius norm and the fact that P A 1 / 2 S Then we take where I  X  R k  X  k and F  X  R n  X  k  X  k is given by F =  X  our assumption that  X  1 has full row-rank. Since the range of Z is contained in the range of  X  1 / 2 U T S , The fact that Z has full column-rank allows us to write
I  X  P Z = I  X  Z ( Z T Z )  X  1 Z T This implies that where  X  = I + F T F .
 Next, we will provide bounds for T 1 , T 2 , and T 3 . Using the fact that 0 I  X  F ( I + F T F )  X  1 F T I , we can bound T 3 with Likewise, the fact that I  X  ( I + F T F )  X  1 F T F (readily verifiable with an SVD) implies that we can bound T To bound T 2 , observe that where M = F T F . It is readily verifiable, using the SVD, that any SPSD matrix M satisfies the semidefi-nite inequality so we conclude that Combining our estimates for T 1 , T 2 , and T 3 with Equa-tion (9) gives The claimed bound follows by applying the subaddi-tivity of the square-root function.
 Trace Norm Bounds. Finally, we state the follow-ing bound on the trace norm of the residual error. Theorem 3. If  X  1 has full row-rank, then 0 , its trace norm simplifies to its trace. Thus where Z is defined in Equation (7). The expression for P Z supplied in Equation (8) implies that The final equality follows from identifying F and the squared Frobenius norm.
 Remark. The assumption that  X  1 has full row-rank is very non-trivial; it is false, for non-trivial param-eter values, for common sketching methods such as uniform sampling. It is satisfied by our procedures in Section 4.2.
 4.2. Stochastic Error Bounds for Low-rank In this section, we apply the theorems from Section 4.1 to bound the reconstruction errors for several random sampling and random projection methods that con-form to our SPSD Sketching Model. Throughout, A is an n  X  n SPSD matrix.
 Lemma 1 (Leverage-based sketches) . Let S be a sketching matrix of size n  X  ` corresponding to a leverage-based probability distribution derived from the top k -dimensional eigenspace of A , satisfying for some  X   X  (0 , 1] . Fix a failure probability  X   X  (0 , 1] and approximation factor  X   X  (0 , 1] .
 If `  X  3200(  X  X  2 )  X  1 k log(4 k/ (  X  X  )) , then
A  X  CW  X  C T A  X  CW  X  C T
A  X  CW  X  C T each hold, individually, with probability at least 1  X  4  X   X  0 . 4 .
 Lemma 2 (SRFT sketches) . Let S  X  R n  X  ` be an SRFT sketching matrix. Fix a failure probability  X   X  (0 , 1 / 9] and approximation factor  X   X  (0 , 1] . If k  X  ln n and `  X  24  X   X  1 [ then each hold, individually, with probability at least 1  X  2  X . Lemma 3 (Gaussian sketches) . Let S  X  R n  X  ` be a Gaussian sketching matrix. If ` = k + p where p = k X   X  2 for  X   X  (0 , 1] and k &gt; 4 , then
A  X  CW  X  C T A  X  CW  X  C T + (11  X  + 544  X  2 ) q k A  X  A k k 2 k A  X  A k k ? + 91
A  X  CW  X  C T each hold, individually, with probability at least 1  X  2 k Lemma 4 (Uniform column sampling) . Let S  X  R n  X  ` be a sketching matrix corresponding to sampling the columns of A uniformly at random (with or without replacement). Let denote the coherence of the top k -dimensional eigenspace of A and fix a failure probability  X   X  (0 , 1) and accuracy factor  X   X  (0 , 1) . If `  X  2  X   X  2  X k ln( k/ X  ) , then each hold, individually, with probability at least 1  X  4  X . Remark. The additive scale factors for the spectral and Frobenius norm bounds are much improved rela-tive to the prior results of (Drineas &amp; Mahoney, 2005). To our knowledge, our results supply the first relative-error trace norm approximation bounds.
 As with previous bounds for uniform sampling, e.g. , (Kumar et al., 2012; Gittens, 2011), the re-sults in Lemma 4 are much weaker than those for projecton-based and leverage score sampling-based SPSD sketches, since the sampling complexity depends on the coherence of the input matrix.

