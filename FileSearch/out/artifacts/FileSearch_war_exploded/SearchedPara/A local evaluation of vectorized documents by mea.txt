 ORIGINAL PAPER R. Raveaux  X  J. C. Burie  X  J. M. Ogier Abstract This paper presents a benchmark for evaluating the raster to vector conversion systems. The benchmark is designed for evaluating the performance of graphics recog-nition systems on images that contain polygons (solid) within the images. Our contribution is two-fold, an object mapping algorithm to spatially locate errors within the drawing and then a cycle graph matching distance that indicates the accu-racy of the polygonal approximation. The performance incor-porates many aspects and factors based on uniform units while the method remains non-rigid (thresholdless). This benchmark gives a scientific comparison at polygon level of coherency and uses practical performance evaluation meth-ods that can be applied to complete polygonization systems. A system dedicated to cadastral map vectorization was eval-uated under this benchmark and its performance results are presented in this paper. By stress testing a given system, we demonstrate that our protocol can reveal strengths and weak-nesses of a system. The behavior of our set of indices was analyzed when increasing image degradation. We hope that this benchmark will help assessing the state of the art in graphics recognition and current vectorization technologies. Keywords Vectorized object comparison  X  Polygon assignment  X  Polygonization accuracy  X  Performance evaluation  X  Polygon detection quality  X  Graphics recognition  X  Machine drawing understanding system 1 Introduction In this paper, the question of performance evaluation is dis-cussed. A bird X  X -eye view methodology is adopted starting from the generic idea about evaluation of vectorization and the main principles of the proposed approach. Then, the introduction turns into a deeper discussion explaining in details both related works and the description of the proposed method. 1.1 Bird X  X -eye view of vectorization evaluation Driven by the need to convert a large number of hard copy engineering drawings into CAD files, raster to vector con-version has been a field of intense research for the last four decades. In addition to research prototypes in several aca-demic and industrial research centers, several commercial software products are currently available to assist users in converting raster images to vector (CAD) files. However, the process of selecting the right software for a given vector-ization task is still a difficult one. Although trade magazines have published surveys of the functionality and ease of use of vectorization products [ 1 ], a scientific, well designed, com-parison of the auto-vectorization capability of the products was still required.

Responding to this need, the International Association for Pattern Recognition X  X  technical committee on graphics recognition (IAPR TC10) organized the series of graphics recognition contests. The first contest, held at the GREC X 95 workshop in University Park, PA, focused on dashed-line detection [ 2  X  4 ]. The second contest, held at the GREC X 97 workshop in Nancy, France, attempted to evaluate complete (automatic) raster to vector conversion systems [ 5  X  7 ]. The third contest, held off-line associated with the GREC X 99 workshop in Jaipur, India, also aimed to evaluate complete (automatic) raster to vector conversion systems. These con-tests tested the abilities of participating algorithms / systems to detect segments and arcs from raster images. They adopted a set of performance metrics based on the published line detection performance evaluation protocol [ 8 ] to evaluate and compare the participating algorithms / systems online at the workshop site with test data of different quality and complexity. Pre-contest training images and the performance evaluation software were provided before the contests, so prospective participants could try their systems and improve them for optimal performance. Test images could be synthe-sized and/or real scanned images.

Performance evaluation and benchmarking have been gaining acceptance in all areas of computer vision and so in the graphics recognition field of science.

Early work on this topic was carried out to evaluate per-formance of thinning algorithms. Haralick [ 9 ] was the first to propose a general approach for performance evaluation of image analysis, with thinning taken as a case in point. Evaluation and comparison of thinning algorithms have also been performed by [ 10  X  12 ] and [ 13 ]. Some of these evalua-tion and comparison works were carried out from the view-point of OCR, while the work of [ 12 ] is domain independent. Although thinning may also be employed as preprocessing of line detection, the latter has different characteristics and therefore requires a different evaluation protocol.
Vectorization and other line detection techniques have been developed to convert images of line drawings in var-ious domains from pixels to vector form (e.g., [ 14  X  16 ]), and a number of methods and systems have been pro-posed and implemented (e.g., [ 17  X  20 ]). Objective evalu-ations and quantitative comparisons among the different shape detection algorithms are available thanks to protocols issued from GREC contests [ 21  X  23 ] that provide quantitative measurements.

Performance evaluation of vectorization and line detec-tion has been reported by [ 3 , 8 , 24 ] and [ 5 ]. Kong et al. [ 3 ] propose a quantitative method for evaluating the recognition of dashed lines. Hori and Doermann [ 24 ] propose a quan-titative performance measurement methodology for task-specific raster to vector conversion. Wenyin and Dori [ 8 ] present a protocol for evaluating the recognition of straight and circular lines. Phillips and Chhabra [ 5 ] define a method-ology for evaluating graphics recognition systems operating on images that contain various objects such as straight lines and text blocks. All of these methods are limited in their applicability and are discussed in the next subsection.
In this article, we present a benchmark designed for eval-uating the performance of graphics recognition systems on images that contain occlusions within the images. Most engi-neering documents could be approximated by polygons and our study focuses on these particular primitives. For instance, parcels into a cadastral map are well modeled by polygons. Accurate and efficient vectorization of line drawings is essen-tial for any higher level processing in document analysis and recognition systems. In spite of the prevalence of vectoriza-tion methods, no standard for their performance evaluation protocol exists at a polygon level. All prior works focused on a lower level of consistency (arcs and segments). We propose a protocol for evaluating polygon extraction to help compare, select, improve, and even design object detection algorithms to be incorporated into drawing recognition and understand-ing systems. The protocol can be seen as an extension to poly-gon level of related approaches by proposing an evaluation, which is closer to the user requirements (i.e. at a semantic level). This new viewpoint on the problem involves two local dissimilarity measures for estimating polygon detection and approximation quality. 1.2 Related work Kongetal.[ 3 ]havedevelopedaprotocolandasystemforsys-tematicallyevaluatingtheperformanceoflinedetectionalgo-rithms, mainly for dashed-line detection algorithms. They define the overlap criteria of the match between a ground-truth and a detected line based on the angle and the distance between them, and the partial overlap is also considered. They do not allow for fragmentation of detected lines. They use several arbitrary and rigid thresholds, for example, the angle should be less than 3  X  and the distance between two lines less than 5 cells.

Hori and Doermann [ 24 ] instantiate and extend Haralick X  X  framework for performance characterization in image anal-ysis [ 9 ], in an application-dependent manner, for measuring the performance of raster to vector conversion algorithms. They provide a set of metrics (evaluation contents), which is specifically geared to vectorization of mechanical engineer-ing drawings. The  X  X pplications X  addressed in the work are thinning, medial line finding, and line fitting all low-level techniques that do not completely constitute vectorization. It is hard to extend the work to evaluate a complete vector-ization system. Hori and Doermann X  X  protocol does not dis-tinguish between detection rate and false alarm rate. It does not include an overall evaluation metric. It does not allow for fragmentation of detected lines. The metrics for line evalu-ation are given in several non-uniform units. It uses length ratio, deviation, and count ratio to evaluate the line length detection, line location detection, and line quantity detection, respectively. There is lack of an overall evaluation metric, which provides an overall combined performance evaluation of the algorithm under consideration.

WenyinandDori[ 8 ]proposeperformanceevaluationindi-ces for straight and circular line detection. Detection and false alarm rates are defined at both the pixel level and the vector level. Use of pixel level performance indices (mea-sures of shape preservation) is not completely appropriate for real images that contain severe distortions such as warp-ing and/or other defects introduced in the hard copy drawing and/ordefectsgeneratedbythescanning/imagingsystem.On such images, attempts to obtain a high pixel recovery index would unnecessarily require the detected vectors to be true to the distorted shape of the imaged lines thereby making the detected lines fragmented. For such images, the pixel recov-ery index needs to be assigned less weight than the vector recovery index. However, there is no way to predetermine the right relative weights for the pixel and vector recovery indices.

Phillips and Chhabra [ 5 ] present the task of evaluation from the opposite angle. They do not look at the complexity of the entities to be recognized. Instead, in their view, the true measure of performance has to be goal directed. The goal of line drawing recognition is to convert a paper copy or a raster image of a line drawing into a useful form (such as a vector CAD file). How well a graphics recognition system works should be measured by how much manual effort is required to correct the mistakes made by the system, not by how well it recognizes difficult shapes. The goal of the evaluation is to measure the cost of post-processing operations that are nec-essary to correct the vectorization mistakes. EditCost is the cost estimate for human post-editing effort to clean up the recognition result.

Based on this synthesis of performance evaluation sys-tems, one can observe that most of these methods remain at a very low level of analysis of the information (vector level), while user requirements often concern high-level analysis. From the related work which focuses on low-level primitives (segments, arcs), we extend the global concept of perfor-mance evaluation of vectorized documents to polygon level. Herein, we present our work, a recovery index which com-bines a local overlapping metric at polygon level when data are closer to the semantic level and a matching distance for evaluating the polygonal approximation correctness in terms of edit operations. 1.3 Our approach Wenyin and Dori [ 8 ] and [ 5 ] are well-suited tools to tackle the performance evaluation problem of vectorized documents. However, to be more realistic and closer to objects handled by humans, [ 8 ] and [ 5 ] also underlined the need to con-sider more complex structures or domain-specific objects into the assessment process. For instance, in [ 5 ], Dr. Chhabra reported as a shortcoming that  X  X he detection of polylines, polygons, objects, symbols, etc. was not tested X . A step in this direction is to address the problem under the prism of grouping of vectors. Unfortunately, prior algorithms cannot be easily modified to reach higher level objects since no match was attempted between solid entities. In fact, if in the case of low-level primitives the matching can be easily reduced to an overlapping criterion; for more complex ele-ments the question is more ambitious. Due to fragmentation phenomena introduced by R2V tools, an entity of the ground-truth can possibly be related to many elements of the auto-vectorized version of the document. Solving this ambiguity requires complex matching algorithms that are not provided by prior works because the underlying problem does not exist at a low level of analysis. Rather than consider polygon frag-mentation and combination as being simply wrong, and only allow the best match with the maximum overlap, we address thequestioninanoptimalmannertofindbestpolygonassign-ments. As a consequence, to address the performance evalu-ation problem at polygon level, we need to provide a robust object matching. In our our approach, this major phase is carried out by a combinatorial framework to perform poly-gon assignments. Secondly, starting from the original idea of EditCost explained by Phillips and Chhabra, the cost esti-mate for human post-editing effort to clean up the recognition result, we propose the use of a graph matching. This para-digm provides more than a value in R , it reveals the sequence of corrections to be made to transform a set of connected line segments into another.

Through the reading of the literature, on the topic of per-formance evaluation of document image algorithms, we took into account comments and limitations of former protocols to detect five desired points: 1. To consider object fragmentation 2. To provide indices in uniform units 3. A generic and a domain-independent protocol 4. An overall evaluation metric 5. To evaluate how much manual effort is required to Our proposal fulfills these five points: (1) A polygon assign-ment method and a graph matching algorithm tackle both polygon and line fragmentation problems; (2) Our two indi-ces are bounded between 0 and 1; (3) No assumptions as to the kind of documents are made by our protocol, the only constraint is that the document must contain polygons; (4) An overall metric is provided by linear combination of the two proposed indices; (5) The EditCost representative of the manual labor to be made to correct a document is envisaged through the graph matching question in terms of basic edit operations (addition, deletion, substitution). Furthermore, working at polygon level offers many advanta-ges. It makes the spotting of errors easier, there are much less polygons than vectors in a drawing, so the visualization of mistakes is pretty fast. This point is very important for indus-trial systems, since it permits to reduce the user correction time, by helping him to focus on errors directly. Furthermore, this facilitates the study of large samples of documents and new error categorizations may arise. Addressing the question from another point of view can help developers to improve and design R2V software.
 We cannot solve problems by using the same kind of thinking we used when we created them. (Albert Einstein)
We propose here a novel and optimal object matching for polygon comparison from a viewpoint that differs from prior works. We consider the coherency of the document at a polygon level. Our polygonization evaluation is based on a polygon mapping constrained by the topological informa-tion. While this measure appreciates the quality of the poly-gon overlapping, a cycle graph matching takes a closer look at a lower level of information: the segment layout within the polygons. In this way, we express the consistency of the drawing from a polygon point of view.

The smallest item that can be found in a engineering draw-ing is the segment.
 Definition 1 (Segment) In geometry, a line segment is a part of a line that is bounded by two end points and contains every point on the line between its end points.
 This object is considerably versatile, the number of line seg-ments present in a wire drawing can be significantly impacted by the vectorization algorithm due to the noise that occurred in the original image of documents (Noise due to storage conditions, digitization steps). On the opposite, we decide to investigate a more consistent and reliable object called polygon.
 Definition 2 (Polygon) In geometry, a polygon is tradition-ally a plane figure that is bounded by a closed path or circuit, composed of a finite sequence of straight line segments (i.e., by a closed polygonal chain). These segments are called its edges or sides, and the points where two edges meet are the polygon X  X  vertices or corners. A polygon is a 2-dimen-sional example of the more general polytope in any number of dimensions.

The polygons are formed by running a cycle detec-tion algorithm on the heap of segments that composed the drawing. Invented in the late 1960s, Floyd X  X  cycle-finding algorithm [ 25 ] is a pointer algorithm that uses only two point-ers, which move through the sequence of points at different speeds. This polygon layer is more reliable and so it pro-vides a better foundation to build a dissimilarity measure on top of it. A conventional way of defining measures of dis-similarity between complex objects (maps, drawing obtained by vectorization) is to base the measure on the quantity of shared terms. Between two complex objects o 1 and o 2 ,the aim is to find the matching coefficient mc , which is based on the number of shared terms. The polygon organization of a document is a good viewpoint, more stable and less sub-ject to variations than the segment layer. In the mean time, it represents a complimentary view of the problem.

Polygonized elements issued from a raster to vector con-version method are assigned and measured up to a manually vectorized ground-truth. The assignment problem is one of the fundamental combinatorial optimization problems in the branch of optimization or operations research in mathemat-ics. It consists of finding a maximum weight matching in a weighted bipartite graph.

In its proposed form, the problem is as follows:  X  X et D GT , D CG be a ground-truth document and a com- X  There are | D CG | number of polygons in D CG and | D GT
Our combinatorial framework cuts down the algorithmic complexity to an O ( n 3 ) upper bound, depending on the num-ber of polygons in the largest drawing. Hence, the matching can be achieved in polynomial time, which tackles the com-putational barrier. We stand apart from the prior approaches by grouping low-level primitives into polygons and then con-sidering their matching at this high-level point of view. Once polygons are mapped, it is interesting to take a closer look at a lower level by checking out segment layouts within the mapped polygons. This presents some advantages as elements are locally affected to define a local dissimilarity measure, which is visually interesting; it makes easier the spotting of mis-detected areas. A complete data flow pro-cess for polygonized document evaluation is proposed. Our contribution in this domain is two-fold. First, we compare a ground-truth document and a computer-generated document thanks to an optimal framework that proceeds with object mapping. Finally, another operator provides estimates on the relation between the segments within two mapped polygons in terms of edit operations, by means of a cycle graph match-ing. Figure 1 depicts an overview of our methodology. 1.4 Organization The organization of the paper is as follows: Sect. 2.1 describes theoretically and in terms of algorithm the polygon mapping method, Sect. 2.2 explains the cycle graph matching process in order to assess the quality of the polygonal approx-imation. Section 2.3 sets forth the type of errors that are likely to occur in object retrieval systems. Section 3 describes the experimental protocol, this section also explains how to inter-pret our new set of indices in a application to cadastral map evaluation. A summary is included in Sect. 4 , followed by discussions and concluding remarks. 2 A set of indices for polygonization evaluation In this section, we define the two criteria involved in our proposal for a performance evaluation tool dedicated to polygonization. In the first part, a polygon assignment method is described. It aims at taking into account shape distortions caused by retrieval systems. Secondly, a matched edit distance is defined. This measure represents the varia-tions introduced when a given system approximates digital curves. It is a synthesis on vectorization precision. Finally, mis-detection or over-detection errors due to raster to poly-gon conversion are introduced in a third part. This breakdown leads to the definition of specific notations and error catego-rizations. 2.1 Polygon mapping using the Hungarian method Once polygons are located within the vectorized document, it can be seen as a partition of polygons. Comparing two doc-uments ( D 1 , D 2 ) comes down to matching each polygon of D 1 with each polygon of D 2 . This assignment is performed using the Hungarian method, which is formally described in the next part. 2.1.1 Algorithmic of the Hungarian method Our approach for vectorized document comparison is based on the assignment problem. The assignment problem con-siders the task of finding an optimal assignment of the ele-mentsofaset D 1 to the elements of a set D 2 . Without loss of generality, we assume that | D 1 | X | D 2 | . The complete bipartite graph G pm = D 1  X  D 2  X  , D 1  X  ( D 2  X  ) , where represents empty dummy polygons, is called the poly-gon matching of D 1 and D 2 . A polygon matching between D 1 and D 2 is defined as a maximal matching in G pm .We define the matching distance between D 1 and D 2 , denoted by PMD ( D 1 , D 2 ) , as the cost of the minimum-weight polygon matching between D 1 and D 2 with respect to the cost func-tion K . The cost function is especially dedicated to our prob-lemandisfullyexplainedinSect. 2.1.2 .Thisoptimalpolygon assignment induces a univalent vertex mapping between D 1 and D 2 , such that the function PMD : D 1  X  ( D 2  X  )  X  R 0 minimizes the cost of polygon matching. If the numbers of polygons are not equal in both documents, then empty  X  X ummy X  polygons are added until equality | D 1 |=| D 2 | is reached. The cost to match an empty  X  X ummy X  polygon is equal to the cost of inserting a whole unmapped polygon ( K (  X  , P ) ). A shortcoming of the method is the one-to-one mapping aspect of the algorithm, however, this latter is per-formed at a high level of perception where data are less likely to be fragmented. Finally, this disadvantage should not dis-courage the use of the PMD distance considering the impor-tant speed-up it provides while being optimal, deterministic and quite accurate. In addition, unmapped elements are not left behind, they are considered either as  X  X alse alarm X  or as  X  X alse negative X  according to the kind of mistakes they induced (see Sect. 2.3 ). 2.1.2 Cost function for polygon assignments Munkres X  algorithm as introduced in the last section pro-vides us an optimal solution to the assignment problem in O ( n 3 ) time. In its generic form, the assignment problem considers the task of finding an optimal assignment of the elements of a set GT to the elements of a set CG assuming that numerical costs are given for each assignment pair. In fact, a cost function does exist between each pair of poly-gons to express numerically their similarity, in the same way a  X  X ero X  will represent two identical polygons and  X  X ne X  two polygons not sharing any common features. The polygon overlay, inspired by the theory of sets, measures the simi-larity between polygons. When polygons are compared into the same axis system, the overlay takes into account spa-tial adjustment between polygons. The process of overlay-ing polygons shares common points with set theory. Let X  X  assume that A and B are two sets, the intersection can be reformulated through the set theory. I nter secti on , where the result includes all those set parts that occur in A and B. A way to compare them is to find out how A differs from B (see Fig. 2 ): In mathematics, the difference of two sets is the set of elements which are in one of the sets, but not in both. This operation is the set-the-oretic kin of the exclusive disjunction in Boolean logic. The symmetric difference of the sets A and B is com-monly denoted by A B . The symmetric difference is equivalent to the union of both relative complements, that is: A B = ( A \ B )  X  ( B \ A ) and it can also be expressed as the union of the two sets, minus their intersection: A B = ( A  X  B ) \ ( B  X  A ) (1) The symmetric difference is commutative and associative: A B = B A The empty set is neutral, and every set is its own inverse: A  X = A A A = X  The set difference can be expressed as the union of the two sets, minus their intersection and represents the number of shared terms. In order to define measures of dissimilarity between complex objects (sets, polygons,...), a suitable pos-sibility is to settle the measure on the quantity of shared terms. Based on this paradigm, the simplest similarity mea-sure between two complex objects o 1 and o 2 is the matching coefficient mc : mc = Where o 1  X  o 2 denotes the intersection of o 1 , o 2 and o stands for the union between the two objects.

Derived from the Eq. 2 , dissimilarity measures which take into account the maximal common shared terms ( mcs )oftwo sets were put forward: d ( o 1 , o 2 ) = 1  X  Where | o | denotes the size of o .FromtheEq. 2 , the expres-sion o 1  X  o 2 is substituted by the size of the largest object and the intersection of two objects ( o 1  X  o 2 ) is represented by the maximum common subset.

Finally, to obtain a dissimilarity measure between poly-gons, let us define a function K that induces a mapping of P 1 and P 2into R ( K : P 1  X  P 2  X  R ) where P 1 and P 2 are two polygons: K ( P 1 , P 2 ) = 1  X  mcs Where | P | denotes the area of the polygon P and where mcs ( P 1 , P 2 ) is the largest set common to P 1 and P 2, i.e. it cannot be extended to another common set by the addition of any element. Straightforwardly, the largest common subset between P 1 and P 2 can be seen as the area in interception between P 1 and P 2. Consequently, Eq. 4 can be re-written as follows: K ( P 1 , P 2 ) = 1  X  In a general way, the Eq. 3 has been proved to be a metric in [ 26 ] and [ 27 ]. As long as, | P 1  X  P 2 | is a valid maximum common shared terms for polygons, the metric properties hold true for the Eq. 5 .

Now, let us take a closer look at the basic properties of this dissimilarity measure: K ( P 1 , P 2 )&gt; = 0  X  P 1 , P 2(6) K ( P 1 , P 2 ) = 0  X  P 1 = P 2(7) K :[ 0 ; 1 ] (8) 2.1.3 Theoretical discussion on our dissimilarity measure Two questions are addressed in this part, the first one con-cerns the unity of PMD when facing heterogeneous docu-ments (different scales and orientations) and the second point is devoted to the proof of PMD as being a metric. This point is crucial to demonstrate the ability of providing a rank infor-mation, which is representative of the error level of a given document with respect to the entire collection.

The cost function behavior: One condition imposed by the Hungarian method is that the cost function has to be strictly positive or zero, this assumption is respected (see Eq. 6 ). In addition, the normalization between zero and one (see Eqs. 5 , 8 ) confers some interesting aspects to the distance.

Without normalization, the distance is highly dependent on the polygon surfaces. A higher importance would be given tolargepolygonsandtheycouldhighlyimpactthefinalscore, while small polygons would not be treated with significance.
Hence, this measure considers with equity whether the concerned polygons are small or not. It means that no bias will be introduced when facing large polygons. Therefore, a highly over-segmented vectorization with many small areas will be roughly as bad as an under-segmented vectorization with few large polygons. Finally, the normalization leads to a lower and an upper bound (Eq. 8 ) of our distance, which is useful to compare a document collection with different scales. In this way, PMD is dependent on scale, translation and rotation variations. Nevertheless, these are desired prop-erties for a distance, which is meant to represent the exactness between two polygonized drawings.
 The polygon matching distance is a metric (PMD): Proof To show that our measure of similarity between doc-uments is a metric, we have to prove four properties for this similarity measure.  X  PMD ( D 1 , D 2 )  X  0  X  PMD ( D 1 , D 2 ) = PMD ( D 2 , D 1 )  X  PMD ( D 1 , D 3 )  X  PMD ( D 1 , D 2 ) + PMD ( D 2 , D 3  X  PMD ( D 1 , D 2 ) = 0  X  D 1 = D 2 2.2 Matched edit distance for polygon comparison The Hungarian method provides a formal framework to per-form a one-to-one mapping between polygons. Each mapped pair of polygons minimizes its symmetric difference provid-ing a topological information. However, this measure does not take into account the labor work that has to be done to change a polygon from the CG to a correct polygon from the GT. In order to compensate this weakness, we decide to include an additional measure which reveals how many edit operations have to be done to change a polygon into another according to some basic operations. That X  X  why we present an edit distance for polygon comparison. From Definition 2 and Fig. 3 , a clear link exists between a polygon and its representation by a cycle graph. The next part defines a cycle graph edit distance (CGED) for polygon comparison, this latter deals with the graph matching problem applied cycle graph. 2.2.1 A cycle graph matching distance for polygon Visually, two chains of segments are similar if the length attributes and angles between consecutive segments can be aligned. In the literature on polygonal shape recognition, most approaches base the distance definition between two polygonal shapes on length and angle differences. For exam-ple, Arkin et al. used in [ 28 ] the turning function, which gives the angle between the counterclockwise tangent and the x-axis as a function of the arc length. Their results are in accordance with the intuitive notion of shape similarity. More recently, in [ 29 ], Llad X s et al. represented regions by polylines and string matching techniques are used to measure their similarity. The algorithm follows a branch and bound approach driven by the RAG edit operations. This formula-tion allows matching computing under distorted inputs. The algorithm has been used for recognizing symbols in hand-drawn diagrams.

Polygonal shapes require to characterize the segments (their length) but also their relationships by the angle information.

The graph-based representation was preferred to string representation. In fact, the protocol is designed for polygons but may also be extended to other line shapes, for instance this could be made by completing the graph representation to connected vectors instead of searching for cyclic polygons. In this way, the graph-based viewpoint could be the container of a wider range of entities. It leaves the door open to a more global paradigm, the object matching question.

The concept of edit distance has been extended from strings to trees and to graphs [ 30 , 31 ]. Similarly to string edit distance, the key idea of graph edit distance is to define the dissimilarity, or distance, of graphs by the minimum amount of distortion that is needed to transform one graph into another. Compared to other approaches to graph match-ing, graph edit distance is known to be very flexible since it can handle arbitrary graphs and any type of node and edge labels. Furthermore, by defining costs for edit operations, the concept of edit distance can be tailored to specific applica-tions. A standard set of distortion operations is given by inser-tions, deletions, and substitutions of both nodes and edges. We denote the substitution of two nodes u and v by ( u  X  v) the deletion of node u by ( u  X   X ) , and the insertion of node vby ( X   X  v) . For edges, we use a similar notation.
Given two graphs, the source graph G 1 and the target graph G 2 , the idea of graph edit distance is to delete some nodes and edges from G 1 , relabel (substitute) some of the remaining nodes and edges, and insert some nodes and edges in G 2 , such that G 1 is finally transformed into G 2 . A sequence of edit operations e 1 ; ... ; e k that transforms G 1 completely into G 2 is called an edit path between G 1 and G 2 . Obviously, for every pair of graphs ( G 1 ; G 2 exist a number of different edit paths transforming G 1 into G 2 . To find the most suitable edit path, one introduces a cost for each edit operation, measuring the strength of the cor-responding operation. The idea of such a cost function is to define whether or not an edit operation represents a strong modification of the graph. Obviously, the cost function is defined with respect to the underlying node or edge labels. Clearly, between two similar graphs, there should exist an inexpensiveeditpath,representinglowcostoperations,while for dissimilar graphs an edit path with high costs is needed. Consequently, the edit distance of two graphs is defined by the minimum cost edit path between two graphs. The com-putation of the edit distance is carried out by means of a tree search algorithm, which explores the space of all possible mappings of the nodes and edges of the first graph to the nodes and edges of the second graph.
 Definition 3 ( Cycle Graph Matching ) In this work, the prob-lem which is considered concerns the matching of cycle directed labeled graphs. Such graphs can be defined as fol-lows: Let L V and L E denote the set of node and edge labels, respectively. A labeled graph G is a 4-tuple G = (
V , E , X , X ) , where  X  V is the set of nodes,  X  E  X  V  X  V is the set of edges  X   X  : V  X  L V is a function assigning labels to the nodes,  X   X  : E  X  L E is a function assigning labels to the edges.  X  | V |=| E |
Now, let us define a function CGED based on the cycle graph edit distance that induces a mapping of G 1 and G 2 R ( CGED : G 1  X  G 2  X  R ): CGED ( G 1 , G 2 ) = min Where  X ( G 1 , G 2 ) denotes the set of edit paths transforming G 1 into G 2 , and edi t denotes the cost function measuring the strength edi t ( e i ) of edit operation e i . A cycle graph is a cycle which visits each vertex exactly once and also returns to the starting vertex. As a consequence, the set of all edit paths is considerably reduced and the cycle graph matching can be solved in O ( nm log m ) time.
In order to use cycle graph matching for polygon accuracy evaluation, we use an attributed graph representation. Start-ing from a polygonal approximation of the shape, a graph is built. We use the segments as primitives, encoding them with a set of nodes. Each node is labeled with a real num-ber l i , where l i denotes the length of the segment s i edges are built using the following rule: two nodes are linked with a directed and attributed edge if two constitutive seg-ments share a common point. Each edge is labeled with a real number i that denotes the angle between s i and s i  X  1 in the counterclockwise direction. We can appreciate an example of how these attributes are computed for a sample shape in Fig. 3 .

Let A and B be two chains of adjacent segments, repre-sented as cycle graphs, with total lengths | A | = n and | and with respectively attributed cycle graph representations: G and, G
The cost functions for attributed cycle graph matching are reported in Table 1 . 2.2.2 Interpretation of the edit operations There are the proposed cost functions inspired by the ones designed by Tsay and Tsai in [ 32 ] where they use string matching for shape recognition.

The operation attributes decrease the edit costs for primi-tives undergoing noisy transformations, as the inherent seg-ment fragmentation from the raster to vector process. And it aims to compare polygons with different number of segments making the system tolerant to segment cardinality.
Furthermore, our edition functions can describe real trans-formations applied to polygons. When editing a vectoriza-tion, basic operations are: Remove, Add or Move a segment; a visual illustration of theses operations is given in Fig. 4 . Through the linear combination of the cost functions, it is possibletorecreatetheusagesofapersonmodifyingavector-ization, and the definitions below present the combinations to obtain different polygon transformations. Conceptually, we are here very close to the ideas proposed by Chhabra [ 5 ].
Reasoning in a segment deletion context, we proceed by two simple case analysis to detail the elaboration of the gen-erated operation sequence and then to describe the impact of the involved cost functions.
 Case 1 Edit operation for segment deletion At first, deleting a segment comes with the deletions of a node and an edge into the graph representation.
 We conclude that deletions =  X (( l A i )  X   X ) +  X  (( A i )  X   X ) In addition, the deletions of a node and an edge in a cycle graph creates an orphan edge that must be reconnected. To take into account this modification, an edge label substitution is operated.

Consequently we conclude: substitution =  X  (( A i )  X  ( B j ))
Finally, the sequence of operations is:  X ( s i  X   X ) = deletions + substitution Plainly, we change the formal expressions by their corre-sponding costs to obtain the following formula:  X ( s i  X   X ) = Case 2 Cost function for a segment deletion A segment deletion is operated to remove a segment errone-ously inserted by a given R2V system. We can show that the cost function associated with the edit operation  X ( s i  X   X ) is representative of the mistake importance. We proceed by a simple case analysis. The global cost function is impacted by two parameters, the size and the misalignment of the removed segment.  X ( s i  X   X ) expresses two deformations: (i) How large was the segment to be removed and (ii) How misa-ligned was the segment to be removed compared with the adjacency vectors. In this way, larger is the segment to be removed and bigger is the mistake made by the R2V. The cost function responds correctly, larger is the segment and bigger is the quantity  X ( s i  X   X ) by the increase of the l A i | The second kind of errors generated by R2V systems is the segment misalignment. Larger is the angle variations of the removed segment with the adjacency segments in the poly-gon and bigger is the committed error. Accordingly, larger is the angle variations of the removed segment and bigger is the quantity  X ( s i  X   X ) by the increase of A i 360 + combination.

Therefore, we provide the definitions for segment dele-tion, addition and move.
 Definition 4 Segment deletion transformation  X ( s i  X   X ) =  X (( l A i )  X   X ) +  X  (( A i )  X   X )  X ( s i  X   X ) = Definition 5 Segment addition transformation  X ( X   X  s  X ( X   X  s Definition 6 Segment move transformation  X ( s i )  X  ( s j ) =  X (( l A i )  X  ( l B j )) +  X  (( A i )  X  ( B  X ( s i )  X  ( s j ) = 2.2.3 Matched edit distance To complete the process, the cycle graph matching distance ( CGED ) has to be performed on every pair of mapped poly-gons found by the Hungarian methods when it is based on the symmetric difference. Note that if one polygon is associ-ated with an empty dummy item then the cycle graph match-ing distance will be only composed of addition operations. The matched edit distance(MED) is then composed of the sum of all CGED ( G 1 , G 2 ) computed on every pair of graphs extracted from the polygons.
 MED ( D 1 , D 2 ) = 2.3 Types of error and notations Here, we sum up a set of two criteria, which will help us to evaluate a given raster to vector conversion. Each measure is a viewpoint on the vectorization process. However, every cri-terion can still be divided into two categories according to the nature of the error it expresses. Hence, the next part defines the different kinds of errors that can occur when dealing with object retrieval systems. 2.3.1 Types of error  X  Type I error, also known as an  X  X rror of the first kind X ,  X  Type II error, also known as an  X  X rror of the second kind X , 2.3.2 Notations For the understanding of these tests, we first introduce nota-tions that will make the reading much simpler. A dissimilarity measure between vectorized documents is a function: d : X  X  X  X  R where X is a vectorized document. We report in Table 2 ,the notations derived from this general form.
 mapped polygons. This is a synonym of accuracy, it denotes how well suited is the detected polygon from the D CG . X takes the stock of the over-detections issued from the raster to vector conversion step. On the other hand, X fn represents the mis-detections, it occurs when the software used to vectorize has a strict policy of rejection which leads to an under-detec-tion of objects. For clarity reasons, when no precision is spec-ified, X refers to X all . Finally, a desirable information is the number of false alarms, false negative and true positive poly-gons retrieved by the retro-conversion system. These val-ues are normalized as follows to obtain a comparable rate between documents.  X   X  3 Experiments This section is devoted to the experimental evaluation of the proposed approach. First, we describe databases that are used to benchmark our measures. Then, the protocol of our exper-iments is defined by enumerating the kind of assessments we performed. The two first tests are dedicated to graphi-cal symbols from GREC contests. On this basis, we aim at illustrating the ability of polygon matching distance (PMD) and matched edit distance (MED) of being representative of polygon deformations (shape variation and polygonal approximation modification, respectively). The last evalu-ation concerns the cadastral map subject, we show results on a large collection of maps. We provide guidelines to under-stand the meaning of our set of indices. In this pedagogic objective, a visualization of detection errors is proposed.
In this practical work, methods were implemented in Java 1.5 and run on a 2.14GHz computer with 2G RAM. Both dat-abases and performance evaluation tools are freely available on this web site: http://alpage-l3i.univ-lr.fr/ Both datasets and the experimental protocol are first described before investigating and discussing the merits of the proposed approach. 3.1 Databases in use In recent years the subject of performance evaluation has gained popularity in pattern recognition and machine learn-ing. In the graphics recognition community, a huge amount of efforts was made to elaborate standard and publicly available data sets. Especially, E. Valveny [ 33 , 34 ] and M. Delalandre [ 35 ] published online symbol datasets for a symbol classi-fication purpose. In this section, we describe two databases derived from [ 34 ] and [ 35 ] and we also present a cadastral map collection. The content of each database is summarized in Tables 3 , 4 .

Base A: Shape distortion. The paper presented in [ 35 ] gave birth to a publicly available database of symbols. 1 this setting, we removed all polygon-less symbols to fit our purpose which was to evaluate polygon detection methods. Hence, we selected 70 symbols from the GREC X 05 contest [ 36 ] and a sample is presented in Fig. 5 .

On perfect symbols, a vectorial noise is applied to gener-ate a collection of degraded elements. We could not afford to use real data because of the difficulty of collecting images with all kinds of transformations and noise. Besides, it is not easy to quantify the degree of noise in a real image. Then, it is not possible to define a ranking of difficulty of images according to the degree of noise. In our experiments, we have re-used methods for the generation of shape transformation (based on active shape models [ 33 ]).

Vectorial Distortion: The goal of vectorial distortion is to deform the ideal shape of a symbol in order to simulate the shape variability produced by hand-drawing. The method for the generation of vectorial distortions of a symbol is based on the Active Shape Models [ 37 ]. This model aims to build a model of the shape, statistically capturing the variability of a set of annotated training samples. In order to be able to apply this method, we need to generate a good set of training samples. This is not a straightforward task due to the statis-tical nature of the method. The number of samples must be high enough, and the samples must reflect the usual kind of variations produced by hand-drawing. However, it is difficult to have a great number of hand-drawn samples of each sym-bol. To be really significant, these samples should be drawn by many different people. Thus, the decision of generating automatically the set of samples has arisen. Based on the generation of deformed samples through the random modi-fication of a different number of vertices of the symbol each time [ 38 ].

Each sample is represented using the model described in [ 39 ], which permits easy generation of deformed shapes. Each symbol is described as a set of straight lines, and each line is defined by four parameters: coordinates of mid-point, orientation and length. Thus, each deformed sample can be seen as a point x i in a 4 n -dimensional space, where n is the number of lines of the symbol. Then, principal compo-nent analysis (PCA) can be used to capture the variability in the sample set. Given a set of samples of a symbol, we can compute the mean x and the covariance matrix S .Themain modes of variation are described by the first eigenvectors p of the covariance matrix S . The variance explained by each eigenvector is equal to its corresponding eigenvalue. Thus, each shape in the training set can be approximated using the mean shape and a weighted sum of the eigenvectors: x = x + Pb where P = p 1 ,..., p m is the matrix of the first m eigenvec-tors and b is a vector of weights. This way, new images of a symbol can be generated by randomly selecting a vector of weights b . Increasing values of b i will result in increasing levels of distortion (see Fig. 6 ).

The model of vectorial distortion described in the former paragraph has been applied with four increasing levels of dis-tortion to generate 280 (70*4) images of symbols. The vari-ance was tuned from 0.00025 to 0.00100 by step of 0.00025. This way of changing the variance is coherent with the pro-tocol presented in [ 35 ]. The entire database is then made up of 350 elements, 280 degraded symbols and 70 models. The shape distortion generator, 3gT system was provided by M. Delalandre. 2 3gT  X  X eneration of graphical ground Truth X  isasystemtogeneraterandomgraphicaldocuments(exported into SVG) of low-level primitives (vectors, circles, ...) with their corresponding ground-truth. Base A is a reliable source to evaluate the shape distortion sensitivity of our polygon location measure. Numerical details concerning this data set are presented in Table 4 .

Base B: Binary degradation. First of all, we decided to use the data set provided by the GREC X 03 contest. Mainly two application domains, architecture and electronics, were adopted as a representative sample of a wide range of shapes. GREC X 03 database is originally constituted of 59 symbols from which we removed symbols without polygons. This pruning step led us to a database of 53 symbols. From the 9 noise levels available, we only focused on the 6 first levels. Consequently, database B is made up of 318 (6*53) binary degraded symbols plus 53 ideal models, i.e. a total of 371 polygonized elements according to the process explained in the next paragraph.

Binary Degradation: Kanungo et al. have proposed a method to introduce some noise on bitmap images [ 40 ]. The purpose of this method is to modelize noises obtained by operations like printing, photocopying, or scanning pro-cesses. The problem is approached from a statistical point of view. The core principle of this method is to flip black and white pixels by considering, for each candidate pixel, the distance between it and the closest inverse region. The degradation method is validated using a statistical method-ology. Its flexibility in the choice of the parameters requires some adaptations. Indeed, a large set of degradations can be obtained. Themethoditself accepts noless than6parameters, allowing to tune the strength of white and black noise, the size of the influence area of these noises, a global noise (which does not depend of the presence of white/black pixels), and a post-processing closing based on well-known morphological operators. Of course, these 6 parameters may generate a large number of combinations, and thus, of models of degradation. So, if the core method used for the degradation tests is formal and validated for its correctness, the determination of the set of parameters used for the contest is more empirical. This framework was applied to the organization of the GREC X 03 contest on symbol recognition. In [ 34 ], authors attempted to reproduce a set of degradations representing some realistic artifacts (to simulate noise produced when printing, photo-copying and scanning). Six levels of degradation (see Fig. 7 ) were determined by [ 34 ]. They took care to represent some standard noises: local, global troubles.
 Binarydegradationimpactsonpolygonalapproximations: The higher is the noise level the higher are the distortions on the polygonal approximation. The noise level has a direct influence on the vectorization algorithm. In this experiment, we used a standard data flow process to polygonize the symbols: (i) Cleaning; 3 (ii) Skeletonization; (iii) Polygonal approximation and (iv) Polygonizer. Arbitrarily, we adopted the well-known di Baja X  X  skeletonizer [ 41 ] and the Wall and Danielsson X  X  vectorization [ 42 ]. Then, a polygonizer was applied to transform the set of segments into polygons. These steps are summed up in Fig. 8 . A piece of polygon is zoomed-in to show the perturbation applied on the polygo-nal approximation when noise increases. The method only requires a single threshold i.e., the ratio between the alge-braic surface and the length of the segments which makes this linear time algorithm fast and efficient. This parameter is set to 60 pixels for all the experiments. We did not want to assess the impact of the approximation threshold but rather the impact of noise on polygonization when the threshold is frozen.
 More information concerning those data is detailed in Table 4 .

Base C: Cadastral map collection. In the context of a project called ALPAGE, a closer look is given to ancient French cadastral maps related to the Parisian urban space during the 19th century (Fig. 9 ). Hence, the map collection is made up of 1100 images coming from the digitalization of Atlas books. On each map, domain-objects called Par-cels are drawn by using color to distinguish between them. From a computer science perspective, the challenge consists in the extraction of information from color documents with the aim of providing a vector layer to be inserted in a GIS (Geographical Information System).

Automatic polygon detection: In this project, a bottom-up strategy is adopted. In bottom-up strategies, algorithms are performed in a fixed sequence, usually starting low-level analysis of the gray level or black and white image, from which primitives are extracted. From this starting point, the four stages for extracting parcels from a cadastral map are put forward. (i) At first, a color gradient is performed to locate objects within the image. (ii) Then, a text/graphic segmen-tation is run on the gradient image to preserve only graphic elements [ 43 , 44 ]. (iii) Thirdly, a digital curve approximation is performed to transform pixels into vectors [ 45 ]. (iv) finally, vectors are gathered to form polygons using a polygonizer algorithm [ 46 ]. The parcel extractor is evaluated using our set of indices.

Ground-Truthing: With the help of experts in several fields of sciences such as Historians, Archaeologists and Geogra-phers, a campaign of handmade vectorization was carried out. This work was intensively labor consuming yet nec-essary. It was the only way to give us the opportunity to fully evaluate the accuracy of our work. The main goal was to build a reference database to investigate the merit of our parcelretrievalscheme.Manually,100rastermapswerecare-fully and precisely vectorized to constitute a reliable collec-tion of 2335 parcels of lands. These units are encoded as polygons according to the Definition 2 . Therefore, a real link does exist between a parcel and its polygon repre-sentation. This labor intensive procedure represents a real reference to measure up the accuracy and the validity of our automatic vectorization [ 44 , 47 ]. The content of the database is summarized in Table 3 . In average, there are 25 parcels per map and this accounts for about 75 line segments per parcel. The ground-truth was manually made according to simple rules. Each parcel had to be described by a poly-gon. The median line was favored in the line tracking phase. The precision question was solved by imposing to fit at best the parcel contour and consequently, in each polygon, a ver-tex corresponds to a significant direction change. For each image of document, there exists exactly one pair of vec-torized maps, one map called ground-truth and one map named computer generated, respectively &lt; D GT ; D CG &gt; An example of a pair of vectorization to be matched is dis-played in Fig. 10 . Further details on this data set are pre-sented in Table 3 . Note that this database is made up of real data.

A synthesis about this database is reported in Table 3 while the content is publicly available at http://alpage-l3i.univ-lr.fr/PE/alpagedb.zip . 3.2 Protocol Three different ways of evaluating our indices are proposed.
Polygon Matching Distance evaluation: To assess the abil-ity of the polygon mapping distance to increase when doc-uments get badly reconstituted, we focus on Base A. Base A is representative of different shape distortions and conse-quently,polygonshapesareaffectedbythisnoise.OnBaseA, we performed a ranking test using PMD as a dissimilarity measure. A visual explanation of how ranks are obtained is brought to view in Fig. 11 . Then, ranks are compared thanks to a statistical method called a Kendall X  X  test and defined as follows: Definition 7 Kendall X  X  test.

We assess the correlation concerning the responses to k -NN queries when using PMD as dissimilarity measures. The setting is the following: in a given Base X , we select a number N of symbols, that are used to query by similarity the rest of the dataset. Top k responses to each query obtained using PMD are compared with the ground-truth. The ground-truth ranks are obtained thanks to the control of noise level. The similarity of the PMD ranks and the ground-truth ranks is measured using Kendall correlation coefficient. We con-sider a null hypothesis of independence( H 0) between the two responses and then, we compute, by means of a two-sided statistical hypothesis test, the probability (p-value) of get-ting a value of the statistic as extreme or more extreme than observed by chance alone, if H 0 is true. The Kendall X  X  rank correlation measures the strength of monotonic association between the vectors x and y ( x and y may represent ranks or ordered categorical variables). Kendall X  X  rank correlation coefficient  X  may be expressed as  X  = S Where, S = And, D = k
Polygonal approximation sensitivity: In this second exper-iment, we aim at assessing the capacity of the matched edit distance (MED) to increase when the polygonal approxima-tion gets badly reconstituted by the retrieval systems. Base B is involved in this test. Base B is a binary degraded set of symbols and the higher is the noise level on symbols and the more disturbed is the polygonal approximation from the original one. In this way, we control the distortion level of the digital curve approximation and consequently, we obtain a ground-truth order from an ideal symbol by controlling the noise level, in Fig. 8 . Finally, the ranks returned by MED and the ground-truth are compared according to the Kendall X  X  test described in Definition 7 . using a Kendall test.
Application to the evaluation of parcel detection: Our last experiments are based on real data that composed Base C. At first, we performed the PMD distance on a single pair of given maps and this in order to highlight dissimilarities issued from the raster to vector conversion. Then, an exper-iment was dedicated to the evaluation of the entire collec-tion of cadastral maps. We provided an interpretation of the results through the viewpoints of our set of indices. Finally, a statistical framework was described to figure out relations between the different indices. 3.3 Polygon matching distance evaluation Using N = 70 , k = 4 equal to the number of noise levels available in Base A, we present in Fig. 12 and Table 5 ,the results obtained in terms of  X  values. From the 70 tests, only 9 have a p-value greater than 0.05, so we can say that the hypothesis H 0 of independence can be rejected in 87.4% cases, with a risk of 5%. The observed correlation between the responses to k -NN queries when using the ground-truth and polygon matching distance ( PMD ) tends to reveal a rank relation between both (median value of  X  = 0 . 800). By stress testing a given system, we aim at demonstrating that our protocol can reveal strengths and weaknesses of a system. The PMD index increases when image degradation increases. 3.4 Polygonal approximation sensitivity Using N = 53 , k = 6 equal to the number of noise lev-els in Base A, we present in Fig. 13 and Table 6 , the results obtainedintermsof  X  values.Fromtheseresults,werejectthe null hypothesis of mutual independence between MED and the ground-truth rankings for the students. With a two-sided test we are considering the possibility of concordance or discordance (akin to positive or negative correlation). A one-sided test would have been restricted to either discordance or concordance, which would be an unusual assumption. In our experiment, we can conclude that there is a statistically sig-nificant lack of independence between MED and the ground-truth rankings of the symbols by MED. MED tended to rank symbols with apparently greater noise as being farther from the ideal symbol than those with apparently less noise and vice versa.
 3.5 Application to the evaluation of parcel detection A visual dissimilarity measure of local anomalies: In this part, we focus on comparing maps two by two. This dif-ficult task requires a good observation of the local differ-ences between the compared documents. On a randomly picked pair &lt; D GT ; D CG &gt; , we computed the polygon matching distance ( PMD all ). A bi-dimensional representa-tion of the costs to assign each element from the D CG to the D GT is displayed in Fig. 14 , whereas values of the dif-ferent measures are reported in Table 7 . Figure 14 provides a visual understanding of where the anomalies are located. First, it facilitates the spotting of errors and other aberra-tions and especially, this framework can help domain experts understanding the limits and advantages of a vectorization software. Figure 14 is worth a thousand words, it makes eas-ier the communication and the implementation of mutual-ized working tools for both Information and Communication Technologies (ICT) -Humanities and Social Sciences (HSS) communities.

To conclude, it can help users to spot where the mistakes are located and so save them a lot of time (time saver). It can help software designers to locate easily where the R2V conversion failed and consequently, this local visualization at a polygon level facilitates the categorization of detection errors.

Evaluation of a collection of maps: From the data set of vectorized maps, we attempted to evaluate the quality of the overall conversion process through the viewpoints offered by the two main criteria that we have described, PMD , MED .
Over the map collection, we observed in Fig. 15 an over-detection tendency. In average, 31% of the retrieved polygons are misleading. 71% of these wrong polygons are concerned by an over-detection behavior (  X  fp = 0 . 22).

Now, we want to figure out the nature of the mistakes, if these over-detected polygons are just some tiny polygons due to noise in the raster or if they represent a major infor-mation altered during the process of conversion. To this aim, we pay attention to Fig. 16 . Figure 16 shows that only 36% of the overall cost PMD all is due to the well-detected poly-gons, hence, most of the information is accurately retrieved from the rasters and the retrieved polygons do fit precisely the ground-truth. On the opposite 64% of the mistakes are attrib-utable to the wrongly detected polygons. Figure 16 strength-ens theideathat anomalies arecausedbytheover-acceptation policy of the automatic application.

In another step, we aim at assessing how much manual work has to be made to correct the automatically vectorized polygons. A fact observed from Fig. 17 is that 54% of the MED all mistakes are caused by the operations to be made when correcting the polygons MED tp . A non-negligible part of the errors are caused by the corrections to be made to fit in the ground-truth. An explanation could be a fragmenta-tion phenomenon; many noisy strokes are broken into small pieces during the polygonal approximation process.
The rest of the errors, that is to say the MED md values, is mainly due to an intensive use of the deletion operator in order to remove the over-detected polygons.
 Finally, based on a common work with the historians Helene Noizet and Laurent Costa, 4 an algorithm with a combined index ( PMD all + MED all )of0.70orlessmay be considered good with respect to human vision evaluation. However, more work should make use of this protocol on a series of algorithms and degraded drawings to obtain an objective assessment on commonly accepted criteria.
Inter-indicescorrelation:Acorrelationmatrixisbuiltfrom the data series of indices (illustrated in Fig. 18 c), the Pearson correlation is 1 in the case of an increasing linear relation-ship,  X  1 in the case of a decreasing linear relationship, and some value between  X  1 and 1 in all other cases, indicat-ing the degree of linear dependence between the variables. The closer the coefficient is to either  X  1 or 1, the stronger the correlation between the variables. This matrix aims to com-pare the different quality measures between them. A matrix is not expressive enough, so, a 256 shades of gray image is generated to express its substantial meaning in a 2D repre-sentation, called image of correlations (Fig. 18 b). In addition, the matrix of scatter plots between the different measures of quality is given (Fig. 18 a). From these data representations, a straightforward remark deals with the proportional behav-iorofthe  X  tp and  X  md , which are closely coupled and share the same information. On the other hand, there is no evident relation between MED and the  X  measures, the Pearson cor-relation coefficient between these two series is not indicative enough, nevertheless, the coefficient is low enough (0.60) to indicate no significant redundancy of information. Finally, a clear tendency appears between PMD and MED , it reveals a low correlation (0.24) between PMD and MED . A situa-tion of independence between the two series can be accepted. These variables really express two different kinds of infor-mation. They represent original viewpoints on the underlying problem. 4 Conclusion and perspectives In this paper, we defined a protocol for performance evalua-tion of polygon detection algorithms. A discussion between the proposed protocol and the literature is also presented. As a consequence, our protocol is positioned as an extension of prior works, an extension at polygon level. In this way, it is closer to the semantic level and closer to objects han-dled by humans. Former benchmarks only include synthetic images with image degradation but we completed these arti-ficial samples by real images with manually created ground-truth. Gathering real data to test and comparing graphics recognition systems is very time-consuming that is why we propose our data set to the community.

Our contribution is two-fold, an object mapping algorithm to roughly locate errors within the drawing and then a cycle graph matching distance that depicts the accuracy of the polygonal approximation. Both were theoretically defined and adapted to the performance evaluation of polygonized documents. Especially, cost functions were reconsidered, using a set distance for the polygon matching distance (PMD) and defining particular edit costs for the graph matching method.

The proposed protocol is objective and comprehensive, both detection and false alarm rates are considered. By stress testing a given system, we demonstrated that our protocol can reveal strengths and weaknesses of a system. The behavior of our set of indices was analyzed when increasing image degradation.

The results presented in Figs. 16 and 17 indicate that the proposed protocol reflects polygon detection and approximation performance accurately. In Fig. 18 , the sta-tistical tests demonstrated that the two proposed measures offer different kinds of information.

We have also confronted our measures of quality to a human-based evaluation. However, more work should be done in this respect to obtain an objective assessment on commonly accepted criteria.

The protocol is designed for polygons but may also be extended to other line shapes by completing the graph repre-sentation to connected vectors instead of searching for cyclic polygons. In this context, the PMD would not have to be mod-ified at all. The MED which is representative of the manual effort to be made to correct mistakes engendered by a R2V system is envisaged through the graph matching question in terms of basic edit operations (addition, deletion, substitu-tion). The graph formalism confers to the approach a more generic nature and opens the way to future works on more complex objects. This graph-based viewpoint could be the container of a wider range of entities. Instead of focusing on polygon items, a given element could be constituted of all connected segments to form a more complex structure than a polygon while the entire principle would remain unchanged. The graph representation is an open way to a more global paradigm, the object matching question. This could change the scope of our performance evaluation tool to the direction of object spotting.
 References
 ORIGINAL PAPER R. Raveaux  X  J. C. Burie  X  J. M. Ogier Abstract This paper presents a benchmark for evaluating the raster to vector conversion systems. The benchmark is designed for evaluating the performance of graphics recog-nition systems on images that contain polygons (solid) within the images. Our contribution is two-fold, an object mapping algorithm to spatially locate errors within the drawing and then a cycle graph matching distance that indicates the accu-racy of the polygonal approximation. The performance incor-porates many aspects and factors based on uniform units while the method remains non-rigid (thresholdless). This benchmark gives a scientific comparison at polygon level of coherency and uses practical performance evaluation meth-ods that can be applied to complete polygonization systems. A system dedicated to cadastral map vectorization was eval-uated under this benchmark and its performance results are presented in this paper. By stress testing a given system, we demonstrate that our protocol can reveal strengths and weak-nesses of a system. The behavior of our set of indices was analyzed when increasing image degradation. We hope that this benchmark will help assessing the state of the art in graphics recognition and current vectorization technologies. Keywords Vectorized object comparison  X  Polygon assignment  X  Polygonization accuracy  X  Performance evaluation  X  Polygon detection quality  X  Graphics recognition  X  Machine drawing understanding system 1 Introduction In this paper, the question of performance evaluation is dis-cussed. A bird X  X -eye view methodology is adopted starting from the generic idea about evaluation of vectorization and the main principles of the proposed approach. Then, the introduction turns into a deeper discussion explaining in details both related works and the description of the proposed method. 1.1 Bird X  X -eye view of vectorization evaluation Driven by the need to convert a large number of hard copy engineering drawings into CAD files, raster to vector con-version has been a field of intense research for the last four decades. In addition to research prototypes in several aca-demic and industrial research centers, several commercial software products are currently available to assist users in converting raster images to vector (CAD) files. However, the process of selecting the right software for a given vector-ization task is still a difficult one. Although trade magazines have published surveys of the functionality and ease of use of vectorization products [ 1 ], a scientific, well designed, com-parison of the auto-vectorization capability of the products was still required.

Responding to this need, the International Association for Pattern Recognition X  X  technical committee on graphics recognition (IAPR TC10) organized the series of graphics recognition contests. The first contest, held at the GREC X 95 workshop in University Park, PA, focused on dashed-line detection [ 2  X  4 ]. The second contest, held at the GREC X 97 workshop in Nancy, France, attempted to evaluate complete (automatic) raster to vector conversion systems [ 5  X  7 ]. The third contest, held off-line associated with the GREC X 99 workshop in Jaipur, India, also aimed to evaluate complete (automatic) raster to vector conversion systems. These con-tests tested the abilities of participating algorithms / systems to detect segments and arcs from raster images. They adopted a set of performance metrics based on the published line detection performance evaluation protocol [ 8 ] to evaluate and compare the participating algorithms / systems online at the workshop site with test data of different quality and complexity. Pre-contest training images and the performance evaluation software were provided before the contests, so prospective participants could try their systems and improve them for optimal performance. Test images could be synthe-sized and/or real scanned images.

Performance evaluation and benchmarking have been gaining acceptance in all areas of computer vision and so in the graphics recognition field of science.

Early work on this topic was carried out to evaluate per-formance of thinning algorithms. Haralick [ 9 ] was the first to propose a general approach for performance evaluation of image analysis, with thinning taken as a case in point. Evaluation and comparison of thinning algorithms have also been performed by [ 10  X  12 ] and [ 13 ]. Some of these evalua-tion and comparison works were carried out from the view-point of OCR, while the work of [ 12 ] is domain independent. Although thinning may also be employed as preprocessing of line detection, the latter has different characteristics and therefore requires a different evaluation protocol.
Vectorization and other line detection techniques have been developed to convert images of line drawings in var-ious domains from pixels to vector form (e.g., [ 14  X  16 ]), and a number of methods and systems have been pro-posed and implemented (e.g., [ 17  X  20 ]). Objective evalu-ations and quantitative comparisons among the different shape detection algorithms are available thanks to protocols issued from GREC contests [ 21  X  23 ] that provide quantitative measurements.

Performance evaluation of vectorization and line detec-tion has been reported by [ 3 , 8 , 24 ] and [ 5 ]. Kong et al. [ 3 ] propose a quantitative method for evaluating the recognition of dashed lines. Hori and Doermann [ 24 ] propose a quan-titative performance measurement methodology for task-specific raster to vector conversion. Wenyin and Dori [ 8 ] present a protocol for evaluating the recognition of straight and circular lines. Phillips and Chhabra [ 5 ] define a method-ology for evaluating graphics recognition systems operating on images that contain various objects such as straight lines and text blocks. All of these methods are limited in their applicability and are discussed in the next subsection.
In this article, we present a benchmark designed for eval-uating the performance of graphics recognition systems on images that contain occlusions within the images. Most engi-neering documents could be approximated by polygons and our study focuses on these particular primitives. For instance, parcels into a cadastral map are well modeled by polygons. Accurate and efficient vectorization of line drawings is essen-tial for any higher level processing in document analysis and recognition systems. In spite of the prevalence of vectoriza-tion methods, no standard for their performance evaluation protocol exists at a polygon level. All prior works focused on a lower level of consistency (arcs and segments). We propose a protocol for evaluating polygon extraction to help compare, select, improve, and even design object detection algorithms to be incorporated into drawing recognition and understand-ing systems. The protocol can be seen as an extension to poly-gon level of related approaches by proposing an evaluation, which is closer to the user requirements (i.e. at a semantic level). This new viewpoint on the problem involves two local dissimilarity measures for estimating polygon detection and approximation quality. 1.2 Related work Kongetal.[ 3 ]havedevelopedaprotocolandasystemforsys-tematicallyevaluatingtheperformanceoflinedetectionalgo-rithms, mainly for dashed-line detection algorithms. They define the overlap criteria of the match between a ground-truth and a detected line based on the angle and the distance between them, and the partial overlap is also considered. They do not allow for fragmentation of detected lines. They use several arbitrary and rigid thresholds, for example, the angle should be less than 3  X  and the distance between two lines less than 5 cells.

Hori and Doermann [ 24 ] instantiate and extend Haralick X  X  framework for performance characterization in image anal-ysis [ 9 ], in an application-dependent manner, for measuring the performance of raster to vector conversion algorithms. They provide a set of metrics (evaluation contents), which is specifically geared to vectorization of mechanical engineer-ing drawings. The  X  X pplications X  addressed in the work are thinning, medial line finding, and line fitting all low-level techniques that do not completely constitute vectorization. It is hard to extend the work to evaluate a complete vector-ization system. Hori and Doermann X  X  protocol does not dis-tinguish between detection rate and false alarm rate. It does not include an overall evaluation metric. It does not allow for fragmentation of detected lines. The metrics for line evalu-ation are given in several non-uniform units. It uses length ratio, deviation, and count ratio to evaluate the line length detection, line location detection, and line quantity detection, respectively. There is lack of an overall evaluation metric, which provides an overall combined performance evaluation of the algorithm under consideration.

WenyinandDori[ 8 ]proposeperformanceevaluationindi-ces for straight and circular line detection. Detection and false alarm rates are defined at both the pixel level and the vector level. Use of pixel level performance indices (mea-sures of shape preservation) is not completely appropriate for real images that contain severe distortions such as warp-ing and/or other defects introduced in the hard copy drawing and/ordefectsgeneratedbythescanning/imagingsystem.On such images, attempts to obtain a high pixel recovery index would unnecessarily require the detected vectors to be true to the distorted shape of the imaged lines thereby making the detected lines fragmented. For such images, the pixel recov-ery index needs to be assigned less weight than the vector recovery index. However, there is no way to predetermine the right relative weights for the pixel and vector recovery indices.

Phillips and Chhabra [ 5 ] present the task of evaluation from the opposite angle. They do not look at the complexity of the entities to be recognized. Instead, in their view, the true measure of performance has to be goal directed. The goal of line drawing recognition is to convert a paper copy or a raster image of a line drawing into a useful form (such as a vector CAD file). How well a graphics recognition system works should be measured by how much manual effort is required to correct the mistakes made by the system, not by how well it recognizes difficult shapes. The goal of the evaluation is to measure the cost of post-processing operations that are nec-essary to correct the vectorization mistakes. EditCost is the cost estimate for human post-editing effort to clean up the recognition result.

Based on this synthesis of performance evaluation sys-tems, one can observe that most of these methods remain at a very low level of analysis of the information (vector level), while user requirements often concern high-level analysis. From the related work which focuses on low-level primitives (segments, arcs), we extend the global concept of perfor-mance evaluation of vectorized documents to polygon level. Herein, we present our work, a recovery index which com-bines a local overlapping metric at polygon level when data are closer to the semantic level and a matching distance for evaluating the polygonal approximation correctness in terms of edit operations. 1.3 Our approach Wenyin and Dori [ 8 ] and [ 5 ] are well-suited tools to tackle the performance evaluation problem of vectorized documents. However, to be more realistic and closer to objects handled by humans, [ 8 ] and [ 5 ] also underlined the need to con-sider more complex structures or domain-specific objects into the assessment process. For instance, in [ 5 ], Dr. Chhabra reported as a shortcoming that  X  X he detection of polylines, polygons, objects, symbols, etc. was not tested X . A step in this direction is to address the problem under the prism of grouping of vectors. Unfortunately, prior algorithms cannot be easily modified to reach higher level objects since no match was attempted between solid entities. In fact, if in the case of low-level primitives the matching can be easily reduced to an overlapping criterion; for more complex ele-ments the question is more ambitious. Due to fragmentation phenomena introduced by R2V tools, an entity of the ground-truth can possibly be related to many elements of the auto-vectorized version of the document. Solving this ambiguity requires complex matching algorithms that are not provided by prior works because the underlying problem does not exist at a low level of analysis. Rather than consider polygon frag-mentation and combination as being simply wrong, and only allow the best match with the maximum overlap, we address thequestioninanoptimalmannertofindbestpolygonassign-ments. As a consequence, to address the performance evalu-ation problem at polygon level, we need to provide a robust object matching. In our our approach, this major phase is carried out by a combinatorial framework to perform poly-gon assignments. Secondly, starting from the original idea of EditCost explained by Phillips and Chhabra, the cost esti-mate for human post-editing effort to clean up the recognition result, we propose the use of a graph matching. This para-digm provides more than a value in R , it reveals the sequence of corrections to be made to transform a set of connected line segments into another.

Through the reading of the literature, on the topic of per-formance evaluation of document image algorithms, we took into account comments and limitations of former protocols to detect five desired points: 1. To consider object fragmentation 2. To provide indices in uniform units 3. A generic and a domain-independent protocol 4. An overall evaluation metric 5. To evaluate how much manual effort is required to Our proposal fulfills these five points: (1) A polygon assign-ment method and a graph matching algorithm tackle both polygon and line fragmentation problems; (2) Our two indi-ces are bounded between 0 and 1; (3) No assumptions as to the kind of documents are made by our protocol, the only constraint is that the document must contain polygons; (4) An overall metric is provided by linear combination of the two proposed indices; (5) The EditCost representative of the manual labor to be made to correct a document is envisaged through the graph matching question in terms of basic edit operations (addition, deletion, substitution). Furthermore, working at polygon level offers many advanta-ges. It makes the spotting of errors easier, there are much less polygons than vectors in a drawing, so the visualization of mistakes is pretty fast. This point is very important for indus-trial systems, since it permits to reduce the user correction time, by helping him to focus on errors directly. Furthermore, this facilitates the study of large samples of documents and new error categorizations may arise. Addressing the question from another point of view can help developers to improve and design R2V software.
 We cannot solve problems by using the same kind of thinking we used when we created them. (Albert Einstein)
We propose here a novel and optimal object matching for polygon comparison from a viewpoint that differs from prior works. We consider the coherency of the document at a polygon level. Our polygonization evaluation is based on a polygon mapping constrained by the topological informa-tion. While this measure appreciates the quality of the poly-gon overlapping, a cycle graph matching takes a closer look at a lower level of information: the segment layout within the polygons. In this way, we express the consistency of the drawing from a polygon point of view.

The smallest item that can be found in a engineering draw-ing is the segment.
 Definition 1 (Segment) In geometry, a line segment is a part of a line that is bounded by two end points and contains every point on the line between its end points.
 This object is considerably versatile, the number of line seg-ments present in a wire drawing can be significantly impacted by the vectorization algorithm due to the noise that occurred in the original image of documents (Noise due to storage conditions, digitization steps). On the opposite, we decide to investigate a more consistent and reliable object called polygon.
 Definition 2 (Polygon) In geometry, a polygon is tradition-ally a plane figure that is bounded by a closed path or circuit, composed of a finite sequence of straight line segments (i.e., by a closed polygonal chain). These segments are called its edges or sides, and the points where two edges meet are the polygon X  X  vertices or corners. A polygon is a 2-dimen-sional example of the more general polytope in any number of dimensions.

The polygons are formed by running a cycle detec-tion algorithm on the heap of segments that composed the drawing. Invented in the late 1960s, Floyd X  X  cycle-finding algorithm [ 25 ] is a pointer algorithm that uses only two point-ers, which move through the sequence of points at different speeds. This polygon layer is more reliable and so it pro-vides a better foundation to build a dissimilarity measure on top of it. A conventional way of defining measures of dis-similarity between complex objects (maps, drawing obtained by vectorization) is to base the measure on the quantity of shared terms. Between two complex objects o 1 and o 2 ,the aim is to find the matching coefficient mc , which is based on the number of shared terms. The polygon organization of a document is a good viewpoint, more stable and less sub-ject to variations than the segment layer. In the mean time, it represents a complimentary view of the problem.

Polygonized elements issued from a raster to vector con-version method are assigned and measured up to a manually vectorized ground-truth. The assignment problem is one of the fundamental combinatorial optimization problems in the branch of optimization or operations research in mathemat-ics. It consists of finding a maximum weight matching in a weighted bipartite graph.

In its proposed form, the problem is as follows:  X  X et D GT , D CG be a ground-truth document and a com- X  There are | D CG | number of polygons in D CG and | D GT
Our combinatorial framework cuts down the algorithmic complexity to an O ( n 3 ) upper bound, depending on the num-ber of polygons in the largest drawing. Hence, the matching can be achieved in polynomial time, which tackles the com-putational barrier. We stand apart from the prior approaches by grouping low-level primitives into polygons and then con-sidering their matching at this high-level point of view. Once polygons are mapped, it is interesting to take a closer look at a lower level by checking out segment layouts within the mapped polygons. This presents some advantages as elements are locally affected to define a local dissimilarity measure, which is visually interesting; it makes easier the spotting of mis-detected areas. A complete data flow pro-cess for polygonized document evaluation is proposed. Our contribution in this domain is two-fold. First, we compare a ground-truth document and a computer-generated document thanks to an optimal framework that proceeds with object mapping. Finally, another operator provides estimates on the relation between the segments within two mapped polygons in terms of edit operations, by means of a cycle graph match-ing. Figure 1 depicts an overview of our methodology. 1.4 Organization The organization of the paper is as follows: Sect. 2.1 describes theoretically and in terms of algorithm the polygon mapping method, Sect. 2.2 explains the cycle graph matching process in order to assess the quality of the polygonal approx-imation. Section 2.3 sets forth the type of errors that are likely to occur in object retrieval systems. Section 3 describes the experimental protocol, this section also explains how to inter-pret our new set of indices in a application to cadastral map evaluation. A summary is included in Sect. 4 , followed by discussions and concluding remarks. 2 A set of indices for polygonization evaluation In this section, we define the two criteria involved in our proposal for a performance evaluation tool dedicated to polygonization. In the first part, a polygon assignment method is described. It aims at taking into account shape distortions caused by retrieval systems. Secondly, a matched edit distance is defined. This measure represents the varia-tions introduced when a given system approximates digital curves. It is a synthesis on vectorization precision. Finally, mis-detection or over-detection errors due to raster to poly-gon conversion are introduced in a third part. This breakdown leads to the definition of specific notations and error catego-rizations. 2.1 Polygon mapping using the Hungarian method Once polygons are located within the vectorized document, it can be seen as a partition of polygons. Comparing two doc-uments ( D 1 , D 2 ) comes down to matching each polygon of D 1 with each polygon of D 2 . This assignment is performed using the Hungarian method, which is formally described in the next part. 2.1.1 Algorithmic of the Hungarian method Our approach for vectorized document comparison is based on the assignment problem. The assignment problem con-siders the task of finding an optimal assignment of the ele-mentsofaset D 1 to the elements of a set D 2 . Without loss of generality, we assume that | D 1 | X | D 2 | . The complete bipartite graph G pm = D 1  X  D 2  X  , D 1  X  ( D 2  X  ) , where represents empty dummy polygons, is called the poly-gon matching of D 1 and D 2 . A polygon matching between D 1 and D 2 is defined as a maximal matching in G pm .We define the matching distance between D 1 and D 2 , denoted by PMD ( D 1 , D 2 ) , as the cost of the minimum-weight polygon matching between D 1 and D 2 with respect to the cost func-tion K . The cost function is especially dedicated to our prob-lemandisfullyexplainedinSect. 2.1.2 .Thisoptimalpolygon assignment induces a univalent vertex mapping between D 1 and D 2 , such that the function PMD : D 1  X  ( D 2  X  )  X  R 0 minimizes the cost of polygon matching. If the numbers of polygons are not equal in both documents, then empty  X  X ummy X  polygons are added until equality | D 1 |=| D 2 | is reached. The cost to match an empty  X  X ummy X  polygon is equal to the cost of inserting a whole unmapped polygon ( K (  X  , P ) ). A shortcoming of the method is the one-to-one mapping aspect of the algorithm, however, this latter is per-formed at a high level of perception where data are less likely to be fragmented. Finally, this disadvantage should not dis-courage the use of the PMD distance considering the impor-tant speed-up it provides while being optimal, deterministic and quite accurate. In addition, unmapped elements are not left behind, they are considered either as  X  X alse alarm X  or as  X  X alse negative X  according to the kind of mistakes they induced (see Sect. 2.3 ). 2.1.2 Cost function for polygon assignments Munkres X  algorithm as introduced in the last section pro-vides us an optimal solution to the assignment problem in O ( n 3 ) time. In its generic form, the assignment problem considers the task of finding an optimal assignment of the elements of a set GT to the elements of a set CG assuming that numerical costs are given for each assignment pair. In fact, a cost function does exist between each pair of poly-gons to express numerically their similarity, in the same way a  X  X ero X  will represent two identical polygons and  X  X ne X  two polygons not sharing any common features. The polygon overlay, inspired by the theory of sets, measures the simi-larity between polygons. When polygons are compared into the same axis system, the overlay takes into account spa-tial adjustment between polygons. The process of overlay-ing polygons shares common points with set theory. Let X  X  assume that A and B are two sets, the intersection can be reformulated through the set theory. I nter secti on , where the result includes all those set parts that occur in A and B. A way to compare them is to find out how A differs from B (see Fig. 2 ): In mathematics, the difference of two sets is the set of elements which are in one of the sets, but not in both. This operation is the set-the-oretic kin of the exclusive disjunction in Boolean logic. The symmetric difference of the sets A and B is com-monly denoted by A B . The symmetric difference is equivalent to the union of both relative complements, that is: A B = ( A \ B )  X  ( B \ A ) and it can also be expressed as the union of the two sets, minus their intersection: A B = ( A  X  B ) \ ( B  X  A ) (1) The symmetric difference is commutative and associative: A B = B A The empty set is neutral, and every set is its own inverse: A  X = A A A = X  The set difference can be expressed as the union of the two sets, minus their intersection and represents the number of shared terms. In order to define measures of dissimilarity between complex objects (sets, polygons,...), a suitable pos-sibility is to settle the measure on the quantity of shared terms. Based on this paradigm, the simplest similarity mea-sure between two complex objects o 1 and o 2 is the matching coefficient mc : mc = Where o 1  X  o 2 denotes the intersection of o 1 , o 2 and o stands for the union between the two objects.

Derived from the Eq. 2 , dissimilarity measures which take into account the maximal common shared terms ( mcs )oftwo sets were put forward: d ( o 1 , o 2 ) = 1  X  Where | o | denotes the size of o .FromtheEq. 2 , the expres-sion o 1  X  o 2 is substituted by the size of the largest object and the intersection of two objects ( o 1  X  o 2 ) is represented by the maximum common subset.

Finally, to obtain a dissimilarity measure between poly-gons, let us define a function K that induces a mapping of P 1 and P 2into R ( K : P 1  X  P 2  X  R ) where P 1 and P 2 are two polygons: K ( P 1 , P 2 ) = 1  X  mcs Where | P | denotes the area of the polygon P and where mcs ( P 1 , P 2 ) is the largest set common to P 1 and P 2, i.e. it cannot be extended to another common set by the addition of any element. Straightforwardly, the largest common subset between P 1 and P 2 can be seen as the area in interception between P 1 and P 2. Consequently, Eq. 4 can be re-written as follows: K ( P 1 , P 2 ) = 1  X  In a general way, the Eq. 3 has been proved to be a metric in [ 26 ] and [ 27 ]. As long as, | P 1  X  P 2 | is a valid maximum common shared terms for polygons, the metric properties hold true for the Eq. 5 .

Now, let us take a closer look at the basic properties of this dissimilarity measure: K ( P 1 , P 2 )&gt; = 0  X  P 1 , P 2(6) K ( P 1 , P 2 ) = 0  X  P 1 = P 2(7) K :[ 0 ; 1 ] (8) 2.1.3 Theoretical discussion on our dissimilarity measure Two questions are addressed in this part, the first one con-cerns the unity of PMD when facing heterogeneous docu-ments (different scales and orientations) and the second point is devoted to the proof of PMD as being a metric. This point is crucial to demonstrate the ability of providing a rank infor-mation, which is representative of the error level of a given document with respect to the entire collection.

The cost function behavior: One condition imposed by the Hungarian method is that the cost function has to be strictly positive or zero, this assumption is respected (see Eq. 6 ). In addition, the normalization between zero and one (see Eqs. 5 , 8 ) confers some interesting aspects to the distance.

Without normalization, the distance is highly dependent on the polygon surfaces. A higher importance would be given tolargepolygonsandtheycouldhighlyimpactthefinalscore, while small polygons would not be treated with significance.
Hence, this measure considers with equity whether the concerned polygons are small or not. It means that no bias will be introduced when facing large polygons. Therefore, a highly over-segmented vectorization with many small areas will be roughly as bad as an under-segmented vectorization with few large polygons. Finally, the normalization leads to a lower and an upper bound (Eq. 8 ) of our distance, which is useful to compare a document collection with different scales. In this way, PMD is dependent on scale, translation and rotation variations. Nevertheless, these are desired prop-erties for a distance, which is meant to represent the exactness between two polygonized drawings.
 The polygon matching distance is a metric (PMD): Proof To show that our measure of similarity between doc-uments is a metric, we have to prove four properties for this similarity measure.  X  PMD ( D 1 , D 2 )  X  0  X  PMD ( D 1 , D 2 ) = PMD ( D 2 , D 1 )  X  PMD ( D 1 , D 3 )  X  PMD ( D 1 , D 2 ) + PMD ( D 2 , D 3  X  PMD ( D 1 , D 2 ) = 0  X  D 1 = D 2 2.2 Matched edit distance for polygon comparison The Hungarian method provides a formal framework to per-form a one-to-one mapping between polygons. Each mapped pair of polygons minimizes its symmetric difference provid-ing a topological information. However, this measure does not take into account the labor work that has to be done to change a polygon from the CG to a correct polygon from the GT. In order to compensate this weakness, we decide to include an additional measure which reveals how many edit operations have to be done to change a polygon into another according to some basic operations. That X  X  why we present an edit distance for polygon comparison. From Definition 2 and Fig. 3 , a clear link exists between a polygon and its representation by a cycle graph. The next part defines a cycle graph edit distance (CGED) for polygon comparison, this latter deals with the graph matching problem applied cycle graph. 2.2.1 A cycle graph matching distance for polygon Visually, two chains of segments are similar if the length attributes and angles between consecutive segments can be aligned. In the literature on polygonal shape recognition, most approaches base the distance definition between two polygonal shapes on length and angle differences. For exam-ple, Arkin et al. used in [ 28 ] the turning function, which gives the angle between the counterclockwise tangent and the x-axis as a function of the arc length. Their results are in accordance with the intuitive notion of shape similarity. More recently, in [ 29 ], Llad X s et al. represented regions by polylines and string matching techniques are used to measure their similarity. The algorithm follows a branch and bound approach driven by the RAG edit operations. This formula-tion allows matching computing under distorted inputs. The algorithm has been used for recognizing symbols in hand-drawn diagrams.

Polygonal shapes require to characterize the segments (their length) but also their relationships by the angle information.

The graph-based representation was preferred to string representation. In fact, the protocol is designed for polygons but may also be extended to other line shapes, for instance this could be made by completing the graph representation to connected vectors instead of searching for cyclic polygons. In this way, the graph-based viewpoint could be the container of a wider range of entities. It leaves the door open to a more global paradigm, the object matching question.

The concept of edit distance has been extended from strings to trees and to graphs [ 30 , 31 ]. Similarly to string edit distance, the key idea of graph edit distance is to define the dissimilarity, or distance, of graphs by the minimum amount of distortion that is needed to transform one graph into another. Compared to other approaches to graph match-ing, graph edit distance is known to be very flexible since it can handle arbitrary graphs and any type of node and edge labels. Furthermore, by defining costs for edit operations, the concept of edit distance can be tailored to specific applica-tions. A standard set of distortion operations is given by inser-tions, deletions, and substitutions of both nodes and edges. We denote the substitution of two nodes u and v by ( u  X  v) the deletion of node u by ( u  X   X ) , and the insertion of node vby ( X   X  v) . For edges, we use a similar notation.
Given two graphs, the source graph G 1 and the target graph G 2 , the idea of graph edit distance is to delete some nodes and edges from G 1 , relabel (substitute) some of the remaining nodes and edges, and insert some nodes and edges in G 2 , such that G 1 is finally transformed into G 2 . A sequence of edit operations e 1 ; ... ; e k that transforms G 1 completely into G 2 is called an edit path between G 1 and G 2 . Obviously, for every pair of graphs ( G 1 ; G 2 exist a number of different edit paths transforming G 1 into G 2 . To find the most suitable edit path, one introduces a cost for each edit operation, measuring the strength of the cor-responding operation. The idea of such a cost function is to define whether or not an edit operation represents a strong modification of the graph. Obviously, the cost function is defined with respect to the underlying node or edge labels. Clearly, between two similar graphs, there should exist an inexpensiveeditpath,representinglowcostoperations,while for dissimilar graphs an edit path with high costs is needed. Consequently, the edit distance of two graphs is defined by the minimum cost edit path between two graphs. The com-putation of the edit distance is carried out by means of a tree search algorithm, which explores the space of all possible mappings of the nodes and edges of the first graph to the nodes and edges of the second graph.
 Definition 3 ( Cycle Graph Matching ) In this work, the prob-lem which is considered concerns the matching of cycle directed labeled graphs. Such graphs can be defined as fol-lows: Let L V and L E denote the set of node and edge labels, respectively. A labeled graph G is a 4-tuple G = (
V , E , X , X ) , where  X  V is the set of nodes,  X  E  X  V  X  V is the set of edges  X   X  : V  X  L V is a function assigning labels to the nodes,  X   X  : E  X  L E is a function assigning labels to the edges.  X  | V |=| E |
Now, let us define a function CGED based on the cycle graph edit distance that induces a mapping of G 1 and G 2 R ( CGED : G 1  X  G 2  X  R ): CGED ( G 1 , G 2 ) = min Where  X ( G 1 , G 2 ) denotes the set of edit paths transforming G 1 into G 2 , and edi t denotes the cost function measuring the strength edi t ( e i ) of edit operation e i . A cycle graph is a cycle which visits each vertex exactly once and also returns to the starting vertex. As a consequence, the set of all edit paths is considerably reduced and the cycle graph matching can be solved in O ( nm log m ) time.
In order to use cycle graph matching for polygon accuracy evaluation, we use an attributed graph representation. Start-ing from a polygonal approximation of the shape, a graph is built. We use the segments as primitives, encoding them with a set of nodes. Each node is labeled with a real num-ber l i , where l i denotes the length of the segment s i edges are built using the following rule: two nodes are linked with a directed and attributed edge if two constitutive seg-ments share a common point. Each edge is labeled with a real number i that denotes the angle between s i and s i  X  1 in the counterclockwise direction. We can appreciate an example of how these attributes are computed for a sample shape in Fig. 3 .

Let A and B be two chains of adjacent segments, repre-sented as cycle graphs, with total lengths | A | = n and | and with respectively attributed cycle graph representations: G and, G
The cost functions for attributed cycle graph matching are reported in Table 1 . 2.2.2 Interpretation of the edit operations There are the proposed cost functions inspired by the ones designed by Tsay and Tsai in [ 32 ] where they use string matching for shape recognition.

The operation attributes decrease the edit costs for primi-tives undergoing noisy transformations, as the inherent seg-ment fragmentation from the raster to vector process. And it aims to compare polygons with different number of segments making the system tolerant to segment cardinality.
Furthermore, our edition functions can describe real trans-formations applied to polygons. When editing a vectoriza-tion, basic operations are: Remove, Add or Move a segment; a visual illustration of theses operations is given in Fig. 4 . Through the linear combination of the cost functions, it is possibletorecreatetheusagesofapersonmodifyingavector-ization, and the definitions below present the combinations to obtain different polygon transformations. Conceptually, we are here very close to the ideas proposed by Chhabra [ 5 ].
Reasoning in a segment deletion context, we proceed by two simple case analysis to detail the elaboration of the gen-erated operation sequence and then to describe the impact of the involved cost functions.
 Case 1 Edit operation for segment deletion At first, deleting a segment comes with the deletions of a node and an edge into the graph representation.
 We conclude that deletions =  X (( l A i )  X   X ) +  X  (( A i )  X   X ) In addition, the deletions of a node and an edge in a cycle graph creates an orphan edge that must be reconnected. To take into account this modification, an edge label substitution is operated.

Consequently we conclude: substitution =  X  (( A i )  X  ( B j ))
Finally, the sequence of operations is:  X ( s i  X   X ) = deletions + substitution Plainly, we change the formal expressions by their corre-sponding costs to obtain the following formula:  X ( s i  X   X ) = Case 2 Cost function for a segment deletion A segment deletion is operated to remove a segment errone-ously inserted by a given R2V system. We can show that the cost function associated with the edit operation  X ( s i  X   X ) is representative of the mistake importance. We proceed by a simple case analysis. The global cost function is impacted by two parameters, the size and the misalignment of the removed segment.  X ( s i  X   X ) expresses two deformations: (i) How large was the segment to be removed and (ii) How misa-ligned was the segment to be removed compared with the adjacency vectors. In this way, larger is the segment to be removed and bigger is the mistake made by the R2V. The cost function responds correctly, larger is the segment and bigger is the quantity  X ( s i  X   X ) by the increase of the l A i | The second kind of errors generated by R2V systems is the segment misalignment. Larger is the angle variations of the removed segment with the adjacency segments in the poly-gon and bigger is the committed error. Accordingly, larger is the angle variations of the removed segment and bigger is the quantity  X ( s i  X   X ) by the increase of A i 360 + combination.

Therefore, we provide the definitions for segment dele-tion, addition and move.
 Definition 4 Segment deletion transformation  X ( s i  X   X ) =  X (( l A i )  X   X ) +  X  (( A i )  X   X )  X ( s i  X   X ) = Definition 5 Segment addition transformation  X ( X   X  s  X ( X   X  s Definition 6 Segment move transformation  X ( s i )  X  ( s j ) =  X (( l A i )  X  ( l B j )) +  X  (( A i )  X  ( B  X ( s i )  X  ( s j ) = 2.2.3 Matched edit distance To complete the process, the cycle graph matching distance ( CGED ) has to be performed on every pair of mapped poly-gons found by the Hungarian methods when it is based on the symmetric difference. Note that if one polygon is associ-ated with an empty dummy item then the cycle graph match-ing distance will be only composed of addition operations. The matched edit distance(MED) is then composed of the sum of all CGED ( G 1 , G 2 ) computed on every pair of graphs extracted from the polygons.
 MED ( D 1 , D 2 ) = 2.3 Types of error and notations Here, we sum up a set of two criteria, which will help us to evaluate a given raster to vector conversion. Each measure is a viewpoint on the vectorization process. However, every cri-terion can still be divided into two categories according to the nature of the error it expresses. Hence, the next part defines the different kinds of errors that can occur when dealing with object retrieval systems. 2.3.1 Types of error  X  Type I error, also known as an  X  X rror of the first kind X ,  X  Type II error, also known as an  X  X rror of the second kind X , 2.3.2 Notations For the understanding of these tests, we first introduce nota-tions that will make the reading much simpler. A dissimilarity measure between vectorized documents is a function: d : X  X  X  X  R where X is a vectorized document. We report in Table 2 ,the notations derived from this general form.
 mapped polygons. This is a synonym of accuracy, it denotes how well suited is the detected polygon from the D CG . X takes the stock of the over-detections issued from the raster to vector conversion step. On the other hand, X fn represents the mis-detections, it occurs when the software used to vectorize has a strict policy of rejection which leads to an under-detec-tion of objects. For clarity reasons, when no precision is spec-ified, X refers to X all . Finally, a desirable information is the number of false alarms, false negative and true positive poly-gons retrieved by the retro-conversion system. These val-ues are normalized as follows to obtain a comparable rate between documents.  X   X  3 Experiments This section is devoted to the experimental evaluation of the proposed approach. First, we describe databases that are used to benchmark our measures. Then, the protocol of our exper-iments is defined by enumerating the kind of assessments we performed. The two first tests are dedicated to graphi-cal symbols from GREC contests. On this basis, we aim at illustrating the ability of polygon matching distance (PMD) and matched edit distance (MED) of being representative of polygon deformations (shape variation and polygonal approximation modification, respectively). The last evalu-ation concerns the cadastral map subject, we show results on a large collection of maps. We provide guidelines to under-stand the meaning of our set of indices. In this pedagogic objective, a visualization of detection errors is proposed.
In this practical work, methods were implemented in Java 1.5 and run on a 2.14GHz computer with 2G RAM. Both dat-abases and performance evaluation tools are freely available on this web site: http://alpage-l3i.univ-lr.fr/ Both datasets and the experimental protocol are first described before investigating and discussing the merits of the proposed approach. 3.1 Databases in use In recent years the subject of performance evaluation has gained popularity in pattern recognition and machine learn-ing. In the graphics recognition community, a huge amount of efforts was made to elaborate standard and publicly available data sets. Especially, E. Valveny [ 33 , 34 ] and M. Delalandre [ 35 ] published online symbol datasets for a symbol classi-fication purpose. In this section, we describe two databases derived from [ 34 ] and [ 35 ] and we also present a cadastral map collection. The content of each database is summarized in Tables 3 , 4 .

Base A: Shape distortion. The paper presented in [ 35 ] gave birth to a publicly available database of symbols. 1 this setting, we removed all polygon-less symbols to fit our purpose which was to evaluate polygon detection methods. Hence, we selected 70 symbols from the GREC X 05 contest [ 36 ] and a sample is presented in Fig. 5 .

On perfect symbols, a vectorial noise is applied to gener-ate a collection of degraded elements. We could not afford to use real data because of the difficulty of collecting images with all kinds of transformations and noise. Besides, it is not easy to quantify the degree of noise in a real image. Then, it is not possible to define a ranking of difficulty of images according to the degree of noise. In our experiments, we have re-used methods for the generation of shape transformation (based on active shape models [ 33 ]).

Vectorial Distortion: The goal of vectorial distortion is to deform the ideal shape of a symbol in order to simulate the shape variability produced by hand-drawing. The method for the generation of vectorial distortions of a symbol is based on the Active Shape Models [ 37 ]. This model aims to build a model of the shape, statistically capturing the variability of a set of annotated training samples. In order to be able to apply this method, we need to generate a good set of training samples. This is not a straightforward task due to the statis-tical nature of the method. The number of samples must be high enough, and the samples must reflect the usual kind of variations produced by hand-drawing. However, it is difficult to have a great number of hand-drawn samples of each sym-bol. To be really significant, these samples should be drawn by many different people. Thus, the decision of generating automatically the set of samples has arisen. Based on the generation of deformed samples through the random modi-fication of a different number of vertices of the symbol each time [ 38 ].

Each sample is represented using the model described in [ 39 ], which permits easy generation of deformed shapes. Each symbol is described as a set of straight lines, and each line is defined by four parameters: coordinates of mid-point, orientation and length. Thus, each deformed sample can be seen as a point x i in a 4 n -dimensional space, where n is the number of lines of the symbol. Then, principal compo-nent analysis (PCA) can be used to capture the variability in the sample set. Given a set of samples of a symbol, we can compute the mean x and the covariance matrix S .Themain modes of variation are described by the first eigenvectors p of the covariance matrix S . The variance explained by each eigenvector is equal to its corresponding eigenvalue. Thus, each shape in the training set can be approximated using the mean shape and a weighted sum of the eigenvectors: x = x + Pb where P = p 1 ,..., p m is the matrix of the first m eigenvec-tors and b is a vector of weights. This way, new images of a symbol can be generated by randomly selecting a vector of weights b . Increasing values of b i will result in increasing levels of distortion (see Fig. 6 ).

The model of vectorial distortion described in the former paragraph has been applied with four increasing levels of dis-tortion to generate 280 (70*4) images of symbols. The vari-ance was tuned from 0.00025 to 0.00100 by step of 0.00025. This way of changing the variance is coherent with the pro-tocol presented in [ 35 ]. The entire database is then made up of 350 elements, 280 degraded symbols and 70 models. The shape distortion generator, 3gT system was provided by M. Delalandre. 2 3gT  X  X eneration of graphical ground Truth X  isasystemtogeneraterandomgraphicaldocuments(exported into SVG) of low-level primitives (vectors, circles, ...) with their corresponding ground-truth. Base A is a reliable source to evaluate the shape distortion sensitivity of our polygon location measure. Numerical details concerning this data set are presented in Table 4 .

Base B: Binary degradation. First of all, we decided to use the data set provided by the GREC X 03 contest. Mainly two application domains, architecture and electronics, were adopted as a representative sample of a wide range of shapes. GREC X 03 database is originally constituted of 59 symbols from which we removed symbols without polygons. This pruning step led us to a database of 53 symbols. From the 9 noise levels available, we only focused on the 6 first levels. Consequently, database B is made up of 318 (6*53) binary degraded symbols plus 53 ideal models, i.e. a total of 371 polygonized elements according to the process explained in the next paragraph.

Binary Degradation: Kanungo et al. have proposed a method to introduce some noise on bitmap images [ 40 ]. The purpose of this method is to modelize noises obtained by operations like printing, photocopying, or scanning pro-cesses. The problem is approached from a statistical point of view. The core principle of this method is to flip black and white pixels by considering, for each candidate pixel, the distance between it and the closest inverse region. The degradation method is validated using a statistical method-ology. Its flexibility in the choice of the parameters requires some adaptations. Indeed, a large set of degradations can be obtained. Themethoditself accepts noless than6parameters, allowing to tune the strength of white and black noise, the size of the influence area of these noises, a global noise (which does not depend of the presence of white/black pixels), and a post-processing closing based on well-known morphological operators. Of course, these 6 parameters may generate a large number of combinations, and thus, of models of degradation. So, if the core method used for the degradation tests is formal and validated for its correctness, the determination of the set of parameters used for the contest is more empirical. This framework was applied to the organization of the GREC X 03 contest on symbol recognition. In [ 34 ], authors attempted to reproduce a set of degradations representing some realistic artifacts (to simulate noise produced when printing, photo-copying and scanning). Six levels of degradation (see Fig. 7 ) were determined by [ 34 ]. They took care to represent some standard noises: local, global troubles.
 Binarydegradationimpactsonpolygonalapproximations: The higher is the noise level the higher are the distortions on the polygonal approximation. The noise level has a direct influence on the vectorization algorithm. In this experiment, we used a standard data flow process to polygonize the symbols: (i) Cleaning; 3 (ii) Skeletonization; (iii) Polygonal approximation and (iv) Polygonizer. Arbitrarily, we adopted the well-known di Baja X  X  skeletonizer [ 41 ] and the Wall and Danielsson X  X  vectorization [ 42 ]. Then, a polygonizer was applied to transform the set of segments into polygons. These steps are summed up in Fig. 8 . A piece of polygon is zoomed-in to show the perturbation applied on the polygo-nal approximation when noise increases. The method only requires a single threshold i.e., the ratio between the alge-braic surface and the length of the segments which makes this linear time algorithm fast and efficient. This parameter is set to 60 pixels for all the experiments. We did not want to assess the impact of the approximation threshold but rather the impact of noise on polygonization when the threshold is frozen.
 More information concerning those data is detailed in Table 4 .

Base C: Cadastral map collection. In the context of a project called ALPAGE, a closer look is given to ancient French cadastral maps related to the Parisian urban space during the 19th century (Fig. 9 ). Hence, the map collection is made up of 1100 images coming from the digitalization of Atlas books. On each map, domain-objects called Par-cels are drawn by using color to distinguish between them. From a computer science perspective, the challenge consists in the extraction of information from color documents with the aim of providing a vector layer to be inserted in a GIS (Geographical Information System).

Automatic polygon detection: In this project, a bottom-up strategy is adopted. In bottom-up strategies, algorithms are performed in a fixed sequence, usually starting low-level analysis of the gray level or black and white image, from which primitives are extracted. From this starting point, the four stages for extracting parcels from a cadastral map are put forward. (i) At first, a color gradient is performed to locate objects within the image. (ii) Then, a text/graphic segmen-tation is run on the gradient image to preserve only graphic elements [ 43 , 44 ]. (iii) Thirdly, a digital curve approximation is performed to transform pixels into vectors [ 45 ]. (iv) finally, vectors are gathered to form polygons using a polygonizer algorithm [ 46 ]. The parcel extractor is evaluated using our set of indices.

Ground-Truthing: With the help of experts in several fields of sciences such as Historians, Archaeologists and Geogra-phers, a campaign of handmade vectorization was carried out. This work was intensively labor consuming yet nec-essary. It was the only way to give us the opportunity to fully evaluate the accuracy of our work. The main goal was to build a reference database to investigate the merit of our parcelretrievalscheme.Manually,100rastermapswerecare-fully and precisely vectorized to constitute a reliable collec-tion of 2335 parcels of lands. These units are encoded as polygons according to the Definition 2 . Therefore, a real link does exist between a parcel and its polygon repre-sentation. This labor intensive procedure represents a real reference to measure up the accuracy and the validity of our automatic vectorization [ 44 , 47 ]. The content of the database is summarized in Table 3 . In average, there are 25 parcels per map and this accounts for about 75 line segments per parcel. The ground-truth was manually made according to simple rules. Each parcel had to be described by a poly-gon. The median line was favored in the line tracking phase. The precision question was solved by imposing to fit at best the parcel contour and consequently, in each polygon, a ver-tex corresponds to a significant direction change. For each image of document, there exists exactly one pair of vec-torized maps, one map called ground-truth and one map named computer generated, respectively &lt; D GT ; D CG &gt; An example of a pair of vectorization to be matched is dis-played in Fig. 10 . Further details on this data set are pre-sented in Table 3 . Note that this database is made up of real data.

A synthesis about this database is reported in Table 3 while the content is publicly available at http://alpage-l3i.univ-lr.fr/PE/alpagedb.zip . 3.2 Protocol Three different ways of evaluating our indices are proposed.
Polygon Matching Distance evaluation: To assess the abil-ity of the polygon mapping distance to increase when doc-uments get badly reconstituted, we focus on Base A. Base A is representative of different shape distortions and conse-quently,polygonshapesareaffectedbythisnoise.OnBaseA, we performed a ranking test using PMD as a dissimilarity measure. A visual explanation of how ranks are obtained is brought to view in Fig. 11 . Then, ranks are compared thanks to a statistical method called a Kendall X  X  test and defined as follows: Definition 7 Kendall X  X  test.

We assess the correlation concerning the responses to k -NN queries when using PMD as dissimilarity measures. The setting is the following: in a given Base X , we select a number N of symbols, that are used to query by similarity the rest of the dataset. Top k responses to each query obtained using PMD are compared with the ground-truth. The ground-truth ranks are obtained thanks to the control of noise level. The similarity of the PMD ranks and the ground-truth ranks is measured using Kendall correlation coefficient. We con-sider a null hypothesis of independence( H 0) between the two responses and then, we compute, by means of a two-sided statistical hypothesis test, the probability (p-value) of get-ting a value of the statistic as extreme or more extreme than observed by chance alone, if H 0 is true. The Kendall X  X  rank correlation measures the strength of monotonic association between the vectors x and y ( x and y may represent ranks or ordered categorical variables). Kendall X  X  rank correlation coefficient  X  may be expressed as  X  = S Where, S = And, D = k
Polygonal approximation sensitivity: In this second exper-iment, we aim at assessing the capacity of the matched edit distance (MED) to increase when the polygonal approxima-tion gets badly reconstituted by the retrieval systems. Base B is involved in this test. Base B is a binary degraded set of symbols and the higher is the noise level on symbols and the more disturbed is the polygonal approximation from the original one. In this way, we control the distortion level of the digital curve approximation and consequently, we obtain a ground-truth order from an ideal symbol by controlling the noise level, in Fig. 8 . Finally, the ranks returned by MED and the ground-truth are compared according to the Kendall X  X  test described in Definition 7 . using a Kendall test.
Application to the evaluation of parcel detection: Our last experiments are based on real data that composed Base C. At first, we performed the PMD distance on a single pair of given maps and this in order to highlight dissimilarities issued from the raster to vector conversion. Then, an exper-iment was dedicated to the evaluation of the entire collec-tion of cadastral maps. We provided an interpretation of the results through the viewpoints of our set of indices. Finally, a statistical framework was described to figure out relations between the different indices. 3.3 Polygon matching distance evaluation Using N = 70 , k = 4 equal to the number of noise levels available in Base A, we present in Fig. 12 and Table 5 ,the results obtained in terms of  X  values. From the 70 tests, only 9 have a p-value greater than 0.05, so we can say that the hypothesis H 0 of independence can be rejected in 87.4% cases, with a risk of 5%. The observed correlation between the responses to k -NN queries when using the ground-truth and polygon matching distance ( PMD ) tends to reveal a rank relation between both (median value of  X  = 0 . 800). By stress testing a given system, we aim at demonstrating that our protocol can reveal strengths and weaknesses of a system. The PMD index increases when image degradation increases. 3.4 Polygonal approximation sensitivity Using N = 53 , k = 6 equal to the number of noise lev-els in Base A, we present in Fig. 13 and Table 6 , the results obtainedintermsof  X  values.Fromtheseresults,werejectthe null hypothesis of mutual independence between MED and the ground-truth rankings for the students. With a two-sided test we are considering the possibility of concordance or discordance (akin to positive or negative correlation). A one-sided test would have been restricted to either discordance or concordance, which would be an unusual assumption. In our experiment, we can conclude that there is a statistically sig-nificant lack of independence between MED and the ground-truth rankings of the symbols by MED. MED tended to rank symbols with apparently greater noise as being farther from the ideal symbol than those with apparently less noise and vice versa.
 3.5 Application to the evaluation of parcel detection A visual dissimilarity measure of local anomalies: In this part, we focus on comparing maps two by two. This dif-ficult task requires a good observation of the local differ-ences between the compared documents. On a randomly picked pair &lt; D GT ; D CG &gt; , we computed the polygon matching distance ( PMD all ). A bi-dimensional representa-tion of the costs to assign each element from the D CG to the D GT is displayed in Fig. 14 , whereas values of the dif-ferent measures are reported in Table 7 . Figure 14 provides a visual understanding of where the anomalies are located. First, it facilitates the spotting of errors and other aberra-tions and especially, this framework can help domain experts understanding the limits and advantages of a vectorization software. Figure 14 is worth a thousand words, it makes eas-ier the communication and the implementation of mutual-ized working tools for both Information and Communication Technologies (ICT) -Humanities and Social Sciences (HSS) communities.

To conclude, it can help users to spot where the mistakes are located and so save them a lot of time (time saver). It can help software designers to locate easily where the R2V conversion failed and consequently, this local visualization at a polygon level facilitates the categorization of detection errors.

Evaluation of a collection of maps: From the data set of vectorized maps, we attempted to evaluate the quality of the overall conversion process through the viewpoints offered by the two main criteria that we have described, PMD , MED .
Over the map collection, we observed in Fig. 15 an over-detection tendency. In average, 31% of the retrieved polygons are misleading. 71% of these wrong polygons are concerned by an over-detection behavior (  X  fp = 0 . 22).

Now, we want to figure out the nature of the mistakes, if these over-detected polygons are just some tiny polygons due to noise in the raster or if they represent a major infor-mation altered during the process of conversion. To this aim, we pay attention to Fig. 16 . Figure 16 shows that only 36% of the overall cost PMD all is due to the well-detected poly-gons, hence, most of the information is accurately retrieved from the rasters and the retrieved polygons do fit precisely the ground-truth. On the opposite 64% of the mistakes are attrib-utable to the wrongly detected polygons. Figure 16 strength-ens theideathat anomalies arecausedbytheover-acceptation policy of the automatic application.

In another step, we aim at assessing how much manual work has to be made to correct the automatically vectorized polygons. A fact observed from Fig. 17 is that 54% of the MED all mistakes are caused by the operations to be made when correcting the polygons MED tp . A non-negligible part of the errors are caused by the corrections to be made to fit in the ground-truth. An explanation could be a fragmenta-tion phenomenon; many noisy strokes are broken into small pieces during the polygonal approximation process.
The rest of the errors, that is to say the MED md values, is mainly due to an intensive use of the deletion operator in order to remove the over-detected polygons.
 Finally, based on a common work with the historians Helene Noizet and Laurent Costa, 4 an algorithm with a combined index ( PMD all + MED all )of0.70orlessmay be considered good with respect to human vision evaluation. However, more work should make use of this protocol on a series of algorithms and degraded drawings to obtain an objective assessment on commonly accepted criteria.
Inter-indicescorrelation:Acorrelationmatrixisbuiltfrom the data series of indices (illustrated in Fig. 18 c), the Pearson correlation is 1 in the case of an increasing linear relation-ship,  X  1 in the case of a decreasing linear relationship, and some value between  X  1 and 1 in all other cases, indicat-ing the degree of linear dependence between the variables. The closer the coefficient is to either  X  1 or 1, the stronger the correlation between the variables. This matrix aims to com-pare the different quality measures between them. A matrix is not expressive enough, so, a 256 shades of gray image is generated to express its substantial meaning in a 2D repre-sentation, called image of correlations (Fig. 18 b). In addition, the matrix of scatter plots between the different measures of quality is given (Fig. 18 a). From these data representations, a straightforward remark deals with the proportional behav-iorofthe  X  tp and  X  md , which are closely coupled and share the same information. On the other hand, there is no evident relation between MED and the  X  measures, the Pearson cor-relation coefficient between these two series is not indicative enough, nevertheless, the coefficient is low enough (0.60) to indicate no significant redundancy of information. Finally, a clear tendency appears between PMD and MED , it reveals a low correlation (0.24) between PMD and MED . A situa-tion of independence between the two series can be accepted. These variables really express two different kinds of infor-mation. They represent original viewpoints on the underlying problem. 4 Conclusion and perspectives In this paper, we defined a protocol for performance evalua-tion of polygon detection algorithms. A discussion between the proposed protocol and the literature is also presented. As a consequence, our protocol is positioned as an extension of prior works, an extension at polygon level. In this way, it is closer to the semantic level and closer to objects han-dled by humans. Former benchmarks only include synthetic images with image degradation but we completed these arti-ficial samples by real images with manually created ground-truth. Gathering real data to test and comparing graphics recognition systems is very time-consuming that is why we propose our data set to the community.

Our contribution is two-fold, an object mapping algorithm to roughly locate errors within the drawing and then a cycle graph matching distance that depicts the accuracy of the polygonal approximation. Both were theoretically defined and adapted to the performance evaluation of polygonized documents. Especially, cost functions were reconsidered, using a set distance for the polygon matching distance (PMD) and defining particular edit costs for the graph matching method.

The proposed protocol is objective and comprehensive, both detection and false alarm rates are considered. By stress testing a given system, we demonstrated that our protocol can reveal strengths and weaknesses of a system. The behavior of our set of indices was analyzed when increasing image degradation.

The results presented in Figs. 16 and 17 indicate that the proposed protocol reflects polygon detection and approximation performance accurately. In Fig. 18 , the sta-tistical tests demonstrated that the two proposed measures offer different kinds of information.

We have also confronted our measures of quality to a human-based evaluation. However, more work should be done in this respect to obtain an objective assessment on commonly accepted criteria.

The protocol is designed for polygons but may also be extended to other line shapes by completing the graph repre-sentation to connected vectors instead of searching for cyclic polygons. In this context, the PMD would not have to be mod-ified at all. The MED which is representative of the manual effort to be made to correct mistakes engendered by a R2V system is envisaged through the graph matching question in terms of basic edit operations (addition, deletion, substitu-tion). The graph formalism confers to the approach a more generic nature and opens the way to future works on more complex objects. This graph-based viewpoint could be the container of a wider range of entities. Instead of focusing on polygon items, a given element could be constituted of all connected segments to form a more complex structure than a polygon while the entire principle would remain unchanged. The graph representation is an open way to a more global paradigm, the object matching question. This could change the scope of our performance evaluation tool to the direction of object spotting.
 References
