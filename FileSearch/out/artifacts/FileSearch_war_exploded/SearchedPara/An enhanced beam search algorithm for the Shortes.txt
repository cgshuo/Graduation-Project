 1. Introduction
The Shortest Common Supersequence (SCS) problem asks to obtain a shortest string that is a supersequence of every member of a given set of strings. A supersequence of a given string is a string that can be obtained by inserting zero or more characters anywhere in the given string. Among various applications of this problem are data compression ( Storer, 1988 ; Timkovskii, 1989 ),
AI planning ( Foulser et al., 1992 ), query optimization in databases ( Chaudhuri and Bruno, 2008 ; Sellis, 1988 ), and bioinformatics, particularly DNA oligonucleotide microarray production ( Hubbell et al., 1996 ; Kasif et al., 2002 ; Ning et al., 2005 ; Rahmann, 2003 ;
Sankoff and Kruskal, 1983 ). Microarrays are precious tools successfully used, among others, in gene clustering and identifi-cation, SNP detection, and fusion transcript detection( Ning et al., 2005 ; Rahmann, 2003 ; Skotheim et al., 2009 ). Two well-known types of microarrays are cDNA and oligonucleotide microarrays ( Kasif et al., 2002 ; Ning et al., 2005 ), the latter known to be of higher sensitivity due to its lower cross-hybridization possibility ( Kasif et al., 2002 ; Ning et al., 2005 ). Oligonucleotide microarrays are usually manufactured by the photolithographic method. This method involves several synthesis steps, each to append a same nucleotide, which corresponds to a letter in {A,T,C,G}, to several designated probes. Since the process is accomplished by means of light exposure, the other probes, which are not to receive the nucleotide, are protected by a mask. The sequence of the nucleo-tides used in the synthesis steps is called the deposition string , whose length determines the number of the synthesis steps. For several reasons, it is desirable to keep the deposition string as short as possible ( Kasif et al., 2002 ; Ning et al., 2005 ; Rahmann, 2003 ). First, the masks and the synthesis steps are expensive.
Even a small reduction in the length of the deposition string could lead to a significant reduction in the production cost ( Rahmann, 2003 ). Second, the total manufacturing time is increased as the number of synthesis steps is raised. Third, there exist possibilities for errors in microarray fabrication, because the masking task is not perfect; the probability for a masked probe to be exposed to the light is nonzero. Consequently, the probability for fabrication errors is usually increased as the number of the synthesis steps is raised. Therefore, a shorter deposition sequence is desirable to reduce the manufacturing cost, time, and error. On the other hand, the deposition sequence is a common supersequence of the underlying probes. This motivates the design of high quality algorithms for the SCS problem. Fig. 1 illustrates how the use of a shorter deposition sequence can lead to fewer synthesis steps, hence reducing the production cost, time, and error.
 for a fixed number of input strings, but it is NP-hard in general ( Maier, 1978 ). Consequently, it is highly unlikely to obtain a polynomial-time exact algorithm for the problem, unless P  X  NP ( Garey and Johnson, 1979 ). Exact algorithms proposed for the problem include a dynamic programming algorithm ( Jiang and Li, 1995 ) and a branch and bound algorithm ( Fraser, 1995 ), which are both exponential, the former in the number of strings and the latter in the size of the corresponding alphabet. Therefore, these algorithms are especially beneficial when, respectively, the num-ber of strings or the alphabet size is restricted. Other research has aimed at devising approximation and (meta) heuristic algorithms, which achieve  X  X ood X , but not necessarily optimal, solutions in acceptable time. Approximation algorithms for the SCS include
Alphabet ( Barone et al., 2001 ), an approximate A n algorithm ( Nicosia and Oriolo, 2003 ), Reduce_Expand ( Barone et al., 2001 ), and Deposition and Reduction ( DR )( Ning and Leong, 2006 ).
The approximation ratio of the algorithms Alphabet , Redu-ce_Expand , and DR is 9 S 9 , which is not appealing. The algorithm
DR is in fact a trivial combination of a heuristic mechanism with
Alphabet , which, therefore, guarantees the approximation ratio of 9
S 9 . The approximate A n algorithm provides a 1  X  e approxima-tion ratio, for any fixed e 4 0, particularly e  X  0.2 in the experi-ments in Nicosia and Oriolo (2003) . However, the algorithm is not efficient (i.e. is not of polynomial time complexity) and the size of the search tree can grow exponentially with the size of the given problem instance.

Among (meta) heuristic algorithms for the SCS are Tournament and
Greedy ( Irving and Fraser, 1993 ), Majority Merge ( Branke et al., 1998 ), algorithms based on Genetic Algorithms ( Branke and Middendorf, 1996 ; Branke et al., 1998 ), Ant System and Ant Colony Optimization ( Michel and Middendorf, 1998 ; Michel and Middendorf, 1999 ), and Min_Height and Sum_Height ( Kasif et al., 2002 ); the latter two specifically proposed for DNA sequences. More recent metaheuristic algorithms in clude a hybridization of Memetic Algorithms with Beam Search called Hybrid MA_BS ( Gallardo et al., 2007 ), to which we simply refer as MA_BS ,andarandomizedBeam Search called Probabilistic Beam Search ( PBS )( Blum et al., 2007 ). Another recent algorithm POEMS , together with its variants POEMS_f and POEMS_fw , was also proposed in Kubalik (2010) . However, as reported in Kubalik (2010) , it was outperformed by MA_BS in all the experimental cases. Based on the results reported in Blum et al. (2007) , PBS outperforms MA_BS in most the experi-mental cases. On the other hand, DR outperforms Alphabet , T ournament , Greedy, and Majority merge in all the experimen-tal cases as reported in Ning and Leong (2006) . DR also outperforms Reduce_Expand for strings of length 50 X 100 ( Ning and Leong, 2006 ). However, no comparison of DR and PBS has yet been made, leaving unclear which one is the sta te-of-the-art. The time complex-ity of DR , as specified in Ning and Leong (2006) ,is O ( where 9 S 9 , n and m are, respectively, the size of the alphabet, the number of strings and the maximum length of the strings. No complexity of PBS or Hybrid MA_BS was reported in their proposing papers ( Gallardo et al., 2007 ; Blum et al., 2007 ).

In this paper, we provide an improved beam search algorithm called IBS_SCS fortheSCSproblem,which,onaverage,outperforms all the three recent algorithms, namely DR , MA_BS ,and PBS experimental cases. A similar approach has been successfully used for the Longest Common Subsequence (LCS) problem in Mousavi and Tabataba (2012) . The proposed IBS_SCS algorithm has been inspired by the Blum et al. X  X  PBS algorithm but has the following heuristic function than the one used in PBS .Usingdynamic programming and at polynomial time and space costs, an array of probabilistic values is populated to facilitate the calculation of the heuristic values. Second, we use a technique called domination to further prune the search tree. The domination pruning technique has been inspired by Easton and Singireddy (2007) and Blum et al., (2009) used for the purpose of the LCS problem. However, our usage of this technique is different from those in Easton and Singireddy (2007) and Blum et al. (2009) ). To be precise, in Blum et al. (2009) ,a candidate solution is checked for being dominated by every existing candidate solution, which is rath er time-consuming. On the other hand, in Easton and Singireddy (2007) ,onlythe X  X est-so-far X  X olution is used as the potential dominator for a new candidate solution. In our algorithm, we use the k best-so-far solutions for this purpose, where k is a control parameter in the algorithm. This approach gives us a control to achieve a good amount of pruning in reasonable time. Finally, given the same beam size, PBS does more work than
IBS_SCS , which implies that IBS_SCS can benefit from a larger beam size than that of PBS , given the same amount of execution time. The IBS_SCS algorithm outperforms PBS in all the experi-mental cases, as reported in thi s paper. It also outperforms providing solutions of the same or higher (average) quality in all the experimental cases, including the cases where PBS was outper-formed by MA_BS as reported in Blum et al. (2007) .

Finally, IBS_SCS outperforms DR in all the experimental cases, which were set up based on the experiments conducted in Ning and Leong (2006) . The DR algorithm consists of two stages called deposition and reduction. In the deposition stage, a number of candidate supersequences are created, which are then (tried to be) improved in the reduction stage. It uses Alphabet and a variant of
Majority Merge in the deposition stage to generate the candi-date supersequence. In fact, the use of Alphabet makes DR an approximation algorithm, which guarantees an approximation ratio of 9 P 9 . The DR algorithm is not deterministic only because of using a random tie braking; the rest of the logic is deterministic.
The proposed algorithm IBS_SCS is scalable. Its time com-plexity is polynomial in input size, and its computational cost can be arbitrarily reduced by reducing the beam size. The heuristic function used in the algorithm to evaluate candidate solutions does not suffer from the scalability issue of the heuristic proposed in Mousavi and Tabataba (2012) as an estimation mechanism is used for long strings. While the algorithm is significantly faster than other recent algorithm for the SCS, it yields superior solution quality in most of the cases. Because the proposed algorithm is scalable and sufficiently fast compared to other recent algorithms for the SCS, the main concentration of the reported experimental results is on the solution quality, i.e. on the length of the returned supersequences.

The rest of the paper is organized as follows. Section 2.1 provides the basic notations and definitions used in the paper. In
Section 2.2 , we describe how candidate solutions are evaluated and compared using the employed heuristic function. The pro-posed algorithm, together with its complexity analysis, is pre-sented in Section 2.3 . Section 3 reports the experimental results, and Section 4 concludes the paper. 2. Methods 2.1. Basic notations and definitions
Let s be a string of length m . We denote the length of s by .

We use s [ k ], where k is an integer between 1 and m inclusive, to denote the k th character of s . We also use s [ k 1 .. k 1 r k 1 r k 2 r 9 s 9 , to indicate the substring of s obtained by remov-ing its first k 1 1 characters and its last 9 s 9 k 2 characters. Let s and s 2 be two strings, A 1  X f i 9 i A N  X  , i r 9 s 1 9 g and A N  X  , i r 9 s 2 9 g , where N  X  is the set of integers greater than zero.
We say that s 2 is a supersequence of s 1 , and write s 1 exists a monotone increasing total function g from A 1 to A s 2 . Note that such a map is not necessarily unique. Every string is considered to be a supersequence of the null string, i.e. the string of zero length. Finally, we say that s 1 is a subsequence of s supersequence of s 1 .
 over the alphabet S .Wewrite S ! x if 8 s i A S , s i ! x .TheShortest
Common Supersequence (SCS) problem is then defined, given an input set S of strings, as to obtain a string x of the minimum length such that S ! x . By an input string, we mean a string in S .Sincethe
SCS can be efficiently solved for n  X  2, we assume n 4 2. We use m (possibly indexed) x to denote a candidate solution. A candidate feasible candidate solution x is optimal if no feasible solution of a shorter length exists.
 maximum possible integer k such that s i [1.. k ] ! x .By r mean the string obtained by deleting the first p i ( x ) characters from s i (see Fig. 2 ), and R ( x ) is defined as the set ( r
By a random string, we mean a string each character of which is obtained by selecting uniformly at random one of the characters in S . Finally, we use pr (.) to denote the statistical probability function. Although there are two types of beam search, namely constructive and perturbative (local search), we use beam search in this paper to refer to the former. 2.2. Evaluation of candidate solutions uated and compared. The method used here to evaluate candidate solutions is adapted from Mousavi and Tabataba (2012) where a similar problem, the LCS, was addressed. To evaluate a candidate solution x , we use the probability of R ( x ) ! y , where y is a random string and the strings in R ( x ) are assumed to be independent in the sense that Pr  X  r i  X  x  X  ! y  X  X  Pr  X  r i  X  x  X  ! y 9 r
Our intuition for this heuristic function is that a candidate solution x 1 is likely to be superior to another candidate solution x same length) if, given a random string y of length k , x 1 likely than x 2 . y to be a common supersequence of the input strings, where x i . y, i  X  1or2 , indicates the string obtained by appending y at the end of x i . Of course, the probability of R ( x ) depends on the candidate solution x . In the extreme case where x is a supersequence of all the input strings, i.e., when S probability is 1 because R ( x ) would only include null strings.
In another extreme, where x is the null string, it become the probability of a random string being a supersequence of the input strings. As a rule of thumb, a higher value of Pr ( R ( x ) expected for a longer random string y , although this does not necessarily hold. We use h k ( x ) to denote the heuristic value of a candidate solution x . we have used the subscript k to emphasis the dependency of heuristic values on the length k of the random string y . Of course, if k is less than the length of the longest string in R ( x ), the heuristic value, i.e. the probability of R ( x ) zero. For a fair comparison of candidate solutions, we use the same value of k when evaluating the candidate solutions that are to be compared. A formula used to determine k is presented further in this section. We now show how to calculate heuristic values.
Theorem 1. Let r be a string of length q and y be a random string of length k. Then:
Pr  X  r ! y  X  X 
Proof. First note that in the third case (the otherwise case), the definition of supersequence, every string is a supersequence of the null string and a string cannot be a supersequence of a longer one. Therefore, the first two cases of q  X  0 and q 4 k hold trivially. In the remaining case, because 0 o q r k , the strings r and y are of at least length 1 and both r [1] and y [1] exist. Depending on whether or not the characters r [1] and y [1] are equal, exactly one of the following two cases holds:
Case (i): r [1]  X  y [1]. In this case, we will prove that r which we refer as the concatenation property: 8 s 1 , 8 s 8 s ,  X  s 1 ! s 2  X  4  X  s 3 ! s 4  X  3 s 1 : s 2 ! s 3 : s 4 .

The  X  X  X f X  X  direction. We assume r [2.. q ] ! y [2.. k ] and show r r  X  2 :: q ) r  X  1 :: q ! y  X  1 :: q ) r ! y
The  X  X  X nly if X  X  direction. We now assume r ! y and show sequence, a total monotone increasing function g (.) from
A
There are two possible cases: either g (1)  X  1or g (1) 4 1. In either
Now let g 0 (.) be a total function from {1, y , q 1} to {1, monotone increasing. Let r 0 and y 0 denote, respectively, r [2.. q ]  X  y  X  g  X  i  X  1  X  This means y 0 is a supersequence of r 0 using the mapping g 0 (.). That is, r [2.. q ] ! y [2.. k ].

Case (ii): r [1] a y [1]. In this case, we show that r ! y y [2.. k ].

The  X  X  X f X  X  direction. We assume r ! y [2 y q ] and show r r ! y  X  2 :: q ) e : r ! y  X  1 : y  X  2 :: q  X  because e ! y  X  1 and by the concatenation property  X  ) r ! y  X  1 :: q ) r ! y The  X  X  X nly-if X  X  direction. We assume r ! y and show r ! y [2.. q ]. Because r ! y , there is a total monotone increasing function g (.) However, g (1) a 1 because r [1] a y [1]. Therefore, g ( i ) 4 1, 8 i  X  Therefore, 8 i  X  1 , ::: , q , y 0  X  g 0  X  i  X  X  y  X  g 0  X  i  X  X  1 This means y 0 is a supersequence of r using the mapping g 0 (.). That is, r ! y [2.. k ].

We have so far shown that in Case (i), where r [1]  X  y [1], r ! y 3 r [2.. q ] ! y [2.. k ] and that in Case (ii), where r [1] r ! y 3 r ! y [2.. k ]. Therefore, Pr ( r ! y )  X  Pr ( r [2.. q ] random string based on the uniform probability distribution and the probability for case (i) is 1/ 9 S 9 . Consequently, the probability for case (ii) is 1 1/ 9 S 9  X  ( 9 S 9 1)/ 9 S 9 . Therefore, Pr  X  r ! y  X  X  1 9 S 9 Pr  X  r 2 :: q  X  ! y  X  2 :: k  X  X  Proposition. Given a candidate solution x and a positive integer k, the heuristic value for a candidate solution x is calculated as h  X  x  X  X  where y is a random string of length k .
 Proof. By the definition of the heuristic function, we have h is assumed Pr  X  r i  X  x  X  ! y  X  X  Pr  X  r i  X  x  X  ! y 9 r j Therefore, h  X  x  X  X  Pr  X  r i  X  x  X  ! y , 8 i A f 1 , ... , n g X  X  Let C be a set of candidate solutions that are to be compared. Then, we use the formula Max i A f 1 , ::: , n g the value for k . We have used the fact that a greater alphabet size and longer strings in R ( x ) usually correspond to a longer super-sequence of them. However, how to determine the best value for k requires further investigation and remains as an open question. 2.3. The IBS_SCS algorithm
In this section the proposed algorithm IBS_SCS is proposed, which is a constructive beam search metaheuristic algorithm. The standard beam search algorithm is a deterministic heuristic tree search. It is similar to the breadth-first search in the sense that it incrementally constructs partial solutions and explores the search tree one level at a time. However, contrary to breadth-first search, it does not keep all the candidate solutions. The maximum number of candidate solutions to keep is called beam size , which we denote as b . Informally speaking, in the extreme case where the beam size is sufficiently large, the algorithm will act as the breadth-first search.
Another extreme case is when the beam size is only 1, in which case it will act as a purely greedy algorithm. The beam search algorithm is also similar to the best-first search in the sense that it also uses a heuristic function to evaluate and compare candidate solutions.
Finally, there exists a scalability issue with the described heuristic function for large problem instances. To be precise, the probability Pr ( r ! y ) decreases rapidly as the length of r becomes close to the length of y , especially when r and y are long strings.
To overcome this issue, we estimate P ( q , k ) with P ( q cut , k cut ), where cut is dynamically determined in such a way that q cut is positive and is either less than or as close as possible to 100. This approximation overcomes the scalability issue of the heuristic function and makes the algorithm sufficiently robust for long biological sequences.

Algorithm 1 presents a high-level pseudo-code of our pro-posed IBS_SCS algorithm for the SCS. As a beam search, the algorithm starts with an initially-singleton (the null string) set B of candidate solutions and incrementally builds longer ones by appending to them alphabet characters from the alphabet S , until a feasible candidate solution is obtained; the feasible solution is then returned and the algorithm terminates. However, (at most) b best candidate solutions are kept, using a heuristic function, and the others are eliminated.

The algorithm consists of an initialization section followed by a while loop, which consists of four steps. In the initialization section, the algorithm constructs an efficient data structure to speed up the calculation of heuristic values. The core idea behind using this data structure is the property of the heuristic function h (.), described in the previous section, that, given an alphabet S , a string r of length q and a random string y of length k , both over S , the probability of r ! y is only dependent on q and k (see
Theorem 1 ). Therefore, we construct a two-dimensional array P such that P [ q ][ k ] holds the probability Pr ( r ! y ). Using dynamic programming, and based on Theorem 1 , the array P is populated by the following recurrence:
P  X  q , k  X  X 
In the initialization section, the set B of candidate solutions is also initialized to a singleton containing the null string only.
Having completed the initialization section, the while loop is run, which consists of four steps. In Step 1, each candidate solution x in B is extended by appending at its right end a character drawn from S , so obtaining 9 S 9 new candidate solutions. The algorithm ends as soon as a feasible solution, determined using the function feasible(.) , is obtained. Every (infeasible) candidate solution is added to a set C , provided that it is  X  X sable X . More specifically, a candidate solution x new obtained by appending a character l to a string x is called usable if the first character of at least one of the strings r i ( x ), i  X  1, y , n ,is l . This is checked by the function usable(.) in the algorithm. Therefore, the set C contains at most b 9 S 9 (infeasible yet usable) candidate solutions. In Step 2, the heuristic values of the candidate solutions in C are computed, based on which the k best candidate solutions comprise a list k _Best_List of potential dominators to be used for dominance pruning. That is, in Step 3, each member of C is checked against the designated best solutions to decide whether it is dominated by any of them, in which case it is discarded from C . A candidate solution x k is dominated by another candidate solution x date solutions in C are compared and the best b of them are selected to construct the new set B of candidate solutions. The proposed algorithm runs in polynomial time in its input size ( n , m , and 9 S 9 ) and the values of the parameters b and k . Proposition. Algorithm IBS_SCS (Algorithm 1) is of complexity is the length of the returned solution .

Proof. In the initialization section before the While loop, the two-dimensional array P is populated using dynamic program-ming. The value of each entry is determined in Y (1), in terms of two entries in its previous rows and columns. As can be seen, there are two nested loops and it takes Y ( m M ) to populate the array. Because M is the maximum value used for k , and that we have used the formula k  X  Max i A f 1 , ::: , n g mine the values of k (see Step 2 inside the While loop), M  X  m lg 9 S 9 . Therefore, the array P is populated in Y  X  m m lg 9 S 9  X  X  Y  X  m 2 lg 9 S 9  X  .

There are four Steps inside the While loop, which we analyze in turn. Step 1 consists of two nested For loops, one iterating O ( b ) times and the other iterating 9 S 9 times. Inside these loops, a feasibility check (using the function feasible(.)) and a usability check (using the function usable(.)) are performed. The feasibility check involves checking whether a given candidate solution, here x , is a supersequence of the input strings. If we keep n indices of p ( x ) associated with each candidate solution x in B , then we only need to update these indices for x new and check whether p ( x in Y ( n ). The usability check returns true if, and only if, p ( x Therefore, Step 1 is run in O ( b 9 S 9 n ).

Step 2 determines the heuristic values for all candidate solu-tions in C, the number of which is O ( b 9 S 9 ). For each candidate solution x in C , the For loop iterates n times. Because of using the
Y (1) inside the For loop, which are all multiplied together, making the heuristic value h k ( x ) for the candidate solution x . Therefore, Step 2 requires O ( b 9 S 9 n ) (which is the same as that of Step 1).

Step 3 first determines the k best solutions in C (stored in the k _Best_List) and then examines each member of C against the designated best solutions for dominance pruning. Recall that there are at most b 9 S 9 candidate solutions in C. Therefore, to determine the k _Best_List can be performed in O ( k b 9 S whether a candidate solution in C is dominated by a member of k _Best_List is performed in O ( n ); hence all the dominance checks are performed in O ( b 9 S 9 k n ). Therefore, the total complexity of Step 3 is O ( b 9 S 9 k n ).

Finally, Step 4 selects the best (at-most) b members of C to construct the new B , which can also be performed in O ( b using red-black trees. 1 Therefore, Steps 1 X 4 inside the while loop are performed in
O ( b 9 tion section is run in Y ( m 2 lg 9 S 9 ) and the While loop iterates L n times, the whole algorithm is run in O ( m 2 lg 9 S 9  X  L b 9
S 9 lg ( b 9 S 9 )), which completes the proof. &amp;
Note that the time complexity of the algorithm is polynomial that m is the length of the longest input string), because, informally speaking-each character of a candidate solution must contribute to covering a character of at least one input string and there are at most a total of n m such characters. Therefore, the time complexity of the algorithm may also be presented as
O ( m provide a tight bound.

How to determine the appropriate values for the parameters k and b depends on the underlying problem. In particular, a larger beam size b usually, but not necessarily, corresponds to a more accurate solution at a greater computational cost. However, if the solution quality using a specific beam size is near the optimal value, there will be not much point further increasing the beam size. On the other hand, using a  X  X  X oo small X  X  beam size may adversely affect the solution quality. In fact, it depends on the underlying problem instance and such factors as what level of accuracy is required and how much run-time is affordable. Similarly, there is no strict rule for determining the best value for k . A larger k corresponds to a more computational time spent for dominance pruning. However, the pruning can lead to the reduction of computational cost that would otherwise be required for processing the pruned sub trees. 3. Results
In this section, we report the results of comparing our proposed algorithm with DR ( Ning and Leong, 2006 ), MA_BS ( Gallardo et al., 2007 ), and PBS ( Blum et al., 2007 ), as three recent algorithms proposed for the SCS problem. Although there are other algorithms proposed in the literature as mentioned earlier in this paper, we do not compare our algorithm with them, because the most significant of them have already been shown to be outperformed by these three recent algorithms as reported in Ning and Leong (2006) , Gallardo et al. (2007) , and Blum et al. (2007) .
 The implementations of DR , MA_BS ,and PBS were not available. The whole datasets used in Gallardo et al. (2007) and Blum et al. (2007) were available, and we used the reported results in Gallardo et al. (2007) ,and( Blum et al., 2007 ) to compare our algorithms with MA_BS and PBS . However, only real instances used in ( Ning and Leong, 2006 ) were available ( http://www.biomedcentral.com/ content/supplementary/1471-2105-7-S4-S12-S1.zip ; http://www. biomedcentral.com/content/supp lementary/1471-2105-7-S4-S12-S2. zip ). For random instances, we used their random instance generator ( http://www-person al.umich.edu/ kning/random.html ), and to compare IBS_SCS with DR on random instances, we implemented DR , precisely based on its specifications in ( Ning and Leong, 2006 ). We implemented DR and IBS_SCS in Java using the Eclipse Platform onaPentiumIVmachinewith2.4GHzclockspeed,2GBofRAM,and 2 MB of L2 cache. We allowed Java to use (at most) 1 GB of RAM.
In order to compare IBS_SCS with DR , the random instances were generated with exactly the same values for the parameters n , m , and 9 S 9 as used in ( Ning and Leong, 2006 ). There are altogether sixteen problem instances; the first eight, which are of relatively smaller numbers and lengths of strings, correspond to those in Table 4 and the other eight correspond to those in Table 1 of Ning and Leong (2006) . The real instances are the DNA/ protein instances used, respectively, in Tables 3 and 5 of ( Ning and Leong, 2006 ). There are altogether 11 datasets, the first six for DNA and the other five for protein sequences, and each datasets includes ten instances. It is important to note that in some of the sequences in the real data, there were characters such as outside the underlying alphabet. Such characters were randomly replaced with one of their candidate characters. We ran IBS_SCS with the parameters b  X  100 and k  X  7. We ran our implementa-tion of DR on the random instances but used the reported results in Ning and Leong (2006) for real instances.

Table 1 provides the comparison of IBS_SCS with DR on random DNA sequences. The first and the second columns show, respectively, the number and the length of the sequences in each problem instance. Each row of the table corresponds to ten problem instances of the specified n and m . The third and the fourth columns report the average length of the string returned by DR and IBS_SCS , respectively, over the ten instances of each row. The fifth column reports the average run-time of IBS_SCS , including the time needed to read in the data files. Finally, the last column calculates the (average) reduction percentage r % in the length of the string by IBS_SCS , defined as r %  X  ( L DR L
DR 100, where L DR and L IBS_SCS denote the average lengths of the strings returned by DR and IBS_SCS , respectively.
As can be seen in Table 1 , IBS_SCS outperforms DR by achieving shorter strings in all the sixteen cases. The reduction percentage r % varies from 0.21 (for the case n  X  5000 and m  X  100) to 7.18 (for the case n  X  10 and m  X  100), with an average of 2.95 (not shown in the table). No complete run-time report was provided in Ning and Leong (2006) ; it was only mentioned that took less than 10 s for the random instance ( n  X  100, m  X  100) and an average of 5 X 10 min for the random instance ( n  X  1000, m  X  1000). The run-time of our (efficient) implementation of observed for these two cases are 20 s and 18091 s (about 30 min), respectively. However, the run-times of IBS_SCS for the corre-sponding instances are 1 s and 81 s (less than two minutes). This suggests that IBS_SCS should be significantly faster than important to note the DR was observed to take more than 27 h for the last instance ( n  X  5000, m  X  1000), for which no run-time was reported in Ning and Leong (2006) . The time taken by IBS_SCS for that instance was 469 s (less than 8 min). Fig. 3 depicts the growth of run-time for both our implemented DR and IBS_SCS with the number of strings n for a fixed sequence length of m  X  100 (our implemented DR and IBS_SCS algorithms are available on request).

Table 2 provides the comparison of IBS_SCS with DR on real biological sequences. The first column in this table represents the dataset name. The definitions for the second to the seventh columns are as those for the first to the sixth columns of
Table 1 . As indicated by Table 2 , IBS_SCS outperforms DR obtaining shorter strings in all the eleven cases. The reduction percentage r % ranges from 2.93 (for DNA-6) to 9.72 (for PROT-1 and PROT-4), with an average of 5.98 (not shown in the table). Again IBS_SCS was observed to be significantly faster than
An interesting observation is that the reduction percentage gained by IBS_SCS is significantly higher for real than random biological sequences. The minimum r % for real instances is, as already-mentioned, 2.93, which is about the average r % (2.95) for random instances. This indicates that IBS_SCS is promising for practical use and further research for this purpose.
 same random and real instances as used in Blum et al. (2007) .The datasets consist of one random and five biological benchmarks.
The random datasets are categorized into 5 classes, each of which is specified with a different alphabet size, namely 2, 4, 8, 16, and 24. Each class contains five instances, and each instance consists of eight strings, four of length 40 and the other four of length 80.
On the other hand, each biological instance is characterized by a biological sequence s and a probability p . More specifically, the strings within each instance are obtained from the same biologi-cal sequence s by removing each of its symbols with a fixed probability p . The number of the strings in each instance is 10.
Five biological sequences each with three probabilities of 0.1, 0.15, and 0.20 have been used to construct a total of 15 instances. The five biological sequences are two SARS Coronavirus
DNA sequences obtained from a genomic database ( http:// gel.ym.edu.tw/sars/genomes.html ) and three protein sequences obtained from Swiss-Prot ( http://www.expasy.org/sprot ). The
DNA sequences are of the lengths 158 and 1269, and the protein sequences are Oxytocin , p53 and Estrogen , which are of the lengths 125, 393, and 595, respectively. The lengths of the optimal
SCSs for these instances are, respectively, 158, 1269, 125, 393, and 595 ( Blum et al., 2007 ).

For MA_BS and PBS , we used the reported results in ( Blum et al., 2007 ). Contrary to IBS_SCS , MA_BS and PBS are not deterministic; they were run more than once in Blum et al. (2007) , and the best, the mean, and the standard deviation of their solution quality and run-time were reported. However, we ran only IBS_SCS once on each instance. We used k  X  7 and b  X  700 for random and b  X  100 for real instances.

Table 3 compares IBS_SCS with MA_BS and PBS on the random datasets. The first column shows the alphabet size. The second and the third columns show the length of the solutions returned by MA_BS and PBS , respectively. The fourth column reports the length of the solutions returned by IBS_SCS . The fifth column reports the average run-time of IBS_SCS . Finally, the last column calculates the reduction percentage r % achieved by IBS_SCS with respect to PBS (which is almost superior to MA_BS ), defined as r %  X  ( L PBS L IBS_SCS )/ L PBS 100, where L and L
IBS_SCS denote the (average) length of the solutions returned by PBS and IBS_SCS , respectively.

As can be seen in Table 3 ,in all the five cases, IBS_SCS outperforms MA_BS and PBS , even with respect to their best runs.
We do not intend to provide a precise run-time comparison, because of using different machines. However, it can be inferred that
IBS_SCS should not be any slower than the other two algorithms; the run-time limit for PBS and MA_BS were reported in Blum et al. (2007) and Gallardo et al. (2007) as to be 350 and 600 s, respectively, whereas the longest run-time of IBS_SCS only 8 s (the last row of Table 3 ). Note that with a smaller beam size of 200, the longest run-time of our algorithm was even less than 1.5 s (not shown in the table), while it still outperformed
PBS in all the five cases. Finally, as can be seen in the last column of Table 3 , IBS_SCS achieves the reduction percentage of 1.35 X 3.52 over PBS , with an average of more than 2.5%. Note that MA_BS outperforms PBS with respect to the average length of the solutions for the case 9 S 9  X  2, but it is still outperformed by IBS_SCS in this case.

The results of experiments over the biological sequences are reported in Tables 4 X 8 . The first columns in these tables show the value for the probability p . The next column shows the maximum length m of input strings. The next three columns show the results of the algorithms MA_BS , PBS , and IBS_SCS , respectively. The last column shows the run-time of IBS_SCS . Tables 4 and 5 report the results of the experiments on the Nucleotide SARS datasets (hence of 9 S 9  X  4), and Tables 6 X 8 provide the results for the Aminoacid protein datasets (hence of 9 S 9  X  20).

As can be seen in Tables 4 X 8 , both MA_BS and PBS obtain optimal solutions in all but the last two datasets of Table 5 and the last dataset of Table 8 , where PBS is outperformed by .

However, IBS_SCS obtains optimal solutions in all the cases in these tables. As shown in the last columns of Tables 4 X 8 , needs, at worst, about a couple of seconds to find the optimal solutions; the run-time limit reported in Gallardo et al. (2007) and Blum et al. (2007) are 600 and 350 s, respectively. with the parameters k and b , we conducted two more types of experiments, on the datasets of Table 3 . In the first series of experiments, we used the same value of 7 for k but used the values 100, 400, 700, 1000, and 1300 for b . We used the results for the case b  X  700 as the reference and calculated the percentages of the changes due to using the other values of b . To be precise, for each value v  X  100, 400, 1000, 1300, we calculated ( L 2 L
L 1 100, where L 2 and L 1 are the lengths of the solutions obtained using, respectively, b  X  v and b  X  700. Because there are five instances for each alphabet size 9 S 9 in Table 3 , we then averaged the percentages of changes over the five instances of each alphabet size.
 shows the alphabet size. The second and the third columns show, respectively, the average and the variance of the percentages of changes due to using b  X  100, as opposed to 700, over the five instances with 9 S 9  X  2. These values are denoted by D and V , respectively. The next three pairs of columns report the respective values for the other beam sizes of 400, 1000, and 1300. The last row shows the average of the values D over all the instances. sensitive to the specified beam size values in that the average percentage of the changes ( D ) is under 1% in majority of cases. It is also observed that there is no obvious pattern for changing the results with the beam size. However, on average (shown in the last row of the table), the worst results correspond to the smallest beam size 100 (with the average D of 1.28%).
 with the beam size, for different values of alphabet size. As can be seen in Fig. 4 , the curves tend to fall with increasing the beam size, but a number of exceptions are also observed, e.g. for 9 S size of b  X  700 but different values of 3, 5, 7, 9, and 11 for the parameter k . Similarly, we used the results for the case ( b  X  700 and) k  X  7 as the reference and calculated the percentages of the changes due to using the other values of k . More specifically, for each value v  X  3, 5, 9, 11, we calculated ( L 2 L 1 )/ L 1 and L 1 are the lengths of the solutions obtained using, respec-tively k  X  v and k  X  7. Then, we averaged the percentages of changes D over the five instances of each alphabet size. similar to that of Table 9 ,exceptthattheresultsin Table 10 are presented for different values of k , as opposed to b .Asshownin Table 10 , except for two cases of k  X  3and k  X  5inthelastrow 9
S 9  X  24, the percentage of the changes is under 1%. This suggests that the algorithm is sufficien tly tolerant to the choice of k . However, the solution quality usually increases with increasing k .
This is better observed in Fig. 5 , which shows how the average percentage of changes D varies with k , for different values of alphabet size. As can be seen in Fig. 5 , the curves tend to fall with increasing k , although there are still exceptions, e.g. for the case 9
S 9  X  8. 4. Conclusion
In this paper, a deterministic heuristic algorithm for the shortest common supersequence problem was proposed. The algorithm is a constructive beam search and uses a heuristic function different from those already proposed in the literature for the SCS problem. The algorithm also uses the dominance property to effectively prune the search tree. However, it does not check for dominance with respect to every existing candidate solution as it would lead to a significant time-consumption. Neither is it restricted to using only the best solution found so far as it would then be not using the true power of dominance pruning. Instead, it selects the k best solutions found so far as potential dominators of candidate solutions at each iteration, where k is a control parameter in the algorithm. The proposed algorithm was compared with three recent algorithms proposed for the problem on both simulated and real biological sequences. It outperformed all the three algorithms in all of the experimental cases. This justifies that the proposed algorithm is promising for further research and improvements.

Possible avenues for future work include (i) to devise a method to determine appropriate values for k (see Eq. (3)), because its proper setting can significantly improve the solution quality and there is enough room for improvement in this regard, (ii) to generalize the employed heuristic to the case where the input strings are correlated to further improve the performance of the algorithm in such domains, and (iii) to dynamically determine the appropriate values for the control parameters k and b . Acknowledgment
The authors would like to thank Dr. C. Blum, Dr. J.E. Gallardo and Dr. C. Cotta for kindly providing us with their random and real datasets. We would also like to thank Dr. K. Ning and Dr. H.W. Leong for their real benchmarks and random-instance generator. Thanks are also due to the anonymous reviewers for their constructive comments.
 Appendix A. Supplementary materials
Supplementary data associated with this article can be found in the online version at doi:10.1016/j.engappai.2011.08.006 . References -1 -0.5 0.5 1.5 2.5
