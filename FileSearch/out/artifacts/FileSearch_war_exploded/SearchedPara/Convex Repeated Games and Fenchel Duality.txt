 Several problems arising in machine learning can be modeled as a convex repeated game. Convex repeated games are closely related to online convex programming (see [ 19 , 9 ] and the discussion in the last section). A convex repeated game is a two players game that is performed in a sequence of consecutive rounds. On round t of the repeated game, the first player chooses a vector w t from a convex set S . Next, the second player responds with a convex function g t : S  X  R . Finally, the player. The goal of the first player is to minimize its cumulative loss, P t g t ( w t ) . convex repeated game. Online learning is performed in a sequence of consecutive rounds. On round question. For example, x t can be an encoding of an email message and the question is whether the email is spam or not. The prediction of the learner is performed based on an hypothesis, h t : X  X  X  , where X is the set of questions and Y is the set of possible answers. In the aforementioned example, Y would be { +1 ,  X  1 } where +1 stands for a spam email and  X  1 stands for a benign one. After come from a parameterized set of hypotheses, H = { h w : w  X  S } . For example, the set of linear classifiers, which is used for answering yes/no questions, is defined as H = { h w ( x ) = we can say that the learner chooses a vector w t and its hypothesis is h w t . Next, we note that once the hypotheses space or equivalently over the set of parameter vectors S . We can therefore redefine the online learning process as follows. On round t , the learner chooses a vector w t  X  S , which defines a hypothesis h w t to be used for prediction. Then, the environment chooses a question-g therefore described the process of online learning as a convex repeated game.
 of rounds T and a fixed vector u  X  S , we define the regret of the first player as the excess loss for not consistently playing the vector u , Our main result is an algorithmic framework for the first player which guarantees low regret with respect to any vector u  X  S . Specifically, we derive regret bounds that take the following form where f : S  X  R and L  X  R + . Informally, the function f measures the  X  X omplexity X  of vectors in defer the exact requirements we impose on f and L to later sections.
 Our algorithmic framework emerges from a representation of the regret bound given in Eq. ( 1 ) using an optimization problem. Specifically, we rewrite Eq. ( 1 ) as follows That is, the average loss of the first player should be bounded above by the minimum value of an optimization problem in which we jointly minimize the average loss of u and the  X  X omplexity X  of u as measured by the function f . Note that the optimization problem on the right-hand side of Eq. ( 2 ) can only be solved in hindsight after observing the entire sequence of loss functions. Nevertheless, bound for a minimization problem.
 The notion of duality, commonly used in convex optimization theory, plays an important role in obtaining lower bounds for the minimal value of a minimization problem (see for example [ 14 ]). By generalizing the notion of Fenchel duality, we are able to derive a dual optimization problem, which bounds we make an immediate use of the fact that dual objective lower bounds the primal objective. We therefore reduce the process of playing convex repeated games to the task of incrementally increasing the dual objective function. The amount by which the dual increases serves as a new and of the first player, and the increase in the dual.
 The rest of this paper is organized as follows. In Sec. 2 we establish our notation and point to a few mathematical tools that we use throughout the paper. Our main tool for deriving algorithms for playing convex repeated games is a generalization of Fenchel duality, described in Sec. 3 . Our algorithmic framework is given in Sec. 4 and analyzed in Sec. 5 . The generality of our framework allows us to utilize it in different problems arising in machine learning. Specifically, in Sec. 6 we underscore the applicability of our framework for online learning and in Sec. 7 we outline and analyze boosting algorithms based on our framework. We conclude with a discussion and point to related work in Sec. 8 . Due to the lack of space, some of the details are omitted from the paper and can be found in [ 16 ]. We denote scalars with lower case letters (e.g. x and w ), and vectors with bold face letters (e.g. x and w ). The inner product between vectors x and w is denoted by  X  x , w  X  . Sets are designated by upper case letters (e.g. S ). The set of non-negative real numbers is denoted by R + . For any The dual norm is defined as k  X  k ? = sup { X  x ,  X   X  : k x k  X  1 } . For example, the Euclidean norm, k x k  X  = max i | x i | .
 We next recall a few definitions from convex analysis. The reader familiar with convex analysis may proceed to Lemma 1 while for a more thorough introduction see for example [ 1 ]. A set S is neighborhood lying in S . A set S is closed if its complement is an open set. A function f : S  X  R is closed and convex if for any scalar  X   X  R , the level set { w : f ( w )  X   X  } is closed and convex. f is closed and convex then the Fenchel conjugate of f ? is f itself. The Fenchel-Young inequality then  X  X  ( w ) consists of a single vector which amounts to the gradient of f at w and is denoted by the following lemma states that if  X   X   X  X  ( w ) then Fenchel-Young inequality holds with equality. for all  X  0  X   X  X  ( w 0 ) we have, f ( w 0 ) + f ? (  X  0 ) =  X   X  0 , w 0  X  .
 A continuous function f is  X  -strongly convex over a convex set S with respect to a norm k X k if S is contained in the domain of f and for all v , u  X  S and  X   X  [0 , 1] we have Strongly convex functions play an important role in our analysis primarily due to the following lemma.
 Lemma 2 Let k  X  k be a norm over R n and let k  X  k ? be its dual norm. Let f be a  X  -strongly arg max x  X  S  X   X  , x  X  X  X  f ( x ) . Furthermore, for any  X  ,  X   X  R n we have Two notable examples of strongly convex functions which we use are as follows.
 norm. Its conjugate function is f ? (  X  ) = 1 2 k  X  k 2 2 .
 simplex, S = { w  X  R n + : k w k 1 = 1 } , with respect to the ` 1 norm. Its conjugate function is f (  X  ) = log( 1 n P n i =1 exp(  X  i )) . In this section we derive our main analysis tool. We start by considering the following optimization problem, where c is a non-negative scalar. An equivalent problem is constraint w t = w 0 , we obtain the following Lagrangian The dual problem is the task of maximizing the following dual objective value, D (  X  1 , . . . ,  X  T ) = inf f, g 1 , . . . , g T . Therefore, the generalized Fenchel dual problem is Note that when T = 1 and c = 1 , the above duality is the so called Fenchel duality. In this section we describe a template learning algorithm for playing convex repeated games. As mentioned before, we study convex repeated games from the viewpoint of the first player which we shortly denote as P1. Recall that we would like our learning algorithm to achieve a regret bound of the form given in Eq. ( 2 ). We start by rewriting Eq. ( 2 ) as follows where c = optimum of the minimization problem on the right-hand side of Eq. ( 5 ). In the previous section we derived the generalized Fenchel dual of the right-hand side of Eq. ( 5 ). Our construction is based on the weak duality theorem stating that any value of the dual problem is smaller than the optimum value of the primal problem. The algorithmic framework we propose is therefore derived by incrementally ascending the dual objective function. Intuitively, by ascending the dual objective we move closer to the optimal primal value and therefore our performance becomes similar to the performance of the best fixed weight vector which minimizes the right-hand side of Eq. ( 5 ).  X  -strongly convex. Therefore, based on Lemma 2 , the function f ? is differentiable. At trial t , P1 uses for prediction the vector dual variables as follows. Denote by  X  t the differential set of g t at w t , that is, two conditions: In the next section we show that condition (i) ensures that the increase of the dual at trial t is at trial t without any knowledge on the yet to be seen loss functions g t +1 , . . . , g T . We conclude this section with two update rules that trivially satisfy the above two conditions. The first update scheme simply finds  X  0  X   X  t and set The second update defines In this section we analyze the performance of the template algorithm given in the previous section. Our proof technique is based on monitoring the value of the dual objective function. The main result is the following lemma which gives upper and lower bounds for the final value of the dual objective function.
 Lemma 3 Let f be a  X  -strongly convex function with respect to a norm k  X  k over a set S and w where  X  0 t  X   X  t for all t , such that Proof The second inequality follows directly from the weak duality theorem. Turning to D (  X  T +1 1 , . . . ,  X  T +1 T ) can be rewritten as  X  t  X   X  c ( f ? (  X  t  X   X  that Since  X  0 t  X   X  t and since we assume that g t is closed and convex, we can apply Lemma 1 to get that that Combining the above inequality with Eq. ( 11 ) concludes our proof.
 The following regret bound follows as a direct corollary of Lemma 3 .
 Theorem 1 Under the same conditions of Lemma 3 . Denote L = 1 T P T t =1 k  X  0 t k 2 ? . Then, for all w  X  S we have, In particular, if c = In Sec. 1 we cast the task of online learning as a convex repeated game. We now demonstrate the applicability of our algorithmic framework for the problem of instance ranking. We analyze Recall that on each online round, the learner receives a question-answer pair. In instance ranking, the question is encoded by a matrix X t of dimension k t  X  n and the answer is a vector y t  X  R k t . the i  X  X h row of X t ahead of the j  X  X h row of X t . We also interpret y t,i  X  y t,j as the confidence in which the i  X  X h row should be ranked ahead of the j  X  X h row. For example, each row of X t en-compasses a representation of a movie while y t,i is the movie X  X  rating, expressed as the number of stars this movie has received by a movie reviewer. The predictions of the learner are determined based on a weight vector w t  X  R n and are defined to be  X  y t = X t w t . Finally, let us define two loss functions for ranking, both generalize the hinge-loss used in binary classification prob-x ( i, j )  X  E t . If this is not the case, we are being penalized according to some combination of the ` avg ( w ; ( X t , y t )) = 1 (see for example [ 18 ]). Another popular approach (see for example [ 5 ]) penalizes according to the maximal loss over the individual pairs, ` max ( w ; ( X t , y t )) = max ( i,j )  X  E We can apply our algorithmic framework given in Sec. 4 for ranking, using for g t ( w ) either ` dition under which the regret bound from Thm. 1 holds for ranking as well.
 Theorem 2 Let f be a  X  -strongly convex function over S with respect to a norm k X k . Denote by L g ( w ) = ` max ( w ; ( X t , y t )) , the following regret bound holds In this section we describe the applicability of our algorithmic framework to the analysis of boosting algorithms. A boosting algorithm uses a weak learning algorithm that generates weak-hypotheses whose performances are just slightly better than random guessing to build a strong-hypothesis which can attain an arbitrarily low error. The AdaBoost algorithm, proposed by Freund and Schapire [ 6 ], taken from an instance domain X , and y i is a binary label, y i  X  { +1 ,  X  1 } . The boosting process w , over the set of examples. Then, the booster passes the training set along with the distribution w t to the weak learner. The weak learner is assumed to return a hypothesis h t : X  X  X  +1 ,  X  1 } whose average error is slightly smaller than 1 2 . That is, there exists a constant  X  &gt; 0 such that, The goal of the boosting algorithm is to invoke the weak learner several times with different distri-butions, and to combine the hypotheses returned by the weak learner into a final, so called strong, hypothesis whose error is small. The final hypothesis combines linearly the T hypotheses returned h w most exp(  X  2  X  2 T ) .
 the exp-loss function, defined as exp(  X  y h f ( x )) , is a smooth upper bound of the zero-one error, which equals to 1 if y h f ( x )  X  0 and to 0 otherwise. Thus, we can restate the goal of boosting as To simplify our derivation in the sequel, we prefer to say that boosting maximizes the negation of the loss, that is, learner to be general functions into the reals, h t : X  X  R (see for instance [ 15 ]). In this paper we view boosting as a convex repeated game between a booster and a weak learner. To motivate our construction, we would like to note that boosting algorithms define weights in two different domains: the vectors w t  X  R m which assign weights to examples and the weights {  X  t : t  X  [ T ] } over weak-hypotheses . In the terminology used throughout this paper, the weights w t  X  R m are primal vectors while (as we show in the sequel) each weight  X  t of the hypothesis h t primal problem for a convex repeated game, thus the algorithmic framework described thus far for playing games naturally fits the problem of iteratively solving Eq. ( 14 ).
 To derive the primal problem whose Fenchel dual is the problem given in Eq. ( 14 ) let us first denote g w t is a distribution over the m examples (as is the case in AdaBoost), g t ( w t ) reduces to 1  X  2 t (see Eq. ( 13 )). In this case, minimizing g t is equivalent to maximizing the error of the individual f ( w ) is the relative entropy given in Example 2 and c = 1 / (2  X  ) (see Eq. ( 13 )). To derive its g dual, we can restrict  X  t to take the form  X  t =  X  t v t =  X  t 2  X  v t , and get that Minimizing the exp-loss of the strong hypothesis is therefore the dual problem of the following primal minimization problem: find a distribution over the examples, whose relative entropy to the uniform distribution is as small as possible while the correlation of the distribution with each v t is as small as possible. Since the correlation of w with v t is inversely proportional to the error of h t with respect to w , we obtain that in the primal problem we are trying to maximize the error of each individual hypothesis, while in the dual problem we minimize the global error of the strong hypothesis. The intuition of finding distributions which in retrospect result in large error rates of individual hypotheses was also alluded in [ 15 , 8 ].
 We can now apply our algorithmic framework from Sec. 4 to boosting. We describe the game the beginning of the game the booster sets all dual variables to be zero,  X  t  X  t = 0 . At trial t of the boosting game, the booster first constructs a primal weight vector w t  X  R m , which assigns importance weights to the examples in the training set. The primal vector w t is constructed as in We have thus obtained a variant of AdaBoost in which the weights  X  t are capped above by 2  X  . A disadvantage of this variant is that we need to know the parameter  X  . We would like to note in details due to the lack of space.
 To analyze our game of boosting, we note that the conditions given in Lemma 3 holds and therefore the left-hand side inequality given in Lemma 3 tells us that P T t =1 g t ( w t )  X  is [+1 ,  X  1] we get that k  X  0 t k  X   X  1 . Combining all the above with the left-hand side inequality We presented a new framework for designing and analyzing algorithms for playing convex repeated games. Our framework was used for the analysis of known algorithms for both online learning and boosting settings. The framework also paves the way to new algorithms. In a previous paper [ 17 ], we suggested the use of duality for the design of online algorithms in the context of mistake bound analysis. The contribution of this paper over [ 17 ] is three fold as we now briefly discuss. First, we generalize the applicability of the framework beyond the specific setting of online learning with the hinge-loss to the general setting of convex repeated games. The setting of convex repeated games was formally termed  X  X nline convex programming X  by Zinkevich [ 19 ] and was first pre-sented by Gordon in [ 9 ]. There is voluminous amount of work on unifying approaches for deriving of this paper. By generalizing our previously studied algorithmic framework [ 17 ] beyond online learning, we can automatically utilize well known online learning algorithms, such as the EG and p-norm algorithms [ 12 , 11 ], to the setting of online convex programming. We would like to note that the algorithms presented in [ 19 ] can be derived as special cases of our algorithmic framework other algorithmic framework for online convex programming that is closely related to the potential based algorithms described by Cesa-Bianchi and Lugosi [ 3 ]. Gordon also considered the problem of defining appropriate potential functions. Our work generalizes some of the theorems in [ 10 ] while providing a somewhat simpler analysis.
 Second, the usage of generalized Fenchel duality rather than the Lagrange duality given in [ 17 ] en-ables us to analyze boosting algorithms based on the framework. Many authors derived unifying frameworks for boosting algorithms [ 13 , 8 , 4 ]. Nonetheless, our general framework and the connec-tion between game playing and Fenchel duality underscores an interesting perspective of both online learning and boosting. We believe that this viewpoint has the potential of yielding new algorithms in both domains.
 Last, despite the generality of the framework introduced in this paper, the resulting analysis is more distilled than the earlier analysis given in [ 17 ] for two reasons. (i) The usage of Lagrange duality in [ 17 ] is somehow restricted while the notion of generalized Fenchel duality is more appropriate to the general and broader problems we consider in this paper. (ii) The strongly convex property we employ both simplifies the analysis and enables more intuitive conditions in our theorems. For instanc, our framework can naturally be used for the analysis of other settings such as repeated games (see [ 7 , 19 ]). The applicability of our framework to online learning can also be extended to other prediction problems such as regression and sequence prediction. Last, we conjecture that our primal-dual view of boosting will lead to new methods for regularizing boosting algorithms, thus improving their generalization capabilities.

