 High performance computing on various multi-core processors remains as a challenge in the software industry. Amidst the presence and growth of CPU based parallel and distributed computing, the field of General Purpose Computation on Graphics Processing Unit (GPGPU) has shown tremendous achievements in increasing the computational speed by few folds. [1, 2, 9, 10, 11]. 1.1 Computing Trends and Challenges Using GPU The launch of NVIDIA X  X  Compute Unified Device Architecture (CUDA) technology is a catalyst to the phenomenal growth of the application of Graphics Processing Units needed in invoking the internal parallel processors of a GPU should be viable to Data mining programmers who might not be expert Graphics Programmers. The intension of this work is to implement Hierarchical Agglomerative Clustering (HAC) algorithms using CUDA and demonstrate the speed gains. 1.2 Graphics Processors and CUDA Technology The GPU is designed to perform computations with extreme speed. The raw computational power of GPU can be expressed and compared with that of the CPU in terms of  X  X eak floating-point operations per second X  and  X  X emory Bandwidth X . Fig. 1 shows the growth trend of computational power of the GPU and the CPU in terms of Peak Giga Flops (GFlops). The NVIDIA 8800 GTS GPU has a Peak performance of 624 GFlops and Memory Bandwidth of 62 Giga Bytes per second whereas a 3.0GHz Dual Core Intel CPU has a Peak Floating point rate of 12 GFlops and Memory Bandwidth of 12.8 Giga Bytes per second. Such form of raw power of the Graphics hardware is thus available to be utilized for non-image processing related computations [4, 8, 10]. 2.1 HAC Algorithms HAC is a common and important algorithm used in Data mining in the domains of micro array analysis, genome clustering, image processing and web page clustering. Hierarchical clustering seeks to build up a hierarchy of clusters. In the agglomerative clusters are merged and the hierarchy moves up [3]. A measure of similarity between the clusters is required to decide which clusters should be merged. We use the Euclidean distance as the metric to determine the similarities between pair of clusters. [7, 8] where the vector norms a i and b i can be calculated using Equation (1), where n is the number of cluster vectors to be merged. Selection of a pair of clusters to merge depends on the linkage criteria. There are two commonly used linkage criteria and the type of HAC depends on the linkage criteria used to merge the pair of clusters. The criteria used by the complete linkage clustering and the single linkage clustering are given in Equation (2) and Equation (3) respectively. updated. The centroid of the individual cluster vectors within the merged pair is computed to replace the original cluster vectors. The centroid C j of k cluster vectors to be merged can be computed using Equation (4). The resultant hierarchy of clusters can be presented as a dendrogram. In this research paper, we intend to implement and analyze the results of HAC based on complete linkage and the single linkage methods. The HAC single linkage method has been previously implemented using CUDA [3]. We find that the merging of clusters was clusters selected for the merge and that short coming is rectified. 2.2 HAC Implementations Using CUDA The computational steps that can be made parallel are implemented on the Graphics processor. Table 1 summarizes the functions used in the implementation of HAC complete linkage method using CUDA on GPU. This implementation architecture is common for both the single linkage with Centroids and the complete linkage methods. Understanding the CUDA architecture is vital in effectively implementing computational algorithms on the GPU and is well explained. [1, 3, 5] The CUDA implementations of the HAC Algorithms are executed and tested on a NVIDIA GeForce 8800 GTS GPU with a memory of 512MB. The corresponding CPU implementation is run on a desktop computer with Pentium Dual Core CPU, 1.8 GHz. 1.0GB RAM on MS Windows Professional 2002. Gene expressions of Human Mammary Epithelial Cells (HMEC) with 60000 Genes and 31 features each were used to evaluate the performance comp ared to the CPU implementation. This microarray dataset has been obtained from experiments conducted on HMEC. The performance of the GPU over the CPU is expressed as computational  X  Speed Gain  X , which is simply the ratio between the CPU computational time and the GPU computational time. The GPU computational time includes the time taken to transfer the input vectors from the CPU to the GPU and transfer cluster results back to CPU. 3.1 Determination of Optimal Block Size Based on Speed Gains One of the parameter that affects the computational performance of GPU is the Block size. Block size in CUDA determines the number of threads to be invoked during run time. A block size of 8 invokes 8 x 8 = 64 threads during runtime which could be run in parallel. Each thread independently operates on a vector and thus exploits the implementing the complete linkage HAC algorithm on the GPU with various block sizes using 5000 genes versus different dimensions. Results show that the block size of 8 is optimal for any selected number of dimensions which was used further. 3.2 Speed Gain Profile Using the Gene Expression Data Set Fig. 3 shows the Speed Gain versus the number of Genes with 31 features for both the the single linkage method can be about 44 times faster than the CPU implementation when there are 10000 Genes to be clustered, whereas the complete linkage method reaches only about 20 times the speed of the CPU implementation above which the CPU took too long to complete, hence aborted. 3.3 Effect of Gene Size and Dimensions on Speed Gain in HAC Single Linkage Significant resources of the GPU are utilized for the calculation of half distance matrices. For this experiment with single linkage HAC algorithm, the number of dimensions is artificially increased and the computational time taken is measured for 10000 Genes. Fig. 4 shows the effect of increase in dimensions of Genes on computational Speed Gains and the % of time taken to compute the half similarity matrices. It can be noticed that speed up to about 90 times is gained at low dimensions performance with 6 and 31 dimensions. Data mining algorithms are often computationally intense and repetitive in nature which exhibit rich amounts of data parallelism. Data parallelism is a characteristic of a computational program whereby arithmetic operations can be performed on data vectors simultaneously. The inherent parallelism in the graphics hardware is invoked by CUDA features. Fig. 6 shows the CUDA architecture and the hardware resources which are used in the invocation of HAC computations on the GPU [5]. 4.1 CUDA Process Block Size and Threads Each CUDA processing block is run on one Multiprocessor (MP). In an 8800 GTS GPU there are 16 such MPs with 8 internal processors each that makes the total number of internal processors to 128. Thus while using a block size of 8, the use of 8 during runtime. Internal processors whic h do not belong to a block cannot be accessed by that block. So there will be a maximu m of 8 execution cycles to process 64 threads while the block size is 8. If 8 such blocks can be used simultaneously, then all the 128 internal processors can be used simultaneously, thus fully harnessing the power of the GPU. This also explains why the Speed Gains with block size of 8 is high. 
There is no direct control possible at block-level for the programmer and the block allocation to internal processors cannot be determined. When a grid is launched, threads created per block, where n is the number of observations and k is the possible number of threads per block. In Hierarchical clustering,  X  n * n /2 X  is the size of the half-similarity matrix, which is also the number of threads needed to simultaneously operate the entire matrix. In this HAC implementation only one block is used per grid. Hence only 1 MP is used and thus for block size 8, the number of threads invoked per block is 64. For the total number of threads generated to be invoked in the block, only  X  k * number of blocks  X  will be executed simultaneously. Though the total number of Within a grid, a number of blocks used will be queued up with threads and allocated to processors in a MP. 
The CUDA program should use 16 or 12 or 4 blocks to fill the GPU depending on more blocks within the grid. When a grid is launched, the number of blocks processing is equal to  X  number of MP used *8 X . The number of block is designated as twice as the number of MP because computations in some of the blocks may finish earlier than the others. When a computational queue is complete, the processor will be idle and that is a waste. One way to overcome this issue is to use multiple blocks thus managing and utilizing the hardware resources of the GPU more effectively. 4.2 Analysis of Threads in CUDA for Data Parallelism The number of threads invoked via a program is dependent on the algorithmic design. For example, for computing the vector distance of array A and array B of size n , there design theoretically would provide maximum parallelization. In the distance computations, n threads are arranged in a way to naturally group into blocks of similar size, satisfying the relation:  X  n = number of threads = ( number of blocks ) * ( number of threads per blocks ) X . 
Each thread in a block is given a unique thread-index , in order to identify threads conceived as follows:  X  thread ID = ( block-index, thread-index ) X  where block-index is unique among any block. Blocks are organized into a grid. Thread-index and block-index may be formed of one, two or three dimensions. For the computations in HAC methods, it is found easier to conceive a one-dimensional grid [3]. We implemented single linkage centroid and the complete linkage HAC methods. computational speed gain on HAC single linkage method is almost twice as obtained for the complete linkage method. This is due to the fact that the identifying maximum distance pair needs a custom developed function whereas the identification of minimum distance pair uses the built in CUDA library (cublasIsamin) function. The issues rising from the implementation of HAC methods using CUDA have been discussed and generalized. The optimal block size for CUDA processing on GPU with 128 internal processors should be 8. Maximum number of observations that can be currently clustered is limited by the size of distance matrix. Future plans include the use of  X  X ultiple Blocks X  and implementing variants of HAC algorithm. 
