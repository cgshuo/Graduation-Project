 1. Introduction
The vast size of the World Wide Web (WWW) nowadays makes it the largest database ever existed. Back to the beginning of this decade it was estimated to contain over 350 million pages [4] while a couple of years ago, it had been estimated that only the indexed part of WWW by a web search engine consists of at least 11.3 billion pages [15]. Every attempt to shape this huge volume of data, that follows a very loose schema, is quite difficult and extreme challenging. According to [10] the appli-cation of data mining techniques in order to extract useful information that implicitly lay among web data is a very essential task. Web data may be either web data pages or data describing the activity of users. Actual web data consists of the web pages, the web page structure, the linkage structure between the web pages, the surfing navigational behavior of the users and the user profiles including demographic and registration information about the users [10,41] .

Web data mining can be divided in three general categories: web content mining, web structure mining and finally web usage mining [49]. Here we focus to the later area of web data mining that tries to exploit the navigational traces of the users in order to extract knowledge about their preferences and their behavior. The task of modeling and predicting a user X  X  nav-igational behavior on a web site or on a web domain can be useful in quite many web applications such as web caching [33,43] , web page recommendation [8,5], web search engines [2,36] and personalization [11]. Consult [13] for several other applications of web usage mining.

According to [13] most of the web usage mining techniques are based on association rules, sequential patterns and clus-tering. The approaches based on association rules build a set of rules of the form: page 1 : html ; page 2 : html ! page 3 : html which means that users that have visited page1 and page2 have also visited page3. Methods that use association rules can be found in [28,31,22,45] . Sequential patterns maintain navigational patterns as sequences and try to discover frequent subse-quences that describe better the data. Sequences are either used to produce association rules or to produce tree structures or Markov chains to represent navigation patterns. Methods proposed in [28,35,34,27,9,33,47] fall into this general category.
Lately, in [50,37,16] the authors proposed the use of web access motifs and string matching techniques. The idea behind the use of these methods is that string algorithmics seem to adapt quite well in the analysis of hypertext navigation. Also, clustering techniques have been used to group similar user sessions according to a distance function, like longest common subsequence [1] or sequence alignment [21]. More complex methods have been proposed in [39,48,38] in order to enhance the results of the clustering method. Recently, various techniques have been proposed that exploit either the web access se-quences in combination with knowledge extracted by the content of the web pages. More specifically in [17] they tackle the problem by extracting association rules from the web server log files using content based knowledge, by means of clustering the web pages by content, in order to enhance the quality of the association rules. In [29], they try to combine web access log files with web content, by extracting and using N -grams from the web pages.

In this paper we propose two novel methods for modeling the user navigation history. Both methods are based upon several ideas from the existing literature and upon a string processing structure used previously in computational biology problems; the weighted suffix tree [23]. The provided estimation of user X  X  navigational intention can be exploited either in an on-line recommendation system in a web site or in a web page cache system. The methods proposed here have the advantage that they demand a constant amount of computational effort per one user X  X  action and consume a relatively small amount of extra memory bytes. These features make our method ideal for an on-line working environment. The first method, which its preliminary version presented in [30], exploits knowledge extracted only from user access sequences from the web server log file. All the access sequences are grouped according to how they look alike together or not. Each one of the pro-duced groups is then summarized and treated as an access pattern from the recommendation model. The second method enhances the first one by utilizing web page content during the phase of access pattern extraction. The content of a web page capture crucial information of how two or more similar web pages can play semantically the same role in a set of access sequences. To achieve this extra knowledge extraction some classic content similarity measures combined with sequence similarity measures.

The structure of the paper is as follows. In Section 2, we present some basic definitions and background information. Sec-tion 3 presents the methods proposed by this paper. In Section 4, we describe the methodology followed to evaluate our methods and we present experimental results. Finally in Section 5, we conclude and discuss future directions. 2. Definitions and background
Let R be a finite alphabet which consists of a set of characters (or symbols). The cardinality of an alphabet, denoted by j R j , expresses the number of distinct characters in the alphabet. A string or word is a sequence of zero or more characters drawn from an alphabet. The set of all words over the alphabet R is denoted by R w  X  1 :: n  X  w  X  1 w  X  2 ... w  X  n , where w  X  i 2 R for 1
A subword u of length p is said to occur at position i in the word w if u  X  w  X  i :: i  X  p 1 . In other words u is a substring of length p occurring at position i in word w . A word has a repeat when it has two equal subwords. A subword u of w is called a cover of w if and only if w can be constructed by concatenations and superpositions of u , so that every position of w lies with-in some occurrence of u in w . In the case that for a given position of a word w we consider the presence of a set of characters each with a given probability of appearance, we have the concept of a weighted sequence X .

A weighted sequence/word X  X  X  X  1 X  X  2 ... X  X  n is a sequence of positions, where each position X  X  i consists of a set of or-position X  X  i ; 1 6 i 6 n ; P p  X  r  X  X  1.

For example, if we consider the DNA alphabet R  X f A ; C ; G ; T g , the sequence X shown in Fig. 1 represents a word having 11 of the words that can finally be produced are: w 1=ACTTATCATTT, w 2=ACTTCTCATTT1, etc. The probability of presence of a word is the cumulative probability, which is calculated by multiplying the relative probabilities of appearance of each char-subsequences.

The suffix tree is a fundamental data structure supporting a wide variety of efficient string processing algorithms. In par-ticular, the suffix tree is well known to allow efficient and simple solutions to many problems concerning the identification and location either of a set of patterns or repeated substrings (contiguous or not) in a given sequence. The reader can find an extended literature on such applications in [18].
 below v . Linear time algorithms for suffix tree construction are presented in [18].

There is a data structure for storing the set of suffixes of a weighted sequence with probability of appearance greater than 1 = k , where k is a given constant; that makes use as fundamental data structure the suffix tree, incorporating the notion of probability of appearance for every suffix stored in a leaf. Thus, the introduced data structure is called the weighted suffix tree (abbrev. WST). The weighted suffix tree can be considered as a generalization of the ordinary suffix tree in order to han-dle weighted sequences by incorporating the notion of probability of appearance for every suffix stored in a leaf. A construc-tion of this structure has been proposed in [23], plus a set of several applications.

Let X be a weighted sequence. For every suffix starting at position i we define a list of possible weighted subwords so that the probability of appearance for each one of them is greater than 1 = k . We denote each of them as X rank in arbitrary numbering. We define WST  X  X  X  the weighted suffix tree of a weighted sequence X , as the compressed trie of a portion of all the weighted subwords starting within each suffix X the path from the root to v . Leaf v of WST  X  X  X  is labeled with index i if 9 j &gt; 0 such that L  X  v  X  X  X where j &gt; 0 denotes the j th weighted subword starting at position i .

We will use an example to illustrate the above definition. Consider again the weighted sequence shown in Fig. 1 and sup-pose that we are interested in storing all suffixes with probability of appearance greater than a predefined parameter. We will construct the suffix tree for the sequence incorporating the notion of probability of appearance for each suffix. For the sequence in Fig. 1 with X i ; j P 1 = 4, for some subword X for suffix X  X  1 ... 11 : X 1 ; 1  X  ACTTATCATTT ; p  X  X 1 ; 1
X  X  2 ... 11 : X 2 ; 1  X  CTTATCATTT ; p  X  X 2 ; 1  X  X  0 : 25, and X above subwords appears in Fig. 2 .

We suppose that the navigation history in a web site is maintained as web access sequences (WAS). Web access se-quences are sequences of web pages that express each session (clicking sequence) of the users and can be generated by the log files of the web site; if we represent each web page as a symbol in an appropriate alphabet then the WAS is essentially a string. We assume that each page of the website is named by a unique character that is used to produce the WASes. 3. Prediction/recommendation model
In our model we assume that each navigation session is described by a WAS set of all existing web access sequences that have taken place in the web site. A sketchy description of the function of our method follows: In an offline manner, running either in an idle time period or in the background, the system processes the set S of the navigation sessions in order to group them in clusters. Each of the clusters contains sequences very similar to each other. Then each of the clusters C i is represented by a weighted sequence WS eralized weighted suffix tree gWST( i : WAS i ) is constructed. This structure is further maintained and used as a web page pre-diction/recommendation tool. The overall steps of preprocessing are shown in Fig. 3 .
 Each one of the preprocessing steps is going to be described in a more detailed way in the following sections. 3.1. WAS maintenance
Each of the user navigation sessions is implicitly maintained in the log files of the web sites server. Either we program properly the web server to store each WAS in separate repository or we can program an extraction process from the log files that is executed at the beginning of the preprocessing procedure. Each one of the web site X  X  pages has been labeled with a unique symbol. All these symbols form the alphabet of the web access sequences. Assume for the sake of description that there are N sequences that form a set S  X f WAS 1 ; WAS 2 ; ... ; WAS 3.2. WAS clustering
At this step of the preprocessing procedure we construct the similarity matrix of S . This N N matrix expresses the sim-ilarity of each pair  X  i ; j  X  of sequences. As a metric of the distance/similarity between WAS metric taking into account the global alignment and the local alignment of the two sequences. More formally: where LA  X  WAS i ; WAS j  X  is the score of the local alignment of the sequences WAS global alignment for these sequences, and p is a parameter that expresses the importance that we give to the scores of the two different alignments.

In order to define the value of p there are several possibilities. One choice is to use p equal to 0.5 giving the same gravity to both alignments. The other one, and more proper choice, is to define the value of p to be relative to the ratio of the lengths of the two sequences. More formally, assuming without loss of generality that j WAS
The intuition behind this definition is that when the two sequences, and thus navigational behaviors, have almost the same length we should take into account the global alignment more than the local. When the lengths of the two sequences are very different p is close to 0 and local alignment has more gravity than the global one. Our claim is that common navigational preferences of the users that are depicted by the two sequences are not captured only by aligning the sequences in their whole length; very equal subsections of the sequences can capture common behavior too.

The score function for the alignment has to be appropriate in order to express navigational properties of the sessions. A straightforward approach would assume that the match of two characters/pages in the alignments should be prized with a positive constant score while a mismatch or a match with a space with a negative constant score. With a deeper consider-ation a different approach is needed. A specific page of a web site may play dissimilar roles to different users so a simple character/page match may mean nothing in the task of similarity testing of two sequences. However, a web page may have no importance to a user; we call these web pages Unimportant , such as web site start-pages or a page visited by a wrong click and immediately a back-step is performed. Usually a user stays in unimportant pages a very short period of time, a few sec-onds at most, and then goes to another one. Also a page is unimportant for a user when it is recorded in log file for inordi-nately large period of time, usually above 25 X 30 min. This usually means that the user has changed web page by typing a new address or by clicking a banner. Another role that can play a page is the one of index, we call these pages Hubs . Users try in these pages to locate the link to the desired page by reading roughly the content and staying in those pages not much than 5 X 6 min. Finally, there are the Content pages with the desired for the user content. Users spend in Content pages approximately 10 to 20 minutes. Consequently there is need to store in each WAS the corresponding period of time along with each character/page. Techniques for session extraction from log files and extracting additional information can be found in [46,40,7,3] . As mentioned before from all the pages we form initially an alphabet O which then is tripled at most by char-acterizing each one of the existing symbols as Unimportant, Hub and Content.

For calculating the global and the local alignments the classical approach of dynamic programming [32] has been chosen using recursive formulas for the alignments of the prefixes WAS quences the alignments of symbols with spaces should not be gathered in consecutive positions. Consecutive alignments with spaces are defined as gaps. Very long gaps mean that in the one WAS the user has followed a very different path (maybe lost) in comparison with the other so we would not like to favor such gaps. In order to abate this fact we apply affine gap penalties in the alignments. The objective is then to find an alignment that maximizes the quantity: Wm  X  # matches  X  Wms  X  # mismatches  X  Wg  X  # gaps  X  Ws  X  # spaces  X  ; where Wm = 1 denotes the weight of a match,
Wms = 1 denotes the weight of a mismatch, Wg denotes the weight of starting a new gap and Ws denotes the weight of an alignment with a space. We would like to favor small gaps so we have chosen Wg equal to 1 while WS  X  5. The gap opening penalty is intentionally kept smaller than the gap penalty in order to favor frequent gap openings with small length instead of longer and more infrequent gaps. The classic recursive equations can easily adapt to incorporate affine gaps too [18].

When the similarity matrix D has been constructed we use it in order to group the sequences into clusters. The distance matrix can be viewed as N points in N -dimensional space. Running a clustering algorithm upon these N points will produce clusters consisting of very similar web access sequences. D is given as an input to the k -windows clustering algorithm [42];a well known unsupervised partitional clustering algorithm. Our decision to use k -windows as a clustering method was driven by a variety of reason, such as the enhanced quality of the produced clusters and its inherent parallel nature (more informa-tion can be found in [42]).

K -windows is extensively studied in the literature and is a fair enough representative partitioning algorithm. This feature is crucial if the web server and this application is developed in a multi-processor machine. However, it is obvious that the k -windows algorithm is non-deterministic. It uses random initialization so the same algorithm applied on the same data may produce different results. A non-deterministic algorithm does not always give a useful answer, but it never gives a wrong answer. Hence, the algorithm was applied several times. The repeated runs lead to superiority of this algorithm against other partitioning algorithms. K -windows and other algorithms which use random initialization can give a correct useful answer much faster than a deterministic algorithm, especially if the initialization is appropriate. Furthermore, the non-deterministic aspect of the k -windows or algorithms of this kind can help us in validating the clustering. If the same clusters are coming up after different initialization in k -windows, then the clustering is performing well.

One of the main drawbacks of the k -windows algorithm is that it uses as an underlying data structure a d -dimensional range tree [44]. Although range trees answer efficiently range queries in O  X  log problems with large dimensionality because their space utilization O  X  Nlog where d  X  N is the number of dimensions). Most of the current systems even with several gigabytes of memory come un-done to maintain a range tree with a large set of points of high dimensionality in main memory. In order to overcome this drawback we propose a variant of k -windows algorithm that utilizes R-trees [19] as an underlying data structure for the operation of the algorithm. R-trees trade-off memory utilization with range query time but in practice they perform fairly well. Interpretations upon this issue are collocated in the next section along with experimental observations. The practice showed to us that this trade-off of memory with computation time is quite beneficial in our case. As an alternative k -means clustering or an agglomerative hierarchical clustering for sequential data, as the one presented in [26], can be applied. 3.3. WAS clustering exploiting web page content
Our second method enhances the previous proposed technique by utilizing the content of the web pages in order to en-rich the quality of the clustering of the WASes capturing the same accessing patterns. The phases of WAS extraction and WAS
Cluster Representation remain unchanged. We alter the step of Clustering to the provided concept of web content utilization as described below.

Initially, we cluster the WASes in groups taking in mind their content. As a metric of the distance/similarity between WAS and WAS j we chose to use a hybrid metric taking into account the alignment of the sequences in combination withe the sim-content of each web page. For processing the content of the web pages we use the vector space model with the term fre-quency-inverse document frequency term weighting and as a similarity metric between each pair of web pages the cosine of the corresponding vectors [10]. For this purpose all the classic methods of text parsing, such as term blocking, stemming, html tag stripping, etc. have been applied. All parts of the html document, such as headers, body, emphasized text, have been processed equally playing the same importance. The only special care is that terms from an image file included in the html, or a link to a document like: {pdf, doc, ppt, jpg}, etc. have been processed in the model. There are two ways to incorporate web page content into our base method. The first one is to utilize the content similarity directly into the process of sequence alignment while the second one is to preprocess the collection sequences into a set of clusters and then utilize this clustering into the process of sequence alignment. 3.3.1. Direct sequence alignment (DSA)
As mentioned before, in the base approach, from all the pages we form initially an alphabet O from the web pages which then is tripled at most by characterizing each one of the existing symbols as U nimportant, H ub and C ontent according to the role the symbol plays in the web access sequence that appears. So under all these considerations, in the alignment (global or local) of a pair the scoring function of aligning two characters/web pages is a combination of the importance label of each page and the similarity metric between them.
 More formally: where P i ; P j are the corresponding symbols/web pages that are aligned, r  X  P pages playing in their sequences, U is the unimportant role, and T scoring function web pages that are playing the same role in their sequences and have almost the same content (expressed by their cosine distance) are favored to be aligned in contrary to the other possibilities. 3.3.2. Sequence alignment with clustering preprocess (SACP)
Another way to incorporate the content of web pages into the sequence alignment algorithm is to perform a clustering by content of the web pages.This initial web page clustering can be done by applying k -means or another clustering algorithm.
Hence to achieve that, initially we express the collection of web pages with the space vector model and form a distance ma-trix for each pair of web pages. Then we apply a clustering algorithm upon this distance matrix using the cosine distance
We interpret this clustering as a content proximity measure during the sequence alignment step. For this purpose the scor-ing function is adapted to be the following:
This score function can be viewed as a more coarse interpretation of the first one used in DSA. For each pair of web pages we don X  X  use the actual value, as in DSA, but a granular score. As an alternative a specialized web document clustering scheme could be used, as the one presented in [20], where a phrase-based document indexing is utilized for web document cluster-ing taking in mind phrased and their weight in the procedure of clustering. 3.4. WAS cluster representation
When the WAS clustering procedure is over each one of the clusters is expressed as a weighted sequence. The produced weighted sequences implicitly capture most of the navigational behavior that is observed in the corresponding cluster.
Assuming that we examine a cluster with f web access sequences, in order to produce the weighted sequences the web ac-cess sequences are combined using their multiple sequence alignment. Although the calculation of a multiple sequence alignment is extremely time and space consuming using dynamic programming approaches, in some cases it may be afford-able because the length of the sessions is usually short (5 X 10 pages is a very frequently observed session length) due to lim-ited human recall [12] or to bounded web site height [6].

As an alternative someone could possibly use the approach of progressive or iterative pairwise alignment in order to pro-duce the multiple sequence alignment [18]. Progressive of iterative algorithms are based on the idea that the solution can be computed by modifying an already suboptimal solution. The simplest approach is to initially align the pair of WAS with the maximum similarity. Then successively merge in the WAS with smallest distance from any of the strings already in the mul-tiple alignment. This method can be viewed as finding the multiple alignments consistent with a maximum spanning tree formed from similarity data (matrix D). A more realistic variant of the above approach might choose the sequence with the maximum average similarity in order to incorporate it in the multiple sequence alignment aligning that sequence with the most alike. In our case we have used the Star-alignment heuristic presented in [18].

Finally, when the multiple sequence alignment of the cluster X  X  sequences has been calculated, we construct the weighted sequence that represents the cluster. At each position and for each character/page we maintain the percentage of their appearance, without taking the space into account (see Fig. 4 ). 4. WST utilization  X  recommendation/prediction method
As soon as all weighted sequences have been produced, their corresponding generalized weighted suffix tree is con-structed. For this construction the algorithm of [23] is used tuned by a parameter 1 = k . The computational time and the space utilized by this structure are linear to the sum of the lengths of all weighted sequences. The generalized weighted suffix tree will represent all possible weighted subwords of all of the weighted sequences such that the probability of appearance for each one of them is greater than 1 = k . The experimental evaluation showed up that a proper value for parameter 1 = k is be-tween 0.01 and 0.1.

The weighted suffix tree implicitly can capture the most important navigational experience and with small computational and memory prize. It works like a prediction model without storing explicitly strings and probabilities of occurrences. Also it inherits all the virtues of suffix trees (which is a well studied data structure): the linear space, the linear construction time and many well designed practical implementations. This structure not only captures the most frequent and probable naviga-tional experience but also the most important navigational experience with discrete semantics. Hence, the intuition behind this interpretation of the problem is that from a huge collection of web access sequences we coarsely cluster them by means of web content and then by means of frequency of occurrence. These clusters are then summarized in weighted sequences and indexed by the weighted suffix tree (see Fig. 5 ).

The recommendation/prediction algorithm works as follows: when a new user is arrived in the system, he is assigned to the root of the generalized weighted suffix tree (gWST). As the user navigates through the web site selecting web pages by clicking links, his position in the gWST is advanced too. Assume that he currently is at an internal node and he is ready to perform his next step. The system proposes to him the web pages indicated by the outgoing edges of u . Each of the outgoing edges is labeled by a substring and starts with a different character. All those different characters/web pages are proposed to the user. The proposition of all descendants is not realistic since the cardinality of them can grow more than 100 (we refer to this approach as unbounded in the evaluation section just for comparisons sake). If the number of the proposed web pages is relatively large, above 20, a selection of at most 20 web pages is presented (or prefetched) to the user. Those pages are se-lected either randomly ( bounded-random ) or we pick the  X  X  X eaviest X  ones( bounded-weight ). The weight of each one of the choices is defined as the number of times that this node has been visited by a user before and it is implemented by a simple counter and a priority queue upon each node.

As the user has chosen a web page/outgoing edge he starts to traverse the characters of that edge. Notice that due the nature of the suffix tree, upon an edge may lay several characters. This means that all those characters are in a common pre-fix to all the paths that traverse that edge. Thus, while the user is descending on an edge would be only one possibility for suggestion. Hence as an alternative, at each step the system proposes two web pages; the next one upon the edge and the last one leading to the target node of the edge. This behavior is proposed because the nodes indicate pages that the users that had started a specific equal sequence of accesses at some point they had split the course of their navigation.In the extreme case that under an internal node w there is no edge starting with the character that corresponds to the user X  X  choice then the user is assigned to an internal node u specially determined. The path from the root to this node forms a maximal suffix to the for each internal node w to keep a pointer to it proper node u we apply a preprocessing step that is called failure-function like in the Aho-Corasick Automaton [18].Its preprocessing takes linear time to the size of the structure. With consecutive fails the procedure is backtracking toward the root to nodes with smaller paths. This backtracking cannot be very large (usually &lt; 15 steps) because of the limited length of the WASes. Though after 2 X 3 fails the procedure should alternatively query the root for a proper outgoing edge in parallel with the internal nodes and then pick the successive one.

In Fig. 6 , we have a sample run of the recommendation algorithm. Suppose that a new user has entered in the system and his access sequence is BCA and the number of recommendations is two. When he firstly enters in the system and no selection has been made, he is assigned to the tree node. The system recommends to him the two most visited nodes (5,3). When he selects the B web page according to his access sequence the recommendation changes to the most two visited nodes (3,2) under the BC node. 5. Evaluation
For the evaluation of the proposed system various experiments have been performed. Through this process we would like to study and estimate: the demands in memory of the proposed system, the preprocessing effort to construct the structure and the success of the prediction/recommendation that the system creates.

We have performed several experiments, choosing different values for the parameters of the system in each one. All experiments have been performed on a AMD64 dual core@ 2 Ghz with 1 Gbyte of main memory under Windows XP. The programs were coded in Perl and C. The experiments simulate the user behavior by using a web server log file. The 3 X  10% of the information of the log file has been used in order to construct the data structure (gWST) of the system while the rest has been used to evaluate the recommendations. As a cluster algorithm K -windows, mentioned in the previous sec-tion, was used. The number of clusters in our experiments varied from 5 to 20 and typical parameters of window growth (20%) and window movement (5%) were used.

Among the techniques utilizing web access sequences and web page content [16,17,29] , which have comparable perfor-mance, our approach has several virtues and advantages and in some cases better performance. Although that our method has a fair large preprocessing step, in means of memory and computational time, the extracted model is quite small and can be maintained and updated, though node weights, for a long period of time. When the failure-function seems to be executed too often then it is proper time to rebuild the model from the start. All the other approaches [16,17,29] have lack of adap-tation capability witch means that there is a more frequent demand of re-preprocessing. Also the approach with the extrac-tion of association rules seems to work well only with small web access sequencing in comparison with alignment and maximal subsequence method that adapt better to longer access sequences. 5.1. Evaluation of access based method For the purposes of this experiment we have utilized the web server X  X  log file of our department (www.ceid.upatras.gr ).
This log file corresponds to a two month period and has approximate 260 Mbytes size. Also we have used for this set of experiments the well-known data sets in many studies in this area; the log file of NASA web server over the months of July and August 1995 [52]. For extracting the web access sequences from the log files we have extended the Follow web log anal-ysis tool [51], under version 1.5, in order to have the properties described in Section 3.2. The initial tool and the extension were both written in Perl. With the preprocessing of the log files a set of web access sequences has been produced. A portion of these (3 X 10%), selected randomly, has been utilized for capturing the user behavior of the web site by clustering them and constructing the weighted suffix tree while the rest has been used as an evaluation set. For each WAS, we perform the move-ments of the user described by the sequence and the system behavior as described in Section 4. In order to evaluate the per-formance of our method we have used two known metrics as in [38,17] :
Hit-ratio metric: For each WAS we maintain an evaluation score. For each character of the sequence/step of the user in the site we increase the score by one if the next step (page of the web site) has been proposed by the system while in the case that the user performs a step not in the set recommended by the WST (Section 4) we keep the score unchanged. Finally for each WAS, we calculate a weighted score defined by the ratio of the previous score over the length of the sequence (which is the best score).

Click-soon ratio metric: For each WAS we maintain an evaluation score. For each character of the sequence/step of the user in the site we increase the score by one if at least one of the recommendations/predictions is requested by the user until the end of the sequence (thus while the user session is active). If none of the recommendations is used while the session is active we keep the score unchanged. Finally for each WAS, we calculate a weighted score defined by the ratio of the previous score over the length of the sequence (which is the best score too).

Table 1 recapitulates the settings and the results of the experiments upon the NASA log file. A small portion (2%) of the data set has been used to train our model in both experiments. In the first one the 1 = k building paramete r of the weighted suffix tree is fine-tuned while in the second one the number of initial clusters is varied. At a first glance we observe that the weighted scheme for the selection of the recommendations, in both metrics, is always much better from the random selec-tion but slightly worse from impractical unbounded proposing. Decreasing 1 = k implies increasing the effectiveness of the model in both metrics but with trading off memory. Suitable selection for 1 = k seems to be values bellow 0.1. The number of utilized clusters seems to affect the capability of the model with less finally memory consumption but in a more mild way than the 1 = k parameter. Finally, in the third experiment we have varied the size of the training data set from 2% to 10%. It seems that with greater training sets and small values of 1 = k the model can achieve even more better prediction with the weighted scheme in both metrics. (84.12% and 97.29% resp.). With such high hit-ratio scores this model is capable to perform as an on-line recommendation system and with those click-soon ratio scores as a prefetching web page system.
Comparing our experimental results with those of [16], which is one of the latest techniques competitive to ours and the latest comparison work in the topic, we can claim that our method outperforms most of them. More especially: our model seems to have similar performance and sometimes slightly better in click-soon metric score (the best scores in [16] vary from 85 X 94%); in hit-ratio metrics our method is achieving clearly better performance (the best scores in [16] vary from 50 X 60%) in both cases by utilizing a much smaller training set which says that our model has a stronger predictive capability.
The results of experiments on the other data set, the log file of www.ceid.upatras.gr, are quite similar to the above ones and slightly better again using the same portion of information for training purposes. This comes from the fact that this web-site is a smaller one with much less navigational patterns that are captured better by our model. This data set consists of 27,805 web access sequences while from NASA log file they have been produced over 55,000 sequences.

Concerning, memory utilization, the consumption of memory of the main data structure is quite reasonable and does not exceed the size of few MBytes while the preprocessing procedure is more demanding due to the computation of the distance matrix D , whose size is quadratic to the size of the training set. 5.2. Evaluation of access and content based methods
For the purposes of this experiment we have utilized the web server X  X  log file of our department (www.ceid.upatras.gr )as in the previous subsection and the source code of the site. This dataset was the only choice for experimenting because there is no one public web site testing dataset providing the source pages and the access log files of the server. The context of the experiment was exactly the same as the evaluation as described in the previous section. The performance is also evaluated with the hit-ratio and the click-soon ratio metrics.

As we can see in figures Tables 2 and 3 the exploitation of web content has positive impact to the performance of our method. We run tests for various values of the parameters. We observe that the number of clusters and the 1 = k parameter play large role to the tuning of the method. The DSA preprocessing method seems to has better performance against the SACP method but not all of the times. For example for probability of occurrence 1 = k  X  0 : 01 and # clusters  X  10 which are values producing fairly good behavior we have observed in both metrics a 2 X 3% increase of the performance in comparison to WAS clustering without content exploitation. In concern of computational time for preprocessing, SACP and DSA are more demanding due to the parsing of the web pages. SACP method needs even more time due to the additional web page clus-tering step. 6. Conclusions and open issues
In this paper we have proposed various techniques for predicting web page usage patterns by modeling the users X  nav-igation history using string processing techniques, and we have validated experimentally the significance of our proposed technique. Future work includes different ways of modeling web user access patterns, choice of different clustering tech-niques, investigation of different metrics, exhaustive experimental comparisons with similar techniques and exploitation of the trade-off between the degree of the weighted suffix tree (is directly affected from the threshold), and the performance of our scheme.

References
