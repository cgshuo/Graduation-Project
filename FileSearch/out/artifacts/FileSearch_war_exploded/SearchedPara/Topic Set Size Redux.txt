 The cost as well as the power and reliability of a retrieval test collection are all proportional to the number of topics included in it. Test collections created through community evaluations such as TREC generally use 50 topics. Prior work estimated the reliability of 50-topic sets by extrapo-lating confidence levels from those of smaller sets, and con-cluded that 50 topics are sufficient to have high confidence in a comparison, especially when the comparison is statistica lly significant. Using topic sets that actually contain 50 top-ics, this paper shows that statistically significant differe nces can be wrong, even when statistical significance is accom-panied by moderately large ( &gt; 10%) relative differences in scores. Further, using standardized evaluation scores rat her than raw evaluation scores does not increase the reliabilit y of these paired comparisons. Researchers should continue to be skeptical of conclusions demonstrated on only a single test collection.
 H.3.4 [ Information Storage and Retrieval ]: Systems and Software X  X erformance evaluation Experimentation, Measurement
The cost of constructing a retrieval test collection is dom-inated by the cost of obtaining relevance judgments, and thus cost is proportional to the number of topics the collec-tions contains. The power and reliability of a test collecti on (that is, the ability to reach a decision and the amount of confidence that can be placed in that decision) are also pro-portional to the number of topics in the test set because systems X  effectiveness on individual topics is highly varia ble.
The question of how many topics to include in a test set has received considerable attention. For test collections as-Voorhees empirically showed that the reliability of a sin-gle comparison of two retrieval approaches depends on the evaluation measure used in the comparison, the size of the difference of the evaluation scores, and the number of topics used in the comparison [6, 2]. Sakai provided a theoretical underpinning for these findings [4]. The empirical results suggested a test should include at least 25 topics, and that even with 50 topics X  X he topic set size traditionally used in collection-building exercises such as TREC X  X igh confi-dence required score differences that are larger than those routinely reported in the literature. Sanderson and Zobel then showed that using statistical significance rather than simply size-of-difference greatly increased the reliabili ty of a comparison [5]. They, too, showed that 25-topic sets are too small to be reliable, but concluded that an experimenter who observes a statistically significant result on a 50-topi c test set can be very confident in the comparison, especially if the relative difference in scores is greater than 10%.
Test collections must include many topics because individ-ual topic results are highly variable [1]. If individual top ic results were less variable, it would be easier to reliably de -tect differences between retrieval approaches, and retriev al experiments would require fewer topics for a given level of confidence. Webber, Moffat, and Zobel suggest the use of standardized scores as one way to reduce topic variability [7]. Standardized scores are defined relative to a set of retrieva l runs known as the standardization set. The standardiza-tion process computes per-topic standardization factors t hat scale raw scores close to the typical score (of the standard-ization set) to approximately 0.5, while minimum and max-imum raw scores are scaled toward 0 and 1, respectively.
Previous work estimated the reliability of 50-topic sets by extrapolating results from 25-topic sets due to the lack of sufficiently large retrieval result sets at the time. Since then, several TREC tracks have used substantially larger topics sets, enabling a direct empirical investigation of t he reliability of 50-topic test sets. This paper reports on the outcome of that investigation, for both raw evaluation scor es and corresponding standardized scores. The result for both types of scores is the same: different topic sets each contain -ing 50 topics drawn from a homogenous population of topics can differ as to which run is more effective, even when the plete judgments. Recent work such as that reported by Carterette, et al. [3] examines the trade-offs between num-ber of judgments per topic and total number of topics. This paper focuses on traditional collections. difference is statistically significant and the relative diff er-ence in mean scores is greater than 10%.
Our goal is to test the reliability of experimental outcomes based on topic sets of size 50. For this purpose, we will consider an outcome unreliable if the comparisons of a pair o f runs on two different topic sets arrive at different conclusio ns as to which is the more effective run.

The data are taken from the retrieval runs submitted to the TREC 2004 Robust track. Each run submitted to this track contained retrieval results for 249 topics, includin g the 100 topics originally used in the TREC-7 and TREC-8 ad hoc tracks. These 100 topics were developed using the same methodology, using mostly the same set of assessors, and judged using the same pooling protocol with roughly equal numbers of runs contributing to the pools. That is, these two sets of topics are as close as possible to being two different samples drawn from the same universe of topics. Fourteen different groups submitted a total of 110 runs to the track. To avoid confounding results by including error-ful runs in the comparisons, only the top 3/4 of the runs as measured by MAP over the entire set of 249 topics are used. This results in a set of 83 runs from 12 groups.

The 100 topics were randomly divided into two sets of 50 topics each 1000 times; call one such division a topic set pair. For each topic set pair, each run pair was evaluated on both component topic sets using the current evaluation measure. A paired, two-tailed Student X  X  t test with  X  = 0 was applied to each component X  X  comparison. A minor con-flict was detected when a statistically significant difference was found for one component, while the other component favored the other run (but that difference was not signifi-cant). A major conflict was detected when both component topic sets resulted in a statistically significant differenc e and each favored a different run. Evaluation measures included MAP, NDCG, R-Precision, and Precision at 10 documents retrieved, using both raw and standardized scores for each. Raw scores were computed using trec_eval ; standardiza-tion factors were obtained from http://www.csse.unimelb. edu.au/~alistair/ir_eval/ .

Table 1 shows the results. With 83 runs, there are 3403 run pairs. Each run pair was compared on two different topic sets in each of 1000 trials, resulting in 6,806,000 tot al comparisons per evaluation measure. The second column of the table gives the number of those comparisons that were found to be statistically significant. The third and fourth columns give the number of major and minor conflicts ob-served. The final column gives the percentage of significant differences that were involved in a conflict.

A majority of the significant differences involved in a con-flict were accompanied by relative differences in mean scores that were at least moderately large. For example, of the 88,924 significant differences involved in a conflict for MAP, 68,061 had relative differences between 10 and 20%; 14,784 had relative differences greater than 20%. The distribution of conflicts was highly skewed across both run pairs and topic set pairs. Since the majority of run pairs involved runs that were submitted by different groups, the majority of conflicts also involved runs submitted by different groups. However, same-group run pairs were not immune to conflicts. This suggests that a single retrieval experiment can experience similar conflicts.
 Measure differences conflicts conflicts conflicted
MAP 3,223,017 1280 86,364 2.8 std MAP 3,294,715 1366 92,974 2.9
NDCG 3,601,022 1038 83,312 2.4 std NDCG 3,643,442 972 83,986 2.4
R-prec 2,385,525 1432 126,279 5.4 std R-prec 2,415,186 1576 122,493 5.2
P(10) 1,391,452 2147 151,708 10.9 std P(10) 1,286,041 2376 164,851 13.2
It is common in the IR literature for a statistically sig-nificant finding accompanied by a moderately large relative difference in mean scores to be treated as as a sure thing. Yet statistical significance tests are explicitly not guara ntees: each test has a stated likelihood of a type I error (conclud-ing significance when no true difference exists). This paper demonstrates that such errors do in fact happen for topic sets containing 50 topics (the de facto standard size), even when accompanied by large relative differences in scores and for topics drawn from a homongenous universe. The frac-tion of conflicting conclusions to total number of significan t conclusions is well within the 5%  X  value used here for the more stable measures (MAP and NDCG), but far exceeds  X  for P(10). Fifty-topic sets are clearly too small to have con -fidence in a conclusion when using a measure as unstable as P(10). Even for stable measures, researchers should remain skeptical of conclusions demonstrated on only a single test collection.

Standardized scores were introduced as a way of reducing topic variability to facilitate system comparisons across dif-ferent topic sets using two-sample significance tests [7]. T his study shows the use of standardized scores does not reduce the variability of score deltas, the effect of importance in paired significance tests. [1] D. Banks, P. Over, and N.-F. Zhang. Blind men and [2] C. Buckley and E. M. Voorhees. Evaluating evaluation [3] B. Carterette, V. Pavlu, E. Kanoulas, J. A. Aslam, and [4] T. Sakai. Evaluating evaluation metrics based on the [5] M. Sanderson and J. Zobel. Information retrieval [6] E. M. Voorhees and C. Buckley. The effect of topic set [7] W. Webber, A. Moffat, and J. Zobel. Score
