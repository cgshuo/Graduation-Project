 Named entity recognition (NER) is widely used in text mining applications. English NER achieves a high performance, but Chinese NER needs to be improved substantially. A named entity (NE) is a phrase that contains predefined names, such as person names, location names, and organization names. Named Entity Recognition (NER) is the process used to extract named entities in many applications, such as question answering systems, relation extractio n, and social network analysis. Several conferences have been held to evaluate NER systems, for example, CONLL2002, CONLL2003, ACE (automatic context understanding), and SIGHAN 2006 NER Bakeoff. In many works, the NER task is formulated as a sequence labeling problem. Such problems have been discussed extensively in the past decade and several practical machine learning models have proposed, for example, the maximum entropy (ME) model[1], the hidden Markov model (HMM) [8], memory-based learning[5], support vector machines (SVM)[6] and conditional random fields (CRFs)[13]. Chinese NER is particularly difficult b ecause of the word segmentation problem. Unlike English, Chinese sentences do not have spaces to separate words. Therefore, word segmentation information is important in many Chinese natural language applications. Depending on the way such information is incorporated, NER approaches can be classified as either ch aracter-based or word-based. In character-based approaches, segmentation information is used as features, whereas word-based methods use the output of the word segmentation tagger as the basic tagging unit. However, irrespective of the method used, the interactions between NER and word segmentation tags can not be considered jointly and dynamically. 
One solution for handling multiple related sub-tasks like word segmentation and parts-of-speech and noun phrase chucking using dynamic CRFs [13], incorporating features into different semantic levels using a log-linear joint model [3], and using a re-learning methods yield richer interactions between sub-tasks, which they consider dynamically. 
In this paper, based on the concept of joint learning, we propose a novel Chinese NER between the named entity and word segmentation states. It also facilitates dynamic problem of potentially higher computation costs incurred by multiple layer tagging. Because it uses semi-joint tagging, the proposed system outperforms state-of-the-art systems. The remainder of this paper is organized as follows. In Section 2, we introduce Chinese NER and word segmentation. In Section 3, we describe the proposed method. Then, in Section 4, we discuss the features of our system. Section 5 details the experiment results, and Section 6 contains our conclusions. In this section, we introduce the Chinese word segmentation and named entity recognition task, and consider existing approaches that incorporate word segmentation information in NER models. In Table 1, the first row shows a series of Chinese characters with word segmentation and named entity labels. We list two segmentation character is at beginning of a word and I denotes that a character is in a word . In the EI format, E denotes that a character is at the end of a word and I denotes the inside character of a word. In the named entity tagging format, a label is defined as a named entity type extended with a boundary tag. For example, B-LOC denotes that a character is at the beginning of a location name, while O denotes that the character is not part of a named entity. Word segmentation can provide valuable information for NER. For example, the boundaries between a word and a named entity can not cross or overlap. Previous works, such as Guo et. al.[4] and Chen et. al.[2], have shown that word segmentation information can improve NER performance. 
There are two ways to incorporate word segmentation information into an NER model, namely, character-based approaches and word-based approaches. Unlike English NER, Chinese character-based NER uses characters as the basic tokens in the tagging process. Chen et. al.[2] and Wu et. al.[14] use a character-based approach in their NER models. The advantage of this approach is that it avoids the propagation of potential errors by the segmentation tagger. However, this approach does not consider the word segmentation information. One common approach employs a cascaded training and model. For example, Guo et. al.[4] use word segmentation information as a feature in a character-based model. Word-based NER uses words as the basic tokens. A number of systems, like those of first row of the word-based section shows an example of a phrase with word-based NER Chinese word-based and English word-based segmentation are the same. However, since word-based segmentation needs the output of a word segmentation tagger as the basic tagging token, propagated errors will be passed on to the NER model. 
No matter whether the cascaded approach uses word segmentation information in character-based tagging or uses word-based tagging directly, the interactions between word segmentation and NER can be represented in is limited and can not be considered dynamically. Next, we introduce dynamic CRFs and the semi-joint labeling format used to represent more complex interactions. 3.1 Dynamic Conditional Random Fields Dynamic conditional random fields (DCRF) [13] are generalizations of linear-chain conditional random fields (CRF) in which each time slot contains a set of state variables and edges. The form of a dynamic CRF can be written as follows: where y is a label sequence over observation sequence x; c denotes a clique in a graph;  X  and f k are, respectively, the weights and feature function associated with the clique definitions of c , DCRFs can represent various interactions within a time slot. For example, if we define c as a combination of labels in multiple tagging layers, then y t,c interactions between word segmentation information and named entity recognition. 
We use DCRFs to present a graphical model that considers the interactions of named Using DCRFs, we can represent this structure by the following equation: where y n denotes the named entity label sequence and y s denotes the segmentation label named entities in the adjacent time slot; and  X  denotes the function of the interactions between the named entities and word segmentation labels in the same time slot. Based on the feature f k and the parameter  X  k ,  X  and  X  can also be presented as: 
Figure 1(b) is a three-chain structure in which the chains corresponding to the tagging sequence from the top to the bottom represent named entity segmentation, BI-format word segmentation, and EI-format segmentation, respectively. Using DCRFs, we can represent this structure by the following equation, Ptt where  X  denotes the interactions between the state sequence of named entity labels and the EI-format word segmentation labels. 3.2 Semi-joint Labels in Linear Conditional Random Fields If DCRFs are used to represent complicated structures, such as multiple layers of tags, the number of cross-products of states between the layers will cause the inference space increases. For example, the cross-product space of the segmentation labels and named entity labels is twice as large as the original named entity label space. We propose a semi-joint model to reduce the inference spaces. joint labeled CRFs are linear-chain CRFs transformed from multiple chain CRFs by applying the semi-joint label set. Next, we define a semi-joint label set. selected from the Cartesian product of the original label set. The selection rule can be decided manually or systematically For example, Table 1 shows a Chinese phrase with word segmentation tags and named entity tags that are integrated by semi-joint labeling tags. The second column shows the phrase X  X  corresponding English translation. Each character has segmentation tags in two formats, a named entity tag and two semi-joint labeling tags, as shown in the last two columns Note that semi-joint labeling only integrates a segmentation tag with a named entity tag if the named entity is  X  X  X . Other named entity tags will be reserved. The number of distinct tags in the semi-joint labeling format is only one more than in the original named entity format. Even in semi-joint labeling II, which integrates two kinds of segmentation format, there are only three more distinct tags than in the original format. 
In this example, we also find that integrating the word segmentation tag with the named entity tag  X  X  X  can provide boundary information, which can not be derived from boundary. This constraint helps us rule out impossible inference paths and thereby improve the precision of named entity diction boundaries. Equation 5. By combining the BI and IE formats, we can identify more significant interactions, such as constraints, when considering the transition of labels. For example, in the last row of Table 2, the tag before B-PER can only be I-E-O and B-E-O; otherwise, it would be against the cross-overlap constraint on words and named entities.  X  Russian B I B-LOC B-LOC B-LOC 4.1 Basic Features The basic features of our NER model are: current character and C n C n+1 denotes its bi-gram feature, which is a combination of the corresponds to the current label, whereas transition features relate the previous and current labels. 4.2 Knowledge Features Knowledge features are semantic hints that help an NER model identify more named entities. Several Chinese NER models use knowledge features; for example, Youzheng Wu [14] collects named entity lists to identify more named entities and thereby resolves the data sparseness problem in Chinese NE. 
To compare our system with other approaches, we observe the closed task rules, which do not allow the use of external resources. Therefore, we only generate knowledge lists from the training corpus. For example, we compile the surname list from the tagged person names in the corpus. The knowledge feature types are listed in the Table 3. Since the features are generated automatically, we filter out those that occur less than twice [2], training corpus. 
Next, we consider how we represent knowledge in feature functions. If a character is included in a list of knowledge features, the feature X  X  value is set at 1; otherwise, it is set at 0. In this section, we describe the experimental data, introduce the parameters used in the CRF model, and detail the experiment results. 5.1 Data Source To evaluate our methods, we use the City University of Hong Kong (CityU) Chinese corpus from SIGHAN 2006 [10], the Special Interest Group for Chinese Language Processing of the Association for Computational Linguistics. We choose the CityU corpus because it provides both segmentation tags and named entity tags. The corpus contains 1.6M words and 76K named entities in the training part, and 220K words and names, organization names, and location names. 5.2 Settings We use CRF++ 1 to implement our CRF models. The parameters we use in CRF++ are f, the cut-off threshold, which is set to 1; and c, the C value that prevents over fitting, which is set to 3. The maximum number of training iterations is 1000, and the training environment is Windows Server 2003 with an AMD 2.39GHz CPU and a 10 Gigabyte RAM. 5.3 Results and Discussion Table 4 shows the results achieved by the three NER models. Each row shows the performance of an NER model for three types of NE with specific tagging formats, as well as the model X  X  overall performance. The models are evaluated on the full test set (220K words and 23K NEs) of the CityU corpus. BIO uses the traditional format, i.e., a named entity type extended with a boundary, while the Semi-Joint labeling and Semi-models using semi-joint formats do not include word segmentation features. By includes word segmentation features. The results show that the Semi-joint format outperforms the BIO format for all three NE types with an overall F-score of approximately 1.41%. Meanwhile, the Semi-joint II format outperforms the Semi-joint format with an overall F-score of approximately 0.24%. Since the proposed semi-joint labeling method integrates word segmentation with an NER model, and word segmentation can help detect the boundaries of named entities, it We define a boundary error as a named entity is identified and their lengths are different with the correct ones. Each row in Table 5 shows the reduced boundary error rate achieved by using semi-joint labeling. The error rate is computed by dividing the number of named entities with boundary errors in the semi-joint labeling method by those in the baseline system. We observe that semi-joint labeling reduces boundary errors, especially the semi-joint labeling II, which integrates two word segmentation formats. Next we consider different types of boundary error. Suppose the boundary of a named with the IE-format word segmentation information, the beginning character of a named entity can be identified more easily by the  X  X  X  label, which refers to the end of a word. We list the performance of the top five teams at the SIGAHN NER bakeoff for the CityU corpus. The performance of the proposed model is better than the top one in 1.2% approach uses word segmentation information as features, while we partially join word segmentation tags with named entity tags. We propose a semi-joint tagging format that partially combines word segmentation and named entity recognition labels. The format allows us to consider the interactions between multiple labeling layers in a linear-chain CRF model. To evaluate our model, we use the CityU corpus of SIGHAN 2006 NER bakeoff. The model based on semi-joint labeling outperforms the model that uses word segmentation tags as features, with an overall F-score of approximately 1.41%. Because of the novel labeling format, the proposed model outperforms the top one system by about 1.2% in terms of the F-score. 
In our future work, we will explore other possible interactions between word segmentation information and NER. We also plan to apply our method to other applications that would be improved by incorporating word segmentation information. We are grateful for the support of thematic program of Academia Sinica under Grant AS 95ASIA02 and the Research Center for Humanities and Social Sciences, Academia Sinica. 
