 A researcher decides to build a test collection for comparing her new information retrieval (IR) systems with several state-of-the-art baselines. She wants to know the number of topics ( n )she needs to create in advance, so that she can start looking for (say) a query log large enough for sampling n good topics, and estimat-ing the relevance assessment cost. We provide practical solutions to researchers like her using power analysis and sample size de-sign techniques, and demonstrate its usefulness for several IR tasks and evaluation measures. We consider not only the paired t -test but also one-way analysis of variance (ANOVA) for significance testing to accommodate comparison of m (  X  2) systems under a given set of statistical requirements (  X  : the Type I error rate,  X  : the Type II error rate, and min D : the minimum detectable differ-ence between the best and the wor st systems). Using our simple Excel tools and some pooled variance estimates from past data, re-searchers can design statistically well-designed test collections. We demonstrate that, as different evaluation measures have different variances across topics, they inevitably require different topic set sizes. This suggests that the evaluation measures should be chosen at the test collection design phase. Moreover, through a pool depth reduction experiment with past data, we show how the relevance assessment cost can be reduced dramatically while freezing the set of statistical requirements. Based on the cost analysis and the avail-able budget, researchers can determine the right balance betweeen n and the pool depth pd . Our techniques and tools are applicable to test collections for non-IR tasks as well.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval effect sizes; evaluation; evaluation measures; power; sample sizes; statistical significance; test collections; variances Reliable experimentation is crucial to the progress of IR research. The present study concerns laboratory experiments with test collec-tions, each consisting of a target document corpus, a set of topics and relevance assessments for each topic. More specifically, we address the following situation. A researcher decides to build a test collection for comparing her new information retrieval (IR) sys-tems with several state-of-the-art baselines. She wants to know the number of topics ( n ) she needs to create in advance, so that she can start looking for (say) a query log large enough for sampling n good topics, and estimating the relevance assessment cost. We pro-vide practical solutions to researchers like her using power analy-sis [11] and sample size design [15] techniques, and demonstrate its usefulness for several IR tasks and evaluation measures. We con-sider not only the paired t -test but also one-way analysis of variance (ANOVA) for significance testing to accommodate comparison of m (  X  2) systems under a given set of statistical requirements (  X  : theTypeIerrorrate,  X  : the Type II error rate, and min D : the mini-mum detectable difference between the best and the worst systems). Using our simple Excel tools and some pooled variance estimates from past data, researchers can design statistically well-designed test collections. We demonstrate that, as different evaluation mea-sures have different variances across topics, they inevitably require different topic set sizes. This suggests that the evaluation mea-sures should be chosen at the test collection design phase. More-over, through a pool depth reduction experiment with past data, we show how the relevance assessment cost can be reduced dramati-cally while freezing the set of statistical requirements. Based on the cost analysis and the available budget, researchers can deter-mine the right balance betweeen n and the pool depth pd .Our techniques, as well as our Excel tools, are applicable to any non-IR tasks (e.g., question answering, summarisation, machine transla-tion, recommendation etc.) as well, as long as the paired t -test or ANOVA is applicable to the task and test collection in question.
In some research disciplines, top-tier journals require reporting of effect sizes and confidence intervals along with p -values, as it is known that p -values are not informative enough [10, 11, 12, 20]. For example, suppose we have per-topic performance scores in terms of some measure M for systems X and Y with n topics: ( x paired t -test: where d j = x j  X  y j ,  X  d = n j =1 d j /n (the sample mean) and V tion variance). Now, as t 0 becomes larger (i.e., more extreme), the corresponding p -value becomes smaller and therefore the between-system delta is more likely to be considered statistically significant. However, it is clear from Eq. 1 that a large t 0 may mean either (a) the sample size n is large; or (b) the sample effect size difference between X and Y measured in standard deviation units, is large. In other words, p -values confound the effect of n and the magnitude of the  X  X eal X  difference [11, 15, 20].

In the IR research discipline (and in related disciplines such as natural language processing), the aforementioned  X  X tatistical re-form X  is yet to happen [20]. Thus, unfortunately, it is not uncom-mon for IR researchers to discuss the Type I error rate (  X  ) without heeding the Type II error rate (  X  ) and the effect size. Exceptions in the IR literature include the work of Nelson [16] who pointed out the usefulness of statistical power analysis for IR research, and that of Carterette and Smucker [4] who investigated the relationship be-tween the number of topics evaluated and the statistical power of the sign test ,where Average Precision (AP) was computed with in-complete relevance assessments. In several ways, the present study extends the work of Webber, Moffat and Zobel [27] who proposed to design test collections based on power analysis. Our new contri-butions over their work are as follows:
Ellis [11] remarks that the Bonferroni correction to counter this problem  X  X ay be a bit like spending $1,000 to buy insurance for a $500 watch. X 
One-way ANOVA is equivalent to the unpaired t -test generalised to the case of m  X  2 systems.

Evaluation forums such as TREC, NTCIR and CLEF typically build (say) 50 topics every year for each IR task 3 . However, it is not appropriate to combine these topic sets in the hope of conducting an experiment with high statistical power unless each topic set is known to be reusable. Moreover, note that the choice of n arbitrary; topic set splitting tests have been used in the literature to answer retrospective questions such as  X  X as n =50 large enough for conducting reliable experiments? X  (e.g. [26]). In contrast, the present study provides a method to systematically determine n for a new test collection, based on variance estimates from past data. Using this methodology, it is possible to improve the test collection design over past rounds of IR tasks.

Alternatives to classical significance testing include Killeen X  X  p rep [14] and the Baysian approach to hypothesis testing [2, 13], both of which are beyond the scope of this study. The Generalis-ability Theory (GT) has been shown to be useful for assessing the test collection reliability [1, 3, 23]: while both the GT approach and ours rely on variance estimates from past data, Urbano, Marrero and Mart X n [23] point out that the reliability indicators obtained from GT are difficult to interpret. We leave the comparison of our methods with the GT approach for the purpose of topic set size design as future work.
This section discusses how to set the topic set size n when we want to compare m =2 systems ( X and Y ) in terms of some mea-sure M using the two-sided paired t -test, where it is assumed that the per-topic performance scores { x j } and { y j } ( j =1 ulation means of X and Y are identical) and H 1 :  X  X =  X 
Let t be a random variable that obeys a t distribution with  X  de-grees of freedom; let t (  X  ; P ) denote the two-sided critical t value for probability P (i.e., Pr {| t | X  t (  X  ; P ) } = P ). Under H the test statistic t 0 (Eq. 1 in Section 2) obeys a t distribution with  X  = n  X  1 degrees of freedom. Given a significance criterion  X  , we reject H 0 if | t 0 | X  t (  X  ;  X  ) .(The p -value is the probability of observing t 0 or something more extreme, Pr {| t | X  t 0 H ence that does not exist) is exactly  X  by construction. Whereas, the probability of Type II error (i.e., missing a difference that actually exists) is denoted by  X  , and therefore the statistical power (i.e., the ability to detect a real difference) is given by 1  X   X  . Put another way,  X  is the probability of rejecting H 0 when H 0 is true , while the power is the probability of rejecting H 0 when H 1 is true . In either case, the probability of rejecting H 0 is given by Under H 0 , Eq. 2 amounts to  X  ,where t 0 (Eq. 1) obeys a (central) t distribution as mentioned above. Under H 1 , Eq. 2 represents the power ( 1  X   X  ), where t 0 obeys a noncentral t distribution with  X  = n  X  1 degrees of freedom and a noncentrality parameter  X  t  X  n  X  t . Here,  X  t is a simple form of effect size , given by:
Exceptions include the TREC Million Query track that was de-signed specifically to construct a  X  X inimal X  test collection for a given set of systems and a particular evaluation measure (AP) [3]. where  X  2 t =  X  2 X +  X  2 Y is the population variance of the score dif-ferences. Thus,  X  t quantifies the difference between X and Y in standard deviation units , regardless of the evaluation measure used.
While computations involving a noncentral t distribution can be complex, a normal approximation is available: let t denote a ran-dom variable that obeys the aforementioned noncentral t distribu-tion; let u denote a random variable that obeys N (0 , 1 2 ) Hence, given the topic set size n , the effect size  X  t and the signif-icance criterion  X  , the power can be computed from Eqs. 2 and 4 as [15]: where w = t ( n  X  1;  X  ) . But what we are more interested in is: given (  X ,  X ,  X  t ) , what is the required n ? Under H 0 , we know that  X  t =0 (See Eq. 3). However, under H 1 , all we know is that  X  t =0 . In order to require that an ex-periment has a statistical power of 1  X   X  ,a minimum detectable with 100(1  X   X  ) % confidence whenever |  X  t | X  min  X  t .Thatis, we should not miss a real difference if its effect size is min larger. Cohen calls min  X  t =0 . 2 a small effect, min  X  t medium effect, and min  X  t =0 . 8 a large effect [9, 11] 4
Let z P denote the one-sided critical z value of u (  X  N (0 for probability P (i.e., Pr { u  X  z P } = P ). Given (  X ,  X , min it is known that the required n can be approximated by [15]: For example, if we let (  X ,  X , min  X  t )=( . 05 ,. 20 ,. 50) hen X  X  five-eighty convention [9, 11] with Cohen X  X  medium effect), As this is only an approximation, we need to check that the desired power is actually achieved with an integer n close to 33 . we let n =33 . Then, by substituting w = t (33  X  1; . 05) = 2 and  X  t = min  X  t = . 50 to Eq. 5, we obtain: 1  X   X   X  Pr { u  X  X  X  4 . 742 } +1  X  Pr { u  X  X  X  . 825 } = . 795 which means that the desired power of 0.8 is not quite achieved. So we let n =34 , and the achieved power can be computed similarly: 1  X   X  = . 808 . Therefore n =34 is the topic set size we want.
Our Excel tool samplesizeTTEST (See Section 7) automates the above procedure for any given combination of (  X ,  X , min Table 1 shows the required topic set sizes for the paired t -test for some typical combinations. For example, under Cohen X  X  five-eighty convention (  X  = . 05 , X  = . 20 ) 5 , if we want the minimum de-tectable effect to be min  X  t = . 2 (i.e., one-fifth of the score-difference standard deviation), we need n = 199 topics.
Strictly speaking, Cohen X  X  criteria are for unpaired tests; effect sizes for paired and unpaired tests are not directly comparable [17].
Note that this convention, which implies that a Type I error is four times as serious as a Type II error, is only a convention [11]. Researchers should consider whether this is appropriate for their experiments, and should not follow it blindly.

The above approach starts by requiring a min  X  t , which is in-dependent of the evaluation method (i.e., the measure, pool depth and the measurement depth). However, researchers may want to require a minimum detectable absolute difference min D t in terms of a particular evaluation measure M instead (e.g.,  X  X  want high power guaranteed whenever the true absolute difference in mean AP is 0.05 or larger. X ). In this case, instead of setting a mini-the numerator of Eq. 3: we guarantee a power of 1  X   X  when- X  (=  X  2 X +  X  2 Y ) , which we denote by  X   X  2 t , so that we can convert min D t to min  X  t = min D t /  X   X  2 t and follow the aforementioned procedure for finding the right n .The samplesizeTTEST tool has a separate sheet for computing n from (  X ,  X , min D t shall discuss how to obtain  X   X  2 t from past data in Section 5.
This section discusses how to set the topic set size n when we as-sume that there are m  X  2 systems to be compared using one-way ANOVA. Let x ij denote the score of the i -th system for topic j in terms of some measure M ; we assume that { x ij } are independent and that x ij  X  N (  X  i , X  2 ) . Note the homoscedasticity assumption: the variance  X  2 is assumed to be common across systems. (We did not assume this when we discussed the paired t -test.) We define the population grand mean  X  and the i -th system effect a i as follows: a m =0 ) while the alternative hypothesis H 1 is that at least one of the system effects is not zero. The basic statistics that we compute for the ANOVA are as follows. Let  X  x i = 1 n n j =1 x ij (sample sys-into between-system and within-system variations S A and S E (i.e., S m (
Let F be a random variable that obeys an F distribution with (  X  A , X  E ) degrees of freedom; let F (  X  A , X  E ; P ) denote the critical F value for probability P (i.e., Pr { F  X  F (  X  A , X  E ; P Under H 0 , the test statistic F 0 defined below obeys a (central) F distribution with (  X  A , X  E ) degrees of freedom: Table 2: Linear approximation of  X  , the noncentrality param-eter of a noncentral  X  2 distribution [15]. Given a significance criterion  X  , we reject H 0 if F 0  X  (The p -value is given by Pr { F  X  F 0 } under H 0 .) From Eq. 11, it can be observed that H 0 is rejected if the between-system variation S if the sample size n is large. Again, the p -value does not tell us which is the case.
 The probability of rejecting H 0 is given by Pr { F 0  X  F (  X  A , X  E ;  X  ) } =1  X  Pr { F 0  X  F (  X  A , X  Under H 0 , Eq. 12 amounts to  X  by construction, where F 0 obeys a (central) F distribution as mentioned above. Under H 1 , Eq. 12 represents the power (1  X   X  ) ,where F 0 obeys a noncentral F dis-tribution with (  X  A , X  B ) degrees of freedom and a noncentrality parameter  X  = n  X  ,where Thus  X  measures the total system effects in variance units .
While computations involving a noncentral F distribution can be complex, a normal approximation is available: let F denote a random variable that obeys the aforementioned noncentral F dis-tribution; let u  X  N (0 , 1 2 ) . Then: Pr { F  X  w } X  Pr { u  X  where Hence, given ( n,  X  , X  ) , the power (1  X   X  ) can be computed from Eqs. 12-15 as [15]: 1  X  Pr { u  X  where w = F ( m  X  1 ,m ( n  X  1);  X  ) . But what we are more inter-ested in is: given (  X ,  X ,  X ) , what is the required n ? Under H 0 , we know that  X =0 (See Eq. 13). However, under H 1 , all we know is that  X  =0 . In order to require that an experi-ment has a statistical power of 1  X   X  , a minimum detectable delta min  X  must be specified in advance. Let us require that we cor-rectly reject H 0 with 100(1  X   X  ) % confidence whenever the range of the population means ( D =max i a i  X  min i a i )isatleastas large as a specified value ( min D ), and let min  X = min D That is,  X  is bounded below by min  X  as defined above. Hence, although specifying min D does not uniquely determine  X  (as depends on systems other than the best and the worst ones), we can plug in  X = min  X  toEqs.15and16toobtaintheworst-case estimate of the power.

Unfortunately, no closed formula similar to Eq. 6 is available for ANOVA. However, since the worse-case estimate of n can be obtained as n =  X / min  X =2  X  2  X / min D 2 , we can first estimate  X  as follows. (How to obtain  X   X  2 , the estimate of  X  2 , is discussed in Section 5.) Recall that, under H 1 , Eq. 12 represents the power (1  X   X  ) where F 0 obeys a noncentral F distribution with degrees of freedom and the noncentrality parameter  X  . By letting  X  E = m ( n  X  1)  X  X  X  , the power can be approximated by: where  X  2 is a random variable that obeys a noncentral  X  2 tribution with  X  A degrees of freedom whose noncentrality param-eter is  X  ,and  X  2 (  X  ; P ) is the critical  X  2 value for probability P of a random variable that obeys a (central)  X  2 distribution with  X  degrees of freedom (i.e., Pr {  X  2  X   X  2 (  X  ; P ) } = P ). For non-central  X  2 distributions, some linear approximations of  X  are avail-able, as shown in Table 2 [15]. Hence an initial estimate of n given (  X ,  X , min D,  X   X  2 ,m ) can be obtained as shown below.
Suppose we let (  X ,  X , min D,m )=( . 05 ,. 20 ,. 5 , 3) and that we obtained  X   X  2 = . 5 2 from past data so that min  X = min D . / (2  X  . 5 2 )= . 5 .Then  X  A = m  X  1=2 and  X  =4 . 860 + 3 . 584  X  n =19 ,then  X  E =3(19  X  1) = 54 , w = F (2 , 54; . 05) = 3 . From Eq. 15, c A =1 . 826 , X   X  A =6 . 298 , and from Eq. 16, the achieved power is 1  X  Pr { u  X  X  X  . 809 } = . 791 , which does not quite satisfy the desired power of 80%. On the other hand, if n 20 , the achieved power can be computed similarly as . 813 . Hence n =20 is what we want. Our Excel tool samplesizeANOVA (See Section 7) 7 automates the above procedure for given (  X ,  X , min D,  X   X  2 ,m ) .
Recall that for our topic set size design based on the paired t -test, we need an estimate of  X  2 t =  X  2 X +  X  2 Y to compute min (Section 3.2). Similarly, for our topic set size design based on one-way ANOVA, we need an estimate of  X  2 to compute min  X  (Sec-tion 4.2). There are time-honoured methods for estimating the pop-ulation variances from ANOVA statistics; for one-way ANOVA, the following estimate is available [17]: where the symbols used represent n , m , V A and V E as already de-fined, except that here we emphasise that they are computed from
Let A =max i a i and a =min i a i .Then D 2 / 2=( A 2 + a 2  X  2 Aa ) / 2  X  A 2 + a 2  X  m A = D/ 2 ,a =  X  D/ 2 and a i =0 for all other systems.
While samplesizeTTEST handles arbitrary values of (  X ,  X  ) samplezieANOVA can only handle the four combinations shown in Table 2. Table 3: TREC test collections and runs used for estimating  X  levels as follows:  X  2 and 0  X  L0 (i.e., nonrelevant); 1  X  3  X  L3; 4  X  L4. Table 5:  X   X  2 for different evaluation measures with measure-ment depth l . past data. The first term is an estimate of the population between-system variance  X  2 A ; the second term is an estimate of the popula-tion within-system variance  X  2 E . These estimates are also used for computing accurate effect size estimates for ANOVA [17].
Given an n -by-m topic-by-system score matrix for a particular evaluation measure M from past data, we can easily obtain substituting the ANOVA statistics to Eq. 19, under the homoscedas-ticity assumption. Now, if we introduce homoscedasticity to the paired t -test as well, it seems reasonable to obtain the required vari-ance estimate for the score differences as  X   X  2 t = X   X  2
To enhance the reliability of the variance estimates, it is possible to pool variances across data sets for a given IR task. Let C denote and the variance estimate obtained from C using Eq. 19. We then use the following pooled variance estimate:
Table 3 provides some statistics of the past data that we used for obtaining  X   X  2  X  X . We considered three IR tasks : (a) adhoc news re-trieval; (b) adhoc web search; and (c) diversified web search; for each task, we used two data sets to obtain pooled variance esti-mates. The adhoc/news data sets are from the TREC robust tracks, with  X  X ew X  topics from each year [24, 25]. (We also conducted some experiments with the  X  X ld X  topics of the TREC 2004 robust track, following Webber, Moffat and Zobel [27], and the results ob-tained were similar to the ones reported in this paper. However, the data set is not ideal for our pool depth reduction experiment (Sec-tion 6.3), as the relevance assessment pools for the old topics come from old TREC ad hoc runs, not the TREC 2004 robust track runs. Moreover, the relevance assessments for the old topics are binary, even though we are primarily interested in graded-relevance mea-sures.) The web data sets are from the TREC web tracks [7, 8]. While we considered the measurement depths of l =10 , 1000 adhoc/news, we considered only l =10 for the web tasks as we are interested in the quality of the first search engine result page.
The actual variance depends on the evaluation measure and con-ditions associated with it. Table 4 shows the evaluation measures considered in this study. For the adhoc/news and adhoc web tasks, we consider the binary Average Precision (AP), Q-measure (Q), normalised Discounted Cumulative Gain (nDCG) and normalised Expected Reciprocal Rank (nERR), all computed using the NTCIREVAL toolkit 8 . For the diversity/web task, we consider  X  -nDCG and Intent-Aware nERR (nERR-IA) computed using ndeval 9 ,aswellasD-nDCGandD -nDCG computed using NTCIREVAL . When using NTCIREVAL , the gain value for each L x -relevant document was set to g ( r )=2 x  X  1 : for example, the gain for an L3-relevant document is 7, while that for an L1-relevant document is 1. As for ndeval , the default settings were used: this program ignores per-intent graded relevance levels.

Table 5 shows the variance estimates obtained for each evalua-tion measure and for each task, using Eqs. 19 and 20. It can be observed that nERR is highly unstable for Tasks (a1), (a2) and (b). Due to its diminishing return property [5, 19], (n)ERR ba-sically ignores most retrieved relevant documents except for the ones retrieved at the very top. Relying on fewer data points hurts statistical stability, and hence calls for a very large topic set size, as we shall see later. As for AP, it is much more unstable than Q and nDCG for Task (b) (adhoc/web), which suggests that the use of graded relevance is important in evaluating this task. Note also that AP is less stable than Q and nDCG for Task (a2), that is, adhoc/news with a small measurement depth. Finally, for Task (c) (diversity/web), it can be observed that  X  -nDCG and nERR-IA have high variances compared to D-nDCG and D -nDCG: this is http://research.nii.ac.jp/ntcir/tools/ ntcireval-en.html . For computing AP and Q, we fol-low Sakai and Song [21] and divide by min( l, R ) rather than by R in order to properly handle small measurement depths. http://trec.nist.gov/data/web/12/ndeval.c Table 6: Topic set size table for (  X ,  X  )=(0 . 01 , 0 . 10) 2 : t -test vs. one-way ANOVA. because  X  -nDCG and nERR-IA possess the per-intent diminish-ing return property; it is known that these two measures behave similarly [5, 6, 22]. While D-nDCG does not have the diminish-ing return property, D -nDCG compensates for this by averaging D-nDCG with intent recall (a.k.a. subtopic recall ) [19, 21].
Note that within each IR task, the variance estimates are very similar. Henceforth, we shall use the pooled variance estimates (shown in bold in Table 5) to compute the required topic set sizes n based on ANOVA. As for the case with the t -test, we use the same pooled estimates to obtain  X   X  2 t =2 X   X  2 .
Table 6 compares the required topic set sizes computed based on the t -test and one-way ANOVA, when we want to design a test col-lection for comparing a pair of systems ( m =2 ) under (  X ,  X  (0 . 01 , 0 . 10) .The t -test row uses samplesizeTTEST (See Sec-tion 3.2) while the ANOVA row uses samplesizeANOVA (See Section 4.2), using the pooled variance estimates shown in Table 5. Recall that while min D t is the minimum detectable difference be-tween two systems being compared with the t -test, min D is the minimum detectable difference between the best and the worst sys-tems when m systems are being compared with ANOVA. Hence min D is equivalent to min D t when m =2 . Table 7 provides similar information under a less demanding condition of ( (0 . 05 , 0 . 20) , i.e., Cohen X  X  five-eighty convention. (Throughout this paper, we use boldface whenever we show topic set size estimates under Cohen X  X  convention within tables.) For example, Table 7(a1) (adhoc/news, l = 1000 ) says that, if we require the minimum de-tectable difference of min D t =0 . 05 under Cohen X  X  five-eighty convention, t -tests with AP, Q, nDCG and nERR would require 331, 336, 353, 753 topics, respectively; similarly, ANOVA ( m 2 ) with AP, Q, nDCG and nERR would require 322, 326, 343, 733 topics, respectively. It can be observed that the t -test and ANOVA results are very similar, and that the ANOVA-based estimates are slightly smaller, despite the fact that the t -test exploits the paired data information (i.e., that the per-topic score x i corresponds to y i ). This may be because the  X   X  2 values are overestimates: if this Table 7: Topic set size table for (  X ,  X  )=(0 . 05 , 0 . 20) 2 : t -test vs. one-way ANOVA. is the case, the error is doubled when we compute  X   X  2 t for the t -test-based sample size design. Clearly, if the variance is overestimated, the required topic set size will also be overesti-mated. However, since we want to guarantee low probabilities of Type I and Type II errors, it seems appropriate to  X  X rr on the side of oversampling X  as suggested by Ellis [11]. Henceforth, we use samplesizeANOVA to discuss the general case of m  X  2 .
Tables 8-11 show the required topic set sizes for different IR tasks under different statistical requirements, for m =10 Again, the pooled variance estimates shown in Table 5 were used for the computation. The interested reader can use samplesizeANOVA to easily reproduce our results or try other parameter settings, with her own evaluation measures and variance estimates.

Tables 8 and 9 are the topic set size tables for the adhoc/news task with l = 1000 and l =10 . We can observe that: Table 8: Topic set size table ( m =10 , 100 ) for adhoc/news ( l 1000 ) with AP/Q/nDCG/nERR. Table 9: Topic set size table ( m =10 , 100 ) for adhoc/news ( l 10 ) with AP/Q/nDCG/nERR.
Table 10 is the topic set size table for the adhoc/web task with l =10 . It can be observed that: Table 10: Topic set size table ( m =10 , 100 ) for adhoc/web ( l 10 ) with AP/Q/nDCG/nERR ( l =10 ). Table 11: Topic set size table ( m =10 , 100 ) for diversity/web ( l =10 )with  X  -nDCG/nERR-IA/D-nDCG/D -nDCG. The advantage of Q and nDCG over AP as demonstrated in both Table 9 (adhoc/news) and Table 10 (adhoc/web) strongly suggests the importance of utilising graded relevance assessments when the measurement depth is shallow. On the other hand, when the mea-surement depth is large (e.g., l = 1000 ), how many relevant doc-uments have been retrieved, and at what positions, probably out-weigh whether each document is highly or partially relevant.
Table 11 is the topic set size table for the diversity/web task with l =10 ; note that this table discusses four diversity measures. It can be observed that: As was mentioned in Section 5.2, the statistical stability of D-nDCG arises from the fact that it lacks the per-intent diminshing return property: unlike nERR-IA and  X  -nDCG, it pays attention to every relevant document returned for each intent. Note that the statistical stability of an evaluation measure does not imply that the measure measures  X  X hat we want to measure. X  10 .

Figures 1-3 visualise the relationships between the required topic set size ( n ) and the number of systems to be compared ( m ) under (  X ,  X , min D )=(0 . 05 , 0 . 20 , 0 . 05) . For example, Figure 2 shows that, if we expect to compare m = 200 adhoc/web systems 11 the above set of requirements, AP and nERR would require 3,819 and 3,982 topics, while Q and nDCG would require only 1,690 and 2,033 topics, respectively. Similarly, Figure 3 shows that, if we expect to compare m = 200 diversity/web systems under the above set of requirements,  X  -nDCG and nERR-IA would require 3,630 and 3,911 topics, while D-nDCG and D -nDCG would require only 1,637 and 2,385 topics, respectively.

From all of the results we have reported so far, it seems advisable to choose evaluation measures at the test collection design phase, as different evaluation measures have different variances and there-fore require different topic set sizes under the same set of statistical requirements. Note that our method provides a method to compare evaluation measures in terms of practical significance [11], which in our case means the assessment cost. For example, while nERR has an intuitive user model (i.e., the diminishing return property, which says that the user does not value  X  X edundant X  documents), it is important to see beforehand that it can be twice as costly as some of the other alternatives.
The analysis in Section 6.2 covered adhoc/news, adhoc/web and diversity/web search tasks, but assumed that the pool depth was a given. In this section, we focus our attention to the adhoc/news task (with l = 1000 ), where we have depth-100 and depth-125 pools (See Table 3), which gives us the option of reducing the pool depth. Hence we can discuss the total assessment cost by multiplying n by the average number of documents that need to be judged per topic for a given pool depth pd .

From the original TREC03new and TREC04new relevance as-sessments, we created depth-pd ( pd = 100 , 90 , 70 , 50 sions of the relevance assessments by filtering out all topic-document pairs that were not contained in the top pd documents of any run. Using each set of the depth-pd relevance assessments, we re-evaluated all runs using AP, Q, nDCG and nERR. Then, using these new
Sakai and Song [22] reported that D-nDCG and D -nDCG outper-form nERR-IA and  X  -nDCG in terms of the concordance test ,that is, how often they agree with straightforward measures like preci-sion and intent recall when two ranked lists are being compared.
This setting is not unrealistic. For example, the TREC 2011 Mi-croblog track received 184 runs fro m 59 participating teams [18]. topic-by-run matrices, new variance estimates were obtained and pooled as described in Section 5.

Table 12 shows the pooled variance estimates obtained from the depth-pd versions of the TREC03new and TREC04new relevance assessments. It also shows the average number of documents judged per topic for each pd . For example, while the original depth-125 relevance assessments for TREC03new contain 47,932 topic-document pairs, its depth-100 version has 37,605 pairs across 50 topics; the original TREC04new depth-100 relevance assess-ments have 34,792 pairs across 49 topics. Hence, on average, (37 , 605 + 34 , 792) / (50 + 49) = 731 documents are judged per topic when pd = 100 . Similarly, (4 , 905+4 , 581) / (50+49) = 96 documents are judged per topic when pd =10 . In our analysis dis-cussed below, we assume that the average number of documents judged is a constant for a given pd , though in reality it depends on the number and the diversity of runs besides pd .

Figures 4-5 plot the required topic set size n against the average number of documents judged per topic, for min D =0 . 05 , and m =10 , 100 under Cohen X  X  five-eighty convention. Note that while the plots in the four figures look identical (as they should), the y -axis scales are very different. For example, Figure 5 (bottom) shows that, under (  X ,  X , min D,m )=(0 . 05 , 0 . 20 , 0 .
It is a well-known fact that it is better to have many topics with few judgments per topic than to have few topics with many judg-ments per topic (e.g., [3, 4, 27]). Our present analysis confirms this through a simple visualisation with a theoretical underpinning by means of sample size design for ANOVA. Similar results can be obtained using sample size design for the t -test for m =2 courage researchers who plan to build a test collection to use some past data and the expected number of systems to compare ( m )and conduct an analysis such as the one we have demonstrated: then they can choose the combination of n and pd depending on the budget. For example, a researcher with a budget for 150,000 rele-vance assessments may look at Figure 5 (bottom) and decide to go with pd =30 ( n = 476 ) in order to use AP: the actual cost in this case would be about 476  X  253 = 120 , 428 &lt; 150 , 000 (See Ta-ble 12). While this is 2.2 times as expensive as the aforementioned case with n = 569 , pd =10 , having more judgments is of course desirable if the test collection is going to be reused later.
We demonstrated how a researcher can design a new test collec-tion for comparing m (  X  2) systems using power analysis and sam-ple size design techniques with variance estimates from past data. While both t -test-based and ANOVA-based methods are available, the t -test approach suffers from the family-wise error rate problem and does not provide a completely sound solution to the evaluation of m ( &gt; 2) systems. We thus recommend the use of our ANOVA-based method for designing new test collections based on statistical requirements and the expected number of systems to compare. Our Table 12: Number of relevance assessments and pooled  X   X  2 experiments with several IR tasks suggest that as different eval-uation measures have different variances, test collection builders should carefully choose evaluation measures at the test collection design phase. We also showed how a cost analysis can be con-ducted through a pool depth reduction experiment using past test collections and runs. Although it is possible to reduce the assess-ment cost dramatically while preserving the statistical reliability by having many topics with shallow pools, the actual design should probably be determined based on the available budget if the test collection will be reused later. The relationship between statistical reliability and reusability will be examined in our future work.
Our methods can also be used to compare evaluation measures in terms of practical significance:  X  X ow much assessment cost will each of the candidate evaluation measures require under the same set of statistical requirements? X  While discriminative power [19] is often used to compare the statistical reliability of evaluation mea-sures, our methods can translate t he reliability into actual cost.
The experiments reported in this paper are reproducible: the topic set size computation tools and all topic-by-run performance matrices used in this study are available from our website encourage the interested reader to try computing topic set sizes for their own new test collections and evaluation measures, with their own variance estimates. We believe that improving test collection design based on past experience is important: perhaps it is time to stop producing (say) n =50 topics every year without heeding statistical power. Note that, if the research community shares the basic ANOVA statistics used in Eq. 19, then variance estimates can easily be obtained from past data, and the estimation accuracy can be improved as we accumulate more test collections. This is much easier than sharing a repository of the actual run data, so we hope to put this into practice at evaluation venues such as TREC and NTCIR.

Finally, we stress again that our techniques and tools are applica-ble to any non-IR tasks (e.g., question answering, summarisation, machine translation, recommendation etc.) as well, as long as the paired t -test or ANOVA is applicable with the task and test collec-tion in question.
 This research is a part of Waseda University X  X  project  X  X axonomis-ing and Evaluating Web Search Engine User Behaviours, X  sup-ported by Microsoft Research. http://www.f.waseda.jp/tetsuya/tools.html and http://www.f.waseda.jp/tetsuya/data.html Figure 1: Required number of topics n against the num-ber of systems ( m ), for adhoc/news with (  X ,  X , minD )= (0 . 05 , 0 . 20 , 0 . 05) . Figure 2: Required number of topics n against the num-ber of systems ( m ), for adhoc/web with (  X ,  X , min D )= (0 . 05 , 0 . 20 , 0 . 05) . Figure 3: Required number of topics n against the num-ber of systems ( m ), for diversity/web with (  X ,  X , min D (0 . 05 , 0 . 20 , 0 . 05) . Figure 4: Required number of topics n against the average number of documents judged for a given pool depth, for ad-hoc/news ( l = 1000 )with (  X ,  X  )=(0 . 05 , 0 . 20) ,m =10 Figure 5: Required number of topics n against the average number of documents judged for a given pool depth, for ad-hoc/news ( l = 1000 )with (  X ,  X  )=(0 . 05 , 0 . 20) ,m = 100
