 The query-performance prediction (QPP) task is estimat-ing retrieval effectiveness with no relevance judgments. We present a novel probabilistic framework for QPP that gives rise to an important aspect that was not addressed in previ-ous work; namely, the extent to which the query effectively represents the information need for retrieval. Accordingly, we devise a few query-representativeness measures that uti-lize relevance language models. Experiments show that inte-grating the most effective measures with state-of-the-art pre-dictors in our framework often yields prediction quality that significantly transcends that of using the predictors alone.
The task of estimating retrieval effectiveness in the ab-sence of relevance judgments  X  a.k.a. query-performance prediction (QPP)  X  has attracted much research attention [2]. Interestingly, an important aspect of search effectiveness has been overlooked, or not explicitly modeled, in previously proposed prediction approaches; namely, the presumed ex-tent to which the query effectively represents the underlying information need for retrieval.

Indeed, an information need can be represented by various queries which in turn might represent various information needs. Some of these queries might be more effective for retrieval over a given corpus than others for the information need at hand. Furthermore, relevance is determined with respect to the information need rather than with respect to the query. These basic observations underlie the develop-ment of the novel query-performance prediction framework that we present. A key component of the framework is the use of measures for the query representativeness of the in-formation need. We propose several such measures that are based on using relevance language models [8].

Empirical evaluation shows that integrating the most ef-fective representativeness measures with state-of-the-art pre-dictors in our framework yields prediction quality that often significantly transcends that of using these predictors alone.
Our query-performance prediction framework essentially generalizes a recently proposed framework [7], the basis of which was the estimation of the relevance of a result list to a query. Our framework relies on the basic definition of relevance with respect to the information need, and there-fore accounts for the connection between the query and the information need. This connection was not (explicitly) ad-dressed in previous work including [7]. For example, pre-retrieval predictors, which use only the query and corpus-based statistics, are mostly based on estimating the discrim-inative power of the query with respect to the corpus, but do not account for the query-information need connection.
Post-retrieval predictors analyze also the result list of top-retrieved documents [2]. Our framework provides formal grounds to integrating pre-retrieval, post-retrieval, and query-representativeness, which turn out to be three complemen-tary aspects of the prediction task. Furthermore, we demon-strate the merits of integrating post-retrieval predictors with query representativeness measures in the framework.
The query representativeness measures that we devise uti-lize relevance language models [8]. Relevance models were used for other purposes in various predictors [3, 14, 5, 10]. We demonstrate the merits of integrating in our framework one such state-of-the-art predictor [14].
Let q , d and D denote a query, a document, and a corpus of documents, respectively. The task we pursue is estimating the effectiveness of a retrieval performed over D in response to q when no relevance judgments are available [2]  X  i.e., query performance prediction (QPP).

Let I q be the information need that q represents. Since relevance is determined with respect to I q rather than with respect to q , the QPP task amounts, in probabilistic terms, to answering the following question:  X  X hat is the probability that the result list D res , of the most highly ranked documents with respect to q , is relevant to I
Formally, the task is estimating where r i s the relevance event and p ( r | I q , D res ) is the prob-ability that the result list D res satisfies I q .
Estimating p ( D res | I q , r ) is the (implicit) basis of many post-retrieval prediction methods, if q serves for I q , as re-cently observed [7]. The denominator, p ( D res | I q ), is the probability that the result list D res is retrieved using some representation of I q regardless of relevance. If q is used for I , then the probability of retrieving D res depends on the properties of the retrieval method employed. Accordingly, the denominator in Equation 1 can serve as a normalizer across different retrieval methods [7]. However, standard QPP evaluation [2] is based on estimating the retrieval effec-tiveness of a fixed retrieval method across different queries. Thus, the denominator in Equation 1 need not be computed for such evaluation, if q serves for I q [7].

The (novel) task we focus on is estimating the probability p ( r | I q ) from Equation 1 that a relevance event happens for I . Obviously, the ability to satisfy I q depends on the cor-pus D ; e.g., if there are no documents in D that pertain to I then the estimate should be zero. Furthermore, the sat-isfaction of I q also depends on the query q used to represent it. Thus, the estimate for p ( r | I q ) can be approximated by: where  X  p (  X  ) is an estimate for p (  X  ).

The estimate  X  p ( q | I q , D ) for the probability that q is chosen to represent I q for retrieval over D can be used to account, for example, for personalization aspects. We leave this task for future work, and assume here a fixed user model, and accordingly, a fixed (across queries)  X  p ( q | I q , D ).
If we use q for I q in the estimate  X  p ( r | I q , D ), we get the probabilistic basis for pre-retrieval prediction methods [6, 4]. These predictors implicitly estimate the probability for a relevance event using information induced from the query and the corpus, but not from the result list ( D res ).
The task left for completing the instantiation of Equation 2, and as a result that of Equation 1, is devising  X  p ( q | I  X  the estimate for the probability that q is the most likely query to effectively represent I q for retrieval over D .
The only signal about the information need I q is the (short) query q . To induce a  X  X icher X  representation for I q , we use the generative theory for relevance [8]. Specifically, we con-struct a (unigram) relevance language model R from doc-uments in the corpus D . (Details are provided in Section 4.1.) Then, estimating q  X  X  representativeness amounts to es-timating the probability p ( q | R, D , r ) of generating q by R . Henceforth, we refer to such estimates as measures of q  X  X   X  X epresentativeness X , denoted X ( q ; R ).

We assume, as in the original relevance model X  X  formula-tion [8], that q  X  X  terms ( { q i } ) are generated independently by R :  X  p ( q | R, D , r ) def = Q q ity assigned to q i by R . To prevent the query-length bias, we use the geometric mean of the generation probabilities which results in the GEO measure: | q | is the number of terms in q .

We also consider the arithmetic mean of the generation probabilities, ARITH , as a representativeness measure: F or comparison purposes, we study the min and max aggre-gators of the generation probabilities:
Another measure that we consider is the weighted entropy of R , where q  X  X  terms are assigned with a unit weight and all other terms in the vocabulary are assigned a zero weight: The underlying assumption is that high entropy, which im-plies to a relatively uniform importance assigned to q  X  X  terms by R , is indicative of effective representation by q . Indeed, too little emphasis on some query aspects was identified as a major cause for retrieval failures [1].
We next present an evaluation of our query-performance prediction (QPP) framework. We begin by describing the experimental setup in Section 4.1. In Section 4.2.1 we fo-cus on using the query-representativeness measures. To that end, we use an oracle-based experiment where the relevance model is constructed only from relevant documents. In Sec-tion 4.2.2 we study the integration of the representativeness measures with post-retrieval predictors in our framework.
Table 1: TREC datasets used for experiments. Table 1 presents the TREC datasets used for experiments. TREC12, TREC5 and ROBUST are composed (mostly) of newswire documents, while WT10G is a noisy Web collec-tion. Titles of TREC topics serve for queries. Documents and queries were stemmed with the Krovetz stemmer and stopwords (on the INQUERY list) were removed. The Indri toolkit (www.lemurproject.org) was used for experiments.
Following common practice [2], prediction quality is mea-sured by the Pearson correlation between the true average precision (AP@1000) for the queries, as determined using the relevance judgments in the qrels files, and the values assigned to these queries by a predictor.

The query likelihood method [11] serves for the retrieval method, the effectiveness of which we predict. Document d  X  X  retrieval score is the log query likelihood: log Q q p ( q i | d ) is the probability assigned to q i by a Dirichlet smoothed unigram language model induced from d w ith the smoothing parameter set to 1000 [13].

We use relevance model #1 (RM1) [8] in the query repre-a set of documents; p ( w | d ) is the maximum likelihood esti-mate of term w with respect to d ; p ( d | q ) is (i) 1 | S | a set of relevant documents as is the case in Section 4.2.1; and, (ii) d  X  X  normalized query likelihood: p ( q | d ) P S is the set of all documents in the corpus that contain at least one query term as is the case in Section 4.2.2. No term clipping was employed for RM1.
The query-representativeness measures play an important role in our QPP framework, and are novel to this study. Thus, we first perform a controlled experiment to explore the potential extent to which these measures can attest to query performance. To that end, we let the measures use a relevance model of a (very) high quality. Specifically, RM1 is constructed from all relevant documents in the qrels files as described in Section 4.1. Table 2 presents the prediction quality of using the representativeness measures by them-selves as query-performance predictors. As can be seen, the prediction quality numbers are in many cases quite high. All these numbers  X  which are Pearson correlations  X  are dif-ferent than zero to a statistically significant degree according to the two-tailed t-test with a 95% confidence level.
We can also see in Table 2 that GEO is the most effec-tive measure except for TREC5. ARITH and MIN are also quite effective, although often less than GEO. ENT is highly effective for TREC5 and WT10G but much less effective for TREC12 and ROBUST. The MAX measure is evidently less effective than the others, except for TREC5. All in all, we see that different statistics of the generation proba-bilities assigned by the relevance model to the query terms can serve as effective query representativeness measures for query-performance prediction.
 Table 2: Using the representativeness measures by t hemselves as query-performance predictors with RM1 constructed from relevant documents. Bold-face: the best result in a column.  X  X  X ,  X  X  X ,  X  X  X ,  X  X  X  and  X  X  X  mark statistically significant differences in cor-relation [12] with GEO, ARITH, MIN, MAX, and ENT, respectively.
Query-representativeness measures are one component of our QPP framework. Other important components are post-retrieval and pre-retrieval prediction as described in Section 3. Since (i) the query representativeness measures constitute a novel contribution of this paper, (ii) the merits of the in-tegration of post-retrieval and pre-retrieval prediction were already demonstrated in previous work [7], and, (iii) post-retrieval predictors often yield prediction quality that is sub-stantially better than that of pre-retrieval predictors [2], we focus on the integration of the representativeness measures with the post-retrieval predictors in our framework. The in-tegration is performed using Equations 1 and 2. In contrast to the case in Section 4.2.1, we use the standard practical QPP setting; that is, no relevance judgments are available. The relevance model used by the query-representativeness measures is constructed as described in Section 4.1 from all the documents in the corpus that contain at least one query term. Using only top-retrieved documents for constructing the relevance model resulted in inferior prediction quality. Three state-of-the-art post-retrieval predictors, NQC [9], WIG [14] and QF [14], are used. As these predictors incor-porate free parameters, we apply a train-test approach to set the values of the parameters. Since Pearson correlation is the evaluation metric for prediction quality, there should be as many queries as possible in both the train and test sets. Thus, each query set is randomly spit into two folds (train and test) of equal size. We use 40 such splits and report the average prediction quality over the test folds. For each split, we set the free-parameter values of each predictor by maximizing prediction quality over the train fold.
NQC and WIG analyze the retrieval scores of top-retrieved documents, the number of which is set to values in { 5 , 10 , 50 , 100 , 500 , 1000 } . QF incorporates three parameters. The number of top-retrieved documents used to construct the rel-evance model (RM1) utilized by QF is selected from { 5 , 10 , 25 , 50 , 75 , 100 , 200 , 500 , 700 , 1000 } and the number of terms used by this RM1 is set to 100 following previous recommenda-tions [10]. The cuttoff used by the overlap-based similarity measure in QF is set to values in { 5 , 10 , 50 , 100 , 500 , 1000 } .
In Table 3 we present the average (over the 40 test folds) prediction quality of using the query-representativeness mea-sures alone; using the post-retrieval predictors alone; and, integrating the representativeness measures with the post-retrieval predictors in our framework. Although the query-representativeness measures do not incorporate free param-eters, we report their prediction quality when used alone us-ing the same test splits. When the measures are integrated with the post-retrieval predictors, the free-parameters of the integration are those of the post-retrieval predictors. In this case, the parameters are tuned by optimizing the prediction quality of the integration over the train folds, as is the case when using the post-retrieval predictors alone. Differences of prediction quality (i.e., Pearson correlations) are tested for statistical significance using the two tailed paired t-test computed over the 40 splits with a 95% confidence level. 1
We first see in Table 3  X  specifically, by referring to the underlined numbers  X  that the best prediction quality for the majority of the corpora is attained by integrating a rep-resentativeness measure with a post-retrieval predictor. Further exploration of Table 3 reveals the following. The GEO and ARITH measures are effective  X  specifically, in comparison to the other representativeness measures which is reminiscent of the case in Table 2  X  both as stand-alone
N ote that the numbers in Table 2 are not comparable to those in Table 3. This is because the latter presents aver-ages over the train-test splits while the former is based on using the all queries for the test set. Furthermore, as noted above, the relevance models used for the representativeness measures are constructed using different sets of documents. Table 3: Average prediction quality over the t est folds of the query-representativeness mea-sures, post-retrieval predictors, and their integra-tion (marked with  X  ). Boldface: the best re-sult per corpus and a post-retrieval block; under-line: the best result in a column.  X  X  X  and  X  X  X  mark statistically significant differences with using the query-representativeness measure alone and the post-retrieval predictor alone, respectively. predictors and when integrated with the post-retrieval pre-dictors. Indeed, integrating each of GEO and ARITH with a post-retrieval predictor yields prediction quality that tran-scends that of using the post-retrieval predictor alone in 9 out of the 12 relevant comparisons (three post-retrieval pre-dictors and four corpora); many of these improvements are substantial and statistically significant.

These findings, as those presented above, attest to the merits of our QPP framework that integrates two differ-ent, and evidently complementary, aspects of prediction; namely, post-retrieval analysis of the result list and query-representativeness estimation. 2
In comparing the prediction quality numbers in Table 3 for the three post-retrieval predictors we make the following ob-servation. For QF and WIG the integration with the query-representativeness measures yields the highest and lowest number, respectively, of cases of improvement over using the post-retrieval predictor alone.
I t is not a surprise, therefore, that the post-retrieval pre-dictors when used alone outperform in most cases the rep-resentativeness measures when used alone. This is because the post-retrieval predictors analyze the result list, while the representativeness measures do not. For TREC5, how-ever, the reverse holds. Presumably, this is because there are only 50 queries for TREC5, while for all other corpora there are at least 100 queries. A relatively small query set makes it difficult to learn the free-parameter values of the post-retrieval predictors, while representativeness measures do not incorporate free parameters.
We presented a novel probabilistic framework for the query-performance prediction task. The framework gives rise to an important aspect that was not addressed in previous work: the extent to which the query effectively represents the un-derlying information need for retrieval. We devised query-representativeness measures using relevance language mod-els. Empirical evaluation showed that integrating the most effective measures with state-of-the-art post-retrieval pre-dictors in our framework often yields prediction quality that significantly transcends that of using the predictors alone.
Devising additional query-representativeness measures, and integrating pre-retrieval predictors with post-retrieval pre-dictors and query-representativeness measures in our frame-work, are future venues to explore.
We thank the reviewers for their comments. This work has been supported in part by the Israel Science Foundation under grant no. 433/12 and by a Google faculty research award. Any opinions, findings and conclusions or recom-mendations expressed in this material are the authors X  and do not necessarily reflect those of the sponsors.
