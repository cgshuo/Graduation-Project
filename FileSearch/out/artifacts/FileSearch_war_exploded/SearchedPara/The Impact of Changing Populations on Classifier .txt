 in medical technology it is likely to be unimportant. 
In commercial applications, such as the credit granting example we discuss below, it is likely to be very important: the population of applicants will change in response to changing economic conditions and a changing competitive environment over the life of a financial product. 
Machine learning work and Bayesian algorithms of-ten emphasise adaptive models, developing methods for sequential estimation, in which the points are added one at a time to produce the final classification rule. 
Such methods can be readily adapted to the case in which populations evolve over time by extending them so that more recent points are included in the model and the effect of  X  X arlier X  data points is gradually removed from the estimates (either by removing the impact of an individual point completely, or by downweighting it). Apart from these approaches, there has been some more direct work on dynamic updating. Nearest neigh-bour methods also have particular advantages in per-mitting ready dynamic updating of classification rules, provided no extensive pre-processing to reduce the num-ber of stored points, choose metrics, or construct a  X  X ul-tivariate ordering X  is used. More recently, Taylor et al [7], Nakhaeizadeh et al [5] and Nakhaeizadeh et al [S] have developed a control chart type of system in which the action taken depends on the performance of the rule. 
Population drift is related to concept drift. Whereas we regard population drift as referring to changes in the probability distributions of the phenomena under study, the term  X  X oncept drift X  has been used both for these and other changes. For example, it can refer to situations where the definitions of the classes in supervised classification problems can change over time (see, for example, Kelly and Hand [3]). Some recent work on concept drift is described in a special issue of 
Machine Learning (1998, Vol. 32, No.2) and in Widmer and Kubat [8] and Lane and Brodley [4]. 
To explore these ideas we studied a data set consisting of 92,258 unsecured personal loans with a 24 month term given by a major UK bank during the period 1 January 1993 to 30 November 1997. We define an values and is not affecting the classes differentially. In general, if the z variables include all the determinants of class membership probability, then changes in p(zli) will not translate into changes in p(ila). However, in situations where the class membership probabilities are also functions of other variables, then the p(ilz) may also change. This is the case with our credit data. We examine each of these different potential kinds of drift in subsequent sections. In Section 3 we look at changing priors, in Section 4 we look at changing distributions over x, and in Section 5 we outline a strategy for modelling changes to the posterior distribution p(ilx) and illustrate its application to our data. First, however, we demonstrate that each of the three different kinds of population drift does occur with our data. Evidence for the first type of drift, changing priors, is given in Figure 2. This shows the monthly proportion of bad customers plotted against time. The irregularity, month on month, is quite striking. Overall, the rate follows a gradual upward trend, before a sudden dramatic fall. The dramatic fall, though striking, is in fact of little interest. It simply reflects the fact that customers recruited during this period have had less than two years in which to go bad. That is, especially towards the end of this period, many customers are coded as  X  X ood X  who will in fact turn out to be bad. 
Evidence for the second kind of drift, changing x distributions, is given in Figures 3(a) to (d). These show plots, over the five year period, of weekly averages for four of the predictor variables from our data. Figure 3(a) (proportion of applicants aged between 30 and 35) shows no apparent drift. Figure 3(b) (proportion of applicants who have a cheque guarantee card) shows a definite trend. Figure 3(c) (a binary indicator of loan purpose) shows some seasonal variation. Figure 3(d) (a binary  X  X epayment method X  score) results from a policy change. Figure 4 shows a graph of bad rate amongst accepts. 
This shows a gradually increasing slope, albeit again surprisingly irregular, and then a sudden fall, parallel-ing that of the change in bad rate. Thus classification performance is decreasing with time (and with increas-
Figure 6: The effect of changing class priors on perfor-rate is one frequently adopted in the context of banking data of the kind used above. (Other rules could equally be adopted. One might simply want to minimise error rate, although this often results in assigning all objects to one class, especially if the other class is small. More generally, one might want to use a cost weighted loss, as described in, for example, Adams and Hand [l]). The results are shown in Figure 6 (with p = 0.5 being the top curve in each case, and lower curves corresponding to p increasing in steps of 0.5). The bad rate amongst accepts increases monotonically, though not linearly, with increasing bad prior. When all customers are bad, the bad rate amongst accepts, with a fixed accept rate of 20%, is, of course, 1.0. The implication is that, for poorly separated classes, as we have in our data above, the bad rate amongst accepts is almost linearly related to the proportion of bads in the population. 
The misclassification rate curves are rather more complicated. For well separated classes (high p) the misclassification rate can decrease initially as the proportion bad in the population increases. 
These two families of curves illustrate firstly, the importance of choosing an appropriate performance criterion for the problem in hand, and secondly that the impact of changing class priors can be quite complicated. model might be permitted to adapt as time progresses and the populations change. A basic dynamic linear model form (which can, of course, be generalised in obvious ways) is Here yt is a vector of observations made at time t, Pt is a vector of system parameters at time t, G is a matrix describing the system, Xt is a matrix of independent variables at time t, et and vt and are random normal vectors, N(O,Et), N(O,Vt) , respectively. As an initial exploration of such methods in the context of our classification problems, we explored the simple special case yielding a linear prediction. It is easy to extend this to generalised linear models, which may appear to be more appropriate for our context, in which the aim is to produce probability estimates which can be compared with a threshold. However, there is evidence that linear methods, with an appropriate choice of threshold, perform as well as logistic methods for the sorts of problems with which we are concerned here (Henley [2]), even though y can take only two values. Moreover, because we are especially concerned with handling large data sets we required a quick and efficient updating procedure. For this reason, for our initial investigation, we have kept to linear models and used the updating procedure described below. 
For a given n x d matrix, X, of n observations on the d independent predictor variables, and n-vector Y of observations of the class variable, the standard regression estimate is (X X  X )- X  X  X  X . Incorporating a new observation provides no problem for the X X  X  factor in this, but the (X X  X )- X  requires a fresh matrix inversion, for the matrix incorporating the new point. This can be avoided by updating (XIX)- X  X sing the expression: Since (X X  X )- X  is a d x d matrix, storage is trivial. Using this expression, the orientation of the decision surface can easily be updated as soon as a new point, with known class membership, becomes available. A similar expression allows one to remove points without inverting the matrix from scratch. We also need to consider how to update the threshold with which the linear rule is compared. We are especially interested in the bad rate when 80% of applicants are accepted, so we update the threshold so that this is maintained. Figure 8 shows the sorts of results one can obtain. This is one of the simplest of models, in which we have taken a year X  X  worth of data (customers taking out loans in 1994) and want to let the length of the historical record of points used in the rule vary as time progresses: in times of dramatic change, perhaps the classification rule should be based on a shorter span of previous points. 
The work of MGK was supported by a CASE stu-dentship from the UK X  X  Engineering and Physical Sci-ences Research Council, with additional support from 
Abbey National Plc. We would like to express our ap-preciation to Sam Korman and Steve Bull for their in-terest in and encouragement of this work. 151 PI 
Adams N.M. and Hand D.J. (1999) Comparing clas-sifiers when the misallocation costs are uncertain. To appear in Pattern Recognition, 32. 
Henley W.E. (1995) Statistical aspects of credit scor-ing. Unpublished PhD thesis, The Open University, Milton Keynes, UK. 
Kelly M.G. and Hand D.J. (1999) Credit scoring with uncertain class definitions. To appear in IMA Journal of Mathematics Applied in Business and Industry. 
Lane T. and Brodley C.E. (1998) Approaches to on-line learning and concept drift for user identifica-tion in computer security. Proceedings of the Fourth 
International Conference on Knowledge Discovery and Data Mining, ed. R.Agrawal, P.Stolorz, and 
G.Piatetsky-Shapiro. AAAI Press, Menlo Park, Cal-ifornia. 259-263. Nakhaeizadeh G., Taylor C.C., Kunisch G. (1997) 
Dynamic supervised learning: some basic issues and application aspects. Classification and Knowledge Organization, ed. R. Klar and 0. Optiz, Berlin: Springer-Verlag, 123-135. 
Nakhaeizadeh, G., Taylor, C. and Lanquillon, C. (1998). Evaluating usefulness for dynamic classifica-tion Knowledge Discovery and Data Mining KDD-98. ed. R. Agrawal, P. Stolorz, G. Piatetsky-Shapiro. AAAI, 87-93. 
Taylor, C.C., Nakhaeizadeh, G., and Kunisch, G. (1997) Statistical aspects of classification in drifting populations. Proceedings of the Sixth International Fort Lauderdale, 521-528. 
Widmer, G. and Kubat M. (1996) Learning in the presence of concept drift and hidden contexts. 
Machine Learning, 23, 69-101. 
