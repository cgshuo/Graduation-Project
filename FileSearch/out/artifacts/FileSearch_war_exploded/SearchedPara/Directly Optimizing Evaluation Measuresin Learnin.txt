 One of the central issues in learning to rank for information re-trieval is to develop algorithms that construct ranking models by directly optimizing evaluation measures used in information re-trieval such as Mean Average Precision (MAP) and Normalized Discounted Cumulative Gain (NDCG). Several such algorithms in-fectiveness has been verified. However, the relationships between the algorithms are not clear, and furthermore no comparisons have been conducted between them. In this paper, we conduct a study on the approach of directly optimizing evaluation measures in learning to rank for Information Retrieval (IR). We focus on the methods that minimize loss functions upper bounding the basic loss function defined on the IR measures. We first provide a general framework AdaRank within the framework. The framework is based on upper bound analysis and two types of upper bounds are discussed. More-over, we show that we can derive new algorithms on the basis of this analysis and create one example algorithm called PermuRank. We muRank, and conventional methods of Ranking SVM and Rank-Boost, using benchmark datasets. Experimental results show that the methods based on direct optimization of evaluation measures can always outperform conventional methods of Ranking SVM and RankBoost. However, no significant di ff erence exists among the performances of the direct optimization methods themselves. H.3.3 [ Information Search and Retrieval ]: Retrieval models Algorithms, Experimentation, Theory Evaluation measure, Learning to rank, Information retrieval
Learning to rank for Information Retrieval (IR) is a problem as follows. In learning, a ranking model is constructed with train-ing data that consists of queries, their corresponding retrieved doc-uments, and relevance levels provided by human annotators. In ranking, given a new query, the retrieved documents are ranked by using the trained ranking model.

In IR, ranking results are generally evaluated in terms of eval-uation measures such as Mean Average Precision (MAP) [1] and Normalized Discounted Cumulative Gain (NDCG) [14]. Ideally, a learning algorithm trains a ranking model by optimizing the perfor-mance in terms of a given evaluation measure. In this way, higher accuracy in ranking is expected. However, this is usually di ffi cult due to the non-continuous and non-di ff erentiable nature of IR mea-sures.

Many learning to rank algorithms proposed typically minimize a loss function loosely related to the IR measures. For example, Ranking SVM [13] and RankBoost [10] minimize loss functions based on classification errors in document pairs.

Recently, researchers have developed several new algorithms that manage to directly optimize the performance in terms of IR mea-sures. The e ff ectiveness of these methods have also been verified. From the viewpoint of loss function optimization, these methods fall into three categories. First, one can minimize upper bounds of the basic loss function defined on the IR measures [30, 17, 27]. Second, one can approximate the IR measures with functions that are easy to handle [7, 23]. Third, one can use specially designed technologies for optimizing the non-smooth IR measures [3, 8].
There are open questions regarding the direct optimization ap-proach . (1) Is there a general theory that can guide the development of new algorithms? (2) What is the relationship between existing methods? (3) Which direct optimization method empirically per-forms best?
In this paper, we conduct a study on direct optimization of IR measures in learning to rank and answer the above questions. Specif-ically, we focus on the first category of methods that minimize loss functions upper bounding the basic loss function defined on the IR measures. This has become one of the hottest research topics in learning to rank. (1) We conduct a general analysis of the approach. We indicate that direct optimization of IR measures amounts to minimizing dif-ferent loss functions based on the measures. We first introduce one basic loss function, which is directly defined on the basis of IR measures, and indicate that there are two types of upper bounds on the basic loss function. We refer to them as type one bound and type two bound, respectively. Minimizing the two types of upper bounds leads to di ff erent learning algorithms. With this analysis, di ff erent algorithms can be easily studied and compared. More-over, new algorithms can be easily derived. As an example, we create a new algorithm called PermuRank. manage to minimize loss functions which are type one upper bound and type two upper bound, respectively. (3) We compare the performances of the existing direct optimiza-datasets. Experimental results show that the direct optimization prove upon the baseline methods of Ranking SVM and RankBoost. Furthermore, the direct optimization methods themselves can work equally well.

The rest of the paper is organized as follows. After a summary of related work in Section 2, we formally describe the problem of learning to rank for Information Retrieval in Section 3. In section 4, we propose a general framework for directly optimizing evaluation a new algorithm PermuRank are analyzed and discussed within this framework. Section 5 reports our experimental results and Section 6 concludes this paper.
The key problem for document retrieval is ranking, specifically, to create a ranking model that can sort documents based on their relevance to the given query. Traditional ranking models such as BM25 [22] and Language Models for Information Retrieval (LMIR) [20, 16] only have a few parameters to tune. As the ranking models become more sophisticated (with more features) and more labeled data become available, how to tune or train a ranking model be-comes a challenging issue.

In recent years, methods of learning to rank have been applied to ranking model construction and promising results have been ob-tained. Learning to rank is to automatically create a ranking model by using labeled training data and machine learning techniques. Several approaches have been proposed. The pairwise approach transforms the ranking problem into binary classification on docu-ment pairs. Typical methods include Ranking SVM [13, 15], Rank-Boost [10], and RankNet [4]. For other methods belonging to the approach, refer to [12, 29, 6, 25, 21, 30, 24, 31]. The methods of Ranking SVM, RankBoost, and RankNet minimize loss functions that are loosely related to the evaluation measures such as MAP and NDCG.

Recently, the approach of directly optimizing the performance in terms of IR measures has also been proposed. There are three categories:
First, one can minimize loss functions upper bounding the basic [30] minimizes a hinge loss function, which upper bounds the basic loss function based on Average Precision. AdaRank [27] minimizes an exponential loss function upper bounding the basic loss function. (See also [17].)
Second, one can approximate the IR measures with easy-to-handle functions. For example, the work in [23] proposes a smoothed ap-proximation to NDCG [14]. (See also [7].)
Third, one can use specially designed technologies for optimiz-ing non-smooth IR measures. For example, LambdaRank [3] im-plicitly minimizes a loss function related to IR measures. Genetic Programming (GP) is also used to optimize IR measures [2]. For example, [8] proposed a specifically designed GP for learn a rank-ing model for IR. (See also [28, 9, 19]).
 and AdaRank as examples of existing methods.
Learning to rank for Information Retrieval is a problem as fol-lows. In retrieval (testing), given a query, the system returns a ranked list of documents in descending order of their relevance scores. In learning (training), a number of queries and their cor-responding retrieved documents are given. Furthermore, the labels of the documents with respect to the queries are also provided. The labels represent ranks (i.e., categories in a total order). The objec-tive of learning is to construct a ranking model that achieves the best result on test data in the sense of minimization of a loss func-tion. Ideally the loss function is defined directly on the IR measure used in testing.
 denotes the number of ranks. There exists a total order between the ranks r ` r `  X  1  X  X  X  r 1 , where denotes the order. Sup-Each query q i is associated with a list of retrieved documents d { d where n ( q i ) denotes the sizes of lists d i and y i , d the j th document in d i , and y i j  X  Y denotes the label of document d . A feature vector  X  ( q i , d i j ) is created from each query-document pair ( q i , d i j ) , i = 1 , 2 ,  X  X  X  , m ; j = 1 , 2 ,  X  X  X  , n ( q denoted as S = { ( q i , d i , y i ) } m i = 1 .

Let the documents in d i be identified by the integers { 1 , 2 ,  X  X  X  , n ( q We define permutation  X  i on d i as a bijection from { 1 , 2 ,  X  X  X  , n ( q to itself. We use  X  i to denote the set of all possible permutations on d , and use  X  i ( j ) to denote the position of item j (i.e., d is nothing but to select a permutation  X  i  X   X  i for the given query q and the associated list of documents d i using the ranking model.
The ranking model is a real valued function of features. There are two types of ranking models. We refer to them as f and F respectively.

Ranking model f is a document level function, which is a linear combination of the features in a feature vector  X  ( q i , d where w denotes the weight vector. In ranking for query q assign a score to each of the documents using f ( q i , d documents based on their scores. We obtain a permutation denoted as  X  i .

Ranking model F is a query level function. We first introduce a query level feature vector for each triple of q i , d i and  X  as  X  ( q i , d i , X  i ). We calculate  X  by linearly combining the feature vectors  X  of query-document pairs for q i :
 X  ( q i , d i , X  i ) = where z kl = + 1 if  X  i ( k ) &lt;  X  i ( l ) ( d ik is ranked ahead of d and  X  1 otherwise. (A slightly di ff erent definition on  X  is given in [30].) We define F as a linear combination of the features in feature vector  X  : where w denotes the weight vector. In ranking, the permutation with the largest score given by F is selected:
It can be shown that, the two types of ranking models are equiv-alent, if the parameter vectors w  X  X  in the two models are identical.
T  X  X  X  X  X  X  X  X  1. Given a fixed parameter vector w , the two ranking models f and F generate the same ranking result. That is, permu-tations  X  i and  X  i are identical. Proof of the theorem can be found in the Appendix. Theorem 1 implies that Equation (4) can be computed e ffi ciently by sorting documents using Equation (1).

In IR, evaluation measures are used to evaluate the goodness of a ranking model, which are usually query-based. By query based, we mean that the measure is defined on a ranking list of documents with respect to the query. These include MAP, NDCG, MRR (Mean Reciprocal Rank), WTA (Winners Take ALL), and Precision@n [1, 14]. We utilize a general function E (  X  i , y i )  X  [0 , + 1] to represent the evaluation measures. The first argument of E is the permutation  X  created using the ranking model. The second argument is the list of ranks y i given as ground truth. E measures the agreement between  X  i and y i . Most evaluation measures return real values in [0, + 1]. We denote the perfect permutation as  X   X  i . Note that there may be more than one perfect permutation for a query, and we use i to denote the set of all possible perfect permutations for query q . For  X   X  i  X   X   X  i , we have E (  X   X  i , y i ) = 1.

Table 1 gives a summary of notations described above.
In this section, we give a general framework for analyzing learn-ing to rank algorithms that directly optimize evaluation measures.
Ideally, we would create a ranking model that maximize the ac-curacy in terms of an IR measure on training data, or equivalently, minimizes the loss function defined as follows: where  X  i is the permutation selected for query q i by ranking model F (or f ). We refer to the loss function R ( F ) (or R ( f )) as the  X  X asic loss function X  and those methods which minimize the basic loss function as the  X  X irect optimization approach X .

The rest of this paper will focus on the first category of direct optimization methods (defined in Section 1) which minimize loss functions upper bounding the basic loss function. Practically, in order to leverage existing optimization technologies like Boosting and SVM, bound optimization has been widely used. We can con-sider two types of upper bounds. The first one is defined directly on the IR measures (type one bound). The second one is defined on the pairs between the perfect and imperfect permutations (type minimize one of the two upper bounds, respectively. PermuRank, which we propose in this paper, is an algorithm minimizes a type two bound.
The basic loss function can be upper bounded directly by the ex-ponential function, logistic function, which is widely used in ma-chine learning. The logistic function is defined as The exponential function is defined as
We can use the logistic function and exponential function as  X  X ur-rogate X  loss functions in learning. Note that both functions are continuous, di ff erentiable, and even convex w.r.t. E . The expo-nential loss function is tighter than the logistic loss function since E  X  [0 , + 1].

The AdaRank algorithm proposed in [27] actually minimizes the exponential loss function (type one bound), by taking a boost-ing approach. Motivated by the famous AdaBoost algorithm [11], AdaRank optimizes the exponential loss function through contin-uously re-weighting the distribution over the training queries and creating weak rankers. To do so, AdaRank repeats the process of re-weighting the training query, creating a weak ranker, and calcu-lating a weight for the weak ranker, according to the performances (in terms of one IR measure) of the weak rankers on the training queries. Finally, AdaRank linearly combines the weak rankers as the final ranking model.
Here, we introduce a new loss function. where ~  X   X  is one if the condition is satisfied, otherwise zero.
The loss function measures the loss when the worst prediction is made, specifically, the di ff erence between the performance of the perfect permutation (it equals one) and the minimum performance of an incorrect permutation (it is less than one).
 The following theorem holds with regard to the new loss function.
T  X  X  X  X  X  X  X  X  2. The basic loss function in (5) is upper bounded by the new loss function in (6).
 Proof of Theorem 2 can be found in the Appendix.

The loss function (6) is still not continuous and di ff erentiable be-cause it contains the 0-1 function ~  X   X  , which is not continuous and di ff erentiable. We can consider using continuous, di ff erentiable, and even convex upper bounds on the loss function (6), which are also upper bounds on the basic loss function (5). 1) The 0-1 function ~  X   X  in (6) can be replaced with its upper bounds, for example, hinge, exponential, and logistic functions, yielding , where [  X  ] + denotes the hinge function.

Figure 1 shows the relationship between the loss function (6) and its upper bounds, where E (  X   X  i , y i )  X  E (  X  i , y i figure, we can see that it is not possible to say which upper bound is the tightest. Di ff erent upper bounds may be suitable for di ff erent datasets. 2) The max function can also be replaced with its upper bound, the sum function. This is because for all i . 3) Relaxations 1 and 2 can be applied simultaneously.

For example, replacing ~  X   X  with the hinge function and max with sum, we obtain: .
We can derive di ff erent algorithms by using the upper bounds as ples.
 where C is the coe ffi cient for trade-o ff between total empirical loss and model complexity, and  X  i represents the empirical loss for q One can easily verify that in the constraints the empirical loss  X  the maximum among all the losses of permutations for query q loss function Intuitively, the first term calculates the total maximum empirical loss when selecting the best permutation for each of the queries. Specifically, if the di ff erence between the permutations F ( q F ( q i , d i , X  i ) is less than the di ff erence between the corresponding otherwise not. Next, the maximum loss is selected for each query and they are summed up over all the queries.

Since c  X  ~ x  X  0  X   X  [ c  X  x ] + holds for all c  X  &lt; + and x  X  &lt; , it is easy to see that the upper bound in (9) also bounds the basic loss function in (5) (See also Figure 1). In [30], the authors have proved this fact (see also [26]).
In principle, any type two bound can be optimized using opti-mization techniques such as those in Perceptron, Support Vector Machines, and Boosting. However, the sizes of permutation sets i and  X  i \  X   X  i are both of order O ( n !), which makes the optimiza-tion infeasible. Here n denotes the number of documents associated with query q i .

In this paper, we propose a new direct optimization algorithm which e ffi ciently minimizes one of the type two bounds as loss function in a greedy way. The algorithm is referred to as Permu-Rank and is shown in Figure 2. The key idea in PermuRank is to maintain a set of perfect permutations and a set of imperfect per-mutations as working sets, in stead of using the entire set of perfect permutations and the entire set of imperfect permutations.
PermuRank takes a training set S = { ( q i , d i , y i ) } m takes an evaluation measure E and number of iterations T as pa-rameters. PermuRank runs T rounds and at each round it creates a ranking model F t ( t = 1 ,  X  X  X  , T ). Finally, it outputs a ranking model F created during the last round.

At each round t , PermuRank maintains a set of perfect permuta-tions and a set of imperfect permutations for each query q as B t i and C t i , respectively. These two sets are initialized with an arbitrary perfect permutation  X   X  i  X   X   X  i and an arbitrary imperfect permutation  X  i  X   X  i \  X   X  i . At each round, the two sets are updated by adding the most violated perfect and imperfect permutations re-spectively:
At each round t , a ranking model F t is created using the permu-tation sets B t i and C t i , i = 1 ,  X  X  X  , m created so far
In this paper, without loss of generality, we use the hinge loss function of Equation (7). The total empirical loss L becomes where
In this paper, we employ the SVM technique to minimize the regularized hinge loss function.
 i for training the next ranking model F t + 1 .
 At each round, PermuRank checks whether the permutation sets and C t i are changed. If there is no change, the algorithm will stop and return F t as the final ranking model. Input: S = { ( q i , d i , y i ) } m i = 1 , parameters E and T Initialize B 1 i and C 1 i , for all i = 1 ,  X  X  X  , m .
 For t = 1 ,  X  X  X  , T End For return F t .

We give a summary of the upper bounds on the basic loss func-tion. Figure 3 shows the relationship. There is a basic loss function (5). On the left hand side is the type one bound. The upper bounds of the exponential loss function, logistic loss function can be used. On the right hand side is the type two bound. Equation (6) is the loss function for type two bound, which also upper bounds the ba-sic loss function. Furthermore, the upper bounds of exponential loss function, logistic loss function, hinge loss functions, etc can be considered.
We conducted experiments to test the performances of the learn-SVM, and RankBoost.

AdaRank and PermuRank can optimize any evaluation measure in [0 , + 1]. In our experiments, we chose MAP as the evaluation measure for them, denoted as AdaRank.MAP and PermuRank.MAP respectively. For AdaRank, we utilized features as weak rankers.
As measures for evaluation, we actually used MAP and NDCG at the positions of 1, 3, 5, and 10.
In the first experiment, we used the Letor benchmark datasets [18]: OHSUMED, TD2003, and TD2004.

Letor OHSUMED dataset consists of articles from medical jour-nals. There are 106 queries in the collection. For each query, there are a number of associated documents. The relevance degrees of documents with respect to the queries are given by humans, on three levels: definitely, possibly, or not relevant. There are 16,140 query-document pairs with relevance labels. In Letor, the data is represented as feature vectors and their corresponding relevance la-bels. Features in Letor OHSUMED dataset consists of  X  X ow-level X  features and  X  X igh-level X  features. Low-level features include term frequency (tf), inverse document frequency (idf), document length (dl), and their combinations. High-level features include BM25 and LMIR scores. In total, there are 25 features.

Letor TD2003 and TD2004 datasets are from the topic distilla-tion task of TREC 2003 and TREC 2004. TD2003 has 50 queries and TD2004 has 75 queries. The document collection is a crawl of the .gov domain. For each query, there are about 1,000 associated documents. Each query document pair is given a binary judgment: relevant or irrelevant. The features of Letor TD2003 and TD2004 datasets include low-level features such as term frequency (tf), in-verse document frequency (idf), and document length (dl), as well as high-level features such as BM25, LMIR, PageRank, and HITS. In total, there are 44 features. Figure 4: Ranking accuracies on Letor OHSUMED data.

We conducted 5-fold cross validation experiments, following the guideline of Letor. Figure 4 shows the results on Letor OHSUMED dataset in terms of MAP and NDCG, averaged over five trials. In calculation of MAP, we viewed  X  X efinitely X  and  X  X artially relevant X  as relevant. (We tried treating  X  X artially relevant X  as  X  X rrelevant X , it results on the Letor TD2003 and TD2004 datasets.
 We also conducted experiments to observe the training curve of PermuRank.MAP in terms of MAP on OHSUMED. We found that, in each fold of the cross validation, the training accuracy in terms of MAP would converge after 40  X  100 iterations. Equivalently, the sizes of the working sets are also 40  X  100, which is signifi-cantly smaller than n !, where n denotes the number of documents associated with the query. Similar results were also observed in the experiments on TD2003 and TD2004.
 AdaRank, and PermuRank almost always outperform the baselines of Ranking SVM and RankBoost. We conducted t-tests on the im-provements between the methods in terms of NDCG@1. The re-sults show that on OHSUMED, the improvements of the direct op-timization methods over the baselines are statistically significant (p-value &lt; 0 . 05). The t-test results also show that no statistically significant di ff erence exists among the performances of the direct optimization methods.

However, on TD2003 and TD2004 all the t-tests show that there is no statistically significant di ff erence among the performances of all the methods. This is because the numbers of queries in TD2003 and TD2004 are too small, which is a common problem for the major publicly available datasets.
In the second experiment, we made use of the WSJ and AP datasets used in [27].

The WSJ and AP datasets are from the TREC ad-hoc retrieval track. WSJ contains news articles by the Wall Street Journal, and AP contains 158,240 news articles of Associated Press. 200 queries are selected from the TREC topics (No.101  X  No.300). Each query has a number of documents associated and they are labeled as  X  X el-evant X  or  X  X rrelevant X . As features, we adopted those used in docu-ment retrieval [5] and [27]. They are tf (term frequency), idf (in-verse document frequency), dl (document length), and BM25. De-tails of the datasets and features can be found in [5, 27].
WSJ and AP data were split into four even subsets and 4-fold cross-validation experiments were conducted. Figure 7 and Fig-ure 8 respectively show the results in terms of MAP and NDCG, averaged over four trials.
 and PermuRank almost always outperform the baselines of Rank-ing SVM and RankBoost on WSJ and AP. With the help of t-muRank over the baselines are statistically significant (p-value &lt; 0 . 05). Furthermore, SVM ma p and PermuRank work equally well, without statistically significant di ff erence between their performances. In addition, this time AdaRank does not perform as well as ex-pected: its performance is similar to the baselines, and significantly
Table 2 and Table 3 show the ranking accuracies of the five methods on the datasets in terms of MAP and NDCG@3, respec-tively. Ranks of the five methods based on their performances on the datasets are also shown. The top ranked methods on the five datasets are highlighted. Note that the results on TD2003 and TD2004 are not statistically reliable; we list them here only for reference. From the results, we can conclude that the direct opti-better than the baselines. Also, we conclude that these direct opti-mization methods themselves perform equally well.
In this paper, we have studied the direct optimization approach to learning to rank, in which one trains a ranking model that can directly optimize the evaluation measures used in IR.
 and AdaRank have been proposed. However, further theoretical and empirical investigations on this approach are still needed. In this paper we have tried to clarify many of the problems with re-gard to the direct optimization approach. We believe that this is extremely important for enhancing the state-of-the-art in learning to rank.

We conducted a theoretical analysis on the direct optimization approach. According to our study, the direct optimization approach is one that minimizes the basic loss function defined on the IR mea-try to minimize two types of upper bounds upon the basic loss func-tion, respectively called type one bound and type two bound in the paper. With this analysis we are also able to derive a new direct optimization algorithm, called PermuRank.
 and PermuRank using a number of benchmark datasets. Experi-AdaRank, and PermuRank can always perform better than the con-ventional methods of Ranking SVM and RankBoost. Furthermore, these direct optimization methods themselves can work equally well.
Future directions for this research include studying the tightness of di ff erent upper bounds upon the basic loss function and improv-ing the framework so that more algorithms can be analyzed and compared.
We thank Andrew Arnold, Lex Stein, Tao Qin, and Dwight Daniels for their valuable comments and suggestions to this paper. [1] R. Baeza-Yates and B. Ribeiro-Neto. Modern Information [2] W. Banzhaf, F. D. Francone, R. E. Keller, and P. Nordin. [3] C. Burges, R. Ragno, and Q. Le. Learning to rank with [4] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, [5] Y. Cao, J. Xu, T.-Y. Liu, H. Li, Y. Huang, and H.-W. Hon. [6] Z. Cao, T. Qin, T.-Y. Liu, M.-F. Tsai, and H. Li. Learning to [7] D. Cossock and T. Zhang. Subset ranking using regression. [8] H. M. de Almeida, M. A. Gon X alves, M. Cristo, and [9] W. Fan, P. Pathak, and L. Wallace. Nonlinear ranking [10] Y. Freund, R. D. Iyer, R. E. Schapire, and Y. Singer. An [11] Y. Freund and R. E. Schapire. A decision-theoretic [12] G. Fung, R. Rosales, and B. Krishnapuram. Learning [13] R. Herbrich, T. Graepel, and K. Obermayer. Large Margin [14] K. Jarvelin and J. Kekalainen. IR evaluation methods for [15] T. Joachims. Optimizing search engines using clickthrough [16] J. La ff erty and C. Zhai. Document language models, query [17] Q. Le and A. Smola. Direct optimization of ranking [18] T.-Y. Liu, J. Xu, T. Qin, W. Xiong, and H. Li. Letor: [19] D. Metzler. Direct maximization of rank-based metrics. [20] J. M. Ponte and W. B. Croft. A language modeling approach [21] T. Qin, X.-D. Zhang, D.-S. Wang, T.-Y. Liu, W. Lai, and [22] S. E. Robertson and D. A. Hull. The TREC-9 filtering track [23] M. Taylor, J. Guiver, S. Robertson, and T. Minka. Softrank: [24] A. Trotman. Learning to rank. Inf. Retr. , 8(3):359 X 381, 2005. [25] M.-F. Tsai, T.-Y. Liu, T. Qin, H.-H. Chen, and W.-Y. Ma. [26] I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Altun. [27] J. Xu and H. Li. Adarank: a boosting algorithm for [28] J.-Y. Yeh, J.-Y. Lin, H.-R. Ke, and W.-P. Yang. Learning to [29] H. Yu. SVM selective sampling for ranking with application [30] Y. Yue, T. Finley, F. Radlinski, and T. Joachims. A support [31] Z. Zheng, H. Zha, T. Zhang, O. Chapelle, K. Chen, and Proof of Theorem 1.

P  X  X  X  X  X  . Without loss of generality, assume that we have a query q with n associated documents d 1 , d 2 ,  X  X  X  , d n .

With the use of model f , the relevance scores of the n documents become s 1 = f ( q , d 1 ) = w &gt;  X  ( q , d 1 ) , s 2 = f ( q , d = f ( q , d n ) = w &gt;  X  ( q , d n ).

With the use of F and the features defined in Equation (2), F ( q , d , X  ) can be written as
F ( q , d , X  ) = w &gt; is only related to the variables z kl  X  X  in the equation, the equation is maximized w.r.t.  X  , if and only if all the terms in the summation are not negative.

Next, we prove that the permutation given by model f is equiva-lent to the permutation given by model F , and vice versa. 1.  X  is obtained by sorting documents in descending order with 2.  X  is obtained by maximizing F :  X  = arg max  X   X   X  F ( q , d , X  ).
Summarizing 1 and 2, we conclude that with the same parameter vector w , the ranking models f and F generate the same ranking result.
 Proof of Theorem 2.

P  X  X  X  X  X  . Let l ( q query q i by model F . There are two cases: Case 1  X  i  X   X   X  i : If  X  i  X   X   X  i , E (  X  i , y i ) = E (  X   X 
