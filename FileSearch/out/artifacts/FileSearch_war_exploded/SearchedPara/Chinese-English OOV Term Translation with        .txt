 In Cross-Language Information Retrieval (CLIR), users X  queries are generally com-posed of short terms, in which there are many Out-of-Vocabulary (OOV) terms like Named Entities (NEs), new words, terminologies [1][5][6][12]. T he translation quali-ty of OOV term directly influences the precision of querying mu ltilingual information and OOV term translation has become a challenging issue in CLIR [9][15][17]. With the increasing growth of Web information which includes multili ngual hypertext re-sources with abundant topics, it appears that Web information c an mitigate the prob-lem of the restricted OOV term translation accuracy [11][13][18 ]. However, how to select the correct translations f rom Web and locate the appropr iate translation re-sources rapidly is still the main goal for OOV term translation [14][16][19]. Hence, finding the effective feature re presentation and the optimal ra nking pattern for transla-tion candidates is the core part for the Web-based OOV term tra nslation. 
Many researchers have utilized Web search engines to find trans lation candidates for Chinese-English OOV term translation [8][10][13]. Zhang et al. [25] extracted the translation candidates for OOV query terms from Web in Chinese-English CLIR, and improved the CLIR performance. Zhang et al. [24] searched the t ranslation candidates by using cross-language query expansion and Web, and obtained t he Top-1 accuracy of 81.0% in Chinese-English OOV word translation. Fang et al. [ 4] used semantic prediction and query expansion to get the translation candidate s, and acquired the Top-3 accuracy of 82.9% in Chines e-English OOV term translation . Chen et al. [3] used the combination of Web statistics and the vocabulary, and acquired the Top-1 accuracy of 87.6% in Chinese-Eng lish OOV word tra nslation. Yang et al. [21] utilized the combination of transliteration, Web mining and ranking base d on AdaBoost, and got the Top-5 accuracy of 76.35% fo r Chinese-English backward t ransliteration. Yang et al. [22] utilized heuristic Web mining and asymmetric alignm ent, and got the Top-1 accuracy of 48.71% in Chinese-En glish organization name transla tion. Yang et al. [23] combined Web mining and ranking by SVM and Ranking SVM, an d obtained the Top-1 accuracy of 65.75% in Chinese-English organization na me translation. 
Unfortunately, there are still three common problems in Chinese -English OOV term translation based on Web mining. (1) The noises in English translation candi-dates cannot be processed appropriately. Although there does not exist the issue of word segmentation in English key term extraction, many noises m ay be introduced into the candidates extracted from Web documents. However, such noises are often simply processed, or even without any processing. (2) The feature information for the evaluation of translation ca ndidates is not enough and comp rehensive. Most methods implement the evaluation for candidates through mining simple local and Boolean features. However, if on ly a certain Web document that an OOV term ap-pears is explored, the global information contained in the whol e Web document set is ignored, and the inconsistency and polysemy of candidates canno t be considered. (3) The relevance measurement for translation pairs is simple, or t he computation cost is too high. For ranking candidates, most approaches adopt the simple combin a-tion computation of feature values, or get assessment based on classification models. The feature weights are determined according to the general ind uction and suitable for specific fields, and cannot guara ntee the accuracy for ranking. The Ranking SVM model can effectively express multiple ranking constraints, and has better universality and applicability [2][20]. 
To support more precise Chinese-English OOV term translation, w e establish a multiple-feature-based translation pattern based on Web mining and Ranking SVM. An English key term extraction mechanism is built on the simpli fied selection, and then the emphasis is put on the noise filtering. Heuristic rule s summarized from trans-lation candidates are used to remove insignificant noises, and Information Entropy is introduced to further discard meaningless substrings. On the ot her hand, translation candidates are chosen by the fusion of multiple features. The r epresentation forms of local, global and Boolean feature are constructed under the con sideration for the char-acteristics of Chinese/English OOV term and Web information. Fo r the relevance measurement between an OOV term and its translation candidates, the supervised learning based on Ranking SVM is utilized to rank candidates ac curately. By utilizing the SIGHAN2005 corpus for the Chinese Named Entity Recognition (NER) task and manually selected new terms in various fields, our model can  X  filter  X  the most possi-ble translation candidates with better ability. This paper also attempts to apply our model in Chinese-English CLIR. It can be observed from the expe rimental results on the data sets of TREC that the obvious improvement for query tr anslation is obtained. In Web mining of OOV term transl ation, a crucial problem is to select the translation candidates from the returned Web documents, that is, the key te rm extraction task. The Initial Extraction mechanism is first established to extract the initial English key terms from the webpage snippets obtained by using the Chinese O OV term as a query for the search engine. The English fragments segmented by the n on-English charac-ters in each snippet are selected. Given the following snippet,  X  Naruto wallpapers  X ,  X  Naruto  X ,  X  Two destinys two different fates  X  and  X  Recognize my existence  X  are chosen as the initial key terms. 
Obviously, there are a lot of noises among the initial key term s. Therefore, some noise patterns are regarded as Heuristic Filtering Rules (HFR) and utilized to re-move the noisy strings. (1) If an initial key term appears in t he stoplist, then it is re-moved as a noisy string. The stoplist contains the stopwords wi th high frequency in common use, which are usually irrelevant with the original OOV term, such as  X  Translate this page  X  and  X  Retrieved from Wikipedia  X . (2) If an initial key term be-gins or ends with a preposition or conjunction, then it is remo ved as a noisy string. (3) If an initial key term satisfies some filtering patterns, then it is removed as a noisy string. Such patterns are used to select some frequent and obvi ously incorrect key terms. For example, an initial key term for the OOV term  X   X  X  X  X  X  X  X  X  X  reasonable form composed of both letters and numbers. (4) If mu ltiple initial key terms are same by ignoring the case sensitivity, then the form with the highest fre-quency is reserved and the others are removed as the noisy stri ngs. For example, for the OOV term  X   X  X  X  X  X  X  [ Felix ] X , all the related information for three initial key terms,  X  Felix  X ,  X  FELIX  X  and  X  felix  X , must be considered in the subsequent feature selection and computation. (5) For initial key terms with a sin gle word corresponding to the same original OOV term, if a term is a prefix/suffix sub string of the other terms, then it is removed as a noisy string. 
In the key terms obtained by HFR-based filtering, there are sti ll some redundant substrings, thus the optimization based on Information Entropy is proposed to fur-ther filter such noise s. For a key term x , its entropy is expressed as: whole snippet set; N is the total snippet number. 
Information Entropy can not only represent the amount of inform ation content for key terms, but also the distribu tion similarity between two key terms in the snippet set. for  X  is shown in Section 6.2), then kt 1 is removed as a noisy string. However, if only using Information Ent ropy to filter substri ngs, the relations b etween an OOV term and its key terms cannot be considered. For key terms with low freq uency, they often co-occur with some noisy strings. For example, for the OOV term  X   X  X  X  X  X  [ Samaranch ] X , its correct translation  X  Samaranch  X  always occurs in the key term  X  Juan Antonio Samaranch  X . If only determined by using Information Entropy,  X  Sa-maranch  X  will be removed. Thu s the special feature P &amp; S _ IF (defined in Section 4), which describes the phonetic and sense relations between an OOV term and its trans-Local Feature (LF) is constructed based on neighboring tokens and the token itsel f. There are two types of contextual information to be considered when extracting LFs, namely internal lexical and ext ernal contextual information. (1#) Term length ( Len )  X  Aims to consider the lengt h of the translation candidate. (2#) Phonetic Value ( PV )  X  Aims to investigate the phonetic similarity between an OOV term and its translation candi dates. Because the associated syllabification repre-sentations can often be found between Chinese and English sylla bles with fewer am-biguities, the syllabification has become a very effective way in the phonetic feature expression. PV means that for measuring the ed it distance similarity between the syl-labification sequences of an OOV term and its candidates, the c orresponding pro-cessing is executed according t o the specific linguistic rules. where S OOV and T OOV denote the OOV term and its translation candidate respectively , S
OOV  X  and T OOV  X  are the character strings after the syllabification and remov ing the vowels, EditDist ( , ) indicates the edit dist ance between two strings. (3#) Length Ratio of OOV Term and Its Translation Candidate ( LR )  X  Aims to explore the composition possibility that the translation candid ate can be regarded as the final correct translation for an OOV term. An OOV term and its translation should have the similar length, so the LR value is close to 1 as possible. A Chinese term is segmented into significant pieces first, and the number of piec es is taken as its length. For example,  X   X  X  X  X  X  X  X  [ SARS ] X  is segmented into  X   X  [ non ] X ,  X   X  X  X  [ typical ] X  and  X   X  X  X  [ pneumonia ] X , and its length is 3. For an English term, the number of wor ds is counted as the length. If there is only one word composed of ca pital letters, its length is defined as the number of letters, e.g.,  X  SARS  X  has the length of 4. Thus the LR value of  X   X  X  X  X  X  X  X  [ SARS ] X  and its candidate  X  SARS  X  is 3/4=0.75. (4#) Phonetic and Sense I ntegration Feature ( P &amp; S _ IF )  X  Aims to consider the phonetic information and senses of an OOV term and its candidat es synthetically. It is set up for multi-word OOV terms . Each constituent can be transl ated by the phonetic information or senses. where LScore ( , ) is the matching word numbe r of non-transliteration words in S OOV computing LScore . For example, given S OOV  X   X  X  X  X  X  X  X  X  X  X  X  [ Scandinavian Pen-insula ] X  and its T OOV  X  Scandinavian Peninsula  X  , the non-transliteration words  X   X  X  X  [ peninsula ] X  and  X  Peninsula  X  are matched, then LScore ( S OOV , T OOV )=1; the PV value between the remaining strings  X   X  X  X  X  X  X  X  X  [ Scandinavian ] X  and  X  Scandinavian  X  is 0.928, so the final P &amp; S _ IF value is 1.928/2=0.964. (5#) Un-Covered Ratio ( UCR )  X  Aims to explore the ratio of the overlap between an OOV term and the translations of its candidates acquired from C hinese Basic Diction-ary (Yang et al. 2009b) . It is set up for m ulti-word OOV terms. where unTrans is the part in S OOV uncovered by the translation of T OOV . For example, given S OOV  X   X  X  X  X  X  X  X  [ Suez Canal ] X  and its T OOV  X  Suez Canal  X , the part in T OOV which can be translated by Basic Dictionary is  X  Canal  X  and its translation is  X   X  X  X  [ canal ] X . Thus the unTrans part in S OOV is  X   X  X  X  X  [ Suez ] X , then the final UCR value is 1-3/5=0.4. 
Global Feature (GF) is extracted from other occurrences of the same or similar tokens in the Web document set. The common case in the Web-base d OOV term translation is that th e translation candidates in the previous parts of Web documents often occur with the same or similar forms in the latter parts. The contextual infor-mation from the same and other We b documents may be beneficial to determine the final translation. To utilize gl obal information, GFs are built based on the characteris-tics of Web documents. (1#) Global Term Frequency ( G _ Freq )  X  Aims to utilize the frequency information that an OOV term and its transla tion candidates appear in the W eb document set. It is always the most important feature and includes four parameters. Freq SOOV denotes the frequency of S OOV in all the returned snippets. TF TOOV indicates the number of T OOV s in all the snippets. DF TOOV represents the number of snippets that contain T OOV . CO_Freq means the number of snippets that contain both S OOV and T OOV , i.e., co-occurrence frequency. (2#) Global Statistical Feature ( G _ SF )  X  Aims to explore the statistical measure for the strength of the interdependence between an OOV term and its translation candi-dates to judge the possibility of a translation candidate being taken as the final correct translation [7]. 
Chi-Square (  X  2 ) Feature Value ( CV )  X  Aims to evaluate the semantic similarity between S OOV and T OOV by their occurrence in Web documents. where a is the number of snippets with both S OOV and T OOV , b is the number of snip-pets that contain S OOV but do not contain T OOV , c is the number of snippets that do not contain S OOV but contain T OOV , d is the number of snippets t hat do not contain neither of S OOV and T OOV , and N = a + b + c + d . 
Information Gain ( IG )  X  Aims to compute the probability that T OOV appears in the snippets with S OOV . The larger IG shows that T OOV is a more possible translation for S
Correlation Coefficient ( CC )  X  Aims to measure the lin ear association degree be-Relevance Score ( RS )  X  Aims to measure the direct relevance between S OOV and T
OOV . It X  X  computed as the ratio bet ween the occurrence probability of T OOV in the snippets with S OOV and that of T OOV in the snippets without S OOV . The larger RS indi-cates that S OOV and T OOV are more relevant. where m is used to smooth the RS and usually set as 1. Odds Ratio ( OR )  X  Aims to measure the indirect relevance between S OOV and T
OOV . The distribution of features on relevant candidates is differ ent from that on ir-relevant candidates. The larger OR indicates that S OOV and T OOV are more relevant. 
GSS Coefficient ( GSS )  X  Aims to measure the relevance between S OOV and T OOV . It is another simplified variant of CV . The larger GSS also represents the stronger rele-vance. (3#) Pointwise Mutual Information ( PMI )  X  Aims to evaluate the co-occurrence relation between an OOV term and its candidates. If both appear with the higher co-occurrence frequency in the same snippet, they are more relevan t. (4#) Co-Occurrence Distance ( CO _ Dist )  X  Aims to investigate the distance between an OOV term and its candidates in Web documents. This distance is often very closer. 
For each snippet that contains both S OOV and T OOV , three positions are considered, the last one ( p 3). For example, in the following snippet, S OOV is  X   X  X  X  X  X  X  X  X  [ Asian Development Bank , ADB ] X  and T OOV is  X  Asian Development Bank  X . 
The position is indexed from 0. Then the nearest position pair p 2 SOOV and p 1 TOOV can be found for this e xample. The distance Dist between S OOV and T OOV is: 
Given the example above, Dist = p 2 SOOV -p 1 TOOV -6=36-29-6 =1, S OOV and T OOV are a left bracket  X ( X  apart. Thus the average distance CO -Dist in the snippet set is: where Sum ( ) is the sum of Dist in each snippet. (5#) Rank Value ( RV )  X  Aims to consider the rank for translation candidates in the Web document set. It includes six parameters. Top _ Rank ( T_Rank ) is the rank of the snippet that first contains T OOV and given by the search engine. Average _ Rank ( A_Rank ) is the average position of T OOV in the returned snippets. where Sum ( ) denotes the rank sum of each snippet. Simple _ Rank ( S _ Rank ) is com-frequency and length of T OOV on ranking. R _ Rank is utilized as a co mparison basis. where  X  is set as 0.25 empirically, | T OOV | is the length of T OOV , and MAX _ WL denotes the maximum length of candidates. DF _ Rank ( D _ Rank ) is similar to S _ Rank , and TF
TOOV ( T OOV ), which aims at investigating the impact of the frequency of T OOV . (6#) Similarity of Context Vector ( SCV )  X  Aims to evaluate the distribution similari-ty between an OOV term and its candidates in the snippet set. T he OOV term S OOV and its candidate T OOV are first represented as two context vectors, CV SOOV =( ts 1 , ..., ts T
OOV s in the i th snippet respectively. Thus the SCV can be computed as: 
Boolean Feature (BF) is a binary feature and equivalent to a heuristic rule de-signed for the particular relations between an OOV term and its translation candi-dates. BFs are used to explore the different occurrence forms w ith higher possibility for the candidates in Web documents. (1#) Position Distance with OOV Term 1. (2#) Neighbor Relation with OOV Term ( NR _ S OOV )  X  If T OOV occurs prior or next to S OOV , this feature is set as 1. (3#) Bracket Neighbor Relation with OOV Term ( BNR _ S OOV )  X  If T OOV locates prior or next to S OOV and occurs with the form  X  T OOV ( S OOV ) X  or  X  S OOV ( T OOV ) X , this feature is set as 1. (4#) Special Mark Word ( SMW )  X  Within a certain co-occurrence distance (less than 10 character s) between an OOV term and its candidates, if there is such a term like  X   X  X  X  [ full name ] X ,  X   X  [ be named English translation terms and so on, this feature is set as 1. (5#) Capitalized First Letter ( CFL )  X  If T OOV begins with a capitalized lette r, this feature is set as 1. For the OOV term translation based on Web mining, another diffi culty is how to evaluate the relevance between an OOV term and its translation candidates, that is, how to rank all the tran slation candidates from  X  best  X  to  X  worst  X . 
The candidate ranking can be regarded as a binary classificatio n problem. However, usually only highly related fragments of OOV terms can be found , rather than their correct translations. Instead of regarding the candidate rankin g as binary classification, it is solved as an Ordinal Regression problem. Ranking SVM maps different objects into a certain kind of order rel ation. The key is modeling the judgements for user X  X  preferences, and then the constra int relations for ranking can be derived. 
For a S OOV , if there are two translation candidates T OOVi and T OOVj , the preference judgement can be formulated as T OOVi &gt; S OOV T OOV j . Thus more training samples are con-structed, which contain multiple constraint features. The judge ment can be trans-formed into the feature function as: can also be expressed as: where LF k ( , ), GF l ( , ) and BF m ( , ) are the local, global and Boolean feature repre-sentation respectively. These t hree kinds of feature representa tion can be incorporated as a whole and represented as a feature function family with th e multi-dimensional feature vector in Formula (19). Thus the relevance for each feature vector x (translation candidate) containing a group of features can be evaluated. 4,170 NEs are selected from th e Chinese NER corpus in SIGHAN200 5. The test set contains 310 Person Names (PRNs), 324 Location Names (LCNs) and 252 Organiza-tion Names (OGNs), and the remaining is taken as the training s et. 300 Chinese new terms chosen randomly from 9 categories (movie name, book title , brand name, ter-minology, idiom, rare animal name and NE), are used to investig ate the generalization ability of our model. Top -N -Inclusion -Rate is defined as the percentage of the OOV terms whose correct translations could be found in the first N translation candidates. 
To verify the effec tiveness for multiple feature fusion, the te st on the feature com-bination for our model is implemented. As shown in Table 1, the highest Top -1-Inclusion -Rate of 88.8889% can be acquired by u sing all the features. It can be seen from Table 1 that the most important features are P &amp; S _ IF , NR_S OOV , BNR_S OOV and UCR . As for the frequency feature, its contribution is limited, be cause many candi-dates with higher P &amp; S _ IF values are the terms with low frequency. However, when training based on only the featu res that are beneficial to the whole performance, the best translation accuracy is 85.80 24%, which is worse than that by combining all the features. Multiple feature fusion can indeed impr ove the transl ation accuracy. 
Yang et al. [23] is very simila r to our approach, we accomplish ed this method on the same data set to make a contrast, as shown in Table 2. It c an be concluded that the ranking based on the supervised learning outperforms the existi ng conventional strat-egies, Ranking SVM is better than SVM for ranking, and our appr oach is superior to Yang et al. X  X . Meanwhile, the best performance is obtained for PRNs. It shows that our model is sensitive to the cat egory and the po pularity of OO V term. 
Another test for the other kinds of Chinese OOV term is perform ed on the selected new terms and the consistent res ults can be observed in Table 3 . Four CLIR runs are carried out on the Chinese topic set and Eng lish corpus from TREC-9. (1) C -E _ LongCLIR 1  X  using Long Query (LQ, terms in both title and de-scription fields) and the Dicti onary-Based Translation (DBT); ( 2) C -E _ LongCLIR 2  X  using LQ, DBT and our model; (3) C -E _ ShortCLIR 1  X  using Short Query (SQ, only terms in the title field) and DBT; (4) C -E _ ShortCLIR 2  X  using SQ, DBT and our model. The Precision-Recall curves and Median Average Precision (MAP) are shown in Fig. 1. It can be seen from Fig. 1 that the best run is C -E _ LongCLIR 2, and its re-sults exceed those of C -E _ LongCLIR 1. By adopting both query translation based on bilingual dictionary and OOV term translation, Chinese-English CLIR for long query has gained the significant retrieval performance improvement. T he same conclusion can be obtained for the other two runs C -E _ ShortCLIR 1 and C -E _ ShortCLIR 2. 
Through analyzing the results, it can be found that the transla tion quality is highly related to the following aspects. (1) The translation results are associated with the search engine used, especially for some specific OOV terms. For example, given an OOV term  X   X  X  X  X  X  X  X   X , the mining result base d on Google in China is  X  to man-age economic affai rs according to 1aw  X , which is more reasonable than  X  Economic law  X  acquired by Bing. (2) S ome terms are idioms, conve ntional and po litical ter-minologies with Chinese charact eristics, and cannot be translat ed literally. For example,  X   X  X  X  X  X  X  [ party masses relationship ] X  should be translated into  X  party masses relationship  X , rather than  X  ties between the party  X  given by Google Translate. (3) The proposed model is sensitiv e to the notability degree of OOV term. This phenomenon is the main reason why there is an obvious differenc e among the transla-tion performance for PRN, LCN and OGN. (4) There are some particular and in-herent noises in the extracted translation candidates. For example, a candidate for the Chinese OOV term  X   X  X  X  X  X  X  X  X  X  X  [ Guangdong People  X  s Publishing House ] X  is  X  Guangdong ren min chu ban she  X . (5) Word Sense Disambiguation (WSD) should be added to improve the translation performance. Although most of OOV terms have a unique sense defi nition, there are still a few OOV terms with sense am-biguity, e.g.,  X   X  X  X  X  X  [ Northeastern University or Tohoku University ] X . Traditional OOV term translation methods concern two aspects, t hat is, transliteration and sense translation. However, more and more Chinese OOV terms cannot be meas-ured by phonetic or meaning information separately. Our propose d model improves the acquirement ability for Chine se-English OOV term translatio n through Web min-ing, and solves the translation pair selection and evaluation i n a novel way by fusing multiple features and introducing the supervised learning based on Ranking SVM. Our future research will focus on applying the key techniques o n statistical machine learning, alignment of sentence and phoneme, and WSD into Chine se-English OOV term translation. Acknowledgments. This work is supported by Natio nal Science &amp; Technology Pillar Program of China (No. 2012BAH59F04), National Natural Science F und of China (No. 61170095; No. 71171126), and Shanghai Municipal R&amp;D Founda tion (No. 12dz1500203, 12511505300). Cheng Jin is the corresponding autho r. 
