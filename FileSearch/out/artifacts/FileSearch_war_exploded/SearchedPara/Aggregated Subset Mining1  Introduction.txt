 Data mining essentially comes in two flavors, descriptive mining, finding de-scriptions of the data, and predictive mining, constructing f eatures for effective classification. In predictive mining, class-correlating patterns, patterns showing strong correlation with a class valu e, are often a good choice. No matter the reliability of statistic measures, however, they can still fall prey to over-fitting, which in turn may harm classifiers. This becomes even more problematic if the amount of patterns is large, or pairs and combinations of patterns reinforce each other X  X  bias.

In a recent work [3], the effect of decreasing redundancy between patterns on the accuracy of classifiers using those particular features was evaluated. While we could show that reducing redundancy  X  in some cases rather strongly  X  did in fact improve accuracy, we used the entire data set set for mining patterns which we then filtered. This setting, which is the standard data mining setting, is well-suited for descriptive mining. Predictive mining is closer related to machine learning, however, which knows differen t techniques using parts of the labeled data for verification purposes of found patterns/built classifiers.

We therefore take a page out of the playbook of ML, first mining several sets of correlating patterns, and then using different criteria to create final result sets from them. These are used as features for learning an SVM [5] classifier.
The paper is structured as follows: In the next section, we explain the basic mechanisms for mining patterns, creating subsets of the data for mining and selection purposes, and lay out several selection methods for deriving the final result set. In Section 3, we report on the experimental evaluation of the proposed methods before concluding in Section 4.
 We start from set of instances D m each being labeled with one of the class labels { pos, neg } .Inthisset D m we search for patterns drawn from a language L .More specifically for a set of k patterns whose occurrence on the instances correlates best with the presence of the target class according to  X  2 [4]. Further we require the found patterns to be free according to [1].
 The solutions to the mining task can then be conveniently modeled using
As said before, this is the standard data mining setting which operates on the full dataset D m , which we will use as a base-line technique. In the following sections we propose different metho ds for selecting the final pattern set. 2.1 Using a Validation Set The most basic approach consist of using a certain fraction q of the total data D m as the actual mining set D m , with size q  X |D m | . The rest would be used as a validation set  X  D m = D m \ D m ,ofsize(1  X  q )  X |D m | . After termination of the mining process on D m ,the k m patterns T h k m ( D m ) returned by the miner are evaluated on  X  D m and re-ranked, according t o their correlation score  X  2 achieved on this validation set. Out of those the k s best scoring patterns are returned to the user. It can easily be derived that k s should be chosen such that k s &lt;k m since for k m  X  k s the validation scores (and re-ranking) have no effect on the selection of patterns. The final result set is then
Given the use of statistically significant patterns, one would expect a certain robustness against statistical quirks. The degree to which the full distribution can be modeled by a subset could however very well be governed by q .Anot unusual choice for q in the machine learning literature is 2 3 . 2.2 Aggregating Subset Results In the second approach, subsets D i m of D m are created, and the top k m patterns mined from each of them. For the union of their results  X  all = i T h k m ( D i m )we know that |  X  all | X  k m . All patterns p  X   X  all are re-evaluated according to some aggregation metric, and a subset (e.g. the top-k m patterns) returned to the user.
This approach is illustrated in Figure 1, with merge denoting the merging/re-evaluation step. What should be immediately obvious from this figure is that this kind of approach lends itself to distributed/parallel mining, although the merging step needs to be perfo rmed on one particular site.

There are two main decisions that influence the result of this approach, namely the choice of subsets and the aggregation metric used. The size of the final set to be returned is obviously also important but has less effect than the afore-mentioned two choices, we believe. Formation of subsets. We investigate two approaches to forming subsets of D m . Their main difference lies in whether there is overlap among subsets used or not. The straightest-forward approach consists of segmenting D m into f disjunct folds F i . We define  X  D i m = F i and D the data have an effect on the final result with the same weight.
 Aggregation metrics. The goal of any aggregation metric used lies in ranking the patterns in  X  all by using information from all subsets D i m mined on. To this end, we propose three metrics: 1. A first measure takes the form  X  count ( p )= |{ i : p  X  X  h k m ( D i m ) }| 2. Hence, a second metric,  X  rank would consist of the following: 3. A final metric,  X   X  2 would take the form:  X   X  2 ( p )=(  X  2 of p on D m )calcu-Selection criteria. Due to the fact that |  X  all | X  k m , one can simply return the top-k m after the re-ranking via one of the metrics  X  ( p ). Thus, given a value k m ,a such that the p i  X   X  k m ( M , X  )arethe k m highest ranked pattern in  X  all w.r.t.  X  .
Additionally, this framework does allow for a second k -value ( k s ), similar to the one of the validation set approach which is used to define the size of the final result set leading to  X  all = i T h k m ( D i m )with  X  k s ( M , X  )  X   X  all . For the experimental evaluation, we arbitrarily picked 8 data sets from the NCI-60 data set collection [6] and mine sequential patterns on them. Each has about 3500 instances (one outlier having only 2778) with a class distri-bution of 50  X  53% (another outlier of 63 . 7%) for the positive class. We chose k m  X  X  10 , 25 , 50 , 75 , 100 } , giving a reasonable range of values across which to compare. We evaluated two aspects of the outlined techniques experimentally: Q1 Quantitative Analysis: The effect of different subset formation strategies Q2 Qualitative Analysis: The effect of different aggregation methods on the To get a robust accuracy estimate, a 10-fold cross-validation was performed. All folds  X  both for accuracy and selection pur poses  X  were stratified. As mentioned above, an SVM classifier was used for accuracy estimates. SVMs possess cer-tain inherent feature selection capabilities, giving low-relevance features small weights. In addition, an SVM attempts to find a separating hyperplane with a maximal margin to both classes. Both of these characteristics guard against over-fitting at the classifier level, allowing to evaluate the quality of the feature set . The SVM X  X  C parameter was tuned via a 5-fold cross-validation on the training data, with potential values 2 i ,i  X  [  X  2 , 14]. 3.1 Validating Patterns on Additional Data In the first setting, using a validation set for assessing found patterns X  quality, we evaluated two stratified random splits of the mining data, with q = 2 3 and q = 4 5 , Quantitative results. A useful measure for assessing quantitative charac-teristicsisthatofoverlap.Giventwosetsofpatterns S , S , we simply define ovlp ( S , S )= | S  X  S | . For evaluating the quantitativ e characteristics of selected pattern sets, we calculate the overlap with the standard mining operation (de-noted by non-validated ).
The pattern set overlap values show that the greater k m for a given k s ,the smaller the overlap between pattern se ts becomes. This means that patterns are ranked rather differently on the validation set although similar underlying distributions should be expected. Overlap is of course higher for comparison against the non-validated set since the validated set is constructed from this. It is interesting however, that the differe nce between comparison to the standard and the non-validated setting are not that great. Furthermore, there is no big change between the results for q = 2 3 and q = 4 5 .
 Qualitative results. Regarding Q2, we use the selected features to encode D m as binary vectors and evaluate the SVM X  X  performance. The main focus of our comparison lies on determining which q is better suited to the mining of  X  X ood X  features, and whether there ar e particularly well-suited k m  X  k s combinations.
We report the results on a representative data set in Table 2. Unfortunately, the answer seems to be that neither q is a good choice. Using a validation set selects features that are less well suited for classification than mining on the full data. This indicates that randomly splitting the data can give rise to so radically different distributions (hinted at in the quantitative analysis) that top-k selections based on  X  2 becomes meaningless.
 3.2 Aggregated Pattern Selection For the second setting  X  X sing different subsets to mine the data and using ag-gregation metrics X  we chose f  X  X  3 , 5 , 7 } , thus allowing for different sizes of D m . In addition to the standard setting, we compare to a post-processing method which uniformly picks k s patterns from  X  all at random. Since this method does not use explicit information on patterns X  quality, nor their relationship, we use it as a baseline to see whether the better informed methods enjoy an advantage. Quantitative results. Regarding Q1 and given similar results for all data sets, we report quantitat ive characteristics of  X  all in Tables 3 and 4 on one example. For both alternatives regarding construction of the D m we list the minimum and maximum  X  count and  X  rank for patterns in  X  all , |  X  all | /k m ,and ovlp (  X  k m ( M , X   X  ) , T h k m ( M )). We would expect that:  X  ovlp D  X  min p  X   X  all  X  count ( p ) &gt; 1  X  No pattern appears in only one result set  X  max p  X   X  all  X  count ( p )  X  f  X  The best patterns generalize over most D m  X  min p  X   X  all  X  rank ( p ) &gt; 1 /f  X  No pattern is always ranked worst  X  max p  X   X  all  X  rank ( p )  X  k m  X  The best patterns generalize over most D m ,ap-The evaluation shows that most of our expectations hold, the only serious excep-tions being our assumptions about the  X  X orst X  patterns  X  which often appear in only one T h k ( D m ). This indicates that even when using correlation measures, different data sets quickly lead to differing mining results. It is interesting to setting for a given k m , but depend on the value of f for the  X  D m setting. Qualitative results. Given the findings above, the more interesting question is which of the proposed techniques select patterns which are useful features for classification. Again, we used an SVM and 10-fold cross-validation to estimate the quality of pattern sets. Inasmuch as d ifferences in accuracy were almost never significant, we omit the actual accuracy estimates here. Instead we report how the different methods (each time a combination of D m composition and selection method) compare given a fixed k m  X  X  10 , 25 , 50 , 75 , 100 } and f  X  X  3 , 5 , 7 } (Table 5). Note that the table shows the total number of wins for each approach.
Each number denotes how often a particular technique has performed better than any other on any data set. We evaluated 9 techniques against each other on 8 data sets. Thus any given approach can have maximally 64 wins. Bold values denote the best-performing technique, given a k m and value of f , while a circle (  X  ) shows for which k m a technique performed best, given f .

The first, somewhat surprising, insight is that using large, overlapping D m , which should recreate phenomena over different mining situations, does not lead to good pattern selection. D m settings never perform best for a given k m and usually perform better if only relativel y few patterns are sel ected, suggesting that resampling does too little to counteract bias. Given that resampling forms the basis for, e.g., Bagging [2] techniques, we did not expect this outcome.
It is also noticeable that the standard approach produces suboptimal pattern sets. Only once is this baseline approach best, for f = 3, meaning relatively large folds where informed selection techniques such as count and rank do not enjoy a large advantage. Even there it is closely followed by the random selection -essentially the least informed one. This means that an unwritten paradigm of data mining (using large amounts of data to the fullest leads to meaningful patterns) turns out to be questionable in this case.

The random technique is the big winner of the entire comparison, given its simplicity. While reducing redundancy entirely by chance, it performs well in 4 of 15 settings. It is outperformed by rank (7 wins), but count is weaker (3 wins). Moreover, adding up all wins by technique, random outperforms rank and count , slightly for  X  D m settings, more pronounced for D m . So the information which patterns generalize well over different s ubsets does not give a strong advantage in our case study. However, the variety of patterns caused by several subsets is helpful. Re-evaluating patterns X   X  2 score does again not work satisfactory. In this work, we investigated ways of using data for pattern mining to produce good features for classification of complex data. Two main insights arise from the experimental evaluation: 1) usual assumptions on how to best use data in data mining turn out to be questionable. Neither the standard data mining set-ting (using large data sets to smooth over-fitting effects), nor a single mining and validation set, nor re-sampling techniques producing overlapping mining sets to uncover true underlying phenomena proved to be the most effective use. The best usage we observed consisted of splitting data into small, independent subsets instead, mining patterns on these and evaluating those patterns X  gener-alization capability on different subsets. 2) the actual s election method matters far less than could be expected. Given a l arge enough variety of patterns, pick-ing patterns at random proved to be rather effective, as proved the average rank selector, which picks patterns that wer e highly ranked at least once, even if not in all subsets. Using a validation set (either independent or involving the data patterns were mined from) for reassessing the  X  2 -score did not work satisfactory.
An unexpected boon of these results is that pattern mining can apparently be easily parallelised without having to fear the loss of valuable information in terms of patterns. Quite contrary, we have seen that merging pattern sets extracted from small independent data sets improves the merit of the found patterns.
There are still several open questions to pursue w.r.t. the evaluated tech-niques. As we have observed, the interplay between k m and k v for the validation set technique has an effect on the composition of resulting pattern sets, and different k m seem to favor certain aggregation techniques. It would therefore be valuable to perform stability studies, e.g. investigating whether final pattern sets stabilize for a certain value of k m . Additionally, there are potential further selection criteria which time and space co nstraints did not allow us to investigate.
