 Column generation (CG) [3] is a technique widely used in line ar programming (LP) for solving The proposed work here X  X hich we dub matrix generation (MG) X  X x tends the column generation the work presented here is of importance for many real applic ations. k -NN classifier and some clustering algorithms. Much effort h as been spent on learning a good Mahalanobis metric is learned from examples of proximity co mparison among triples of training data. For example, assuming that we are given triples of imag es a labels and a such that the distance from a margin between distances dist We overview some relevant work in this section.
 that in an LP framework, unknown weak hypotheses can be learn ed from the dual although the inspired our work.
 shrinks and the solution obtained is not necessarily a solut ion of the original SDP. We begin with some notational conventions and basic definiti ons that will be useful. denote the space of D  X  D symmetric matrices by S D , and positive semidefinite matrices by S D product of two matrices. An element-wise inequality betwee n two vectors writes u  X  v , which means u following statements are equivalent: (1) X &lt; 0 ( X  X  S D (  X  i ( X )  X  0 3.1 Extreme Points of Trace-one Semidefinite Matrices proposed algorithm.
 Definition 3.1 For any positive integer M , given a set of points { x matrix space Sp , the convex hull of Sp spanned by M elements in Sp is defined as: Define the convex hull 1 of Sp as: Here Z Definition 3.2 Let us define  X  trace equaling one: and  X  We also define  X  Lemma 3.3 Let  X  P are the extreme points (vertexes) of  X  extreme point of  X   X  :  X   X  = P M combination of the above-defined extreme points. So they can not be extreme points. Theorem 3.4  X  forms the set of extreme points of  X  Proof: It is easy to check that any convex combination P Tr P i  X  i Z i = P i  X  i Tr ( Z i ) = 1 .
 By denoting  X  P and P D only candidates for extreme points are those rank-one matri ces (  X  Hence, all Z  X   X  set is equal to the convex hull of its extreme points.
 PSD matrix constraint X  X   X   X  . At the first glance, this is a highly counterintuitive propo sition because  X  complicated constraints. Both  X  could be extremely (or even indefinitely) large. 3.2 Boosting and runs it repeatedly on modified data that are outputs from t he previous iterations. labels y . The final output strong classifier takes the form Here f as using boosting techniques.
 A sparse greedy approximation algorithm proposed by Zhang [ 2] is an efficient way of solving a solution u i is updated as u i = (1  X   X  ) u i  X  1 +  X  u i and the iteration goes on. We consider the Mahalanobis metric learning problem as an ex ample although the proposed tech-nique can be applied to many other problems in machine learni ng such as nonparametric kernel matrix learning [13].
 We are given a set of training examples a we are given a set S which contains the training triplets: S = { ( a where dist S an SDP problem [14]. We wish to maximize the margin that is defi ned as the distance between program we want to optimize is: ambiguity because the distance inequalities are scale inva riant.
 To simplify our exposition, we write The last constraint in (3) is then written solvers can only solve problems up to a few thousand variable s, which makes many applications of rank-one unitary PSD matrices: X = P M a semi-infinite linear program (SILP) because it has an infini tely large set of variables  X  . has extremely many variables (columns) but much fewer const raints, CG can be very beneficial. this may not be possible. But for some types of problems it is p ossible. We now consider Problem (P derived: the primal and dual problems coincide. For LPs and SDPs, stro ng duality holds under very mild conditions (almost always satisfied by LPs and SDPs consider ed here). relaxed version of the dual problem. With a finite  X  Z , the first set of constraints in (D we can solve the LP that satisfies all the existing constraint s.
 to RMP leads to a new RMP that needs to be re-optimized. In our c ase, by finding the violated algorithm as an oracle that either finds a new Z  X  such that To make convergence fast, we find the one that has largest devi ation. That is, Again here  X  w us denote Opt (B that guarantees the optimal convex combination over all Z  X  X  satisfying the constraints in  X  found. If Opt (B ated, hence the name matrix generation . 5.1 Base Learning Algorithm In this section, we show that the optimization problem (B using eigen-decomposition.
 means k u k By denoting the optimization in (B It is clear that the largest eigenvalue of  X  H ,  X  original problem (B There are approximate eigenvalue solvers, which guarantee that for a symmetric matrix U and any method [16]. Algorithm 1 : PSDBoost for semidefinite metric learning.
 Input : Training set triplets ( a Initialization : while true do end Output : criterion is Opt (B rank ( B ) ,  X  matrices A and B .
 An advantage of the proposed PSDBoost algorithm over standa rd boosting schemes is the totally-for details. learned metric is of size 24  X  24 . The triplets are obtained in this way: For a point a nearest neighbor in the same class a the dual problem (D for many problems such as metric and kernel learning.
 by PSDBoost and a standard SDP solver. Both are 1 . 3% . We have presented a new boosting algorithm, PSDBoost, for le arning a positive semidefinite ma-are currently exploring new applications with PSDBoost. Al so we want to know what kind of SDP optimization problems can be approximately solved by PSDBo ost.

