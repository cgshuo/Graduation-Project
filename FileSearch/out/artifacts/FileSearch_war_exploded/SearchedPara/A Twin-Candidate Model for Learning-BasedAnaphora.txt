 Institute for Infocomm Research Institute for Infocomm Research School of Computing, National University of Singapore
Thetraditionalsingle-candidatelearningmodelforanaphoraresolutionconsiderstheantecedent we propose a twin-candidate model for anaphora resolution. The main idea behind the model learns a classifier that determines the preference between competing candidates, and, during present in detail the framework of the twin-candidate model for anaphora resolution. Further, we explore how to deploy the model in the more complicated coreference resolution task. We evaluate the twin-candidate model in different domains using the Automatic Content Extraction data sets. The experimental results indicate that our twin-candidate model is superior to the single-candidatemodelforthetaskofpronominalanaphoraresolution.Forthetaskofcoreference resolution, it also performs equally well, or better. 1. Introduction
Anaphora is reference to an entity that has been previously introduced into the dis-course (Jurafsky and Martin 2000). The referring expression used is called the anaphor and the expression being referred to is its antecedent . The anaphor is usually used to refer to the same entity as the antecedent; hence, they are coreferential with each other. The process of determining the antecedent of an anaphor is called anaphora resolution . As a key problem in discourse and language understanding, anaphora resolution is crucial in many natural language applications, such as machine translation, text summarization, question answering, information extraction, and so on. In recent years, supervised learning approaches have been widely applied to anaphora resolu-tion, and they have achieved considerable success (Aone and Bennett 1995; McCarthy and Lehnert 1995; Connolly, Burger, and Day 1997; Kehler 1997; Ge, Hale, and Charniak 1998; Soon, Ng, and Lim 2001; Ng and Cardie 2002b; Strube and Mueller 2003; Luo et al. 2004; Ng et al. 2005).
 can be automatically learned from annotated data. Traditionally, learning-based ap-proaches to anaphora resolution adopt the single-candidate model, in which the po-tential antecedents (i.e., antecedent candidates ) are considered in isolation for both learning and resolution. In such a model, the purpose of classification is to determine if a candidate is the antecedent of a given anaphor. A training or testing instance is formed by an anaphor and each of its candidates, with features describing the properties of the anaphor and the individual candidate. During resolution, the antecedent of an anaphor is selected based on the classification results for each candidate.
 is the antecedent of an anaphor is completely independent of the other competing candidates. However, anaphora resolution can be more accurately represented as a ranking problem in which candidates are ordered based on their preference and the best one is the antecedent of the anaphor (Jurafsky and Martin 2000). The single-candidate model, which only considers the candidates of an anaphor in isolation, is incapable of effectively capturing the preference relationship between candidates for its training.
Consequently, the learned classifier cannot produce reliable results for preference deter-mination during resolution.
 anaphora resolution. The main idea behind the model is to recast anaphora resolution as a preference classification problem. The purpose of the classification is to determine the preference between two competing candidates for the antecedent of a given anaphor. In the model, an instance is formed by an anaphor and two of its antecedent candidates, with features used to describe their properties and relationships. The antecedent is selected based on the judged preference among the candidates.
 part, we will introduce the framework of the twin-candidate model for anaphora reso-lution, including detailed training procedures and resolution schemes. In the second part, we will further explore how to deploy the twin-candidate model in the more complicated task of coreference resolution. We will present an empirical evaluation of the twin-candidate model in different domains, using the Automatic Content Extraction (ACE) data sets. The experimental results indicate that the twin-candidate model is superior to the single-candidate model for the task of pronominal anaphora resolution.
For the coreference resolution task, it also performs equally well, or better. 2. Related Work
To our knowledge, the first work on the twin-candidate model for anaphora resolution was proposed by Connolly, Burger, and Day (1997). Their work relied on a set of features that included lexical type, grammatical role, recency, and number/gender/semantic agreement, and employed a simple linear search scheme to choose the most preferred candidate. Their system produced a relatively low accuracy rate for pronoun reso-lution (55.3%) and definite NP resolution (37.4%) on a set of selected news articles.
Iida et al. (2003) used the twin-candidate model (called the tournament model in their work) to perform Japanese zero-anaphora resolution. They utilized the same linear 328 scheme to search for antecedents. Compared with Connolly, Burger, and Day (1997), they adopted richer features in which centering information was incorporated to cap-ture contextual knowledge. Their system achieved an accuracy of around 70% on a data set drawn from a corpus of newspaper articles. Both of these studies were carried out on uncommon data sets, which makes it difficult to compare their results with other baseline systems. In contrast to the previous work, we will explore the twin-candidate model comprehensively by describing the model in more detail, trying more effective resolution schemes, deploying the model in the more complicated coreference resolution task, performing more extensive experiments, and evaluating the model in more depth.
 used a ranking learning algorithm (based on Maximal Entropy) to train a preference classifier for antecedent selection. They reported an accuracy of around 72 X 76% for the different domains in the ACE data set. In our study, we will also investigate the solution of using a general ranking learner (e.g., Ranking-SVM). By comparison, the twin-candidate model is applicable to any discriminative learning algorithm, no matter whether it is capable of ranking learning or not. Moreover, as the model is trained and tested on pairwise candidates, it can effectively capture various relationships between candidates for better preference learning and determination.
 on the preference between the potential partitions of NPs, instead of the potential antecedents of an NP as in our work. Given an input document, the model first em-ployed n pre-selected coreference resolution systems to generate n candidate partitions of NPs. The model learned a preference classifier (trained using Ranking-SVM) that could distinguish good and bad partitions during testing. The best rank partition would be selected as the resolution output of the current text. The author evaluated the model on the ACE data set and reported an F-measure of 55 X 69% for the different domains.
Although ranking-based, Ng X  X  model is quite different from ours as it operates at the cluster-level whereas ours operates at the mention-level. In fact, the result of our twin-candidate system can be used as an input to his model. 3. The Twin-Candidate Model for Anaphora Resolution 3.1 The Single-Candidate Model
Learning-based anaphora resolution uses a machine learning method to obtain p ( ante ( C anaphor ana in the context of its antecedent candidates, C candidate model assumes that the probability that C k is the antecedent is only de-pendent on the anaphor ana and C k , and independent of all the other candidates.
That is:
Thus, the probability of a candidate C k being the antecedent can be approximated using the classification result on the instance describing the anaphor and C tems (Aone and Bennett 1995; Ge, Hale, and Charniak 1998; Preiss 2001; Strube and
Mueller 2003; Kehler et al. 2004; Ng et al. 2005). In our study, we also build as the baseline a system for pronominal anaphora resolution based on the single-candidate model.
 is an anaphor and candi is an antecedent candidate. 1 For training, instances are created for each anaphor occurring in an annotated text. Specifically, given an anaphor ana and its antecedent candidates, a set of negative instances (labeled  X 0 X ) is formed by pairing ana and each of the candidates that is not coreferential with ana . In addition, a single positive instance (labeled  X 1 X ) is formed by pairing ana and the closest antecedent, that is, the closest candidate that is coreferential with ana . anaphor has two or more antecedents, but we only create one positive instance for the closest antecedent as its reference relationship with the anaphor is usually the most direct and thus the most confident.
 their closest antecedents, respectively. Supposing that the antecedent candidates of the two anaphors are just all their preceding NPs in the current text, the training instances to be created for the text segment are listed in Table 2. 330 instance as it is not the closest candidate that is coreferential with the anaphor. the characteristics of the anaphor and the candidate, as well as their relationships from lexical, syntactic, semantic, and positional aspects. Table 3 lists the features used in our study. All these features can be computed with high reliability, and have been proven effective for pronoun resolution in previous work.
 algorithm. During resolution, given a newly encountered anaphor, a test instance is formed for each of the antecedent candidates. The instance is passed to the classifier, which then returns a confidence value indicating the likelihood that the candidate is the antecedent of the anaphor. The candidate with the highest confidence is selected as the antecedent. For example, suppose [ 7 it ] is an anaphor to be resolved. Six test instances is supposed to give the highest confidence to i { [ 7 it ],[ candidate [ 5 the government ] is the antecedent of [ 7 it ]. 3.2 A Problem with the Single-Candidate Model
As described, the assumption behind the single-candidate model is that the probability of a candidate being the antecedent of a given anaphor is completely independent of the other competing candidates. However, for an anaphor, the determination of the antecedent is often subject to preference among the candidates (Jurafsky and Martin 2000). Whether a candidate is the antecedent depends on whether it is the  X  X est X  among the candidate set, that is, whether there exists no other candidate that is preferred over it. Hence, simply considering one candidate individually is an indirect and unreliable way to select the correct antecedent.
 (2001) summarizes different factors that influence the interpretation of anaphoric expressions. Some factors such as morphology (gender, number, animacy, and case) or syntax (e.g., the role of binding and commanding relations [Chomsky 1981]) are  X  X liminating, X  forbidding certain NPs from being antecedents. However, many others are  X  X referential, X  giving more preference to certain candidates over others; examples include: we think of the act of eliminating candidates as giving them low preference. pronominal anaphora resolution. For example, the SHRDLU system by Winograd (1972) prefers antecedent candidates in the subject position over those in the object position.
The system by Wilks (1973) prefers candidates that satisfy selectional restrictions with the anaphor. Hobbs X  X  algorithm (Hobbs 1978) prefers candidates that are closer to the anaphor in the syntax tree, and the RAP algorithm (Lappin and Leass 1994) prefers candidates that have a high salience value computed by aggregating the weights of different factors.
 preference by using classification confidence for candidates; that is, the higher con-fidence value the classifier returns, the more likely the candidate is preferred as the antecedent. Nevertheless, as the model considers only one candidate at a time during training, it cannot effectively capture the preference between candidates for classifier learning. For example, consider an anaphor and a candidate C 332 candidates in the candidate set, C i is the antecedent and forms a positive instance. Otherwise, C i is not selected as the antecedent and thus forms a negative instance.
Simply looking at a candidate alone cannot explain this, and may possibly result in inconsistent training instances (i.e., the same feature vector but different class labels).
Consequently, the confidence values returned by the learned classifier cannot reliably reflect the preference relationship between candidates. 3.3 The Twin-Candidate Model
To address the problem with the single-candidate model, we propose a twin-candidate model to handle anaphora resolution. As opposed to the single-candidate model, the model explicitly learns a preference classifier to determine the preference relationship between candidates. Formally, the model considers the probability that a candidate is the antecedent as the probability that the candidate is preferred over all the other competing candidates. That is:
Assuming that the preference between C k and C i is independent of the preference between C k and the candidates other than C i , we have:
Thus: mated using the classification results on the set of instances describing C the other competing candidates. To do this, we learn a classifier that, given any two can-didates of a given anaphor, can determine which one is preferred to be the antecedent of the anaphor. The final antecedent is identified based on the classified preference relationships among the candidates. This is the main idea of the twin-candidate model. is an anaphor, and C i and C j are two of its antecedent candidates. The class label of an instance represents the preference between the two candidates for the antecedent, for example,  X 01 X  indicating C j is preferred over C i and  X 10 X  indicating C
Being trained with instances built based on this principle, the classifier is capable of de-termining the preference between any two candidates of a given anaphor by returning a class label, either  X 01 X  or  X 10 X , accordingly. In the next section, we will introduce in detail a system based on the twin-candidate model for anaphora resolution. 3.4 Framework of the Twin-Candidate Model 3.4.1 Instance Representation. In the twin-candidate model, an instance takes the form i { ana , C i , C j } , where ana is an anaphor and C i and C labeled  X 10 X  if C i is preferred over C j as the antecedent, or  X 01 X  if otherwise. and relationships between ana and each of the candidates, C system with the twin-candidate model adopts the same feature set as the baseline system with the single-candidate model (shown in Table 3). The difference is that a feature for the single candidate, candi X , has to be replaced by a pair of features for the twin candidates, candi1 X and candi2 X . For example, feature candi Pron , which describes whether a candidate is a pronoun, will be replaced by two features candi1 Pron and candi2 Pron , which describe whether C i and C j are pronouns, respectively. anaphor should be composed of two candidates with an explicit preference relationship, for example, one being an antecedent and the other being a non-antecedent. A pair of candidates that are both antecedents or both non-antecedents are not suitable for instance creation because their preference cannot be explicitly represented for training, although it does exist.
 closest antecedent, C ante , as the anchor candidate. 3 C candidates C nc that is not coreferential with ana .If C ante instance i { ana , C nc , C ante } is created and labeled  X 01 X . Otherwise, if C instance i { ana , C ante , C nc } is created and labeled  X 10 X  instead.
 anaphor [ 7 it ], the closest antecedent, [ 5 the government ] (denoted as NP the anchor candidate. It is paired with the four non-coreferential candidates (i.e., NP
NP 3 , NP 4 ,and NP 6 ) to create four training instances. Among them, the instances formed with NP 1 , NP 3 or NP 4 are labeled  X 01 X  and the one with NP all the training instances to be generated for the text. 3.4.3 Classifier Generation. Based on the feature vectors for the generated training in-stances, a classifier can be trained using a discriminative learning algorithm. Given a 334
C is preferred. 3.4.4 Antecedent Identification. After training, the preference classifier can be used to resolve anaphors. The process of determining the antecedent of a given anaphor, called antecedent identification , could be thought of as a tournament, a competition in which many participants play against each other in individual matches. The candidates are like players in a tournament. A series of matches between candidates is held to determine the champion of the tournament, that is, the final antecedent of the anaphor under con-sideration. Here, the preference classifier is like the referee who judges which candidate wins or loses in a match.
 directly. For anaphors that have more than one candidate, two possible schemes can be employed to find the antecedent.
 the loser in a match is immediately eliminated. Such a scheme is also applicable to antecedent identification. In the scheme, candidates are compared linearly from the beginning to the end. Specifically, the first candidate is compared with the second one, forming a test instance, which is then passed to the classifier to determine the prefer-ence. The  X  X osing X  candidate that is judged less preferred by the classifier is eliminated and never considered. The winner, that is, the preferred candidate, is compared with the third candidate. The process continues until all the candidates are compared, and the candidate that wins in the last comparison is selected as the antecedent.  X  X erfect X  classifier that can correctly determine the preference between candidates. That is, the candidates that are coreferential with the anaphor will be classified as preferred over those that are not. (If the two candidates are both coreferential or both non-coreferential with the anaphor, the one closer to the anaphor in position is preferred.)
To resolve the anaphor [ 7 it ], the candidate NP 1 is first compared with NP instance is classified as  X 01 X , indicating NP 2 is preferred. Thus, NP
NP 2 continues to compete with NP 3 and NP 4 until it fails in the comparison with NP
Finally, NP 5 beats NP 6 in the last match and is selected as the antecedent. All the test instances to be generated in sequence for the resolution of [ Table 7.
 where N is the number of the candidates. Thus, it enables a relatively large number of candidates to be processed. However, as our twin-candidate model imposes no constraints that enforce transitivity of the preference relation, the preference classifier would likely output C 1 C 2 , C 2 C 3 ,and C 3 C 1 . Hence, it is unreliable to eliminate a candidate once it happens to lose in one comparison, without considering all of its winning/losing results against the other candidates.
 the antecedent can be calculated using the preference classification results between the candidate and its opponents. The candidate with the highest preference is selected as the antecedent, that is: be preferred over C j as the antecedent of ana . If we define the score of C
Then, the most preferred candidate is the candidate that has the maximum score. If we simply use 1 to denote the result that C i is classified as preferred over C preferred otherwise, then:
That is, the score of a candidate is the number of the opponents to which it is preferred, less the number of the opponents to which it is less preferred. To obtain the scores, the antecedent candidates are compared with each other. For each candidate, its comparison 336 result against every other candidate is recorded. Its score increases by one if it wins a match, or decreases by one if it loses. The candidate with the highest score is selected as the antecedent.
 ment called Round Robin in which each participant plays every other participant once, and the final champion is selected based on the winning X  X osing records of the players.
In contrast to the Elimination scheme, the Round Robin scheme is more reliable in that the preference of a candidate is determined by overall comparisons with the other competing candidates. The computational complexity of the scheme is O( N is the number of the candidates.
 generated for resolving the anaphor [ 7 it ] are listed in Table 8. As shown, each of the candidates is compared with every other competing candidate. The scores of the candidates are summarized in Table 9. Here, the candidate NP in the comparisons and obtains the maximum score of five. Thus it will be selected as the antecedent.
 Robin scheme. In the weighted version, the confidence values returned by the classifier, instead of the simple 0 and 1, are employed to calculate the score of a candidate based on the formula
Here, CF is the confidence value that the classifier returns for the corresponding instance. 3.5 Evaluation 3.5.1 Experimental Setup. We used the ACE (Automatic Content Extraction) data set for evaluation. All the experiments were done on the ACE-2 V1.0 corpus. It contains two data sets, training and devtest, which were used for training and testing, respectively. Each of these sets is further divided into three domains: newswire (NWire), newspaper (NPaper), and broadcast news (BNews). Statistics for the data sets are sum-marized in Table 10.
 pipeline of NLP modules including a Tokenizer, Part-of-Speech tagger, NP chunker,
Named-Entity (NE) Recognizer, and so on. These preprocessing modules were meant to determine the boundary of each NP in a text, and to provide the necessary information about an NP for subsequent processing. Trained and tested on the UPEN WSJ TreeBank, the POS tagger (Zhou and Su 2000) could obtain an accuracy of 97% and the NP chunker (Zhou and Su 2000) could produce an F-measure above 94%. Evaluated for the MUC-6 and MUC-7 Named-Entity task, the NER module (Zhou and Su 2002) could provide an F-measure of 96.6% (MUC-6) and 94.1% (MUC-7).
 that had at least one preceding NP in their respective annotated coreferential chains. We used the accuracy rate as the evaluation metric, and defined it as follows:
Here, an anaphor is deemed  X  X orrectly resolved X  if the found antecedent is in the co-referential chain of the anaphor. 338 anaphor is usually short, predominantly (98% for the current data set) limited to only one or two sentences (McEnery, Tanaka, and Botley 1997). For this reason, given an anaphor, we only took the NPs occurring within the current and previous two sentences as initial antecedent candidates. The candidates with mismatched number and gender agreement were filtered automatically from the candidate set. Also, pronouns or NEs that disagreed in person with the anaphor were removed in advance. For training, there were 1,241 (NWire), 1,466 (NPaper), and 1,291 (BNews) anaphors found with at least one antecedent in the candidate set. For testing, the numbers were 313 (NWire), 399 (NPaper), and 271 (BNews). On average, an anaphor had nine antecedent candidates. distribution. Note that for the single-candidate model, the number of  X 1 X  instances was identical to the number of anaphors in the training data, because we only used the closest antecedents of anaphors to create the positive instances. The number of  X 0 X  instances was equal to the total number of  X 01 X  and  X 10 X  training instances for the twin-candidate model.

Entropy (Berger, Della Pietra, and Della Pietra 1996), and SVM (linear kernel) (Vapnik 1995), 5 using the software See5, 6 OpenNlp.MaxEnt, 7 and SVM-light, the classifiers were learned with the default learning parameters set in the respective learning software. 3.5.2 Results and Discussions. Table 12 lists the performance of the different anaphora resolution systems with the single-candidate (SC) and the twin-candidate (TC) models. For the TC model, two antecedent identification schemes, Tournament Elimination and Round Robin, were compared.
 can obtain accuracy of up to 72.9% (NWire), 77.1% (NPaper), and 74.9% (BNews).
The average accuracy is comparable to that reported by Kehler et al. (2004) (around 75%), who also used the single-candidate model to do pronoun resolution with similar features (using MaxEnt) on the ACE data sets. By contrast, the systems with the twin-candidate model are able to achieve accuracy of up to 75.7% (NWire), 82.0% (NPaper), and 78.9% (BNews). The average accuracy is 76.9% for C5, 77.4% for MaxEnt, and 78.7% 2.2%, and 3.6% in accuracy). These results confirm our claim that the twin-candidate model is more effective than the single-candidate model for the task of pronominal anaphora resolution.
 produced by the two antecedent identification schemes, Tournament Elimination and
Round Robin. This is in contrast to our belief that the Round Robin scheme, which is more reliable than the Tournament Elimination, should lead to much better results. One possible reason could be that the classifier in our systems can make a correct preference judgement (with accuracy above 92% as in our test) in the cases where one candidate is the antecedent and the other is not. As a consequence, the simple linear search can find the final antecedent as well as the Round Robin method. These results suggest that we can use the Elimination scheme in a practical system to make antecedent identification more efficient. (Recall that the Elimination scheme requires complexity of O ( N ), instead of O ( N 2 ) as in Round Robin.) the results using the twin-candidate model and those directly using a preference learn-ing algorithm. For this purpose, we built a system based on Ranking-SVM (Joachims 2002), an extension of SVM capable of preference learning. 340 training, given an anaphor, a set of instances is created for each of the antecedent candi-dates. To learn the preference between competing candidates, a  X  X uery-ID X  is specified for each training instance in such a way that the instances formed by the candidates of the same anaphor bear the same query-ID. The label of an instance represents the rank of the candidate in the candidate set; here,  X 1 X  for the instances formed by the candidates that are the antecedents, and  X 0 X  for the instances formed by the others. The training instances are associated with features as defined in Table 3, to which the Ranking-
SVM algorithm is then applied to generate a preference classifier. During resolution, for each candidate of a given anaphor, a test instance is formed and passed to the learned classifier, which in turn returns a value to represent the rank of the candidate among all the candidates. The anaphor is resolved to the one with the highest value. that the algorithm will, in the background, pair any two instances that have the same query-ID but different rank labels. This is quite similar to the twin-candidate model, which creates an instance by putting together two candidates with different preferences.
However, one advantage of the twin-candidate model is that it can explicitly record various relationships between two competing candidates, for example,  X  X hich one of the two candidates is closer to the anaphor in position/syntax/semantics? X  inter-candidate information can make the preference between candidates clearer, and thus facilitate both preference learning and determination. In contrast, Ranking-SVM, which constructs instances in the single-candidate form, cannot effectively capture this kind of information.

SVM. We can see that the system achieves an average accuracy of 76.7%, statistically significantly better than the baseline system with the single-candidate model by 1.6% (0.4% for NWire, 2.0% for NPaper, and 2.2% for BNews). The results lend support to our claim that the preference relationships between candidates, if taken into consideration for classifier training, can lead to better resolution performance. Still, we observe that our twin-candidate model beats Ranking-SVM in average accuracy by 1.8% (Elimina-tion scheme) and 2.0% (Round Robin).
 classifier can be easily interpreted by humans, and the importance of the features can be visually illustrated. In Figures 1 and 2, we show the decision trees (top four levels) output by C5 for the NWire domain, based on the single-candidate and the twin-candidate models, respectively. As the twin-candidate model uses a larger pool of features, the tree for the twin-candidate model is more complicated (180 nodes) than the one for the single-candidate model (36 nodes).
 cal, positional, and grammatical properties for pronoun resolution. However, we can see that the preferential factors (e.g., subject preference, parallelism preference, and distance preference as discussed in Section 3.2) are more clearly presented in the twin-candidate-based tree. For example, if two candidates are both pronouns, the twin-candidate-based tree will suggest that the one closer to the anaphor has a higher preference to be the antecedent. By contrast, such a preference relationship has to be implicitly represented in the single-candidate-based tree, with different confidence values being assigned to the candidates in different sentences.
 data size might influence anaphora resolution performance. For this purpose, we di-vided the anaphors in the training documents into 10 batches, and then performed resolution using the classifiers trained with 1, 2, ..., 10 batches of anaphors. Figure 3 plots the learning curves of the systems with the single-candidate model and the twin-candidate model (Round Robin scheme) for the NPaper domain. Each accuracy rate shown in the figure is the average of the results from three trials trained on different anaphors.
 candidate model reach their peak performance with around six batches (around 880 anaphors). As shown, the twin-candidate model is not apparently superior to the single-candidate model when the size of the training data is small (below two batches, 290 anaphors). This is due to the fact that the number of features in the twin-candidate model is nearly double that in the single-candidate model. As a result, the twin-candidate model requires more training data than the single-candidate model to avoid the data sparseness problem. Nevertheless, it does not need too much training data to beat the latter; it can produce the accuracy rates consistently higher than the 342 single-candidate model when trained with more than two batches of anaphors. This figure further demonstrates that the twin-candidate model is reliable and effective for the pronominal anaphora resolution task. 4. Deploying the Twin-Candidate Model to Coreference Resolution
One task that is closely related to anaphora resolution is coreference resolution ,the process of identifying all the coreferential expressions in texts. is different from anaphor resolution. The latter focuses on how an anaphor can be suc-cessfully resolved, and the resolution is done on given anaphors. The former, in contrast, focuses on how the NPs that are coreferential with each other can be found correctly and completely, and the resolution is done on all possible NPs. In a text, many NPs, especially the non-pronouns, are non-anaphors that have no antecedent to be found in the previous text. Hence, the task of coreference resolution is a more complicated challenge than anaphora resolution, as a solution should not only be able to resolve an anaphor to the correct antecedent, but should also refrain from resolving a non-anaphor. In this section, we will explore how to deploy the learning models for anaphor resolution in the coreference resolution task. As pronouns are usually anaphors, we will focus mainly on the resolution of non-pronouns. 4.1 Coreference Resolution Based on the Single-Candidate Model
In practice, the single-candidate model can be applied to coreference resolution directly, using the similar training and testing procedures to those used in anaphora resolution (described in Section 2).
 that is, the NP that is coreferential with at least one preceding NP. Specifically, given an anaphor and its antecedent candidates, a positive instance is generated for the closest antecedent and a set of negative instances is generated for each of the candidates that is not coreferential with the anaphor. 12 money ] are two anaphors, with [ 1 Globalstar ]and[ 2 $600 million ] being their antecedents, respectively. Table 14 lists the training instances to be created for this text. 344 are similar to those proposed in Soon, Ng, and Lim X  X  (2001) system. are domain independent and the values can be computed with low cost but high reliability.

Given an NP to be resolved, a test instance is generated for each of its antecedent candidates. The classifier, being given the instance, will determine the likelihood that the candidate is the antecedent of the possible anaphor. If the confidence is below a pre-specified threshold, the candidate is discarded. In the case where none of the candidates have a confidence higher than the threshold, the current NP is deemed a non-anaphor and left unresolved. Otherwise, it is resolved to the candidate with the highest confidence. 14 4.2 Coreference Resolution Based on the Twin-Candidate Model
The twin-candidate model presented in the previous section focuses on the preference between candidates. The model will always select a  X  X est X  candidate as the antecedent, even if the current NP is a non-anaphor. To deal with this problem, we will teach the preference classifier how to identify non-anaphors, by incorporating non-anaphors to create a special class of training instances. For resolution, if the newly learned classifier returns the special class label, we will know that the current NP is a non-anaphor, and no preference relationship holds between the two candidates under consideration. In this way, the twin-candidate model is capable of carrying out both antecedent identification and anaphoricity determination by itself, and thus can be deployed for coreference resolution directly. In this section, we will describe the modified training and resolution procedures of the twin-candidate model. 4.2.1 Training. As with anaphora resolution, an instance of the twin-candidate model for coreference resolution takes the form i { ana , C i , C that for the single-candidate model as defined in Table 15, except that a candi X feature is replaced by a pair of features, cand1 x and candi2 x , for the two competing candi-dates, respectively.
 instances in the same way as in the original learning framework. If the NP is a non-anaphor, we do the following: The instances formed by the non-anaphors are labeled  X 00. X  [ the money ], we create the  X 01 X  and  X 10 X  instances as usual. For the non-anaphors [
Schwartz ]and[ 6 the debt market ], we generate two sets of  X 00 X  instances. Table 16 lists all the training instances for the text (supposing [ 1 Globalstar ]and[ anchor candidates for [ 3 Schwartz ]and[ 6 the debt market ], respectively). a classifier. Given a test instance i { ana , C i , C j } supposed to return  X 01 X  (or  X 10 X ), indicating ana is an anaphor and C as its antecedent, or return  X 00 X , indicating ana is a non-anaphor and no preference exists between C i and C j . 346 4.2.2 Antecedent Identification. Accordingly, we make a modification to the original Tour-nament Elimination and the Round Robin schemes: resolved, candidates are compared linearly from the beginning to the end. If an instance for two competing candidates is classified as  X 01 X  or  X 10 X , the preferred candidate will be compared with subsequent competitors while the loser is eliminated immediately.
If the instance is classified as  X 00 X , both the two candidates are discarded and the comparison restarts with the next two candidates. 16 The process continues until all the candidates have been compared. If both of the candidates in the last match are judged to be  X 00 X , the current NP is left unresolved. Otherwise, the NP will be resolved to the final winner, on the condition that the highest confidence that the winner has ever obtained is above a pre-specified threshold.
 with every other candidate. If two candidates are labeled  X 00 X  in a match, both candi-dates receive a penalty of  X  1 in their respective scores. If no candidate has a positive final score, then the NP is considered non-anaphoric and left unresolved. Otherwise, it is resolved to the candidate with the highest score as usual. Here, we can also use a threshold. That is, we will update the scores of the two candidates in a match if and only if the preference confidence returned by the classifier is higher than a pre-specified threshold.
 pseudo-instance is created by pairing the candidate with itself. The NP will be resolved to the candidate unless the instance is labeled  X 00 X . 4. 3Evaluation 4.3.1 Experimental Setup. We used the same ACE data sets for coreference resolution evaluation, as described in the previous section for anaphora resolution. A raw input document was processed in advance by the same pipeline of NLP modules including
POS-tagger, NP chunker, NE recognizer, and so on, to obtain all possible NPs and related information (see Section 3.5.1).
 and precision 17 were computed by comparing the key chains (i.e., the annotated  X  X tan-dard X  coreferential chains) and the response chains (i.e., the chains generated by the coreference resolution system).
 meant for non-pronouns that are often not anaphoric. To better examine the utility of the model in our experiments, we first focused on coreference resolution for non-pronominal NPs. The recall and precision to be reported were computed based on the response chains and the key chains from which all the pronouns are removed. We will later show the results of overall coreference resolution for whole NPs by combining the resolution of pronouns and non-pronouns.
 distance apart as they do in pronoun resolution. For this reason, during training, we took as antecedent candidates all the preceding non-pronominal NPs and previous four sentences; while during testing, we used all the preceding non-pronouns, regardless of distance, as candidates. 19 The statistics of the training instances for each data set are summarized in Table 17.
 both the single-candidate and the twin-candidate models used a threshold to block low-confidence coreferential pairs, we performed three-fold cross-evaluation on the training data to determine the thresholds for the coreference resolution systems. 4.3.2 Results and Discussions. Table 18 lists the results for the different systems on the non-pronominal NP coreference resolution. We used as the baseline the system with the single-candidate model described in Section 4.1. As mentioned, the system was trained 348 on the instances formed by anaphors. For better comparison with the twin-candidate model, we built another single-candidate-based system in which the non-anaphors were also incorporated for training. Specifically, for each encountered non-anaphor during training, we created a set of  X 0 X  instances by pairing the non-anaphor with each of the candidates. These instances were added to the original instances formed by anaphors to learn a classifier, 21 which was then applied for the resolution as usual. trained with the instances formed only by anaphors, the system could achieve recall above 60% and precision of around 50% for the three domains. When trained with the instances formed by both anaphors and non-anaphors, the system yielded a significant improvement in precision. In the case of using C5 and SVM, the system is capable of producing precision rates of up to 80%. The increase in precision is reasonable since the classifier tends to be stricter in blocking non-anaphors. Unfortunately, however, at the same time recall drops significantly, and no apparent improvement can be observed in the resulting overall F-measure.
 candidate model, described in Section 4.2, are capable of yielding higher precision against the baseline. Although recall also drops at the same time, the increase in precision can compensate it well: We observe that in most cases, the system with the twin-candidate model can achieve a better F-measure than the baseline system with the single-candidate model. Also, the improvement is statistically significant ( t -test, p &lt; 0.05) in the NWire domain when C5 is used (3.6%), and in the NPaper domain when any of the three learning algorithms, C5 (5.0%), MaxEnt (1.4%), and SVM (4.6%), is used. These results suggest that our twin-candidate model can effectively identify non-anaphors and block their invalid resolution, without affecting the accuracy of determining antecedents for anaphors.
 we find that for non-pronoun resolution the superiority of the twin-candidate model against the single-candidate model is not apparent. In some domains such as BNews, the difference between the two models is not statistically significant. One possible explanation is that for non-pronoun resolution, the features that really matter are quite limited, that is, NameAlias, String-Matching, and Appositive (we will later show this in the decision trees). A candidate that has any one of these features is most likely the antecedent, regardless of the other competing candidates. In this situation, the single-candidate model, which considers candidates in isolation, does as well as the twin-candidate model. Still, the results suggest that the twin-candidate model is suitable for both resolution tasks, no matter whether the features involved are strongly indicative (as with non-pronoun resolution) or not (as with pronoun resolution).
 ence between the two twin-candidate identification schemes, Tournament Elimination and Round Robin. The Round Robin scheme performs better than Elimination when trained using C5 and SVM, by up to 2.8% and 2.9% in F-measure, respectively. However, the Elimination scheme, when trained using MaxEnt, is capable of performing equally well or slightly better (0.5% F-measure) than the Round Robin scheme.
 precision patterns for different systems. The baseline system with the single-candidate model tends to yield higher recall while the system with the twin-candidate model tends to produce higher precision. Thus, a fairer comparison of the two systems is to examine the precision rates that these systems achieve under the same recall rates. For this purpose, in Figure 4, we plot the variant recall and precision rates that the two systems are capable of obtaining (tested using MaxEnt, Round Robin scheme, for the NPaper domain), focusing on precision rates above 50% and recall rates above 40%.
From the figure, we find that the system with the twin-candidate model achieves higher precision for recall rates ranging from 40% and 55%, and performs equally well for recall rates above 55%, which further proves the reliability of our twin-candidate model for coreference resolution.
 generated by the systems with the single-candidate model and the twin-candidate model, respectively. The tree from the single-candidate model contains only 13 nodes, considerably smaller than that from the twin-candidate model, which contains around 1.2k nodes. From the figure, we can see that both models heavily rely on string-matching, name-alias, and appositive features to perform non-pronoun resolution, in contrast to pronoun resolution where lexical and positional features seem more impor-tant (as shown in Figures 1 and 2).
 resolution performance of the two learning models on different quantities of training data. Figure 7 plots the learning curves for the systems using the single-candidate model and the system using the twin-candidate model (NPaper domain). The F-measure is averagedoverthreerandomtrialstrainedon5,10,15,...documents.Consistentwith the curves for the anaphora resolution task as depicted in Figure 3, the system with the twin-candidate model outperforms the one with the single-candidate model on a small amount of training data (less than five documents). When more data is available, 350 the twin-candidate model also yields a consistently better F-measure than the single-candidate model.
 twin-candidate model on coreference resolution for non-pronouns, we now further examine overall coreference resolution for whole NPs, combining both pronoun resolution and non-pronoun resolution. Specifically, given an input test document, we check each encountered NP from beginning to end. If it is a pronoun, the pronominal anaphora resolution systems, as described in the previous section, to resolve it to an antecedent. Otherwise, we use the non-pronoun coreference resolution systems described in this section to resolve the NP to an antecedent, if any is found. All the coreferential pairs are put together in a coreferential chain. The recall and precision rates are computed by comparing the standard key chains and generated response chains using Vilain et al. X  X  (1995) algorithm. 352 models. We observe that the results for overall coreference resolution are better than those of non-pronoun coreference resolution as shown in Table 18, which is due to the comparatively high accuracy of the resolution of pronouns.
 tion, the twin-candidate model outperforms the single-candidate model in coreference resolution for whole NPs. Consider the system trained with MaxEnt as an example.
The single-candidate-based system obtains F-measures of 58.3%, 61.5%, and 62.2% for the NWire, NPaper, and BNews domains. 23 By comparison, the twin-candidate-based system (Round Robin scheme) can achieve F-measures of 59.2%, 63.3%, and 63.0% for the three domains. The improvement over the single-candidate model in F-measure (0.9%, 1.8%, and 0.8%) is larger than that for non-pronoun resolution (0.4%, 1.4%, and 0.6% as shown in Table 18), owing to the higher gains obtained from pronoun resolution. For the systems trained using C5 and SVM, similar patterns of performance improvement may be observed. 5. Conclusion
In this article, we have presented a twin-candidate model for learning-based anaphora resolution. The traditional single-candidate model considers candidates in isolation, and thus cannot accurately capture the preference relationships between competing candidates to provide reliable resolution. To deal with this problem, our proposed twin-candidate model recasts anaphora resolution as a preference classification problem.
It learns a classifier that can explicitly determine the preference between competing candidates, and then during resolution, choose the antecedent of an anaphor based on the ranking of the candidates. anaphora resolution, including instance representation, training procedure, and the an-tecedent identification scheme. The efficacy of the twin-candidate model for pronominal anaphora resolution has been evaluated in different domains, using ACE data sets. The experimental results show that the model yields statistically significantly higher accu-racy rates than the traditional single-candidate model (up to 4.2% in average accuracy rate), suggesting that the twin-candidate model is superior to the latter for pronominal anaphora resolution.
 more complicated coreference resolution task, where not all the encountered NPs are anaphoric. We have modified the model to make it directly applicable for coreference resolution. The experimental results for non-pronoun resolution indicate that the twin-candidate-based system performs equally well, and, in some domains, statistically significantly better than the single-candidate based systems. When combined with the results for pronoun resolution, the twin-candidate based system achieves further improvement against the single-candidate-based systems in all the domains. directions. Currently, we only adopt simple domain-independent features for learning.
Our recent work (Yang, Su, and Tan 2005) suggests that more complicated features, such as statistics-based semantic compatibility, can be effectively incorporated in the twin-candidate model for pronoun resolution. In future work, we intend to provide a more in-depth investigation into the various kinds of knowledge that are suitable for the twin-candidate model. Furthermore, in our current work for coreference resolution, all the
NPs preceding an anaphor are used as antecedent candidates, and all encountered non-anaphors in texts are incorporated without filtering into training instance creation. For more balanced training data and better classifier learning, we intend to explore some instance-sampling techniques, such as those proposed by Ng and Cardie (2002a), to remove in advance low-confidence candidates and the less informative non-anaphors.
We hope that these efforts can further improve the performance of the twin-candidate model in both anaphora resolution and coreference resolution.
 Acknowledgments References 354
