 Representing words into vectors in continuous space can form up a potentially powerful basis to generate high-quality textual features for many text mining and natural language processing tasks. Some recent efforts, such as the skip-gram model, have attempted to learn word representations that can capture both syntactic and semantic information among text corpus. However, they still lack the capability of encod-ing the properties of words and the complex relationships among words very well, since text itself often contains in-complete and ambiguous information. Fortunately, knowl-edge graphs provide a golden mine for enhancing the quality of learned word representations. In particular, a knowledge graph, usually composed by entities (words, phrases, etc.), relations between entities, and some corresponding meta in-formation, can supply invaluable relational knowledge that encodes the relationship between entities as well as categor-ical knowledge that encodes the attributes or properties of entities. Hence, in this paper, we introduce a novel frame-work called RC-NET to leverage both the relational and categorical knowledge to produce word representations of higher quality. Specifically, we build the relational knowl-edge and the categorical knowledge into two separate reg-ularization functions, and combine both of them with the original objective function of the skip-gram model. By solv-ing this combined optimization problem using back propa-gation neural networks, we can obtain word representations enhanced by the knowledge graph. Experiments on popular text mining and natural language processing tasks, including analogical reasoning, word similarity, and topic prediction, have all demonstrated that our model can significantly im-prove the quality of word representations.
 I.2.6 [ Artificial Intelligence ]: Learning; I.5.1 [ Pattern Recognition ]: Models Algorithms; Experimentation Distributed word representations; deep learning; knowledge graph
Deep learning techniques have been frequently used to solve natural language processing (NLP) tasks [8, 1, 12, 21, 22]. The main purpose of them is to learn distributed rep-resentations of words (i.e., word embedding) from text, and use them as components or the basis to generate textual fea-tures for solving NLP tasks. Recently, some efficient meth-ods, such as the continuous bag-of-word model (CBOW) and the continuous skip-gram model [18], have been proposed to leverage the context of each word in text streams to learn word embedding, which can capture both the syntactic and the semantic information among words. The principle be-hind these models is that words that are syntactically or semantically similar should also have similar context words.
Although these works have demonstrated their effective-ness in a number of NLP tasks, they still suffer from some limitations. In particular, as these works learn word rep-resentations mainly based on the word co-occurrence infor-mation, the obtained word embedding cannot capture the relationship between two syntactically or semantically sim-ilar words if either of them yields very little context infor-mation. On the other hand, even enough amount of context could be noisy or biased such that they cannot reflect the inherent relationship between words and further mislead the training process. To solve these problems, we propose to in-corporate the information from knowledge graphs into the learning process in order to produce better word represen-tations.

Knowledge graph is a kind of knowledge base, which has been widely used to store complex structured and unstruc-tured knowledge. It is usually in the form of a directed or undirected graph that leverages vertices and edges to represents entities (words, phrases, etc.) and their rela-tionships, respectively. Knowledge graphs, such as Free-base [10] and WordNet [24], have started playing important roles in many text mining and NLP applications, including expert system, question-answer system, etc. A knowledge graph commonly contains two forms of knowledge: rela-tional knowledge and categorical knowledge. Specifically, re-lational knowledge (like is-a, part-of, child-of, etc.) encodes the relationship between entities so as to differentiate word pairs with analogy relationships; categorical knowledge (like gender, location, etc.) encodes the attributes and properties of entities, according to which similar words can be grouped into the meaningful categories. Both relational and cate-gorical knowledge extracted from the knowledge graph, an example of which is shown in Figure 1, can serve as valuable external information to enhance learning word representa-tions. Specifically, in the learning process, the relational knowledge can be leveraged to infer certain explicit connec-tions between the embeddings of related words, and the cat-egorical knowledge can be used to reflect coherence between the embeddings of those words with the same attributes, even if some of them yield very little context information, or biased/noisy context information.

In this paper, we propose a novel framework to take ad-vantage of both relational and categorical knowledge to pro-duce high-quality word representations. This framework is built upon the skip-gram model [18], in which we extend its objective function by incorporating the external knowledge as regularization functions. In particular, to leverage the relational knowledge, we define a corresponding regulariza-tion function by inheriting the similar idea from a recent study on multi-relation model [5], which characterizing the relationships between entities by interpreting them as trans-lations in the low-dimensional embeddings of the entities. To incorporate the categorical knowledge, we define another regularization function by minimizing the weighted distance between those words with the same attributes. Then, we combine these two regularization functions with the original objective function of the skip-gram model. After solving this combined optimization problem via back propagation neu-ral networks, we can obtain the continuous representations of words. We call the proposed framework as RC-NET , indi-cating the incorporation of both R elational and C ategorical knowledge into neural NET works to learn word embeddings. We have conducted empirical experiments on three popular text mining and NLP tasks, including analogical reasoning, word similarity, and topic prediction, with large-scale pub-lic datasets, and the results all demonstrate that, compared with the state-of-the-art methods, our proposed approach can significantly improve the quality of word representations by encoding both the word co-occurrence information and the external knowledge.

The rest of the paper is organized as follows. We briefly review the related work on learning word embedding via deep neural networks in Section 2. In Section 3, we describe the proposed framework to incorporate relational and cat-egorical knowledge in learning word representations. The experimental setup and results are reported in Section 4. The paper is concluded in Section 5.
Building distributed word representations [14] has attracted increasing attention in the area of machine learning. Re-cently, to show its effectiveness in a variety of text min-ing and NLP tasks, a series of works applied deep learning techniques to learn high-quality word representations. For example, Collobert et al. [7, 8] proposed a neural network that can learn a unified word representations suited for sev-eral NLP tasks simultaneously. Furthermore, Mikolov et al. proposed efficient neural network models for learning word representations, i.e., word2vec [18]. This work introduced two specific models, including the continuous bag-of-words model (CBOW) and the continuous skip-gram model (skip-gram), both of which are unsupervised models learned from large-scale text corpora. Under the assumption that similar words yield similar context, these models maximize the log likelihood of each word given its context words within a slid-ing window. The learned word representations amazingly show that they can indicate both syntactic and semantic regularities.

Nevertheless, since most of existing works learned word representations mainly based on the word co-occurrence in-formation, it is quite difficult to obtain high quality embed-dings for those words with very little context information; on the other hand, large amount of noisy or biased con-text could give rise to ineffective word embeddings either. Therefore, it is necessary to introduce extra knowledge into the learning process to regularize the quality of word embed-ding. Unfortunately, there are very few previous studies that attempt to explore knowledge powered word embedding.
Some efforts have paid attention to learn word embedding in order to address knowledge base completion and enhance-ment [6, 21, 23]; however, they did not investigate the other side of the coin, i.e., leveraging knowledge to enhance word representations. Recently, there have been some early at-tempts on this direction. For example, Luong et al. [16] pro-posed a neural model to learn morphologically-aware word representations by combining recursive neural network and neural language model. In this model, they explicitly uti-lize the knowledge in terms of morphological structure inside a word and regard each morpheme as a basic unit. While being restricted to the morpheme-level knowledge, this at-tempt has not taken investigation on more important word-level knowledge, such as analogical relation between words. In contrast, we will mainly explore how to take advantage of word-level knowledge to enhance word embedding in this paper.

Most recently, Yu et al. [25] attempted to use knowledge to improve word representations. In particular, they pro-posed a new learning objective that incorporates both a neural language model objective and a semantic prior knowl-edge objective. By leveraging the knowledge in terms of semantic similarity between words during the learning pro-cess, they demonstrate that their new method can result in improvement by evaluations on three tasks: language mod-eling, measuring semantic similarity, and predicting human judgments. However, this model is specified on incorporat-ing semantic knowledge and it does not explicitly distin-guish different kinds of relational knowledge. Bian et al. [2] recently proposed to leverage morphological, syntactic, and semantic knowledge to advance the learning of word embed-dings. Particularly, they explored these types of knowledge to define new basis for word representation, provide addi-tional input information, and serve as auxiliary supervision in the learning process. Although they did intensive empiri-cal study, they did not make model-level innovation to lever-age external knowledge to improve word representations.
In contrast to all the aforementioned works, in this pa-per, we present a general method to leverage various types of knowledge into learning word representations. With the target at incorporating more extensive forms of knowledge, we define a new learning objective as a combination between that of the raw text and that of external knowledge. As a result, our new model is able to learn word representations with encoding both contextual information and extra knowl-edge, which is much more general and flexible than previous works.
In this section, we first introduce the continuous skip-gram model, which serves as the basis of the proposed frame-work. Then, we describe how we model relational knowledge and categorical knowledge as regularization functions. Af-ter that, we introduce the proposed RC-NET framework by incorporating these regularization functions into the skip-gram model to strengthen the learning of word representa-tions. At last, we describe how we solve the optimization problem in the proposed framework.
We take the continuous skip-gram model [18] as the ba-sis of our proposed framework. 1 Skip-gram is a recently
Note that although we use the skip-gram model as an ex-ample to illustrate our framework, the similar framework can be developed on the basis of any other word embedding models. proposed algorithm for learning word representations using a neural network model, whose underlying principle lies in that similar words should have similar contexts. In the skip-gram model (see Figure 2), a sliding window is employed on the input text stream to generate the training samples. In each sliding window, the model tries to use the central word as input to predict the surrounding words. Specifically, the input word is represented in the 1-of-V format, where V is the size of the entire vocabulary of the training data and each word in the vocabulary is represented as a long vec-tor with only one non-zero element. In the feed-forward process, the input word is first mapped into the embedding space by the weight matrix M . After that, the embedding vector is mapped back to the 1-of-V space by another weight matrix M 0 , and the resulting vector is used to predict the surrounding words after conducting softmax function on it. In the back-propagation process, the prediction errors are propagated back to the network to update the two weight matrices. After the training process converges, the weight matrix M is regarded as the learned word representations.
Specifically, given a sequence of training text stream w 1 w ,...,w K , the objective of the skip-gram model is to maxi-mize the following average log probability: where w k is the central word, w k + j is a surrounding word, and N indicates the context window size to be 2 N + 1. The conditional probability p ( w k + j | w k ) is defined in the follow-ing softmax function: where v w and v 0 w are the input and the output latent vari-ables, i.e., the input and output representation vectors of w , and V is the vocabulary size.

To calculate the prediction errors for back propagation, we need to compute the derivative of p ( w k + j | w computation cost is proportional to the vocabulary size V . As V is often very large, it is difficult and sometimes im-practical to directly compute the derivative. The typical method to solve this problem is noise-contrastive estima-tion (NCE) [13], which aims at fitting unnormalized prob-abilistic models. NCE can approximate the log probabil-ity of softmax by performing logistic regression to discrim-inate between the observed data and some artificially gen-erated noise. It has been applied to the neural probabilis-tic language model [20] and the inverse vector log-bilinear model [19]. A simpler method to deal with the problem is negative sampling (NEG) [18], which generates l noise samples for each input word to estimate the target word, in which l is a very small number compared with V . Therefore, the training time yields linear scale to the number of noise samples and it becomes independent of the vocabulary size. Suppose the frequency of word w is u ( w ), then the proba-bility of sampling w is usually set to p ( w )  X  u ( w ) 3 / 4
After briefing the skip-gram model, we introduce how we equip it with the relational knowledge. According to the left part of Figure 3, relational knowledge encodes the re-lationship between words. Inspired by a recent study on multi-relation model [5] that builds relationships between entities by interpreting them as translations operating on the low-dimensional representations of the entities, we pro-pose to use a function E r as described below to capture the relational knowledge.

Specifically, the existing relational knowledge in knowl-edge graphs is usually represented in the triplet ( head , re-lation , tail ) (denoted by ( h,r,t )  X  S , where S is the set of relational knowledge), which consists of two words h,t  X  W ( W is the set of words) and a relationship r  X  R ( R is the set of relationships). To learn the relation representations, we make an assumption that relationships between words can be interpreted as translation operations and they can be represented by vectors. The principle in our model is that if the relationship ( h,r,t ) holds, the representation of the tail word t should be close to the representation of the head word h plus the representation vector of the relation-ship r , i.e., h + r ; otherwise, h + r should be far away from t . Note that this model learns word representations and relation representations in the same continuous embedding space.

According to the above principle, we define E r as a margin-based regularization function over the set of relational knowl-edge S .
 E
In the above formulation, [ x ] + denotes the positive part of x ,  X  &gt; 0 is a margin hyperparameter, and d ( x,y ) is the distance measure for the words in the embedding space. For simplicity, we define d ( x,y ) as the Euclidean distance be-tween x and y . The set of corrupted triplets S 0 ( h,r,t ) fined as: which is constructed from S by replacing either the head word or the tail word by another random selected word such that S T S 0 =  X  .

Note that the optimization process might trivially mini-mize E r by simply increasing the norms of word represen-tations and relation representations. To avoid this problem, we use an additional constraint on the norms, which is a commonly-used trick in the literature [5, 4, 6, 15]. However, instead of enforcing the L 2 -norm of the word representa-tions to 1, we adopt a soft norm constraint on the relation representations as below: where  X  (  X  ) is the sigmoid function  X  ( x i ) = 1 / (1 + e r is the i -th dimension of relation vector r , and x latent variable, which guarantees that every dimension of the relation representation vector is within the range (  X  1 , 1).
By combining the skip-gram objective function and the regularization function derived from relational knowledge, we get the following combined objective J r that incorporates relational knowledge into the word representations learning system, where  X  is the combination coefficient. Our goal is to min-imize the combined objective J r , which can be optimized using back propagation neural networks. We call this model as Relational Knowledge Powered Model, and denote it by R-NET for ease of reference.
After introducing R-NET, we describe how we equip the skip-gram model with the categorical knowledge. According to the right part of Figure 3, categorical knowledge encodes the attributes or properties of words, from which we can group similar words according to their attributes. Then we may require the representations of words that belong to the same category to be close to each other.

For a specific kind of categorical knowledge, it can be represented by a similarity matrix Q , in which the element q ( w i ,w j ) is the similarity score between w i and w j that many kinds of categorical knowledge can be mined from knowledge graphs, and they might vary a lot in their simi-larity properties. For example, in Figure 1, we can see that representations. the categorical knowledge  X  X ynonyms of United Kingdom X  only consists of several entities including United Kingdom, Great Britain, U.K., etc., and these words are strongly sim-ilar to each other since they are all aliases of the United Kingdom; in the same figure, we can also find that the cat-egorical knowledge  X  X ale X  or  X  X emale X  consists of a massive number of person names, but these persons are similar only because they are all men or women, which is a very weak similarity. From the above examples, we can observe that, in most cases, categorical knowledge with smaller capacity is more likely to contain more specific information, so that we are more confident in grouping the words with that cate-gorical knowledge close to each other. On the contrary, the categorical knowledge with larger capacity is more likely to contain more general information, so that we are less con-fident in grouping the corresponding words. We use this heuristic to constrain the similarity scores: where if a word shares the same category with many other words, their mutual similarity scores will become small. Then, we encode the categorical knowledge using another regular-ization function E c : where d ( w i ,w j ) is the distance measure for the words in the embedding space and s ( w i ,w j ) serves as a weighting function. Again, for simplicity, we define d ( w i ,w Euclidean distance between w i and w j .

By combining the skip-gram objective function and the regularization function derived from the categorical knowl-edge, we get the following combined objective J c that incor-porates categorical knowledge into the word representations learning system, where  X  is the combination coefficient. Our goal is to min-imize the combined objective J c , which can be optimized using back propagation neural networks. We call this model as Categorical Knowledge Powered Model, and denote it by C-NET for ease of reference.
After describing the R-NET and C-NET models, it is nat-ural to combine them into a global framework which can leverage both relational knowledge and categorical knowl-edge to learn word representations. Specifically, in the global framework, we want to minimize the following combined ob-jective function: We call this framework as Joint Knowledge Powered Model, and denote it by RC-NET for ease of reference.

Figure 4 shows the architecture of the proposed RC-NET framework. Compared to either of R-NET and C-NET, RC-NET shows strong superiorities. Relational knowledge mainly helps build the global structure of the learned word representations by utilizing the relationship between differ-ent words; while categorical knowledge helps improve the local structure of the learned word representations by clus-tering similar words together. Hence, RC-NET might yield to a structured embedding space and reduce the randomness of word representations caused by the incomplete or biased training information. Actually with the RC-NET frame-work, the relational knowledge and categorical knowledge can compensate each other. On one hand, sometimes the relational knowledge of some words might be absent, but we can obtain their similar words from categorical knowl-edge and then make inference on their relations according to the relationships of their similar words. On the other hand, sometimes the categorical knowledge of a word might be missing. However, if the word share the same relation-ships with a number of other words, we will be able to infer its categorical knowledge from the categories of these related words.
In the implementation, we optimize the regularization func-tions derived from both the relational knowledge and the
Relation embedding  X  categorical knowledge along with the training process of the skip-gram model. During the procedure of learning word representations from the context words in the sliding win-dow, if the central word w k hits the external knowledge, the corresponding optimization process of the knowledge based regularization function will be activated.
The two knowledge branches and the skip-gram branch share the same word representations in the learning pro-cess. In our implementation, the optimization is conducted by stochastic gradient descent in a mini-batch mode, whose computational complexity is comparable to that of the op-timization process of the skip-gram model.
In this section, we conduct experiments to examine whether incorporating relational knowledge and categorical knowl-edge into learning continuous word representations can sig-nificantly improve the quality of word embeddings. In par-ticular, we compare the performance of our knowledge pow-ered model and that of the state-of-the-art baselines by eval-uating the quality of respective learned word embedding on three text mining and NLP tasks, including analogical rea-soning, word similarity, and topic prediction. In the rest of this section, we first introduce the experimental setup, and then report evaluation results and further analysis on the analogical reasoning task, the word similarity task, and the topic prediction task, respectively.
In our experiments, we trained word embeddings on a publicly available text corpus 2 , a dataset about the first bil-lion characters from Wikipedia. After being pre-processed by removing all the HTML meta-data and hyper-links and replacing the digit numbers into English words, the final training corpus contains totally 123.4 million words, where the number of unique words, i.e., the vocabulary size, is about 220 thousand.
In the following experiments, we will compare four meth-ods: Skip-gram (baseline), R-NET , C-NET , and RC-NET . To train the word embedding using these four meth-ods, we apply the same setting for their common parameters. Specifically, the count of negative samples was set to 3; the context window size was set to 5; each model was trained through 1 epoch; the learning rate was initialized as 0.025 and was set to decrease linearly so that it approached zero at the end of training.

Moreover, the combination weights in R-NET, C-NET, and RC-NET also play a critical role in producing high-quality word embedding. Overemphasizing the weight of the original objective of Skip-gram may result in weakened influence of knowledge, while putting too large weight on knowledge powered objectives may hurt the generality of learned word embedding. According to our empirical expe-rience, it is a better way to decide the objective combination weights of the Skip-gram model, relational knowledge, and categorical knowledge based on the scale of their respective derivatives during optimization. Specifically, it is better to set the objective weight of C-NET (  X  ) as a smaller value http://mattmahoney.net/dc/enwik9.zip than the objective weight of Skip-gram and R-NET since the derivative of C-NET objective usually yields a larger scale than that of Skip-gram and R-NET. Along our experiments in the following, we set  X  = 1 for R-NET,  X  = 0 . 001 for C-NET, and  X  = 1,  X  = 0 . 0001 for RC-NET. Note that, this parameter setting may not be optimal for different training corpus or various tasks, but the following experiments may illustrate its robustness to some extent. The analogical reasoning task was originally introduced by Mikolov et al [18, 17], which defines a comprehensive test set that contains five types of semantic analogies and nine types of syntactic analogies 3 . For example, to solve semantic analogies such as Germany : Berlin = France : ? , we need to find a vector x such that the embedding of x , denoted as vec( x ) is the closest to vec( Berlin ) -vec( Germany ) + vec( France ) according to the cosine distance. This specific example is considered to have been answered correctly if x is Paris . Another example of syntactic analogies is quick : quickly = slow : ? , the correct answer of which should be slowly .
 In our experiments, we use an enlarged dataset called WordRep [11] which extends the original evaluation dataset of analogical reasoning task. In particular, this larger dataset is generated by extracting more analogy pairs from Longman dictionary 4 . Finally, we collect totally 34,773 relevant word pairs in the enlarged dataset. In our experiments, we split the whole dataset into two parts with a ratio of 4:1, in which the larger part is used for training and the smaller part for testing. To form up the testing set from the smaller part of dataset, we connect every two-word pairs from the same relation together to generate a set of four-word tuple as ana-logical questions. Note that we avoid using those word pairs for training if at least one of their two words also appears in the testing set .
R-NET. For training R-NET, we directly use relation pairs and relation types as supervised information to learn representation vectors of different relations.

C-NET. For training C-NET, we extract the categorical knowledge from those relations. Specifically, given one rela-tion, the set of head words extracted from all pairs of this relation forms up a category, and the collection of tail words forms up another category. For instance, there are 1467  X  X ity-in-state X  word pairs in the training part of the Wor-dRep dataset. We split them into two categories: one is the collection of cities, while the other corresponds to the set of states. Each of them will be treated as a type of categorical knowledge for training C-NET.

RC-NET. Finally, we employ all the relational and cat-egorical knowledge applied for training R-NET and C-NET in the learning process of RC-NET.
In our experiments on the analogical reasoning task, we compared the baseline word embedding trained by Skip-http://code.google.com/p/word2vec/source/browse/trunk/ questions-words.txt http://www.longmandictionariesonline.com/ gram against those trained by R-NET, C-NET, and RC-NET. The dimension of word embedding is set as 100 and 300. Table 1 illustrates the semantic, syntactic, and to-tal accuracy by using the four methods. From this table, we can find that all of the knowledge powered models out-perform the baseline skip-gram model, and RC-NET yields the largest improvements. These results can imply that the knowledge powered word embedding is of higher quality than the baseline model with no knowledge regularizations.
From Table 1, we can also observe that incorporating re-lational knowledge to the skip-gram model can increase the accuracy of all three sub-types of the analogical reasoning task; meanwhile, incorporating the categorical knowledge can give rise to a higher accuracy on semantic analogies but a decreasing performance on the syntactic analogies. We hypothesize the reason as, there are some syntactic relation-ships, such as  X  X pposite X , whose head or tail word collection do not strictly form up a word group representing a coherent category.

In order for deeper understanding on why our new meth-ods can learn higher-quality word embedding, we take case studies on a specific syntactic relationship called  X  X djective to Adverb X  and a specific semantic relationship called  X  X ale to Female X . In particular, we apply the two-dimensional PCA projection on the 100-dimensional learned word em-bedding of randomly selected word pairs. Figure 5 reveals the RC-NET X  X  capability of learning the representations of relational knowledge and that of constructing the distribu-tions of words in the embedding space. In other words, from this figure, it is easy to see that, by incorporating relational knowledge, R-NET can produce word embedding such that the offset vector of any word pair in the same relationship tends to yield a common direction with similar distance, while by incorporating categorical knowledge, C-NET at-tempts to generate word embedding such that those words corresponding to the same topic or domain tend to be close to each other.
Another standard dataset for evaluating vector space mod-els is the WordSim-353 dataset [9], which consists of 353 pairs of nouns. Each pair is presented without context and associated with 13 to 16 human judgments on similarity and relatedness on a scale from 0 to 10. For example, ( cup , drink ) received an average score of 7.25, while ( cup , substance ) re-ceived an average score of 1.92. To evaluate the quality of learned word embeddings, we compute Spearman X  X   X  cor-relation between the similarity scores computed based on learned word embeddings and human judgments.

Since this task expects those similar or highly-correlated words are close to each other, it could only need to incor-porate the categorical knowledge extracted from similar or highly-correlated words. Thus, we only evaluate the effec-tiveness of C-NET for this task.
To train C-NET, it is necessary to collect the categorical knowledge that can reflect the topic or concept information of words. In our experiments, we extract such knowledge for training from Freebase [3]. As a structured knowledge base, Freebase organizes words according to a couple of basic task based on our proposed models.
 word pairs were chosen randomly. relations. Among them, we take advantage of the  X  X ype of X  relation to generate the categorical knowledge since this type of relation can naturally reflect the correlation between entities (words) with topics.

While Freebase contains many domain-specific words, such as professional terminologies and names (person, location, business), these words are so rare in the general training corpus that they yield quite limited contribution to improve word embedding quality. Therefore, to address this prob-lem, we only collect the categorical knowledge related to a pre-defined vocabulary, which only contains common nouns in the Longman Dictionary and filters our all multi-word Table 2: Results obtained by the different methods on the word similarity task.
 Methods Vector Dimensionality Spearman X  X   X  correlation Skip-gram 100 0.652 300 0.678 phrases and non-alphabetic characters. Finally, we select the top 10 human rated topic sets, including astronomy, biology, boats, chemistry, computer, fashion, food, geology, interests, and language, as the categorical knowledge for training, the vocabulary size of which is 3,174. Table 2 compares the performance of C-NET against Skip-gram on the word similarity task. From this table, we can find that C-NET can achieve better performance than Skip-gram on this task no matter 100 or 300 vector dimen-sion is applied. These results indicate that C-NET can more effectively let those words similar in terms of topic or concept be close to each other in the obtained representation space, as it incorporates the categorical knowledge extracted from Freebase explicitly into the learning process so as to encode the similarity and correlation between words into the word representations.
In many text mining and NLP applications, it is important to identify the topic of any given word since it can provide useful semantic information. For instance, both the word  X  X tar X  and  X  X arth X  correspond to the topic of  X  X stronomy X , and both  X  X ell X  and  X  X euron X  belong to the topic of  X  X iology X . In the rest of this section, we evaluate word embedding via the topic prediction task, whose goal is to find the most related topic word for a given word.

Our proposed methods, especially R-NET and RC-NET can be naturally applied to solve this task. In particular, since R-NET and RC-NET can learn the relation embed-ding beyond word embedding, given a word h and the topic relation embeddings r , we can predict the topic word t as the one that has the shortest Euclidean distance to h + r over the whole vocabulary. Although Skip-gram and C-NET do not explicitly produce the relation embedding in the train-ing process, for a specific relation r , we are able to compute the average offset vector t  X  h for any word pair  X  h,t  X  be-longing to this relation as the embedding of r . Then, we can follow the same way of R-NET and RC-NET to solve the topic prediction task.
For training C-NET for this task, we leverage the same categorical knowledge used for the word similarity task, as described in Section 4.3.2. To obtain relational knowledge for training R-NET and RC-NET, we simply transform the dataset about categorical knowledge into a new format to represent relational knowledge. Specifically, the relational knowledge is in the triple format ( h,r,t ), where h is a spe-Table 3: Results obtained by the comparing meth-ods on the topic prediction task.

Relation Error Rate[%] Skip-gram C-NET R-NET RC-NET astronomy 2.00 2.00 8.00 2.00 chemistry 6.67 5.71 9.52 15.24 computer 19.54 8.62 6.32 4.02 language 33.33 33.33 18.52 7.41 cific word under a certain topic, r is the corresponding topic relation, and t is the name of topic or concept.
In the following experiments, we split the generated knowl-edge data into training set and testing set by the ratio of 1:1 for each relation. Note that there is no overlap between the training set and the testing set in the vocabulary except for the topic word t . The dimension of word representations is set as 100.

Table 3 reports the error rates of different word embed-ding models on the topic prediction task. From this table, we can see that knowledge powered models can achieve lower error rates than Skip-gram on most of the relations. Further-more, RC-NET can reach better performance than R-NET and C-NET, which indicates that both relational and cate-gorical knowledge are important for predicting the topic for the word.

From the table, we also observe that there are some rela-tions, where knowledge powered models do not yield better performance. Our further analysis reveals that these rela-tions can be classified into two types. One type includes relations that have inadequate training pairs such that the relation embedding cannot be trained sufficiently. For ex-ample, it is quite difficult to train high quality embedding for the relation  X  X stronomy X  and  X  X eology X  since they merely have 25 and 13 pairs for training, respectively. The other type contains relations which have so many rare words that they yield less chance to be trained either. For example, as there are a lot of uncommon words in the relation  X  X hem-istry X , it is not easy to collect enough training samples for this relation.
Learning high-quality word embedding is quite valuable for many text mining and NLP tasks. To address the limi-tation of the state-of-the-art methods in terms of their inca-pability of encoding the properties of words and the complex relationships among words very well, this paper proposes to incorporate knowledge graphs into the learning process since it contains invaluable relational knowledge that encodes the relationship between entities as well as categorical knowl-edge that encodes the attributes or properties of entities. In this paper, we introduce a new knowledge powered method, called RC-NET, to leverage both the relational and categor-ical knowledge to obtain word representations. Experiments on three popular text mining and NLP tasks have illustrated that the knowledge powered method can significantly im-prove the quality of word representations.

For the future work, we will explore how to incorporate more types of knowledge, such as the morphological knowl-edge of words, into the learning process to obtain more pow-erful word representations. Meanwhile, we will study how to define more general regularization functions to represent the effect of various types of knowledge. [1] Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin. A [2] J. Bian, B. Gao, and T.-Y. Liu. Knowledge-powered [3] K. Bollacker, C. Evans, P. Paritosh, T. Sturge, and [4] A. Bordes, X. Glorot, J. Weston, and Y. Bengio. A [5] A. Bordes, N. Usunier, A. Garcia-Duran, J. Weston, [6] A. Bordes, J. Weston, R. Collobert, Y. Bengio, et al. [7] R. Collobert and J. Weston. A unified architecture for [8] R. Collobert, J. Weston, L. Bottou, M. Karlen, [9] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, [10] Freebase. http://www.freebase.com. [11] B. Gao, J. Bian, and T.-Y. Liu. Wordrep: A [12] X. Glorot, A. Bordes, and Y. Bengio. Domain [13] M. U. Gutmann and A. Hyv  X  arinen. Noise-contrastive [14] G. E. Hinton. Distributed representations. 1984. [15] R. Jenatton, N. Le Roux, A. Bordes, G. Obozinski, [16] M.-T. Luong, R. Socher, and C. D. Manning. Better [17] T. Mikolov, K. Chen, G. Corrado, and J. Dean. [18] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and [19] A. Mnih and K. Kavukcuoglu. Learning word [20] A. Mnih and Y. W. Teh. A fast and simple algorithm [21] R. Socher, D. Chen, C. D. Manning, and A. Ng. [22] G. Tur, L. Deng, D. Hakkani-Tur, and X. He. Towards [23] J. Weston, A. Bordes, O. Yakhnenko, and N. Usunier. [24] WordNet.  X  X bout wordnet X , princeton university. [25] M. Yu and M. Dredze. Improving lexical embeddings
