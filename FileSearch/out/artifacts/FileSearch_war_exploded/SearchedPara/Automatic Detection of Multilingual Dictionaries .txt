 Translation dictionaries and other multilingual lexical resources are valuable in a myriad of contexts, from language preservation (Thieberger and Berez, 2012) to language learning (Laufer and Hadar, 1997), cross-language information retrieval (Nie, 2010) and machine translation (Munteanu and Marcu, 2005; Soderland et al., 2009). While there are syndicated efforts to produce multilingual dictionaries for differ-ent pairings of the world X  X  languages such as freedict.org , more commonly, multilingual dictionaries are developed in isolation for a spe-cific set of languages, with ad hoc formatting, great variability in lexical coverage, and no cen-tral indexing of the content or existence of that dictionary (Baldwin et al., 2010). Projects such as panlex.org aspire to aggregate these dic-tionaries into a single lexical database, but are hampered by the need to identify individual multi-lingual dictionaries, especially for language pairs where there is a sparsity of data from existing dic-tionaries (Baldwin et al., 2010; Kamholz and Pool, to appear). This paper is an attempt to automate the detection of multilingual dictionaries on the web, through query construction for an arbitrary language pair. Note that for the method to work, we require that the dictionary occurs in  X  X ist form X , that is it takes the form of a single document (or at least, a significant number of dictionary entries on a single page), and is not split across multiple small-scale sub-documents. This research seeks to identify documents of a particular type on the web, namely multilingual dictionaries. Related work broadly falls into four categories: (1) mining of parallel corpora; (2) automatic construction of bilingual dictionar-ies/thesauri; (3) automatic detection of multilin-gual documents; and (4) classification of docu-ment genre.

Parallel corpus construction is the task of au-tomatically detecting document sets that contain the same content in different languages, com-monly based on a combination of site-structural and content-based features (Chen and Nie, 2000; Resnik and Smith, 2003). Such methods could potentially identify parallel word lists from which to construct a bilingual dictionary, although more realistically, bilingual dictionaries exist as single documents and are not well suited to this style of analysis.

Methods have also been proposed to automat-ically construct bilingual dictionaries or thesauri, e.g. based on crosslingual glossing in predictable patterns such as a technical term being immedi-ately proceeded by that term in a lingua franca source language such as English (Nagata et al., 2001; Yu and Tsujii, 2009). Alternatively, com-parable or parallel corpora can be used to extract bilingual dictionaries based on crosslingual distri-butional similarity (Melamed, 1996; Fung, 1998). While the precision of these methods is generally relatively high, the recall is often very low, as there is a strong bias towards novel technical terms be-ing glossed but more conventional terms not.
Also relevant to this work is research on lan-guage identification, and specifically the detection of multilingual documents (Prager, 1999; Yam-aguchi and Tanaka-Ishii, 2012; Lui et al., 2014). Here, multi-label document classification meth-ods have been adapted to identify what mix of languages is present in a given document, which could be used as a pre-filter to locate documents containing a given mixture of languages, although there is, of course, no guarantee that a multilingual document is a dictionary.

Finally, document genre classification is rele-vant in that it is theoretically possible to develop a document categorisation method which classi-fies documents as multilingual dictionaries or not, with the obvious downside that it would need to be applied exhaustively to all documents on the web. The general assumption in genre classification is that the type of a document should be judged not by its content but rather by its form. A variety of document genre methods have been proposed, generally based on a mixture of structural and content-based features (Matsuda and Fukushima, 1999; Finn et al., 2002; zu Eissen and Stein, 2005).
While all of these lines of research are relevant to this work, as far as we are aware, there has not been work which has proposed a direct method for identifying pre-existing multilingual dictionar-ies in document collections. Our method is based on a query formulation ap-proach, and querying against a pre-existing index of a document collection (e.g. the web) via an in-formation retrieval system.

The first intuition underlying our approach is that certain words are a priori more  X  X anguage-discriminating X  than others, and should be pre-ferred in query construction (e.g. sushi occurs as a [transliterated] word in a wide variety of lan-guages, whereas anti-discriminatory is found pre-dominantly in English documents). As such, we prefer search terms w i with a higher value for max l P ( l | w i ) , where l is the language of interest.
The second intuition is that the lexical cover-age of dictionaries varies considerably, especially with multilingual lexicons, which are often com-piled by a single developer or small community of developers, with little systematicity in what is including or not included in the dictionary. As such, if we are to follow a query construction ap-proach to lexicon discovery, we need to be able to predict the likelihood of a given word w i be-ing included in an arbitrarily-selected dictionary D l incorporating language l (i.e. P ( w i | D l ) ). Fac-tors which impact on this include the lexical prior of the word in the language (e.g. P ( paper | en ) &gt; P ( papyrus | en ) ), whether they are lemmas or not (noting that multilingual dictionaries tend not to contain inflected word forms), and their word class (e.g. multilingual dictionaries tend to contain more nouns and verbs than function words).

The third intuition is that certain word combi-nations are more selective of multilingual dictio-naries than others, i.e. if certain words are found together (e.g. cruiser , gospel and noodle ), the con-taining document is highly likely to be a dictionary of some description rather than a  X  X onventional X  document.

Below, we describe our methodology for query construction based on these elements in greater de-tail. The only assumption on the method is that we have access to a selection of dictionaries D (mono-or multilingual) and a corpus of conven-tional (non-dictionary) documents C , and knowl-edge of the language(s) contained in each dictio-nary and document.

Given a set of dictionaries D l for a language l and the complement set D struct the lexicon L l for that language as follows: This creates a language-discriminating lexicon for each language, satisfying the first criterion.
Lexical resources differ in size, scope and cov-erage. For instance, a well-developed, mature multilingual dictionary may contain over 100,000 multilingual lexical records, while a specialised 5-way multilingual domain dictionary may contain as few as 100 multilingual lexical records. In line with our second criterion, we want to select words which have a higher likelihood of occurrence in a multilingual dictionary involving that language. To this end, we calculate the weight sdict ( w i,l ) for each word w i,l  X  L l : where | d | is the size of dictionary d in terms of the number of lexemes it contains.

The final step is to weight words by their typ-icality in a given language, as calculated by their likelihood of occurrence in a random document in that language. This is estimated by the proportion of Wikipedia documents in that language which contain the word in question: where df ( w i,l ) is the count of Wikipedia docu-ments of language l which contain w i , and N l is the total number of Wikipedia documents in lan-guage l .

In all experiments in this paper, we assume that we have access to at least one multilingual dictio-nary containing each of our target languages, but in absence of such a dictionary, sdict ( w i,l ) could be set to 1 for all words w i,l in the language.
The result of this term weighing is a ranked list of words for each language. The next step is to identify combinations of words that are likely to be found in multilingual dictionaries and not stan-dard documents for a given language, in accor-dance with our third criterion. 3.1 Apriori-based query generation We perform query construction for each language based on frequent item set mining, using the Apri-ori algorithm (Agrawal et al., 1993). For a given combination of languages (e.g. English and Swa-heli), queries are then formed simply by combin-ing monolingual queries for the component lan-guages.

The basic approach is to use a modified support formulation within the Apriori algorithm to prefer word combinations that do not cooccur in regular documents. Based on the assumption that query-ing a (pre-indexed) document collection is rela-tively simple, we generate a range of queries of de-creasing length and increasing likelihood of term co-occurrence in standard documents, and query until a non-empty set of results is returned.
The modified support formulation is as follows: cscore ( w 1 , ..., w n ) = where co d ( w i , w j ) is a Boolean function which evaluates to true iff w i and w j co-occur in doc-ument d . That is, we reject any combinations of words which are found to co-occur in Wikipedia documents for that language. Note that the actual calculation of this co-occurrence can be performed Figure 1: Examples of learned queries for different languages efficiently, as: (a) for a given iteration of Apri-ori, it only needs to be performed between the new word that we are adding to the query ( X  X tem set X  in the terminology of Apriori) and each of the other words in a non-zero support itemset from the pre-vious iteration of the algorithm (which are guaran-teed to not co-occur with each other); and (b) the determination of whether two terms collocate can be performed efficiently using an inverted index of Wikipedia for that language.

In our experiments, we apply the Apriori al-gorithm exhaustively for a given language with a support threshold of 0.5, and return the resultant item sets in ranked order of combined score for the component words.

A random selection of queries learned for each of the 8 languages targeted in this research is pre-sented in Figure 1. We evaluate our proposed methodology in two ways: 1. against a synthetic dataset, whereby we in-2. against the open web via the Google search Note that the first evaluation with the synthetic dataset is based on monolingual dictionary re-trieval effectiveness because we have very few (and often no) multilingual dictionaries for a given pairing of our target languages. For a given lan-guage, we are thus evaluating the ability of our method to retrieve multilingual dictionaries con-taining that language (and other indeterminate lan-guages).

For both the synthetic dataset and open web ex-periments, we evaluate our method based on mean average precision (MAP), that is the mean of the average precision scores for each query which re-turns a non-empty result set.

To train our method, we use 52 bilingual Free-documents for each of our target languages. As there are no bilingual dictionaries in Freedict for Chinese and Japanese, the training of Score values is based on the Wikipedia documents only. Mor-phological segmentation for these two languages was carried out using MeCab (MeCab, 2011) and the Stanford Word Segmenter (Tseng et al., 2005), respectively. See Table 1 for details of the num-ber of Wikipedia articles and dictionaries for each language.

Below, we detail the construction of the syn-thetic dataset. 4.1 Synthetic dataset The synthetic dataset was constructed using a sub-set of ClueWeb09 (ClueWeb09, 2009) as the back-ground web document collection. The original ClueWeb09 dataset consists of around 1 billion web pages in ten languages that were collected in January and February 2009. The relative propor-tions of documents in the different languages in the original dataset are as detailed in Table 2.
We randomly downsampled ClueWeb09 to 10
Table 2: Language proportions in ClueWeb09. million documents for the 8 languages targeted in this research (the original 10 ClueWeb09 lan-guages minus Korean and Portuguese). We then sourced a random set of 246 multilingual dic-tionaries that were used in the construction of panlex.org , and injected them into the docu-ment collection. Each of these dictionaries con-tains at least one of our 8 target languages, with the second language potentially being outside the 8. A total of 49 languages are contained in the dictionaries.

We indexed the synthetic dataset using Indri (In-dri, 2009). First, we present results over the synthetic dataset in Table 3. As our baseline, we simply query for the language name and the term dictionary in the local language (e.g. English dictionary , for En-glish) in the given language.

For languages that had bilingual dictionaries for training, the best results were obtained for Span-ish, German, Italian and Arabic. Encouragingly, the results for languages with only Wikipedia doc-uments (and no dictionaries) were largely com-parable to those for languages with dictionaries, with Japanese achieving a MAP score compara-ble to the best results for languages with dictio-nary training data. The comparably low result for Table 3: Dictionary retrieval results over the syn-thetic dataset ( X  X icts X  = the number of dictionaries in the document collection for that language. English is potentially affected by its prevalence both in the bilingual dictionaries in training (re-stricting the effective vocabulary size due to our L l filtering), and in the document collection. Re-call also that our MAP scores are an underestimate of the true results, and some of the ClueWeb09 documents returned for our queries are potentially relevant documents (i.e. multilingual dictionaries including the language of interest). For all lan-guages, the baseline results were below 0.1, and substantially lower than the results for our method.
Looking next to the open web, we present in Ta-ble 4 results based on querying the Google search API with the 1000 longest queries for English paired with each of the other 7 target languages. Most queries returned no results; indeed, for the en-ar language pair, only 49/1000 queries returned documents. The results in Table 4 are based on manual evaluation of all documents returned for the first 50 queries, and determination of whether they were multilingual dictionaries containing the indicated languages.

The baseline results are substantially higher than those for the synthetic dataset, almost cer-tainly a direct result of the greater sophistication and optimisation of the Google search engine (in-cluding query log analysis, and link and anchor text analysis). Despite this, the results for our method are lower than those over the synthetic dataset, we suspect largely as a result of the style of queries we issue being so far removed from standard Google query patterns. Having said this, MAP scores of 0.32 X 0.92 suggest that the method is highly usable (i.e. at any given cutoff in the doc-ument ranking, an average of at least one in three documents is a genuine multilingual dictionary), and any non-dictionary documents returned by the method could easily be pruned by a lexicographer. Table 4: Dictionary retrieval results over the open web for dictionaries containing English and each of the indicated languages ( X  X icts X  = the number of unique multilingual dictionaries retrieved for that language).
 Among the 7 language pairs, en-es, en-de, en-fr and en-it achieved the highest MAP scores. In terms of unique lexical resources found with 50 queries, the most successful language pairs were en-fr, en-de and en-it. We have described initial results for a method de-signed to automatically detect multilingual dictio-naries on the web, and attained highly credible re-sults over both a synthetic dataset and an exper-iment over the open web using a web search en-gine.

In future work, we hope to explore the ability of the method to detect domain-specific dictionar-ies (e.g. training over domain-specific dictionar-ies from other language pairs), and low-density languages where there are few dictionaries and Wikipedia articles to train the method on.
 Acknowledgements We wish to thank the anonymous reviewers for their valuable comments, and the Panlex devel-opers for assistance with the dictionaries and ex-perimental design. This research was supported by funding from the Group of Eight and the Aus-tralian Research Council.

