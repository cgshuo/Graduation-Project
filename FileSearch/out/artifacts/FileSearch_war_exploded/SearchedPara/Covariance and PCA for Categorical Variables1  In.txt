 There are large collections of categorical data in many applications, such as in-formation retrieval, web browsing, telecommunications, and market basket anal-ysis. While the dimensionality of such data sets can be large, the variables (or attributes) are seldom completely independent. Rather, it is natural to assume of variables whose occurrences are somehow correlated to each other. to view the data using a method like Principle Components Analysis (PCA) [4]. This approach gives us a clear picture of the data using KL-plot of the PCA. However, the method is not settled for the data including categorical data. Multi-nomial PCA [2] is analogues to PCA for handling discrete or categorical data. However, multinomial PCA is a method based on the parametric model and it is difficult to construct a KL-plot for the estimated result. Multiple Corre-spondence Analysis (MCA) [3] is analogous to PCA and can handle discrete categorical data. MCA is also known as homogeneity analysis, dual scaling, or reciprocal averaging. The basic premise of the technique is that complicated mul-tivariate data can be made more accessible by displaying their main regularities and patterns as plots ( X  X L-plot X ) . MCA is not based on a parametric model and can give a  X  X L-plot X  for the estimated result. In order to represent the structure of the data, sometimes we need to ignore meaningless variables. How-ever, MCA does not give covariances or correlation coefficients between a pair categorical variables using MCA.
 ables using the regular simplex expression of categorical data. This can give a criterion for selecting appropriate categorical variables. We also propose a new PCA method for categorical data. Let us consider the contingency table shown in Table 1, which is known as variable for eye color x eye and the categorical variable for hair color x hair : Before defining the covariances among such categorical variables,  X  hair,eye ,let us consider the variance of a categorical variable. Gini successfully defined the variance for categorical data [6]. instance, and N is the number of instances. The distance of a categorical variable between instances is defined as x ia  X  x ib = 0 if their values are identical, and = 1 otherwise. A simple extension of this definition to the covariance  X  ij by [8]. The vector expression for a categorical variable with three categories x i  X  { r regular triangle.
 This is a straightforward extension of a regular triangle when the dimension of space is greater than two. For example, a regular simplex in the 3-dimensional space is a regular tetrahedron. Using a regular simplex, we can extend and generalize the definition of covariance to
Definition 1. The covariance between a categorical variable x i  X  { r i { r j unitary matrix expressing the rotation between the regular simplexes for x i and x j .
 multipliers, this procedure can be converted into a simpler problem of simulta-neous equations, which can be solved using the Newton method. The following theorem enables this problem transformation.

Theorem 1. The covariance between categorical variable x i with k i cate-gories and categorical variable x j with k j categories is expressed by where A ij is ( k i  X  1)  X  ( k j  X  1) matrix : L ij is given by the solution of the following simultaneous equations. Proof. Here, we consider the case where k i = k j for the sake of simplicity. Definition 1 gives a conditional maximization problem : the Lagrangian function: where  X  is k i  X  k i matrix. A stationary point of the Lagrangian function V is a solution of the simultaneous equations (6).
 covariance by solving the equations (6), which can be solved easily using the Newton method.
 for Table 1 is 0.2277. 3.1 Principal Component Analysis of Categorical Data Using can be represented by the concatenation of the vertex coordinate vectors of all the categorical variables: Let us call this concatenated vector the List of Regular Simplex Vertices (LRSV). The covariance matrix of LRSV can be written as covariance matrix of LRSV. Since its eigenvalue decomposition can be regarded as a kind of Principal Component Analysis (PCA) on LRSV, we call it the Principal Component Analysis using the Regular Simplex for categorical data (RS-PCA).
 the eigenvector as a linear combination of the following vectors. The first basis set, d , shows vectors from one vertex to another vertex in the regular simplex. The other basis set, c , show vectors from the center of the regular simplex to one of the veritices.
 Eigenvectors defined in this way change their basis set depending on its direction to the regular simplex, but this has the advantage of allowing us to grasp its meaning easily. For example, the first two principal component vectors from the data in Table 1 are expressed using the following linear combination. v v 2 =0 . 64 between x eye = light and x eye = medium values, and the difference between x hair = medium and x hair = fair values. The KL-plot using these components is shown in Figure 1 for Fisher X  X  data. In this figure, the lower side is mainly occupied by data with values: x eye = medium or x hair = medium . The upper side is mainly occupied by data with values x eye = light or x hair = fair . Therefore, we can confirm that ( d eye ( medium  X  light )+ d hair ( medium  X  fair )) is the first principal component. In this way, we can easily interpret the data distribution on the KL-plot when we use the RS-PCA method.
 ology to that of RS-PCA. It uses the representation of categorical values as an indicator matrix (also known as a dummy matrix). MCA gives a similar KL-plot. However, the explanation of its principal components is difficult, because their basis vectors contain one redundant dimension compared to the regular simplex expression. Therefore, a conclusion from MCA can only be drawn after making a great effort to inspect the KL-plot of the data. We studied the covariances between a pair of categorical variables based on Gini X  X  definition of the variance for categorical data. The introduction of the regular simplex expression for categorical values enabled a reasonable definition of covariances, and an algorithm for computing the covariance was proposed. The regular simplex expression was also shown to be useful in the PCA analysis. We showed these merits through numerical experiments using Fisher X  X  data. but it is much easier to interpret the KL-plot in RS-PCA than in MCA. Acknowledgments. This research was partially supported by the Ministry of Education, Culture, Sport, Science and Technology, of Japan, with a Grant-in-Aid for Scientific Research on Priority Areas, 13131210 and a Grant-in-Aid for Scientific Research (A) 14208032.

