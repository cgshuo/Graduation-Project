 Figure 1 is a fragment of one research paper, we call the fragment citation context. How much time do people have to spent in finding which two papers should be located in the placeholders of  X  X 10, 6] X . Looking for what papers to cite is often really time consuming. Paradoxically, the more paper an author knows, the harder it might be for them to figure out where the idea come from. There is a massive existing literature thesaurus in the world, and it is still expanding at an extremely impressive speed annually. Take the research field of Computer Science as an example, about 16000 research papers have been published within a single year, and will keep growing in the foreseeable years. For example, the number of publications in 2009 almost triples than that of 10 year before[16]. The information overload makes the citation recommendation problem more challenge and even more necessary: hence to find what you might be willing to cite is not just a piece of cake.

When people are writing their papers, it would be really energy saving that if there exist a system to recommend a candidate paper list for every citation placeholder in need of citation, so tha t the only concern for the author is to write and write while the system takes over citation stuffs. There are some re-searchers who are already aware of the necessity of citation recommendation and they have figured out some algorithms for computers to manage this. For example, in some previous works, cita tion recommendation is considered as an information retrieval problem, using t he citation context as the query to issue, their systems search for papers to cite b ased on the  X  X ueries X . The whole process is analogous to the standard procedure of search engines. Here, many methods could be employed to model the similari ty between citation context and can-didate papers, e.g., language model [8] and translation model [7]. Anchor text is actually another way of citation, like the method used in search engine, we can also compare the current citation cont ext with other citation contexts whose reference is already known, then determine which paper should the citation con-text refers to. In a word, all these methods focus on the content based analysis. An obvious weakness for these methods is that they fail to take account of per-sonalized user preference. In the real world, different users might have different reading scopes, different citing habits, and different tendencies to cite papers. To this end, citation recommendation should be personalized according to diverse users X  preference. In this paper, we ai m to recommend papers not only based on the content, but also the users X  preference. The combined recommendation for literature citation shall be named as  X  X ersonalized citation recommendation X . Personalized citation r ecommendation is a new work. So there exist many chal-lenges in the work: 1) How to get user information and build user profiles; 2) How to model user personalization; 3) How to combine user profile with content based algorithms.

In this paper, to the best of our knowledge, we are the first to consider ci-tation recommendation in a personalized way. Our work can be incorporated into pervious research outputs and can further improve their performance. In our experiment, we incorporate the personalized component to language model and translation model for citation recommendation, while both of the models achieve a significant performance improvement. 2.1 Literature Study Citation recommendation is a paper recommendation task. There exist some achievements in paper recommendation re searches. Blei et al. [1] analyze in-formation on the web, get the reviewers X  information, and recommend suitable papers to the reviewers. Chandrasekaran et al. [2] use the users X  information in CiteSeer to recommend papers for them. They use the Hierarchical Tree struc-ture to describe the users and papers X  information, and edit distance is employed to measure the similarity between the user and the paper. B Shaparenko et al. [11] used language model and convex optimization to do the recommendation work, they use the cos similarity to recommend the top k related papers to the user. S. McNee et al. [9] use the already exist citation relationship between researchers, papers and other information to recommend paper to readers. Kazu-nari Sugiyamad et al. [13] recommend papers which may attract users based on users X  recent research interests. They u se users X  paper information and users X  citation information to form the personal profile. The similarity between user profile and paper information is used to get the rank list of recommend papers. D. Zhou et al. [17] combine multiple graph to one graph and use the graph in-formation to recommend papers to users. Tang et al. [15] recommend papers according to users X  interests and knowl edge level in an e-learning system. Af-ter user finish one paper, they update the user X  X  knowledge level according to the paper they read. Collaborative filtering[3,6,10] is also widely used in paper recommendation task. It works by recommending papers to the users based on papers which other similar users have preferred previously. 2.2 Citation Recommendation Citation recommendation is a relatively n ew direction. In this direction not so many works have appeared. But recent yea rs, some researchers realize the im-portance and meaningful of this work. An d get some achievements in this filed. Trevor Strohman et al. [12] firstly consider the whole manuscript as the input of a retrieval system, and recommend a citation list for the whole manuscript. J. Tang et al. [14] apply Topic Model to the citation recommendation problem, they measure the similarity between the citation context and other papers according to the topic difference. Yang Lu et al. [7] use translation model to calculate the possibility of one citation context  X  X ranslate X  to one paper. Then recommend papers according to the possibilities. Qi He et al. [5] use the citation context in other papers as the description of the target paper, and recommend papers mainly on the basis of the description. Then, Qi He et al. expand their work in [5] and get the achievement in [4], they don X  X  use the position information of citation context and predict the posi tions where a paper need citations. In this paper, we need the following information as the system input:
Input: 1) The paper meta data information set P , which could contain most content information of the paper, including author, conference and citation in-formation;2) a piece of citation context and the authors of the citation contexts.
Output: According to the input, the output of the algorithm is a ranking list of papers, ranked by the probability of being cited.
 For convenience, we hence set some variables:
Citation Context (CC) , indicates the context around the citation placeholder, and the paper fragment in Figure 1 is a citation context.

Content Relevance Degree (CRD) , indicate the relevant measurement between a CC and a paper, which is based on the content of the citation context and paper content. This measurement can be a value getting from all previous works X  model, such as language model or translation model.

User Tendency Degree (UTD) . This value measures the tendency for a user to cite a paper.

Cite Possibility Degree (CPD) . This measurement indicates the probability for a particular citation context to be a ssociated with one particular paper.
In all, the citation recommendation is to provide such a system which outputs a paper list according to the calculated CP D given the specific citation context. We call our model PCR (Personalized Ci tation Recommendation) model. This chapter will give a detailed introduction to PCR model.

Note that the whole process for a user u to cite a paper t . u got an opportunity to know paper t , got interests in it and read it; someday when u wrote a paper p and recalled t is relevant, the author u might write a description d about t and cited t at last. Before writing p , t already got a higher opportunity than other papers which u has not read.

The measurement between d and t is the CRD, and the measurement between u and t is the UTD. In previous works, CRD is used as the CPD, while UTD is ignored, which is quite insufficient. Actually, the author u will first get different UTD for different papers, then the individual citing behavior occurs. In other word, UTD is a prior of the CPD, it means the following formulas: 4.1 UTD (User Tendency Degree) As mentioned previously, for every  X  X aper-user X  pair, we need to evaluate the UTD. Firstly, a user profile is needed at this step. It seems that there is no place where we can get existing information about the users. For junior researchers who have not published any paper yet, that might be correct. But for senior researchers, they have some publications, and their publications are actually the key factor to establish their preference pr ofiles. For senior researchers we can get all we need from their publication history: 1). The paper set published by current user. 2). The author set who have collaborated with current user. 3). The author set who have been cited by current user.

Then, for junior researchers, we can recommend papers based on their mentor or other direction similar senior researche rs X  preferences. After getting one user X  X  profile, given a paper, how can we measure the UTD between the current user and the target paper? In our work we consider the key points to the UTD is some  X  X ecommendations X  and  X  X xpansions X .  X  X ecommendations X  from three levels of persons: user-self, user X  X  coauthors and user X  X  citation authors. They get different influence on user.  X  X ecommendations X  get two types, one is written by the person, and the other is cited by the person, here we consider the two types as the same thing. After  X  X ecomme nded X  by 3 types of persons. User can  X  X xpand X  the  X  X ecommendation X  in three ways: pay attention to the paper-self, pay attention to the paper X  X  authors and pay attention to the paper X  X  conference. According to these  X  X ecommendations X  and  X  X xpansions X , we can get 3 by 3 equals 9 probabilities which can be considered as the UTD prior (Table 1).
The following is a detailed description to every UTD, and list the formula of UTDs. In the formulas below, count(x, y) means the times which x cite y. count(x) means all the times x cite papers. count X (x) means all the times x cite authors. For example x only cite one paper and the paper has 3 authors, then count(x) is 1 and count X (x) is 3.

Variable u represent the current user, t represent the target paper. A repre-sent target paper X  X  author set, c represent the current paper X  X  conference. A co represent the author set who have collaborations with current user. A ci represent the author set who have been cited by current user.

The first part is the  X  X ecommend X  behavior of user-self. Obviously, compared with other papers, users are more familiar with papers which written or cited by themselves, this will lead to a relatively high UTD to these papers. High UTD not only influence the behavior to paper-self, it will also influence behaviors to paper X  X  authors and conference. So we can get UTD 1 1 to UTD 1 3 . 1). UTD 1 1 : The ratio of user-self X  X  recommendation count to the target paper t, to user-self X  X  recommendation count to all papers. The feature takes two users X  behaviors into consideration: citing papers written by user-self, citing papers used to be cited by user-self. Firstly, every researcher have a relatively stable research interest; that means researchers X  current research topic has a strong connection with past topics. So, paper s written or cited by user-self have a relatively higher possibility of related with target paper t. At the other hand, users are usually more familiar with works published or cited by themselves. The two points may bring a higher tendency to target paper. So, given a target paper t, user-self X  X  recommendation behavior is the first feature which should be considered. The formula of the feature like below: 2). UTD 1 2 : The ratio of user-self X  X  recommendation count to the target paper t X  X  author set A, to user-s elf X  X  recommendation count t o all authors. The feature takes one user X  X  behavior into consideration: citing authors used to be cited by user-self. If the user cited one author multiple times, that means the author is highly understood, recognized and accepted by the user. After citing part of the author X  X  papers, the user X  X  focus may expand, he may get interests in all papers written by the author. Then, given one paper, except considering the paper-self, we can also consider the number of times that user cited the paper X  X  authors. The measurement can be calculated in the following formula: 3). UTD 1 3 : The ratio of user-self X  X  recommendation count to the target pa-per t X  X  conference, to user-self X  X  reco mmendation count to a ll conferences. The feature takes one user X  X  habit into consideration: read and cite papers from user X  X  familiar conferences. If a user publishes in one conference multiple times or cites many papers from one conference, that means the user is familiar with the con-ference. And the user may read most of the papers published in the conference. So, papers from the conference may g et a higher UTD compared with other papers. The feature expand in conference X  X  way. Its formula like below:
The past three features are all focus on user-self X  X  behaviors, except user-self. User X  X  coauthors also get the ability to  X  X ecommend X  papers to users. So coauthors X  papers or citations also give contributions in our model. The next 3 features all about user coauthors X   X  X ecommend X  behavior. 4). UTD 2 1 : The ratio of user X  X  coauthor set A co  X  X  recommendation count to the target paper t, to A co  X  X  recommendation count to all papers. This feature focus on the user X  X  behavior: know well about coauthor X  X  work, read coauthor X  X  papers. Users usually connect with their coauthors closely, this will lead to a high possibility for users to familiar with coauthors X  works and read their papers. So papers  X  X ecommended X  by coauthors give an influence on users, UTD 2 1 is to measure the influence. The formula can be describe below.
 5). UTD 2 2 : The ratio of user X  X  coauthor set A co  X  X  recommendation count to the target paper t X  X  author set A, to A co  X  X  recommendation count to all authors. This feature focus on the authors who write the user X  X  coauthors X  citations. The more times user X  X  coauthors cite one author, the more possibility user can familiar with the author, and the familiarity will lead to future X  X  citing behavior, the following is the formula which can calculate the feature X  X  value: 6). UTD 2 3 : The ratio of user X  X  coauthor set A co  X  X  recommendation count to the target paper t X  X  conference c, to A co  X  X  recommendation co unt to all confer-ences. Like UTD 1 3 , when considering in coauthor X  X  point of view, we can also expand in the conference way. The feature can be calculate by the formula below:
Except user-self and coauthors, user X  X  citation authors also get the ability to  X  X ecommend X , if a paper, an author or a conference be  X  X ecommended X  by user X  X  citation authors many times, the user will also get a relatively high possibility to cite it. The following 3 UTDs expand in 3 ways(target paper-self, target paper X  X  author, target paper X  X  conf erence) to model user X  X  citation authors X  behavior. These features X  calculations are similar with UTD 2 1 to UTD 2 3 . 7). UTD 3 1 : The ratio of user X  X  citation author set A ci  X  X  recommendation count to the target paper t, to A ci  X  X  recommendation count to all papers. Like UTD 1 1 and UTD 2 1 , this feature consider in paper-self way. Here is the calcu-lation formula: 8). UTD 3 2 : The ratio of user X  X  citation author set A ci  X  X  recommendation count to the target paper t X  X  author set A ,to A ci  X  X  recommendation count to all authors. This feature consider in target paper X  X  authors way. Here is the formula to calculate the value: 9). UTD 3 3 : The ratio of user X  X  citation author set A ci  X  X  recommendation count to the target pa per t X  X  conference, to A ci  X  X  recommendation count to all conferences. Like UTD 2 3 , the conference view is also taken into consideration, the feature X  X  value can be calculated by the following formula:
The 9 features are 9 priors to the CRD, then we can multiply one CRD value calculated by one previous model, then get 9 different CPDs. Combine the 9 CPDs, we can get the final score to rank the candidate papers for one citation context. The following chapter is detailed steps for the combination. 4.2 Combine UTDs with CRD In this paper, we employ two models as the CRD measurement and consider them as our baselines. The first one is widely known language model[8], here we use one gram language model; the second is the new model from paper [7], named translation model with self-boosting on abstract. Then combine our UTD priors to the CRD. There also exist some problems in the process of combination, to solve these, we do the process in the following 2 steps: Fill Up Value Gap. In the formula (1), we need to multiply UTD with CRD. But there exist a problem when do the multiplication. There may exist a value gap between UTD and CRD, the difference between different points for UTD and CRD doesn X  X  in an order of magnitude. So, when multiply UTD with CRD, the effect for one of them may be very small. The situation due to the scale of data and the length of citation context. To deal with the situation, we add a shrink variable to the values we multiply, then the formula (1) becomes: Combine Scores. In our model, we have 9 different priors, after multiply with CRD, 9 different scores will appear. To g et a final score for one author, we need to combine the 9 scores.

The citing problem is actually a classification problem. Given a CC and paper pair, it get two relationships: cite or n ot cite. We can get 9 scores to represent the pair. So the pair actually is a 9-D point. Some of the points are positive (the CC cite the paper), some of them are negative. This is a standard problem which SVM can solve. After doing classification, we can rank the papers according to the positive possibilities given by SVM.

But a new problem appears when we use SVM. One CC just cites one or a few papers, while all other papers are all negative points when combine with the CC. So negative points get the overwhelming majority. After tried, we solve the problem in the following way: 1). Add the positive points to the training set, randomly add equal quantity X  X  negative point. 2). Train one SVM model. 3). Get a result according to the model in 2) 4). Repeat steps above n times, get n results, then calculate the average result asthefinalscore.Herewefixnto5,fori t get a stable and good performance.
Then we can get one score for one auth or when considering a CC and paper pair. Many CC get more than one authors, average score from every authors perform best when compare with highest score or lowest score. So here we use the average score of CC authors as the final score.

Till now, we can get one score for every CC and paper pair. And then rank the papers according to the score for one CC. It X  X  the output the citation rec-ommendation problem want to get. 5.1 Dataset We get data mainly from three original: MAS (Microsoft Academic Search) API, MAS web site 1 , Open access papers in the internet. The data was collected in the following steps: 1. Pick up several seed literature venues such as ACL, CIKM, EMNLP, ICDE, ICDM, KDD, SIGIR, VLDB, WSDM, WWW, etc. 2. Get papers X  meta data out of the selected seed venues ranged from 2000 to 2012 via MAS API, the meta data include: paper ID in MAS, paper title, paper published year, paper conference, paper authors, citing papers X  ID, paper abstract, paper open access URL. At last we get 9492 papers X  meta data. 3. Then we get the papers X  meta data which cited by the 9492 papers. In the end we get 55823 papers X  meta data. 4. According to the open access URL, we download 20171 pdf files and 22.49% of them are papers from the seed venues. 5. We get all citation links in our data set, and then we filter out the CC that we can get from the MAS web site, then 73236 cite relationships which send out by the papers from seed venues was fetched. Here a paper may cite another paper multiple times, because a paper may cite another paper in many places.
We pick up 1000 authors whose information is relatively complete in our data set, for every author, we put the last CC written by the author into the test data set. All the test data CCs X  answer form the candidate data set. Then, we use the rest data as the training data. 5.2 Evaluation Metric For every CC, every model can return a ra nking list for papers. We consider the CC X  X  citing papers as the answer. The following metrics can be used:
Recall: The fraction of citation conte xts which can return the answers at top kresult.
 MAP (Mean Average Precision):
R ( d i ) is a boolean function to indicate whether CC refer d i . 5.3 Results We use random result, one gram language model and translation model with self-boosting on abstract as the compared method. Our model, separately use language model and translation model as the CRD. Table 2 shows the experiment result, Figure 2 shows detailed information about recall.

In Table 2, RDM is the random result, LM means language model, LM PCR means PCR use language model as CRD, TM means translation model, TM PCR means PCR use translation model as CRD. From the results, we can see: 1. TM PCR perform best and LM PCR perform also better than not person-alized language model. 2. No matter Language model or translation model, after employed in PCR model, all get an obvious improvement from 0.14 to 0.2 in MAP.

Because, the PCR model not only can use the content related information, but also can use authors X  tendency information, it can maintain the results which CRD perform good, and promote the results which CRD perform pool, so the PCR model is effective and can improve th e performance of the existing model. 5.4 Parameter Tuning In this chapter we tune the shrink parameter  X  as described in 4.2 In this paper, data set scale is described in 4.1, our ci tation context is fetched from MAS web site, after delete the stop words, the average citation contexts X  length is 13.4 words. Figure 3 shows the performance of  X  value from 0.1 to 0.9, we can see set  X  to 0.8 performs best. 5.5 Feature Analysis In our model, we chose 3 by 3 features, Figure 4 shows the performance after remove one of them. The item x ymeansremove UTD x y . The last item  X  X one X  means remove none of the features which is the final result of our model. From the figure we can see the  X  X ecommendation s X  from user-self (written or cited by user-self) contribute more, and  X  X xpa nsions X  in paper-self way give the most prompt to the model. The conclusion X  X  reason is that, we get a relatively big effect decrease after removing one of them.
 We propose recommend citation papers in a personalized way. Different users have different tendencies to papers, our PCR model quantize the tendencies and combine them with language model and the state-of-art translation model. Then get a performance prompt for both of them. In the future, we will try to use other models such as collaborative filtering or graph model as the UTD (user tendency degree). And other factors for citation recommendation will also take into consideration, for example papers X  authority and popularity. We think the performance of personalized citation r ecommendation can be f urther enhanced. Acknowledgments. This work is supported by FSSP 2012 Grant 2012115, NSFC Grant 61272340 and 61073082.

