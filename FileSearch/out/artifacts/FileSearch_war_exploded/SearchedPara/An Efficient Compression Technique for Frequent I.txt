 One of the widely used data mining techniques is association rule mining [1]. Asso-ciation rule mining algorithms iterate dataset many times to enumerate frequent item-sets that exist in the transactions of a given dataset. However, dataset scan is consid-ered as an I/O exhaustive process [1]. Therefore, the performance degrades if mining algorithm requires multiple dataset scans. 
Since main memory plays a significant role in the association rule mining perform-ance, in recent year several novel techniques have been proposed [4, 6, 9, 10] in order to efficiently use main memory. These techniques generally cut down the dataset size, so that the mining algorithms will be able to finish the mining task on bigger datasets or with a low support. We can categorize these existing techniques into three groups: ( i ) vertical compression [8, 9], and ( ii ) horizontal compression [7] and ( iii ) vertical tid compression [4, 10]. Vertical Compression: It uses vertical bit vector to represent the presence and ab-sence of an item, and adopts a lossless compression. However, several researches show that these compression techniques heavily depend on user specified support or compression techniques may cause expansion, not compression [4]. Horizontal Compression: It uses a tree based data structure [7] to compress horizon-tal dataset. As a result, they are able to condense a dense dataset when the user speci-fied support is high. On the other hand, it also causes an expansion rather than com-pression, if the dataset is sparse or the user support is low [10]. Vertical Tid Compression: It stores the vertical tid dataset in an alternative way, for example, diffset [4] stores tids from transactions of a dataset where a particular item is absent. And subsequently, it gains a good compression ratio if the user specified is high. However, when the support is low, the diffset technique is unable to compress the dataset, because the number of diffset is larger than the actual tids (i.e. number of times each of the items occurs in the transaction). 
Since all of the abovementioned compression techniques heavily depend on user specified support and/or dataset characteristics, these techniques often do not allow us to enhance main memory utilization. However, from the above discussion it is appar-ent that without main memory enhancement techniques, it becomes quite difficult to achieve any performance improvement. 
We are motivated by the abovementioned fact that main memory is an important resource and to improve performance we need to use it in an efficient way without exceeding its capacity. To enhance main me mory capacity, in this paper we propose an algorithm that uses a bit-oriented appro ach to compress vertical tid dataset. The proposed technique keeps track of difference between two tids and converts the dif-ferences into a bit format, and finally, stores these bits into a bit vector in an efficient way, so the resultant bit vector has only a few unused bits. 
The important outcome of this method is that the proposed technique is not bias, which means that it does not depend on a particular dataset characteristics (i.e. dense or sparse) or the user specified support. Rath er, it has an ability to compress the origi-nal dataset regardless of dataset size, type or user specified support. Our performance evaluation also shows that it achieves good compression ratio in all scenarios. There-fore, it is able to keep large datasets and allows the mining algorithms to perform mining tasks on such datasets. 
The rest of paper is organized as follows. We describe the reason why we need ef-ficient main memory in section 2. Next, we present our proposed efficient main mem-ory compression algorithm. Then, we presented the performance evaluation and com-parison followed by the conclusion. Performance improvement of association rule mining can normally be attained in two ways including the use of efficient mining techniques and the reduction in using main memory. However, during the mining task, if the algorithm exceeds the main memory limit, the mining process will take a long time regardless of how efficient the mining technique is. In other words, efficient mining techniques will only be efficient if and only if there is an abundant space in the main memory, so that the mining process will not exceed the main memory limit. 
Before we detail of our proposed memory enhancement technique, let us first ana-lyze some of the existing algorithms and corresponding main memory management techniques. In the following few paragraphs we will discuss a number of well known association rule mining algorithms and the amount of main memory needed by these algorithms to perform the mining task. 
The Apriori algorithm [2] uses a horizont al dataset layout and a breadth-first bot-tom-up search technique to enumerate all frequent itemsets that meet the user speci-fied support. It achieves good performance with a high support threshold when the total number frequent itemsets and the length of frequent itemsets are small. How-ever, it will exceed main memory when the number of frequent items is large. 
Another novel algorithm that uses a horizontal dataset layout but employs a depth-first search technique is FP growth [7]. It aims to compress the horizontal dataset into the compressed dataset and it consequently solves the multiple dataset scan problem. However, the main problem of this approach is the size of compressed dataset. It is often unable to reduce the size if the number of items after the first iteration is large. 
A few algorithms [4, 5, 8, 9] use a vertical dataset layout to generate frequent item-set. For example, Eclat [4] uses a depth-first search technique. However, the main problem of such approach is that when the dataset is dense, it has too many tids , and holding the intermediate results of these tids often exceeds the main memory capacity. To overcome such a memory constraint, Zaki et al [5] present another algorithm known as dEclat [5]. This algorithm stores diffset of two itemsets and generates fre-quent itemsets from diffset, not from the entire tid . However, this algorithm is still not able to reduce the size of the difsets dataset compared to tid dataset, especially when the dataset is sparse and/or the support is low. 
VIPER [9] algorithm uses a vertical bit v ector to represent items occurrence in the then it sets  X  1  X  to the n th position of A . This approach also requires a huge memory, when the number of transactions and the number of attributes in a dataset is large. VIPER algorithm then uses a compression technique known as skinning to reduce the size of the bit vector. However, in some cas es this compression technique is not able to compress the bit vector [5], but rather its size increases. 
From the above discussions, it is clear that main memory is one of the key re-sources to improve performance of association rule mining. However, due to a limited main memory capacity, the mining algorithms experience performance bottleneck particularly when performing the mining task on a large dataset. To enhance main memory capacity, in this paper we propose an algorithm that uses a bit-oriented ap-proach to compress vertical tid dataset. The proposed technique keeps track of the difference between two tids and converts the differences into a bit format, and finally, only a few unused bits. To increase the performance of association rule mining, it is necessary to exploit main memory efficiently without exceeding its capacity. In order to achieve this goal in this paper we propose a main memory optimization technique to be used to perform the first phase of any association rule mining task, that is generating the frequent itemset. 
We use a vertical dataset layout to compress the dataset because it allows us to per-form intersection between one or more items in order to find their co-occurrence in the dataset. We can represent the vertical dataset into two different ways: vertical tions and the amount of memory needed by them in order to find the possibilities where we can optimize the memory usage. 
Since vertical bitmaps vector needs to register both the absence and presence of an times, that is the bitmap vector has more  X  1  X  X  than  X  0  X  X , subsequently the size of verti-beyond the original dataset size. The total number of bits we require to hold the whole dataset in main memory can be calculated using the following formulae: where I BV is total number of bits require to hold all x 1 and x 0 ( i.e. the number of trans-hold N number of items in the main memory.

The vertical tids is an alternative representation of vertical bit vectors. Each item of this representation has a list, consisting of all transaction ids where that item appears. Since each item only represents the trans action id where it appears, we need less memory compared to vertical bit vectors when dataset is sparse and/or when the user specified support is low. However, this representation becomes more expensive com-pare to bit vector in terms of space if user specified support is more than 3%. Because calculate the total number of bits we need to hold vertical tids in main memory in the following formulae: where m is number of times item I occur in the dataset, I TID is the total number of bits required to hold item I and T TID is total number of bit we need to hold all N number of items in the main memory. 3.1 Observation The vertical tid representation always n eeds a minimum 32 bits to hold each occur-rence of an item (i.e. word) appeared in the transaction regardless of the dataset size. However, when dataset is small, one can reduce a significant amount of space, if bits small dataset, the total number of transactions is also small, and hence to accommo-date these tids into main memory, we need less than 32 bits, if we convert integer value of all tids to corresponding bits. Converting any integer to bits or vice versa is quite straightforward; one can do it on the fly without any performance degradation. For simplicity, we name this representation as tid-bits . Nevertheless, this representa-tion requires less than 32 bits to hold a word, and therefore it becomes less expensive in terms of space compared to the bit vectors, if the user specified support is less than (100/ n )%, where n is number of bits when converting the last transaction id to bit. To illustrate the above rationale more clearly, let us consider the following example: 
Suppose a dataset has 1,000 items and 1 million transactions with an average trans-action size of 20. Now, we will calculate number of bits required to hold this dataset in the main memory using the abovementioned three different techniques: vertical bitmaps , vertical tids and tid-bits format. Firstly, we employ formula (2) and calculate the total number of bits required in vertical bitmaps which is equal to 1,000,000,000 bits. Secondly, using formula (4) we found that vertical tids needs 640,000,000 bits. Finally to find the total number of bits required in tid-bits format, we first convert the last tid of that dataset into bits; in this case we convert 1,000,000 to bit. Since the last transaction id is 20 bit long, therefore we can accommodate a word (i.e. any tid of this dataset) within 20 bits. And to hold entire dataset we only need 400,000,000 bits. 
From the above example, it is clear that tid -bits approach requires fewer bits than the other two vertical representations. However, we have not yet used any user speci-fied support, and hence one may think that this calculation may not be appropriate in the presence of user specified support. In this regard, we would like to mention here specified support is. In contrast, with bitmap vector, it requires less space as long as the user specified support is less than 100/ n % (where n is number of bits when con-verting the last transaction id to bit). Because indeed the bitmap vector representations will have more  X 0 X  than  X 1 X  when user specified support is less than 100/ n %. 3.2 Algorithm Before we move to the proposed algorithm in details, it is important to explain why we need a specific number of bits for all tids (i.e. equal to the bit representation of last tid of a dataset) in the tid-bits approach. For example, when a dataset has 100,000 transactions, we need 17 bits for each entry, and it increases up to 20 bits when data-set has 1,000,000 transactions, although we can accommodate the first 100,000 trans-actions within 17 bits. However we can not put different number of bits (i.e. size) for different tids, because when we convert each entry of bit vector to find its correspond-ing tid, we need to read a specific number of bits for every entry; otherwise it is diffi-cult to convert these bits to its corresponding tids. 
For example, an item that occurs in the 1st and 100th transaction of a dataset is converted its tids into a bit format (i.e. 1 and 1100100 ) and put these bits directly to a vector. However, doing such an insertion causes ambiguity when we try to convert there are or how many bits each entry has. Thus, we are unable to obtain the tid values of each entry of that vector. To overcome from such ambiguity, we can only use a specific number of bits for all entries regardless the exact bit size of each individual entry. And if we come across a bit representatio n of a tid that is less than that number then we pad it with  X  0  X . 
Since the length of bits (i.e. each entry) increases as tids increases, we can only achieve scalability when we are able to find an alternative approach that does not increase bits length as tids increases in the dataset. On the other hand, the above goal is only achievable when we are able to compress bit vector. In other words, we need to condense the bits representation of all tids, in such a way that its size remains con-stant as the number of tids increases. 3.2.1 Dif-Tid To achieve the above goal, we propose an enhancement technique that compresses format on the fly as it does with the tid-bits. Rather, during the dataset reading it finds the difference between current (i.e. transaction number of current reading position) and previous (i.e. the transaction number where this item appears last time) tid of every item and converts that value into a bit representation. For simplicity we called it dif-bit . Finally, it places these dif-bit into a bit vector. The analogy behind this can be described in the following property: Property 1 : The difference between two tids is always smaller than the largest tid of those two. tween those two tids is D = T 2 -T 1 . Since D is the subtraction of two tids, it is always smaller than T 2 . understanding, let us consider the following example: 
Suppose, we have a dataset, as shown in figure 1(a), where item  X  A  X  appears in the {1st, 3rd, 5th, 6th and 7th} transactions. To store these tids directly in the main mem-appearance of item  X  A  X  within 7 bits as shown in figure 1(b and c). 
The above example clearly depicts that if we put dif-bit rather than the original tid value in the bit vector, we will be able to reduce bit vector size significantly. In addi-tion, it is worth to mention that the proposed method only needs 32 bits when the that an item occurs in the dataset after such a large interval. Nevertheless, if any item occurs in the dataset after such a long interval, the corresponding item support is also tions, its support must be less than 4.4  X  10-9%. Indeed, it is apparent that association rule mining algorithms rarely or never use such small support threshold for generating frequent itemsets, thus the proposed technique rarely requires 32 bits to store dif-bits no matter how big the dataset is. difficult to convert bit vector to its original tids format because from that vector we do not know the exact number of bits we need to construct the original differences of two tids. To elevate this, we have modified our proposed technique and put n number of has. Therefore, during the conversion we know the exact size of the difference if we read that n number of bits in advance for every entry. 
Since inserting n bits in advance for every entry incurs some overhead, one may raise a question about the efficiency of our proposed technique. In this regard we argue that the size of n is very small. For example if n is equal 5, then we can put dif-bit that has 32 bits. However from previous discussion we say that we rarely need 32 bits because using 32 bits we can represent a difference of two tids that occur after 2.3 billions transactions. Nevertheless, we can further reduce the dif-bit size, if we remove the left most bit. Since the left most bit of dif-bit always has  X  1  X , hence we remove that from the dif-value. For example, if the difference between two tids is 9 (i.e. 1001 ) and n is 3 (i.e. corresponding tid, we place  X  1  X  after the n bits, and in this case after  X  11  X  we obtain the original dif-bit value i.e.  X  1001  X . 
The pseudocode of the proposed algorithm is shown in figure 2. Initially, it reads previous tid where that item appears last ti me) during reading. Then, it converts those During the conversion, it first reads the range bits, and from that range bits it finds the ence value with the previous tid. We have done an extensive performance study on our proposed technique to confirm describes the number of items, the average size of each transaction, and the number of transactions of each dataset has. 
When we put dif-bit into a bit vector we put  X  n  X  number of bits prior to dif-bit in-sertion in order to specify the size of that dif-bit. However, for choosing the value of  X  n  X  we have two alternatives: exact size and range. The former specifies the exact size of a dif-bit, hence the size of  X  n  X  never goes beyond 5, since using 5 bits we can ex-press the exact bit size of any number that has 32 or less bits. The latter approach is specified a dif-bit size, and therefore the size of  X  n  X  becomes smaller. For example, 32 bits can be divided into 4 groups (i.e. 1-8, 9-16, etc.) and to represent each group we need 2 bits only. And during the insertion, a particular range value that is suitable for fewer bits, in the performance evaluation we adopt the range approach. 
In the first experimentation, we evaluate the efficiency of our proposed method by comparing it with the tid-based approaches. It is worth to mention that this tid based approach uses a well known vertical dataset layout and have been used in many dif-ferent association mining algorithms [4, 5, 10]. These algorithms discover frequent itemsets in two phases: at first it stores tids of all items separately into the main mem-ory, then intersects tids of one item with other item X  X  and find the exact support. 
Because the aims of this experiment is to find out the total memory that each of the approaches consumes, therefore we are only keen to know how much memory space we plot a detail comparison between dif-bit (our proposed method) and tid. It shows how much memory each of the approaches takes at different support thresholds. 
From the graph shown in figure 3, it is clear that the dif-bit approach always re-quires less memory than those of the tid approach. The size of dif-bit approach is 5-8 times smaller than the corresponding tid approach, as the dif-bit approach finds the bits into a bit vector. Therefore, it often requires only fewer bits to represent a differ-ence value. In contrast, the tid approach requ ires 4 bytes (32 bits) to hold each of the word (i.e each appearance), hence requires more memory. In addition, the proposed dif-bit approach increases the compression ra tio as the support increases because in a higher support the average difference value reduces and subsequently requires fewer bits to represent a difference. For example, when we consider T40I10D100K dataset at 500 (500/100,000 = 0.005%) support the proposed dif-bit size is 5.85 times smaller approach when support is 2,500 (2,500/100,000 = 0.025%). Main memory is one of the important resources that can be used to improve the per-formance of association rule mining. However, due to small main memory capacity, mining algorithms often experience performance bottleneck when performing a min-ing task on a large dataset. Since association mining algorithms inheritably depend-able on amount of main memory, hence when that amount is not sufficient then these algorithms will be unable to finish the mini ng task. Therefore, we critically need effi-cient memory enhancement techniques to make any mining algorithm complete the mining task on large datasets. To achieve this goal in this paper we present a tech-nique, known as  X  Diff-tid  X , which gain good compression ratio regardless of dataset characteristics or user specified support. The performance evaluation confirms that the proposed algorithm needs only a small amount of memory compared to other compression techniques that had been used by many association rule mining. 
