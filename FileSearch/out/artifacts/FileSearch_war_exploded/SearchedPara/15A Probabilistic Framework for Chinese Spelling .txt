 Chinese is a tonal syllabic and character (symbol) language, in which each charac-ter is pronounced as a tonal syllable. A Chinese  X  X ord X  is usually comprised of two or more characters. The difficulty of Chinese processing is that many Chinese char-acters have similar shapes or similar (or same) pronunciations. Some characters are even similar in both shape and pronunciation [Wu et al. 2010; Liu et al. 2011]. How-ever, the meanings of these characters (or words composed of these characters) may be widely divergent. Due to this reason, all the students in elementary schools in Asia (such as Taiwan) or the foreign Chinese learners need to practice to identify and correct  X  X rroneous characters X  in a sentence, which is called the Incorrect Character Correction (ICC) test. In fact, the ICC test is not a simple task even for some adult native speakers.

Since most Chinese characters have their own similar characters in either shape or pronunciation, an intuitive idea for a Chinese spelling check (CSC) is to construct a confusion set for each character. Currently, many CSC systems use the confusion sets [Zhang et al. 2000; Wu et al. 2010; Liu et al. 2011] to detect and correct erroneous char-acters by recursively substituting the confusion characters and evaluating the quality of the resulting sentences. Consequently, much research has been focused on automat-ically constructing the confusion sets from various knowledge sources [Kuo et al. 2004; Lee et al. 2006; Tsai et al. 2006; Chen et al. 2009; Liu et al. 2011].

By virtue that a language model can be used to quantify the quality of a given word (character) string in a natural language, it is widely used in CSC systems, and most systems use the n -gram language models. The n -gram language models, aiming at capturing the local contextual information or the lexical regularity of a language, in-evitably face two fundamental problems. First, the n -gram language models are brittle across domains, and their performance is sensitive to changes in the genre or topic of the text on which they are trained. Second, they fail to capture the information (either semantic or syntactic information) conveyed beyond the n -1 immediately preceding words (or characters). Even though language modeling plays an important role in most CSC systems, to our best knowledge, there is still a dearth of research concentrating on developing or analyzing a language modeling framework for CSC [Chen et al. 2013; Han and Chang 2013].

Although existing CSC systems can achieve a certain level of performance, we are aware that most of them are pieced together from a variety of components without a thorough framework. In view of the problems mentioned above, in this article, we first propose a novel probabilistic framework, which not only naturally combines several important components, such as the substitution model and the language model, as usual, but also provides a systematic and flexible way, for developing a CSC system. Instead of only leveraging the n -gram language models, we also explore the long-span semantic information by incorporating topic language models. In addition, we further make a step forward to recruit a search engine to provide extra information from the Web resources to make a more robust CSC system.
 The remainder of this article is organized as follows. In Section 2, we review the Chinese spelling check problem and related works. Section 3 details the proposed prob-abilistic framework and our CSC system. The experimental setup and a series of ex-periments are presented in Section 4. Finally, conclusions and future work are given in Section 5. Roughly speaking, languages can be categorized into two major systems: one is the alphabetical languages (e.g., English and Arabic), and the other is the character-based languages (e.g., Chinese and Japanese). The different philosophic principles of the two language systems lead to several intrinsic and extrinsic differences, which cause dis-tinct characteristics when dealing with the spelling check tasks for different languages [Kernighan et al. 1990; Jurafsky and Martin 2009; Yang et al. 2012]. (i) Spelling errors can be classified into two classes: nonword errors and real-word (ii) In the character-based languages, a character usually has its own meaning and (iii) Segmentation (or tokenization) is still an unsolved and challenging problem for Due to the differences between the language systems, the spelling check technique should be considered individually for different languages. In this article, we focus on the Chinese spelling check task. Chinese spelling check (CSC) is an active problem. CSC systems have been proposed each year with enhanced performance. Since Chinese is a character-based language, we only need to take into account real-word errors as discussed previously. These real-word errors can be finely classified into two categories: shape errors and pronunciation errors . They are mainly caused by different input methods in Chinese [Yang et al. 2012]. For example, Pinyin and Cang-jie are two well representative input methods. The former usually yields pronunciation errors (i.e., homonymous characters with sim-ilar or same pronunciations to the target characters) while the latter usually yields shape errors (i.e., homomorphous characters with similar shapes to the target charac-ters). Some examples are given in Table I. To analyze and evaluate them in detail is beyond the scope of this article; therefore, we ignore this part in this article.
To mitigate the challenge of real-word errors, many CSC systems leverage confusion sets [Zhang et al. 2000; Wu et al. 2010; Liu et al. 2011] to detect and correct erroneous characters (or words) by recursively substituting characters and finally selecting the best one from all the resulting sentences. Therefore, many approaches focus on au-tomatically constructing the confusion sets from various knowledge sources, such as the Cang-jie code [Liu et al. 2011], psycholinguistic experimental results [Kuo et al. 2004; Lee et al. 2006; Tsai et al. 2006], templates generated from a large corpus [Chen et al. 2009], scores measured by using Web resources (e.g., Wikipedia or Google) [Yu et al. 2013; Wang et al. 2013a; Chen et al. 2013; Hsieh et al. 2013], and quantified phonetic confusion scores between characters given by automatic speech recognition [Yeh et al. 2013].

In addition, a part-of-speech tagger was used to provide extra syntactic information to detect and correct misspellings [Wang et al. 2013b]. The motivation is that, if a sen-tence contains some (one or more) erroneous characters, its syntactic structure may become irregular. Moreover, some approaches recast the spelling check problem as a machine translation (MT) task [Liu et al. 2013; Chiu et al. 2013]. Each input string is considered a source language sentence (which may or may not contain erroneous char-acters) and the MT system translates the input to a foreign language sentence, which does not contain any erroneous characters. On the other hand, a school of research casts the spelling check task as a two-class classification problem, i.e., with or without erroneous characters. A string with a set of indicative features is input to the system and a decision score (or label) is then returned on the basis of these features. Strings without erroneous characters should have higher scores than strings with erroneous characters [Liu et al. 2013].

Among the different CSC approaches mentioned, language modeling is one of the common and important components. Language modeling can be used to quantify the quality of a given word (character) string, thus previous approaches have adopted it to predict which character might be a correct one to replace the possible misspelling to yield a higher quality score. Even though language modeling plays an important role in most CSC systems, to the best of our knowledge, there is still a dearth of re-search concentrating on developing or analyzing a language modeling framework for CSC [Chen et al. 2013; Han and Chang 2013]. In view of this scarcity, in this article, we follow the line of research and extend our previous work [Chen et al. 2013] by incor-porating the topic language models into the CSC system. The topic language models not only capture long-span structural and semantic information from previous words (characters), but also complement the conventional n -gram language models in many natural language processing (NLP) tasks. Language modeling is an important component in most CSC systems today. In this section, we will review several well-known or state-of-the-art language models, among which the n -gram language models and the topic language models will be incorporated into the proposed CSC system in this article. 2.3.1. N-gram Language Modeling. From the late 20th century, statistical language modeling has been successfully applied to various NLP-related applications, such as speech recognition [Chen and Goodman 1999; Chen and Chen 2011], information re-trieval [Ponte and Croft 1998; Lavrenko and Croft 2001; Lavrenko 2009], document summarization [Lin and Chen 2010], and spelling correction [Chen 2009; Liu et al. 2011; Wu et al. 2010]. The most widely-used and well-practiced language model, by far, is the n -gram language model [Jelinek 1999], because of its simplicity and mod-erately good predictive power. Quantifying the quality of a word language is the most common task. Take the trigram model, for example. Given a word product of a series of conditional probabilities as follows [Jelinek 1999]:
In the trigram model, we make the approximation (or assumption) that the proba-bility of a word depends only on the two immediately preceding words.
The easiest way to estimate the conditional probability in Equation (1) is to use the maximum likelihood (ML) estimation as follows: where c ( w l  X  2 , w l  X  1 , w l ) and c ( w l  X  2 , w l  X  w generality, the trigram model can be extended to higher order models, such as the four-gram model and the five-gram model, but the high-order n -gram models usually suffer from the data sparseness problem, which leads to some zero conditional probabilities. As a result, various smoothing techniques have been proposed to deal with the zero probability problem, for example, Good-Turing [Chen and Goodman 1999], Kneser-Ney [Chen and Goodman 1999], and Pitman-Yor [Huang and Renals 2007]. The general formulation of these approaches is [Chen and Goodman 1999]: where f (  X  ) denotes a discounting probability function and h ( weighting factor that makes the distribution sum to 1. 2.3.2. Topic Modeling. The n -gram language model, aimed at capturing only the local contextual information or the lexical regularity of a language, inevitably faces the problem of missing the information (either semantic or syntactic information) con-veyed by the words before the n -1 immediately preceding words. To mitigate the weak-ness of the n -gram model, various topic language models have been proposed and widely used in various NLP-related tasks. We can roughly organize these topic models into two categories [Chen et al. 2010]: document topic models (DTMs) and word topic models (WTMs). The graphical model representations of various DTMs and WTMs are shown in Figure 1, respectively.

DTMs introduce a set of latent topic variables to describe the  X  X ord-document X  cooccurrence characteristics. The dependence between a word and its preceding words (regarded as a document) is not computed directly based on the frequency counts as in the conventional n -gram model. The probability is based instead on the frequency of the word in the latent topics as well as the likelihood that the preceding words to-gether generate the respective topics. Probabilistic latent semantic analysis (PLSA) [Hofmann 1999] and latent Dirichlet allocation (LDA) [Blei et al. 2003; Griffiths and Steyvers 2004] are two representatives of this category. LDA, having a formula analo-gous to PLSA, is regarded as an extension to PLSA and has enjoyed much success for various NLP-related tasks. The major difference between PLSA and LDA is the infer-ence of model parameters [Chen et al. 2010]. PLSA assumes that the model parameters are fixed and unknown while LDA places additional a priori constraints on the model parameters by thinking of them as random variables that follow some Dirichlet distri-butions. Since LDA has a more complex form for model optimization, which is hard to solve by exact inference, several approximate inference algorithms, such as the varia-tional approximation algorithm, the expectation propagation method [Blei et al. 2003], and the Gibbs sampling algorithm [Griffiths and Steyvers 2004], have been proposed for estimating the parameters of LDA.

Instead of treating the preceding word string as a document topic model, we can also regard each word w l of the language as a word topic model (WTM) [Chen 2009]. Each WTM model M w occurring within the vicinity of each occurrence of w l in a training corpus, which are postulated to be relevant to w l . To this end, a sliding window with a size of S words is placed on each occurrence of w l , and a pseudo-document associated with such vicinity information of w l is aggregated consequently. The WTM model of each word can be estimated by maximizing the total log-likelihood of words occurring in their associated  X  X icinity documents X  using the expectation-maximization (EM) algorithm. Word vicin-ity model (WVM) [Chen et al. 2010] bears a certain similarity to WTM in its motivation of modeling the  X  X ord-word X  cooccurrences, but has a more concise parameterization. WVM explores the word vicinity information by directly modeling the joint probability of any word pair in the language. In a similar vein, WVM is trained by maximizing the probabilities of all word pairs, respectively, cooccurring within a sliding window of S words in the training corpus, using the EM algorithm. 2.3.3. Other Language Models. In addition to topic language models, many other lan-guage modeling techniques have been proposed to glean the semantic information from the historical words explicitly or implicitly to complement the n -gram language models, such as the recurrent neural network language model (RNNLM) [Tom  X  a  X  setal. 2010; Chen et al. 2014b], the i-vector based language model (IVLM) [Glembek et al. 2011; Chen et al. 2014a], and the relevance-based language model (RM) [Lavrenko and Croft 2001; Chen and Chen 2011; Chen and Chen 2013; Liu et al. 2014]. RNNLM tries to project W L  X  1 1 and W L into a continuous space, and estimate the conditional prob-ability in a recursive way by incorporating the full information about W first discovers a set of total variability information from a training Corpus, and then each document (or a set of historical words, W L  X  1 1 ) has a document-specific identify vector, which indicates the combination of the variability of the document. After that, the corresponding language models can be made by combining the identity vector and the set of variability information. RM assumes that each word sequence W ciated with a relevance class R , and all the words in W L It usually employs a local feedback-like procedure to obtain a set of pseudo-relevant documents to approximate R in the practical implementation. Interested readers may refer to [Tom  X  a  X  s et al. 2010; Chen et al. 2014a; Lavrenko and Croft 2001] for compre-hensive reviews and new insights into the major methods that have been developed and applied with good success to a wide range of NLP-related tasks.
 We are aware that most of the existing CSC systems are pieced together from a variety of components without a thorough framework. To crystalize a systematic and flexible method into this field, we hence propose a fully probabilistic framework. The funda-mental scenario is that, given a character string W L 1 , W contain several or none misspellings, an effective CSC system should produce a fully correct string  X  W L 1 as an output. Intuitively, we can formulate the task as where the second term (i.e., P ( W L 1 ) ) can be computed by a language model to quantify the degree of naturalness of the string W L 1 in the language. On the other hand, the first term in Equation (4) (i.e., P ( W L 1 | W L 1 ) ) can be further decomposed by We have made two assumptions to drive the formulations in Equation (5). First, we as-sume that the characters are generated independently, which essentially corresponds to the bag-of-words assumption. Second, we assume that, given a character w the entire character string W L 1 is conditionally independent with w that W L 1 and W L 1 are well-aligned. Therefore, a further reasonable assumption is that the l th character w l in W L 1 only correlates with the l quently, the contribution of the summation over all characters in W and interpreted as P ( w l | w l ) is the substitution probability, which models how likely w character W l appears in the context of string W L 1 . It should be emphasized that the proposed framework is similar to the machine translation task, which has been widely studied over a period of years, but the fundamental principles (such as the inferences, the derivations, and the underlying meaning) are quite dissimilar.

As a side note, without the bag-of-words assumption, the occurrence of a character will depend on its context, and P ( W L 1 | W L 1 ) can be approximated as Consequently, the model will become much more complex, and the parameters are hard to be estimated accurately. Comprehensive examples and reviews can be found in [Nie et al. 2007; Wallach 2006; Haidar and O X  X haughnessy 2013]. This section details the estimations of the parameters used in the proposed probabilis-tic framework and CSC system. 3.2.1. The Substitution Probability. The substitution probability (i.e., P ( w determine the degree of confidence in replacing a character with another one. Due to the fact that each character can be replaced by any other characters, the computational cost for searching the best string based on Equation (6) will increase exponentially with the length of the character string. To consider the practical implementation issue, we first leverage the segmentation results to reduce the incredible cost. The idea is to assume that only the single-character words can be erroneous. Therefore, the CSC sys-tem only needs to concentrate on the single-character words. We have implemented our own forward and backward word segmentation tools based on the maximum matching algorithm, instead of using the well-known CKIP Chinese word segmentation system [Ma and Chen 2003], because the latter has a built-in merging algorithm, which might merge some erroneous characters to a new word. We also leverage the confusion sets, which are constructed from a predefined confusion corpus [Wu et al. 2010; Liu et al. 2011] and augmented by referring to the basic units of Chinese characters. We calcu-late the Levenshtein distance between any pair of Chinese characters based on their Cang-jie codes. If the distance is smaller than a pre-defined threshold, the character pair is added to the confusion sets. Based on the above two simplifications, we only need to calculate the substitution probabilities for those single-word characters with their corresponding confusion characters. Moreover, because we do not have any corpus to learn the reliable substitution probabilities for each pair of confusion characters, for each character, the substitution probabilities are simply set to 1/ N number of confusion characters that the character has. 3.2.2. The Prior Probability. The prior (confidence) probability of character w appears in the context of string W L l . Given a training corpus, several well practiced modeling techniques, such as the conditional random fields [Lafferty et al. 2001], can be leveraged to predict the score. Since we do not have a training corpus, we assume that the prior probability follows a uniform distribution in this article. 3.2.3. The String Probability. Although language modeling has been widely used in CSC systems, most approaches only use the conventional n -gram models. The n -gram mod-els can only preserve the local regularity information, thus this article tries to com-plement the n -gram models by using the topic language models, which can capture the long-span semantic information from a word (character) string. To the best of our CSC system.

When DTMs are applied to CSC, take PLSA for example, we can interpret the pre-ceding words (or characters), W l  X  1 1 = w 1 , w 2 ,  X  X  X  for predicting the occurrence probability of the next candidate unit, w where T k is the k -th latent topic; P ( w l | T k ) and P ( T occurs in T k and the probability of T k conditioned on the preceding string W spectively. The latent topic distribution P ( w l | T k ) can be estimated beforehand by max-imizing the total log-likelihood of the training corpus. However, the preceding string varies with context, so that the corresponding topic mixture weight P ( T be estimated on the fly using inference algorithms like the EM algorithm: E-step: M-step: On the other hand, when WTMs are applied to calculate the string probability, we can linearly combine the associated WTM models of the words (characters) occurring in W 1 to form a composite WTM model for predicting w l : where an exponential decay function can be utilized to compute the nonnegative weighting coefficients  X  l , which should decay exponentially and sum to one [Chen 2009]. In short, for DTMs (i.e., PLSA and LDA), the topic mixture weights of a string have to be estimated online using EM or other more sophisticated algorithms, which would be time-consuming; on the contrary, for WTMs (i.e., WTM and WVM), the topic mixture weights of a string can be simply obtained on the basis of the topic mixture weights of all units (words or characters) involved in the string without using any com-plex inference procedure. Despite of the difference described above, all of these models have demonstrated their capabilities and flexibilities when paired with the n -gram language models in several NLP-related tasks.

In view of these previous works, we combine the n -gram models and the topic models by linear interpolation in our CSC system. The language models can be trained with word index units or character index units, while we use word-based language models in our experiments. The word-based lexicon consists of 97 thousand words. To balance the performance and the computational cost, the trigram language model is used in this article, which is estimated from a background text corpus consisting of over 170 million Chinese characters collected from Central News Agency (CNA) in 2001 and 2002 (the Chinese Gigaword Corpus released by LDC) and Sinica Corpus using the SRI Language Modeling Toolkit [Stolcke 2000, 2005] with the Good-Turing smoothing technique. The topic models are also trained by using the same text corpus with 32 latent topics. In this article, we linearly combine PLSA and the trigram model in our CSC system, where the interpolation weight  X  is set to 0.94. Other attractive topic models, which have been proposed to glean the semantic information from the whole string, will be investigated in our future work. Figure 2 shows the flowchart of our CSC system. From an engineering perspective, the system is mainly composed of three components: the text segmenter, the confusion sets, and the language models. It performs CSC in the following steps. (i) Given a test string, the CSC system will segment the input string by using the (ii) After segmentation, the system will iteratively substitute the single-character Note that, in step (ii), we have made a simplified assumption that only single-character words can be erroneous to balance the performance and the computational complexity in our current CSC system. Due to the flexibility of the proposed probabilistic frame-work, it is easy to relax the assumption, and we leave it for future work.
In addition to the topic models, we have further incorporated Web information into our CSC system by using a search engine. For a test string, our system first divides the string into several segments according to the punctuation marks, and then treats each segment as a query and submits it to a search engine to obtain query suggestions. These query suggestions will also be treated as possible word strings and scored by using the language model (i.e., the n -gram models, the topic models, or both). Conse-quently, the final output is the one whose score is the highest among all of the possible word strings. We use Baidu (http://www.baidu.com/) as the search engine, and the top-1 suggestion for each segment is considered in the study. The dataset employed in this study is collected from 13 X 14 years old students X  es-says in formal written tests released by the SIGHAN Bake-off 2013 [Wu et al. 2013]. The dataset contains two subtasks, error detection and error correction. Given an in-put string, the error detection task aims at detecting if there are misspellings in the string. The second subtask further focuses on the quality of error correction. In ad-dition to indicating the error locations in a given string, a CSC system also has to suggest the correct characters for the misspellings. Table II shows some basic statis-tics of the dataset. Another dataset, which contains 700 strings and is also provided by the SIGHAN Bake-off 2013, is used as the development set to decide the parameters in the experiments. For subtask 1 (i.e., error detection), we adopt both the sentence-wise metrics (including the false-alarm rate (FAR), detection accuracy (DA), and detection F1 (DF1)) and the location-wise metrics (including the error location accuracy (ELA) and error location F1 (ELF1)) to quantify the performance of a CSC system. On the other hand, the loca-tion accuracy (LA), correction accuracy (CA) and correction precision (CP) are used to judge the performance of a CSC system for subtask 2 [Wu et al. 2013]. First, we report on the error detection (subtask 1) performance of the baseline CSC system, which only leverages the smoothed trigram language model along with our proposed probabilistic framework. As shown in Table III, the trigram-based baseline system (denoted as  X  X rigram X ) can achieve a certain level of performance while the high false alarm rate (FAR) is obviously a fly in the ointment for practical usage.
From Table III, we can also see that the hybrid language model approach (denoted as  X  X rigram + PLSA X ), which combines both the trigram language model and the PLSA-based topic language model, slightly outperforms the trigram only approach in all evaluation metrics. The results demonstrate that the semantic information covered by the topic model indeed provides extra clues for detecting erroneous characters in our CSC system.

Table III also shows that incorporating the suggestions from a search engine (de-noted as Trigram + Search Engine) into the CSC system yields considerable improve-ments over Trigram in all evaluation metrics. The Trigram also outperforms the Trigram + PLSA approach. Since the search engine might sug-gest candidates for both single-character words and multicharacter words, the effect is twofold. First, it can be considered as a relaxation of the simplified assumption that only single-character words can be erroneous. Second, the suggestions from the search engine further augment our confusion sets. Comparing the Trigram system with the Trigram and Trigram + PLSA systems, the results demonstrate that our proposed probabilistic framework indeed performs better when the assumption that only the single-character words can be erroneous is relaxed and the confusion sets are augmented. The results also demonstrate that the Web information is an indispensable reference for error detection.

Last, we combine the trigram language model, the PLSA-based topic language model, and the suggestions from the search engine (denoted as Trigram Engine + PLSA) into the CSC system. As can be seen from Table III, the enhanced CSC system can further slightly improve the error detection performance in almost all evaluation metrics. Subtask 2 focuses on the evaluation of error correction. Each input string includes at least one misspelling. The experimental results are listed in the right part of Table III. Two observations can be made from the experimental results. On the one hand, as ex-pected, the PLSA-based topic model can further boost the error correction performance when combined with the conventional trigram model (Trigram For example, the Trigram + PLSA approach successfully corrected the erroneous word  X   X   X   X (NID=00541)  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X   X  X  X  X  X  X  X  X  X  X  X   X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X   X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X   X  X  X  X   X   X   X  X  X  X  X  X  X   X  But the Trigram approach failed. Obviously, the trigram language model could not discriminate between context word-regularity information. In the second example (NID approach falsely revised  X   X   X (NID=0150)  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X   X   X   X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X   X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X   X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X   X  In Chinese, both  X   X  indicates the noun  X  usage in the context. This example shows that the long-span structural and semantic information from the previous words can be used to complement the n -gram language model in the CSC task.

On the other hand, surprisingly, the use of Web information cannot complement the conventional trigram language model to achieve better error correction performance (Trigram + Search Engine vs. Trigram). The reasons could be twofold. First, although the search engine helps improve the error detection ability of the CSC system, the suggestions from the Web could be too noisy to be used. Stronger language models, substitution models (i.e., P ( w 1 | w l ) ), and prior probability models (i.e., P ( w be highly desirable. Second, the confusion sets should be further modified by some unsupervised or supervised methods to separate the wheat from the chaff. To figure out the capability of our CSC system, we evaluate the upper-bound perfor-mances (denoted as UPB) of both subtasks, and the results are shown in Figure 3. In the UPB case, we assume that the system has a perfect set of model parameters, which includes the confusion sets, the substitution models (i.e., P ( w ity models (i.e., P ( w l | W L 1 ) ), and the string probability models (i.e., P ( W seen from Figure 3, the false alarm rate (FAR) of UPB is zero as expected, while the re-maining metrics of UPB are not perfect. This is because only single-character word er-rors in a test string are detected and corrected. However, the UPB results still indicate an acceptable level of performance for real applications in our daily life. There is an obvious gap between the results of our CSC system and the UPB performance, which indicates much room for improvement. We may seek remedies for the performance gap, for instance, constructing more robust confusion sets and reliable substitution models. In this section, we compare the performance of our CSC system with various existing approaches, which have been proposed recently and evaluated on the same dataset. These methods, which cover most of the research lines in CSC, respectively, leverage a maximum entropy-based language model, integrate with an advanced word segmenter and a part of speech (POS) tagger, construct the confusion sets with acoustic informa-tion or Web resources, leverage a binary classifier to discriminate correct word strings from erroneous ones, and adopt a statistical machine translation technique. The corresponding results are shown in Figure 4.

In Han and Chang [2013], maximum entropy-based language modeling was em-ployed to enhance the CSC performance, but the results were only comparable with (or even worse than) that of our baseline system Trigram. We believe that language mod-eling plays an important role in CSC systems, and the performance can be improved by incorporating advanced language models. Since, the maximum entropy language model implemented in Han and Chang [2013] can only capture the local context in-formation as the n -gram models, it may not be able to improve the performance in a large margin.
 In Wang et al. [2013b], a conditional random fields-based word segmenter and a POS tagger were employed to detect and correct possible misspellings. Although this approach can achieve a certain level of performance in subtask 1, the FAR is very high, compared with our system and other systems.

In Yeh et al. [2013], acoustic models were used to calculate the confusion posterior probabilities, and more reliable confusion sets based on pronunciation similarity were constructed. The system outperforms all other systems in the error correction task (subtask 2). The results indicate that well-prepared confusion sets can not only reduce the computational cost, but also enhance the performance of a CSC system.
In Yu et al. [2013], a semantic relatedness score, which has been used in English text processing tasks, was calculated from the Wikipedia resources to measure the degree of semantic closeness between a character and its corresponding word (character) string. The system has the lowest false alarm rate among all systems, while its F1 scores (both DF1 and ELF1) are the worst. Moreover, this kind of resource seems not to be useful for the error correction task (subtask 2). A possible reason might be that the content of Wikipedia is quite different from the corpus; therefore, the semantic score cannot be calculated accurately from Wikipedia.

In Liu et al. [2013], a machine translation system and a language model based method were used to generate possible correct examples for an input string, and a supervised classifier was employed to select the most probable example as the final output. As can be seen from Figure 4, the supervised CSC system can only achieve a certain level of performance, compared with other unsupervised approaches.
Chen et al. [2013] adopted a statistical machine translation technique. Although the system achieves the best results in subtask 1 and obtains a certain level of perfor-mance in subtask 2, the framework leverages an ad-hoc formulation to calculate the translation probability, which might be difficult to be transferred to other CSC corpora as well as to other languages. In this article, we have proposed a probabilistic framework for Chinese spelling check, which can naturally inherit several merits from previous approaches. Moreover, we have also tried to render the semantic clues and Web resources to improve the CSC performance. The experimental results have demonstrated that our proposed frame-work can improve the error detection performance in terms of the false-alarm rate, detection accuracy, detection F-score, and error location F-score. Our future research directions include: (1) investigating more elaborate language models for CSC, (2) seek-ing the use of discriminative training algorithms for training the language models to directly optimize the detection and correction performance, and (3) applying and exploring unsupervised or supervised methods to construct the confusion sets.
