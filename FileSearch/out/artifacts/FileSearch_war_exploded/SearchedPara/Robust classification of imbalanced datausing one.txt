
Universidad de Los Andes, Mons. Alvaro del Portillo, Las Condes, Santiago, Chile Operations Management Master Program, Universidad de Talca, Curic X , Chile 1. Introduction
Credit card customer churn is an important task for most financial institutions, given the increasing number of customers and the compe tition amongst vari ous banks in attracting new ones. Hence, there is a pressing need to develop models which can identify existing customers who will not return.
The process of churn prediction is important for developing policies regarding customer retention by creating customer loyalty via targeting and aggressive marketing campaigns. Churn prediction models may lead to important benefits for a company, such as:  X  An efficient strategy of customer retention, focusing the resources on clients with a higher risk of  X  A higher level of personalization in service supply and marketing strategies, aimed at increased  X  A direct impact on profitability. A 5% increment in customer retention policies may lead to a reduc-
In the context of binary classification, the class imbalance problem arises when the class distribution is too skewed [39]. Technically speaking, any data set that exhibits an unequal distribution between both classes can be considered imbalanced. However, the common understanding in the community is that im-balanced data correspond to data sets exhibiting significant, and in some cases extreme, imbalances [11]. When this situation happens, standard classification methods such as Support Vector Machine (SVM) will generate a trivial model by predicting everything to the majority class, i.e., the negative class. At-tempts have been made to deal with this problem in the context of business analytics, such as churn prediction [13], credit scoring [26], and fraud detection [7]; and also different domains such as text categorization [42], spam filtering [30] and anomaly detection [21].

Class imbalance is not a problem by itself, because standard classification methods work perfectly in (also referred as the level of noise [35]) in the data set, which together with class imbalance produces a negative impact on classification performance, but the interaction between both characteristics is still a matter of research [23].

This paper is organized as follows. In Section 2 we briefly introduce standard two-class SVM and its adaptation to one-class classification. Section 3 provides an overview on the class imbalance problem. Section 4 introduces the proposed SVM-based ensemble method. Results using artificial data and a real credit card churn data set from a Chilean financial institution are given in Section 5. A summary of this paper can be found in Section 6, where we provide its main conclusions and address future developments. 2. Support vector machine
In this section we recall the basic concepts of SVM suitable to this paper. Given training samples { ( x i ,y i ) } m i =1 ,where x i  X  n is drawn from a domain X and each of the label y i is an integer from Y = { X  1 , +1 } , SVM provides the optimal hyperplane f ( x )= w T  X  x + b that separates the training patterns. In order to obtain this optimal hyperplane, SVM maximizes the margin , which is the sum of the distances to the closest positive and negative training patterns, and is equivalent to minimize the norm of coefficients w [37]. The case of major interest in this paper is that in which the target class, labeled +1 , is much smaller than the background class, labeled  X  1 .

For a non-linear classifier, the solution will be given in a form of a Kernel machine, where training data are mapped to the higher dimensional space H by the function  X  : x  X   X  ( x )  X  H . The mapping is performed by a kernel function K ( x , y )=  X  ( x )  X   X  ( y ) which defines an inner product in H [25].
A set of slack variables  X  is introduced for each training vector and C is a penalty parameter on the training error [37]. The optimal hyperplane is thus with maximal distance (in H ) to the closest image  X  ( x i ) from the training data. The dual formulation of SVM for binary classification can be stated as follows: subject to
From a variety of available kernel functions, the linear, polynomial, and the Gaussian kernel are chosen in many applications [2,17,25] as follows: 1. Linear Kernel: K ( x i , x s )= x i  X  x s . 2. Polynomial Kernel: K ( x i , x s )=( x i  X  x s +1) d ,where d  X  N is the degree of the polynomial. In this document we also consider one-class SVM as developed by Tax and Duin, namely Support Vector Data Description (SVDD) [31]. This m ethod attempts to find a sphere with Radius R with min-imum volume, containing most of the data objects. Outliers in the training set result in a very large sphere which will not represent the data very well, therefore slack variables  X  are introduced. The dual formulation of SVDD follows: subject to 3. The class imbalance problem Several developments have been presented mainly in three subareas of the class imbalance problem: Sampling, Cost-Sensitive Learning, and One Class Learning [3,11]. In this section we briefly describe these three areas. We also refer to assessment techniques used to evaluate models based on imbalanced data at the end of this section. 3.1. Sampling
Several methods exist regarding data resampling, which mainly differ in their nature (random or in-formed resampling). The two most common data sampling techniques are random oversampling and random undersampling . Random oversampling duplicates randomly selected examples of the minor-ity class. While this approach does help to balance the class distribution, no new information is added to the data set and this may lead to overfitting [34]. Also, this procedure increases the training size, causing longer model training times. Random undersampling discards instances from the majority class randomly, downsizing this class. This approach, however, may lead to the loss of important informa-tion [34].

Chawla et al. [4] proposed SMOTE, an intelligent oversampling method. This approach generates new examples for the minority class, which are created artificially by interpolating the preexisting minority instances, improving classification performance on unbalanced data sets [13,39]. Another interesting approach for synthetic oversampling (ADASYN) was proposed by He et al. [10]. The intuition behind this approach is the automatic generation of synthetic samples for each positive instance, considering a distribution function for each minority example based on the number of negative instances in the K -nearest neighbors of the positive example.

Intelligent undersampling has also been proposed in combination with evolutionary algorithms. It shows good results for rule extraction from imbalanced data compared to SMOTE-based oversam-pling [15]. 3.2. Cost-sensitive learning
Cost-sensitive techniques provide a viable alternative to sampling methods for imbalanced learning domains [11]. The objective of cost-sensitive learning is to develop a classification function that mini-mizes the overall cost on the training data set. This approach is based on the concept of the cost matrix, which is a numerical representation of the penalty when classifying instances from one class to an-other. For example, we define C  X  as the cost of misclassifying a majority class instance as a minority class instance and let C + represent the cost of the contrary case. Typically, there is no cost for correct C + &gt;C  X  .

There are different ways of implementing cost-sensitive learning. One group of techniques applies misclassification costs to the data set as a form of data space weighting, for example, by introducing cost items into the weight updating strategy of AdaBoost [28].

Other approaches consider cost-sensitive adjustments of different classification methods, which can be applied to the decision threshold or modifying their formulations [11]. For the case of SVM, there are some approaches for adjusting the class boundary. These methods apply boundary alignment techniques to improve SVM classification [11]. For instance, in Wu and Chang [41], three approaches for adjusting boundary skews were presented: the boundary movement approach, the biased penalties approach, and the class-boundary alignment approach. 3.3. One-class learning When negative examples greatly outnumber the positive ones, certain classifiers tend to overfit [3]. This is particulary true when facing high dimensional data sets [24]. In this case, one-class SVM trained only with the target class may lead to a better predictive performance [24]. Here, the method attempts to measure the similarity between a query object and the target class, where classification is accomplished to apply [24]. 3.4. Assessment metrics for classification of imbalanced data Traditionally, the most frequently used metrics for binary classification are accuracy (Eq. (3)) and error rate . Accuracy is the proportion of true results; where TP = true positives, TN = true negatives, FP=false positives and FN = false negatives. The error rate is 1  X  Accuracy . These metrics provide a simple way of describing the classification performance in a given data set. However, they are not appropriate for classification of imbalanced data [11]. For example, if a given data set includes 5% of target class instances and 95% of majority examples, a naive approach of classifying every instance to be a majority class example would provide an accuracy of 95%, which can be considered very accurate. This measure, however, fails to reflect the fact that none of the target instances are identified, which may have higher misclassification cost [11].

Alternatively, several assessment metrics are frequently adopted in the research community for im-balanced learning problems [3]. The first two measures ( sensitivity -Eq. (4)-and specificity -Eq. (5)-) separately estimate the classification performance in different classes [27]:
For imbalanced data, we want to concentrate on the target class. Recall is a function of correctly classified and misclassified positive examples, equivalent to sensitivity (Eq. (4)). Precision (Eq. (6)) is a function of correctly classified positive examples and examples misclassified as positives. The (bal-anced) F-measure (Eq. (7)) combines both metrics as a measure of the effectiveness of classification, providing more insight into the functionality of the classifier, assuming both precision and recall as equally important [11]. We also consider the Matthews Correlation Coefficient (MCC) (Eq. (8)), which measures the correlation of the actual and predicted class [18].

We consider for this work the metric G-mean (Eq. (9)) as the main performance measure. This measure evaluates the degree of inductive bias in terms of a ratio of positive accuracy and negative accuracy [11]. Another comprehensive evaluation of classifier performance can be obtained by the receiver operating characteristic (ROC curve), which shows the results from the most positive classification performance to the most negative [27]. The area under the (ROC) curve (AUC) represents the probability that a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative one, and can be obtained by changing the decision threshold of the classifier [27]. In this work we study the area under the curve defined by one run (Eq. (10)), namely AUC b , which is widely known as balanced accuracy [27]. The AUC b can be rewritten considering the tradeoff between the benefits ( TP rate )andcosts( FP rate ). Building the ROC space by plotting on a two-dimensional chart the true positive rate (Y-axis) against the false positive rate (X-axis), AUC b is computed by obtaining the area of this graphic: 4. The proposed approach for classification of imbalanced data
The machine learning literature provides a plethora of classification methods that adjust well to data under certain conditions, but in this context it is hard to clearly define which one performs better in general. There are some works that establish advantages and weaknesses of the approaches consider-ing meta-learning measures, which are simple distance and distribution-based statistical measures that describe the data sets [1,2]. The main objective of this work is to compare different classification ap-proaches for imbalanced datasets using meta-learning measures, and to construct robust classifiers via ensembles, that take into account the diversity of the individual classifiers. We limit ourselves to two main classification strategies: standard two-class learning via SVM, and one-class classification using SVDD and Parzen density estimation [20], and to two meta-learning characteristic: class imbalance and class overlapping.

Two-class and one-class classification strategies differ in the methodology used to construct the clas-sifier. First a discriminative approach using cost-sensitive SVM for binary classification is considered. Secondly, we employ a methodology that provides a description of a set of objects, detecting those who differ in some sense significantly from the rest of the dataset. For the latter approach we consider SVDD as described in Section 2 and the method known as Parzen density estimation as alternative approach [20]. This method is based on a mixture of Gaussian kernels centered on the individual train-ing instances, usually considering a diagonal covariance matrix  X  i = hI , assuming equally weighted features. The bandwidth h is the only parameter that needs to be calibrated during training, which is normally done via maximum likelihood.

The intuition behin d our approach is that each individual cla ssifier is capable of modeling certain regions of the training set adequately, given the different characteristics of the data in terms of balance and overlapping. Therefore, the combination of those classifiers using ensembles may lead to a more robust final solution.

The idea of ensemble learning is to employ multiple learners and combine their predictions, under the principle that individual models combined appropriately should have a better overall accuracy than any individual predictor [5]. To accomplish this task, a necessary and sufficient condition is that the classifiers are accurate and diverse [9]. Diversity was the main characteristic we wanted to exploit in this work. Formally, we consider L different classifiers h 1 ,...,h L drawn from a space of hypotheses H , each one presenting an hypothesis about the true function f . The task is to identify the best hypothesis in H , but in some cases the true function f cannot be represented by any of the hypotheses in H .By combining the classifiers h 1 ,...,h L , it may be possible to provide a better representation of f [5].
We consider two ensemble strategies: combination of rules and stacking. Next we briefly describe both approaches, including how to transform the SVM output to a probabilistic outcome for stacking. 4.1. Ensembles via combination of rules
The first strategy for ensemble learning considers the use of different arithmetic rules to combine k  X  X  X  1 , 1 } [12]. The simplest rule (majority voting) operates on labels only, where p depending on whether classifier l chooses k , or not, respectively. The ensemble then chooses the class that receives the largest total vote.

When the a posteriori probability for every classifier l is available, it is possible to apply several this work: the arithmetic mean (Eq. (12)), the geometric mean (Eq. (13)), the minimum (Eq. (14)) and the maximum (Eq. (15)):
The values of q k ( x i ) can be rescaled, obtaining a probabilistic outcome for each class:
The final decision corresponds to assigning object i to class k with highest confidence q k ( x i ) . 4.2. Stacking
The second proposed strategy, stacking, reconsiders the concept of supervised learning to combine the individual outcomes. A particular classification method, called meta-classifier , is therefore trained predicted labels of each classifier or the probabilistic outcomes. In this work, we use the latter because it provided better results in the literature [33]. We consider the whole training data set to train each individual classifier instead of using bootstrapped samples of the training data. As an extension of this approach we also combine sampling techniques presented in Section 3.1 with the proposed ensemble strategies in Section 5: Experimental results and discussions.
 In this work we use Na X ve Bayes [5] and SVM as meta-classifiers. The first method uses the Bayes Theorem which basically computes prior probabilities for a given class based on the probability that a given term belongs to the specified class. Bayes theorized that the probability of future events could be calculated by determining their earlier frequency. Bayes theorem states that: hypothesis y i (prior) and training data X (evidence) respectively. The method is based on the simplifying assumption that the attribute values are conditionally independent given the target value. Assuming that each feature x j is conditionally independent of every other feature implies that the model can be expressed as:
Under the independence assumption, the conditional distribution of the class variable y i can be ex-pressed as: Despite the fact that the far-reaching independence assumptions are often inaccurate, the na X   X  c  X  Ue Bayes classifier has several properties that make it surprisingly useful in practice. As a meta-classifier, it shows good performance given its simplicity (no additional parameters must be tuned), reducing the risk of overfitting while stacking individual classifiers [5]. 4.3. A posteriori probability estimation
One of the most relevant requisites for ensembles is that the outputs of each individual classifier should be comparable [6]. However, given the structure of the respective approach, each proposed method results in a different output. Density methods like Parzen window, for instance, are usually capable of two-class SVM becomes the distance between each object and the hyperplane. For SVDD, the outcome also correspond to a distance measure, the distance between each object to the center of the hypersphere, where the membership is then obtained by defining a suitable threshold.

According to the arguments presented above, it is necessary to define a suitable measure for all meth-ods to make them comparable, namely the classification confidence . For the case of two-class SVM, the simple sigmoid or logistic function is then used to map the model X  X  outcome f ( x i ) into the interval [0 , 1] [38]: For SVDD, a similar link function is considered for the same purpose: where  X  is an adjustable parameter that acts as a threshold.
 5. Experimental results and discussions
The experiments that we report here use artificial and real-world datasets. First we compared the dif-ferent classification approaches presented in this work, considering them as individual classifiers and as ensembles. We used several bidimensional artificially generated data sets, testing their performance un-der different conditions of class overlapping. In the second case, the proposed methodology was applied to a real data set for credit card churn prediction.

Each experiment was done with simple validation (hold out), considering stratified sampling, assur-ing that both training (70% of the whole data set) and test data set (the remaining 30%) had the same balance ratio. The model was constructed in the training data set, using crossvalidation for model selec-tion, and the performance of each classification approach was finally compared in the test data set using the measures presented in Section 3.4. This procedure assures an unbiased comparison of approaches by evaluating the models in an unseen test data set. For two-class SVM we used the Spider Toolbox for Mat-lab [40], while for one-class classification the experiments were performed using the Data Description Toolbox for Matlab [32]. 5.1. Experiments using artificially generated data sets The artificial data set contained 1,100 instances and two variables, considering a 19% class balance. This data set is further modified to obtain different levels of overlapping, generating six different data sets. Figure 2 shows the graphical representation of all six artificially generated datasets, considering the marginal distribution of one feature and using the label as split variable. These charts ilustraste the increasing overlapping between both classes along the datasets.

The generalized Fisher ratio is used as a measure for class overlapping [19]. This metric computes the degree of separation between the classes and is defined as: where n k denotes the number of examples of class k ,  X  is a distance metric (usually Euclidean norm), m is the global mean, m k is the mean of class k and x i k represents instance i from class k .Table1 presents the level of overlapping given by this measure for each data set (DS). In this table we observe noise, which translates in a low Fisher ratio.
 Two-class SVMs
The first step for standard two-class SVMs is to set the hyperparameters via 10-fold crossvalidation using training data. The following set of values for the parameters were used [16,17]: where C is the penalty parameter considering balanced ridge for cost-sensitive learning, d is the degree of the polynomial function and  X  is the Gaussian Kernel width. Table 2 presents the test results of the best model found by the model selection procedure considering g-mean, F-measure, AUC b ,MCCand overall accuracy. Linear SVM obtains a poor performance for the data sets with higher overlapping (data sets 4 to 6), where SVM fails in predicting the target class, assigning all test instances to the majority class. Non-linear SVM behaves better in each data set, especially when using gaussian kernel, which provides a better g-mean value for all six data sets. There is a natural degradation of the performance for increasing overlap.
 One-class classification For one-class learning, the following set of parameter values were used, as presented in Tax and Duin [31]: where r is the expected rejection rate of the target data for SVDD and h is the bandwidth parameter for Parzen density estimation. Tables 3 and 4 present the test results for SVDD and Parzen density estimation considering g-mean, F-measure, AUC b , MCC and overall accuracy. As in two-class SVM, we obtained the best results using the gaussian kernel. SVDD performed far better than Parzen estimation, even using a linear kernel. Linear SVDD also performs better than linear two-class SVM, but gaussian SVM yielded better results than gaussian SVDD overall. Parzen density estimation, on the other hand, was strongly affected by data overlapping, performing poorly on data sets 4 to 6. Finally, Fig. 1 summarizes the performance of each approach for all data sets using g-mean.
 Ensemble of classifiers
The first step for ensemble learning was to select the individual classifiers. For each data set, only the methods with a test performance (g-mean) above 50% were selected. This was done in order to assure a minimal predictive performance of each method and to avoid potentially irrelevant approaches that may have introduce noise into the ensemble. To evaluate the diversity of the individual classifiers, we considered three common measures, namely the disagreement measure, the Q statistic, and the Kohavi-Wolpert variance [14]. The results are presented in Table 5.

As can be observed in Table 5, the first three data sets present a very small diversity (Disagreement below 0.15 and Q&gt; 0.8). However, the diversity increases significatively with the level of overlapping in the last three data sets. This fact presumes that the use of ensembles may improve the classification performance when facing both imbalanced and overlapped data.

For ensembles via combination of rules, the a posteriori probabilities are obtained for each individual classifier, as described in Subsection 4.3, and then combined using Eqs (12) to (15). In the case of stacking, we consider two meta learners: the combination of classifiers using Na X ve Bayes, as presented in Section 4.2, and the ensembling via SVM. Table 6 presents the results obtained for each data set over a test data set (g-mean). The best two-class (Gau ssian SVM) and one-class (Gaussian SVDD) are also provided for comparison purposes.

The first three data sets show good performance, and in most cases the ensembles do not represent a real improvement compared to the best individual classifier. However, the ensembles present superior performance on the remaining data sets compared with the best individual classifier. In particular, ensem-bles improve significantly the results for datasets 4 to 6, thanks to the diversity achieved by considering performance is still slightly better or at least as good as the best individual classifier (Gaussian SVM). This fact confirms the initial hypothesis, that both standard classification methods and density methods are capable of correctly modeling certain regions of the feature space for complex datasets in terms of balance and noise, but may commit different errors. This situation can be exploited via ensembles, resulting in an important improvement in terms of predictive performance. 5.2. Real-world churn data
The proposed approach has been applied to a real world dataset for churn prediction for a Chilean financial institution. The source of this dataset is the Business Intelligence Cup, organized by the Uni-versity of Chile in 2004 [13].

The original dataset contains 14,814 observations described by 21 predictive variables and a binary relevant attributes, using Fisher Correlation Score. For the final solution six variables were considered, namely the amount of active credit cards in the last three periods, and the quantity of web transactions in the same periods. Each variable was scaled between [0 1].
 Two-class SVMs
For standard two-class we follow the same methodology mentioned in the previous subsection. Table 7 presents the test results of the best model found by the model selection procedure considering g-mean, F-measure, AUC b , Matthews Correlation Coefficient, and overall accuracy. For linear and polynomial SVM, the results are not satisfactory because all elements are labeled as no-churn. Slightly better results are achieved using a Gaussian kernel, with a g-mean of 9.9%. We also consider a second combination of parameter for the Gaussian kernel, which achieved 26.8% but with poor accuracy (30.7%). One-class classification
For one-class learning, the model selection was also performed considering the parameters presented in the previous subsection. Table 8 presents the test results for SVDD and Parzen density estimation con-sidering g-mean, F-measure, AUC b , Matthews Correlation Coefficient and overall accuracy. The results for SVDD are much better than two-class SVM in terms of g-mean, reaching 77.9% in the best case (Gaussian kernel). There are no si gnificant differences between the kernel functions for SVDD, but the performance of SVDD is again significantly better than using Parzen Estimation.
 Ensemble of classifiers
For the construction of the ensembles, only the methods with a test performance (g-mean) above 50% were considered. This constraint, however, can only be accomplished by the one-class methods. Table 9 presents the evaluation of the diversity measures. As can be seen in Table 9, the diversity is relatively low (Disagreement below 0.15 in average).
 Table 10 presents the results obtained for each data set over a test data set using g-mean, F-measure, AUC b , Matthews Correlation Coefficient, and overall accuracy for ensembles via combination of rules and stacking using both a Bayes meta-learner and an SVM meta-learner. Considering the best perfor-mance for an individual classifier, a 4.2% improvement can be achieved via ensembles, where the best method is Stacking via SVM.
 Combining ensemble learning and data resampling
We propose an alternative framework that combines two-class SVM and data resampling, as pre-sented in Section 3.1, in order to improve classification performance. The final classifier is obtained by combining two-class and one-class methods via ensembles, using different sampling techniques. We consider random undersampling and random oversampling, which generates new datasets with different complexity in terms of balance and overlapping. Table 11 presents the characteristics of the six gener-ated datasets. In the last column of this table we observe the high level of overlapping that this data set presents, which can be reduced slightly via resampling.

Tables 12 and 13 show the test performance (g-mean) for each individual method considered in this work over the six resampled datasets, following the same validation methodology used earlier in this sec-tion. An important improvement can be observed for two class SVM considering a 25% undersampling. An additional 100% oversampling resulted in a small improvement for Gaussian SVM. For one-class learning, resampling represents a slight improvement compared to the results obtained with the whole data set. This improvement is achieved via undersampling, while oversampling represents no improve-ments for these datasets.

Finally, Table 14 presents the comparison between the best individual classifier considering g-mean and the ensembles of classifiers, given by the increment in terms of g-mean for the best combination rule and stacking. Stacking with 50% undersampling had the best predictive performance for this churn data set, representing an improvement of 6.1% compared to the best individual method, namely SVDD with Gaussian kernel. 6. Conclusions
In this paper we present an approach for classifying imbalanced data via ensembles considering meta-learning measures for class balance and overlapping. Empirically, we observed that standard two-class SVM achieves a very good performance for imbalanced datasets with a low level of overlap. For noisy data, however, this method performs poorly. Data resampling, namely undersampling and/or oversam-pling, helps to achieve a better predictive performance in this scenario. One class classification methods like SVDD, on the other hand, are outperformed by two-class SVM for imbalanced and weakly over-lapped datasets, but it leads to better results in the presence of noise.

Since no individual approach performs best in general, we propose a novel framework that leads to robust classification, outperforming individual methods. The methods SVM and SVDD perform classi-fication using different strategies which translates to an important level of diversity of their respective solutions, when facing overlapped, imbalanced data. We take advantage of this diversity via ensembles, achieving a significant improvement in terms of performance for all presented datasets. The main con-clusion of this work is that by itself, imbalanced data is not a problem for standard classifiers, but the presence of noise affects their performance dramatically. This effect can be reduced with the combina-tion of data description methods, which are not as accurate as SVM but are robust in the presence of noise, using simple ensembles strategies.

Considering the experiments with ensembles in more detail, Stacking tends to perform better than the lead to best performance in general. We recommend considering all proposed ensemble strategies and compare their predictive performance in an unseen test data set via holdout. Regarding data sampling, undersampling can be very effective in standard two-class SVM for imbalanced and noisy datasets, while providing only a marginal improvement in one-class classification. Random oversampling, on the other hand, is not as effective as undersampling in this context. More sophisticated approaches for oversam-pling like SMOTE, for example, have proven to be more effective in cases of imbalanced datasets [4, 13].

Future work can be done in several areas. It would be interesting to adapt the proposed approach to a multi-class problem, for example in the domain of credit scoring [36]. It would also be interesting to adapt this approach to high dimensional data sets, in order to establish a robust feature selection framework that accomplishes best predictive performance for imbalanced and noisy data while achieving an important dimensionality reduction at the same time. This problem has been addressed by Van Hulse et al. [34] via filter methods for feature selection, but an interesting challenge is to consider advanced feature selection strategies, such as wrapper and embedded methods [16,17] in order to address the interactions between SVM and the predictors when facing imbalanced data.
 Acknowledgments Support from the Chilean  X  X nstituto Sistemas Complejos de Ingenier X a X  (ICM: P-05-004-F, CONI-CYT: FBO16, www.sistemasdeingenieria.cl) is greatly acknowledged.
 References
