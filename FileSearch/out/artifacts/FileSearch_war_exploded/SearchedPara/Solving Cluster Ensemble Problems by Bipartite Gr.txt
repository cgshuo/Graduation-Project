 Xiaoli Zhang Fern xz@ecn.purdue.edu Carla E. Brodley brodley@ecn.purdue.edu Clustering for unsupervised data exploration and anal-ysis has been investigated for decades in the statistics, data mining, and machine learning communities. A recent advance of clustering techniques is the develop-ment of cluster ensemble or consensus clustering tech-niques (Strehl &amp; Ghosh, 2002; Fern &amp; Brodley, 2003; Monti et al., 2003; Topchy et al., 2003), which seek to improve clustering performance by first generating multiple partitions of a given data set and then com-bining them to form a final (presumably superior) clus-tering solution. Such techniques have been shown to provide a generic tool for improving the performance of basic clustering algorithms.
 A critical problem in designing a cluster ensemble sys-tem is how to combine a given ensemble of cluster-ings in order to produce a final solution, referred to as the cluster ensemble problem here. In this paper we approach this problem by reducing it to a graph partitioning problem. In graph partitioning, the in-put is a graph that consists of vertices and weighted edges. The goal is to partition the graph into K roughly equal-sized parts with the objective of min-imizing the cut (the sum of the weights of those edges connecting different parts). We choose to solve clus-ter ensemble problems using graph partitioning tech-niques for two reasons. First, graph partitioning is a well studied area and algorithms such as spectral clustering have been successful in a variety of appli-cations (Shi &amp; Malik, 2000; Dhillon, 2001). Second, cluster ensembles provide a natural way to define sim-ilarity measures for computing the weight of the edges in a graph, which is an important and sometimes hard to satisfy prerequisite for the success of graph parti-tioning techniques (Bach &amp; Jordan, 2004). Previously, Strehl and Ghosh (2002) proposed two approaches to formulating graph partitioning prob-lems for cluster ensembles. The first formulation is an instance-based approach that models instances as vertices in a graph and computes the weight of each edge as the similarity between the pair of instances it connects based on how frequently they are clustered together. The second formulation is a cluster-based approach that models clusters as vertices and com-putes the weight of each edge as the similarity between the clusters based on the percentage of instances they share. Note that we cannot reconstruct the original cluster ensemble based on a graph formed by either the instance-based or cluster-based approach, indicat-ing that both approaches incur information loss from a given ensemble. This paper proposes a new graph for-mulation that simultaneously models both instances and clusters as vertices in a bipartite graph. Such a graph retains all of the information of an ensemble, allowing both the similarity among instances and the similarity among clusters to be considered collectively to construct the final clusters. Moreover, the resulting graph partitioning problem can be solved efficiently. We experimentally compare the proposed graph for-mulation to the instance-based and cluster-based ap-proaches on five data sets. To understand the impact of different cluster ensemble types, two different ap-proaches of generating cluster ensembles are applied. Our experiments show that among the three formu-lations, the proposed bipartite approach achieves the most robust clustering performance. For each data set, the bipartite formulation is always comparable and in some cases significantly better than the other two ap-proaches. In addition, we point out a natural connec-tion between cluster ensemble problems and keyword-based document clustering. This connection raises in-teresting questions for cluster ensemble research and suggests directions for future research.
 The remainder of this paper is arranged as follows. Section 2 introduces the basics of cluster ensembles fol-lowed by a brief review of the related work. Section 3 introduces the problem of graph partitioning. In Sec-tion 4 we describe the instance-based and cluster-based graph formulations. Our bipartite graph formulation is then presented in Section 5. In Section 6 we de-scribe our experimental design and present the results in Section 7. Finally, Section 8 concludes the paper with an overview of the contributions and a discussion of the connection between cluster ensemble problems and document clustering. This section introduces the basics of cluster ensembles and briefly reviews the existing techniques for solving cluster ensemble problems that do not involve graph partitioning. 2.1. Cluster Ensembles A cluster ensemble system solves a clustering problem in two steps. The first step takes a data set as in-put and outputs an ensemble of clustering solutions. The second step takes the cluster ensemble as input and combines the solutions to produce a single clus-tering as the final output. In this paper we assume hard clusterings are used to form the ensembles. But it should be noted that all of the graph-formulation approaches examined in this paper can be applied to cluster ensembles with soft clusterings, directly or with minor modifications. Below we formally define cluster ensembles and state the cluster ensemble problem. Given a data set X = { X 1 ,X 2 ,  X  X  X  ,X n } ,a cluster en-semble is a set of clustering solutions, represented as C = { C 1 ,C 2 ,  X  X  X  ,C R } , where R is the ensemble size, i.e., the number of clusterings in the ensemble. Each clustering solution C r is simply a partition of the data set X into K r disjoint clusters of instances, represented Generally speaking, the value of K r for different clus-tering runs can be either the same or different. The techniques we study here can be applied in both cases. Given a cluster ensemble C and a number K , the de-sired number of clusters, to solve the cluster ensemble problem is to use the information provided by C and partition X into K disjoint clusters as the final clus-tering solution. Note that in some cases, the original features of X may also be used with C to produce the final clustering solution. This study focuses on the case where X is only used to generate the ensemble. 2.2. Related Work on Combining Clusterings While our paper focuses on combining clusterings by graph partitioning, other alternative approaches ex-ist. A commonly used approach (Fred &amp; Jain, 2002; Fern &amp; Brodley, 2003; Monti et al., 2003) combines the clusterings by first generating a similarity matrix for instances and then applying agglomerative cluster-ing algorithms to produce a final clustering. Recently Topchy et al. (2003; 2004) propose to represent a clus-ter ensemble as a new set of features describing the instances and produce final clusters by applying K -means and EM to the new features. See Dimitriadou et al., 2001 and Dudoit &amp; Fridlyand, 2003 for other representative technqiues for combining clusterings. While performing a thorough comparison of all avail-able techniques is beyond the scope of this paper, in recent studies (Strehl &amp; Ghosh, 2002; Topchy et al., 2004), graph-partitioning based approaches appear to be highly competitive compared to other techniques. This section describes the basics of graph partitioning. A weighted graph is represented by G =( V, W ), where V is a set of vertices and W is a nonnegative and symmetric | V | X | V | similarity matrix characterizing the similarity between each pair of vertices. The input to a graph partitioning problem is a weighted graph G and a number K . To partition a graph into K parts is to find K disjoint clusters of vertices P = { P 1 ,P 2 ,  X  X  X  ,P K } , where  X  k P k = V . Un-less a given graph has K , or more than K , strongly connected components, any K -way partition will cross some of the graph edges. The sum of the weights of these crossed edges is defined as the cut of a partition P : Cut ( P, W )= not belong to the same cluster.
 The general goal of graph partitioning is to find a K -way partition that minimizes the cut, subject to the constraint that each part should contain roughly the same number of vertices. 1 In practice, various graph partitioning algorithms define different optimization criteria based on the above goal. Examples include the normalized cut criterion (Shi &amp; Malik, 2000) and the ratio cut criterion (Hagen &amp; Kahng, 1992). See (Fjallstrom, 1998) for an in-depth discussion. Here we defer the discussion of our choice of graph partitioning algorithm to Section 6.2. Given the basics of cluster ensembles and graph partitioning, we are now ready to explore various techniques for reducing a cluster ensemble problem to a graph partitioning problem. This section introduces two existing techniques pro-posed by Strehl and Ghosh (2002) for formulating graphs from cluster ensembles. We rename these two techniques as instance-based and cluster-based ap-proaches to characterize the differences between them. 4.1. Instance-Based Graph Formulation Instance-Based Graph Formulation (IBGF) constructs a graph to model the pairwise relationships among in-stances of the data set X . Recall that the commonly used agglomerative approach (Fred &amp; Jain, 2002; Fern &amp; Brodley, 2003; Monti et al., 2003) generates a sim-ilarity matrix from the cluster ensemble and then performs agglomerative clustering using the similar-ity matrix. IBGF uses this matrix in conjunction with graph partitioning. Below we formally describe IBGF. Given a cluster ensemble C = { C 1 ,  X  X  X  ,C R } , IBGF constructs a fully connected graph G =( V, W ), where  X  V is a set of n vertices, each representing an in- X  W is a similarity matrix and W ( i, j )= W ( i, j ) measures how frequently the instances i and j are clustered together in the given ensemble. In re-cent work (Fern &amp; Brodley, 2003; Monti et al., 2003), this similarity measure has been shown to give satisfac-tory performance in domains where a good similarity (or distance) metric is otherwise hard to find. Once a graph is constructed, one can solve the graph par-titioning problem using any graph partitioning tech-nique and the resulting partition can be directly out-put as the final clustering solution.
 Note that IBGF constructs a fully connected graph, resulting in a graph partitioning problem of size n 2 , where n is the number of instances. Depending on the algorithm used to partition the graph, the computa-tional complexity of IBGF may vary. But generally it is computationally more expensive than the cluster-based approach and our proposed approach, which is a key disadvantage of IBGF. 4.2. Cluster-Based Graph Formulation Note that clusters formed in different clusterings may contain the same set of instances or largely over-lap with each other. Such clusters are considered to be corresponding (similar) to one another. Cluster-Based Graph Formulation (CBGF) constructs a graph to model the correspondence (similarity) relationship among different clusters in a given ensemble and parti-tions the graph into groups so that the clusters of the same group correspond to one another.
 Given a cluster ensemble C = { C 1 ,  X  X  X  ,C R } , we first rewrite C as C = { C 1 1 ,  X  X  X  ,C 1 K 1 ,  X  X  X  ,C R 1 ,  X  X  X  where C i j represents the j th cluster formed in the i th clustering run in the ensemble C . Denote the total number of clusters in C as t = structs a graph G =( V, W ), where  X  V is a set of t vertices, each representing a cluster  X  W is a matrix such that W ( i, j ) is the similarity Once a partition of the clusters is obtained, we can produce a final clustering of instances as follows. First we consider each group of clusters as a metacluster. For each clustering, an instance is considered to be as-sociated with a metacluster if it contains the cluster to which the instance belongs. Note that an instance may be associated with different metaclusters in dif-ferent runs, we assign an instance to the metacluster with which it is most frequently associated. Ties are broken randomly.
 The basic assumption of CBGF is the existence of a correspondence structure among different clusters formed in the ensemble. This poses a potential prob-lem  X  in cases where no such correspondence struc-ture exists, this approach may fail to provide satisfac-tory performance. The advantage of CBGF is that it is computationally efficient. The size of the resulting graph partitioning problem is t 2 , where t is the total number of clusters in the ensemble. This is signifi-cantly smaller than the n 2 of IBGF, assuming t  X  n . Strehl and Ghosh (2002) also proposed a hypergraph-based approach, which models clusters as hyperedges and instances as vertices in a hypergraph and uses a hypergraph partitioning algorithm to produce a final partition. Conceptually, this approach forms a differ-ent type of graph and has the limitation that it can not model soft clustering. Practically, we observed that it performed worse than IBGF and CBGF on our data sets. Due to the above reasons, we choose not to fur-ther discuss this approach in this paper. This section presents our Hybrid Bipartite Graph For-mulation (HBGF) and explains its conceptual advan-tages over IBGF and CBGF.
 Description of HBGF Given a cluster ensemble C = { C 1 ,  X  X  X  ,C R } , HBGF constructs a graph G = ( V, W ), where  X  V = V C  X  V I , where V C contains t vertices each  X  W is defined as follows. If the vertices i and j are Note that W can be written as: W = where A is a connectivity matrix whose rows corre-spond to the instances and columns correspond to the clusters. A ( i, j ) is an indicator that takes value 1 if instance i belongs to the j -th cluster and 0 otherwise. Figure 1 shows an example of HBGF. Particularly, Fig-ures 1(a) and (b) depict two different clusterings of nine instances and Figure 1(c) shows the graph con-structed by HBGF, in which the diamond vertices rep-resent the clusters and the round vertices represent the instances. An edge between an instance vertex and a cluster vertex indicates that the cluster contains the instance. All the edges in the graph have weight one (edges with zero weights are omitted from the graph). In this graph, cluster vertices are only connected to instance vertices and vice versa, forming a bipartite graph. If a new clustering is added to the ensemble, a new set of cluster vertices will be added to the graph and each of them will be connected to the instances that it contains.
 Shown in Figure 1(c) as a dashed line, a partition of the bipartite graph partitions the cluster vertices and the instance vertices simultaneously. The partition of the instances can then be output as the final clustering. Although HBGF X  X  vertex set is the sum of IBGF X  X  and CBGF X  X  vertex sets, it can be shown (Dhillon, 2001) that due to its special structure, the real size of a bi-partite graph partitioning problem is n  X  t , where n is the number of instances and t is the total number of clusters in the ensemble C . This is significantly smaller compared to the size n 2 of IBGF, assuming that t  X  n .
 Conceptual advantages of HBGF Comparing HBGF with IBGF and CBGF, we argue that it has two important conceptual advantages. First, the reduction of HBGF is lossless  X  the original cluster ensemble can be easily reconstructed from an HBGF graph. In contrast, IBGF and CBGF do not have this property. To understand the second advantage of HBGF, it should be noted that IBGF and CBGF consider the similarity of instances and the similarity of clusters in-dependently and, as shown below, such independent treatment may be problematic.
 Comparing two pairs of instances (A, B) and (C, D), we assume that A and B are never clustered together in the ensemble and the same is true for pair (C, D). However, the instances A and B are each frequently clustered together with the same group of instances in the ensemble, i.e., A and B are frequently assigned to clusters that are similar to each other. In contrast, this is not true for C and D. Intuitively we consider A and B to be more similar to one another than C and D. However, IBGF will fail to differentiate these two cases and assign both similarities to be zero. This is because IBGF ignores the information about the similarity of clusters while computing the similarity of instances. A similar problem exists for CBGF. For example, con-sider two pairs of clusters ( C 1 , C 2 ) and ( C 3 , C 4 sume that C 1  X  C 2 =  X  and C 3  X  C 4 =  X  . And further assume that the instances of C 1 and those of C 2 are of-ten clustered together in other clustering runs, whereas this is not the case for C 3 and C 4 . Note that CBGF will assign both similarities to zero while intuitively we would consider C 1 and C 2 to be more similar to one another than C 3 and C 4 . CBGF fails to differentiate these two cases, because it does not take the similarity of instances into account.
 In constrast, HBGF allows the similarity of instances and the similarity of clusters to be considered simul-taneously in producing the final clustering. Thus it avoids the above problems of IBGF and CBGF. In this section we first describe the methods used in our experiments for generating cluster ensembles and then introduce the graph partitioning algorithms and how they are used in our experiments. 6.1. Generating Cluster Ensembles Cluster ensembles can be generated in different ways. The resulting ensembles may differ and the same ap-proach for solving the ensemble problems may perform differently accordingly. It is thus important for our ex-periments to consider different ways to generate clus-ter ensembles. Our experiments use two approaches, random subsampling (Dudoit &amp; Fridlyand, 2003) and random projection (Fern &amp; Brodley, 2003), to gener-ate the ensembles. Note that for both approaches, K -means is used as the base clustering algorithm and the number K is pre-specified for each data set and remains the same for all clustering runs.
 Note that we also examined a third approach, ran-domly restarting K -means, and it produced similar results to those of random subsampling. So we omit these results in the discussion of our experiments. 6.1.1. Random Subsampling For each clustering run, we randomly subsample the original data set with a sampling rate of 70%. The sub-sampling is performed without replacement to avoid duplicating instances. We then cluster the subsam-ple and assign each instance absent from the current subsample to its closest cluster based on its Euclidean distance to the cluster centers to ensure that all the instances are clustered in each clustering run. 6.1.2. Random Projection For each clustering run, we first randomly generate a projection matrix P d  X  d 0 to project the given data set onto a lower dimensional space, where d is the origi-nal dimension of the data set and d 0 is the dimension that we project the data onto. We then cluster the projected low-dimensional data set.
 Recently, Fern and Brodley (2003) showed that both the diversity and quality of a cluster ensemble signifi-cantly impact what can be achieved by combining the clusterings of the ensemble. We choose the above two approaches because we expect them to produce ensem-bles with different properties. On one hand, we expect the clusterings generated by random projection to be diverse because it provides the base learner with dif-ferent views of the data. On the other hand, we ex-pect the quality of the clusterings produced by random subsampling to be higher because it provides the base learner with more complete information of the data. 6.2. Graph Partitioning Algorithms Our goal is to evaluate different graph formulation ap-proaches. To reduce the influence of any chosen graph partitioning algorithm on our evaluation, we use two well-known graph partitioning algorithms that differ with respect to their search for the best partition. 6.2.1. Spectral Graph Partitioning Spectral graph partitioning is a well studied area with many successful applications. We choose a popular multi-way spectral graph partitioning algorithm pro-posed by Ng et al. (2002), which seeks to optimize the normalized cut criterion (Shi &amp; Malik, 2000). We refer to this algorithm as SPEC.
 SPEC can be simply described as follows. Given a graph G =( V, W ), it first computes the degree ma-trix D , which is a diagonal matrix such that D ( i, i )= P j W ( i, j ). Based on D , it then computes a nor-malized weight matrix L = D  X  1 W and finds L  X  X  K largest eigenvectors u 1 ,u 2 ,  X  X  X  ,u K to form matrix U =[ u 1 ,  X  X  X  ,u K ]. The rows of U are then normalized to have unit length. Treating the rows of U as K -dimensional embeddings of the vertices of the graph, SPEC produces the final clustering solution by clus-tering the embedded points using K -means.
 Intuitively, SPEC first embeds the vertices of a graph onto a K -dimensional space and then performs cluster-ing in the K -dimensional space. For graphs generated by IBGF and CBGF, the clusters and instances are embedded and clustered separately. Interestingly, for HBGF, the clusters and instances are simultaneously embedded onto the same space and clustered together. Here we argue that this offers potential advantages over IBGF and CBGF. Compared to IBGF, the inclu-sion of the cluster vertices may help define the struc-ture of the data and make it easier for K -means to find the structure in the K -dimensional space. In compari-son to CBGF, it is expected to be more robust because even when the cluster vertices are not well structured, possibly due to the lack of a correspondence structure in the clusters, K -means can still perform reasonably well using the instance vertices. 6.2.2. Multilevel Graph Partition: Metis Metis (Karypis &amp; Kumar, 1998), a multilevel graph partitioning system, approaches the graph partitioning problem from a different angle. It partitions a graph using three basic steps: (1) coarsen the graph by col-lapsing vertices and edges; (2) partition the coarsened graph and (3) refine the partitions. In comparison to other graph partitioning algorithms, Metis is highly efficient and achieves competitive performance. Comparing Metis and SPEC in practice, we observe mixed results  X  neither approach is consistently bet-ter than the other, indicating that both approaches have different advantages and weaknesses. Note that the goal of our experiments is to evaluate the different graph representations generated by IBGF, CBGF and HBGF, not the search bias of SPEC and Metis. To this end, we run both SPEC and Metis for each graph and report the maximum NMI (Our evaluation metric, See Section 7.2) obtained. This is done for all three types of graphs to ensure a fair comparison. The goal of the experiments is to evaluate the three graph formulations -IBGF, CBGF and HBGF -given different cluster ensembles.
 7.1. Data Sets and Parameter Settings Five data sets are used in our experiments. The char-acteristics of the data sets are summarized in Table 1 with related parameter choices. HRCT is a high reso-lution computed tomography lung image data set with eight classes (Dy et al., 1999). MODIS and EOS are land cover data sets described by different feature sets. ISOLET6 and GLASS are from the UCI machine learning repository (Blake &amp; Merz, 1998), where ISO-LET6 is a subset of the ISOLET spoken letter recog-nition training set. In particular, ISOLET6 contains the instances of six classes (letters) randomly selected out of twenty six classes (letters).
 To construct cluster ensembles using random projec-tion, we need to specify the number of dimensions that random projection uses for each data set. The num-bers are listed in the fifth row of Table 1. 2 When random subsampling is used, for the EOS and GLASS data sets, we construct the cluster ensembles directly as described in Section 6.1.1. Note that the other three data sets are of high dimensionality. For these data sets, we reduce the dimension by Principal Component Analysis (PCA) prior to generating the ensembles. The dimensions that PCA uses are listed in the sixth row of Table 1. They are selected such that 85% of the data variance is preserved.
 In all clustering runs, which include both the runs dur-ing ensemble construction and the partitioning of the final graphs, the cluster number K is set to be the same as the number of classes in a given data set. Note that the class labels are not used during clustering. 7.2. Evaluation Criterion Because our data sets are labeled, we can assess the quality of the clustering solutions using external cri-teria that measure the discrepancy between the struc-ture defined by a clustering and what is defined by the class labels. Here we choose to use an information the-oretic criterion  X  the Normalized Mutual Information (NMI) criterion (Strehl &amp; Ghosh, 2002). Treating clus-ter labels and class labels as random variables, NMI measures the mutual information (Cover &amp; Thomas, 1991) shared by the two random variables and normal-izes it to a [0 , 1] range. Note that the expected NMI value of a random partition of the data is 0 and the optimal value 1 is attained when the class labels and cluster labels define the same partition of the data. 7.3. Results Table 2 presents the performance of IBGF, CBGF and HBGF on cluster ensembles generated by both random subsampling (columns 2-4) and random pro-jection (columns 5-7). For each ensemble generating method, we report the results with three different en-semble sizes (20, 40 and 60). 3 Each number reported here is obtained by averaging across ten random runs, where for each run we use both SPEC and Metis to partition the generated graph and record the maxi-mum NMI obtained as the result of that run. Finally, the last row of each data set reports the average per-formance of the base learner for each ensemble type. Comparing IBGF, CBGF and HBGF First let us look at the performance of the three approaches on cluster ensembles generated by random subsampling, shown in the first set of three columns of Table 2. We see that the performance of IBGF and CBGF are com-parable for three of the five data sets (EOS, GLASS and MODIS) and mixed for the other two. In par-ticular, IBGF performs better on the HRCT data set while CBGF is the winner for the ISOLET6 data set. When HBGF is used, we observe that in most cases its performance is comparable to the better of the other two approaches and is significantly better for EOS. Similar observations can also be made for the random projection ensembles except that here CBGF appears to be less competitive and performs significantly worse than both IBGF and HBGF in two (EOS and HRCT) of the five data sets. We conjecture that this is because random projection ensembles tend to be more diverse 4 so that a correspondence structure is less likely to exist in the clusters.
 Comparing with the base learner From Table 2, we observe that HBGF is the only graph formula-tion approach that leads to consistent performance im-provement over the base learner in all cases. CBGF ap-pears to be the least stable approach among the three graph formulations. Interestingly, we see that both IBGF and CBGF fail to improve over the base learner for EOS when ran-dom subsampling ensembles are used, whereas HBGF achieves significantly better clustering results using the same cluster ensembles. We consider this as an indi-cation that the constructed cluster ensembles indeed provide helpful information for clustering but was not captured by IBGF or CBGF in their graphs. We ar-gue that HBGF X  X  success may be attributed to the fact that it retains all the information of the ensembles and that it allows the similarity of instances and the simi-larity of clusters to be considered simultaneously. Another interesting fact about the EOS data set is that for both types of cluster ensembles, the performance of HBGF degrades as the ensemble size increases. We plan to further explore this issue by inspecting the behavior of the graph partitioning algorithms. From the above observations, we conclude that HBGF is more robust than both IBGF and CBGF. In our ex-periments, it yields competitive and sometimes better performance compared to both IBGF and CBGF. This paper presented HBGF, a graph formulation that reduces a cluster ensemble problem to a bipartite graph partitioning problem. HBGF simultaneously models the instances and clusters of a given ensemble as vertices of a bipartite graph. It achieves lossless re-duction and allows the similarity among instances and the similarity among clusters to be considered simul-taneously in thefinal clustering. Further, the resulting graph partitioning problem can be solved efficiently. We compare HBGF with two commonly used graph formulations for cluster ensembles. Our experiments on five data sets show that HBGF achieves compa-rable or better performance in comparison with the other two approaches and results in the most robust performance improvement over the base learner. Finally, we want to point out an interesting connection between cluster ensemble problems and keyword-based document clustering. In document clustering, a set of keywords is used to represent a document as a vector. Each element of the vector indicates if the document contains a specific keyword. Considering an instance as a document and the clusters the instance belongs to as the keywords the document contains, clustering instances according to the clusters can be related to clustering documents based on the keywords. Nat-urally, we can relate CBGF to document clustering approaches based on clustering keywords (Slonim &amp; Tishby, 2000) and HBGF to the bipartite co-clustering approach proposed by Dhillon (2001). This connection raises an interesting question. Can we borrow ideas from document clustering, a highly advanced area, to help solve cluster ensemble problems? This sug-gests two directions for future work. First, document clustering with keyword clustering has been shown to yield better performance than without keyword clus-tering (Slonim &amp; Tishby, 2000). However, for cluster ensemble problems, CBGF appears to be inferior to both IBGF and HBGF. Such an inconsistency mer-its further exploration of the cluster-based approach. Secondly, our hybrid approach may also benefit from a recently proposed information theoretic co-clustering technique (Dhillon et al., 2003).
 We thank the reviewers for their helpful comments. The authors were supported by NASA under Award number NCC2-1245.

