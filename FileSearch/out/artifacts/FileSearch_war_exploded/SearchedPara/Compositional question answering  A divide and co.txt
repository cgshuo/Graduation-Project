 1. Introduction
In question answering (QA), the goal is to deliver an answer rather than a set of documents containing query terms for a question in a full natural language sentence form. Much of the initial effort in QA research, ignited by the QA Track in TREC (Text REtrieval Conference; Voorhees, 2000a, 2000b ) in 1999, has focused on the capability of extracting short direct answers for factoid questions. For example, a correct response to  X  X  What is the population of the Bahamas?  X  is a single word or phrase that can be extracted directly from a sentence in a relevant document. As the research has progressed, more realistic and difficult questions were introduced in the subsequent TRECs. TREC-12 in 2003 ( Voorhees, 2004 ) introduced definition ques-tions, such as  X  X  What is global warming?  X , which need to be answered with a longer description or explanation but still can be is there for transport of drugs from Mexico to the US?  X  are more challenging in that they usually require inference with domain-specific knowledge. The latest QA track in TREC, 2006 ( Dang &amp; Lin, 2007a ) and TREC, 2007 ( Dang, Lin, &amp; Kelly, 2008 ) contained a complex interactive QA (ciQA) task, a blend of the relationship task and the HARD track that focused on single-iteration clarification dialogues ( Allan, 2006 ).

This paper focuses on questions that cannot be answered directly from a single answer text or a QA module. For example, the answer for  X  X  What is the length of the longest river in the world?  X  may not exist in a single document and requires a two-step process. Unless the answer together with the conditions to be satisfied in the question is found, the name of the longest river must be searched first and then the length of the river must be found to generate the final answer. A general process is to divide a question into simpler ones that can be answered directly by the existing QA techniques and generate the final answer by composing those for the simpler questions. We call this process of answering question as compositional QA , the goal of which is to answer composite questions using existing QA capabilities instead of developing an entirely new method or a QA module tailored specifically to the class of questions.

We assume that a QA system has multiple QA modules specializing in answering different types of questions, such as fac-toid, superlative, list, and description questions. When a question cannot be first answered by an existing QA module with a sufficient level of confidence, even after the confidence level boosting process using other QA modules ( Oh &amp; Myaeng, 2009 ), it is considered a complex question. In contrast, a question that can be answered with an existing QA module is called an atomic question. A composite question, a kind of complex question, can be divided into atomic ones so that they can be an-swered individually and completely first. The answers are then somehow combined to provide an answer for the original question. Therefore, the distinction between atomic and composite questions is not made intrinsically but made from a prag-matic point of view based on what types of QA modules exit and what knowledge sources are made available in the current system. In other words, a question that appears to have two components after question analysis is considered an atomic question if the answer can be obtained directly from one QA module. A composite question that requires answers from more than one QA modules can be an atomic question when a new QA module can provide an answer directly or when the system is given additional data.

Some complex questions cannot be answered with the compositional QA method if they require entirely new text anal-ysis techniques or some inference beyond the operations in compositional QA. For instance,  X  X  What familial ties exist between to  X  X  X inosaurs X  and the other for  X  X  X irds X , it is not clear what types of answers should be sought for each and how the answers should be linked together to satisfy the  X  X  X ies X . This type of relationship questions is outside the scope of the compositional QA approach to be described in this paper.
 In the following, we (1) discuss the characteristics of questions in general and of the composite questions particularly in
Section 2 , (2) investigate technical challenges for solving composite questions relative to some past studies in Section 3 , (3) illustrate an overview of the proposed compositional QA model and describe the impact of compositional QA for composite questions in Section 4 , and (4) analyze the effect of the proposed QA model with several experiments based on Korean eval-uation set, together with an in-depth analysis of errors in Section 5 . Finally we conclude with a suggestion for possible future works in Section 6 . 2. Characteristics of questions
In order to make full use of various QA techniques corresponding to different types of questions for compositional QA and sought after. Availability of a question taxonomy or categorization scheme would help not only analyzing an incoming user question but also identifying QA capabilities and techniques to be developed in the future.

To this end, we collected more than 7000 questions: 2569 from a few commercial web logs, 3000 from general web users, and 1485 from elementary students 1 as in Table 1 . They were analyzed to characterize the types of questions and answers. We used the first set containing 2569 questions from the commercial Web logs for the analysis of question/answer types and the other sets for tuning and validation of the system at a later stage.

Question categories are tightly related to the types of answers being sought after. A question may look for a short factoid answer for a list question is found with a particular syntactic structure such as a parallel phrase that can serve as a clue in finding an answer. Many questions in a web log require an answer in the form of a description (hence descriptive questions ) much longer than factoid answers. 2.1. Question distribution analysis
The basis for this analysis was the set of 2569 questions collected for three months from the Naver  X  Manual QA Service ( http://kin.naver.com ), the Nate  X  Encyclopedia Service ( http://100.nate.com ), and the Empas  X  Knowledge Brain Service ( http://kdaq.empas.com ). The result of our analysis is shown in Table 2 for the surface-level question type classes deter-mined by the interrogative pronouns. They are further divided into answer types corresponding to the classes used in TREC ( Voorhees, 2004 ), which is shown in the right-most column in Table 2 . While TREC used  X  X  X efinitional X  question type, we generalized it to  X  X  X escriptive X  because it is more format-related than the term  X  X  X efinitional X  that focuses on the function of a description. Our question collection contains other types of descriptive questions like  X  X  X easons X  and  X  X  X ethods X  signified by  X  X  X hy X  and  X  X  X ow X , respectively.

Among the 2569 questions, 901 (35%) and 85 (3%) are descriptive and list answer questions, respectively, and all the oth-and 75 other questions 2 ( Dang &amp; Lin, 2007a ). Contrary to TREC, however, there were many descriptive answer questions start-ing with  X  X  X hy X  or  X  X  X ow X  in addition to those starting with  X  X  X hat X  or  X  X  X ho X  as shown in Table 2 . A more semantically oriented the total) of the questions belongs to this category in our data that reflect real users X  information needs. 2.2. Three facets for question type determination
To determine the types of questions, we first identified and used three facets that help characterizing questions and pre-scribing system functionality accordingly, for which different techniques need to be developed. They are: answer format, an-swer theme, and question qualifier.  X  Answer format (AF) has four possible values: single, multiple, descriptive, and yes/no. They can be distinguished based on the surface-level description of a question. For example,  X  X  Where was Mozart born?  X  looks for a single answer whereas  X  X  What countries expert oil?  X  requires multiple answers. Yes/No questions like  X  X  Is Saddam Hussein alive?  X  require a yes or no answer. A descriptive question needs an answer that contains definitional information about the key term as in  X  X  What is X?  X  X r X  X  Who is X?  X .  X  Answer theme (AT) is the class of the object sought by the question, such as PERSON, LOCATION, and DATE. In this paper, we use a total of 147 Single Answer Themes (SAT), which is organized in a hierarchical structure ( Lee et al., 2006 ). The sub-type/super-type relations among SAT give flexibility in matching. It should be noted that this facet can be determined by the lexical level analysis of a question.  X  Question qualifier (QQ) indicates the semantic or pragmatic nature of a question and requires corresponding operations.
For example, a comparison question requires some operations such as arithmetic calculation or ordering of facts (e.g.,  X  X  What French cities are larger than Bordeaux?  X ). A report question, as another example, seeks an answer whose pragmatic purpose is to report on something that is related to the question content.

Different QA techniques and resources have been and will be developed for different formats and themes of answers and different question qualifiers. As these facets are the basis for question classifications and subsequent invocations of different
QA modules, they are prerequisite for analyzing composite questions and decompose them into atomic ones, for which cor-responding QA modules should exist.

The question classification scheme in Table 3 derived from the question collection is the basis for designing the QA mod-ules to handle different types of questions. For efficiency, a variety of answer units need to be generated in advance, in antic-ipation of different types of questions by applying various natural language processing techniques. They need to be associated with the corresponding QA modules so that when the type of a question is determined vis- X -vis the question clas-sification scheme, the corresponding QA module looks for the answer units that are likely to contain a particular answer for-mat and answer theme and satisfy the question qualifier.
 In Fig. 1 , the following questions are used to illustrate how the types of questions can be identified: Q1: When was Mozart born? Q2: What French cities are larger than Bordeaux? Q3: Why did Iraq invade Kuwait?
The question type for Q1 is h single, DATE, specification i in that it requires a single answer (answer format) belonging to the DATE category (answer theme), which is a specification of the birth information of Mozart (question qualifier). Q2 is a comparative question whose answer consists of multiple CITY names that are specific entities under the LOCATION type, resulting in h multiple, LOCATION, comparison i . Finally, Q3 looks for the REASON why Iraq attacked Kuwait, for which long descriptive explanations are required. Therefore, the question type is h descriptive, NULL, reason i . 2.3. Atomic vs. composite questions
As the technology evolves, QA systems are expected to handle increasingly more difficult questions. As such, QA systems should be evaluated not only based on the accuracy of answering the questions they are designed for, but also based on the levels of difficulties they can handle. An ultimate testing would be to evaluate on questions of varying levels of difficulties.
From the system development point of view, however, it is important to explicitly define the types of questions based on nology developed for factoid QA, TREC began a new track including complex questions from 2005:  X  X  X efinition/other X  ques-
Ecuador? ; and a small-scale pilot of  X  X  X pinion X  questions such as  X  X  Do the American people think that Elian Gonzalez should be returned to his father?  X ( Lin &amp; Demner-Fushman, 2006 ).

In the QA roadmap document that provides a vision of QA research and development for a five year span, NIST distin-guishes a broad spectrum of questioners with four levels: casual questioner, template questioner, cub reporter, and profes-sional information analyst ( Burger et al., 2001 ). Table 4 shows the four categories and some instances of answer extractions at different sophistication levels.

It should be noted that most QA systems are still at the level of casual questioners, except for some advanced systems ( Hickl, Lehmann, Williams, &amp; Harabagiu, 2004; Hickl et al., 2007; Moldovan, Bowden, &amp; Tatu, 2007; Saquete, Martinez, Mu-tionality at level 2 and some at level 3.

The dichotomy between atomic and composite questions in our work is not intended to cover the entire space of questions, but to distinguish the questions that can be answered by a single QA module from those that require a composition of an-swers from more than one QA module. Considering that factoid QA attempts to extract a single short answer from a piece of text, it certainly handles an atomic question. On the other hand, a composite question cannot be answered from a single more than two parts. While complex questions in the literature share the same characteristics, composite questions are more narrowly defined such that their answers are composed of partial answers from existing QA modules based on composition plans.

Consider an example question,  X  X  How long is the longest river in the world?  X . If the answer can be found directly as it is, the question is atomic. However, there is only a small chance to find a sentence in which the exact answer exists in the way the question is described, such as  X  X  X he length of the longest river in the world is 6690 km X . Instead, it makes more sense to di-vide the sentence into two so that the system notices  X  X  X he longest river in the world X  is the Nile River and then finds the length of the Nile River. This case is a composite question that involves interactions with multiple answering methods.
The following chapter investigates technical challenges for solving complex questions as they appear in the literature and describes the scope of questions dealt with in this paper. 3. Approaches to answering complex questions
If a QA system cannot identify an appropriate answer type  X  or if the answer type does not exist in the semantic ontology  X  no answer can be returned ( Hickl et al., 2004 ). Another problem is generating a coherent answer when its information is distributed across a document or throughout different documents. This problem can be extended to the case when an answer part is found in a given data source, whereas the other parts, with which it must be fused, are retrieved from different data sources, in different formats ( Burger et al., 2001 ). For both cases, collaboration based on decomposition of complex questions into simpler questions and integration of partial answers from multiple sources is a crucial consideration. 3.1. Past approaches
Understanding and answering a complex question involve many rich natural language processing and advanced inference techniques, some of which are still under active research. They include: recognizing syntactic alternations, resolving anaph-ora, making commonsense inference, performing relative date calculations, and so on. Since complex questions contain diverse informational goals, even the sub-questions produced by a human analyst are not simple enough to be processed by the state-of-art QA modules. As such, past research for complex QA has focused on particular types of complexness, such as ship X  questions ( Litkowski, 2006 ; Harabagiu et al., 2006 ). Other than the complex temporal questions, however, questions were decomposed for specific topics or based on a template or scenario.

Hickl et al. (2004) implemented a scenario based interactive QA to answer complex questions. They proposed that ques-tion decomposition can be approached in one of two ways: either by approximating the domain-specific knowledge for a particular set of domains, or by identifying the decomposition strategies employed by human users. As in Fig. 2 , the term  X  X  X ecomposition X  is used with a slightly different meaning because it focuses on clarification or alternation of questions.
On the contrary, our focus is on reducing the complexness of the answering process by splitting a question into more answerable ones using the existing QA modules.

Saquete et al. (2004) tried to decompose a complex temporal question into simpler ones based on the temporal relation-ships in the question. Processing this sort of questions usually requires identifying implicit or explicit temporal expressions in questions as well as in relevant documents, in order to gather the necessary information. They attempted to detect tem-poral signals that denote the relationship between the dates of related events and manually established ordering rules for each temporal signal. Given that F 1 and F 2 are the dates related to the first and second events in the question, respectively, the signal determines the order between them. In  X  X  What happened to world oil prices after the Iraqi annexation of Kuwait?  X , for example, the question would be decomposed into the following with the temporal signal  X  X  X fter X : Q1: What happen to world oil prices? Q2: When did the Iraqi annexation of Kuwait occur? Since  X  X  X fter X  has ordering key  X  X  F 1&gt; F 2 X , Q2 is executed first. They evaluated this method with 123 temporal questions in
TERQAS ( Pustejovsky, 2002 ). This work is limited to the complex questions that contain sub-questions connected by one of the 17 temporal signals. As such, the question decomposition process is done in a fairly straightforward way with a set of rules.

Katz, Gary, and Sue (2005) also dealt with decomposition of complex questions. Similarly to Saquete et al. (2004) , they used linguistic constraints that govern decomposition of complex questions into sub-questions in the form of semantic decomposition rules. Their work was restricted to a specific domain.

In TREC, 2005 ( Voorhees &amp; Dang, 2006 ), the relationship task was newly introduced. Systems were given TREC-like topic statements to set a context, where the topic was specific about the type of relationship being sought (generally, the ability of one entity to influence another, including both the means to influence and the motivation for doing so). The topic ended with a question that is either a yes/no question, which is to be understood as a request for evidence supporting the answer, or a request for the evidence itself. The system response is a set of information nuggets that provides evidence for the answer ( Dang &amp; Lin, 2007b ).

The clr05rl by Litkowsky (2006) showed the best score on this task. He implemented a mode of operation which consti-tutes a truncated version of the document exploration functionality (which is specifically designed to examine relationships) by Boolean queries in a Lucene index of document repositories. Each topic was reformulated into a single question that cap-tures the essence of what the analyst was seeking. Fig. 3 shows the Lucene search queries and the associated questions for seven of the relationship questions. They  X  X  X orced X  the system to answer the questions as if they were definition questions.
PLANTIR by Harabagiu et al. (2006) combined multiple answer finding strategies (i.e. QA modules). Since answering rela-tionship questions depends on sophisticated representation of the information need of these complex questions, their ap-proach employed three question representation strategies: keyword selection, topic representation, and automatic lexicon generation. Relationship questions were automatically decomposed based on their syntax and heuristics into separate ques-tions. For example, a complex question context:  X  X  X he analyst is concerned with a possible relationship between the Cuban and Congolese governments. Specifically, the analyst would like to know of any attempts by these governments to form trade or military alliances.  X  was automatically split into the three questions in Fig. 4 . Answers for the decomposed questions were merged by heuristi-cally normalizing the scores assigned by the QA modules. In the merge process, duplicate and overlapping answers were fil-tered. In other words, non-redundant answers were just put together as the final set of answers without question-specific compositions we propose to do. 3.2. Plan-based compositional approach
Our approach differs from the previous ones in that the focus is on decomposability of a complex question into atomic ones that can be solved individually by existing QA modules. Instead of developing a set of scenarios or templates for a spe-cific type of questions, our focus is on development of relatively simple high-level plans for different types of answer inter-actions with a general purpose automatic question analysis.

A composition plan prescribes how the answers for atomic questions are composed to generate the final answer. Given a set of QA modules or capabilities corresponding to atomic question types, the QA problem becomes determining whether the given question is atomic or not, dividing it into atomic questions if it is not atomic, and composing the answers for sub-ques-tions based on the designated plan for the question type. If a complex question cannot be divided into sub-questions that can be answered by existing QA modules, it is essentially beyond the scope of the system capabilities, requiring development of a new plan for the question type and a specific QA module that was lacking in answering the unsolvable sub-question.
For the plan-based compositional QA framework, the first task is to delineate types of questions in terms of decompos-ability and interaction modes among the atomic QA modules or functions. Table 5 depicts different interaction modes, their examples, and some required capabilities for various types of complex questions ( Bloom, 1956; Graesser &amp; Black, 1985;
Lehnert, 1978 ). It is based on an examination of our question collection, some of the TREC complex questions, and the ques-tion types that can be derived from the analysis results in Section 2 . As such it excludes the questions that require advanced reasoning capabilities of humans. The interaction modes are listed in the table in the order of perceived difficulties in han-dling them automatically. Included in our current implementation are: master/slave, ordering, integration, and affirmation/ negation (yes/no).
 4. Compositional QA framework 4.1. System overview
The goal of compositional QA is to make it possible to answer composite questions through question decomposition and integration of partial answers. Before the compositional QA method is applied, each question must be checked to see if it can be answered by an existing atomic level QA module. 3 When a question is entered, the system analyzes it and selects a relevant
QA module appropriate for its type. If the answer from the chosen module is judged to be sufficiently reliable with a confidence other QA modules. This process is called atomic QA ( Oh &amp; Myaeng, 2009 )asin Fig. 5 because the question is not decomposed. If none of the answers from individual QA modules has a confidence value above the threshold even after confidence level boost-ing, and if the question can be divided into smaller ones, the system takes a path to compositional QA . Strategies in atomic QA and the thresholds including the alpha and beta were determined by a strategy-learning algorithm, which is described in ( Oh &amp;
Myaeng, 2009 ). The algorithm determines the sequence of QA modules to be invoked and decides when to stop invoking addi-tional modules for atomic questions. This paper focuses on the compositional aspect of the system.

As in Fig. 6 , the architecture for a compositional QA system consists of question analysis , plan execution , answer selection, and multiple QA modules based on several knowledge bases generated by answer annotation . A user question in the form of natural language is entered to the system and analyzed by the question analysis module that employs various linguistic anal-ysis techniques to be explained in the next Section. Based on the question analysis result of the given question, an appro-priate plan is generated according the pre-constructed plan types that are currently  X  X  master/slave  X ,  X  X  ordering  X , tion analysis module to individual QA modules. The answer selection module determines whether returned answers are good for the final answer and composes them according to the types.

An answer for an atomic question can also come from multiple QA modules corresponding to various types of answer units, for which different information extraction methods are employed ( Kim et al., 2004; Oh &amp; Myaeng, 2009 ). When an atomic question gets an answer from a QA module, its confidence is checked first. If it is not sufficiently high, other QA mod-ules are invoked according to the chosen strategy for weight boosting ( Oh &amp; Myaeng, 2009 ). While the QA modules are com-plementary to each other in providing answers of different types, their answer spaces are not completely disjoint. The strategy-driven QA for atomic questions, corresponding to the right side of the system, is described in detail in ( Oh &amp; Myaeng, 2009 ); this paper focuses on the left part.

In atomic QA, the goal of the answer selection module is to determine which answer candidate has the maximum con-fidence value to support for the given question. Let Q and a be the user question and a candidate, respectively. Then the final answer a is selected by: where C ( a , Q ) computes the confidence value between a and Q .As Q is translated into a set of internal query forms h q fined as follows: where S ( a , q i ) is the semantic distance between the answer candidate and the original question term and w assigned by the i th QA module to the answer candidate a . Semantic distance values can be computed with a lexical database ( Choi, Hur, &amp; Jang, 2004 ) like Korean WordNet. When the original question looks for a  X  X  X ocation, X  for example, it can be matched with  X  X  X ity, X   X  X  X rovince, X  or  X  X  X ountry X  with a decreasing value of semantic distance. Additional details can be found in ( Oh &amp; Myaeng, 2009 ). 4.2. Question analysis for compositional QA
A user question in the form of a natural language is analyzed with various linguistic analysis techniques such as POS tag-ging, chunking, named entity (NE) tagging ( Lee et al., 2006 ), and some semantic analysis such as word sense disambiguation ( Choi et al., 2004 ). An internal question generated by the question analysis component has the following form: where AF , AT , QQ are the answer format, answer theme, and question qualifier in the question classification schema described dependency relation between predicates of sentences in the parse tree of a question and helps determining whether a ques-tion is composite or atomic. If the question is composite, its sub-questions can be identified from this structure.
Among the five elements, answer format ( AF ) can be determined relatively easily with the semantic class of the word after the interrogative in a question. Answer theme ( AT ), which is trickier to find, is determined by a hybrid classifier that com-bines maximum entropy (ME) based ( Berger, Pietar, &amp; Pietra, 1996 ) and rule-based methods. The machine learning based method alone with relatively simple linguistic features was not sufficient to catch subtle nuances in Korean questions, neces-sitating the rule-based method that primarily relies on 1113 Lexico-Semantic Patterns (LSPs, Jacobs, Krupka, &amp; Rau, 1991 )we created manually. The final expected AT is assigned by the following equation in the hybrid classifier: where H LSP ( T i , Q ) and H ME ( T i , Q ) compute the weight of a candidate type T respectively. Based on preliminary experiments, we set the constant a and b to 0.7 and 0.3 empirically. It means the result from LSP pattern matching is more weighted than the ME result because only using simple linguistic features in Korean questions is insufficient to categorize subtle nuances by machine learning.

As mentioned previously, question target ( QT ) consists of object and focus . The key element in detecting them is the pred-icate of a sentence in the parsing result, which also plays a critical role in judging whether a question is composite or not. A icate arguments in the dependency structure of a sentence ( Kim et al., 2005 ).
 For the example above,  X  X  Who killed the President Kennedy?  X , its QT becomes: Object : J. F. Kennedy Focus : killer (agent of kill) Predicate: kill(&lt;subj:@who?&gt;, &lt;obj:Kennedy&gt;, NULL) Answer theme: PERSON
The predicate structure makes it possible to identify the focus being  X  X  X iller X  with the combination of  X  X  X ho X  (an agent) and the predicate  X  X  X ill X  in the sentence analysis.

Even if this analysis is done successfully, a difficult problem remains: the difference in the terms identified in the question and in the potential answers. For example, the sentence,  X  X  X ennedy was assassinated by Lee Harvey Oswald on November 22, 1963, in Dallas, Texas. X , contains the answer  X  X  X ee Harvey Oswald X , but there is a mismatch of the predicates between the question and the answer sentences, namely,  X  X  X ill X  and  X  X  X ssassinate X . To tackle this problem, we devised a method that helps matching lexically different expressions by referring to a lexical database called the Korean Lexical Concept Net for Nouns (LCNN), which was manually constructed ( Choi et al., 2004 ). Unlike WordNet ( Fellbaum, 1998 ), where words are grouped into synonym sets that are in turn related to each other through several semantic relationships, individual nouns in the Kor-ean LCNN are related with each other. The Korean LCNN consists of 120,000 nodes (nouns) and 224,000 named entities that are hierarchically organized with a maximum depth of 12. The semantic relations in the Korean LCNN are  X  X  X S-A, X   X  X  X art-of, X   X  X  X nstance-of, X   X  X  X ynonym-of, X  and  X  X  X ntonym-of, X  among which the  X  X  X S-A X  relation is used for the hierarchical relationships.
Lexically different predicates can be also matched by referring to the Korean Lexical Concept Net for Verbs (LCNV) ( Choi et al., 2004 ). This lexical database consists of about 50,000 verbs, some of which have multiple meanings, together with sev-eral relations among verbs, such as hyponymy, synonymy, and antonymy. For example,  X  X  (die) X  has an extended verb set containing the verbs like  X  X  (pass away) X  and  X  X  (die suddenly) X  as synonyms and  X  X  (be assassinated) X  as having a passive-form relation. For the previous example, the data structure is extended with synonyms and alternative forms for each keyword as follows:  X  X  X ho killed President Kennedy?  X  Object: J.F. Kennedy Focus: killer, assassin, criminal, ...

Predicate: kill(&lt;subj:@who?&gt;,&lt;obj:Kennedy&gt;, NULL) snipe(&lt;subj:@who?&gt;,&lt;obj:Kennedy&gt;, NULL) be assassinated(&lt;subj:Kennedy&gt;, NULL,&lt;adj:by@who?&gt;) ...

The QQ of a question indicates the  X  X  X enre X  or  X  X  X unction X  of an answer text, such as method , definition ,or ordering of some-which themselves are obtained from the same parse structure. To do this, we defined more than 1500 patterns (e.g. 161 pat-terns for superlative questions) organized into templates. The parsing result and the three elements obtained from it are matched against the rules to select one of the QQ values. If none of the rules applies, the question X  X  QQ is determined to spec-ification in Table 3 by default.

The large number of rules had to be defined to deal with fine level question qualifiers. For example, while  X  X  How can we prevent a cold?  X  and  X  X  How can we cure a cold?  X  are descriptive questions asking for a method in a similar sentence structure, there is a subtlety that the first question asks for a prevention method that is different from a curing method in the second question. For the current work reported in the paper, we defined 774 patterns for descriptive questions. The above two examples can be analyzed as follows: How can we prevent a cold? ? (&lt; X  X  X ow X &gt;[&lt; X  X  X o X &gt;]&lt;A:NounPhrase&gt;&lt;Qualifier: prevent&gt;) (&lt;AT: Method.&gt;) How can we cure a cold? ? (&lt; X  X  X ow X &gt;[&lt; X  X  X o X &gt;]&lt;A:NounPhrase&gt;&lt;Qualifier: cure&gt;) (&lt;AT: Method.&gt;)
Another example is the subtle difference between  X  X  What is the longest river in the world?  X  and  X  X  What is the second longest river in the world?  X  that belong to superlative and ordering questions in terms of QQ .
 The final step in question analysis is to determine the type of a question, if it is composite, which corresponds to a plan.
The question structure ( QS ) of a question can immediately tell whether it is composite or not by counting the number of predicates: if there is more than one predicates, it must be a composite question. A question can be composite even when ordering question type is signalled by the word  X  X  X he second X  in the set of pattern rules mentioned above. Unless the answer is found directly from a single text unit, it has to be split into two atomic questions that need to be found from two different text sources. 4.3. Plan execution and answer selection
The proposed compositional QA goes through the following steps that include plan execution (steps 3 and 4): (1) Initially all question are assumed to be atomic and treated as such, regardless of the result of the question analysis (2) If the returned answer from the atomic QA module has a confidence value above the threshold, it is the final answer. If (3) Based on the question analysis result, an appropriate plan is selected and each atomic question is sent to the atomic (4) Partial answers returned from the QA modules are composed to produce the final answer.

It should be noted that even if a question is determined to be composite, the system first attempts to obtain an answer with atomic QA.

We developed several plans for different types of composite questions: master/slav e, ordering , integration , and yes/no. For the master/slave type, a question is decomposed into two questions with their order dependency and represented as Q2 (sla-ve) ? Q1 (master). It means Q2 must be executed first to get the answer, which is used to make Q1 more complete and di-rectly answerable. The detailed procedure can be best explained with an example.
 Fig. 7 depicts an example from our system run for a master/slave questions,  X  X  How long is the longest river in the world?  X .
Given the question, the system first recognizes that the expected AT (answer theme) for the question is QUANTITY, more specifically QT_LENGTH, and then tries to find the answer at the atomic level. For the question target (QT),  X  X  X he longest river in the world X  and  X  X  X ength X  are identified as the object and the focus, respectively. Assuming that the object and the focus are not found directly because the object is not explicit like  X  X  X ississippi River X  that can be extracted from a question like  X  X  How long is the Mississippi River ? X , the atomic QA fails to find an answer with a sufficiently high confidence value. Based on the result of the question analysis, now the question is decomposed into the following atomic questions with the  X  X  X 1 ? Q2 X  relationship: Q1: What is the longest river in the world? Q2: How long is [Q1]? The atomic QA module finds the answer,  X  X  X he Nile River X , for Q1 using the Superlative QA module, which is plugged into
Q2 to generate  X  X  How long is the Nile River?  X . The KB QA module invoked by the atomic QA module finds the answer  X  X 6690 km X  easily as the final answer.

For the ordering type of composite questions, the ordinal expressions or numeric qualifiers in a question are recognized to generate a question like  X  X  X hat is the [@X] of [@Y]? X , where X and Y are place holders for a measure like LENGTH and a specific answer theme like RIVER. In the answer selection module, answers returned from the atomic QA module for the indi-vidual questions are converted into a normalized form for comparisons and ranked based on the question constraint to select the final answer. Fig. 8 illustrates the process for an example question  X  X  Where is the second longest river in the world?  X .
For the integration type, a series of alternative questions are generated based on automatically calculated semantic dis-tances based on Korean Lexical Concept Net for Nouns (LCNN). For  X  X  What is a tsunami?  X , for example, other questions can be  X  X  what is a tidal wave?  X ,  X  X  what is a harbor wave?  X , and so on. In LCNN, question alternative rules are defined for different semantic classes of question objects. Given a question, its object and the semantic class are analyzed as  X  X  X sunami X  and  X  X  X at-ural phenomena X , respectively, and a definition for a particular natural phenomenon can be regarded as an answer. Other alternative questions can be  X  X  why does a tsunami occur?  X  and  X  X  How does a tsunami occur?  X  Answers for the questions are col-lected across multiple QA modules and juxtaposed but with an ordering. A final answer for the example question would be generated from the following answers returned from the different QA modules:
A tsunami ( ?)(pronounced/(t)sun A mi/) is a series of waves that is created when a large volume of a body of water, such as an ocean, is rapidly displaced.
 A tsunami is a very large wave, often caused by an earthquake, that flows onto the land and destroys things. A tidal wave is a very large wave, often caused by an earthquake, that flows onto the land and destroys things. The Japanese term is literally translated into  X  X (great) harbor wave X .

For the yes/no type, a question is translated into multiple forms to increase the accuracy of an answer when multiple an-swers are given. Based on the semantic interpretation, the original question is converted into multiple semantically equiv-alent questions for which the system attempts to find answers. The more answers are found, the higher confidence with which the question can be answered with  X  X  X es X . Given  X  X  Did Oswald kill J.F. Kennedy?  X , for example, it is translated two inter-rogatives which can be answered by Atomic QA: Q1: Who killed J.F.K? ? A1: Oswald.
 Q2: Who was killed by Oswald? ? A2: J.F. Kennedy.

If the answers for the question Q1 and Q2 from atomic QA are returned with high confidence, the final answer will be  X  X  X es X . If no answer is found, the answer should be  X  X  X  don X  X  know X . 5. Evaluation and analysis 5.1. Experimental setup
We chose to use Pascal X  Encyclopedia ( http://www.epascal.co.kr ) for our experiments based on our analysis of 1485 questions of various types collected from real users and the answers from the Web and an encyclopedia in Korean. The anal-ysis showed that over 80% of the answers were obtainable from the encyclopedia. While the web was a useful source for the rest of the answers, it was problematic to use it for experiments because web answers sometimes contradict among them-selves and are not always confirmative. Moreover, encyclopedia answers were richer with fuller information in the articles concentrated on a topic. These factors made us believe that encyclopedia is a liable source to construct a test collection for
QA tasks. Nonetheless, it does not mean that our proposed methodologies are biased on a specific type of knowledge source or language.

The TREC QA evaluation sets were deemed inappropriate for our purpose since the target functionality of our composi-tional QA is to make use of existing QA technologies to solve more complex questions. While earlier TREC evaluation sets do not include questions that are complex enough to require the proposed method, the later releases for complex questions re-quire more advanced QA capabilities than what the state-of-art QA technology can offer. For instance, ciQA ( Dang et al., 2008 ), focusing on  X  X  X elationship X , requires the ability of identifying one entity to influence another, including both the means to influence and the motivation for doing so. While this question type is positioned at level 4 of difficulty, the proposed method is targeted mostly at the functionality at level 2 in Table 4 .

Pascal X  Encyclopedia ( http://www.epascal.co.kr ) currently consists of 100,373 entries (articles) and 1,017,807 sentences belonging to 14 domains such as  X  X  X erson, X   X  X  X rt, X  and  X  X  X cience X . The reliability and balanced diversity of information in the encyclopedia were deemed desirable for testing the proposed QA framework utilizing multiple QA modules. Since the con-struction of the QA modules and their answer qualities depend on the well-formedness of the source data, an encyclopedia-based QA test collection would allow the underlying QA modules make less mistakes than web-based collections, for exam-ple, making it easier to see the effect of the compositional QA approach.

Like the TREC QA track ( Voorhees, 2000b ), we have considered various levels of question/answer types and gradually ex-tended our evaluation set from simple to complex questions. In our experiments, we divided the questions into two catego-ries: simple and complex questions. Out of more than 7000 questions, we first selected a subset by ensuring the balance among different answer themes and question structures. The resulting set was divided into atomic and complex questions based on the understanding of the human judges, from which 926 questions were selected finally, making sure that they can be handled by the current implementation and that all atomic answers can be found in the encyclopedia. Among 926 ques-tions, 311 and 615 were used for building our framework at the training stage and for evaluation, respectively. The evalu-ation set consisting of 615 questions has two parts: 500 simple questions and 115 compositional questions. For each question, exact answers or key phrases are specified. To build the answer sheets, we generated answer pools which were answers with evidences that consist of answer sentences and passages from the answer pools. All were experienced com-puter users and also experienced with search engine tasks. Each answer sheet for the question was cross-judged by the three assessors.

Evaluations were made by the human judges who understand the functionality of the existing QA modules. For factoid questions, we used the TREC  X  X  X xact answer X  criterion. For descriptive questions including definitional questions, our eval-uation was based on whether candidate answer sentences contain  X  X  X ey phrases X . It is similar to TREC  X  X  X ugget X  criterion ( Dang &amp; Lin, 2007b ).

For effectiveness comparisons, we employed mean reciprocal rank ( MRR , Voorhees, 2000a, 2000b ). We also used precision , recall , and F-score 4 with the well-known  X  X  X op-5 X  measure that considers whether a correct nugget is found in the top 5 answers returned by the system ( Dang &amp; Lin, 2007b; Monz, 2003 ). We used  X  X  a@n  X  have at least one correct answer in the first n answers to the number of all the questions ( Monz, 2003 ). While F -scores and MRR are widely used for research purposes, we added a@n as a more practical measure to see how human users actually judge the quality of the answers as they make semantically oriented judgments rather than verbatim matches. For statistical significance of the results, we employed paired t-test that assesses whether the means of two groups are statistically different from each other. 5.2. Overall comparison
The proposed compositional QA method was compared against the following: (1) a traditional QA approach of using gen-eral indexing and passage retrieval ( Oh et al., 2007 ), (2) a simple routing approach where a question is sent to all the avail-able QA modules and the results are combined, and (3) strategy-driven QA ( Oh &amp; Myaeng, 2009 ) that corresponds to the step 1 of compositional QA described in Section 4.3 , where each answer from a primary QA module are verified and its confidence value boosted based on the chosen strategy. All the cases are considered atomic QA because the questions are not decom-posed into simpler ones. The simple routing approach was used as the baseline since it makes use of all the available QA modules in the system. The traditional approach is used just as a reference. While the proposed compositional QA method is intended to specialize in handling composite questions, it is used to provide the answers for the atomic as well as com-posite questions. As explained in Section 4.3 , an answer obtained from the atomic QA procedure is final if its confidence va-lue is above the threshold regardless of whether or not the question can be divided into atomic one or if the question cannot be decomposed into atomic questions.
 Multiple QA modules in our system are tailored to various answer classes that are identifiable from documents. While an
AT refers to a named entity type being asked for in a question,  X  X  X nswer classes X  are used to make a distinction among different traits of the answers, such as record, list, description, and general answer classes. In the current implementation, the QA modules represent six different answer classes. Additional details can be found in ( Oh &amp; Myaeng, 2009 ).
Since our goal is to see the value of the composition aspect of the proposed method, we optimized all the four different QA systems (traditional, simple routing, strategy-driven, and the compositional) and the individual QA modules. For the simple routing QA case, in particular, all candidate answers from the six QA modules were merged into a single ranked list with a linear combination of the weights after normalizing the confidence values calculated by the individual QA modules. For opti-mization of the linear combination, the parameter values were set by 260 h question X  X nswer i pairs ( Oh &amp; Myaeng, 2009 ) while assuming the six individual QA modules were optimized. With the same training set, strategy-driven QA system was built based on a strategy-learning algorithm, which is described in ( Oh &amp; Myaeng, 2009 ).

In order to see the effect of the compositional QA method, we measured effectiveness for simple and complex questions separately. Since the compositional QA method was developed to handle complex questions (composite questions, more pre-cisely), the performance difference should be observed more easily. It should be noted that sometimes a complex question, based on the human judgment, can be answered correctly by atomic QA. Table 6 shows the overall performance of the four different QA methods in F -score, whereas Table 7 shows the performance of the three cases with MRR and a@n .
When both atomic and complex questions are considered, the performance increases are 16.82% and 8.43% over the base-line and the strategy-driven QA, respectively. These modest improvements are due to the fact that the number of atomic questions, most of which can be answered by a single QA module, is much larger than that of the complex questions, 500 vs. 115. The net effects of the compositional QA over the baseline and the strategy-driven QA are much larger when only the complex questions are considered: about 183% and 175% over the two atomic QA methods, respectively, which are phe-nomenal gains obtained by the additional automatic question analysis and the manually constructed plans for four different types of answer compositions. The low F -scores, 0.214 and 0.221, for the baseline and the strategy-driven QA indicate that most of the questions deemed complex by the human judges indeed require a special treatment like compositional QA, which improved the performance dramatically. Nonetheless, the not-so-high F -score, 0.607, for the compositional QA indi-cates that there is room for improvement.

The same effects of the compositional QA on MRR as well as a@n are shown in Table 7 . The highest-performing scores are 0.715 (+15.21%) for MRR and 0.850 (+24.82%) for a@n when all the questions are considered. Again a dramatic improvement was achieved when only the complex questions were considered separately because the atomic QA methods are not capable of handling them. The a@n scores are generally higher than those in MRR because human judges do not make verbatim judg-ments. However, they are consistent with MRR in terms of the relative rankings of the different methods.

Table 8 shows the result of the pair-wise t -test for the MRR scores using all the questions, both atomic and complex. The differences are shown to be statistically significant ( p &lt; 0.0001). Because the simple routing QA also merged results from multiple QA modules, it is similar in the compositional effect to our model. Accordingly, the correlation is high (0.803).
A further analysis using 115 complex questions was conducted to see the direct effect of various composition methods implemented in the system: master/slave , ordering , integration , and yes/no. As in Table 9 , the strategy-driven QA (baseline) returned answers for 84 questions but only 22 were correct, resulting in the low F -score of 0.221. Although the system was not designed to handle complex questions, the relatively larger numbers of keywords in complex questions helped extracting some answers.

For the entire set of 115 questions, we observed a steep increase in F -score by about 175% (the number of correct answers from 22 to 68) when the compositional QA was employed. There were wide variations in performance across the different composition methods. At the higher end, integration questions are answered with a precision score of 0.857. Comparatively, the precision of master/slave questions is 0.481. That indicates the question analysis module made numerous errors in iden-tifying sub-questions. Similarly, ordering questions show low precision, and that arithmetic expression identification and an-swer normalization are difficult. The decent score for yes/no questions (0.765) resulted because there were only few errors in translating them to an interrogative form. The wide variations of the scores among the different composition methods were primarily due to the different types of errors made when the questions were analyzed. The following section has further fail-ure analysis results for the 47 questions for which incorrect answers were extracted by the compositional QA system. 5.3. Error analysis
Errors made in an earlier module are propagated to later modules in the QA system. For example, the question analysis component determines the question type for each question, with which an appropriate plan is selected. If the question anal-ysis module makes a mistake, it negatively affects the next process (e.g. selecting a wrong plan). The goal of this analysis is to identify the earliest module in the chain that prevents the system from finding the right answer ( Moldovan et al., 2003 ). Table 10 describes the factors of errors in each component.

Table 11 shows the distribution of 47 errors that occurred when 115 complex questions were handled by the composi-tional QA system. The largest proportion (55.3%) of the errors among different sub-components was attributed to the ad-vanced analysis module where the question structure analysis (C1) caused the largest number (13) of errors, covering 26.68% of all. It indicates that the questions requiring decomposition have more complicated structures and affect the per-formance of handling the master/slave questions. The next highest error rate (12.77%), also in the Advanced Analysis module, is associated with C6, semantic interpretation for the integration and yes/no types. The next highest (8.51%) in the same mod-ule is for C4 that has something to do with answer ordering . All together, it is clear that the compositional QA method can be more effective with more precise question analysis techniques that will fail less frequently with complex questions.
Table 12 summarizes error distributions to the key components across the two systems. Second to the question analysis that caused the largest proportion of errors, whose causes are explained above, was the answer retrieval (24%). These errors, together with others caused by incomplete answer annotations (16.8%), have nothing to do with the compositional QA meth-od per se. Nonetheless, these errors need to be reduced as they directly affect the answer selection module. Compositional QA is bound to fail when erroneous answers are returned from the underlying QA modules.
 6. Conclusion
The main motivation behind this work was to devise a way to utilize existing QA techniques to answer complex questions that cannot be answered without building new QA techniques specifically designed for their types. To this end, we analyzed real-life questions for their characteristics and classified them using three different facets that help identifying atomic ques-tions that can be answered by existing QA modules in a system. In the proposed compositional QA framework, composite questions are divided into atomic questions that are handled with four different composition methods for master/slav e, ordering , integration , yes/no question types.
 We ran a series of experiments to see the effects of the proposed framework against three different cases: (1) a traditional
QA approach of using general indexing and passage retrieval, (2) a simple routing approach where a question is sent to all the available QA modules and the results are combined, and (3) strategy-driven QA. The results based on 615 questions show that the compositional QA approach outperforms the simple routing method by about 17%. Considering 115 complex ques-tions only, which is the ultimate testing of the effect of compositional QA, the F -score was almost tripled from 0.214 to 0.607, a phenomenal gain obtained by the additional automatic question analysis and the manually constructed plans for four dif-ferent types of answer compositions. Additional failure analysis revealed that most errors stemmed from the question anal-ysis module that could not analyze the structure of the complex questions.

While this paper focuses on the compositional framework for composite questions, together with the characterization of questions, there should be more research on automatic question analysis to reduce the errors, which should be expanded further to handle other types of answer compositions for more variety types of complex questions. To make the composi-tional QA approach more viable, the process of generating composition plans needs to be automated as much as possible, perhaps using some training data as done in the strategy learning work.
 Acknowledgement
This work was supported in part by the Korea Ministry of Knowledge Economy (MKE) under Grant No. 2008-S-020-02 and by Brain Korea 21 project sponsored by Ministry of Education and Human Resources Development, Korea.
 References
