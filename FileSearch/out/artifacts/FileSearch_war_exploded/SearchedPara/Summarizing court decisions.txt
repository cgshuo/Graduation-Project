 1. Introduction
In the legal field the term court decision refers both to the process in which a court resolves a legal dispute and to the record of this process. The term case is also used as a synonym. Decisions of judges are a source of law. This is especially true in countries with a  X  X ommon law X  tradition such as the United States, the UK and
Australia, but also countries with a  X  X ivil law X  tradition increasingly value court decisions. Court decisions or cases are instructive when they introduce or appear to introduce a new principle or rule, modify or interpret an existing principle or rule, or settle or tend to settle a question upon which the law is doubtful. Lawyers and courts value past cases and use them in precedent reasoning. Decisions of the higher courts bind courts lower in the curial hierarchy and sometimes the courts hold themselves bound by their own decisions. The binding effect of legal precedents on subsequent cases is embodied in the doctrine of stare decisis , which states that once a point of law has been settled by a judicial decision, it cannot normally be departed from afterwards.
The propositions set forth in a precedent that are binding to subsequent cases are termed the ratio decidendi of the precedent ( Branting, 2000, p. 4 ).

In this paper, we will use the terms court decision and case to refer to the record of a decision, i.e., to a document. These documents are in the form of written text and are electronically stored in databases from which they can be selected or retrieved.

Currently, a lot of effort goes into the manual drafting of case headnotes, synopses and summaries. Legal professionals  X  among which are judges, attorneys, legal scholars or students  X  value these surrogates as they allow an easy selection of cases, draw the attention to specific verdicts and allow a quick assessment of the case X  X  content. Information searches usually comprise a substantial amount of time of a legal professional.
The general goal of information seeking is to build an argument to answer the problem at hand. He or she wants to find a viable argument that will support his or others X  claims. Preferably he obtains the argument from a binding decision or from a persuasive precedent case. His query is composed of legal concepts and the facts related to the issues in his current problem. Facts are often easy to match with the content of a case text (in case of a full text search). A more difficult problem is that certain fact patterns form a legal concept, but the retrieval does not match the concepts of the query. On the other hand when synopses or summaries are available they usually contain the concepts that qualify the arguments. These  X  X  X epresentations X  X  of cases can be searched by a retrieval system or manually consulted by the legal professional. They also facilitate automated reasoning with precedents and statistics gathering.

Building summaries by hand is a slow, labor-intensive and expensive operation. Currently, the selection of published jurisprudence is greatly arbitrary. Judges and lawyers forward certain decisions to legal editors, or courts rely on specialized staff whose task is to summarize cases. If we have automated techniques for drafting the case summaries, the user emancipation when consulting legal information is greatly enhanced: The user can select cases of interest to him/herself, whereas before the selection was made by the editorial staff, while hiding many cases from the user. The disposition of a full overview of jurisprudence in a certain domain of law or from a particular court has a great additional value for policy purposes. It allows an evaluation  X  and if necessary  X  reorientation by the government of its policies. The questions, however, are what content should the case synopses contain, how is it best presented to the user, and how can we automatically extract this con-tent from the case texts or abstract it?
The aims of this article are to define the legal case content that is most valuable to include in the synopsis and to give an overview of past research with regard to the automatic extraction of this content and its summarization. The article also guides the reader into future research avenues that find inspiration in research on precedent reasoning and argument visualization. In the next section we describe the content of court deci-sions, which is followed by a section that discusses the information elements that are valuable to include in their summaries. Section 4 describes the state of the art of summarization of legal texts, while Section 5 ela-borates on the most essential steps in the summarization process, some of which are subject of emerging research. 2. Court decisions
Court decisions are highly structured and usually follow a conventional schematic form . The schematic structure of a particular text type is specified in terms of the ordered parts it is built of. This means that the discourse structure consists of a relatively large number of obligatory discourse segments and a relatively small number of optional ones, both of which have a strong tendency to occur in a fixed order. Specific seg-ments often have very specific functions with regard to the communicative purpose of the discourse. Besides, general information such as the name of the court, the name of the judge(s) that pronounced the decision and the date of the decision, the decisions generally contain: (1) elements that refer to the legal basis of the case: e.g., the plaintiff, defendant, cause of action, type of complaint, and procedural context; (2) the background (e.g. the description of underlying events and undisputed legal concepts); (3) the substantive legal issues or the disputed legal questions (e.g., alleged offences in a criminal case); (4) the justifying theory or the opinion; (5) and the decision, also called verdict or disposition.

Cases contain many explicit and implicit references to the law. They include references to statutes and links to other cases. Case texts also contain many implicit references to specific content in the same document or in other documents that were explicitly referenced. Besides this rather structured information, decisions contain a large amount of natural language texts.

There are very few theoretical studies that investigate legal texts from a linguistic viewpoint . Our experience with legal texts in past projects learns that the vocabulary is very diverse. Legal language employs a number of domain-specific concepts, and a number of terms have a specific semantic meaning in the legal context, yet the texts for a large part contain the vocabulary of ordinary language. Cases have the tendency to discuss many issues tangential to the main topic, and often highly stylized legal phrasings pervade the texts. In addition, legal language often uses vague or open texture concepts, i.e., natural language words that do not have a clearly defined meaning, consequently their interpretation is somewhat uncertain ( Bing, 1991; McCarty, 1989 ). For instance, whether a fact or circumstance would appropriately be classified as a  X  X  X ehicle X  X  would to a large extent depend upon how the vague meaning of the word is molded by its context, for instance the interpretation of the other words making up the text. Studies of the texts X  syntax seem to confirm some differences with ordinary language. Court decisions are close to complex prose. Judges tend to express themselves in exceptionally long sentences and in crucial subclauses with many cross-links and anaphoric references. The texts contain narrative stretches (accounts of factual descriptions) and some parts are argumentative in nature.

Considering the more detailed content, cases essentially contain descriptions of facts . These facts are very important and are often expressed in a narrative form ( Gardner, 1987 ). Facts are the elements on which the judge has based his conclusion. Because they satisfy certain propositions or certain concepts of law, they are especially valuable when the case is used for precedent reasoning. The ratio decidendi of a precedent must be grounded in the specific facts of the case ( Branting, 2000, p. 31 ). The facts are described in a separate section of the text or are interwoven in the texts of other segments such as the type of complaint, the background of the complaint and the opinion part of the decision. The facts do not solely influence the decision: Some back-ground knowledge that is not explicated might influence a decision. It is also possible that apparently irrele-vant facts become important, when the case in which they are contained influences other subsequent decisions.
In the context of the justification of a decision, humans classify cases by factors . A factor refers to a domain-specific or stereotypical collection of facts, which tends to strengthen or to weaken a conclusion. Fac-tors are often hierarchically structured for a particular domain. Such a factor hierarchy can be considered as a classification scheme that provides abstract terms for classification of the factual descriptions of the decision 1987 ). Legal opinions may address multiple issues within the same case. Facts, factors, issues and theories con-stitute the most relevant information of a case. Depending on the positive contribution they deliver to the defense of one party or another, they can be categorized either as pro-defendant or as pro-plaintiff. Facts might have a varying degree of influence on a case. Sometimes a small additional fact makes that a completely different factor or issue is at order. In addition, a legal principle, policy or normative concern may lead one to conclude generally that a particular factual circumstance is very important in the context of a particular prob-lem. Determining the strength of the factors and issues of a case is particularly difficult and is influenced by the co-occurrence of other factual circumstances, factors and applicable principles, policies and concerns.
Arguably, the opinion is the most important part of a legal decision from the point of view of information searching, since here the judge reasons about the case, explains relevant concepts, or interprets legislation based on the facts. The opinion contains the propositions of law that are necessary to the decision in an expli-cit or implicit way ( Branting, 2000, p. 28 ff ). A unique proposition of the law necessary to a decision can sel-dom be determined. Instead a gradation of propositions ranging in abstraction from the specific facts of the case to more abstract factors and issues is found. Factors, issues and legal theories are not always explicitly mentioned in the natural language texts of the cases. The reader himself infers these concepts. However, opin-ions of appeal cases  X  besides a summary of the facts of the case  X  typically contain the identification of the issues of law raised in arguments by the counsel for each of the parties and the pronouncement of the legal propositions to the facts of the case.

In a collection of court decisions, the cases to be classified can roughly be divided into two main groups. On the one hand, there are a large number of  X  X outine X  cases that do not have any legal value at all. Most of what judges decide does neither make nor bind any law, either because the case has no or almost no legal content, or because the case simply involves the application of settled principles to the facts. On the other hand, there are a number of cases that are of legal interest. These are decisions that express or explain a rule of law or a concept of law in their evaluation of the facts, or that describe an important procedural practice in such a way that the decision is or could be of importance for obtaining adequate and detailed knowledge of court practice in the field of law in question. Part of these decisions belong to what is generally called leading cases . Leading cases provide solutions to legal difficulties such as alternative interpretations of facts, uncertain or conflicting law, alternative applications of the law, interpretations of the law, and disputes between law and morality.
These cases very often give an (additional) explanation of a concept or legal term and as such make law or amend an existing law, or they exhibit a certain method of argumentation, which is different from or conflict-ing with earlier applications of the law. Sometimes, leading cases decide the competence of a court, or espe-cially stress a concurring or dissenting opinion. Or, decisions in which a rule of law and/or body of facts that is of general interest are considered as important.

The interestingness or importance of a case often varies over time ( Matthijssen, 2000, p. 34 ). For example, when decisions are used as precedents, subsequent decisions can limit, extend, or overturn earlier precedents.
There are innumerable instances in which the exact scope of a decision is gradually defined by subsequent cases, most commonly when the classification of an earlier opinion is narrowed down and circumscribed by these decisions. In a similar way, the scope of a precedent can be expanded in subsequent opinions. As seen above, the relevance of a case (and especially of an opinion) can be assessed at different levels of abstraction and detail. It is possible  X  and even likely  X  that factors, issues and theories are gradually shaped in a number of subsequent cases and that this influences the bias of the factor, issue or theory in question towards one party of another and also helps to determine its strength. When comparing different cases, corroborating and contrasting factors play a role. For instance, when two cases are compared and a factor means a distinc-tion between them, it can be a corroborating factor and support the interpretation of one of the cases, or it can be a contrasting factor that supports a contrary interpretation. Not only content of the cases, but external factors, such as the hierarchy of the courts that pronounced the decisions play a role when comparing cases. 3. Summaries of decision records
For the reasons stated above, summaries are highly valued in legal information retrieval. The current, handcrafted summaries are often called headnotes and consist of several concept terms (describing the legal question treated in the case) and a short summary of the case or explanation of each concept term (reflecting the legal principles applied by the court and the leading role of the case, if any) ( Fig. 1 ). The drawing of this summary mostly happens according to the following technique: the summary is composed first by extracting one or more interesting paragraphs from the decision. Consequently the appropriate keywords are selected either from a fixed list (related to the classification of the case content), or they are copied from the text of the case. In addition, the headnotes might link to important passages of the case or link to (relevant passages) of other cases, so that its user could reveal a passage or an argument that had not been anticipated.
If a larger summary is aimed at, the following elements might be included: 1. The broad category(ies) of the case (e.g., finance, agriculture); 2. The title of the case including the name of the decision and the court; 3. The undisputed facts and legal issues; 4. The disputed facts and legal issues; 5. The cited cases (especially important in common law countries); 6. The cited legislation that is applied or was in mind by the judge (especially important in civil law 10. The verdict.

Handcrafted summaries regard single documents, although links to related content are provided. Content maps or overviews of a complete collection of cases are also appreciated ( Bruce, 2000 ). Ideally, for the legal professional it is important to have an evolving synthesis of a set of cases that determines their cross-cutting principles, modifications, exceptions, reasons and overall decision rules, which explain why some cases were decided one way and others differently (cf. Nagel, 1991, p. 157 ff ). When comparing different cases, lawyers especially attach importance to the nature of law and to key facts, factors and issues. A summary could evolve over time when certain arguments gain or loose importance.

There is a current interest in information systems that visualize the argument structure of a case and its com-posing elements (e.g., Araucaria project of Reed &amp; Rowe, 2004 : Fig. 2 ). This visualization is successful and adapted by several courts in Ontario, Canada, but its manual drafting severely limits its use. This visualization can also be considered as a kind of summary, while the technology for its automated realization being an inter-esting research topic (see infra).

The above may give the impression that summarizing legal cases, and especially the generation of evolving multi-document summaries is a very difficult task to automatically perform. This is certainly true. Neverthe-less, it seems useful to explore different research paths that might lead to this goal. There are also simple sum-marization tasks. As early as 1673, it was recognized that written decisions frequently contain language unnecessary for the resolution of the issues before the court and that this unnecessary language is not part of the ratio decidendi of the case ( Branting, 2000, p. 20 ff ). This superfluous information in the form of formal phrasings is still part of current decisions. 4. Past research on summarizing legal decisions
Past research on summarizing legal cases focus on structuring the cases into segments or sentences that play a specific role in the decision and into recognizing this role, apart from some work on clustering related pas-sages. This past work introduces the next section in which the different phases of summarizing legal cases are discussed in detail.
 The first attempt of summarizing legal cases is found in the work of Gelbart and Smith (1993) . Their FLEX-
ICON (Fast Legal Expert Information CONsultant) system automatically extracts legal concepts, factual terms, and case and statute citations. Legal concepts are automatically identified in the texts by matching the words of the text with terms contained in a legal phrase dictionary. Facts are recognized as non-common words (computations based on the inverse document frequency), while case and statute citations are detected with the help of handcrafted patterns. This system was evaluated with a corpus of 1000 cases in the domain of economic loss and a number of general British Columbia cases drafted in English. The summaries were con-sidered as indexing representations in a retrieval task. The document collection was queried with eight differ-ent queries, which a user of the retrieval system composed based on the list of terms detected in the cases. The authors expressed their satisfaction of the search when comparing it to a Boolean search.

From 1993 to 1996 we developed the SALOMON system for summarizing Belgian criminal cases (written in Dutch). Like FLEXICON, SALOMON extracts relevant text units from the case text to form a case sum-mary, but to a lesser extent relies on handcrafted patterns. For each criminal case, a profile is generated ( Fig. 3 ), which facilitates the rapid determination of the relevance of the case. Its user is informed of (1) the name of the court that pronounced the decision; (2) the date of the decision; (3) the key paragraphs that describe the crimes committed; (4) the key paragraphs and concepts that appear to express the essence of the opinion of the court; and (5) references to the applied non-routine foundations. First, the structure of the case is automatically recognized based on a handcrafted text grammar augmented with cue phrases. Belgian crim-inal cases are composed of nine ordered elements, some of which are optional: superscription, containing the name of the court and the date; identification of the victim; identification of the accused; alleged offences, describing the crimes and factual evidence; transition formulation, marking the transition to the grounds of the case; opinion of the court, containing the arguments of the court to support its decision; legal foundations, containing statutory provisions applied to the court; verdict; and conclusion. The test set consisted of 1000 cases (882 general ones and 112 specific ones, the latter referring to appeal procedures, internment of people, infringements by foreigners and false translations). When compared to a manual structuring of the cases, the structural elements of a case were recognized with an averaged recall of 88% and precision of 93% for general cases, and with an averaged recall of 66% and precision of 88% for specific ones ( Moens &amp; Uyttendaele, 1997 ).
A low recall for this latter category is explained by the lack of knowledge of textual patterns that the system used for detecting the structure of the specific cases.

The most relevant parts of a case are the alleged offences, the opinion of the court and the legal founda-tions. The alleged offences give a description of the crimes a person is accused of (i.e., the crime themes of the case). A delict description is disclosed in a separate text paragraph. Such a description contains the specific facts of the delict, integrated in the text of the description. The offences may contain several delict descriptions: some of them may be identical, but referring to different facts or different accused. Delict concepts are usually described in a fixed stereotypical way. The opinion contains the argumentation of the judge regarding the crimes committed. The texts of the alleged offences and opinion of the court are summarized by clustering their paragraphs represented as tf  X  idf term vectors and extracting representative paragraphs ( Moens, Uyttendaele, &amp; Dumortier, 1999 ). The aim is to filter redundant content and to identify informative text paragraphs, which are relevant to include in the case summary. The non-hierarchical clustering algorithms ( k -medoid algorithm) rely on the centrality of a representative object in a cluster and the best k (number of clus-ters) is detected. The most central object in each cluster of related sentences or paragraphs is included in the summary. SALOMON detected representative paragraphs in the alleged offences with a recall and precision of 82% and of the opinion with a recall of 75% at a precision of 33%, when evaluating 700 general cases. In this evaluation paragraphs selected by a human expert were compared with the paragraphs extracted by the sys-tem. We think that much of the success of the statistical techniques that rely on a shallow bag of words rep-resentation of the content is due to the fact that the texts exhibit a very stereotyping naming of the terms that describe the crime themes, which are the concepts to include in the case summary. On the other hand, infor-mation that is not relevant to include in the summary (e.g., the description of the circumstances of the crime) exhibits much more variation in their word use. This situation facilitates the thematic grouping around crime concepts both in the alleged offences and opinion parts. Each cluster is represented by a key term and redun-dant terms were removed.
 The legal foundations consist of a complete enumeration of legal texts and articles applied by the court.
Several of these foundations (routine foundations) are cited in each case, while others concern the essence of the case. Routine foundations were manually detected in the 1000 cases mentioned above and compared with the foundations that the system has recognized based on the handcrafted knowledge patterns ( Moens &amp; Uyttendaele, 1997 ). In terms of performance, we measured a recall of 77% and precision of 79% for general cases, and recall of 69% and precision of 77% for special cases.

Ideally, intelligent summarizers of legal cases should be able to discriminate between grounds or motiva-tions that are based on facts from the ones that are based on legal principles. Principle grounds are the para-graphs of the opinion of the court in which the judge gives general, abstract information about the application and the interpretation of some statutes and gives arguments for doing so. The identification of principle grounds is very difficult for both humans and machines requiring an advanced form of content understanding and relating the content to contextual information to be found within and beyond the text of the individual case comprising statutory provisions, legal principles and social customs and norms. Detecting arguments in the cases and qualifying them with legal concepts would already be a step forwards towards such a solution.
Note that SALOMON restricted the summary to extracts, no effort was made to label text with abstract legal concepts (e.g., factors and issues), which are highly valued in handcrafted summaries. Moreover, representa-tive arguments (based on the centrality of the argument in a cluster of arguments that are related based on shared terms) are extracted, but the relationships between arguments are lost in the summary.

The initial processes of the SALOMON system have some correspondence with the LETSum system devel-oped and tested by Farzindar and Lapalme (2004) . Here 3500 judgments of the Federal Court of Canada are submitted to the summarization system. LETSum segments the texts into thematic structural components, i.e., the introduction with the different parties, context which explains the facts in chronological order, the juridical analysis which describes the comments of the judge and finding of the facts, and the application of the law to the facts as found, which is the most interesting part for the legal expert, and the conclusion with the actual decision of the court. Interestingly, the authors filter two categories of segments: first, opinions and arguments that report the point of view of the parties in the litigation, and secondly, citations related to previous issues or references to applicable legislation. From the relevant parts, sentences are selected based on their tf (term fre-quency)  X  idf (inverse document frequency) weights computed on document and corpus respectively consider-ing the relative importance of the sentences in the decision text. The authors report an accuracy of 90% of the automated segmentation and a 97% correct detection for the filtering stage compared to a manual segmenta-tion and filtering respectively. A ROUGE-1 score reveals an average correspondence of 58% of the final sum-maries automatically generated with handcrafted summaries.

Another interesting summarization system regards work done by Hachey and Glover (2005) at the Univer-sity of Edinburgh. Their system trains a classifier on 141 House of Lords judgments and recognizes the rhe-torical status of sentences of 47 judgments based on a number of textual features, where each judgment contains 105 sentences on average. A limited set of rhetorical labels is compiled composing of e.g., fact (the sentence recounts the events or circumstances, which give rise to the legal proceedings), proceedings (the sentence describes legal proceedings taken in lower courts), background (the sentence is a direct quotation or citation of source of law material), framing (the sentence is part of the law lord X  X  argumentation), disposal (the sentence either credits or discredits a claim or previous ruling), textual (the sentence signals the structure of the document or contains formal content unrelated to a case), and other (default class). The authors rely on very simple features such as: location of a sentence within a document and within subsections and paragraphs; sentence length; whether the sentence contains a word from the title; whether the sentence contains significant terms spotted by the tf  X  idf metric; whether the sentence contains a citation; linguistic features of the first finite verb; cue phrases; and the presence of certain named entity types. The authors trained different classi-fiers: decision tree learning algorithms, na X   X  ve Bayes classification, support vector machines and maximum entropy modeling. Among the best results, the maximum entropy classifier shows a precision of 51% at a recall of 17% when precision and recall are averaged over the sentence categories mentioned above. Disposal sen-tences are most accurately recognized, while fact and background sentences yield a precision and recall of 0%. A low recall indicates a lack of sufficient patterns for training. Precision errors might be due to the quite simplistic approach of feature extraction that was inspired by the classification of components in scientific arti-cles. Although the authors do not analyze in depth the causes of the errors, they are critical towards the extrac-tion type of summary where sentences of different text parts are literally concatenated often yielding a different interpretation for the reader than the one of the original case text. 5. Important steps in summarizing decisions
From the above it is clear that summarizing the texts of legal cases could save legal professionals some amount of their expensive time and  X  most importantly  X  would give the lawyer a competitive advantage when he can have access to a summarization tool because this would allow finding relevant arguments for his or her case at hand, which would not be accessible otherwise.

Summarization can be defined as reducing the content to the most salient pieces of information. We explained above the importance of summarizing the opinion part of the case because it contains the justifica-tion of the decision in terms of the legal principles that are applied and of the argumentation of the judge.
Automatically recognizing this information allows describing content with legal abstract concepts (possibly linked to text passages) and visualizing an argumentation. Both results do not constitute what we regard as a classical summary composed of well-formed text, but they do serve the purpose of a summary in legal case management, namely providing a tool that allows the legal professional to judge the relevance of the case for his or her current problem and conveniently find arguments.

What are now the necessary technologies that we should research and develop in order to make such a sce-nario possible? We successively discuss the segmentation of the cases, concept identification, the detection, qualification and visualization of arguments, and link and graph-based approaches. 5.1. Segmentation
Court decisions are examples of documents that follow a stereotypical but court-dependent schematic struc-ture. As seen in the section of related research, segmenting the cases in their major components is an important first step when generating summaries. Although past summarization approaches have considered case segmen-tation as part of the summarization process, we foresee that the structure of future decisions will increasingly be marked with a markup language such as Extensible Markup Language (XML). XML allows for custo-mization of markup languages with application-specific tags. The syntactic conformity of the structure can be validated with a document type definition (DTD) or XML schema. Not only the structure can be marked, but also explicit information such as references to statutes and other court decisions can be tagged. Ideally, explicit references are tagged and follow a standardized syntax making them uniform resource identifiers (e.g., for the reference to a certain article in a certain statute) in order to guarantee their easy processing.
There is no question about the usefulness of markup languages. They provide that certain information can easily be interpreted by the computer and consequently provide more refined information access when this information is relevant to include in the summarization. However, they require that standards are agreed on and are effectively used by the different institutions in order to make interoperability and data exchange possible. For court decisions, this means that there have to be agreements on allowable structures and com-ponents. There are already initiatives to standardize the tag set for legal documents (e.g. legal XML, MetaLex, the European project ESTRELLA). Standardization of the properties of documents is unfortunately often perceived as a curtailment of intellectual initiatives and creativity and as a limitation of the possibilities of expression. Experiences with digital libraries learned that for standards to be useful they must be relatively simple, well understood and well accepted by all parties involved ( Fox &amp; Sornil, 1999 ). We believe that making agreements on the validity of certain structures and naming schemata is only realistic for the broad structure of the cases and for some general information that is indisputable, such as the parties involved or citations of statutes and other cases. The tags can be assigned when the authors of the case texts draft them, possibly while using special and adapted editors, or can be assigned by automated means after drafting ( Moens, 2006b ). The experiments of Moens and Uyttendaele (1997) and of Farzindar and Lapalme (2004) demonstrate that this task can be done with high recall and precision. 5.2. Concept identification
The next step is to qualify the arguments and other information in the cases. The descriptors (or classifi-cation scheme) contains a list of fixed terms by which the cases are described (e.g.,  X  X  X orging of signature X  X ).
This classification scheme can take the form of a domain-specific ontology in which concepts and their rela-tions are made explicit. The task is to assign the conceptual terms to certain phrases, sentences or passages of the case texts that represent legal factors, issues and theories. This task regards real abstracting because text is being replaced by abstract concepts.

From the early days of artificial intelligence and law, the need for automatic concept analysis of the court decisions has been articulated. McCarty (1984) pleaded for an automatic parsing of the texts and assignments of concepts, where, for instance statements of facts in a judicial opinion can be classified in a certain legal category. In most cases concept identification can be brought down to two subtasks: the identification of phrases and words in a text that literally express a legal concept and the generalization of textual content.
Legal terms composed of two or more individual words and domain-specific terms could be detected based on hypothesis testing (likelihood ratio for a binomial distribution) where the occurrence of two or more terms, or the occurrence of the term and the class of the domain do not occur independently so that it can be assumed that they are correlated ( Moens &amp; Angheluta, 2003 ).

Although words and phrases are valuable content descriptors, in legal texts they are often too specific to convey the content and should be translated into abstract concepts ( Bing, 1987; Dick, 1989 ). On one end of the spectrum there are the machine-readable dictionaries or thesauri (e.g., WordNet) that facilitate the replace-ment of individual terms with synonyms or broader terms (hypernyms) provided that word senses in the text can accurately be dissolved. Such an approach is often not sufficient because certain configurations of words and their relationships in texts trigger certain conceptual terms. At the other hand of the spectrum there are approaches that categorize complete decisions with legal concepts based on a bag-of-words representation of the cases. This might include the classification of legal texts into subject domains (e.g.,  X  X  X axation X  X ,  X  X  X nsur-ance X  X ,  X  X  X ankruptcy X  X ,  X  X  X eal property X  X ). An interesting avenue to be researched is assigning concept terms to certain content elements, such as phrases, sentences or passages. The factual elements could then be clas-sified into factors (e.g.,  X  X  X uty of reasonable care X  X ) and certain factor constellations can be classified into legal issues (e.g.,  X  X  X aster and servants X  X ,  X  X  X egligence X  X ).

In the past there have been limited attempts for classifying the complete texts of legal cases with conceptual terms. For instance, Bru  X  ninghaus and Ashley (1997) trained a Rocchio and Winnow classifier on example case texts to which legal factors were assigned. Exact results were not given in this paper, but the authors mention that the results were disappointing. They blame the large variety of words in the texts in combination with the small number of training examples available as the cause of the poor results. Experiments at the company
WESTLAW demonstrate that improving the feature selection by selecting feature words from a fixed list of key terms certainly had a positive effect on the classification accuracy ( Thompson, 2001 ). The k nearest neighbor (kNN), the C4.5 decision tree and the Ripper rule learning algorithms were tested in this experiment.
The classifiers were trained on more than 7500 cases that were manually classified. The kNN classifier achieved an average recall of 26% at a precision of 84%, while the C4.5 algorithm and the Ripper algorithm resulted in ca. 41% recall at 76% precision. Thompson gives an assessment of the causes of the errors. The case texts cite a multitude of topics and exhibit a large variation in style, which makes that a lot of terms are present in the texts that are not related to the categorization. Many irrelevant features require a large number of training examples in order to effectively find the classification patterns, which are usually not available.
In a second experiment Bru  X  ninghaus and Ashley (1999) learned the patterns that relate to a legal factor from small textual elements , i.e., sentences and summaries of cases. Some feature words were replaced by their synonym terms based on a legal thesaurus. The ID3 algorithm gave better results for factor assignment when trained and tested on the small texts in comparison with training from and classifying complete texts. For some factors, recall and precision values of up to 80% were obtained. For other factors, the F combines recall and precision values that are equally weighted, was below 60%. The experiment involves only six legal factors making that firm conclusions cannot be drawn.

Factual descriptions often follow a narrative style. Concept learning not only refers to generalizing individ-ual words (such as classifying a text under the category finance, when it contains the words stocks, money and bonds), but also classifying script-like sequences of actions or complete descriptions into more abstract con-cepts. A typical example is summarizing the sequence  X  X  X nter + order + wait + eat + pay + leave X  X  as a restau-rant visit or summarizing  X  X  X rive in + pay with automated teller machine + open tank + fill + close tank + drive out X  X  as a gasoline fill-up. Technology is certainly not yet advanced enough to solve such an abstracting task. Notwithstanding, some of the ingredients of solving this complex problem are yet available ( Moens, 2006a ). We propose a cascaded model with limited backtracking within a machine learning frame-work, in which individual extraction tasks (and their probabilistic outcome) contributes to more complex extraction tasks, eventually leading to script recognition. According to Hovy and Lin (1999) abstracting requires knowledge about the world, which hopefully can be acquired from large corpora.

In the legal field complex examples can be provided that correspond to legal concepts. The textual phrases, their semantic equivalents and generalizations  X  all of which in some way represent the facts of a case  X  are very useful for classifying facts into factors. However, this is not sufficient. The relations between the words and phrases within a sentence (e.g., syntactic and semantic) and between clauses and sentences (e.g., discourse) also play a role in legal concept classification. For legal cases, relationships that express a temporal or causal order are relevant to recognize as features in a concept classification task. For instance, mentioning a car acci-dent in which a jogger is killed and then the car is driving away refers to a  X  X  X it and run driving X  X , while men-tioning a car driving away followed by an account of the car killing a jogger, will not.

When semantically classifying text, we are usually confronted with two kinds of problems. A concept can be expressed by a large variety of words and phrases that might express a certain likeness to humans, but will be perceived as completely different constructions by the machine. Secondly, when the categorization is solely based on word features, the classification is still very ambiguous. The classification of legal content is often determined by small details that are, for instance, expressed as negation and modality. In addition, a classi-fication based on words is often insufficiently discriminatory, other features such as syntactic, semantic and discourse relationships are needed as features in the classification.

Finally, there is the problem of manual annotation, which is a common issue when classifying text. Addi-tional resources are needed. However, it often happens that when citing other cases, these cases are character-ized by certain conceptual descriptors. This teaches us that we cannot treat legal case summarization as an isolated task. The document collection provides weakly annotated references. The older target cases are sum-marized or described with legal concepts in the new source case. In addition, cases also refer to texts of statute law that mention legal concepts. Both the link patterns and the information to which the cases refer should be taking into account when summarizing an older case to which newer cases refer. When the interpretation of a case is gradually shaped by future decisions, an evolving summary seems most suitable.

Automated concept assignment is a very important task in legal information retrieval and legal knowledge management. The concepts contribute to a better indexing, classification and summarization of the legal sources. In addition, it has been demonstrated that the concepts are especially useful for (automated) case-based reasoning (i.e., reasoning with past cases to find arguments or an outcome for a current case) ( Ashley, 1990; Bru  X  ninghaus &amp; Ashley, 2003 ), harmonization and comparison of law ( De Boer, van Engers, &amp; Winkels, 2003 ) and legal decision support by using expert systems ( Stranieri &amp; Zeleznikow, 2005 ). This technology can very well be ported to the management of electronic files of courts, police reports, and other documents. 5.3. Argument detection, classification and visualization
The general goal of information seeking by the legal professional is to build an argument to answer the problem at hand. He or she wants to find a viable argument that will support his or others X  claims. Detecting arguments in cases, recognizing their function in the ratio decidendi , and qualifying an argument or a set of arguments by the machine certainly help in managing the information contained in the decisions. Hence the current interest in information systems that visualize the argument structure. It has been shown that this visualization offers an excellent overview of the motivation of the judge and is highly valued in information selection. Currently, tools exist that assist the manual drafting of the argumentation structure of a case by manually dragging text into a graph structure that represents the argumentation (as it is done in the Araucaria tool, see Fig. 2 ). However, we would like to support this process with automated means and in this way pro-duce a kind of visual summary of the argumentation. We currently study this topic in the Automatic detection and ClassIfication of arguments in Legal cAses (ACILA) project.

Notwithstanding the urgent practical need in the legal practice, the automatic detection and classification of arguments in a legal case entail many fundamental research questions including research into taxonomies of legal discourse relations and their automated recognition and disambiguation in texts.
 The first approach that comes to mind in argument detection is performing a rhetorical structure analysis .
The term  X  X  X hetorical structure X  X  finds its origin in rhetorical structure theory (RST), which is a theory that describes what parts or segments texts have and what principles of combination can be found to combine parts into entire texts. The rhetorical structure of a text is a main indicator of how information in that text is ordered into a coherent informational structure. One could argue that in court decisions the rhetorical structure is even more important than in other texts, since it is in the nature of court decisions to represent an argumentation.
Rhetorical structure analysis of some form could therefore be a very useful tool for locating important infor-mation and analyzing the structure of the argumentation in this type of texts.
 In the past, some efforts have been made to construct formalized models that can be easily implemented in NLP applications, one of the most successful of which is Rhetorical Structure Theory ( Mann, Matthiessen, &amp;
Thompson, 1992; Mann &amp; Thompson, 1988 ). Rhetorical Structure Theory assumes a text to have a hierarchi-cal organization based on asymmetrical nucleus-satellite relationships. This means that pairs of adjacent ele-mentary textual units combine into parent units  X  text spans  X  which are again recursively merged until at a certain point a unit spanning the entire text  X  the root  X  is reached. The constituent halves of any text span are linked together by a text structuring relation, which typically holds between a semantically more central unit  X  the nucleus  X  and a more peripheral one  X  the satellite (although there also exists a small set of multi-nuclear relations, which consist of two or more equivalent units).

Although in theory the set of rhetorical relations is open, it is generally assumed that the number of relevant rhetorical relations is relatively small. The classification designed by Mann and Thompson (1988) seems to have been accepted by some part of the discourse analysis community, though many additions have been pro-posed. They distinguish the following rhetorical relations:
For instance, the  X  X  X on-volitional Cause X  X  relationship could be used to analyze the following excerpt from a real case (Westlaw No. 33 F.Supp.2d 907): into the structure:
This node represents the fact that the event mentioned in discourse segment 1 is the non-volitional cause of the situation represented in segment 2. The rest of the text could be analyzed in a similar fashion. The result of a complete rhetorical analysis is a tree structure, every node of which represents a rhetorical relation.
Hovy (1993) reports a complete taxonomy of coherence relations, of which some are subtypes for the oth-ers. Rhetorical relations are not mutually exclusive. It is sometimes possible to assign different relations to the same text fragments. Some authors make a distinction between intentional and informational relations.
The former refer to relations that reflect how the role played by one discourse segment (i.e., its purpose) relates to the role played by another segment with respect to the interlocutor X  X  discursive goals ( Grosz &amp;
Sidner, 1986 ), the latter represent how the meaning conveyed by one discourse segment relates to the mean-ing conveyed by another discourse segment ( Hobbs, 1985 ). The second type of relations are the most inter-esting ones if we want to detect and classify arguments in legal cases, while the former could be applied to the broader components of a case text, for which standardization and mark-up efforts are currently underway.
 Nucleus-satellite relations Evidence Justify Antithesis Concession Circumstance Solutionhood Elaboration Background Enablement Motivation Volitional cause Non-volitional cause Volitional result Non-volitional result purpose Condition Otherwise Interpretation Evaluation Restatement Summary Multi-nuclear relations Sequence Contrast Joint
RST may be very successful in providing a formalization of discourse structure; it nevertheless has a num-ber of inherent shortcomings. For example, Mann &amp; Thompson themselves point out that the theory has been primarily designed for analyzing monologue discourse ( Mann &amp; Thompson, 1988, p. 243 ff ) and court deci-sions have some inherent dialogical properties (e.g. the alternation of the plaintiff X  X  and the defendant X  X  arguments). In addition, the nucleus-satellite distinction does not always reflect the importance of the corre-sponding text units correctly, which is an important aspect when using this theory for summarization. Legal discourse theory has defined an additional set of discourse relations that signal argument structure.
Studies on legal reasoning by Toulmin (1958), Bench-Capon (1997), Prakken (2004) and Sartor (2005) have built theoretical models of legal reasoning and represented argumentation structures in a logical formalism (e.g., propositional, first order predicate, deontic and defeasible logic, claim lattices). Legal reasoning is usu-ally performed in a context of debate and disagreement. Accordingly, such notions as arguments, moves, attacks, dialogue and burden of proof are studied. On a practical scale this research has resulted in dialogue and argumentation systems (e.g., Gordon, 1995 ) that offer a useful interface by which users are guided when founding a hypothesis or conclusion, and as such have evaluated the theoretical models. Huang and Cross (1989) cited in Mital and Johnson, 1992 simply propose to represent an argument that underlies a decision with the following components: (1) the one who proposes the argument (defendant or plaintiff); (2) the belief or assertion to be established by the argument; (3) the reasons advanced in favor of the belief. Even such a simple representation would make it possible to visualize or select certain information in the cases.
The most difficult task is the automatic recognition of the arguments in a legal discourse. A first problem regards boundary detection and segmentation. Soricut and Daniel (2003) remark that semantic/discourse seg-mentation is a notoriously under-researched problem, both at a sentence and discourse level and suggest seg-mentation based on the syntactic structure, but are vague on how to translate syntax into discourse segmentation. Another question is how the rhetorical structures and more specifically the rhetorical relations that hold between clauses and sentences and between groups of sentences can be detected. Linguistic surface phenomena that signal rhetorical relations are lexical cues, pronouns and other forms of phoric reference, and tense and aspect ( Hovy, 1993 ). The most prominent indicators of rhetorical structure are lexical cues ( Allen, 1995, p. 504 ff. ), most typically expressed by conjunctions and by certain kinds of adverbial groups as is shown in the following example:
Text fragments (3) and (5) are a justification of text fragment text fragment (3) . Text fragment (1) forms the evidence of text fragment italic.

Research has shown that it is usually possible to manually extract these cues and to unambiguously corre-late them to a specific rhetorical relation, but having the machine to perform this task is much more difficult, because many textual cues might be missing or ambiguous. Research also stresses the possibility of learning corpus-specific rhetorical cues from a corpus of training data ( Marcu, 2000 ) where it is possible to take into account many more features than the ones commonly considered. It is feasible to automatically detect rhetor-ical relations, although it might be difficult to fully disambiguate them or to analyze their full discursive scope.
Supervised or weakly supervised learning techniques that explore different bags of features might here be the most appropriate approach.

Once the rhetorical relations are detected, an important issue is to define a representation that is suitable to capture the discourse structure and that allows computing the salience of content in the context of summari-zation or reasoning with the information (in case of retrieval, question-answering and case-based reasoning).
The most intuitive representation is one in the form of a graph, where the nodes represent the clauses, sen-tences and other discourse units, and labeled arcs describe the relations that hold between the sentences (see Hobbs, 1985 ). Representations in the form of trees have been implemented in a summarization context ( Marcu, 2000 ). Wolf and Gibson (2006) argue and prove with a set of 135 texts that trees cannot adequately represent many coherence structures in natural language texts. The discourse structure of these texts contain various kinds of crossed dependencies as well as nodes with multiple parents. Neither phenomenon can be represented by using trees. These authors plead for a chain graph representation. In a labeled coherence chain graph, an ordered array of nodes represents the discourse segments; the order in which the nodes occur reflects the temporal sequencing of the discourse segments. Labeled directed or undirected edges represent coherence relations that hold among the discourse segments.

In the legal field studies and implementations of case-based reasoning provide us formats for representing the ratio decidendi of a case. Case-based reasoning (CBR) is generally concerned with remembering old prob-lem situations and their solution and using these to find a solution, classification or other inference for the current problem. Legal professionals reason with precedent decisions . Many of the case-based reasoning sys-tems use graph formats for representing cases. For instance, in the CATO system ( Aleven, 1997 ) a case is ana-lyzed and factors are manually identified. When comparing an old case with a new case for precedent reasoning, differences in factors between the two cases are strengthened or weakened dependent on other con-textual factors in the factor hierarchy. In the GREBE system facts and factors are represented as nodes in a graph, the edges are labeled with the relationships between nodes ( Branting, 2000 ). GREBE is especially designed in order to assess the strength of an argument. Legal case-based reasoning systems rely on arguments that are manually detected in the case texts. The Araucaria system mentioned above does not allow any rea-soning, it only visualizes an argument structure. The above systems give insights into a suitable graph format for representing the argument structure of cases. 5.4. Link and graph-based approaches
One of the possible ways to compute salience in a single discourse is to use a graph-based algorithm. Many different approaches exist that differ in the discourse and lexical relations being represented and in the graph walking or graph clustering algorithm.

Marcu (2000) exploits the RST tree structure representing clauses and sentences in the nodes and their dis-course relations in the edges for salient computation of individual nodes based on their position in the tree hierarchy. Erkan and Radev (2004) compute the centrality of entities in a graph as a measure for salience.
The TextRank algorithm takes as input a set of textual entities and relations between them and uses a graph-based ranking algorithm to produce a set of scores that represent the rank for each textual entity ( Mihalcea &amp; Tarau, 2004 ). Moens, Jeuniaux, Angheluta, and Mitra (2006) represent coreferent and referent relationships between noun phrase entities in a graph in order to score the aboutness of the entity in a text.
Many graph-based algorithms used in these experiments are a variation of the algorithms that were originally developed to estimate the importance of linked Web pages. The HITS (Hypertext Induced Topic Selection) ( Kleinberg, 1998 ) and PageRank ( Brin &amp; Page, 1998 ) algorithms are the oldest and most famous algorithms in this category.

We are not aware of any studies that exploit the links between cases and between cases and statute law in order to compute the salience of certain information, that help in classifying and qualifying information in a particular case based on its referenced information in another case, or  X  in some future paradigm  X  to exploit the evolution over time of a link structure between cases. Law is a domain characterized by many explicit and implicit links between information. Although Turtle suggested as early as 1992 to use the link structure in legal information retrieval, his advice has never been followed.

In the ACILA project we construct a graph representation of the recognized argumentation in the texts and we hope to be able to define an algorithm for extracting a subgraph that will represent the main argumenta-tion, which will form the summary. What kind of information nodes and their relationships we will be able to automatically recognize and represent is subject of our current research. Next, we want to go one step further and exploit also the link structure between cases mutually and between cases and statute law. As we have seen above, cross-case and case-statute links give us a weakly supervision when learning the concept classes of a case X  X  content, because the referenced case or statute describes the (older) case with legal concepts. In addition, the in-and out-going links attached to a case X  X  argument provide valuable information about the importance of the argument. And, if one wants to build a summary that evolves over time, different arguments might turn up in the summary when their salience which is computed based on citations of this argument, becomes more or less important.

In the original text the recognized and summarized argumentation could be highlighted and when passing over the text certain legal concepts could pop up. Such a reading aid is different from a classical summary com-posed of well-formed sentences, but could nevertheless fulfill the most important function of a summary. Per-sonal communications of judges and of police officers who have to digest masses of reports have confirmed the need for automated tools that visualize and link important information in texts. Alternatively, we could pres-ent the user of the information system with a visual graph representation of the argumentation as is done in the
Araucaria system ( Reed &amp; Rowe, 2004 ) or superimpose the graph on the text. These presentation forms are perfectly in line with a current interest in exploratory search tools ( Marchionini, 2006 ). This type of summary has another advantage. It allows its user to view recognized arguments and their qualification in context, which is important in the legal field where many open-texture terms and even open-texture classification con-cepts are common. We believe that summarization and especially these alternative forms of summaries have an important role to play in the legal domain. 6. Comparison with summarization in the medical domain
The legal domain deals with the medical domain the problem of the information overload. Massive amounts of documents are created and we need tools to facilitate their selection and use. In the medical domain we witness a current increase in the development of summarization systems ( Afantenos, Karkaletsis, &amp;
Stamatopoulos, 2005 ). The existing systems use traditional sentence extraction techniques, although many of them make use of natural language processing techniques among which named entity recognition takes a pri-mordial place. Some of the systems use domain-specific knowledge that is handcrafted when extracting salient content ( Endres-Niggemeyer, 1995 ). Characteristics of the documents that we also see in the legal domain are: a stereotypical structure for certain types of documents (e.g., patient records) that is useful to exploit in sum-marization; the usefulness of conceptual metadata that describe the documents, like in the legal domain they are currently almost exclusively manually assigned; idiosyncratic linguistic constructs that sometimes need tai-lored linguistic processing; and word meanings that differ depending on the subdomain. In contrast to the legal domain, the language in medical subdomains is often considered as a sublanguage. The term sublanguage is used when texts deal with a restricted subject domain and are processed for specific purpose results. A sublan-guage is more restricted in its linguistic properties (vocabulary, syntax, semantics and discourse organization).
Domain specialization led to an elaboration of the methods of sublanguage analysis ( Sager, 1981 ), in partic-ular as applied to the language of clinical reporting in patient documents and to the extraction of information.
Legal language cannot be considered as a sublanguage. Although for certain texts the syntax and discourse organization might be restricted, the vocabulary of legal texts is very diverse. The texts deal with any aspect of society thus employing words from general language, and legal issues might suddenly turn up in societal contexts where they are totally unexpected ( Moens, Gebruers, &amp; Uyttendaele, 1996 ). Nevertheless, summari-zation in both the legal and medical domains can benefit from sophisticated natural language processing tech-niques, and extraction of arguments is useful in both domains. For instance the choice for a particular diagnosis or drug treatment might be justified in the texts. 7. Conclusions
Legal cases have a rich content, which makes it perfectly possible to intellectually draw up more than one good summary of a case. Some summaries are based upon the facts, which are believed to have influenced the outcome, but without trying to capture the interactions between different facts or attempting to embody the supposed reasoning that led up to the decision. Others reduce legal concepts down to social, legal and general primitives such as factors and issues, which are believed capable of representing complex concepts. Other potential case summaries concentrate on the arguments, which are advanced by the court or by the parties and adapted by the court in reaching a decision.

The past witnesses only very shallow approaches to the automatic summarization of court decisions of which the results are often disappointing and not in line with what legal professionals really expect from a summary. Current markup languages and standardization efforts with regard to the structure of a case and uniform resource identifiers facilitate the identification of certain information and the detection of links between cases, which can be seen as a first step in their summarization.

The opinion part contains the most pertinent content of a legal case because it discusses important facts, factors and issues. Here, the judge assesses the factual circumstances, and clarifies his or her reasoning in for-mulating a decision. Case headnotes and the visualization of cases, that are currently manually constructed and which can be regarded as types of case abstracts, demonstrate the importance of legal concept recognition in passages of the case texts and the detection and qualification of arguments. Both tasks still demand substan-tial amounts of scientific research in the years to come. The handcrafted abstracts also teach us that alternative formats can be relevant in summarization of information besides the classical well-formed textual summary.
As such, the summarization of legal decisions offers many opportunities for innovative technology that can also be used beyond the legal domain.

Finally, legal cases exhibit an interesting discourse structure where content elements are linked through vari-ous relationships and a legal document collection forms a web of citations, which has never been explored for computing the salience of content. We are very hopeful that the current interest in graph-based algorithms for natural language processing and already traditional link-based ranking algorithms used by search engines stimulate research in this unexploited domain of legal documents.
 References
