 Social network spam increases explosively with the rapid de-velopment and wide usage of various social networks on the Internet. To timely detect spam in large social network sites, it is desirable to discover unsupervised schemes that can save the training cost of supervised schemes. In this work, we first show several limitations of existing unsupervised detection schemes. The main reason behind the limitations is that ex-isting schemes heavily rely on spamming patterns that are constantly changing to avoid detection. Motivated by our observations, we first propose a sybil defense based spam detection scheme SD2 that remarkably outperforms exist-ing schemes by taking the social network relationship into consideration. In order to make it highly robust in facing an increased level of spam attacks, we further design an un-supervised spam detection scheme, called UNIK. Instead of detecting spammers directly, UNIK works by deliberately removing non-spammers from the network, leveraging both the social graph and the user-link graph. The underpinning of UNIK is that while spammers constantly change their patterns to evade detection, non-spammers do not have to do so and thus have a relatively non-volatile pattern. UNIK has comparable performance to SD2 when it is applied to a large social network site, and outperforms SD2 significantly when the level of spam attacks increases. Based on detection results of UNIK, we further analyze several identified spam campaigns in this social network site. The result shows that di ff erent spammer clusters demonstrate distinct character-istics, implying the volatility of spamming patterns and the ability of UNIK to automatically extract spam signatures. C.2.0 [ Computer-Communication Networks ]: Gen-eral X  Security and protection Social networks; spam detection; community detection; user-link graph; social graph
Spam in online social networks increases quickly because of the viral distribution of information provided by massive social connections on the Internet. The e ff ectiveness of email spam detection also contributes to such a trend. A study [2] shows that email spam has dropped by half in 2010 and spammers are more aggressively targeting social networks and search engines. It is estimated that 67% of social net-work users have been spammed in a survey [1] conducted by Sophos.

To detect spam in online social networks, many super-vised machine learning based methods have been proposed. For example, Lee et al. [9] proposed to deploy honeypots in social networks, and apply machine learning to detect spam using captured spam as the training set. Benevenuto et al. [3] suggested to detect promoters and spammers in a video social network with user-based, video-based, and so-cial network based features. Markines et al. [11] proposed to use six features at post-, resource-, or user-level to capture spam in a social bookmarking system. However, supervised machine learning methods have some inherent limitations when being applied to a large social network site. Specif-ically, labeling the training set is required for supervised learning, which incurs a high human labor cost. Moreover, the labeling work has to be done repetitively to maintain e ff ectiveness for spam detection given the volatility of the spam content and some spam posting patterns. Lastly, the supervised model always lags behind spam attacks with new patterns of spam content.

Di ff erent from supervised ones, unsupervised schemes that do not have the training cost have been proposed to detect spam in emails and social networks by directly leveraging the spamming patterns. For example, Xie et al. [19] proposed AutoRE to automatically extract spam URL patterns based on the distributed and bursty patterns of botnet-based email spam campaigns. Applying this approach in detecting social spam, however, may su ff er from a high false negative rate since a number of spam posts in social networks are con-tinuously sent over months instead of following the bursty pattern [13]. Gao et al. [6] identified spam by clustering posts based on text and URL similarities and then expect-ing spam posts to form large clusters with bursty posting patterns. This approach assumes that spam clusters are not connected to non-spam ones. However, spam posts may in-clude non-spam URLs to increase their legitimacy as shown in our data and discussed in [19], which e ff ectively connects spam clusters to non-spam clusters, making it highly dif-ficult to distinguish spam from non-spam. Thereby, it is desirable and imperative to design an unsupervised scheme that can address the limitations of existing schemes.
In this work, we first propose a sybil defense based spam detection scheme SD2. In SD2, a user graph is constructed by combining the social graph and the user-link graph. The former represents the social relationship between active non-spammers, while the latter characterizes the spam link shar-ing activity of spammers. Observing that spammers and non-spammers usually form di ff erent communities in the user graph, SD2 applies community detection based sybil de-fense algorithm to the user graph and achieves better spam detection performance than existing schemes. However, be-cause the e ff ectiveness of sybil defense is subject to the spam attack intensity, SD2 does not perform well when the level of attacks increases.

To improve the spam detection performance under an in-creased level of spam attacks, we further design a new UN-supervised socIal networK spam detection scheme, called UNIK. Instead of picking out spam directly, UNIK works by capturing the properties of non-spammers in the net-work first, and then clustering suspicious spammers based on the landing pages they are advertising, leveraging both the social graph and the user-link graph. The underpinning of UNIK is that while spammers constantly change their patterns to evade detection, non-spammers do not have to do so and thus have a relatively non-volatile pattern. UNIK first constructs a user-link graph connecting users who share URL links. Given that a spammer often uses di ff erent ac-counts to post spam URLs, the user-link graph constructed would include almost all spammers in the system, although non-spammers who share URLs are also included. UNIK then constructs the social graph according to the mutual so-cial connections between users, and identifies non-spammers with the help of the social graph. The URLs mainly posted by these identified non-spammers are collected as a URL whitelist, which captures the patterns of non-spam URLs. By trimming non-spam URL edges matching the whitelist in the user-link graph, UNIK isolates a large portion of non-spammers in the user-link graph. Finally, UNIK di ff erenti-ates spammers from non-spammers with respect to the node degree in the trimmed user-link graph and detects the ma-jority of spammers.

UNIK is expected to overcome the limitations of two exist-ing unsupervised spam detection schemes [19], [6] and SD2, as UNIK exploits non-spam patterns to detect spam. First, the AutoRE scheme [19] works by detecting spam that is sent with two patterns: distributed and bursty. Correspondingly, spam that is typically posted in the same long duration as normal posts will not be detected. UNIK works by removing non-spammers from the user-link graph which covers most spammers, so it is able to detect most of the spam. Sec-ond, the spam clustering scheme [6] relies on the assumption that spam and non-spam posts can be clustered into di ff er-ent groups utilizing the sharing URLs between them. How-ever, as shown in [19], spam content often includes non-spam URLs to increase the legitimacy, which e ff ectively breaks the assumption even if only a handful legitimate URLs are in-cluded. UNIK overcomes this limitation by using the non-spam pattern to remove non-spam URLs from the user-link graph, therefore it is robust to the spam attacks with legit-imate URLs. Third, SD2 uses a sybil defense algorithm to cluster non-spammers and spammers, whose performance is subject to the spam attack intensity. UNIK identifies non-spam URL signatures based on the social graph and the URL sharing pattern, and then removes non-spam URLs from the user-link graph, thus its e ff ectiveness is maintained in spite of the spam attack intensity since non-spam patterns are largely not a ff ected.

We evaluate the performance of SD2 and UNIK with a 10-month dataset from a commercial social blog site. SD2 shows its superior performance compared to AutoRE [19] and the spam clustering scheme [6] by reducing both the false positive rate and the false negative rate in detecting spam. UNIK also shows comparable performance to SD2 when being applied to the dataset. Furthermore, UNIK maintains its performance when the spam attack increases, while the performance of SD2 degrades accordingly.
Based on the spam detection result of UNIK, we have identified a number of large spam campaigns in this social blog dataset. Our analysis shows that di ff erent spam cam-paigns demonstrate distinct characteristics. On one hand, this indicates the ine ff ectiveness of some detection schemes relying on these spamming patterns. On the other hand, this also means that UNIK is able to e ff ectively group spammers into spam campaigns, which provides an opportunity to ex-tract spam signatures from these campaigns and use them to detect spam in other systems.

The rest of the paper is organized as follows. Section 2 evaluates the limitations of existing work and motivates our work. Section 3 presents the design, evaluation and analysis of SD2. Section 4 illustrates the design of UNIK, and Sec-tion 5 evaluates its performance. Section 6 analyzes the spammer clusters detected by UNIK. Section 7 discusses other related work, and Section 8 concludes this paper.
In this section we present the limitations of existing un-supervised spam detection schemes based on a dataset from alargesocialnetworksite. Theresultsmotivatedournew designs in this study.
We have collected posts and social connections of users for over 10 months from a large commercial social blog site in 2009 [16]. The number of user IDs who have at least one URL in their posts is more than 176 thousands. These IDs have more than 2 million posts with URL(s) in the trace collection duration. We developed a supervised machine learning algorithm to detect spammers in this dataset. The spammers detected by the supervised algorithm have both the false positive rate and the false negative rate around 1% to 2%. Because of the accuracy of the supervised algorithm and the infeasibility to label every user in the dataset, we use the results of the supervised algorithm as a ground truth in evaluating the performance of unsupervised spam detection schemes. As we have aforementioned, the good performance of a supervised algorithm is achieved with a price, thus it is still desirable to discover an unsupervised algorithm with similar performance.

Figure 1(a) shows the number of new user accounts cre-ated every day, stacked with non-spammers on top of spam-mers. Although the number of spammers is relatively small, some of the spammers posted a number of spam articles to the site as shown in Figure 1(b). (a) New users created every day (a) AutoRE performance
We first discuss the performance of AutoRE [19] that de-tects email spam with distributed and bursty patterns. We applied AutoRE to our social blog dataset by replacing the AS number with the user ID as hinted by [6]. Figure 2(a) shows that the suggested 5-day threshold of spam URL ac-tive duration has a false negative rate of 26.8%. We further changed the threshold from 5 days to 320 days. As a re-sult, most spam is detected, but the false positive rate is increased to 44.3%. Figure 2(a) shows the performance re-sults of AutoRE by tuning the threshold between 5 days and 320 days. Clearly, tuning the threshold cannot help to improve the overall performance.

AutoRE suggests that most spam URLs have a bursty active duration. However, Figure 2(b) shows that in our dataset, more than 50% of spam URLs have an active dura-tion of more than 5 days. This finding indicates that if we rely on the bursty pattern of spamming to detect spam, we risk to miss a significant portion of spam that is as active as non-spam [17].

Our study shows that the main reason for the performance degradation of AutoRE is due to the change of the spam pattern: lots of spam has been posted with a non-bursty pattern instead of a bursty one.
Gao et al. [6] proposed a scheme, referred to as FBCluster, to detect spam clusters in a social network site. FBCluster constructs a similarity graph of posts on which posts shar-ing the same URL or similar text are connected. Spam clus-ters in the graph are detected if a cluster is distributed and bursty, i.e., the cluster is posted by more than 5 user IDs and the median interval of posts is less than 1.5 hours as suggested by FBCluster.
 According to the FBCluster scheme, we first construct a URL sharing graph connecting posts sharing the same URL in our dataset. Then we apply the suggested thresholds (5, 1.5 hrs) to detect spam, and we get a false positive rate of 39.3% with a false negative rate of 0.2%. The low false negative rate suggests that FBCluster is able to detect most spam in our dataset as spam URLs are often shared by spam posts. However, the high false positive rate indicates that FBCluster has mistakenly included a number of non-spam posts in the detected spam clusters. We examine the largest cluster in our dataset, and find it has a non-trivial percent-age (14.4%) of non-spam posts. Even if we only select the largest cluster as the detected spam cluster, the false pos-itive rate is still as high as 30.1% while the false negative rate increases to 7.0%.

To understand why FBCluster has such a high false posi-tive rate, we further check the URLs that are posted by both spam and non-spam posts. We find there are about 0.7% of non-spam URLs that are also presented in spam posts. We call this phenomenon the legitimate URL inclusion at-tack ,asspammersincludelegitimateURLsintheirpoststo avoid being detected [19]. Although the scale of this attack is small in the dataset, any occurrence of such attacks will e ff ectively connect a spam cluster and a non-spam cluster. Therefore, the reason for the high false positives is that FB-Cluster is unable to separate spam clusters from non-spam clusters when there exist legitimate URL inclusion attacks. This limitation of FBCluster is rooted from the fact that spammers can intentionally add legitimate URLs to their posts, increasing the di ffi culty of distinguishing spam from non-spam.
In the previous section, we have shown the limitations of existing unsupervised schemes in detecting spam. We notice that FBCluster [6] is able to detect the majority of spam in the trace, however, it fails to separate spam clusters from non-spam clusters, and leads to a high false positive rate. This phenomenon is very similar to the sybil attack, in which sybil nodes are connected with non-sybil nodes. This moti-vates us to investigate sybil defense based schemes [20, 4, 18] to detect spam, given such studies have not been conducted before.

However, directly applying sybil defense schemes for spam detection has several challenges. First, existing sybil de-fense schemes use the social network graph to identify non-sybil/sybil nodes. As a result, a non-trivial portion of low degree nodes need to be removed in the pre-processing [20, 4] of the social graph, in order to shorten the mixing time [12]. This prevents sybil defense schemes from detecting all the spam as a number of spammer IDs will be removed in the pre-processing. Second, spammer IDs are not necessarily participating in the social network at all because it is hard for spammers to convince non-spammers to be their friends.
Motivated by our findings, we propose SD2, a Sybil De-fense based Spam Detection scheme by using the social graph and a user-link graph that connects users sharing the same URL. SD2 overcomes the problem of FBCluster by e ff ec-tively separating non-spammers from spammers with the sybil defense scheme. SD2 also includes most spammers for detection by capturing the intensive URL sharing ac-tivities among spammers with the user-link graph to make the best use of the sybil defense scheme. Ideally, SD2 only removes inactive/new non-s pammers and few spammers in the pre-processing, and then detects non-sybil nodes as the non-spammers and sybil nodes as the spammers, resulting few false positives and few false negatives.

In general, SD2 works as follows. First, a social graph con-necting users with mutual social connections is constructed. Second, a user-link graph connecting users sharing URLs is added to the social graph, generating a user graph in-cluding almost all users we are interested in. Third, a pre-processing procedure is applied to remove nodes with less than 3 degrees [4]. Fourth, community detection based sybil defense [18] is applied to the user graph to rank nodes with the expectation that non-sybil nodes have higher ranks and sybil nodes have lower ranks. Finally, a cuto ff is applied to ranked nodes to identify sybil nodes as spammers.
In SD2, a critical step is to find out the right cuto ff point, for which we propose a method based on conductance ratio. For a set of nodes A in a graph, conductance [10] reflects the community structure of A .Define B =  X  A ,orthecomple-ment of A ,thenconductanceisthenumberofedgesbetween A and B ,dividedbythesumofnodedegreesin A (or B if the number is smaller). If we define e AB as the number of edges between A and B , e AA ( e BB )asthenumberofedges within A ( B ). Then conductance of A is defined as:
Clearly, conductance indicates the intensity of connections between A and the rest of the graph B .Conductanceof0 means strongest community structure (no outside connec-tions), while 1 means weakest community structure (all con-nections are external).

The community detection based sybil defense algorithm [18] outperforms existing sybil defense approaches by utiliz-ing the conductance metric. The algorithm starts from a trust node or trust node set s .Newneighbornodesof s is repeatedly added to the s with the preference of minimiz-ing conductance, until all connected nodes are added. The community detection algorithm ranks nodes by the order of adding to the trust node set s ,asthenodesrankedhigher are more likely to be non-sybil nodes.

In applying the community detection based sybil defense algorithm, SD2 needs to find an optimal cuto ff point to sep-arate non-sybil nodes from sybil-nodes. However, there is no suggestion on the cuto ff point selection from the algorithm. Therefore, we propose a cuto ff method for SD2 based on our observations from the dataset. Figure 3 shows the conduc-tance and conductance-ratio values of ranked nodes in the user graph constructed with the dataset. The conductance value is computed when the node is added to the trust node set. The conductance-ratio is computed as the ratio of new conductance to previous conductance upon adding a new node. As shown in Figure 3, the conductance-ratio is very close to 1 until it hits the middle part, where the value starts to vibrate sharply. SD2 thus selects the cuto ff point upon the sudden increase of the conductance-ratio.

In terms of spam posts detection, SD2 outperforms exist-ing schemes substantially as shown in Figure 9(b): the false positive rate is 2.8% and the false negative rate is 1.4%. If we consider the spammers detection performance, the false positive rate is 0.9% and the false negative rate is 3.0%. Figure 3: Conductance of the user graph (social graph + user-link graph) of original dataset Figure 4: Conductance of the user graph with sim-ulated sybil attacks
The reason for SD2 to outperform existing schemes is twofold. First, it is able to e ff ectively separate non-spammers from spammers by detecting the community of non-spammers with the social graph. Second, it is able to detect most spammers by including them with the user-link graph, even if most of them are not in the social graph.
Although SD2 shows very good performance when it is evaluated with our real world dataset, it does have some limitations. Specifically, the performance of sybil defense in separating non-spammers from spammers degrades when the number of sybil attack increases, which results the degra-dation of SD2 performance.

Figure 4 shows the conductance and conductance-ratio of our dataset with simulated sybil attacks. We randomly select 10% of users in the social graph, and add the same number of spammers connecting to these users, forming a scale free social network among them. As a result, the cuto ff point leads to higher false negative rate of 10.0% in terms of spammer detection performance.
Because the performance of sybil defense based unsuper-vised spam detection scheme SD2 degrades when there is an increasing level of attacks, in this section, we further design anewschemeUNIK:UNsupervisedsocIalnetworKspam detection. UNIK aims to overcome the limitations of SD2 by exploring the social graph and user-link graph separately.
Constructing the social graph is straightforward based on mutual relationships between any two users. In addition to that, in an online social network, users post content and Figure 6: An example of edge trimming in UNIK URLs. For spammers to promote some spam sites or adver-tisements, they keep posting the URLs pointing to the spam sites. And they often do this using di ff erent user accounts. Thus, UNIK also constructs a separate graph based on the posted URLs by each user in the network. On this graph, users are defined as nodes, and the shared URLs posted by them are defined as edges, and we call it a user-link graph .
At a high level, UNIK starts with a user-link graph based on the relationship between users and the URLs posted by them. Instead of directly detecting spammers, UNIK takes careful steps to remove non-spammers from this user-link graph with high confidence by leveraging both the social graph of social network users and the user-link graph based on the content. Because non-spammers do not have to con-stantly change their patterns as spammers do to escape from detection, non-spammers have relatively stable patterns. By deliberately removing non-spammers based on their pat-terns, UNIK is minimally a ff ected by the sybil or legitimate URL inclusion attack by spammers, and achieves a better performance.

The underpinnings of UNIK are twofold. First, non-spammers are the main body of the social network (we as-sume that if spammers become do minant, the social network is no longer valuable to perform any detection). Second, spammers usually keep posting the same spam URLs with the same or di ff erent user accounts. Thus, spam URLs have high occurrences in the content on the social network.
Accordingly, UNIK works as follows. First, it constructs auser-linkgraphbasedonthepostedURLsbytheusers, and a social graph based on the mutual relationship of the users. Second, it leverages the social graph to identify non-spammers. A URL whitelist is constructed by identifying the sharing activities of their posted URLs. Third, when user-link graph URL edges are filtered by the URL whitelist, UNIK makes most non-spammers become isolated or low-degree nodes on the user-link graph, which can be removed safely and left spammers node detectable. Figure 5 depicts the workflow of UNIK and Figure 6 shows an example on the edge trimming in separating non-spammers from spammers.
UNIK first tries to identify non-spammers by applying the SD2 algorithm to the social graph only .Incontrast, the standalone SD2 algorithm detects spammers by using the combination of the social graph and the user-link graph, as the latter is required to connect spammers. Since non-spammers are mostly active in the social graph, we do not need to combine the two graphs in this step.

As shown in the last section, SD2 might have a non-trivial false negative rate in detecting spammers upon a sybil attack to the social graph. Therefore, the identified non-spammers are not 100% correct. Fortunately, UNIK manages to toler-ate the error in the identified non-spammers list as shown in follows.

Based on the identified non-spammers list, UNIK gen-erates a whitelist covering the URLs in identified non-spammers X  posts, so that more non-spammers can be iden-tified with the whitelist. However, if a spam URL is in-correctly included in the whitelist, the spammers sharing this spam URL may be removed from the user-link graph, causing false negatives. This situation is even worse for those widely shared URLs. To deal with such situations, UNIK uses the identified non-spammers and the user-link graph to help detect such spam URLs. For a URL shared among users, we have more confidence to include it on the whitelist if more than half of the users sharing this URL are non-spammers. That is, UNIK requires shared URLs to meet this condition to avoid inclusion of spam URLs in the whitelist. Note that the whitelist can be built based on do-mains or hosts, other than the URL itself, because a wider coverage of the whitelist is able to decrease false positives while errors in the whitelist only lead to the increase of false negatives.

Based on the generated whitelist, UNIK examines the edges on the user-link graph. Shared URL edges in the user-link graph are trimmed if they match the whitelist. After these removals, non-spammers who only share whitelisted URLs become isolated on the user-link graph because all their edges are trimmed. Thus, they can be removed, and the remaining nodes on the user-link graph are mostly spam-mers, with a limited number of non-spammers whose edges are mostly trimmed.
The trimmed user-link graph may include some non-spammers because the whitelist is not likely to cover every non-spam URL. To further improve the detection perfor-mance, UNIK aims to remove as many non-spammers as possible based on the URL sharing properties of spammers. Typically, the URL sharing activities of spammers are much more intensive than non-spammers in terms of the num-ber of shared URLs or the number of sharing users. This means that a spammer node often has more edges than a non-spammer node in the user-link graph. UNIK thus ex-amines the node degree, and detects users whose degree is beyond a threshold as spammers.

To compute the node degree on the trimmed user-link graph, intuitively, edge(s) should exist between every two users sharing URL(s), and the edge weight is set to 1. This however has a time complexity of O ( n 2 )asalltheusers sharing a URL are fully connected. To reduce the process-ing time, instead, for each shared URL, UNIK organizes the users sharing the URL into a circular node list, and only adds two edges to a node, one connecting the node to its preceding node and the other connecting it to its succeeding node, with each edge weight set as half of the number of other sharing users. By the increase of the edge weight, for each shared URL, the sum of edge weight of a node increases with the same amount as in a fully connected graph, which is the number of other sharing users. In this way, the user-link graph is quickly constructed with a linear number of edges connecting nodes, while the sum of edge weights of each node exactly the same as the node degree in a fully connected graph weight, UNIK applies a heuristic threshold on the sum of edge weights, below which the connected nodes are deemed as non-spammers and get removed. In the end, only spam-mers exist on the user-link graph, possibly with few non-spammers who have intensive URL sharing activities as well.
The threshold of the node degree or the sum of edge weights needs to be determined beforehand for UNIK to work. In the next section, we will show that such a thresh-old can be independently determined (Figure 7(b)). For the best spam detection result, this threshold can be estimated by gauging a small sample of spammers comparing to non-spammers from time to time.
 Algorithm 1 UNIK spam detection
The UNIK spam detection algorithm is shown by Algo-rithm 1.
We have evaluated the unsupervised spam detection scheme UNIK with our dataset shown in Section 2.1. Ta-ble 1 shows the statistics of the social graph and user-link graph (constructed with a linear number of edges) built with the dataset.

As UNIK applies whitelist and edge-weight-sum threshold to detect spammers, both of which can have di ff erent choices and both of which can be used independently, we first eval-uate their di ff erent choices individually in order to find the best suitable ones. (a) Applying di ff erent types of whitelist only Figure 7: Whitelist and edge-weight-sum threshold evaluation
We first evaluate the spammer detection e ff ectiveness by only applying di ff erent types of whitelist, including URL-based, host-based, and domain-based whitelist. We also evaluate Host+1Path based whitelist, where the whitelist matches the hostname and the first path in the URL. For example, if the whitelist pattern is http://a.com/service , then URLs like http://a.com/service/1.html or http://a.com/service?q=1 will be matched. Host+1Path whitelist is a hybrid between URL-based and host-based whitelist. Lastly, we evaluate the approach of not using whitelist to trim edges, but only removing identified non-spammer nodes, namely the WhiteUser approach.
Figure 7(a) shows the spammer detection results after ap-plying the whitelist or WhiteUser only. Because WhiteUser only covers identified non-spammers and the URL-based whitelist has the narrowest coverage of non-spam URLs, the user-link graph still has a number of non-spammers remain-ing, which results the highest false positive rate, but the lowest false negative rate as most spammers are still re-maining. The domain-based whitelist and the host-based whitelist further decrease the false positive rate, since the coverage of non-spammers increases. However, any errors in the whitelist may result in a higher false negative rate as the spammers will be removed from the graph. The Host+1Path based whitelist outperforms the URL-based whitelist in terms of the false positive rate, and its false negative rate is also smaller than that of the host-based whitelist. Fig-ure 7(a) also shows that if we include all URL hosts of identi-fied non-spammers in the whitelist (all-host-based whitelist), the false negative rate increases substantially. This indicates that it is necessary to check whether the URL is mainly shared among identified non-spammers to avoid the inclu-sion of spam link in the whitelist.

We then evaluate applying the edge-weight-sum threshold alone on the original user-link graph to detect spammers. The threshold increases exponentially (base 2) from 1 to 16,384, and nodes are detected as spammers with the sum of their edge weights larger than or equal to the threshold. Figure 7(b) shows that applying this threshold alone could detect spammers with a false positive rate of 10.7% and a false negative rate of 3.6% when the threshold is set to 256. Although the performance of applying the threshold alone is not satisfactory, it does show that using this threshold can help improve the detection performance. Figure 8: Evaluation of Whitelist and Edge-weight-sum
We now evaluate the e ff ectiveness of UNIK with the help of the whitelist and the edge-weight-sum threshold together. Figure 8(a) shows the spammer detection re-sults. The Host+1Path based whitelist detects spammers (a) Applying the edge-weight-sum threshold (256) before or after applying the whitelist with a false positive rate of 0.6% and a false negative rate of 3.7% when the edge-weight-sum threshold is 256. The host-based whitelist has a false positive rate of 0.5% and afalsenegativerateof4.0%withthesamethreshold. In contrast, WhiteUser and the all-host-based whitelist show worse performance. For WhiteUser, only non-spammers identified based on the social graph are removed from the user-link graph, and no edges are further removed for other non-spammers. As a result, applying the edge-weight-sum threshold can only help improve the performance to a lim-ited extent.

Figure 8(b) shows the distribution of the sum of edge weights for spammers and non-spammers in the user-link graph using the host-based whitelist. Most spammers have a sum of more than 64, while more than 70% of non-spammers have a sum less than 64 before the edge trimming. After the edges are trimmed by the whitelist, more than 80% of non-spammers have a sum less than 64. This means the whitelist could help further di ff erentiate spammers with non-spammers when applying the edge-weight-sum thresh-old. Note that although the threshold cuto ff still marks 20% of non-spammers in the user-link graph as spammers, the small number of non-spa mmers remaining in the graph results in a small number of false positives in the spam detec-tion. Figure 9(a) shows that if we apply the edge-weight-sum threshold earlier before applying the whitelist, the perfor-mance is worse than applying the threshold after applying the whitelist. This further validates that trimming edges by the whitelist in advance is helpful to applying the edge-weight-sum threshold in detecting spammers. On the other hand, WhiteUser does not trim any edge, so it is indi ff erent to applying the threshold earlier or later.
Our UNIK scheme can detect spammers with a false posi-tive rate of 0.6% and a false negative rate of 3.7%. In terms of the spam post being detected, the false positive rate is 3.7% and the false negative rate is 1.0% as shown in Fig-ure 9(b). This suggests that UNIK is able to achieve the same level of detection performance as SD2. Figure 9(b) also shows that FBCluster [6] has high false positives. When we apply the suggested (5, 1.5 hrs) thresholds, the false posi-tive rate is 39.3% while the false negative rate is 0.2%. We also have evaluated the AutoRE algorithm used in [19] by replacing the AS with the user account, which however has a26.8%falsenegativeratewhenapplyingthesuggested20-AS and 5-day thresholds. Section 2 shows that tuning the threshold for FBCluster and AutoRE still cannot improve their performance.
Although UNIK works well in the evaluations with our dataset, its e ff ectiveness is still subject to other possible en-hanced attacks launched by spammers. To investigate the robustness of UNIK under attacks, we first evaluate UNIK performance by launching sybil attacks to the social graph of our dataset. To do so, we randomly select 10% of users in the social graph, and add the same number of spammers connecting to these users, forming a scale free social network among them. Then we double the number of spammers con-nected to the social network.
 Figure 10(a) shows the impact of this sybil attack on UNIK spammer detection performance. We observe that the host-based whitelist is subject to sybil attacks: the false negative rate increases substantially when the sybil attack increases. This is because an increased level of sybil attacks increases errors in generating the host-based whitelist, trim-ming corresponding spam URL edges, and resulting in false negatives. However, if the whitelist is based on Host+1Path , then the errors are limited to URLs strictly matching the prefixes, which e ff ectively limits the false negatives. In fac-ing such attacks in the social graph, we should choose to limit the whitelist coverage for better performance. The false positive rate is 0.6% and the false negative rate is 4.3% for Host+1Path based whitelist with the edge-weight-sum threshold of 256, when the sybil attack intensity is 20% of social network users. This indicates that UNIK overcomes the limitation of SD2 when sybil attacks increase in the so-cial network.
Similar to the sybil attack occurred in the social graph, spammers can include legitimate URL in the spam so that they can escape from being detected. Such inclusion in-creases the connectivity of the spammers and non-spammers in the user-link graph. For this kind of attacks, spammers need to crawl legitimate URLs of non-spammers in the user-link graph, which is often limited in crawling speed or scope.
We simulate legitimate URL inclusion attack to our dataset by assuming the spammers have made the e ff ort crawling a major portion of total legitimate URLs in the user-link graph, and have inserted the crawled legitimate URLs into every spam posts. Since the total number of legitimate URLs is much larger than that of spam URLs, each spam post in this simulated attack only contains a mi-nor fraction of spam URLs while the majority of URLs are legitimate.

Figure 10(b) shows the evaluation results of UNIK under such attacks. For a legitimate URL being attacked, it will not be included in the whitelist, as the majority of sharing users of that URL are mainly spammers after the attack. However, the host or Host+1Path prefix of the URL may still be included in the whitelist, because it is unlikely that other URLs with the prefix are all attacked. Even if the URL prefix is not included in the whitelist, the edges of attacked non-spammers will still be trimmed by the whitelist if not all URLs are attacked. As shown in Figure 10(b), the host-based whitelist shows slightly better performance due to its wider coverage which is harder to be defeated by the attack. The Host+1Path based whitelist still has a false positive rate of 2.7% and a false negative rate of 1.9% (applying the edge-weight-sum threshold of 256) even if 50% of legitimate URLs are attacked by spammers. In summary, UNIK works well under legitimate URL inclusion attacks for di ff erent kinds of whitelist.
We have shown the promising performance of UNIK eval-uated with our social blog dataset. And we also have shown the stable performance of UNIK facing di ff erence scales of spam attacks. However, because UNIK is based on applying sybil defense scheme to detect spammers, UNIK has limi-tations in facing some types of spam attacks. For example, we have seen reports from [6] and [7] that a high percentage of spammers are using compromised user accounts to post spam in private social networks. The reason is that in pri-vate social networks, users primarily interact with a small list of other users, so spammers have to compromise nor-mal users X  accounts to widely promote their spam content. In this case, UNIK will have trouble to detect such com-promised accounts as spammers since they are an integral part of the social graph. Therefore, UNIK is more suitable for fighting spam in an open social network such as groups, forums, or blogs, where the spamming activities are more intensive.

UNIK also needs to address the issue of URL shorten-ing that is widely used in Twitter-like social networks. In facing shortened URLs, UNIK may need to fetch the final destination redirected by such a URL, so that the user-link graph can correctly represent the link sharing activities of di ff erent users. This increases the complexity of the sys-tem implementation of UNIK by introducing the cost of re-solving URL redirects. In practice, we have seen systems incorporated such mechanisms in fighting social spam with reasonable cost [17].
Based on the UNIK scheme presented in the last section, we are able to group spammers into di ff erent clusters in the user-link graph in our dataset. Each of these spammer clus-ters represents a group of spammer accounts that are inter-connected with shared URLs in their posts, corresponding to di ff erent spam campaigns. Studying the spammer clus-ters can enable us to understand the patterns of di ff erent spam campaigns, and can also help develop spam signatures in fighting against future spam.

We first plot the number of spammer cluster sizes in Fig-ure 11(a). Interestingly, the cluster size distribution follows power law, indicating that spam campaigns also have a nat-ural distribution on their sizes. We are interested in the largest clusters, so we pick the top-4 clusters to study their characteristics. The top-4 clusters have 63699, 6634, 3159, and 724 spammer IDs, respectively. Figure 11(b) shows the number of new accounts created over time for each of the top-4 clusters. We observe that in #1 cluster new accounts were continuously created over the trace duration, while new accounts were created at di ff erent periods in other clusters. This clearly indicates that di ff erent spammer clusters have di ff erent activities patterns. There exists some correlation between the #1 cluster and the #3 cluster as their ID cre-ation activities are both bursty around day 50. (a) Cluster size distribution Figure 11: Spammer cluster size and activity over time
We also study the user activities of each spammer ID in the top-4 clusters. Figure 12(a) shows the median of posting interval for each user in the top-4 clusters. The #4 cluster demonstrates the shortest interval distribution: more than 90% IDs have a median of interval within 1 minute. On the contrary, the #3 cluster shows the longest interval distri-bution: most IDs have a median of interval larger than 40 days. Figure 12(b) shows the active duration of each ID in the top-4 spammer clusters. The #4 cluster shows a very short active duration while the #2 cluster shows a median duration of 100-day per user ID. The active duration distri-bution of #3 cluster shows three discretely stages including 0, 50, 137 days. The significant di ff erences between di ff er-ent clusters imply that the behavior of spammer ID varies substantially. Therefore it is highly di ffi cult to capture the spammer behavior with only a handful patterns. Figure 13: Top-4 clusters: host sharing intensity
Figure 13 shows the sharing intensity of each host in the top-4 clusters. The sharing intensity of a host is defined as the number of users that ever posted link(s) pointing to the host, which captures the intensity of advertising activities of aspamhost. The#2clustershowsthestrongestsharing intensity of the advertised spam hosts, while the other clus-ters show similar intensity distributions. This is correlated with the longest active duration distribution of # 2 cluster as shown in Figure 12(b). This finding implies that the host sharing intensity is increased with the increase of spamming activity by spammers over time.
Spam in online social networks has been actively studied through measurement-based analysis recently. For example, Thomas et al. [17] proposed a real-time system Monarch to detect spam based on URL-related features, and they found Twitter spam campaigns are long lasting than email spam campaigns. Grier et al. [7] analyzed spam in Twitter with public blacklists, and showed that 8% of all URLs are on popular blacklists, although the blacklists are too slow in preventing the damage of harmful URLs. Gao et al. [5] pro-posed social network features such as social degree for online spam filtering based on Facebook and Twitter datasets.
Several other approaches have been proposed to detect spam in emails with spam templates, network-level features, shared IP addresses, or email target domains. Qian et al. [14] proposed an unsupervised learning based email spam fil-ter since spam from the same campaign often contains un-changed textual snippets by using the same template. Hao et al. [8] studied using network-level features to distinguish email spammers from non-spammers as the first level de-fense since it is di ffi cult to maintain IP-based blacklists. BotGraph [21] evaluated a graph-based approach with the MapReduce model to cluster millions of bot-users observing that bot-users share IP addresses to log in or send emails. SpamTracker [15] tried to catch email spammers earlier than traditional IP-based blacklists do by using the target do-mains that a IP address sends emails to as a behavioral feature to cluster IP addresses.
Albeit a number of spam detection schemes have been de-signed, spam in online social networks increases significantly in recent years. In this study, we first analyze the limitations of existing representative schemes, and then design a sybil defense based spam detection scheme, SD2. SD2 is an unsu-pervised scheme, which outperforms existing unsupervised schemes significantly with the help of the social network graph. But it su ff ers from escalating sybil attacks to the so-cial network. Thus, we further propose UNIK that is highly robust to an increased level of spam attacks. UNIK di ff ers from existing schemes in that it detects non-spam URL pat-terns from the social network instead of spam URLs directly, because the non-spammer patterns are relatively more stable than spammers. UNIK demonstrates its performance with a10-monthsocialnetworkdataset. BasedonUNIKdetec-tion, we have also analyzed spammer clusters in the dataset, and find distinct spamming patterns of di ff erent spam cam-paigns, indicating the volatility of spamming patterns.
We thank the anonymous reviewers for their constructive comments. The work was supported in part by the Na-tional Science F oundation under gra nts CCF-0913050, CNS-1162165, CNS-0746649 and CNS-1117300. [1] http://nakedsecurity.sophos.com/2011/01/19/sophos-[2] Barracuda labs 2010 annual security report. [3] F. Benevenuto, T. Rodrigues, V. Almeida, J. Almeida, [4] G. Danezis and P. Mittal. SybilInfer: Detecting sybil [5] H. Gao, Y. Chen, K. Lee, D. Palsetia, and [6] H. Gao, J. Hu, C. Wilson, Z. Li, Y. Chen, and B. Y. [7] C. Grier, K. Thomas, V. Paxson, and M. Zhang. [8] S. Hao, N. Syed, N. Feamster, A. Gray, and S. Krasser. [9] K. Lee, J. Caverlee, and S. Webb. Uncovering social [10] J. Leskovec, K. J. Lang, A. Dasgupta, and M. W. [11] B. Markines, C. Cattuto, and F. Menczer. Social spam [12] A. Mohaisen, A. Yun, and Y. Kim. Measuring the [13] A. Pathak, F. Qian, Y. C. Hu, Z. M. Mao, and [14] F. Qian, A. Pathak, Y. C. Hu, Z. M. Mao, and Y. Xie. [15] A. Ramachandran, N. Feamster, and S. Vempala. [16] E. Tan, L. Guo, S. Chen, X. Zhang, and Y. E. Zhao. [17] K. Thomas, C. Grier, J. Ma, V. Paxson, and D. Song. [18] B. Viswanath, A. Post, K. P. Gummadi, and [19] Y. Xie, F. Yu, K. Achan, R. Panigrahy, G. Hulten, [20] H. Yu, P. Gibbons, M. Kaminsky, and F. Xiao. [21] Y. Zhao, Y. Xie, F. Yu, Q. Ke, Y. Yu, Y. Chen, and
