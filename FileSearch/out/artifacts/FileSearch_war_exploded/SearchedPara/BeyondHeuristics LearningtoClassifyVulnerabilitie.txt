 The security demands on modern system administration are enor -mous and getting worse. Chief among these demands, adminis-trators must monitor the continual ongoing disclosure of softw are vulnerabilities that have the potential to compromise their systems in some way. Such vulnerabilities include buffer overo w errors, improperly validated inputs, and other unanticipated attack modal-ities. In 2008, over 7,400 new vulnerabilities were disclosed X  well over 100 per week. While no enterprise is affected by all of these disclosures, administrators commonly face man y outstanding vulnerabilities across the softw are systems the y manage. Vulner -abilities can be addressed by patches, recongurations, and other workarounds; howe ver, these actions may incur down-time or un-foreseen side-ef fects. Thus, a key question for systems adminis-trators is which vulnerabilities to prioritize. From publicly avail-able databases that document past vulnerabilities, we sho w how to train classiers that predict whether and how soon a vulnerability is lik ely to be exploited. As input, our classiers operate on high di-mensional feature vectors that we extract from the text elds, time stamps, cross-references, and other entries in existing vulnerability disclosure reports. Compared to current industry-standard heuris-tics based on expert kno wledge and static formulas, our classiers predict much more accurately whether and how soon indi vidual vulnerabilities are lik ely to be exploited.
 C.2.0 [ Computer -Communication Netw orks ]: General X  Secu-rity and protection ; I.5.1 [ Patter n Recognition ]: Models X  tical ; I.5.2 [ Patter n Recognition ]: Design Methodology X  evaluation and selection Algorithms, Security supervised learning, SVM, vulnerabilities, exploits
Among the man y requests made of researchers in computer se-curity , few are as frequent or as urgent as the call for meaningful security metrics. The requests are dri ven by a widespread need to quantify security risks ( X ho w lik ely is it that an attack er will thw art my security measures? X ) in a way that informs operational polic y choices. Unfortunately , the adv ersarial nature of security has re-sisted traditional methods of quantifying risk and has even led some to argue that such metrics are inherently unattainable [4]. Ne ver-theless, even absent a comprehensi ve solution to this conundrum, there remains a need to evaluate the value of distinct operational security choices. Thus, a range of ad hoc approaches have emer ged in indi vidual domains where the needs are particularly acute. In this paper we focus on one such domain  X  evaluating vulnerability disclosures  X  and we sho w that it is possible to mak e meaningful predictions using tools from data mining and machine learning.
Public vulnerability disclosure has long been a staple of the softw are security industry , with man y thousands of new softw are vulnerabilities identied and publicized each year [11]. In turn, these vulnerabilities are communicated, via a variety of channels, to system administrators who must then determine if the y have sus-ceptible systems and decide what action to tak e if so. Unfortu-nately , patching and other mitigations can incur signicant man-power overheads (even more so for mission critical services that require quality assurance testing before deplo ying new softw are). Since few organizations have the resources to address every vulner -ability disclosure that might impact their enterprise, administrators must prioritize their efforts, triaging the most critical vulnerabilities to address rst.

To inform these decisions, a variety of  X vulnerability scoring X  frame works have been designed to assess risk qualitati vely (e.g., Microsoft' s critical, important, moderate, or low severity rating) or quantitati vely (e.g., US-CER T's severity metric). Indeed, one such frame work  X  FIRST' s Common Vulnerability Scoring Sys-tem (CVSS) [9]  X  appears to be emer ging as the de facto standard in the community . The use of CVSS is mandated in the Payment Card Industry' s Data Security Standard (PCI-DSS), it is the rank-ing used in NIST' s National Vulnerability Database (NVD), and its use is recommended by a wide range of computer , netw orking and softw are vendors (e.g., Cisco' s Risk Triage whitepaper [5]).
Ho we ver, while these systems are carefully designed using ex-pert kno wledge, the y are inherently ad hoc in nature. For example, the CVSS (v2) overall  X Base Score X  is expressed in terms of Impact (
I ) and Exploitability ( E ) components by: By con vention, this score is rounded to one decimal place, and it is set to zero (re gardless of the abo ve formula) if I = 0 further that the Impact and Exploitability components in eq. (1) are themselv es combinations of cate gorical magic numbers. (For ex-ample, the Exploitability component depends on an Access Vector score which tak es the value 0 : 395 if the vulnerability requires local access, 0 : 646 if the vulnerability requires adjacent netw ork access, and 1 : 0 if the vulnerability global netw ork access). While we have little doubt that these scoring metrics were carefully considered and of great value when rst developed, we suspect that any single x ed equation, such as eq. (1), is unlik ely to pro vide a rob ust and lasting model of vulnerability severity .

To this end, our paper seeks to place this problem on a more sys-tematic footing. Using tools from machine learning, we sho w how to train classiers that predict whether vulnerabilities are lik ely to be exploited, and if so, how soon. Our results suggest that our trained classiers are lik ely to outperform current measures of ex-ploitability . In particular , our classier outputs correlate much bet-ter with vulnerability outcomes than the  X Exploitability" compo-nent of the CVSS Base Score in eq. (1).
Softw are vulnerabilities are exploitable aws in softw are sys-tems that pose signicant security risks. Production softw are in-evitably ships with man y such aws, a subset of which are subse-quently disco vered and become kno wn over time. When aws are disco vered, vendors distrib ute patches and mitigations to their cus-tomers, who ideally implement such measures before an exploit is developed and tar geted against them. This vulnerability life-cycle (described in more detail by Arbaugh et al. [1]) has in turn dri ven the creation of a comple x ecosystem of players: vulnerabil-ity researchers, softw are and security vendors, security information pro viders and a range of netw orks, information sources and mar -kets connecting them together (for a comprehensi ve analysis of the vulnerability ecosystem see Frei et al. [10]).
At the end of this process, vulnerabilities are documented and disclosed to the public. These reports not only list various dis-crete attrib utes of each vulnerability (e.g., softw are affected, date disclosed), the y also describe (in plain text) how each vulnerabil-ity works, why it presents a threat, and how it can be mitigated. This information is disclosed to the public via multiple sources, in-cluding moderated forums (e.g., Bugtraq [21]), indi vidual vendors (e.g., Microsoft [14], commercial aggre gators (e.g., Secunia [20]), and open source databases (e.g., OSVDB [17]). Vulnerabilities are also quickly assigned a unique identier  X  both local to indi vidual information pro viders/repositories and global across multiple vul-nerability databases using MITRE' s Common Vulnerabilities and Exposures (CVE) service [6].

The precise timing of vulnerability disclosures depends consid-erably on how the y were found and the polic y of the organizations involv ed. Some vulnerabilities are disclosed immediately upon dis-covery while others may be kept pri vate for signicant periods of time to allo w vendors to develop and test appropriate patches and mitigations. Still other vulnerabilities are exploited before or at the same time as public disclosure (so-called 0-day exploits). Vulner -ability disco very and disclosure polic y has generated a great deal of both contro versy and research [2, 3, 18, 16]. A number of stud-ies have also examined the probability that vulnerabilities are able to be patched [15, 19]. By contrast, our work focuses on predict-ing if a vulnerability is lik ely to be exploited shortly (thus meriting immediate attention from system administrators).
To aid system administrators, vulnerability disclosures typically include a qualitati ve or quantitati ve assessment of each vulnera-bility' s severity . The overall severity score depends on both pact (ho w signicant are the consequences of exploitation) and ploitability (ho w dif cult is the vulnerability to exploit). Severity scores are deri ved primarily from expert kno wledge and/or commu-nal input. For example, US-CER T generates a quantitati ve sever-ity score ranging from 0 to 180, calculated directly from answers to a range of qualitati ve questions (e.g.,  X Is information about the vulnerability widely available or kno wn? X  and  X What are the pre-conditions required to exploit the vulnerability? X ) [7]. Microsoft' s Security Bulletin documents vulnerability severity using a quali-tati ve scheme (critical, important, moderate, or low) as do Secu-nia' s reports (extremely critical, highly critical, moderately criti-cal, less critical, or not critical). More recently , a group of vendors and researchers came together , under the sponsorship of the Fo-rum of Incident Response and Security Teams (FIRST), to create a new severity metric, the Common Vulnerability Scoring System (CVSS). No w in its second iteration, CVSS denes several inde-pendent metrics, but it is the  X base metric X  which is typically used in third-party vulnerability databases. This score combines impact and exploitability components according to a carefully designed formula [9].

Unfortunately , each of these systems measures dif ferent things and weights them in dif ferent ways. We are una ware of any em-pirical study evaluating the effecti veness of any of these metrics or comparing them to one another . Thus, it is hard to mak e con-crete statements about which approach is best, or why . Indeed, this problem is inherently dif cult since some aspects of  X se verity X  are either conte xt dependent (e.g., a mission critical serv er being shut down may be more  X se vere X  than a print serv er) or may be inher -ently dif cult to quantify . Ho we ver, the issue of exploitation is far more clear cut  X  a vulnerability is either exploited or not and the date upon which a working exploit becomes kno wn is frequently documented. Thus, in this paper we focus on the exploitability as-pect of severity .

Given this scope, we argue that existing scoring systems are probably too limited to offer strong predicti ve power . The y in-clude only a few factors in each vulnerability' s assessment X which may not be the key distinguishing features and frequently depend on the judgment of evaluators X and the y combine these features in the same way to produce a score for widely dif ferent sorts of vul-nerabilities. For example, the current CVSS Exploitability score is calculated as follo ws [13]:
Exploitabilit y = 20 AV AC Authen tication (2) where AV stands for Access Vector and AC stands for Access Complexity , and each of these variables are assigned particular x ed values based on other qualitati ve or subjecti ve eval-uations. For example, AC is set to 0.35 if access comple xity is deemed to be  X high X , 0.61 if  X medium X  and 0.71 if  X lo w X . It is not entirely clear how this formula or its constants were designed. Moreo ver, it seems unlik ely that this simple formula can model the probability of exploitation across man y dif ferent sorts of vulnera-bilities.
In this study , we use two well-kno wn, online sources of vulnera-bility data, the Open Source Vulnerability Database (OSVDB) [17] and the MITRE Common Vulnerabilities and Exposures (CVE) database [6].
OSVDB is a lar ge database containing reports on over 57,000 vulnerabilities. As an example, gure 1 sho ws the OSVDB report for a vulnerability in a Web services library . These reports contain a wealth of information about each vulnerability , inde xed using a unique OSVDB ID, including a detailed description, technical de-tails, the softw are products affected, solutions (such as patches and mitigations to pre vent exploitation), and references to other sources of information about the vulnerability . The reports also include classication information, such as the type of attack required to exploit the vulnerability , the origins from which an attack can be launched, and the manner in which the vulnerability was disclosed. Section 4.1 describes how we extract information from these re-ports as features for classication and prediction.

The reports also pro vide temporal information for each vulner -ability , such as the dates of disco very , disclosure, and rst kno wn exploits. We augment this data with additional temporal informa-tion pro vided by Frei et al. as described in their WEIS 2009 pa-per [10]. As described belo w, we use this additional information to create labeled training and test sets.

From the OSVDB database we extracted a lar ge set of vulner -abilities for classication and prediction. We used only vulnera-bilities that were disclosed during the years 1991 X 2007, inclusi ve. Vulnerabilities before 1991 represent a dif ferent era of softw are; we excluded later vulnerabilities because, at the time we started the project, the y were recent and still in ux (e.g., man y of them had undetermined outcomes). We also excluded vulnerabilities that did not have a description.

Table 1 sho ws the number of vulnerabilities we use in our exper -iments and how we label them. It cate gorizes vulnerabilities using their OSVDB  X Exploit Classication X  status. If a vulnerability has an available, rumored, or pri vate exploit, we label it as a  X positi ve X  vulnerability , indicating that it has been exploited. Similarly , if a vulnerability has no kno wn exploits or exploits are una vailable, we label it a  X ne gati ve X  vulnerability indicating that it is not exploited. If a vulnerability report does not classify its exploit status, we ex-clude it from consideration since we cannot determine the accurac y of our predictions. Section 4.2 describes how we train a classier from these labeled examples of vulnerabilities.

We use the CVE database to augment the vulnerability reports from the OSVDB database. Similar to OSVDB, CVE entries in-clude summaries, references to related products and reports, infor -mation about the type of vulnerability , time stamps, and severity scores. In addition to pro viding more information that can be ex-tracted as features for classication, for some vulnerabilities the CVE entries also pro vides information missing in the OSVDB re-ports. We inte grate these records by cross-referencing their CVE and OSVDB identiers. Most OSVDB reports reference the cor -responding CVE reports for the same vulnerability and con versely , some CVE entries have corresponding OSVDB IDs as well.

Finally , we note that the quality of our results are inherently tied to the quality of this disclosure data and in particular the quality of the temporal labels (when a vulnerability was disclosed and ex-ploited). This creates two potential classes of problems. In princi-ple, there are adv ersarial training risks since bad vulnerability data could inuence what the classier learns during training. Ho we ver, we belie ve this is a particularly unlik ely scenario since vulnerabil-ity databases are generated by lar ge numbers of independent ac-tors. It seems unlik ely that an adv ersary would disco ver and dis-close enough new vulnerabilities (in turn validated and accepted by third parties) to inuence the overall feature set used in training. Similarly , while an adv ersary might try to  X game" our predictions (e.g., by only exploiting vulnerabilities which we had classied as unlik ely to be exploited), the risk seems low, and certainly such a counterstrate gy is no easier than it is under current vulnerability scoring systems. A some what more lik ely limitation is systematic bias. In particular , we note that lar ge numbers of vulnerabilities in the complete database have unkno wn exploitation status or dates, which limits our ability to train on these records. In this work, we assume that the remainder of disclosures (with kno wn status and dates) are representati ve and accurate. A selection bias would emer ge if the omitted records were systematically dif ferent than complete records; howe ver, we do not belie ve such a bias exists.
We aim to impro ve on existing approaches by casting vulnera-bility classication as a problem in machine learning. In a nut-shell, our goal is to replace small-scale heuristics by lar ge-scale statistics. This section describes our statistical model for vulnera-bility classication. The model is estimated from a lar ge database of vulnerabilities that have been labeled as  X exploited" or  X not ex-ploited". Section 4.1 describes how we extract information from this database and distill it into feature vectors for classication, and section 4.2 describes how we classify these feature vectors using support vector machines [22]. The training and test sets of fea-ture vectors in our experiments are available at http://www. sysnet.ucsd.edu/projects/exploit-learn/ . Table 2: Extracted featur es from the vulnerability data. (B) denotes binary and (I) denotes integer featur es.
Our database of vulnerabilities contains a wealth of informa-tion, both factual and textual, about their histories and distinguish-ing characteristics. For each vulnerability , we extract a high-dimensional ( d = 93578 ) feature vector of binary and inte ger -valued features. Though man y of these features will ultimately turn out to be irrele vant or redundant for classication, the goal of our feature extraction is to distill as much information as possible for subsequent statistical analysis.

Much of our information about vulnerabilities is contained in text elds. We deri ve binary features using a bag-of-w ords repre-sentation for each text eld [12]. Essentially , these features record whether or not particular tok ens (e.g.,  X buffer",  X heap",  X DNS") ap-pear in specic text elds (e.g.,  X title",  X solution",  X product name") associated with each vulnerability .

Table 2 sho ws the breakdo wn of features that we extract for each vulnerability in our database. Each row in the table indicates the number of features deri ved from a particular type of information. Most of the features are generated from bag-of-w ords representa-tions of text elds. Ho we ver, inte ger -valued features also encode useful information, such as the date when a vulnerability was rst disclosed, the length of text describing its symptoms, or the ranking of its severity according to other popular heuristics.
We build classiers by training linear support vector machines (SVMs) [22] on the feature vectors described in the pre vious sec-tion. (As preprocessing, howe ver, the non-binary features are nor -malized to lie between zero and one so that the y do not overshado w the binary features.) Linear SVMs are trained by computing the maximum mar gin hyperplane that separates the positi ve and nega-tive examples in feature space. The decision rule mapping feature vectors x 2 &lt; d to labels y 2 f 1 ; +1 g is given by: where w 2 &lt; d is the normal (weight) vector to the separating hyperplane and b is the distance of the separating hyperplane from the origin.

Linear SVMs are particularly appropriate for our application to vulnerability classication because we have man y more input fea-tures ( d ) than training examples ( n ). In particular , for the experi-ments in section 5, the ratio of features to examples is never less than 10-to-1. In this regime of small sample size ( n d ) are man y hyperplane decision boundaries that can perfectly sepa-rate all n examples f ( x i ; y compute the hyperplane that (roughly speaking) maximizes the dis-tance of the most borderline training examples to the linear decision boundary . This hyperplane is not only uniquely specied, but a lar ge body of work in statistical learning theory also sho ws that it generalizes better to new data, yielding lower expected error rates when used to classify pre viously unseen examples [22]. Man y soft-ware packages are available for tting models of this form; for the results in this paper , we used the LIBLINEAR implementation of SVMs [8].
In this section we present our experimental results using SVMs for vulnerability classication. We consider several dif ferent sce-narios. We rst evaluate the prediction accurac y in an ofine exper -iment, representing a best-case scenario where we consider the data set of vulnerabilities as a single, static snapshot; we also examine the features that have the most prominent role in making predic-tions. We then evaluate the prediction accurac y of our model in an online experiment emulating a real-w orld deplo yment: we dynam-ically update the classier and mak e predictions over time as new vulnerabilities appear . We also use SVMs to predict if vulnerabil-ities will be exploited within a particular time frame. Finally , we compare the results from SVMs to current heuristic approaches to vulnerability classication.
As discussed in Section 3, we use the OSVDB and CVE databases as our data set of vulnerability examples. In addition to pro viding the features we use for learning and classication, the y also pro vide the ground truth for evaluating the accurac y of our classiers. In general, we label those vulnerabilities that have exploits as positi ve examples and vulnerabilities that do not have exploits as negati ve examples. Table 1 sho ws a breakdo wn of the vulnerabilities based on these labels.

Note that there are more positi ve examples (10,020) than nega-tive examples (3,745), i.e., more vulnerabilities with exploits than those without. When conducting balanced experiments, which re-mo ve any such bias in the data used for classication, we randomly choose the same number of examples from both sets multiple times and average the results. When conducting unbalanced experiments, an una voidable aspect of practical deplo yments, we explicitly quan-tify and report the bias in the input data. Finally , a subset of the vulnerabilities do not have exploit information; we do not use these examples in our experiments because, without true labels, we can-not evaluate the accurac y of classication.
In our rst experiment, we evaluate how well SVMs classify vul-nerabilities in an ofine setting. For this experiment, we use a bal-anced data set of roughly 4000 positi ve and negati ve examples X  that is, divided almost evenly between vulnerabilities with and without exploits. We train SVMs on 40% of these examples (as training data) and evaluate them on 50% of these examples (as test data). We use the remaining 10% of examples as a development set to choose tuning parameters for the SVMs. We report averaged results from ten-fold cross validation: that is, we learn ten dif ferent classiers, randomly choosing which examples fall into the train-ing, test, and development sets, then average the results across all runs.

Table 3 sho ws the results. Here, true positives are positi ve ex-amples correctly classied as vulnerabilities that will be exploited; true negatives are negati ve examples correctly classied as vulner -abilities with no kno wn exploits; false positives are positi ve exam-ples incorrectly classied as vulnerabilities that will be exploited, but were not; and false negatives are negati ve examples that were incorrectly classied as vulnerabilities that have no kno wn exploits, but in fact do. The overall accurac y is nearly 90%, demonstrating the viability of statistical methods for vulnerability classication.
The ofine classication results sho w that the vulnerability re-ports contain useful features for predicting whether a vulnerability will be exploited. We now examine which features play a promi-nent role in these predictions. In the linear SVMs that we use, the decision rule in eq. (3) multiplies each feature by a positi ve or neg-ative weight. Recall that all features are normalized to lie between zero and one before the weights are learned. Thus the magnitudes of these weights reect the relati ve contrib ution of each feature to the decision rule.

Tables 4 and 5 sho w the top 10 features with the highest positi ve and negati ve normalized weights, respecti vely , from the experiment in Section 5.2. Positi vely weighted features suggest to the classier that the vulnerability has an exploit; negati vely weighted features suggest to the classier that the vulnerability does not. The rst column lists the feature family (Table 2) and the second column the specic feature in the family . For example, the feature  X Refer -ences: BUGTRA Q ID X  in the rst row of Table 4 corresponds to the tok en  X B UGTRA Q ID X  appearing in the  X References X  feature family of a vulnerability report. The third column lists the num-ber of vulnerabilities N data. For example, the feature  X References: BUGTRA Q ID X  oc-curs in 2,045 of the 3,100 vulnerabilities in the training set. The fourth column lists the normalized weight of the feature. The nor -malized weight ~ w the classier multiplied by the ratio of the number of vulnerabili-ties N Table 4: Top 10 featur es with the highest positi ve normalized weights, and the number of vulnerabilities N j in which they appear in the training set. Featur es prefixed with  X  X VE X  are deri ved from CVE entries, otherwise they come from OSVDB reports. of vulnerabilities in the training set ( N = 3100 ). By sorting the normalized weights, we reveal the features with the lar gest overall effect across all vulnerabilities (as opposed to the lar gest effect on a possibly miniscule number of vulnerabilities).

The features in Table 4 are those that suggest most strongly to the classier that a vulnerability will be exploited. Man y of them cor -respond to the number of tok ens in particular feature families, such as the  X Authors X ,  X T itle X , and  X Notes X  sections of the vulnerabil-ity reports. These weights suggest that vulnerabilities with exploits generally have longer reports in the vulnerability databases than those without exploits; when looking at the vulnerability reports manually , we nd that this situation is indeed the case. Other top features are references to other security databases, suggesting that vulnerabilities with exploits are often track ed by multiple sources. Finally , we note that there are man y features with positi ve weights beyond those in Table 4. The top 100 features span nearly all of the feature families listed in Table 2; in other words, there are useful features in all parts of the vulnerability reports.

The features in Table 5 are those that suggest most strongly to the classier that a vulnerability will not be exploited. Among these features, we observ e two trends. First, we see that multiple fea-tures measuring the passage of time have strongly negati ve weights. Thus it appears that vulnerabilities with  X dusty" reports are less lik ely to be exploited. Second, we see that vulnerabilities whose product-related elds are undened ( X Full Product Name X ,  X Prod-ucts X ,  X V endors X ) also appear less lik ely to be exploited. Presum-ably , such  X incomplete X  reports indicate vulnerabilities that have not recei ved much attention from the community (nor from attack-ers).
The ofine experiment in Section 5.2 sho wed the potential for learning to classify vulnerabilities that were randomly divided into training, test, and development sets. In a real-w orld deplo yment, howe ver, system administrators would train the classier on kno wn vulnerabilities to mak e predictions about new ones. Moreo ver, as time goes, the kno wledge that vulnerabilities have or have not been exploited can be used to create new training examples. We can then extend the training set with these new vulnerabilities and learn a new classier based on the most up-to-date information. This process can continue indenitely as time progresses. Last Modied Date Table 5: Top 10 featur es (including ties) with the lowest nega-tive normalized weights, and the number of vulnerabilities in which they appear in the training set. Featur es prefixed with  X  X VE X  are deri ved from CVE entries, otherwise they come from OSVDB reports.

In our next experiment we emulate this online scenario using the time-stamp information in our vulnerability databases: 1. Initialize a baseline classier  X  Consider all vulnerabilities f V g t 0 reported between time 0 and time t that have kno wn out-comes (exploited or not) represented with labels f L g t baseline classier C labels f V; L g t 2. Predict exploited vulnerabilities that appear in the next time interv al  X  Suppose new vulnerabilities f V g t + T tion of T time elapses. We can use C for these examples. 3. Update with kno wn vulnerability outcomes  X  Once we kno w whether or not vulnerabilities are exploited, we kno w the true labels f L g t + T t +1 of the vulnerabilities. With their true labels, we can now include these vulnerabilities f V g t + T and reb uild a new classier C 4. Calculate error  X  We also calculate the cumulati ve error of the classier C number of predicted labels f L 0 g that dif fer from their true labels f L g in that interv al, and sum all of the counts across all interv als. We then divide the sum by the total number of vulnerabilities seen up until that time. Calculating the cumulati ve error sho ws the sta-bility of the classier over time.

Figure 2 sho ws the results of this experiment. We train a classi-er starting at January 2005 and initialize it with all prior vulner -abilities appearing before 2005. We then emulate the appearance of vulnerabilities and online reclassication and prediction through December 2007 (the end of our data set). We evaluate two update interv als T , once a week (Figure 2a) and once a month (Figure 2b). We sho w three curv es for the total classication error as well as the false positi ve and negati ve rates over time.

These results sho w that, after initial uctuations, the classier stabilizes and impro ves its accurac y with more examples over time. At the end the classier has an overall error rate of 14%, a false neg-ative rate of 9% and a false positi ve rate of 5%. Further , classi-cation accurac y is relati vely insensiti ve to the update period: when Figur e 2: Cumulati ve err or, false negati ve, and false positi ve per centages for predicting whether vulnerabilities will be ex-ploited in an online, deploy ed setting . We evaluate tw o time in-ter vals for updating the classifier , every (a) week and (b) month. the classier stabilizes, the weekly and monthly results dif fer very little. These results demonstrate the viability of deplo ying a clas-sier in an online setting to predict whether vulnerabilities will be exploited.
Ne xt we use SVMs to predict other metrics that help assess the severity of vulnerabilities. In practice, in addition to kno w-ing whether a vulnerability will be exploited, it is also useful to kno w how soon it will be exploited. (Ev en if all vulnerabilities will eventually be exploited, it is valuable to kno w when.) With this kno wledge, softw are vendors can prioritize the patches the y release; system administrators can similarly prioritize the installa-tion of these patches.

In general, there are three kinds of time-dependent exploits: positi ve-day exploits where an exploit is reported after the vulnera-bility is disclosed; 0-day exploits where an exploit is reported at the same time that the vulnerability is disclosed; and negati ve-day ex-ploits where the exploit precedes the vulnerability disclosure date (e.g., an attack er exploits a vulnerability before the softw are ven-dor realizes the existence and nature of the vulnerability). Ideally , for each vulnerability , we would lik e kno w the probability distrib u-tion over days when it will be exploited. Such a distrib ution cannot Table 6: Pr edicting whether vulnerabilities will be exploited within t days. be modeled by SVMs, which are designed for binary classication. Ho we ver, we can use SVMs to mak e similarly rele vant predictions.
Ne xt we use SVMs to predict whether or not a vulnerability will be exploited within some time t , where t is the dif ference between the exploit and disclosure dates of the vulnerability . To train such SVMs, we use the same examples as before, merely altering the labels to reect whether a vulnerability has been exploited within some time frame (as opposed to whether it has been exploited at all). The set of  X positi ve X  examples P contains all vulnerabilities with positi ve-day exploits that are exploited within time of  X ne gati ve X  examples N contains vulnerabilities with positi ve-day exploits that are not exploited within time t . (Those that have 0-day or negati ve-day exploits need no prediction since their reports arri ve with the vulnerability already exploited.) We then evaluate a range of time frames for t , from two days to one month.
For this experiment, we used an additional source of information with more accurate dates of vulnerability events. (Unfortunately , the OSVDB database has mix ed-quality date information.) From his recent work developing a detailed empirical model of the vul-nerability disco very , disclosure, and patch process [10], Stef an Frei generously shared the date information on vulnerabilities with CVE identiers from his carefully collected data sets. We incorporated his data on disco very , exploit, and disclosure dates for the vulnera-bilities contained in our data sets (Table 1).

To evaluate the accurac y of predicting time-to-e xploit for vulner -abilities, we perform both ofine and online experiments similar to those in Sections 5.2 and 5.4. In the ofine experiment we train and test classiers on the entire data set, and in the online experiment we retrain the classiers and mak e predictions on vulnerabilities over time.

Table 6 sho ws the results of the ofine experiment. As in Sec-tion 5.2, we partition the examples into training and testing sets and report averaged results from cross-v alidation with ten dif fer -ent random partitions. For each experiment in the table, we sho w the predicted time frame t , the number of positi ve j P j ative j N j examples, the accurac y max( j P j ; j N j ) = ( j P j + j N j ) the def ault classier that always predict the dominant label, and the accurac y from SVMs. The predictions from SVMs are 75 X 80% ac-curate across the dif ferent time frames; note that these results are signicantly better than the raw bias induced from the imbalance of positi ve and negati ve training examples. Considering that we have not tuned the classier , features, or thresholds to optimize the ac-curac y for this scenario, we belie ve that these results demonstrate the viability of predicting time-to-e xploit from statistical analyses of vulnerability disclosure reports.

Figure 3 sho ws the results for the online version of the experi-ment. In the online version, we emulate a real-w orld deplo yment where we dynamically update the classier and mak e predictions over time as new vulnerabilities appear . We sho w the results for predicting whether a vulnerability will be exploited within days, the most severe positi ve-day case. (Other time frames, not sho wn, yielded similar results.) The classier uctuates initially , then stabilizes after training on a suf cient number of examples. The long-term trend sho ws a decrease in the false negati ve and Figur e 3: Cumulati ve err or, false negati ve, and false positi ve per centages for predicting time to exploit in an online, deploy ed setting . We evaluate tw o time inter vals for updating the classi-fier , every (a) week and (b) month. cumulati ve error rates while the false positi ve error rate remains at. For a simple linear classier , the overall results are extremely promising: at the end of training, the classier has an overall cumu-lati ve error rate of 15%. Finally , in terms of errors, there are man y more false negati ves (13%) than false positi ves (2%).
Finally , we consider the issue of scoring metrics for vulnerabili-ties. Specically , we compare two metrics for assessing how lik ely a reported vulnerability is lik ely to be exploited: one based on prior (expert) kno wledge and handcrafted formulas, the other based on statistical methods and data mining.

As discussed in Section 2.2, the Common Vulnerabilities Scor -ing System (CVSS) denes a metric for scoring the  X Exploitability X  of a vulnerability; see eq. 2. We use this CVSS score as a repre-sentati ve formula-based metric. To be fair , the CVSS specication does not state how to interpret the  X Exploitability X  score; its in-tended purpose may not have been to represent the lik elihood that a vulnerability is exploited. Ho we ver, given its name and the factors that determine the score  X  e.g., dif culty and comple xity of pro-grammatically accessing the vulnerability in an exploit attempt  X  it seems reasonable to expect that the score correlates with exploit lik elihood.

Our data-dri ven approach to vulnerability classication suggests an alternati ve scoring method. Recall that the decision rule in ploitability X  formula (Eq. 2) with values normalized to 1 eq. (3) computes the signed distance to the maximum mar gin hy-perplane separating positi ve and negati ve examples. The signed distance ( w x + b ) serv es as a natural score for the exploitability of a vulnerability: the sign predicts whether it will be exploited, and for positi vely labeled examples, the magnitude indicates its sever-ity.

We compare the effecti veness of these scoring methods by illus-trating the distrib utions of their scores computed on the vulnerabil-ities in our data set. Visually , these distrib utions tell a compelling story .

Figure 4(a) sho ws histograms of CVSS  X Exploitability X  scores for exploited vulnerabilities (top) and vulnerabilities without ex-ploits (bottom); we have normalized the scores to a maximum of Note that CVSS automatically assigns a normalized score of newly disco vered vulnerabilities as a precautionary step. Vulnera-bilities with that def ault score dominate the distrib ution, though, so we have remo ved them from the histogram to more clearly sho w the distrib ution of values computed by the CVSS formula. Figure 4(a) suggests that the CVSS exploitability scores on kno wn vulnerabil-ities do not consistently reect what happens in practice. Man y vulnerabilities without exploits have high CVSS scores, and man y vulnerabilities with exploits have low CVSS scores. As a result, no threshold CVSS score can dif ferentiate well between the exploited and non-e xploited vulnerabilities.

Figure 4(b) sho ws histograms of the classier scores (i.e., the signed distances w x + b ) for the same vulnerabilities. The verti-cal dashed line indicates the def ault threshold of zero used to pre-dict whether a vulnerability should be labeled as  X exploited X  or not: values abo ve the threshold are predicted as  X exploited X , and values belo w the threshold as  X not exploited X . As suggested by the re-sults in Section 5.2, the histograms sho w that the classier separates these distrib utions well: few exploited vulnerabilities have scores belo w the threshold (the false negati ves), and few non-e xploited vulnerabilities have scores abo ve the threshold (the false positi ves). We note that preceding experiments included the CVSS score as a feature since it is available when a vulnerability is reported. Ho w-ever, we found that excluding the CVSS score as a feature did not noticeably change any of the results.

Ov erall, our results suggest that the security community should consider statistical models in addition, or as an alternati ve to cur -rent scoring practices. Such models have man y compelling fea-tures. First, with little tuning, standard models such as SVMs can pro vide metrics that correlate well with exploit beha vior . Second, the models can dynamically adapt over time to incorporate new features and data sets. Third, such models can be exibly adapted to yield a variety of predictions X for example, whether a vulnera-bility will be exploited, or in what time frame it will be exploited. Fourth, the models pro vide real-v alued scores that practitioners can use to prioritize vulnerabilities. Finally , these models can inte grate the results from other scoring systems simply by incorporating the metrics dened by other systems as additional features used for classication. Ranking vulnerabilities is a critical task for softw are companies. With thousands of vulnerabilities in hand and limited resources to x them, it is important to prioritize any operational actions. Cur -rent methods, while easy to calculate, rely on static combinations of a small number of human-mediated qualitati ve variables that seem unlik ely to capture the full comple xity that dri ves vulnera-bility exploitation. In this paper we have described a complemen-tary approach for vulnerability assessment using tools from data mining and machine learning. By considering a far broader range of features and relying on contemporary empirical data rather than  X gut instinct X  to determine their importance, we demonstrate that this approach can classify vulnerabilities signicantly better than at least one currently (and widely) used system for severity scor -ing.

In general, we belie ve that machine learning is well-suited to man y such security assessment tasks and offers considerable exi-bility for consolidating disparate data sources so long as desirable security outcomes can be identied. For example, while this paper has focused specically on exploitability , it would be straightfor -ward for softw are vendors to use our approach in triaging ered vulnerabilities to determine how to prioritize the development and deplo yment of patches. Finally , one limitation with existing vulnerability scoring approaches is the y are generally  X one size ts all X ; the y do not pro vide an easy mechanism for incorporating envi-ronment or conte xt-specic information (aside from manually ad-justing the ad hoc magic numbers in the formulas). In contrast, our data-dri ven approach pro vides a consistent way to inte grate man y local data sources, such as vulnerability scanners, IDS logs and in-cident tick eting systems, to specialize vulnerability assessment to a particular organization.

For man y years, security assessment acti vities have been more art than science. While we concede that the  X holy grail X  security metric remains elusi ve, we see no reason to ignore the power of well-founded statistical methods that can impro ve the state of the practice.
 We gratefully ackno wledge the assistance of Stef an Frei, who gen-erously shared his carefully collected data on vulnerability event dates [10]. [1] W. A. Arbaugh, W. L. Fithen, and J. McHugh. Windo ws of [2] A. Arora, A. Nandkumar , and R. Telang. Does information [3] A. Arora, R. Telang, and H. Xu. Optimal polic y for softw are [4] S. M. Bello vin. On the Brittleness of Softw are and the [5] Cisco. Risk Assessment: Risk Triage for Security [6] CVE Editorial Board. Common Vulnerabilities and [7] C. Dougherty . Vulnerability metric, Updated on July 24, [8] R.-E. Fan, K.-W . Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. [9] Forum of Incident Response and Security Teams (FIRST). [10] S. Frei, D. Schatzmann, B. Plattner, and B. Trammel. [11] IBM. IBM Internet Security Systems X-Force 2008 Trend [12] D. Lewis. Naive (Bayes) at Forty: The Independence [13] P. Mell, K. Scarfone, and S. Romanosky. A complete guide [14] Microsoft TechNet Security Team. Microsoft Security [15] D. Moore, C. Shannon, and k. claffy. Code-red: a case study [16] D. Nizovtsev and M. Thursby. Economic analysis of [17] OSVDB. The Open Source Vulnerability Database. [18] A. Ozment. The likelihood of vulnerability rediscovery and [19] E. Rescorla. Security holes... who cares? In Proc. of the 12th [20] Secunia Corporation. Secunia Advisories. [21] Symantec Corporation. Security Focus. [22] V. Vapnik. Statistical Learning Theory . John Wiley &amp; Sons,
