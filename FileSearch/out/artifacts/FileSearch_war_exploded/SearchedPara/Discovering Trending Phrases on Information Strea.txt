 We study the problem of efficient discovery of trending phrases from high-volume text streams  X  be they sequences of Twit-ter messages, email messages, news articles, or other time-stamped text documents. Most exisiting approaches return top-k trending phrases. But, this approach neither guar-antees that the top-k phrases returned are all trending, nor that all trending phrases are returned. In addition, the value of k is difficult to set and is indifferent to stream dynamics. Hence, we propose an approach that identifies all the trend-ing phrases in a stream and is flexible to the changing stream properties.
 Categories and Subject Descriptors: H.3.4 [ Information Storage and Retrieval ]: Systems and Software X  Current awareness systems General Terms: Algorithms, Experimentation Keywords: trending phrases, social media, real-time web
High-volume text information streams have grown at an astonishing rate in the past several years. As one exam-ple, Twitter has rapidly grown from handling 5,000 tweets per day in 2007 to 50 million tweets per day in 2010 to 140 million per tweets per day in 2011. Making sense of these high-volume text streams offers rich opportunities and there is a growing body of research aimed at mining these streams, e.g., [3, 7, 8, 9]. In this paper, we explore the possi-bility of extracting trending phrases from these streams for understanding the relationship among phrases, topics, and keywords as they rapidly evolve over time.
The database community has long studied stream opera-tions, like finding aggr egates on time-ordered data streams, say for performing counts, calculating averages, and so forth about stream objects. The trending phrase discovery ap-proach at the core of the method described in this paper is similar to the frequent items (FI) finding problem. In the FI problem, a data structure maintains the score of objects seen on the stream, with which the top-k items may be de-termined. Cormode and Hadjieleftheriou in [3] give details of several algorithms that have been proposed for the FI problem. Based on [3], the solutions to the FI problem can be split into counter-based algorithms [9, 7, 8], quantile al-gorithms [5, 10] and sketches [2]. Several approaches have been proposed to deal with a variant of the FI problem that decays items over time using a sliding window [4, 1, 6]. We will describe later in the paper how FI problem is different from the problem we are dealing with.
In this section, we first describe some concepts and then formally define the trending phrases discovery problem. We then propose our solution to this problem using trend thresh-old parameters.

Definition 3.1. ( Information Stream ) An information stream D = { ( d 1 ,t 1 ) , ( d 2 ,t 1 ) , ( d 3 ,t 2 ) ... of document tuples. The stream D is of infinite size and is ordered in a non-decreasing fashion by time-stamp values of the documents. Note that, there might be multiple documents in the stream that share the same time-stamp.

Examples of streams that satisfy this property are high volume sequence of Twitter messages, email messages, news articles, or other time-stamped text documents. Our ulti-mate goal is to map from a stream to a time-sensitive con-cept hierarchy reflecting the relationship among concepts for a particular degree of temporal granularity (e.g., by minute, hour, or day):
Definition 3.2. ( Candidate Phrase ) Consider a docu-ment d  X  D ,where | d | equal to the number of terms in the document. Let P k be a set of phrases of length k .Then, d can be represented using the set of phrases P = P 1  X  P 2  X  X  X  X  P | and the set of all the candidate phrases, observed in the in-formation stream up until time t ,isdenoted C t .
For example, the document  X  X ackers win Superbowl X  can be represented by P = {  X  X ackers X , X  X in X , X  X uperbowl X ,  X  X ack-ers win X ,  X  X in superbowl X ,  X  X ackers win superbowl X  } .The phrases  X  X uperbowl X ,  X  X ackers win X , etc are examples of can-didate phrases and P  X  C . A single candidate phrase p may appear multiple times in the stream. Each such instance of the phrase p i is called a candidate phrase instance .For example, consider the document tuples ( X  X ackers win X , t 1 and ( X  X hampions packers  X , t 2 ). In this case, the candidate phrase  X  X ackers X  has two instances each with time-stamp t and t 2 respectively. Similarly,  X  X in X  and  X  X hampions X  have single instances.
 Problem 1. ( Trending Phrases Discovery Problem ) Given an infinite information stream D and the set of can-didate phrases C t observed in D , the trending phrases dis-covery problem is to identify the subset of phrases  X  t  X  that are trending at time t .

One straightforward solution approach is to map this prob-lem to the frequent items finding (FI) problem, where the item score decay with time, and select top-k items as trend-ing phrases. But this approach neither guarantees that the top-k phrases returned are all trending, nor that all trending phrases are returned. In addition, the value of k is difficult to set and is indifferent to stream dynamics. Hence, we pro-pose an approach that identifies all the trending phrases in a stream and is flexible to the changing stream properties.
The approach we take to identify trending phrases is to associate with each candidate phrase p a score at a par-ticular time S ( p, t ) and to keep only phrases with a score greater than some threshold, where the threshold may vary in time (as the stream becomes more or less bursty, or as more phrases become  X  X ctivated X ). The challenge is to com-pute this score efficiently (ideally O(1) time).

Formally, every candidate phrase instance p i is associated with a score, that decreases as the phrase ages, and a time-stamp t i of the document in which the instance was ob-served. The score S ( p i ,t )for p i at time t is given by: where  X  c  X  (0 , 1) is a constant know as the phrase score decay rate . In a stream we can observe several instances of a candidate phrase p .Let E ( p, t )bethesetofallthe instances of p that have been observed until time t . Then, the candidate phrase score at t , S ( p, t ) is defined as the sum of all the candidate phrase instances of P .

Unfortunately, this definition of S ( p, t )requiresallofthe previous observed instances E ( p, t )for p and its calculation takes O( | E ( p, t ) | ) time, which is inefficient for high-volume text information streams with a large number of candidate phrases. However, we can prove a proposition that will help us calculate this score efficiently in O(1) time and does not have the requirement of storing E ( p, t ). The proof of the proposition is dropped because of space constraints. Proposition 3.1. Given a candidate phrase p with score S ( p, t l ) at time t l , if a phrase instance p i for p is observed at time t n ,where t n &gt;t l then the new score for p at t S ( p, t n ) is given as:
Given the scores for all candidate phrases, we can easily determine if a candidate phrase p is a trending phrase if its score is greater than a trending threshold  X  tt , which yields the set of all trending phrases  X  t : Note that the value of  X  tt is dependent on the number of candidate phrases in C t and so can vary with the temporal changes in the information stream.

For example, Figure 1(a) shows the scores for two can-didate phrases  X  X uper bowl X  and  X  X rammys X  over time. To keep the example simple we assume that the phrase rate in the information stream does not change and hence the value of  X  tt is unchanged. So, using (2) we have,  X  t 1 = {  X  X uper bowl X  } , X  t 2 = {  X  X uper bowl X , X  X rammys X  } and  X  t 3 = { Pruning Candidate Phrases : In real-world applications, the number of candidate phrases C t can become very large, meaning that finding  X  using (2) will become expensive. Hence, we can further prune the set of candidate phrases using a pruning threshold  X  pt ,where C t  X  C t .Likethe trending threshold, the pruning threshold is also dependent on C t . Using the candidate phrase score, the two thresholds  X   X  tt and  X  pt  X  divide the candidate phrases C t into 3 disjoint sets: (i) trending; (ii) transitional; and (iii) non-trending. An example of this is shown in Figure 1(a). Though the candidate phrases to prune are in the non-trending set, not all the phrases in this set can be pruned since the candidate phrases that are in the non-trending set can either be trend-ing up or trending down. Figure 1(a) shows both the cases for the phrase  X  X uper bowl X .

The phrases to be pruned are the ones that are trending down, but we cannot determine the direction of the trend just based on the candidate phrase score. Hence, we adopt a heuristic to prune the candidate phrases. For a candidate phrase p  X  C t that is in the non-trending set we use the last time a phrase instance of p was observed, t p to decide if it should be pruned. To determine C t ,weproposetwo approaches that use t p : (i) a deterministic approach; and (ii) a randomized approach. Both approaches also use a parameter maximum phrase inactivity time T pi .
 Using the deterministic approach C is defined as follows:
The randomized approach uses a function R ( p, t )  X  p  X  C defined as: where, coinF lip :[0 , 1]  X  X  0 , 1 } , is a function where the probability of 1 increases as input is closer to 1. Now, C defined as follows:
In the previous section we showed how we can efficiently identify trending phrases given some stream-dependent thresh-old parameters  X  tt and  X  pt . But how are these parameters determined in the first place? A poor selection can impact the quality and efficiency of concept extraction. A high value (a) Examples for trending phrases dis-covery problem. of  X  tt may result in false negatives (i.e., incorrectly excluding from concept hierarchy generation a legitimately important concept), while a low value for  X  tt may result in false pos-itives (i.e., incorrectly including non-trending phrase con-cepts). Similarly, a low value of  X  pt canresultinalarge set of candidate phrases C t making the determination of  X  expensive, while a large value of  X  pt results in removal of phrases that could trend in the future, thereby reducing the quality of the results.

Hence, it is important that these trend threshold param-eters by determined carefully . Concretely, we first state a lemma that gives us the upper bound on the sum of scores for all the candidate phrase instances that have been ob-served on the information stream. We will state a theorem, which can be proved using the lemma, that guides the deter-mination of thresholds. The proof of the lemma and theorem are dropped because of space constraints.

Lemma 3.1. Given an information stream D with a phrase rate of  X  and the set of all candidate phrase instances P have been observed on D during time intervals 0 , 1 , ...t ,we have:
Theorem 3.1. Given an information stream D at a phrase rate  X  and the set of candidate phrases C t at time t ,the trending threshold  X  tt and the pruning threshold  X  pt are given as: where, S t &gt; 1 and S p &lt;S t are stream specific constants.
To summarize, this section has shown how we can effi-ciently identify the significant base conceptual units (phrases) from an information stream. The two critical trend thresh-old parameters guiding this identification can be calculated with little overhead and can be tailored for stream and application-specific properties.
In this experiments section, our goal is to see if we can discover trending phrases over a real high-volume text in-formation stream. We want to see if we can estimate the stream parameters required to do such discovery and ana-lyze the properties of the trending phrases discovered.
We begin by examining the algorithm presented in Sec-tion 3 to extract trending phrases. Recall that the algorithm requires threshold parameters  X  tt and  X  pt to determine which phrases to extract, and that these parameters are dependent on stream-specific constants S t and S p , respectively. We use a sample of 3.4 million Twitter messages collected from the Twitter Streaming API, resulting in 640 messages per minute.
 Phrase Score Distribution: First, we analyze the distri-bution of phrase scores in a sample from the trending topic stream, updating the phrase scores using equation Propo-sition 3.1. How are these scores distributed? Our goal is to identify a reasonable expected split for trending, transi-tional, and non-trending phrases (Recall Figure 1(a)).
We set the phrase score decay rate  X  c =0 . 80 and process the stream for 2 . 5 hours (150 minutes), noting down the phrase scores in C t every 5 minutes. We then take these 30 sets of phrase scores and sort all of them to calculate the average phrase scores at different levels of top phrases asshowninFigure1(b). Basedonthisfigureweseethat the scores follow a skewed distribution, with most phrases having very low scores (and hence, being of less importance at a particular time in the stream). Based on this score distribution, we fix the following split: (i) Trending (green): top-0 . 5% phrases (ii) Transitional (blue): middle 0 . 5 phrases (iii) Non-trending (red): bottom 90%. 1 Parameter Estimation: With these splits in mind, we next estimate the stream-specific constants S t and S p which guide the threshold parameters  X  tt and  X  pt for trending phrase discovery as the stream rate (and burstiness) changes over time. To estimate the value of S t we process the messages from the trending topic stream, and for every 5 minutes interval, we calculate the percentage of phrases above  X  tt calculated using Theorem 3.1. We calculate  X  tt for different values of S t .Thevaluesfor  X  and C t are obtained from the stream, and we set the phrase score decay rate  X  c =0 . 80. The variation of percentage of trending phrases above  X  tt with S t is shown in Figure 2(a). Using this curve we get S as a function of the percentage of trending phrases p .
Hence, if our goal is to consider the top-0 . 5% of phrases as trending phrases, then for p =0 . 005 we have the stream-specific constant S t =18 . 96.

Similar to S t , to can calculate S p over the stream by cal-culating  X  pt for different values of S p (according to The-orem 3.1). At every 5 minutes interval, we calculate the percentage of phrases that are below  X  pt .Thevariationof percentage of top phrases below  X  pt with S p is shown in Figure 2(b). Using this curve we get S p as a function of percentage of trending phrases p .
 Hence, for a choice of 90% phrases as non-trending, we have for p =0 . 9ascoreof S p =3 . 02.

Thevalueswedeterminedfor S t and S p are interesting because, using the definitions for threshold parameters  X  and  X  pt , defined in Theorem 3.1, we realize that phrases that have a score more than 19 times the average score are trending while phrases that have score lesser than 3 times the average score can be pruned.

Finally, recall that the set of candidate phrases C t can be pruned occasionally to remove inactive phrases. The last parameter to be estimated is the maximum phrase inactivity time T pi which guides this pruning. If T pi is low, then the phrases are not allowed to trend, while if it is very high C could become too large. Hence, we show in Figure 2(c) the variation of the size of C t with increasing T pi .Weseethat the choice of T pi results in a quasi-stepwise change in C so for this case a choice of T pi = 7 minutes would result in a smaller C t with confidence that the choice is fairly stable. Stream Properties Vs Trending Phrases: To verify the robustness of our approach to changing stream properties, we observe the number of trending phrases identified by our approach as the input stream changes. This comparison is shown in Figure 1(c). The top plot in the figure shows variation of phrase rate in the input stream for 2 days. We observe that the number of trending phrases identified for concept hierarchy detection (shown in the bottom plot of the figure) correlates with top plot, since the trending threshold  X  tt is defined as a function of the phrase rate  X  . Sample Trending Phrases: A sample of trending phrases discovered during the first week of Feb, 2011 is shown in Table 1.
We have studied the problem of efficient discovery of trend-ing phrases from high volume text information streams. We have seen how the proposed method can efficiently identify significant phrases from high-volume information streams. For our future work we are interested in developing applica-tions that use the solution proposed in this paper to discover trending phrases. In this table, we observe various events of the week like Middle East revolution, Super Bowl, transfer of Fernando Torres to Chelsea from Liverpool, etc.
