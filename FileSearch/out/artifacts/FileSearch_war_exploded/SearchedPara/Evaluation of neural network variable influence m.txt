 Christopher W. Zobel, Deborah F. Cook n 1. Introduction seeking to improve product characteristics or quality, decision-making in a business and manufacturing context often involves making a choice about how to adjust current operating conditions so as to effect directed change in the output of the process. In order to consistently and effectively implement such adjustments it can be very important to have a good understanding not only of the current state of the process but also of the contribution that each process input is currently making to the behavior of that process. An appropriate characterization of these contributions can help to guide not only minor adjustments in the process but also more strategic decision-making plans if it is able to indicate the relative importance and impact of each decision variable under the current conditions.
 and industry to develop an implicit representation of the relation-ship between the inputs and outputs of a manufacturing or business process. Although there is a perception among some users that neural networks are  X  X  X lack-boxes X  X  with no mechanism for explicitly describing the learned input X  X utput relationships, various measures of variable influence exist that can help to characterize network behavior. When interpreted appropriately, such measures can provide very effective decision-making support by indicating the relative contribution of each network input to the observed network behavior, and without requiring additional experimentation or sensitivity analysis of the trained neural net-work. Assuming that the neural network is a good representation of the underlying process, the decision-maker can then effect appropriate changes based upon this modeled behavior.
 approach to process control and decision-making through the selective interpretation and combination of neural network vari-able influence measures. We begin our discussion with a brief overview of the use of neural network models for decision-making and a summary of their reported shortcomings. We then review the literature on variable influence measures for characterizing the behavior of a trained neural network model, and provide recom-mendations about the most appropriate use of these measures to support effective decision-making. Finally, we illustrate the use of the selected measures for improving process management and for guiding the business decision-making process. 2. Background approximators, have been applied e xtensively to forecast, classify, cluster, and predict outcomes of bu siness and engineering processes.
The successful application of neural network models has been widely reported in many business and engineering disciplines including scheduling, accounting, finance, human resources, information sys-tems, marketing, and production and operations management. network models is the difficulty in extracting meaningful infor-mation or knowledge from the underlying mathematical model. This mathematical model is a complex combination of input values, connection weights, biases, and calculations from trans-formation and activation functions. The result of this complex combination is a functional approximation that maps input data values to resulting output data values. The shortcoming occurs when the neural network model developer wishes to extract specific information about the functional relationship between the input parameters and the output parameters. Baesens et al. (2003) reported that the complex mathematical inner workings of neural networks prevent them from being used as effective management tools in real-life situations where explanations of predictions being made are essential. Howes and Crook (1999) reported that a major drawback of neural networks was that their knowledge and expertise are encoded implicitly and therefore are not directly available to a decision maker.

Tam and Kiang (1992) also discussed the limitations of neural network techniques with respect to testing the significance of individual inputs and noted that there was, at the time, no formal method to derive the relative importance of an input from the weights of a neural network. More recently, however, there has been extensive work in the area of explaining the internal behavior of a neural network model, and various rule and information extraction strategies have been developed and applied. Rule extraction has centered on extracting knowledge from a trained neural network in the form of explicit if X  X hen rules and decision trees. Summaries and descriptions of such techni-ques can be found in Andrews et al. (1995) , Gallant (1988) , Setiono et al. (1998) , Mak and Munakata (2002) , and Lam (2004) .
Although if X  X hen rules are easily understandable, they do not illustrate the relative importance of each individual input para-meter and the rigidity and inflexibility of rules as a basis for explanations has been widely recognized and reported ( Howes and Crook, 1999, Gilbert, 1989, and Moore and Swartout, 1989 ). In addition, a complex response surface can result in a rule set that quickly becomes quite large and difficult to manage. Furthermore, in the case of dynamic processes, neural networks may require regular re-training which will result in the necessity of re-deriving the rule base.

Because of these limitations, a neural network developer may not wish to replace the neural network model with a set of symbolic rules. Instead, the developer may be interested in the ability to see the influence or contribution of specific input parameter values and the impact of changes in those values on the resulting output classification or prediction. There are several existing input variable contribution analysis techniques which attempt to provide such functionality. The focus of the following discussion is to identify which of these techniques can best be used to improve the explanatory power of neural network models and then to illustrate how an appropriate combination of techni-ques can be used to guide process adjustments in dynamic applications. 3. Variable influence techniques
Papadokonstantakis et al. (2006) categorize variable influence techniques for neural networks based on whether the approaches are applied before, during, or after the creation of the neural network model. The first category of approaches involves manip-ulating only the data set since no neural network model has yet been created. The approaches in the second category interfere in the training process to incorporate prior knowledge or to penalize the network X  X  complexity. The third category, which is the focus of this research effort, involves approaches applied to neural networks after the training and validation phase.

Garson (1991) , Yoon et al. (1994) , Yang and Zhang (1997) , Mak and Blanning (1998) , Howes and Crook (1999) , and Nord and Jacobsson (1998) have all developed measures of input variable influence that are based on the connection strengths within a trained, stabilized neural network. Within this group, two cate-gories of influence measures are represented: (1) general mea-sures, which provide an estimate of each variable X  X  influence on the overall behavior of the network; and (2) specific measures, which estimate each variable X  X  influence on the network X  X  output for a particular instance of an input vector. The following discus-sion briefly describes each of the measures of variable influence within the context of these two categories. 3.1. General measures of variable influence
For explanatory purposes, let us assume that each influence measure under consideration is being applied to a trained neural network which has n input nodes, a single hidden layer with h nodes, and a single output node. Furthermore, let w ji represent the weight of the connection between input node i and hidden node j ,andlet v represent the weight of the connection between hidden node j and the output node. Standard nodes in each layer are indexed starting at 1, and bias nodes are indexed by 0. Although several of the influence measures discussed below were originally presented using notation that differed slightly from this convention, the common notation was adopted in order to more easily compare the different techniques to one another. Since our discussion also assumes a single output node, several of the original equations also have been adjusted to corre-spond to this specific situation; in every other way, however, each formula is mathematically consistent with the version presented by its original source.
 The first so-called  X  X  X eneral X  X  measure of influence is due to Garson (1991) . He proposes the following measure for determin-ing the relative impact of each input variable upon the output of the network ( Mak and Blanning, 1998 ) IMP i  X 
This measure represents the percentage of output weight values that are associated with the contribution of input node i . Because the equation incorporates only the fixed network weights, it calculates a single value for each input variable across the entire network and thus provides a concise description of the overall network behavior with respect to each input.

Yoon et al. (1994) also provide a representation of the relative contribution of input i with respect to the overall behavior of the network RS
This measure of the relative strength of variable influence is slightly simpler than that given in Eq. (1) and it indicates the direction (positive or negative) as well as the magnitude of the influence.

Mak and Blanning (1998) propose an alternate measure that is similar to Eq. (2) but which also incorporates the mean rate of change for each hidden element across all of the training data inputs. It is derived as follows:
If we let x i represent the value of the i th variable in the current input vector and let f h represent the (differentiable) activation function for the hidden layer of our trained neural network, then we may write the output of the j th hidden input element as f where s  X  tion, f h 0 ( s j ) also will be a function of s j . If we let { x } represent the set of T input vectors used to train the neural change of hidden element j corresponding to input vector x mean rate of change then can be calculated across all training data as follows: b j  X  1 T with the corresponding input and hidden weights to give a network-level measure of relative incremental contribution for each input i INC i  X  (5) which gives the contribution of each hidden layer element to the network output
HLC ij  X  layer in different regions within the domain of the network.
Crook (1999) also is similar to Yoon X  X  relative strength of influence, however, it incorporates an additional component to normalize the effect of extreme weights connecting input nodes and hidden nodes. The GI measure is generated as follows, where net is a representation of the trained neural network GI  X  i , net  X  X  measure avoids the issues associated with the possibility of a zero-valued numerator or denominator, such as may occur with
Eq. (2). Furthermore, in contrast to Garson X  X  measure of relative impact, the GI measure explicitly incorporates the contribution of the bias terms within the network. As Howes and Crook (1999) assert, this can be very important in cases where the bias forms a significant portion of the contribution to the network output. 3.2. Specific measures of variable influence relative effect of each individual input for each particular instance of the input vector. Given the network-wide measure of relative strength, RS i , as defined in Eq. (2), we let x  X  ( x 1 , a specific input vector and let and O be its corresponding network output., The relative effect of each individual input value, x then given as follows ( Yoon et al., 1994 ):
E i  X  O *  X  x i * RS i  X  P n measure of variable influence. Their Specific Influence ( SI ) tech-nique measures the degree to which each input parameter contributes to the output of the network for each instance of an input vector. The SI value is generated by iteratively replacing each input with a value of zero, re-interrogating the network, and recording the difference in the network X  X  output prior to applying any output activation function. These differences are then normalized and the specific influence of each input component is determined. This can be formalized as follows: above, let x I 0  X  ( x 1 , y , x i 1 ,0, x i  X  1 , y , x n input vector with the i th component set equal to zero. If f represents the activation function for the output layer of the trained neural network and f o 1 represents the network X  X  output value immediately before it is passed through the activation function, then the Specific Influence ( SI ) of input component x is given by
SI i  X  x , net  X  X  f 1 o net  X  x  X   X  X  f 1 o net  X  x 0 i  X  P where SI bias  X  x , net  X  X  f 1 o  X  net  X  x  X  X  is the residual effect of the bias term originating in the hidden layer.
 also propose a third specific measure of variable influence that provides a different interpretation of  X  X  X nfluence X  X  than does the SI measure. Whereas the specific influence represents the relative contribution of an input to the overall magnitude of the network output, the Potential Influence ( PI ) of an input parameter repre-sents the contribution of that input to the potential for change in the network output. In calculating the PI of a particular para-meter, x i , each of the other input parameters is maintained at its respective pattern value while x i is allowed to vary over its valid range. The level of PI is then determined from calculating the difference between the maximum and minimum values of the network X  X  output under these conditions PI i  X  x , net  X  X  max i  X  net  X  x  X  X  min i  X  net  X  x  X  X  X  11  X  in the network X  X  output, given changes in only one of the n input parameters.
 approaches to study variable influence in neural networks. The first approach, developed by Nord and Jacobsson (1998) , is based on the sequential removal of variables by zeroing the connection weights related to that variable in the first layer of the neural network. The prediction error of the network with the zero-ed weights is then used as a measure of the network. This approach is generally similar to that taken by Howes and Crook (1999) in the development of their measure of specific influence (Eqs. (9) and (10)).
 systematic variation of specific variables while other variables remain constant or are themselves systematically varied synchro-nously. This methodology is based on traditional statistical sensitivity analysis and both of these approaches require unique experimentation within trained neural network models. Their experimentation with several real and simulated data sets led them to conclude that neural networks are able to represent the nature of the input variables to a high degree. Consequently, they concluded that neural networks can serve as a guide for the interpretation of influence and sensitivity of the modeled processes.
 that also represents the relative contribution of each input variable to the potential change in the output of a neural network, rather than to the actual value of the output. Their Relative
Strength of Effects ( RSE ) technique is similar to Mak and Blan-ning X  X  network-level measure of relative contribution (Eq. (5)), however, it provides a potentially unique set of values for each instance of an input vector by effectively calculating the partial derivative of the network output with respect to each individual input variable. The RSE is also related to the work done by Hashem (1992) on sensitivity analysis, but it extends these earlier results by scaling the final values to provide a specific measure of the relative influence of each input. The RSE measure is derived as follows:
Let f o represent the activation function for the output layer of a trained neural network, and let f h represent the corresponding activation function for the hidden layer of that network. If we let x represent the value of the i th variable in the current input vector, we may then write the output, y , of the network as y  X  f o  X  s  X  for s  X  where h  X  f h  X  s j  X  and s j  X 
As long as each of the activation functions is differentiable, we may calculate the relative change in y with respect to each x differentiating the resulting combined equation: @ y =@ x i  X 
For common activation functions, such as the sigmoid func-respectively. They are, therefore. functions of x  X  ( x 1 will typically take on different values for different input vectors. for a given input instance by scaling the appropriate derivative RSE i  X  x  X  X 
For any given input vector, each individual RSE result takes on a value between 1 and 1, where the sign of the RSE indicates the direction of influence. The larger the value of the RSE , the greater impact that corresponding input has on the value of the output unit; values close to 1 (and 1) therefore indicate a strong positive (negative) effect of that input factor on the output of the trained neural network. In turn, RSE values close to 0 indicate that the specific input factor had little impact on the value of the output of the trained neural network. 3.3. Selection of variable analysis techniques
Each of the preceding measures provides a value for every input variable that represents that variable X  X  influence on the network X  X  output. Frequently, however, the actual value of the measure for a given variable will be less important than its relative value with respect to each of the other variables. In such situations, it can be far more useful, and ultimately more informative, to generate a relative ranking of variable influence based on actual values that are obtained.

The general measures of variable influence provide an overall measure of the impact of a parameter on the prediction or classification output of a trained neural network. Each general measure attempts to characterize the behavior of the neural network across its entire domain by generating a single repre-sentative value for each input variable or hidden input element. This can be a powerful approach to summarizing network behavior in that it provides a concise summary of the underlying system and the relative contributions of its inputs. Unless the relationship between the inputs and the network output is consistent across the input domain, however, such network-wide measures generally will not be very useful for analyzing the impact of specific changes to the system. From a dynamic decision-making standpoint, it is much more useful to consider measures that can provide an estimate of the relative relationship between the inputs and the corresponding output for specific instances of the input vector.

Of the specific measures of variable influence considered above, we identify the SI and RSE measures as the most appro-priate choices for modeling the input X  X utput relationship within a neural network and provide justification of this selection in the following discussion. The two measures provide differing but complementary interpretations of the influence of a specific input variable upon the output of a trained neural network: the relative contribution of each variable to the magnitude of the output, and the relative contribution to changes in the output. Together, the two measures allow a process manager to make more detailed explanations of neural network predictions or classifications, consequently providing improved decision making capability.
The SI measure is preferred to the relative effect measure provided by Yoon et al. (Eq. (8)) because it is not subject to the possibility that its denominator may take on a value of zero ( Howes and Crook, 1999 ). The RSE measureisalsoconsideredtobemore appropriate than its counterpart, the PI measure, because it will typically generate a unique value for each instance of a variable. The PI measure, as presented, is not unique to each input vector; for example, both x  X  ( x 1 , y , x n )and x 0 I  X  ( x 1 , y , x would return the same value of PI for variable i . For this reason, the RSE measure provides a more detailed description of the net-work X  X  behavior with respect to a given observation.

Because the RSE measure is a rescaled estimate of the partial derivative of the output variable with respect to each input variable, it effectively gives information about the relative rate of change, or slope, at a particular point on the response surface for the neural network. If we consider only the relative ranking of each input X  X  RSE value, rather than the value itself, the relation-ship between the RSE values, with respect to their absolute magnitude, is consistent with the corresponding relationship between the partial derivatives.

The RSE provides information about which input or inputs the network is currently most sensitive to adjustments in, and conse-quently provides a process manager with the opportunity to effect directed changes in the process output for dynamic process control or optimization. Variables with high SI ranking can be identified as making large contributions to the magnitude of the response, thus giving the process manager information about the overall signifi-cance of those variables under current conditions. By combining these two different measures, the manager can achieve a more complete characterization of the trained neural network. Table 1 provides an illustration of how the combined values of RSE and SI pairs can be used to interpret the relative importance of input parameters in local areas of the response surface, and thus to support future process design and operation. Suppose, for example, that a manager has access to a trained neural network that predicts the strength of a manufactured pr oduct based on a combination of measured process parameters. The combination of SI and RSE measures would allow him to identify that a particular process adjustment has little possibility of further increasing the product strength, given current o perating conditions (low RSE value), but identifies that the process parameter makes a significant contribu-tion to the current magnitude of the response (high SI value). That is, this variable is a strong contribut or to the product strength char-acteristic (given the current settings of the other parameters) but small increases in this value w ould not be expected to further increase the product strength. This situation would be evidenced by avalueinthetopleftquadrantof Table 1 . Similarly, combined values of RSE and SI that fall in the bottom left quadrant of Table 1 are representative of parameters that ar e neither significant contributors to the product strength nor would be expected to provide significant change in the value of product str ength. The manager can use this information to tailor a process control approach based on the identification of parameters that are and are not significant con-tributors under current operating conditions.
 provide local characterizations of network behavior, but also that they can be used to generate more representative descriptions of behavior at the overall network level as well. For example, the average RSE and SI value (or ranking) overall input patterns can provide an alternative network-level measure of variable influ-ence for each input. In addition to a single representative value, however, the corresponding standard deviations can also be calculated. This provides not only insight into which variables regularly have the highest influence on the network output but also information about the variation present in the rankings. generate additional aggregate measures of variable influence. For example, the SI  X  RSE analyses can provide information about the number of times that a specific ranking pattern occurs, or even the number of times that a particular cluster of variables has a high impact on the network output. Because the existing general measures described earlier provide only a single number for each input variable, with no associated information about variability or frequency, considering additional factors such as these in the context of a network-wide measure would support a much more sophisticated level of decision-making. 4. Testing methodology 4.1. Neural network model development ments or nodes, linked by weighted directed arcs. The basic multilayer perceptron neural network model with backpropaga-tion learning was utilized in this research effort. The backpropa-gation neural network is the most frequently used neural network architecture in many business and commercial applications. The network typically consists of an input layer, one or more hidden layers, and an output layer, with several neurons in each layer.
Each neuron processes its inputs and generates one output value which is transmitted to the neurons in the next layer. network with a set of input and output vectors representing the conditions in the system being modeled. Training is an iterative process where the connection weights are adjusted to reduce the disparity between the network output values and known output values. Information is acquired or changed by altering the strengths of the existing connections, and learning occurs by modifying the weights between the nodes or processing elements that make up the network. The learning or training procedure must be capable of modifying the connection strengths in such a way that the internal hidden nodes come to represent important features of the task domain. The connection weights are adjusted in response to errors in hitting the target output values. and the network adjusts the set of weights in all the connecting links such that the desired output is obtained at the output node.
When this adjustment has been accomplished, the next pair of input and output target values is presented and the network learns that association. This process continues until the network finds a single set of connection weights that satisfies all the input/ output pairs presented to it. 4.2. Data sets ability of the RSE and SI measures to extract information from a trained neural network. The first data set was generated from a 3rd degree polynomial and was used to validate the behavior of the RSE and SI measures. The variable influence techniques were then applied to two additional data sets: a data set of computer activity measures from the University of Toronto X  X  Delve website (University of Toronto, 2006 ), and a data set representing slump flow of concrete ( Yeh, 2007 ). Neural networks have been exten-sively applied and studied using manufacturing data sets, so we selected data sets representing service processes to illustrate the effectiveness of neural networks for process control and adjust-ment in service scenarios. In each case, we trained a neural network to represent the underlying input X  X utput relationships and then applied the SI and RSE measures to characterize the behavior of the resulting neural network response surface. 5. Analysis and results examples was constructed with a single output node and a single layer of hidden nodes. A unit bias was incorporated at both the input layer and the hidden layer, and each input variable was independently normalized to the interval [ 1, 1] to reduce the impact of different size inputs on the training process. A rescaled sigmoid function: (2 * (1  X  exp( x )) 1 ) 1 was used as the activa-tion function for the hidden layer, and a linear function was used to rescale the network output to correspond to the scale of the original inputs. The neural network application used in the testing was built using Visual Basic for Applications within Excel 2003 and is an extension of the Tiberius for XL package created by Brierley (2003 ). 5.1. Third-degree polynomial data results and analysis
The first set of tests is based on a simulated process that is represented by a simple third degree polynomial y  X  x 1  X  x 2 2  X  x 3 3  X  16  X  where the domain is restricted to X  X  {( x 1 , x 2 , x 3 ) 8 i A I  X  {1, 2, 3}}.

We can easily characterize the expected behavior of both the SI and the RSE measures on this domain by analyzing the relation-ship of each input variable to the output. Given our interpretation of the SI and RSE measures, we would expect the SI value of a given input variable to be greater than that of another variable if the extent of the first variable X  X  contribution to the magnitude of the output is greater than the contribution of the second variable. Similarly, the RSE value of a given input should be greater than that of another variable if the absolute value of the partial derivative of the output with respect to the first variable is greater than the corresponding value for the second variable.
We have the ability to calculate the actual relative impact of each input variable within different regions of the domain for Eq. (16). For example, for all ( u , v , w ) A {( x 1 , x 2 , x 8 i
A I  X  {1, 2, 3}}, x 3 is the most significant input variable and x the least significant with respect to changes in the objective turn, within the sub-region represented by ( u , v , w ) A x ) 9 x 1 A [0.9, 1], x 2 A [0.9, 0.94], x 3 A [0.9, 0.93]}, it is x most significant variable and x 3 the least significant with respect to the magnitude of the objective function, since u 4 v 2 each such observation. Because we can make such calculations for any set of input vectors, we can validate both the RSE measure and the SI measure by training a neural network to represent Eq. (16) and by then comparing the predicted relationships generated from the network against the calculated relationships from the underlying function. Let us look at the results of training a series of neural networks for this purpose.

Given Eq. (16), we generated a set of 1000 unique input vectors with their corresponding output results: {( y 9 x x ) 9 x i  X  0.1 * n , 8 i  X  1, 2, 3; 8 n  X  1, y , 10, y  X  x 1 percent of this data was set aside as testing data, and the rest was used as training data. Ten neural networks were then trained on the training data set and RSE and SI values were calculated for both the training data and the test data with respect to each of these 10 networks.

As discussed above, we used the underlying objective function (Eq. 16) to generate the actual relative ranking for each instance of an input variable, with respect to both its contributions to the magnitude of the output and to its contribution to changes in the output. In a small number of cases, the actual contribution associated with two different input variables was the same, giving a ranking of {1, 2, 2} or {1, 1, 2} rather than {1, 2, 3}. Since the SI and RSE measures generate decimal values from which their rankings are derived, such ties are unlikely within the predicted results. A ranking heuristic was applied in these situations that identified a match between the actual ranking and the predicted ranking if the variables were ranked in the same sequence.
Using the neural network which had the best results on the testing data set, we calculated the set of RSE values for each input vector used either for training or testing the neural network. We then generated relative rankings of the individual inputs for each such input vector and compared them against the actual rankings generated by calculating the partial derivatives of the underlying function. On the data set used to train the neural network, the two sets of rankings matched 96.75% ( 774 800 ) of the time. On the testing data set, the RSE rankings matched the actual rankings 99.0% of the time ( 198 200 ). In both data sets, every observation to which the duplicate ranking heuristic was applied was subse-quently ranked correctly by the RSE technique.

The same approach was taken in order to examine the relative performance of the SI measure. The individual SI values and their relative rankings were generated from the neural network and then compared against the actual rankings created by calculating the relative contribution of each variable to the magnitude of the underlying functional output. On the training data set, the two sets of rankings matched 96.5% ( 771 800 ) of the time. Similarly, on the testing data set the SI rankings matched the actual rankings 96.5% of the time ( 192 200 ). Once again, in both data sets every observation to which the duplicate ranking heuristic was applied was subse-quently ranked correctly by the SI technique.

Although both the RSE measure and the SI measure did a very good job of describing the relative impact of each variable on the output of the underlying function, they are representing very different interpretations of variable influence, as discussed above. This can be seen in a different context by looking at the propor-tion of time that the RSE rankings are the same as those given by the SI measure. On the training data set, the two techniques the time. The two measures clearly are not representing the same type of relationship within the network, and it thus is important to make use of them both in order to more completely character-ize the behavior of the neural network model.

The RSE and SI values are generated from a neural network representation of an underlying functional relationship, and thus both measures describe effects and influences only on that particular network. In applying these techniques, one must realize that, as with any modeling technique, the extent to which any resulting conclusions may be applied to the true underlying model depends on the accuracy of the neural network approx-imation. With this in mind, the next two examples apply the RSE and SI measures to neural networks that have been generated from more complex real-world data sets, in order to explore their behavior under such conditions. 5.2. Computer activity data results and analysis
The computer activity data set consists of 1024 observations of various measures that were collected from a multi-processor system accessed by multiple users in a university environment. These measures include variables such as numbers of reads and writes between system memory and user memory, the number of various types of systems calls per second, and the number of various types of page operations (see Table 2 ). The associated output measure to be predicted is the proportion of time that the system runs in user mode ( usr ). Eighty percent of the observations were randomly selected and used to train 10 neural networks to predict the given output measure. The remaining observations were then used to test the performance of each network, and the network with the best performance on this test set was chosen for further analysis.
 that the percentage of time the run queue is occupied ( runocc )is always the variable for which a change will have the largest impact on the value of the system output ( usr ). This signals that runocc is a critical parameter to monitor and control when trying to adjust the portion of time the cpus run in user mode. Both the number of system calls of all types per second ( scall ) and the number of page faults caused by address translation ( v flt ) are ranked third or fourth in RSE value 96% of the time. Thus, changes in these two input variables would also be expected to result in significant changes in the output variable value.
 number of disk blocks available for page swapping ( freeswap ) and the number of system calls of all types per second ( scall ) are ranked in the top five SI values at least 94% of the time, signaling their contribution to the magnitude of the usr value. The fact that the number of system calls of all types ( scall ) is ranked highly by both the RSE and SI measures indicates the overall importance of this variable in the trained neural network model.
 rankings of each variable can be compared within a particular instance of an input vector. Based on the relative location of a given input variable, a manager can easily judge its importance with respect to both represented types of variable influence. For example, not only are the parameters scall , vflt , and others in the upper right quadrant significant contributors to the magnitude of the response variable (proportion of time the system runs in user mode) but also changes in those parameters would be expected to have a large impact on that response. Conversely, parameters with RSE and SI values in the lower left quadrant, such as sread and lwrite , contribute at a lower rate to the response and thus changes in those parameter values would not be expected to result in significant changes in the proportion of time the system spends in user mode.
 each instance of the input vector, a process manager can have quick access to information about the behavior of the neural network model under various sets of operating conditions. She also can generate a plot of the average ranking of each variable within a particular region of the response surface, including an indication of variability such as error bars, thus providing a more general characterization of system behavior. In either case, the knowledge gained from the RSE and SI measures provides a process manager with critical information about the impact of input values and of changes in those input values on the output value within the neural network model. This characterization of the neural network model then can be used to make appropriate changes and adjustments in the process or to alert a process manager to an impending condition. 5.3. Concrete slump flow data and analysis available at the UCI machine learning repository and described in Yeh (2007) . The attributes in the data set include 7 input variables known to effect slump, which is a measure of the mobility of fresh concrete. The input variables are all concrete ingredients: Cement , Fly ash , Blast furnace slag , Water , Super plasticizer ( SP ), Coarse aggregate , and Fine aggregate . This data set is used to illustrate the development of network level measures of influence using RSE to provide an example of the use of the RSE measure for dynamic process control. We randomly selected 83 observations to train 10 neural networks for classification. The remaining 20 observations were then used to test the performance of each network and to identify the best performing network for further analysis. The possibility of using the RSE and SI measures to develop an improved measure of network level behavior was discussed in Section 3.3. We illustrate this idea in the following section using the concrete data set. 5.4. Dynamic process control
This section uses the best performing neural network, of the 10 that were trained, to demonstrate how the RSE measure can be used to guide process control based on current operating condi-tions. The mean overall rankings of both the RSE and SI values for this network are shown in Tables 5 and 6 . Similar to the computer activity results discussed above, there is a fair amount of varia-bility inherent in the rankings for the concrete slump data, for both measures. Fig. 2 provides a different representation of this variability by plotting all of the actual RSE and SI values for each input variable in the training data set, where values close to the outside edges of each graph indicate the most significant impact on the process output.

Fig. 2 indicates, in particular, that Slag , Fly ash , and Water tend to have a higher number of large RSE values (i.e., close to either 1or 1) than do the other variables (and thus higher mean RSE rankings), but they also have a number of instances of smaller values as well. Similarly, it is relatively easy to see that Water , and Fine and Coarse aggregates generally have the largest SI values. Although these values are distributed widely, they nevertheless all tend to be more significant than the SI values for either Super plasticizer or Fly ash .

The extent of the variability illustrated in Fig. 2 indicates the importance of examining the  X  X  X ocal X  X  behavior around a particular input vector in order to characterize the impact that specific changes in the value of a given variable may have on the process. Given this indication of what such an impact might be, the process manager can guide process changes, in the form of ingredient levels, when an increase or decrease in slump flow is needed.
 We illustrate several such ingredient level adjustments below. Each adjustment applies a change in a variable X  X  value as a percentage of the difference between that variable X  X  largest and smallest observed values, rather than as a percentage of its starting value, in order to facilitate direct comparison between the effects of changing different variables. A sample data vector was selected from the data set and used as the basis for the illustration (see Table 7 and Fig. 3 ). The data vector selected has a low value for predicted and actual slump flow, and we describe how process adjustments could be made in order to raise this value.

Using the RSE to guide such process adjustments implies that we should focus on adjusting the ingredients with the highest ranking RSE values given the current operating conditions (i.e., the current set of input vector values). Table 7 indicates that the highest ranking ingredients in this particular situation are Water , Cement , and Coarse aggregate . Since water has a positive RSE value in this situation, and the other two ingredients have a negative value, increasing slump flow should require increasing the water content, decreasing the cement or coarse aggregate content, or making combined changes to two or three of the variable values at the same time.

As an example of making such a change, an increase of 2% in the amount of water results in a 9.9% increase in slump flow
Cement Slag Fly 332 0 170 161.6 6 900 806 8.941
Similarly, a decrease of 2% in the value of cement results in a 5.75% increase in the predicted value of slump flow
Cement Slag Fly 327.26 0 170 160 6 900 806 8.600
As expected, according to the RSE rankings, the increase gained from a similar change in cement is less than which was gained by the change in water content.

When the two individual changes are combined, a 2% increase in water level coupled with a 2% decrease in the cement level results in an overall increase of 15.75% in the predicted value of slump flow
Cement Slag Fly 327.26 0 170 161.6 6 900 806 9.413
We can quickly see the value of providing this type of information to guide process adjustment. The process operator would be able to make informed decisions about which ingredi-ents to increase or decrease to achieve the desired level of slump flow.

One important consideration in the interpretation of such RSE results is that the RSE is an estimate of the partial derivative, and as such it signals which variable has the highest rate of change only at a specific point on the response surface for the neural network model. As indicated by the variability inherent in Fig. 2 , this response surface is likely to be quite complex and may exhibit very different behaviors within even a small neighborhood of the original observation. The SI measure, when used in conjunction with the RSE , can help to characterize this complexity in a slightly different way.
 magnitude of the response, the SI provides a complementary indication of how  X  X  X mportant X  X  each individual process parameter is to the level of the output. This importance is not measured in terms of the immediate impact of changes in a parameter X  X  value, as with the RSE , but rather in terms of the impact of that input X  X  presence on process performance. The specifics of this are most easily seen by looking at the SI rankings as well as the actual SI values.
 the three process parameters with the highest SI rankings for our chosen sample observation. This implies that completely remov-ing water from the mixture, while keeping each of the other process parameters at their current values, will result in the largest change in the amount of slump flow. The results in Table 7 also indicate that even though Coarse aggregate had the third highest RSE ranking, its low SI ranking implies that removing it
Fly ash
Water from the mixture would have relatively little impact (compared to that of individually removing the other parameters) on the process output.

The positive RSE value for Water indicates that the immediate impact of reducing the amount included in the mixture is a corresponding decrease in the slump flow, whereas the negative RSE values for Cement , Fly ash , and Coarse aggregate imply that reducing each of them slightly will result in increasing the amount of slump flow. The same basic cause and effect behavior is displayed by the SI values for Water , Cement , and Fly Ash , in that removing the water content will decrease the overall slump flow but entirely removing either cement or fly ash content would be expected to increase the slump flow. In the case of the coarse aggregate content, however, the positive SI value indicates that removing coarse aggregate from the mixture entirely will actually result in a decrease in the process output. Thus, a small decrease in the concentration of the coarse aggregate results in an increased slump flow, but a much larger decrease results in a slump flow that is actually reduced. This provides more informa-tion about the overall behavior of the parameter outside of just the small neighborhood corresponding to the RSE value, and thus provides a better picture of the anticipated behavior in other regions of the response surface.

In order to further and more effectively characterize such behavior, a dynamic response surface exploration Decision Sup-port System (DSS) will be developed in the future to automate the characterization of the trained neural network model. The future DSS could use techniques such as genetic algorithms or simulated annealing to explore the relationships between input and output variables and to more fully characterize the relationships between the different measures and the effect of changes within the system. 6. Conclusions
Neural network models are used extensively in business and manufacturing applications to predict outcomes or to determine classifications, but the literature reports only limited use of variable influence techniques within the process control domain. Improved understanding of use of the variable influence techni-ques can lead to improved process control and decision making capabilities. The RSE technique, for example, has not previously been applied outside of the domain of rock engineering. We believe that both the SI and RSE methodologies have the potential to contribute a great deal to the understanding and characteriza-tion of neural network models, and their ability to extract the relationships between input parameters and output values from a trained neural network offers a possible answer to the neural network  X  X  X lack box X  X  shortcomings reported by Baesens et al. (2003) , Tam and Kiang (1992) , Papadokonstantakis et al. (2006) , and others.

By compiling the various variable influence techniques and by demonstrating the potential benefits of applying both the SI and RSE measures to example data sets, we hope to facilitate increased application of variable influence techniques for improved process control and decision making using neural networks. We showed that the SI and the RSE measures can be used to characterize different but complementary aspects of the behavior of a trained neural network model and provided several examples of their use in supporting business decision-making. We also introduced the idea of applying the RSE measure to problems of dynamic process control and discussed the signifi-cant process optimization opportunities that such an application would support.
 References SI
