 Cluster analysis is an important task in data mining. It is widely used in a lot of applications, including pattern recognition, data analysis, image processing, etc. By clustering, one can discover overall pattern distributions and interesting cor-relations among data attributes. Unlike classification, clustering does not rely on predefined classes and class-labeled training examples. Conventional clustering categorizes objects precisely into one of the clusters based on their attributes, in-cluding partitioning methods such as k-means and k-medoids, hierarchical meth-ods such as agglomerative and divisive algorithms, and density-based, grid-based methods, etc[1]. However, based on the clustering results, it needs further gen-erate descriptions for each class, induce conceptual interpretations or decision rules.
 attention in the field of data mining since it provides a tool to treat the roughness of concepts mathematically[3]. To combine the clustering with rough set theory, we proposes a new method to define the equivalent relation on objects according to the closeness of their attributes in the initial stage of a bottom-up clustering. So the objects are divided into cliques and outliers. A clique is a multielement set of objects with compact density distribution, viz. its variance is below some given threshold. An outlier is an isolated object introduced by noise or incomplete information. According to rough set theory, the cliques form the elementary concepts of the given data set. Clustering results can be interpreted in the terms of lower approximation and upper approximation of rough set theory. Rules are inducted based on the elementary concepts to describe the clustering results. Finally, clusters based on different groups of attributes are integrated in the same rough set model, by means of a family of equivalent relations. Attribute reduction and clustering results comparison can be implemented by the mathematical tools of rough set.
 ing algorithms, and describes the agglomerative approach we choose to construct the rough set. Section 3 introduces the rough set theory and its current appli-cations to clustering. Then in section 4 we develop the new method to define the equivalence classes in the initial stage of clustering and its applications to analysis the clustering results. Experiments of artificial and real life data sets are described in section 5. The conclusion is given in section 6. Assume that an object is represented by an N -dimensional feature vector. Each component of the vector is an attribute of the object. Let V = { v 1 ,..., v Q } , v i  X  R
N , be a set of objects. The prototype (or centroid ) for each cluster C k is rep-resented by c k . The vectors are standardized independently in each component to the N -cube [0 , 1] N . This permits each attribute of the objects have the same influence on the clustering.
 tistical clustering algorithms. The name K-means originates from the value K , i.e. the number K of clusters. It is given in advance and K centroids of the clus-ters are chosen randomly. Then each object v i is assigned to the cluster whose centroid c k is the nearest one among the K clusters to v i . The new centroid vectors of the clusters are calculated as follows: The above process continues until all centroids of the clusters are invariant dur-ing the iteration. K-means algorithms are very fast and are often used as the initialization of many other clustering algorithms. But its disadvantages are ob-vious: the number K is hard to be determined; the clusters formed may be very different for different initially given centroids and for different input orders of the objects; only unit spherical-shaped clusters can be found and discovering clusters with arbitrary shapes is difficult.
 ters. This is represented by the fuzzy membership values u jk so that each object v j belongs to multiple clusters c k ,k =1 ,...,C in some degree. The feature vec-tors and membership values are chosen to minimize an objective function, the weighting squared error, during the iteration process: where U is a membership matrix, u jk is its element, c is a set of cluster prototype vectors, c = { c 1 ,..., c C } , and the exponent m of u jk is a severe constriction for cluster overlap, usually assigned a value among 1.5 to 2.5. The convergence of the membership matrix U does not assure the minimum of the objective function J ( U, c ) defined in (2) due to the local minima and saddle points.
 come many perplexing problems in clustering, such as (i) determine the optimal number K of clusters; (ii) prevent the selection of initial prototypes from af-fecting the clustering results; (iii) prevent the order of the cluster merging from affecting the clustering results; (iv) permit the clusters to form more natural shapes rather than only unit spheres. This approach can be easily generalized to fuzzy or non-fuzzy bottom-up clustering algorithms. Looney X  X  agglomerative clustering algorithm is as follows:
Step 1: Standardize independently each component of the vectors to the N -
Step 2: Determine a relatively large number K of prototypes uniformly and
Step 3: Thin the set of initial cluster prototypes out by successively finding
Step 4: Assign vectors to a cluster by means of minimal distance to the pro-
Step 5: Compute cluster prototypes, by means of non-fuzzy formula (1) or the Step 6: Main loop goto step 4, until clusters converge.

Step 7: Eliminate empty clusters and any other clusters with p or fewer vec-
Step 8: Merge clusters that are too close together. The mean or modified
Step 9: Interact with the user to gradually increase the parameters  X  and p , the above Looney agglomerative clustering algorithm and analyze the clustering results in section 4. Rough set theory has widely been used in data mining. This section briefly intro-duces the preliminary of rough set theory and current application in clustering. and A = { a 1 ,...,a A } be a finite non-empty set of attributes(features). Each attribute a  X  A defines an information function f a : U  X  X  X  V a , where V a = { f a ( v i ) v i  X  U } called the domain of attribute a . The two-tuples ( U, A )canbe seen as an information system S (or an approximation space).If A = C  X  D, C  X  D =  X  , information system ( U, A ) called a decision table, where the elements of C are conditional attributes, the elements of D are decision attributes. lence relation ind ( R ) partitions the set U into disjoint subsets, denoted by U/ind ( R )= E 1 ,...,E m , where E i is an equivalence class of ind ( R ). Each E i is called elementary set in R because the elements in the same equivalence class are indistinguishable under R .
 imations using the elementary sets of R .The lower approximation ind ( R ) X is the union of all element sets which are subsets of X . The upper approximation ind ( R ) X is the union of all element sets which intersect the X non-empty. The pair ( ind ( R ) X, ind ( R ) X ) is the rough set of X .
 clustering for managing imprecise concepts or induce cluster rules. Lingras and West[5] propose a variation of the K-means clustering algorithm based on the properties of rough sets. The proposed algorithm represents each cluster C p as an interval set by using the lower bound C p and upper bound C p . For each object v ,let d ( v t , c i ) be the distance to the centroid of cluster C i .Let d ( v t , c i )= 1. If T =  X  , v t  X  C i . 2. Otherwise, T =  X  , v t  X  C i and v t  X  C j ,  X  j  X  T . Furthermore, v t is not part The above rough K-means algorithm can give unsupervised descriptions of im-precise cluster concepts. Its computation is simpler than that of the cut-subset method of fuzzy C-means clustering.
 clustering uses the selected subset of condition attributes and is evaluated by the decision attribute. The procedure starts from a small pre-specified number c of clusters. If all the variances of the decision attribute of the cluster are below the given threshold, the clustering results are accepted. Otherwise, increase the number c of clusters and the clustering algorithm continues. Then the attributes are discretized into a few intervals and the decision rules are inducted accord-ing to the rough data model(RDM). Actually, this can be done more effectively by supervised learning, which assigns different weight values to conditional at-tributes for computing the classification surface.
 implement clustering. Each object v i forms an equivalence relation R i by parti-d ( v i , v j ) &gt; threshold }} , denoted by [ v i ] R is the distance between two objects. Then modify each equivalence relation R i  X  is another threshold. The intersection of all modified equivalence classes em-bodies the clustering results. Obviously this approach can not form more natural cluster rather than the sphere shape.
 and is sensitive to initial conditions. P. Mitra and S.K. Pal et al.[9] suggest initializing the parameters of EM algorithm by rough set. Each feature of an object is represented in terms of the membership to three fuzzy linguistic sets low , medium and high . Threshold the fuzzy membership values to obtain a high dimensional binary feature vector for each object. This forms an attribute-value table whose reducts and logic rules are generated by the discernibility function of rough set theory. The initial paremeters of EM are estimated as follows: the num-ber of component Gaussian density functions ( K ) is the number of the distinct logic rules; the weights, means and diagonal covariance matrices of component Gaussians can be computed from the construction of fuzzy sets and derivation of logic rules. In this section, we present an approach to construct the rough set during the bottom-up clustering process. It is obvious that the data objects can be divided into outlier points and the points distributed densely over some small spaces, called cliques here. Each cluster usually consists of some cliques and outliers, but the objects within the same clique may belong to different clusters due to the different given number K of clusters or some clusters including fewer objects may be deleted. Thus we can regard a clique as a part of a multielement equivalence class and a outlier point as a singleton equivalence class.
 Definition 1. Let the variance of a multielement set E v of objects be  X  2 v ,if  X  v  X   X  , we called E v a clique , where  X  is a threshold experimentally assigned. If E v is a maximum clique, i.e. E v is not a proper subset of other cliques, E v forms a part of a multielement equivalence class.
 Definition 2. A clique expansion is formed by merging two of existed cliques or clique expansions that are close enough each other. The detail criterion of generating clique expansions is shown below. The maximums of clique expansions are the multielement equivalence classes of objects.
 Definition 3. An object v i not belonging to any clique or clique expansion is called an outlier point. { v i } is a singleton equivalence class.
 and formed the distance matrix D = { d ij } , where d ij is the distance of two objects v i and v j . The cliques and outlier points can be discovered by netting clustering on the distance matrix. But this method can X  X  avoid the chaining effect of spanning tree. Thus we will try another way to alter the Looney agglomerative clustering algorithm to construct the maximal cliques and the outlier points in the clustering process. Let the CliquesEx be the set of the intermediate results of singletons, cliques and clique expansions during the clustering. For a set S of objects, S and S are its upper approximation and lower approximation under the equivalence classes CliquesEx . The key issues in the algorithm implementation are described as follows: Clique Initialization. For a cluster C t as the intermediate result of clustering process, compute the variance  X  2 C t about its centroid. If the  X  2 C t  X  , the cluster C t is accepted as a clique and added to CliquesEx .
 Clique Expansion. When two clusters C s ,C t are merged in the clustering pro-cess, compare all pair &lt;E i ,E j &gt;  X  ( CliquesEx  X  CliquesEx )for E i  X  C s and E j  X  C t .Let c S be the centroid of the set S in a distance space. If the sphere defined with radius ( E i  X  E i + E j  X  E j ) / ( E i + E j ) and center point c E i  X  E j contains a certain percentage of the objects in the two cliques respectively, we construct a clique expansion E k = E i  X  E j .Add E k to CliquesEx and eliminate E i and E j from CliquesEx .
 Clique Elimination. For any E i  X  CliquesEx , if there exist E j  X  CliquesEx and E i  X  E j , eliminate E i from CliquesEx .
 follows:
Step 1: Initialize the clustering, including step 1 to step 4 of Looney agglom-
Step 2: Execute the conventional clustering algorithms, such as K-means or
Step 3(Clique Initialization): Compare the variance of each cluster with
Step 4: Eliminate the clusters with p or fewer objects. Reassigned the vectors
Step 5: Mergeeachpossiblepair &lt;C s ,C t &gt; of the clusters according to the
Step 6: Interact with the user to gradually increase the parameters  X  and p , the bottom-up clustering algorithms. It is obvious that algorithm CRS has the similar efficiency with Looney X  X  agglomerative clustering algorithm. The algo-rithm CRS is applicable to more natural cluster shapes, while several algorithms in section 3 are only limited to the case of mixed Gaussian models. If objects have decision attributes just like the Wisconsin breast cancer data, let M be the number of all distinct decision attribute values. The algorithm CRS is iter-ated when the current number of clusters is not less the M . We adopt the total decision errors as the clustering validity measure: where the K j is the number of clusters in the j-th iteration of the step 6 of CRS and DError defined in next section. If there is not a decision attribute, the modified Xie-Beni (MXB) measure[4] can be used as the evaluation of validity. The first experiment is a simple two-dimensional data set used by Looney[4] to demonstrate the clique initialization and merge. We directly use the coordinate values not standardized to [0 , 1] 2 . Let the threshold  X  =0 . 75. After the step 1-3 of algorithm CRS, all the elements of the CliquesEx are singletons. In the step 5 clusters and the elements of CliquesEx are merged. Then we have the result: three clusters, and five multielement equivalence classes and one singleton equivalence classes (outlier point) in the CliquesEx , depicted in Figure 1. masses based on the digital images of the sections of Fine Needle Aspiration(FNA) samples. Ten real-valued features are computed for each cell nucleus: radius, tex-ture, perimeter, area, smoothness, compactness, concavity, concave points, sym-metry and fractal dimension. The mean, standard error, and  X  X orst X  or largest (mean of the three largest values) of these features were computed for each im-age, resulting in 30 features. There are 569 instances in the data set, labeled for two classes: benign with 357 instances and malignant with 212 instances. clusters randomly and uniformly, then thin the set of clusters by deleting the close ones, we get 325 cluster prototypes. After the initial K-means clustering and delete all empty cluster, we get 33 clusters as the first stage of clustering result presented in Table 1, where the column  X  X ount X  is the total elements of each cluster, the column  X  X ecision value X  is the benign or malignant which the most elements of the cluster belong to, and the column  X  X Error X  is the exception ratio of the decision value of the cluster. Since the maximum standard deviation of the clusters is 0.728, we choose the threshold  X  =0 . 530, i.e. the square of 0.728. Now each cluster satisfies the criteria to form cliques, CliquesEx is initialized by these 33 sets.
 brought K down to 16, 13, 10, 5 and 2, and total decision errors V down to 0.471, 0.413, 0.385, 0.302 and 0.226 respectively. The final cluster sizes are 316 and 253. And we get the clique expansions from the original CliquesEx pre-sented in Table 2. And there are seven singletons as the outlier points of the data set. This paper proposes a new approach to construct rough sets during the bottom-up clustering process. With the rough set theory, we can identify the outliers and the concept granularities to interpret the constructions of clusters. Experiments on the artificial and real life data show the algorithm effective and efficient. It remains as a future work to investigate the attribute reduction and rule induction based on rough set to compare and improve the clustering according to different groups of attributes.

