 1. Introduction
Searching a large lexicon is a fundamental activity in information retrieval: the first step in resolving a query to a document collection index is finding query terms in a lexicon (Baeza-Yates &amp; Ribeiro-Neto, 1999; Witten, Moffat, &amp; Bell, 1999). Lexicons, being relatively small, can be stored in main memory and searched very fast, but it is worth considering the gains that indexing the lexicon separately might give. A lexicon index could allow for partially-specified query terms (e.g. terms with a wildcard character repre-senting multiple unspecified characters) by pattern matching.

Partially-specified terms have uses in cross-language retrieval (Guthrie, Pustejovsky, Wilks, &amp; Slator, 1996), spell checking (Kukich, 1992), approximate matching (Zobel &amp; Dart, 1994), crossword puzzle generation (Harris, Forster, &amp; Rankin, 1993; Harris, Roach, Smith, &amp; Berghel, 1992), and library catalog retrieval (Crane, 1996), to name a few. Another application is query expansion: a preprocessing step could use a lexicon search to translate a pattern into a set of terms for a disjunctive search. Glimpse (Manber &amp;
Wu, 1993) uses approximate matching for file system search. Partial and approximate matching have turned out to be especially useful on hand-held computers (Personal Digital Assistants, PDAs) such as
Palm Pilots (Buyukkokten, Kaljuvee, Garcia-Molina, Paepcke, &amp; Winograd, 2002; Kaljuvee, Buyukkok-ten, Garcia-Molina, &amp; Paepcke, 2001). A method better than brute force searching and matching is needed if partial term resolution is to be a frequent activity, especially for the aforementioned PDA. An index to the lexicon is needed for maximum efficiency.

Much research has been done into two indexing methods X  X  X nverted files and signature files. Most of the research has been focused on searching a full-text database. Inverted files have been used to search a large lexicon for partially-specified terms (Zobel, Moffat, &amp; Sacks-Davis, 1993). Enhancements to increase the speed of signature file processing include a multiorganizational scheme (Kent, Sacks-Davis, &amp; Ramamo-horizontal partitioning (Zezula, Rabitti, &amp; Tiberio, 1991), tree structures (Tousidou, Nanopoulos, &amp;
Manolopoulos, 2000) and multilevel superimposed coding (Lee, Kim, &amp; Patel, 1995), among others (Aktug &amp; Can, 1993; Faloutsos, 1992). Inverted files have been compared to signature files for searching a full-text database (Zobel, Moffat, &amp; Ramamohanarao, 1998), with inverted files claiming victory. A key point of that comparison was that the performance of signature files suffers because records in a full-text database vary greatly in length. Since the length of records in a lexicon is more uniform, we hypothesize that a signature file index will compare favorably to an inverted file index for searching a lexicon. Our primary intent in this paper is to compare the two for this purpose.

Our motivation in this study is to index and search a large lexicon for partially-specified queries using signature files, which has not been done before, and to show, by mathematical argument and experimental comparison, that a signature file approach can be as good as or better than an inverted file approach for this application. Specifically, we will show that signature files are about as fast as inverted files while using less memory for the index and allowing more flexibility in index size, which is important in an environment in which memory is at a premium. We also address some of the criticisms of signature files by Zobel et al. (1998) by showing that run length compression is useful for reducing the size of a signature file and by comparing the two in terms of flexibility and extensibility. We believe our work will open the door to the use of signature files for indexes in low-memory environments.

The paper is organized in the following way. Section 2 presents previous work on partial matching, including the inverted file method used by Zobel et al. (1993). Section 3 summarizes signature file research and the signature file method used for comparisons. In Section 4 we will discuss our test data, test queries, and the two programs used in the comparison, and in Section 5 we undertake a direct comparison of the two methods, first by reasoning, then by mathematical modeling, and finally by experimentation. In Section 6 we discuss other bases of comparison and consider the application of our results to the hand-held environment, and Section 7 is the conclusion. 2. Previous work
Before considering inverted files and signature files for indexing a lexicon, we will look at other ap-proaches to searching for partially-specified queries.

Bratley and Choueka (1982) propose a permuted dictionary in which each term is  X  X  X yclically shifted, X  X  using wrapping to generate the string starting at each character in the word. A termination character is appended to the word to mark its boundary. The word file , for example, would contribute to the dic-tionary the terms file| , ile|f , le|fi , e|fil , and |file . The dictionary is sorted alphabetically to allow fast binary search. Partially-specified queries can be found by a simple query conversion ( X becomes |X ; X becomes X| ; X Y becomes Y|X , where X and Y are substrings) and dictionary search. The obvious drawback of this approach is the space requirement. Bratley and Choueka propose a prefix-omission technique, but the overhead is still large.

Owolabi and McGregor (1988) use n -grams, sequences of n consecutive characters in words, to index terms. The parameter n can be any integer greater than one; for example, the 2-grams that comprise information are in , nf , fo , or , rm , ma , at , ti , io , and on . Processing partially-specified query terms is accomplished by extracting the n -grams from the query and finding the terms that contain those n -grams.
In Owolabi and McGregor  X  s case this is done with a bitmap matrix. A query on inf would find words that contain the 2-grams in and nf and return the conjunction of those sets. The word information would be returned as a match, as would reinforces .

This turns out to be a very useful method for indexing a lexicon (Robertson &amp; Willett, 1998; Witten et al., 1999; Zobel et al., 1993). It requires considerably less space than the permuted index and is much faster than brute force matching. One problem with n -grams is that the possibility of false matches arises: in the example above, confine would also be among the returned words, as it too contains the 2-grams in and nf . False matches can be reduced, but not eliminated, by the inclusion of a start-of-word character and an end-of-word character $ , granting each term two additional n -grams. Despite the false matches, the work of Zobel et al. (1993) convinces us that n -grams are the best method for indexing a lexicon, and it is the method that we will use in this work.

Zobel et al. (1993) implement a compressed inverted file in which each n -gram has a posting list of terms in which it appears. Inverted files are a well-known index structure usually consisting of an access structure for a set of terms drawn from a collection of documents and posting lists of pointers to the documents that terms appear in. When n -grams are used to index terms, the analogy to document indexing is that the terms become  X  X  X ocuments X  X  and n -grams become  X  X  X erms X  X . Here we will summarize the inverted file method used for lexicon search.

When a query to an inverted file is made, the query  X  s n -grams are extracted and located in the access potential matches. Each potential match must be checked against the query to ensure it is a true match. The
UNIX library regex can be used to resolve matches by regular expression matching (other algorithms exist and may be faster; for instance, the nrgrep algorithm of Navarro (2001)). See Fig. 1 for an example of query resolution with an inverted file.
 Posting lists can grow very long in a large lexicon. Run length encoding can be used to compress the lists. A run length is the distance between two pointers in a list. For example, the posting list for an n -gram that 1, 2). The run lengths expose patterns that can be used for compression. The standard method for com-pressing inverted files is the d run-length encoding of Elias (1975).

If space is at a premium, the inverted lists can be stored as pointers to the start of a block of records instead of as pointers to individual records. In this case, a blocking factor B is chosen, and terms are grouped into blocks of B terms each. The posting list for an n -gram contains pointers to the block number in which a term that contains that n -gram appears; as long as n -grams appear more than once in a block, memory is saved in the posting list lengths. When the lists are retrieved and conjoined, all of the terms in all of the matching blocks must be tested against the query term to resolve false drops. Therefore there is an expected tradeoff between index size reduced by blocking and the increased number of false drops.
Skips, or synchronization points , can be added into an inverted list (Moffat &amp; Zobel, 1996). A syn-chronization point is a point in the list at which decoding can begin; they exist so that the irrelevant parts of an inverted list can be skipped over and just the relevant portions decompressed, ideally saving some time in list processing. Specifically, if there are to be p synchronization points, then each inverted list has an index of p pointers to positions in the compressed list where decompression can begin.

For example, the list (1, 5, 10, 13, 17, 22, 50, 57, 58, 60) could be given four synchronization points. The skip list would be (1, 13, 50, 60). If another list has already been processed and it is known that the query has no answers between terms 13 and 50, then the original list only needs to be decompressed up to term 13 and after term 50 X  X 7 run lengths to be decompressed instead of 10. An additional list of address pointers pression. To decompress the original posting list after term 50, the address pointer list is decompressed to 57. Decompression commences at that point. The necessity of two additional lists (skip list and address pointer list) increases index size, but both lists can be compressed with run length compression to save space.
Since false match resolution (in our case by using regex ) is so fast, only a few of the lists associated with the n -grams need to be decompressed and merged. A limit can be placed on the number of matches, and lists can be processed until that limit, the threshold , is reached, at which point false match resolution commences. The optimal threshold can be found using the ratio of the time it takes to process a posting list increasing size, the most discriminating n -gram lists are processed first, and the threshold can be reached in false match resolution and list processing. 3. Signature files
A signature is a bit mapped abstraction of a record. There are two main methods of generating signa-tures: word signatures and superimposed coding (Faloutsos &amp; Christodoulakis, 1987). In the word sig-nature approach, identifiers (words, or n -grams in our case) of a record are hashed to bit patterns X  X  X ord signatures X  X  X hich are later concatenated to form the record signature. The superimposed coding method hashes each unique identifier to S bit positions in a bit string with a fixed width F and superimposes (via bitwise OR) the resulting signatures to generate the record signature (some of the frequently used symbols of the paper are listed with their definitions in Table 1). An example superimposed signature for the word
Katerina based on 3-grams with F  X  16 and S  X  2 is shown in Fig. 2. We use the superimposed coding method for this study.

In a na  X   X ve signature approach, hashing is random and uniform, and any given n -gram always hashes to the same S bit positions. There is a chance that two different n -grams will hash to the same bit position; this is referred to as a collision . Since we choose F much less than the total number of unique n -grams, collisions are possible.
 A sequential signature file , or SSF, is made up of a series of signatures, one for each record to be indexed.
A query is processed by generating its signature (via the same process used to generate record signatures) and comparing it to each signature in the SSF by bitwise AND. The matching records are retrieved using an address table; they then must each be compared to the query because some of them can be false matches due to collisions.

To improve processing time, a signature file can be processed by slices. This is referred to as a bit-sliced signature file , or BSSF. Conceptually, the signature file is a matrix with N rows (the number of records) and
F columns. A column a in the matrix is similar to a posting list in an inverted file: it represents a list of records in which the identifier that hashed to bit position a might appear. To resolve a query, the columns corresponding to on-bits in the query signature are conjoined to give the set of possible matches. Matches are retrieved using an address table and compared against the query. See Fig. 3 for a diagram of query resolution with a BSSF.

In this study BSSF is the  X  X  X ignature File X  X  structure that we have chosen to measure the signature file performance; for brevity we will use SF to indicate BSSF in the rest of the article.

A signature file for a large lexicon can be relatively large. A lexicon with 232,435 terms and a signature width of 1000 would yield a 28 MB signature file. A signature file can be compressed by the same run length encodings described earlier, and it will compress well given a long and sparse signature.

Compression is only viable for a signature file if its density (measured by the proportion of on-bits to total bits) is very low. If density is too high, the rate of compression will be low, and decompression will be slow. Signature files have fewer run lengths than inverted files due to collisions, and since there are many fewer slices in a signature file than lists in an inverted file, the size of the compressed signature file is ex-pected to be less than that of the compressed inverted file.

If space is at a premium, the same blocking method described above for an inverted file can also be applied to a signature file. A signature would represent a block of B terms instead of a single term. Col-lisions would be somewhat more likely, as more n -grams are combined in a single signature; as a result, compression would be slightly better. Overall processing time when blocking is used will increase, as there will be many more false matches.
 first bit slice, skip lists are used to save processing time by only decompressing the pieces of bit slices that need to be examined.
 Above we discussed thresholding as a method of partially evaluating a query to an inverted file index. Bit-sliced signature files can also be partially processed. Slices are processed (decompressed and bitwise
ANDed) until the stopping condition is satisfied. The stopping condition dictates that bit slice processing matches FD i  X  1 is greater than the time it would take to process the current set of matches FD
Can, 1996). With a sparse signature file and fast false match resolution, as little as one slice can narrow down the lexicon to a small set of possible matches.

We have decided that it is best to keep it simple. Other signature file methods attempt to reduce false matches, but the tradeoff is more computationally-expensive processing of bit slices. Since matches in a lexicon can be resolved so quickly using regex , a query can return many false drops but still be processed quickly. We have therefore focused on keeping the time required to process a bit slice low. 4. Experimental design and data 4.1. Lexicons
We culled our lexicons from three sets of documents: the United States Code, the federal laws available from http://uscode.house.gov/download.htm ; all United States Supreme Court opinions (decisions, concurrences, and dissents) from 1893 to 2002, downloaded from findlaw.com; and articles from the Financial Times from the TREC 4 CD.

The words were extracted from the collections of documents as follows: First, all markup tags were removed. The Supreme Court cases were marked up with HTML and the TREC database with SGML; everything between and including a less-than and a greater-than was removed. All non-alphanumeric characters were removed, so for instance don  X  t became dont and baseball-only became baseballonly . This was done on the assumption that someone might want to search on a term like baseball-only , and that removing the hyphen and contracting the terms would narrow the search more than a conjunctive query on baseball and only . Numbers were not removed on the assumption that someone might want to search for a section number, date, monetary value, etc. Of course since non-alphanumeric were removed, a date like 1/11 (January 11 or November 1) becomes 111 and indistinguishable from section 1.11 , which also becomes 111 .
There was no modification of case, so Frank is distinguished from frank . 4.2. Lexicon statistics
The US Code lexicon (henceforth referred to as uscode ) has 232,435 unique terms. It uses 1.7 MB of disk space. The average length of a term is about 6.69 characters. The Supreme Court lexicon ( scotus ) has 372,760 unique terms and uses 3.1 MB of disk space. The average term length is about 7.81 characters. The lexicon from Financial Times articles from TREC ( ft ) has 803,400 unique terms and uses 7.1 MB of disk space. The average term length is about 8.25 characters. Table 2 shows these numbers along with length distribution statistics and the number of unique n -grams for each sorted lexicon for n  X  2, 3, 4, and 5. Some 99.5% of the terms in uscode are 20 characters long or less, and the other two lexicons are similar. Since the length of terms does not vary much compared to a collection of documents like the Wall Street nature width can be chosen such that the number of on-bits is a low percentage of total bits, giving each term a unique signature. Long records indexed along with short records adversely affect the performance of signature files, since those long records are much more likely to be returned as possible matches and take longer to resolve (Zobel et al., 1998). In a lexicon, the records are of a more uniform length; very long terms are rare enough so as not to noticeably affect performance.

Non-na  X   X ve signature file approaches have been used to index documents of non-uniform length with a small amount of space overhead. See (Koc  X  berber &amp; Can, 1997; Koc  X  berber, Can, &amp; Patton, 1999). 4.3. Binary search
Binary search is a fast algorithm for searching for strings in a sorted lexicon. Because of the possibility of queries that begin with a wildcard, binary search cannot be used alone, but it may be combined with an index to speed up processing. Queries that start with ^ can quickly be reduced to a range of potential matches. For example, the three characters ^ fo in the query in scotus to 1208, from fo to foyers . With enough characters specified, the index structure is bypassed entirely. We elected to use only queries that would access the index structure since a binary search is equally fast regardless of the indexing method.

Furthermore, if binary search is to be used, n -grams starting with some space in an inverted index and reduces the number of collisions in a signature file. It will also save some space in a compressed signature file. 4.4. Queries Query terms were randomly selected from the ispell dictionary used by UNIX systems for spell checking. It consists entirely of English words and thus is a good representation of normal queries to a lexicon search.
Full queries can be processed quickly with a binary search; the speed with which partial queries are pro-cessed is the true measure of the system. Partial queries were generated by replacing a random number of sequences of random numbers of characters with the wildcard character . Each query had at least one n -gram so that no brute force matching (which would take equally long for inverted files or signature files) would be done. About half the queries used the end character
TWO is a collection of 100 queries with an average of two 3-grams per query; no query in this set has more than three 3-grams. Query set SIX is a collection of 100 queries with an average of six 3-grams per query; all queries in this set have five, six, or seven 3-grams. Each query ran 100 times to get a good average for processing time. Table 3 shows examples from both query sets. 4.5. Description of programs
For our comparison, we used the inverted file program described by Zobel et al. (1993) and available online at ftp://ftp.cs.rmit.edu.au/pub/rmit/fnetik/src/vrank . We modified the pro-gram to do signature processing. The inverted file method implemented by the program can be seen as a signature file method with minimal perfect hashing and a signature width equal to the number of unique n -grams. Likewise, the signature file method we use can be seen as an inverted file method with imperfect hashing and fewer lists. They are programmatically very similar. We have made both programs (including a bug fix in the inverted file code X  X  X ee Note at the end of this work) available on our website at http:// cs.umass.edu/ ~ carteret/bssf.html .

The inverted file program used for query processing stores n -grams in a hash table along with their inverted lists. Our signature file program uses an array to store the F bit slices. Hashing of n -grams is done on the fly, so no n -gram hash table is needed. It may in fact be possible to increase the speed of signature files by finding a faster hash function. See, for instance, (Cohen, 1997). When signature width was fixed at compile time, the signature file program reported slightly faster times than when it was dynamically allocated at runtime. We used the latter, but in an environment in which F is known beforehand and reindexing will be very rare, it might make sense to fix the array size.
 Tests were done on a dual-processor 1266 MHz Pentium III with 512 MB of RAM and a 512 K cache.
The machine is running Linux 2.4.17. Tests were done with both sorted and unsorted lexicons, with binary search enabled on the sorted lexicons (making the indexes slightly smaller). The threshold point was the same for both programs. Results given throughout this paper are for sorted lexicons indexed using 3-grams, unless otherwise noted. The use of 3-grams follows Zobel et al. (1993), Owolabi and McGregor (1992), and others who have found 3-grams to give a good ratio of speed to indexing overhead (Adams &amp; Meltzer, 1993; Angell, Freund, &amp; Willett, 1983).

Although our results have implications for indexing in environments such as PDAs with limited memory availability and slow processing time, implementing full inverted file and signature file systems on a PDA is beyond the scope of this work. Complications involved in a full implementation include the necessity of using data structures more suited to the Palm environment (recursion is nearly impossible because of the small stack size (Noble &amp; Weir, 2001)) and the necessity of writing code to minimize heap size use first and memory use second (Rhodes &amp; McKeehan, 1999). In Section 6, we attempt to indicate how our results can be mapped to the Palm environment by considering the efficiency of pieces of the algorithms. 5. Comparison 5.1. Direct comparison by reasoning
The argument against signature files being faster than inverted files is that bit slices are longer than inverted lists and thus take longer to process; that more slices will have to be processed than inverted lists processed; and that there will be more false matches (Zobel et al., 1998). This is all true, but we contend that for searching a large lexicon (or any collection of documents with close to uniform length), the number of slices that needs to be processed is only slightly larger; that the number of matches to be resolved is only slightly more; and that slices are on average only slightly longer, so that the extra time required because of extra processing is miniscule. Furthermore, because there are fewer bit slices in a signature file than inverted lists in an inverted file, the relevant slices can be accessed faster, decreasing the overall time and making the difference between the two negligible.

In terms of space, the argument in favor of inverted files is that they can be compressed while signature been studied (Faloutsos, 1992; Faloutsos &amp; Christodoulakis, 1987) and shown to be beneficial. As long as the signatures are sparse, run length compression works well. However, there are fewer bit slices in a signature file than posting lists in an inverted list, and the signature file does not require an access structure for n -grams, so the signature file is smaller. Also, because signature width is variable, a signature file can be reindexed to fit into a very small amount of memory if needed. This is discussed in more detail in Section 6. Another argument against signature files is that they are more expensive to generate (Zobel et al., 1998).
This is a one-time cost, so perhaps not that important, but because bit slices can be generated sui generis (the signature matrix does not need to be generated and then transposed), it takes less time to generate a signature file index than an inverted file index. There are fewer lists to be added to as processing continues, and the lists are accessed faster by hashing on the fly instead of searching an access structure for an n -gram. 5.2. Direct comparison by mathematical modeling Our intent in this section is to modify the mathematical model presented elsewhere (Koc  X  berber, 1996;
Koc  X  berber et al., 1999) for our lexicon search application, and to show that for this application, inverted files are a special case of signature files.
 5.2.1. A mathematical model for signature files The parameters in a basic signature method are shown in Table 1. They are F , the width of the signature;
S , the number of on-bits set by each n -gram; and D , the average number of n -grams in a term. The signature method described here also uses parameters B , the blocking factor, b , the number of unique n -grams in a block, and h , the threshold. The variables t slice and t required to resolve a match (i.e., to determine whether it is a true match or a false match). Equations for signature file evaluation have been derived (Koc  X  berber &amp; Can, 1996; Roberts, 1979); we use slightly modified equations here that take term blocking into account. The density of the signature file, its op value, is estimated probabilistically as 1  X  1 S = F  X  b . Note that this estimate of op value assumes that there will be no collisions in the S bits set per n -gram, but that there may be collisions in the S b bits set by the bn -grams in a block. Note also that when B  X  1 it follows that b  X  D . The expected number of potential matches remaining after processing i slices is estimated as FD The total expected time it will take to process a query of dn -grams using a signature file index is where i is the number of slices to be processed, at maximum S d .
 Note that op is a function of S , F , and b (which itself is a function of D and B ). The parameters S , F , and
B are set by the user when the lexicon is indexed; the parameter D is directly related to n -gram length. It is trivial to show that as F increases, op decreases (and vice versa), and that as S or B increase, op increases (and vice versa). The time t slice is a function of op and N .As op increases, t
Likewise for N . Sparser, shorter bit slices process faster. Overall, as F increases, query time T decreases (Fig. 4).

The time t resolve is a function of the complexity of the query. Queries with more wildcard characters take slightly longer to match; if full regex querying is enabled, t average, t resolve is very fast-timed at about 0.002 ms for partial queries.
 Although false match resolution is fast, a low false drop rate is desirable for fast overall processing.
A false drop rate of 1 in 100,000 is stated mathematically as where i is the number of slices to be processed. Solving for i gives
In some cases i slices cannot be processed. The number of slices that can be processed depends on the weight of the query. The weight of a query with dn -grams is given as
At least one slice must be processed; at most W Q  X  d  X  slices can be processed. In other words, i is bounded:
The above equations are useful for finding parameters that give good results. Note that op remains approximately the same when S and F are increased by the same factor. The time to process a slice also remains approximately constant, as seen in Fig. 5. If query weight is low, processing W eliminate enough false drops to achieve a good query time. In that case, increasing S and F by the same factor may decrease query time significantly. Specifically, S and F should be increased by the same factor if the following inequality is true.
In other words, if the time it would take to process the false matches that could be eliminated by adding another on-bit to the query signature while keeping op constant is greater than the time it takes to process a slice, S and F should be increased in such a way that op remains constant. This is only good to a point, as F cannot be larger than the number of unique n -grams. Also, if op is sufficiently small, or if W ciently large, the difference in the number of false drops achieved by increasing S and F is small and not worth the attendant increase in index structure size. The op is sufficiently small when F is large and S is small. An op of 1 in 1000 is small enough that S can be fixed at one.

For an op of less than one in 1000, Eq. (2) gives
Fewer than 1.67 slices will need to be processed per query to achieve a false match rate of one in one hundred thousand. An op of 0.00035 (achieved for uscode when F  X  17 ; 000) would only require 1.45 slices to be processed per query.

We introduced the concept of skipping earlier, and consider it now. Before any skip lists can be pro-cessed, one full slice must be decompressed and processed. Because we predict that fewer than two slices will be processed on average, it is unlikely that skipping will have a beneficial effect. Even in cases where more than two lists need to be processed, the additional time required to decompressed the skip list and address pointers will likely offset the time gained by skipping. When the signature file is dense, however, skipping can reduce processing time. The signature file may be dense if F is small, S is large, or B is large. More slices need to be processed in a dense signature file; each additional slice that is processed reduces the number of possible matches and therefore increases the chance that sections of the next slice can be skipped.
The optimal threshold h is based on the formal stopping condition, which says to stop processing when the estimated time it would take to process another bit slice and resolve the resulting set of matches is greater than or equal to the time it would take to resolve the current set of matches. Mathematically,
By rearranging terms and noting that N op i  X  1 op  X  N op i , processing should stop when:
The threshold value can be based on an estimate or a timing of the two values rather than timing them during program execution. 5.2.2. A mathematical model for inverted files
The inverted file index can be considered a signature file index with minimal perfect hashing and F equal to the number of unique n -grams. In that case, the op value of that index is approximately equal to b
Eq. (2) gives the number of slices that will need to be processed to achieve a false drop rate of 1/100,000, and in fact, the number we come up with is remarkably close to the number we see in experiments.
In the case of uscode , where F  X  33 ; 942 and b  X  5 : 69 without blocking, the op value is less than 1/5000, which means the number of lists that will need to be processed is about 1.35. Again, it is unlikely that the skip lists will be helpful, since so few slices need to be processed.

Eq. (5) applies for determining the optimal threshold. Therefore the threshold for inverted files will be t slice = t resolve . Although the inverted file is less dense, experiments show t t slice for a sparse bit slice (specifically, within 1% difference).

Now we have shown that the number of slices processed by a signature file is only slightly larger than the number of lists processed by an inverted file; and that the number of matches will be similar since the optimal thresholds are identical. Experimentation will verify that query time is similar and that the sig-nature file index size is smaller. 5.3. Experimental comparison
In this section we present selected results from the experiments described in Section 4. Section 5.3.1 gives results using 3-grams, which give the best ratio of query time to space overhead for inverted files (Zobel et al., 1993). In Section 5.3.2 we explore using longer or shorter n -grams. 5.3.1. Experiments using 3-grams
Tables 4 and 5 show the best observed query times along with the average number of slices processed per query and the average number of matches resolved (true and false) per query for each lexicon for both inverted files and signature files. The signature width was 17,000, well under the number of unique 3-grams.
Both tables show that signature files do only slightly more processing per query than inverted files. The inverted file is faster than the signature file, but only by microseconds.

Table 4 shows results for query set TWO on sorted lexicons. A sorted lexicon is searched faster than an unsorted lexicon, since in the latter, the 3-grams starting with the unsorted lexicon is slower for both SF and IF, the percent difference is about the same. Short queries typically return many potential matches; resolving them is a large percentage of the total processing time.
Table 5 shows results for query set SIX on unsorted lexicons. With longer queries there is a greater chance of a sparse bit slice or short list being used, so there are fewer potential matches. Resolving those accounts for less of the overall processing time; a greater percentage of processing is done in decompressing in tens of microseconds.

Table 6 shows the sizes on disk and in memory of the indexes that gave the best observed times for both methods for each lexicon. The signature file index is significantly smaller in every case. The signature file index size is the sum of the sizes of the compressed slices. Once again, signature width was 17,000. The access structure size for the signature file includes a pointer to each bit vector, plus the space required for extra information about each bit vector (its length, current position, and so on), plus the size of the pointers to the terms in the lexicon. The inverted file index size is the sum of the sizes of the compressed inverted lists. The access structure size for the inverted file includes the space required for the unique n -grams, the size of the hash table nodes that the n -grams are stored in, and the size of the pointers to the terms in the lexicon. (Calculation of access structure size is shown in more detail in Section 6.1.) The percentage by which the inverted file size exceeds that of the signature file is given in the final row. The signature file is significantly smaller while being nearly as fast.

Table 7 shows the index generation time for the indexes that gave the best observed query times. The signature file index is generated faster in every case. Of course, query time is the most important measure; we will not attempt to argue otherwise. But note that inverted files do not have a very large advantage there; the difference is less than 3% for short queries and less than 10% for long queries. 5.3.2. Experiments with other gram lengths
Using longer n -grams gives interesting results. It is expected that increasing n will speed up query processing, as there are more n -grams and each one appears in fewer terms, and that space overhead will increase, as more bit slices or inverted lists are needed to accommodate the increase in n -grams. The results for inverted files conformed to our expectation, but the results for signature files were surprising: query times equally good or better with higher values of n in the same amount of space overhead. The numbers in
Table 8 were obtained using a query set made up of 10,000 five-letter strings, so the queries had four 2-grams, or three 3-grams, or two 4-grams, or one 5-gram, depending on the length of the n -gram used for indexing.

The reason good query times are still obtained without increasing space overhead is that although there are many more 4-and 5-grams than 3-grams, each 4-or 5-gram occurs in far fewer terms than 3-grams. The average number of terms a 4-or 5-gram appears in is less than the average number of terms a 3-gram appears in, and the variance is less. Thus bit slices are more uniformly dense, and the op value used to optimize query time more closely reflects the actual density of the slices being processed. In the context of our mathematical model, the op is lower for higher values of n because of the parameter b in its calculation.
This also explains why the observed numbers for 3-grams are greater than the predicted numbers. The predicted numbers are based on slices of uniform density; the observed numbers were obtained using queries taken from English words, which means the density of each slice processed was greater than the average density of the signature file.

Table 9 compares signature files and inverted files for different gram lengths. Signature width was chosen low for n  X  2, because F cannot be larger than the number of unique n -grams. See Table 2 for the number of unique n -grams in each lexicon. The memory usage for inverted files is very large because of the addi-tional n -grams. 6. Other points of comparison In this section we will compare signature files and inverted files using some of the criteria set out by
Zobel, Moffat, and Ramamohanarao (1996). Specifically, we will compare them in terms of memory usage, scalability, index generation time, ease of update, and extensibility. 6.1. Memory usage
Both methods store the entire lexicon and the index structure in memory. The inverted file method additionally must store the lookup table. For the uscode lexicon, with 33,942 3-grams, the lookup table adds 101,826 bytes. The 8 bytes required by each 3-gram for the hash table plus the 28 bytes needed for each bit vector bring the total amount of space required by the lookup table to 1293 KB. Adding in a 4-byte the total to 2234 KB. The signature file method needs the 4-byte pointer for each term, the 28 bytes for each bit vector, and 4-byte pointers for storing the bit vectors in an array. Since there are many fewer bit vectors in the signature file, the amount of additional memory required before considering the term pointers is 531
KB when the number of bit slices is 17,000 X  X  X alf the number of lists in the inverted file. The total amount of overhead is 1439 KB.

The biggest cost in either approach is the lexicon and its pointers. The lexicon may be compressed using methods described by Faloutsos (1985), Bell, Witten, and Cleary (1989), or Navarro, de Moura, Neubert,
Ziviani, and Baeza-Yates (2000). Compressing the lexicon would presumably incur a tradeoff in processing time as lexicon terms would need to be decompressed, or query terms represented in some compatible way, before matching. Another way to reduce overhead is by storing lexicon terms continuously and using a d log 2 C e -bit pointer (where C is the total number of characters in the lexicon) to access them.
Not counting lexicon overhead, the inverted file for uscode requires 2593 KB, while the signature file requires 1786 KB, a 31% difference. Table 10 shows how index overhead is calculated.

While memory on modern desktops is cheap and plentiful, memory usage may be extremely important in a PDA environment. Typical PDAs have 8 X 32 MB of memory. Table 11 shows the amount of space our three index structures use under various conditions. Storing the three inverted file structures for our three lexicons would use 13.46 MB, 37% more than the three signature files at 9.84 MB (at F  X  17 ; 000). The ability to shrink the index structure could be a huge bonus. The total size of the inverted files could be reduced to 9.55 MB by using a blocking factor of 20; the signature file size is reduced to 5.92 MB using the same blocking factor. The signature file size could be reduced to 4.83 MB using a signature width of 100 and no blocking factor (query time is much slower, of course, but still measured in milliseconds). Finally, using a signature width of 100 and a blocking factor of 20, the combined size of the three signature files would be only 1.05 MB. The capability to shrink an index structure to any size could be invaluable to the international businessman or foreign diplomat.

On a PDA, the lexicon and lexicon index would be stored in permanent memory, in the database structure. One database record can hold up to 64 KB; the average inverted list is 39 bytes and the average bit slice is 76 bytes. On average, then, all inverted lists or bit slices could be stored in about 20 records to minimize storage overhead (each database record requires some additional overhead containing informa-tion about the record (Palm, 2003)). The inverted files also must store the n -grams and some way of associating a list with a n -gram, which would require at least one more record. Palm OS 4.0 has 182 KB of dynamic heap space available for processing (Palm, 2003), which is more than enough to decompress and process several lists. The lexicon can span records, but would require an additional structure within each record to point to term heads. 6.2. Scalability
We have demonstrated the scalability of signature files by showing results from lexicons with 232,435 terms to lexicons with 803,400 terms. Time differences appear to favor signature files as the lexicon gets bigger, due to the increase in the number of unique 3-grams. The advantage signature files hold in memory usage shrinks as lexicon size increases due to the greater percentage of total space consumed by the access structure. If the access structure can be shrunk, perhaps using methods mentioned in Section 6.1, the advantage of the signature file index remains high.

Generally as lexicon size increases query time increases. In the query time equation (1), t functions of N . Index size is expected to rise with lexicon size, as is generation time. The same things will happen to an inverted file index.

It is worth considering whether the comparison scales downward; i.e. whether the results will hold in the low-memory hand-held environment. For query resolution time, the points of comparison are the amount of additional time needed by the inverted file to find an n -gram in its hash table; the additional time re-quired by the signature file to decompress bit slices; and the additional time required by the signature file to resolve false matches. Again, it is beyond the scope of this work to quantify these points, but if we assume proportionality, the percent difference in processing time will remain the same. 6.3. Index generation time
While index generation time will usually be a one-time cost, adding a lexicon to a PDA may require that the other lexicons stored on the PDA be reindexed for memory usage.

With each n -gram setting one bit in the signature (i.e., S  X  1), the number of run lengths that will be compressed while creating the signature file index is slightly less than the number of run lengths that will be compressed while creating the inverted file index, due to collisions. Furthermore, n -grams can be hashed on the fly during signature file generation, saving the time required to look them up in a structure.
Parameter choice affects index generation time as follows: as F increases, generation time increases. As S increases, generation time increases. As B increases, generation time decreases. Indexes for larger lexicons are generated slower, as are indexes for lexicons with more n -grams per term.

It should be noted that increasing S has a huge effect on generation time. Doubling S almost doubles generation time. Therefore for this analysis to hold, signatures need to be long enough that S can be fixed at one. On the other hand, increasing F has a very small effect on generation time (see Fig. 6).
Palm OS 4.0 has only 256 KB of dynamic heap space, 72 KB of which are reserved for system use (Palm, 2003). This makes indexing a large lexicon in memory infeasible. In a production application, this would probably be moved to the desktop machine, where databases would be indexed and then uploaded. The signature file application retains the ability to index in memory if needed, provided the number and size of bit slices is low enough to fit in memory. Indexing can also be done within the Palm  X  s permanent database structure, although it would presumably be much slower. 6.4. Index update
To add a term to an unsorted lexicon, the run lengths for the n -grams in the term are compressed and appended to the relevant posting list or bit slice. A small amount of overhead is needed to lengthen the bit the inverted file lookup table, it needs to be added and the lookup table rewritten to disk.
Previous literature has asserted that insertion to a sorted bit-sliced signature file is practically impossible, due to the need to decompress the entire signature file, add the new signature in the correct spot, regenerate must be decompressed and recompressed, inserting to an inverted file entails the same overhead. It is not the case that slices need to be regenerated, as run length encoded bit slices are essentially the same as in-gram appears in combined with some terms it does not appear in. In either case insertion is accomplished by splitting a currently existing run length in two. In fact, other forms of signature compression, such as VBC (Faloutsos &amp; Christodoulakis, 1987), allow for partial decompression and recompression.
We also consider that because signature files are shorter (due to collisions) and thus compress faster, because there are fewer bit slices than inverted lists, and because n -grams do not have to be located in a lookup table, signature files will have an advantage in insertion time. 6.5. Extensibility
Signature files are extensible as long as inaccuracy is acceptable. The inaccuracy can be minimized with appropriate selection of parameters at index time.

Approximate matching, or ranking, can be useful in a lexicon search for a spell-check application or a translation dictionary. Partial query matching has been shown to be useful for approximate matching (Zobel &amp; Dart, 1994). The n -gram distance algorithm seems to be one of the best available, providing another justification for our use of n -grams for indexing. The n -gram distance algorithm works as follows: the bit slices or posting lists relevant to query n -grams are located and unioned (instead of intersected; we want all terms that have one n -gram in common with a query term). The gram-dist (Ukkonen, 1992) be-tween the query term and a retrieved term is computed as the sum of the differences in the number of times each n -gram that occurs in both terms occurs in each term. If we assume that terms don  X  t contain repeated n -grams (a reasonable assumption usually), the formula is gram -dist  X  s ; t  X  X  G
G x is the set of n -grams in term x . For example, gram-dist ( file, filing )is2  X  4 2 1  X  4 because they have one 3-gram in common ( fil ). Terms are ranked from lowest gram-dist to highest. Zobel and Dart show that this algorithm is as good as or better than most others while being simple to calculate.

This algorithm is easily implemented using inverted files, in which the n -gram count is equal to the number of on-bits in the inverted list. It can be implemented using a signature file with the following tradeoff: if greater accuracy is desired, processing time will be greater. If faster processing time is desired, accuracy will be lower. The gram-dist can be computed using bit slice lengths, but because bit slices are stochastic, the ranking will be inaccurate. For accuracy, the n -grams in each term can be found.
Research has been done into ranking methods with signature files; however, practical use of signatures for ranking has not been reported in the literature. Croft and Savino (1988) describe a method of ranking using bit-sliced signature files similar to an inverted file. Lee and Ren (1996) describe a method of parti-tioning signature files based on term weights. Their query evaluation method is sequential instead of bit-sliced, but it entails many fewer false matches. Luk and Chen (2001) extend VBC to support term frequencies. Their WVBC method is sequential as well, but a VBC-coded bit vector can be partially decompressed, saving processing time. Both of these methods reduce the problem of terms that are not similar to the query term.

A lookup table like that used in an inverted file can also be added to a signature file, if additional space overhead is acceptable. The nodes in the lookup table can hold additional information about n -grams. binary; they can only say whether or not a record has a specific property or not. Without additional storage overhead, they cannot compete with inverted files in this respect. 7. Conclusion
The lexicon search is a ubiquitous activity in any kind of information retrieval system and its index structure needs careful design and implementation. A lexicon is small enough to be stored in main memory, and using n -grams, partial query matching is efficient. We have shown by mathematical argument and experimental comparison that for searching a large lexicon, a signature file index is as good as an inverted file index. Although more slices need to be processed than (inverted index posting) lists, slices are longer than lists, and more false matches will need to be processed, the differences are small.

Efficient searching of a large lexicon is useful and important by itself, but the conclusion can be generalized. Our mathematical and experimental analyses above demonstrate that a signature file index can be as good as an inverted file index given a set of records and queries with the following prop-erties: 1. The number of unique identifiers per record is small compared to the total number of unique identifiers, which is large, allowing sparse signatures. 2. Query weight is high relative to record length. This allows just one bit to be set per unique identifier. This property is conditional X  X  X f signatures are sufficiently sparse, query weight is irrelevant. to partially process queries to compete with signature files.

When these properties hold, the compressed signature file index will be smaller than the compressed inverted file index, it will be generated faster, and queries will be processed nearly as fast. Collections that might index well with signature files are library catalogs, multimedia files with many attributes, medical cross references, or lists of streets for a GPS system. Quantifying the conditions listed above would be useful in determining whether a signature file index is good for an application.

The most obvious and immediate implication of our results is the application to the low-memory environment of hand-helds such as Palms. While some of the basics of the Palm environment were dis-cussed, complete production-level inverted file and signature file applications are needed to draw a meaningful conclusion. It is apparent that the ability of signature files to quickly reindex into smaller spaces would be very useful; since we predict and observe a small query time difference, see this as a worthy direction for future research.

Note . A bug in the publicly-available inverted file code results in fewer matches reported when binary search is enabled. Partial queries that end with $ are not processed correctly due to an error in pointer arithmetic. Specifically, line 175 in mkvind.c should read: if (!dorange) len +  X  2; else len++. The bug was corrected for our comparison.
 Acknowledgements
We greatly appreciate the TREC 4 CD made available by NIST. We thank Justin Zobel for making his inverted file code freely available. We are very grateful to the referees, whose comments and suggestions greatly improved the focus of this work.
 References
