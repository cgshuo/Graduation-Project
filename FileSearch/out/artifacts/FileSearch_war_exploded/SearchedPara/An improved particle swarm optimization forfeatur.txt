
Department of Business Administration, Fu-Jen Catholic University, New Taipei City, Taiwan Department of Industrial Engineering and Engineering Management, National Tsing Hua University, Hsinchu, Taiwan Notation 1. Introduction
Data mining is the process of automatically discovering useful knowledge from large data sets. This process includes a series of transformation steps, from data preprocessing to post-processing of the mining results. Data preprocessing, where feature extraction and selection methods play important roles, is perhaps the most time-consuming step in the entire process. However, traditional approaches include the presence of high-dimensional data, heterogeneous and complex data, class imbalance, and class-overlapping data. In addition, the number of features directly affects classi fi cation accuracy and processing time in recognition systems. In medical science and image processing, as well as in complicated pattern-recognition applications, the number of features can be signi fi cant.
To deal with large amounts of data, feature selection is usually applied to select a set of essential with as little performance degradation as possible. Feature selection reduces the number of features especially important when handling a very large data set with dimensions that run into the thousands. in exhaustive searches. These strategies fi nd the optimal subsets of features using an optimization algorithm. Meanwhile, searching for an optimal feature subset in a high-dimensional feature space is an NP-complete problem [8]; hence, traditional optimization algorithms are inef fi cient when solving large-scale feature selection problems. Therefore, addressing the problem of feature selection through meta-heuristic methods is a worthwhile research direction.

Genetic algorithms (GA) and particle swarm optimization (PSO) are commonly used meta-heuristic methods. GA is a numerical search technique used to fi nd exact or approximate solutions. PSO is a stochastic optimization technique introduced recently by Kennedy and Eberhart who were inspired by the social behavior of bird fl ocking and fi sh schooling [17]. Oh et al. [9] used GA and hybrid GA to solve the feature selection problem. However, their approaches are time consuming. In contrast, in PSO only a few parameters require adjusting; it is simple, ef fi cient, and memory enabled [7].
These advantages allow PSO to be applied to different categories of combinatorial optimization prob-crossover operations in PSO to improve global search capability by preventing premature convergence. circumvent the problem of premature convergence and local minima in conventional PSO. However, PSO still has disadvantages, such as local optimal trapping due to premature convergence (the exploration problem) [22] and insuf fi cient ability to fi nd nearby extreme points (the exploitation problem) [13].
This paper proposes a PSO-based approach to address the problem of feature selection. To overcome the drawbacks of PSO, we propose an improved PSO algorithm using the opposite sign test (OST). The test can increase population diversity in the PSO mechanism. Hence, using OST in PSO can improve global search capability, which can prevent premature convergence and increase population diversity in the PSO mechanism.

The remainder of the paper is organized as follows: Section 2 de fi nes the problem of feature selection and classical algorithms. Section 3 presents a simple PSO for feature selection. The proposed approaches are shown in Section 4. Section 5 presents the results. Finally, Section 6 concludes the paper. 2. Feature selection 2.1. Feature selection problem d )! such subsets. Usually, the data dimensionality D is large. A large D increases the dif fi culty of estimating classi fi er parameters. This phenomenon is known as the curse of dimensionality [20].
The problem of feature selection has been studied by many researchers. For instance, Tsanas et al. [2] examined the potential of sustained vowel phonations for predicting the progression of Parkinson X  X  disease symptoms. They selected an optimally reduced subset of measures, and produced a clinically useful model where each measure in the extracted subsetis a non-overlappingphysiologicalcharacteristic of the speech signals. Fan [29], on the other hand, proposed a new optimization framework for improving features that shows strong separability between two classes. He concluded that his method can be used feature transformations based on a wrapper-based genetic algorithm approach. Accordingly, selecting effect of feature selection and clustering on the automatic categorization of newswire articles. 2.2. Feature selection algorithms approaches, features are scored and ranked based on certain statistical criteria and the features with the highest ranked values are selected. Frequently employed fi lter methods include t-test, chi-square test, Wilcoxon Mann-Whitney test, mutual information, Pearson correlation coef fi cients and principal select signi fi cant features and to exclude noise.

Wrappers make use of a chosen classi fi er to select feature subsets by maximizing classi fi cation accuracy. Generally, wrapper methods are more effective than fi lter methods. An exhaustive search is not computationally feasible; therefore, wrapper methods must employ a search algorithm to search for an optimal subset of features. Wrapper methods can be classi fi ed into two categories based on search strategy: sequential search algorithms and stochastic algorithms. 2.2.1. Enumeration algorithms
Enumeration algorithms conduct a complete search for the optimal subset according to the evaluation function used. Different sequential search algorithms and stochastic algorithms are used to reduce the search space without jeopardizing the chances of fi nding the optimal subset [18]. Exhaustive search, as well as branch and bound, belong to this category. Branch and bound is equivalent to the exhaustive search method when the criterion function is monotonic but too time consuming. 2.2.2. Sequential search algorithms quadratically with the number of features [18]. Common sequential search algorithms include sequen-sequential forward fl oating search (SFFS). 2.2.3. Stochastic algorithms
A random search is simply an operation that involves selecting random potential solutions and evalu-ating them. The best solution over a number of samples is the result of a random search. A stochastic algorithm is nothing more than a random search assisted by chosen heuristics to guide the evaluation of the next potential solution. GA is a common stochastic algorithm. Yang and Honavar [16] introduced GA for the problem of feature selection. In the GA approach, a feature subset is represented by a binary string (called a chromosome) with length D . Oh et al. [9] used simple GA (SGA) for feature selection. However, GA is weak in fi ne-tuning near local optimum points, which results in long process times. To improve the fi ne-tuning capability of simple GAs, hybrid GAs (HGAs) were developed in their study. are: where X denotes the subset of selected features and Y denotes the set of remaining features. Hence, U = X  X  Y at any time. Further, J(X) denotes a function evaluating the performance of X, and J may evaluate choice of an evaluation function J depends on particular applications. By applying those in a proper sequence to accomplish local improvements in chromosomes, HGAs have the operations ripple rem and SFS, PTA, and SFFS to evaluate the effectiveness of SGA and four HGAs (HGA(1), HGA(2), HGA(3) ripple factor.

Complete feature selection algorithms can be further subdivided into  X  X numeration X  and  X  X on-enumeration X  algorithms. Non-enumeration algorithms include sequentialsearchalgorithms andstochas-tic algorithms. Sequential search algorithms fi nd fairly good solutions in a moderate amount of time. However, sequential search algorithms are not always superior to other algorithms. Stochastic algo-They sometimes fi nd better solutions than sequential search algorithms because sequential search algo-rithms are monotonic [18,19]. Therefore, this study uses a stochastic algorithm to solve the problem of feature selection.

PSO, similar to GA, is a population-based optimization algorithm inspired by bird fl ocking and fi sh schooling. PSO approaches converge faster than GA techniques, and require less computational complexity [21]. However, PSO has drawbacks, such as local optimal trapping due to premature con-vergence (the exploration problem) [22]. Several studies have investigated PSO in feature selection. Agra fi otis and Cedeno [4] fi rst applied PSO in feature selection. Melgani and Bazi [5] used PSO in classifying electrocardiogram signals. They proposed a PSO-based approach to optimize the performance ofSVMclassi fi ers. Cervantes etal. [1]proposed an AMPOS algorithm,based on differentapproaches,for problems.

To overcome the risk of premature convergence, previous studies suggest one primary goal: change traditional PSO operations to regroup swarms within a plausible subset of the original search space. 3. Simple PSO for feature selection 3.1. PSO algorithm
The PSO algorithm uses a population (called a swarm) of individuals (called particles) to fi nd the best solution. A particle represents a candidate solution to the problem being addressed. Assume that a search space is D -dimensional, and the i th particle of a swarm is a D -dimensional position vector X also consider that the best visited position that gives the best fi tness value for the particle is PB [ pb updated according to Eq. (1). reduces the velocity of particles gradually; hence; it keeps the swarm under control. The parameter w is usually between 0.4 and 0.9. Random variables r 1 and r 2 are distributed uniformly between 0 and 1. Upper and lower bound clamp velocity vectors are set to avoid very rapid movements of particles in the search space. Theref ore, the velocities of p articles are restri cted to the range of [ v particle moves to a new solution based on Eq. (2). where N is the size of the swarms. 3.2. Simple PSO for feature selection 3.2.1. Particles encoding
In applying PSO to the problem of feature selection, we use a binary digit to represent a feature. The bit values 0 and 1 represent non-selected and selected features, respectively. Each particle is coded to a binary alphabetic string. The PSO for the problem of feature selection in this study is called simple PSO (SPSO). For example, the particle 101000 with six features means that the fi rst and third features are selected. In addition, we update dimension d of particle i according to Eq. (3). where the sigmoid ( v new 3.2.2. Initial population
We initialize a population of particles randomly or in a manner well-adapted to the SPSO. However, a total randomized choice is made for bit values 0 a nd 1 from a probability of 0.5. In particular, if U (0,1) &gt; 0.5, then x 0 3.2.3. Measure the fi tness of each particle in population
The fi tness function in an SPSO is a measure of a solution to the objective function. Based on particle with the best fi tness values are recorded.

The pseudo code of SPSO for feature selection algorithm is given in Fig. 1. 3.3. k-nearest neighbor method
The k -nearest neighbor ( k -NN) method was introduced by Fix and Hodges [6]. The method is one of medicine [24]. When applying the k -NN method, we need to determine the number of nearest neighbors k .The k -NN method calculates the distances between the query instance and all the training samples. The method also arranges the distances and determines the categories of the nearest neighbors [17]. If the class of test data matches the expected class of the pattern, we assume that it will be counted as a correctly predicted example. The fi tness function is de fi ned as the accuracy of classi fi cation. rate of 1-NN classi fi ers.
 4. The improved PSO for feature selection
To overcome the problem of the premature convergence of PSO, we test the state of each feature (0  X  1or1  X  0); this approach is called the opposite sign test (OST). Take particle 101000 as an example. second state. We continue this process until all bits are changed sequentially.

This study proposes an improved PSO (IPSO) algorithm using OST. The proposed algorithm can fi nd better particles before velocities and positions are updated. The purpose is to promote more correct directions through OST. The OST procedure is described as follows: Step1: Set d = 1
Step2: Set X new
Step3: If x
Step4: If the fi tness value of X Step5: d = d + 1 Step6: If d &lt; D ,gotostep2;otherwisestop.
 The IPSO fl owchart using OST for feature selection is shown in Fig. 2. The IPSO pseudo code for the feature selection algorithm is given in Fig. 3.
 5. Experimental result and discussion 5.1. Environment
To evaluate the effectiveness of PSO algorithms, ten data sets were used to test the PSOs algorithms, which are summarized in Table 1. The ten data sets from UCI machine learning databases (http:// archive.ics.uci.edu/ml/) had data sizes ranging from hundreds to tens of thousands. They had no missing features and all features are numeric. Additionally the dimensionalities covered by the feature size had a large spectrum ranging from 10 to 649.

We followed the categorization of problem size describedby Kudo and Sklansky[19]. The problem can be divided into three sizes: small with 0 D &lt; 20 (Vowel, Wine, Letter, Vehicle, and Segmentation), medium with 20 D &lt; 50 (WDBC, Ionosphere, and Satellite), and large with D 50 (Sonar and Mfeatures).
 measured through the recognition rate of 1-NN. In addition, we used K -fold cross validation and random sampling. To ensure an impartial comparison of the classi fi cation results and to avoid random results, the K -fold cross-validation ( K = 5) strategy was adopted (except for both Segmentation and Satellite data sets; these two data sets represent the training and test samples, respectively). The advantage of this method over repeated random sub-sampling is that all observations were used for both training and validation.

To achieve higher IPSO optimization ability, we adopt ed two OSTs: forward opposite sign test (FOST) and random opposite sign test (ROST). The FOST algorithm is a simple test that aims to acquire a better optimum for each particle. The algorithm successively changes the particular states of particles in a If the fi tness value of the old particle ( X alteration of the new particle; otherwise, we retain the alteration of the old particle. The second bit bits are tested sequentially. The above example for the FOST process is shown in Fig. 4.
On the other hand, the ROST algorithm randomly changes the particular states of particles. Take particle 10100 as an example. We fi rst generate a random key, such as 32145. Following the order of the random key, the third bit changes fi rst from 1 to 0. If the fi tness value of the old particle ( X than that of the new particle ( X new is tested. We continue this process until all bits are changed. In this study, the SPSO algorithm using FOST and ROST are called improved PSO(1) (IPSO(1)) and improved PSO(2) (IPSO(2)), respectively. Our PSO heuristics were coded in Visual Studio C++ 2005 and run on a Core 2 Duo E6600 (2.4 Ghz) PC equipped with 2,048 MB of RAM running the Windows XP operating system. Parameter selection parameters settings were used: the two factors ( c 1 and c 2 ) were both set at 2, and the lower bound of velocity ( v 1.0 or the number of iterations reached the default value T . In this study, we expected that OST would enhance diversity adequately to allow an escape from the local optima and reduce the time of operation. Therefore, we set T for SPSO, IPSO(1), and IPSO(2) at 1,000, 100, and 100, respectively. 5.2. Numerical experiments To evaluate the effectiveness of PSO algorithms, this study compared the eight algorithms including SFS, PTA, SFFS, SGA, HGA(1), HGA(2), HGA(3), and HGA(4) with SPSO, IPSO(1), and IPSO(2). the d* value to set the population of particles in a swarm, and each swarm is expected to reveal the performance of PSOs. The implementation results for these algorithms on each data set are summarized in Table 2. Data on the eight algorithms (SFS, PTA, SFFS, SGA, HGA(1), HGA(2), HGA(3), and HGA(4)) were directly copi ed from Oh et al. [9]. Figure 5 shows the 0.95 con fi dence interval of the mean of classi fi cation accuracy. IPSO(2) clearly outperforms all other algorithms. Also, for the large data set, Mfeatures, which contains 649 features, a very high accuracy (around 98%).
 accuracy, two-way ANOVA was performed. We took the classi fi cation algorithm as  X  X actor X  and the data SFFS, SGA, HGA(1), HGA(2), and HGA(3). 5.3. Discussion In this study, we used ten data sets characterized by various kinds, data sizes, features, and classes. number of conclusions based on the results and analyses. 1. PSOs, including SPSO, IPSO(1) and IPSO(2), obtain high accuracy over 90% on all data sets except 2. PSOs outperform sequential search algorithms regardless of feature size. 3. PSOs outperform GAs (SGA and HGAs). In addition, IPSO(2) outperforms ten other algorithms. 4. IPSOs using OST are slightly better than SPSO. 5. The performance of GAs and sequential search algorithms can be dramatically affected by the 7. The proposed approach demonstrated good performance for high dimensional data set (Mfeatures PSO and GA algorithms both involve the following three operations: evaluating the population with a fi tness function, changing the population, and re-evaluating the population. The main difference between PSO and GA lies in how the population is changed. GA, an unconventional search technique proposed by Holland, is a stochastic search technique operating on individuals in a given population. Each individual in the population is called a chromosome, and represents a solution to a given problem. Mutation and crossover are stochastically chosen to create new chromosomes that are desired to be better convergence of GA is highly dependent on the correct choice of parameters; certain parameters may be more appropriate in some cases depending on the dimension of the problem. Improper GA parameters may lead to local optima [22].

Unlike GA, PSO contains no genetic operators, such as mutation and crossover. Particles are updated by relying on the fact that each particle within a swarm has a memory; hence, the information-sharing mechanism is signi fi cant in PSO. In contrast, chromosomes share information with one another in GA. GAs. PSO may encounter the premature convergence problem, but two OSTs are designed into PSO as a compensation. Applying the FOST and ROST algorithms improves the perturbation for each particle; hence, better solutions are obtained. Differences between GA and PSO are shown in Table 5. 6. Conclusion
This study proposes an IPSO algorithm using OST. The OST can be utilized to increase population diversity in the PSO mechanism and to avoid local optimal trapping by improving the jump ability of fl ying particles. The experiment shows the average accuracy of the proposed IPSO approaches 93%, selection. It can handle highly dimensional data set as well. The proposed approach not only overcomes the disadvantages of PSO, but also improves feature selection performance.

The novel contributions of this paper include: (1) The development of the two opposite sign test (OST): forward opposite sign test (FOST) and random opposite sign test (ROST); (2) To overcome the drawbacks of PSO, we proposed an improved PSO (IPSO) algorithm using the OST. The test can increase population diversity in the PSO mechanism; (3) The performance of IPSO is shown to be comparable to that of GA and HGA algorithms when dealing with data sets that have diverse sizes and large numbers of classes.

Further studies may be conducted in the future. Fir st, with different parameter settings, PSO may constant weight factors are worth developing. Second, adding new search algorithms in PSO, such as swarms with mixed particles, may further enhance the effectiveness. Third, the execution time of the proposed PSO algorithm for small and medium-sized data set is satisfactory; however, the improvement in the execution time for the large-sized data set could be another future research subject. Acknowledgments This work was supported in part by the National Science Council, Taiwan, under grant NSC 99-2410-H-030-058.
 References
