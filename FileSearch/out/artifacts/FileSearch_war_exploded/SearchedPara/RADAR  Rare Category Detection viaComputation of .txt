 Rare category detection is a n interesting task which is derived from anomaly detection. This task was firstly proposed by Pelleg et al [2] to help the user to select useful and interesting anoma lies. Compared with traditional anomaly detection, it aims to find representative data points of the compact rare classes that differ from the individual and isolated instances in the low-density regions. Furthermore, a human expert is required for labeling the selected data point under a known class or a previously undiscovered class. A good rare category detection algorithm should discover at least one example from each class with the least label requests.
 Rare category detection has many applications in real world. In Sloan Digital Sky Survey [2], it helps astronomer to find the useful anomalies from mass sky sur-vey images, which may lead to some new astronomic discoveries. In financial fraud detection [3], although most of the financial transactions are legitimate, there are few fraudulent ones. Compared with checking them one by one, using rare cate-gory detection is much more efficient to de tect instances of the fraud patterns. In intrusion detection [4], the authors adopted an active learning framework to se-lect  X  X nteresting traffic X  from huge volu me traffic data sets. Th en engineers could find out the meaningful malicious network activities. In visual analytics [5], by lo-cating the attractive changes in mass remote sensing imagery, geographers could determine which changes on a particular geographic area are significant.
Up until now, several approaches have been proposed for rare category detec-tion. The main techniques can be categori zed into model-based [2], density-based [7] [8] [9], and clustering-based [10] methods. The model-based methods assume a mixture model to fit the data, and select the strangest records in the mix-ture components for class labeling. However, this assumption has limited their applicable scope. For example, they require that the majority classes and the rare classes be separable or work best in the separable case [7]. The densities-based methods employ essentially a local-d ensity-differential -sampling strategy, which selects the points from the regions where the local densities fall the most. This kind of approaches can discover examples of the rare classes rapidly, de-spite non-separability with the majority classes. But when the local densities of some rare classes are not dramatically higher than that of the majority classes, their performance is not as good as that in the high density-differential case. The clustering-based methods first perfor m a hierarchical mean shift clustering. Then, select the clusters which are compa ct and isolated and query the cluster modes. Intuitively, if each rare class has a high local density and is isolated, its points will easily converge at the mode of density by using mean shift. But in real-world data sets, it is actually not the case. First, the rare classes are often hidden in the majority classes. Second, if the local densities of the rare classes are not high enough, their points may converge to the other clusters. In a word, although the density-based and clustering-based methods work reasonably well compared with model-based methods, their performance is still affected by the local densities of the rare classes.

In order to avoid the effect of the local densities of the rare classes, we propose a density insensitive approach called RADAR. To the best of our knowledge, RADAR is the first sophisticated density insensitive method for rare category detection. In our approach, we use the change in the number of RkNN to estimate the boundary degree for each data point. The point with a higher boundary degree has a higher probability to be the boundary point of the rare class. Then we sort the data points by their boundary degrees and query their class labels with human experts.

The key contribution of our work is twofold: (1) We proposed a density insensitive method for rare category detection. (2) Our approach has a higher efficiency on finding new classes, effectively re-The rest of the paper is organized as fo llows. Section 2 formalizes the problem and defines its scope. Section 3 explains the working principle and working steps of our approach. In Section 4, we compare RADAR with existing approaches on both synthetic data sets and real data sets. Section 5 is the conclusion of this paper. Following the definition of He et al. [7], we are given a set of unlabeled examples S= { x 1 , x 2 ,..., x n } , x i  X  R d ,whichcomefrom m distinct categories, i.e. y i = { 1,2,..., m } . Our goal is to find at least one example from each category by as few label requests as possible. For c onvenience, assume that there is only one majority class, which corresponds to y i = 1, and all the other categories are minority classes with prior p c , c= 2,..., m .Let p 1 denote the prior of the majority category. Notice that p i , i = 1, is much smaller than p 1 .
Our rare category detection strategy i s selecting the points which get the highest boundary degree for labeling. To understand our approach clearly, we introduce the following definitions to be used for the rest of the paper. Definition 1. (Reverse k-nearest neighbor)Th e reverse k-nearest neighbor (RkNN) of a point denoted as [6]: Given a data set DB ,apoint p ,aposi-tive integer k and a distance metric M , reverse k-nearest neighbors of p , i.e. RkN N p ( k ), is a set of points p i that p i  X  DB and  X  p i , p  X  kNN p i ( k ), where kNN p i ( k ) are the k-nearest neighbors of points p i .
 Definition 2. (Significant point) A point is significant point if its point number of RkNN is above a certain threshold  X  : Definition 3. (Coarctation index) The coarctation index of a point p , i.e. CI , denoted as: Definition 4. (Uneven index) The uneven index of a point p , i.e. UI , denoted as: Definition 5. (Boundary degree) The boundary degree of a point p , i.e. BD , denoted as: 3.1 Working Principle In this subsection, we explain why we have adopted a RkNN-based measurement for boundary degree, and illustrate the reason for adopting the conception of significant point.
 RkNN-based boundary point detection. RkNN has some unique properties [6]: (1)The cardinality of a point X  X  rev erse k-nearest neighbors varies by data distribution. (2)The RkNN of the boundary points is fewer than that of the inner points.

The first property means that RkNN only considers the relative position be-tween data points, and thus has nothing to do with the Euclidean distance or the local density. There is a simple example: two clusters have the same data distri-bution but differ in absolute distances between data points. Obviously, although the two clusters have different densities, the kNN and the RkNN of correspond-ing data points in two clusters remain the same. According to Definition 3, 4 and 5, CI , UI and BD of corresponding data points in two clusters are equal too. In other words, CI , UI and BD are not designed to evaluate the local den-sities of data points. Instead, they are used to find the local regions where data distribution changes.

The second property is the reason why the boundary degree can be used to detect the change in the data distribution. Generally, the kNN of boundary points include partial inner points of the cluster, several outer points and some other boundary points nearby. These three types of data points are very different in the number of RkNN. By contrast, usually an inner point X  X  kNN are still inner points. According to Definition 5, UI indicates the variation of the number of RkNN between a data point and its kNN. Thus, the boundary points have higher UI than the inner points. On the other hand, CI of a data point represents the sum of the number of RkNN. When the number of kNN is fixed, a point with more inner nearest neighbors has a higher CI .Inotherwords, CI of inner points is higher than that of the boundary points. Therefore, the boundary points have higher UI ,lower CI , and thus higher BD . By querying the class labels for the data points with high BD , we can discover examples of rare classes hidden in the majority class. Just like using radar to scan the data set, we do not consider the situation in which the local data distribution is even. However, if there is some changes in local data distribution, we will get a feedback signal and locate the target.
 Significant point. Before discussing the significant points which have more than  X  RkNN, we begin with an example illustrated in Fig. 1 which comes from literature [6]. When k = 2, the Table 1 shows the kNN and the RKNN of each point in Fig. 1. The cardinality of each point X  X  RkNN is as follows: p 2 , p 3 , p 5 and p 7 has 3 RkNN; p 6 has 2 RkNN; p 1 and p 4 has 1 RkNN; p 8 has none. Notice that p  X  X  nearest neighbors are in a relati vely compact cluster consisted of p 5 , p 6 , p 7 . However, points in this cluster are each other X  X  kNN. Since the capacity of each point X  X  kNN list is limited, p 8 is not in the kNN lists of its nearest neighbors and thus has no RkNN. According to Fig. 1, it is hard to say that p 8 is a candidate of minority-class points. In other words, if the RkNN of a point is extremely few, it means this point is relatively far from the other points. It is not significant to query its class label because of the low probability of this point belonging to a compact cluster. Therefore, in our approach, we will query a point only if it X  X  a significant point in advance. 3.2 Algorithm The RADAR algorithm is presented in Algorithm 1. It works as follows. Firstly, we estimate the point number k i of each minority category i . Then, we find the k i -nearest neighbors for each point, and calculate the number of RkNN. Furthermore, we calculate the r i for each minority class. r i is the global minimum distance between each point and its k th i nearest neighbor. It will be used for updating the querying-duty-exemption list EL in Step 13. In the outer loop of Step 9, firstly, we choose the smallest undiscovered class i and set k to be the corresponding k i . Then, in Step 11, we calculate each point X  X  boundary degree ( BD ), and determine which points are significant points. By setting BD of the non-significant points to be negative infinity, we can prevent them from the Algorithm 1. Rare Category Detection via Computation of Boundary Degree (RADAR) class-label querying and thus save the querying budget. In addition, setting the parameter w to be 1 is a suitable experimental choice. In the inner loop of Step 12, we query the point which has the maximum boundary degree with human experts. When we find an example from a previously undiscovered class, we quit the inner loop. In order to reduce the number of queries caused by repeatedly selecting examples from the same disc overed category, we employ a discreet querying-duty-exemption strategy: (1) in Step 8, we build a empty point list EL to record the points which do not need to be queried; (2) in Step 13, if a point x j from class y j is labeled, the points falling inside a hyper-ball B of radius r y j centered at x j will be added into EL .

A good exemption strategy can help us to reduce the querying cost. But if the exemption strategy is greed y, more points near to the labeled points will be added into EL . Then, the risk of preventing some minority classes from querying will be higher, especially when the minority classes are near to each other. In order to avoid such case, we should ensure that the number of querying-duty-exemption points will not be too large. In our discr eet exemption strat egy, when we label a point under a minority class i , the number of points in the hyper-ball B will not be more than k i , i.e. | B | X  k i . The reason is that the radius r i is the global minimum distance between each point and its k th i nearest neighbor. When we label a point under the majority class, we do the querying-duty exemption more carefully because this point is usually close to a rare category X  X  boundary. We do not set the corresponding radius of B to be min sake of discreetness, we set the r 1 =min m i =2 r i so that the nearby rare-category points can keep their querying dut ies completely or partially. In this section, we compare RADAR with NNDM (density-based method pro-posed in [7]), HMS (clustering-based method proposed in [10]) and random sam-pling (RS) on both synthetic and real dat e sets. For RS, we run the experiments 50 times and take the average numbers of queries as the results. 4.1 Synthetic Data Sets In this subsection, we compare RADAR with NNDM, HMS and RS on 3 syn-thetic data sets.

In Fig. 2(a) and Fig. 2(b), the first two synthetic data sets contain the same majority class, which has 1000 examples (green points) with Gaussian distribu-tion. Each minority class (red points) in Fig. 2(a) has 20 examples, which fall inside a hyper-ball of radius 5. In Fig. 2(b), we double the distances between the 20 examples of each minority class (red p oints). Then, each minority class in the second data set falls inside a hyper-ball of radius 10. Obviously, the densities of minority classes in Fig. 2(a) are about 4 times higher than that in Fig. 2(b).
The corresponding comparison results are illustrate in Fig. 3(a) and Fig. 3(b) respectively. The curves in Fig. 3 show the number of discovered classes as the function of the number of selected examples which is equal to the number of queries to the user. To discover all the c lasses in the first two data sets, RS needs 101 and 100 queries respectively; HMS n eeds 62 and 89 queries respectively; NNDM needs 10 and 31 queries respectively; RADAR needs 8 and 10 queries respectively. From these results we ca n see that the performance of NNDM and HMS are dramatically affected by the local densities of the rare classes. By con-trast, RADAR and RS are more insensitive to these local densities. Furthermore, our approach is much more sophisticated than the straightforward method RS, and has a high efficiency on finding new classes.

The third synthetic data set in Fig. 4(a) is a multi-density data set. The majority class has 1000 examples (green points) with Gaussian distribution. Each minority class (red points) has 2 0 examples and a different density from each other. The comparison results are shown in Fig. 4(b). From this figure, we can learn that the performance of RA DAR is better than NNDM, HMS and RS in this multi-density data set. To find all the classes, RS needs 103 queries; HMS needs 343 queries; NNDM needs 55 queries; RADAR needs 17 queries. 4.2 Real Date Sets In this section, we compare RADAR with NNDM, HMS, and RS on 4 real data sets from the UCI data repository [1]: the Abalone, Statlog, Wine Quality and Yeast data sets. The properties of these data sets are summarized in Table 2. In addition, the Statlog is sub-sampled because the original Image Segmenta-tion (Statlog) data set contains almost the same number of examples for each class. The sub-sampling can create an imbalanced data set which suits the rare category detection scenario. With the sub-sampling, the largest class in Statlog contains 256 examples; the examples of the next class are half as many as that of the former one; the smallest classes all have 8 examples. The results are sum-marized in Table 3. The mark  X - X  indicates that the algorithm cannot find out all classes in the data set.

These real data sets are multi-densit y data sets. To estimate the local den-sity of each minority class, we adopt a measurement for the local density of a data point. We first calculate the average distance between a data point and its k -nearest neighbors. Next, multiply the reciprocal of this average distance by the global maximum distance between the points. The product is roughly in proportion to the local density of the data point. Finally, we calculate aver-age value of the products for each minority class and take this value as the local-density metric. For the sake of generalization and convenience, we set k = min { k i | 2  X  i  X  m } . Fig. 5 shows the local-density values of the mi-nority classes in real each data set. The standard deviation of these local-density values is 55.32 in Abalone, 20.47 in Statlog, 28.03 in Wine Quality and 3.99 in Yeast. Therefore, the Aba lone data set is more  X  X xtreme X  on the change of local densities than the other data sets. By contrast, the Yeast data set is the most  X  X oderate X  one.
 Fig. 6 illustrates the comparison results on the 4 real data sets in details. From this figure, we can learn that RADA R effectively reduces the number of queries to human experts in each data set . It takes the least number of queries to discover all classes in Abalone, Statlog and Wine Quality. In Yeast, which is the most  X  X oderate X  date set, the HMS method owns the highest efficiency of finding new classes. But in Abalone, which is the most  X  X xtreme X  data set, HMS is not as good as RADAR, NNDM or even the RS method. Furthermore, in Wine Quality, the NNDM method falls short for its performance, and finds out only 5 classes.

In summary, RADAR has a high efficiency on finding new classes, and is more suitable for processing multi-density data because of its stability. We have proposed a novel approach (RADAR) for rare category detection. Com-pared with existing algorithms, RADAR is a density insensitive method, which is based on reverse k-nearest neighbors (RkNN). In this paper, the boundary degree of each point is measured by variation of RkNN. Data points with high boundary degrees are selected for the class-label q uerying. Experimental results on both synthetic and real-world data sets demonstrate that the number of queries has dramatically decreased by using our approach. Moreover, RADAR has a more attractive property. It is more suitable to handle the multi-density data sets. Future works involve adopting a technique of parameter automatization to set the parameter w and adapting our approach to the prior-free case.

