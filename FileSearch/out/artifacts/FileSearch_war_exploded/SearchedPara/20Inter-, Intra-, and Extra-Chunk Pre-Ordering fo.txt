 CHENCHEN DING, KEISUKE SAKANUSHI, HIRONA TOUJI, and MIKIO YAMAMOTO , The state-of-the-art technique of Statistical Machine Translation (SMT) [Koehn et al. 2003, 2007] demonstrates good performance on translation of languages with relatively similar word orders [Koehn 2005]. However, word reordering is still a problematic issue for those language pairs with significantly different word orders. A typical example is translation between Japanese and English because Japanese has a Subject-Object-Verb (SOV) word order and English has a Subject-Verb-Object (SVO) word order. As a consequence, correct translation between Japanese and English cannot be achieved unless the word order is correctly modified.

Many efforts have been made to resolve the word reordering problem in SMT. Broadly, these efforts are categorized into language-independent approaches and language-dependent approaches. Language-independent approaches learn both a reordering model and a translation model from the aligned training parallel corpus [Tillmann 2004; Al-Onaizan and Papineni 2006; Xiong et al. 2006; Galley and Manning 2008], or by combining the reordering and translation in a unified framework [Chiang 2007]. In contrast, language-dependent approaches rely on analysis of the syntax or semantics of the languages involved in a translation task [Quirk et al. 2005; Liu et al. 2006; Xiong et al. 2012; Yang et al. 2012]. Although introducing linguistic information will necessi-tate additional analysis, such as parsing, these approaches efficiently handle difficult word reordering tasks for particular language pairs; with the help of well-developed, high-precision linguistic analysis tools.

In addition to approaches that use linguistic information, another line of research handles the word reordering as a separate process. That is, the process of translating words is conducted separately from the process of reordering words. There are pre-ordering approaches (e.g., Isozaki et al. [2012]) and post-ordering approaches (e.g., Sudoh et al. [2013]), depending on whether the reordering process precedes translation. Many pre-ordering approaches, both language-independent and language-dependent, have been proposed. Language-independent pre-ordering approaches typically utilize designed statistical models to learn and conduct the reordering (e.g., Neubig et al. [2012]); language-dependent approaches typically utilize a trained parser with reordering rules, which can be either manually designed (e.g., Isozaki et al. [2012]) or automatically extracted 1 (e.g., Genzel [2010] and Wu et al. [2011]). Generally, a rule-based pre-ordering approach offers fast and crisp reordering, but parsing errors and a lack of robustness are drawbacks. In contrast, a statistical pre-ordering approach is more robust but also slower. Further, training data are an important factor for the feasibility of pre-ordering approaches. Statistical pre-ordering approaches always need training data, the quality of which significantly affects performance. Manually constructed data [Neubig et al. 2012] or a refinement of automatically generated data [Navr  X  atil et al. 2012] are needed during model training to achieve good performance. A rule-based approach with automatically extracted rules also needs training data [Genzel 2010]. However, rule-based approaches with manually designed rules do not need training data. Instead, they require a reliable parser and elaborately designed rules. In recent research, interesting results can be found, such as that even with a huge training dataset, automatically extracted rules do not always outperform manually designed rules (e.g., the comparison of Genzel [2010] and Xu et al. [2009] in Table 4 of Genzel [2010]); and a statistical pre-ordering approach does not always outperform a rule-based approach [Lerner and Petrov 2013].

In this article, we investigate the manual rules for a rule-based pre-ordering approach to Japanese-to-English translation. One very effective approach, head finalization has been proposed for English-to-Japanese translation [Isozaki et al. 2012]. The approach takes advantage of the head-final property of Japanese on the target side. It designs a head-finalization rule to move the head word based on the parsing result by a head-driven phrase structure grammar parser. However, head finalization cannot be applied to the pre-ordering in the reversed Japanese-to-English translation task. Many pre-ordering approaches for Japanese-to-English translation take advantage of the chunk 2 in the source-side Japanese sentences. Such approaches use inter-chunk rules to arrange the order of chunks in an English-like order [Katz-Brown and Collins 2008; Sudoh et al. 2011]. Further, intra-chunk rules are also designed in some approaches to arrange the morphemes within a chunk in a way that realizes a more correct reordering [Katz-Brown and Collins 2008; Hoshino et al. 2013]. In this study, we propose an extra-chunk reordering scheme for morphemes, which can move morphemes out of the chunk that they belong to. We give linguistically oriented discussions to demonstrate that the proposed extra-chunk rule is actually required for correct Japanese-to-English pre-ordering. We test our approach by exper-imental comparison with five different rule-based pre-ordering approaches designed for Japanese-to-English translation and with a statistical language-independent pre-ordering approach on a standard patent dataset and on a news dataset obtained by crawling Internet news sites. Two state-of-the-art SMT systems, the Phrase-Based (PB) SMT and the hierarchical phrase-based SMT (HIERO 3 ), are used in experiments. All tested approaches are evaluated by automatic reordering measures: Kendall X  X   X  [Isozaki et al. 2012; Hoshino et al. 2013], Spearman X  X   X  , fuzzy reordering score [Talbot et al. 2011], test set RIBES [Isozaki et al. 2010], and the automatic translation precision measure of test set BLEU score [Papineni et al. 2002]. Experimental results show that the proposed approach outperforms the other approaches on different measures, regardless of SMT systems and datasets. We present detailed discussions to reach a conclusion, that the movement of morphemes and noun chunks is im-portant and indispensable when designing a pre-ordering approach for the Japanese language.
 This article is organized as follows. In Section 2, we give an introduction to the Japanese language as background for the linguistic discussions in this article. In Section 3, we discuss the reference approaches used in this article X  X  experiments. In Section 4, we describe the proposed approach. In Section 5, we show the experiments X  settings and the results. In Section 6, we discuss different aspects based on the experimental results. Section 7 contains the conclusion and discussion of future work. The Japanese language is a typical head-final, agglutinative language. In the orthog-raphy of Japanese, there is no space used in a sentence as is done in many European languages to show the boundary of words. That is, word is not a natural unit of a sen-tence in Japanese. Generally, the basic unit in a Japanese sentence is morpheme .As to the agglutinative character of Japanese, the functional morphemes succeed content morphemes. Such a sequence of morphemes forms a chunk , which becomes a relatively independent unit of meaning in a sentence. 4 Further syntactic or semantic analysis in Japanese is traditionally based on the chunk level. So, the steps of a Japanese sentence analysis follow a pipeline:
As to the syntactic analysis, rather than the constituency-based parsing, the chunk-based dependency parsing is more suitable for Japanese and widely used in practice. This is because Japanese has a relatively free order of chunks in its expression as long as the head-final restriction is satisfied. So, the Japanese is usually described as an SOV language, though the OSV order is also grammatically correct. 5 The dependency structure of a sentence also forms a foundation of semantic analysis, for example, predicate-argument analysis in Japanese. We show a Japanese sentence example in Figure 1 with the analysis of it. There have been many pre-ordering approaches proposed in Japanese-to-English trans-lation. Here, we first refer to several rule-based ones.
 Two pre-ordering approaches are proposed in this article. The first approach is a very simple one called reverse pre-ordering , requiring only morphological analysis of a Japanese sentence. The approach focuses on a special morpheme, the topic marker wa in Japanese. It reverses the morpheme sequences before and after the topic marker separately. That is, the parts before the topic marker and the part after the topic marker are reordered backwards, but the reordering will not cross the topic marker.
The second approach is a dependency tree pre-ordering , requiring chunk-level depen-dency parsing. The approach utilizes inter-chunk reordering rules for verb and noun separately. The verb rule is a series of if-else judgments to put a verb after its topic, sub-ject, or before its direct object and all the other verbs it governs; the noun rule simply moves a noun before all its modifiers, which may be referred to as head initialization . The approach also introduces the intra-chunk reordering to reverse the morphemes within a chunk.
 They design a more detailed inter-chunk reordering rule for verbs. The approach also requires a chunk-level dependency tree. The chunk precedence (shown by &gt; here) is defined as follows for reordering chunks. They give detailed pseudocode to show how these syntactic roles of chunks should be figured out by the part of speech of morphemes in corresponding chunks. No intra-chunk reordering for morphemes is used in this approach.
 This is an early attempt to utilize the predicate-argument analysis in the Japanese-to-English pre-ordering. They propose a predicate-argument-based inter-chunk rule for verbs in their approach. Specifically, the nominative case chunk ( NOM ), the accusative case chunk ( ACC ), and the locative case chunk ( LOC ) of a predicate are detected and arranged in order as follows: First, NOM and ACC are arranged immediately before and after the predicate; then, LOC is moved after the predicate. Also, the approach does not use intra-chunk rules for detailed reordering of morphemes.
 Their approach is also based on predicate-argument analysis and can be viewed as a development of Komachi et al. [2006]. They use both inter-chunk and intra-chunk rules in this approach. For the inter-chunk reordering, first head initialization is conducted and then detailed arrangement is applied. Generally, the inter-chunk reordering can be considered as a combination of Katz-Brown and Collins [2008] and Komachi et al. [2006]. The intra-chunk reordering in this approach is more reasonable than the simple reversing in Katz-Brown and Collins [2008]. As we have described, the functional morphemes always succeed the content morphemes in a chunk. So, the approach halves a chunk to a content part and a functional part and reverses the two parts. As a result, the functional part comes before the content part, which can be considered as an analog of a preposition or an auxiliary verb in English.
 Besides these listed rule-based pre-ordering approaches designed for Japanese-to-English translation, statistical approaches are also proposed for the pre-ordering task. In Neubig et al. [2012], they propose a method to construct a discriminative parser for pre-ordering from aligned training parallel corpus. The approach takes the deriva-tion tree as a latent variable and trains a model to maximize reordering measures. In Navr  X  atil et al. [2012], an approach taking the reordering problem as a traveling salesman problem is given. Both of the approaches need relatively high-quality train-ing data (i.e., a word-aligned parallel corpus). In the experiments reported in Neubig et al. [2012], they show that a model trained by a manually aligned parallel corpus outperforms a model trained by an automatically aligned parallel corpus of more than 10 times the size. In Navr  X  atil et al. [2012], they also introduce an approach to conduct sentence preselection for the reordering model training to achieve good performance. In the discussed rule-based pre-ordering approaches, it can be observed that once the chunk is taken as a reordering unit, the reordering rules are naturally divided into inter-chunk and intra-chunk rules. With these two types of rules, the morphemes within a chunk will always remain in the same chunk, never moving across the boundary of the chunk they belong to. By the example in Figure 2, we demonstrate that these two types of rules are not sufficient to achieve a linguistically correct pre-ordering from the Japanese order to an English-like order in all cases.

The phenomenon demonstrated in Figure 2 reveals a problem that cannot be resolved by inter-and intra-chunk reordering. That is, the functional morphemes in a chunk syntactically govern not only the content morphemes within their own chunk, but also those modifier chunks of their own chunk. Thus, for a chunk without modifier chunks, intra-chunk reordering will be enough, where the intra-chunk reordering becomes equivalent to the extra-chunk reordering. 6 However, for a chunk with many modifiers, a functional morpheme, such as one equivalent to a preposition or a conjunction in English, must be moved to the boundary of the range it governs to agree with the English-like order. In this situation, extra-chunk reordering is indispensable. More generally, the movement of morphemes should not be confined to the boundary of the chunk, and it is desirable that the intra-and extra-chunk reordering should be handled in a unified framework for morpheme reordering.
 After the preceding discussion, we now describe our pre-ordering approach. Algorithm 1 gives the overall process, where the foreach loop from line 1 to line 12 is the inter-chunk reordering process and the process from line 13 to line 16 is the combined intra-and extra-chunk reordering process. The intra-and extra-chunk reordering is conducted in a unified framework in which chunks are first segmented (line 14) to identify the intra-and extra-chunk parts and then moved to the corre-sponding destination (line 16). Generally, all the chunks in an input sentence will be examined within Algorithm 1 and processed by the corresponding algorithms according to Algorithm 2, then all the chunks will be processed by Algorithm 6 and Algorithm 7. For all the inter-, intra-, and extra-chunk rules listed in the following, the reordering processes involve a certain head chunk and all of its modifier chunks; that is, the pre-ordering approach manipulates one order head-modifier relation in a dependency structure. In this article, the expressions  X  head morpheme  X  X nd  X  functional morpheme  X  are, respectively, conceptually identical to the  X  headword  X  X nd  X  functionword  X  used in Algorithm 1 of Sudoh et al. [2011].

Three kinds of inter-chunk pre-ordering rules are used in the proposed approach: rules for verb-, noun-,and copula-chunk pre-ordering. Algorithm 2 identifies of the kinds of chunks from among these types. The verb-chunk rule illustrated in Algorithm 3 mainly combines features from previous works. In the rule, the modifier chunk with conjunction morphemes takes the front position, as in Sudoh et al. [2011] ( Sup in Algorithm 3), and the subject, head verb, direct object, and indirect object are arranged as in Komachi et al. [2006] and Sudoh et al. [2011] ( Core in Algorithm 3). The topic of a verb is also considered in the verb-chunk rule as is done in Katz-Brown and Collins [2008] ( Pre in Algorithm 3). A noun-chunk rule is also designed for the proposed approach. This rule is more detailed than the previous head-initialization rule in Katz-Brown and Collins [2008] and Hoshino et al. [2013]. We show the noun-chunk rule in Algorithm 4. Essentially, the difference between Algorithm 4 and Algorithm 3 occurs around the contents of only the Core part in the process. In the noun-chunk rule, adjective morphemes are arranged before the head noun and modifiers with the genitive case marker no are arranged after the head noun. This is because we consider the Japanese genitive case marker no as corresponding to the English preposition of .We also carefully handle prenominal morphemes in Japanese, which work as determiners in English (e.g., Japanese kono with the meaning this , and Japanese sono with the meaning that ). The order of the coordinating structure is not disturbed in the noun-chunk rule. Because Japanese is a strictly head-final language, the head of phrases in a coordinating structure is always taken by the last phrase, so we arrange the  X  parallel marker  X  to precede other morphemes in the noun-chunk rule X  X  Core . Additionally, a copula-chunk rule is designed and illustrated in Algorithm 5. This rule can be regarded as a simplified verb-chunk rule. Because there is no verb in Japanese that works like to be does in English, Japanese uses functional morphemes adhering to a noun to express the meaning of the English to be . Because a chunk with copula morphemes still has a noun morpheme as its head morpheme, we separate the copula-chunk and give it a reordering rule similar to that for the verb-chunk.
 An example of the verb-chunk rule (and the copula-chunk rule) is illustrated in Figure 3, and an example of the noun-chunk rule is illustrated in Figure 4. Because real sentences rarely require all of the features (i.e., different kinds of modifier chunks) processed by the inter-chunk rules, the figures are only diagrams rather than real examples. However, they show the essential arrangements of the inter-chunk rules.
Algorithm 6 is utilized to segment a chunk in order to identify the parts of morphemes to which intra-and extra-chunk rules in Algorithm 7 should be applied. For a chunk with a verb, adjective, or copula morpheme, the functional morphemes will be moved forward: conjunctive particles (e.g., Japanese ba , which has meaning if ) will be treated by extra-chunk movement and others are treated by intra-chunk movement (the foreach loop from line 4 to line 10). For noun morphemes, the functional morphemes will be treated by extra-chunk movement: the parallel markers (e.g., Japanese to which means and ) point backward and others point forward (the foreach loop from line 12 to line 18). After identification by Algorithm 6, Algorithm 7 is applied in a top-down manner to the inter-chunk reordered dependency tree so as to finally accomplish the intra-and extra-chunk reordering.

We use the expressions  X  leftmost  X  X nd X  rightmost  X  in Algorithm 7 to refer to the corresponding chunks in the inter-chunk reordered dependency tree (i.e., D r ). Thus, they are the modifier chunks at the corresponding positions, that is, the leftmost and rightmost modifier chunks of a head chunk after inter-chunk reordering. For a better understanding, we give a specific example in Figure 5, which illustrates the segmenta-tion of Algorithm 6 and the movement of morphemes in extra-chunk reordering during Algorithm 7. We compared the proposed pre-ordering approach with five Japanese-to-English oriented rule-based pre-ordering approaches: the reverse pre-ordering in Katz-Brown and Collins [2008], two dependency tree-based approaches (in Katz-Brown and Collins [2008] and Sudoh et al. [2011]), and two predicate-argument-based approaches (in Komachi et al. [2006] and Hoshino et al. [2013]). For further comparison, we also applied the statistical approach presented in Neubig et al. [2012].

We conducted experiments on two different datasets along with two different types of SMT systems to investigate the characteristics of different comparison approaches under different situations. Specifically, we conducted experiments using PB SMT [Koehn et al. 2003] and HIERO [Chiang 2007] on the Japanese-English parallel corpus of the NTCIR-7 patent machine translation task [Fujii et al. 2008]. The parallel corpus is a standard dataset composed of 1 . 8 million sentence pairs for the training set, 915 sentence pairs for the development set, and 1 , 381 sentence pairs for the test set (the fmlrun set). Due to the restricted field of the NTCIR-7 corpus, it features long sentences with rigid syntactic structures. We also conducted experiments by using PB SMT on a news dataset, which was obtained by crawling the site of the DONG-A ILBO. 7 Several supplementary results of using HIERO, and from experiments with cross-domain application of LADER, were also obtained for the dataset. Compared with the NTCIR-7 corpus, where the expressions are fixed and sentences are mainly literally translated, this dataset contains much more diversity in expressions and freer translations. We collected Japanese and English article pairs on the site within the period from February 2012 to September 2013, and used the approach of Utiyama and Isahara [2007] to obtain the aligned sentence pairs for our experiments. We took the data for February 2012 to May 2013 as a training set, the data for June to July 2013 as a development set, and the data for August to September 2013 as a test set. In the three sets, a portion of sentence pairs with low alignment scores were excluded during training and evaluation. There were finally 400 , 000 sentences in the training set, 2 , 000 in the development set, and 2 , 000 in the test set, which together make up approximately 80%, 50%, and 50%, respectively, of the original raw data. We refer to this dataset as the DONG-A set in this article.
 We used PB SMT and HIERO in Moses 8 [Koehn et al. 2007] as baseline SMT systems. For all the tested approaches, after the pre-ordering, the translation model was learned by Moses X  training script. Word alignment was automatically generated by GIZA++ 9 [Och and Ney 2003] with the default setting of Moses, and symmetrized by the grow-diag-final-and heuristics [Koehn et al. 2003]. In PB SMT, the max-phrase-length was set to 5 and the reordering model was trained with using the msd-bidirectional-fe option. In HIERO, the max-phrase-length was set to 5 and the MaxSpan was set to 10. Due to the model often becoming too large in HIERO, we used only one-tenth of the training data from the NTCIR-7 corpus in training the HIERO model. 10 In decoding, the ttable-limit was set to 10 and the stack was set to 100 for Moses X  decoders. The language model used in decoding was an interpolated modified Kneser-Ney discounting 5-gram model, trained on the English side of the two training corpora by SRILM 11 [Stolcke 2002]. MERT [Och 2003] was used to tune the feature weights for the development sets and the translation performance was evaluated on the test sets with the tuned weights. The decoding settings were identical for the development and test sets in our experiments. Under the described settings, the baseline PB SMT and HIERO of Moses reached best test set BLEU scores of 28 . 8and28 . 6, respectively, on the NTCIR-7 corpus, which are notably higher than the 27 . 14 (by PB SMT) reported in Fujii et al. [2008], obtained by the organizer X  X  Moses system in 2008. We attribute the improvement in PB SMT to the significant development of the Moses system since then. Because HIERO typically has a better performance than PB SMT in Japanese-to-English translation [Ding et al. 2011], we think the performance of the baseline HIERO system is reasonable and satisfactory as well, even though it was obtained by using only one-tenth of the training data of NTCIR-7.

For the processing of the source-side Japanese sentences, we used MeCab 12 (IPA dictionary) for morpheme analysis, CaboCha 13 [Kudo and Matsumoto 2002] for the chunking and dependency parsing, and SynCha 14 [Iida and Poesio 2011] for predicative-argument analysis. We used LADER 15 to apply the approach of Neubig et al. [2012]. In handling the training data for LADER, we used the approach of Navr  X  atil et al. [2012] and selected the 1 , 000 sentence pairs with the highest trans-lation scores from the automatically aligned training corpus. Because short sentence pairs tend to have higher translation scores, the 1 , 000 training sentence pairs were chosen so as to comprise 500 pairs with 10 X 30 words and 500 sentences with 31 X 50 words. We evaluated the performance of the compared approaches on both reordering and translation accuracy.

The pre-ordering performances on the NTCIR-7 and DONG-A training sets are shown in Table I and Table II, respectively. We calculate two widely used measures: Kendall X  X   X  [Isozaki et al. 2012], by and the Fuzzy Reordering Score (FRS) [Talbot et al. 2011], by where c is the number of chunks composed of monotonically aligned words and m is the number of words. Additionally, we calculate Spearman X  X   X  , which is a measure similar to Kendall X  X   X  , according to where x i and y i are the ranks of two sequences with n words. In the calculations for the three measures, we use only the one-to-one aligned words from the GIZA++ alignment file, as in Isozaki et al. [2012]. The directions obtained are opposite those from Isozaki et al. [2012] due to the reversed translation direction (i.e., the same as in Hoshino et al. [2013]). The word alignment automatically generated by GIZA++ may contain noise and bias; however, it is used to evaluate only the overall tendency for word order on plentiful data (the training sets). Thus, the results are suggestive even though they are not necessarily precise.

From Table I, it can be observed that Kendall X  X   X  and Spearman X  X   X  have nearly the same tendency on NTCIR-7 dataset. The proposed approach achieves the highest scores on all three measures, and the approaches of Hoshino et al. [2013] and Neubig et al. [2012] also have good performance. From Table II, it can be observed that the three measures for each approach are lower than in Table I, which is reasonable because the DONG-A dataset has more diversity than the NTCIR-7 dataset. Kendall X  X   X  and the Spearman X  X   X  still nearly have the same tendency on the DONG-A dataset, and the proposed approach achieves the best scores, which are slightly higher than those obtained by the approach of Hoshino et al. [2013]. However, for the FRS, the statistical approach of Neubig et al. [2012] has the highest score, which is slightly higher than with our proposed approach. Broadly speaking, the proposed approach and the approach of Hoshino et al. [2013] consistently have good performance on pre-ordering measures in our experimental results.
 As described, we conducted three groups of translation experiments: PB SMT on NTCIR-7, HIERO on NTCIR-7, and PB SMT on DONG-A, so we illustrate the evalua-tion results on the three groups as follows. Supplementary results on the DONG-A set are reported at the end of the section.
 Table III (a graph of which can be seen in Figure 6) shows the test set BLEU scores of the evaluated approaches with different values for the Distortion Limit (DL) in decoding. We think the most impressive result is that the simplest reverse pre-ordering proposed in Katz-Brown and Collins [2008], has good performance, achieving a best test set BLEU score of 29 . 7whenDL = 9. This performance is also nearly the best among all the previous approaches that we tested. The method of Hoshino et al. [2013] obtained 29 . 7, and the method of Neubig et al. [2012] obtained 29 . 8. Among the results reported in Katz-Brown and Collins [2008], the reverse pre-ordering method had the same performance as the baseline, however, in our experiment, the reverse pre-ordering has better performance. We attribute this to the sophisticated PB SMT system of Moses, which can offer a better baseline and better cooperate with the pre-ordering approach. The approach of Sudoh et al. [2011] does not perform well, which is in agreement with their report that the approach works well in combination with other systems to offer diversity in translation candidates, rather than as a stand-alone SMT system.

The best test set BLEU score of our proposed approach is 30 . 8, when DL = 12. We use the bootstrap method [Koehn 2004] to test the statistical significance of differences between the best test set BLEU scores from among all tested approaches despite their different distortion limits in decoding; as a result, the proposed approach outperforms all other approaches with statistical significance at the p  X  0 . 01 level. Another feature of the proposed approach is that it has already achieved relatively good performance in monotonic translation (test set BLEU score of 30 . 3whenDL = 0), which actually does not increase much with larger values for the distortion limit. We consider this phenomenon to indicate that the proposed approach achieves a substantially correct pre-ordering, compared with the effect of the distortion limit on the other approaches (e.g., the reverse pre-ordering).

Table IV shows the test set RIBES [Isozaki et al. 2010] for the tested approaches at different distortion limits in decoding. Because we ran MERT in the Moses system to tune the weights of features so as to optimize the development set BLEU score, the test set RIBES we show here is only a complementary evaluation of translation performance on word order. We can see that the tendency of the test set RIBES is approximately similar to that of the test set BLEU score between different approaches. However, the distortion limits do not affect the test set RIBES much because we did not tune the parameters to optimize the measure in our experiments.

We also measure the running time for different approaches in this group of exper-iments. We find that the time taken is affected by only the distortion-limit setting in decoding, no matter which pre-ordering approach is used. The translation speed is ap-0, 3, 6, 9, 12, 20, 30, and  X  , respectively. For the pre-ordering process, all the rule-based approaches are implemented in Python and times are not over 0 . 002s per sentence. For the dependency structure analysis, CaboCha can chunk and parse with a speed faster than 0 . 001s per sentence. SynCha, used for the predicate-argument analysis, is rela-tively slower, with a speed of 0 . 9s per sentence, which is nearly the same as needed for decoding. As to the statistical approach of Neubig et al. [2012], the training and application of LADER becomes extremely time consuming. With the default setting of LADER, the model training with the previously described 1 , 000 sentence pairs takes nearly 3 days. 16 When we applied the trained LADER to prereorder the training set, we divided the training set into three sets according to sentence length: 1 X 30 words, 31 X 50 words, and 51 X   X  words. 17 We then applied LADER on the three sets separately with eight threads. The running time is approximately 3 days, 10 days, and 20 days, respectively, on the three sets. On the test set, the speed of LADER is around 8 . 9s per sentence. LADER does perform well even without any linguistic heuristics, though; the drawback is the method X  X  slowness.
 We here draw a general conclusion about the required time for different approaches. Assuming that the training data contain millions of sentence pairs, reverse pre-ordering and dependency-based approaches (including the proposed one), require nearly identical decoding time as the baseline PB SMT system, albeit hours of ex-tra time in training; predicate-argument-based approaches double the decoding time, with weeks of extra time in training; and LADER increase the decoding time tenfold , with months of extra time in training. The conclusion is listed in Table V. We consider it possible to further accelerate predicate-argument-based approaches by development of semantic parsing, while the application of LADER must be supported by extremely powerful computing resources.
 Table VI (see Figure 7 for a graph of this) shows the test set BLEU scores of the eval-uated approaches with different values for the Maximum Span 18 (MS) in decoding. Comparing with the results listed in Table III, it can be seen that the baseline HI-ERO system reaches nearly the same performance as PB SMT with only one-tenth the amount of training data. Among the compared approaches, the dependency tree pre-ordering of Katz-Brown and Collins [2008], and the approaches of Sudoh et al. [2011] and Komachi et al. [2006] cannot improve the baseline, which agrees with the results from experiments using PB SMT. The reverse pre-ordering of Katz-Brown and Collins [2008] and the approaches of Hoshino et al. [2013] and Neubig et al. [2012], which improve the baseline of PB SMT significantly, still improve the HIERO baseline, but not markedly. We attribute this phenomenon to the more powerful reordering ability of HIERO than PB SMT in the context of Japanese-to-English translation, where the effect of pre-ordering is weakened. The best test set BLEU score of our proposed ap-proach is 30 . 1, when MS = 30, which outperforms all the other comparison approaches, with statistical significance at the p  X  0 . 01 level.

Table VII shows the test set RIBES of the tested approaches for different maxi-mum spans in decoding. We observe that the tendency of the test set RIBES is sim-ilar to that of the test set BLEU scores between different approaches, as seen in Table IV.

Compared with the results for PB SMT, we observe a difference with HIERO: that for all the pre-ordering approaches, performances are relatively stable once the maximum span is greater than 10, which is the MaxSpan setting used in the HIERO model training phase. A larger maximum span, such as 20 or 30, in experiments usually results in little gain in evaluation. 19 This phenomenon also indicates that HIERO has a strong and stable reordering ability as a baseline SMT system. Broadly, the experimental results on the NTCIR-7 dataset show a similar tendency for different pre-ordering approaches, despite the baseline SMT systems.
 Table VIII (shown graphically in Figure 8) and Table IX show the test set BLEU scores and test set RIBES, respectively, from DONG-A data by the evaluated approaches with different values for the distortion limit in decoding. Compared with the results listed in Table III, the scores become markedly low on this Internet-obtained dataset. Another different feature in this group of experiments from the experiments using PB SMT on NTCIR-7 data is that most of the pre-ordering approaches do not improve the baseline PB SMT but instead result in nearly identical (or a little worse) performance. The exception is reverse pre-ordering and the proposed approach, which have better performance than the baseline, although the improvement is limited. The best test set BLEU score of our proposed approach is 12 . 5, when DL = 9 and when DL = 12. This performance stil beats the baseline PB SMT with statistical significance at the p  X  0 . 01 level, but the results are not significantly different from that of the approach of reverse pre-ordering.

As mentioned previously, the DONG-A dataset has much freer translations and greater diversity in expressions. Consequently, the robustness of rule-based ap-proaches, which tend to be rigid and fragile, plays a more important role. On this dataset, the performance of reverse pre-ordering is still excellent due to its high ro-bustness among all the tested rule-based approaches. Compared with the results in Table III and Table VI, we find that the reverse pre-ordering performs better when used with PB SMT than with HIERO, and the improvement is consistent across differ-ent datasets. Because the proposed approach contains complicated and sophisticated reordering rules, it cannot have a higher robustness than the reverse pre-ordering. De-spite this, it obviously has a higher correctness than reverse pre-ordering, and this leads to good performance. The trade-off between robustness and correctness is unavoidable in designing a rule-based pre-ordering approach, and the experimental results show that a pre-ordering approach needs to be either robust or correct to achieve good per-formance. The statistical approach of Neubig et al. [2012], which performs well in the previous experiments, fails on this dataset. We consider the main reason for this to be that the training data for LADER is not good enough; this set was selected from the automatically aligned DONG-A data. Although statistical systems can be robust to unseen input, a high quality of training data is also needed, and this is sometimes difficult to provide when adapting different datasets.
 Because the automatic evaluation measures are very low for the DONG-A data, we conducted several further experiments to provide supplementary results.

First, we applied HIERO to the DONG-A data, using the full training data. The experiments using HIERO were not exhaustive in terms of maximum span, but were performed with a fixed maximum span of 20, and all the other settings were kept the same as in the experiments on NTCIR-7. 20 The results are listed in Table X. The baseline HIERO system reached a test set BLEU score of 12 . 5, which was better that the best result of 12 . 0 yielded by the baseline PB SMT system with statistical significance. In agreement with the results of PB SMT, only the proposed approach and reverse pre-ordering gave better performance than baseline, not significantly different from the HIERO baseline. Broadly, the HIERO can improve PB SMT by + . 2to + . 5 for different approaches, and the reordering ability of HIERO reduces the benefit of pre-ordering.
 We additionally applied the model of LADER trained on NTCIR-7 data to the DONG-A data with the aim of investigating the cross-domain performance of LADER. We find that pre-ordering measures are obviously improved, which suggests the data from NTCIR-7 trained a better model for LADER. Specifically, Kendall X  X   X  is . 529, Spear-man X  X   X  is . 616, and FRS is . 447. Compared with the results in Table II, the results are almost exclusively lower than results for the proposed approach. However, the trans-lation performance was not improved. By PB SMT, the test set BLEU scores are listed in Table XI for distortion limit of 6, 9, 12, 20, 30, and  X  . By HIERO, we had a test set BLEU score of 11 . 4whenMS = 20. We consider this evidence that the cross-domain application of LADER affects its robustness, 21 which prevents improvements from the test set. In discussing Table I and Table II, we noted that Kendall X  X   X  and Spearman X  X   X  exhibit nearly the same tendency in evaluation of different pre-ordering approaches. This phenomenon can be attributed to the two measures both revealing the global difference between two ranks. At the same time, we find that Kendall X  X   X  and the FRS disagree about certain cases. The most obvious example of this phenomenon is that reverse pre-ordering has a moderate Kendall X  X   X  ( . 504 and . 464) but quite low FRS ( . 154 and . 298). We consider the reason for this phenomenon to be that FRS is sensitive to local monotonicity of the two ranks, which makes it a more sensitive measure than Kendall X  X   X  . For the reverse pre-ordering, the simple rule can handle global reordering but is still too rough for detailed local reordering. As a consequence, it has medium Kendall X  X   X  but quite low FRS. Accordingly, the reverse pre-ordering performs poorly in monotonic translation but can achieve good performance with the help of the reordering ability of the SMT system, and this effect can be observed in all groups of experiments.
Previous reports of rule-based approaches [Isozaki et al. 2012; Hoshino et al. 2013] have illustrated that a high Kendall X  X   X  leads to high test set BLEU scores, but Kendall X  X   X  and test set BLEU scores do not have strong relation in LADER [Neubig et al. 2012]. In our opinion, the relations between different measures are complicated, depending on different approaches and different datasets. According to our experimental results, here we can give only a weak conclusion: that a rule-based pre-ordering approach with high Kendall X  X   X  and FRS simultaneously is likely to yield good performance in translation. We intend to investigate this topic more in the future. From the evaluation of compared approaches, we observe that those approaches using chunks but not morpheme operations typically have worse performance (e.g., Sudoh et al. [2011] and Komachi et al. [2006]). Conversely, the reverse pre-ordering of Katz-Brown and Collins [2008] and the statistical approach of Neubig et al. [2012], which are morpheme based but do not use chunks, have relatively good performance. From this observation, we consider the movement of morphemes in a pre-ordering approach to be a critical issue. A further investigation into the effects of the inter-, intra-, and extra-chunk rules used in the proposed approach is shown in the following tables. Table XII and Table XIII show the results using PB SMT on NTCIR-7 data; Table XIV and Table XV show the results using HIERO on NTCIR-7 data; and Table XVI and Table XVII show the results using PB SMT on DONG-A data.
 In the tables,  X  + MorphemeModification X  represents an attempt to delete some Japanese morphemes that do not have precise English analogs. Specifically, we delete the topic marker wa , the nominative case marker ga , the accusative case marker wo , and the conjunctive particle te from the Japanese source. To generate these Japanese morphemes is quite important in the English-to-Japanese translation [Isozaki et al. 2012], but the trimming of them hardly improves the performance in our Japanese-to-English translation. (In Table XVI, a slight improvement can be observed, but this is the only improvement by deleting the morphemes.)
In the rows of  X   X  Extra, X  we disable extra-chunk reordering and use intra-chunk reordering instead. That is, Ex Pre and Ex Post in Algorithm 6 are merged with InPre and InPost . This modification worsens the performance, especially for the test set BLEU scores when DL = 0 in PB SMT (from 30 . 3to29 . 1 in Table XII and from 11 . 6to 11 . 1 in Table XVI), illustrating the decrease in pre-ordering quality. Although PB SMT can remedy this by using a larger distortion limit on the NTCIR-7 data (achieving a test set BLEU score of 30 . 4whenDL = 12 in Table XII), it cannot remedy it for the DONG-A data. The results of HIERO on NTCIR-7 in Table VI show that the decrease caused by  X   X 
Extra X  cannot be recovered by allowing a larger maximum span. We conclude that the extra-chunk reordering does have an important effect on the translation performance, independently of SMT system and dataset.

When we disable all the pre-ordering of morphemes and use only the inter-chunk rules (i.e., disable the process from line 13 to line 16 in Algorithm 1), the performance is obviously worsened, as shown in the rows of  X   X  Intra  X  Extra. X  This result supports our conclusion that moving morphemes is indispensable.

We further check the effect of three inter-chunk rules in the proposed approach after disabling the operations on morphemes. In the  X   X  Copula  X  Intra  X  Extra X  rows, we disable the inter-chunk rule for copula chunks by changing the if-else block from line 5 to line 9 of Algorithm 2 to always proceed as if  X  C is a noun-chunk . X  In the experimental results of PB SMT on NTCIR-7, the best performance does not change much (best test BLEU scores from 29 . 5to29 . 4 in Table XII), though the performance decreases quite a lot when DL = 0 (from 28 . 2to27 . 7 in Table XII). From the experimental results of HIERO on NTCIR-7, we consider that the reordering ability of HIERO can cover the movement of Japanese copula morphemes. Although the reordering may become a long-distance operation, the reordering pattern for copulas is not complex. On DONG-A data, the inter-chunk rule for copula chunks does not distinguish the performance at all. We consider it possible that the expressions using copula on news data may cause more diversity in sentence structures than on patent data, where the translations become difficult.

Finally, the results of using only the inter-chunk rule for verb chunks are shown in rows with  X   X  Noun  X  Copula  X  Intra  X  Extra. X  Performance is quite similar to that in Komachi et al. [2006], which also describes an approach that focuses on only verbs and their arguments. We can conclude that although reordering around verbs is important, reordering around nouns is not less important in Japanese-to-English translation. In the previous section, we have investigated the effect of different rules in the proposed approach. Here, we give statistics about the different rules to illustrate how often they are applied in the pre-ordering process.

Table XVIII illustrates the percentage of time that the three inter-chunk rules are ap-plied in processing the NTCIR-7 dataset. The  X  X ther X  column contains the percentage of unprocessed chunks. For each row, the sum of all percentages is 1 . 0. Table XIX illus-trates the percentage of intra-and extra-chunk movement in application to NTCIR-7, with the left ( X -Pre X ) and right ( X -Post X ) directions listed separately. The sum of percent-ages in each row is not necessarily 1 . 0 because some chunks are counted multiple times and others are not counted at all. Further, all the inter-, intra-, and extra-chunk move-ments in reordering are compared in Table XX, where the number of applications of different kinds of reordering are counted and normalized. 22 Corresponding statistics for the DONG-A dataset are listed in Table XXI, Table XXII, and Table XXIII, respectively.
The statistics on the two different datasets, which have quite different character-istics, are stable with respect to the inter-chunk rules for verbs and nouns, and to extra-chunk movement. We consider this phenomenon to reveal inherent properties of the Japanese language. A noticeable result is that nearly two-thirds of Japanese chunks are noun chunks, and less than one-fourth are verb chunks. Thus, the reorder-ing of nouns must play a much more (or at least, no less) important role than reordering of verbs in Japanese; this point has not been given enough weight in previous works. Additionally, we find that the extra-chunk movement is frequently and steadily ap-plied in our proposed approach (more than half of chunks are marked with  X  X xPre X  and about one-fourth with  X  X xPost X ), and so it is much more common than intra-chunk movement. When we put all types of reordering together, a noticeable phenomenon emerges:  X  X oun X  and  X  X xPre X  are the two most frequent reordering processes, with nearly the same percentage. This is reasonable because most Japanese nouns appear-ing in a sentence are succeeded by case markers or particles, which play a role similar to that of prepositions in English, and so  X  X xPre X  is applied. The investigation in this section achieves the same conclusion as was reached in the previous section; that is, the movement of morphemes and noun chunks is important and indispensable when designing a pre-ordering approach for the Japanese language. In the experimental results and discussion illustrated in the previous sections, the good performance of the proposed approach has been tested on different datasets and by different SMT systems. However, our approach is originally designed for Japanese-to-English translation and the experiments are conducted on the very language pair. A natural question is whether the proposed approach is applicable for the other translation tasks of language pairs having the same features. Several previous researches have demonstrated it is possible to transform pre-ordering techniques among languages sharing identical features. A typical example is the pre-ordering approach proposed originally designed for English-to-Korean translation also works on English-to-Japanese, -Hindi, -Urdu, and -Turkish translation [Xu et al. 2009]. In a recent research [Ding et al. 2014], the pre-ordering techniques for English-to-Japanese translations are proved efficient for Chinese-, English-, and French-to-Burmese translation. We consider the proposed approach, especially the extra-chunk reordering, will also work for Korean-to-English translation due to the extremely similar syntactic structure and word order between Korean and Japanese. Also, the reverse pre-ordering should work on Korean-to-English translation. These are our most interesting and primary future work. In this article, we proposed extra-chunk pre-ordering of morphemes in a rule-based pre-ordering approach for statistical Japanese-to-English machine translation. By us-ing extra-chunk rules, Japanese functional morphemes can have a more linguistically correct arrangement than from previous approaches. The proposed approach was com-pared with five rule-based pre-ordering approaches designed for Japanese-to-English translation and with a state-of-the-art statistical pre-ordering approach, on a standard patent dataset and on news data crawled from Internet. Two state-of-the-art statis-tical machine translation systems, PB SMT and HIERO, were used in experiments. The experiment X  X  results have demonstrated that the proposed approach outperforms the other approaches on automatic reordering measures as well as on the transla-tion precision as measured by the test set BLEU scores. We also found that reverse pre-ordering [Katz-Brown and Collins 2008] worked very well, particularly consider-ing its simplicity. We think that the reverse pre-ordering should be widely used as a baseline approach for Japanese-to-English translation. We further investigated the experimental results to elucidate the effect of different reordering rules.
In future work, we plan to test the proposed approach on additional Japanese-English parallel corpora and extend the approach to the other SOV-to-English translation tasks, such as Korean-to-English translation.

