 Matrix factorization (MF) has become the most popular technique for recommender systems due to its promising performance. Recently, distributed (parallel) MF models have received much attention from researchers of big da-ta community. In this paper, we propose a novel model, called d istributed s tochastic a lternating d irection m ethods of m ultipliers (DS-ADMM), for large-scale MF problems. DS-ADMM is a distributed stochastic variant of ADMM. In particular, we first devise a new data split strategy to make the distributed MF problem fit for the ADMM framework. Then, a stochastic ADMM scheme is designed for learning. Finally, we implement DS-ADMM based on m essage p assing i nterface (MPI), which can run on clusters with multiple ma-chines (nodes). Experiments on several data sets from rec-ommendation applications show that our DS-ADMM model can outperform other state-of-the-art distributed MF mod-els in terms of both efficiency and accuracy.
 Matrix Factorization; Recommender Systems; ADMM; Dis-tributed Computing; Stochastic Learning
Recommender systems try to recommend products (item-s) to customers (users) by utilizing the customers X  historic preferences. Matrix factorization (MF) [8] and its exten-sions [9, 22, 16, 14, 10, 1, 18] have become the most popular models for recommender systems due to their promising per-formance [8]. In this big data era, more and more large-scale data sets have emerged in many real-world recommender systems. Hence, parallel or distributed 1 MF models with the potential of high scalability have recently captured much attention from researchers.

The basic idea of MF is to use the multiplication of two latent matrices, the user matrix and the item matrix, to ap-proximate the original rating matrix. Least square method is usually used to find a solution. In recent years, several parallel models have been proposed for MF. These existing models can be roughly divided into two main categories: al-ternating least square (ALS) [23] based models and stochas-tic gradient descent (SGD) based models.

ALS [23] adopts the alternating learning strategy to up-date one matrix with the other one fixed. With one of the matrices fixed, the optimization problem of MF can be re-duced to a least square problem on the other matrix, which can be further decomposed into several independent least square problems on the latent feature vector of each user or item. Hence, it is easy to design parallel strategies for ALS, which has been implemented in [23]. However, the time complexity for each iteration in ALS is cubic in k ,where k is the number of latent features for each user or item. The cyclic coordinate descent (CCD) method [13] adopt-s coordinate descent strategy to improve the ALS method by decreasing the time complexity for each iteration to be linear in k . The CCD++ [21] further improves the efficien-cy of CCD by using similar coordinate descent strategy but different updating sequence of the variables. Because both CCD and CCD++ are based on ALS, they can also be easily parallelized [21].
In existing literatures, distributed models refer to those im-plemented on clusters with multiple machines (nodes), while paral lel models refer to those implemented either on multi-core systems with a single node or on clusters. We will also follow this tradition in this paper. The specific meaning of paral lel can be determined from the context in the paper.
Due to its efficiency and ease of implementation, SGD has become one of the most popular optimization strategies for MF in recommender systems [8]. The basic idea of SGD is to randomly select one rating each time from the rating matrix and then use the gradient based on this selected rating to update the latent features. It is easy to see that SGD is essentially a sequential method, which makes it difficult to be parallelized.

The main reason that SGD can not be directly parallelized is that two randomly selected ratings may share the same la-tent features corresponding to the same user or item. Hence, there exist conflicts between two processes or nodes which simultaneously update the same latent features. Recently, several strategies have been proposed to parallelize SGD for MF. The Hogwild! [11] model just ignores the conflicts by assuming that the probability of conflict is small when two ratings are randomly selected f rom a sparse rating matrix. However, the conflicts do exist in the learning procedure, which makes Hogwild! not effective enough [21, 24]. More-over, Hogwild! requires all the processes share the whole training set which is hard to be satisfied in distributed sys-tems. Hence, Hogwild! cannot be directly used in distribut-ed systems.

Distributed SGD (DSGD) [4] utilizes the property that there exist several sub-blocks without overlapping rows and columns in the rating matrix. These sub-blocks are mutually independent of each other, thus can be processed in parallel by different processes or nodes at the same time. Exper-iments in [21, 24] have shown that DSGD can outperform Hogwild! in terms of both efficiency and accuracy. However, after a set of independent sub-blocks have been processed, the updated variables from all processes or nodes should be synchronized before processing the other sets of independent sub-blocks. It is these frequent synchronization operations that make DSGD inefficient because the slowest node will become the bottleneck of the whole system. Things go even worse if data skew exists, which is not rare in real applica-tions. Very recently, fast parallel SGD (FPSGD) [24] tries to solve the issues in DSGD by changing the scheduler into an asynchronous one, which has achieved better performance than DSGD. However, FPSGD can only be used in shared-memory systems with a single node. Hence, FPSGD is still not scalable to handle large-scale problems.

In this paper, a novel model, called d istributed s tochastic a lternating d irection m ethods of m ultipliers (DS-ADMM), is proposed for large-scale MF problems. DS-ADMM is a distributed stochastic variant of ADMM [3]. The main con-tributions of DS-ADMM are briefly outlined as follows:
In this section, we introduce the background of this pa-per, including notations, MF formulation, ALS-based mod-els, SGD-based models and ADMM.
We use boldface uppercase letters like M to denote matri-ces and boldface lowercase letters like m to denote vectors. M i  X  and M  X  j denote the i th row and the j th column of M , respectively. M ij denotes the element at the i th row and j th column in M . M T denotes the transpose of M ,and M  X  1 denotes the inverse of M .tr(  X  ) denotes the trace of a matrix. I k is an identity matrix of size k  X  k . Assume there are m users and n items in the data set. We use R  X  R m  X  n to denote the rating matrix. Please note that there exist many missing entries in R . All the missing entries are filled with 0. We use  X   X  X  1 , 2 ,  X  X  X  ,m } X { 1 , 2 ,  X  X  X  ,n } to denote the set of indices for the observed ratings.  X  i denotes the column indices of the observed ratings in the i th row of R , and  X   X  j denotes the row indices of the observed ratings in the j th column of R . U  X  R k  X  m denotes the users X  laten-t factors (matrix) with each column U  X  i representing the latent feature vector for user i ,where k is the number of latent factors for each user or item. V  X  R k  X  n denotes the senting the latent feature vector for item j . P denotes the total number of nodes in the cluster, and we use the letter p on the superscript like M p to denote the computer node id.  X  F denotes the Frobenius norm of a matrix or a vector.
Matrix factorization (MF) can be formulated as the fol-lowing optimization problem: min where  X  1 and  X  2 are hyper-parameters for regularization.
There are two categories of parallel models to solve the above MF problem, i.e., the ALS-based models and SGD-based models, which will be briefly reviewed in the following subsections.
By adopting the alternating learning strategy, ALS [23] alternatively switches between updating U and updating V with the other latent matrix fixed. With U fixed, the MF problem can be decomposed into n independent least square problems, each of which corresponds to a column of the ma-trix V . Similar m independent least square problems can be got by fixing V . Furthermore, each of these independent problems has a closed-form solution in ALS: where V  X  i denotes a sub-matrix formed by the columns in V indexed by  X  i , U  X   X  j is similarly defined, m i = | n j = |  X   X  j | . Please note that all the missing entries in R have been filled by zeros. The columns in both U and V can be independently updated by following (2). Hence, it is easy to design the parallel strategy for ALS, which has been implemented in [23].

Instead of optimizing the whole vector U  X  i or V  X  j at one time, CCD [13] adopts the coordinate descent method to optimize each element of U  X  i or V  X  j separately, which can avoid the matrix inverse operation in (2). CCD++ [21] fur-ther improves CCD X  X  performance by changing the updating sequence in CCD. It rewrites U T V = k d =1 U T d  X  V d  X  updates one element in U d  X  or V d  X  each time by using simi-lar coordinate descent method in CCD. Changing the updat-ing sequence may improve the convergence rate, which has been verified by the experimental results in CCD++ [21].
The idea of SGD is to randomly select one rating index ( i, j ) from  X  each time, and then update the corresponding variables U  X  i an V  X  j as follows: where ij = R ij  X  U T  X  i V  X  j ,and  X  is the learning rate.
Due to the demand of many large-scale problems, several parallel SGD models have been proposed. Some of them, such as those described in [25] and [20], are not for MF problems. Here, we just focus on those parallel SGD models for MF, including Hogwild! [11], DSGD [4] and FPSGD [24].
From (3), it is easy to find that conflicts exist between t-wo processes or nodes when their randomly selected ratings share either the same user index or the same item index. Hogwild! [11] allows overwriting each other X  X  work when con-flicts happen. It also shows that if the optimization problem is sparse enough, the Hogwild! will get a nearly optimal rate of convergence.

DSGD [4] divides the whole rating matrix into P stra-ta and each stratum contains P mutually independent sub-blocks without sharing any column or row indices. Conse-quently, sub-blocks in the same stratum can be processed in parallel since they don X  X  share any U  X  i or V  X  j . One iteration of DSGD is divided into P steps, in each of which DSGD picks a stratum containing P independent sub-blocks and then processes these sub-blocks in parallel in a cluster of P nodes with each node responsible for one sub-block. After all the P sub-blocks in each step are processed, the whole U and V have been updated separately. They should be syn-chronized in order to let all nodes get the latest U and V . It is obvious that during one iteration of processing all the ratings in the whole matrix, P synchronization operations should be performed for DSGD. This frequent synchroniza-tion will make DSGD inefficient because the slowest node will become the bottleneck of the whole system.

FPSGD [24], which is proposed for shared-memory sys-tems, tries to improve the performance by changing the scheduler of DSGD into an asynchronous one. Its experi-ments show that FPSGD can achieve better efficiency and accuracy than DSGD.

Both Hogwild! and FPSGD are only for shared memory systems with one single node and thus their scalability is limited. DSGD can be used for distributed systems while it costs too much on synchronization.
ADMM [3] is used to solve the constrained problems as follows: where f (  X  )and g (  X  ) are functions, x and z are variables, A , B and c are known values.

To solve the problem in (4), ADMM first gets the aug-mented Lagrangian as follows: where y is the Lagrangian multiplier and  X  is the penalty parameter. The ADMM solution can be got be repeating the following three steps: where x t denotes the value of x at the t th iteration, y and z t are similarly defined. If f ( x )or g ( z ) are separable, the corresponding steps of ADMM can be done in parallel. Hence, ADMM can be used to design distributed learning algorithms for large-scale problems [3].

In recent years, ADMM has captured more and more attention with wide applications, such as matrix comple-tion [5], compressive sensing [19], image restoration [6] and response prediction [1]. Moreover, many variants of ADMM are also devised, including the stochastic and online exten-sions [15, 12, 17]. However, to the best of our knowledge, few works have been proposed to use stochastic ADMM for distributed MF problems.
In this section, we present the details of our DS-ADMM model. We first introduce our data split strategy to divide the whole problem into several sub-problems. Then we pro-pose a distributed ADMM framework to handle these sub-problems in parallel. After that, a stochastic learning algo-rithm is designed to speed up the distributed ADMM frame-work. Subsequently, we compare the scheduler of DS-ADMM with those of DSGD and CCD++. Finally, the complexity analysis of DS-ADMM will be provided.
In our data split strategy, we divide R and U into P sub-blocks according to users. More specifically, each sub-block will contain m P rows of R and m P columns of U .From(1),we find that U and V are coupled together in the loss function. Updating one of them needs the other X  X  latest value, which makes the problem hardly separable. To decouple U and V , we keep a local item latent matrix for all items in each node, which is denoted as V p . Please note that V p is not a sub-block of V , but it has the same size with V .Wealsohave a global item latent matrix which is denoted as V .Because only the local V p couples with U , we can independently up-date U and V p for each node. This split strategy can make the MF problem fit for the distributed ADMM framework, which will be introduced in the following subsection.
Our split strategy is called LocalMFSplit , which is briefly summarized in Algorithm 1. Note that the size of V p is k  X  n , but that of U p is k  X  m p with m p being the number of columns (about m P ) assigned to node p .
 Algorithm 1 LocalMFSplit 1: Input: R , P 2: for i =1: m do 3: Generate a random number p from { 1 , 2 ,  X  X  X  ,P } ,and 4: end for 5: for p =1: P parallel do 6: Allocate memory for U p , V p and V 7: end for
Based on our split strategy LocalMFSplit , the MF problem in (1) can be reformulated as follows: where V = { V p } P p =1 , X  p denotes the ( i, j ) indices of the ratings located in node p . Notethathereweomitthe p in U p for simplicity. It is not hard to determine whether U refers to the whole latent matrix or a sub-block U p located in node p from the context.

If we define where we can transform the constrained problem in (6) to an un-constrained problem with augmented Lagrangian method, and get the following objective function: where l p ( V p , V ,  X  p )=  X  Here,  X  is a hyper-parameter and O = {  X  p } P p =1 denotes the Lagrangian multiplier.
 If we define L ( U , V p ,  X  p , V )= f p ( U , V p )+ l p ( V p , V ,  X  we can get
The ADMM will solve this problem by repeating the fol-lowing steps:
U
It is easy to see that U , V p and  X  p can be locally updated on each node. Because the whole MF problem has been divided into P sub-problems which can be solved in parallel, our method is actually a distributed ADMM framework.
To learn the parameters in (6), we just need to find the solutions in (10a), (10b) and (10c). After getting the op-timal U t +1 and { V p t +1 } , it is easy to solve the problem in (10b). More specifically, if we set  X  p 0 =0,wecanprove that P p =1  X  p t = 0. Hence, the update rule for V is: The problem in (10c) directly shows the update rule, which can be computed locally and efficiently. Therefore, the key learning part lies in how to efficiently solve the problem in (10a). In the following content of this subsection, we first design a batch learning algorithm for the problem in (10a), and then a stochastic learning algorithm inspired by the batch learning is also designed to further improve the ef-ficiency.
With  X  p t and V t fixed, (10a) is an MF problem. How-ever, we can not easily get the solution because U and V p are coupled together and the objective function of the MF problem is non-convex. To get an efficient solution, we use a technique similar to that in [12] to construct a surrogate objective function, which is convex and can make U and V decouple from each other. For each iteration of minimizing the constructed function, we can easily get the closed form solution of U and V p by setting their gradients to zero. The surrogate objective function is defined as follows:
G p ( U , V p ,  X  p t , V t , X  t | U t , V p t ) where with  X  t being a value which will be useful for specifying the step-size in the stochastic learning method introduced later, and the function f p ( U , V p ) being defined in (8).
Lemma 1. For an arbitrary positive value  X  2 , we can al-properties within the domain D = { U , V p | U  X  i  X  [ U  X   X  The proof of Lemma 1 can be found in Appendix A.
From Lemma 1 , we can find that G p (  X  ) is an upper bound of L p (  X  ), and G p (  X  )= L p (  X  )atthepoint( U t , V with L p (  X  ), U and V p are decoupled in G p (  X  ), and G convex in ( U , V p ). Hence, it is much easier to optimize G p (  X  )than L p (  X  ).

Instead of optimizing the original function L p (  X  ), we op-timize the surrogate function G p (  X  ) in the first step of the ADMM: The objective function in (14a) is convex in both U and V p . Hence, we can easily get the solution by setting the gradients to be zero. The optimal solution is computed as follows:
Lemma 2. By following the update rules in (15) and (16) , the original objective function L p (  X  ) will not increase in each step. That is to say, The proof of Lemma 2 can be found in Appendix B.
By combining the update rules in (15), (16), (11) and (10c), we can get a batch learning algorithm for the problem in (6) with the distributed ADMM framework.
 Theorem 1. Our batch learning algorithm will converge.
Proof. BasedonLemma2,wecanprovethattheobjec-tive function L (  X  ) in (9) will decrease in each iteration of AD-MM. Furthermore, L (  X  ) is lower bounded by  X  Hence, our batch learning algorithm will converge. Because L (  X  ) is not convex, it might converge to a local minimum.
From (15), we can find that it will access all ratings relat-ed to U  X  i to update each U  X  i , and the same also goes for updating each V p  X  j in (16). Hence, the batch learning algo-rithm presented above is not efficient, especially when the number of ratings becomes very large. To further improve the efficiency, we propose a stochastic learning strategy for the distributed ADMM, which is called DS-ADMM. In par-ticular, the update rules for DS-ADMM is as follows: the stochastic learning algorithm is derived from the batch learning algorithm by treating only U  X  i and V p  X  j as variables in (14a).

By combining the split strategy and the update rules stat-ed above, we can get our DS-ADMM algorithm. The whole procedure of DS-ADMM is briefly listed in Algorithm 2. Algorithm 2 DS-ADMM 2: Use Algorithm 1 to distribute R to P different nodes. 3: Randomly initialize U 0 , V p 0 ; 4: Calculate V 0 by (11) 5: Set  X  p 0 =0. 6: for t =1: MaxIter do 7: for p =1: P parallel do 8: for each R i,j in node p do 9: Update U  X  i and V p  X  j by (18) and (19) 10: end for 11: end for 12: Update V by (11) 13: for p =1: P parallel do 14: Update  X  p by (10c) 15: end for 16: Update  X  t 17: end for CCD++ and DSGD are two state-of-the-art distributed MF models. We compare the scheduler of our DS-ADMM with those of CCD++ and DSGD to illustrate the synchro-nization cost.

Figure 1 (a), (b) and (c) show the number of synchro-nization operations in one iteration of CCD++, DSGD and DS-ADMM, respectively. Here, one iteration means all the training ratings are processed for one time. We can find that CCD++ needs 2 k times of synchronization and DSGD needs P times. From Algorithm 2, we can easily find that DS-ADMM needs only one synchronization for each itera-tion, which is shown in line 12. This synchronization step is used to gather all V p . Hence, it is obvious that the syn-chronization cost of our DS-ADMM is much less than those of CCD++ and DSGD.
DS-ADMM updates all variables once by three steps. Step one updates U and V p .Foreachrating R ij , the time com-plexity to update U  X  i and V p  X  j is O ( k 2 ). Because the total number of observed ratings is |  X  | , the time complexity of step one is O ( k 2 |  X  | ). Step two is actually a summation of P matrices of size k  X  n , thus the time complexity is O ( Pnk ). Step three need to update P matrices of size k  X  n ,andthe update operation only contains constant times of addition, sothetimecomplexityis O ( Pnk ). In total, the time com-plexity of DS-ADMM for each iteration is O ( k 2 |  X  | + Pnk ).
All the experiments are run on an MPI-cluster with twenty nodes, each of which is a 24-core server with 2.2GHz Intel(R) Xeon(R) E5-2430 processor and 96GB of RAM. To evaluate the scale-out performance of our model, we use only one 1 core (thread) and 10GB memory for each node.
We run our experiments on three public collaborative fil-tering data sets: Netflix 2 , Yahoo! Music R1, and Yahoo! Mu-sic R2 3 . The Netflix data set contains the users X  ratings to movies. Yahoo! Music R1 data set contains the users X  rat-ings to artists. Yahoo! Music R2 contains the users X  ratings to songs.

As the original ratings of Yahoo! Music R1 are ranging from 0  X  100 and have value Never play again ,wetreatthe ratings with value 0 and Never play again as the explicit negative feedback and filter them out. For the other ratings, we normalize them by multiplying each rating by 0.05. After preprocessing, all the ratings lie in the range of [0 . 05 , 5].
The Netflix and Yahoo! Music R2 data sets also contain public test data sets, which are used in our experiments. For the Yahoo! Music R1 data set, we randomly select 10% of the http://www.netflixprize.com/ http://webscope.sandbox.yahoo.com/catalog.php?datatype=r ratings for testing and the remaining are used for training. Detailed information of these data sets are shown in the first four rows in Table 1.

FPSGD and Hogwild! can only run on multi-core systems while we focus on distributed algorithms in this paper. So CCD++, DSGD and DSGD-Bias are adopted as our base-lines.

CCD++ is implemented by referring to the public OpenMP version 4 . DSGD is implemented according to [4] with an asynchronous scheduler to improve its performance.
We also implement a variant of DSGD called DSGD-Bias by using the prediction function: R i,j =  X  + b i + b j + U where b i ,b j are the user and item bias for ratings and  X  is the global mean of the ratings. The model with bias is an http://www.cs.utexas.edu/  X  rofuyu/libpmf/ easy and natural extension of MF and has been proved to be more accurate than those without bias [8]. DSGD-Bias model is optimized and paralleled by using the same strategy as that in DSGD.

During our experiments, we find that good results can be achieved by setting  X  1 =  X  2 for all the algorithms. So we simply set  X  1 =  X  2 in our experiments. All the hyper-parameters for each model in our experiments are selected by ten-fold cross validation on the training set except the latent factor number k and the number of computing nodes P . k is selected by following the CCD++ [21]. Actually, we find that on our data sets larger k doesn X  X  achieve much better accuracy for CCD++ while increasing the running time. The node number P is set according to the size of the data sets. All the algorithms have the same P for the same data set.

We use a general and simple update rule for DSGD X  X  learn-ing rate. More specifically, we first initialize the learning rate  X  with a relatively large value, then decrease it by  X  +1 =  X  t  X   X  (0 &lt; X &lt; 1) after each iteration, and stop decreasing when  X  becomes smaller than some threshold  X  . This strategy results in a fast convergence rate and avoids early stopping.

DS-ADMM has a similar step-size parameter  X  t .From the detailed proof in Appendix A, we find that  X  t should be smaller than some threshold computed based on U t and V p Because the exact threshold value for  X  t is hard to calculate, we approximately update it as  X  t +1 =  X  t  X   X  (0 &lt; X &lt; 1) for the t th iteration. We also set a threshold  X  .When  X  t  X  we stop decreasing  X  t . It is easy to find that the update rule for  X  t is the same as that for the DSGD X  X  learning rate  X  .
The parameter settings are shown in the last eight rows in Table 1.
The root mean squared error (RMSE) is a widely used metric to measure an MF model X  X  performance [21]. The test RMSE is defined as: 1 Q ( R i,j  X  U T  X  i V  X  j ) 2 ,where Q is the number of testing ratings. Figure 2 shows the test RMSE versus running time for our DS-ADMM and other baselines. We can easily find that DS-ADMM performs the best on all three data sets. RMSE value of CCD++ and the DSGD-Bias model decreases fast at first for some da-ta sets because CCD++ updates the parameters by their closed-form solutions and the DSGD-Bias model extracts more explicit information from the training data set. How-ever, our DS-ADMM decreases much faster afterwards and finally converges to the best accuracy, which is much better than the DSGD-Bias model and CCD++. DSGD performs the worst among all the models in most cases. Moreover, as the scale of the data set grows, the difference between C-CD++, DSGD and DSGD-bias X  X  convergence value becomes smaller, but their difference to DS-ADMM becomes larger. Note that Yahoo! Music R2 has about 0.7 billion ratings, which is much larger than many real world applications.
In some application scenarios, the learning process stops when the RMSE value reaches some threshold. So we de-velop experiments to test those algorithms X  running time to reach the given threshold with different number of com-puting nodes. When conducting experiments on the Netflix data set, we set the threshold of RMSE value as 0.922. This value is chosen because it is the place where the RMSE curves of DS-ADMM and CCD++ become smooth, and it is also a value that DSGD and DSGD-Bias can reach if pro-vided with enough running time. We report the log-scale of the running time in Figure 3. Results on the other two data sets are similar, which are omitted due to the limit-ed space. From Figure 3, we can find that our DS-ADMM outperforms all the other algorithms no matter how many nodes are used. DS-ADMM needs relatively fewer iterations to reach a test RMSE of 0.922 and its running time for each iteration is the smallest among all algorithms. DSGD per-forms the worst since it should run more iterations to reach a test RMSE of 0.922 and the running time for each iteration is also relatively large.
Another important metric that can be used to measure a distributed algorithm is its speedup or scalability. It mea-sures the performance when more machines are used or larg-er data sets are processed. In general, more machines and Figure 3: Average running time to reach a test RMSE larger data sets will increase the communication and schedul-ing costs. Algorithms with poor scalability consume more computing resources without offering a good reward, so they are not suitable for large-scale applications.

To study the scalability of our algorithm, we compute the speedup factor relative to the running time with 2 nodes by varying the number of nodes from 2 to 8. Speedup factors of CCD++ and DSGD are provided for comparison. The results on the Netflix data set are shown in Figure 4. Results on other data sets are similar, which are omitted for space saving. Figure 4: Speedup comparison between CCD++,
It is obvious from Figure 4 that DS-ADMM achieves the best speedup among all three algorithms. Figure 4 also shows that both DSGD and DS-ADMM have a super-linear speedup. This might be reasonable. More specifically, as the number of computing nodes increases, the scale of the data set on each node decreases. With a relatively large cache size, most data or even the whole data set can be loaded in-to caches and thus the accessing cost is reduced, which will gain extra speedup [2, 7] 5 .

By separately measuring the computing cost and synchro-nization cost in our experiments, we find that DSGD cost-s much more time on communication and synchronization than DS-ADMM. Taking the Netflix experiment as an ex-ample, although the total running time for each iteration are nearly the same for DSGD and DS-ADMM, DSGD spends about 18% of the total time on communication and synchro-nization, while that is only 8% for DS-ADMM. This result verifies our previous analysis on the synchronization issue in DSGD.
We vary the values of  X  to study its effect on the per-formance of DS-ADMM, the results of which are shown in Figure 5. We can find that very good performance can be achieved when  X  is around 0 . 05 for all data sets, and our DS-ADMM is not sensitive to  X  in the range [0.03, 0.1]. This relatively stable property of  X  makes hyper-parameter selection much easier.
In this paper, we propose a new distributed algorithm called DS-ADMM for large-scale matrix factorization in rec-ommender systems. We first design a data split strategy to divide the large-scale problem into several sub-problems and then derive a distributed stochastic variant of ADMM for efficient learning. Experiments on real world data set-s show that our DS-ADMM algorithm can outperform the state-of-the-art methods like DSGD and CCD++ in terms of accuracy, efficiency and scalability.
This work is supported by the NSFC (No. 61100125, No. 61472182), the 863 Program of China (No. 2012AA011003), and the Program for Changjiang Scholars and Innovative Research Team in University of China (IRT1158, PCSIRT). [1] D. Agarwal. Computational advertising: the linkedin [2] J. Benzi and M. Damodaran. Parallel three [3] S.P.Boyd,N.Parikh,E.Chu,B.Peleato,and [4] R. Gemulla, E. Nijkamp, P. J. Haas, and Y. Sismanis. http://en.wikipedia.org/wiki/Speedup [5] D. Goldfarb, S. Ma, and K. Scheinberg. Fast [6] T. Goldstein and S. Osher. The split bregman method [7] B. John, X.-J. Gu, and D. R. Emerson. Investigation [8] Y. Koren, R. M. Bell, and C. Volinsky. Matrix [9] W.-J. Li and D.-Y. Yeung. Relation regularized matrix [10] Y. Li, M. Yang, and Z. M. Zhang. Scientific articles [11] F. Niu, B. Recht, C. Re, and S. J. Wright. Hogwild!: [12] H. Ouyang, N. He, L. Tran, and A. G. Gray.
 [13] I. Pil` e X rl X  X zy, D. Zibriczky, and D. Tikk. Fast als-based [14] S. Purushotham and Y. Liu. Collaborative topic [15] T. Suzuki. Dual averaging and proximal gradient [16] C. Wang and D. M. Blei. Collaborative topic modeling [17] H. Wang and A. Banerjee. Online alternating [18] H. Wang, B. Chen, and W.-J. Li. Collaborative topic [19] J. Yang and Y. Zhang. Alternating direction [20] T. Yang. Trading computation for communication: [21] H.-F. Yu, C.-J. Hsieh, S. Si, and I. S. Dhillon. Scalable [22] Y. Zhen, W.-J. Li, and D.-Y. Yeung. Tagicofi: tag [23] Y. Zhou, D. M. Wilkinson, R. Schreiber, and R. Pan. [24] Y. Zhuang, W.-S. Chin, Y.-C. Juan, and C.-J. Lin. A [25] M. Zinkevich, M. Weimer, A. J. Smola, and L. Li.
Proof. We can rewrite g p ( U , V p , X  t | U t , V p where with m i = |  X  p i | and n j = |  X   X  p j | .
 Then, we have
L p ( U , V p ,  X  p t , V t )  X  G p ( U , V p ,  X  p t , V and v t , respectively. Then we have where h ( u , v ) contains all the second order terms, and the third order and forth order terms are contained in o ( u , v ). Hence, we can get and
Because and we have
Because we have  X | 2( u  X  u t ) T v t | + | 2( v  X  v t ) T u t | + | ( u  X   X  u  X  u t 2
Then we can prove
Because || u  X  u t || 2 F  X   X  2 and || v  X  v t || 2 F  X   X  So if we let we can prove that That is to say,  X  t should be smaller than some value which is dependent on the current U t and V p t .
 The second equation in Lemma 1 can be easily proved. Proof. From Lemma 1, we have Furthermore, we have Hence, we can get
