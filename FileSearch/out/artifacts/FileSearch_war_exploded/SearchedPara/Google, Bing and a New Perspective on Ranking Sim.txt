 In this paper, we propose a framework to characterize and compare two search engine results. Typical user -queries are ambiguous and , consequentially , each search engine will compute ranks in different manners , attempting to answer them in the best possible way . Thus, each search e ngine will have its own bias. Given the importance of the first page results in Web Search Engines, i n this paper we propose a framework to assess the information presented in the first page by measuring the information entropy and the correlations between two ranks. Employing the recently proposed Rank-Biased Overlap measure [2] we compare to which extent do Bing and Google rankings in fact differ. We also extend t his measure and propose a measure for comparing the information entropy present in two ranks. The proposed measure is based on the correlation of two ranks and the application of Jensen-Shannon X  X  divergence among two document set s. Our methodology starts with 40,000 user queries and crawls the search results for these queries on both search engines. The results allow us to determine the search engines correlations, crawling coverage, information overlap, and information entropy. H.3.5 [ On -line Information Services ].
 Measurement, Experimentation. Search engines comparison, Rank -biased Overlap , Jensen -Shannon divergence , Content Aware Rank Similarity. Nowad ays, Web users depend upon Web Search E ngines (WSE) to find the information they need. In fact, given the uncontrolled organization of the Web , about 85% of the users use WSEs to ease the task of locating information [5]. The short textual length of the queries traditionally used in these engines, added to the myriad of all possible user intentions and information needs, account for leveraging their ambiguity [4]. As a countermeasure, instead of responding to user queries with single documents, WSEs attempt to do so by returning lists of diverse docume nts to cover some of the different senses of the query . Thus , when querying different WSEs with the same query, their returned rank s won X  X  necessarily be the same, even though they index the same data  X  the Web. O ne of the most immediate comparisons relates to search result s usefulness, i.e. , how relevant are the set s of items returned by the search engines as responses to submitted quer ies . This type of evaluation is directly linked to human relevance judgments [1] and there is, at least, one extended study that compares WSE s through the perspective of result usefulness [7], relying upon relevance judgments for 1,000 queries. Yet , another question would be: given the same queries , and without relevance judgments, will Bing and Google produce the same rank s, with similar contents, or will their returned ranks tend entially diverge? There are a number of proposals that attempt to establish ranking similarity evaluations, although, to the extent of our knowledge, they usually make specific evaluations addressing the problem by comparing the differences on the lists X  order ing . In the present work, we do not use relevance judgments and assume ranks as an intermediary step that takes user s to their real intere st: the information contained in the documents pointed by the ranked URLs. To encompass this conception , we X  X e developed a new measure, Content Aware Ranking Similarity , to assess not only the divergence of the returned rankings but also the diversity of the retrieved information. The proposed measure is based on the correlation of two ranks and the application of Jensen -Shannon X  X  divergence among two set of documents. Our methodology starts with 40,000 user queries (obtained from TREC Web Track) and crawls the search results for these queries on both search engines. We present results for comparing two search engine s in terms of (i) indexed domains, (ii) rank correlation, (iii) mutual diversity, and (iv) joint analysis of rank correlation and mutual diversity. To address the problem of measuring ranking similarity, there are several popular statistic al measures that could be used right out of the box, such as Kendall X  X   X . However, given the scale of their search domain (the Web), WSEs do not enjoy the same properties as domain -specific search engines , s uch as a complete rank of the entire set of documents and full access to the same set of documents. In fact, WSE X  X  first page results are obtained through the truncation of larger, ordered by relevance , URL listings and this truncation may occur at any depth ; f urthermore, the retrieved rankings are usually non -conjoint, since the items returned by two different search engine s may not be the same; finally, when browsing through rankings, users expect to find their answers amongst the first ranked results. These are all aspects that are not fully dealt by these traditional measures. In fact , when comparing two rankings, Kendall X  X   X  would require them to be conjoint, would not consider the increased importance of top documents and the computed  X  values are heav ily related to a rank X  X  depth. As such, this measure is not the most adequate [2] to evaluate WS E rankings . On the other hand, Rank-Biased Overlap ( RBO) is a measure proposed by Webber et al. [1] to evaluate the similarity of rankings with particular characteristics, namely: incomplet eness , not listing all elements in the search domain; top-weightiness , as the top of the list is more important than the tail and indefiniteness , as the decision to truncate the ra nking at any depth is arbitrary . Even though the authors did acknowledge the existence of some metrics that account for top -weighted and non-conj oint rankings, their work revealed that none of them handle d the arbitrary cutoff depth and do not maintain monotonicity as it varies. Moreover , there is a point to RBO that adds to its interest even further: it models  X  to a degree  X  a very human -like cha racteristic, and one that is especially present when browsing through WSE X  X  results -the user X  X  decreasing patience. Indeed, RBO considers list entry matching closer to the top as having more influence on ranking similarity than matching occurrences furth er down the rank ing -much like a web surfer X  X  fading patience would take him to consider, if asked to compare two long rankings. Rank -biased Overla p metric computes the correlations of non -conjoint ranks, i.e., 0, 1 RBO  X  X  X   X   X  X  , where uncorrelated ranks correspond to 0 and completely correlated ranks correspond to 1 . Divergence measures provide a numeric value to assess the difference between two statistical distributions . Kullback -Lei bler divergence is the usual choi ce for assessing the divergence among two statistical distributions : In our case, we X  X e opted for Jensen -Shannon divergence since it has some important properties, such as symmetry and its value is always finite . See [6] for details. In the present work , we used a set of 40 ,000 distinct queries from the TREC Web Track . This query collection is representative of the heterogeneity of the universe of user queries . Each query was submitted to Bing and Google X  X  online search API through simple HTTP requests and their returned rankings were stored in a local database. Each list of retrieved results was truncated to a maximum of 16 results per query , resulting in 639, 004 retrieved documents for Google and 639 ,846 for Bing. Since the research we X  X e conducted required analysis for each of the retrieved documents X  contents and given their sheer number, we X  X e only considered pure HTML documents or, at least, documents containing HTML formatted information . The aforementioned process provided a perspective on the way that each search engine crawling coverage of the Web , yielding 95,354 common domains, 114, 583 domains exclusively crawled by Bing and 121 ,877 domains exclusive to Google. From here, it follows that Google has a broader view of the W eb , having retrieved more 7, 294 domains than Bing , in our test s. To provide a better response to the challenge of comparing Bing and Google, we X  X e delved into the task of measuring the correlation between the URLs of the two ranks . Due to the previously presented reasons, we X  X e found RBO to be the most adequate measure t o assess WSE X  X  ranks. S o, we X  X e applied RBO to the rank pairs returned by Google and Bing when responding to the submitted queries . W e summarize our RBO findings in the histogram below.
 From the histogram , we can observe that the rankings computed by each of the engines were , for the most part , dissimilar. In fact, the RBO score distribution has a mean of 0.314 , with a standard deviation of 0.138. Moreover, only 8.64% (approx.) of the queries returned rankings having RBO similarity above 0.5. This fact, added by the large amount of queries scoring RBOs near 0, is enough to let us conclude that there are significant differences to be found on the two WSE X  X  ranking algorithms. So far, we  X  X e compared two search engines through their indexed domains and the correlation of their retrieved URLs . However, since the most likely reason that takes a user to perform a query on a WSE is a real need for information, it makes sense to exten d ranking similarity evaluation beyond the URL list ing . For instance, consider the case of two different engines that returned totally disjoint rankings, but where each of the featured URL points to documents with the same content . Although RBO  X  as well as any rank similarity measure that only considers rank positions  X  would promptly evaluate the ranks as completely uncorrelated, it is expectable that a user would deem them as having the same information value: in essence, these tw o rankings would be very similar indeed . From this perspective, we argue that evaluating correlation solely through ranked URLs is an incomplete way of assessing the similarity between two ranks. Consequentially, we X  X e extended o ur research to compare the contents of the retrieved documents . In this section we will address the human perception of information similarity and measure the information contained in the two ranks to assess the information content of each rank . In contrast to m ore traditional Information Retrieval systems, queries used on WSE are usual ly very short descriptions of a user X  X  information need (on the average, composed by 2.21 terms) [3]. Not surprisingly, such concise descriptions may be subject to high degree s of ambiguity and this is further aggravated by th e daunting heterogeneity of the universe of users . Since the answers to queries are returned as sets of ranked lists, it is possible to include documents for more than one interpretation, so, perfect query disambiguation is not a requirement. If we consider a WSE return results as a whole , i.e. an amount of documents/ information aiming at answer ing a particular query , it makes sense to ponder the use of measures such diversity . Indeed, after querying a search engine, it is expectable for all of the retrieved documents to ha ve different contents -although all of them should be relate d to , at least, one plausible interpretation of the submitted query. The measure we X  X e chosen to evaluate a ranking diversity was the Jensen -Shannon divergence , div JS . This divergence is, by definition, a measure of similarity between two probability distributio ns. T o use it, we compute d a histogram of word occurrences out of a given document textual contents  X  a bag -of-words representation  X , and obtain ed a simple probability distribution s that can be directly used in the divergence X  X  calculus 1 . For the particular case of two documents, Jensen-Shannon X  X  divergence provides an evaluation about the extent to which they are similar to one another . However, since this work is about comparing two search engine rankings rather than two single documents, we X  X e adapted the divergence X  X  application context in the following way: for a gi ven rank , we define its internal diversity , i D as the sum of the Shannon -Jensen divergences found between the docume nts of each possible combination of tw o. For a hypothetical ranking r , we formalize: where 2 r C is a combination of two documents of rank r . Since Jensen -Shannon diversity X  X  image set is 0  X   X  , it follows immediately that the sum of any arbitrary number of diversity values are also mapped to the same range, i.e. 0 i Equation (1) will return a value for the diversity of a single ranking, which will be higher for more dissimilar documents. If this same metric is used to directly compar e documents of two ranks , th e resu lts may be somewhat misleading. I n fact, although equation (1) is computed out of document textual content, it does not encompass any relevance judgments and, as a consequence, there is a possibility for even two fully distinct rankings to yield the same internal diversity score. For instance, the ambiguous query  X  X ight X  may be reasonably interpreted as concerning the electromagnetic radiation or the absence of weight (other interpretations being possible) . Of course, if two engines followed divergent inte rpretations, it X  X  fair to expect each returned set of documents to be essentially different. However , to obtain the same internal diversity values , it X  X  enough for them to share the same amount of word diversity . To address the issue above , we extend the internal diversity definition and create a new measure that establishes a dir ect relation between two ranks , the relat ive diversit y r two different rankings 1 r and 2 r , we propose the diversity of relative to 2 r , to be the sum of the diversities computed between each possible combination of two documents , with one of them being present on 1 r and 2 r and the other being exclusive to This can be expressed as the Cartesian product of the set of common documents and the set of 1 r  X  X  exclusive documents. Formalizing for the aforementioned rankings: T o in crease cross -document matches , we X  X e removed all HTML tags, stopwords , and stemmed the resulting words . with 0 r D  X   X  X  . Since the explained procedure explicitl y depends upon the existence of a subset of documents that exclus ively belong to 1 r , there is a possibility for be different from  X   X  21 , r D rr . That way, in order to perform conclusive comparison s between the rankings , both values must be calculated . After accomplishing the described steps, we have two relative diversity values: one for the diversity of r and another for the diversity of 2 r relative to 1 r . Finally, we can obtain an indicative measure out of these values by simply calculating their difference . T o this end we propose a new definition, the mutual diversity , where m D  X  X  . As a consequence a special case occurs when two ranks have no documents in common. Following from the constraints of equation (2) , the resulting value will implicitly be zero . Thus, when lacking shared documents to compute a meaningful m D value , we replace the relative diversities in ( 2) by the values of each ranking X  X  inter nal divergence (1). Then, for this particular case, we assume the mutual diversity to be : Thus , f or two rankings 1 r and 2 r , m D will be 0 when both have the same diversity; m D will tend to negative infinity as gets more diverse than 1 r , and to positive infinity as diverse than 2 r . Due to time constraints, we have performed t he diversity evaluations on the returned rankings for the first 33,50 0 queries, out of the 40,000 that compose the original set. Our mutual diversity findings are summarized in the following histogram (when Google X  X  rankings are more diverse than Bing X  X , the values fall in the range  X   X  0,  X  X  and in ,0  X  X   X  X  X   X  X  X  , otherwise): As dep icted in the graph, the Mutual D iversity approximately follows a normal distribution around the mean value of -4.0697, with a standard deviation value of 68.204. Moreover, only 1.3% of the queries yielded rankings with Mutual Diversities outside the interval  X   X  200, 200  X  . These measurements hint that the differen ces at the document information level may not be as obvious as the ranking related ones, as measured by RBO . In fact, the narrowness of the curve in ( Figure 2) tells us that the retrieved information, for the majority of the queries, is not very divergent and this allows us to infer that the query interpretation also does not drastically differ gre atly between engines. Nevertheless , the Mutual Diversity mean value of -4.0697 also indicates that Bing tends to answer queries in more diverse ways than Google and this can happen as a result of particular differences in query interpretation, (perhaps int entionally) weaker query disambiguation strategies, or due to the rank ing of longer, more entropic, less concise documents. So far, we have presented two disjoint perspectives on ranking similarity evaluation: one, RBO, which assumes rank s as ordered lists of URLs; and another, the Mutual Diversity, that sees ranks as document sets whose significance lies solely on their textual contents . Even though they produce expressive ranking similarity evaluations all by themselves, each one evaluates specific properties of search results . It would be more convenient to combine both measures into a single one  X  especia lly one that does not jeopardizes their individual expressiveness. To this end, we now propose a new , conjoint , simil arity definition , the Content Aware Rank Similarity ( CARS ). Then, for a pair of rankings, and 2 r , we assume a two dimensional vector, having the first and second components set to be their Mutual Diversity and RBO evaluations, respectively. If we normalize all of the Mutual Diversity  X  X  values so t hat 0, 1 m D  X  X  X   X   X  X  (for instance, by computing the absolute value of their division by the maximum registered value of m D ), we obtain similarity vectors mapped onto the space bounded by (0,0) and (1,1). It follows immediately that, in this space, the vector (0,1) stands for total similarity between two rank s, both in terms of Mutual Diversity and RBO. We say that CARS is the distance between the ranking pair similarity vector and the total similarity vector. The normalize d expression of CARS is , resulting in values in the interval [0, 1] , with 0 standing for equal ranks and 1 for completely different ranks.
 We represent our CARS  X  X  similarity findings in Figure 3 (the circle X  X  diameters represent the density of vectors in the center of the circle ). The chart shows that, despite the obvious differences on the rankings as plain, relevance ordered , URL lists (significant vector density near RBO=0) , the differences at the level of the conveyed information are more subtle. In fact, it now appears that RBO is too quick to dismiss the ranking pairs as dissimilar, since, after evaluating the information contained on the documents, the rankings are no longer considered so dissimilar ( they X  X e close to the point ( 0,1)). To further support the comprehensiveness of the proposed mea sure , CARS , it is interesting to note how most of the ranks for the same queries are highly uncorrelated (RBO=0), but share some information as measured by the Mutual Divergence of those ranks (most of the ranks fall in the 0.5 m D  X  zone) In the present work, we have measured the differences in Google and Bing X  X  search results for the same set of queries. We X  X e conducted an evaluation from two distinct points of view : a more classic one, based on RBO, that assumes returned ranks as relevance ordered URL lists and, afterwards, we X  X e proposed a new one, the Mutual Diversity, that sees rankings as sets of documents convey ing information . Lastly , we X  X e proposed a way of joining these two disjoint evaluations into another, CARS, that provides a simple but more profound evaluation on ranking similarity. Since it forces a second, content based, iteration when evaluating ranking similarity , it was a positive result to see that CARS had a direct impact on pure RBO evaluations.
 Since we X  X e attempted to extend the concept of ranking similarity to the level of the information conveyed by the retrieved documents , and given the n-grams potentially increased linguistic expressivity when compared to that of single words, we would consider them as an important future work.
 Finally, it is important to observe that CARS is not, like RBO, a ranking similarity evaluator but is rather a pair -wise comparison of two search engine results . Acknowledgments. This work has been funded by the Portuguese Foundation for Science and Technology under project references PTDC/EIA -EIA/111518/2009 and PEst -OE/EEI/UI0527/2011, Centro de Inform X tica e Tecnologias da Informa X  X o (CITI/ FCT/ UNL) -2011-2012. [1] J. Bar -Ilan. 2005. Comparing rankings of search results on [2] W. Webber, A. Moffat, and J. Zobel. A similarity measure [3] B. J. Jansen, A. Spink, and T. Saracevic. 2000. Real life, real [4] B. J. Jansen, D. L. Booth, and A. Spink. Determining the user [5] S. Lawrence and C. Lee Giles. Accessibility of information [6] T. M. Cover and J. A. Thomas, Elements of information [7] H. Zaragoza, B. Cambazoglu, and R. Baeza-Yates. Web 
