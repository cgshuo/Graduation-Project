 1. Introduction
Question answering (QA) is considered a key technology for handling information overload caused by the ever increasing amount of information. With the advances in natural language processing (NLP) techniques and the need to deliver more fine-grained information or answers than a set of documents returned by a search engine, various QA techniques have been developed corresponding to different question and answer types. It is obvious that future QA systems will be equipped with more and more capabilities to handle different types of advanced questions and with more effective technologies for the existing capabilities. To make best use of additional QA technologies, a QA framework was proposed where the invocation sequence of multiple QA modules was determined by a learned strategy for the particular type of question at hand ( Oh, Mya-dling diverse types of questions.

The main core of the strategy-driven QA method is to invoke individual QA modules in the sequence based on the learned strategy and boost the weight of returned answers. A set of strategies are learned automatically from the past history of QA operations involving individual QA modules, which reflect how well different QA techniques provided correct answers for different types of questions. Strategies prescribe not only the sequence of invocation but also ways to verify an answer re-turned from a QA module and boost its weight if necessary.

The main benefit of the strategy-driven QA method lies in finding answers more accurately with the reduced number of transactions when compared against methods without a strategy and a method with manually constructed strategies ( Oh et al., 2009 ). More specifically, such a strategy-driven QA system with multiple QA modules can:  X   X  provide an answer with an increased level of confidence because an answer returned by a QA module can be verified by another,  X  improve efficiency, without sacrificing the quality (i.e. effectiveness), compared to the straightforward approach of blindly routing the question to all the available QA modules and combining the results, and  X  save efforts in manually constructing strategies and therefore make it easier to incorporate newly available QA techniques
Building on the previous work that proposed the method and showed its overall values, this article investigates the effects and the roles of the answer verification and weight boosting method, which is the main core of the strategy-driven QA framework, in comparison with a strategy-less, straightforward answer-merging approach and a strategy-driven but with manually constructed strategies. The main contribution of this paper is detailed analyses of the strategy-driven QA frame-work through a series of experiments, from which we obtain new insights on its internal workings and its sensitivity to key elements such as question types, the number of QA modules, and types/amounts of errors made by QA modules. 2. Related work
Some recent research attempts to break away from the strict single pipeline architecture of traditional question answer-ing systems with the aim of improving the performance. They exploited a  X  X  X ulti-strategy X  X  several answering modules for different question types and showed that it gave a better performance than a single pipeline approach.

One way to utilize multiple QA modules is to route a question to all the modules as in a  X  X  X eta-search X  X  engine. A pro-cessed question is simply distributed to the individual QA modules in parallel, and then the results are merged with re-rank-
Katz, 2003 ) adopt this straightforward strategy, obvious weaknesses of this approach are the inefficient use of resources, especially with a large number of QA agents, and the one-for-all result combination strategy. One can also argue that getting the same answer from multiple sources would increase its confidence level ( Clarke, Cormack, &amp; Lyman, 2001; Nyberg et al., 2004 ). For example, Clarke et al. (2001) obtained 18% improvement (23 X 41%) using the redundancy. However, this type of redundancy may not be necessary all the time.

Another direction is to  X  X  X ard-code X  X  answering strategies manually. When implementing a QA system, the developer has to understand the capability of individual QA agents and carefully craft the answer selection strategy for the given agents.
This method seeks to approximate domain-specific expert knowledge by employing human experts to develop combining is configurable. Each system consists of a set of  X  X  X trategies X  X  (modules in our terminology), one or two of which are selected based on the question processing result, to solve different classes of questions either independently or together. Aside from the fact that it is more like a one-best approach, there is no explicit mechanism for answer verification and weight boosting.
QA systems described above assign a suitable  X  X  X trategy X  X  (a QA module in our terminology) for a user question based on some built-in rules or scenarios. The process of manually determining the strategies and setting some parameters for com-bining multiple results requires a significant amount of human efforts in understanding the capabilities of and the interplay tional QA modules to be added at a later stage. When a new technique is added, the entire set of strategies must be re-con-figured manually.

The work in ( Oh et al., 2009 ) is unique in that the question analysis process determines a strategy for a module invocation sequence and result integration, not just a single module or all modules to which the question is sent. The answers from the selected modules are verified and combined based on the strategy that was learned automatically. While the paper intro-duces the new method and shows its efficacy in comparison with the other approaches in an experiment, it treats the meth-od as a black box in the experimental analysis. This paper investigates the roles and effects of the answer verification and weight boosting method and attempts to provide insights on its internal workings and its sensitivity to key elements such as question types, the number of QA modules, and types/amounts of errors made by QA modules. 3. QA driven by automatically learned strategies
To provide a context in which we can analyze the roles and effects of the strategy-driven QA and the automatically learned strategies for weight boosting, we give a brief explanation of the underlying QA system equipped with a strategy learning and execution method utilizing multiple QA. The right hand side in Fig. 1 corresponds to the offline process of text analysis for answer generations and of strategy learning. The other half on the left roughly corresponds to the process of QA with a user question that needs to go through the Question Analysis module. The result from the module is passed to the
Strategy Selection and Execution module that generates answers, which need to interact with various QA modules based on the selected strategy.

Given a question in the form of natural language sentence, the Question Analysis component processes it so that a strategy can be selected. The module employs various linguistic analysis techniques such as POS tagging, chunking, named entity tag-question generated from the Question Analysis component has the following form: where AF , AT , QT , and AS stand for the answer format, the answer theme, and the target of the question and the expected answer source, respectively ( Oh et al., 2009 ).
 The answer verification and weight boosting methods is implemented in the Strategy Selection and Execution component.
It selects a strategy based on the internal query, invokes one or more QA modules in the strategy, depending on whether the calculated evidence for each answer candidate from the earlier module is strong enough, and determines the final answer by incorporating the answers and their evidence values returned from multiple QA modules. The top-ranked answer from the first QA module is not accepted as the final unless its confidence value is higher than the threshold associated with the mod-ule. If none of the answers from the first module have a confidence weight higher than the threshold, their confidence weights can be boosted while they are merged with those from the second QA module, producing a new ranked list of an-swers. This process goes on with the remaining QA modules in the strategy until the confidence level of the top-ranked an-swer exceeds the threshold or there is no more QA module to be invoked in the strategy, whichever comes first.
Given a question,  X  Where was Mozart born? X  , for example, the Question Analysis component determines the answer format being singl e, the answer theme DATE , the question target object Mozart , and the focus Birth Place . When  X  X  KB QA ? General answer with its confidence value. If the confidence value is lower than the threshold or no answer is found, the General QA and the Passage Retrieval modules are invoked in parallel (i.e. without any dependency). Depending on the returned answers, they are either merged with the first answer, if any, or used for its verification and confidence weight boosting.
The overall flow of the strategy learning algorithm is described as a pseudo-code in Fig. 2 . It begins with m n matrix where m and n represent the numbers of questions and different QA modules, respectively, and an element is a list top-five answers from each module for each question in the training data. The learning algorithm accepts the input matrix and di-vides it into m subsets, each of which will be used to create a strategy that invokes one of the m QA modules as the first AS (Preparation Step). The outer loop in the main body corresponds to the process of selecting a particular QA module as the main AS and includes three steps inside to determine the other modules to be invoked subsequently for the selected module and their threshold values. Given a question, each invoked module is examined by comparing its answer against the gold standard to evaluate its goodness for that question. Additional details for the learning algorithm are beyond the scope of this article and can be found in Oh et al. (2009) . 4. Evaluation and analysis 4.1. Evaluation data
For our QA experiments and analysis, we chose to use a set of encyclopedia articles as the source of answering questions, mainly because the answers in those articles are trustable, unique, and bounded. various types, collected from real users, and the answers from the Web and an encyclopedia, we found that over 80% of the answers were obtainable from the encyclopedia while the Web answers sometimes contradicted among themselves and were not always confirmative. Encyclopedia answers were richer with fuller information in the articles concentrated on a topic. Be-sides, since the textual descriptions are almost error-free, we did not have to deal with compounded errors stemmed from ill-formedness of web text; after all, our goal was to test the strategy-driven QA with automatic strategy learning.
Consequently we chose to use Pascal tm Encyclopedia ( http://www.epascal.co.kr ), currently consisting of 100,373 entries anced diversity of information in the encyclopedia were deemed desirable for testing the proposed QA framework utilizing multiple QA modules. Nonetheless, it does not mean that our proposed methodologies are biased on a specific type of knowl-edge source. We show this by running experiments with web documents separately.

Like the TREC QA track ( Lin, 2007 ), we considered various levels of question/answer types and gradually extended our evaluation set from simple to complex questions step by step. To create answers for the given set of collected questions, we fist generated an answer pool for each question, by obtaining two different results from a traditional information retrie-( Voorhees &amp; Tice, 2000 ). The final answers were generated manually by a group of human assessors: the answer sheet for each question was judged by three assessors. Each question in the final evaluation set consists of the question, answer, and answer passage session parts as shown in Fig. 3 .

For strategy learning, we used 260 h question, answer i pairs of the training data, which is part of the entire evaluation set of 760 pairs of various sorts corresponding to answer sources and difficulty levels. Table 1 summarizes distribution of the questions in the training and evaluation sets for different question types. 4.2. Experimental setup
For effectiveness comparisons, we employ precision , recall , and F-score , sometimes with the mean reciprocal rank (MRR) ( Voorhees, 2000 ) and the well-known  X  X  X op-5 X  X  measure that considers whether a correct nugget is found in the top 5 an-swers. Because of the large number of comparisons to be made for different cases, we use F -scores for summary tables.
For the purpose of showing the overall efficacy of the strategy-driven QA method, five cases were examined: (1) tradi-tional QA using general indexing and passage retrieval, (2) one-best individual QA where a question was sent to the most appropriate QA module only, (3) simple routing QA where a question was routed to all the available QA modules and the answers were combined, (4) QA by manually constructed strategies, and (5) QA by automatically learned strategies. The tra-answers from the six QA modules were merged into single ranked list after normalizing the weights so that all the answer lists would have the same weight ranges. The methods (4) and (5) are basically the same except that for the former, the strat-egies were constructed by human experts who had to run six different QA modules repeatedly for a large number of ques-tions and determined the order of invocations and the threshold values ( Oh et al., 2006 ). 4.3. Analysis results
There are three directions along which we investigated on the roles of the strategies and the weight boosting method: (1) effects of the strategies on different question types, (2) effects of the strategies on reducing the number of transactions or improving efficiency, and (3) effects of adding new QA modules incrementally in the proposed QA framework. Besides, we examined the resilience of the framework in terms of consistency in improving effectiveness regardless of the question analysis errors and the source text from which answers are extracted. 4.4. Overall benefits
As in Table 2 , the automatic strategy-driven method shows the highest overall performance among the five treatments ( Oh et al., 2009 ). When a single best QA module was chosen for a given query (the one-best case), a total of 356 answers were correct. On the other hand, 395 correct answers were returned when all the QA modules were invoked and the answers merged (the simple routing case). This indicates that about 10% of the correct answers were returned by  X  X  X ther X  X  QA modules that were deemed inappropriate for the given questions, necessitating invocation of multiple QA modules for individual questions.

In comparison with the simple routing method, the strategy-driven method based on the strategy learning algorithm shows improvements: 10.5% (0.687 X 0.756 MRR) for effectiveness and 27.2% (4.667 X 3.643 s. per question) for efficiency. This result indicates that by executing only necessary QA agents with corresponding threshold values, we can not only save time but also provide better answers, suppressing erroneous ones. Even compared with the manually built strategy-driven QA method, it shows the better results in both effectiveness and efficiency. Aside from the significant amount of human efforts in constructing strategies based on trials and errors, the execution sequence and/or threshold values in some strategies are not always optimal, affecting the overall performance. 4.5. Effects on different question types
We analyzed the experimental data to see the effects of the strategy-driven QA method ( X  X  X utomatic strategy method X  X  in modules exist. Table 3 shows the comparisons among the four QA methods for the five question types. Across all the ques-tion types, the automatic strategy method outperforms the one-best, simple routing and manual strategy methods except for the superlative question type whose answers are so readily available from the Superlative QA module that there is little dif-ference among the three methods except for the one-best: as long as questions of that type are sent to the right module, the answers are returned with high confidence. This indicates that the automatic strategy method is most valuable when ques-tions of different types are not unambiguously mapped to the correct QA modules. It is also interesting to note that for none of the question types, the manual method outperforms the automatic one because of the inaccuracies in the threshold values of the manually constructed strategies.

A further analysis of how different question types were answered by the simple routing method and the automatic strat-egy method shows why the above performance differences were obtained. By comparing the number of answers returned (the first line in each row in Tables 4 and 5 ) and the number of correct answers (the second line), we can see that the  X  X  X on-expert X  X  QA modules (the General QA and Passage Retrieval modules) were invoked much more frequently with lower success rates in the simple routing case. For the strategy-based method, both the numbers of invocations and correct an-swers were reduced because the  X  X  X on-expert X  X  module did not have to be invoked so often and many of the correct answers were already returned by the other  X  X  X xpert X  X  modules invoked. For example, as in Table 5 , out of 313 questions sent to the
Passage Retrieval module, only 73 were counted as correct answers because they were not retrieved by any other modules invoked previously. Other correct answers from the module were only used for weight boosting. In addition, an invocation of a QA module may not be fruitful at all as in the case of the KB questions for the simple routing, where the eight answers returned by the Descriptive QA module were all incorrect. Obtaining multiple answers is not always helpful because some may serve as a noise. 4.6. Reduction on the use of computing resources
Besides the improved quality of the final answers, we obtain savings in the use of computing resources because the auto-matic strategy method invokes QA modules more judiciously. To see the role of the learning algorithm in choosing the right
QA modules, we first analyzed effectiveness of individual QA modules by checking the ratio of the correct answers to the total number of answers generated by each QA module. As in Table 6 , the precision values are quite high for the Superlative , of the number of calls by moving from the simple routing method to the automatic strategy method: Superlative QA (52 ? 52 in Table 7 ), Descriptive (92 ? 84), KB (52 ? 50), General (396 ? 293), and Passage Retrieval (500 ? 313). The lower the accu-racy, the less number of times it is invoked. The learning algorithm ends up discriminating against less effective QA modules. The only exception is the List QA module that returns an answer only when its confidence is very high.

The changes in the total numbers of the transactions (invocations) from the simple routing to the manual strategy to the automatic strategy methods (1099 ? 817 ? 799 in Table 7 ) indicate how much computing time was saved progressively. Another way of interpreting this data is that the weights from  X  X  X xpert X  X  QA modules were so high that the General QA and ler number of calls were made in the automatic strategy than in the manual strategy case because the threshold values were set correctly to avoid superfluous calls for other QA modules. The reduction on the number of calls resulted in savings in execution time by more than 27% on the average ( Oh et al., 2009 ). 4.7. Effects of adding new QA modules
We started with four QA modules first and added two specialty modules to the system one by one. The initial set consists of the KB , Descriptive , General QA modules and Passage Retrieval , and the Superlative and List QA modules were added subse-quently. As expected and shown in Table 8 , improvements were made with additional specialty QA modules for both cases.
The amounts of the improvement were greater with the first addition ( Superlative QA module) because there were more questions that could be handled by it than those by the second one ( List QA module).

While the amounts of improvement in effectiveness were similar for both of the methods, it is clear that the numbers of transactions are quite different as in the last column of Table 8 . While the number of transactions must increase with an addition of a new QA module in the simple routing case (1040 ? 1092 ? 1099), it is reduced in the strategy-driven case be-cause superfluous invocations, especially for unreliable modules, can be avoided with a specialty QA module that returns an answer with a high weight (906 ? 806 ? 799). In other words, we can obtain more benefits as we employ more specialized QA modules.
 4.8. Effects of correcting question analysis errors
Since the proposed strategy-driven QA method relies on judicious invocations of various QA modules, it is imperative to see how the errors made at different stages of the question analysis phase influence the final results. The question analysis stages with which we differentiate the types of errors are shown in Table 9 . The number of errors and the error rate for each error type corresponding to a processing stage are shown in Table 10 .

One might think at the first glance that errors in query analysis can degrade the performance greatly because an incorrect decision on the question type would fire a wrong strategy that in turn invokes a wrong set of QA modules. On the other hand, the strategy-driven method appears to have a capability of remedying the invocation errors because when an answer from a
QA module does not have a strong weight, it is reinforced by calling additional QA modules and executing the weight boost-ing process.

Our analysis results seem to support both of the arguments. Table 11 shows the effects of correcting different types of question analysis errors for the three different methods: simple routing, manual strategy-driven and automatic strategy-dri-ven. The number of errors reduced is greater for the simple routing method when the first three types of errors (Q1 + Q2 + Q3) were corrected. That is, the strategy-driven methods appear to be less sensitive to the errors made at the query analysis step. Since the number of QA errors decreases as different types of preprocessing errors were corrected, how-ever, building a more accurate question analysis module would help invoking more accurate QA modules in the first place. 4.9. Experiments with web data
Despite the reasons for using the encyclopedia as stated earlier, we ran a set of experiments with web pages that are not as predictable in their discourse and sentence formats as encyclopedia articles. The main purpose was to understand the ex-tent to which the benefits of the proposed QA framework can be transferable to a knowledge source that is likely to incur more errors in generating answers. As such, we compared the proposed method against the traditional, one-best, and simple routing methods again. In addition, we show the performance differences in applying various linguistic analysis tools to the two different knowledge sources.

We crawled web documents from some commercial web portals for entertainment and news media to construct a web corpus consisting of 91,500 web documents in the areas of entertainments and sports. It was divided into a training set of documents are distributed among the different types of questions.

Table 13 shows overall comparisons among the four different methods. As in the case of the encyclopedia collection, the strategy-driven QA outperforms the others. While the absolute F-score and MRR-5 values (0.643 and 0.541) are much lower with the web collection than with the encyclopedia collection (0.859 and 0.750), the benefits or improvements are greater with the web collection: the percentages of improvement for the encyclopedia collection were 10.5%, 19.6%, and 49.1% over the simple routing, one-best, and baseline methods whereas here they are 9.9%, 43.6%, and 78.0%. An interpretation of the differences is that although it is usually more difficult to extract answers from the web collection, especially with the base-line and one-best because of the errors in text processing, the benefits of employing multiple QA modules are greater be-cause the errors from single sources (i.e. individual QA modules) can be remedied to a greater effect. Nevertheless, it should be noted that the two collections consist of different sets of questions as well as different sets of document, making direct comparisons of the numbers not meaningful.

Table 14 shows the performance differences of various linguistic analysis tools between the two different collections. The performance values are lower with the web collection across all the steps to result in lower QA performance, but the biggest gap lies in the sentence structure analysis. 5. Conclusion
This paper reports on investigations of the effects and the roles of the strategy-driven QA method by conducting a series of experiments and analyzing the results. Our experimental work reveals a number of useful insights about the effects of the strategy-driven QA method. First of all, the automatically constructed strategies are more effective and efficient than the manually constructed ones for which a significant amount of human efforts are required, let alone others like simple routing and one-best methods. The accurate settings of threshold values in the learned strategies help determine the correct answers by calling the right QA modules at the right time. In addition, since more reliable and appropriate QA modules are chosen to avoid superfluous calls, it requires less computing resources. The positive effect of reducing computing resources is increased as additional QA modules are added. The strategy-driven method appears to be more forgiving than the simple routing method in that question analysis errors affect the QA results less negatively.

The experiments with web pages, as opposed to the encyclopedia articles, allow us to see whether the benefits of the strategy-driven methods is transferable to a knowledge source that is likely to incur more errors in generating answers.
The improvements are greater with the web data that cause more errors in extracting answers than with the encyclopedia, confirming that the strategy-driven method is more resilient with errors.
 Acknowledgement
This work was supported in part by the Korea Ministry of Knowledge Economy (MKE) under Grant No. 2008-S-020-02 and by Brain Korea 21 project sponsored by Ministry of Education and Human Resources Development, Korea.
 References
