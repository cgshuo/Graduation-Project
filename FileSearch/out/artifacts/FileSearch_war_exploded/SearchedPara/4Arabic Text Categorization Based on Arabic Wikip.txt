 ADNAN YAHYA and ALI SALHI , Birzeit University Over the World Wide Web, the continuous increase in content creation in general and Arabic content in particular comes with a great need for tools to overcome the many challenges facing the processing and retrieval of Web content. For Arabic content, these challenges include understating the content, efficient retrieval of useful information from this content, improving the quality and efficiency of searching Arabic data by providing tools, such as spelling, correction, named entity extraction, document cate-gorization, query optimization, filtration, and more.

The process of categorizing (or classifying) documents by assigning one of a set of given categories to the document is an important challenge when it comes to Arabic. Being able to search precategorized documents helps improve search results due to the level of ambiguity in everyday Arabic text. For example, if a user searched for the word  X  (Riyal), he might be looking for  X   X  X  X  X  X  (Real Madrid) team or (Saudi Riyal) currency. If the search is done based on categorized documents, then the user will have the option to search either under  X  X  X  X  X  (Sports) or under (Finance). Moreover, text categorization can be used to define better spell-checking sys-tems, as in our earlier work, where we used categorization and categorized datasets to narrow the possibilities of outputs in a spell checker by first categorizing the input doc-ument then use a categorized dataset (dictionary) as the reference for the correction process. This improved the results and performance [Yahya and Salhi 2012]. Also, cat-egorization may help improve question answering tasks by resolving text ambiguities by reference to categories. (As we said before,  X   X  X iyal X  in economics is different from  X   X  X iyal X  in sports,  X  X  X  X  as  X  X nflation X  in the economic context or  X  X nflammation X  in the medical context.)
In this article, we are offering an algorithm for categorizing Arabic text, built by adopting a simple categorization idea, then moving forward to a more complex one. The algorithm relies on a highly categorized corpus-based datasets obtained from the Arabic Wikipedia by using a combination of manual and automated processes to build and customize categories. Our algorithm is hierarchical and supports two levels of cat-egorization of Arabic text. That is, our categories are grouped into a hierarchy of main categories and subcategories. This introduces challenges resulting from the correlation between certain subcategories and the overlaps between main categories, something we will also discuss in this article.

The rest of the article is organized as follows: in Section 2, will discuss some earlier and related work. In Section 3, we will discuss our corpora and how they were built. In Section 4, we will discuss the text filtration methods that we applied in some of our tests. Discussing our categorization process will be in Section 5, where we talk in detail about our algorithm and the different approaches and filters we tested. Comparisons with related work will be discussed in Section 6, followed by conclusions and pointers to future work in Section 7. In this section, we will summarize and discuss some earlier related work, and then we will select some resources from early work for comparison tests. The comparison is done by comparing the results reported by others with the results of using their resources in training and testing our algorithm.

Work on Arabic text classification used several approaches with different data re-sources. Some use Manhattan and Dice measures on N-Gram sets extracted from a corpus of text documents covering four categories: Sports, Economy, Technology, and Weather [Khreisat 2006]. The corpus there was collected from different online Arabic newspapers and was split into 60% training data and 40% testing data. The accuracy value (F1 score) for Manhattan measure was 60.7% and for Dice measure was 85.6%. Al-Harbi et al. [2008] used Chi-Squared statistics to select the best N (where N = 30) terms to represent a certain category. They built their experiments using several sources of Arabic text obtained from different news agencies and websites. The data was split into 70% training and 30% testing, and two classification algorithms were applied: the support vector machines (SVM) algorithm and Clementine for the C5.0 decision tree algorithm with average success rates of 68.65% for SVM and 78.42% for C5.0.

Another study used Na  X   X ve Bayes algorithm and reported 68.78% accuracy [El-Kourdi et al. 2004]. Another study used kNN algorithm and reported 96% accu-racy results based on six categories: Politics, Economics, Health, Sports, Cancer, and Agriculture [Al-Shalabi et al. 2006].

Syiam et al. [2006] built an intelligent system for Arabic text categorization by adopting machine learning algorithms, different stemming algorithms, and feature selection and term weighting methods with kNN and Rocchio classifiers. The tests were done over six categories (Arts, Economics, Politics, Sports, Woman, and Informa-tion Technology), and concluded that the Rocchio classifier has an advantage over the kNN classifier with accuracy of 98%.

Alsaleem [2011] investigated the Na  X   X ve Bayesian method (NB) and SVM algorithms on Arabic corpora of seven categories extracted from The Saudi Newspapers (SNP). His experiments reveal that SVM algorithm outperforms NB, which agrees with the results in Saad [2011].

Saad [2011] studied the impact of text preprocessing on classification by analyzing input text, changing term weighting schemes and Arabic morphological analysis (stemming and light stemming), and using approaches, such as Decision Tree, k -Nearest Neighbors, Support Vector Machines, Na  X   X ve Bayes and its variations to clas-sify the input text. The researcher applied the classification algorithms to seven dif-ferent corpora (splitting each corpus to training and testing texts). He concludes that light stemming with support vector machines outperforms other algorithms.
Yang et al. [2003] studied well-known categorization algorithms, such as support vector machines, k -nearest neighbor, ridge regression, linear least square fit, and lo-gistic regression by an investigation on the usage of those algorithms in a hierarchical setting for categorization. They proved that the scalability of a method depends on the topology of the hierarchy and the category distributions, in other words, the distribu-tion of categories and subcategories affects the categorization process, which we will see in Section 5.4.

Qiu et al. [2011] highlights three approaches for hierarchical text classification: flat, local, and global approaches. The flat approach uses only the classes of end categories in the leaf nodes (categorize by subcategories only) and works without hierarchical class information. The local approach is based on a top-down fashion, which starts by categorizing the text into main categories on the top-level then re-categorizes the sub-categories (low-level) under the main category (top-level). The global approach builds only one categorizer to discriminate all categories in a hierarchy. In our work, we first will adopt a local approach and prove later in Section 5.4 that using a flat approach gives better results.

As noted, results vary from one experiment to another because of the data, algo-rithms, and measures used. In most of the work we reviewed, authors use few distinct categories in their experiments and do not address the challenge of having a large set of categories or of having a set of highly correlated categories.

It is not easy to compare our results with others. However, after checking the earlier work discussed, we considered the work of Saad [2011] due to the nature of the results obtained and availability of the used corpora. Saad [2011] is not a single approach to categorization but rather an application of several categorization algorithms with the use of different corpora, so we thought the best way to compare our work with others is simply by comparing our work with this author X  X  work, since he already compared well-known algorithms and highlighted the best of them. Thus, using his corpora with our algorithm is just like comparing our work with the algorithms he tested. Considering the corpora mentioned in Saad [2011] and our predefined corpora, we did some tests to compare his results with ours. That is, we applied his corpora (most of it is available online) to our categorizing algorithm and compared our results with the results he obtained. This will be discussed in Section 6. In this section, we will talk about our training and testing data for the developed categorizing algorithms.
 In this work, we focus on building different categorized datasets of words. The idea is to provide a wide range of categories, forming a hierarchy where some are subcategories of others, and use them in building and testing different categorizing approaches.
We built our corpora using the Arabic Wikipedia by applying our own dynamic cate-gory extraction algorithm, which will be discussed next.

Wikipedia-based categories were built using a partial copy of the Arabic Wikipedia that holds around 96,128 titles with their content. The copy is not directly obtained from Wikipedia dumps. Rather, it was obtained from The Arabic Online Content Indi-cations Project. 1
In Wikipedia, each article is associated with a set of manual tags. The over-lapping tags are not well defined. That is, one can find tags such as Ministers of Palestine, Presidents of the Palestinian Authority). These tags can be merged in one major tag, such as  X  X  X  X   X  X  X  X  X  X  X  X  X  (Palestinian Leaders), or in the more gen-eral tag  X  X  X  X  X   X  X  X  X  X  X  (Palestine News) or the general tag in Wikipedia may be too specific, on one hand, and on another, can be repeated using different words.

To build the Wikipedia-based categories, we need first to define the categories and then add as many articles as possible under each category. To do that, an automated process of connecting related articles based on manual tags was built, followed by a manual verification process.

Using Wikipedia manual tags, we can link articles based on the shared tags between the articles; the more shared tags, the more the articles are related. Also, this means that there is a possible relation between tags if the tags appear jointly in different arti-cles. For example, if text A (in the Wikipedia) is tagged under: and  X  X  X  X  X  X  X  X  (Mechanics) and text B is tagged under:  X  X  X  X  X  X  X  X  (Kinetic Energy), then we can conclude that these three tags are related as they have  X  X  X  X  X  X  X  X  (Mechanics) in common. However, if we go deep in this relation analysis, we may end up connecting all tags in the Wikipedia (which is not desirable). So one should be wise in selecting the limit of relation depth and interfere manually to have control over how deep the tags/articles relation goes. For that we developed the Related Tags Approach .

The approach can be illustrated by the following steps. (a) We start by defining the category we want to build (starting point), say, (b) We parse our list of Arabic Wikipedia articles to extract the articles that contain (c) For each extracted article, the tags found while parsing the article are added to a (d) After repeating step (c) for all articles, we move to the next tag in the queue. In our (e) The process repeats itself here: steps (b) and (c) and each repeated tag will only (f) When the total number of Wikipedia articles processed reaches N, the process (g) The queue now will hold a set of tags and their frequencies., The tags are sorted (h) The selected final set of tags will be used to parse all the articles containing any of Table I shows some categories with their top ten related tags.

Using this technique, we built a Wikipedia-based categorized corpus. Table II gives statistics about the categories we adopted; of course, the data in this corpus is sub-ject to change due to continuous data processing. So far, we defined 25 categories. In Table II, # of distinct words represents the total number of distinct words in each cor-pus. Words are extracted from the text by splitting on  X  X hite space X  characters. Please note that some earlier and related work refer to word(s) as  X  X erm(s) X  [Goweder and De Reock 2001; Sarkar et al. 2004] 3 .

Using the Related Tags Approach , we can create new categories by starting from the desired category. For example, if a user is interested in the category adopting the same steps as for  X   X   X   X   X  X  (Physics) will output a new corpus specialized in  X   X  X  X  X   X  (Birds) from the Wikipedia articles. Before we discuss the categorization process, let us introduce the testing sample that will be used in testing our algorithm. The testing was done on a sample of 400 docu-ments distributed among 10 categories with 40 documents in each category. Table III shows the categories used in the tests and their sources (websites). The documents were precategorized manually by human experts, thus when testing our categorizing algorithms, we compare the output category for each test document with the origi-nal category of the test document set by human experts to calculate the success rates which will be reported for our experiments. Our testing samples were not derived from the same source as the training data. That is, the testing source is not Wikipedia: rather, it was collected from random webpages. We believe that it makes more sense to have training and testing data come from different sources in order for the tests to be more credible and indicative of per-formance in real-life environments. We needed a way to make sure that our testing data sources don X  X  cross directly with Wikipedia articles used in building the training data. To do that, we applied a simple test. We used Google search engine to search the Arabic Wikipedia for pages that include (1) the domains of the test documents sources and (2) the category of each set of test documents. For example, in Table III, we have a category  X   X   X   X   X   X  (Football) with  X  X bc.net X  as one source of testing documents for (Football), we applied the query shown in Figure 1.

This was done for all the websites for each category in the testing documents, and none of the testing documents gave a match with any Wikipedia page included in our training data.
 In this section, we will discuss some filtration tools used in some of our testing. The filtration tools are not a basic part of the categorization process. However, we test their effect on the performance of the categorization process. Root extraction is based on our earlier work [Yahya and Salhi 2011], and the idea is to process the training and testing data to extract word roots and use roots rather than words in the categorizing algorithm.

Our root extraction filter recursively removes prefixes, suffixes, and infixes, then attempts to find a root for the stripped form. Affix removal is based on input word length to judge what, which, and when to remove a certain affix. For more information about the extraction process, see Yahya and Salhi [2011]. Similar to root extraction, however we only filter prefixes and suffixes and keep in-fixes, if present. One of the best light stemmers is called light10 [Larkey et al. 2007]. However, we used our own light stemmer which employs nouns/verbs/adjectives lists obtained from Attia et al. [2011].
 Our approach extracts three lists of reference stems (nouns, verbs, adjectives) from Attia et al. [2011] datasets. Each list is related to a map of prefix/suffix strings. When the stemmer receives an input word, it removes prefix and suffix by applying the three maps (one at a time), then normalizes letters such as {  X   X   X   X  the result will be three suggested stems for each word, then the suggested stems are compared with the lists of nouns, verbs, and adjectives. If a match is found, it will be considered the stem of the word; if more than one stem is found, then the first match will only be considered; if no stem in the reference lists matches the suggested stems, then the largest suggested stem with less prefix/suffix removal is considered to be the stem of the word. If the input word is a stop word, it will be returned as is.
Table IV shows a sample comparison result between our approach and Light10 in terms of the stems returned for an input word. Later (Section 5.7) and based on exper-imental results of categorization, we will prove that the use of our light stemmer gives better results than the use of Light10. The tool starts by extracting lists of single, double, and triple expressions from the input list. Then each expression in each list is checked by a stop words filter to remove any expression with a stop word, then the expressions in each list are checked again to select expressions that start with (1) the definite article Al Ta X  X eef  X   X   X   X  (in all words), and (2) the expressions with nouns or adjectives (using the lists in Attia et al. [2011]). Expressions with verbs are dropped.

The final filtered expressions are ordered by frequency of appearance and considered to be the representation of the input text.

Please note that we are not building or using any NER system here. We are only filtering single, double, and triple expressions based on the simple filters previously mentioned. Our categorization process is based on the idea of categorizing the input text in two phases. In phase one, we categorize the text into one of the main categories, and in phase two, we further categorize the input text based on subcategories. For example, if a text was assigned  X  X  X  X  X  (Sports) in phase one, in phase two, it will be further cate-gorized into one of  X  X  X  X  X  (Sports) categories (football, basketball, tennis, etc.). Table V shows the adopted main categories and their subcategories.

We started our categorizing process by adopting a simple categorization idea BCA and then moved to a more complex one. During that, we applied a testing sample to make sure that the results are improving by the modifications we adopt. We start by introducing our basic categorization approach then introducing the more complex one which we named Percentage and Difference Categorization (PDC) Algorithm. The basic categorization algorithm assigns the input text represented by vector Z to a category by calculating a weight for Z in each reference category X : W category with the highest weight is considered to be the correct category for the input text. The weighting function W X ( Z ) is based on Equation (1), where z m is the number of distinct words in Z found also in X , n is the number of words ( x X ,and  X ( t ) is the frequency of word t . W X ( Z ) is defined by the total number of words found in both the input text Z and category X to the total number of category words normalized by the relative sizes of Z and X .

To understand Eq. (1), take the following example: assume that an input text Z has words { z 1 , z 2 , z 3 , z 4 , z 5 } and { z 1 , z 2 cies f 1 , f 2 , and f 5 , respectively, in reference category X with n words in the form of { x 1 , x 2 , x 3 , ..., x n This means that Eq. (1) will generate Eq. (1 X ).

The category with the highest weight is considered to be the correct category for the input text Z . Of course, the categorization process is done after removing stop words from the input text, using stop words filtration method discussed in Yahya and Salhi [2011]. This algorithm focuses on the relation between ratios in the input text words and the corresponding ratios in the reference texts (our training data) to decide which category to assign each word in the input text. This means it will calculate the percentage of each word (word frequency/total words) in the input and compare it with that word percentage in each category (if it exists), then find the difference between the two values and assign to the word the category with the smallest difference.

The difference will give us an idea of how close a word z in input Z is to the frequency percentage for z in category X . We can say (in general) that this algorithm works as if it were deciding for each word of the input text how closely it is related to each of the given categories.

For example, if word z has frequency 7 in the 300 -word input text Z , then the per-centage of z in the input table is 7/300 = 0.023333. Next, z ratio is calculated in each category in the reference data (if it exists), for example, z has a frequency of 500 in category X 1 with 10,000 words ,then z in X 1 has the ratio 500/10000 the relation between z in X 1 and z in the input text Z will be the absolute value of (0.023333  X  0.05) ,whichis 0.026667 . This is done for all categories ( X X m ) , and the category with the minimum difference is assigned to the word z (not the input text).

The word z flag of category X is set to 1, where category X gives the minimal distance between the frequencies of z in the input text and its counterparts in all categories.
This process is repeated for all words of Z , after removing stop words from input text using stop words filtration method in Yahya and Salhi [2011]. Basically we are categorizing each non-stop word of the text separately. After the processing of all input words, a flag matrix similar to Table VI will be generated. Note that each row has a single 1 in the column representing the word assigned category.

The category with the highest sum of flag values (as seen in Table VI) is considered to be the best match for the input text.
 Before we move on with our categorization process, we need to select which algorithm is better in order to adopt for further processing.
 To do that, we applied both algorithms to the testing samples discussed in Section 3. As mentioned earlier, two phases of categorization were applied for both algorithms: first into the main categories (  X   X   X   X  X  ,  X  X   X   X  ,  X   X  X  X  X  ence, Engineering, Religion, History, Medicine, Economics, Politics), then as the second phase, into the subcategories of the main category selected in phase one.
One may argue that the subcategories might be highly correlated, and some words such as  X   X  X  X  X   X  (Game) in  X   X   X   X  X  (Sports) might not help differentiate between inner sub-categories (such as  X  X  X  ...  X  X  X   X  X  X  X  ,  X  X   X   X   X  X  ,  X  X   X   X   X  X  That X  X  true, but for the current test, we didn X  X  apply any inner word (within the same major category) filtration. However, this issue will be discussed later in Section 5.5.
Table VII shows the comparison between the PDC Algorithm and BCA. As can be seen in Table VII, the PDC algorithm gives better success rates, thus we will adopt this algorithm in further processing. When it comes to categorizing using main/sub categories, one of the main problems is the possible high correlation between subcategories of different main categories. For example,  X  X  X  X  X   X  X  X  X  X  X  X  X  (Electrical Engineering) and  X  X   X  X  X   X  However,  X  X   X  X  X   X  (Physics) comes from main category  X   X  X  X  ing), thus if a  X  X   X  X  X   X  (Physics) document was categorized as  X  X  X  X   X  (Engineering) in phase one, then it will never be categorized as  X  X   X  X  X   X  (Physics) in phase two, since is not a subcategory of  X  X  X  X   X  (Engineering) in the hierarchy. To solve this, we adopted the two approaches discussed next. 5.4.1. Overlapping Main Categories for Phase Two. The idea is to allow main categories to overlap by having shared subcategories (from other categories) that are related to the inner subcategories. This is to preserve the ability to correctly categorize in phase two even when phase one categorization fails due to common features of subcategories from different main categories. For example, if an input  X  X  X  X   X   X  X  X  X  X  X  X  X  (text was categorized as  X   X  X  X  (Science) in phase one (say due to the presence of many physics terms), then in phase two, it will not only be categorized under one of (Science) subcategories but also with the added  X   X  X  X   X  (Engineering) subcategories and  X  X  X  (Health) subcategories. That is, we add subcategories (from other main categories) related to each subcategory in  X   X  X  X  (Science).
 5.4.2. Replacing Main Categories by Groups of Related Categories. We believe that main categories used so far are not adequately related internally. It is not clear that (Biology) is closer to  X  X   X  X  X   X  (Physics) than to  X  X  (Medicine). Since these divisions are transparent to the final categorizing process, one may modify the first phase main categories to assist the process. The idea here is to redefine the main categories and replace them by groups of related subcategories (dummy/working categories), as shown in Table VIII.

In phase one, an input text will be categorized under one of the nine groups shown in Table VIII, then it will be subcategorized within the selected group. We redid the testing on PDC algorithm using the same test sample and using the discussed two approaches. Table IX shows the results.

So what we did here is very simple: we redefined the main categories in a way that the inner subcategories of those new main categories (groups) are highly correlated, then we applied hierarchical categorization by first categorizing an input text into one of the groups, then the subcategories of the selected group (as an output) will be used to categorize the text into a subcategory under that group.

As can be seen in Table IX, the results improved under approach two from 84.72% to 88.61%, so we will adopt approach two and the groups in Table VIII as our new main reference categories. Note that here we are comparing under the second level of categorization (using subcategories results). The output is not comparable regarding main categories, because we redefined the main categories into groups in approach two, but under subcategories, it X  X  still comparable. Next we try to remove or reduce the effect of correlation between subcategories of each group. For example, the word  X   X  X  X  X  (Match) can help categorize a document as  X   X  X  X  X  (Sports); however, it might not help when deciding between subcategories of (Sports), that is, the word is used in most of the subcategories. Thus, it may be treated like a stop, nondiscriminating word in phase two.

We investigated three techniques to filter out such words and to check the filtering effect on the results. Those techniques depend on the definition of inverse document frequency (idf): a measure of whether the word (term) is common or rare across all documents. It is obtained by dividing the total number of documents by the number of documents containing the word, and then taking the logarithm of that quotient
Technique 5.5.1. ( T-2 ) . Remove any word seen in two or more subcategories of the given main category. For example, if word w is seen in subcategories xi and xj in main category X , then remove w from all subcategories of X . It X  X  as if we are removing all words for which Equation (2) applies. n is the number of all subcategories in category X and m is the number of all subcategories that have the word w .
Technique 5.5.2. ( T-All ) . Remove any word that is shared in all subcategories of a main category. For example, if a word w is seen in all subcategories of X , then remove w from all subcategories of X during the test. It X  X  as if we are removing all words for which Equation (3) applies.
Technique 5.5.3. ( T + hi ) . Detect any word that is seen in two or more subcategories in a given category, and then only keep the word in the subcategory in which it has the highest percentage. For example, if a word w is seen in subcategories x main category X with frequencies P i , P j ,and P k , respectively, and max(P then keep w in the subcategory x j and remove it from the rest of the subcategories of X . This is the same as with Technique 5.5.1, but we keep the word in the subcategory with the highest percentage.
 Table X shows the result of applying the three techniques with the testing sample.
It is seen that adopting Technique 4.5.3 outperforms others, so using PDC algorithm with redefined categories (as groups) and the third filtration technique gave the best results so far. To investigate our categorization algorithm more, we edited the measurements in the PDC algorithm to allow for multi-valued instead of binary scaling. In Section 5.2, we mentioned that when categorizing a word z , it will be assigned to the category with minimum difference, and for each category, there is a flag that is set to 1 if the cate-gory holds the minimum difference for z and is set to zero for all other categories. We investigated the behavior of the algorithm when the assignment can have more values, such as three values [1 or 0.5 or 0], or five values [0, 0.25, 0.5, 0.75, and 1]. In order to define a scaling of [1 or 0.5 or 0], we need to introduce the following rules.
Rule 5.6.1 ( Minimum Value ) . This will be the minimum value found after calculating all differences between each of the categories and the given word.

Rule 5.6.2 ( Maximum Value ) . This will be the maximum value found after calculat-ing all differences between each of the categories and the given word.

Rule 5.6.3 ( Middle Value ) . This will be the result of (Minimum Value Value) / 2 (the midpoint of the range).
 Rule 5.6.4 ( Break1 Value ) . [Minimum Value + Middle Value] / 2.
 Rule 5.6.5 ( Break2 Value ) . [Maximum Value + Middle Value] / 2.
 And for [0, 0.25, 0.5, 0.75, and 1], scaling, we need also the following. Rule 5.6.6 ( Value A ) . [Break1 Value + Minimum Value] / 2.
 Rule 5.6.7 ( Value B ) . [Break1 Value + Middle Value] / 2.
 Rule 5.6.8 ( Value C ) . [Middle Value + Break2 Value] / 2.
 Rule 5.6.9 ( Value D ) . [Maximum Value + Break2 Value] / 2.
 Here is the line of values.

Min Brk1 Mid Brk2 Max | -------o-------x-------o-------| -------o-------x-------o-------Min A B Mid C D Max Table XI shows which values will convert to what.

The operation from here on is the same as the original algorithm: after assigning a category with a value for each word in the input depending on the interval, the category with highest sum is considered to be the category of the input text. Table XII shows a comparison between PDC algorithm with and without scales.
As can be noticed from Table XII, the binary scale gave the best results, then the 3-valued scale, followed by the 5-valued scale, so we can predict that if we continue dividing the scale to more points, the results will not actually improve, thus having a continuous scale will not improve the results, so we will keep the algorithm as is. (with the binary scale).

Other experiments done on the PDC algorithm consisted of applying (on both the reference/training and testing sets) the preprocessing tools discussed in Section 4. In order to study the effect of other preprocessing tools (both on the input and reference data), we applied the following four tools.

Tool 5.7.1. ( Root Extraction ) . Extracting the roots of both the input text and the reference categorized data before applying the categorizing algorithm.

Tool 5.7.2. ( Light Stemming &amp; Light10 ) . Light stemming both the input text and reference categorized data before applying the categorizing algorithm.

Tool 5.7.3. ( Double Words ) . Processing both the input and reference text as expres-sions of double (not as single) words, before applying the categorizing algorithm.
Tool 5.7.4. ( Expressions Extraction ) . Filter expressions from both input and refer-ence data and use them with the categorizing algorithm.
 Table XIII shows the results after applying these tools.

As seen in Table XIII, the algorithm with no additional tools gives the best results, thus we will keep the PDC without these tools.

Also, it can be noticed that using our light stemmer with PDC gave better results than using Light10, and since the categorization algorithm is not changed while test-ing, we can conclude that the impact of using our stemmer on categorization is better than the impact of using Light10. It seems to be the case that using a reference set of stemmed words in a stemmer, as we do, is better than not using one, as is the case for Light10, of course at the expense of the added cost of the lookup step for checking the generated stem in the reference list.
 Our testing was based on external testing datasets, as we explained in Section 3. How-ever we did investigate the results when the training data and testing data came from the same source (Arabic Wikipedia). This was done by splitting our corpus data to 66% training and 34% testing; the selection of the testing data and training data from the same corpus was done three times by selecting the first 34%, the middle 34%, and the last 34% as test (also by using 100% of the corpus for training and the overlapping last 34% as testing data (last 34% gave best results)). Table XIV shows the results. Note that when the training and testing data come from the same source (Arabic Wikipedia), the results are better (96.3% vs. 89.4%). Also, one can note that the location (first, mid, or last third) of selected testing data in the corpus can result in slightly different results. As mentioned in Section 2, we want to compare our results with the work of Saad [2011]. His work does not present a single approach but rather an application of sev-eral categorization algorithms; the author applied different algorithms on Arabic by applying also different Arabic corpora; so the best way to compare our work with oth-ers is simply comparing our work with this author X  X  work, since he already compared well-known algorithms and highlighted the best of them, thus using his corpora on our algorithm is just like comparing our work with all the algorithms he tested. We will not need to reimplement the algorithms and methods used there, but rather, we need only to use the available data resources he used and apply them to our algorithm, that is, we use his training data instead of the Arabic Wikipedia.

The author applied different classification algorithms, such as C4.5 decision tree (TD), k -nearest neighbors (KNN), support vector machines (SVMs), na  X   X ve Bayes (NB), na  X   X ve Bayes variants (na  X   X ve Bayes multinomial -NBM), complement na  X   X ve Bayes (CNB), and discriminative multinomial na  X   X ve Bayes (DMNB) in his testing, and con-cluded that light stemming with SVMs outperforms other algorithms.

The author tested seven corpora differing in size and the number of categories in each, and most of them are available for free online as raw data. Thus they need pro-cessing to be ready for use with our algorithm by removing stop words, punctuation marks, non-Arabic characters, extraction into database, and calculating frequencies of appearance in order to fit the needs of our algorithm. Table XV shows the characteris-tics of the training data (66% of the total documents) of the corpora we used from this author (after processing them to fit our algorithm).

We used the corpora (one at a time) with our algorithm, by using each of the corpora (66% of total documents) as reference data (categories) and the rest of the documents (34%) of each of the corpora as the testing data. Table XVI compares our results with the best results of Saad [2011].
 From Table XVI, it can be noted that, on average, our algorithm gives better results. One important note here is that we don X  X  have information about how the 66% and 34% splitting (in the compared work) is done; it X  X  likely that our 66% training data might not hold the same content as the 66% training data of the compared work, and the same can be said about the testing data. So there is a possibility that if we selected a different 66% of the content (i.e., selecting different documents), we may end up with different results (we proved that in Table XIV on own our data) which may explain why we had a lower (but close) pass percentage (96.6% vs. 99.3%) in the test of OSAC corpus. In this article, we presented our experiments on categorizing Arabic text with a focus on a hierarchy of main categories and subcategories. For that, we employed categorized corpora obtained from the Arabic Wikipedia which we built using a combination of automated and manual processes.

We introduced a categorization algorithm that started with a simple weighting idea and progressed to a more complex one that considers the relation of weights in the input text and the training data.

We designed and implemented different text preprocessing tools, such as root extrac-tor, light stemmer, and expression extractor, and tested their effect on the performance of our categorization algorithm. We noted that light stemming with a reference list of pre-stemmed words is a better approach for light stemming, even though light stem-ming (in general) didn X  X  give better results when incorporated into our categorization algorithm.

Another important conclusion is that there are two methods for testing categoriza-tion algorithms: the first is to use training and testing data from same source by splitting the corpus into test and training components. This consistently gives better results than the second method in which training and testing data come from differ-ent sources. Most of the early work use the first method. However, we believe that the second method makes more sense, as the tests will be more credible and indicative of performance in real-life environments.

Regarding future work on categorization, we are interested in investigating the manual tags found in articles in the Arabic Wikipedia (tags that are added by edi-tors at the end of each article). We plan to group related tags into more general tags to end up with well defined major tags, and those tags will be used with the article titles in the process of further categorization.

