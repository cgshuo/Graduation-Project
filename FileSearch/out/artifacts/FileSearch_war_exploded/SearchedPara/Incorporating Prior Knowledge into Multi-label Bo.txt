 With the growing amount of digital information such as image and video archives, multimedia information retrieval has become increasingly important in the last few years and draws much more attention from computer vision and object recognition community. Through the sustained efforts of many researchers in image retrieval domain, two successful retrieval architectures have been proposed: one is query-by-example (QBE), a mono-media retrieval paradigm based on visual similarity and the other one is query-by-keyword (QBK), a cross-media retrieval paradigm based on associated text. More specifically, query-by-keyword is a more intuitive and desirable choice for common users to conduct image search, since both users X  information needs and natural image semantics can be described more accurately by keywords or captions. However, for other applications , especially face recognition and texture mation in natural languages in most cases. However, the key issue for QBK is the semantic gap, in other words, how to deduce the high-level semantic concepts from low-level perceptual features. To bridge this gap, two effective methods base on ma-chine learning techniques have been explored: automatic image annotation and query concept learning. In this paper, we focus on the branch of automatic image annota-tion. Automatic image annotation is the task of automatically generating multiple semantic labels to describe image semantics based on the image appearance, which is a crucial step for object recognition and semantic scene interpretation. In recent years, many generative models and discriminative approaches have been proposed to auto-matically annotate images with descriptive textual words to support multi-modal image retrieval using different keywords at different semantic levels. Many of them have achieved state-of-the-art performance. However, most of the approaches may instead of image regions, large amount of han d-segmented, hand-labeled images with fundamental problem for image annotation is the ambiguity or incompatibility of label assignment, contextual constraints are little examined in the annotation process, that is to say, each image segment or block is identified independently without considering the word-to-word correlations, which may degrade annotation accuracy due to the ambiguities inherent to visual properties. For example, like  X  sky  X  and  X  ocean  X  region, without context. In literature, relevance models, coherent language models and con-text-dependent classifica tion models have been explored to solve these above prob-lems. In this paper, we propose a simple yet effective annotation model in which multi-label boosting algorithm and contextual semantic constraints are integrated seamlessly. 
The main contribution of this paper is two-fold: First, we formulate the task of im-age annotation as a multi-label, multi class semantic image classification problem under ensemble classification framework. Seco nd, as a prior knowledge, contextual constraints between the semantic labels are taken into consideration to avoid label ambiguity or label incompatibility problem. To our best knowledge, multi-label boosting combined with contextual semantic constraints has not been carefully inves-tigated in the domain of automatic image annotation. 
This paper is organized as follows: Section 2 discusses related work. Section 3 first reviews the underlying theory of boosting, and then describes image annotation model based on the multi-label boosting and contextual constraints. Section 4 demonstrates our experimental results and some theoretical analysis. Conclusions and future work are discussed in Section 5. Recently, many models using machine learning techniques have been proposed for automatic and semi-automatic image annotation and retrieval, Including hierarchical aspect model [1][2], translation model [3][19], relevance model [4][9][17][22], classi-fication methods [8][10][12-15][30], latent space approaches [20][21] and random fields models [23][24]. Mo ri et al [5] is the earliest to performance image annotation, they collects statistical co-occurrence information between keywords and image grids and uses it to predict annotated keywords to unseen images. Dyugulu et al [3] views annotating images as similar to a process of translation from  X  X isual information X  to  X  X extual information X  by the estimation of the alignment probability between visual blob-tokens and textual keywords based on IBM model 2. K. Fang [19] improved IBM model 1 by regularizing the imbalance of keywords distribution in the training set. Barnard et al [1][2] proposed a hierarchical asp ect model to capture the joint dis-cross-media relevance model (CMRM) sim ilar to the cross-lingual retrieval techniques to perform the image annotation and ranked image retrieval. Lavrenko et al [8] proposed continuous relevance model (CRM) to extend the cross-media rele-vance model using actual continuous-valued features extracted from image regions. This method avoids the clustering and constructing the discrete visual vocabulary stage. S. L. Feng et al [17] improved the CRM model by assuming a multiple Ber-noulli distribution to generate the keyword annotations instead of multinomial distri-bution to model the Blei et al [11] proposed a correspondence LDA and assumes that further relates words and image regions. Wang and Li [8] introduced a 2-D multi-resolution HMM model to automate linguistic indexing of images. Clusters of fixed-size blocks at multiple resolution and the relationships between these clusters is sum-marized both across and within the resolutions. E. Chang et al [6] proposed content-based soft annotation (CBSA) for providing images with semantic labels using (BPM) Bayesian Point Machine. Cusano C et al [10] proposed using Multi-class SVM to and then combine the partial decision of each classifier to produce the overall descrip-tion for the unseen image. E. Chang, B. Li and K. Goh [12-15] introduced a multi-level confidence-based ensemble scheme to assist the discovery of new semantics and useful low-level perceptual features. F. Monay [20][21] presented to use the latent variables to link image features and words based on the latent semantic analysis (LSA) and probabilistic latent semantic analysis (pLSA). Instead of predicting the coherent language model for each image to infer a set of words with the word-to-word correlation to be considered . J. Fan et al [16] presented a concept hierarchy model and adaptive EM algorithm to deduce multi-level image semantics. More recently, R. Zhang et al [22] introduced a latent variable model to connect image features and textural words, X. He et al [23] and Kumar. S et al [24] have used context-dependent classifiers such as random fields to perform image annotation. 3.1 Formulation of Automatic Image Annotation Given a training set of annotated images, where each image is associated with a num-ber of semantic labels. We make an assumption that each image can be considered as a multi-modal document containing both th e visual component and semantic compo-nent. Visual component provides the im age representation in visual feature space component captures the image semantics in semantic feature space based on textual annotations derived from a generic vocabulary, such as  X  sky  X ,  X  ocean  X , etc. Auto-matic image annotation is the task of discovering the association model between vis-ual and semantic component from such a labeled image database and then applying the association model to generate annotations for unlabeled images. More formally, let ID denote the training set of annotated images:  X  each image  X  semantic component  X  visual component 3.2 Underlying Theory of Boosting Boosting is a general framework for improving the label prediction accuracy of any given learning algorithm under the PAC (probably approximately correct) style model. The basic idea behind boosting is to combine many inaccurate or moderately iterative scheme. More specifically, a distribution or set of weights is associated with each training instance. These weighted training instances are used to train the simple increased so that the simple rule is forced to concentrate on hard examples which are most difficult to classify by the preceding rules [26]. 3.3 Multi-label Boosting with Semantic Knowledge for Annotation Model In traditional classification problems, to reduce the model complexity, class labels are one class. However, in the context of image annotation or semantic image classifica-tion, it is natural that one image belongs to multiple classes simultaneously due to the richness of image content, causing the actual classes overlap in the feature space. For example, as shown in Fig. 4.1, it is quite hard and insufficient to describe the image content using only a keyword because image semantics is represented by both basic semantic entities contained in that image and the relationships between them. In lit-erature, only a few papers are concer ned with multi-label classification problems, R. Schapire et al [25] extended AdaBoost to address multi-label text categorization which motivated us to attack the problem of multi-label image annotation. More for-classified and L be the finite set of semantic labels, each image D I with a binary label vector ages into several single-label image pairs serves as an observation for each of the cla sses to which it belongs, finally these con-verted single-label instances are taken as input to train a multi-label annotation model based on the boosting framework. The detailed boosting algorithm for multi-label, multi class semantic image classification is shown as follows. Multi-label Boosting for Image Annotation Algorithm Output: () l I f , final accurate annotation model 
Algorithm: where t  X  is a normalization factor to ensure that Distribution images together with the tor  X   X  L ID l I h next weak annotator will pay more attention to these example pairs with higher weights accurate annotation model is constructed by combining all the weak models through weighted voting. To annotate an unlabeled image, the final annotation model can used as annotations, we map the associated weights in the weight vector via the follow-ing logistic function to get confidence scores or annotation probabilities for each label. where f is the output of the final annotation model for a given unseen image, A and B are real-valued parameters estimated by Maximum Likelihood criterion. To solve the ambiguity or incompatibility of label assignment, label co-occurrence statistics and pair-wise semantic similarity defined by WordNet [27] [28] are incorporated respec-tively. Finally the confidence scores associated with each label can be re-weighted and sorted again by using the label-to-label correlations. Our experiments are carried out using a mid-sized image collection, comprising about 200 images with the size of 180*116 pixels from Corel Stock Photo CDs. In this col-lection, each image is manually labeled with 2-5 keywords to describe the image content. In our experiments, 130 images are randomly selected for training and the remaining 70 images for test. 
Fig. 4.1 shows the image example used in our experiments, left-most one is the sults by Ncuts [7] and the right-most is blob-token representation. In general, the per-formance of such patch-based image annotation may be affected by two factors: one is semantic objects don X  X  correspond to segmented regions in most cases. Second, blob-tokens are generated by clustering algorithm usually based on visual features. How-ever, in some cases, two image patches with different semantics can share the same visual appearances which may lead to poor clustering results. For example, in the above images, part of  X  sky  X  and  X  water  X  patch is represented by the same blob-token. visual features including color and texture is considered for the weakly labeled images (no correspondence between labels and image patches is provided) used in this paper. 
Fig.4.2 demonstrates comparison of training error between Multi-label boosting and SVM-boosting which illustrates that SVMs [29] is not suitable for multi-label and boosting scheme fail to improve the performance of SVMs. In our experiments, round of iterations, a threshold is generated from the corresponding feature value, decision is determined based on whether the observation value is above or below the given threshold and 
Fig. 4.3 shows the performance of our method compared to Cross-Media Rele-vance Model (CMRM). In this paper, we also use the precision and recall to evaluate the performance of the proposed method, for a single query word w , precision and recall are defined as follows, and we use t and image. the finite set of semantic labels. By mapping the associated weights with each label via logistic function, confidence scores for each predicted label can be produced. Then we can re-weight the confidence scores by considering the label co-occurrence frequency and pair-wise semantic similarity using the following re-weighting formula. malization factor, ) , ( j i p and j -th label, ) / ( WordNet. When considering label co-occurrence statistics, we use ) , ( j i p
However, experimental results show that although label ambiguity and incompati-bility can be solved in some way, nevertheless the precision and recall has not im-proved. Furthermore, empirical studies show that co-occurrence statistics can give a better result than pair-wise similarity concerning label incompatibility. For example, by using label co-occurrence statistics, all the co-occurrence frequency between  X  ele-these two labels will not co-occur in image annotations, which is more reasonable in command sense and more compatible with the characteristics of Corel image collec- X  animal  X  category, they may be relevant to some degree in terms of the definition in WordNet, so pair-wise similarity may be not so much effective in solving label ambi-guity and label incompatibility in our image collection. After re-weighting process, images. 
To further verify the effectiveness of multi-label boosting algorithm, we carried out experiments on a large collection of 4999 images, in which the semantic vo-cabulary contains 374 keywords (mainly English nouns), 4500 images are used for training, and the remaining 499 images for test. Fig. 4.5 shows the frequency distri-bution of keywords in the semantic vocabulary. The precision and recall curves using the top 20 keywords with highest frequency as single word queries are shown in Fig. 4.6. 
An inverted-file can be constructed for each semantic label in the pre-defined vo-single word query. In this paper, we propose a general framework for automatic image annotation and retrieval based on multi-label boosting and semantic prior knowledge. Experimental yet effective method for multi-label, multi class classification problems, especially forimage annotation and semantic scene interpretation. By mapping the output of multi-label boosting via logistic function, we can obtain confidence scores for each label, then we can re-weight the confid ence scores by incorporating the prior knowl-edge such as co-occurrence statistics and pair-wise semantic similarity to avoid the label ambiguity, label incompatibility problem to some degree and get a relatively reasonable annotation results. Ne vertheless, empirical studies show that this post-processing step based on language models has not too much effect on the annotation accuracy in some cases. 
In the future, more work should be done to provide more expressive image content representation, efficient algorithms and precise models for semantic image classifica-tion. In addition, more sophisticated language models will be taken into consideration as a post-processing step to achieve a more scalable and accurate image annotation model. We would like to express our deepest gratitude to Michael Ortega-Binderberger, Kobus Barnard and J.Wang for making their image datasets available. This work would not have been possible without the help of R. Schapire. The research is sup-ported by the National Natural Science Foundation of China under grant number 60573187 and 60321002, and the Tsinghua-ALVIS Project co-sponsored by the Na-tional Natural Science Foundation of China under grant number 60520130299 and EU FP6. 
