 Many real world objects have states that change over time. By tracking the state sequences of these objects, we can study their behavior and take preventive measures before they reach some un-desirable states. In this paper, we propose a new kind of pattern called progressive confident rules to describe sequences of states with an increasing confidence that lead to a particular end state. We give a formal definition of progressive confident rules and their concise set. We devise pruning strategies to reduce the enormous search space. Experiment result shows that the proposed algorithm is efficient and scalable. We also demonstrate the application of progressive confident rules in classification.
 H.2.8 [ Information Systems ]: DATABASE MANAGEMENT X  Database Applications, Data Mining Algorithms, Experimentation, Performance Progressive confident, sequence, classification
Real life objects can be described by their attribute values. Con-sider a person with attributes gender and job title. While the gender of a person does not change, the job title may change with time. If we denote the set of attribute values of an object as its state, then the state of an object changes as the attribute values change with time. The states of an object at different time stamps form a state sequence. In many applications, an object X  X  state sequence over time is usually more interesting than its current state because the state sequence depicts the object X  X  behavior characteristics. The state sequence of an object also captures more information than the current state for classification. For example, it is hard to de-termine whether a patient has chronic lymphocytic leukemia if we only knows that he currently has anemia. However, if we track the Copyright 2006 ACM 1-59593-339-5/06/0008 ... $ 5.00. patient X  X  medical history over a period of time, then the doctor will be able to make a better judgement.

Table 1 shows a sample database that records the symptom se-quences of patients and whether they have Chronic Lymphocytic Leukemia (CLL). For example, patient ID 1 first has night sweat and hypodynamia, followed by fever, then achroacytosis, and fi-nally anemia. We regard the set of symptoms at a time point as the state of a patient at that time. Thus, the first state of patient 1 is { night sweat, hypodynamia } , and his last state is anemia. The last column in Table 1 is the doctor X  X  diagnosis. The first four patients have CLL, while the last four do not.
 Table 1: Example Diagnosis on Chro nic Lymphocytic Leukemia
If we examine the last state of each patient, or their last symp-tom, 3 people with anemia have CLL (IDs 1 X 3), while the other 3 with anemia do not (IDs 6 X 8). Thus it is not clear whether a patient with anemia will have CLL. However, if we study the pa-tients X  state sequences, or their sequence of symptoms, we see that most CLL patients (IDs 1 X 3) have night sweat  X  fever  X  achroacytosis  X  anemia . This symptom sequence does not occur in non-CLL patients.
 Suppose a new patient comes in with symptom night sweat . The doctor knows the current probability of the patient catching CLL is not high ( conf ( night sweat, CLL )=0 . 5 ). If this is fol-lowed by fever and subsequently achroacytosis, the doctor could advise on preventive measures because the chance of having CLL has increased ( conf ( night sweat  X  fever  X  achroacytosis, CLL )=0 . 8 ). However, if the patient becomes sleepy, then we have conf ( night sweat  X  sleepy, CLL )=0 and we can con-clude that the chance of having CLL is small.

In this paper, we propose a new kind of pattern called progressive confident rules that describes the changing states of objects with in-creasing probabilities that lead to some end state. The general form of a progressive confident rule is X 1  X  X 2  X  ...  X  X n , where X i is a state, and a state in the left hand side of  X   X   X  occurs earlier in time than a state in the right hand side of  X   X   X . By progressive confident, we mean the probability of an object achieving state C in the future increases as state X i (1  X  i  X  n ) appears one after another. We call this the progressive c onfident c ondition .
For meaningful rules, we have three additional criteria. First, there is a minimum support of the rule to ensure that the rule is gen-eral. Second, the probability of the first state X 1 leading to the end state C should be no less than some threshold min conf 1 .This removes rules that begin with unrelated states. Third, the probabil-ity of the entire rule leading to C should be no less than a threshold min conf 2 to ensure that the rule is interesting.

We describe a depth-first mining algorithm to discover progres-sive confident rules and devise new pruning strategies for the large search space. The algorithm utilizes the concise set analysis in the mining process to further reduce the search space. Experimental results indicate the efficiency and scalability of the algorithm.
Progressive confident rules can also be utilized to predict an ob-ject X  X  future state. This is a classification problem in essence. We incorporate the rules into three representative classifiers, C4.5, Sup-port Vector Machine (SVM) and Bayes Classifier and in so doing, we show that their classification accuracies are greatly improved.
Let I = { i 1 ,i 2 ,...,i m } be a set of literals called items. A state or an itemset X  X  I is a set of items. The semantics of item and itemset is as follows. Every item is related with a characteristic. If a state (itemset) contains an item, it means the item X  X  corresponding characteristic appears in the state. For example, if item a represents the symptom  X  X ick X , item b represents the symptom  X  X ever X , then state { a, b } means patient is sick and has fever.

A pattern P = X 1  X  X 2  X  ...  X  X n is an ordered set of states. The length of P is defined as the number of states in P .We use | P | to represent the length of P . For example, if P { b, c } X  X  d } ,then | P | =3 .

An observation o is an itemset T with a time stamp ( tid ), i.e., o = tid, T .Ifastate X  X  T ,wesay X is contained in the observation o . Sometimes we omit the time stamp tid and represent an observation by its itemset T only.

A sequence s is composed of a sequence id ( sid ) and an or-dered list of observations, i.e. s = sid, o 1 ,o 2 ,...,o a sequence s = sid, o 1 ,o 2 ,...,o n and a pattern P = X X 2  X  ...  X  X m , we say that s matches P , or equivalently, P is contained in s , if there exist integers j 1 ,j 2 ,...,j 1  X  j 1 &lt;j 2 &lt; ... &lt; j m  X  n and X 1 is contained in o is contained in o j 2 , ... , X m is contained in o j m . We represent this relationship by P s .

For example, pattern P = { a } X  X  b, c } X  X  d } is contained in sequence s 1 = sid 1 , { e } , { a, d } , { g } , { b, c, f { a } X  X  a, d } , { b, c } X  X  b, c, f } ,and { d } X  X  d } . Hence, P s . On the other hand, P s 2 = sid 2 , { a, d } , { d } , { because { b, c } occurs before { d } in P , which is not the case in s
A database D is composed of a number of sequences whose end states are known. We use D C to represent the set of sequences in D that end with state C ,and D C to represent the set of sequences in D that does not end with state C . Thus, D = D C  X  D C sequences in D C do not necessarily have the same end states, that is, there can be more than two end states in the database. We say that sequences with end state C are in class C .

The support of a pattern P in class C (represented by sup is defined as the number of sequences in D C that match P .If sup ( P, C ) is no less than a user specified support threshold  X  say P is a frequent pattern in class C .
 We use sup ( P, C ) to represent the number of sequences in D that match P . The confidence of a pattern P resulting in class C is
Given a pattern P = X 1  X  X 2  X  ...  X  X n ,if conf ( X 1 ,C )  X  conf ( X 1  X  X 2 ,C )  X  ...  X  conf ( P, C we say P satisfies the progressive confident condition.
Finally, if a pattern P satisfies the following four conditions where min conf 1 , min conf 2 and  X  s are three user specified pa-rameters, we say P together with its confidence values ( conf ( X 1 ) ,conf ( X 1  X  ...  X  X i ,C ) ,where 1 &lt;i a progressive confident rule.

We observe that the set of all progressive confident rules contains redundant information.
 Example 1. Suppose P = { a } X  X  b, c } and Q = { a } X  { b, c } X  X  d } are two progressive confident rules. If conf 1 , by definition of confidence, conf ( Q, C ) must also be 1, and the information captured in Q is redundant.
 Example 2. Suppose P = { a } X  X  b, c } and Q = { a } X  { b, c } X  X  d } are two progressive confident rules. If conf 1 , then all information in P is already contained in Q .
Given a pattern Q ,weuse Q [ i ] to represent the i -th state (item-set) in Q ,and Q [ i, j ] ( j&gt;i ) to represent the pattern Q 1]  X  ...  X  Q [ j ] . For example, if Q = { a } X  X  b, c } X  X  then Q [1] = { a } , Q [2] = { b, c } , Q [3] = { d } ,and Q { a } X  X  b, c } .

A base-pattern of a pattern Q is obtained by removing the last few states (itemsets) from Q . For example, if Q = { a } X  X  { d } ,then { a } X  X  b, c } is Q  X  X  base-pattern, and { a } X  X  b If P is a base-pattern of Q ,wealsosay Q is an extender of P .
Given a pattern Q ,if conf ( Q, C )=1 and none of its base-pattern P satisfies the condition conf ( P, C )=1 ,wesay Q is a terminator pattern .

Given a set of patterns V and a pattern P  X  V ,if P is not a base-pattern of any other pattern P in V ,wesay P is a maximal pattern in V .

In Example 1, all the extenders of a terminator pattern can be ignored without loss of information. Further, if a pattern is neither a terminator pattern nor a maximal pattern (such as P in Example 2), then that pattern can be excluded from the concise set.
Based on the above discussion, we can obtain a concise set of rules from a set of progressive confident rules R in two steps: (1) delete all extenders of terminator patterns in R ; (2) remove non-maximal patterns.

We define the problem of mining progressive confident rules as finding the concise set of all progressive confident rules in a given database D and a class C .
One common issue in data mining problems is that the search space is huge. Hence, many mining algorithms make use of the Apriori property to reduce the search space. The Apriori property states that if a pattern P satisfies the mining requirement, then all its subpatterns also meet the requirement. For example, the Apriori algorithm for finding large itemsets [1] and the GSP algorithm for mining sequential patterns [6] are based on this property. Unfortu-nately, the Apriori property does not hold in the progressive confi-dent condition (i.e. conf ( P [1] ,C )  X  conf ( P [1 , 2] conf ( P, C ) ).
Consider the database in Table 2. We set the user input parame-ters  X  s =2 , min conf 1=0 . 5 and min conf 2=0 . 9 . Pattern P = { a } X  X  b, c } X  X  d } is a progressive confident rule because
However, P  X  X  subpattern Q = { a } X  X  b } does not satisfy the progressive confident condition, because conf ( { a } X  X  b 2+3 =0 . 4 &lt;conf ( { a } ,C )=0 . 5 . Hence, the apriori property does not hold.

Another property that can be used to prune the search space is the prefix anti-monotonic property [5]. This property states that if a pattern P 1 meets the mining requirement and another pattern P is obtained by removing some ending items from P 1 ,then P satisfies the mining requirement. In our example, Q is obtained by removing ending items c and d from P . Although P is a progres-sive confident rule, Q does not satisfy the progressive confident condition. Hence, the prefix anti-monotonic property is also not applicable here.

We put forward two theorems to help reduce the huge search space. The proofs of these theorems can be found in [8].
T HEOREM 1. Suppose a pattern P satisfies the conditions: If Q is a base-pattern of P ,then Q also satisfies the conditions:
Theorem 1 states that if a pattern Q does not satisfy either one of the support requirement, min conf 1 requirement or progressive confident condition, nor is any of Q  X  X  extenders. Thus, if we find such a pattern Q in the mining process, we do not need to consider Q  X  X  extenders as potential progressive confident rules. This will prune off a huge amount of search space. We denote the property described in Theorem 1 as the base anti-monotonic property .
The work in [5] developed a framework for mining sequential patterns with constraints that satisfy the prefix anti-monotonic prop-erty. Since our mining problem does not follow the prefix anti-monotonic property, we can use Theorem 1 to modify the frame-work in [5] by projecting on itemsets, instead of items. But this will be inefficient.

If a pattern P satisfies the progressive confident condition and the support requirement, we need to consider P  X  X  extender P X . At this point, conf ( P, C ) is already known. To check whether conf ( P  X  X, C )  X  conf ( P, C ) , we need conf ( P  X  X, C From the definition of conf ( P  X  X, C ) , we need to calculate both sup ( P  X  X, C ) and sup ( P  X  X, C ) . The following theo-rem helps us to avoid computing sup ( P  X  X, C ) when P  X  does not satisfies the progressive confident condition
T HEOREM 2. Given a pattern P and a state X ,if conf ( P, C ) &lt; 1 and conf ( P  X  X, C )  X  conf ( P, C ) Theorem 2 provides a test on whether to extend a pattern P to P  X  X .If sup ( P, C ) or sup ( X, C ) is not large enough (com-X, C ) &lt;conf ( P, C ) . This contradicts the progressive confident condition, and we do not need to extend P to P  X  X ,whichsaves the time for counting sup ( P  X  X, C ) .
Based on Theorem 1 and Theorem 2, we design an efficient depth-first algorithm called FMP (Fast Mining of Progressive con-fident rules) to mine the concise set of progressive confident rules.
Algorithm FMP (see Figure 1) first finds all frequent itemsets in class C . Then for each frequent itemset that satisfies min conf requirement, FMP processes its extenders in a depth-first order to search for other potential progressive confident rules. The core of FMP lies in the extend function (lines 7 X 28 of Figure 1). Given a pattern P , the function finds out all extenders of P that are in the concise set of progressive confident rules.
 The extend function checks if conf ( P, C ) is equal to 1. If so, P is a terminator pattern and we can ignore all P  X  X  extenders. P will be output. If conf ( P, C ) =1 , we check whether we need to append a state X to the end of P (lines 13 X 18). If X passes the test, we check if P  X  X meets the support requirement and progressive confide nt condition (lines 19 X 24). If yes, we re-call the extend function with a new value P  X  X . Finally, if P cannot be extended anymore (i.e. P is a maximal pattern) and conf ( P, C )  X  min conf 2 , we output P and the function ter-minates. The support of P  X  X is determined from the ID lists of P and X , not by scanning the database. The ID list of a pattern records the sequences and the time stamps where a pattern occurs.
The depth-first order and the careful control of the extend func-tion guarantees that the rules obtained belongs to the concise set. The FMP algorithm is efficient for two reasons. First, it utilizes the two theorems to effectively reduce the search space. Second, it in-corporates the concise set analysis into the mining process to avoid checking the extenders of terminator patterns. We compare Algorithm FMP with a naive method DMP (Direct Mining of Progressive confident rules). Algorithm DMP utilizes SPADE [7] to find all frequent patterns. Then it removes the fre-quent patterns that do not satisfy the other requirements of progres-sive confident rules. Finally, DMP computes the concise set of rules. Both FMP and DMP utilize ID lists to count the support of a pattern. We implemented both algorithms in C++ and carried out the exper-iments on a 750MHz Ultra Sparc III CPU with 1GB RAM, running SunOS 5.8. Table 3: Parameters and default values of data generator.
The test dataset is generated as follows. We first create a table of potentially frequent itemsets. The size of each itemset is computed using a Poisson distributed random number with mean= I . Except for the first itemset, every other itemset shares some common items with its immediate preceding one. The number of items shared is determined by an exponentially distributed random number. We will generate N I itemsets from N different items. Each generated itemset is assigned a confidence value c which determines the prob-ability an itemset will appear in class C . c is given by: where r is a normal-distributed random number with mean=0.5.
Next, we generate two pattern tables for datasets D C (corre-sponds to class C )and D C (corresponds to classes other than C ) respectively. We call the tables T C and T C . The length of a pat-tern p is determined by a Poisson distributed random number with mean= S . The content of p is a series of itemsets selected randomly from the itemset table. If the confidence of a selected itemset is c , we append it to a pattern belonging to T C with a probability of c , and append it to a pattern in T C with the probability of When a pattern reaches its length, we insert it into the correspond-ing pattern table. We will generate N s patterns for both T T . We assign a weight and a corruption level to each generated pattern p . The weight indicates the probability that p is chosen to generate a sequence, and the corruption level controls how many items is dropped from p before it is inserted into a sequence.
Finally, we generate sequences for our test dataset. Sequences in D C is computed from pattern table T C , and sequences in D is generated from T C . A sequence length is determined by a Pois-son distributed variable whose mean is O . The average number of items in a sequence X  X  itemsets is determined by a Poisson distrib-uted variable with mean= T . We randomly choose a pattern from the corresponding pattern table to fill it. If the pattern is too large for the sequence, we discard it in half the cases, and push it into the sequence in the other half cases. We will generate | D C | sequences from Tables T C and T C respectively.

Table 3 summarizes the parameters of the data generator and their default values.
We first test the performance of the two algorithms under differ-ent support thresholds. Figure 2 shows that FMP is much faster than DMP (the runtime is given in log scale). When the support thresh-old is low (  X  s  X  0 . 0019 ), FMP is one order faster than DMP .Thisis because a low  X  s results in many frequent patterns and DMP has to mine all of them; while FMP only needs to deal with patterns that satisfy both the support and confidence requirements. When the support threshold increases, DMP requires less effort to mine the fewer frequent patterns, thus narrowing the gap between the two algorithms. However, FMP is still about 2 times faster than DMP .
Next, we examine the influence of the parameters min conf and min conf 2 on the two algorithms. Figure 3 shows the runtime for DMP and FMP in log scale when min conf 1 changes from 0.4 to 0.65. When min conf 1 increases, the running times of both algorithms decrease. For DMP , a higher min conf 1 means more patterns could be pruned before the concise set computation, which decreases the running time. When min conf 1 increases from 0.4 to 0.65, DMP saves (566 . 36  X  481 . 23) / 566 . 36 = 15% time. Algorithm FMP incorporates the min conf 1 test into the mining process, i.e., it does not extend an itemset if its confidence is less than min conf 1 . Hence, the effect of min conf 1 is greater than that on DMP .When min conf 1 changes from 0.4 to 0.65, FMP reduces (42 . 63  X  29 . 40) / 42 . 63 = 31% running time.
Figure 4 shows the runtime for DMP and FMP when min conf 2 changes from 0.75 to 1. Since FMP does not use min conf 2 pruning, min conf 2 has nearly no effect on its performance. For DMP , the savings obtained from a smaller min conf 2 value is also negligible. We examine the influence of generator parameters O , T , S and I on the performance of the algorithms. We first fix the values of T , S , I ,andvary O (average number of observations per se-quences) from 6 to 12. Figure 5 shows that as O increases, the running time of both algorithms increases as well. A larger O im-plies more frequent patterns and more progressive confident rules, and hence additional execution time. The increase in running time
Figure 9: Varying No. of sequences
Table 5: Classification result on diabetic retinal data of FMP is much smaller than that of DMP . The reason is that FMP could efficiently avoid processing many new frequent patterns that do not contribute to new progressive confident rules in the concise set; while DMP has to process all the new frequent patterns.
Next, we fix the values of O , S , I ,andvary T (the average number of items in the itemset). Figure 6 shows the performance of DMP and FMP when T changes from 2 to 5. The runtime of DMP varies greatly under different T values. This is because T is used to control the size limit of a sequence, and different T values result in different set of frequent sequences. On the other hand, T does not influence the pattern table, which contains potential progressive confident rules. By mining progressive confident rules directly, the performance of FMP is hardly influenced by T .

We also study the effect of varying S (average length of poten-tially maximal frequent patterns) from 3 to 8. The result in Figure 7 shows that the execution times of both algorithms increase since a larger S implies more frequent patterns and more progressive con-fident rules. However, the runtime of FMP increases slowly, while that of DMP increases dramatically.

We examine the performance of DMP and FMP when the average number of items in an itemset ( I ) changes. With a larger I ,the frequencies of items increase in the database, which results in more frequent patterns and more progressive confident rules. Hence, the execution times of both DMP and FMP increase as I becomes larger (see Figure 8). Again, the rate of increase for FMP is lower than that for DMP , because FMP incorporates other condition checks in the mining process to prune the search space.
Finally, we test the scalability of DMP and FMP . Figure 9 shows that FMP scales linearly with the number of sequences. Algorithm DMP scales linearly at first, and runs out of memory when the num-ber of sequences reaches 400,000. Increasing the number of se-quences leads to longer ID lists. DMP mines all frequent patterns before filtering and thus, requires a lot of memory to store the long ID lists. In contrast, FMP combines the mining and filtering processes and is able to prune a lot of patterns and their ID lists directly.
One application of progressive confident rules is to predict a fu-ture state of an object based on its past state sequence. In this sec-tion, we describe how progressive confident rules can be utilized in three representative classifiers, C4.5, Support Vector Machine (SVM) and Bayes Classifier (BC).

We map each progressive confident rule r in the concise set to a new feature f r . If the state sequence of an object matches r ,the value of f r is set to 1. Otherwise, it is set to 0. Given n progressive confident rules, the state sequence of an object is transformed to n { feature, value } pairs. Three classifiers, C4.5, SVM and BC are built from the converted feature set. We use C4.5 PCR (C4.5 with Progressive Confident Rules), SVM PCR and BC PCR to indicate classifiers incorporated with PCRs, while C4.5, SVM, and BC to indicate classifiers running on the last states of objects.
The first set of experiments is carried out on our synthetic dataset with two classes C and C . Each class has 50,000 cases. We select 50% cases from each class for training, and use the remaining cases for testing. The mining algorithm generated 1,154 progressive con-fident rules from two classes. Table 4 shows the various classifica-tion accuracies in two classes and in the whole test data. We see that classifiers incorporated with PCRs, i.e. C4.5 PCR, SVM PCR and BC PCR, outperform their counterparts C4.5, SVM, BC. On average, they could achieve about 12% more accuracy.

We also carried out experiments on a real-life dataset which cap-tures the retinal examination data of 10,845 diabetic patients. Each patient has around 2-6 examination records at different times. There are two classes in this dataset: FMAC-Y and FMAC-N . This real-life dataset is very biased. The size of class FMAC-N is about 100 times that of class FMAC-Y . For such biased data, geometric mean ( g mean ) is typically used to measure the total classification accu-racy for all classes. g mean = or true positive rate, and TN is true negative rate. We set the train-ing size of two classes to be 90% that of class FMAC-Y .There are 1,268 progressive confident rules in the training data. Table 5 shows the classification accuracy of the classifiers in two classes and the g mean values. Our work is related to the sequential pattern mining problem [2]. Efficient algorithms for this problem include GSP [6], SPADE [7] and PrefixSpan [4]. GSP utilizes the apriori property to prune search space [6]. SPADE [7] works on a vertical representation of the database where every database item is associated with an id-list and the supports of sequences can be computed from the id-lists directly. PrefixSpan [4] mines frequent sequences by projecting databases on individual items, thus avoiding the time-consuming subset testing and candidate generation. Our work mines frequent sequences with extra constraints.

The work in [3] uses regular expressions as a tool for users to specify the kind of frequent sequences the system should return and propose a family of algorithms to mine frequent sequences with regular expression constraints. Jian et al. [5] examine sequen-tial patterns with constraints that obey the prefix anti-monotonic property and design the prefix-growth algorithm which also utilizes a database projection m ethod. Our work is different from theirs in that our constraints neither follow regular expressions nor obey the prefix anti-monotonic property.
In this paper, we have described a new kind of pattern called progressive confident rules. The rules capture the state change of objects that leads to a certain end state with increasing confidence. We formalized the concept of progressive confident rules and put forward new pruning strategies to reduce the huge search space. We designed a depth-first mining algorithm FMP and demonstrated its efficiency. We have also shown how progressive confident rules can be used to predict a future state of an object given its past state sequence. Experiments on both synthetic and real datasets show that classifiers incorporated with progressive confident rules could improve the classification accuracy greatly.
