 Uwe Dick dick@mpi-sb.mpg.de Peter Haider haider@mpi-sb.mpg.de Tobias Scheffer scheffer@mpi-sb.mpg.de Max Planck Institute for Computer Science, Saarbr  X ucken, Germany In many applications, one has to deal with training data with incompletely observed attributes. For in-stance, training data may be aggregated from differ-ent sources. If not all sources are capable of providing the same set of input attributes, the combined train-ing sample contains incompletely observed data. This situation occurs in email spam detection, where it is helpful to augment the content of an email with real-time information about the sending server, such as its blacklist status. This information is available for all training emails that arrive at a mail server under one X  X  own control, and it is also available at application time. But if one wants to utilize training emails from public archives, this information is missing.
 We adress a learning setting in which values are miss-ing at random: here, the presence or absence of values does not convey information about the class labels. If this condition is not met, it is informative to consider the presence or absence of values as additional input to the decision function. Techniques for learning from in-complete data typically involve a distributional model that imputes missing values, and the desired final pre-dictive model. Prior work on learning from incomplete data is manifold in the literature, and may be grouped by the way the distributional model is used.
 The first group models the distribution of missing val-ues in a first step, and learns the decision function based on the distributional model in a second step. Shivaswamy et al. (2006) formulate a loss function that takes a fixed proportion of the probability mass of each instance into account, with respect to the es-timated distribution of missing values. They derive second order cone programs which renders the method applicable only to very small problems. Other exam-ples include Williams and Carin (2005), Williams et al. (2005), and Smola et al. (2005).
 The second group estimates the parameters of a distri-butional model and the final predictive model jointly. As an example, recently Liao et al. (2007) propose an EM-algorithm for jointly estimating the imputa-tion model and a logistic regression classifier with lin-ear kernel, assuming the data arises from a mixture of multivariate Gaussians.
 The third group makes no model assumption about the missing values, but learns the decision function based on the visible input alone. For example, Chechik et al. (2007) derive a geometrically motivated approach. For each example, the margin is re-scaled according to the visible attributes. This procedure specifically aims at learning from data with values that are structurally missing  X  X s opposed to missing at random . Chechik et al. (2007) find empirically that the procedure is not adequate when values are missing at random.
 Jointly learning a distributional model and a kernel predictive model relates to the problem of learning a kernel function from a prescribed set of parameterized kernels. This problen drew a lot of attention recently; see, for example, Argyriou et al. (2005) and Micchelli and Pontil (2007).
 Estimating the distributional model first and training the predictive model in a second step leaves the user free to choose any learning algorithm for this second step. However, a harder problem has to be solved than would be necessary. If one is only interested in a deci-sion function that minimizes the desired loss, knowing the values or distribution of the missing attributes in the training set is not actually required. Furthermore, errors made in the imputation step and errors made in estimating the parameters of the predictive model can add up in a sequential procedure.
 Consequently, we investigate learning the decision function and the distribution of imputations depen-dently. Unlike prior work on this topic, we develop a solution for a very general class of optimization crite-ria. Our solution covers a wide range of loss functions for classification and regression problems. It comes with all the usual benefits of kernel methods. We de-rive an optimization problem in which the distribution governing the missing values is a free parameter. The optimization problem searches for a decision function and a distribution governing the missing values which together minimize a regularized empirical risk. No fixed parametric form of the distributional model is assumed. A regularizer that can be motivated by a distributional assumption may bias the distributional model towards a prior belief. However, the regularizer may be overruled by the data, and the resulting distri-butional model may be different from any parametric form. We are able to prove that there exists an opti-mal solution based on a distribution that is supported by finitely many imputations. This justifies a greedy algorithm for finding a solution. We derive manifesta-tions of the general learning method and study them empirically.
 The paper is structured as follows. After introducing the problem setting in Section 2, we derive an opti-mization problem in Section 3. Section 4 proves that there is an optimal solution that concentrates the den-sity mass on finitely many imputations and presents an algorithm. Example instantiations of the general solution are presented in Section 5. We empirically evaluate the method in Section 6. Section 7 concludes. We address the problem of learning a decision func-tion f from a training sample in which some attribute values are unobserved.
 Let X be a matrix of n training instances x i and let y be the vector of corresponding target values y i . In-stances and target values are drawn iid from an un-known distribution p ( x , y ) with x i  X  R d and y i  X  Y , where Y denotes the set of possible target values. Ma-trix Z indicates which features are observed. A value of z il = 1 indicates that x il , the l -th feature of the i -th example, is observed. Values are missing at random: y is conditionally independent of z i given x i . The goal is to learn a function f : x 7 X  y that pre-dicts target values for completely observed examples. The decision function should incur only a minimal true function for the task at hand.
 As a means to minimizing the true risk, we seek a function f in the reproducing kernel Hilbert space H k induced by a kernel k that minimizes a regularized empirical risk functional R ( f ) = P n i =1 l ( y i , f ( x  X  k f k 2 k . We demand k to be a Mercer kernel. Loss function l approximates the true loss L . The represen-ter theorem allows us to write the minimizer as a sum over functions in H k centered at training instances: f ( x ) = P n j =1 c j k ( x j , x ).
 The learning problem from completely observed data would amount to solving Optimization Problem 1. Optimization Problem 1 (Primal learning prob-lem, observed data). Over c , minimize R ( c , k )= We require that the loss function be defined in such a way that Optimization Problem 1 can be written in the dual form of Optimization Problem 2. A wide range of loss functions satisfies this demand; we will later see that this includes hinge loss and squared loss. Optimization Problem 2 (Dual of learning problem). Given a &lt; 0 , over c , maximize subject to the constraints R  X  ( c ) denotes a differentiable convex function of the dual variables c which we demand to be independent of the kernel matrix K . The inequality constraints g  X  i are differentiable convex and the equality constraints h j differentiable affine. We like to note that the re-quirement of independence between R  X  and K is not very restrictive in practice, as we will see in chapter 5. Furthermore, we demand strong duality to hold between Optimization problems 1 and 2. If any instance x i has unobserved features, then k ( x i , x ) and, consequently, the decision function f are not properly defined. In order to learn from incom-plete data, we will marginalize the decision function and risk functional by the observable attributes and integrate over all unobserved quantities. To this end, we define  X   X   X  Z X  X  R n  X  d as a matrix of imputations be compact for the rest of this paper. Let  X  i denote the i -th row of  X  . Then we can define a family of ker-nels K (  X  )( x j , x i ) = k (  X  j ,  X  i ). Any probability mea-sure p (  X  ) on imputations induces a marginalization of the kernel by the observable variables. Equation 2 in-tegrates over all imputations of unobserved values; it can be evaluated based on the observed values. Any probability measure p (  X  ) constitutes an optimiza-tion criterion R ( c , K ( p )). In the absence of knowledge about the true distribution of missing values, p (  X  ) be-comes a free parameter. Note that p (  X  ) is a continu-ous probability measure that is not constrained to any particular parametric form; the space of parameters is therefore of infinite dimensionality.
 It is natural to add a regularizer Q ( p ) that reflects prior belief on the distribution of imputations p (  X  ) to the optimization criterion, in addition to the empiri-cal risk and regularizer on the predictive model. The regularizer is assumed to be continuous in p . The reg-ularizer does not constrain p (  X  ) to any specific class of distribution, but it reflects that some distributions are believed to be more likely. Without a regularizer, the criterion can often be minimized by imputations which move instances with missing values far away from the separator, thereby removing their influence on the outcome of the learning process. This leads to Optimization Problem 3.
 Optimization Problem 3 (Learning problem with infinite imputations). Given n training ex-amples with incomplete feature values,  X  &gt; 0 , kernel function k , over all c and p , minimize subject to the constraints Each solution to Optimization Problem 3 integrates over infinitely many different imputations. The search space contains all continuous probability measures on imputations, the search is guided by the regularizer Q . The regularization parameter  X  determines the influ-ence of the regularization on the resulting distribution. For  X   X   X  the solution of the optimization reduces to the solution obtained by first estimating the distri-bution of missing attribute values that minimizes the regularizer. For  X   X  0 the solution is constituted by the distribution minimizing the risk functional R . In this section, we devise a method for efficiently find-ing a solution to Optimization Problem 3. Firstly, we show that there exists an optimal solution  X  c ,  X  p with  X  p supported on at most n +2 imputations  X   X   X  Z X . Sec-ondly, we present an algorithm that iteratively finds the optimal imputations and parameters minimizing the regularized empirical risk. 4.1. Optimal Solution with Finite Combination In addition to the parameters c of the predictive mod-els, continuous probability measure p (  X  ) contributes an infinite set of parameters to Optimization Problem 3. The implementation of imputations as parameters of a kernel family allows us to show that there exists an optimal probability measure  X  p for Equation 3 such that  X  p consists of finitely many different imputations. Theorem 1. Optimization Problem 3 has an optimal solution  X  c ,  X  p in which  X  p is supported by at most n + 2 imputations  X   X   X  Z X .
 Proof. The compactness of  X  Z X and the continuity of K immediately imply that there exists some solution to Optimization Problem 3. It remains to be shown that at least one of the solutions is supported by at most n + 2 imputations. Let  X  c ,  X  p be any solution and let all requirements of the previous section hold. The idea of this proof is to construct a correspondence between where a finite support set is known to exist. Define S (  X  ) = K (  X  )  X  c  X  R n and D = { ( S (  X  )  X  , Q (  X  ))  X  are continuous by definition, D is compact as well. We define a measure over D as ( A  X  B ) =  X  p ( {  X  : S (  X  A  X  Q (  X  )  X  B } ).
 Then, by Carath  X eodory X  X  convex hull theorem, there exists a set of k vectors { ( s  X  1 , q 1 )  X  , . . . , ( s with k  X  n + 2 and nonnegative constants  X  i with P For each i , select any  X  i such that ( S (  X  i )  X  , Q ( ( s i , q i ). We construct  X  p by setting  X  p (  X  ) = P where  X   X  i denotes the Dirac measure at  X  i . The op-timal  X  c results as arg min c R ( c , K ( X  p )). We have Then K (  X  p )  X  c = Likewise, Since Q ( p ) does not depend on c ,  X  c = arg min c R ( c , K (  X  p )), and by strong duality, that the Karush-Kuhn-Tucker conditions hold for  X  c , namely there exist constants  X  i  X  0 and  X  j such that a K (  X  p )  X  c  X  X  X  R  X  (  X  c ) + X It is easy to see that therefore  X  c is also a maximizer the Karush-Kuhn-Tucker conditions still hold. Their sufficiency follows from the fact that K ( p ) is positive semi-definite for any p , and the convexity and affinity premises. Thus, We have now established that there exists a solution with at most n + 2 imputations. 4.2. Iterative Optimization Algorithm This result justifies the following greedy algorithm to find an optimal solution to Optimization Problem 3. The algorithm works by iteratively optimizing Prob-lem 1 (or, equivalently, 2), and updating the distribu-tion over the missing attribute values. Let p  X   X  denote the distribution p (  X  ) =  X   X   X  . Algorithm 1 shows the steps.
 Algorithm 1 Compute optimal distribution of impu-
Initialization: Choose p (1) = p all z il 6 = 1 for t = 1 . . . do end for Step 1 consists of minimizing the regularized empiri-cal risk functional R , given the current distribution. In step 2 a new imputation is constructed which im-proves on the current objective value. Since in gen-eral  X  R k, X  ( c , p  X  ) is not convex in  X  , one cannot find the optimal  X  efficiently. But the algorithm only re-quires to find any better  X  . Thus it is reasonable to perform gradient ascent on  X  , with random restarts in case the found local optimum does not satisfy the inequality of step 2. In step 3 and 4 the optimal dis-tribution consisting of the weighted sum of currently used Dirac impulses P t i =1  X  i  X   X  i and the new imputa-tion  X   X  R at Optimization Problem 2, we see that this is the case for R . Thus the convexity depends on the choice for Q (see Sect. 5.2). Step 5 updates the weights of the previous imputations.
 The algorithm finds t imputations  X  ( j ) and their weights  X  j , as well as the optimal example coefficients c . We can construct the classification function f as Note that the value n + 2 is an upper bound for the number of basic kernels which constitute the optimal solution. The algorithm is not guaranteed to terminate after n + 2 iterations, because the calculated imputa-tions are not necessarily optimal. In practice, however, the number of iterations is usually much lower. In our experiments, the objective value of the optimization problem converges in less than 50 iterations. In this chapter we present manifestations of the generic method, which we call weighted infinite imputations , for learning from incomplete data that we use in the experimental evaluation.
 Recall from Section 3 the goal to learn a decision func-tion f from incomplete data that minimizes the ex-sification problems the natural loss function L be-comes the zero-one loss , whereas in regression prob-lems the loss depends on the specific application; com-mon choices are the squared error or the  X  -insensitive loss. The considerations in the previous chapters show that, in order to learn regression or classification func-tions from training instances with missing attribute values, we only have to specify the dual formulation of the preferred learning algorithm on complete data and a regularizer on the distribution of imputations p . 5.1. Two Standard Learning Algorithms For binary classification problems, we choose to ap-proximate the zero-one by the hinge loss and perform support vector machine learning. The dual formula-tion of the SVM is given by R SV M ( c , k ) = P n i =1 c
P n i,j =1 c i c j k ( x j , x i ) subject to the constraints 0  X  of Optimization Problem 2 are met and a finite solu-tion can be found. Taking the SVM formulation as the dual Optimization Problem 2 gives us the means  X  in conjunction with an appropriate regularizer Q  X  to learn a classification function f from incomplete data. For regression problems, the loss depends on the task at hand, as noted above. We focus on penalizing the squared error , though we like to mention that the ap-proach works for other losses likewise. One widely used learning algorithm for solving the problem is kernel ridge regression . Again, we can learn the regression function f from incomplete data by using the same principles as described above. Kernel ridge regression minimizes the regularized empirical risk P n i =1 ( y i  X  f ( x i )) 2 +  X  k f k 2 . The dual formulation R KRR ( c , k ) = P meets the demands of the dual optimization problem 2. Substituting its primal formulation for R in step 1 of Algorithm 1 and in Eqn. 3 solves the problem of learning the regression function from incomplete data after specifying a regularizer Q . 5.2. Regularizing towards Prior Belief in A regularizer on the distribution of missing values can guide the search towards distributions  X   X  that we be-lieve to be likely. We introduce a regularization term which penalizes imputations that are different from our prior belief  X   X  . We choose to penalize the sum of squared distances between instances x i and  X   X  i in feature space H k induced by kernel k . We define the squared distance regularization term Q sq as
Q sq ( k,  X   X  ) = Note that when using Q sq , step 3 of Algorithm 1 be-comes a convex minimization procedure. 5.3. Imputing the Mean in Feature Space In principle any imputation we believe is useful for learning a good classifier can be used as  X   X  . Sev-eral models of the data can be assumed to com-pute corresponding optimal imputations. We like to mention one interesting model, namely the class-based mean imputation in the feature space H k in-duced by kernel k . This model imputes missing values such that the sum of squared distances be-tween completed instances to the class-dependent mean in feature space is minimal over all possi-ble imputations.  X   X  = arg min  X  P n i =1 k  X  k (  X  i )  X  ber of instances with label y . Simple alge-braic manipulations show that this is equivalent to minimizing the sum of squared distances between  X  2 k (  X  i ,  X  j ) + k (  X  j ,  X  j ) Definition 1 (Mean in Feature Space). The class-based mean in feature space imputation method im-putes missing values  X   X  which optimize Note that this model reduces to the standard mean in input space when using the linear kernel. We evaluate the performance of our generic approach weighted infinite imputations for two example realiza-tions. We test for classification performance on the email spam data set which motivates our investiga-tion. Furthermore, we test on seven additional binary classification problems and three regression problems. 6.1. Classification We choose to learn the decision function for the binary classification task by substituting the risk functional of the support vector machine ,  X  R SV M , as presented in section 5.1 for R and the squared distance regularizer Q sq (Section 5.2) for Q in Optimization Problem 3. For the motivating problem setting, we assemble a data set of 2509 spam and non-spam emails, which are preprocessed by a linear text classifier which is currently in use at a large webspace hosting company. This classifier discriminates reasonably well between spam and non-spam, but there is still a small fraction of misclassified emails. The classifier has been trained on about 1 million emails from a variety of sources, in-cluding spam-traps as well as emails from the hosting company itself, recognizing more than 10 million dis-tinct text features. On this scale, training a support vector machine with Gaussian kernel is impractical, therefore we employ a two-step procedure. We discard the contents of the emails and retain only their spam score from the text classifier and their size in bytes as content features in the second-step classifier. At the time of collection of the emails, we record auxiliary real-time information about the sending servers. This includes the number of valid and invalid receiver ad-dresses of all emails seen from the server so far, and the mean and standard deviation of the sizes and spam scores of all emails from the server. Such information is not available for emails from external sources, but will be available when classifying unseen emails. We randomly draw 1259 emails, both spam and non-spam, with server information, whereas half of those were drawn from a set of misclassified spam-emails. We aug-ment this set with 1250 emails drawn randomly from a source without server information for which only 2 of the 8 attributes are observed.
 To evaluate the common odd versus even digits dis-crimination, random subsets of 1000 training examples from the USPS handwritten digit recognition set are used. We test on the remaining 6291 examples. Ad-ditionally, we test on KDD Cup 2004 Physics (1000 train, 5179 test, 78 attributes) data set and on the 4-view land mine detection data (500, 213, 41) as used by Williams and Carin (2005). In the latter, instances consist of 4 views on the data, each from a separate sensor. Consequently, we randomly select complete views as missing. From the UCI machine learning repository we take the Breast (277 instances, 9 features), Diabetes (768, 8), German (1000, 20), and Waveform (5000, 21) data sets. Selection criteria for this subset of the repository were minimum require-ments on sample size and number of attributes. On each data set we test the performance of weighted infinite imputation using four different regularization imputations  X   X  for the regularizer Q sq ( K ( p ) ,  X   X  ). These imputations are computed by mean imputation in in-put space ( MeanInput ) and mean imputation in fea-ture space ( MeanFeat ) as by Definition 1. Addi-tionally we use the EM algorithm to compute the at-tributes imputed by the maximum likelihood parame-ters of an assumed multivariate Gaussian distribution with no restrictions on the covariate matrix ( Gauss ), and a Gaussian Mixture Model with 10 Gauss centers and spherical covariances ( GMM ).
 Four learning procedures based on single imputations serve as reference methods: the MeanInput , Mean-Feat , Gauss , and GMM reference methods first de-termine a single imputation, and then invoke the learn-ing algorithm.
 All experiments use a spheric Gaussian kernel. Its vari-ance parameter  X  as well as the SVM-parameter  X  are adjusted using the regular SVM with a training and test split on fully observed data. All experiments on the same data set use this resulting parameter setting. Results are averaged over 100 runs were in each run training and test split as well as missing attributes are chosen randomly. If not stated otherwise, 85% of at-tributes are marked missing on all data sets. In order to evaluate our method on the email data set, we per-form 20-fold cross-validation. Since the emails with missing attributes cannot be used as test examples, the test sets are only taken from the fully observed part of the data set.
 Table 6.1 shows accuracies and standard errors for the weighted infinite imputations (WII) method with squared distance regularization compared to all single imputations  X   X  on each data set. Regularization pa-rameter  X  is automatically chosen for each run based on the performance on a separate tuning set. Base-lines are obtained by first imputing  X   X  and learning the classifier in a second step. The weighted infinite impu-tations method outperforms the single imputation in virtually all settings. We test for significant improve-ments with a paired t-test on the 5% significance level. Significant improvements are marked with a  X   X   X  in the table.
 We explore the dependence of classification perfor-mance on training sample size and the percentage of missing attribute values in more detail. The first graph in Figure 1 shows improvements in classification accu-racy of our method over the single imputations de-pending on the percentage of missing values. Graph 2 shows classification accuracy improvements depend-ing on the size of the labeled training set. Both ex-periments are performed on USPS data set and we again adjust  X  separately for each run based on the performance on the tuning set. We note that similar results are obtained for the other classification prob-lems. The weighted infinite imputation method can improve classification accuracy even when only 30% of the attribute values are missing. It shows, though, that it works best if at least 60% are missing, depend-ing on  X   X  . On the other hand, we see that it works for all training set sizes, again depending on  X   X  . Similar results are obtained for the other data sets. To evaluate the convergence of our method, we mea-sure classification accuracy after each iteration of the learning algorithm. It shows that classification accu-racy does not change significantly after about 5 itera-tions for a typical  X  , in this case  X  = 10 5 for the USPS data set. On average the algorithm terminates after about 30-40 iterations. The computational demands of the weighted infinite imputation method are approxi-mately quadratic in the training set size for the classifi-cation task, as can be seen in Graph 3 of Figure 1. This result depends on the specific risk functional R and its optimization implementation. Nevertheless, it shows that risk functionals which are solvable in quadratic time do not change their computational complexity class when learned with incomplete data. 6.2. Regression We evaluate the weighted infinite imputations method on regression problems using the squared error as loss function. Consequently, risk functional R KRR (Sect. 5.1) is used as R and again the squared distance reg-ularizer Q sq for Q in Optimization Problem 3. From UCI we take the Housing data (506, 14), and from the Weka homepage cpu act (1500, 21) and ailerons (2000, 40). Ridge parameter  X  and RBF-kernel parameter  X  were again chosen such that they lead to best results on the completely observed data. Regularization pa-rameter  X  was chosen based on the performance on a tuning set consisting of 150 examples. Results are shown in Table 2. We can see that our method outper-forms the results obtained with the single imputations significantly for all settings. We devised an optimization problem for learning de-cision functions from incomplete data, where the dis-tribution p of the missing attribute values is a free parameter. The investigated method makes only mi-nor assumptions on the distribution by the means of a regularizer on p that can be chosen freely. By simul-taneously optimizing the function and the distribution of imputations, their dependency is taken into account properly. We presented a proof that the optimal so-lution for the joint learning problem concentrates the density mass of the distribution on finitely many impu-tations. This justifies the presented iterative algorithm that finds a solution. We showed that instantiations of the general learning method consistently outperform single imputations.
 Acknowledgments We gratefully acknowledge support from STRATO Rechenzentrum AG.

