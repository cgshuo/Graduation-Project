 In recent years, heterogeneous informa tion network analysis has become a hot research topic in data mining field. Different from widely used homogeneous networks which include only same-typed objects or links, Heterogeneous Infor-mation Network (HIN) organizes the networked data as a network including different-typed objects and links. For example, in the case of bibliographic net-work, the object types include authors, papers, venues and links between ob-jects correspond to different relations , such as write relation between authors and papers, and citation relation between papers. Fig.1(a) and Fig.1(b) shows two bibliographic information network schemas which are ACM dataset and DBLP dataset. Combination of different-typed objects and links results in more comprehensive structure information and rich semantics information. Thus, het-erogeneous information network analysis will mine more interesting patterns.
Many data mining tasks have been exploited in heterogeneous information network, such as clustering [1], classification [2]. Among these data mining tasks, similarity measure is a basic and important function, which evaluate the simi-larity of object pairs on networks. Although similarity measure on homogeneous networks have been extensively studied in the past decades, such as PageR-ank [3] and SimRank [4], the similarity measure in heterogeneous network is just beginning now and several measures have been proposed including PathSim [5], PCRW [6] and HeteSim [7]. All the three methods are based on Meta-Path whose definition can be found in the rel ated work [7]. Specially, HeteSim, proposed by Shi et al., has the ability to measure relatedness of objects with the same or different types in a uniform framework. HeteSim has some good properties (e.g., self-maximum and symmetric), and has shown its potential in several data mining tasks. However, we can also find that it has several disadvan-tages. (1) HeteSim has relatively high computational complexity, in particular, the adoption of path decomposition approach while measuring the relevance on odd-length path further increases complexity of calculation. (2) Besides, Het-eSim cannot be extended to large-scale network with massive data, since its calculation process is based on memory co mputing. Therefore, it is desired to design a new similarity measure, which not only contains some good properties of HeteSim but also overcomes the disadvantages on computation.

In this paper, we propose a new relevance measure method -AvgSim ,whichis a symmetric and uniform measure to evaluate the relevance of same or different-typed objects. Since AvgSim can also measure the relevance of different-typed objects, we use the relevance measure instead of similarity measure in the fol-lowing section. AvgSim value of two objects is the average of reachable prob-ability under the given path and the reverse path. It guarantees that AvgSim can measure relevance of same or differ ent-typed objects and it has symmetric property. In addition, we take parallelization of this new algorithm on MapRe-duce in order to eliminate restriction of memory size and deal with massive data more efficiently in practical applications . Experiments on real dataset show that AvgSim can achieve comparative perform ances with high efficiency and effective-ness, compared with other methods including HeteSim, PathSim and PCRW. Moreover, experiments on large-scale d ataset also validate the effectiveness of parallelized AvgSim.

The rest of this paper is organized as follows: Section 2 describes AvgSim in detail. And the method of parallelization of AvgSim is explained in Section 3. Section 4 analyzes performance experiment results of AvgSim to validate its effectiveness and efficiency. And some matr ix parallelization experiments are also in this section. Finally we conclude this paper in Section 5. In this section, we will introduce you a new meta-path based relevance measure which is called AvgSim and the definition of it is as follows.
 Definition 1 AvgSim: Given a meta-path P which is defined on the composite relation R = R 1  X  R 2  X  ...  X  R l , AvgSim between two objects s and t ( s is the source object and t is the target object) is:
Equation (1) shows the relevance of source object and target object based on meta-path P is the arithmetic mean value of random walk result from s to t along P and reversed random walk result from t to s along P  X  1 . Equation (2) shows the decomposed step of AvgSim, namely the measure of random walk. The measure takes a random walk step by step from starting point s to end point t along path P using iterative method, where | O ( s | R 1 ) | is the out-neighbors of s basedonrelation R 1 . If there is no out-neighbors of s on R 1 , then the relevance value of s and t is 0 because s cannot reach t . We need to calculate random walk probabilities for each out-neighbor of s to t iteratively, and then sum them up. Finally the summation should be normalized by the number of out-neighbors to get average relatedness. The stop sign of iteration is that s meets t at t node along P . In contrast to simple random work method, AvgSim shows its comprehensiveness and the eff ectiveness reflected in later experiments verifies its advantages.

We take the simple network showed in Fig.2 as an example to calculate the relevance between Mike and the subject DataMining ( DM for short) based on path APS ( X  X uthor-Paper-Subject  X ).

We notice from Fig.2 that O ( Mike | AP )= { P 1 ,P 2 ,P 3 } , thus we need to calculate relatedness between each out-neighbor of Mike and DM , like RW ( P 1 ,DM | PS ).

Since that O ( P 1 | PS )= { DM } , out-neighbors of P 1 basedonrelation PS will meet with DM ,thus RW ( P 1 ,DM | PS ) = 1. Finally, we can easily calculate the relatedness value of random walk from Mike to DM along path APS is 2/3. Likewise, relatedness value of reverse random walk along path SPA is 2/3. Thus the relevance value (i.e. AvgSim) between author Mike and subject DataMining is 0.67 (2/3).

The example above shows the operation process of AvgSim measuring rele-vance of two arbitrary objects along a meta-path. Next we will study on how to calculate AvgSim generally using matrices .

Given a simple directed meta-path A R  X   X  B , where object A and B are linked though relation R . The relationship between A and B can be expressed by ad-jacent matrix, denoted as M AB . Two normalized matrix R AB and C AB are generated by normalizing M AB according to row vector and column vector re-spectively. R AB and C AB are transition probability matrix which represent A R  X   X  B and B R derive relations R AB = C BA and C AB = R BA ,where R AB is the transpose of R R is a composite relation R = R 1  X  R 2  X  ...  X  R l , then the relationship between A 1 and A l +1 is expressed as reachable probability matrix which is obtained by computation on the basis of transition probability matrix. The reachable RW suggests RW P is the random walk relatedness matrix from object A 1 to A l +1 along path P .

Then we can rewrite AvgSim using reachable probability matrix according to equation (1) and (2) as follows.

Applied relation C AB = R BA , equation (8) is derived below. We notice that the calculation of AvgSim is unified as two chain matrix multiplication of tran-sition probability matrices. The only difference between two chains is the nor-malization form of original adjacent matrix.
AvgSim can measure relevance of any heterogeneous or homogeneous objects basedonsymmetricalpath( e.g.APCPA ) or asymmetrical path ( e.g.APS ). Be-sides, the method has symmetric property , which can be verified easily from the definition equation of AvgSim and the symmetric property has a positive effect on clustering. However, the calculation of AvgSim mainly the chain matrix mul-tiplication is time-consuming and restricted of memory size. In order to apply our algorithm in real large-scale heterogeneous information network, we have to consider how to improve the efficiency of AvgSim. Parallelism is an effective method for processing of massive data and improving algorithm X  X  efficiency. According to the features and application scenarios of AvgSim, we will realize it using parallelization method and the specific steps are as follows. 1. Since the core calculation of AvgSim is the chain matrix multiplication, we 2. After step 1, we turn to focus on single large-scale matrix multiplication
As we know, different orders of operations in chain matrix multiplication leads to different time of computation. There exists an optimal order of chain ma-trix multiplication using Dynamic Programming, which consumes the shortest computation time. Thus, we can apply Dynamic Programming to improve the efficiency of parallelized AvgSim. And the parallelization of AvgSim is mainly the parallelization of matrix multiplication after Dynamic Programming process. Here we use  X  X lock matrix multiplication  X  X ethod on MapReduce to transform multiplication of two large matrices into several multiplications of smaller matri-ces. This method is flexible with selecting dimensions of block matrix according to the configuration of Hadoop cluster and avoids exceeding the memory size.
Applying  X  X lock matrix multiplication  X  X teratively to the chain matrix mul-tiplication which is re-ordered by Dynamic Programming, we can get one of the two reachable probability matrices of AvgSim (e.g., RW P , which is measured in the given meta-path P ), and the other probability matrix ( RW P  X  1 )canbe obtained in exactly the same procedure. Finally, the relevance matrix is derived by taking arithmetic mean of these two reachable probability matrices. 4.1 Data Sets Twodatasets, DBLP dataset and Matrix dataset , are used in experiments and the previous network schema is shown in Fig. 1(b). In detail, the DBLP dataset contains 14K papers, 14K authors, 20 conferences and 8.9K terms. And we la-bel 20 conferences, 100 papers, and 4057 authors in the dataset with four research areas including database, data mining, information retrieval and artificial intelli-gence for experiments use. And the Matrix dataset (40 matrices in total) contains several artificially generated large-scale sparse square matrices, whose dimensions are 1000  X  1000, 5000  X  5000, 10000  X  10000, 20000  X  20000, 40000  X  40000, 80000  X  80000, 100000  X  100000 and 150000  X  150000 respectively. And the spar-sity of each matrix includes 0.0001, 0.0003, 0.0005, 0.0007 and 0.001. 4.2 Performance of AvgSim Performance on Query Task and Clustering Task. In the query task, we compare the performance of AvgSim with both HeteSim and PCRW though measuring the relevance of heterogeneous objects on DBLP dataset. Based on labels of the dataset, we calculate the AUC (Area Under ROC Curve) score to evaluate the performance of the results which are the related authors ranked by relevance scores for each conference on meta-path CPA . We evaluated 9 out of 20 marked conferences, whose AUC values are shown in Table 1. We notice that AvgSim gets the highest value on 8 conf erences, which means AvgSim performs better than other two methods in the query task.
 In the clustering task, we compare the performance of AvgSim with both HeteSim and PathSim though measuring the relevance of homogeneous ob-jects on DBLP dataset. We firstly apply thr ee algorithms respectively to derive the relevance matrices on three meta-paths including CPAPC , APCPA and PAPCPAP . Based on the result matrices and applied Normalized Cut, we per-form clustering task and then evaluate the performances on conferences, authors, and papers using NMI criterion (Norma lized Mutual Information). The clus-tering accuracy result is shown in Table 2 and AvgSim gets the highest NMI value in all the three tasks. The results of query task and clustering task suggest that AvgSim performs well in effectiveness.
 Performance of Parallelized Matrix Multiplication. All parallelized ma-trix multiplication experiments are conducted on Matrix dataset in a cluster composed of 7 machines with 4-cores E3-1220 V2 CPUs of 3.10GHz and 32 GB RAM running on RedHat 4 operating system. The experiments will measure sev-eral factors affecting block matrix multiplication, including matrix dimensions, matrix sparsity and partition strategy (i.e. dimensions of blocks).
Fig.3(a) shows the relationship among matrix dimensions, matrix sparsity and running time of parallelized block matrix multiplication together with the com-parison between stand-alone and parallelized matrix multiplication. We notice that the larger dimensions or sparsity of matrix are, the more time in matrix multiplication is required. And the stand-alone algorithm costs shorter time for quite small matrix dimension because parallelized algorithm spends lots of time in starting task nodes of Hadoop cluster and resources of cluster are not fully utilized for small amount of calculations. However, efficiency of parallelized al-gorithm is much better as matrix dimension increasing. Besides, stand-alone algorithm is restricted of memory size for there are no results derived in the last three large-scale matrix multiplications.

Fig.3(b) shows the relationship among running time, intermediate data amount and partition strategy of block matrix multiplication. There are 11 kinds of partition strategies with square block matrix dimensions of 300  X  300, 500  X  500, 700  X  700, 900  X  900, 1000  X  1000, 1100  X  1100, 1300  X  1300, 1500  X  1500, 2000  X  2000, 4000  X  4000 and 6000  X  6000 respectively applying in the square matrix with dimension of 100000  X  100000 and a sparsity of 0.0001 in the experi-ment. We notice that intermediate data amount of matrix multiplication decrease gradually with the increase of block dimension. In contrast, running time reaches its minimum value at 5-th data point shown in figure. Smaller intermediate data amount results in less disk IO operations and data amount transmitted by shuf-fle, which also means shorter time and b etter performance to a certain extent as front several data points reflected. How ever, excessive large block dimension will reduce the concurrent granularity and increase the amount of calculations for single node, which conversely results in longer time of com putation as several data points behind reflected.

In conclusion, appropriate partition strategy and sufficient sizes of cluster greatly affect the efficiency in parallelized block matrix multiplications. Applying parallelization method, AvgSim gains the ability to measure relevance in larger-scale networks with massive data efficiently. In this paper, we introduced a novel algorithm with symmetrical features named AvgSim for measuring relevance of arbitrary objects in heterogeneous informa-tion network. In addition, using Dynamic Programming and  X  X lock matrix mul-tiplication  X  X ethods, parallelized AvgSim is able to be applied to actual large-scale networks. Experiments given in the paper verified the effectiveness and efficiency of AvgSim while measuring the relevance of heterogeneous or homo-geneous objects based on meta-paths.
 Acknowledgment. This work is supported by the National Key Basic Research and Department(973) Program of China (No.2013CB329603), the National Sci-ence Foundation of Chin a (Nos.61375058, and 71231002), t he Ministry of Ed-ucation of China and China Mobile Research Fund (MCM20123021) and the Special Co-construction Project of Beijing Municipal Commission of Education.
