 In the past, most traditional classification techniques usually assumed a sin-gle category for each object to be classified by means of minimum distance. However, in some tasks it is natural to assign more than one categories to an object. For examples, some news art icles can be categorized into both politic and crime , or some movies can be labeled as action and comedy , simultane-ously. As a special type of task, multi-label classification was initially studied by Schapire and Singer (2000) [10] in text categorization. Later many techniques in multi-label classification have been proposed for various applications such as se-mantic scene classification [2], music emotion categorization [12] and automated tag recommendation [8]. However, these m ethods can be grouped into two main approaches: Algorithm Adaptation (AA) and Problem Transformation (PT) as suggested in [13]. The former approach modifies existing classification methods to handle multi-label data [4,10]. On the other hand, the latter approach trans-forms a multi-label classification task into several single classification tasks and then applies traditional classification method on each task [2,9,14].
Residing in these two main approaches, one major issue is curse of dimen-sionality, which causes a well-known overfitting problem. To solve this issue, many techniques have been proposed, e.g., sparse regularization [6], feature se-lection [17] and dimensionality reduction [15,16,18]. Among these methods, the dimensionality reduction which transforms data in a high-dimensional space to those in the lower-dimensional space, has been focused for multi-label classifi-cation problem. The dimensionality reduction in multi-label data was formerly studied by Yu et al. [16]. In their work, Multi-label Latent Semantic Index-ing (MLSI) was proposed to project the original feature space into a reduced feature space. Motivated by MLSI, Multi-label Dimensionality Reduction via Dependence Maximization (MDDM) was introduced by Zhang and Zhou in [18]. In MDDM, Hilbert-Schmidt Independence Criterion (HSIC) was applied rather than LSI and its aim was to identify reduced feature space that maximizes de-pendency between the original feature space and the label space. Recently, Wang et al. [15] has proposed a method to exten d Linear Discriminant Analysis (LDA), a well-known dimensionality reduction method, to handle multi-label data. Such methods mainly focused on how to project the original feature space into a smaller one, but still suffered with high dimensionality in the label space. By this reason, these methods usually have high time complexity in the classification task.

On the other hand, for the label space reduction, to improve the efficiency of multi-label classification, Hsu et al. [7] posed a sparseness problem that mostly occurred in the label space and then applied Compressive Sensing (CS) tech-nique, widely used in the image processing field, to encode and decode the label space. While the encoding step of this CS method seems efficient but the de-coding step does not. Toward this issue, Tai and Lin [11] proposed Principle Label Space Transformation (PLST) to transform the label space into a smaller linear label space using Singular Value Decomposition (SVD) [5] with a simple threshold setting (i.e., 0.5). More recently, Bi and Kwok (2011) [1] extended the PLST to handle the labels which are organized in the form of a tree or directed acyclic graph (DAG). Althoug h the PLST-based methods are effective by reducing dimensions in the label space, it seems not handle neither the curse of dimensionality in the feature space nor the correlation (dependency) among labels in the label space.

Toward these issues, this paper presents an approach that considers both the curse of dimensionality problem in the f eature space and the sparseness problem in the label space. Moreover, the dependency profile among features and labels, the dependency profile among features and the dependency profile among labels are also taken into account. Two alternative methods, namely Dependent Dual Space Reduction (DDSR) and Independent Dual Space Reduction (IDSR), are proposed to reduce dimensions in the dual spaces to eliminate redundancy as well as noise using Singular Value Decomposition (SVD) for better prediction and lower computational cost.

In the rest of this paper, Section 2 gives a formal description of the multi-label classification task and literature review to the SVD method. Section 3 presents two dual dimensionality reduction approaches, Dependent Dual Space Reduction (DDSR) and Independent Dual Space Reduction (IDSR). The multi-label benchmark datasets an d experimental settings are described in Section 4. In Section 5, the experimental results using seven datasets are given and finally Section 6 provides conclusion of this work. 2.1 Definition of Multi-label Classification Task Let X = R M and Y = { 0 , 1 } L be an M -dimensional feature space and L -dimensional binary l abel space, where M is the number of features and L is a is a set of N objects (e.g., documents, images, etc.) in a training dataset, where x object belongs (1) or not (0) to the j -th class (the j -th label or not).
In general, two main phases are exploited in a multi-label classification prob-lem: (1) model training phase and (2) classification phase. The goal of the model training phase is to build a classification model that can predict the label vector y t for a new object with the feature vector x t . This classification model is a mapping function H : R M  X  X  0 , 1 } L can predict a target value closest to its actual value in total. The classification phase uses this classification model to assign labels. For convenience, X N  X  M =[ x 1 , ..., x N ] T denotes the feature matrix with N rows and M columns and Y N  X  L =[ y 1 , ..., y N ] T represents the label matrix with N rows and L columns , where [  X  ] T denotes matrix transpose. 2.2 Singular Value Decomposition (SVD) This subsection gives a brief introduction to SVD, which was developed as a method for dimensionality reduction using a least-squared technique [5]. The SVD transforms a feature matrix X to a lower-dimensional matrix X such that the distance between the original matrix and a matrix in a lower-dimensional space (i.e., the 2-norm X  X  X 2 ) are minimum.
Generally, a feature matrix X can be decomposed into the product of three matrices as shown in (1).
 where N is a number of objects, M is a number of features and M&lt;N .The matrices U and V are two orthogonal matrices, where U T  X  U = I and V T  X  V = I . The columns in the matrix U are called the left singular vectors while as the columns in matrix V are called the right singular vectors .Thematrix  X  is a diagonal matrix, where  X  i,j =0for i = j , and the diagonal elements of  X  are the singular values of matrix X . The singular values in the matrix  X  are sorted by descending order such that  X  1 , 1  X   X  2 , 2  X  ...  X   X  M,M . To discard noise, it is possible to ignore singular values less than  X  K,K ,where K M .Bythis ignorance the three matrices are reduced to (2).
 where X N  X  M is expected to be close X N  X  M , i.e. X  X  X 2 &lt; X  , U N  X  K is a reduced matrix of U N  X  M ,  X  K  X  K is the reduced version of  X  M  X  M from M to K dimensions and V M  X  K is a reduced matrix of V M  X  M .

In the next section, we show our two approaches that deploy the SVD tech-nique to construct lower-dimensional space for both features and labels for multi-label classification. As mentioned earlier, most of previous approaches were presented to handle ei-ther the problem of a curse of dimensionality in the feature space or the sparse-ness problem in the label space.

This work presents two alternative a pproaches to deal with these aforemen-tioned problems. In both approaches, the Singular Value Decomposition (SVD) is used to project both feature and labe l spaces into reduced spaces then a clas-sification method can be applied. In the classification phase, on the other hand, SVD is used to reconstruct the original higher-dimensional label space from the prediction result in the construct ed lower-dimensional label space. In this work, we propose two alternative methods, called Dependent Dual Space Reduction (DDSR) and Independent Dual Space Reduction (IDSR). To promote the dependency among features and labels, DDSR computes depen-dency matrix between features and labels then applies SVD to eliminate the less correlated data. These low er-dimensional matrices computed from SVD are used to project both feature and label spaces in to a common lower-dimensional space. While the DDSR approach retains only data with high correlated between fea-tures and labels, it neither considers dependency among features nor dependency among labels. As our second proposed method, the Independent Dual Space Re-duction (IDSR) uses the feature dependency matrix built from the feature space and label dependency matrix computed from the label space as two projection matrices. After that two independent SVDs are applied to these matrices and project feature space and label space to l ower-dimensional spaces. The predic-tion can be done on lower-dimensional spaces before transforming back to the original space. The next subsections describe DDSR and IDSR in order. 3.1 Dependent Dual Space Reduction (DDSR) As previously mentioned, it is possible to utilize the characteristic that the fea-ture space and the label space may have some dependency with each other. While it is possible to characterize a dependency among feature and label spaces, for example, cosine similarity, entropy and symmetric uncertainty, with limited space, we considered only covariance ma trix. In this work, both spaces can be simultaneously compressed by performin g SVD on the feature-label covariance, viewed as a dependency profile between features and labels. Equation (3) shows construction of a covariance matrix S M  X  L to represent a dependency between feature and label spaces.
 where X is the feature matrix, Y is the label matrix and E [  X  ] is an expected value of the matrix.
 Applying SVD, the covariance matrix S M  X  L is later decompos ed to matrices U ,  X  and V . To retain only significant dimensions and reduce noise, the first K (  X  min ( M,L )) dimensions from matrices U and V are selected as U M  X  K and V M  X  K . Here, a lower-dimensi onal feature matrix X , can be created as X Y can be computed as Y N  X  K = Y N  X  L  X  V L  X  K . These tasks constitute the pre-processing phase.

In the next step, these two lower-dimensional matrices, X and Y ,areusedfor building a classification model. Among existing methods on multi-label classifi-cation, Binary Relevance (BR) is a simple approach and widely used. BR simply reduces the multi-label classification task to a set of binary classifications and then builds a classification model for each class. However, in this approach, the projected label matrix Y contains numeric values rather than discrete classes. By this situation, a regression method can be applied to estimate these numeric values. While the projected label matrix Y has K dimensions, it is possible to construct a regression model for each dimension. That is, K regression models are constructed for K lower-dimensions. Moreover, each model, later denoted by r ( X ) is a regression model built for predicting each column Y [ k ]usingthe matrix X . While the regression model returns continuous values, we propose a method to find the optimal threshold for mapping a continuous value to binary decision (0 or 1) as described in Section 3.3.

In the classification phase, firstly a test feature vector  X  X is transformed to the lower-dimensional feature vector  X  X using  X  X 1  X  K =  X  X 1  X  M  X  U M  X  K . Then this vector is fed to a series of regression models r (  X  X ) to estimate the numeric value in each dimension of the predicte d lower-dimensi onal label vector  X  Y 1  X  K [ k ]. After that, a matrix V T , an orthogonal matrix of the matrix V , is multiplied to reconstruct the lower-dimensional label vector  X  Y 1  X  K back to the higher-dimensional label vector  X  Y 1  X  L . Next the predicted values in the label vector  X  Y 1  X  L are rounded to the value in predicted multiple labels is the union of the dimensions which have the value of 1. 3.2 Independent Dual Space Reduction (IDSR) As opposed to the former approach, the Independent Dual Space Reduction (IDSR) approach presents how to use two independent SVDs for transforming the feature space and label space into the two lower-dimensional spaces similar to DDSR even there are several possibilities of dependency calculation. In this work, to consider the dependency in the feature space, the covariance matrix S
M  X  M is computed from the feature matrix X N  X  M . On the other hand, the dependency among labels in the label space can be derived by calculating the covariance matrix R L  X  L .
 In the pre-processing phase of this approach, a feature dependency matrix S
M  X  M is built from a feature matrix X N  X  M and then it is decomposed to three matrices U x ,  X  x and V x and select the top D dimensions from the matrix U x . Then the lower-dimensional feature matrix X can be constructed by X N  X  D = X label matrix Y . Likewise, this matrix is d ecomposed to three matrices U y ,  X  y and V y and the top K dimensions are selected from the matrix U y .Thelower-dimensional label matrix Y canbeformulatedby Y N  X  K = Y N  X  L  X  U y While the original label matrix Y N  X  L contains either 0 or 1 as its members, its lower-dimensional label matrix Y N  X  K may include non-binary numeric values. Moreover, it is not necessary that the dimension of the lower-dimensional feature space D and that of the lower-dimensional label space K are identical. Note that this condition is not the same with the DDSR approach, where D always equals to K .Afterthat,asthe model training phase, we can construct K regression models to predict Y N  X  K from X N  X  D . Note that each regression model is for each of K dimensions of Y . To transform a numeric value to a binary value a threshold is established. Section 3.3 describes our proposed method for searching the best threshold for each label. This step is done in the model training phase.
In the classification phase, the feature vector  X  X of an unseen object will be reduced to the lower-dim ensional feature vector  X  X using  X  X 1  X  D =  X  X 1  X  M  X  U x M  X  D . Then the regression models estimate the numeric value in the lower-dimensional label vector  X  Y based on the feature vector  X  X .Nextthematrix U inal higher-dimensional label vector  X  Y from the prediction result in the lower-dimensional label vector  X  Y i.e.,  X  Y N  X  L =  X  Y N  X  K  X  U y T an unseen object, the prediction values in the label vector  X  Y need to be rounded to { 0,1 } . At this point, the threshold found in the model training phase can be applied. Finally, the assigned label set is the union set of the dimensions that have the value of 1.
 3.3 Threshold Selection As stated above, by the orthogonal property of SVD, it can be used to reconstruct an original label space from a lower-dimensional label space. As the result, the reconstructed label vector may include non-binary values. To interpret the values as binary decision, a threshold need to be set to map these values to either 0 or 1 for representing whether the object belongs to the class or not. As a naive approach, the fixed value of 0.5 is used to assign 0 if the value is less than 0.5, otherwise 1 [11]. As a more efficient method, it is possible to apply an adaptive threshold. In this work, we propose a method to determine an optimal threshold by selecting the value that max imizes classification accuracy in the training dataset that is similar to the mechanism in Han et al [6]. In other words, the threshold selection is done by first sorting prediction values in each label dimension in a descending orde r and examine performance (e.g., macro F-measure ) for each rank position from the top to the bottom to find the point that maximize the performance. Then, the threshold for binary decision is set basedonthatpoint. To evaluate the performance of our two proposed approaches, the benchmark multi-label datasets are downloaded from MULAN 1 . Table 1 shows the char-acteristics of seven multi-label datasets. For each dataset, N , M and L denote the total number of objects, the number of features and the number of labels, respectively. L C represents the label cardinality , the average number of labels per example and L D stands for label density , the normalized value of label cardinality as introduced by Read et al [9].

Since each object in the dataset can be associated with multiple labels simul-taneously, the traditional evaluation metric of single-label classification could not be applied. The well-known multi-label evaluation metrics are of two types [9]. As the first type, a label-based metric evaluates each label separately such as hamming loss and macro F-measure . As the second type, a label set-based metric considers a set of labels simultaneously, i.e., accuracy and 0/1 loss .In this work, hamming loss , macro F-measure , accuracy and 0/1 loss are used to assess the effectiveness of the multi-label classification methods. Their detailed descriptions can be found in several literatures such as those in Read et al [9].
In this work, DDSR and IDSR are compared with four multi-label classifica-tion techniques; BR, BR+ [3], CC [9] and PLST [11]. BR+ (Binary Relevance with label dependency consideration) and CC (Classifier Chains) are two well-known methods, which incorporate label dependency in multi-label classification. PLST (Principle Label Space Transformation) is an efficient algorithm that uses the reduction of label space dimension. Using ten-fold cross validation method, the results of the four evaluation met rics and the execution time are recorded and shown in Table 2 and 3, respectively. All multi-label classification methods used in this work is implemented i n R environment version 2.11.1 2 and linear regression is used for the regression model in the model training phase. For PLST and IDSR method, we experiments with K ranging from 20% to 100% of the dimension of the original label matrix, with 20% as interval. Likewise, the pa-rameter D is also varied from 20% to 100% of the dimension of the original feature matrix, with 20% as interval. Though, the K parameter in DDSR ap-proach is calculated from the minimum value between a number of features and labels, this parameter is also used the same criteria as PLST and IDSR method. To compare the computational time, all methods were performed on the AMD Opteron Quad Core 8356 1.1 GHz Processor with 512 KB of cache, 64GB RAM. To evaluate our proposed approaches, seven datasets are used to compare per-formance of BR, BR+, CC, PLST, DDSR and IDSR. Table 2 reports the best value for each evaluation metric computed from all datasets. The numbers in the superscript (x,y) represents the pe rcentages of dimensions reduced in the feature space and that in the label space, respectively. In the DDSR method, the maximum number of reduced dimensions K cannot excess the minimum be-tween the number of features ( M ) and the number of labels ( L ) since the reduced dimension has the same size of both feature and label space. The superscript [y] in the PLST approach means the percent age of reduce labels, compared to the original. Note that PLST does not reduce the feature space. In the Table 2, the best value for each row is emphasized by bold font.

From the table, we can make some observations as follows. First, we observe that both DDSR and IDSR give comparable performance in terms of hamming loss , compared to BR, BR+, CC and PLST. On the other hand, DDSR and IDSR approaches gain an average gap of 16% macro F-measure increment. Moreover, the DDSR approach shows an average gap of 11% accuracy improvement while the IDSR approach improves with an average gap of 15%. Likewise, the DDSR approach can reduce the 0/1 loss with decrement of 5% while IDSR can reduce with 8% gap. Note that the medical dataset whose number of features are greater than the number of objects, gives the maximum improvement when both spaces are reduced.

As shown in Table 3, the execution time of the DDSR approach was reduced with a factor of 10, compared to PLST and approximately 100 times, com-pared to the traditional BR approach. Likewise, the IDSR approach with lower-dimensional features used less time than the PLST and the BR method. We can conclude that our two proposed methods, DDSR and IDSR, could transform the feature and label spaces into the reduced spaces with less computational time than the traditional BR, BR+, CC and PLST. As an additional observation, IDSR is better than DDSR in several datasets while DDSR can be executed faster than IDSR. The smaller K and D are, the faster we can compute.
In more details, Table 4 presents the complexity of learning process. However, the time used for the transformation process and covariance calculation is trivial. The N , M and L denote the number of objects, feat ures and labels, respectively. For our two approaches, D and K are the reduced number of dimensions. The f ( X,Y ) is the complexity of the model that depends on the number of objects ( X ) and the number of features ( Y ). When linear regression is applied for the model training phase, it requires O (4 XY 2 + X 3 +2 XY )and O ( Y )forthe clas-sification phase. We can observe that the BR+ method is recognized as the slowest algorithm since it appends labels to the feature space for incorporating label dependency and it requires two learning process, initial prediction step and final prediction step, to complete the classification process. On the other hand, our DDSR approach is the fastest method because the feature and label spaces are transformed to the lower-dimensional space before classification technique is applied. This paper presents two alternative approaches to handle the curse of dimen-sionality problem in the feature space a s well as the sparseness problem in the label space. The Dependent Dual Dimensionality Reduction (DDSR) considers the dependency between feature and label spaces before transforming the feature and label spaces into a single reduced space. On the other hand, the Independent Dual Space Reduction (IDSR) approach transforms the feature space and label space into the two lower-dimensionalit y spaces. Experiments with a broad range of multi-label datasets show that our two proposed approaches achieve a better performance, compared to PLST and B R, as well as other recent methods such as Classifier Chains (CC) and BRplus (BR+). In addition, the DDSR approach helps saving computational time while the IDSR approach tends to obtain better classification performance As our future work, we will analyze three dependen-cies, feature-label, feature-feature, a nd label-label, in detail. The ensemble of these dependencies may help in improving the performance.
 Acknowledgement. This work has been supported by the TRF Royal Golden Jubilee Ph.D. Program [PHD/0304/2551] and partially supported by the Na-tional Research University Project of Thailand Office of Higher Education Com-mission as well as the National Electronics and Computer Technology Center (NECTEC) under Project Number NT-B-22-KE-38-54-01.

