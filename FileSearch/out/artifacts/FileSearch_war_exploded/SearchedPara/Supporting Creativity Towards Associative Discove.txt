 Data collection and generation methods continue to increase their ability to fill up information repositories at an alarming rate. In many industries it is nowadays commonly accepted  X  although often not op enly admitted  X  that only a fraction of available information is taken into account when making decisions or trying to uncover interesting, potentially crucial links between previously unconnected pieces of information.

In order to allow users to be able to find important pieces of information it is necessary to replace classical question answering systems with tools that allow for the interactive exploration of potentially related information  X  which can often trigger new insights and spark new ideas which the user did not ex-pect at start and was therefore unable to formulate as a query initially. It is especially crucial for such systems to ena ble the seamless crossing of repository boundaries to trigger new discoveries across domains. Since we will not know at the start which types of information are needed or which kind of questions will be asked throughout this explorative process, the system always needs to be able to provide access to heterogeneous i nformation repositories. These can be structured, well annotated repositories, such as an ontology or a database of human annotations ( X  X nown facts X ) but it needs to incorporate other types of information as well, such as experimental data or the vast amounts of results from the mining of e.g. published texts ( X  X ieces of evidence X  ). The real chal-lenge lies in providing the user with eas y access to all of this information so that she can quickly discard uninteresting paths to information that is not currently relevant and at the same time focus on areas of interest. Similar to drill down operations in Visual Data Mining, such a system will need to be able to show summarizations according to different dim ensions or levels of detail and allow parallel changes of focus to enable the user to ultimately navigate to the infor-mation entities that explain the connections of interest. Of course, the system cannot be static but will require not only means for continuous updating of the underlying information repositories to accommodate new data, but also new and better methods to extract connections. In [1] we have argued that such a system will truly support the discovery of new insights. Related work investigating the nature of creativity (see [2] among othe rs) describes similar requirements for creative discoveries, based on broad but at the same time context dependent more focused exploration of associations as the underlying backbone.
In this paper we outline an approach to realize such a system using a network-based model to continuously integrate and update heterogeneous information repositories and at the same time allow f or explorative acces s to navigate both semantic and evidential links. Before describing our prototypical system in more detail we review existing network-based systems for knowledge or information modeling. We conclude the paper by discussing open issues and challenges. Different network-based models have been applied to Information Retrieval, such as artificial neural networks, probabilistic inference networks, Hopfield or knowl-edge networks [3]. The first two are mainly used to match documents to queries and to find relevant documents related t o a certain query. Documents and index terms, which are the most discriminativ e terms, are represented as vertices in these networks. Edges can be created to c onnect documents citing each other, documents with their index terms, as well as cooccurring index terms. Hopfield and knowledge networks are additionally used for automatic thesaurus creation and consultation [4]. In this case only v ertices of index terms cooccurring in doc-uments or sentences are connected via e dges. Another connectionist approach, Adaptive Information Retrieval (AIR), creates additional vertices for each doc-ument author and connects them by their author co-author relationships [5,6].
The majority of these approaches use weighted networks. In these networks a weight is assigned to each edge, which depends on the underlying network model as well as the computation and interpretation of the relation. In probabilistic inference networks the weights represent probabilities of terms occurring in doc-uments being relevant to a certain query [ 3,7]. Whereas the weights of knowledge or Hopfield networks as discussed in [4] represent the relatedness of cooccurring terms. Usually the weights of these approaches are only computed once and not changed afterwards. In contrast to thes e approaches, Belew enables each user of an AIR model to adapt the weights according to their relevance feedback [5]. After initialization of the weights where the edges between documents and terms are weighted with the term X  X  inverse doc ument frequency, a user can send queries to the network. The user then rates the resu lting nodes, representing terms, doc-uments or authors, as relevant or irrelevant. This relevance feedback is passed to the network again in order to adjust the edge weight and process another query. This kind of iterative process is continued until the result fits the users needs. One essential disadvantage of such an adaptive system is that it adapts to the user X  X  opinion of which documents are more relevant than others related to a certain query. This means that the network will, over time, be strongly biased by the opinion of the majority of the users.

In a number of other domains, networks have been applied to combine, rep-resent, integrate and analyze information, such as bioinformatics and life sci-ence, with a strong emphasis on the extraction of pharmacological targets [8], protein functions [9], gene-gene [10], gen e-protein [11] or protein-protein inter-actions [12,13] from different biological databases and biomedical literature [14]. To mine texts and find this kind of interaction Blaschke et al. [12] proposed to parse the sentences into gra mmatical units. Patterns or regular expressions have been used as well to extract genes, protein s and their relations in texts [10,13].
Once the units of information and their relations are found, they can be represented in a network. Additional algorithms can be used to cluster and analyze these networks in order to identify meaningful subnetworks (commu-nities) [15,13]. The analysis of network structures also reveals new insights into complex processes such as regulator strategies in yeast cells [16]. Additionally the edges can be evaluated and their qu ality can be specified based on several features like edge reliability, relevance and rarity [17]. Note that also the increas-ingly popular social networks fall into this category. In general much work has been done when it comes to methods for network analysis [18]. 2.1 Adaptive and Expl orative Approaches To visually analyze graphs, different layou t algorithms such as the force-directed Fruchterman-Reingold algorithm [19] have been developed. But large networks with several million vertices and many more edges cannot be visualized com-pletely in a reasonable manner. Therefore the visualization has to be focused on a subgraph or at least summarized to match the current user X  X  interest or give an overview. Various visualization tec hniques have been developed to address this problem. Examples are the generalized Fisheye views [20], the splitting of a network into several smaller semantical d istinct regions [21] or the interactive navigation through different levels of abstractions [22].

Another way to analyze large networks is to extract subgraphs that contain most of the relevant information. One way to do this is to query a graph. On the one hand queries can be generated by manually drawing a sub-graph or by using a particular query language, i.e. GenoLink [23]. The results of such queries are represented as sub-graphs which themselves could be the starting point of further analyses. On the other hand Spreading Activation techniques are very common techniques to explore networks and handle queries [24]. In general the idea of activity spreading is based on assumed mechanisms of human cognitive mem-ory operations, originated from psychological studies [25]. These techniques are adopted to many different areas such as Cognitive Science, Databases, Artificial Intelligence, Psychology, Biology and Information Retrieval. The basic activity spreading technique is quite simple. First, one or more vertices, representing the query terms, are activated. The initial activation is distributed (spread) over the outgoing edges and activates in subsequent iterations the adjacent vertices. This iterative process will continue until a certain termination condition, such as a maximum number of activated nodes or iterations or a minimum edge or vertex weight is reached. The activation itself can also be weighted and can de-crease over time or when propagating over certain edges. Furthermore different activation functions can be used for the vertices [24]. In [4] the networks are explored by usage of a branch-and-bound search and a Hopfield net activation. Due to the restriction that a Hopfield activation algorithm only guarantees to converge if the graph X  X  adj acency matrix is symmetric, meaning that the graph is undirected, this technique is only applicable for certain kinds of networks. Other approaches cope with the complexity by clustering or pruning the graph based on their topology [26] or based on additional information such as a given ontology [27]. 2.2 Combining Heterogeneous Information Repositories The integration of heterogeneous data sources facilitates insights across different domains. Such insights are important especially in complex application areas such as life sciences, which deal with different kinds of data, e.g. gene expression experiments, gene ontologies, scientific literature, expert notes, etc. During the last few years several approaches have been developed that attempt to tackle this problem. The authors of [28] classified these systems into three general classes: navigational integration, mediator-based integration and warehouse integration.
Navigational integration approaches like SRS [29], Entrez [30] and LinkDB [20] aim to integrate heterogeneous data by providing links between units of infor-mation derived from different sources. Links can be created based on database entries as well as on the similarity of the units of information, or manually by experts [20]. Most of the applications consist of one or more indexed flat files containing the relations between the different concepts.

The second category is the mediator-based integration systems such as Discov-eryLink [31], BioMediator [32], Kleisli [33] and its derivatives like TAMBIS [34] or K2 [35]. These systems act as a mediator, which maps the schema of different data sources onto a unified schema. Each query is converted and split up into a set of sub-queries, which are than redi rected to the wrapper of the integrated data source. Finally the results of the sub-queries are combined to a single result and returned by the mediator.

Warehouse approaches like GUS [35], Atlas [36], BIOZON [37] and BNDB [38] are similar to the mediator-based approach since they also provide a unified schema for all data sources. But instead of creating a sub-query for each data source the data itself is loaded into the unified schema.

Navigational integration and mediator-based approaches do not integrate all the detailed data of a concept. The amount and complexity to handle additional data is much smaller in comparison to systems that integrate the detailed infor-mation of a concept like the warehouse approach. The advantage of this kind of light integration is the ability to keep the detailed information up to date since it is stored in the external sources itself. The drawback of such an integration is the dependency on all the integrated systems with respect to reliability and performance. In contrast, the warehouse approach also integrates all the detailed information from the distributed repositories. The data can be preprocessed and enriched with additional information such as similarity measures or user anno-tations. The local storage of all data leads to a better performance and system reliability. However the huge amount of data itself and continued maintenance to detect changes and inconsitencies are the major drawback of such systems.
In summary, warehouse and mediator-based approaches provide the user with a unified, mostly relational schema. This allows professional users the ability to use powerful query languages like SQL to perform complex joins and queries. The unification leads mostly to a complex data model including link tables to combine the different data sources. Navigational approaches only maintain link information between concepts and provi de simple point and click interfaces vi-sualizing links between them. These interfaces are also manageable by semi pro-fessional users but restricted in their query capabilities like the lack of complex joins. A common goal of all the mentioned integration approaches is the com-bination of equal or similar concepts from different data sources. An obvious approach to link these concepts is the usage of a flexible graph structure. An example of integrating high confidence biological data is PathSys [39]. PathSys is a graph-based data warehouse, which is used to analyze relations between genes and proteins. To predict protein-protein interactions several approaches adopted Bayesian Networks to model the mostly noisy or uncorrelated evidences of bio-logical experiments [40,41]. As we have suggested above, simply finding classical associations is not sufficient to detect interesting connections across different information repositories and contexts. Existing systems either tend to be to application focussed or restricted to only a few type of information sources or types. However, in order to support creative discoveries across domains we cannot assume that we know from the be-ginning which information repositories will need to be combined in which way.
In 1964 Arthur Koestler introduced the term bisociation [42] to indicate the  X ...joining of unrelated, often conflicting information in a new way... X  .Using this terminology we use the term Bisociative Information Networks, or short BisoNets to denote a type of information network addressing the above concerns, fusing the following requirements:  X  Heterogeneous Information: BisoNets integrate information from various  X  Merging Evidence and Facts: BisoNets provide a unifying mechanism to  X  Continuous Update: BisoNets can be refined online and continuously inte- X  Exploration/Navigation: Finally, in order to allow access to the resulting in-There is strong evidence that such a co mplex system of loosely, not necessar-ily semantically coupled information granules exhibits surprisingly sophisticated features. In [43] Hecht-Nielsen describes a network which generates grammati-cally correct and semantically meaningful sentences purely based on links created from word co-occurrence without any additional syntactical or semantical anal-ysis. In addition, [2] discusses requirements for creativity, supporting this type of domain bridging bisociations. 3.1 First Steps: A BisoNet Prototype In order to evaluate the concept of BisoNets, we have implemented a first proto-type and so far have mainly applied it to life science related data. However, the toolkit is not restricted to this type of data. The BisoNet prototype creates one vertex for each arbitrary unit of information, i.e. a gene or protein name, a spe-cific molecule, an index term or a document, and other types of named entities. Relations between vertices are represe nted by edges. Vertices are identified by their unique name and edges by the vert ices they connect. In order to model not only facts but also more or less precise pieces of evidence, edges are weighted to reflect the degree of certainty and specificity of the relation.

Due to the uniqueness of a vertex name, a vertex can be ambiguous and represent different units of informatio n, i.e. a vertex can represent a term ex-tracted from a document and a gene or protein name derived from a certain database. For example a vertex could represent the animal  X  X aguar X  or the make of car. To distinguish the different kinds of meanings, an annotation can be ap-plied to vertices and edges. An annotation specifies the origin and the type of the information unit. A vertex representing different units of information will contain different annotations: one annotation for each meaning. Edges with dif-ferent annotations represent relations derived from different data sources. Each annotation of an edge contains its own weight in order to specify the evidence of the relation according to the da ta sources it was derived from.

The structure of the knowledge network is rather lightweight, that is it simply consists of vertices and edges, but contains no detailed information of the vertices or edges itself. In order to access this val uable, more detailed i nformation as well, so-called data agents have been implemented. For each annotation, representing a particular kind of information of a certain data source, a data agent is available, which can be used to access the corresp onding data source and extract the detailed information for a particular vertex or edge annotation.

To analyze and explore the network in order to find new and hopefully use-ful information, potentially uninteresting information has to be filtered. The prototype provides several filtering methods. One method allows particular an-notation types of vertices and edges to be hidden, such as terms, species, or chemical compounds to focus on a specific context. Another one filters edges by their weight to filter out all relation s below a certain degree of evidence. To extract information related to a particular issue, an activity spreading algorithm has been implemented, similar to the branch-and-bound algorithm of [4], which is able to extract subgraphs consisting of the most relevant vertices related to a specified set of initially activated vertices.

We implemented the BisoNet prototype within the modular information min-ing platform KNIME [44] due to the large set of data preprocessing and analysis methods available already. Each procedure and algorithm dealing with the net-work was implemented as a module or KNIME node respectively. This allows them to be used and combined individually and networks can be created, an-alyzed and explored in a flexible manner. Figure 1 shows an example KNIME workflow in which a network was created consisting of PubMed [45] abstracts as text data, gene subgroup information derived from gene expression data, gene-gene interaction data from Genetwork [46] and Gene Ontology [47] information. One by one all data sources are integrated into the network and at the end of the pipeline various filters can be applied to concentrate on a particular subgraph.
To visualize the network we used Cytoscape [48] an open source software platform for graph visualization. Note that this graph visualization toolkit does not offer sophisticated means to navigate the underlying BisoNet.

To create the complete network PubMed ab stracts, related to the drug Plavix, treating thrombotic events, were analy zed and all content bearing index terms, gene and compound names were extracted and inserted into the network as vertices. Co-occurring terms above a cer tain frequency are connected by an edge. In addition gene-gene interaction data of Genetwork was integrated and, by applying different filters such as gene annotation filter or edge weight filter, the subgraph shown in Figure 2 can be extra cted. The graph consists of 27 vertices representing gene names and 33 edges representing gene-gene interactions. The green vertices stem from the Genetwork data, the brown vertices from PubMed text data. In the subgraph illustrated in Figure 2 the four genes derived from text data connect and supplement the gene subgraphs of the Genetwork data nicely. Note how connections between subgraphs based on one data source are connected by information d erived from a second source.
 3.2 Open Issues and Challenges The BisoNet prototype as described above is a first attempt at implementing the concepts listed in Section 1. Many open i ssues and challenges are still awaiting solutions and usable realizations. Within the EU Project  X  X ISON X  many of these challenges will be tackled over the coming years, focussing among others on issues related to:  X  Scalability: addressing problems related to the increasing size of the resulting  X  Weight and Network Aggregation: that is, issues related to information  X  Graph Abstraction: relating to methods that are especially crucial for prob- X  Disambiguation: that is, the differentiation of named entities with different Without doubt, many other issues will be encountered along the way and soon cognitive issues will also become increasingly important, i.e., developing interfaces that are adopted to the way humans think and work and therefore truly support human creativity instead of asking the user to adopt to the way the system has been designed. In this paper we have outlined a new approach to support associative information access, enabling the user to find links across different information repositories and contexts. The underlying network combines pieces of information of vari-ous degrees of precision and reliability and allows for the exploration of both connections and original information fragments. We believe these types of biso-ciative information networks are a promising basis for the interactive exploration of loosely connected, semi-or unstructured information repositories, ultimately leading to fully fledged Discovery Support Systems.
 Acknowledgements. We would like to thank the members of the European Frame-work 7 project BISON for many stimulating discussions, which have helped to refine the concept of BisoNets.

