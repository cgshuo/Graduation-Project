 To establish an automatic conversation system between humans and computers is regarded as one of the most hardcore problems in computer science, which involves interdisciplinary techniques in information retrieval, natural language processing, artificial intelli-gence, etc. The challenges lie in how to respond so as to maintain a relevant and continuous conversation with humans. Along with the prosperity of Web 2.0, we are now able to collect extremely massive conversational data, which are publicly available. It casts a great opportunity to launch automatic conversation systems. Ow-ing to the diversity of Web resources, a retrieval-based conversa-tion system will be able to find at least some responses from the massive repository for any user inputs. Given a human issued mes-sage, i.e., query, our system would provide a reply after adequate training and learning of how to respond. In this paper, we propose a retrieval-based conversation system with the deep learning-to-respond schema through a deep neural network framework driven by web data. The proposed model is general and unified for dif-ferent conversation scenarios in open domain. We incorporate the impact of multiple data inputs, and formulate various features and factors with optimization into the deep learning framework. In the experiments, we investigate the effectiveness of the proposed deep neural network structures with better combinations of all differen-t evidence. We demonstrate significant performance improvement against a series of standard and state-of-art baselines in terms of p@1, MAP, nDCG, and MRR for conversational purposes.
 H.3.3 [ Information Systems ]: Information Search and Retrieval; H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing; I.5.1 [ Pattern Recognition ]: Models X  Deep learn-ing Learning-to-respond; conversation system; contextual modeling; deep neural networks
To have a virtual assistant and/or chat companion system in open domains with adequate artificial intelligence has seemed illusive, and might only exist in Sci-Fi movies for a long time. Recently, the goal of creating an automatic human-computer conversation sys-tem, as our personal assistant or chat companion, is no longer an illusion far away. Due to easily accessible  X  X ig data X  for conver-sations on the Web, we might be able to learn how to respond and what to respond given (almost) any inputs. It is likely to be a great timing to build data-driven, open-domain conversation systems be-tween humans and computers.

Building conversation systems, in fact, has attracted much atten-tion over the past decades. In early years, researchers have inves-tigated into task-oriented conversation systems [44, 33, 36], which are basically for vertical domains. The conversational inputs are re-stricted and predictable; hence it would be easier X  X ompared with open-domain systems X  X o design the logic, create the rules, prepare the data and construct the candidate replies to handle the particular task [23]. For instance, in a conversation system for flight booking or bus route inquiring, the computer side only needs to capture the origin, destination and flight/bus information, and then respond ac-cordingly with templates [44]. One of the most obvious limitation of task-specific service is that the conversation cannot exceed the system topic scope. Illegible inputs will not be accepted, which is regarded as a hard constraint. The underlying system design phi-losophy is nearly impossible to generalize to the open domain.
It is only recently that researchers focus on non-task-oriented (i.e., open-domain) conversation systems for their functional, so-cial, and entertainment roles in real-world applications [2, 26, 9, 35, 17, 31, 6]. Creating an open-domain conversation system to interact with humans is an interesting but notoriously challenging problem. Since people are free to say anything to the system, it is impossible to prepare the interaction logic and domain knowledge, which can be, in contrast, specified in task-specific systems before hand. Besides, the number of possible combinations of conversa-tion status are literally infinite, so that conventional hand-crafted rules and templates would fail beyond any doubt [34].

Along with the maturity of Web 2.0, there has been an explo-sion in the number of people having public conversations on web-sites such as Bulletin Board System (BBS) forums, social medi-a (e.g., Facebook 1 , Twitter 2 ) and community question answering (cQA) platforms (e.g., Baidu Zhidao 3 , Yahoo! Answers 4 ). These resources provide a unique opportunity to build collections of nat-urally occurring conversations that are orders of magnitude larger http://www .facebook.com http://www.twitter.com http://www.zhidao.baidu.com https://answers.yahoo.com than those previously available. They also propel the development of retrieval-based techniques in the field of open-domain conversa-tion research. The merit is that, owing to the diversity on the Web, the system will be able to retrieve at least some responses for any user input, and return a sensible response.
 The big data era, however, seems like a double-edged sword. On one side, it brings the great opportunity, as mentioned above, to build practical human-computer conversation systems in open domain. On the other side, there are also challenges. Given a user-issued query, we ought to identify appropriate candidate replies from a very large volume of data. Besides, the proposed model should also be general and unified for different conversation sce-narios. In a conversation system, usually there is additional infor-mation to use such as  X  X ontexts X  (a.k.a. previous utterance sen-tences in a continuous conversation session). Therefore, capturing and integrating as much information as possible in a proper way is important for conversation systems.

In this paper, we propose a  X  X eep learning-to-respond X  frame-work for open-domain conversation systems. We create a huge conversational dataset from Web, and the crawled data are stored as an atomic unit of natural conversations: an utterance, namely a posting , and its reply . Each  X  posting -reply  X  can be regarded as a single-turn conversation. For a given query, we first apply tradition-al keyword-based retrieval methods and obtain a list of candidate replies; each reply is associated with its antecedent posting. We then enhance the current query by adding its contexts, i.e., one or more previous utterances in the current conversation session. Thus, we obtain a set of reformulated queries as well as the original query. A deep neural network (DNN)-based ranker thereafter tells how each candidate reply/post is related to a (reformulated) query, and yields a ranking list for each (reformulated) query. We merge the ranking lists corresponding to different reformulations. In this way, we are able to organically incorporate into the conversation system multi-dimension of ranking evidences including queries, contexts, candidate postings and/or replies, which is a novel insight.
The proposed reformulation approach and merging strategy pro-vide a new means of conversation modeling, especially multi-turn conversations. By using previous utterances, we are aware of back-ground information of the query, which might be informative. More-over, different reformulations can capture different aspects of back-ground information; their resulting ranked lists are further merged by a novel formula, in which we consider the relatedness between the reformulated queries (with context) and the original one.
The DNN ranker, serving as the core of  X  X eep learning-to-rank X  schema, models the relation between two sentences (query versus context / posting / reply). We use a bi-directional recurrent neural network to propagate information across words; a convolutional neural network layer further captures patterns of adjacent words. Then a matching layer combines the information in each individual sentence. Note that our DNN is a generic framework and applies to Query-Reply , Query-Posting and Query-Context in a unified way.
We conduct extensive experiments in a variety of conversation setups between humans and computers. In particular, we build the system upon an extremely large conversation resource, i.e., al-most 10 million pairs of human conversation resources. We run experiments against several other rival algorithms to verify the ef-fectiveness of the proposed DNN model. Our system outperforms standard and state-of-the-art baselines regarding a variety of eval-uation metrics in terms of p@1, MAP, nDCG and MRR metrics. The result indicates that our conversation system is rather helpful to facilitate conversations between human and computer.
To sum up, our contributions are mainly as follows:
The rest of the paper is organized as follows. We start by review-ing related work. In Section 3, we describe the task modeling and proposed framework for conversation systems. In Sections 4 and 5, we introduce the detailed mechanisms of contextual query refor-mulation and the deep learning-to-respond architecture. We devise experimental setups and evaluations against a variety of baselines and discuss results in Section 6. Finally we draw conclusions in Section 7.
Early work on conversation systems is generally based on rules or templates and is designed for specific domains [33, 36]. These rule-based approaches requires no data or little data for training, while instead require much manual effort to build the model, or to handcraft rules, which is usually very costly. The conversation structure and status tracking in vertical domains are more feasible to learn and infer [44]. However, the coverage of such systems are also far from satisfaction. Later, people begin to pay more attention to automatic conversation systems in open domains [31, 6].
From specific domains to open domain, the need for a huge amount of data is increasing substantially to build a conversation system. As information retrieval techniques are developing fast, researchers obtain promising achievements in (deep) question and answering systems. In this way, an alternative approach is to build a conversation system with a knowledge base consisting of a num-ber of question-answer pairs. Leuski et al. build systems to s-elect the most suitable response to the current message from the question-answer pairs using a statistical language model in cross-lingual information retrieval [12], but have a major bottleneck of the creation of the knowledge base (i.e., question-answer pairs) [13]. Researchers propose to augment the knowledge base with question-answer pairs derived from plain texts [24, 3]. The num-ber of resource pairs can be, to some extent, expanded, but are still relatively small while the performance is not quite stable either.
Nowadays, with the prosperity of social media and other Web 2.0 resources, such as community question and answering (cQA) or microblogging services, a very large amount of conversation da-ta become available [35]. A series of information retrieval-based methods are applied to short text conversation using microblog da-ta [9, 14, 17, 16]. Higashinaka et al. also combine template gen-eration with the search-based methods [6]. Ritter et al. have in-vestigated the feasibility of conducting short text conversation by using statistical machine translation (SMT) techniques, as well as millions of naturally occurring conversation data in Twitter [26]. In the approach, a response is generated from a model, not retrieved from a repository, and thus it cannot be guaranteed to be a legiti-mate natural language text.
Unlik e previous work, we conduct a novel study of retrieval-based automatic conversation systems with a deep learning-to-respond schema via deep learning paradigm. We formulate the possible factors into a deep neural network architecture and further investigate the potential of combining different ranking evidences for the candidate responses. Deep learning structures are well for-mulated to describe instinct semantic representations.
In recent years, deep neural networks (DNNs, also known as deep learning ) have made significant improvement in NLP [11]. DNNs are highly automated learning machines; they can extrac-t underlying abstract features of data automatically by exploring multiple layers of non-linear transformation [1].

In NLP models, a word typically acts as an atomic unit. Howev-er, words are discrete by nature; it seems nonsensical to feed word indexes to DNNs. A typical approach is to map a discrete word to a dense, low-dimensional, real-valued vector, called an embedding [19]. Each dimension in the vector captures some (anonymous) aspect of underlying word meanings.

Prevailing DNNs for sentence-level modeling include convolu-tional neural networks (CNNs) and recurrent neural networks (RNNs). In CNNs, we have a fixed-size sliding window to capture local pat-terns of successive words [10], whereas RNNs keep one or a few hidden states, and collect information along the word sequence in an iterative fashion [38, 37, 39]. Socher et al. leverage sentence parse trees and build recursive networks [30]. Mou et al. [21, 20] propose syntax-aware convolution based on parse trees. Howev-er, conversational utterances are usually casual, and hence recur-sive models are less applicable in conversation systems. We prefer structure-insensitive models like CNNs and RNNs.

Beyond a single sentence, some studies are aimed to capture the relationship between two sentences X  X nown as sentence pair modeling X  X ith applications like paraphrase detection [5], discourse unit recognition [45], textual entailment recognition [27], etc. A sentence-pair DNN model is typically built upon underlying sentence-level models (CNNs / RNNs). Then two sentences X  information is combined by matching heuristics like concatenation, cosine mea-sure, or inner-product [5, 28]. Hu et al. develop word-by-word matching approaches [7], and obtain a similarity matrix between two sentences. Very recently, Rockt X schel et al. propose context-aware matching approaches [27], where the first sentence X  X  infor-mation is available when modeling the second one. Such context-awareness interweaves individual sentence modeling and sentence matching, prohibiting pre-calculating the vector representation of a sentence; hence these methods are considerably more computa-tional intensive, especially with multiple query reformulations in our scenario. For efficiency consideration, we leverage vector con-catenation, which is simple yet effective.

Although the studies of sentence-pair modeling described above are similar to our DNN to some extent, the proposed learning-to-rank model is more than traditional ranking or matching. We have multiple query reformulations with  X  X ontexts X . After computing the similarity between the query and reply/post/context, our DNN further merges the ranking results corresponding to different refor-mulated queries.
In this section, we provide a big picture of the proposed learning-to-respond schema for conversation systems. We illustrate the task modeling for conversations, and establish the pipeline with sever-al processing procedures including data collection, search and re-trieval, contextual query reformulation, DNN-based ranking with Table 1: An example of the original microblog posting and the associated replies . Each posting might have more than one re-ply, e.g., Reply 1 and Reply 2 . To create our database of conver-sation data, we separate different replies to a same post, and ob-tain  X  post-reply  X  pairs. We store two Posting-Reply pairs in the conversational dataset, i.e.,  X  P osting -Reply 1  X  and  X  Reply 2  X  . User accounts are anonymized.
 T able 2: Part (I) indicates a real human (denoted by A ) -com-puter (denoted by B ) conversation scenario, while Part (II) in-dicates our proposed task modeling and formulations. A 2 is the current user-issued query. We have contexts and reformulated queries as listed.  X   X  is the literal concatenation action. Note that the selected response Reply 1 is associated with a P osting in the conversational database shown in Table 1. scoring, and ranked list fusion. We briefly go over through the pipeline and elaborate the details in the next section.
Data collection. With the prosperity of Web 2.0, people inter-actively communicate with each other on the Web, which provides a huge thesaurus for conversation data. We collect a large amount of data samples from social media such as microblog websites, fo-rums, cQA bases, etc. Users can publish a posting message visible to the public, and then receive one or more replies or comments in response to their posting. The communication can have a single turn as well as multiple turns. We illustrate an example in Table 1. Due to the heterogeneity of the sources, we treat each utterance, in multi-turn conversations, with its subsequent one as a posting-reply pair (i.e., our database is context-free). For a posting with multi-ply replies, we separate them and construct different  X  p, r Table 1 shows the pre-processed samples in our dataset, applied to a real human-computer conversation illustrated in Table 2. In the sample shown in Table 1, the first message of a conversation is typi-cally unique. There are many flexible forms to  X  X espond X  to a given message, which is exactly the nature of real conversations: various responses are all possibly appropriate, with different aspects of in-formation to fulfill a conversation. We separate the posting and replies as a group of  X  posting -reply  X  pairs. The data repository is demonstrated to be a rich resource to facilitate human-computer conversations.

Search and retrieval. In the scenario of conversations, the user issues a query ( q 0 in Table 2), which may be a sentence or a few terms. We apply a standard retrieval process via keyword search using traditional tf.idf weighting schema [18] on the conversation data-base (formatted as an inverted index prepared off-line) based on the light-weight search provided by Baidu 5 . Note that we treat each pair of posting and reply as a short virtual document , which is not a traditional process. In this way, the retrieved  X  X irtual doc-ument X  comprises two parts: the candidate reply , namely r , along with its antecedent posting , namely p .

Contextual query reformulation. A single query may not fully convey user intentions in (multi-turn) conversations, as illustrated in Table 2. Under such conditions, we usually have context infor-mation to use. We propose a novel insight to model the conver-sation task. In particular, we view previous utterances from both sides as contexts , denoted as C = { c i } . One or more utterances in can be used to enhance q 0 so as to provide more information. We call this a contextual query reformulation process. Moreover, the context may comprise several sentences, and hence we have sever-al strategies to reformulate the original query. Each reformulated query is denoted as q i . More details will be given in Section 4.
DNN-based scoring, ranking, and ranked list fusion. We ap-ply a deep neural network (DNN)-based model to rank optimiza-tion. In particular, we design a bi-directional long short term mem-ory (LSTM) neural network to capture sentence-level semantic-s of a query q 0 , candidate reply and the associated posting, i.e.,  X  p, r  X  , as well as context C . A matching layer combines multi-dimensions of the sentence information, so that we know how can-didate replies with associated postings are related to the query and contexts. Analogous to the traditional Query-Document matching, the relevance ranking can be measured via Query-Reply matching, the additional Query-Posting matching, as well as Query-Context matching. Intuitively, when a query and a posting look similar, they might share the same response (Tables 1 X 2). For each (reformu-lated) query, DNN ranks candidates replies with relevance scores from Query-Query and Query-Posting . We further merge all can-didate ranked lists of all contextual reformulations using weighted fusion. The weight is controlled by the relevance between the orig-inal query and the reformulated one with contextual information, i.e., Query-Context . For continuous conversations, contexts can be used to optimize the response selection for the given query.
Table 3 summarizes the input and output of the proposed system with deep learning-to-respond schema. Given a query with context, the proposed model would return a response X  X hich has the high-est overall (merged) ranking score F ( . )  X  X rom the pre-constructed repository. We use DNN to assess the relevance between candidate replies, postings and reformulated queries with different combina-tions of contexts. The DNN also merges the ranking scores corre-sponding to different contextual query reformulations.
 We apply hinge loss with negative sampling to train the network. Gradients can be back-propagated all the way back from merging, ranking, sentence pairing, to individual sentence modeling. There-fore, all these heterogeneous ranking evidences are integrated to-gether through the proposed Deep Learning-to-Respond schema.
Generally, context information may be informative (but some-times may be not) when modeling a query. It is non-trivial to ex-plore different strategies to utilize context information for conver-sations. In this section, we describe the proposed contextual query reformulation approach.

The contextual query reformulation strategies are mainly inspired by the following observations:
Baidu is the largest Chinese search engine provider. Some of the services are available at http://www.baidu.com.

Table 3: Symbols and annotations for problem formulation. C = { c i } conte xts (utterances before q 0 ). c i is a utterance in
Q = { q i } reformulated query: q 0 concatenates with some c Input Query: q 0
Output Selected response: r = argmax
Without loss of generosity, let us assume there are N sentences in the context C , i.e., c 1 ,  X  X  X  , c N  X  C being previous utterances in the current conversation session. We add one or more context sentences to the query q 0 and obtain a set of reformulated queries (each is denoted as q i  X  X  ). To reduce the explosive number of all possible 2 N combinations, we restrict the contextual query refor-mulation strategies in practice as follows:  X   X  refers to literal concatenation, where the order of context and the query is preserved.  X  C \ c i  X  indicates c i is filtered out from The benefits for all the contextual query reformulation strategies (for Q Add-One and Q Drop-Out ) along as N grows. The intuition for Add-One and Drop-Out strategies is based on a finer-granularity: to incorporate relevant context sentences only, or to exempt one irrelevant context sentences. The last strategy combines all these strategies to approximate all possibilities.
In this section, we describe the deep model for query-reply rank-ing and merging. Our model first determines the score of a can-didate reply given the (reformulated) query, based on the candi-date reply and its associated posting (Subsection 5.1). Then the model merges the score by summing over all query reformulation-s including the original query with a gating (product) mechanism (Subsection 5.2). In this way, all heterogeneous ranking evidence is combined organically in a differentiable manner. As mentioned, our model first determines the relationship for Query-Reply , Query-Posting , and Query-Context matchings. The three scoring functions are defined as:
The scoring function f ( q, r ) outputs a scalar in R (appropriate-ness or inappropriateness) in for a particular candidate reply, while the latter two functions serving as an adjustment or  X  gate  X , which are squashed by a logistic function into the (0 , 1) range. Neverthe-less, all the above functions are computed by the same deep neural network architecture (except for the last activation function), but their parameters are different so that the three scoring functions can depict different meanings. In particular, the deep structure for sentence pair modeling includes the following components.
Traditional models usually treat a word as a discrete token; thus, the internal relation between similar words would be lost. Word embeddings [19] are a standard apparatus in neural network-based text processing. A word is mapped to a low dimensional, real-valued vector. This process, known as vectorization, captures some underlying meanings. Given enough data, usage, and context, word embeddings can make highly accurate guesses about the meaning of a particular word. Embeddings can equivalently be viewed that a word is first represented as a one-hot vector and multiplied by a look-up table [19].

In our model, we first vectorize all words using their embed-dings, which serve as the foundation of our deep neural networks. Word embeddings are initialized randomly, and then tuned during training as part of model parameters.
We use a bi-directional long short term memory (Bi-LSTM) re-current network to propagate information along the word sequence.
As reviewed in Section 2, a recurrent neural network (RNN) keeps a hidden state vector, which changes according to the input in each time step. As RNNs can iteratively aggregate information along a sequence, they are naturally suitable for sentence modeling.
LSTM is an advanced type of RNN by further using memory cells and gates to learn long term dependencies within a sequence [32, 25]. LSTM models are defined as follows: given a sequence of inputs, an LSTM associates each position with input , forget , and output gates , denoted as i t , f t , and o t respectively. The vector l is used to additively modify the memory contents. Given an input sentence S = { x 0 , x 1 , . . . , x T } , where x t is the word embedding at position t in the sentence. LSTM outputs a representation h position t , given by  X  where  X  h is an auxiliary variable and can be viewed as the infor-mation stored in memory cell.  X  (  X  ) = 1 1+ e is a known as a sigmoid/logistic function.

A single directional LSTM typically propagates information from the first word to the last; hence the hidden state at a certain step is dependent on its previous words only and blind of future word-s. The variant Bi-LSTM [4] is proposed to utilize both previous and future words by two separate RNNs, propagating forward and backward, and generating two independent hidden state vectors  X  X  X  h t and ed to represent the meaning of the t -th word in the sentence, i.e., h =
We further apply a convolutional neural network (CNN) to ex-tract local neighboring features of successive words X  X .e., discrim-inative word sequences can be detected X  X ielding a more compos-ite representation of the sentences. The structure of CNN in this work is similar to [10], shown in Figure 1. Unlike RNNs, CNNs only impose local interactions between successive words within a filter (size m ).

Concretely, we build a CNN upon the output of Bi-LSTM. For every window with the size of m in Bi-LSTM output vectors, i.e., ( H t ) m = [ h t , h t +1 ,  X  X  X  , h t + m 1 ] , where t is a certain position, the convolutional filter F = [ F (0) , . . . , F ( m  X  1)] will generate a vector sequence using the convolution operation  X   X   X  between the two vectors. More formally, the convolution results in a vector where each component is as follows:
In practice, we also add a scalar bias b to the result of convolu-tion. In this way, we obtain the vector o F is a vector, each dimen-sion corresponding to each word in the sentence.

Notice that the above equation describes a single  X  X lice X  of con-volution. In fact, we may have multiple feature filters and thus multiple feature maps. Different filters do not share parameters ( F and b ), so that they can capture different meanings. On the basis of sentence representations using Bi-LSTM with CNN, we can model the interactions between two sentences. We apply pooling to aggregate information along the word sequence. In particular, a max pooling layer chooses the maximum value in each dimension in the feature maps after the convolution, indicating how much the feature is most satisfied along the sequence.
We concatenate two individual sentences X  vector representations (with possible additional features), which are then fed to an ensuing network for further information mixing. Vector concatenation for sentence matching is also applied in other studies like [45, 22], which is effective yet of low complexity order, compared with other word-by-word matching [7], or attention methods [27].

The joint vector is then passed through a 3-layer, fully-connected, feed-forward neural network, also known as multi-layer perception (MLP) [1], which allows rich interactions between a sentence pair from one of the three components. The network enables to extract features automatically, starting from lower-level representations to higher-level ones.

Finally, a single neuron outputs the matching score of two sen-tences. As mentioned, f ( q, r ) is in R ; hence the final scoring neu-ron is essentially a linear regression. For g ( q, p ) , h ( q, q we apply a sigmoid/logistic function given by  X  (  X  ) = 1
In the previous subsection, we have described how the model captures the relation among sentence pairs. Now, we merge the scores of a particular candidate r in terms of different contextu-al query reformulations. As discussed in Section 5.1, if a posting is more related to the (reformulated) query and the reformulated query is more related to the original query, then f ( q, r ) would be more reliable. Inspired by this observation, g ( q, p ) and h ( q, q as designed as two adjusting  X  X ates X . In particular, scores from d-ifferent query reformulations are summed, weighted by these two gates. The overall ranking score of a candidate reply r and q fined as follows. The equation and spirit also appear similar to the integration of a sum -product process, which combines the sum operations and product operations:
Here we propose to sum over all postings associated with the reply. Different data repository will have different settings: a can-didate reply is possible to associate with more than one postings. In our data settings, each reply is associated with only one posting. However, Equation (3) is general and extensible.

Ranking problems can apply pairwise ranking loss such as hinge loss or cross-entropy loss. Here we apply hinge loss to train our DNN network. Given a triple F ( q 0 , r + ) in the training set, we ran-domly sample a negative instance r . The objective is to maximize the scores of positive samples while minimizing that of the nega-tive samples. Concretely, we would like F ( q 0 , r + ) to be as least F ( q 0 , r ) plus a margin  X  . Thus, the training objective is to minimize where we add an  X  2 penalty with coefficient  X  for all the parameters  X  = {  X  ;  X  ;  X  } which are weight and bias values optimized by the network from multi-dimensions of ranking evidences, i.e., Query-Reply , Query-Posting and Query-Context , correspondingly.
As our model is (almost) everywhere differentiable, the parame-ters of the networks are optimized with stochastic gradient descen-t using the back prorogation algorithm to compute the gradients. The gradients can be propagated all the way back through merging, gating, matching, and individual sentence modeling. In this way, heterogeneous information (ranking evidences) can be incorporat-ed organically with our model under the unified deep architecture. Accordingly to the objective function to optimize O ( . ) in Equation (4), it is sufficient to learn the model by computing the gradients with respect to the model parameters  X  ,  X  and  X  ; that is, our goals T able 4: Data statistics. Postings and replies are all unique. are to compute  X  O  X   X  ,  X  O  X   X  and  X  O  X   X  for Query-Reply , Query-Posting and Query-Context .
In this section, we evaluate our model for conversation task a-gainst a series of baselines based on a huge conversation resource. The objectives of our experiments are to 1) evaluate the effective-ness of our proposed deep learning-to-respond schema, and 2) eval-uate contextual reformulation strategies and components of multi-dimension of ranking evidences for the conversational task.
As mentioned, we collected massive conversation resources from various forums, microblog websites, and cQA platforms including Baidu Zhidao 6 , Douban forum 7 , Baidu Tieba 8 , Sina Weibo We conducted data filtering and cleaning procedures by removing extremely short replies and those of low linguistic quality such as meaningless babblings according to the evaluation framework put forward in [40, 42], so as to maintain a meaningful, high-quality conversation record. In total, the database contains  X  10 million  X  posting , reply  X  pairs. Some statistics are summarized in Table 4.
We constructed the dataset of 1,606,583 samples to train the deep neural networks, 357,018 for validation, and 11,097 for testing. It is important that the dataset for learning does not overlap with the database for retrieval, so that we strictly comply with the machine learning regime. For each training and validation sample, we ran-domly chose a reply as a negative sample. Validation was based on the accuracy of positive/negative classification. For the test set, we hired workers on a crowdsourcing platform to judge the appro-priateness of 30 candidate replies retrieved for each query. Each sample was judged by 7 annotators via majority voting based on the appropriateness for the response given the query and contexts (if any):  X 1 X  denotes an appropriate response and  X 0 X  indicates an inappropriate one.
In our proposed model, we used 128-dimensional word embed-dings, and they were initialized randomly and learned during train-ing. As our dataset is in Chinese, we performed standard Chinese word segmentation. We maintained a vocabulary of 177,044 phras-es by choosing those with more than 2 occurrences.

The bi-directional LSTM has 128 hidden units for each dimen-sion; CNN is 256 dimensional with a window size of 3. We used stochastic gradient descent (with a mini-batch size of 100) for opti-mization, gradient computed by standard back-propagation. Initial learning rate was set to 0.8, and a multiplicative learning rate decay was applied. The above parameters were chosen empirically. We used the validation set for early stopping.
Given the ranking lists (annotated by crowdsourced workers) for test queries, we evaluated the performance in terms of the follow-ing metrics: precision@1 (p@1), mean average precision (MAP) [31, 43], and normalized discounted cumulative gain (nDCG) [8, 41]. Since the system outputs the best selected reply, p@1 is the precision at the 1st position, and should be the most natural way to indicate the fraction of suitable responses among the top-1 reply re-trieved. Besides, we also provided the top-k ranking list for the test queries using nDCG and MAP, which test the potential for a system to provide more than one appropriate responses as candidates. We aimed at selecting as many appropriate responses as possible into the top-k list and rewarding methods that return suitable replies on the top.

Formally, the metrics are computed as follows. where T indicates the testing query set, k denotes the top-k posi-tion in the ranking list, and Z is a normalization factor obtained from a perfect ranking. r i is the relevance score for the i -th candi-date reply in the ranking list (i.e., 1: appropriate, 0: inappropriate). MAP is computed by
Here N q is the number of appropriate responses selected, and P is the precision at i -th position for the query.

Since we use real conversations for testing, we also have the hu-man response taken from the human-human conversation session as one of candidate replies ordered in the ranked list. Hence we in-clude the Mean Reciprocal Rank (MRR) evaluation computed as: where rank ( q ) is the position of the original response in the can-didate ranking list. Unlike MAP and nDCG, which examine the ranks of all appropriate responses, MRR focuses on evaluating the capability of retrieval systems to find (perhaps) the best response. MRR is useful but does not test the full capability because there can be more than one appropriate responses to fulfill a conversation.
To illustrate the performance of our approach, we include several alternative algorithms as baselines for comparison. The baselines can be divided into two categories, i.e., 1) generation-based meth-ods and 2) retrieval-based methods for conversation systems from very recent studies. Since our proposed approach is technically a retrieval-based method, we mainly focus on the second category. For fairness we conducted the same pre-processing procedures and data cleaning for all algorithms.
 Generation-based Conversation. For this group of algorithms, the conversation system will generate a response from a given in-put, i.e., a query from the user under the conversational scenario.  X  Statistical Machine Translation (SMT) : SMT is a machine trans-lation paradigm which translates one sentence in the source lan-guage to a sentence in the target language. If we treat queries and replies as separate languages, we can train a translation model to p@1 in fact refers to accuracy. Other metrics are not applicable.  X  X ranslate X  queries into replies. We implemented the phrase-based translation idea for conversation proposed in [26].  X  LSTM-RNN : LSTM-RNN is basically a Recurrent Neural Net-work (RNN) using the Long Short Term Memory (LSTM) archi-tecture. The RNN with LSTM units consists of memory cells in order to store information for extended periods of time. We use LSTM-RNN for both generation and retrieval baselines. For gen-eration, we first use an LSTM-RNN to encode the input sequence (query) to a vector space, and then use another LSTM-RNN to de-code the vector into the output sequence (reply) [32]; for retrievals, we adopt the LSTM-RNN to construct sentence representations and use cosine similarity to output the matching score [25].  X  Neural Responding Machine. We implement the neural re-sponding machine (NRM) proposed in [29], which is an RNN-based generation approach with a global-local attention schema. Retrieval-based Conversation. The approaches within this group of baselines are based on retrieval systems, which return the best matched candidate reply out of the conversational repository given a particular query. Since our approach is retrieval-based, we select strong retrieval-based methods to make a thorough comparison.  X  Random Match. The method randomly selects replies from the retrieved list for each query. Be aware it is not true random because it only randomizes the order of the retrieved results. The true random match is too weak to be included as a decent baseline.  X  Okapi BM25. We include the standard retrieval technique to rank candidate replies. For each query, we retrieve the most rele-vant reply using BM25 model [18] from the corpus.  X  DeepMatch. The DeepMatch method considers multiple gran-ularity from the perspective of topics, obtained via LDA [17].  X  ARC. The ARC approach is a CNN based method with convo-lutionary layers which construct sentence representations and pro-duce the final matching scores via a MLP layer [7].  X  Deep Learning-to-Respond (DL2R) . We propose the DL2R system based on three novel insights: 1) the integration of multi-dimension of ranking evidences, 2) context-based query reformu-lations with ranked lists fusion, and 3) deep learning framework for the conversational task. There is actually a series of variants of DL2R model with different components and different context utilization strategies. We will first report the performance compar-isons between DL2R against baselines and then show the details of components and strategies in Section 6.4.
We compare the performance of all methods including baselines and our proposed DL2R model measured in terms of p@1, MAP, nDCG and MRR. In Table 5 we list the overall results against al-l baseline methods. Our proposed method DL2R shows clearly better performance than the baseline methods. On average, DL2R achieves an average +38.63% improvement (averaged on all met-rics) compared with the strongest baseline group with context adap-tion (in Table 5). We then discuss the comparisons in details.
We illustrate the result from generative methods. Given one us-er query, the generative methods generally provide one generation as the response to output. Hence we do not compare MAP or nD-CG@1 for this algorithm group. Note that the original response is not likely to be generated; thus it is infeasible to calculate the M-RR. In general, the generative algorithms have relatively high p@1 scores, while LSTM-RNN and NRM perform better than the SMT method. The reasons can be ascribed to two aspects: firstly, SMT is not instinctively tailored for conversation systems and secondly, deep neural networks for LSTM-RNN and NRM will be more like-ly to learn a better representation for the queries and then return a better decoded generation as response. The generated responses are in general quite ambiguous or broad to answer a wide range of queries, but not specific enough. Such responses might be relevan-t but not appropriate enough to make a meaningful conversation, which have been raised as a problem in [15, 29].

We can see great improvement for DL2R against original retrieval-based baselines. Random Match is a lower bound for all base-lines. As we mentioned, it randomizes the order of the retrieved results. Hence the result is comparable to that of BM25 but s-lightly worse. Okapi BM25 represents the standard (and simple) retrieval system. The performance for BM25 is not as good as the other deep learning-based retrieval systems, which is not surpris-ing. Deep learning systems are proved to have strong capabilities to learn the abstractive representation [1, 10, 30], while BM25 only utilizes the shallow representation of term-level retrieval. The deep learning algorithm groups clearly overwhelm shallow learning al-gorithms, yet it is interesting to see that DL2R still outperforms the other deep learning baselines in Table 5. The benefits might be due to the context information we have managed to use, while the other deep learning baselines are matching metrics for single turns only. For a more fair comparison, we adapt the original baselines into our proposed contextual reformulation framework to incorporate con-text information.

In Table 5, we can see with the usage of contextual reformula-tion, the performances for DeepMatch, ARC, and LSTM-RNN all get boosted, which greatly indicates the effectiveness of our pro-posed contextual query reformulation for conversation systems. It is a useful way to incorporate context information for conversa-tional scenarios. Our proposed DL2R has obvious improvement against the modified baseline systems. The most probable credits come from the retrieval formulation: we frame the virtual docu-ment as a posting and reply, and we integrate multi-dimensions of ranking evidences to facilitate a better ranking list. The contextu-al DeepMatch method very slightly outperforms DL2R on MRR evaluation. As mentioned before, MRR is useful when trying the find the best response. Since conversations are open with more than one appropriate responses, MAP and nDCG scores indicate the full capacity of the retrieval systems.

Till now, we have validated that deep learning structures, contex-tual reformulations and integrations of multi-dimensions of ranking evidences are effective. Next we will come to a closer look at these strategies and components for further analysis and discussions.
We examine the relative contributions of different strategies and individual components of our proposed model. Other than the pro-posed deep neural network-based learning framework, we have t-wo other contributions: 1) contextual query reformulation to utilize context information, and 2) integration of multi-dimension of rank-ing evidences. We now analyze the strategies and components.
As mentioned in Section 4, we have different ways to use con-textual information via contextual query reformulation strategies: No Context , Whole Context , Add-One , Drop-Out and Combined . The Whole Context strategy incorporates context information in a coarse granularity, while the Add-One and Drop-Out strategies are in a finer-granularity. We illustrate the different performance with different strategies using the DL2R framework in Table 6. We can see that the improvement from No Context to Whole Context is rather obvious, indicating that context information is quite beneficial to find better responses under the conversational scenarios. We also have an interesting observation that Add-One and Drop-Out strategies are better than the Whole Context strate-gy. Whole Context strategy is a coarse-grained method which uses context information without distinguishing irrelevant contexts from relevant ones. The results indicate a proper way to use context in-formation is important. Drop-Out strategy is slightly better than the Add-One strategy, which confirms the exemption of irrelevant context information is necessary, and in most cases, there are more relevant context sentences than irrelevant ones. The combination of all strategies performs best since it balances both the coarse-grained and fine-grained context modeling. We combine the best approxi-mations for all possible utilizations of contexts, avoiding explosive number of combinations.
Since we have three major components from the multi-dimensions of ranking evidences, i.e., Query-Reply , Query-Posting and Query-Context , we examine the contributions of such components. Note that we can retrieve responses from the virtual document consisting of a posting-reply pair, from the reply side and/or the posting side. But we cannot directly retrieve the response using Query-Context . It is pointless to run solely on the Query-Context part. The first group is to run only based on Query-Reply and Query-Posting . Neither of the two components incorporates Query-Context information. Without context information, the proposed framework might handle single-turn conversation well enough, while general-ly multi-turn conversation is beyond the capability of the system. With the incorporation of the contextual information, the perfor-mance of both components get boosted. It is not surprising that the combination of all three components yield to the best results: each component characterizes the appropriateness of the response from a different aspect, and all aspects should be integrated for scoring.
In this paper, we propose to establish an automatic conversation system between humans and computers. Given a human-issued message as the query, our proposed system will return the corre-sponding responses based on a deep learning-to-respond schema. There are 3 major contributions in this work: 1) we propose a con-textual query reformulation framework with ranking fusions for the conversation task; 2) we integrate multi-dimension of ranking evi-dences, i.e., queries , postings , replies and contexts ; 3) we establish the deep neural network architecture featured with above strategies and components. We launch the conversation system based on a massive repository (  X  10 million posting-reply pairs) and run ex-periments to validate the proposed paradigm.

We examine the effect of our proposed DL2R model with several baselines on a series of evaluation metrics. Our method consistent-ly and significantly outperforms the alternative baselines in terms of p@1, MAP, nDCG, and MRR. Furthermore, we have investigat-ed further experiments for component contributions and strategy analysis. In general, context information is demonstrated to be use-ful for conversations, especially multi-turn conversations and all di-mensions of ranking evidences are helpful. This work opens to sev-eral interesting directions for future work with regard to automat-ic conversation between humans and computers. For instance, we can incorporate more additional features and more conversation-oriented formulations, such as dialogue acts, conversational logics, and discourse structures, etc.
This work is supported by the National Basic Research Program of China (No. 2014CB340505). We thank all the reviewers for their valuable comments, and thank the support from the Deep Learning Platform in Baidu Natural Language Processing Department.
