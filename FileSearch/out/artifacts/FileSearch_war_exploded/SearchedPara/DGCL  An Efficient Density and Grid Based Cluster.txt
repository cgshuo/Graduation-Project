 nature. Because of the huge amounts of spatial data that may be obtained from satel-lite images, medical equipments, Geographic Information System (GIS) etc, it X  X  ex-mining aims to automate the process of understanding spatial data by representing the data in a concise manner and reorganizing spatial databases to accommodate data semantics. 
The aim of data clustering algorithms is to group the objects in spatial databases into meaningful subclasses. A good clustering algorithm should have the following for clustering algorithm is to achieve good time efficiency. Second, a good clustering should be order insensitive with respect to input data. Last, the parameter count should be minimized for users. 
This paper presents an enhanced density-grid based clustering algorithm, DGCL, clusters correctly. In this algorithm, we set a default number of intervals according to the number of input data. The time complexity mostly depends on the grid number N, and it can be O(N). It X  X  also order insensitive. The rest of this paper is organized as follows. Section 2 reviews related work. The detail algorithm is described in Section 3. Section 4 we analyze the time complexity. section. Density-based and grid-based clustering are two main clustering approaches. The former is famous for its capability of discovering clusters of various shapes and elimi-GDILC are two kinds of clustering methods which are based on density and grid. Both of them have advantages and disadvantages. 
Density X  X ased [1] clustering algorithms regard clusters as dense regions of objects in the data space that are separated by regions of low density. A density-based cluster is a set of density-connected objects that is maximal with respect to density-reachability. Every object not contained in any cluster is considered to be noise . Typi-cal example is DBSCAN. It grows regions with sufficiently high density into clusters parameters ( values of these two parameters for some data sets. Other drawback of this technique is the high computational complexity because of examining all the neighborhoods in checking the core condition for each obj ect. Specially when the algorithm runs on where n is the number of data objects [2]. quantizes the space into a finite number of cells that form a grid structure on which all of the operations for clustering are performed. The main advantage is its fast process-ing time, which is typically independent of the number of data objects, yet dependent on only the number of cells in each dimension in the quantized space. A shifting grid [3] clustering algorithm uses the concept of shifting grid. This algo-rithm does not require users inputting parameters. It divides each dimension of the data space into certain interval to form a grid structure in the data space. Based on the concept of sliding window, shifting of the whole grid structure is introduced to obtain a more descriptive density profile. It clusters data in a way of cell rather than in points can enhance the accuracy of the results. GDILC [5] is a grid-based density-isoline clustering algorithm. The idea of cluster-ing using density-isoline comes from contour figures, density-based and grid-based clustering algorithms. When clustering, density-isoline figure obtained from the algorithm needs to calculate the distance between each data sample and every data sample in its neighbor cells, the cost is also too high, especially for huge data set. In this part, the structure of this algorithm and the detail of the sub-procedure will be described. 3.1 Procedure of DGCL DGCL. In step 1, get the total number of the data points. According to the total num-sufficient memory to contain the whole data set, this algorithm divides them to sev-the useful cells will be considered. So the number of useful cells is small compared to the number of data. In step 5, from the remainder cells, assign the adjacent cells to the become one group in step 6. There are still some outliers exiting in the groups, so in the last step, DGCL removes the outliers again to optimize the result. In the end, each group is a cluster. The sketch of DGCL is shown in Fig.1. 3.2 Number of Intervals In this step, setting the number of intervals is a very critical procedure. If the size of the cells is too large, the algorithm will merge two or more clusters into one. Another many blank spaces in the large size of the cell. So the result is not exact and satisfy-ing. If the size of cell is too small, the algorithm may make the number of cells equal necessary to find a method to set a suitable interval value to get both a better cluster-ing result and a good efficiency. Here we adopt the method of GDILC [5]. The fol-lowing formula is used to calculate the number of intervals m. In equation 1, n is the number of data points. coefM is a coefficient to adjust the value of m . we propose it as an positive integer. In fact, it stands for the average number of data samples in a cell. But we don X  X  want to regard it as a fixed value. Because in the experiments, coefM changes according to the following curve in Fig. 2. And it can get a better result as we expect. 3.3 Density of the Cell In general, we term the density as the number of data points in the cell. The original grid-based clustering algorithm just considers about the density of the current cell which have similar data points. The attributes of a spatial object stored in a database the data points in its neighbor cells such as what the shifting grid clustering algorithm sider about the empty cells. The definition of neighbor cells satisfies the following inequation. Assume that cell C i 2 and C j 1 j 2 are neighbor cells. m is the number of intervals. 3.4 Selection of the Density Threshold DT The dense cells are surrounded by the sparse cells which are regarded as outliers. From the experiments, we find that the density of the dense cells usually decrease gradually from the core of the cluster to the boundary as illustrated in Fig. 4. And the density of sparse cells is obviously smaller than that of the boundary cells. So before clustering the cell set, we define a measurement to remove the sparse cells. The aim is to decrease the cost of calculation and increase the efficiency. The measurement, we call it density threshold (DT) [5], is defined using the following equation. experiments show that when setting it to 0.95, good clustering result can be achieved removed from the cell set. The remainder cells will be regarded as useful cells . In the following steps, the algorithm only considers about the useful cells, so compared with the number of data points, they X  X e really few enough. The remainder cells also contain some unexpected cells. They will be further removed in step 7 (optimization). 3.5 Group Assignment useful neighbors , all belonging to set S i , haven X  X  been assigned a group ID, the algo-rithm assigns a new group ID for all the cells in the set S i . If some cells in S i have had group IDs, the algorithm finds the ID of the cell with the maximum density in S i and been checked. Fig. 5 shows the sketch of group assignment. 3.6 Group Mergence The procedure of group assignment only considers the current cell and its neighbors, space, the adjacent groups need to be merged together to form one group as we ex-pected. The sketch of group mergence is shown in Fig. 6. 3.7 Optimization some cells which we are not expect. 
In DGCL, we calculate the cell density with considering about its neighbors. There experiments show that these situations often happen at the boundary of the group. Fig. 8 shows one situation of the problem. The fuscous cell is the considered cell. It only contains a relatively few data points compared with its neighbors when calculat-unuseful cells . And the final result only contains the useful cells. We remove the cells which satisfy the following inequation. 
In equation 4, RD means the real number of data points which are contained in the considered cell. TD means the total density of the considered cell. Num is the number of its useful neighbors and 1 means the considered cell itself. In DGCL, we use the following equation which we have mentioned previously to calculate the number of intervals. The total number of cells is m 2 , namely n/coefM, in which n is the number of data points. After we get the data set, both the data distribution and removing outlier pro-checked for group assignment, merge procedure and optimization. So in most of the number of the useful cells . We performed experiments using a personal computer with 768MB of RAM and Pentium(R) 4 CPU 1.8GHz and running Windows XP Professional. The data sets are generated ourselves. In the data set, the data points are generated randomly according points, including 30000 for circle, 40000 for rectangle, 25000 for sine curve and 5000 noises. 
The result shows that DGCL can find the clusters correctly and eliminate outliers they can adjust the coefficient coefM . The smaller the coefM is, the more exact result DGCL gets. But it need more time for calculation. Because GDILC has higher per-formance than shifting grid algorithm, we only show the comparison of DGCL and GDILC in Fig. 10. From that graph, we know that this algorithm has higher perform-shown in Fig. 11. In this paper, we present an enhanced density and grid based clustering algorithm, DGCL. By considering the neighbor cells when calculating the density of current cell and removing the outliers efficiently with the help of a proper measurement, a better clustering algorithm and GDILC, it X  X  much faster because it doesn X  X  need to cluster recursively or calculate the distance between data points. The time complexity de-pends on the number of cells. Drawback of this algorithm is that all the cluster boundaries are either horizontal or vertical. Mo reover, it is not affected by the outliers and can handle them properly. And DGCL is order insensitive. A faster method to do I/O, such as an efficient indexing method or parallel control, will make the algorithm a whole lot faster. In conclusion, the experiments show that DGCL is a stable and efficient clustering method for large spatial data set. 
