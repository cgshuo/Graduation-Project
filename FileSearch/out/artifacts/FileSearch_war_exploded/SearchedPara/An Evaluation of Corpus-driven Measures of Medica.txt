 Measures of semantic similarity between medical concepts are central to a number of techniques in medical informatics, including query expansion in medical information retrieval. Previous work has mainly considered thesaurus-based path measures of semantic similarity and has not compared differ-ent corpus-driven approaches in depth. We evaluate the ef-fectiveness of eight common corpus-driven measures in cap-turing semantic similarity and compare these against human judged concept pairs assessed by medical professionals. Our results show that certain corpus-driven measures correlate strongly (  X  0 . 8) with human judgements. An important finding is that performance was significantly affected by the choice of corpus used in priming the measure, i.e., used as evidence from which corpus-driven similarities are drawn. This paper provides guidelines for the implementation of se-mantic similarity measures for medical informatics and con-cludes with implications for medical information retrieval. Categories and Subject Descriptors: H.3.3 [Informa-tion Storage and Retrieval] Keywords: Semantic Similarity, Medical Information Re-trieval
The advent of electronic medical records, and large cor-pora of medical documents (e.g. MEDLINE), create an in-creasing need for information retrieval (IR) systems tailored to searching medical free-text [9]. Searching medical records presents some specific challenges  X  vocabulary mismatch is prevalent as the same concepts can be expressed using differ-ent terms. More challenging are situations where relevance must be inferred, e.g., the presence of a certain organism in a laboratory report denoting a certain disease, even though the disease is not stated explicitly (for example, Varicella zoster virus  X  Chicken pox ). Overcoming these challenges requires IR models capable of accurately determining se-mantic similarity between medical concepts. Semantic sim-ilarity measures are central to several techniques used in medical informatics, including: query expansion and rele-vance feedback [16, 7], literature-based discovery (e.g., drug discovery [1]), clustering (e.g., gene clustering [8]), and on-tology construction or maintenance [6].

Medical domain knowledge has been formally represented in a number of medical thesauri/ontologies such as Medical Subject Headings (MeSH) and Unified Medical Language System (UMLS). In these symbolic representations medi-cal concepts are organised into inheritance hierarchies and into inter-concept relationships (e.g. &lt; organism &gt; causes proaches to measuring semantic similarity used thesaurus-based path measures between medical concepts 1 . An alter-native to path-based measures are corpus-driven approaches, e.g., Latent Semantic Analysis (LSA), which commonly ex-ploit co-occurrence statistics to determine similarity.
Corpus-driven approaches are used extensively in medi-cal IR, particularly for query expansion. This is evident by the number of teams using these approaches in the TREC 2011 Medical Records track [16]. An evaluation by Peder-sen et al. [12] using human judged concept pairs provided by medical professionals found that a corpus-driven approach adapted from LSA (which they call Context Vector ) out-performed a number of existing path-based measures. Sim-ilarly, Sanchez et al. [14] showed that using the Web as a corpus also outperformed path-based measures. Finally, Trieschnigg et al. [15] proposed Cross Entropy Reduction (CER), a language modelling approach that outperformed a path baseline. These different approaches demonstrate the plethora of corpus-driven approaches available as measures of semantic similarity. To our knowledge, no previous work compares these corpus-based approaches for the purposes of similarity judgements in the medical domain. Pedersen X  X  et al. evaluation only used a single LSA adaptation, while CER was compared against only two basic corpus-driven baselines. Additionally, important implementation issues have not been explored, like the choice of dimensionality and robustness across multiple collections, a factor that we show has significant effect on performance. This paper con-siders these issues by evaluating 8 different corpus-driven measures on two corpora against two separate datasets of human judged concept pairs.
Evaluation of 8 corpus-driven measures was performed against two separate datasets of human judged medical con-cept pairs. An example of a concept pair is ( Congestive heart failure , Pulmonary edema ). Semantic similarity be-tween concept pairs was computed using the following mea-sures: 1. Random Indexing [13] ( RI ): a technique that constructs 2. Latent Semantic Analysis ( LSA ): evaluated on 50, 150, 3. Hyperspace Analogue to Language [11] ( HAL ): constructs 4. Document Vector Cosine Similarity ( DocCosine ): cosine 5. Positive Pointwise Mutual Information [4] ( +PMI ): vari-6. Cross Entropy Reduction [15] ( CER ): Trieschnigg et al. 7. Language Model + Jensen-Shannon divergence ( LM JSD ): 8. Latent Dirichlet Allocation ( LDA ): topic model evalu-
Two separate datasets of human judged concept pairs were used for evaluation. The first dataset consists of twenty-nine 4 UMLS medical concept pairs, as developed by Peder-sen et al. [12], involving 3 physician and 9 clinical termi-nologists; inter-coder correlation was reported to be 0 . 85. A concept pair example is ( Brain tumor , Intracranial hemor-rhage ), judged as having a similarity of 2 . 0 on a scale of 1.0 (unrelated) to 4.0 (synonymous). We refer to this dataset as Ped . The second dataset, from Cavides and Cimino [5], contains forty-five MeSH/UMLS concept pairs 5 judged by three physicians on a scale of 1 to 10; Cavides and Cimino report  X  X onsensus X  amongst judges, but no precise value was reported. This dataset is referred to as Cav .

Two separate corpora were used as data to prime each corpus-driven method. The first corpus was MedTrack, a collection of 100,866 clinical record documents used in the TREC 2011 Medical Records Track. Documents belonging to a single patient X  X  admission were treated as sub-documents and were concatenated together into a single document called a patient visit document. The corpus then contained 17,198 patient visit documents. This was done to encapsulate the closely related content of different reports (e.g. pathology report and surgical report) belonging to the same patient ad-mission 6 . The second corpus used was OHSUMED, a MED-LINE subset consisting of 348,566 medical journal abstracts, as used in TREC 2000 Filtering Track. Statistics for each corpus are provided in Table 1.
 Corpus #Docs Avg. doc. len. #Vocab.
 MedTrack 17,198  X  932 54,546 OHSUMED 293,856 100 55,390 100,866 original reports collapsed to 17,198 patient visit documents.
 Table 1: Collection statistics of the test corpora: MedTrack, collection of clinical patient records; and OHSUMED, MEDLINE abstracts.

For both corpora, the original textual documents were translated into UMLS concept identifiers using MetaMap, the biomedical concept identification system [2]. After pro-cessing, the individual documents contained only UMLS con-cept ids, for example the phrase Congestive heart failure in the original document will be replaced with C0018802 in the new document; more details of this approach are provided in [10]. Both test datasets, Ped and Cav , contained UMLS concept pairs (which may actually represent term phrases rather than single terms); converting the test corpora to concepts thus allows direct comparison of the single concept pairs contained in the two datasets.

Each of the 8 models outlined in the Methods section pro-vide a representation of a concept, for example, in DocCosine a concept is a vector based on the documents the given con-cept appears in. Similarity can be determined by comparing two concepts X  representations. For each similarity measure, comparison was made against human judges for each dataset ( Ped and Cav ) using Pearson X  X  correlation coefficient.
Results showing the correlation coefficient against human judges for each corpus-driven method are reported in Fig-correlation coefficient against human judges confidence interval at 95%. ure 1. The x -axis is ordered by decreasing correlation aver-aged across all datasets/corpora 7 .

The first observation we make is that similar types of measures demonstrate similar results: the three probabilis-tic language model measures, +PMI , CER and LM (JSD) exhibit comparable performance profiles across datasets / corpora. Similarly, the vector-based measures ( RI and LSA and DocCosine ) exhibit similar profiles between each other and across different dimensions.

Considering the best performing measures, Table 2 pro-vides a breakdown of the top 3 semantic similarity measures for each dataset / corpus.
 Corpus Ped Cav MedTrack RI300 , LSA150 , DC LSA50 , +PMI , DC OHSUMED CER , +PMI , LM / DC CER , +PMI , LM Table 2: Top 3 semantic similarity measures for each corpus and dataset. DocCosine abbreviated to DC .
 Consensus is observed between the two datasets, Ped and Cav . However, the best measure differs significantly between the two corpora. In general, vector-based measures perform best when primed with the MedTrack corpus, while proba-bilistic measures are most effective primed with OHSUMED. This may be explained by the different characteristics of the two corpora: MedTrack contains detailed clinical notes from patient encounters, whereas OHSUMED contains MEDLINE article abstracts. As a result, the scope of concepts found in a documents differs between the two collections. Clini-cal notes relating to a patient X  X  admission may cover a wide range of different concepts, especially if they have been ad-mitted with multiple conditions or for a lengthy period. In contrast, journal abstracts are descriptions of a particular topic and are therefore typically narrower in scope. The probabilistic measures use the whole document as the  X  X on-text window X  for determining co-occurrence, OHSUMED X  X  documents of narrower scope therefore offer more precise context windows, whereas the wider scoped MedTrack doc-uments may contain more noise. In addition to the nature of the documents found in each corpus, the average doc-ument length differs considerably  X  MedTrack documents are about an order of magnitude larger (Table 1). Intu-itively, longer documents will, in general, cover more topics and be wider in scope. The vector-based measures benefit from the addition context found in the longer documents, which is in contrast to the probabilistic measures.
The nature of the language also differs between the two corpora. MEDLINE abstracts contain precise descriptions of a particular topic, whereas clinical records are often terse narratives with considerable jargon and shorthand  X  and in some cases typographic errors.

Given the differences in scope , document length and lan-guage of the two corpora we could hypothesis that OHSUMED appears a higher quality corpus for similarity judgements, and that measures primed with MedTrack would exhibit de-graded performance. However, the results do not affirm this hypothesis. Probabilistic measures primed with OHSUMED display excellent results, however, the longer, less consis-tent documents found in MedTrack still provide good evi-dence for similarity judgements when used with vector-based methods.
 Table 2 also highlights the robustness of +PMI and Doc-Cosine , which both occupy three out of four cells. The tradi-tional IR measure of DocCosine , although not producing the best results on a single test, is particularly stable across both corpora and datasets. Both +PMI and DocCosine are simple and computationally efficient, making them more attractive than more computationally intensive measures such as LSA and language model-based measures. Certain measures may perform well on one particular collection / dataset, but have poor performance on others  X  LM (JSD) , LDA and HAL all exhibit this behaviour.

More generally, the results reaffirm the findings of Peder-sen et al. that corpus-driven approaches outperform path-based measures, which failed to yield a correlation greater than 0 . 5 8 . Additionally, our findings using vector-based mea-sures are in line with Petersen et al. who reported a 0 . 69 correlation obtained using their Context Vector measure on the Mayo Clinic Corpus of Clinical Notes; our vector-based measure results using MedTrack were  X  0 . 7. MedTrack and the Mayo Clinic Corpus are of similar size and nature (both being clinical records) 9 .

An outcome of this study are a set of guidelines for the im-plementation of corpus-based semantic similarity measures for medical text: 1. The choice of corpus used to prime the similarity mea-2. More specifically, the characteristics of individual doc-3. +PMI and DocCosine are robust across collections and 4. When implementing a semantic similarity on a particu-
The reported findings may have important impacts for medical information retrieval, specifically for systems mak-ing significant use of query expansion and relevance feed-back, as was the case with participants of TREC MedTrack. Firstly, the effective of corpus-based query expansion varied significantly between participants of TREC MedTrack  X  some techniques showed gains, while others degraded per-formance. Although a number of factors affect query expan-sion performance, a poor semantic similarity measure could certainly be a major contributor. The most appropriate sim-ilarly measure, based on the findings of this study, should be considered when employing corpus-based query expansion.
Finally, having highlighted the choice of corpus as an im-portant consideration, we conjecture that in some cases it may be advantageous to prime the similarity measure with a separate corpus from the one being used for retrieval. For ex-ample, when searching medical literature (e.g. OHSUMED), priming with clinical records (e.g. those found in MedTrack) may increase effectiveness. In the literature there is evidence supporting the use of Wikipedia as a background priming corpus [3]. An in-depth evaluation of this aspect is left to future work.
In this paper we evaluate eight different corpus-driven approaches to determining the semantic similarity between medical concepts. Corpus-driven approaches exhibit strong correlations (up to  X  0 . 8) with human judged concept pairs provided by medical professionals. Our findings show that the choice of corpus used to prime the similarity measure can significantly affect performance. We provide a number of guidelines for the use of semantic similarity measures that in-clude consideration of document scope, length and language. Simple measures such as +PMI and DocCosine demonstrate effective and robustness across evaluations. This work pro-vides an in depth review of corpus-driven semantic similarity measures, a technique central to medical informatics. Acknowledgements Dolf Trieschnigg kindly provided the data from his experiments on CER [15] and was used to validate our CER implementation.
