 1. Introduction 1.1. General introduction
This work presents a general methodology for clustering the nodes of a weighted, undirected, graph. Graph nodes clus-science: applied mathematics, computer science, social science, physics, pattern recognition; see for instance is supposed to have dense node-to-node connections within the community, and sparse connections with nodes from an-other community. Usually, community detection algorithms are also able to detect the natural number of communities. This we are interested in clustering the nodes of a graph: there is no attempt to find the number of clusters, except for the tions. Detecting dense groups in a network has a number of direct applications in telecommunications, web mining, social networking, to name a few.

On the other hand, kernel-based algorithms [110,112] are characterized by two important properties: they allow (i) to compute implicitly similarities in a high-dimensional space where the data are more likely to be well-separated and (ii) ilarity measure between them. It relies on two independent steps: pressed as an optimization problem in the sample space that mimics the standard formulation in the feature space. on a weighted, undirected, graph for nodes clustering. It is a straightforward extension of the commute-time (CT) kernel graph partitioning. The new kernel, called the sigmoid commute-time kernel ( K nodes clustering algorithms in the experimental section. Even if the K tion of the CT kernel, based on electrical networks, is also provided in this paper.

Thus, inspired by previous work [133] , we introduce a general methodology for defining prototype-based kernel cluster-rithms, these prototype-based methods could easily be extended to variable-metric or multi-prototype kernel clustering in a straightforward manner kernel clustering algorithms, working in the sample space, from their feature space counterpart.

The performances are evaluated, in a systematic way, on three real-world graph mining problems. The proposed algo-examined. 1.2. Contributions and organization of the paper
The present paper is an extended and improved follow-up to an earlier paper [133] which introduced our basic model as
In short, the paper has six contributions: (1) A brief survey of community detection in networks is provided. (2) A new interpretation of the commute-time kernel, based on electrical networks, is developed. While there exists a (3) A natural two-step methodology for finding communities in a graph is introduced: (i) compute a kernel on a graph (4) A sample-space prototype-based kernel formulation of the gaussian mixture model is introduced. In this framework, (5) It is shown that taking a sigmoid function of a graph kernel, and more specifically of the commute-time kernel (6) A comprehensive comparison of four kernels on a graph and four kernel clustering algorithms on three real-world net-
The paper is organized as follows. Section 2 discusses previous work. Section 3 introduces the sigmoid commute-time of the k -means, the fuzzy k -means and the gaussian mixture model while Section 5 introduces a kernel version of Ward X  X  datasets. Section 7 is the conclusion. 2. A brief survey of related work on kernel clustering and link-based community detection 2.1. Kernel clustering graph.

When dealing with kernel clustering, three different spaces can be defined:
When using agglomerative clustering techniques, prototype vectors (such as centroids) are usually defined. The authors of [41] broadly categorize the kernel clustering methods into three groups: 1. Methods looking for prototype vectors in the input space. 2. Methods looking for prototype vectors in the feature space. 3. Methods relying on one-class SVM.We add two additional families of methods to this list: 4. Methods looking for prototype vectors in the sample space. This is our technique of choice introduced in this paper. 5. Methods simply avoiding the computation of a prototype vector.

Most existing kernel clustering algorithms either compute prototypes in the input space [139] or simply avoid the com-space are [50,55,83,138] and, very recently [123] .

In [50], Girolami directly optimizes the prototype vector by using a stochastic optimization method over a set of binary with the objective of clustering nodes of a graph with a self-organizing map. In contrast, the method proposed in [138] in Section 4.1, where the kernel k -means is developed.

Finally, methods relying on one-class SVM [7,15] are seeking for a minimum enclosing sphere in feature space containing almost all the data, and excluding outliers. 2.2. Link-based community detection
As already stated before, the problem of community detection in a weighted graph has been the subject of a large amount categorize this work into three different approaches:
Similarity-based approaches. These techniques first compute some meaningful distances/similarities between the nodes of The methodology proposed in this paper belongs to this category.
 which are as disconnected as possible.

Spectral clustering approaches. These techniques use the spectrum of matrices derived from the adjacency matrix of the graph in order to embed the nodes in a Euclidean space and perform a clustering in this space.
Of course, most of these approaches are in fact interrelated; we now provide a short survey of the main features charac-terizing each of them. 2.2.1. Similarity-based approaches
The primary objective of these approaches is to define meaningful similarities (or distances) between the nodes of the as a positive semi-definite matrix, and is therefore called a graph kernel .
More sophisticated measures of similarities have been proposed as well. In their pioneering work, Klein and Randic [73] proposed to use the effective resistance between two nodes as a meaningful distance measure. They call this quantity the resistance distance between nodes. Indeed, it can be shown that the effective resistance is indeed a Euclidean distance [5,73] . The close link between the effective resistance and the commute-time of a random walker on the graph was estab-tance was already used by Cohen in the contex of graph drawing [23]. Recently, Alves [3] proposed to define a new ran-dom-walk on the graph where transition probabilities are proportional to the effective conductance between the nodes. A new deviation measure between the nodes is then derived from this random walk model.

On the other hand, Chebotarev and Shamis proposed in [19] a similarity measure between nodes integrating indirect sure yields a relative importance score intermediate between co-citation/bibliographic coupling and the HITS importance
Laplacian kernel, by introducing a new parameter controlling importance and relatedness. Moreover, in [114] it is shown that the regularized Laplacian kernel overcomes some limitations of the von Neumann kernel, when ranking linked docu-ments. This modified regularized Laplacian kernel is also closely related to a graph regularization framework introduced iment investigating its effectiveness was performed.
 The exponential and von Neumann diffusion kernels, based this time on the adjacency matrix, are introduced in [67,112].
The defined kernel matrices are computed through a power series of the adjacency matrix of the graph; they are therefore closely related to graph regularization models [75].

Moreover, some authors recently considered similarity measures based on random-walk or electrical concepts. For in-the context of image segmentation is [53].

On the other hand, Kondor and Lafferty [75] as well as Smola and Kondor [116] defined a graph regularization model re-ment to the labeled data.

Still another approach has been investigated by Palmer and Faloutsos [98] who define a similarity function between cat-quantity provides a reasonably good measure for clustering and classifying categorical attributes. The  X  X  X ommute-time X  (CT) kernel has been introduced in [46,107] and was inspired by the already mentioned work of and go back to the starting node. The CT kernel is defined as the inner product in a Euclidean space where the nodes are path distance at one end and to the commute-time (or resistance) distance on the other hand is proposed in [134] .
CT embedding, preserving the commute-time distance, and applied it to image segmentation and multi-body motion track-eigenmap [6] and the commute-time embedding [103,107] . On the other hand, Zhou [143] uses the average first-passage paper, it is shown that the CT kernel is a discrete Green function (see also [103] who made the same observation). This databases. Average first-passage times and commute times were also exploited for inducing hidden Markov models from effective conductance by a cycle-free effective conductance.

In two recent papers, Nadler et al. [90] and Pons and Latapy [100] proposed a well-formulated distance measure between approach for clustering the nodes according to the squared diffusion distance.

Two recent meaningful relatedness measures between nodes based on the PageRank procedure [13,97] were proposed in and Pucci define a random-walk process starting from the node of interest, controlled by some pre-computed correlation matrix between nodes. These two algorithms are inspired from the well-known PageRank procedure, adapted in order to provide relative similarities between nodes. Yet another PageRank-inspired algorithm defining similarities between nodes ly, a similarity between nodes based on the number of different paths connecting two nodes, and therefore on the maximum flow/minimum cut appears in [80]. This measure has been tested in two collaborative recommendation tasks [46], but did not perform well in this context.

Some recent attempts to define similarity measures on nodes of a graph for clustering purposes are [123,140] (performed matrix. Instead, their k -means-like procedure involves solving as many linear systems of equations as clusters at each iteration.
 graph from which the Laplacian matrix is computed. It is then used for ranking and clustering images.
Notice that a comprehensive experimental comparison between seven kernels on a graph on two collaborative recom-mendation tasks appeared in [47]. The main findings of this experimental comparison are that (i) Laplacian-based methods kernels provide the best results, comparable to state-of-the-art collaborative recommendation techniques. 2.2.2. Top-down, divisive, and bottom-up, agglomerative, approaches
Several algorithms for partitioning a graph have been developed; only a few papers, the most relevant to the present been generalized and extended in various ways; see for instance [43] for a review. However, most of these methods were restricted to partitions of equal size.
 nodes [81]. Therefore, bottom-up hierarchical algorithms can be used to compute the LS-sets of a graph [10].
Recursive spectral bisection (RSB) is a spectral-based technique for finding a minimum cut graph bisection [40].

More recently, in the context of image segmentation, Shi and Malik [113] introduced the normalized cut (NCut) criterion the isometric cut [54] were also investigated. The MinMaxCut seemed to provide the best results on experiments performed on the newsgroup database [32]; see [30] for a thorough review. Links between these bipartition techniques and spectral niques for graph partitioning.

Another approach aims to discover communities within graphs based on the notion of electrical voltage drop [129] .It dom-walk based technique, called the Markov cluster algorithm. This method simulates random walks within the graph by separated components. Yet another method relies on information-theoretic concepts [105]. It decomposes the network into smaller parts by finding an optimal compression of its topology.

Finally, Newman and Girvan [51] recently introduced a new measure of the cohesion of communities in a social network, clustering problem [128,94] . 2.2.3. Spectral clustering approaches including high performance computing [102], image segmentation [113] , web pages ranking [74,97] , information retrieval work will be mentioned here.
 matrix containing similarities between the observations (see for instance [96,126] ; see also the survey of von Luxburg [124] and Ding [30]). One of the primary application of spectral clustering is graph drawing and clustering some relevant works are [12,28,31,136] ; see [30,124] for a survey.

The related problem of the Laplacian eigenmap is a dimensionality reduction procedure that has been proposed by Belkin and Niyogi [6]. It defines an embedding of the observations based on the generalized eigendecomposition of the Laplacian matrix. This appears to be the same embedding that is computed with the spectral clustering algorithm from Shi and Malik [113] , as noted in [126] , as well as the same embedding as proposed by Zien et al. in [145] . kernel (see [46]). Moreover, Meila and Shi present an interesting link between spectral segmentation and Markov random walks [87]. The random-walk model they introduced is identical to the one defined in [46,107] , but different properties to find communities. 3. The sigmoid commute-time kernel on a graph 3.1. Basic notations and definitions
Let us consider a weighted, undirected, graph, G , composed of n nodes and with symmetric weights w couple of nodes, i and j , which are linked by an edge. The weight w the value of w ij , and consequently the easier the communication through the edge. The elements a of the graph are defined in a standard way as a ij = w ij matrix, the Laplacian matrix L of the graph is defined in the usual manner: L = D A , where D = Diag ( a matrix, with diagonal entries d ii  X  X  D ii  X  a i :  X  P v  X  vol  X  G  X  X  reached from any other node of the graph. In this case, L has rank n 1, where n is the number of nodes [21]. Moreover, it can be shown that L is symmetric and positive semi-definite (see for instance [21]). 3.2. The sigmoid commute-time kernel node ( n in total), and the transition probabilities are given by p age commute-time can be computed thanks to where every node i of the graph is represented by a basis vector, e [18,36,46,73] .
 these node vectors are exactly separated by commute-time distances [46]. In other words, the entries of L similarities between nodes and L + is a kernel matrix: space in which the coordinates of the nodes are provided by the eigenvectors of L value. The coordinate that contributes most is the dominant eigenvector of L and is, in fact, the same as the smallest non-trivial eigenvector of L ( L and L beit to a lesser extend, etc.
 The sigmoid commute-time kernel K S CT is obtained by applying a sigmoid transformation [110] on K element of the kernel matrix is given by the formula where l  X  ij  X  X  L  X  ij , that is, element i , j of the matrix L formation, up to a rescaling factor.

Notice, however, that, even if the sigmoid transformation of a kernel matrix is presented as a kernel in [110] ,itis not (see Section 6, describing the experiments) show that the sigmoid commute-time kernel (Eq. (3)) performs much better than the commute-time kernel for clustering, basically because the range of values of the commute-time kernel has quite a large spread, which is reduced by applying the sigmoid function. This large spread of values causes the presence of a number of outliers and perturbs the k -means which is known to be quite sensitive to outliers. Thus, taking a sigmoid ing to negative eigenvalues) are imaginary. Although, some of the coordinates become imaginary, all the quantities of interest are nevertheless real numbers since they are obtained through inner products, providing real numbers. This, of could be negative.
 order to adapt automatically the kernel parameters. Indeed, there exists some adaptive methods for choosing the kernel parameters, e.g. [137] ; this is left for future work. 3.3. An electrical network interpretation of the commute-time kernel
In the previous section, the commute-time kernel was justified by a random-walk model on a graph. But it is also well-a scaling factor, to the effective resistance [18] between nodes in a derived electrical network. We provide here an electrical interpretation of the commute-time kernel K electrical network from the original graph G , where a capacity (the inverse of the resistance) a ing link; the capacity being 0 otherwise. A voltage v i is associated to each node i as well as a current i of the network, and (iii) the voltage is centered, v T e = 0, where v contains the v From Kirchhoff X  X  first law, we have simply states that the current traversing a link is directly related to the difference of voltage, i last equation in (4) provides Or, in matrix form, where e e k  X  X  I ee T = n  X  e k is the projection of e k on the column space of L ( e e vectors orthogonal to e ). The general solution to this linear system of equations is v  X  L We now pick up the voltage vector v that is centered, i.e. such that e
L e k  X  L  X  e k  X  l  X  k is also centered. Thus e T v  X  e T L  X  e tain v  X  l  X  k .

Consequently, the similarities provided by the commute-time kernel correspond to the centered voltages when a unit current is injected into the node of interest and uniformly removed from each node of the network. 4. Sample-space prototype-based kernel clustering
We now introduce a procedure for deriving prototype-based kernel versions of agglomerative clustering algorithms, where the prototype vectors are defined in the sample space. It relies on three steps. (2) Express the prototype vectors in the feature space in terms of the prototype vectors in the sample space (denoted as (3) Optimize the criterion with respect to the prototype vectors, h
This procedure is now applied in order to derive various kernel-based agglomerative clustering algorithms. 4.1. Kernel k-means be defined, in the feature space, as the total within-cluster inertia:
Eq. (7), x i is the node vector corresponding to node i and g k x m , is provided a priori by the user. Here, the prototype vector g tors belonging to this cluster k . Remember that the node vectors x preserving exactly the distances between nodes  X  the feature space.

We denote by X the n p ( p is the number of features in the feature space) data matrix containing the transposed node vectors as rows; that is, X =[ x 1 , x 2 , ... , x n ] T . Let us now define the following change of parameter: corresponding to the  X  X  X ernel trick X  (see [112] ). It aims to express the prototype vectors, g recompute the within-class inertia in terms of the h k and the inner products: where K  X  XX T ; k ii  X  X  K ii  X  x T i x i ; k i  X  Xx i  X  col
The k -means iteratively minimizes J by proceeding in two steps, (1) re-allocation of the node vectors while keeping the prototype vectors fixed, and (2) re-computation of the prototype vectors, h nodes fixed. Clearly, the re-allocation step minimizing J is to h k and setting the result equal to 0 provides left-hand side that Kh k is a linear combination of the k the k i . Therefore, one solution 3 to this linear system of equations is sample space. Therefore the prototype re-computation step is cluster, these degrees of membership are positive and sum to one.

Notice that Zhang and Chen proposed instead [138] proposed to compute the inverse of K in Eq. (11), which is not needed and does not allow to interpret h k as a membership prototype as in Eq. (13). On the other hand, MacDonald and Fyfe [83] derived their kernel self-organizing map algorithm (SOM) by translating the standard SOM from the feature space to the sample space, without expressing it as an optimization problem.

This generic methodology can easily be used in order to derive sample-space prototype-based kernel versions of other standard clustering algorithms, as demonstrated in the following sections. 4.2. Kernel fuzzy k-means
We now apply the same methodology for deriving a kernel fuzzy k -means. This time, the criterion is (see for instance [120]) where the u ik define the degree of membership of node i to cluster C ness of the membership functions. As for the kernel k -means, we perform the change of parameters (see (8)), leading to We thus introduce the following Lagrange function Moreover, taking the gradient with respect to h k provides
Thus, the re-computation of the prototype vectors is simply or component-wise,
Eqs. (17) and (20) are iterated until convergence. 4.3. Kernel Gaussian mixtures model by p k ). We consequently introduce the following Lagrange function
By taking the corresponding gradients with respect to u ik
The parameters p k are representing the a priori probabilities of belonging to a cluster and the membership functions, u cluster. 4.4. Convergence of the kernel clustering algorithms and some computational issues is symmetric and contains positive values on its diagonal, k the criterion J , leading to a monotonic decrease of this criterion at each iteration t . Let us denote this decrease as ( J ( t ) J ( t 1)) = d t 6 0 with d t P 0.

Now, by examining the constraints on the u ik and the h ki
Remember that the membership values satisfy 0 6 u ik 6 1 while the prototype vectors satisfy 0
Now, a rough lower bound can easily be obtained, which is enough for proving convergence. Considering each term of the 2
Therefore, the criterion J is bounded from below by J P J must have P 1 t  X  1 d t 6 D with each d t P 0, which implies lim minimum of the criterion J .

On the other hand, when considering the sigmoid commute-time kernel, the graph kernel clustering procedures neces-sitate the knowledge of the inverse of the Laplacian matrix of the graph, which can be an issue when dealing with large the inverse is usually dense and could not fit into main memory. One can conclude that our kernel-based methods do not scale well on large graphs. Therefore, density-based methods, which are exploiting local information, are probably more appropriate when dealing with large graphs.
 putation time. Designing kernel-based clustering algorithms able to mine large graphs will be tackled in further work. 5. Kernel hierarchical clustering 5.1. A kernel version of Ward X  X  hierarchical clustering
In this section, we propose a kernel version of Ward X  X  bottom-up hierarchical clustering. The algorithm starts with one the total within-cluster inertia (see for instance [120,125] ).

Initially, each observation forms a cluster, so that, in the feature space, g defined by Eq. (8), we thus have X T h k = x k ; pre-multiplying this equation by X , we obtain Kh initialized by
Now, when merging two clusters, say cluster k and cluster l , the new centroid g
By applying the transformation (8), we obtain the update equation for the prototypes h where h m is the prototype vector for the resulting merged cluster (once more, if X one solution, the particular solution (27) is chosen). We observe that if h h is simply 1/ n k if individual i belongs to cluster k and zero otherwise.

D J  X  J  X  after merge  X  J  X  before merge  X  X  n k n l n clusters k and l before merging them. When applying the standard transformation (8), we obtain single cluster containing all the data remains. 5.2. Kernel hierarchical clustering and spectral clustering
Let us now consider instead a top-down approach to hierarchical clustering: we want to split the data into two clusters maximum. Now, define a new prototype vector h  X  as lowing related optimization problem: maximize the quadratic form (29) with some additional constraints, 0).

If we choose the commute-time kernel, K = L + , we obtain a standard graph partitioning or spectral clustering technique on the first principal component of the graph, according to the commute-time distance (see [46,107] ). 6. Experiments
The main objectives of this section are (i) to compare the introduced kernel-based algorithms to other well-known algo-kernel clustering procedures ( k -means, fuzzy k -means, gaussian mixture, hierarchical). 6.1. Datasets description
Systematic comparisons between the different algorithms are performed on three real-world graphs. A short description is provided for each graph. 6.1.1. Newsgroup dataset
The Newsgroup dataset 4 is composed of about 20,000 unstructured documents, taken from 20 discussion groups (news-such as politics/mideast and politics/guns in subset G-5cl-A.

In order to reduce the high dimensionality of the feature space (terms), the following standard preprocessing steps are performed. (2) Porter X  X  stemming algorithm [101] is applied so that each term is reduced to its  X  X  X oot X . (4) The mutual information between terms and documents is computed. For a term y , the mutual information with each (5) The term-document matrix W is constructed with the remaining terms and documents. Element [ W ] (6) Each row of the term-document matrix W is normalized to 1.

Finally, the adjacency matrix defining the links between documents is given by the sum of all document-term-document paths connecting all pairs of documents through the terms they have in common. In other words, if W represents the term-document matrix containing the tf.idf factors, the adjacency matrix of the resulting document X  X ocument graph is provided by A = W T W .
 For example, the subset G-2cl-A is composed of 400 documents, and 2898 terms (stopwords being already eliminated).
After the preprocessing step, the term-document matrix is composed of 1,490 terms and 400 documents. The conversion to a document X  X ocument adjacency matrix finally provides a graph composed of 400 document nodes. 6.1.2. IMDb dataset This dataset is extracted from the well-known movie database website IMDb, as preprocessed by Neville et al. [92] and Macskassy et al. [84]. Movies released in the United States between 1996 and 2001 were gathered to form a network where the edge between two movie nodes are weighted by the number of production companies in common. A connected subgraph first weekend box-office receipts) or not (low-revenue).

The clustering task is to discriminate the high-revenue movies from the low-revenue ones, based on their shared produc-tion companies. Table 2 shows the number of nodes for each of the two clusters. 6.1.3. CORA dataset
First proposed by McCallum et al. [86] and then preprocessed by Macskassy et al. [84], Cora is originally a computer re-to cluster the seven different research domains (shown in Table 3 ) based on the citation network. 6.2. Experimental settings and parameter tuning
Let us now describe the experimental settings used in all our experiments. Suppose we have a graph of n nodes to be par-titioned into m clusters. For all kernel clustering algorithms, the prototype vectors h domly selecting m columns of the identity matrix I .
 provided below in this section.
 Notice that some parameters need to be tuned: The parameter a when computing the sigmoid transformation of the K The parameter q which controls the degree of fuzzyness for the kernel fuzzy k -means (see Eq. (17)). The parameter l for the kernel gaussian mixtures model (see Eq. (22)).

The parameters tuning was performed on an independent dataset. It was extracted from the newsgroup dataset and is the best results). Once the a value settled, the parameter value of the K gaussian mixtures.

The same methodology is then applied to the other kernels (see [47] for another experimental comparison between these kernels on a graph) in order to fix the parameter values: The parameter a for the Laplacian exponential diffusion kernel, defined as K The parameter a for the von Neumann diffusion kernel, defined as K The parameter a for the regularized Laplacian kernel, defined as K Based on the averaged results obtained on 30 runs of the clustering procedure, a is set to 1 for K for K RL .

To assess the impact of the sigmoid transformation, changes in the clustering performance when applying the sigmoid (denoted as K S VND ), 1 and 4 for the sigmoid transformation of the regularized Laplacian kernel (denoted as K 6.3. Results and discussions tering methods providing good performances on the newsgroup datasets are further investigated on the IMDb and the CORA datasets, and compared to the competing clustering methods. 6.3.1. Results on the newsgroup dataset
A full comparison was carried out on the nine subsets of the newsgroup dataset by considering the four different kernel applied with the 8 (4 2) different kernels (i.e., the commute-time kernel K K LED , the von Neumann diffusion kernel K VND , and the regularized Laplacian kernel K in Table 5 while the results for the five-classes subsets (G-5cl-A, G-3cl-B and G-3cl-C) are listed in Table 6 .
All these kernel-based methods are compared to the performances obtained by the spherical k -means [29], Ng X  X  spectral [37], and Reichardt X  X  simulated annealing [104] . The last four algorithms come from the communities detection domain a fair comparison of the clustering performances, the number of communities will be fixed. This adaptation can be easily obtained by stopping the iteration when the desired number of clusters is reached. These algorithms were chosen because they performed well in the comparative study published by Danon et al. [26].

As for the kernel-based methods, each algorithm is run 30 times (excepting the Reichardt X  X  simulated annealing which is Reichardt X  X  simulated annealing on the seven smallest, due to their large execution time. kernels. Moreover, the kernel k -means and the kernel fuzzy k -means combined with the K results on these datasets, with very stable performances. K and, indeed, it can be shown that the regularized Laplacian kernel tends to an approximation of the commute-time kernel with a low number of clusters, but its performance degrades when the number of clusters increases. This can be explained estimation of a gaussian distribution is quite sensible to outliers.
 van X  X  edge centrality-based method to discriminate communities when the graph fuzziness increases. We made the same observation in our experiments where Girvan X  X  method provides bad results. For a small number of clusters, Reichardt X  X  method offers competitive results, similar to the kernel k -means combined with K increases (especially for five clusters), the performances of Reichardt X  X  method drop down in comparison with K mal test on artificial graphs shows a decrease of performance for Duch X  X  extremal optimization when the graph node X  X  de-gree is low. On the other hand, the simulated annealing suffers from its large execution time. The results should be rithms to propose a natural number of clusters.
 Notice that an informal experiment was also performed, centering the von Neumann diffusion kernel and the regularized
Laplacian kernel before applying the sigmoid transformation, without any performance improvement. 6.3.2. Results on the CORA and the IMDb datasets
Based on the results obtained on the newsgroup subsets, only the kernel k -means and the kernel fuzzy k -means, which clusions are quite the same as for the Newsgroup dataset: the sigmoid commute-time kernel K
Laplacian kernel K S RL still obtain the best performance. 6.3.3. General discussion of the results It can further be observed that the K S CT fuzzy k -means provides slightly better results than the K can be discussed. Moreover, a close examination of the membership functions shows that, for some documents, the mem-bership value is almost equal for the five clusters. An example of such a document is shown here below. It is document 21759, picked from the topic religion/christian:
Document 21759: The text contains 80 lines devoted to a defence of the doctrine of predestination as applied to the sal-the post that one is replying to, but when the focus shifts, keeping the same Subject can cause confusion. The fuzzy membership vector for this document provided by the K to the topic computer/windowsx with a membership value of 0.25, cryptography/general with 0.20, politics/mideast with the correct topic of this message.

Finally, we can conclude that the kernel k -means and the kernel fuzzy k -means based on the sigmoid commute-time ker-
Moreover, both methods provide quite stable performances. One advantage of the commute-time over the regularized Lapla-cian kernel lies in the fact that it does not need any parameter adjustment. 7. Conclusions and further works
We introduced a family of methods allowing to cluster the nodes of a weighted graph by exploiting the links between sults are promising; indeed, the proposed methodology outperforms the other tested clustering and community detection algorithms on several graph nodes clustering problems.
 original matrix is sparse.

Further work will be devoted to the development and to the study of robust kernel clustering algorithms (inspired for eters [137] . Moreover, it would be interesting to study the behavior of the commute-time distance when the size of the
We also plan to develop kernel versions of other clustering algorithms and to tackle other challenging problems, such as the clustering of sequences.
 Acknowledgements
We thank the anonymous reviewers for their interesting remarks and suggestions that allowed us to improve signifi-cantly the quality of the paper.

This work was partially supported by the STRATEGO project funded by the Walloon Region, Belgium, and the OASIS+ pro-ject funded by the Belgian Science Policy.

References
