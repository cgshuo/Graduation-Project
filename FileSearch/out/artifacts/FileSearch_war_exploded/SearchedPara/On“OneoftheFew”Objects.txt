 Objects with multiple numeric attributes can be compared within any  X  X ubspace X  (subset of attributes). In applications such as com-putational journalism, users are interested in claims of the form: Karl Malone is one of the only two players in NBA history with at least 25,000 points, 12,000 rebounds, and 5,000 assists in one X  X  career. One challenge in identifying such  X  X ne-of-the-k  X  claims ( k =2 above) is ensuring their  X  X nterestingness. X  A small k is not a good indicator for interestingness, as one can often make such claims for many objects by increasing the dimensionality of the subspace considered. We propose a uniqueness-based interesting-ness measure for one-of-the-few claims that is intuitive for non-technical users, and we design algorithms for finding all interesting claims (across all subspaces) from a dataset. Sometimes, users are interested primarily in the objects appearing in these claims. Build-ing on our notion of interesting claims, we propose a scheme for ranking objects and an algorithm for computing the top-ranked ob-jects. Using real-world datasets, we evaluate the efficiency of our algorithms as well as the advantage of our object-ranking scheme over popular methods such as Kemeny optimal rank aggregation and weighted-sum ranking.
 H.2.8 [ Database Management ]: Database Applications X  data min-ing ; H.2.4 [ Database Management ]: Systems X  query processing ; K.4.0 [ Computers and Society ]: General Computational journalism, fact checking, skyband, ranking Raw data in various domains are becoming more widely accessible, e.g., play-by-play game logs for sports, databases of campaign fi-nance and voting records for politics, etc. An increasing number of news stories are driven by information extracted from data. Com-putational journalism [5, 6] is a nascent field about using computing to help improve effectiveness and reduce cost for journalism, which serves a vital role in our society. One important goal in computa-tional journalism is (semi-)automatic lead identification from data, i.e., finding interesting information nuggets from raw data that lead to further investigation and/or news stories around them. In this paper, we take a first step toward this goal by considering a popular form of claims exemplified by the following:  X  There is no player in NBA history with more points, more re- X  Rick Perry is one of the only three candidates in the 2012 US These two claims share a common structure: both are about an ob-ject being one of the few that  X  X tand out X  when compared according to a set of numeric attributes. More precisely, given a set of objects O , each with a set A of numeric attributes, we consider one-of-the-few claims of the following form: Here, we say o dominates o in B if o is no worse than o for all at-tributes in B ,and o is strictly better than o for at least one attribute in
B . Journalists are interested in two tasks: 1) finding all  X  X nterest-ing X  one-of-the-few claims from a given dataset; 2) ranking objects based on what one-of-the-few claims can be made about them. The first task allows journalists to identify claims that can be used in stories or serve as leads for further investigation. The second task provides an object-centric view that allows journalists to prioritize their investigation of particular objects (especially when there are many interesting one-of-the-few claims).
 Task 1: Finding Claims Our goal is to find interesting one-of-the-few claims in all  X  X ubspaces X  (i.e., subsets of attributes). One-of-the-few claims are closely related to the concept of k -skyband for multi-dimensional data [15]. Intuitively, the k -skyband of a set O of objects in subspace B is the subset of objects each dominated by fewer than k other objects in B . (The better-known notion of skyline is a special case of k -skyband where k =1 .) From the k -skyband in B , a one-of-the-few claim can be generated for each object in the skyband straightforwardly. While various algorithms exist for computing a k -skyband given k and B , they do not ad-dress the key challenge of ensuring  X  X nterestingness X  of the claims they find in a way that is easy for users to control and interpret. Although the parameter k can be tuned, it is a poor indicator of interestingness, as illustrate d by the following example. Example 1. Consider the set of nearly 4000 players in NBA his-tory, with stats such as career total points , rebounds , and assists . Being one of the top 50 leading scorers X  X .e., in the 50 -skyband for the single-attribute subspace { points }  X  X s quite an impressive feat. However, if we expand the subspace to { points , rebounds 142 players now fit the bill X  X .e., each is dominated by fewer than 50 others in { points , rebounds } . If we further include assists in the subspace, 324 (almost 9% of all) players will be in the 50 -skyband.
Example 1 clearly illustrates why we cannot use a universal k to ensure interestingness of one-of-the-few claims for different sub-spaces: the size of the skyband tends to increase rapidly as dimen-sionality goes up. For example, the expected number of 1 -skyband dividual attributes are uniformly and independently distributed [1], e.g., when objects are uniformly distributed in a box. With a fixed k , too many claims may be in high-dimensional subspaces, mak-ing them uninteresting. Clearly, k must be adjusted as | B | Furthermore, the appropriate setting for k is not simply as a func-tion of dimensionality, because data characteristics X  X .g., correla-tion among attributes X  X lso matter, as illustrated below.
 Example 2. We plot a subset of NBA players, with attributes points , rebounds , and assists per game, as points in subspaces rebounds } (Figure 1a) and { rebounds , assists } (Figure 1b). It is easy to see that the skybands in Figure 1a tend to be smaller than those in Figure 1b, because of the positive correlation between points and rebounds and the negative correlation between rebounds and assists . For example, the 3 -skyband in { points , rebounds tains 5 players (Jordan, C hamberlain, James, Baylor, and Pettit), which translate into 5 one-of-the-3 claims; on the other hand, the 3 -skyband in { rebounds , assists } contains 9 players (all except Jor-dan). Hence, no single choice of k is appropriate for these two subspaces of the same dimensionality.

Example 2 above clearly illustrates that we cannot hope to de-fine interestingness, which is data-dependent, by a function of k and | B | alone. Asking the user to pick the right k manually for each and every subspace is also infeasible. Our quest is to find an effective way of ensuring claim interestingness such that: 1) users are not required to tune lots of parameters; 2) the results are easy to understand and explain in layman X  X  terms. Both properties are critical for computational journalism, where journalists are often non-technical and the results need to be explained to the general public in stories. Furthermore, given the interestingness measure, we need to address the challenge of finding interesting one-of-the-few claims in all subspaces efficiently.
 Task 2: Ranking Objects Even with an appropriate definition of  X  X nterestingness, X  many objects may be the subject of at least one interesting one-of-the-few claim in some subspace. The task of ranking objects allows users to prioritize their effort in investigating objects. From our experience analyzing real data and preliminary user studies, different data domains and user preferences call for some degree of customization in ranking, as illustrated below. Example 3. B oth John Stockton and Larry Bird are inductees into the Naismith Memorial Basketball Hall of Fame, but they have dif-ferent playing styles. Stockton has the second highest assists per game in NBA history, but is not very impressive in points or re-bounds. Bird ranks 17th in points, 60th rebounds, and 44th in as-sists. Stockton and Bird exemplify what we call  X  X pecialized X  and  X  X ell-rounded X  objects, respectively. How to rank specialized ob-jects relative to well-rounded ones often depends on the context, or may simply be a matter of personal opinion.

A popular method for ranking objects based on multiple input rankings is Kemeny optimal rank aggregation [7], or Kemeny for short. It produces a  X  X onsensus X  ranking that minimizes the num-ber of pairwise disagreements (in the relative ordering of two ob-jects). When aggregating object rankings under individual attributes, Kemeny tends to downgrade objects that rank extremely high in few attributes but considerably low in other attributes. For Exam-ple 3 above, Bird, who is well-rounded, would be ranked as the 9th by Kemeny, while Stockton, who is specialized, would be as low as the 139th, which may not be acceptable to some.

While Kemeny leaves no option for customization, another pop-ular method, weighted-sum ranking , exposes too many knobs. With weighted-sum, a user specifies a preference vector, whose compo-nents represent weights assigned to attributes; objects are ranked according to their projection onto this vector (i.e., weighted com-bination of their attribute values). For a d -dimensional dataset, the user needs to specify d weights; even if we learn these weights automatically, training examples must be provided. Such require-ments may overwhelm journalists with little time or technical ex-pertise. Our goal is to devise a ranking scheme with as few knobs as possible, which allows customization without overwhelming users. Main Contributions First, we propose a simple but effective def-inition for the interestingness of a claim based on its  X  X niqueness. X  For a one-of-the-few claim (with a particular k in a particular sub-space B ) to be interesting, we require that this claim (with the same k and B ) cannot be made for more than  X  objects. Unlike k ,  X  is a user-defined threshold that applies universally to all subspaces, sig-nificantly reducing the burden on the user to define interestingness. For each subspace, our definition automatically adapts k in a data-dependent way, and naturally excludes those high-dimensional sub-spaces where no claims are unique enough. Furthermore,  X  is easy to understand for non-technical users.

Based on this definition, we introduce the problem of finding all interesting one-of-the-few claims across all non-empty subspaces, given the uniqueness threshold  X  . The fact that our k is data-dependent and not fixed raises unique challenges not addressed by previous work on computing skylines and skybands. We devise ef-ficient algorithms that avoid redundant computation. In particular, we are able to improve the worst-case complexity of finding all in-teresting claims in a subspace from O ( | O | 2 ) to O (  X  attractive because  X  in practice is small for claims to be unique.
Building on the definition of interestingness, we propose a novel scheme for scoring and ranking objects based on the aggregated interestingness of claims involving them. One key insight distin-guishing our scheme from others is that we in effect aggregate ranks across all non-empty subspaces as opposed to just individ-ual attributes. Our scheme supports tuning by a single parameter  X  , which captures user preference between specialized and well-rounded objects, and overcomes th e inflexibility of Kemeny with-out resorting to an overwhelming number of knobs like weighted-sum. Extending the algorithms for finding all interesting claims, we show how to compute top-ranked objects efficiently given  X  . We experimentally demonstrate, on real datasets, that our scheme is able to produce rankings comparable to Kemeny and weighted-sum while offering more effective customization. Preliminaries Consider a set O of n objects, each with d numeric attributes A = { A 1 ,A 2 ,...,A d } .A subspace is (more precisely, spanned by) a subset of the attributes. The set of all subspaces of A forms a lattice under containment r elation. We say that subspace B 1 is an ancestor ( descendant ) of subspace B 2 if B 1  X  B B 1  X  B 2 ). We say B 1 is a parent ( child )of B 2 if B 1  X  B B 1  X  B 2 ) and their cardinalities differ by one.

We say o 1 dominates o 2 in subspace B , denoted o 1 B o i)  X  A  X  B ,o 1 .A  X  o 2 .A , and ii)  X  A  X  B ,o 1 .A &gt; o dominance is transitive: if o 1 B o 2 and o 2 B o 3 ,then o Definition 1 (Dominating Subset, k -Skyband, Skyline, Tier) .  X  The dominating subset of o  X  O in subspace B  X  A , denoted D B ( O ,o ) , is the subset of objects that dominate o in D B ( O ,o )= { o  X  O | o B o } .Let  X  B ( O ,o )= | D B ( denote the size of the dominating subset.  X 
The k -skyband ( k  X  1 )of O in subspace B , denoted S k B is the subset of objects in O where each is dominated by fewer than k objects in O ; i.e., S k B ( O )= { o  X  O |  X  B ( O  X 
The skyline of O in subspace B is S 1 B ( O ) , i.e., the 1 -skyband.  X 
The i -th tier ( i  X  1 )of O in subspace B is the subset of objects in O where each is dominated by exactly i  X  1 objects in O { o  X  O |  X  B ( O ,o )= i  X  1 } .

Clearly, by definition, the k -skyband S k B ( O ) is the disjoint union of all i -th tiers with i  X  k , and the difference between the k -skyband and the ( k  X  1) -skyband is the k -th tier. 1 To illustrate, consider the set O of 10 NBA players and subspace B = { rebounds , assists } showninFigure1b. D B ( O , Stockton ) = { Johnson } ,so  X  B ( O , Stockton )=1 . S 1 B ( O )= { Robertson , Bird , Chamberlain } is the skyline (or 1 -skyband), which is also the 1 -st tier. S 2 B ( O )= S 1 B ( O )  X  X  Stockton , Baylor , Pettit is the 2 -skyband, where Stockton, Baylor, and Pettit are in the 2nd tier and each dominated by exactly one object in O . S 3 B skyband, additionally includes the 3rd tier { James , Abdul-Jabbar leaving only Jordan out, as mentioned in Example 2.
 Problem Statement As motivated in Section 1, while each object in the k -skyband translates into a one-of-the-few claim, we measure the interestingness of this claim by the number of objects for which similar claims can be made, i.e., the size of the k -skyband. Instead of struggling with setting k , which depends on the subspace and ob-ject distribution, a user should be able to specify a single threshold  X  that caps the number of similar claims. Therefore, we introduce the concept of top- X  skyband below. While the concept is closely related to k -skyband, a crucial difference is that a top- X  skyband is defined by its size, while a k -skyband, defined by its number of tiers, can be arbitrarily large.
 Definition 2 (Top- X  Skyband) . Given  X   X  1 ,the top- X  skyband of a set of objects O in subspace B , denoted  X   X  B ( O ) (or  X   X  if context is clear), is the  X  k -skyband where  X  k =max { k |  X   X | S k In other words, the top- X  skyband is the largest skyband whose size does not exceed  X  .

The fact that an object o belongs to the top- X  skyband with the k -th tier as its last non-empty tier X  X r alternatively, the k -skyband with size no more than  X   X  X ranslates into the following statement: Intuitively,  X  measures the uniqueness of the claim made by the first part of the above statement. For example, in Figure 1a, suppose we set  X  =3 .  X   X  = { Chamberlain , Jordan } is the 1 -skyband. The 2 -skyband would be too big, because it also contains Pettit, Baylor, and James, and has size 5 &gt; X  . Note that the 3rd tier is empty X  X o player is dominated by exactly two others in this example X  X o the 3 -skyband (recall Example 2) is the same as the 2 -skyband.
The problem of finding all interesting one-of-the-few claims can now be formulated as follows: Definition 3 (Finding Top- X  Skybands in All Subspaces) . Given the set O of objects and a user-specified threshold  X  , find  X  for every non-empty subspace B  X  A .

For each subspace, the membership of an object in  X  sponds to a one-of-the-few claim that cannot be made for more than  X  objects. This definition leads naturally to some desired fea-tures. In a single-attribute subspace,  X   X  contains essentially the top  X  objects ranked by this attribute. As the subspace dimensionality goes up, the number of tiers in  X   X  decreases in an automatic, data-dependent manner until these tiers contain no more than  X  objects. Thus, with this problem formulation, users need not manually pick the number of tiers in the skyband for each subspace. To illustrate, consider again Example 2. Suppose the user sets  X  =8 . For sub-space { points , rebounds } (Figure 1a),  X   X  would be the 5 -skyband. In contrast, for { rebounds , assists } (Figure 1b),  X   X  would be the 2 -skyband. Finally, in high-dimensional subspaces where so many objects are on the skyline that no claims are interesting, our prob-lem formulation correctly leads to an empty  X   X  .
 Overview of Solutions The rest of this section discusses how to find all interesting one-of-the-few claims. At a high level, we 1) tra-verse the lattice of subspaces in some manner, and 2) compute  X  for each subspace we visit. Techniques for improving efficiency ex-ist both across subspaces and within each subspace. For finding  X  in a given subspace B , we propose two algorithms in Sections 2.1 and 2.2. We then show in Section 2.3 how to explore the lattice of subspaces using these algorithms as subroutines.
 Note that computing  X   X  for a single-attribute subspace { ply involves finding the top  X  objects sorted by A ; algorithms in Sections 2.1 and 2.2 are needed only when | B | &gt; 1 .

Also note that it is possible to devise solutions by adapting exist-ing techniques from literature. We present one such solution here, which we call Baseline . For lattice traversal, Baseline adopts the strategy of bottom-up skyline (BUS) of Pei et al. [17], who study the problem of computing the skyline in every subspace. BUS can be extended to compute k -skyband if k is given. In each subspace, Baseline starts with k =1 , and computes the k -skyband and incre-ments k , iteratively, until the k -skyband contains at least  X  objects. Obviously, this iterative process of finding the right k leads to a lot of redundant computation. Our new algorithms avoid such redun-dant computation, and we experimentally validate their advantages over Baseline in Section 4. As it turns out, all objects of O in the ( k +1) -th tier must lie on the skyline of the set of remaining objects after those in the k -skyband are taken out. This simple but useful observation has probably been made in other contexts too; we state it as a lemma here for com-pleteness (see [23] for proof): Algorithm 1 : Progressive ( O , B , X  )
Lemma 1 suggests the following strategy, which we call Pro-gressive (Algorithm 1), for computing the top- X  skyband for a given subspace. Given  X  , Progressive computes the answer  X  tier by tier, starting from the first. By Lemma 1, to obtain the next non-empty tier, we first compute (Line 3) the skyline S for the set of remaining objects (those outside the current  X  ). Objects in the next non-empty tier (  X  X  on Line 5) are those in S whose dominating subsets in O are the smallest in size. We add this tier to  X  as long as their set union has no more than  X  objects. It is easy to see that at the end of each iteration, the invariant that  X = S k B Progressive terminates if the addition of the next non-empty tier causes the size of  X  to exceed  X  .

Note that in Lines 4 and 5 we compute the size of the dominating subset for o  X  S in  X  instead of O . This optimization is correct because, being on the skyline of O \  X  , o is not dominated by any object in O \  X  ,so  X  B ( X  ,o )=  X  B ( O ,o ) .
 Further Optimizations For clarity of presentation, Algorithm 1 leaves out some details, which we describe below. Suppose that in the previous iteration, the skyline computed was  X  S ,and  X  was added to the answer set. For the current iteration, it is easy to see that  X  S \  X   X   X   X  S ; i.e., all remaining objects in again in the skyline S to be computed. This observation leads to two optimizations. 1) We can speed up successive skyline com-putations on Line 3 across iterations. Specifically, we adapt the SUBSKY algorithm of Tao et al. [19] (in general any skyline algo-rithm can be used). The original SUBSKY uses an index whose size is linear in | O | to help compute the skyline of O in any sub-space. During execution, SUBSKY always maintains the skyline for the subset of objects that it has examined. In our adaption of SUBSKY,weremovefrom(acopyof)theSUBSKYindexany object added to  X  , and we  X  X eed X  each invocation of SUBSKY with  X  S \  X   X   X  instead of starting it with an empty skyline. 2) We can reduce the number of dominance tests involved in determining  X  ( X  ,o ) for o  X  S on Lines 4 and 5. For any o  X   X  S \  X   X  have already computed  X  B ( O ,o ) in the previous iteration, so there is no need to recompute it.
 Complexity Following convention [19], we measure the perfor-mance of our algorithms by the number of dominance tests. Pro-gressive performs two types of such tests: 1) those involved in com-puting the skyline of the remaining objects, and 2) those involved in computing the sizes of the dominating subsets in O for objects on that skyline. The number of type-1 tests depends on the skyline algorithm; Tao et al. [19] describes various techniques to reduce it for SUBSKY. However, in the worst case, |  X  | is almost  X  in the final iteration, while | S | can be roughly | O | X   X  (i.e., the next non-empty tier contains nearly all remaining objects). In this case, the total number of dominance tests would be  X ( | O | 2 ) . Progressive has poor worst-case complexity: it computes the entire next tier in order to decide whether to add that tier to the answer, but Algorithm 2 : OnePass ( O , B , X  ) that tier can be larger than  X  . In this section, we show how to tame the complexity with another algorithm OnePass . This algorithm works by considering objects one by one in a particular order. Each object is either added to the answer set  X  or dropped. Once  X  has more than  X  objects, the last tier is peeled off. The processing order is chosen in a  X  X afe X  way, as defined below, which allows OnePass to cap |  X  | at  X  at all times, thereby bounding the number of dominance tests to  X  | O | , in contrast to O ( | O | 2 Definition 4 (Safe Order 2 ) . An order for a set O of objects is safe (for OnePass )if o precedes o whenever o B o .

Algorithm 2 describes OnePass . The details of implementing the safe order will be given later in this section. Here, we first ex-plain the algorithm and establish its correctness, the crux of which is captured by the lemma below (see [23] for proof).
 Lemma 2. The following invariants are true at the end of each iteration of OnePass  X  X  main loop, where O denotes the set of all objects processed so far. (I-1) For all o  X   X  , c [ o ]=  X  B ( O ,o ) . (I-2)  X = S k B ( O ) . (I-3) | S k +1 B ( O ) | &gt; X  (if | O | &gt; X  ).

Consider the next object o in the safe order. OnePass first checks whether o is dominated by at least k objects in  X  (Lines 3 X 6). If yes, o is ignored because it would be in the ( k +1) -th or later tier (by (I-1)) and therefore cannot be in  X   X  B ( O stop counting as soon as c [ o ] reaches k . Heuristically, we check o against  X  X etter X  objects in  X  (i.e., those with smaller dominating sets) first, in hope of reaching k sooner with fewer dominance tests.
If c [ o ] &lt;k , OnePass adds o to  X  (Lines 8) and remember the count c [ o ] . If doing so makes |  X  | exceed  X  we remove the last tier from  X  and update k accordingly (Lines 9 X 11), to preserve (I-2) and (I-3). If k drops to 0 (Line 12), that means even the skyline has size bigger than  X  (by (I-3)), so we can terminate (and return without processing the remaining objects. After all objects in processed,  X  is the top- X  skyband of O in B , by (I-2) and (I-3). Producing a Safe Order The simplest approach for OnePass to produce a safe processing order is to sort O by B (lexicographi-cally, with an arbitrary ordering among the attributes), which takes O ( | O | log | O | ) time. To avoid the cost of a full sort, we precom-pute, for each of the d attributes in A ,aversionof O sorted by that attribute. Given a subspace B , One Pass picks the attribute A with the largest domain (a heuristic also adopted in [2, 17]), and use the version of O sorted by A . During processing, if OnePass finds that a group of objects tie in A , OnePass further sorts this group by the full B . As a further optimization, before sorting this group, OnePass first performs Lines 3 X 6 on each object o in the group, and removes those with c [ o ]  X  k ; the filtered group is then sorted and further processed. In the worst case, OnePass still needs to pay O ( | O | log | O | ) for sorting. In practice, however, we have found our heuristics effective in eliminating sorting, because most ties tend to occur later in processing; by that time, most objects will be eliminated by Lines 3 X 6. Furthermore, the cost of filtering is low because  X  and k are typically small.
 Complexity Because OnePass caps |  X  | at  X  at all times, the number of dominance tests is bounded by  X  | O | . As explained above, sorting adds O ( | O | log | O | ) in the worst case, though in practice the extra cost is rarely incurred. Furthermore, in high-dimensional sub-spaces, k decreases rapidly as OnePass examines more objects. It is likely that an object will be discarded after a few dominance tests. OnePass also detects the case when the size of the skyline would exceed  X  , and is able to terminate without examining the whole Thus, OnePass is expected to run faster in practice than the bound suggests, particularly for high-dimensional subspaces. Finding all interesting one-of-the-few claims requires computing how to accomplish this task using either Progressive or OnePass as the building block; details can be found in [23].

For Progressive , computation in every subspace starts with find-ing the skyline. For this part, we directly apply the techniques of Pei et al. [17], who study the problem of computing the skyline in every subspace. Their techniques traverse the lattice of subspaces in either bottom-up or top-down order, and try to share computation across subspaces. For a given subspace B , once we have computed the skyline using these techniques, we proceed with Progressive to compute the rest of  X   X  as discussed in Section 2.1.

For OnePass , we traverse the lattice of subspaces top-down, i.e., going from low-to high-dimensional subspaces. We use the top-down technique from Pei et al. [17] to help compute the skyline in a subspace using those found in its parent subspaces. Moreover, we use the  X   X  in parent subspaces to fine-tune the safe processing order in the current subspace.

Furthermore, during a top-down lattice traversal, we use two tests to prune uninteresting hi gh-dimensional subspaces from the search. 1) If the  X  X istinct-count X  of skyline objects in number of distinct projections onto B ) is greater than  X  ,  X  be empty for all descendant subspaces. 2) Given B , if the union of skyline objects from parent subspaces already contains more than  X  objects,  X   X  B ( O ) must be empty. This section presents our solution for ranking the set jects with attributes A , based on what one-of-the-few claims can be made about them across subspaces, and how interesting these claims are. Section 3.1 proposes a novel scoring scheme that cap-tures varying user preferences w ith a single parameter, while Sec-tion 3.2 describes how the algorithms in Section 2 can be leveraged to compute top-ranked objects. A common approach for ranking a set O of objects (e.g., political candidates) is to combine multiple ranked lists of them (e.g., by voters). Traditionally, the scores for objects in a single list are as-signed according to a positional scoring vector (or function) [25], v , which maps a rank i ( 1  X  i  X | O | ) to a numeric score v ( i ) , such that v ( i )  X  v ( i +1) . Some examples include Borda ,where v ( i )= | O | X  i ,and Plurality ,where v (1) = 1 and v ( i )=0 for i&gt; 1 . Then, the overall object ranking is done according to the aggregate score of each object, usually defined as the sum of its scores across all ranked lists.
 A straightforward approach would be to have one ranked list of by each attribute in A , and sum up an object X  X  scores across these lists. However, this approach has serious drawbacks, particularly in handling data with correlation. Suppose o 1 is ranked high for two correlated attributes (e.g., contributions from finance and real estate sectors, or minutes played and points scored), while o ranked equally high in two anti-correlated attributes (e.g., contri-butions from oil companies and environmental groups, or rebounds and assists). Since it is harder to rank high for two anti-correlated attributes, we should score o 2 higher than o 1 overall. However, the approach of summing scores over individual attributes will assign thesamescoreto o 1 and o 2 (assuming all other factors are equal). All-Subspace Positional Scoring with Ties (APST) To r es ol ve the issue above, we propose All-Subspace Positional Scoring with Ties . The key novelty is to aggregate object scores not just across individual attributes, but instead over all non-empty subspaces. The dominance relationship in a multi-dimensional subspace B likely does not induce a totally ranked list; hence, we draw insight from Section 2 to score objects using the partial order in a way that re-flects the uniqueness of one-of-the-few claims in B . The result is a scoring scheme that naturally adapts to data distributions. Definition 5 (All-Subspace Positional Scoring with Ties (APST)) . For each non-empty subspace B  X  A ,sort O in non-descending order of  X  B ( O ,o ) , the size of the dominating subset for each o O ; ties are broken arbitrarily. Denote this ranking by  X  B  X  ( o )  X  [1 , | O | ] is the rank of o in this ranking. Let [  X  denote the range of ranks occupied by objects that tie with o in  X  ( O ,o ) ; i.e.,  X  B ( o )=min {  X  B ( o ) |  X  B ( O ,o )=  X  and  X  B ( o )=max {  X  B ( o ) |  X  B ( O ,o )=  X  B ( O ,o ) positional scoring function v where v (1) &gt;v (2) &gt;  X  X  X  the (APST) score of object o in B , denoted  X  B ( o ) , is given by: Overall, the APST score of o , denoted  X ( o ) , is the total of o  X  X  score over all non-empty subspaces:  X ( o )= B  X  A , B =  X   X  B ( o ) .
Intuitively, for each subspace B , APST sorts objects by tiers, and each tier occupies a range of ranks. The total score assigned by v to such a range of ranks is distributed equally among objects in the corresponding tier, as we consider them ties in B .Thelarger the tier, the less  X  X nique X  each object, and the smaller the share each object will receive. A number of desirable properties follow directly from the definition. For example, APST favors tiers that are smaller in size or occupy earlier ranges of ranks; see [23] for a more formal discussion.

Furthermore, aggregating scores over combinations of attributes make APST naturally adaptive to correlations in data. Consider again the example earlier in this section, where o 1 is ranked high for two correlated attributes { A 1 ,B 1 } and o 2 is ranked equally high in two anti-correlated attributes { A 2 ,B 2 } . Since it is rare to rank high for anti-correlated attributes, few objects will be in o an earlier tier in { A 2 ,B 2 } ; therefore, APST will score o {
A 2 ,B 2 } . On the other hand, for o 1 ,since A 1 and B 2 lated, there will likely be more objects dominating o 1 in than those dominating o 2 in { A 2 ,B 2 } . Thus, all other factors being equal, APST will score o 2 higher than o 1 overall. Adjustable Discounted APST (APST- X   X   X  ) What remains to be discussed is the choice of the positional scoring function v .To make our scoring scheme easy to use, we have so far consciously avoided introducing any tuning parameters. However, as motivated in Example 3, we would like some degree of customization for ranking  X  X pecialized X  objects relative to  X  X ell-rounded X  ones. Re-call that a specialized object is exceptional in few attributes (such as Stockton in assists), while a well-rounded object is exceptional in none, but reasonably good in many, so as to be exceptional when all those attributes are combined (such as Bird). To capture user X  X  wide ranging preferences for one or the other, we design our posi-tional scoring function with a single tunable parameter to achieve that flexibility while retaining APST X  X  desirable properties. Definition 6 (Adjustable Discounted APST (APST- X  )) . Let  X  be a real number in (0 , 1) .The adjustable discounted APST (APST- X  ) score of an object o  X  O is o  X  X  APST score with positional scoring function v  X  ( i )=  X  i  X  1 .

Intuitively, the discounting factor,  X  , controls how much more higher ranked objects weigh against lower ranked objects, thereby affecting how specialized and well-rounded objects score relatively overall. In a higher-dimensional subspace B , tiers tend to be larger. Hence, an object that is dominated by few others in B (but by more objects in subsets of B ) tend to score lower in B , compared with an object with the same number of dominating objects in a lower-dimensional subspace B . With a smaller  X  , the score gap is wider, so overall, it is more difficult for well-rounded objects to surpass specialized ones, whose scores come mostly from contributions by low-dimensional subspaces. In Section 4, we will see how effective  X  is as a tuning knob on real data X  X ompared with weighted-sum (discussed in Section 1), APST- X  has only 1 knob instead of d ,but produces comparable rankings as highly tuned weighted-sum. Comparison with Kemeny Optimal Rank Aggregation In ad-dition to flexible approaches where users are allowed to specify their preferences, studies in social choice theory have also investi-gated an alternative where some notion of  X  X ptimality X  is defined for the resulting ranks. A popular re presentative of this approach is Kemeny (optimal rank aggregation) . As discussed in Section 1, given a set of rankings for O , a Kemeny optimal rank aggregation is a ranking that minimizes the total number of pairwise disagree-ments between this ranking and the input rankings.

One natural way to use Kemeny is to apply it to the d rankings ac-cording to the individual attributes in A . This approach, which we call Kemeny-d , has several issues. First, finding a Kemeny optimal aggregation is NP-hard in | O | [7]. We would prefer an approach that is computationally more tractable. Indeed, ranking objects us-ing APST- X  has complexity polynomial in | O | .

Second, by definition, Kemeny-d is strongly biased against spe-cialized objects, which rank high in few attributes but low in many, because ranking them high overall will incur many disagreements. As discussed in Section 1 following Example 3, based on points, rebounds, and assists per game, Kemeny-d would rank John Stock-ton low, even though he is a Hall-of-Famer who has the second highest assists per game in NBA history. In contrast, with a small  X  , APST- X  would rank it high, because it likely lies on a small sky-band for roughly half of the 2 d  X  1 non-empty subspaces X  X amely those containing the attribute in which the object specializes.
Third, Kemeny-d sometimes fails to suggest a rank for a worthy object p . More precisely, there may be many consensus rankings that are all optimal in the Kemeny sense, where p is ranked differ-ently, sometimes below objects that can be considered obviously inferior to p . For an illustrative example, as well as how APST- X  avoids this issue, see [23].

Fourth, Kemeny-d does not allow for customization X  X f it fails to recognize a worthy object ther e is little that a use r can do. On the other hand, the discounting factor  X  in APST- X  provides a single, effective knob that allows users to choose their preference between specialized and well-rounded objects. Indeed, we have seen from above how the choice of  X  helps overcome the issues faced by Ke-meny. More detailed results on real data are shown in Section 4.
Finally, instead of Kemeny-d , it is conceivable to use Kemeny to aggregate all 2 d  X  1 rankings according to all non-empty subsets of attributes in A . 3 We are not aware of any previous work ex-ploring this approach. While this approach could potentially alle-viate the second and third issues above, it still suffers from the first (high computational complexity) and last (lack of any customiza-tion). This approach would require the same amount of effort as APST- X  just to prepare the 2 d  X  1 input rankings in all subspaces; then, its rank aggregation is more expensive and not tunable. Computing the exact APST- X  scores of all objects in subspace en-tails computing  X  B ( O ,o ) for all o  X  O and non-empty B  X  A which takes O (2 d | O | 2 ) time using a naive algorithm. However, for the purpose of identifying objects most worthy of further investiga-tion, we care only about the top-ranked objects. This observation leads to the following problem definition: Definition 7 (Finding Top  X  Objects) . Given a set O of objects, a discounting factor  X   X  (0 , 1) , and  X   X  1 , find the top  X  objects in O ranked by APST- X  scores.

We show how to extend the algorithms in Section 2 to efficiently compute approximate answers to the above question. We provide only high-level insights here; see [23] for a full discussion with more technical details. The key observation is that, in each sub-space, we need to compute only enough number of tiers in order to not miss any top objects overall, because score contributions from memberships in subsequent tiers are so small that they have little influence over whether an object can be in the top  X  overall.
Given an error tolerance &gt; 0 , we can compute a list of objects, where each object o gets an approximate score  X   X ( o )  X  ,  X ( o )] . Roughly speaking, we divide the error tolerance among the subspaces to be considered. Within a subspace B with share of error tolerance, we use Progressive or OnePass to  X  X row X  the skyband up to some top- X  skyband such that any object outside it will receive a score below B in B . We add the objects in this sky-band to the output list (or update their scores if they are already in it). With some care, we can ensure that OnePass retains its advan-tage over Progressive ; i.e., it avoids computing the entire  X  X ext X  tier, which could include all remaining objects. The idea is that we only need to see enough number of objects in this tier before know-ing that all its objects must score below B , because APST scores get  X  X iluted X  by larger tiers. When we are done with B , we can derive a tighter bound (hopefully much less than B ) for errors in B , and use it to update the error tolerance for remaining subspaces.
The user does not need to choose manually. A reasonable de-fault is  X   X   X  1 , the value of the positional scoring function at rank  X  . With this setting, we can guarantee that any object not in the output list cannot be among the top  X  overall. The output list may contain more than  X  objects, and can be used to determine which part of this ranking is guaranteed to be accurate. As future work, we are developing an  X  X nline X  version of the algorithm where is incrementally tightened when given additional time or until the top  X  objects can be accurately separated from the rest.
Figure 3: Running time on synthetic data (varying d ). All algorithms were implemented in C++ and tested on a machine with Intel Core i7-2600 3.4GHz processor and 7.8GB memory.
We use three datasets on NBA players. 4 NBA1 contains the ca-reer total statistics for n  X  4 K players. There are d =15 per-formance attributes, including the total number of games played , points , rebounds , etc., over the players X  whole careers. NBA2 con-tains the career average statistics for the same set of n ers. It has the same set of attributes as NBA1 except number of games played (hence d =14 ), and the attribute values for a player are derived by dividing the corresponding values in NBA1 by the number of games played by the player. NBA3 contains the game-by-game statistics for each player. There are a total of n records with d =14 performance attributes.

We also use synthetic datasets to test algorithm performance X  correlated ( CORR ), anti-correlated ( ANTICORR ), and indepen-dent ( IND ), with varying size and dimensionality. CORR and AN-TICORR are generated by first sampling data points randomly from a multivariate Gaussian distribution; then, we stretch all points in the direction of (1 , 1 ,..., 1) to produce CORR, and we shrink them in the direction of (1 , 1 ,..., 1) to yield ANTICORR. This way, the attributes are pairwise correlated or anti-correlated. Additional results are in [23], including those on the National Research Council survey of 127 computer science programs. Given a dataset, a particular  X  value, and an algorithm, we use the algorithm to find the top- X  skyband for each and every subspace. The total elapsed time for the 2 d  X  1 nonempty subspaces mea-sures the algorithm X  X  efficiency. We compare Baseline (Section 2), Progressive (Algorithm 1), and OnePass (Algorithm 2). Figure 2 shows the execution time of Baseline , Progressive ,and OnePass , under varying  X  =10 , 20 ,..., 100 ,onbothNBA1( n = 4 K)andNBA3( n = 400 K). On both the small and large datasets, our algorithms significantly outperform the baseline approach.
On the small NBA1 dataset (Figure 2(a)), OnePass is less ef-ficient for small  X  and starts to outperform Progressive as  X  in-creases. The reason is that the dimensionalities of subspaces in which top- X  skyband is empty but cannot be pruned (by techniques in Section 2.3) increase with  X  . Computing the skylines for these subspaces gets more expensive as SUBSKY becomes less efficient. Results on NBA2 are similar to NBA1 and hence omitted.

On the large NBA3 dataset (Figure 2(b)), we also see that the running time of Progressive grows faster than OnePass as  X  in-creases, for the same reason above. Moreover, attributes in NBA3 have smaller correlations than in NBA1, which induce larger sky-lines, making Progressive even less efficient. Hence, OnePass starts to outperform Progressive when  X  is small.

On both datasets, we observe the running time of OnePass to be roughly linear in  X  , confirming the analysis in Section 2.2. Varying d ,Fixed n = 100 Kand  X  = 100 : Figure 3 shows that both Progressive and OnePass are faster than Baseline by orders of magnitude (vertical axis has logarithmic scale). Progressive runs faster than OnePass on correlated data (a). Their performances are comparable on anti-correlated (b) and independent data (c). We observe that the running times on CORR are much longer than those on ANTICORR and IND for all algorithms. The reason is that top- X  skybands in CORR may be non-empty even in high di-mensional subspaces, while for ANTICORR and IND, they tend to be empty and can be quickly detected and pruned as discussed in Section 2.3. When the dimensionality is high, due to the way AN-TICORR is generated for ensuring pairwise anti-correlation among all attributes, ANTICORR becomes similar to IND; hence, we omit the results on ANTICORR in the following discussion.
 Varying n ,Fixed d =15 and  X  = 100 : From Figure 4, we ob-serve again that Progressive and OnePass outperform Baseline by orders of magnitude, and Progressive slightly outperforms OnePass . Their running times increase roughly linearly by n .
 Varying  X  ,Fixed n = 100 Kand d =15 : Figure 5 shows that Progressive and OnePass significantly outperform Baseline on both CORR and IND. For all three algorithms, their running times grow nearly exponentially in  X  (vertical axis has logarithmic scale). Progressive is slightly faster than OnePass on CORR. On IND, OnePass is slower than Progressive under small  X  but be-comes faster as  X  increases. Figure 6: Comparison of rankings by number of HoFers in top-k . Figure 8: Effect of  X  on the APST- X  ranking of NBA players.
Overall, these experiments further confirm the significant perfor-mance advantage of Progressive and OnePass over Baseline . We evaluate the quality of the rankings by APST- X  , Kemeny-d , and weighted-sum for the NBA2 dataset (career average statistics). For each ranking, we measure its quality by the number of Hall of Fame inductees (HoFers for short) found among the top-k players according the the ranking, under various k values.

We use 7 out of the 14 attributes in NBA2 for ranking because the dataset did not record statistics such as steals , blocks ,and minutes for games played before 1971. Including these attributes would unfairly underrate the players from the early days. We do not use NBA1 (career total performance) because it would also underrate earlier players, as they did not play as many games as recent ones. APST- X  vs. Weighted-Sum We var y  X  in APST- X  to produce different rankings. For weighted-sum, the weights on the attributes are determined as follows. We first find a linear classifier to sepa-rate HoFers from non-HoFers that minimizes the number of inaccu-rately classified players. We then use the unit vector perpendicular to the linear classifier as the weight vector in weighted-sum.
Figure 6 compares weighted-sum, APST-.99, APST-.5, and APST-.01, by the numbers of HoFers found among the top k players in their rankings, respectively (ignore the curve for Kemeny-d for now). We can see that APST- X  contains visibly more HoFers in top-k under most combinations of  X  and k considered. Note that the performance of all ranking methods are negatively affected by active players who would be good candidate HoFers in the future, but will not be eligible until 5 years after retirement. 5 at k =80 ,thetop k players identified by weighted-sum include 48 HoFers. Among the remaining 32 , 23 are not yet eligible; 9 are eligible but not HoFers.
 APST- X  vs. Kemeny-d For Kemeny-d , solving for the optimal ranking proved computationally challenging on this real dataset, re-inforcing our discussion in Section 3.1. We model the optimization as an integer program (IP) and solve it with CPLEX. 6 Recall that there are about 4K players. On our reasonably powerful machine, CPLEX caused memory overflow with close to 300 players, and it failed to find solutions with more than 200 players because of pre-cision issues. Thus, we improvise by first identifying 200 players, who are the set union of the 7 top-40 lists for the 7 attributes used for ranking. Then, we run Kemeny-d on these 200 players only.
In Figure 6, we can see that in most cases APST- X  rankings con-tain noticeably more HoFers in top-k than Kemeny-d . The scale limitation Kemeny-d is to blame: there are 91 HoFers in the 4K players in NBA2, but only 59 of them are among those 200 consid-ered by Kemeny-d . In contrast, APST- X  and weighted-sum have been computed over all 4K players. For comparison disregarding Kemeny X  X  scale limitation, we have also used APST- X  to rank the same subset of 200 players; in that case APST- X  and Kemeny-d showed comparable performan ce (see [23] for results). To understand how the parameter  X  in APST- X  helps in promoting specialized vs. well-rounded objects, we inject an artificial player into NBA2. Again, we use 7 attributes, as explained in Section 4.2. This artificial player has the highest points but the lowest values for all other attributes. Figure 7 shows how this player X  X  rank changes in the APST- X  ranking when  X  varies. As  X  decreases from 1 , the artificial player quickly gets ranked high among real players. Its rank converges as  X  approaches 0 , but never becomes the first because it cannot surpass real players who have the highest values on some other attributes. We have observed similar behaviors when the artificial player specializes in other attributes. This result agrees with the discussion in Section 3 that a smaller  X  favors specialized objects over well-rounded ones. An example of a specialized player is John Stockton, a HoFer, who is second only to Magic Johnson in assists per game, but is not ranked high in any other attributes considered. His ranks under various  X  values are also plotted in Figure 7, which show a similar trend to that for the artificial player.
In Figure 8, we focus on the top 30 players bound by APST-.9 in NBA2, under the three attributes: points , rebounds ,and assists . The figure shows how their ranking positions change by  X  (hori-zontal axis has logarithmic scale). At the bottom of the figure, the top 9 players always remain in the top 13 , because of their excep-tional performance in multiple attributes. The ranks of the remain-ing players respond well to changes in  X  . We can categorize these players into two types: I) specialized players, who are exceptional on very few attributes; II) well-rounded players, who are not ex-ceptional on any single attribute, but are reasonably good on mul-tiple attributes. In Figure 8, the curves of Type-I players go down as  X  decreases, because small  X  favors them more against other players in at least half of the subspaces, i.e., those containing the attribute on which they specialize. On the contrary, the curves of Type-II players go up as  X  decreases. They get rewarded in higher-dimensional subspaces for not being dominated by many players in such subspaces, while they can be easily dominated by others on individual dimensions.

For a concrete example, consider the lowest-ranked players at largest and smallest  X  , respectively. The lowest ranked curve (53rd) at the largest  X  (on the leftmost) corresponds to Nate Thurmond, a HoFer, ranked 277th in points , 6th in rebounds , and 579th in as-sists . As a representative Type-I player, when  X  decreases, his over-all rank becomes higher, ending eventually at 18th. In contrast, the lowest ranked player (42nd) at the smallest  X  , Charles Barkley (an-other HoFer), ranked 33rd, 23rd, and 223rd respectively in points , rebounds ,and assists . As a representative Type-II player, when  X  decreases, his rank goes all the way down from 11th. A tunable  X  thus allows a user to specify personal preference between Thur-mond and Barkley effectively. Our work is closely related to skyline computation for multidimen-sional data, which can be roughly grouped into two classes. The first class [3, 4, 8] includes Block Nested Loop (BNL), Divide and Conquer (D&amp;C) and their variants, and do not require precomputed indices. The second class [18, 11, 15] adopts B + -tree or R-tree in-dices to prune unnecessary computation. Skyline computation for multiple (or all) subspaces are first studied by Pei et al. [17] (BUS and TDS algorithms) and by Tao et al. [19] (SUBSKY algorithm). The former are extensions of BNL and D&amp;C algorithms that tra-verse the subspace lattice without using indices, while the latter fol-lows the index-based approaches. Extending skyline to k -skyband was first studied in [15].

While our notion of top- X  skyband is the first to provide a univer-sal parameter suitable for selecting the right set of objects regard-less of the subspaces being considered, there are previous works proposing various advanced selection criteria when the size of the skyline is too large. For example, BBS [15] and PBT [24] focus on selecting points that dominate the most other points, and Lin et al. [13] focus on selecting a fixed sized set of points that collec-tively dominate the most other points. More recently, Lu et al. [14] propose the notion of layered skylines, where each layer is defined as the skyline after objects in previous layers are removed. Com-pared with these works, we believe our notion of top- X  skyband is more appealing in terms of usability, because it has a simple inter-pretation and a single parameter works across all subspaces.
Linear ranking of objects for the purpose of answering top-k queries has been studied extensively, including approaches using skyline and skybands [21, 22]. These studies focus on performance improvements and do not consider providing better control of the semantics of the ranking functions. Kemeny ranking [7], on the other hand, was proposed in social choice theory and offer good semantics. As discussed in Section 3.1, however, it does not pro-vide a flexible knob, such as our  X  parameter, for adjusting the ranking based on the users X  preference towards different types of interesting objects. Moreover, computing Kemeny is NP-hard.
Several lines of work on data mining, to various extents, share our focus on finding interesting claims that can be translated to sim-ple, intuitive English statements. Local pattern discovery [26, 20] and subgroup discovery [12, 10] aim to find semantically describ-able subsets of data whose properties deviate strongly from the overall distribution. However, our subsets of interest, based on dominance, do not fit in their frameworks. Redescription min-ing [16] seeks to describe a given subset of data using a sequence of set operations. In contrast, we mine the subset in the first place, and our desired description of the subset involves dominance and count-ing instead. [9] finds  X  X rominent streaks X  (consecutive high/low values in a data sequence) and captures the significance of such streaks by skyline dominance test on streak length and value. The tasks of finding one-of-the-few claims from data and ranking objects by such claims are important to the nascent field of com-putational journalism. We have introduced a simple and intuitive problem formulation for finding all interesting claims, using a sin-gle uniqueness threshold  X  that automatically adapts to data char-acteristics and applies to all subspaces. We have proposed a novel scheme for ranking objects, overcoming the inflexibility of Kemeny without resorting to a large number of knobs like weighted-sum. We have devised efficient algorithms for both tasks, using tech-niques such as pruning and approximation to tame complexity. We believe that our attention to usability will appeal to journalists and citizens alike.
 Acknowledgments Y.W., P.K.A., and J.Y. are supported by NSF grants CNS-05-40347 and IIS-07-13498. P.K.A. is additionally supported by NSF grants CCF-06-35000, CCF-09-40671, CCF-10-12254, by ARO grants W911NF-07-1-0376 and W911NF-08-1-0452, and by an ARL award W9132V-11-C-0003. J.Y. is addi-tionally supported by NSF grant IIS-09-16027. C.L. is supported by NSF Grants IIS-10-18865 and CCF-11-17369. Both J.Y. and C.L. are also supported by HP Labs Innovation Research Awards.
