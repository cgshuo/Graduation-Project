 In most IR clustering problems, we directly cluster the documents, working in the document space, using cosine similarity between documents as the similarity measure. In man y real-w orld applica-tions, howe ver, we usually have kno wledge on the word side and wish to transform this kno wledge to the document (concept) side. In this paper , we pro vide a mechanism for this kno wledge trans-formation. To the best of our kno wledge, this is the rst model for such type of kno wledge transformation. This model uses a nonne g-ative matrix factorization model X = F SG T , where X is the word-document semantic matrix, F is the posterior probability of a word belonging to a word cluster and represents kno wledge in the word space, G is the posterior probability of a document belonging to a document cluster and represents kno wledge in the document space, and S is a scaled matrix factor which pro vides a condensed vie w of X . We sho w how kno wledge on words can impro ve document clustering, i.e, kno wledge in the word space is transformed into the document space. We perform extensi ve experiments to validate our approach.
 Intelligence ]: Learning; I.5 [ Patter n Recognition ]: Applications Algorithms, Experimentation, Measurement, Performance, Theory Clustering, Kno wledge Transformation
As a fundamental and effecti ve tool for data organization, sum-marization and navigation, clustering has been recei ving a lot of attention. Clustering is the problem of partitioning a nite set of points in a multi-dimensional space into classes (called clusters) so that (i) the points belonging to the same class are similar and (ii) the points belonging to dif ferent classes are dissimilar [12, 14].
Man y clustering algorithms aim at clustering homo geneous data where the data points are all of a single type [3]. In man y real world applications, howe ver, a typical task often involv es more than one type of data points. For example, in document analysis, there are terms and documents . Generally the dif ferent types of data points are not independent to each other and there exist close relationships among them. Ho w to utilize the relationships is a challenging issue. It is dif cult for traditional clustering algorithms to utilize those relationships efciently .

Co-clustering algorithms aim at clustering dif ferent types of data simultaneously by making use of the dual relationship information such as the word-document matrix. For instance, bipartite spectral graph partitioning approaches are proposed in [8, 28] to co-cluster words and documents. Cho et al [5] proposed algorithms to co-cluster the experimental conditions and genes of microarray data by minimizing the sum-squared residue. Long et al. [20] proposed a general principled model, called Relation Summary Network , to co-cluster the heterogeneous data on a k-partite graph .
Ho we ver, these co-clustering algorithms are unsupervised learn-ing methods, based on the relationship information pro vided by the word-document matrix only . In man y applications, we have some additional/e xternal information. If the additional information is in document space and we are interested in document clustering, this topic is called Semi-supervised Clustering, and has been studied by [24, 4, 26, 6, 2, 7, 19], where the prior kno wledge exists in the form of pairwise constraints (In general, howe ver, prior kno wledge is not easy to be incorporated into clustering; K-means, Gaussian mixture, and information bottleneck [22] are some of the examples where prior kno wledge is dif cult to incorporate.).

In man y other cases, we have additional information/kno wledge on the words side and we wish to see if the y can inuence/help the clustering of documents. To the best of our kno wledge, this problem has not been investigated before. In this paper , we pro vide a model to sho w that this can be done. We start with a simple example.
To demonstrate the usefulness of the additional information in the word space, we give a simple example in Figure 1. The syn-thetic dataset contains four research article titles as sho wn in part (a). After remo ving stop words and words appearing in every arti-cle, the dataset is represented as a word-document matrix as sho wn in part (b). The titles are from two topic areas: Information Re-trie val ( IR ) and Computer Vision ( Vision ). If we look at the data matrix directly , D 1 and D 3 are similar based on cosine similarity since their dot product is 1 while D 1 and D 2 are not similar since their dot product is 0. Similarly , D 2 and D 4 are similar while D 3 and D 4 are not similar .

If we run K-means on the simple dataset, D 1 and D 3 will be grouped into one cluster while D 2 and D 4 will be grouped into an-reect the topic areas of the titles. No w suppose we have addi-tional kno wledge on the words side. In our example, clustering , and classication belongs to word cate gory Lear ning ; illumina-tion and textur e belongs to word cate gory Graphics ; webpa ge and hyperlink belongs to word cate gory Web . Using the additional in-formation on word space, we can obtain perfect clustering since D 1 and D 2 are associated with word cate gories Lear ning and Web , while D 3 and D 4 are associated with word cate gories Lear ning and Graphics . Later on, in Section 2.2, we will illustrate in detail on how kno wledge in the word space is transformed into the docu-ment space for this example. We will also present the computation results of our proposed method on the example in Section 2.3. Figur e 1: An Illustrating Example. The standard word-document matrix is the transpose of the table given in Part (b).
In this paper , we pro vide a model to sho w that additional infor -mation/kno wledge on the word side can inuence/help the cluster -ing of documents. This mechanism allo ws us to transform kno wl-edge in the word space to the document space. In the follo wing, we present the model in details, and pro vide a theoretical analysis of the kno wledge transfer model. We pro vide a concrete computa-tional algorithm to solv e the model, and also pro ve the correctness and con vergence of the algorithm based on constrained optimiza-tion theory . Since the word-document matrix is just an example of two-w ay data, our kno wledge transformation mechanism can ap-ply to any two-w ay data, such as the DN A microarray data where kno wledge on the genes (ro ws) can be transformed to that of patient tissue samples (columns).

The rest of the paper is organized as follo ws: Section 2 intro-duces the basic model for enabling kno wledge transformation from the word space to the document space. In particular , Section 2.2 gives a theoretical analysis on the effects of the prior kno wledge in the word space; Section 2.3 presents a computational algorithm for solving the model. Section 3 proposes the method for kno wledge transformation when kno wledge in the word space is in the form of pairwise relations. Section 4 sho w our experiments on real-w orld datasets. Finally Section 5 concludes.
In this paper , we pro vide a mechanism for kno wledge transfor -mation from the word space to the document space. This model uses a nonne gati ve matrix factorization model [11, 18] where X is a m n word-document semantic matrix, F is an m k nonne gati ve matrix representing kno wledge in the word space, i.e., i -th row of F represents the posterior probability of word i belonging to the k classes, and G is an n k nonne gati ve matrix representing kno wledge in document space, i.e., the i -th row of G represents the posterior probability of document i belonging to the k classes. S is an k k nonne gati ve matrix pro viding a condensed vie w of X .
 We sho w how information on F help clustering documents on G . We have two dif ferent ways to incorporate kno wledge in the word space. The rst is a cate gorization of words, represented by a complete specication of F . This is presented in Section 2.1. An-other way is partial kno wledge on words, for example, two words are kno wn to be highly related and must be grouped into the same word cluster . This is discussed in Section 3.

Our model is similar to the probabilistic latent semantic inde xing (PLSI) model [13]. In PLSI, X is treated as the joint distrib ution between words and documents by the scaling X !  X  X = X =  X  thus  X  i j  X  X i j = 1).  X  X is factorized as where X is the m n word-document semantic matrix, X = W SD , W is the word class-conditional probability , and D is the document class-conditional probability and S is the class probability distrib u-tion.

PLSI pro vides a simultaneous solution for the word and docu-ment class conditional distrib ution. Our model pro vides simultane-ous solution for clustering the rows and the columns of X . To avoid ambiguity , we impose the orthogonality condition which enforce each row of F and G has only one nonzero entry . This form gives a good frame work for simultaneously clustering the rows (w ords) and columns (documents) of X [8, 28, 17].
The prior kno wledge in the word space can be represented as F This information is incorporated into the unsupervised clustering frame as a constraint Where a &gt; 0 is a parameter which determines the extent to which we enforce F F 0 . The constraint ensures that the solution for F in the otherwise unsupervised learning problem be close to the prior kno wledge F 0 .

The abo ve model is generic and it allo ws certain exibility . For example, in some cases, our prior kno wledge on F 0 is not very accu-rate and we use smaller a so that the nal results are not dependent on F 0 very much, i.e., the results are mostly unsupervised learning results. Here we give a theoretical analysis to sho w the effects due to F , the prior kno wledge in the word space. For this reason, we as-sume our kno wledge is certain and we set a !  X  ; The optimization simplies to
T HEOREM 1. The optimization of Eq.(5) with ortho gonality con-str aints G T G = I F T 0 F 0 = I, is identical to the optimization of
Pr oof: J = jj X F 0 SG T jj 2 = Tr ( X T X 2 F T 0 X GS T 0 =  X  J =  X  S ) S = F T 0 X G . Thus J = Tr ( X T X G T X T F TrX T X is a constant and the second term is the desired result.
By the K -means and Principle Component Analysis (PCA) equi v-alence theorem [27, 10], the clustering of Eq.(6) uses X T F the pairwise similarity . whereas the standard K -means uses X as the pairwise similarity . For the example of Section 1.1, and the K -means clustering will produce ( D 1 ; D 3) as a cluster and ( D 2 ; D 4 ) as another cluster .
 No w with the kno wledge F 0 in the word space, where we have set Clearly , using this similarity , K -means clustering will generate as a cluster and ( D 3 ; D 4 ) as another cluster .

We may see more directly how kno wledge in the word space is transformed into the document space. Let the square root of the semi-denite positi ve matrix be P : F 0 F T 0 = P T P . We have X T F 0 F T 0 X = ( PX ) T ( PX ) which means we cluster the data using the transformed data For the example in Section 1.1,  X 
X = 2 1 = 2
It is obvious that on this transformed data, D 1 and D 2 will be clustered into one cluster , D 3 and D 4 will be clustered into another cluster . This analysis sho w directly how the kno wledge in the word space is transformed into the document space.
The optimization problem in Eq.( 4) can be solv ed using the fol-lowing update rules The algorithm consists of an iterati ve procedure using the abo ve three rules until con vergence: Initialization . Initialize F = F 0 , G to K-means clustering results, and S = [( F T F ) 1 F T X G ( F T F ) 1 ] + .
 Update G . Fixing F ; S , updating G Update F . Fixing S ; G , updating F Update S . Fixing F ; G , update S
For the example given in Section 1.1, using the abo ve algorithm procedure, we initialize F = F 0 as in Eq.( 7), after con vergence, we obtain Thus based on G , D 1 and D 2 is grouped together into one cluster and D 3 and D 4 is grouped together into another cluster . In addi-tion, S clearly sho ws the association relationships between topic areas and word cate gories. In the rst column of S , lar ge values in the top two entries indicating that topic cluster 1 (e.g., Vision area) is associated with word cate gory 1 (e.g., Lear ning ) and word cate gory 2 (e.g., Graphics ). Similarly , IR area is associated with Lear ning and Graphics cate gories. Ho we ver, if we initialize F randomly , after con vergence, we obtain We can not obtain good clustering results from G .
We pro ve rigorously two theorems on correctness and Con ver-gence of the algorithm: T HEOREM 2. The abo ve iter ative algorithm con ver ge. T HEOREM 3. At con ver gence , the solution satises the Karuc h, Kuhn, Tucker optimality condition, i.e., the algorithm con ver ged corr ectly to a local optima.
 The proof of Theorem 2 is given in the Appendix.
 Pr oof of Theor em 3 . Follo wing the theory of constrained opti-mization [21], we introduce the Lagrangian multipliers l (a sym-metric matrix of size K K ) and minimize the Lagrangian function L ( F ) = jj X F SG T jj 2 + a k F F 0 k 2 + Tr [ l ( F T Note jj X F SG T jj 2 = Tr ( X T X 2 F T X GS T + SG T GS T F T F gradient is The KKT complementarity condition for the non-ne gati vity of F gives This is the x ed point relation that local minima for F must satisfy .
The standard approach is to solv e the couple equations Eq.(13) with constraints using a nonlinear method such as Ne wton' s method. Ho we ver, this system of nonlinear equations is generally dif cult to solv e. In this paper , we pro vide a much simpler algorithm to com-pute the solution. From the KKT complementarity condition, sum over i , we have This gives the diagonal elements of the Lagrangian multiplier For non-diagonal elements, we use the gradient zero condition and obtain, No w substitute l into Eq.(10), the updating rule for F is identical to the KKT condition of Eq.(13). The correctness of updating rules for G in Eq.(8) and S in Eq.(9) have been pro ved in [11].
Sometimes our kno wledge in the word space are in the form of a set of pairwise relations. For example, we think algorithm and computation should be in one class, whereas economy and bask et-ball should be in dif ferent classes. More formally , we have two types of pairwise word association constraints [24]: (1) Must-link wor d pair s encoded by a matrix containing pairs of words w i must be clustered into the same word cluster , and (2) Cannot-link wor d pair s encoded by a matrix where each pair of words are considered dissimilar and are not to be clustered into the same word cluster .
 We treat them as constraints on the class posterior probability F . A must-link pair ( i 1 ; j 1 ) implies that the overlap h for some class k . Thus  X  K k mized. Thus we express the must-link condition as For a cannot-link pair ( i 2 ; j 2 ) , h i as a constraint and minimize  X  K k are nonne gati ve. We write this condition as Putting these conditions together , we can cast the kno wledge trans-formation model as the follo wing optimization problem where b ; g are two positi ve constants to adjust the strength of the kno wledge. A theoretical analysis similar to that of Section 2.2 can be carried out for understanding how kno wledge embedded in pairwise relations in A ; B can transform into the document space.
The procedure for computing an optimal solution is an iterati ve algorithm. The updating rules for G ; S are the same as in Eqs.(8,9). The updating rules for F is where the Lagrangian multiplier k -by-k matrix l for enforcing the orthogonality F T F = I is given by Similar to Section 2.3, we have T HEOREM 4. The abo ve iter ative algorithm con ver ge.
T HEOREM 5. At con ver gence , the solution satises the KKT optimality condition, i.e., the algorithm con ver ged corr ectly to a local optima.
 The proof of Theorem 4 is given in Appendix.
 Pr oof of Theor em 5 We write down the Lagrangian function
L 1 ( F ) = k X F SG T k 2 + Tr [ b F T AF + g F T BF + l ( and obtain the KKT condition for the non-ne gati vity of F : [ 2 X GS T + 2 F ( SG T GS T + l ) 2 b AF + 2 g BF ] ik F From this, we can obtain the Lagrangian multiplier as in Eq.(19) We can easily see that at con vergence, the solution satisfy which is identical to the KKT condition: either the rst factor is have pro ve that if the iteration con verges, the con verged solution satises the KKT condition, i.e., it con verges correctly to a local minima.
We use the follo wing four datasets in our experiments and their characteristics are summarized in Table 1. The Mallet 1 softw are package is used in our experiments for text processing. 1 It can be downloaded from http://mallet.cs.umass. edu/index.php/Obtaining_MALLET . 2 The dblp.xml le is available for download at http://www. informatik.uni-trier.de/~ley/db/ . 3 Available on the page of http://www.computer.org/ portal/pages/ieeecs/publications/author/ ACMtaxonomy.html .
To measure the clustering performance, we use accurac y and normalized mutual information as our performance measures. Ac-curac y disco vers the one-to-one relationship between clusters and classes and measures the extent to which each cluster contained data points from the corresponding class. It sums up the whole matching degree between all pair class-clusters. Its value is be-tween [ 0 ; 1 ] . Accurac y can be represented as: where C i denotes the i -th cluster , and L j is the j -th class. T is the number of entities which belong to class j are assigned to cluster i . Accurac y computes the maximum sum of T ( C i ; pairs of clusters and classes, and these pairs have no overlaps [11]. Generally , the greater accurac y means the better clustering perfor -mance.

Normalized mutual information (NMI) is another widely used performance evaluation measure for determining the quality of clus-ters [23]. For two random variables X and Y , the NMI is dened as where I ( X ; Y ) is the mutual information between X and Y , and H ( X ) and H ( Y ) are the entropies of X and Y , respecti vely . Clearly , NMI ( X ; X ) = 1 and this is the maximum possible value of NMI. Given a clustering result, NMI in Eq.( 23) is estimated as follo ws: where n i denotes the number of data points contained in the cluster C 1 i k ) ,  X  n j is the number of data points belonging to the j -th class (1 j k ), and n i j denotes the number of data points that are in the intersection between the cluster C i and the j -th class. In general, the lar ger the NMI value, the better the clustering quality .
We denote our method utilizing prior kno wledge as CP (Cluster -ing with Prior kno wledge). In our experiments, we compare our method (CP) with the follo wing methods: Figur e 4: Accuracy results with differ ent numbers of words on CSTR dataset.

Figure 2 sho ws the experimental results on four datasets using accurac y as the performance measure, and Figure 3 present the NMI results. The results are obtained by averaging 20 runs. From the experimental comparisons, we observ e that: Figur e 5: Accuracy results with differ ent numbers of words on DBLP dataset.
 Figur e 6: Accuracy results with differ ent numbers of pairwise word relations In this section, we perform experiments on DBLP dataset and CSTR dataset to investigate the effects of the size of word space on clustering performance. The word selection is performed using Mallet toolkit with information gain criteria. Figure 4 and Figure 5 sho w the accurac y results with dif ferent numbers of selected words on CSTR dataset and DBLP dataset, respecti vely . From Figure 4 on CSTR dataset, our proposed method (CP) outperforms all other methods at dif ferent word sizes (except one case at 500 words). From Figure 5 on DBLP dataset, CP outperforms all other meth-ods. The two nai ve ways for incorporating the word space kno wl-Figur e 7: NMI results with differ ent numbers of pairwise word relations edge, FE and CE, do not perform well, due to the unsophisticated clustering methods used. Similar beha vior are also observ ed for the NMI measure (results are not sho wn due to lack of space).
In this section, we perform experiments when prior kno wledge is in the form of pairwise relations. We use the algorithm presented in Section 3 to transform the pairwise relations in the word space into the document space. The relations are generated as follo ws: we pick out a pair of words randomly from the word space (the cat-egory information of which are available). If this pair of words are in the same word cate gory , then we generate a must-link relation. If ated. Figure 6 and Figure 7 present the experimental results of our algorithm with dif ferent number of pairwise relations in the word space. In all the experiments, the results are averaged over 20 trials. As you can observ e from these Figures, the clustering performance generally impro ves as the number of pairwise relations increases. The experimental results conrms the ability of our proposed algo-rithm for transferring the pairwise relations in the word space into the document space which impro ves clustering results.
In this paper , we pro vide a model for enabling kno wledge trans-formation from the word space to the document space. Two forms of prior kno wledge in the word space (cate gorization of words and pairwise relations between words) are presented, which can be ef-fecti vely incorporated into the model and transformed into kno wl-edge in the document space. We give detailed theoretical analysis of the model and propose computational algorithms with rigorous proofs of their correctness and con vergence. We experiment on 4 real-w orld datasets and compare with six other methods. The results sho w our proposed method consistently outperform other methods. Our model can be applied to any two-w ay data to effec-tively transform kno wledge from one side to another side. For ex-ample, on DN A microarray data, our model can transform kno wl-edge on the gene side (ro ws of the data matrix) to kno wledge on the patient tissue sample side (columns of the data matrix). The project is partially supported by NSF CAREER Award IIS-0546280 and IBM Faculty Research Awards.
We use the auxiliary function approach [16]. A function Z is called an auxiliary function of L ( F ) if it satises for any F ;  X  F . Dene By construction, L ( F ( t ) ) = Z ( F ( t ) ; F ( t ) ) Z L ( The key is to nd appropriate Z ( F ;  X  F ) and its global minima. We write L of Eq.(11) as
L ( F ) = Tr [ 2 F T X GS T + ( SG T GS T + l + a ) F T F 2 a F where we ignore the constraints X T X , Tr l , and Tr ( F T sho w that the follo wing function
Z ( F ; F 0 ) =  X  is an auxiliary function of L ( F ) . First, it is obvious that when F Z (
F ; F 0 ) L ( F ) , because: the second term in Z ( F ; F bigger than the second term in L ( F ) , due to an inequality for any A ; F ; F 0 0 (W e skip the proof due to lack of space). Thus the conditions of Eq.(25) are satised.

No w according to Eq.(26), we need to nd the global minimum of f ( F ) = Z ( F ; F 0 ) xing F 0 . We rst compute the gradient and the Hessian (2nd order deri vatives) are Thus the Hessian is semi-positi ve denite, i.e., Z(F ,F') is a con vex function. It's global minima is obtained by setting  X  Z = we obtain No w according to Eq.(26), F ( t + 1 ) = F and F 0 = F ( t Eq.( 10).
First we nd the auxiliary function of L 1 ( F ) in Eq.(20). We can sho w that the follo wing function Z 1 ( F ; F 0 ) = is an auxiliary function, ignoring the constant terms. No w, the gra-dient is and the Hessian is Thus the Hessian is semi-positi ve denite, i.e., Z(F ,F') is a con vex function. It's global minima is obtained by setting  X  Z = we obtain or No w according to Eq.(26), F ( t + 1 ) = F and F 0 = F ( t [1] R. Baeza-Y ates and B. Ribeiro-Neto. Modern Information [2] S. Basu, M. Bilenk o, and R. J. Moone y. A probabilistic [3] P. Berkhin. Surv ey of clustering data mining techniques. [4] M. Bilenk o, S. Basu, and R. Moone y. Inte grating constraints [5] H. Cho, I. Dhillon, Y. Guan, and S. Sra. Minimum sum [6] D. Cohn, R. Caruana, and A. McCallum. Semi-supervised [7] I. Da vidson and S. Ra vi. Clustering under constraints: [8] I. S. Dhillon. Co-clustering documents and words using [9] I. S. Dhillon, S. Mallela, and D. S. Modha.
 [10] C. Ding and X. He. K-means clustering and principal [11] C. Ding, T. Li, W. Peng, and H. Park. Orthogonal [12] J. Hartigan. Clustering Algorithms . Wiley, 1975. [13] T. Hofmann. Probabilistic latent semantic inde xing. Proc. [14] A. K. Jain and R. C. Dubes. Algorithms for Clustering Data . [15] Z. Kou and C. Zhang. Reply netw orks on a bulletin board [16] D. Lee and H. S. Seung. Algorithms for non-ne gati ve matrix [17] T. Li. A general model for clustering binary data. In KDD , [18] T. Li and C. Ding. The relationships among various [19] T. Li, C. Ding, and M. Jordan. Solving consensus and [20] B. Long, X. Wu, Z. M. Zhang, and P. S. Yu. Unsupervised [21] J. Nocedal and S. J. Wright. Numerical Optimization . [22] N. Slonim and N. Tishby . Document clustering using word [23] A. Strehl and J. Ghosh. Cluster ensembles -a kno wledge [24] K. Wagstaf f, C. Cardie, S. Rogers, and S. Schroedl. [25] F. Wang, T. Li, and C. Zhang. Semi-supervised learning via [26] E. Xing, A. Ng, M. Jordan, and S. Russell. Distance metric [27] H. Zha, C. Ding, M. Gu, X. He, and H. Simon. Spectral [28] H. Zha, X. He, C. Ding, M. Gu, and H. Simon. Bipartite
