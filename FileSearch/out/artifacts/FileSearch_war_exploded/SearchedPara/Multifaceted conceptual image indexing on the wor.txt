 1. Introduction
A unique feature of Web images is the available contextual information which typically includes the surrounding text associated to the images, the image filenames, alt descriptions, and page titles. Current commercial Web-based image search engines such as Google Image, Yahoo!, and Bing, automatically index images using this information in their keyword-based image retrieval systems. These systems work very well for most simple or general queries (e.g. cat, dog, Michael Jackson,
Twilight, etc.). However, for queries that involve more complex information needs such as  X  X  X aby boy in blue X  X  or  X  X  X ar hits lorry X  X , which are therefore longer and more specific, the retrieved results are not as satisfactory as shown in Fig. 1 . Studies on user queries have shown that longer or specific queries are common ( Armitage &amp; Enser, 1997; Chung &amp; Yoon, 2010;
Hollink et al., 2004; Pu, 2008 ) and should be addressed. An image indexing model should be able to cater to both short/ generic and long/specific queries. Hence, the focus of this article is on the study of the textual source for indexing to improve the performance of Web-based image retrieval systems.

Research in image retrieval systems has expanded in breadth and depth to produce more efficient systems in managing the ever-increasing collection of images on the Web. There are the Content-Based Image Retrieval (CBIR) research groups who actively study the image content to extract and index images with low-level visual features or to derive high-level
Latest trends involve the fusion of visual features from CBIR systems and the contextual information of Web images, in particular state-of-the-art solutions focus on further characterizing the visual content by leveraging tag information  X  ( Qi et al., 2009; Tang et al., 2009; Wang, Zhang, &amp; Zhang, 2008 ). These hybrid systems have been shown to outperform CBIR systems. However, their coverage is limited to websites where these tags are found (i.e. multimedia sharing websites only).
Tags are furthermore often noisy and ambiguous and more often nonexistent. Authors in Liu et al. (2009) and Wang et al. (2008) therefore expressed the need for more refined text processing techniques as the inaccuracy of the textual information attached to the Web images degrades the effectiveness of the indexing solutions.

The current standard text processing techniques of stemming and stop word removal, along with the  X  X  X ag-of-words X  X  rep-resentation model are unable to provide a good textual description that reflects the semantics of the image. One particular issue concerns the selection of terms  X  Is it appropriate to simply split the contextual information into individual words (which is the common practice)? What will happen to terms such as  X  X  X ountain pen X  X ,  X  X  X iger Balm X  X , and  X  X  X eart attack X  X ? What about removing stop words which are generally defined as functional words such as articles, prepositions, and conjunctions?
This would pose problems for the following terms:  X  X  X ird on house X  X ,  X  X  X at on mat X  X , etc. The terms  X  X  X ird on house X  X  and  X  X  X ird house X  X  (as well as  X  X  X at on mat X  X  and  X  X  X at mat X  X ) give different visualizations but for systems that simply perform stop word removal, the two terms are the same as illustrated in Fig. 2 .

To address this issue, we have conducted a user study to investigate the users X  standpoint on relevant image contextual information ( Fauzi &amp; Belkhatir, 2010 ). Users were required to identify images and words that they thought described the images. Subsequently, we were able to examine the statistical properties of relevant image contextual information in terms of its locations (i.e. HTML tags) and semantic nature. The outcomes of the study include: 1. the user-identified relevant texts of single words, phrases, and/or sentences, where these user-identified descriptors can be associated to an image descriptor model of five facets (each facet will be explained in depth in the next section); 2. the highlighting of several locations that consistently contain relevant texts with respect to the image; 3. the validation of the following assumptions: (i) a webpage is not an atomic unit and can be broken into smaller units of image segments, where each image segment is made up of an image together with its contextual information and (ii) only some texts in the image contextual information are relevant to the image within the image segment; 4. a multimodal dataset with human labeled ground truths consisting of multimedia artifacts that are the images, text chunks (e.g. paragraph texts, caption texts, link texts) and webpage links (URL, ALT, SRC, etc.).

The user-identified relevant texts demonstrate that the typical bag-of-words/concepts representation model is insuffi-cient, whereby the associations between concepts should not be disregarded and phrase or multi-word concept extraction propose a multifaceted concept-based image indexing model which analyzes the semantics of the image contextual infor-mation and compartmentalizes it into five broad semantic concept classes or facets, i.e. signal , object , relational , scene and abstract which relate to the users X  levels of image descriptions. The relationships between the highlighted concepts are iden-tified as well. The terms concept class and facet are used interchangeably throughout this text. If we examine a notion of abstraction targeted at each facet (left part of Fig. 3 ), the Signal Facet corresponds to low abstraction (because it considers only the image low-level features such as colors, textures and shapes), the Object Facet is a medium abstraction representa-tion since it describes the image entities, the Relational Facet is a higher abstraction level because it features further charac-terization between the image objects. The Scene Facet allows interpretation of visual scenes. The Abstract Facet represents knowledge or additional information including abstract characteristics (e.g. emotions, quality) regarding the image or image entities. The structure of Fig. 3 shows that the five visual interpretation facets are obviously not independent from each other. However, the gray parts that represent the transitions between the facets are far from being easy tasks. The right part of Fig. 3 highlights the expressivity of the structures characterizing each facet. It varies from low expressivity for the Signal
Facet , where low-level visual representations or keywords (consisting of color, texture or shape names) are used to high expressivity regarding the Abstract Facet , where conceptual structures are generated.

The proposed framework exploits an external knowledge base and natural language processing techniques which go be-yond the standard text processing techniques typically used in the current keyword-based and hybrid CBIR image retrieval systems. The automatic classification of contextual information into the five different facets and the modeling of relation-ships between concepts are other important contributions of this work. Experimental validation is carried out using the mul-timodal dataset with human labeled ground truth from the user study. Users are therefore taken into account both as far as the indexing process is concerned and in the collection of the evaluation dataset.

Our experimental results show an improvement in terms of precision and recall over experimental frameworks employ-ing tf  X  idf and location-based tf  X  idf weighting schemes for single-facet indexing on the one hand, and systems based on n -gram processing for multifaceted indexing on the other hand on a human-annotated Web collection.

The remainder of this paper is organized as follows: Section 2 explains and analyzes the existing classification frame-works and our proposed classification framework for image contextual information. Section 3 details out the proposed framework. The experimental results are presented in Section 4 . Finally, Section 5 presents the conclusions and outlines the future work. 2. Classification of user image descriptions
One of the challenges faced when building automatic image retrieval systems is to index the images at the right level of indexes will affect the performance of the systems. Prior studies on users X  image descriptions from various fields reveal that there are multiple levels of image descriptions. 2.1. Existing theories
As early as 1962, Erwin Panofsky came up with the theory to structure content descriptions of images in his work titled  X  X  X eaning in the Visual Arts X  X . He suggested three levels of meaning in Renaissance art: the pre-iconographical description (i.e. non-symbolic or factual subject matter of an image includes generic actions, entities, and entity attributes in an image), the iconographical analysis (i.e. individual or specific entities or actions) and the iconological interpretation (i.e. symbolic meaning of an image) ( Panofsky, 1962 ).

Shatford (1986) extended Panofsky X  X  theory to all types of images and categorized the subjects of pictures as  X  X  X eneric of X  X ,  X  X  X pecific of X  X , and  X  X  X bout X  X  and under each level, four sub-categories were highlighted: who, what, when and where. This model is known as the Panofsky/Shatford Model.

Jaimes and Chang (2000) proposed a 10-level image model consisting of ten descriptors  X  type/technique, color, texture, shape, generic X  X bject, generic X  X cene, specific X  X bject, specific X  X cene, abstract X  X bject, abstract X  X cene  X  organized in a pyra-midal structure according to the amount of knowledge required to characterize entities corresponding to these descriptors.
Levels 1 X 4 are associated to the perceptual or visual information within an image and Levels 5 X 10 are conceptual and can be viewed as incorporating the Panofsky/Shatford model (i.e. Generic of, Specific of, About/Abstract). At each level of the Pan-ofsky/Shatford model, they differentiated between  X  X  X bject X  X  and  X  X  X cene X  X ;  X  X  X bject X  X  referring to individual entities within an image and  X  X  X cene X  X  referring to descriptions about the image as a whole, thus, making up the six conceptual levels.
The frameworks above were proposed for the purpose of structuring image indexes. There are other existing frameworks that focused on the searching of images ( Armitage &amp; Enser, 1997; Eakins, 2002 ). Their studies were conducted on a collection of queries and aimed to develop a general-purpose categorization of user queries for still and moving visual images. Armit-age and Enser (1997) discovered the need to sometimes incorporate attributes to do with the medium of imagery as there was an identified issue with the Panofsky/Shatford Model. These were included in Jaimes and Chang X  X  10-level pyramidal model. While Armitage and Enser gave related examples, they omitted attributes such as the creator/photographer and date of the image. Hollink et al. (2004) put this information into perspective and classified it as non-visual. Their combined frame-works are dedicated both for indexing and search by focusing on image descriptions, which can be both query and index terms. They described three levels of image descriptors  X  non-visual, perceptual and concept level. The concept level is fur-ther divided into three sub-levels  X  general, specific and abstract. Hollink et al. X  X  perceptual and concept levels are similar to these of the Panofsky/Shatford Model as well as the Jaimes and Chang X  X  Pyramidal Model. 2.2. The proposed multifaceted image indexing framework
We observe that the classification methods discussed above generally conform to Panofsky X  X  theory. Hollink et al. X  X  (2004) proposed framework is more comprehensive compared to the others as their work integrated the different classification methods and considered the users X  image descriptions from both the description and search perspectives. They, however, did not relate it to high-level CBIR.

Considering the image contextual information as user description and relating it to CBIR, we propose a classification this classification framework is in accordance with Panofsky X  X  theory, whereby we have generic (pre-iconographic) and spe-cific (iconographic) image entities under the object class. Our abstract class can be likened to the iconologic class.
Heidorn (1999) focused on pre-iconographic indexing since he believed that this is the main area, where content-based and concept-based techniques overlap. This is true to some extent since pre-iconographic indexing is concerned with non-symbolic or factual subject matter of an image and to date, CBIR systems that only depend on image content for their indexes can only infer generic subject matters. However, generic actions as well as some entity attributes (e.g. beautiful, happy, etc.) cannot be inferred by such systems. Hence, in our classification framework, we make this distinction and the pre-icono-as elements of the object facet. Low-level entity attributes of color, shape and texture form the visual facet, whereas higher level entity attributes are considered as part of the abstract facet.

Generic actions are classified as relational concepts since action verbs typically associate two or more objects. For exam-locations, scenes or any other global image descriptions are classified under the scene facet.

For completeness, Hollink et al. X  X  non-visual category is integrated into our relational facet since non-visual information is information related to the image such as its medium, photographer, and date of image. Fig. 4 gives an example image with its contextual information categorized according to our classification framework.

We have established our classification model of five facets and propose its instantiation in a concept-based visual index-ing framework illustrated in Fig. 5 to automatically extract images and their contextual information, process and exploit a knowledge base to classify the contextual information into the five facets. Thus far, the classification of image descriptors requires human intervention ( Armitage &amp; Enser, 1997; Hollink et al., 2004; Panofsky, 1962; Yee, Swearingen, Li, &amp; Hearst, 2003 ) or is performed automatically from low-level visual features in the CBIR context limited to generic object and scene as it is hard to derive abstract concepts from the image content. In the related works, only the automatic extraction of spe-cific information in the form of tags (such as people X  X  names, geographical locations, events, dates and time stamps) has been
Simon, Snavely, &amp; Seitz, 2007; Zheng et al., 2009 ). In our work, we tackle the automated characterization of signal, object, relational, scene and abstract concepts through coupled linguistic and knowledge base processing.

The framework consists of two main stages: identification and processing of relevant image contextual information and understanding of Web image contextual information. 2.2.1. Stage 1: Identification and processing of relevant image contextual information
From the perspective of webpage authoring, the basic principle is: an author of a webpage is likely to put an image on a page together with his/her description that, from his/her point of view, is relevant to the image. Hence, the image contextual information is considered as a user X  X  image description. The challenge however, lies in obtaining the user X  X  description.
Due to webpage layout design and restrictions, the description does not always appear nearest to the image. It might just be a particular sentence in a paragraph that describes the image more often than not found within an image segment (i.e. sections of image and texts after a webpage is partitioned). A page segmentation technique is developed ( Fauzi, Hong, &amp; Bel-khatir, 2009 ) to partition the webpage into these image segments. This technique is robust and performs well across diverse websites ( Alcic &amp; Conrad, 2010 ).

Next, an extraction and pre-processing module splits the contextual information into visible and hidden texts and per-forms additional pre-processing steps on the hidden contexts. Hidden contexts contain much noise. HTML tag attributes are to provide additional information about the HTML element. Some key functions of these attributes include the specifi-cation of style, language and events, unique identifier and provision of extra information. Hence, we can broadly classify these attributes into two groups: 1. Attributes that contribute to the formatting of the HTML page (e.g. WIDTH, HEIGHT, BORDER, CLASS, SIZE, FACE, CELLP-
ADING, CELLSPACING, etc.). 2. Attributes that do not (e.g. SRC, ALT, HREF, TITLE, NAME, etc.).

We assume that we can discard information coming from page format-related attributes. In addition, we split words that have been stringed together and drop common words which include file extensions (e.g. jpg, gif, html, cfm, etc.), and HTML diabetes.wpadmin.about.com/?comments_popup=240 X ,400,550 )  X  X  and  X  X  aboutstemcell . jpg X  X  to  X  X  diabetes X  X  and  X  X  X tem cell X  X  respectively.

Visible texts, on the other hand, are usually well formed sentences or phrases but face sentence fragmentation issues due to HTML formatting reasons. To address this, we need to explicitly join the fragments, based on the DOM tree, which can be addressed during or after the segmentation phase.

Generally, the image contextual information contains both relevant and irrelevant (i.e. noisy) information as shown by the low precision obtained in experiments involving baseline experimental frameworks in Section 4.1 . More sophisticated text processing methods are needed to filter out irrelevant information which brings us to the second stage. 2.2.2. Stage 2: Understanding of web image contextual information
This stage consists of three steps: (1) Natural Language Processing (NLP)/pre-classification, (2) classification/semantic characterization, and (3) weighting. 2.2.2.1. Step 1: NLP/pre-classification. NLP by itself is an active research area. There are many problems in both the generation and understanding of language. Here, we are concerned with the understanding of the English language. Among the challenges automated NLP faces are: (i) the rich and growing language, (ii) multiple meanings a word can have (i.e. polysemy), (iii) the combination of two or more words which gives a different meaning (i.e. Non-Compositional
Phrase  X  NCP), (iv) the imperfect text input (i.e. texts that do not adhere to the grammar/syntax), and (v) referencing expressions (e.g. pronouns  X  X  X e X  X ,  X  X  X he X  X  and  X  X  X t X  X  refer to previously mentioned persons and objects).

While we are not able to address all the issues, we focus on the task at hand which is to organize the image contextual information into five semantic concept classes, discover associations between concepts from different facets as well as filter out irrelevant texts (i.e. the semantic characterization process). The main issues that need to be tackled are polysemy and
NCP highlighting. The precision of the characterization will depend on whether we are able to identify the word/phrase in its right meaning within a given context. We suggest the natural language processes of linguistic parsing, NCP identifica-tion, and Word Sense Disambiguation (WSD) to address the issues and perform concept-based indexing (c.f. Fig. 5 ).
A linguistic parser breaks up a sentence into components or phrases by putting them into classes, i.e. semantic roles (e.g. subjects, predicates, direct objects, indirect objects, agents, etc.) and identifying the dependencies between them. It also tags each word with its respective Part-Of-Speech (POS), e.g. nouns, verbs, adjectives, and adverbs, based on the sentence syntax.
These classes and dependencies, as well as the POS tags, are important information for us to identify NCPs and perform the semantic characterization.

WSD is the difficult task of identifying which sense of a word is used in any given sentence. The method which gives supe-rior results in practice consists in using a window of surrounding words to disambiguate word senses. For example, bank can mean the depository financial institution or sloping land near a body of water. If bank has words money or loan nearby, it probably is in the financial institution sense; if bank has the word river nearby, it is probably in the sloping land sense. Sim-ilarly, we will employ this method to perform WSD ( Pedersen &amp; Kolhatkar, 2009 ).

NCP identification tries to recognize phrases (such as proper nouns/named entities, collocations, phrasal verbs, idioms, metaphors and acronyms) and treats them as single entities with their own meanings. Idioms and metaphors pose some problems when visualizing them with images. For example, for the idiom  X  X  X o let the cat out of the bag X  X , some Web authors visualize the literal meaning of the idiom while others consider its metaphoric meaning. Hence, the NCP identification here mainly focuses on recognizing named entities, collocations and phrasal verbs. We will look into extending it to include acro-nyms in the future.

We apply available NLP tools to process the image contextual information and prepare it for the next step. 2.2.2.2. Step 2: Classification/semantic characterization. Once the image contextual information is broken down to tokens with their respective senses and POS tags, it is ready for semantic characterization into the five semantic facets. This aims to give some understanding and orderliness to the contextual information, where we may find numerous and varying semantic con-cepts. Hence, for each token, it will be assigned to one or more facets according to its semantics. An external knowledge base is essential to accomplish this. WordNet, a lexical ontology which organizes concepts in a semantic hierarchy, is used to per-form the semantic characterization of tokens. It is widely diffused in the research community as the knowledge base for the English language to perform automatic concept indexing and classification ( Aslandogan, Thier, Yu, Zou, &amp; Rishe, 1997; Jin, Khan, Wang, &amp; Awad, 2005; Kliegr, Chandramouli, Nemrava, Svatek, &amp; Izquierdo, 2008 ).

Furthermore, the syntactical dependencies given by a linguistic parser are leveraged to discover associations between to-kens for the construction of the multifaceted indexes, thus giving more accurate specificity to the single-facet concepts. 2.2.2.3. Step 3: Weighting. Lastly, to reduce noise, each token is weighted to estimate its importance (i.e. relevance) to the image. We implement an existing weighting scheme for this step.

The next section explores the features in terms of syntax and semantics, followed by a section on the partitioning of a knowledge base to facilitate the classification process. 3. The five facets 3.1. Features
The Signal facet includes concepts that can be extracted automatically from an image. These concepts are usually the low-level visual features such as colors, shapes, textures and spatial concepts (e.g. pink striped circle or a blue spotted tri-angle). From the example  X  X  X ink striped circle X  X , pink and striped are adjectives and circle is a noun. In the example  X  X  X  like pink X  X , the concept  X  X  X ink X  X  is a noun. Semantically, the nouns  X  X  X ink X  X  and  X  X  X lue X  X  are types of colors and if they are used as adjectives, they are related to chroma;  X  X  X triped X  X  and  X  X  X potted X  X  are adjectives related to texture characterization and the nouns  X  X  X ircle X  X  and  X  X  X riangle X  X  are types of shapes.

The Object facet describes the individual entities in the images. Objects could be either living (e.g. human, animals, plants, etc.) or non-living concepts (e.g. table, fan, book, etc.). The example object concepts are typically nouns. Semantically, these concepts have physical and tangible forms. In general, they are all different types of physical entities.
The Relational facet associates two or more objects in terms of visible action or provides non-visible information about the image or image entities (such as author, brand, photographer, and date and type of image). For actions that relate objects (e.g. the concept  X  X  X ick X  X  in  X  X  X he boy kicks the ball X  X ), these concepts are verbs. Categorizing non-visible information is chal-lenging as we need to identify the class of an instance. Following this step, we derive the relational information. For example, in Fig. 4 , a Web surfer would immediately know that  X  X  X . Kavanagh X  X  is the photographer for the image, i.e.  X  X  X hotographer X  X  is the class of the instance  X  X  X . Kavanagh X  X , hence the relation between  X  X  X . Kavanagh X  X  and the image is  X  X  X s_photographer_of X  X .
The challenge lies in getting to recognize such pieces of information through an algorithmic process. The latter usually fol-lows the convention used by Web authors to include this non-visual information.

The Scene facet describes the image as a whole based on all of the objects it contains (e.g. city, landscape, still life, por-trait, park, field, concert, play, Rome, etc.). The example scene concepts are nouns and semantically these concepts could be further grouped into location (e.g. city, park, field, Rome), event (e.g. concert, play), and whole/collection (e.g. still life, portrait).

The Abstract facet contains intangible concepts that represent knowledge or additional information including abstract characteristics (e.g. emotions, quality) regarding the image or image entities. Abstract concepts also include symbolic con-cepts that do not visually describe the image but are deemed relevant (e.g. the concept  X  X  X ourney X  X  is considered relevant for an image containing the objects bag and car). Abstract characteristics are usually adjectival words but could also be used in the nominal form.

These characteristics enable us to utilize WordNet to classify concepts as WordNet organizes concepts in both lexical and semantic hierarchies. Next, we explain how the WordNet lexical ontology is partitioned for the purpose of classification. 3.2. WordNet ontology and the five facets
WordNet is the current de facto knowledge base for the English language. It is a large lexical database and has been widely used in many applications. Specifically, in the area of image retrieval systems, WordNet is used to perform automatic concept annotation in images ( Aslandogan et al., 1997; Datta, 2007; Deschacht &amp; Moens, 2007 ), to classify entities/words 2003 ), to filter out noise through semantic clustering ( Cho et al., 2007; Jin et al., 2005 ), to organize the annotation words topic for a set of annotation words (i.e. topic clustering) ( Mukherjea &amp; Hirata, 1999 ), etc.

Yee et al. (2003) and Kliegr et al. X  X  (2008) usages of WordNet to classify concepts are most similar to our task at hand. Yee et al. (2003) performed a semi-automatic classification of phrasal descriptions attached to the images from the Fine Arts Mu-seum of San Francisco image collection by comparing the words in the descriptions to the higher-level category labels in
WordNet and retaining a subset of the most frequently occurring categories. The categories are then manually organized into a set of hierarchical facets. Kliegr et al. (2008) classified entities (noun phrases) found in free-text image annotations to a custom-defined set of concepts (classes) inferred from the image itself. A WordNet similarity measure is used to determine the similarity between an entity and each of the classes. Then, an entity is classified to the class with the highest similarity.
Both works did not base their categories/classes on any image description model (classes in Kliegr et al. (2008) are typ-ically content-based inferred concepts with no fixed structure, whereas categories in Yee et al. (2003) are to some extent in line with classes defined in image description models and acceptable) and are limited to certain types of textual information in their own way (only noun entities are considered in Kliegr et al. (2008) and the dataset used in Yee et al. (2003) consists of images and textual descriptions from the arts domain). Kliegr et al. (2008) have shown in a user study-based evaluation of their proposed faceted metadata search system that 90% of the study participants strongly prefer the category-based ap-proach, thus, further motivating our faceted model. However, their classification method is semi-automatic and still requires human intervention. We aim to achieve full automation in the classification process.

To employ WordNet for the classification process, its structure is examined. First, WordNet encodes concepts in terms of sets of synonyms (i.e. synsets). Its latest version, WordNet 3.0 (available at wordnet.princeton.edu) contains about 155,000 words organized in over 117,000 synsets. Pairs of word senses and synsets are connected by lexical and semantic relations respectively. Lexical relations (e.g. antonymy, pertainymy, etc.) connect word senses included in the respective synsets and semantic relations (e.g. hypernymy, hyponymy, meronymy, holonymy, similarity, etc.) apply to synsets in their entirety.

As we would like to characterize concepts into our five defined semantic facets, we concentrate on the semantic relations between synsets in WordNet, especially the hypernymy relation (i.e. kind-of or Is-a relation) and hyponymy relation which is the inverse relation of hypernymy. Fig. 6 illustrates WordNet partitioned into five semantic concept sub-structures. Each rectangle represents a synset but only displays the first word in the synset. Rectangles with double bottom lines are leaf nodes (i.e. they have no hyponyms). The solid arrow shows the direct hypernymy relation between synsets, whereas the dashed arrow is the indirect hypernymy relation between the synsets.

With the hypernymy relation, we can determine if a particular concept is a member of the signal, object, abstract, scene, or relational facets. For the nominal synsets in Wordnet, the topmost synset in the semantic hierarchy is  X  X  X ntity X  X ,  X  X  X hing X  X  appears twice in the figure. Hence, it is written as  X  X  X hing#9 X  X  and  X  X  X hing#12 X  X , where #9 and #12 refer to the 9th and 12th senses for the word  X  X  X hing X  X  in WordNet, following the WordNet convention for labeling polysemic words  X  word#sense_number.

However not all elements of the sub-structures rooted at concepts  X  X  X bstract entity X  X  and  X  X  X hysical entity X  X  can be consid-ered as concepts of the abstract and object facets respectively. Indeed, as we traverse down the hierarchy, we find several concepts that can be categorized differently. For example, the concepts  X  X  X vent X  X  and  X  X  X ollection X  X  which are hyponyms of the concept  X  X  X bstract entity X  X  in WordNet and all their respective hyponyms could be considered elements of the scene facet. are also logically related to the scene facet.

As far as the concept  X  X  X hysical entity X  X  is concerned, it has the following hyponyms:  X  X  X hing X  X ,  X  X  X bject X  X ,  X  X  X ausal agent X  X ,  X  X  X ubstance X  X  and  X  X  X rocess X  X . The concepts  X  X  X bject X  X  and  X  X  X ubstance X  X  and their respective hyponyms can clearly be linked to our object facet. The concept  X  X  X hing X  X  itself is unclear and hard to classify. Its hyponyms can be categorized within three cepts depending on the subject or object of reference in some contexts (e.g.  X  X  X nything X  X ,  X  X  X omething X  X ,  X  X  X othing X  X ,  X  X  X nessen-tial X  X ,  X  X  X ecessity X  X  and  X  X  X ariable X  X ). Similarly for the concept  X  X  X ausal agent X  X , while some of its hyponyms (e.g.  X  X  X erson X  X , are undeniably of the abstract facet.

The concept  X  X  X rocess X  X  which is the hyponym of  X  X  X hysical entity X  X  means a sustained phenomenon or one marked by grad-ual changes through a series of states. As the changes are physical and the end results are global in nature and can be cap-tured in images (e.g. the calcification process can be captured in an X-ray image showing the formation of bone), we can consider all terms falling under the  X  X  X rocess X  X  concept as elements of the scene facet.

The signal facet can easily be distinguished from the sub-structures rooted at the concepts  X  X  X olor X  X ,  X  X  X exture X  X  and  X  X  X hape X  X  in WordNet. Adjectival words that relate to these nouns are classified as signal concepts too, here the lexical relations in
WordNet come to use. Other higher-level characteristics such as  X  X  X eautiful X  X ,  X  X  X old X  X , and  X  X  X indy X  X , are classified as abstract concepts.

Action verbs and prepositions are relational concepts. They can be directly classified after applying NLP techniques to identify their POS. For non-visual relational concepts such as photographer X  X  name, brand name, and date taken, WordNet and/or other knowledge bases may assist the identification process. This, however, is a difficult task as there is no standard format of including such details in the image contextual information. 3.3. Associations between the facets
While each word (or group of words for an NCP) can now be classified into a concept of one or more of the five facets, a word does not usually exist in isolation. It might have an associated attribute word, action word, and generic/specific word or head . The association between concepts from different facets guarantees a richer and more refined description of the image content (c.f. Fig. 4 ).

This forms the basis of our proposed multifaceted indexing framework which attempts to address the weakness of exist-ing image search systems illustrated in Fig. 1 , whereby images of a blue monkey and a bear in blue are returned for the first
Fig. 7 which is returned in the first search result page by keeping the association  X  X  X ow X  X  on  X  X  X ar X  X  and  X  X  X nimal X  X  dropped from  X  X  X orry X  X  . Therefore, we can confidently say that  X  X  X ar X  X  is not directly associated to  X  X  X orry X  X  .

We propose to employ NLP techniques to discover such associations and thus, to form our multifaceted indexes. Our solu-tion is based on the analysis of grammatical dependencies that exist within and/or between constituents (i.e. components of phrases such as noun phrase and verb phrase). Most available linguistic parsers provide phrase structure representations (i.e. hierarchies of constituents) and/or dependency structure representations (i.e. hierarchies of grammatical relations). The widespread use of parsers providing dependency structure representations is evident of the importance of the grammatical dependencies in NLP tasks ( De Marneffe &amp; Manning, 2008 ). A grammatical/syntactical relationship exists between a word (governor/head) and its dependents within its constituents. An example of such relationship is the relationship that provides attributive information (e.g. characteristic, quantity, manner, etc.) about the head noun/verb in a noun/verb phrase. The attributive information is referred to as modifier and can typically be an adjective or adverb; for example, a head noun is modified by an adjective such as  X  X  X ellow irises X  X  or  X  X  X wo hour X  X  and a head verb is modified by an adverb, e.g.  X  X  X un swiftly X  X .
Syntactical relationships also exist between constituents, for example, the subject of the verb between two constituents of noun and verb phrases, e.g.  X  X  X he boys run swiftly X  X . Prepositions are sets of words that relate constituents as well, e.g.  X  X  X he boys by the roadside run swiftly X  X .

Primarily, this study is confined to the dependencies (i.e. relational information) that identify subjects and objects of verbs, relate the subjects and objects with their attributes and provide links between entities. These dependencies are considered because queries are usually refined by attribute(s) of an entity, action/motion, or used in relation to another en- X  X  X ead-dress X  X  is refined by an attributive word  X  X  X eather X  X  showing that the user specifically requires an Indian chief with not just any head-dress but a feathered one.

Adjectives modify nouns/pronouns, telling us more about these nouns/pronouns. A linguistic parser identifies the rela-tionship between an adjective and the noun/pronoun as the adjectival modifier. Keeping this relation in an index model will cover most abstract and perceptual refiners (e.g. genre, geographic, emotive, size, color, shape and texture). Adverbs typically modify verbs, telling how, when, where, and to what degree the action is performed. But there is more to adverbs than that; adverbs put the content of a sentence into the proper context, like time, place, manner, reason, condition, ex-tent. While the most common form of adverbs is made out of adjectives by adding the suffix X  ly (e.g. loud ? loudly), we can find noun/noun phrases, prepositional phrases, whole clauses, participles, absolute phrases and infinitives functioning as adverbs. They are difficult to identify as they are realized by many different kinds of structures ( Morenberg, 1997; Sem-melmeyer &amp; Bolander, 1987 ). Hence, other than the simple adverbial, temporal and quantifier modifiers, adverbs are be-yond the scope of this paper.

Three dependencies  X  nominal subject, direct/indirect object and prepositional modifier, are used to identify the relation between two entities. The nominal subject dependency relates to a noun/noun phrase which is the syntactic subject of a clause/sentence to the governing verb while the direct/indirect object dependency gives the object dependent on the gov-erning verb. Hence, two entities (i.e. subject and object) are linked together by the verb. A prepositional modifier defines the relation between a prepositional phrase and either a verb, an adjective, a noun or another preposition that is modified by the prepositional phrase. Complex adverbs are not handled here and we are only concerned with prepositions followed by a noun phrase complement ( Moreno, 2005 ).

A linguistic parser that is able to extract such dependencies from the image contextual information for the construction of multifaceted indexes is required, thus facilitating the construction of multifaceted indexes. For example, two-facet indexes are derived with the following patterns: Signal X  X bject (e.g. yellow irises) Abstract X  X bject (e.g. largest cat) Abstract X  X bstract (e.g. powerful symbol) Also, three-facet indexes are constructed with the following patterns: Object X  X elational X  X bject (e.g. cow on car) Object X  X elational X  X bstract (tiger is a symbol) Abstract X  X elational X  X bject (e.g. vision of tigers)
The patterns are however not restricted to those mentioned above. Possible patterns include object X  X elational (e.g. baby crying), signal X  X cene (e.g. red horizon), abstract X  X bject X  X elational (e.g. fat lady sings), and so on. A combination of the two-facet and three-facet indexes provides the n -facet indexes. 3.4. Processing steps The algorithm for the proposed multifaceted framework shown in Fig. 5 is explained in this section.

In the initial stage, webpage processing which involves webpage segmentation and image segment extraction is em-ployed using the framework in Fauzi et al. (2009) featuring a DOM tree-based webpage segmentation tool which draws on the characteristics of Web images and partitions a single webpage into a number of image segments (a full description of the processing is in Fauzi et al. (2009) ). Each image segment consists of an image and contextual information from multi-ple sources (i.e. the multimedia artifacts). Based on the source location, the image contextual information is differentiated between visible and hidden texts and pre-processed accordingly. The hidden texts go through the standard text processing which involves the stripping of HTML format related keywords, common stop word removal, URL parsing and decoding whilst the visible texts undergo a sentence boundary check to address the sentence fragmentation problem which is impor-tant for the linguistic parsing as the performance of a parser is dependent on the completeness and correctness of a sentence. The pre-processed multimedia artifacts are inputs to the next stage (Stage 2).

In Stage 2, the three steps of NLP/pre-classification, classification and weighting take place. The NLP/pre-classification step is illustrated in the process flowchart in Fig. 8 . It takes the multimedia artifacts as input. Again, the type of contextual information determines the process flow, whereby the visible contextual information has additional processes on top of the processes that both types of contextual information have to undergo.
 In the left flow of Fig. 8 , both the visible and hidden contextual information are unified. The merged text is searched for
NCPs, which is performed by the NCP handler. A lexical resource is looked up to identify these phrases. In addition, a syn-tactic-based phrase construction tool is employed over non-syntactic methods as it yields better results ( Fagan, 1987 ). The
NCP Handler constructs NCPs based on the syntactic term dependencies it receives from the linguistic parser. Thus, this will address the bounded knowledge of the used lexical resource in handling NCPs.

Next, in the WSD module, the text is tokenized (an NCP is considered as one token) and each token is tagged with its approximated POS and sense number as enumerated in WordNet. The sense selection for a token is achieved using Banerjee 2009 ) with extended gloss to measure the semantic relatedness between the token and its neighboring tokens by comparing the glosses of their various senses as well as the glosses of the words semantically related to the token (i.e. the extended gloss). The sense of the token whose gloss has the most words in common with the glosses of the neighboring tokens is cho-sen as its most appropriate sense. The number of neighboring tokens taken into consideration is defined by the window size parameter ( Banerjee &amp; Pedersen, 2003 ). The resulting disambiguated tokens, which are either single-word concepts or multi-word concepts, are input to the next step of classification.

Simultaneously, in the right flow, the visible contextual information is parsed by the Linguistic Parser module to obtain the syntactical dependencies between binary concepts. The obtained dependencies are parser-dependent and dependencies that match the criteria described in Section 3.3 are extracted by the Dependency Extractor: 1. Subject X  X bject relationship : a subject concept that performs an action on an object concept. 2. Attributive dependency to the subject/object : a concept with its attribute. 3. Spatial relationship : a concept relatively positioned with another concept. 4. Nominal noun modifier : a noun as a modifier to a noun concept (i.e. head noun) for the syntactic-based NCP construction.
From the selected dependencies, those that identify compound nouns are used by the NCP Handler to build the multi-word tokens (i.e. NCPs which are single-facet indexes) and the remaining dependencies are passed to the next sub-stage (Classification Step) to construct multifaceted indexes.

The second step (i.e. Step 2 of Stage 2) is where the disambiguated tokens are classified into their respective facets by the classifier. Given the lexical resource and the hypernyms/concepts that characterize the facets as well as the POS information, each token is classified as shown in the pseudo code below: Algorithm: Classification Module Input: disambiguated tokens FOR each token: SWITCH (part_of_speech) CASE noun:
CASE verb:
CASE adjective:
CASE adverb: DEFAULT: token_class = not_set Output: classified tokens
For the first level of classification, the POS information is used. Depending on the token X  X  POS, it can be classified under the relational facet if it is a verb, abstract facet if it is an adjective not similar or related to the color and pattern concepts and adverb if it is an adverb. We can extract valuable information from adverbs such as the manner, time, and extent.

Next, the knowledge from the lexical resource is utilized to classify noun and adjective tokens. Table 1 lists each facet and its characterizing hypernyms and concepts. For each token, we traverse through the hypernymy semantic relation and obtain the token X  X  related general concept that distinguishes between facets. For example, for the token  X  X  X ellow X  X , its hypernym in WordNet is  X  X  X hromatic color X  X . The latter has itself a hypernym:  X  X  X olor X  X . Since  X  X  X olor X  X  is a characterizing hypernym, the token_class variable is set to  X  X  X ignal X  X . All tokens go through the same procedure producing the single-facet concepts.

For the classification of NCPs which are put together based on the syntactic dependency of noun compound modifier (and not found in WordNet), they take on the semantic concept classes (facets) of their head nouns. These NCPs are also consid-ered as single-facet concepts.

Having obtained the single-facet concepts, the multifaceted concepts are constructed based on the shortlisted dependen-cies. These dependencies, which are extracted in the earlier step by the Dependency Extractor, define the relations between two or more single-facet concepts.

Hence, given the list of dependencies, more complex indexes are put together according to the dependency type: 1. Subject X  X bject relationship 2. Attributive dependency/relationship to the subject/object 3. Spatial relationship
Each component in the multifaceted concept maintains its facet. In other words, the multifaceted concept that is con-structed from the attributive dependency is a composition of either: signal and object, abstract and object, signal and ab-stract, abstract and abstract, signal and scene or abstract and scene facets depending on the facet of each of its components (e.g.  X  X ed car X  is a multifaceted concept composed of signal and object concepts respectively). 4. Experiments and results
In this section, we detail out the experimental design for evaluating the performance of our proposed multifaceted image indexing model.
 4.1. Experimental setup
In the empirical setup, four implemented indexing frameworks are compared  X  the first two are based on baseline tech-niques using all contextual texts first unweighted and then weighted using the tf  X  idf weighting scheme; the third framework introduces a term location weight in addition to the term frequency and the fourth is our proposed multifaceted indexing framework. 4.1.1. Framework 1 (F1)  X  All texts
The first baseline experimental framework corresponds to our first stage in the entire framework. The segmentation module implements a DOM tree-based segmentation algorithm ( Fauzi et al., 2009 ) to extract images and their contextual information. For each image, the extraction and pre-processing module splits the contextual information into visible and hidden texts and performs additional pre-processing steps on the hidden contexts (c.f. Fig. 8 ).

The hidden contextual information is pre-processed for reasons described earlier. This includes filtering out information coming from page format-related attributes, URL parsing and decoding as well as stop word removal. URL parsing consists in extracting the path, parameter, query and fragment components from the general structure of a URL: scheme://network_location/path;params?query#fragment . URL decoding involves the replacement of the two-digit hexadeci-mal representation that is preceded by a  X  X % X  X  symbol with its respective ASCII control character (e.g. %20 for the space character, %21 for the character  X  X ! X  X , %26 for the character  X  X &amp; X  X  etc.).

Finally, the pre-processed hidden strings are recombined with the visible strings and go through a stop word removal process. A stop word list is built upon the initial stop word list obtained from http://www.ranks.nl/resources/stopwords.html with common HTML format-related stop words and navigational terms. The remaining texts are used to index the image. 4.1.2. Framework 2 (F2)  X  term frequency (tf)  X  inverse document frequency (idf)
We implement the tf  X  idf scheme on the processed contextual information ( Lan et al., 2005 ) to give higher weights to rel-evant information over noise. This classical weighting scheme works well in the information retrieval and text mining appli-cations. In information retrieval, the tf  X  idf weight is a statistical measure used to evaluate how important (relevant) a word/ term is to a document in a collection. The importance increases proportionally to the number of times a term t appears in the document ( tf t ) but is offset by the frequency of the term in the corpus ( idf the term t given by n t in a document as shown in Eq. (1) . M is the total number of terms in the document, it is used for nor-malization to prevent a bias towards longer documents
The inverse document frequency is a logarithmic value of the total number of documents divided by the number of doc-uments in the collection that contain the term. It is used to discriminate between documents for the purpose of scoring and defined according to where N is the total number of documents in the corpus and df a rare term is high, whereas the idf of a frequent term is low.

In the case of images in a webpage, the tf  X  idf weighting scheme is adapted to suit our condition. A webpage can be par-titioned into one or more image segments. Each segment consists of an image and strings of texts. An image segment is con-sidered as the basic unit, rather than a webpage. So the tf is the number of occurrences of the term within a segment and the number of documents in the idf corresponds to the number of image segments. 4.1.3. Framework 3 (F3)  X  Location-based tf X  X df
The Web image contextual information comes from multiple sources/locations (i.e. HTML tags, e.g. h A i , h P i , and h IMG i or mation of Web images ( Fauzi &amp; Belkhatir, 2010 ), we have shown that the different locations of the contextual texts do have an effect on the relevance of a term to an image  X  there is a higher probability to find terms relevant to the image content in some locations compared to others. The terms that are considered relevant are manually selected by the subjects in the user study. Each of the relevant terms is associated to its location within the HTML source of the webpage. The frequency distri-bution of relevant surrounding descriptors for each location is calculated and tabulated according to the webpage category.
Table 2 summarizes, for each webpage category, the locations that are considered important for the extraction of seman-tically relevant contextual information and statistically significant, ordered by the frequency distribution.
Hence, the tf  X  idf scheme is adapted to take into account the location of the term by considering a probabilistic weight proportional to the probability distribution of the locations of relevant surrounding image information obtained from Fauzi a probabilistic weight proportional to the probability distribution of the locations of relevant surrounding image information obtained from where n tk is the frequency of text t at location k , a k served from the study; m is the total number of terms extracted from the image segment and k is the total number of loca-tions found per image segment. Both parameters m and k are used for normalization to prevent bias towards larger segments with terms coming from more locations. 4.1.4. Framework 4 (F4)  X  Proposed multifaceted indexing framework
Here, the three steps in Stage 2 are implemented to address the multifaceted conceptual indexes (i.e. phrase and sen-tence) highlighted by users. The pre-processed texts from Stage 1 go through the NLP, classification and weighting processes in Stage 2. An organization of concepts into five facets is produced.

In Step 1 of Stage 2, both hidden and visible contextual information go through NCP detection. An NCP identifier is imple-mented using the WordNet knowledge base and an n -gram technique to find sequences of n words in a sentence that are
NCPs. Once the NCPs are detected, POS tagging and WSD are implemented using the WordNet::SenseRelate::AllWord tool which can be obtained from http://www.d.umn.edu/~tpederse/senserelate.html . This tool performs simple POS tagging on the extracted text and uses WordNet to disambiguate words in their context. Hence, it will output the word with its corre-sponding WordNet sense. This is suitable for our classification module as it uses WordNet for its classification. However, this limits our proposed framework to the words within WordNet.

The WSD tool assigns a sense number to the word based on the context of a window of words (by default the window size is equal to 3) and using the lesk semantic relatedness measurement.

The classification module is implemented based on a decision tree algorithm where the classification decision is based on the POS information and sense number obtained from the linguistic parser and WSD tool.

A comprehensive linguistic analysis is performed on the visible text. The reason for choosing only to analyze the visible text is because it contains more well-constructed sentences. Ill-formed text can decrease the accuracy of the parser. We uti-lize the Stanford NLP parser available at http://nlp.stanford.edu to perform the linguistic analysis. An extensive set of 55 dependencies is currently identified by the parser ( De Marneffe &amp; Manning, 2008 ). Out of the 55 dependencies, 11 depen-dencies are found to be most closely related to the subject X  X bject relationship, attributive relationship with respect to the subject/object, spatial relationship as well as the nominal noun modifier (c.f. Table 3 ). The Dependency Extractor extracts these dependencies from the resulting dependency list which will be used to build compound nouns and the n -facet con-cepts by the NCP Handler (of Stage 2: Step 1) and Multifaceted Token Constructor (of Stage 2: Step 2) respectively.
To build the multifaceted concepts, the Multifaceted Token Constructor puts together the three previously defined pat-terns/relationships from the 11 syntactic dependencies in table 3 : Subject X  X bject relationship
The multifaceted token of noun1 [verb] noun2 is constructed from a nsubj(v1, n1) and dobj(v2, n2) pair, where v1 = v2 (both relations share the same verb).
 Attributive dependency to the subject/object
The multifaceted token of noun [has_attribute] attribute is constructed from either of these syntactic dependencies: amod(noun, adj), tmod(noun, time), num(noun, value) or nsubj(noun, adj).
 Spatial relationship
The multifaceted token of noun1 [preposition] noun2 is constructed from the prep_p(n1, n2) dependency, where both n1 and n2 are nouns and p is the relating preposition.

More patterns can be included in the future. However, the effect of this inclusion on precision and recall should be inves-tigated further.

The newly constructed multifaceted tokens are added to the image annotation set. Therefore, the final image annotation set consists of the single-facet tokens of single-word and multi-word concepts as well as multifaceted tokens which link two or more single-facet tokens. 4.2. Evaluation 4.2.1. Dataset
We use Web images from various webpages as our experimental dataset and do not restrict ourselves to any particular domain. The webpage could be a product page, education page, news page or online photo album page. This dataset was put together in a separate user study, where 100 webpages were presented to 33 subjects ( Fauzi &amp; Belkhatir, 2010 ). The subjects were asked to pick out images and semantically relevant contextual information. A total of 1019 images were identified with relevant contextual information. We employed a webpage segmentation algorithm to automatically extract the images and their contextual information, where the contextual information is sorted according to HTML tags, or what we termed as loca-tions. The automated extraction resulted in a multimodal dataset consisting of 5337 multimedia artifacts, consisting of 1019 images and 4318 contextual chunks from various locations. The extracted images have a total of 3943 valid user-selected relevant contextual information (an average of five indexes per image) which is used as our ground truth in the evaluation exercise. Table 4 shows some example images with their user-selected annotations and the top system-generated annotations. 4.2.2. Evaluation criteria
For the evaluation of all experimental frameworks, the images are automatically annotated. These automated annotations are then evaluated against manual annotations by the users in a precision/recall evaluation framework. Hits, misses and noise are all taken into account in this evaluation framework. The two performance measures are the Precision and Recall indicators. Here, Precision is the percentage of correctly annotated words over all annotations suggested by the system and Recall is the percentage of correctly annotated words over the number of genuine annotations in the dataset. However, these two measures do not consider the order in which the annotations are weighted and the ranked annotations for exper-imental frameworks 2 and 3 would not be justly evaluated. Hence, to consider ranked annotations, the precision is calculated at all the 11 recall levels (0.0,0.1,0.2, ... ,1.0), commonly known as the 11-point interpolated average precision . The average precision takes into account both precision and recall and measures the ranking quality of the indexes.
 For F1 (baseline), where all terms are used as image annotations, the annotations are listed in order of their appearance.
For F4, the annotations are ranked using the location-based tf  X  idf weights, as experimentally, it is an improvement over the original tf  X  idf weighting scheme. For multi-word concepts, we apply the tf  X  idf weight to the multi-word concepts similar to the phrase-weighting scheme suggested in Fagan (1987) , where the weights of each individual term in a phrase are summed up. Then the average of the sum is taken as the phrase weight.

To evaluate the overall performance, we report the Mean Average precision (MAP) , another common measure in infor-mation retrieval. MAP is the mean value of the average precision over all images in the dataset.
 Results are illustrated in an averaged 11-point precision/recall graph (cf. Fig. 9 ) and the overall performance, in terms of MAP, is shown in table 5 .

As far as F4 is concerned, in addition to the overall system performance, we report the performance of each facet using the same metrics (cf. Fig. 10 ). The results are discussed in length next. 4.3. Results and discussion
The experiment involving F1 yields a low precision of 13.1% and recall of 48.6% and the MAP is 12.5% . The low precision is expected as it indicates the presence of many irrelevant texts within the image contextual information. One would expect the recall to be higher as we considered all texts within a segment as the annotations. However, this is not the case as many of manual annotations include phrases and sentences. Evidently, a keyword-based method does not cater to this.
In the experiment with F2, a simple weighting scheme such as tf  X  idf can sufficiently position the relevant terms higher in the list above the noisy terms as the MAP is at 21.7% .In Fig. 9 , we notice the marked improvement in the precision of F2 over F1 at all recall values.

Words are equally weighted regardless of their position within a webpage as far as F2 is concerned. For an image and its contextual information, the position of a word with respect to the image is considered to have an effect on its relevance to the image. Hence, for F3, we added a location weight to the existing tf  X  idf weighting scheme and the MAP is 23.4% . We see a marginal improvement in the performance of the location-based tf  X  idf in comparison to F2. However, this is not discourag-ing. The reason behind this might be due to the way the term location feature was used. By simply multiplying the proba-bilistic location weight (Eq. (3) ) to the tf expression, the resulting weight is still very much dependent on frequency. We need to investigate other means of scoring relevance based on location which is independent of term frequency. This is worth looking into because occasionally we can find image contextual information that is short but describes the image precisely. This is the case of contextual information consisting of words with low frequency yet still relevant to the image.
While there is a noticeable improvement in the precision at recall levels between 0 and 0.8 when the annotations are ranked for F2 and F3 over F1, there is a minor improvement at recall levels greater than 0.8 in Fig. 9 . This means that even when all relevant annotations are highlighted, there are too many irrelevant annotations which renders the average preci-sion to be negligible. We reason that this is because the tf  X  idf scheme ignores word semantics, relations between words and typically reduces texts to single word tokens. In practice, there are many words that are used together to give entirely dif-ferent meanings from the individual words (e.g. fountain pen, school bus, White Stripe, etc.).  X  X  X ountain X  X  and  X  X  X en X  X  are both highlighted as indexes even though  X  X  X ountain X  X  is not at all representative of the image. These are the NCPs that must be ta-ken in their entirety and words like  X  X  X ountain X  X ,  X  X  X chool X  X  or  X  X  X hite X  X  should be excluded from the list of image annotations. Clearly, the tf  X  idf weighting of single word terms alone is insufficient even with the added weights based on locations. Semantic understanding of the image contextual information is required.

For F4, we applied natural language processes in our attempt to semantically understand the image contextual informa-tion. We utilized the linguistic knowledge as well as an external knowledge base (WordNet) to identify the NCPs and exclude non-descriptive noun modifiers. We make use of linguistic techniques to discover the semantics of a word and classify it within its high-level facet/concept class that relates to the image description model described in Section 2 . Lastly, we draw on the syntactical dependencies to identify relationships between concepts. Our proposed method is promising. The MAP is 30.5% and in Fig. 9 , the precision is the highest at all recall levels.

Fig. 10 reports the performance for each facet as far as F4 is concerned. Here, the annotations are streamed into the five keeps all words not defined in WordNet while the adverb facet holds all words whose corresponding POS is an adverb.
It can be observed from Fig. 10 that the object facet performs best, followed by the scene, not_set, abstract, signal, and relational facets. The result of the not_set facet highlights the limitation of WordNet, where a number of concepts which are considered relevant to users are not defined in WordNet. A quick inspection of the concepts in the not_set facet shows that these undefined but relevant concepts are typically named entities (i.e. proper nouns) such as  X  X  X harlie Brooker X  X ,  X  X  X arack
Obama X  X , and  X  X  X ose Fritzl X  X . As for the remaining concepts gathered in the not_set facet, we can find erroneous words, acro-nyms and abbreviations. The adverb facet performs worst and this is expected because we did not process it as it is beyond the scope of this paper.

Fig. 11 compares the performance of the proposed framework as far as the automatic annotation of multifaceted concepts is concerned against n -gram (2-gram and 3-gram) indexing techniques. Our proposed method of constructing multifaceted indexes outperforms the statistical n -gram method for both the 2-gram and 3-gram indexes. The simple method of leverag-ing on the syntactical relationships between concepts is promising indeed especially for the three-facet concepts. This indi-cates that relationships between concepts are important for indexing and almost impossible to derive from statistical-based techniques. Concepts and their attributes can be adequately addressed by the n -gram technique but the syntactical approach of taking concepts that are the subject or object of the sentence together with their attributes has reduced the number of irrelevant indexes as shown by the higher precision results in Fig. 11 . It is worth to examine other compounding relation-ships to obtain more elaborate concepts in the future. 5. Conclusions and future work
We introduced in this paper a multifaceted concept-based indexing model which is based on an related description model that takes into account the way users characterize the image content. The proposed model takes a webpage, extracts all images and their image contextual information, converts the contextual information into a set of indexing terms, then for each term, it is classified into one or more of the five semantic facets. NLP techniques and a knowledge resource (WordNet) are used for the classification. The classification is performed automatically with the raw image contextual information ex-tracted from any general webpage and is not solely based on image tags like state-of-the-art solutions. Furthermore, the syn-tactical relationships (from a linguistic analysis) between faceted terms are leveraged to construct the multifaceted indexes.
The proposed model is implemented and evaluated separately for single-facet (i.e. five semantic facets) and multifaceted indexing using a human-labeled dataset. It is shown to outperform baseline techniques implementing the standard usage of contextual information. The MAP values for single-facet and multifaceted indexing are 30.5% and 9% respectively and for the tf X  X df and n -gram indexing techniques, 21.7% and 4% respectively. The results are promising indeed. It should be noted that the results include all flaws of each individual component in the framework, from the extraction to the classification process.
The wrongly segmented images, the wrongly classified tokens (due to incorrect sense disambiguation output) or the wrongly derived syntactic relations (due to ungrammatical text or linguistic parsing errors) are not excluded.
While the focus of this article is mainly on the indexing model, future work will involve applying it in an image retrieval setting. A faceted retrieval model can be both explicit and implicit, where users will have the option to select the desired and match to the index accordingly (i.e. implicit). These facets will make it possible for users to formulate topic-based que-ries, where a topic is defined as a non-trivial multifaceted description of an information need.

The need for a fusion-based system, where both concept and content-based information come together is still necessary as the use of image contextual information is limited by the depth/richness of the context around the image (i.e. some tex-tual features which are indeed representative of an image might not be present in the context and thus will not be indexed).
The categorization of image contextual information into different facets can facilitate the fusion of indexes from the image contextual information and indexes derived from the low-level content-based characterization. This operation will indeed be more straightforward with the line drawn between tangible (i.e. visual, object, and scene) and intangible concepts (i.e. ab-stract) in our proposed five semantic concept type characterization. We could also imagine refining further this classification and propose sub-classes for each of these five classes of semantic concepts. For example, the visual concept class could con-sist of color, texture and shape sub-classes. The scene concept class could be further partitioned into location, event, collec-tion/aggregation sub-classes and so forth.
 References
