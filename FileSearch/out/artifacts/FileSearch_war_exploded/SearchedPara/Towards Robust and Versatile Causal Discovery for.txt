 Causal discovery algorithms can induce some of the causal relations from the data, commonly in the form of a causal network such as a causal Bayesian network. Arguably how-ever, all such algorithms lack far behind what is necessary for a true business application. We develop an initial version of a new, general causal discovery algorithm called ETIO with many features suitable for business applications. These in-clude (a) ability to accept prior causal knowledge (e.g., tak-ing senior driving courses improves driving skills), (b) admit-ting the presence of latent confounding factors, (c) admitting the possibility of (a certain type of) selection bias in the data (e.g., clients sampled mostly from a given region), (d) abil-ity to analyze data with missing-by-design (i.e., not planned to measure) values (e.g., if two companies merge and their databases measure different attributes), and (e) ability to analyze data from different interventions (e.g., prior and pos-terior to an advertisement campaign). ETIO is an instance of the logical approach to integrative causal discovery that has been relatively recently introduced and enables the solu-tion of complex reverse-engineering problems in causal dis-covery. ETIO is compared against the state-of-the-art and is shown to be more effective in terms of speed, with only a slight degradation in terms of learning accuracy, while in-corporating all the features above.The code is available on the mensxmachina.org website.
 Causal Discovery; Semi-Markov Causal Models; Latent Vari-ables; Selection Bias; Multiple Datasets; Bayes-Ball Algo-rithm; Answer Set Programming; Interventional Data
Knowledge of causal relations is necessary to plan effec-tive interventions, such as launching a new advertising cam-paign or a promotion. Causal discovery algorithms attempt to induce some of these relations from data, often presenting them in the form of a network such as a causal Bayesian net-work. Unfortunately, we argue that most approaches leave much to be desired for a true business application. For ex-ample, Bayesian networks assume that there are no latent confounding factors or selection bias, which is unrealistic and leads to erroneous inductions [21, 19]. Other algorithms admit latent confounders but are nevertheless brittle to sta-tistical errors and small sample sizes. Other desired features of the algorithms are also missing, e.g,. the ability to im-pose causal prior knowledge such as  X  X o measured quantity causally affects the age of a client X  or to co-analyze data that follow different distributions, e.g. before and after a promotion.

Recently, the problem of causal discovery has been formu-lated as a logic-based inverse engineering problem [25, 14, 13, 24]. The approach has paved the way for algorithms that are able to handle much more complex settings, impos-ing fewer, less restrictive assumptions. The price to pay is increased computational overhead. The main idea is the fol-lowing: each conditional (in)dependence discovered in the data through a statistical hypothesis test imposes a con-straint on the presence or absence of paths in the (unknown) causal graph, after accounting for the effects of interven-tions and the presence of selection. The graphs that satisfy all constraints are possible solutions to the problem. This approach leverages decades-long research in logic-based in-ference engines such as SAT solvers, logic-programming, and answer set programming. ETIO (from the Greek word for  X  X ause X ) is a proposed algorithm that follows this approach and demonstrates the usefulness and potential of these re-cent advances to causal discovery on business data. Specifi-cally, ETIO has the following features important for a busi-ness application: Admits latent variables : Latent confounding factors pre-sent inherent problems to causal Bayesian networks and lead to erroneous inductions. Confounding factors are quantities that causally affect two or more of the measured quanti-ties. For example, if the true graph contains the subgraph X  X  L  X  Y where L is not measured or included in the data, a dependence is induced between X and Y . Any Bayesian network algorithm will asymptotically return ei-ther the network X  X  Y or X  X  Y trying to explain the dependence. The induced network is equivalent to the true one for predictive purposes but erroneous for causal pur-poses: neither X nor Y causally affect each other. The presence of latent confounders is a possibility that cannot be excluded a priori in most realistic scenarios. ETIO employs more advanced formalisms and theory to account for the presence of latent confounders, namely semi-Markov causal models ( SMCM ) [22].
 Admits selection bias in the data : Selection bias is another source of errors if ignored. Imagine that X and Y are two factors with no causal relation between them that both affect whether a person chooses to stay in New York State denoted as N . A business that operates in New York and deals only with local clients gathers data conditional on N = 1. In their data X and Y will be found dependent. A Bayesian network algorithm will induce that either X  X  Y or X  X  Y trying to explain the dependence which is wrong from a causal perspective. Again, for predictive modeling selection bias presents no problems as long as one applies the model to the same population, in this case New York State clients. However, if ignored, the presence of correlations due to selection (conditioning) leads to causal inductions that are wrong even for the same population. ETIO can handle selection that depends on the observed variables, but not on latent confounders and observed variables at the same time. Handling data with missing-by-design values : Sup-pose a business merges two of their internal databases con-taining some common variables stored and some distinct, e.g., in case of an acquisition of a new company. Thus, the pooled data matrix contains large blocks of missing values. These values are called missing-by-design in the statistical terminology [8]. Using the logic-based approach, ETIO is able to make use of all of the available data and make non-trivial inferences such as inducing relations between vari-ables never jointly measured (similarly to [27]).
 Handling data from different interventions : In an-other scenario, a business initiates an intervention, e.g., a promotion, an ad campaign, or a change in their portal. The data distribution before and after the intervention may be different; however, the internal causal mechanisms that determine customer behavior have not changed, only the exogenous conditions and stimuli to the customers. ETIO looks for causal models that fit to the data after accounting for the effect of manipulations similarly to [14, 13, 24] and can handle data under different interventions and distribu-tions.
 Incorporating Causal or Associative Prior Knowl-edge : Typically, the semantics of the variables carry impor-tant causal information. For example, no client attribute causally affects their age; dropping one X  X  subscription occurs after all other recorded variables and thus cannot be causally affecting any other measured quantity, such as the cost of the insurance plan. More complicated scenarios are also possible to encode such as asserting the belief that taking senior driving courses improves driving skills (i.e., asserting the presence of a causal path in the graph). ETIO accepts this type of causal knowledge facts and employs them to constrain the set of admissible solutions.

In the next sections, we demonstrate and explain the ca-pabilities of ETIO on a use case from the insurance domain. We also perform empirical evaluation experiments against the only other alternative in the literature [13]. The latter does not implement all the features of ETIO but follows a similar approach and could potentially be extended to this direction. The main differences are the encoding scheme and conflict resolution strategy used. In simulated experiments on observational data ETIO shows better scaling up, while performing slightly worse in terms of learning accuracy. We also compare ETIO against FCI [21, 28], a prototypical algo-rithm for causal discovery admitting latent confounders and selection bias in the data. ETIO and FCI perform similarly in terms of learning accuracy. However, the main goal and contribution of this paper is to introduce and illustrate the potential of the logic-based approach to causal discovery to address the needs of business applications.
We assume the reader X  X  familiarity with Bayesian net-works ( BN ); see [21, 19, 17] for an introduction to causal-ity and Bayesian networks. Semi-Markov causal mod-els ( SMCM ) [22] (also known as acyclic directed mixed graphs (ADMG)) are generalizations of causal BNs that rep-resent the presence of latent confounders. An SMCM rep-resents both (in)dependence relations as well as causal rela-tions among variables. The structure of SMCMs is a graph G =  X  V , E  X  , where V is a set of nodes and E a set of edges. Nodes represent variables, whereas edges represent relations between variables. SMCMs may contain both, directed (  X  ) and bi-directed (  X  ) edges. A directed edge X  X  Y denotes that X is a direct cause of Y in the context of the mea-sured variables, while a bi-directed edge X  X  Y denotes that X and Y share a latent common cause. SMCMs rep-resent the causal and probabilistic properties of marginals of BNs. There may be at most one edge of each type (di-rected or bi-directed) between two nodes. The graph G of a SMCM is acyclic , that is, there is no directed cycle in it so that causal feedback is excluded. A node X is a parent ( ancestor ) of Y , and Y is a child ( descendant ) of X , if X  X  Y (a directed path from X to Y , denoted as X 99K Y ) is in G . An edge between X and Y is into Y , if X  X  Y or X  X  Y , and is out of Y otherwise ( X  X  Y ). Similarly, a (not necessarily directed) path between X and Y is into Y if the previous node on the path is into Y , and out of Y otherwise. A triplet  X  X,Y,W  X  of nodes is called a collider in G if both X and W are into Y .

Selection bias [6, 2] arises if the samples are not uni-formly randomly selected from a population but their inclu-sion in the dataset depends on one or more variables, e.g. in case a business mostly has clients from a specific region. Selecting a sample for inclusion in the data can be mod-eled as a dummy binary variable N . All samples are thus conditioned on the fact that N = 1: selection bias is equiv-alent to conditioning . If N is causally affected by observed variables X  X  N  X  Y then X and Y will be found depen-dent in the sample. Thus, selection bias introduces spurious dependencies among variables not present in the full popula-tion. Selection bias cannot be ignored in a practical business application; one needs to model it during causal discovery. ETIO admits the possibility that some observed variables S may be causing selection N . We assume that selection in the sample does not causally affect any other observed variable, e.g., the fact that a client has been included in our database does not affect any other of their attributes. In addition, we assume that selection is not caused by latent confounders (e.g. X  X  L  X  N  X  Y is not permitted); this complicates modeling selection and is left for future work. ETIO does not directly need to model the dummy variable N ; what is important to reason with are the variables that causally affect N . The following concept definition of m-separation , here generalized to include selection, is funda-mental in causal modeling: Algorithm 1 Bayes-Ball for SMCMs with selection Input : SMCM G , Node X , Conditioning Nodes Z Output : Nodes Y m-connected to X given Z 1: Visit X from node X { Case 0 } 2: 3: When visiting a node Y from node W : 4: if W = Y or ( W  X  Y and Y 6 X  Z ) then { Case 1 } 5: Visit U if Y  X  U or Y  X  U or Y  X  U or Y,U  X  S 6: end if 7: if ( W  X  Y or W  X  Y ) and Y  X  Z then { Case 2 } 8: Visit U if Y  X  U or Y  X  U 9: end if 10: if ( W  X  Y or W  X  Y ) and Y 6 X  Z then { Case 3 } 11: Visit U if Y  X  U or Y,U  X  S 12: end if
Definition 1. In a SMCM G with causes of selection in the set S (possibly empty), a path p in G between nodes X and Y is m-connecting relative to (condition to) a set of nodes Z S = Z  X  S , ( X,Y 6 X  Z S ), if: (a) every non-collider on p is not a member of Z S , and (b) every collider on p is a member of Z S , or an ancestor of some member of Z S . X and Y are said to be m-separated by Z S if there is no m-connecting path between them relative to Z S . Otherwise, X and Y are m-connected by Z S .

The major assumption of the prototypical algorithms for causal discovery [21] is that X and Y are independent con-ditioned on Z and selection S if and only if X and Y are m-separated by Z S . The  X  X f X  part is called the Causal Markov Condition and the  X  X nly if X  part the Faithful-ness Condition [21]. Intuitively, both conditions together imply that (in)dependencies appear only due to the causal structure and the selection process, not due to the exact pa-rameterization of the distribution. In(dependencies) are de-termined by performing an appropriate conditional indepen-dence test on the data . ETIO employs Bayesian tests that return the probability that a given conditional dependence holds in the data. The main principle in the logic-based approach to causal discovery is that results of the tests of independence correspond to m -connection or m -separation constraints that should hold in the unknown causal graph, after accounting for selection and interventions (discussed below). For example, if a given independence is more prob-able than the corresponding dependence, an m -separation constraint is imposed. ETIO imposes the constraints that correspond to test results, in order of probability, while re-moving conflicting test results.

Interventions (a.k.a. manipulations, perturbations) mod-ify the structure of the causal graph and change the data distribution, thus requiring special treatment and modeling. Reviews of various types of interventions, as well as discus-sions on their implications in causal discovery can be found in [15, 9]. The type of interventions we focus on are de-scribed following [9]. Such interventions may be modeled by including an additional indicator variable I for each differ-ent intervention with directed edges into the manipulated variables, taking values 1 in the samples where the specific intervention was present and 0 otherwise. Structural in-terventions (also known as hard interventions ) are inter-ventions that completely cut-off the influence of other causes on the manipulated variable (target), that is, they remove all edges into the target. For example, forcing all drivers to install an anti-lock breaking system (ABS) removes all its influences completely. Parametric interventions (also known as soft interventions) on the other hand affect the distribution of the target variable, but do not remove the influence of other variables and the corresponding edges. For example, a promotion to install an ABS increases the probability of installation, but does not eliminate all other causes. An intervention is confounding if it affects multiple target variables simultaneously. Naturally, an experimental dataset may stem from different combinations of interven-tions. Hereafter, we assume that probability distributions resulting from interventions remain faithful to the underly-ing, possibly manipulated, causal structure [10]. In addition, we assume that interventions are exogenous , that is, they are not causally affected by and are not confounded with the modeled variables.

For structural interventions incoming and bidirectional edges of the targets are removed in the graph that mod-els the data where I = 1; for parametric interventions these edges are not removed. In each case, an additional edge is included from I to each target variable (in a structural in-tervention this is actually not necessary). An intervention is uncertain if its targets are not exactly known. For un-certain interventions ETIO will have to figure out from the data the possible targets; for certain interventions, the cor-responding manipulation edge removals and additions can be imposed as facts. Finally, we say that an intervention is possibly ineffective if it is not known whether it is para-metric or structural (see [9] for a discussion). ETIO can handle possibly ineffective interventions too.
The Bayes-Ball algorithm [20] (shown in Algorithm 1 and adapted to fit our notation) identifies all nodes Y m -connect-ed with one or multiple nodes X given a set Z in a Bayesian network (or DAG). Here it is extended to handle SMCMs with selection. In later sections, the algorithm is represented in logic so that m -connection constraints can be imposed to a logic solver. The Bayes-Ball algorithm leads to an efficient encoding and is the reason behind the improved computa-tional performance of ETIO, as shown in Section 7.

The algorithm starts from nodes X and visits other nodes according to the definition of m -connection so that it only visits m -connected nodes. The set S contains all nodes which directly (in the context of modeled variables) affect se-lection. It is easy to see that the extended algorithm shown in Algorithm 1 is correct according to the definition of m -connection; a proof sketch follows. For latent variables, it suffices to see that each bi-directed edge between two nodes Y and U corresponds to a parent L Y U of them. Visiting U from Y through a bi-directed edge (cases 1 and 2) is equiv-alent to two steps in the search in the original algorithm: first, L Y U is visited as a parent of Y (cases 1 or 2) and then, U is visited as a child of L Y U (case 1). Cases 2 and 3 are trivially applicable when W  X  Y , by substituting it with W  X  L WY  X  Y , which matches with the preconditions of cases 2 and 3. The naive approach to handle selection is to include an additional node N in G and in the condition-ing set Z . An alternative way is to mark each node as to whether it affects selection or not, and to extend cases 1 and 3 of the algorithm. This can be done by allowing the algo-rithm in both cases to visit Y and U if both are selected; no Table 1: Logic variables in the encoding and their semantics matter the incoming edge from W , Y and U are trivially m -connected through N , as N is conditioned on, as long as Y is not in Z . Finally, since U  X  N , the resulting m -connecting path is out of U . One important thing to note is that Y and U are not necessarily distinct nodes, otherwise cases such as X  X  Y  X  W would not be handled correctly if Y is in S .
A solution to a causal discovery problem is a causal graph that implies the same conditional (in)dependencies as found in the data, also called as a graph that fits the data. Typ-ical causal discovery algorithms return either an arbitrary solution or another type of graph representing the equiva-lence class of solutions (called essential graph or PDAG for Bayesian networks). In the type of problems handled by ETIO, the set of equivalent solutions cannot be represented by a graph and their number can grow exponentially; com-plete enumeration of solutions is impractical. ETIO instead follows what is called the query-based approach, where the user queries the algorithm about some causal struc-tural features of interest. For example, the user may query whether X causes Y in all solutions and thus necessarily en-tailed by the data ( m -connections and m -separations) and the assumptions. ETIO is able to reason using a possibly in-complete set of dependence, independence and prior knowl-edge constraints, which is not possible using classical causal discovery algorithms. Thus, ETIO can handle missing-by-design data or ignore statistical test results that are deemed unreliable. We proceed by providing a high-level overview of the ETIO algorithm; implementation details, as well as the specific instantiation decisions we used are described in Section 4.2.
 The input to the ETIO algorithm are: (a) a set of datasets D , (b) meta-information about the datasets MI , such as whether selection bias may be present, if they are observa-tional or inverventional data, and if so, which are the known (if any) intervention targets and what type of interventions were performed, (c) a set of structural prior knowledge con-straints PK , and (d) query features Q of the causal struc-ture that should be output (for example, directed edges and ancestral relations). The implementation of the algorithm also accepts parameters that dictate whether to admit la-tent variables and/or selection bias or not, not shown here for brevity; by default, we assume both are possible. ETIO first performs conditional independence tests on the data and represents the results in logic; it also represents known facts about selection and targets of interventions. It then se-lects a consistent subset of dependence and prior knowledge Algorithm 2 Basic ETIO Algorithm Input : Datasets D , Meta-Information MI , Prior Knowl-edge PK , Queries Q Output : Query Results 1: constraints  X  createConstraints( D , MI , PK ) 2: constraints  X  resolveConflicts(constraints) 3: results  X  makeInferences(constraints, Q ) constraints (in case there are conflicts) and finally it identi-fies invariant features implied by the input constraints. The procedure is summarized in Algorithm 2.
In this section we show how to encode dependence, inde-pendence and various kinds of prior knowledge constraints in first-order logic. We proceed by defining the primitive logic variables used in our encoding. For clarity, we will call variable a binary variable in the logic problem and node a random variable of our data distribution that appears in the unknown causal graph. Instead of using a predicate notation such as DirectedEdge( X , Y ) we use the notation X  X  Y as more convenient.

We are seeking the structure of a causal graph with nodes as many random variables appear in the union of the datasets as well as dummy variables I D indicating the presence of a possible intervention in a dataset D . For each pair of nodes X and Y , we introduce a primitive variable for the presence or absence of a directed edge X  X  Y , and one for the bi-directed edge X  X  Y . After imposing logical constraints, the truth assignment of these variables will correspond to an SMCM that fits all datasets after accounting for inter-ventions and selection. We also define the auxiliary variable X 99K Y to denote the presence or absence of a directed path from X to Y .
 A second set of primitive variables denoted as X  X  X  X  notes the presence or absence of the m -connection of X with Y in dataset D given subset Z . It is important to notice that different m -connections may hold after accounting for the interventions or selections in different datasets, thus the dataset parameter is necessary. There is one such variable for each pair X and Y of nodes, each input dataset, and each conditioning set Z that appears in any conditional indepen-dence test performed by the algorithm. The algorithm deter-mines and imposes the truth value of the X  X  X  X  by performing conditional independence tests on the avail-able datasets. Not all possible tests need to be performed or considered, as is the case for example when blocks of data are missing. Section 4.2 explains the details of the strategy for performing tests.

We distinguish between two different types of m -connecting paths: two variables X and Y may be m -connected by a path that is either into Y or out of Y , denoted respectively by the auxiliary variables X  X  X  &gt; tion is made to implicitly keep track of the edge that led to an m -connecting path from X to Y (that is, it is equivalent to recording the previous node W of the Bayes-Ball algo-rithm). As we will see below, those constraints are used to incrementally encode m -connecting paths.
 A third set of primitive variables denoted by X S D and X represent the fact whether node X is causing selection in D or is the target of a hard intervention in D respectively. There are X S D and X I D variables for each node X that ap-pears in any input dataset, and each dataset D . If in a dataset D a hard intervention has occurred with a known target X , then X I D can be set to true. If it is set to false, then the algorithm assumes that X is not a hard target of I in D . If it is not set, the algorithm will try to infer the value of X I D from the rest of constraints, if possible. Simi-larly, for causes of selection, X S D can be set to true or false if it is a known fact, or be left unknown to be determined by the algorithm. Obviously, the more information is known regarding the targets of interventions and causes of selec-tion, the more inferences can be made by the algorithm. All variables defined and their semantics are shown in Table 1.
Having defined the variables, we now present the inference rules that constrain their truth values and ensure their se-mantics are respected. The complete list of inference rules, including selection and interventions, is shown in Table 2. For the sake of simplicity we omit the existential quantifier  X  ; for example, the right part of rule 1 should normally we written as X  X  Y  X  (  X  U X 99K U  X  U 99K Y ) . For the moment, we ignore interventions and prior knowledge con-strains that are dealt with in subsequent subsections.
Rule 1 is used to define ancestral relations, while rule 2 enforces acyclicity. The remaining rules directly encode the Bayes-Ball algorithm in first-order logic. Rule 3 encodes that an m -connecting path between two variables X and Y relative to a separating set and dataset must be either into or out of Y . Rule 4a corresponds to case 0 in the Bayes-Ball algorithm, that is, that each variable is m -connected to itself. Rules 4b, 4d, 5a and 5c correspond to case 1 in Bayes-Ball: the right-hand side of those rules requires that there is an m -connecting path between X and Y that is out of Y , and that Y is not in the separating set. Similarly, rules 4c and 5d match case 2, while rules 4e and 5b match case 3 of the Bayes-Ball algorithm. Note that all rules except 2 are bidirectional which is necessary to be able to make useful inferences; implicitly this stems from the Faithfulness Condition which translates to the fact that if m -connection holds a dependence is implied.
 Encoding Interventions . We consider various different cases of both, structural and parametric interventions. Specif-ically we assume that the interventions are exogenous, non-confounding (also called independent) and not uncertain, but allow for possibly ineffective interventions. Under those assumptions we can handle: (a) single or multiple indepen-dent structural interventions, and (b) single or multiple inde-pendent parametric interventions. In principle the proposed encoding can also be extended to handle confounding, uncer-tain and non-exogenous interventions, but we did not further investigate those cases in this work.

In order to encode structural interventions it suffices to label the nodes as intervened or not intervened; we use vari-ables X I D to encode whether or not X has been intervened in dataset D . For possibly ineffective interventions one can simply omit assigning a truth value to the respective vari-able. Recall that, in case of structural interventions all in-coming edges at the target variable are eliminated and thus not observed in the data. Thus, it suffices to forbid certain m-connecting paths stemming from such constraints. Specif-ically, whenever an inference rule in groups 4 and 5 requires that there is an edge into a variable Y , we must ensure that Y is not manipulated; if Y is manipulated, the manipulated graph would not contain any edges into Y , and thus no such m-connection would be observed.

Parametric interventions, as already mentioned, can be encoded by including an additional indicator variable for each target variable. This variable can only have an edge into its respective target variable. In case it is possibly inef-fective, the value of the edge variable can be set to unknown. Finally, note that when including data from parametric in-terventions, the data have to contain at least two values for the indicator variable, in order to be able to perform con-ditional independence tests. No further special treatment is needed, at least not for the cases we consider.
 Incorporating Causal Prior Knowledge . Prior knowl-edge has several advantages: (i) it can reduce the number of errors in the output, as it helps in filtering out incon-sistent input, (ii) it can increase the number of inferences made by the algorithm, and (iii) it may also decrease the total running time of the algorithm. ETIO accepts struc-tural prior knowledge as in [4], i.e., knowledge about the causal structure in the form of hard constraints: structures not consistent with it are eliminated from the solution set. Interested readers on methods for structure learning with prior knowledge may refer to [1].

In principle, all kinds of structural prior knowledge can be incorporated, as long as it can be expressed in first-order logic. Examples of common types of structural prior knowl-edge are: (a) presence/absence of direct causal edges, (b) presence/absence of direct connections (causal edges and/or latent variables), (c) presence/absence of possibly indirect causal relations (that is, ancestral relations), (d) (condi-tional) dependence and independence constraints, (e) root or leaf nodes, that is, no incoming or outgoing edges respec-tively, (f) limits on the in/out-degree of nodes, (g) complete or partial order of variables. All of those can be easily en-coded using the primitive propositions (logic variables) de-fined in Table 1. Note that, all of those constraints have already appeared in the literature [18, 7, 4]. However, none of the previous approaches is general enough to handle both SMCMs and such a variety of structural prior knowledge. Correctness . A full proof of correctness and completeness is omitted due to lack of space. A proof sketch would fol-low the one-to-one correspondence with the Bayes-Ball algo-rithm to prove that all conditional (in)dependencies found in the data are imposed as m -connections or m -separations in the logical representation, in a way that every truth as-signment to the variables corresponds to a causal graph that entails the same m -connections. The algorithm is query-complete in the sense that it will output true for all queries (e.g., presence of a given edge) that are entailed by the data and the assumptions. Performing Conditional Independence Tests . In step 1, the ETIO algorithm performs several conditional indepen-dence tests on each input dataset D . Typical non-Bayesian tests are the G 2 for nominal nodes and the Fisher z-test for continuous nodes (a.k.a. partial correlation test). One could perform all possible conditional independence tests. This is feasible for small datasets of around 10 variables, as in most of our experiments. It may however be undesirable since conditioning on too many variables reduces the statistical power of the test, as well as increasing the number of con-straints to consider. For larger datasets, one can perform as many tests as possible, e.g., considering all possible condi-tioning sets up to k variables. For sparse networks a value of k = 5 should suffice. For even larger datasets, one could fo-cus on a set of nodes of interests and retrieve their Markov Blankets in a recursive fashion [26]; we intend to explore this direction in future work. Other strategies for selecting which tests to perform and focus on are possible and an in-teresting direction to explore to scale the algorithms. ETIO performs non-Bayesian tests that return p -values of the null hypothesis of conditional independence. It then employs an empirical-Bayesian method introduced in [24] (called MPR) to convert p -values of dependencies and independencies into probabilities. This allows ETIO to rank constraints to sat-isfy, in case there are conflicts. Alternative Bayesian meth-ods have been proposed in [16, 5]; the former is implemented and explored in our experiments. However, the method in [24] has very low computational overhead and is suggested for large problems.
 Conflict Resolution Strategy . In order to resolve con-flicts, a consistent subset of all input constraints has to be selected. Hyttinen et al. [13] use the method described in [16] to compute the probability of dependence or inde-pendence. Their method then tries to identify a subset of constraints that maximizes the product of the weights of all satisfied constraints. Although this has the advantage that it maximizes a well-defined objective function, it comes at a high computational cost. Instead, we chose a greedy approach, following [24]. The constraints are ranked in or-der of confidence (i.e., the maximum of the probability of dependence or independence), and are considered in that order. If including a constraint leads to an inconsistency it is discarded, and is included in the reasoner otherwise. Performing Inferences . In this paper we employ answer set programming (ASP) for conflict resolution, as well as for performing inferences. ASP is a declarative programming language, which is especially suited for computationally hard problems. Due to its declarative nature, it is well suited for encoding complicated problems. Answer set solvers usually consist of two phases: in the first phase, the input program is grounded 1 , and subsequently solved, often using a SAT solver. As an answer set solver we chose Clingo [11] (ver-
For example, in our case the first-order logic rules would be sion 4.5.4). Clingo supports multi-shot solving [12], which allows incremental grounding and solving. This is especially useful for performing the greedy conflict resolution we use. Without multi-shot solving capabilities, the whole ground-ing and solving process would have to be repeated for each constraint checked during conflict resolution, increasing the computational cost dramatically. Finally, in order to iden-tify all invariant features based on the input queries, Clingo can be run in enumeration mode , which identifies the in-tersection of all possible solutions. Alternatively, for each desired output feature f , one can query the solver and ask whether f and  X  f can be satisfied; if one is not satisfiable, then its negation holds. This leads to a linear number of additional solver queries for each input query. A better ap-proach is to also keep track of all encountered solutions and avoid repeating solver queries if some literal has already been encountered in some solution. We plan to further investigate this direction in the future.
We review related constraint-based methods for causal discovery. Methods such as the PC and FCI algorithms [21, 28] learn an equivalence class of BNs and maximal an-cestral graphs (an extension of BNs also admitting latent variables and selection bias; see [24] for its connection to SMCMs) respectively from a single dataset by performing a series of conditional independence tests. Recently, there has been some work on handling more general cases, such as multiple datasets with missing-by-design values, observa-tional and interventional data. A recent overview of such methods can be found in [23]. Apart from methods that try to identify a complete causal model, there also exist some query-based methods that only identify features of the causal model [25, 14, 24]. However, there does not exist any method that is able to handle all possible cases we consider. The methods closest to our approach are the methods by Hyttinen et al. [14] (HHEJ2013 hereafter), Hyttinen et al. [13] (HEJ2014 hereafter), and COmbINE [24] following the logic-based approach to causal discovery. HHEJ2013 does not perform any conflict resolution and thus cannot be ap-plied in practice since statistical tests almost always contain some conflicts. COmbINE does not handle selection, prior knowledge, or soft interventions, but has introduced sev-eral key ideas and is the precursor to ETIO. Unlike ETIO, HEJ2014 is able to also handle cyclic linear models. It uses a Bayesian method [16] to compute the probability of depen-dence or independence, and then tries to find a causal graph that maximizes the product of probabilities of all satisfied constraints which has a high computational cost. There are two major differences with ETIO: the conflict resolution strategy and the way constraints are encoded in logic.
In this section we will present a possible scenario for us-ing ETIO for causal discovery for a car insurance company. The scenario introduces and demonstrates in sequence the features of the algorithm, the range of scenarios it can han-dle, and the type of inferences it makes. In all cases ETIO was executed with an oracle for the (in)dependence condi-tional tests. transformed to propositional logic rules, depending on the number of input variables, datasets and conditioning sets. prior knowledge.
 The insurance network : We assume a true causal model in the form of a Bayesian network that generates the data. The true network is of course unknown to the algorithm. We employ a simplified version of the insurance network [3] that was created by a human expert in the field. The ground-truth network is shown in Figure 1 (a). The nodes correspond to attributes of the customers, their cars, as well as well as how much each customer may cost the company, measured by the medical, liability and property cost in case of an accident. Companies that collect related data have the goal of identifying causal factors of the cost nodes, in order to reduce cost. An example of such a factor is whether or not a car has an airbag (Airbag node) which causally affects MedicalCost. Knowing this, the company may either increase the price for customers that do not have an airbag installed, or may try to persuade customers to install an airbag by promising a reduced price.
 Latent variables : Let us assume that a company measures most of the nodes in Figure 1 (a) for each client in their data. Unfortunately, most likely some of the latent confounders are bound not to be measured. In this case, whether or not a driver will have an accident while being a customer (the Accident node) cannot be measured as it is not known in ad-vance. Other nodes may be omitted from measuring because they did not seem as important at the time of designing the database. In Figure 1 (a) we will assume that Ruggedness and Accident are latent shown in a light font. Because these nodes are common causes of two or more measured nodes, they are latent confounders: Bayesian network algorithms will induce the wrong causal relations. In Figure 1, since Ruggedness has not been measured and affects both Cush-ioning and PropertyCost , a dependence is observed between Cushioning and PropertyCost . Such a dependence cannot be broken by conditioning on any set of the measured variables. Thus, any Bayesian network algorithm will (asymptotically) identify the edge Cushioning  X  PropertyCost or Cushion-ing  X  PropertyCost . The direction of the edge will be arbi-trary. However, as there is no causal relation between them, the edge, if causally interpreted, would lead to wrong conclu-sions : improving the cushioning of the car will not affect the property cost in case of an accident; the reverse edge direc-tion makes even less sense. Therefore, if latent confounders are possible, one should use algorithms for learning mod-els that admit them, such as the FCI [21, 28] algorithm for learning maximal ancestral graphs. Figure 1 (d) shows the direct and indirect causal relations any sound and complete procedure such as ETIO would identify, denoted with solid and dashed edges respectively . Indeed, ETIO does not output any causal relation between Cushioning and PropertyCost . However, ETIO is not able to identify any causal relation as direct (not possibly mediated by any other variable). We remind the reader that the figures with ETIO results (i.e., subfigures (d) -(f )) only show the causal relations ETIO is able to prove: missing edges do not imply that they are not possible.
 Incorporating prior knowledge : For some variables it is safe to make certain assumptions based on their semantics. It is reasonable to assume that Age is a root node, and is not caused nor confounded with any of the measured variables. Furthermore, we can also assume that the cost variables are leaf nodes as they appear after an accident and record-ing of all other nodes. Finally, we assume that SeniorTrain causally influences all costs, possibly indirectly. There may be additional such cases, but for the sake of demonstration we only consider the ones above. Figure 1 (e) shows the inferences made by ETIO when these pieces of causal prior knowledge are included. We can see that, using prior knowl-edge increases the amount of inferences made, while also refining them. For instance, note that without using the prior knowledge (see Figure 1 (d)) it was inferred that Age is a causal factor of PropertyCost , but after including prior knowledge it is inferred that Age only indirectly affects it through SeniorTrain , MakeModel and VehicleYear .
 Including data with selection and overlapping node sets : Assume that there has been a retrospective study on antilock systems and how they affect accident rates; the measured node subset is shown in Figure 1 (b). In this study, the samples were selected such that the proportion of different types of antilock systems is equal: for a number of cars with antilock system, an equal number was selected for inclusion in the study. Thus, sample selection is affected by the Antilock variable. Such data are called case-control data, a type of selected data. Because of selection bias the dataset has a different distribution than the one con-tained in the company X  X  database. In addition, notice that this study measures a different (overlapping) set of nodes as the ones available to the insurance company; this is a case of data with missing-by-design values. Because of selection and missing values the pooled data cannot be analyzed using Bayesian network learning methods, even if there were no la-tent confounders. Due to selection bias in the data, spurious dependencies appear. For example, MakeModel would seem to be dependent with VehicleYear , yet there is no direct or indirect causal relation between them. FCI could han-dle selection bias in a single dataset but not in combination with missing-by-design values and prior knowledge. ETIO will analyze the original and the second available datasets together with the following interventional dataset. Including interventional data : As a final example, con-sider the case were the company wants to reduce the medical cost of some of its clients. They decided to run a campaign that promotes the usage of cushioning. Since not all clients will respond to the campaign, this intervention is a soft in-tervention. On the network this is modeled as an additional indicator variable I Cushioning that has an edge into Cush-ioning (see Figure 1 (c)). After the campaign, the company continues to gather additional data which can then be em-ployed in combination with the previous two datasets and available prior knowledge. The results of the analysis on the three datasets by ETIO that include observational, selected, and interventional data measuring different node sets and including prior knowledge are shown in Figure 1 (f). Note that, including additional data further refines the inferences made compared to Figure 1 (e). For example, the previ-ously inferred causal relations between Age and SeniorTrain , MakeModel and VehicleYear are now found to be direct (in the context of the measured variables), denoted with solid lines. Furthermore, the algorithm also makes the non-trivial inference that Age is a direct cause of MedicalCost .
Although not shown in the figures, the algorithm is also able to make several non-trivial inferences between variables that were never measured together in any of the datasets: (a) Ruggedness is not directly connected (no direct causal relation and not confounded) with neither of CarValue , Med-icalCost and LiabilityCost , (b) Ruggedness does not causally influence CarValue and LiabilityCost and is not causally af-fected by LiabilityCost , and (c) Accident and Carvalue are not causally related.
We evaluated ETIO and compared it with HEJ2014 [13] and FCI [28] on simulated data.

Data Generation: We generated acyclic networks with latent confounders. Direct edges and confounders were in-cluded in the network independently with probabilities P d and P c respectively. We used a linear parameterization and coefficients were sampled uniformly at random from the range  X  [0 . 2 , 0 . 8], following [13]; that is, each node is a linear function of its parents plus an error term, all having param-eters in the aforementioned range. We performed two sets of simulations: in the first simulation we generated 100 net-works with 6 nodes, P d = 0 . 2 and P c = 0 . 1, and generated one dataset with 500 samples from each network. Similarly, in the second simulation we generated 100 networks with 8 nodes, P d = 0 . 15 and P c = 0 . 05, and generated three datasets from each network, with sample sizes of 100, 500 and 1000. In order to compare all algorithms we used only a single observational dataset.

Algorithms: We used the implementation of HEJ2014 by Hyttinen et al. [13]. For ETIO we used both the MPR [24] and Bayesian method [16] used by HEJ2014 to rank constraints. The Bayesian method accepts a prior p , which measures the prior probability of independence. We used the values { 0 . 1 , 0 . 2 ,..., 0 . 8 , 0 . 9 } for p . For both, HEJ2014 and ETIO we ran all possible conditional independence tests. We implemented the complete version of FCI [28] without orientation rules for selection. FCI uses a threshold  X  on the p-value of an independence test to decide dependence or independence. For  X  we used the values { 0.001, 0.003, 0.005, 0.007, 0.01, 0.03, 0.05, 0.07, 0.1 } . As a conditional independence test we used the partial correlation test. As an answer set solver we used Clingo [11] (version 4.5.4) for both HEJ2014 and ETIO. Finally, we set a time limit of 20 minutes for each problem instance.

Comparison: To compare the learning accuracy of all al-gorithms, we followed Hyttinen et al. [13] and compared all dependencies and independencies represented by the output of HEJ2014, ETIO and FCI. The dependencies and inde-pendencies can be read-off the output of HEJ2014 (a SMCM for acyclic graphs with latent variables) and FCI using the Bayes-Ball algorithm. ETIO does not output a causal graph by default, but given the right set of queries a SMCM can be returned, at least in this case where all conditional in-dependence tests are performed. As performance measures we used the true positive rate (TPR) and false positive rate (FPR), using dependencies as positives. Furthermore, we measured the running time of Clingo for HEJ2014 and ETIO only, in order to compare the efficiency of the different en-codings and conflict resolution strategies. The running time of FCI was not measured, as FCI is much faster than both logic-based algorithms, especially for larger networks. Solving Time: Figure 2 (a) shows the solving time for HEJ2014 and ETIO on 100 datasets with 500 samples from 100 networks with 6 nodes. The x-axis shows the percentage of solved instances, sorted by time in ascending order, while the y-axis shows the running time in seconds. This means that for a specific value X on the x-axis, X % of the instances have been solved each in time less than or equal to the cor-responding value on the y-axis. For example, in Figure 2 (a) around 90 % of all instances have been solved by HEJ2014 in less than 10 seconds each. We see that ETIO takes about the same time (2-3 seconds) for all instances, regardless of the weighting scheme used. HEJ2014 on the other hand, al-though slightly faster for some instances, takes much longer for the harder instances. This shows that the encoding and conflict resolution strategy of ETIO are more efficient than the one of HEJ2014 .

Figure 2 (b) only shows the running time of ETIO on 300 datasets, 100 for each sample size (100, 500 and 1000), generated from 100 networks with 8 nodes. HEJ2014 was not run on those data as it took too long to complete. We see that the running times are very similar across sample sizes, although there are a few cases with 500 and 1000 samples that ran slower. This can be attributed to the fact that more samples lead to lower p-values (that is, more dependencies), which may slow down the algorithm. In general, the more independencies are identified the faster ETIO runs.
Quality of Results: Figures 2 (c) and (d) show the TPR and FPR for different parameter values (prior and thresh-old). Although not shown in the figures, smaller priors for the Bayesian scoring method and larger thresholds for FCI correspond to points with higher TPRs. The results of the first simulation (Figure 2 (c)) show that HEJ2014 outper-forms ETIO and FCI, at least in this experimental setting. Specifically, all algorithms give comparable results in terms of TPR, but HEJ2014 consistently has about 5% less FPR than both, ETIO and FCI. This difference in performance between HEJ2014 and ETIO is due to the different conflict resolution strategy used. Recall that ETIO uses a greedy strategy, whereas HEJ2014 identifies the optimal subset of constraints which maximizes the product of weights assigned to each constraint. ETIO with Bayesian probabilities per-forms similarly to FCI. Using the MPR method, ETIO achieves the highest TPR, but the FPR also increases in contrast to the Bayesian method. In the second simulation (Figure 2 (d)) ETIO perform similarly to FCI in most settings. Note that, the comparison favors FCI, as multiple thresholds were used, while ETIO uses the MPR method which does not have any hyperparameters. In practice it is not known in advance which is the best threshold. For example, common thresholds such as 0.01 and 0.05 (the 5th and 7th largest TPRS in the figures) perform similarly, if not worse, than ETIO with MPR. Thus, ETIO with MPR performs at least as well as FCI, without using any hyperparameters.
We propose the ETIO algorithm for causal discovery from multiple heterogeneous datasets, where latent confounders, selection bias, or interventions may have occurred. ETIO can also handle missing-by-design data and incorporate cau-sal prior knowledge. Compared to the state-of-the-art algo-rithm that can also handle multiple datasets it is computa-tional more efficient. ETIO is an instance of the logic-based approach to integrative causal discovery demonstrating its potential for applications to business data. It also points to interesting new directions for future research to increase the scalability and learning performance of this type of methods.
We would like to thank the anonymous reviewers for their comments. This work was funded by the ERC Consolidator Grant No 617393 CAUSALPATH.
 [1] N. Angelopoulos and J. Cussens. Bayesian learning of [2] E. Bareinboim, J. Tian, and J. Pearl. Recovering from [3] J. Binder, D. Koller, S. Russell, and K. Kanazawa. [4] G. Borboudakis and I. Tsamardinos. Incorporating [5] T. Claassen and T. Heskes. A Bayesian approach to [6] G. F. Cooper. A Bayesian method for causal modeling [7] C. P. de Campos, Z. Zeng, and Q. Ji. Structure [8] M. D X  X razio, M. D. Zio, and M. Scanu. Statistical [9] F. Eberhardt. Causation and Intervention . PhD [10] F. Eberhardt. Direct causes and the trouble with soft [11] M. Gebser, R. Kaminski, B. Kaufmann, M. Ostrowski, [12] M. Gebser, R. Kaminski, P. Obermeier, and [13] A. Hyttinen, F. Eberhardt, and M. J  X  arvisalo. [14] A. Hyttinen, P. Hoyer, F. Eberhardt, and [15] K. B. Korb, L. R. Hope, A. E. Nicholson, and [16] D. Margaritis and S. Thrun. A Bayesian [17] R. E. Neapolitan. Learning Bayesian Networks . [18] R. T. O X  X onnell, A. E. Nicholson, B. Han, K. B. [19] J. Pearl. Causality, Models, Reasoning, and Inference . [20] R. D. Shachter. Bayes-ball: Rational pastime (for [21] P. Spirtes, C. Glymour, and R. Scheines. Causation, [22] J. Tian and J. Pearl. On the identification of causal [23] R. E. Tillman and F. Eberhardt. Learning causal [24] S. Triantafillou and I. Tsamardinos. Constraint-based [25] S. Triantafillou, I. Tsamardinos, and I. Tollis. [26] I. Tsamardinos, L. Brown, and C. Aliferis. The [27] I. Tsamardinos, S. Triantafillou, and V. Lagani. [28] J. Zhang. On the completeness of orientation rules for
