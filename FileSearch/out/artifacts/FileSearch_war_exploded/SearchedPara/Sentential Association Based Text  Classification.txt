 Text classification (TC, also called text categorization ) is to realize the task of assign-beled natural language text documents based on their contents. TC has become more Many methods have been proposed for TC, including Na X ve Bayes, decision trees, k-NN, support vector machines (SVM) and association rule based (or simply associa-tion based) methods [2-5]. 
An association rule for TC is indeed similar to an IF-THEN rule which is manually defined by knowledge engineers in the knowledge engineering (KE) method, which is the most popular real-world approach to TC in the 1980s. Naturally association based TC methods have inherent readability and refinability of acquired classification rules. To the best of our knowledge, all the current association based TC methods mainly exploit document-level co-occurring words, which are a group of words co-occurring document is considered as a transaction [1], i.e., a set of items. Frequent words ( item-sets ) are then mined from such transactions to capture document semantics and gener-ate IF-THEN rules accordingly. Usually some heuristic such as database coverage [3,4,5,9] is used to select the most significant itemsets. 
However, the basic semantic unit in a document is actually a sentence. Words co-occurring in the same sentence are usually associated in one way or the other, and are more meaningful than the same group of words spanning several sentences in a document. For example, when word wheat occurs in the first sentence of a document consisting of 100 natural sentences, the co-occurrence of word farm in the same sen-tence appears to be more meaningful than its co-occurrence in the 28th sentence. 
According to above observations, we view sentences rather than documents as the basic semantic units and have presented a novel association based TC method called SAT-MOD ( X  X AT X  means Sentence as Association Transaction and  X  X OD X  means the following MODFIT heuristic) [6]. Based on SAT-MOD, we have developed a prototype system called SAT-Class for text classification. The left parts are organized as follows: Section 2 briefly reviews the basic ideas of SAT-MOD. Section 3 describes the system framework and core components of SAT-Class. We give demonstration scenario in Section 4. In our daily life, usually people are liable to emphasize some core ideas by repeating some representative words in different sentences, thus frequently repeated words tend to represent a facet of the whole  X  document subject  X  of a document. Those representa-tive words are captured by Document Freq uent Itemsets (abbr. DFIs) in our SAT-MOD method. A DFI is a group of words co-occurring in a minimum number of sentences in a document. With each word as an item, and each natural sentence as a transaction, we can use frequent itemsets mining algorithm such as Apriori [1, 10] to mine DFIs in a document, and represent each training document as a set of DFIs. 
DFIs are then used to generate Category Frequent Itemsets (abbr. CFIs). A cate-simultaneously document frequent in a user-specified minimum number of documents in that category. All the CFIs are collected using a category prefix-tree , and the tree is then pruned by our novel heuristic called MODFIT. Intuitively, the MODFIT heuris-tic chooses word groups with moderate number of co-occurring words, which equals to moderately extending a single word with other words along a natural sentence. Using MODFIT pruning, more synonymies are kept to form the classification rules. In addition, we need not apply the step of removing the covered documents, and hence the MODFIT pruning is less expensive. The pruned tree is finally taken as the category classifier. Figure 1 is just an illustration of a category prefix-tree. Our text classification system SAT-Class is implemented by three-tiers architecture, adopted. In the application server tier, web server  X  X omcat X  is leveraged to provide the process, GenClassifier, Classify, and Maintain . 
The details of Preprocess module are described as figure 3. Firstly, input docu-ments are tokenized. In this tokenization procedure, characters such as numbers and html labels are removed, and the boundary symbols of words and sentences are ex-tracted. If the documents are written in orient language, the tokenizer also does word segmentation by exploiting an orient language dictionary. Then, sentence segmenta-tion, stop word removal, stemming, and encoding are performed sequentially. Only for training documents, association rule mining algorithm implemented by Dr. Bor-nally, both training documents represented by DFIs and unlabeled documents repre-sented by sentences are stored in text DB. 
After the preprocessing, the GenClassifier module constructs classifiers by invok-ing the MODFIT pruning procedure. During the classifying procedure performed by the Classify module, the similarity scores of an unlabeled document with respect to all the predefined categories are first computed, an d then that document is assigned to its target categories accordingly. In each category, for the sake of easy browsing, all its subordinate documents are kept in a descendant order of rank scores. Each document similarity score to the maximum one realized among all the subordinate documents in the category. In the Maintain module, three functions are implemented as follows:  X  Reading and modifying rules of classifiers character or words instead of numeric coding. User can also manually refine the rules of classifiers by operations such as  X  X dding X ,  X  X eleting X  or  X  X pdating X . Those func-tions can be exploited to integrate domain experience into our system.  X  Saving and loading classifiers The procedure of classifier training is time-consuming, especially when there are too many training documents and/or too many categories. Thus in our SAT-Class system, XML files. We can deploy our SAT-Class in another application server by just load-ing the classifiers previously stored in the XML files, rather than do another training from the very scratch again.  X  Adding and deleting class node of taxonomy During the adding and deleting operations, only the operated node and corresponding tion of moving class node from one branch to another is not provided yet, for we can accomplish that function by combining functions of saving and loading classifiers. and the Enron [8], and also one Chinese dataset will be exhibited. The attendees will be invited to experience the system. They can browse the rule sets of taxonomy nodes, and modify the rules by adding, deleting and updating operation. They also has the freedom to perform maintenance operation such as creating, deleting a node. 
