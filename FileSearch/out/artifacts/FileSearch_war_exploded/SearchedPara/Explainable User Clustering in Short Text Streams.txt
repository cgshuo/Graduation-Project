 User clustering has been studied from different angles: behavior-based, to identify similar browsing or search patterns, and content-based, to identify shared interests. Once user clusters have been found, they can be used for recommendation and personalization. So far, content-based user clustering has mostly focused on static sets of relatively long documents. Given the dynamic nature of social media, there is a need to dynamically cluster users in the context of short text streams. User clustering in this setting is more challenging than in the case of long documents as it is dif-ficult to capture the users X  dynamic topic distributions in sparse data settings. To address this problem, we propose a dynamic user clustering topic model (or UCT for short). UCT adaptively tracks changes of each user X  X  time-varying topic distribution based both on the short texts the user posts during a given time period and on the previously estimated distribution. To infer changes, we pro-pose a Gibbs sampling algorithm where a set of word-pairs from each user is constructed for sampling. The clustering results are explainable and human-understandable, in contrast to many other clustering algorithms. For evaluation purposes, we work with a dataset consisting of users and tweets from each user. Experimen-tal results demonstrate the effectiveness of our proposed clustering model compared to state-of-the-art baselines.
 User clustering; Short text processing; User topic modeling
With the rising popularity of social media, hundreds of millions of active users are sharing short texts on microblogging platforms such as Twitter 1 and Sina Weibo. 2 A good understanding of users X  dynamic preferences is important for the design of applications that cater for users of such platforms, such as personalized microblog search, twitter summarization, and computational advertising. In this paper, we study the problem of user clustering in the context of short text streams , where users are taken to be people who post messages on a microblogging platform. Our goal is to infer users X  topic distributions over time and dynamically cluster users based on their topic distributions in such a way that users in the same clus-ter share similar interests while users in different clusters differ in their interests. In addition, we aim at making the clustering results explainable and understandable.

Previous work on user clustering [5, 18, 24] mainly clusters users who exhibit similar patterns when accessing information such as clicked documents. For instance, the user clustering method pro-posed by Mobasher et al. [24] constructs vector matrices for URLs and users and then utilizes K-means [16] to cluster users based on browsing vectors. These methods are designed to work with collec-tions of static, long documents and they often make the assumption that users X  interests do not change over time. Unlike previous work, we focus on clustering users at a certain point in time, in the context of streams of short documents.

Accordingly, we propose a dynamic multinomial Dirichlet mix-ture user clustering topic model, UCT for short, to tackle the prob-lem of dynamic user clustering in short text streams. Traditional topic models such as probabilistic latent semantic indexing (PLSI) [12], latent Dirichlet allocation (LDA) [3], author topic models [31, 37] or the user interest topic model [21], have been widely used to uncover topics of documents and users. These topic models do not work well in the context of short text streams due to the problem of sparsity. How to utilize topic distributions for user clustering is still an open problem.

Inspired by previous work [5 X 7, 19, 20, 22, 28, 29], we extract word-pairs in each tweet and form a word-pair set for each user to explicitly capture word co-occurrence patterns. That is, UCT infers each user X  X  interests with hidden topics while topics are cap-tured from the word-pair set of the users. In addition, to track the dynamics of a user X  X  interests, UCT infers a user X  X  current interests by integrating the interests at previous time periods with newly ob-served data in text streams. It then utilizes users X  current interests for clustering. Thus, the result of user clustering is time-varying and users in the same cluster share similar interests at the current time, although their interests may differ at previous times. To the best of knowledge, we are the first to perform dynamic user clus-tering in short text streams based on the distributions of users X  in-terests during a given time period.

Our main research questions are whether UCT outperforms state-of-the-art user clustering methods, and whether our clustering re-sults are explainable and understandable in contrast to results of other methods. We conduct our experiments on a Twitter dataset and demonstrate the effectiveness of our UCT clustering model.
The main contributions of our work are: (1) We propose the task of dynamically clustering users in the con-(2) We propose a dynamic multinomial Dirichlet mixture user clus-(3) We propose a collapsed Gibbs sampling algorithm for the in-(4) Our proposed clustering model can effectively cluster previ-(5) We provide a thorough analysis of UCT and of the impact of
Two major types of research relate to our work: user clustering and text clustering, and topic modeling.
State-of-the-art research on user clustering mainly focuses on web user clustering [4, 18, 24, 32]. These papers study users X  ac-cess information from logged server data including query and click data and then uncovers clusters of these users that exhibit similar information needs. For instance, Buscher et al. [4] cluster users based on user interaction information, including clicks, scrolls and cursor movements for search queries on long text documents. An-other line of work, which mostly focuses on content-based simi-larity, has grouped users by expertise [1, 11]; recent advances in distributed representation learning have given rise to new types of joint topic and entity representations [34] but, so far, these have not been used for user clustering yet. To the best of our knowledge, existing content-based user clustering algorithms work with long documents and do not consider clustering users in the context of short text streams such as Twitter or Weibo. In this paper, we aim at dynamically clustering users in short text streams.

Another relevant line of work is text clustering. Yu et al. [40] and Huang et al. [13] propose a Dirichlet process mixture with feature selection model (DPMFS) and a Dirichlet process mixture with feature partition model (DPMFP) for normal document clus-tering, respectively. They compare DPMFP with four other cluster-ing models: EM text classification (EM-TC) [25], K-means [16], LDA [3] and exponential-family approximation of the Dirichlet compound multinomial distribution (EDCM) [8]; they find that DPMFP performs best. In the context of short text documents, Ran-grej et al. [27] compare three clustering algorithms including K-means, Singular Value Decomposition and Affinity Propagation [9] on a small set of tweets and find that Affinity Propagation outper-forms the other two, but the complexity of Affinity Propagation is quadratic in the number of documents. Tsur et al. [33], Yin [38], and Yu et al. [40] focus on the problem of online clustering of a stream of tweets. They all use an incremental clustering frame-work that first groups a number of tweets into clusters, then assigns the newly arriving tweets to these clusters. Yin and Wang [39] introduce a collapsed Gibbs sampling algorithm for the Dirichlet multinomial mixture model for short text clustering in a static set of short documents. They do not model documents with a distribu-tion of topics. Instead, they assign a single topic to each document, then cluster the documents based on the topic assignments. All of these algorithms aim at clustering short documents X  X he problem of dynamically clustering users in the context of short text streams has so far been ignored, however.
Probabilistic topic models, such as PLSI [12] and LDA [3], aim to analyze latent topics of documents. Various LDA-type topic models have been proposed. The author topic model [31] has been proposed to uncover latent topics of authors; each author is associ-ated with a multinomial distribution over topics and each topic is associated with a multinomial distribution over words. This sug-gests a method for clustering users in short text streams: model users as distributions over topics inferred from their tweets and then cluster users based on their topic distributions.

Various dynamic topic models have been proposed to track chan-ges of topics in streams. The dynamic topic model (DTM) [2] ana-lyzes the time evolution of topics in document collections, in which a document is assumed to have one timestamp. Since DTM uses a Gaussian distribution for the dynamics, the inference is intractable because of the non-conjugacy of the Gaussian and multinomial dis-tributions. The dynamic mixture model (DMM) [36] considers a single dynamic sequence of documents, which corresponds to a sin-gle topic over time. The topic tracking model (TTM) [15] focuses on tracking time-varying consumer behavior, in which consumers X  interests change over time. The topic over time model (ToT) [35] assumes that each topic is associated with a continuous distribution over timestamps, and the topic distribution of a document is influ-enced by both word co-occurrences and the document X  X  timestamp. All of these models assume that the context of the documents is rich enough to infer a topic distribution for the documents, which may not work well for documents in short text streams.

Exploiting external knowledge to enrich the representation of short texts has been proposed to improve the performance of topic modeling for short texts. Phan et al. [26] train latent topics from large external resources. Jin et al. [17] learn hidden topics on short texts via transfer learning from auxiliary long text data. Ren et al. [30] apply a document expansion method that consists of entity linking and sentence extraction. Chen and Liu [6] retain the re-sults learned in the past and using them to help future learning. Cheng et al. [7] extract bi-terms in each tweet to capture word co-occurrence explicitly for enhancing the performance of short text topic modeling. Again, unlike UCT, these algorithms aim at work-ing with a static collection of documents only.
 We work with short text streams and propose a dynamic Dirichlet multinomial mixture user clustering topic model, by which we cap-ture a multinomial distribution of topics specific to each user over time in Twitter and then dynamically cluster users based on their dynamic topic distributions. To enhance the performance of the in-ference in our proposed Gibbs sampling for our topic model, we extract word-pairs in tweets and form a word-pair set for each user to explicitly capture word co-occurrence patterns. To the best of our knowledge, this is the first attempt to use a topic model to infer clusters of users in the context of short text streams.
In this section, we introduce the main notations and task to be addressed in this paper.
Table 1 summarizes our main notation. Term u  X  U t indicates a user, while U t = { u 1 ,u 2 ,...,u m } is a set of users at time period t  X  X  ...,T  X  1 ,T } with T being the most recent time period, and the length of each time period t can be a week, a month, a quarter, half a year, a year etc. Also, z is a topic and K is the number of topics we infer for our UCT model; w is a word in a tweet and b represents a word-pair extracted from a tweet.
 We extract a set of word-pairs B t,u for each user u from their pub-lished tweets D t,u at time period t , and we aggregate all users X  word-pair sets as B t . We use B t as input to monitor each user X  X  interest in the UCT model. The parameters  X  t and  X  t are Dirich-let priors for our topic model at time t . z t,u,b , m t,u,z which are used in the topic model training process, represent the topic assignment on word-pair b for user u , the number of word-pairs published by user u assigned to topic z , and the number times of w is assigned to topic z , respectively.  X  t,u is a cluster to which user u belongs at time t , and the cluster  X  t,u can be changed over time as we assume the user X  X  interest  X  t,u is time-varying.
The task we address is to dynamically track clusters of users over time in the context of short text streams such that users in the same cluster share similar interests. Specifically, for each time period t , given a set of users U t = { u 1 ,u 2 ,...,u m } at time t and a short text stream D t up to t , we focus on uncovering the clusters of users in U t , with  X  t,u being the cluster to which user u belongs at t .
We start by providing an overview of our method in  X 4.1. We then detail each of our three main steps: preprocessing in  X 4.2, the user clustering topic model in  X 4.3, and user clustering in  X 4.4.
We use Twitter as our default setting of short text streams and provide a general scenario of our method for dynamically cluster-ing users in tweet streams in Algorithm 1. We assume that each user X  X  interest is represented by topics, and each user X  X  interests may drift over time. Formally, given a time period t  X  { ...,T  X  1 ,T } , the interest of each user u  X  U t is represented as a multino-mial distribution  X  t,u over topics. The distribution  X  t,u from our proposed dynamic user topic model. Because documents in short text streams are short and sparse, we propose a preprocess-ing step to extract word-pairs (see step 1 in Algorithm 1), where a word-pair contains two words sharing the same topic. We en-rich the context by considering co-occurring words in word-pairs instead of documents.

Algorithm 1: Overview of the algorithm for user clustering in short text streams.
 Input : A set of users U t along with their published tweets
Output : cluster of each user  X  t,u
Extract a set of word-pairs B t,u for each user u , see  X 4.2
Use UCT model to track each user X  X  interests as  X  t,u , see  X 4.3
Cluster users based on their interests distribution  X  t,u
Next, we propose a dynamic Dirichlet multinomial mixture user clustering topic model (UCT) to capture each user X  X  dynamic inter-ests  X  t,u , at time slice t , in the context of short text streams (see step 2 in Algorithm 1). Each user X  X  interests  X  t,u is computed after sampling process has finished.

Based on the multinomial distribution  X  t,u , we explore the clus-ter of users using K-means clustering [16] (see step 3 in Algo-rithm 1). With the time period t moving forward, the result of clustering users changes dynamically.
Traditional topic models [3, 15, 35] detect topics from a docu-ment based on word co-occurrences in documents. The topics are represented as groups of correlated words, while the correlation is revealed by word co-occurrence patterns in documents. In this pa-per, we do not use the words in tweets (our documents) to directly infer topics for users due to the limited length of tweets.
In order to tackle the lack of context in modeling users X  interests, we explicitly consider word correlations via co-occurring words in a word-pair instead of a whole tweet, where a word-pair is a set of two order-exchangeable words being assigned to the same topic. Specifically, after removing stop words and apply Porter stemming, we obtain each user X  X  tweet set D t,u (the tweets user u published at the current time period t ). Following [6, 7], we regard each tweet as an individual context unit, in which word-pairs in a tweet share the same topic. Then, the method to extract word-pairs from each tweet d  X  D t,u is as follows.
 Each word-pair b  X  B d contains two different words ( w i tweet d . For example, from the tweet  X  X ananas and apples are all fruit X  we extract three word-pairs, i.e.,  X  X anana apple X ,  X  X anana fruit X  and  X  X pple fruit X  after removing stop words and stemming. Then, we aggregate all word-pairs extracted from tweets D user u : Thus, for each user u , the set D t,u of their published tweets at time period t is processed to a set of word-pairs B t,u .

We sample the topic assignment z for each word-pair instead of each independent word. In other words, the word correlation constructed to infer topics does not rely on the co-occurrence in tweets but in word-pairs. The next subsection shows how to use the word-pair set B t,u to model the user X  X  interests.
In this subsection, we detail our dynamic multinomial Dirichlet mixture user clustering topic model (UCT). UCT captures a user X  X  interests at time t as  X  t,u , that is, as a distribution over mixtures of topics. We use t  X  X  ...,T  X  1 ,T } to represent a time period, the length of which can be a day, week, month, quarter or even year. Figur e 1: Graphical representation of our dynamic user clus-tering topic model, UCT. Shaded nodes represent observed variables.

Fig. 1 shows a graphical representation of our UCT model, where shaded and unshaded nodes indicate observed and latent variables, respectively. At t , we sample word-pairs ( w i ,w j )  X  B u  X  U t based on the current topic-word distributions  X  t current users X  interests  X  t . From Fig. 1, we see that a dependency is assumed to exist between two adjacent time periods.

We track the dynamics of a user X  X  interests based on the assump-tion that a user X  X  interests during the current time period t are the same as those during the preceding time period t  X  1 unless inter-ests are changed by newly observed data at t . We model a user X  X  interests by combining their topic distribution obtained during the previous time slice t  X  1 as prior knowledge and newly arriving ob-served data for inferring current model for the user X  X  interests. In particular, we use both the user X  X  previous interests  X  t  X  1 ,u rent Dirichlet prior  X  t as a new Dirichlet prior. The K -dimensional variable  X  t,u = {  X  t,u,z } K z =1 has the following probability density: where  X ( x ) is a Gamma function. In contrast with static topic mod-els [3, 31], the Dirichlet prior changes from  X  to  X  t  X  1 ,u the added term  X  t  X  1 ,u represents the influence of previously in-ferred interests.

To model the dynamics of topics over words, we infer topic-word distributions  X  t,z = {  X  t,z,v } V t v =1 at current time period t by using the following Dirichlet distribution: The topic-words distributions are considered to be inferred through priors  X  t  X  1 ,z +  X  t . We estimate  X  t and  X  t for each time pe-riod instead of using simple symmetric priors. Given all users X  word-pairs set B t = S u  X  U pairs specific to user u , from Fig. 1 we know that topic z is re-lated with a distribution of words with the multinomial distribution  X  t,z = {  X  t,z,w | w  X  V t } . In UCT, the multinomial distribution specific to the user u is used to select a topic, thereafter a word in a word-pair is generated according to the distribution  X  t,z that chosen topic z . According to the graphical model, we sample each topic z t,u,b for each word-pair b  X  B t,u . The distributions  X  t  X  1 ,  X  t  X  1 at the previous time period and the priors  X  lized for inferring the current distribution  X  t and  X  t . The generative process is as follows: ii. For each user u , draw a multinomial distribution  X  t,u We sample word-pairs instead of words as shown in the above gen-erative process. Then, the probability of generating a word-pair b = ( w i ,w j ) given a topic z at t is represented as: and the probability of generating a word-pair b at t is represented as: Inference is intractable in this model. Following [3, 10, 15, 36, 39], we propose a collapsed Gibbs sampling method to perform approximate inference. We adopt a conjugate prior (Dirichlet) for the multinomial distributions, and thus we easily integrate out  X  and  X  t , analytically capturing the uncertainty associated with them. In this way we facilitate the sampling, i.e., we need not sample  X  and  X  t at all.

The proposed collapsed Gibbs sampling algorithm for the UCT model is shown in Algorithm 2 (recall that our main notation is shown in Table 1). The input of our Gibbs sampling algorithm is B t (all users X  word-pairs sets at time slice t ) and the output consists of all users X  interests distributions over topics at current time t . For the initialization of our Gibbs sampling, we randomly assign a topic z = z t,u,b to each word-pair b  X  B t,u and update m t,u,z n t,z,w (to be defined below) accordingly.

In the Gibbs sampling procedure above at time slice t , we need to calculate the conditional distribution where Z t,  X  ( u,b ) represents all topics assignments except the cur-rent word pair b from user u . We begin with the joint probability of the current word-pair set B t , the topic assignments Z user set U t given the previous distributions  X  t  X  1 and  X  two Dirichlet priors  X  t and  X  t . Using the chain rule, we obtain the conditional probability conveniently as follows: Algorithm 2: Gibbs Sampling for UCT
Output : multinomial parameter  X  t and  X  t for each user u  X  U t do 3 for each word-pair b  X  B t,u do 4 sample a topic z randomly: 7 while word-pair b contains two words w i and w j
Sampling Phase for iter = 1 ,...,N do 12 for each user u  X  U t do 13 for each word-pair b  X  B t,u do 14 record the current topic, z = z t,u,b 16 while word-pair b contains two words w i and w j 18 draw z b from P ( z t,u,b = z | compute the parameters  X  t and  X  t using equation (8). where m t,u,z represents the number of word-pairs published by user u and assigned to topic z , and n t,z,w represents the number of times word w is assigned to topic z . The two Dirichlet priors  X  and  X  t are estimated by maximizing the joint distribution in the sampling at each iteration. We use the following updating rules in fixed-point iterations for obtaining these two Dirichlet pri-ors, where  X ( x ) =  X  log  X ( x ) x is a Digamma function, and Once the Gibbs sampling has been done, with the fact that a Dirich-let distribution is conjugate to a multinomial distribution, we then conveniently infer the following distributions for  X  t and  X  spectively: Clustering previously seen users. After we have determined each user X  X  u  X  U t dynamic topic distribution at time t ,  X  t,u by (8)), we use K-means [16] to compute the clusters of these users based on each user X  X  topic distribution  X  t,u . Obviously, as time progresses, the clusters of these users dynamically change. Clustering previously unseen users. However, in some cases, we do not have users X  interests  X  t,u new for new arriving users u U . We then infer each new user X  X  interests from their published tweets at time period t , where tweets are preprocessed into a word-pair set B t,u new as discussed in  X 4.2. We compute the probability of the user u new being interested in topic z at t , i.e.,  X  where P ( z | t,b ) is computed as: where P ( w | t,z ) is the probability of word w associated with t . We obtain P ( z | t ) for (10) as: where we use n t ( z,w ) and n t ( w ) to denote the total number of words assigned to topic z and the total number of words at time t , respectively.
 Then we estimate P ( b | t,u new ) in (9) as: where n t,u new ( b ) is the frequency of word-pair b in B
Finally, after applying (10), (11) and (12) to (9), we obtain the new user X  X  interests  X  t,u new . We then group this user into a cluster  X  t,u where they share most interests with other users in the cluster: and update the user set U t as U t  X  U t  X  X  u new } .
In this section, we describe our experimental setup;  X 5.1 lists our research questions;  X 5.2 describes our dataset;  X 5.3 and  X 5.4 list the baselines and metrics for evaluation, respectively;
We list the research questions that guide the remainder of the paper: RQ1 Does our dynamic user clustering method UCT outperform RQ2 What is the impact of the different time slices in our dynamic RQ3 What is the quality of the topical representation inferred by RQ4 Can we capture the dynamics of users X  interests and make
In order to answer our research questions, we work with a dataset collected from Twitter. The data set contains 1,375 active users and their tweets that were posted from the beginning of their registra-tion up to May 31, 2015. In total, we have 3.78 million tweets with each tweet having its own timestamp. The average length of the tweets is 12 words. Due to the crawling restrictions imposed by Twitter, we cannot obtain the follower-followee relationships for each user. So we ignore the possibility of using users X  relationships to improve the performance; we leave this as part of our future work.

We use this dataset as our short text streams and manually judge the clusters of the 1,375 users based on the content of their pub-lished tweets. We obtain ground truth clusters for 5 different par-titions of time periods, i.e., a week, a month, a quarter, half a year and a year. In the ground truth clusters for time periods of a week, the users are manually clustered through their published tweets dur-ing a week, resulting in 48 to 60 clusters. We also create ground truth for times periods of a month, a quarter, half a year and a year, with the number of clusters varying from 43 to 52, 40 to 46, 28 to 30 and 28 to 30, respectively.

For pre-processing, we remove stop words and apply Porter stem-ming using the Lemur toolkit. 3
We compare our proposed method UCT with the following base-lines and state-of-the-art clustering strategies in our experiments: K-means. This is a traditional clustering algorithm [16]. It rep-GSDMM. This is a Dirichlet multinomial mixture model-based Latent Dirichlet Allocation (LDA). This model infers topic dis-Author Topic Model (AuthorT). This model [31] infers topic dis-Dynamic topic model (DTM). This model [2] utilizes a Gaussian Topic over time model (ToT). This model [35] normalizes times-Topic tracking model (TTM). This model [15] captures the dy-For the LDA, DTM, ToT and TTM baselines, we use the averaged topic distribution of all the documents a user posted before gener-ated by LDA, DTM, ToT and TTM, respectively, to represent this user, and cluster users based on their topic distribution similarities. For static topic models, i.e., LDA and AuthorT, we set  X  = 0 . 1 and  X  = 0 . 01 . We set the number of topics K = 50 and the number of clusters equal to the number of topics. Given the number of clusters P and the number of output clusters Q in the ground truth, we set C = { c 1 ,...,c j ,...,c P ground-truth clusters and  X  = {  X  1 ,..., X  i ,..., X  Q } as the set of output clusters at time slice t , respectively. We use the following metrics to evaluate our experimental results, all of which are widely used in the literature [7, 39].
 Precision . At time slice t , each output cluster  X  i is assigned to a ground-truth cluster c j iff the intersection of the two clusters  X  owns the largest number of users. In case of a draw, we randomly assign the output cluster  X  i to one of the ground-truth clusters that call the draw, as the random assignment does not result in different evaluation performance. Then the precision of this assignment is measured by counting the number of user-pairs in the intersection correctly assigned and divided by the total number of user-pairs in the output cluster  X  i : a given set  X  i  X  c j and  X  i , respectively. Obviously, a higher preci-sion indicates better user clustering performance.
 Normalized Mutual Information (NMI) [23]. NMI is a measure that allows us to make the trade-off between the quality of the clus-tering and the number of clusters. It is an entropy-based metric that explicitly measures the amount of statistical information shared by the variables representing the output clusters and the ground truth clusters of users. Let I ( X ; C ) denotes the mutual information of the output cluster set  X  and the ground-truth cluster set C . NMI avoids the value biasing to large number of clusters by using entropy of  X  and C , i.e., E ( X ) and E ( C ) :
NMI ( C ,  X ) = I ( X ; C ) [ E ( X ) + E ( C )] / 2 where n is the total number of users. Note that when C is equal to  X  , NMI reaches 1, its maximum value. Larger NMI value indicate better clustering performance.
 Adjusted Rand Index (ARI) [14, 23]. Consider clustering users based on a series of pair-wise decisions. If two users both in the same cluster are aggregated into the same cluster and two users in different classes are aggregated into different clusters, the decision is considered to be correct. The Rand index shows the percentage of decisions that are correct while the adjusted Rand index is the corrected-for-chance version of the Rand index [14]. The maxi-mum value is 1 for exact match; larger values mean better perfor-mance for clustering. ARI( C ,  X  ) is computed as follows, where n is the total number of users.
 The three metrics introduced so far are for evaluating the perfor-mance of user clustering, whereas the following metric is for eval-uating the quality of topic representations of users in clusters. H-score [7]. As our UCT model builds on topic modeling, we consider to evaluate the quality of the topic representation of each user using the H-score [7] metric, which is computed as: where the average intra-cluster distance IntraDis ( C ) and average inter-cluster distance InterDis ( C ) are computed as: where dis ( u i ,u j ) is the symmetric Kullback-Leibler distance of topic distributions of user u i and user u j . The intuition behind the H-score is that if the average inter-cluster distance is small com-pared to the average intra-cluster distance, the topical representa-tion of users reaches good performance.

We report the Precison, NMI and ARI scores of all eight methods listed above to evaluate clustering performance. Importantly, to evaluate the quality of topical representations, we report H-scores of all baseline methods except GSDMM. We cannot compute H-scores for GSDMM as it assumes each document to be a single topic; GSDMM clusters users based on topic assignments, not topic distributions. We evaluate the performance with the above metrics at each time period, and report the mean of the evaluation results.
In the following subsections we report on our experimental out-comes and formulate answers to our research questions.
To begin, we address research question RQ1 . We evaluate the performance of our UCT model in the context of short text streams, and compare UCT with a traditional clustering method, K-means, GSDMM, which integrates a state-of-the-art clustering topic model for short documents in a static set, and three dynamic user cluster-ing models, DTM, ToT and TTM (see  X 5.3). The training data we use for these eight models are all tweets published from the year 2013 to 2014, which we divide into two parts, each part contain-ing tweets published during a year. We report the precision, NMI and ARI values of the eight methods by averaging the performance across the two parts.

Fig. 2 shows the performance of UCT and the baselines in terms of Precision, ARI and NMI. First, we see that UCT performs signif-icantly better than K-means, GSDMM and the five topic models on all the metrics, which demonstrates the effectiveness of our model for user clustering. UCT and the other 5 topic models outperform K-means, which attests to the merit of utilizing topic models for user clustering. UCT and GSDMM, which infer topic distributions and infer single topic assignments for short documents, outperform all other baselines in most cases and on all three evaluation metrics. This finding demonstrates that considering documents representing users as short texts rather than as long documents during inference help to improve the performance on user clustering. UCT, ToT, TTM and DTM, which infer topic distributions for documents in streams, outperform AuthorT and LDA, which infer topic distribu-tions in static sets of documents. This finding demonstrates that inferring dynamic topic distributions of documents in the context of streams can help to enhance the performance of user clustering over considering documents as a set of static ones for the inference. Table 2: The average number of tweets a user published during a week, a month, a quarter, half a year, a year. Plus the average number of word-pairs extracted for each user. #tweets 16 111 220 418 744 #word-pairs 1012 3586 9199 14348 29810 UCT significantly outperforms all baselines, and this finding con-firms that the way UCT infers dynamic topic distributions of short documents in streams improve the performance of user clustering.
Next, we address research question RQ2 . To understand the in-fluence on UCT of the length of the time period that we use for evaluation, we compare the performance for different time periods: a week, a month, a quarter, half a year and a year, respectively. Fig. 3 shows the evaluation results in terms of Precision, ARI and NMI for time periods of different lengths; we average the scores over periods of six weeks, six months, six quarters, four semi-years and two years, respectively.
 UCT always outperforms LDA, AuthorT, DTM, TTM, ToT, GS-DMM for time periods of all lengths. This finding, again, confirms the fact that UCT, which infers topic distributions of short docu-ments based on the previous distribution and arriving documents, works better than the state-of-the-art algorithms for user cluster-ing in streams. When the length of the time period increases from a week to a month, both UCT and the baseline methods all ob-tain a big improvement, but UCT continues to outperform the other methods. Although the performance of UCT seems to level off on all three metrics when the length of the time period increases from a quarter to a year, it still significantly outperforms the baselines. These findings demonstrate that UCT X  X  performance on the user clustering task is robust in the context of short document streams, and is able to maintain significant improvements over state-of-the-art algorithms.

To further understand why UCT and the baseline methods in-crease their performance when the length of the time period use for evaluation increases, we provide an analysis of word co-occurrence patterns in different time periods. Statistics of the number of tweets users published in different time periods are shown in Table 2. On average, a user only publishes 16 tweets per week, which indicates that there are 16  X  12 word co-occurrence patterns if we assume the average length of each tweet to be 12 words. The number 16  X  12 is not comparable with the number 1012, which is the number of word-pairs. A larger number of word-pairs help to better infer topic distributions in Gibbs sampling. The longer the time period, the more word-pairs can be utilized in our Gibbs sampling for the topic inference.
We now address research question RQ3 . To assess the quality of topics extracted by UCT, we compare UCT and the baseline meth-ods. Fig. 4 shows the comparison of the performance of UCT and the baselines in terms of H-score. When compute the H-scores for evaluating the quality of topical representation in UCT and the baselines, we use the quarterly ground-truth user clustering results. It is clear from Fig. 4 that UCT obtains a significantly smaller H-score compared to all other six models, 4 which indicates that the average inter-cluster distance is small compared to the aver-assumes each document to be in only a single topic Precision, ARI and NMI, respectively.
 year. The performance is evaluated using Precision, ARI and NMI, respectively. Figure 4: Evaluation results for the quality of topic representa-tions for UCT and the baselines, using the H-score metric and time periods of a quarter. age intra-cluster distance. A smaller H-score means that the topi-cal representation of users is more similar to that labeled manually (each cluster in the ground-truth clusters of users has lower aver-age intra-cluster distance and higher inter-cluster distance), which demonstrates a better quality of the topics represented by UCT in contrast to state-of-the-art clustering models.

To further illustrate the quality of topic representations in UCT, we display the top-N words for an output cluster and two users from this cluster. The top-N words of a user are generated as fol-lows. First, we rank the words in decreasing order of the probabil-ity P ( w | t,z )  X  P ( z | t,u ) , i.e.,  X  t,z,w  X   X  t,u,z the user; the words ranked within the top-N are then selected to represent the user. For generating the top-N words for a cluster, the words are ranked by the probabilities associated with the clus-ter, which is computed by 1 | c | P u  X  c P ( w | t,z ) P ( z | t,u ) , i.e.,
P u  X  c  X  t,z,w  X   X  t,u,z . Then, the top-N words that obtain the highest probabilities are selected to represent the cluster. Table 3 shows the top 15 words extracted from a cluster and two users in this cluster. We use ToT as a representative topic model for com-parison as it is the best baseline (GSDMM cannot obtain represen-tative words for users X  interests and clustering results). We see that the two users in the same cluster share more similar interests like  X  X ids, immigration, community, education. X  UCT is able to obtain representative words for a cluster more accurately than ToT. This again, the explainable and human-understandable clustering results further illustrates that the quality of UCT X  X  topic representation is better than that of the baseline methods.
Finally, we address research question RQ4 . As UCT captures each user X  X  dynamic topic distribution, we investigate the content of the users X  interests. We conduct a qualitative analysis and see if the clustering result is explainable. As an example, we randomly choose two users and show their interests over five quarters. Specif-ically, we show each user X  X  interests at each time period by using the top 15 words in Table 4, where the words are selected from the 10 most probable topics of the user and then the 20 most probable words for each topic.

As seen in Table 4, the first user is concerned with  X  X ook, promo-tion, prototyping, ios, etc. X  in the second quarter of 2014 and this is slightly changed to  X  X pp, store, browser, dialog, design, etc. X ,  X  X ro-totype, android, web, internet, etc. X  in the following two quarters, respectively. As time moves on in 2015, their interests change to  X  X ussia, government, designer, app, problem and etc. X  and  X  X roto-media kid toast immigration dog campaign alert Mexico fan advertisement bet slide John image people kid education internal photo fan media ad community game score rio toast process http process kid process soccer child media people dog Mexico reason score song education robbery fan star type, apple, ios, music, mac etc. X . The user X  X  interests are almost stable and mainly focus on the design of apps. In contrast, during the second quarter in 2014, the second user is interested in  X  X enter, partner, WalMart, game, player, Oklahoma X  that are about busi-ness, politics and some sports. Then they talk more about college football and feminism and equality with words like  X  X XST, star, game, campus, feminism, equality and etc. X  in the third quarter of 2014. In the next quarter, this user mostly enjoys college foot-ball as represented by words  X  X SPN, TXST, star, bowl, game etc. X  Then this user is concerned with politics and society with  X  X XST, state, feminism, government, university X  and  X  X iolence, victim, re-sponsibility X  in 2015. This example illustrates how UCT captures dynamic topic distributions to represent the interests of each user and that the result of dynamic clustering is explainable and under-standable in the context of short text streams. We have proposed a content-based method for user clustering. Previous work on content-based user clustering has mostly focused on long documents. In contrast, we have studied the problem of dynamically clustering users in the context of streams of short doc-uments. We have proposed a dynamic Dirichlet multinomial mix-ture user clustering topic model, UCT, to dynamically cluster both previously seen and previously unseen users based on their inter-ests. To better infer the dynamic topic distribution specific to each user, we have proposed to extract word-pairs from each user and apply a Gibbs sampling algorithm for the inference.
 For evaluation purposes, we have compared the performance of UCT to that of a traditional clustering algorithm, K-means, non-dynamic topic models, LDA, the author topic model, and state-of-the-art dynamic topic models, viz. DTM, ToT and TTM. Our ex-perimental results demonstrate the clustering effectiveness of our model for user clustering in the context of short document streams. We have also found that UCT produces higher quality topic repre-sentations than competing methods, and it comes with the benefit of offering explanations of the clustering.

As to future work, we aim to incorporate other information such as users X  social relations to collaboratively group users into clus-ters. Further research that we are keen to do concerns an evaluation of the similarity of topics, which can be used for automatic selec-tion of K . Another line of work is to develop a more efficient user clustering model to utilize previously captured topic distribu-tions of users for inferring a user X  X  current interests, and to improve efficiency of the Gibbs sampling algorithm as the process is time-consuming.
 [1] K. Balog and M. de Rijke. Finding similar experts. In SIGIR , [2] D. M. Blei and J. D. Lafferty. Dynamic topic models. In [3] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet [4] G. Buscher, R. W. White, S. Dumais, and J. Huang.
 [5] W. Chen, J. Wang, Y. Zhang, H. Yan, and X. Li. User based [6] Z. Chen and B. Liu. Mining topics in documents: standing [7] X. Cheng, X. Yan, Y. Lan, and J. Guo. A biterm topic model [8] C. Elkan. Clustering documents with an exponential-family [9] B. J. Frey and D. Dueck. Clustering by passing messages [10] T. L. Griffiths and M. Steyvers. Finding scientific topics. [11] K. Hofmann, K. Balog, T. Bogers, and M. de Rijke.
 [12] T. Hofmann. Probabilistic latent semantic indexing. In promotion book email battle html prototyping tweet feature ios team coding perspective lane motorway car center partner WalMart
TXST mall David belt offer game player im-provement blue enforce-ment county Oklahoma [13] R. Huang, G. Yu, Z. Wang, J. Zhang, and L. Shi. Dirichlet [14] L. Hubert and P. Arabie. Comparing partitions. J.
 [15] T. Iwata, S. Watanabe, T. Yamada, and N. Ueda. Topic [16] A. K. Jain. Data clustering: 50 years beyond k-means. [17] O. Jin, N. N. Liu, K. Zhao, Y. Yu, and Q. Yang. Transferring [18] I. Li, Y. Tian, Q. Yang, and K. Wang. Classification pruning [19] S. Liang and M. de Rijke. Burst-aware data fusion for [20] S. Liang, Z. Ren, and M. de Rijke. Fusion helps [21] S. Liang, Z. Ren, and M. de Rijke. Personalized search result [22] S. Liang, Z. Ren, W. Weerkamp, E. Meij, and M. de Rijke. [23] C. D. Manning, P. Raghavan, and H. Sch X tze. Introduction to [24] B. Mobasher, R. Cooley, and J. Srivastava. Creating adaptive [25] K. Nigam, A. K. McCallum, S. Thrun, and T. Mitchell. Text [26] X.-H. Phan, L.-M. Nguyen, and S. Horiguchi. Learning to [27] A. Rangrej, S. Kulkarni, and A. V. Tendulkar. Comparative [28] Z. Ren and M. de Rijke. Summarizing contrastive themes via [29] Z. Ren, S. Liang, and M. de Rijke. Personalized time-aware [30] Z. Ren, M.-H. Peetz, S. Liang, W. van Dolen, and [31] M. Rosen-Zvi, T. Griffiths, M. Steyvers, and P. Smyth. The [32] J. Srivastava, R. Cooley, M. Deshpande, and P.-N. Tan. Web [33] O. Tsur, A. Littman, and A. Rappoport. Efficient clustering [34] C. Van Gysel, M. de Rijke, and M. Worring. Unsupervised, [35] X. Wang and A. McCallum. Topics over time: a non-markov [36] X. Wei, J. Sun, and X. Wang. Dynamic mixture models for [37] S. Xu, Q. Shi, X. Qiao, et al. A dynamic users X  interest [38] J. Yin. Clustering microtext streams for event identification. [39] J. Yin and J. Wang. A dirichlet multinomial mixture [40] G. Yu, R. Huang, and Z. Wang. Document clustering via
