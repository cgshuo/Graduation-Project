 The Web is rapidly transforming from a pure document collection to the largest connected public data space. Semantic annotations of web pages make it notably easier to extract and reuse data and are increasingly used by both search engines and social media sites to provide better search experiences through rich snippets, faceted search, task completion, etc. In our work, we study the novel prob-lem of crawling structured data embedded inside HTML pages. We describe Anthelion , the first focused crawler addressing this task. We propose new methods of focused crawling specifically designed for collecting data-rich pages with greater efficiency. In particular, we propose a novel combination of online learning and bandit-based explore/exploit approaches to predict data-rich web pages based on the context of the page as well as using feedback from the extraction of metadata from previously seen pages. We show that these techniques significantly outperform state-of-the-art approaches for focused crawling, measured as the ratio of relevant pages and non-relevant pages collected within a given budget. H.3.3 [ INFORMATION STORAGE AND RETRIEVAL ]: Infor-mation Search and Retrieval  X  Selection process , Relevance feed-back ; I.2.6 [ ARTIFICIAL INTELLIGENCE ]: Learning
The adoption of markup languages for structured data has reached considerable levels in recent years due to the increasing support by large consumers and publishers of web content. In early 2010, Facebook announced the so-called Open Graph Protocol (OGP) for marking up the header of HTML pages with simple structured data such as the type of the page and a thumbnail image. Later in 2010, the major web search engines  X  Bing, Google, Yahoo, followed by Yandex  X  have joined forces to provide a common vocabulary for more complex markup, the so-called schema.org vocabulary, which allows complex descriptions of the most common types of objects appearing in web pages (videos, reviews, recipes, addresses, per-sonal profiles, product descriptions, etc.) As of today, large search engines are exploiting this markup for providing richer snippets, vertical search experiences such as Google X  X  Recipe Search well as support for task completion such as checking in to a flight directly from the search results page (e.g. Yandex Islands media sites such as Facebook, Twitter and Pinterest primarily use the structured data extracted from web pages for richer displays of the content that is being shared.

Recent data from Google shows that over 15% of web pages are using schema.org markup, containing over 25 billion descriptions of objects [13] and the number of pages with OGP markup is stip-ulated to be even larger [4]. Large search engines collect this data during their regular crawls of the Web, while social media sites ex-tract it at the time an item is first shared. These commercial web collections are not publicly available however, despite the multi-tude of potential applications of these data outside of web search and social media. For example, small e-commerce sites can bene-fit from additional information about the products they are selling, such as specifications from the manufacturer X  X  website, ratings and reviews from social media, alternative offers from other vendors, etc. Entirely new vertical search engines can be built by having comprehensive, up-to-date information from across the Web about all items of a given type. At the same time, current web crawler implementations and publicly available web collections such as the CommonCrawl 3 data are not targeted at maximizing the amount and value of the structured data in their collections.

In this paper, we introduce the first focused crawler specifically aimed at crawling structured data. As the field of crawlers is well-established, we build on existing methods to the largest extent pos-sible, but design our crawler to maximize the value of the data collected as opposed to maximizing the number of pages crawled. In particular we devise and implement new crawling policies that learn the characteristics of data rich pages and web sites, and steer the crawler toward them. To achieve this, we propose a novel com-bination of online classification and a bandit-based page selection. The former approach overcomes the problem of absence of a-priori knowledge about whether a newly discovered page contains struc-tured data or not. With the latter approach we address the problem of exploitation versus exploration , allowing the crawler to perform random walks and fetch pages potentially better than those already discovered.

This paper thus makes the following contributions: 1. To the best of our knowledge, we are first to introduce the 2. We show that state-of-the-art online classification approaches 3. We introduce a new approach to focused crawling using a 4. We combine online classification and a bandit-based selec-5. We demonstrate that our method can also be adapted in order 6. We make the implementation of our crawler publicly avail-
The paper is structured as follows: In the following section we give a brief introduction into web crawling and focused crawlers. We also discuss related work in the areas of online learning and bandit-based selection. In Section 3, we describe our methodol-ogy and the corresponding implementation. We then outline our experiments and we present the outcomes in Section 5. Lastly, we describe future ideas and open challenges.
The purpose of web crawling is to gather a collection of use-ful web pages as quickly and efficiently as possible, while provid-ing at least the required features for respecting the limitations im-posed by publishers ( politeness ) and avoiding traps ( robustness ). Dhenakarn and Sambanthan [10] provide a brief overview about the four policies whose combination is influencing the behavior of a web crawler. We inherit from an existing crawler implementation to define the policies for re-visits , parallelization , and politeness . We will mainly focus on implementing a novel selection policy , i.e. determining the order in which new URLs are discovered and processed. The selection policy of web crawlers typically use vari-ations of the PageRank [26] algorithm with the aim to collect the most popular pages within the Web, as they are also more likely to be searched for. Even though by definition all crawlers aim to build a collection useful for a given purpose, focused crawlers as described in the literature target pages relevant to a particular topic. Focused crawlers were first mentioned by Menczer [22] who mod-eled the problem inspired by work on agents adapting to different environments. Later, Chakrabarti et al. coined the term focused crawler and introduced an approach using a pre-trained classifier to assign topic-labels to new URLs based on features which could be extracted from the URL itself [7]. Other classification features have been obtained using different NLP techniques [16, 17, 19, 29]. Furthermore, Diligent et al. used information collected using web search engines in order to gather additional features for classifica-tion [11]. Aggarwal et al. incorporated information gathered dur-ing crawling to steer the direction of the crawler and maximize the number of retrieved relevant pages [1]. They use features extracted from the content of the father of the page (i.e. the page where we found the link), retrieving tokens from unseen URL strings and fea-tures collected from sibling pages (i.e. whose URLs were discov-ered in the same page as the one to be crawled). After crawling a page, the probability of the different feature groups for a given topic is evaluated and the combined probability is used to update the pri-orities of unseen pages. Although this model makes use of features gathered during the crawling process, the probabilistic model needs to be manually adjusted beforehand, which Chakrabati et al. try to overcome when first introducing an online classification approach for focused crawling [6]. Chakrabarti et al. crafted two classifiers, one static, pre-trained from an upfront collected and tagged cor-pus, and one online, which was used to improve former decisions based on features extracted from the document object model, e.g. the anchor text in links of crawled pages. Four years later, Bar-bosa and Freire took on the main idea of incorporating information gathered during crawling to steer the crawler with an extended fea-ture set [2]. Besides the context of the page where a URL was found, they made use of the graph-structure of web pages, for ex-ample by distinguishing between direct features retrieved from the father and the siblings of the page, which was later also used by Zheng et al. [33]. Although they incorporate information gathered during crawling, they only replace their classifier with an updated version in batches, solely employing newly gathered information and discarding formerly extracted information. Their results in-dicate that sequentially updated classifiers lead to higher rates of gathering web forms for certain topical domains. Umbrich et al. proposed a pattern-based approach to classify pages, in order to find specific media types in the Web [32]. Jiang et al. [15] used a similar method to learn URL patterns that lead to relevant pages in web forums.

The main difference from this work with respect to mainstream focused crawling is that we are not aiming to perform topic-based classification, but rather looking at the value of web pages from the perspective of the data they contain. Web pages serving structured data have unique characteristics; structured markup is more com-mon to particular types of pages, e.g. item detail pages, and favored by particular web sites, typically large dynamically generated sites serving certain types of content. Our target is also distinct from that of native semantic web crawlers that collect documents in RDF document formats, which follow seeAlso and sameAs references to related data items in order to discover new linked data sources and information. Two examples are Slug and LDSpider [12, 14]. These crawlers deal with the specific issues related to RDF data on the Web such as support for various native RDF formats, supporting various communication protocols etc. In contrast, our work fo-cuses on structured data embedded inside HTML pages which has recently grown into a more popular way of exposing data on the Web. Recent studies have shown the increasing availability and di-versity of data exposed this way [4, 24], offering a broad publicly available data source with large potential for various applications.
State-of-the-art focussed crawlers partially make use of informa-tion gathered during crawling which is incorporated into the classi-fication process in order to improve the accuracy of the prediction for unseen pages. In contrast to the aforementioned works, we pro-pose an online learning method that continuously obtains feedback during crawling and incorporates it directly in an online classifier, rather than replacing the classifier from time to time. Such methods have been used before whenever data is available as stream [34] and the distribution of features within the data change over time [25]. Our underlying classification model will make use of all avail-able feedback which can be successfully exploited for crawling for structured data, independently from its topic, and gather the largest number of relevant pages constrained to a given fetch budget.
Existing crawling policies implemented in the systems above fo-cus largely on maximizing the immediate reward available to the crawler and lack in the discovery of new pages which potentially lead to more other relevant pages, but do not contain relevant in-formation directly [9]. This problem can be described as the trade-off between exploitation , the crawling of pages where the expected value can be predicted with a high confidence and exploration , the search for new sources of relevant pages [21, 18, 28]. We address the issue of trading-off exploitation versus exploration by translat-ing the problem of crawling into a bandit problem. We group newly discovered, not yet crawled pages by their corresponding host, each representing one bandit. During each iteration, where we want to select a new page to be crawled, we either select a page from a bandit, whose expected gain for a given objective function is max-imal (exploitation) or select a page from a randomly chosen bandit (exploration). This approach was analyzed using synthetic data by Pandey et al. [27] and successfully applied by Li et al. in the con-text of news article recommendation [20]. To our knowledge, its value for focused crawling has not been established before.
In the following, we are presenting the two general approaches to machine learning (online classification and bandit-based selection) that we adapt to the domain of focused crawling, and in particular to the task of collecting structured data from web pages.
Crawling pages that embody markup data can be cast as a fo-cused crawling task, as their general aim is to devise an algorithm to gather as quickly as possible web pages relevant for a given objec-tive function. Standard focused crawling approaches target pages that include information about a given topic, like sports, politics, events and so on. In our case, our primary objective function are pages which make use of specific markup standards, although there could be variants that narrow down this subset (see Section 5.3).
Focused crawlers make use of topic specific seeds and operate by training a classifier that is able to predict whether a newly dis-covered web page (before downloading and parsing its content) is relevant for the given target or not. Thus, it is mandatory to as-semble a training set, find suitable topic seeds and learn a classifier before the crawling commences.

On the other hand, online learning approaches adapt the underly-ing model used for classification on the fly with new labeled exam-ples. In the case of a crawler this would be suitable provided that it is possible to automatically acquire a label for a web page as soon as the content of the crawled page has been parsed. This approach is appealing because not only it is not necessary to create a training set in advanced but also the classifier adapts itself over time. In the case of the Web, where the distribution of single features is hard to predict it might happen that, while discovering larger amounts of pages the actual distribution differs strongly from the one of the training set. This adaptability is useful to ensure suitable classifi-cation results [25].

In order to predict the relevance of an unseen newly discovered page it is necessary to extract features which the classifier can take into account to make its prediction. We considered three major sources of features which are (partly) available for a web page be-fore downloading and parsing it: 1. the URL , which can be handled using natural language pro-2. information coming from the parents of a page , whose con-3. information coming from the siblings of a page , meaning
We note that these sources of features may become gradually available during the crawling process. We will always know the URL of candidate pages, but we might not have discovered every parent of a page; furthermore, information about siblings could not be available at all.

There are several possibilities to extract features from the URL of a page. In general the URL is split into tokens whenever there is a non alphanumeric character (punctuation, slashes and so on) and these tokens can be directly used as features of the page. In order to reduce the sparseness of the generated feature vectors, and potentially improve the accuracy of the classifier, it is possible to apply several pre-processing steps for the extracted tokens before finally transforming them into features. Among standard transfor-mations, for example, we include removing tokens consisting of too few or too many characters. Another technique consists in mapping different spellings of a given token into its normalized version, or replacing tokens only made up by numbers with one static token representing any number.

Importantly, crawlers may not be aware of the range of differ-ent tokens (i.e., the dictionary) that can be extracted from the URL of newly discovered pages, which makes it difficult to use a pre-defined feature space for online learning. We overcome this prob-lem by relying on the so-called Hash-Trick [30] and map all tokens into a fixed feature space.

This approach receives a list of pre-processed URL tokens pre-viously split, { t } and it creates a feature vector V with length k for the new page. First, it initializes every component with 0 values. Then, it maps each token t within the list to x t  X  [0 .. ( k  X  1)] using the hash-function described in Equation 1, where n is the number of characters of t , k is the number of selected hashes and t [ i ] is the numeric value of the character at position i . 5 The corresponding position within the feature vector will then be updated V ( x
This way we can ensure that the number of features remains the same during the whole crawling process. Although the known drawback of hash-functions is the potential information loss when-ever a collision happens, the described approach achieved good re-sults in our case, when hashing tokens from URLs.
 We also extract features from the parents and siblings of a page. These features are based on labels assigned to parent/sibling pages previously. For example, we introduce as a feature the number of parents/siblings labeled with the target class, and a binary feature representing the existence of at least one parent or sibling labeled with the target class.

The selection of features and classification algorithm will have a major influence on the final page selection performance. In order
The numerator is equal to the hashCode -function implemented for string objects in Java.
Table 1: Results of feature and classification pre-experiments to determine the most adequate combination of features, classifier and parameter configuration (number of hashes, classifier depen-dent settings, etc.), we ran a number of experiments on an indepen-dent development set. The dataset we compiled for these experi-ments was independently crawled and labels were acquired apply-ing the method used in Section 4.2. The dataset consisted of 100 K pages from over 1 K different hosts with a balanced distribution of labels (same number of pages containing structured data and pages without any structured data). We randomly selected one page after the other, first letting the classifier predict the label and then train-ing it with the real label. 6 We repeated this process ten times for each different configuration and measured both the overall accu-racy of the classifier and the running time needed for classification and training of the whole dataset.

We experimented with two different online classification algo-rithms, namely Naive Bayes (also used by [6]) and Hoeffding Trees [34], and used three different major feature sets: (a) only tokens from the URL, (b) features from parents and (c) the combination of both. We engineered those features using a variety of configurations, for in-stance filtering by token length and replacing number tokens by the constant string [NUMBER] . Finally, we evaluated the performance of the classifiers with the tokens hashed into different number of features, ranging from 5 K to 20 K .

Table 1 outlines the results of some of these experiments. We report the best performing configuration for each combination and omit the remaining results due to space limitations. In every case, ignoring tokens shorter than three characters and a replacement of numbers by a constant string worked the best. Hoeffding Trees (HT) performed overall better in comparison to Naive Bayes (NB) but needed up to 10 -times more time to finish processing the whole dataset. In addition, we noticed that using 10 K hashes produces the best results. Finally, it is worth remarking that adding sibling information into the feature set downgraded the performance con-sistently, so we excluded them in subsequent experiments.
In the following Section, we introduce the notion of bandit-based selection and explain how we combine online classification with the bandit-based approach.
A bandit-based selection approach estimates the relevance of a group of items for a given target, and performs this selection based on the expected gain (or relevance) of the groups. The bandit op-erates as follows: at each round t we have a set of actions A (or arms 7 ), and we choose one of them a t  X  X  ; then, we observe a re-ward r a,t , and the goal is to find a policy for selecting actions such as the cumulative reward over time is maximized. The main idea is that the algorithm can improve its arm-selection strategy over time with every new observation. It is important to remark that the algorithm receives no feedback for unchosen arms a 6 = a t
This reflects the real operating mode of a crawler, except that a real crawler might have some delay in when feedback is available. Some bandits use contextual information as well [20].

An ideal bandit would like to maximize the expected reward max a E ( r | a,  X   X  ) , where  X   X  is the true (unknown) parameter. If we just want to maximize the immediate reward (exploitation) we need to choose an action to maximize E ( r | a ) = R E ( r | a,  X  ) p (  X  | D ) d X  , where D is the past set of observations ( a, r a ) . However, in an ex-ploration/exploitation setting we want to randomly select an action a according to its probability of being Bayes-optimal where I is the indicator function. In order to avoid computing this integral it suffices to draw a random parameter  X  at each round t . One of the simplest and most straightforward algorithms is  X  -greedy, where in each trial we first estimate the average payoff of each arm a . Then, with probability 1  X   X  , we choose the one with the highest payoff estimate  X   X  t,a and with probability  X  , we choose a random arm. In the limit, each arm will be tried infinitely often and the estimate  X   X  t,a will converge to the true value  X  a . An adaptation of this straightforward algorithm is the usage of a decaying  X  adaptation faces the problem of coming up with a large number of random selection when the estimated  X   X  t,a is close to the true value  X  . A decaying  X  t approaches 0 faster with each iteration. We will later employ a linear decaying factor,  X  t =  X   X  m t + m , where m is a constant.

In the case of our crawler, we will use our bandit-based approach to make a first selection of the host to be crawled. This is motivated by the observation that the decision to use structured data markup is performed at a host-level in most cases. Informally, we represent each host with a bandit that represents the value of all discovered pages belonging to this host. The available functions to calculate the score for a host and by this the estimated relevance for a target are diverse and described next. It is important to remark that select-ing an arm (action) in this context would mean to select the host, which at a given point in time t has the highest expected value to include pages which are relevant for our target. Once we have se-lected the host, we follow by selecting a page from that host using the online classifier described in the previous Section.
Formally speaking, each host h  X  H t represents one possible arm, which can potentially be selected by the bandit at an iteration t . Each h includes a list of all pages p belonging to this host. An action a t  X  A within our approach is then defined as the selection a host h  X  H t based the estimated parameter  X  h at a given t and  X  . In order to estimate  X  h for an arm, we can think about various different combination of available features. Next we will introduce the general approach and different functions to compute the score s ( h ) using the following notation:
Our general approach is to group all newly discovered pages into the corresponding host. To select a new page, we first use the ban-dit algorithm to identify the host of the page selecting the one with the current highest score or one random (depending on the value of  X  ). From this selected host, we take the page with the highest con-fidence for the target class. This process is depicted in Algorithm 1. Note that the bandit is unable to use single pages as arms , given that we only need to retrieve them once and the feedback loop would be rendered useless. We also classify per-host pages to prioritize them after a host is selected for crawling. (A pure bandit-based approach would select a random page from within the host).

Algorithm 1: Adapted general K-armed Bernoulli  X  -greedy Ban-dit for focused crawling, with a linear decaying factor. We now define several functions to compute the score s ( h ) . Negative Absolute Bad function, where the score of a host is the negative number of already crawled pages not belonging to the tar-get class of this host s ( h ) =  X  X  C bad,h | Best Score function, where the score of a host is defined by the maximal confidence for the target class of one of its containing pages s ( h ) = max p  X  h pred ( p )  X  p  X  R h Success Rate function, where the score of a host is defined by the ratio between the number of pages crawled, belonging to the tar-get class and those not belonging to this class. The ratio is ini-tialized with prior parameters  X  and  X  which we set both to 1 : s ( h ) = ( C good,h +  X  ) / ( C bad,h +  X  ) .
 Thompson Sampling function, where the score of a host is defined as a random number, drawn from a beta-distribution with prior pa-rameters  X  and  X  . This function is based on the K-armed Bernoulli bandit approach introduced by Chapelle et al. [8] and described in algorithm 1. In this case we take as the score at iteration t the ran-dom draw s ( h ) = Beta( C good,h +  X , C bad,h +  X  ) . We initialized the prior with 1 .
 Absolute Good  X  Best Score function, where the score is the prod-uct of the absolute number of already crawled relevant pages: | C and the best score function .
 Thompson Sampling  X  Best Score function, where the score is the product of the thompson sampling function and the best score func-tion .
 Success Rate  X  Best Score function, where the score is the product of the success rate function and the best score function .
Note that the reward depends on the target function of the bandit; in general we assign a positive reward only if the page crawled contains some form of markup data, but the process works similarly for other different objective functions (see Section 5.3).
In this Section we describe the architecture and the process flow implementing the methodology discussed in Section 3. 8 Next, we introduce the dataset employed for our evaluation, and the different experiments performed.
We implemented our solution as part of a full-fledged web crawler, although our component is modular and can be integrated into other existing systems.

As input the application takes a queue of newly discovered, al-ready filtered URLs 9  X  named input queue Q I . The output of the application is another queue where the URLs are ordered by the expected relevance for a given target  X  called ready queue Q URLs coming from Q I are internally grouped by their host h  X  H . Whenever a host h is selected, it is enqueued into the ready host queue Q H . Note that Q H can include the same h multiple times, whereas Q I and Q R consist of a list of unique p pages. Beside this, the application orchestrates several sub-processes:
Figure 1 illustrates the flow throughout our approach. The crawl-ing process starts with a number of initial seed pages (0), which are fed into Q I . Then, P input pulls the first page p from Q fore adding p into the corresponding h , the page is classified by P empty model as no training data (pages) are available so far. When-ever | H | 6 = 0 and  X  h  X  H : R t h &gt; 0 , P bandit selects one host h based on the given s ( h ) and  X  (1). The selected h is inserted into Q H and hosts in Q H are processed by P output . For each host, the URL with the highest confidence for the target class is selected and pushed into Q R (2). The reordered pages are now ready to be handled by other components of the crawler. After downloading (3) and parsing (4) the page, the newly found links are added into Q
I (5). In addition, the label of the crawled pages is returned as feedback to P classifier which updates its classification model (6).
This component is fully distributed in nature, which in practice means that processes operate independently and some of them work faster than others. We optimized all the underlying processes in order to maximize the system throughput, this is, to minimize the probability that Q R gets empty and the crawler has to wait for new
The source code of the crawler is available at http://
By filtering we mean the removal of duplicate and unwanted pages (like certain file extensions like videos, images, etc.).
We use the MOA Java library 2012 . 08 from http://moa.cms. waikato.ac.nz introduced by Bifet et al. [3]. pages. Additionally, we implemented a mechanism to delay the process P bandit whenever the crawler is busy, as it might occur a slight delay in receiving the feedback for the action a t system calculates the score for a t +1 .
In line with the related work, we employ a static dataset for our experiments in order to isolate ourselves from changes in page con-tent and the web graph, as well as other factors such as the avail-ability of web page hosts. All the datasets we use in the follow-ing experiments have been extracted from the publicly accessible dataset provided by the Common Crawl Foundation. This dataset consists of over 3 . 8 billion web documents, where over 3 . 5 billion belong to the type text/html gathered in the first half of 2012 [31]. We used two derived sub-datasets of this original crawl to create the final datasets for our experiments: First, given that we require the structure of the crawled part of the WWW, we use the web graph dataset which was extracted by the WebDataCommons team, and it is described by Meusel et al. [23]. The data consists of 3 . 5 billion URLs with over 128 billion hyperlinks connecting them. 11 this dataset we extracted a subset of around 5 . 5 million web pages which are reachable from one root URL. This was randomly se-lected from the URLs retrieved by crawling the pages of the Open Directory Project . 12 The dataset includes 455 848 different hosts.
Second, given that we also need to know which pages in our extracted subsets is relevant to our objective function, we use the structured data set , also extracted by the team of WebDataCom-mons from the Common Crawl data set. This dataset was cre-ated by parsing the HTML code of the crawled pages for the three markup standards Microdata, Microformats and RDFa using Any23 and includes, among others, the number of embedded structured data for each page [4]. From this dataset we extracted all web pages which are also present in our 5 . 5 million subset containing (a) at least one structured data statement and (b) more than four statements embedded using Microdata. From (a) we acquired 1 . 5 million pages, which comprise 27 . 4% of the whole 5 . 5 million sub-dataset. From (b) we acquired 179 383 pages, which is 3 . 25% of the whole 5 . 5 million sub-dataset.

With the subset and the structured information retrieved in (a) we will run most of the experiments to evaluate our approach for the general task of gathering efficient structured data from the Web. With the subset and the structured information retrieved in (b) we will run a secondary experiment and show that our approach is adaptable to different objectives in the area of structured data crawl-ing.
Our first series of experiments aims to validate that our approach can effectively steer a crawling process toward web pages that em-bed any kind of structured data (Dataset a).
 In the first step we compare our approach to a standard Breadth-First Search (BFS) approach and the typical approach to building focused crawlers for specific topics, i.e. using a static classifier. We run our implementation on the described dataset with a static classifier which we initially trained with 100 K , 250 K and 1 000 K pages, and in comparison we run several crawls that incorporate online classifiers. In a second step we determine which scoring function for the hosts leads to the highest number of crawled pages that are relevant to our target function. We run several crawls incor-porating different scoring functions for the bandit-selection, turn-ing off the greedy component of the algorithm (  X  = 0 ). Likewise, we selected the page with the highest confidence score from the bandit-chosen host. In a next step, we try different static values for  X  to report the influence of the randomness for the best perform-ing scoring functions. Additionally we will show the effect of a decaying  X  t using different decaying factors m .

In a second series of experiments, we change the objective for our crawling function. Therefore, we have defined relevant pages as those that embed structured data within its HTML code regard-less of its kind and quantity. We now narrow down further this definition in order to measure the adaptability of our techniques to different objective functions. We want to reward only pages that embed at least five statements using the markup standard Microdata (Dataset b). Pages using Microdata typically use the schema.org vocabulary and provide more complex descriptions of the infor-mation present in the page. The number of statements is a rough quality criteria in that we filter out pages that provide only mini-mal detail. As an example, a movie page that contains at least five statements might include the facts that: (1) this page describes a movie, (2) the movie has the title Se7en , (3) the movie has a rat-ing of 8 . 7 , (4) was published in 1995 and (5) this information was maintained by imdb.com . Finally, we analyze the runtime of the different scoring functions. This is an important consideration be-cause crawling is essentially a matter of resources, and it might happen that the crawler requires an unacceptably large time bud-get in order to select a new page being crawled. We are aware that this consideration depends on the crawling strategy and the imple-mented policies, which have been optimized consciously.
The main objective of focused crawlers is to maximize the num-ber of relevant pages gathered while minimizing the number of not relevant pages which are downloaded and parsed during the crawl. In order to evaluate the effectiveness of our approach, we use a pre-cision measure that reports on the ratio of retrieved relevant pages to the total number of pages crawled. A page is considered to be relevant when it supports the objective crawling function, this is, whether the page contains structured data or not at all.
In the following we will present the results of the experiments described before. We used the same dataset (a) and the same initial seeds for each experiments for them to be comparable, except for Section 5.3 where we used dataset (b). In addition, as a large num-ber of our experiments depends on sampling  X  especially those that test different bandit functions  X  we repeated each experiments up to five times and reported the average. The curves in the drawings within the result section are calculated using the smoothing spline method. Figure 2: Percentage of relevant fetched pages during crawling comparing batch and online Naive Bayes classification
Static classification has been a dominant method for focused crawling. Our first set of experiments compared the performance of batch with online leaning in our domain of interest. We ran differ-ent crawls using pre-trained classification models learned on a sub-set of 100 K , 250 K and 1 000 K randomly selected pages. Figure 2 shows the number of relevant retrieved pages of static approaches (blue lines). The orange lines show the ratio of relevant pages gath-ered by a crawler equipped with an adaptive online model which was trained completely from scratch during the crawling process. In addition we include the data series (black line) representing a pure breadth-first search approach (BFS).

The performance numbers of static-based classification are slightly higher than the number of BFS. Remarkably, online learning is able to increase notably the amount of relevant pages crawled af-ter 400 K fetches. At the end of the crawl, the adaptive approach is able to collect 539 K relevant pages whereas the best static one (trained with 250 K examples) fails to collect 200 K of those. This trend is similar with Hoeffding Trees, although the difference in performance diminishes when the model is trained with 1 M pages. Still, we note there is a decreasing performance rate for static clas-sification approaches. The online learner also underperforms on the first half of the crawl. This is because the model is empty at the beginning and needs to be trained in subsequent iterations. On the other hand, static models have a slight edge at the beginning which is due to their knowledge advantage.

Figure 3 reports the accuracy over time of the classifiers present in Figure 2. The x-axis shows the number of fetched websites, whereas the y-axis describes the ratio of correctly classified to crawled pages. The saturated accuracy of the static classification approaches ranges between 0 . 55 and 0 . 45 where the adaptive model reaches 0 . 7 in the long run.
We now look into the interplay of bandit algorithms and the dif-ferent functions to calculate the expected value of a host h (pre-sented in Section 3). This first analysis will not include any ran-domness (  X  = 0 ), to observe the real impact of the different setups, and compare them against a random selection, a BFS approach and the a pure online classification based selection ( best score function ).
Figure 4 shows the percentage of relevant pages retrieved during the crawling of one million pages. Firstly, all tested functions lead to higher number of retrieved relevant pages than the a pure random selection (black line) or a BFS (grey line). Furthermore, except for Figure 3: Development of the classification accuracy of batch and online Naive Bayes learning during crawling Figure 4: Percentage of relevant fetched pages during crawling comparing different bandit functions (  X  = 0 ) the Thompson Sampling based selection (TS) all the scoring func-tions outperform online classification on its own (and therefore us-ing static classification approaches). The highest performance rate is achieved by the success rate function , which simply measures the ratio between relevant and non-relevant pages for a host. Here we are able to fetch around 628 K relevant pages out of one mil-lion. The three combinations of best score functions with (a) TS, (b) absolute good and (c) success rate yield the second best results. Regarding the TS based functions, we see a sharp increase in rel-evant pages retrieved at early stages of the crawl. This decreases toward the end of the measurement series ending up gathering be-tween 550 K and 600 K relevant pages. In comparison the other mentioned functions present a positive trend toward the end of the series.

Having identified the best performing scoring functions we now want to focus on the explore / exploit problem. We run the best per-forming bandit-based selection functions, namely absolute good in combination with the best score , the success rate and the combi-nation of success rate and best score and measure the impact of different values  X  . We tested the named functions, using different fixed values of  X  (we report on the best ones) and compared to the corresponding gathering rate without any random selection. We did not consider the TS approach, as it already includes an element of randomness through sampling from a beta-distribution [8].
Figure 5 shows the impact of different  X  values for our three selected functions. We can state that the usage of a random factor in the cases of the best score and the absolute good function fails to increase the number of crawled relevant pages. Regarding the functions that include best score , using a fixed  X  greater than zero reduces the number of relevant pages. Figure 5: Percentage of relevant fetched pages during crawling pages comparing best performing bandit functions with differ-ent  X  values
The above result may suggest that  X  greater than zero may not be beneficial. Figure 6 zooms into the first 400 K crawled pages and shows that there is a positive impact of including a random factor  X  &gt; 0 , lifting the relevant page rate from 0 . 3 to 0 . 4 . However, this effect diminishes when the amount of crawled pages reaches 1 M . Figure 6: Percentage of relevant fetched pages during crawling of first 400 K pages comparing best performing bandit func-tions with different  X  values
The above results support our initial intuition that a decaying lambda may provide the best results overall. We now compare the performance of linear decaying functions for  X  (described in sec-tion 3), with a fixed m = 10 K (value learned on an independent development set). Figure 7 shows the number of crawled relevant pages of the success rate function for the static and decaying  X  s. In addition to the already used  X  = 0 . 2 we also show the results of a larger  X  = 0 . 5 in order to increase the randomness and potentially learn more in the earlier stages of the crawl. Results show a posi-tive impact of a decaying  X  for the percentage of fetched relevant pages, achieving the maximum amount of relevant pages ( 673 K ). The positive effect is especially noticeable with  X  = 0 . 5  X  with no decaying factor, one out of two page selections are random (yield-Figure 7: Percentage of relevant fetched pages during crawling comparing the success function with decaying and static  X  ing the worst results), however when the decaying factor comes into play this negative effect disappears in the long run.
The results in this Section are summarized in Table 2. The re-sults show a 10% improvement of the best performing method for online classification (Naive Bayes) over the best performing result for static classification (HoeffdingTree with 1 000 K training set) and a 26% improvement of the best combined bandit-based ap-proach (Success Rate with decaying  X  = 0 . 5 ) on top of online classification alone.
In this experiment we change the focus of our crawler and reward only pages with at least five statements using microdata.
We compare the BFS approach and the best score function with the best performing configuration from the former section: (1) ab-solute good  X  best score  X  = 0 . 0 , (2) success rate  X  best score  X  = 0 . 1 , (3) success rate  X  = 0 . 2 and (4) success rate with decay-ing  X  t = 0 . 5 and m = 10 K . Figure 8 shows the percentages of fetched relevant pages for the first one million crawled pages.
From the figure we can observe that all tested functions perform remarkably better than the BFS approach. The overall achieved rates are around five times smaller than the rates we reached for the more general objective function. However, now the amount of relevant pages among all the ones in the crawl is around eight times lower ( 0 . 04 vs. 0 . 27 ). In addition, after crawling one mil-lion pages, the bandit functions also outperform the online classi-fication based selection strategy. Like in the previous experiment using a success rate based function tend to gather the highest num-ber of relevant pages, with the success rate function with  X  = 0 . 2 reaching a percentage of relevant crawled pages of 0 . 12 in the first million crawled pages. In comparison, online classification based selection ends up with a ratio of 0 . 08 . Finally, in this experiment a decaying  X  performed comparably to using a fixed  X  value.
In the previous experiments we have shown that the combina-tion of online classification and a bandit-based approach leads to a higher percentage of relevant crawled pages for both tested ob-jectives. We now assess what is the processing overhead incurred by our classification approaches and the current implementation for page selection. This time is critical, as when comparing to a BFS approach, we cannot venture to drop below the average processing Figure 8: Percentage of relevant fetched pages during crawling aiming for pages with at least five Microdata statements. Table 2: Overview of percentage of crawled relevant pages af-ter one million crawled pages time to crawl and parse a page as in this case the crawler process w ould have to wait for our selection.

The theoretical time which is needed to select one page for crawl-ing is mainly influenced by four factors: (1) the number of hosts, as the bandit needs to go through all of them on each iteration, (2) the runtime of the scoring function for the hosts, (3) the selection of the final page from the selected host, which depends on the num-ber of pages per crawl that are ready to crawl (4) the time to add the feedback to the system  X  including training the classifier and updating internal scores. In terms of a random selection (2) and (4) are omitted.

Figure 9 shows the average time in milliseconds for the ban-dit approaches presented before to determine the next page to be crawled. To make results comparable we also include the fully ran-dom selection approach. We can observe, that scoring functions not making use of the Thompson Sampling, where internally a beta-distribution needs to be calculated perform better than a pure ran-dom selection. The average time to selection one page range below Figure 9: Average processing time to select one page over time 5 0 ms for the dataset we used in our experiments. The two func-tions, making use of a beta-distribution need up to 300 ms to select one page. Looking deeper into these functions we noticed that the creation of the beta-functions and the selection of the random value needs over 75% of the whole processing time.

In order to estimate the overhead of including our selection poli-cies into a fully-fledged system one needs full measurements of the standard crawling cycle: establishing a connection, downloading the page, parsing and extracting new links. Taking a broad gen-eral estimate from a existing BFS crawler Ubi-Crawler [5], which needs 800 ms to fully process one page per thread, our selection policy would incur in an overhead of less than 10% , as we need not more that 50 ms for page selection. In comparison to this, we would boost the percentage of crawled relevant pages by factor three.
This paper introduces Anthelion -the first focused crawler tar-geting web pages containing markup standards for embedding struc-tured data. Anthelion combines a bandit-based selection strategy for web pages with online classification to steer a web crawler to-wards relevant pages. The current implementation, which is pub-licly available, is designed to replace the selection policy of exist-ing crawlers. We have shown that the use of online classification, in comparison to static classifiers, can achieve better results in this domain being able to collect over 10% higher numbers of relevant pages for a given objective function. Furthermore, our results show that grouping pages based on their host and making use of features shared by this group empowers the selection strategy for pages and improves considerably the resulting percentage of relevant crawled pages. We demonstrated that a bandit-based selection strategy, in combination with a decaying learning rate (decaying  X  ) overcomes the explore / exploit problem during the crawling process. Our re-sults show that it is possible to increase the percentage of relevant crawled pages in comparison to a pure online classification-based approach by 26 % (see Table 2).

Narrowing the focus of our crawler to web pages using Micro-data to embed richer structured data (where we can extract at least 5 statements) we have shown that our approach can gather 66% more relevant pages within the first million than a pure online classifica-tion based approach. In general, estimating the expected value of an host using the success rate function in combination with always selecting the pages with the highest confidence for the target class tends to lead to the best results. Going beyond precision consider-ations, we have analyzed the runtime performance needed by the current implementation of our approach to select relevant pages for cra wling and showed that we need, in average, 50 ms to select a new page. The results presented in this paper demonstrate that a focused crawler using a bandit-based selection with online classification is capable of effectively gathering web pages embedding structured data. The two techniques we have described allow for potential improvements. Based on the idea of Lie et al. [20] we want to explore the usage of a contextual bandit approach where the re-sults of one bandit can influence the gain of the other bandits. We could also consider extending the classifier to multi-class problems, which would account for graded relevance of crawled pages. We have shown that this approach can be adapted to more fine grained objective functions in this domain, although we want to face further objectives when gathering rich structured data using Microdata. A natural extension of our work would be to take into further con-sideration the quality of the data being crawled, e.g. considering the extent to which the data conforms to the pre-defined schema or to some model of the expected value of attributes. We could also enable our crawler to answer complex queries while crawling. For example, a used car dealer may be interested in only schema.org Offer instances that are about cars and relate to older models of certain brands, with a given price, etc. On the other side, we are also interested if our approach can help to increase the percent-age of relevant pages in the standard focused crawling task, when searching topic related web pages. [1] C. C. Aggarwal, F. Al-Garawi, and P. S. Yu. Intelligent [2] L. Barbosa and J. Freire. An adaptive crawler for locating [3] A. Bifet, G. Holmes, R. Kirkby, and B. Pfahringer. Moa: [4] C. Bizer, K. Eckert, R. Meusel, H. M X hleisen, [5] P. Boldi, B. Codenotti, M. Santini, and S. Vigna.
 [6] S. Chakrabarti, K. Punera, and M. Subramanyam.
 [7] S. Chakrabarti, M. van den Berg, and B. Dom. Focused [8] O. Chapelle and L. Li. An empirical evaluation of thompson [9] A. Dasgupta, A. Ghosh, R. Kumar, C. Olston, S. Pandey, and [10] S. Dhenakaran and K. T. Sambanthan. Web crawler X  X n [11] M. Diligenti, F. Coetzee, S. Lawrence, C. L. Giles, M. Gori, [12] L. Dodds. Slug: A semantic web crawler. Jena User [13] R. V. Guha. Schema.org update. [14] R. Isele, J. Umbrich, C. Bizer, and A. Harth. LDSpider: An [15] J. Jiang, N. Yu, and C.-Y. Lin. Focus: Learning to crawl web [16] M.-Y. Kan. Web page classification without the web page. In [17] M.-Y. Kan and H. O. N. Thi. Fast webpage classification [18] G. Kane and M. Alavi. Information technology and [19] T. Lei, R. Cai, J.-M. Yang, Y. Ke, X. Fan, and L. Z. 0001. A [20] L. Li, W. Chu, J. Langford, and R. E. Schapire. A [21] J. G. March. Exploration and exploitation in organizational [22] F. Menczer. ARACHNID: Adaptive Retrieval Agents [23] R. Meusel, S. Vigna, O. Lehmberg, and C. Bizer. Graph [24] P. Mika and T. Potter. Metadata statistics for a large web [25] J. G. Moreno-Torres, T. Raeder, R. Alaiz-Rodr X guez, N. V. [26] L. Page, S. Brin, R. Motwani, and T. Winograd. The [27] S. Pandey, D. Chakrabarti, and D. Agarwal. Multi-armed [28] G. Pant, P. Srinivasan, F. Menczer, et al. Exploration versus [29] A. Puurula. Scalable text classification with sparse [30] Q. Shi, J. Petterson, G. Dror, J. Langford, A. Smola, and [31] S. Spiegler. Statistcs of the common crawl corpus 2012. [32] J. Umbrich, M. Karnstedt, and A. Harth. Fast and scalable [33] S. Zheng, P. Dmitriev, and C. L. Giles. Graph based crawler [34] I. Zliobaite, A. Bifet, B. Pfahringer, and G. Holmes. Active
