 Brain image analyses have widely relied on univariate voxel-wise analyses, such as voxel-based morphometry (VBM) for structural MRI [1]. In such analyses, brain images are first spatially reg-istered to a common stereotaxic space, and then mass univariate statistical tests are performed in each voxel to detect significant group differences. However, the sensitivity of theses approaches is limited when the differences are spatially complex and involve a combination of different vox-els or brain structures [2]. Recently, there has been a growing interest in support vector machines (SVM) methods [3, 4] to overcome the limits of these univariate analyses. Theses approaches allow capturing complex multivariate relationships in the data and have been successfully applied to the individual classification of a variety of neurological conditions [5, 6, 7, 8]. Moreover, the output of the SVM can also be analyzed to localize spatial patterns of discrimination, for example by drawing the coefficients of the optimal margin hyperplane (OMH)  X  which, in the case of a linear SVM, live in the same space as the MRI data [7, 8]. However, one of the problems with analyzing directly the OMH coefficients is that the corresponding maps are scattered and lack spatial coherence. This makes it difficult to give a meaningful interpretation of the maps, for example to localize the brain regions altered by a given pathology.
 In this paper, we address this issue by proposing a framework to introduce spatial consistency into SVMs by using regularization operators. Section 2 provides some background information on SVMs and regularization operators. We then show that the regularization operator framework provides a flexible approach to model different types of proximity (section 3). Section 4 presents the first type of regularization, which models spatial proximity, i.e. two features are close if they are spatially close. We then present in section 5 a more complex type of constraint, called anatomical proxim-ity. In the latter case, two features are considered close if they belong to the same brain network; for instance two voxels are close if they belong to the same anatomical or functional region or if they are anatomically or functionally connected (based on fMRI networks or white matter tracts). Finally, in section 6, the proposed framework is illustrated on the analysis of MR images using gray matter concentration maps and cortical thickness measures from 30 patients with AD and 30 elderly controls from the ADNI database (www.adni-info.org). In this section, we first describe the neuroimaging data that we consider in this paper. Then, after some background on SVMs and on how to add prior knowledge in SVMs, we describe the frame-work of regularization operators. 2.1 Brain imaging data In this contribution, we consider any feature computed either at each voxel of a 3D brain image or at any vertex of the cortical surface. Typically, for anatomical studies, the features could be tissue concentration maps such as gray matter (GM) or white matter (WM) for the 3D case or cortical thickness maps for the surface case. The proposed methods are also applicable to functional or diffusion weighted MRI. We further assume that 3D images or cortical surfaces were spatially normalized to a common stereotaxic space (e.g. [9]) as in many group studies or classification methods [5, 6, 7, 8, 10].
 Let V be the domain of the 3D images or surfaces. v will denote an element of V (i.e. a voxel or a vertex). Thus, X = R V , together with the canonical dot product will be the input space . Let x s  X  X be the data of a given subject s . In the case of 3D images, x s can be considered in two function defined on a compact subset of R 3 . Both finite and continuous viewpoints will be studied in be viewed either as an element of R d where d denotes the number of vertices or as a real-valued function on a 2-dimensional compact Riemannian manifold.
 2.2 Linear SVM The linear SVM solves the following optimization problem [3, 4, 11]: where  X   X  R + is the regularization parameter and l hinge the hinge loss function defined as: l hinge : u  X  R 7 X  max(0 , 1  X  u ) .
 With a linear SVM, the feature space is the same as the input space . Thus, when the input features are the voxels of a 3D image, each element of w opt = ( w opt v ) v  X  X  also corresponds to a voxel. Similarly, for the surface-based methods, the elements of w opt can be represented on the vertices of the cortical surface. To be anatomically consistent, if v (1)  X  V and v (2)  X  V are close according similar. In other words, if v (1) and v (2) correpond to two neighboring regions, they should have a similar role in the classifier function. However, this is not guaranteed with the standard linear SVM (as for example in [7]) because the regularization term is not a spatial regularization . The aim of the present paper is to propose methods to ensure that w opt is spatially regularized. 2.3 How to include priors in SVM To spatially regularize the SVM, one has to include some prior knowledge on the proximity of features. In the literature, three main ways have been considered in order to include priors in SVMs. In an SVM, all the information used for classification is encoded in the kernel. Hence, the first way to include prior is to directly design the kernel function [4]. But this implies knowing a metric on the input space X consistent with the prior knowledge.
 Another way is to force the classifier function to be locally invariant to some transformations. This can be done: ( i ) by directly engineering a kernel which leads to locally invariant SVM, ( ii ) by generating artificially transformed examples from the training set to create virtual support vectors (virtual SV), ( iii ) by using a combination of both these approaches called kernel jittering [12, 13, 14]. But the main difficulty here is how to define the transformations to which we would like the kernel to be invariant.
 The last way is to consider SVM from the regularization viewpoint [15, 4]. The idea is to force the classifier function to be smooth with respect to some criteria. This is the viewpoint which is adopted in this paper. 2.4 Regularization operators Our aim is to introduce a spatial regularization on the classifier function of the SVM which can be operator P on f . Following [15, 4], P is defined as a linear map from a space F  X  R X into a dot product space ( D ,  X  X  ,  X  X  D ) .
 G : X  X  X  X  R is a Green X  X  function of a regularization operator P iff: If P admits at least a Green X  X  function called G , then G is a positive semi-definite kernel and the minimization problem: is equivalent to the SVM minimization problem with kernel G .
 Since in linear SVM, the feature space is the input space, f lies in the input space. Therefore, the P . Note that, usually, F is a Reproducing Kernel Hilbert Space (RKHS) with kernel K and D = F . Hence, if P is bounded, injective and compact, P admits a Green X  X  function G = ( P  X  P )  X  1 K where P  X  denotes the adjoint of P .
 One has to define the regularization operator P so as to obtain the suitable regularization for the problem. Spatial regularization requires the notion of proximity between elements of V . This can be done through the definition of a graph in the discrete case or a metric in the continuous case. In this sec-tion, we propose spatial regularizations based on the Laplacian for both of these proximity models. This penalizes the high-frequency components with respect to the topology of V . 3.1 Graphs When V is finite, weighted graphs are a natural framework to take spatial information into consid-eration. Voxels of a brain image can be considered as nodes of a graph which models the voxels X  proximity. This graph can be the voxel connectivity (6, 18 or 26) or a more sophisticated graph. We chose the following regularization operator: where L denotes the graph Laplacian [16] and w  X  the dual vector of w .  X  controls the size of the regularization. The optimization problem then becomes: Such a regularization exponentially penalizes the high-frequency components and thus forces the classifier to consider as similar voxels highly connected according to the graph adjacency matrix. According to the previous section, this new minimization problem (5) is equivalent to an SVM optimization problem. The new kernel K  X  is given by: This is a heat or diffusion kernel on a graph. Our approach differs from the diffusion kernels intro-duced by Kondor et al. [17] because the nodes of the graph are the features, here the voxels, whereas in [17], the nodes were the objects to classify. Laplacian regularization was also used in satellite imaging [18] but, again, the nodes were the objects to classify. Our approach can also be considered as a spectral regularization on the graph [19]. To our knowledge, such spectral regularization has not been applied to brain images but only to the classification of microarray data [20]. 3.2 Compact Riemannian manifolds In this paper, when V is continuous, it can be considered as a 2-dimensional (e.g. surfaces) or a 3-dimensional (e.g. 3D Euclidean or more complex) compact Riemannian manifold. The metric then models the notion of proximity. On such spaces, the heat kernel exists [21, 22]. Therefore, the Laplacian regularization presented in the previous paragraph can be extended to compact Rieman-nian manifolds [22]. Similarly to the graphs, we chose the following regularization operator: where  X  denotes the Laplace-Beltramin operator. The optimization problem is also equivalent to our approach and that of Laferty and Lebanon [22]. In our case, the points of the manifolds are the features, whereas in [22], they were the objects to classify.
 In sections 4 and 5, we present different types of proximity models which correspond to different types of graphs or distances. In this section, we consider the case of regularization based on spatial proximity, i.e. two voxels (or vertices) are close if they are spatially close. 4.1 The 3D case When V are the image voxels (discrete case), the simplest option to encode the spatial proximity is to use the image connectivity (e.g. 6-connectivity) as a regularization graph. Similarly, when V is a compact subset of R 3 (continuous case), the proximity is encoded by a Euclidean distance. In both cases, this is equivalent to pre-process the data with a Gaussian smoothing kernel with standard deviation  X  = However, smoothing the data with a Gaussian kernel would mix gray matter (GM), white mat-ter (WM) and cerebrospinal fluid (CSF). Instead, we propose a graph which takes into considera-tion both the spatial localization and the tissue types. Based on tissue probability maps, in each voxel v , we have the set of probabilities p v that this voxel belongs to GM, WM or CSF. We con-sidered the following graph. Two voxels are connected if and only if they are neighbors in the image (6-connectivity). The weight a u,v of the edge between two connected voxels u and v is a beforehand  X  equal to the standard deviation of d  X  2 ( p u , p v ) .
 To compute the kernel, we computed e  X   X L x s for each subject s in the training set by scaling the Laplacian and using the Taylor series expansion. 4.2 The surface case The connectivity graph is not directly applicable to surfaces. Indeed, the regularization would then strongly depend on the mesh used to discretize the surface. This shortcoming can be overcome by reweighing the graph with conformal weights. In this paper, we chose a different approach by adopting the continuous viewpoint: we consider the cortical surface as a 2-dimensional Riemannian manifold and use the regularization operator defined by equation (7). Indeed, the Laplacian is an intrinsic operator and does not depend on the chosen surface parameterization. The heat kernel has already been used for cortical smoothing for example in [23, 24, 25, 26]. We will therefore not detail this part. We used the implementation described in [26]. In this section, we consider a different type of proximity, which we call anatomical proximity. Two voxels are considered close if they belong to the same brain network. For example, two voxels can be close if they belong to the same anatomical or functional region (defined for example by a probabilistic atlas). This can be seen as a  X  X hort-range X  connectivity. Another example is that of  X  X ong-range X  proximity which models the fact that distant voxels can be anatomically (through white matter tracts) or functionally connected (based on fMRI networks).
 We first focus on the discrete case. The presented framework can be used either for 3D images or surfaces and computed very efficiently. However, such an efficient implementation was obtained at the cost of the spatial proximity. Therefore, we then show a continuous formulation which enables to consider both spatial and anatomical proximity. 5.1 On graphs: atlas and connectivity ity that the voxel v belongs to region A r . Then the probability that two voxels v ( i ) and v ( j ) is the probability that the voxels v ( i ) and v ( j ) belong to the same regions.
 For  X  X ong-range X  connections (structural or functional), one can consider an R -by-R matrix C with matrix becomes: ECE t . We considered the normalized Laplacian  X  L [16], to be sure that the two terms commute: where D is a diagonal matrix. Hence, if CE t D  X  1 E is not singular, we have: The computation requires only the computation of D  X  1 2 , which is done efficiently since D is a diagonal matrix, and the computation of inverse and the matrix exponential of an R -by-R matrix, which is also efficient since R  X  10 2 .
 This method can be directly applied to both 3D images and cortical surfaces. Unfortunately, the efficient implementation was obtained at the cost of the spatial proximity. The next section presents a combination of anatomical and spatial proximity using the continuous viewpoint. 5.2 On statistical manifolds atlas information and spatial proximity. We first show that this can be done by considering the images or surfaces as statistical manifolds together with the Fisher metric. We then give some details about the computation of the kernel. Fisher metric We assume that we are given an anatomical or a functional atlas A composed of each point v  X  V , we have a probability distribution p atlas (  X | v )  X  R T X A which informs about the tissue type and the atlas region in v . Without any loss of generality, one can assume that the tis-We also consider a probability distribution p loc (  X | v )  X  R V which encodes the spatial proxim-M = p (  X | v )  X  R A X V A natural way to encode proximity on M is to use the Fisher metric as in [22]. With some smooth-ness assumption about p , M together with this metric is a compact Riemannian manifold [27]. For clarity, we present this framework only for 3D images but it could be applied to cortical surfaces with minor changes. The metric tensor g is then given for all v  X  V by: If we further assume that p loc (  X | v ) is isotropic we have: p loc (  X | v )  X  N ( v,  X  2 loc I 3 ) , we have: g ij ( v ) = g atlas ij ( v ) + Computing the kernel Once the notion of proximity is defined, one has to compute the kernel of the training set. The eigendecomposition of the Laplace-Beltrami operator is intractable since the number of voxels in a brain images is about 10 6 . Hence e  X   X   X  x s is considered as the solution at t =  X  of the heat equation with the Dirichlet homogeneous boundary conditions: The Laplace-Beltrami operator is given by [21]:  X  u = where h is the inverse tensor of g .
 To solve equation (12), one can use a variational approach [28]. We used the rectangular finite elements in space and the explicit finite difference scheme for the time discretization.  X  x and  X  t denote the space step and the time step respectively.  X  x is fixed by the MRI spatial resolution.  X  t is then chosen so as to respect the Courant-Friedrichs-Lewy (CFL) condition, which can be written in this case as:  X  t  X  2(max  X  i )  X  1 , where  X  i are the eigenvalues of the general eigenproblem: K U =  X  M U with K the stiffness matrix and M the mass matrix. To compute the optimal time step  X  t , we estimated the largest eigenvalue with the power iteration method. 6.1 Material Subjects and MRI acquisition Data were obtained from the Alzheimer X  X  Disease Neuroimaging Initiative (ADNI) database 1 . The Principal Investigator of this initiative is Michael W. Weiner, M.D., VA Medical Center and University of California -San Francisco.For up-to-date information see www.adni-info.org. We studied 30 patients with probable AD (age  X  standard-deviation (SD) = 74  X  4 , range = 60 -80 years, mini-mental score (MMS) = 23  X  2 ) and 30 elderly controls (age  X  SD = 73  X  4 , range = 60 -80 , MMS = 29  X  1 ) which were selected from the ADNI database according to the following criteria. Subjects were excluded if their scan revealed major artifacts or gross structural abnormalities of the white matter, for it makes the tissue segmentation step fail. 80-year-old subjects or older were also excluded. The MR scans are T1-weighted MR images. MRI acquisition was done according to the ADNI acquisition protocol in [29].
 Features extraction For the 3D image analyses, all T1-weighted MR images were segmented into gray matter (GM), white matter (WM) and cerebrospinal fluid (CSF) using the SPM5 (Statistical Parametric Mapping, London, UK) unified segmentation routine [30] and spatially normalized with DARTEL [9]. The features are the GM probability maps in the MNI space. For the surface-based thickness measures were performed with Freesurfer (Massachusetts General Hospital, Boston, MA). 6.2 Proposed experiments As an illustration of the method, we present the results of the AD versus controls analysis. We present the maps associated to the optimal margin hyperplane (OMH). The classification function obtained with a linear SVM is the sign of the inner product of the features with w opt , a vector role in the classifier. Thus the optimal weights w opt allow us to evaluate the anatomical consistency of the classifier. In all experiments, the C parameter of the SVM was fixed to one (  X  = 1 2 NC [4]). 6.3 Results: spatial proximity In this section, we present the results for the spatial proximity in the 3D case (method presented in section 4.1). Due to space limitations, the surface case is not presented. Fig. 1(a) presents the OMH when no spatial regularization is performed. Fig. 1(b) shows the results with spatial proximity but without tissue probability maps. w becomes smoother and spatially consistent. However it mixes tissues and does not respect the topology of the cortex. For instance, it mixes tissues of the temporal lobe with tissues of the frontal and parietal lobes. The results with both spatial proximity and tissue maps are shown on Fig. 1(c). The OMH is much more consistent with the brain anatomy.  X  controls the size of the spatial regularization and was chosen to be equivalent to a 4mm-FWHM of the Gaussian smoothing. The classification accuracy was estimated by a leave-one-out cross validation. The classifiers were able to distinguish AD from CN with similar accuracies ( 83% with no spatial priors and 85% with spatial priors). 6.4 Results: anatomical proximity In this section, we present the results for the anatomical proximity. We first present the discrete surface case. The discrete 3D case leads to comparable results but is omitted here due to space limitations. We then present the continuous 3D case. Extension to surfaces is left for future work. Discrete case For the discrete case, we used  X  X hort-range X  proximity, defined by the cortical atlas The accuracies ranged between 80% and 85% . The highest accuracy was reached for  X  = 3 . The optimal SVM weights w are shown on Fig. 2. When no regularization has been carried out, they are noisy and scattered (Fig. 2 (a)). When the amount of regularization is increased, voxels of a same region tend to be considered as similar by the classifier (Fig. 2(b-d)). Note how the anatomical coherence of the OMH varies with  X  .
 Continuous case We then present the results of the 3D continuous case (section 5.2). The atlas information used was only the tissue types. We chose  X  loc = 10 mm for the spatial confidency.  X  was chosen to be equivalent to a 4mm-FWHM of the Gaussian smoothing. The classifier reached 87% accuracy. The optimal SVM weights w are shown on Fig. 1(d). The tissue knowledge enables the classifier to be more consistent with the anatomy. For instance, note the difference with the Gaussian smoothing (Fig. 1(b)) and how the proposed method avoids mixing the temporal lobe with the parietal and frontal lobes. Figure 1: Normalized w coefficients: (a) no spatial prior, (b) spatial proximity: FWHM=4mm, (c) spatial Figure 2: Normalized w of the left hemisphere when the SVM is regularized with a cortical atlas [31]: In this contribution, we proposed to use regularization operators to add spatial consistency to SVMs for brain image analysis. We show that this provides a flexible approach to model different types of proximity between the features. We proposed derivations for both 3D image features, such as tissue maps, or surface characteristics, such as cortical thickness. We considered two different types of formulations: a discrete viewpoint in which the proximity is encoded via a graph, and a continuous viewpoint in which the data lies on a Riemannian manifold. In particular, the latter viewpoint is useful for surface cases because it overcomes problems due to surface parameterization. This paper introduced two different types of proximity. We first considered the case of regularization based on spatial proximity, which results in spatially consistent OMH making their anatomical interpretation more meaningful. We then considered a different type of proximity which allows modeling higher-level knowledge, which we call anatomical proximity. In this model, two voxels are considered close if they belong to the same brain network. For example, two voxels can be close if they belong to the same anatomical region. This can be seen as a  X  X hort-range X  connectivity. Another example is that of  X  X ong-range X  proximity which models the fact that distant voxels can be anatomically connected, through white matter tracts, or functionally connected, based on fMRI networks.
 Preliminary evaluation was performed on 30 patients with AD and 30 age-matched controls. The results demonstrate that the proposed approaches allow obtaining spatially and anatomically coher-ent discrimination patterns. In particular, the obtained hyperplanes are largely consistent with the neuropathology of AD, with highly discriminant features in the medial temporal lobe, as well as lateral temporal, parietal associative and frontal areas. As for the classification results, they were comparable to those reported in the literature for AD classification (e.g. [5, 8, 7]). The use of regu-larization did not substantially improve the accuracy. However, the most important point is that the proposed approach makes the results more consistent with the anatomy, making their interpretation more meaningful.
 Finally, it should be noted that the proposed approach is not specific to structural MRI, and can be applied to other pathologies and other types of data (e.g. functional or diffusion-weighted MRI). Acknowledgments This work was supported by ANR (project HM-TC, number ANR-09-EMER-006).
 Data collection and sharing for this project was funded by the Alzheimer X  X  Disease Neuroimaging Initiative (ADNI; Principal Investigator: Michael Weiner; NIH grant U01 AG024904). ADNI data are disseminated by the Laboratory of Neuro Imaging at the University of California, Los Angeles.
