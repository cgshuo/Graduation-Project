 In Information Retrieval (IR), the Dirichlet Priors have been applied to the smoothing technique of the language modeling approach. In this paper, we apply the Dirichlet Priors to the term frequency normalisation of the classical BM25 proba-bilistic model and the Divergence from Randomness PL2 model. The contributions of this paper are twofold. First, through extensive experiments on four TREC collections, we show that the newly generated models, to which the Dirich-let Priors normalisation is applied, provide robust and effec-tive performance. Second, we propose a novel theoretically-driven approach to the automatic parameter tuning of the Dirichlet Priors normalisation. Experiments show that this tuning approach optimises the retrieval performance of the newly generated Dirichlet Priors-based weighting models. H.3.3 [ Information Storage and Retrieval ]: Retrieval models Experimentation, Performance, Theory Term frequency normalisation, weighting model, Dirichlet Priors
Document ranking is a crucial issue in Information Re-trieval (IR), which is usually based on a weighting model [9]. Almost all weighting models take term frequency ( tf ), the number of occurrences of a query term in a document, into consideration as a basic factor for document ranking.
However, the term frequency is dependent on the doc-ument length, i.e. the number of tokens in a document. In [11], Singhal et al. summarised the following two aspects of the effect of document length on tf :
As a consequence, tf needs to be normalised by using a technique called term frequency normalisation .

Many normalisation methods have been developed in the past, including the normalisation 2 [1] and BM25 X  X  normal-isation component [10]. Moreover, as suggested in [1], the Dirichlet Priors, which have been applied to the IR language modeling approach [13], can also be applied to the tf nor-malisation. We will introduce these normalisation methods in the next section. In this paper, we denote the tf nor-malisation using the Dirichlet Priors as the Dirichlet Priors normalisation , and denote the weighting model, to which the Dirichlet Priors normalisation is applied, as the Dirich-let Priors-based model .
 In this paper, we study the application of the Dirichlet Priors normalisation on a representative of two families of weighting models, including the classical BM25 probabilis-tic model [10] and the Divergence from Randomness (DFR) PL2 model [1]. In our extensive experiments, we show that the Dirichlet priors-based models achieve robust and effec-tive retrieval performance over diverse TREC collections. Experiments also show that there is a justifiable need for the tuning of the parameter of the Dirichlet Priors nor-malisation. In particular, we propose a novel automatic theoretically-driven tuning methodology for the Dirichlet Priors normalisation. As mentioned above, there is a depen-dence between tf and the document length. Therefore, the purpose of the tf normalisation is to adjust the dependence between the normalised term frequency and the document length. In our tuning method, this dependence is interpreted as the correlation between the two variables. In our exper-iments, we show that the optimal parameter values, which give the best mean average precision, result in stable corre-lation measures. In particular, our tuning approach seems to be both collection-independent and query-independent. The remainder of this paper is organised as follows. In Section 2, we introduce some related work, including the above mentioned normalisation methods and the weighting models to which these normalisation methods are applied. We apply the Dirichlet Priors normalisation to BM25 and PL2 in Section 3 and describe the proposed methodology for tuning the Dirichlet Priors normalisation in Section 4. In Sections 5 and 6, we present our experimental setting and evaluation results. Finally, we conclude the work and suggest future directions in Section 7.
As one of the most established weighting models, BM25 computes the relevance score of a document d for a query Q by the following formula: where qtf is the query term frequency; w (1) is the idf factor, which is given by:
N is the number of documents in the whole collection. N t is the document frequency of term t .

K is: l and avg l are the document length and the average doc-ument length in the collection, respectively. The document length refers to the number of tokens in a document. k 1 , k 3 and b are parameters. The default setting is k 1 = 1 . 2, k 3 = 1000 and b = 0 . 75 [10]. qtf is the number of oc-currences of a given term in the query; tf is the within document frequency of the given term.

In the following derivation, we show how the BM25 model implicitly employs a tf normalisation component.
Let tf n = tf term frequency, we obtain: Hence the term frequency normalisation component of the BM25 formula can be seen as:
The above BM25 X  X  normalisation component can be seen as a generalisation of Singhal et al. X  X  pivoted normalisation for normalising the tf  X  idf weight [11].

PL2 is one of the divergence from randomness (DFR) doc-ument weighting models [2]. Using the PL2 model, the rel-evance score of a document d for a query Q is given by: score ( d, Q ) = where  X  is the mean and variance of a Poisson distribution. qtf is the query term frequency.

The normalised term frequency tf n is given by the so-called normalisation 2 [1, 7]: where l is the document length and avg l is the average document length in the whole collection. tf is the original within document term frequency. c is the free parameter of the normalisation method.

The Dirichlet Priors stand for the priors in a Dirichlet dis-tribution, which is a generalisation of the Beta distribution in a multinomial case. In [13], Zhai &amp; Lafferty have applied the Dirichlet priors in defining their language model for IR. It was also suggested by Amati that the Dirichlet Priors can be used for the tf normalisation [1]: where tf n is the normalised term frequency. l is the doc-ument length. tf c is the frequency of the given query term in the collection. l c is the number of tokens in the whole collection.  X  is the parameter of the Dirichlet Priors nor-malisation.
In this section, we apply the Dirichlet Priors normalisation to both PL2 (see Equation (4)) and BM25 (see Equation (1)).

As shown in the previous section, both BM25 and PL2 weighting models employ a tf normalisation component. By replacing the tf normalisation components of the two mod-els with the Dirichlet Priors normalisation, we generate the following new models:
In Section 6.1, through extensive experiments, we show that the newly generated Dirichlet Priors-based models lead to robust and effective retrieval performance. Experiments also show that the optimal parameter setting of the parame-ter  X  , which gives the best mean average precision, varies on different collections and query types, indicating that there is a need for tuning the parameter. We describe the proposed tuning method in the next section.
In [1], Amati denotes the Dirichlet Priors normalisation as the normalisation 3. In this paper, we follow his notation and denote a Dirichlet Priors-based model M as M3.
In this section, we propose a novel theoretically-driven methodology for the automatic tuning of the Dirichlet Pri-ors normalisation. Our proposed methodology is based on measuring the correlation ( corr ) [4] of the normalised term frequency ( tf n ) with the document length ( l ) for a given query term, which is given by: where COV stands for covariance.  X  ( l ) is the standard de-viation of the length of the documents containing the given query term.  X  ( tf n ) is the standard deviation of the nor-malised term frequency of the given query term in all the documents containing the term.

As introduced in Section 1, the purpose of the tf nor-malisation is to smooth the dependence between tf n and document length, which can be represented by the above correlation formula. We believe that the tf normalisation can be seen as the tf density estimation of the document length. On different collections, the ideal tf density func-tion should result in similar correlation of tf n with the doc-ument length. Therefore, the underlying hypothesis of our tuning method is the following:
In the above hypothesis, the optimal parameter setting refers to the parameter value that provides the highest mean average precision. Based on the hypothesis, the tuning of the Dirichlet Priors normalisation becomes the issue of iden-tifying the parameter setting that gives an optimal average correlation of tf n with l for the given set of query terms. The proposed tuning methodology can be described as fol-lows: 1. On a training collection, we obtain the optimal pa-2. On a given new collection, and for a given new query
Note that in the above tuning process, for a given set of query terms and a particular parameter value, the corr ( tf n, l ) value is the average correlation measure for the given set of query terms. Ideally, we could tune the parameter setting for each query term, and apply different parameter settings for different query terms. However, as it is very expensive to carry out such a term-based parameter tuning mechanism, in this paper, we rather assume an optimal average correla-tion measure so that we do the tuning process for a set of Table 1: The five weighting models involved in our experiments. The last three models are our base-lines.
 query terms in a batch mode, and apply a unique parameter setting for all the terms in the given set of queries.
According to the study by Zhai &amp; Lafferty [13], the op-timal setting of the parameter  X  of the Dirichlet Priors Smoothing changes with different collections and query sets. Based on the above hypothesis, our explanation is that the length distribution of documents containing a given query term varies with the change of data set, including the col-lection and query set. As a consequence, a particular cor-relation corr ( tf n, l ) value corresponds to different  X  values on different data sets, and therefore results in different opti-mal parameter values. In our experiments in Section 6.2, we show that our hypothesis holds on diverse TREC collections.
In our experiments, we evaluate the Dirichlet Priors-based models, i.e. BM3 and PL3, on diverse collections using three baselines, which are BM25, PL2 and the classical tf  X  idf weighting models. Table 1 lists the five models involved in our experiments.

Our implementation of the tf  X  idf baseline weighting model is a combination of the Okapi X  X  tf [10] and Sparck-Jones X  idf [12]: score ( d, Q ) = where qtf is the query term frequency of t . N is the number of documents in the whole collection and N t is the document frequency of t . l and l coll are the number of tokens in d and in the whole collection, respectively. k 1 and b are parameters and their default settings are k 1 = 1 . 2 and b = 0 . 75, respectively [10].
 We experiment on four TREC collections to evaluate the Dirichlet Priors-based models. The four used collections are the disk1&amp;2, disk4&amp;5 (minus the Congressional Record on disk4) of the classical TREC collections 2 , and the WT2G [6] and WT10G [5] Web collections. The test queries are TREC topics that are numbered from 51 to 200 for the disk1&amp;2, from 301 to 450 and from 601-700 for the disk4&amp;5, from 401 to 450 for the WT2G, and from 451 to 550 for the WT10G, respectively (see Table 2).

Table 2 lists the test TREC topics, the number of doc-uments, and the standard deviation of document length in each collection. As shown in the last row, the document length distribution of the four collections is quite different, which indicates that the newly generated Dirichlet Priors-Related information of disk1&amp;2 and disk4&amp;5 of the TREC collections can be found from the following URL: http://trec.nist.gov/data/docs eng.html Table 2: Details of the four TREC collections used in our experiments. The second row gives the num-ber of topics associated to each collection. N is the number of documents in the given collection.  X  lcoll is the standard deviation of document length in the whole collection.
Topics 51-200 301-450 based models and the tuning method are evaluated on di-verse collections.

Each TREC topic consists of three fields, i.e. title, de-scription and narrative. In this paper, we experiment with three types of queries with respect to the use of different topic fields, in order to check the impact of query length on the effectiveness of our models and the tuning method. The three types of queries are:
Regarding the parameter setting of the baseline models, for the BM25 model, we use the default setting, which is b = 0 . 75, k 1 = 1 . 2 and k 3 = 1000 [10]; For the PL2 model, we use the default setting applied in [1], which is c = 1 for short queries and c = 7 for long queries. Since [1] does not report experiments using normal queries, we use the optimal parameter setting on the disk1&amp;2 as the baseline, i.e. c = 1 . 4 for normal queries; For the tf  X  idf model, we also apply the default setting that is k 1 = 1 . 2 and b = 0 . 75 [10]. For the Dirichlet Priors-based models, including BM3 and PL3, we test a series of values for the parameter  X  , rang-ing from 0 (exclude 0) to 10,000, in order to extensively study the performance of the Dirichlet Priors-based models and show the need for the parameter tuning of the Dirichlet Priors normalisation.

Moreover, for testing our automatic tuning approach to the Dirichlet Priors normalisation, we use the disk1&amp;2 as the training collection, and evaluate on the other three col-lections. An advantage of using this training collection is that it has a relatively large number of available training queries, which are the TREC topics numbered from 51 to 200. After obtaining the optimal corr ( tf n, l ) value on the training collection using the corresponding relevance assess-ment, we evaluate our approach on the other three TREC collections. For the computation of the corr ( tf n, l ) value, terms that appear in only one document are ignored in order to avoid a zero correlation.

Our baseline for the evaluation of the tuning method is the optimal setting on the training collection, which is an empirical setting. Moreover, we compare the performance of our tuning method with the best manually obtained pa-rameter setting using relevance assessment.

In all our experiments, standard stopwords removal and the Porter X  X  stemming algorithm are applied. We used one AMD Athlon 1600 processor, running at 1.4GHz.
 Figure 1: The mean average precision (MAP) against the parameter  X  for short queries on the four used TREC collections. Table 3: The best manually obtained settings of the parameter  X  , selected from a wide range of values, on four collections for three types of queries.

In Section 6.1, we start with presenting the results ob-tained using the Dirichlet Priors-based models with various parameter settings, showing the importance of our param-eter tuning method. Then, we compare the performance of the Dirichlet Priors-based models with three baseline mod-els. In Section 6.2, we show that our automatic tuning method achieves robust and effective performance. In par-ticular, the tuning method X  X  performance differs marginally from the best manually obtained setting using relevance as-sessment.
Table 3 contains the manually obtained optimal parame-ter values on different collections with respect to the three query types. As we can see, on diverse collections, the opti-mal parameter values are quite different. Moreover, Figures 1, 2 and 3 sketch the plots of the parameter  X  against mean average precision on the four used TREC collections for the three types of queries, respectively. In the figures, the curve s of the parameter  X  against mean average precision behave differently. Table 3 and Figures 1, 2 and 3 show that there is a justifiable need for the parameter tuning. Figure 2: The mean average precision (MAP) against the parameter  X  for normal queries on the four used TREC collections. Figure 3: The mean average precision (MAP) against the parameter  X  for long queries on the four used TREC collections.
Table 4 provides the mean average precision obtained by using the baselines and the Dirichlet Priors-based models. From the table, we have the following observations: Overall, as shown by the results, the newly generated Dirichlet Priors-based models outperform the tf  X  idf model and achieve comparable performance with the robust BM25 and PL2 models.
 Table 4: The obtained mean average precision on the four collections using the five models. The ap-plied parameter settings for BM3 and PL3 are taken from Table 3. Table 5: The optimal parameter values and the cor-responding correlation measures of the normalised tf with the document length on the training col-lection. The value marked with * is taken as the optimal constant corr ( tf n, l ) of our tuning method.
In this section, we start with presenting how we detect the optimal corr ( tf n, l ) on the training collection 3 discuss the obtained evaluation results.

Table 5 contains the optimal parameter values and corre-sponding corr ( tf n, l ), i.e correlation of the normalised term frequency with the document length on the training collec-tion. Note that the provided correlation values are the mean of the correlation for each query term. We can see that the optimal parameter values for the two Dirichlet Priors-based models are very similar, and for different types of queries, the optimal parameter settings result in relatively similar correlation measures. Although for short queries, the optimal settings are not identical (  X  = 800 for BM3 and  X  = 400 for PL3), for both models,  X  = 800 and  X  = 400 differs marginally from each other in terms of mean aver-age precision 4 , indicating that the underlying hypothesis of our tuning method stands (see Section 4 for the hypothe-
The notion of the  X  X ptimal corr ( tf n, l ) X  refers to the corr ( tf n, l ) value given by the optimal parameter setting. See step 1 of our tuning method in Section 4.
For BM3,  X  = 400 and  X  = 800 provide mean average precision of .2384 and .2381, respectively. For PL3,  X  = 400 and  X  = 800 provide mean average precision of .2242 and .2292, respectively. Table 6: The correlation measures for some query terms on the training collection for different param-eter values. sis). Therefore, for the tuning process, we take the opti-mal corr ( tf n, l ) for short queries using BM3 as the optimal constant corr ( tf n, l ) (see the value marked with a star in Table 3). We choose the optimal corr ( tf n, l ) value for the short queries as the optimal constant correlation because of the fact that query terms in the titles are generally more informative than those in the descriptions and narratives. Therefore, terms in the titles can be more reliable than terms in other topic fields in inferring an optimal parameter set-ting. On a collection other than the training collection, we apply such a parameter setting that it gives this constant.
Table 6 presents some examples of the correlation mea-sures on the training collection. As we can see, with respect to a particular parameter value, the correlation measures of different terms are diverse. Therefore, a term-based tun-ing approach might achieve higher precision/recall than just computing the mean of the correlation measures of query terms. However, as it is quite time-consuming to carry out a tuning process for each query term, we rather follow the pro-posed approach in this paper (see Section 4). Later we show that our approach achieves robust retrieval performance in the evaluation.

Tables 7 and 8 compare the mean average precision (MAP) obtained by using our tuning method with the MAP ob-tained by using the optimal values on the training collection. In the two tables, M AP b and M AP t stands for the mean av-erage precision (MAP) obtained by using the baseline set-ting and the tuning method, respectively.  X  stands for the parameter setting estimated by the tuning method.  X  is the percentage of improvement using the tuning method. A p-value marked with star indicates a significant difference between the results at 0.05 level according to the Wilcoxon test. As we can see, in most cases, our tuning method either significantly outperforms the baseline, or achieves compara-ble performance with the baseline.

Moreover, Tables 9 and 10 compare the performance of the tuning method with that of the manually optimal pa-rameter setting obtained using relevance assessment. The notations in the two tables are the same as in Table 7. As can be seen from the tables, the performance of our tuning method is similar with the manual setting in most cases, and the difference of mean average precision is usually marginal. This indicates that the underlying hypothesis of our tun-ing method indeed holds (see Section 4 for the hypothesis). Overall, our tuning method provides effective and reliable re-trieval performance over diverse TREC document and Web collections.
In this paper, we have studied the application of the Dirich-let Priors to the term frequency normalisation. In particu-Table 7: Results for BM3. This table compares the performance obtained by using the optimal setting on the training collection with that using the tun-ing method. The settings for  X  are automatically obtained using our tuning method. disk4&amp;5 668 .2490 .2499 +0.36 .02569* WT2G 2266 .2692 .3151 +17.05 1.174e-05* WT10G 1782 .2040 .2093 +2.60 .3253 disk4&amp;5 578 .2352 .2377 +1.06 .7677 WT2G 1514 .2427 .2674 +10.18 .08932 WT10G 1168 .1853 .1745 -5.83 .7098 disk4&amp;5 610 .2694 .2646 -1.78 .1841 WT2G 1441 .2324 .2624 +4.30 .1503 WT10G 1212 .2246 .2155 -4.05 .6677 Table 8: Results for PL3. This table compares the performance obtained by using the optimal setting on the training collection with that using the tun-ing method. The settings for  X  are automatically obtained using our tuning method. disk4&amp;5 668 .2260 .2243 -0.75 .0887 WT2G 2266 .1935 .2833 +46.41 3.828e-07* WT10G 1782 .1804 .1923 +6.60 .002791* disk4&amp;5 578 .2083 .2168 +4.08 .00288 WT2G 1514 .1582 .2388 +50.95 1.769e-06 WT10G 1168 .1792 .1746 -2.57 .9238 disk4&amp;5 610 .2385 .2421 +1.51 .1378 WT2G 1441 .1747 .2495 +42.82 2.461e-05
WT10G 1212 .2152 .2135 -0.79 .8997 lar, we have applied the Dirichlet Priors normalisation to a representative of two families of weighting models, i.e. the classical BM25 probabilistic model and the Divergence from Randomness PL2 model. By replacing the tf normalisation components of the two models with the Dirichlet Priors nor-malisation, the newly generated weighting models are shown to be robust and effective in our experiments.

A major contribution of this paper is the proposed novel theoretically-driven automatic tuning method for the Dirich-let Priors normalisation. The proposed approach interprets the dependence between the normalised term frequency and the document length as the correlation between the two vari-ables. Experiments on the TREC collections show that the underlying hypothesis of our tuning approach holds. Eval-uation results also show that the tuning method signifi-cantly outperforms the baseline and its performance differs marginally from the manual setting using relevance assess-ment.

There are some interesting future directions that will help in better understanding the tf normalisation. We plan to study the application of other smoothing methods, e.g. the Jelinek-Mercer smoothing [3, 8], to the tf normalisation. Table 9: Results for BM3. This table compares the performance obtained manually using relevance assessment with that using the automatic tuning method.
 Table 10: Results for PL3. This table compares the performance obtained manually using relevance assessment with that using the automatic tuning method.
 In particular, we will apply the proposed tuning method to these classical smoothing methods.

It will also be interesting to device a term-based tun-ing mechanism for the Dirichlet Priors normalisation. As suggested previously, a term-based tuning mechanism could achieve a better retrieval performance though it would have a high computational cost. A possible solution for lowering the overhead is to enable tuning only for the most informa-tive terms in a query, while applying the default parameter setting for the rest of the query terms.
This work is funded by the Leverhulme Trust, grant num-ber F/00179/S. The project funds the development of the Smooth project, which investigates the term frequency nor-malisation (URL: http://ir.dcs.gla.ac.uk/smooth).
The experiments were conducted using Terrier X  X  IR plat-form, version 1.0.0. It is a modular platform for the rapid development of large-scale IR applications, providing index-ing and retrieval functionalities. More information can be found from http://ir.dcs.gla.ac.uk/terrier. [1] G. Amati. Probabilistic Models for Information [2] G. Amati and C. J. van Rijsbergen. Probabilistic [3] S. Chen and J. Goodman. An empirical study of [4] M. DeGroot. Probability and Statistics . Addison [5] D. Hawking. Overview of the TREC-9 Web Track. In [6] D. Hawking, E. Voorhees, N. Craswell, and P. Bailey. [7] B. He and I. Ounis. Tuning Term Frequency [8] F. Jelinek and R. Mercer. Interpolated estimation of [9] C. J. van Rijsbergen. Information Retrieval, 2nd [10] S. Robertson, S. Walker, M. Beaulieu, M. Gatford, [11] A. Singhal, C. Buckley, and M. Mitra. Pivoted [12] K. Sparck-Jones. A statistical interpretation of term [13] C. Zhai and J. Lafferty. A study of smoothing
