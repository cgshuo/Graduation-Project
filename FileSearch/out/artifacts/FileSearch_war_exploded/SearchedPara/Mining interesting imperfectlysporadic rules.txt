 REGULAR PAPER Yu n S i n g Ko h  X  Nathan Rountree  X  Richard A. O X  X eefe Abstract Detecting association rules with low support but high confidence is a difficult data mining problem. To find such rules using approaches like the Apriori algorithm, minimum support must be set very low, which results in a large number of redundant rules. We are interested in sporadic rules; i.e. those that fall below a maximum support level but above the level of support expected from random coin-cidence. There are two types of sporadic rules: perfectly sporadic and imperfectly sporadic. Here we are more concerned about finding imperfectly sporadic rules, where the support of the antecedent as a whole falls below maximum support, but where items may have quite high support individually. In this paper, we intro-duce an algorithm called Mining Interesting Imperfectly Sporadic Rules (MIISR) to find imperfectly sporadic rules efficiently, e.g. fever, headache, stiff neck  X  meningitis . Our proposed method uses item constraints and coincidence pruning to discover these rules in reasonable time. This paper is an expanded version of Koh et al. [Advances in knowledge discovery and data mining: 10th Pacific-Asia Conference (PAKDD 2006), Singapore. Lecture Notes in Computer Science 3918, Springer, Berlin, pp 473 X 482].
 Keywords Sporadic itemsets  X  Coincidence pruning  X  Minimum absolute support  X  Item constraint 1 Introduction There are many association rule mining algorithms dedicated to frequent itemset mining [ 1, 2, 4, 5, 10, 19, 21, 22, 24]. These algorithms are defined in such a way that they only find rules with high support and high confidence. A much less explored area in association mining is infrequent itemset mining. Recently, Koh and Rountree [ 11] proposed the Apriori-Inverse algorithm to mine infrequent itemsets without generating any frequent rules. It captures the so-called spo-radic rules using maximum support (maxsup) and minimum confidence (min-conf) thresholds. Apriori-Inverse generates rules where the support of item-sets forming each rule is below the maxsup threshold but above a user-defined minimum absolute support value. They define the notion of perfectly sporadic rules, where the itemset forming each rule consists only of items that are be-low the maxsup threshold. In contrast, imperfectly sporadic rules consist of individual items with high support but the support of combinations of items is low.
 considers itemsets that have support above maxsup; therefore, no subset of any itemset that it generates can have support above maxsup. Apriori will miss these rules, because the support for the itemsets forming the rules is too low. Apriori-Inverse will miss them as well, because the support for the individual items is too high. Therefore, both algorithms will miss rules of the form AB  X  C ,where A and B are individually common, but AB is rare and C is rare. This, for example, is the situation where two symptoms  X  both of which commonly occur alone  X  occur together only rarely; but when they do, the combination indicates a rare and serious disease with high confidence. We consider this to be a very interesting type of rule to be able to find.
 rules efficiently. To force any variant of the Apriori algorithm [ 2] to find imper-fectly sporadic rules, the minimum support threshold must be set very low. This, in turn, drastically increases the running time of the algorithm, due to a combi-natorial explosion in the number of frequent itemsets. Apriori-Inverse suffers the same problem in reverse: maximum support has to be set so high that too many itemsets qualify as sporadic.
 Sporadic Rules (MIISR) to find imperfectly sporadic rules using item constraints: we capture rules with a single-item consequent below the maxsup threshold. The maxsup threshold is used to identify all items that are considered rare. These items are then considered to be the only possible consequents for all rules that will be generated. Items in the transactions containing the consequent are then detected. The items found are used to form antecedents that have strong associations with the consequents.
 distinguishable from coincidences (that is, situations where items fall together no more often than would be allowed by chance). Hence, we use coincidence pruning to remove the occurrences of coincidental itemsets. For an itemset to be consid-ered non-coincidental, it must have support above a minimum absolute support (minabssup) value which is generated using a variant of Fisher X  X  exact test. The rest of this paper is organised as follows. Definitions pertinent to infrequent item-set mining and a review of related work are given in Sect. 2. The MIISR algorithm and an explanation of coincidence pruning is presented in Sect. 3. In Sect. 4, we evaluate MIISR on synthetic and real datasets, and in Sect. 5, we conclude the paper. 2 Related work The following is a formal statement of association rule mining for transaction databases. Let I ={ i 1 , i 2 ,..., i m } be the universe of items and D beasetof transactions, where each transaction T is a set of items such that T  X  I .An association rule is an implication of the form X  X  Y ,where X  X  I , Y  X  I ,and X  X  Y = X  . X is referred to as the antecedent of the rule and Y as the consequent . The rule X  X  Y holds in the transaction set D with confidence c %if c %ofthe transactions in D that contain X also contain Y .Therule X  X  Y has support s % in the transaction set D ,if s % of transactions in D contain XY [2]. Throughout this paper we shall use XY to denote an itemset that contains both X and Y . posing item constraints ; i.e., providing a list of those items that may or may not take part in a rule and then modifying the mining process to take advantage of that information [ 3, 14, 15, 17, 18, 20]. One of the restrictions that may be im-posed is called consequent constraint-based rule mining . Among these we shall discuss Dense-Miner, Direction Setting (DS) rules, Emerging Pattern (EP), and Fixed-Consequent Association Rule Mining (ARM).
 numerous in dense data, even when using an item constraint. A dense dataset has many frequently occurring items, strong correlations between several items, and many items in each record. Thus, they used a consequent constraint-based rule mining approach. They require mined rules to have a given consequent C spec-ified by the user. They also introduce an additional metric called improvement . The key idea is to extract rules with confidence above a minimum improvement value greater than any of the simplifications of a rule. A simplification of a rule is formed by removing one or more items from its antecedent. Any positive min-imum improvement value would prevent the unnecessarily complex rules from being generated. A rule is considered overly complex if simplifying its antecedent results in a rule with higher confidence. The improvement of a rule, A  X  C is defined as the minimum difference between its confidence and the confidence of any proper sub-rule with the same consequent: If the improvement of a rule is greater than 0, then removing any non-empty com-bination of items from the antecedent will lower the confidence by at least the improvement. Thus, every item and every combination of items present in the an-tecedent of a rule with a large improvement is an important contributor to its pre-dictive ability. In contrast, it is considered undesirable for the improvement of a rule to be negative, as it suggests that the extra elements in the antecedent detract from the rule X  X  predictive power. Bayardo et al. [ 3] introduced an algorithm for mining association rules with consequent C meeting user-specified minimum sup-port, confidence, and improvement thresholds. A rule is considered to have a large improvement if its improvement is at least the specified minimum improvement. Their system is called Dense-Miner, and its advantage over previous systems is that it can utilise rule constraints to efficiently mine consequent-constrained rules at low support. approach uses a chi-square significance test which tries to prune out spurious and insignificant rules that appeared by coincidence rather than random association. They also introduce DS rules. If the antecedent and consequent of a rule are pos-itively associated, we say that the direction is 1, whereas if they are negatively associated we say that the direction is  X  1. If the antecedent and consequent are in-dependent, then the direction is 0. The set of expected directions for R : AB  X  C is the direction for both R 1 : A  X  C and R 2 : B  X  C . The rule, R isaDSruleifit has a positive association (direction of 1) and its direction is not an element of the set of expected directions .For R to be a DS rule, the direction for both R 1 and/or R 2 should not be 1. Note that in the chi-square test treatment of frequencies, us-ing chi-square is an approximation, which is useful for simplifying calculations [9, 6]. Chi-square gives an estimate of the true probability value, which becomes inaccurate when sample sizes are small, or the data are very unevenly distributed among itemsets in the database [ 8].
 they use a dataset partitioning approach to find top rules, zero-confidence rules, and  X  -level confidence rules. The dataset D is divided into sub-datasets D 1 and D 2 ;where D 1 consists of the transactions containing the known consequent and D 2 consists of transactions which do not contain the consequent. All items in T are then removed from D 1 and D 2 . Using the transformed dataset, EP then finds all itemsets X which occur in D 1 but not in D 2 . For each X ,therule X  X  T is a top rule in D with confidence of 100%. On the other hand, for all itemsets Z that only occur in D 2 , all transactions in D which contain Z must not contain T . Therefore, Z  X  T has a negative association and is a zero-confidence rule. For  X  -level confidence rules Y  X  T , the confidences are greater than or equal to 1  X   X  . The confidences of  X  -level rules must satisfy where sup 1 ( Y ) denotes the support of Y in D 1 and sup 2 ( Y ) denotes the support of Y in D 2 . This approach is considered efficient as it only needs one pass through the dataset to partition and transform it. Of course, in this method, you must supply T .
 erate the highest support rules that match the user X  X  specified minimum without having to specify any support threshold. Fixed-consequent association rule mining generates confident minimal rules using SE trees and P-trees [ 18]. Given two rules R 1 and R 2 , with confidence values higher than the confidence threshold, where R 1 is A  X  C and R 2 is AB  X  C , R 1 is preferred, because the antecedent of R 2 is a superset of the antecedent of R 1 . The support of R 1 is necessarily greater than or equal to R 2 . R 1 is referred to as a minimal rule ( X  X implest X  in Bayardo et al. [ 3]) and R 2 is referred to as a non-minimal rule (more complex). The algorithm was devised to generate the highest support rules that match the user-specified mini-mum confidence threshold without having the user specify any support threshold. prior knowledge that a particular consequent is of interest. For our application, we are interested in searching for imperfectly sporadic rules, without having to wade through a lot of rules that have high support, without generating a large number of trivial rules, and without needing prior knowledge of which consequents ought to be interesting. 3 Proposal of MIISR In the previous section, the techniques discussed might generate some imperfectly sporadic rules, if there is prior knowledge of a rare and interesting consequent, and if minsup is set low enough. So, rather than address the problem in the con-text of frequent itemset mining, we suggest explicitly treating it as a problem of infrequent itemset mining. Hence, we propose the MIISR algorithm to mine interesting imperfectly sporadic rules. This algorithm uses the same definition of maxsup as in Apriori-Inverse [ 11]. Since itemsets lose support as they grow larger, and our guiding constraint is maxsup rather than minsup, we can no longer rely on a downward-closure principle.
 as candidate consequents. For each candidate consequent, we then generate can-didate antecedents from the items within the same transactions. Because we are dealing with candidate itemsets with low support, it is possible that we will see items occurring together in transactions about as many times as chance would al-low  X  we refer to this situation as a coincidence . Itemsets that occur within the database due to coincidence do not add meaningful information and should be ig-nored. Hence, we identify a minabssup value to filter out these itemsets. The im-perfectly sporadic rules are then generated in a similar fashion to Apriori. As we are storing the transactional dataset in an inverted index, we note that our method does not require dataset partitioning. 3.1 Imperfectly sporadic rules A rule is considered imperfectly sporadic if it meets the requirements of maxsup and minconf but contains at least one item that has support above maxsup. For instance, suppose we had an itemset AB with support ( A ) = 12%, support ( B ) = 10%, and support ( AB ) = 10%, with maxsup = 11% and minconf = 75%. Both A  X  B (confidence = 92%) and B  X  A (confidence = 100%) are sporadic in that they have low support and high confidence. Imperfectly sporadic rules are defined as in Koh and Rountree [ 11]: Definition 3.1 (Imperfectly Sporadic) A  X  Bis imperfectly sporadic for max-sup s and minconf c iff Some imperfectly sporadic rules could be completely trivial or uninteresting: for instance, when the antecedent is rare but the consequent has support of 100%. We can characterise four different types of imperfectly sporadic rule: Typ e 1 rules have both frequent and infrequent itemsets in antecedent and con-Typ e 2 rules have only frequent itemsets in both their antecedent and consequent. Typ e 3 rules have consequents that contain only infrequent itemsets; they will only Typ e 4 rules have antecedents that contain only infrequent itemsets; they will only We would prefer a technique that finds imperfectly sporadic rules that are interest-ing : for instance, when the items in the antecedent are above maxsup but the in-tersection of these items is below maxsup and the consequent has a support below maxsup. Clearly, Type 3 rules are interesting under this definition, and the rest of this paper describes our attempt to generate them in a reasonably efficient manner. An example of a Type 3 rule is fever, stiff neck, rash  X  meningitis ,where fever , stiff neck ,and rash are common separately, but just occasionally occur together. When they do, one could potentially diagnose meningitis with some confidence, even though meningitis is quite rare. 3.2 MIISR overview Broadly speaking, the MIISR algorithm performs the following steps. On the first pass through the database an inverted index is built using items as keys and the transaction IDs as the data. At this point, the support of each unique item (the 1-itemsets) in the database is available as the length of each data chain. Items that fall under maxsup are identified and recorded as candidate consequents. For each candidate consequent found, we use the items that reside in the same transactions as the candidate consequent to extend the ( k  X  1 ) -itemsets in precisely the same manner as Apriori to generate candidate k -itemsets. These extensions are consid-ered to be candidate antecedents. We then check the candidate antecedent itemsets against the inverted index to ensure they meet the minimum absolute support re-quirement and prune them out if they do not. This candidate generation process is repeated until no further candidate antecedents are produced. 3.3 Minimum absolute support value When searching for rare itemsets, we consider two circumstances: occurrences of itemsets due to some non-random process that is generating them or occurrences of itemsets by random collision (coincidence). It is important to distinguish be-tween them, as itemsets that have a low support but high confidence that seem interesting may be occurring due to chance and should be considered  X  X oise X . Clearly it makes sense only to consider candidate itemsets that appear together more often than coincidence. We define coincidence in the following way: for N transactions in which the antecedent A occurs in a transactions and consequent B occurs in b transactions, we can calculate the probability that A and B will oc-cur together exactly c times by chance. We refer to this as  X  X robability of chance collision X  [ 12]. We can calculate this probability using Pcc in (1). The probability that A and B will occur together exactly c times is determine that the probability that A and B will occur exactly 250 times is 0 . 05. This equation is the usual calculation for exact probability of a 2  X  2 contingency table [ 23]. Now, we want the least number of collisions above which Pcc is smaller than some small value p (say, 0 . 0001). This is Usually, a 2  X  2 contingency table is provided and a p -value calculated. How-ever, here we are providing two of the four values and a p -value, and calculating the minimum value to complete the table. By selecting the minabssup value for each itemset we are able to prune out associations that appear in the dataset by chance. We calculate the cumulative Pcc of AB together m times (beginning from 0 and incremented by 1). We stop the incrementation when the cumulative value of Pcc  X  1 . 0  X  p and m is set as the minabssup value. For example, given that we set N = 1000, A = B = 500, and p = 0 . 0001, the minabsup value is 274. Candidate itemsets that appear above the minabssup requirement are considered somewhat interesting, and worth retaining to evaluate their confidence. 3.4 Exclusory constraint Even after pruning out candidate antecedents that are indistinguishable from co-incidence, a considerable number of itemsets can still be produced for a modest dataset. One property which an imperfectly sporadic rule must have is as follows: a rule with a low support consequent almost always occurs together with a partic-ular antecedent which may be a combination of frequent items. Note that having a low support rule with high confidence may not always be useful. For example, consider a rule R : A  X  C with support = 0 . 01 and confidence = 0 . 95. What if the confidence of C  X  A is 0.1? This means that C is occurring ten times more often than A ,so A is hardly a good predictor of C . Despite that this rule still has a lift value of 9.5, it is unlikely to give us useful information.
 generating new candidates. Another solution, and the one we adopt, is to prohibit current candidates from being extended if their extensions are not likely to pro-duce interesting rules. Here we introduce an exclusory constraint for this purpose. An imperfectly sporadic rule A  X  C which has high confidence, becomes less meaningful if C  X  A has confidence that is too low. For a candidate antecedent A to be considered worth expanding with respect to consequent C , it must therefore meet the following requirement: produce an interesting rule with C ,but AZ is unlikely to do so no matter what Z is. Thus, we wish to keep A in the pool of candidate itemsets, but we do not wish to extend it  X  we exclude it from the next round of candidate generation. 3.5 The MIISR algorithm Having defined how to calculate the minimum absolute support (minabssup) nec-essary to consider a rule to be non-coincidental, and a procedure to prevent can-didate antecedents from being extended if they are unlikely to produce interesting rules, we are now able to define an algorithm for MIISR.
 Algorithm 3.1 Mining Interesting Imperfectly Sporadic Rules (MIISR) Algorithm 3.2 Invert( D , I) Algorithm 3.3 Count matching transactions using Idx, count(item , Idx) contained in transaction tid . Idx ( i ) returns the list of transactions IDs in which i appears. Thus, U  X  t  X  Idx ( i ) D ( tid ) gives us the list of items in transactions also containing item i .
 indexed by all 1-itemsets that fall under maximum support. It is not necessary to return C , the list of candidate consequents, since if A i is non-empty, i  X  C .For each of these items i , A contains a list of antecedents such that A i , j  X  i should be an imperfectly sporadic rule. Since MIISR does not restrict the antecedents to containing only frequent itemsets, all perfectly sporadic rules would also be produced. 4 Experimental evaluation To assess the performance of MIISR in discovering Type 3 imperfectly sporadic rules, we developed a synthetic data generator which deliberately injects imper-fectly sporadic itemsets. We also tested the MIISR algorithm on 12 different datasets from the UCI Machine Learning Repository [ 16]. 4.1 Synthetic data generator Our synthetic data generator is a modified version of the data generator proposed by Agrawal and Srikant [ 2]. In real databases, there may be both frequent and in-frequent itemsets and rules, but we are only interested in the imperfectly sporadic itemsets. Table 1 summarises the characteristics of several of the datasets gener-ated during our tests. Since we are deliberately ignoring large itemsets, we left | I | set to 2 for all experiments.
 lowing parameters: number of transactions | D | , average size of transactions | T | , average size of large itemsets | I | , number of large itemsets | L | , number of imper-fectly sporadic itemsets | S | , and number of items N .Wefirstdeterminethesize of the next transaction which is generated using a Poisson distribution with the mean set to the average size of the transactions generated so far. We then fill the transactions with items. Each transaction is assigned a series of potentially fre-quent itemsets and/or an imperfectly sporadic itemset. After the frequent and/or sporadic items are filled into the transactions, we also allow in some random items. These items are treated as noise and our proposed algorithm should prune them out. Each itemset in T has a weight associated with it, which corresponds to the probability that this itemset will be picked. 4.2 Precision and recall evaluation Here we attempt to quantify the accuracy of mined association rules based on the ability of our algorithm to derive a known set of association rules. To measure the performance of our algorithm, we propose to use two standard information retrieval measures, precision and recall. Precision and recall are the basic measures used in evaluating search strategies [ 7].
 number of irrelevant and relevant rules generated. Recall is the ratio of the num-ber of relevant rules generated to the total number of relevant rules in the database. The relevant rules generated are considered as true-positive cases, the irrelevant rules generated are considered as false-positive cases, and the relevant rules that were not generated are considered as false-negative cases. Both of these measures are defined as a proportion in the range [0, 1]. In order to compute these evalu-ation metrics, we must have some sort of prior knowledge of possible rules that are correct, rules that are incorrect, and the number of rules that exist within the dataset.
 ject rules and all others are quite literally random. Hence, we would be able to evaluate the results produced by using MIISR. If we were to run Apriori on the dataset, we would definitely be able to detect all the imperfectly sporadic rules that were injected and the average recall would be 1 (and time complexity pro-hibitive). However, we might get an average precision value that is too low. Due to the nature of Apriori, it will find all frequent rules and some trivial rules within the dataset.
 datasets, in which we injected a number of known imperfectly sporadic itemsets into the dataset. In this particular set of datasets, we varied the number of imper-fectly sporadic itemsets injected into the data from 2 to 20 with an interval of 2 for the T30.L50.D10 K datasets. We then checked the rules generated against the list of injected rules. If the rule exists within the list of known rules, it is consid-ered to be a relevant rule, whereas, if the rule is not contained within the set of predetermined rules, it is considered an irrelevant rule. sory constraint. In the first set of experiments, we ran MIISR without the exclu-sory constraint on the datasets. Table 2 shows the results of precision and recall in this experiment. Notice that the precision level with the dataset that contained 14 sporadic items is much lower. In this case, a frequent itemset occurred with a particular sporadic item above the minabssup level. The average precision is 0.976 which means that there is only a small number of false positives. The average re-call is 1.00, meaning that our algorithm was able to detect all rules injected in each of the datasets. Using minabssup we were able to prune out a lot of unnecessary rules. In spite of this, MIISR without the exclusory constraint was still able to de-tect all the injected rules in each of the datasets. It still found some rules that were not deliberately injected into the dataset, but the average precision level was still acceptable.
 exclusory constraint. The average recall value remained at 1.00, and the average precision value increased to 1.00. This means that this algorithm only generated the injected rules. The exclusory constraint was able to filter out the less interesting rules that minabssup was not able to prune out by itself. MIISR with the exclusory constraint was able to detect all the injected rules efficiently. Unlike Apriori, we do not need a specific post-processing algorithm to filter out the less interesting rules. 4.3 Scalability evaluation 4.3.1 Experiments using synthetic datasets Three different experiments varying the number of transactions, average size of transaction, or the number of imperfectly sporadic itemsets injected were con-ducted to assess the efficiency and scalability of MIISR without and with the exclusory constraint. The datasets used were generated using the synthetic data generator described in the previous section. Maxsup was set to 0 . 10 and minconf to 0 . 90. In these experiments, we allow in an additional 30%, 60%, and 90% of the number of items to provide noise. over the T10.L30.S10 dataset. In Fig. 1, we see that the time taken to process the data seems to increase linearly with the number of transactions. When the noise level increases from 30 to 90%, the slopes of the graph become steeper. However, this is not unexpected, because when the noise level is increased the number of items appearing  X  X ogether X  increases. The algorithm then has to check all of these extra collisions in order to differentiate those that are truly associated (i.e., those that are deliberately injected at a rate higher than chance) from those that are not. ing the average transactions size, from 15 to 60 with an increment of 5, for the L60.S10.D10 K dataset. As the average size of the transaction increases, the data generator may allow more noise into particular transactions. The fluctuation in execution time taken for the dataset with 30 and 60% noise levels is a small per-centage of the total execution time. For the dataset that contained the 90% noise level, the execution time increased somewhat worse than linearly as the average size of transactions increased. However, the curvature is gentle over a reasonable range of values.
 ately injected imperfectly sporadic itemsets which ranged from 2 to 20 with an increment of 2 for the T30.L50.D10 K datasets. Figure 3 shows the results of the execution time to process a dataset with a varying number of injected imperfectly sporadic itemsets. Although the number of rules found for each dataset increases as the number of imperfectly sporadic itemsets increases, the fluctuation in exe-cution time is quite a small percentage of the total execution time. The difference between the maximum and minimum time taken for the datasets with 30% noise level, 60% noise level, 90% noise level are 4, 12, and 31 s, respectively. ysed the time taken to rerun these experiments with the additional constraint. In the first experiment, on the T10.L30.S10 dataset, we varied the number of transac-tions from 10 3 to 10 6 . On average, MIISR with the exclusory constraint was faster by 40 s. We then reran the experiment using MIISR with the exclusory constraint on the L60.S10.D10 K dataset which varied the average size of transactions, from 15 to 60 with an increment of 5. On average, MIISR with the exclusory constraint was 4 s slower than MIISR without exclusory constraint. In the third experiment, we ran the experiment on T30.L50.D10 K datasets which investigate the scale-up of the number of deliberately injected imperfectly sporadic itemsets which ranged from 2 to 20 with an increment of 2. On average, it was only 1 s faster than MIISR without exclusory constraint. However, we note that these results are not repre-sentative of performance on a real dataset. Our synthetic data generator did not deliberately inject rules that would not meet the exclusory constraint. Thus, all the itemsets that were injected were allowed to be extended. 4.3.2 Experiments using real datasets The MIISR algorithm was also tested using 12 different datasets from the UCI Machine Learning Repository [ 16 ]. Table 3 displays results from MIISR without the exclusory constraint. Each row of the table represents an attempt to find the number of imperfectly sporadic rules (with minconf 0 . 90, and lift greater than 1 . 0) from the database named in the left-most column. In the table, accept represents the number of itemsets below the maxsup but above the minabssup value and re-ject represents the number of itemsets below the minabssup value. The number of accepted and rejected itemsets depends on the amount of noise within a cer-tain dataset. Passes represents the number of extensions to the candidate sporadic itemset (e.g., passes is 2 when the candidate 2-itemsets are generated). Note that the number of passes here is the total number of passes over all of the possible consequents. Thus, there may be cases in which the number of passes is large but the rules produced is close to 0. Average sporadic itemsets is the average num-ber of the total imperfectly sporadic itemsets found. Note that the increase in the speed efficiency of the results compared to the previously published version is due to tuning of the algorithm X  X  implementation.
 of 0.37 for the dataset House-votes, 0.10 for the datasets Zoo, Teaching Assis-tant Evaluation, Solar-Flare, Bridges, Flag, Liver Diseases, Anneal, and Soybean-Large within reasonable time (below 100 s). However, for the Chess and Mush-room datasets, maxsup was lowered to 0.05 and 0.005. Due to the nature of the Chess and Mushroom datasets, where strong association holds among most of the items, coincidence pruning is not able to prune out many itemsets. Consequently, we end up with a very large number of imperfectly sporadic rules, some of which might be less interesting than others. In some cases, the number of rules generated is larger than 10,000. It would be infeasible to make sense of these rules manually. plays results from using MIISR with the exclusory constraint. Using an assump-tion of minconf 0 . 90, lift greater than 1 . 0, and the same maxsup as in Table 3, we were able to reduce the number of rules generated. Notice that the number of rules pruned out is substantially different for each particular dataset. For the Soybean-Large dataset, MIISR with the exclusory constraint was able to prune up to 13 times more rules, whereas for the Teaching Assistant Evaluation dataset, it did not prune out any rules. The number of rules pruned out using the exclusory constraint depends on the dataset. The number of rules generated in Tables 3 and 4 include redundant rules. We have not pruned out these or any other rules. constraint and the number of rules accepted by the exclusory constraint seem to be entirely dataset-dependent. 5 Conclusions and future work Very few existing algorithms try to find infrequent itemsets, despite the fact that potentially the most interesting things that happen in a database are likely to hap-pen infrequently. In this paper, we present a new algorithm called MIISR for dis-covering imperfectly sporadic rules. We are particularly interested in infrequently occurring associations of frequent itemsets giving rise to infrequent consequents. The supports of imperfectly sporadic rules are, by definition, low, and we therefore run the risk of accepting as interesting rules itemsets that have only fallen together by chance. For this reason, the minimum absolute support value proposed in the paper plays an important role because it does not allow rules that have only chance association to be generated. We acknowledge that this approach, and that based on the exclusory constraint, are heuristic, but we believe that to be unavoidable when generating low support rules. Since the number of low support rules can be very large, but those that are likely to be interesting not as common, exhaustive tech-niques tend rapidly to fall into pathological cases.
 medium-sized real datasets. Our future work will deal with examining the prob-lem of transaction length, to which MIISR seems most sensitive in the synthetic datasets. More importantly, we need to try to characterise the UCI Mushroom dataset in terms of imperfectly sporadic rules, and determine an approach that would narrow down the candidate antecedent itemsets even further. If it turns out that there simply are a lot of high confidence, low support rules in that database, then we need to investigate interestingness metrics that could be used to gener-ate a better subset of them. Hence, we are interested in determining the actual interestingness of imperfectly sporadic rules in real domains.
 References Author Biographies
