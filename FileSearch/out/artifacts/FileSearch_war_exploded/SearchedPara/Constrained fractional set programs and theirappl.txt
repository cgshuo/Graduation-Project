 Thomas B  X uhler tb@cs.uni-saarland.de Saarland University, Saarbr  X ucken, Germany Syama Sundar Rangapuram srangapu@mpi-inf.mpg.de Simon Setzer setzer@mia.uni-saarland.de Matthias Hein hein@cs.uni-saarland.de Saarland University, Saarbr  X ucken, Germany Graph-based data appear in manifold ways in learning problems -either the data have already graph struc-ture as in the case of social networks and biological networks or a similarity graph is constructed using a similarity measure based on features of the data. Sev-eral graph-based problems in clustering and commu-nity detection can be modelled as the optimization of a ratio of set functions (referred to here as fractional set program). Prominent examples are the normalized cut problem, from which the popular spectral cluster-ing method is derived (Shi &amp; Malik, 2000), and the maximum density subgraph problem, which has appli-cations in community detection (Fortunato, 2010) and bioinformatics (Saha et al., 2010).
 It turns out that in practice often additional back-ground or domain knowledge about the learning prob-lem is available. Such prior knowledge can then be incorporated as constraints into the optimization prob-lem. In the case of clustering, Wagstaff et al. (2001) are the first to show how prior information given in the form of must-link and cannot-link constraints be-tween vertices can be integrated into the k -means al-gorithm. Recently, Rangapuram &amp; Hein (2012) pro-posed a generalization of the normalized cut problem that can handle must-link and cannot-link constraints. In the recent work of Mahoney et al. (2012), locality constraints in the form of a seed set and volume con-straint have been integrated into the normalized cut formulation. Furthermore, Khuller &amp; Saha (2009) and Saha et al. (2010) considered size and distance con-straints for the maximum density subgraph problem. Since the above-mentioned combinatorial problems are NP-hard, the standard approach is to consider con-vex or spectral relaxations which can be solved glob-ally optimally in polynomial time. Due to its prac-tical efficiency the spectral relaxation is very popular in machine learning, e.g. spectral clustering (Hagen &amp; Kahng, 1991; Shi &amp; Malik, 2000). However, it is of-ten quite loose and thus leads to a solution far away from the optimal one of the original problem. More-over, spectral-type relaxations (Mahoney et al., 2012) fail to guarantee that the constraints which encode the prior knowledge are satisfied.
 In another line of work (Hein &amp; B  X uhler, 2010; Szlam &amp; Bresson, 2010; Hein &amp; Setzer, 2011; Bresson et al., 2012), it has been shown that tight continuous relax-ations exist for all balanced graph cut problems and the normalized cut subject to must-link and cannot-link constraints (Rangapuram &amp; Hein, 2012). A tight relaxation means that the continuous and the com-binatorial optimization problem are equivalent in the sense that the optimal values agree and the optimal solution of the combinatorial problem can be obtained from the continuous solution. While the resulting al-gorithms provide no guarantee to yield the globally optimal solution, the standard loose relaxations are outperformed by a large margin in practice.
 In this paper we show that any constrained minimiza-tion problem of a ratio of non-negative set functions allows a tight relaxation into a continuous optimiza-tion problem. This result together with our efficient minimization techniques enables the easy integration of prior information in form of constraints into many problems in graph-based clustering and community de-tection. While the general framework introduced in this paper is applicable to all problems discussed so far, we will focus on two particular applications: lo-cal clustering by constrained balanced graph cuts, and community detection via constrained densest subgraph problems. Compared to previous work, the algorithms developed in this paper are the first to guarantee that all given constraints are fulfilled by the obtained solu-tion. Note that in principle our method could also be applied to a setting with soft or noisy constraints, how-ever we will focus here on the case of hard constraints. In the experimental section we will show the supe-rior performance compared to state of the art methods (Andersen &amp; Lang, 2006; Mahoney et al., 2012). In the following, G = ( V,W ) denotes an undirected, weighted graph with a non-negative, symmetric weight matrix W  X  R n  X  n , where n = | V | . Moreover, by assigning a non-negative weight g i to each vertex i , we can define the general volume of a subset A  X  V as vol g ( A ) = P i  X  A g i . As special cases, we obtain for g = 1 the cardinality | A | and for g i equal to the degree d = P j  X  V w ij the classical volume vol( A ) = vol d ( A ). Furthermore, A = V \ A denotes the complement of A . The balanced graph cut problem is a well-known problem in computer science with applications rang-ing from parallel computing to image segmentation (Pothen et al., 1990; Shi &amp; Malik, 2000). A very popu-lar balanced graph cut criterion is the normalized cut 1 , where cut( C, C ) := P i  X  C,j  X  C w ij . The spectral re-laxation of the normalized cut leads to the popular spectral clustering method (von Luxburg, 2007). A related criterion is the normalized Cheeger cut,
NCC( C, C ) = More general balanced graph cuts were studied by Hein &amp; Setzer (2011). In practice, often additional in-formation about the desired solution is available which can be incorporated into the problem via constraints. This motivates us to consider a more general class of problems where one optimizes a ratio of set functions 2 subject to constraints. In the following, we discuss two examples of constrained problems in network analysis. Constrained balanced graph cuts for local clus-tering. Recently, there has been a strong interest in balanced graph cut methods for local clustering. Start-ing with the work of Spielman &amp; Teng (2004), initially, the goal was to develop an algorithm that finds a sub-set near a given seed vertex with small normalized cut or normalized Cheeger cut value with running time lin-ear in the size of the obtained cluster. The proposed algorithm and subsequent work (Andersen et al., 2006; Chung, 2009) use random walks to explore the graph locally, without considering the whole graph. Algo-rithms of this type have been applied for community detection in networks (Andersen &amp; Lang, 2006). In contrast, Mahoney et al. (2012) give up the runtime requirement and formulate the task as an explicit op-timization problem, where one aims at finding the op-timal normalized cut subject to a seed constraint and an upper bound on the volume of the set containing ter around a given seed set. Motivated by the stan-dard spectral relaxation of the normalized cut prob-lem, they derive a spectral-type relaxation which is biased towards solutions fulfilling the seed constraint. Their method has been successfully applied in semi-supervised image segmentation (Maji et al., 2011) and for community detection around a given query set (Ma-honey et al., 2012). However, while they provide an ap-proximation guarantee for their relaxation, they can-not guarantee that the returned solution satisfies seed and volume constraints.
 In this paper we consider an extended version of the problem of Mahoney et al. (2012). Let J denote the set of seed vertices, b S a symmetric balancing function (e.g. b S ( C ) = vol d ( C ) vol d ( C ) for the normalized cut) and let vol g ( C ) be the general volume of set C , where g  X  R n + are vertex weights. The general local cluster-ing problem can then be formulated as The choice of the balancing function b S allows the user to influence the trade-off between getting a partition with small cut and a balanced partition. One could also combine this with must-and cannot-link con-straints (see Rangapuram &amp; Hein, 2012) or add even more complex constraints such as an upper bound on the diameter of C . However, in order to compare to the method of Mahoney et al. (2012), we restrict our-selves in this paper to the normalized cut with volume Constrained local community detection. A sec-ond related problem is constrained local community detection. In community detection it makes more sense to find a highly connected set instead of em-phasizing the separation to the remaining part of the graph by minimizing the cut. Thus, we are search-ing for a set C which has high association, defined as assoc( C ) = P i,j  X  C w ij . Dividing the association of C by its size yields the density of C . The subgraph of maximum density can be computed in polynomial time (Goldberg, 1984). However, the obtained commu-nities in the unconstrained problem are typically either too large or too small, which calls for size constraints. Note that the introduction of such constraints makes the problem NP-hard (Khuller &amp; Saha, 2009). A general class of (local) community detection prob-lems can thus be formulated as where g,h  X  R n + are vertex weights. This formu-lation generalizes the above-mentioned density-based approaches by replacing the denominator by a general volume function vol g . One can use the vertex weights g to bias the obtained community towards one with desired properties by assigning small weights to ver-tices which one would prefer to occur in the solution and larger weights to ones which are less preferred. The problem (2) with only lower bound constraints has been considered in team selection (Gajewar &amp; Das Sarma, 2012) and bioinformatics (Saha et al., 2010) where constant factor approximation algorithms were developed. However, in the case of equality and up-per bound constraints the problem is very hard even when using only cardinality constraints (i.e., h i = 1), and it has been shown that there is no polynomial time approximation scheme in these cases (Khot, 2006; Khuller &amp; Saha, 2009). Our method can handle such hard upper bound and equality constraints. In the ex-periments we show results for a community detection problem with a specified query set J and an upper bound on the size for a co-author network.
 Note that if vol g ( C ) = vol d ( C ), one can decompose the objective of (2) analogously to the argument for the normalized cut (Shi &amp; Malik, 2000) as b S ( C ) = vol d ( C ) in (1), the problem (2) is equivalent to (1) if we choose the same constraints. If one has only the constraint vol d ( C )  X  1 2 vol d ( V ) both problems are equivalent to the normalized Cheeger cut.
 Contributions of this paper. We show that all constrained non-negative fractional set programs have an equivalent tight continuous relaxation. This gen-eral result enables the integration of prior information in form of constraints into clustering and community detection problems. In particular, it allows us to de-rive efficient algorithms for problems (1) and (2). Our algorithms consistently outperform competing meth-ods (Andersen &amp; Lang, 2006; Mahoney et al., 2012). Moreover, we are not aware of any other methods for the above problems which can guarantee that the so-lution always satisfies volume and seed constraints. Although the tight relaxation results in Hein &amp; Setzer (2011) and Rangapuram &amp; Hein (2012) encompass a large class of problems, they are not applicable to the problems considered in this paper because of the fol-lowing limitations: First, tight relaxations were shown by Hein &amp; Setzer (2011) only for a ratio of symmet-ric non-negative set functions, where the numerator is restricted to be submodular . We extend the results to arbitrary ratios of non-negative set functions without any restrictions concerning symmetry or submodular-ity. Second, only equality constraints for non-negative set functions restricted to be either submodular or su-permodular could be handled by Rangapuram &amp; Hein (2012). We generalize this to inequality constraints without any restrictions on the constraint set functions in order to handle the constraints in (1) and (2). The problems discussed in the last section can be writ-ten in the following general form: where b R, b S, c M i : 2 V  X  R are set functions on a set V = { 1 ,...,n } . We assume here that b R, b S are non-negative and that b R (  X  ) = b S (  X  ) = 0. No assumptions are made on the set functions c M i , in particular they are not required to be non-negative. Thus also lower bound constraints can be written in the above form. Moreover, the formulation in (3) also encompasses the subset constraint J  X  C in (1) and (2) as it can be written as equality constraint | J | X  X  J  X  C | = 0. Al-ternatively, we will discuss a direct integration of the subset constraint into the objective in Section 5. The connection between the set-valued and the contin-uous space is achieved via thresholding. Let f  X  R n , and we assume wlog that f is ordered in ascending order f 1  X  f 2  X  X  X  X  X  X  f n . One defines the sets We frequently make use of this notation in the fol-lowing. Furthermore, we use 1 C  X  R n to denote the indicator vector of the set C , i.e. the vector which is 1 at entry j if j  X  C and 0 otherwise. A key tool for the derivation of the results of this paper is the Lovasz extension as a way to extend a set function (seen as function on the hypercube) to a function on R n . Definition 1 Let b R : 2 V  X  R be a set function with b R (  X  ) = 0 , and f  X  R n in ascending order f 1  X  f 2  X  X  X   X  f n . The Lovasz extension R : R n  X  R of b R is Note that R ( 1 C ) = b R ( C ) for all C  X  V , i.e. R is indeed an extension of b R from 2 V to R n . In the following, we always use the hat-symbol ( and omit it for the corresponding Lovasz extension. A particular important class of set functions are sub-modular set functions since their Lovasz extension is convex (Bach, 2011).
 Definition 2 A set function b R : 2 V  X  R is submod-ular if for all A,B  X  V , b R ( A  X  B ) + b R ( A  X  B )  X  b equality holds true, and modular if we have equality. The connection between submodular set functions and convex functions is as follows (see Bach, 2011). Proposition 1 Let R : R V  X  R be the Lovasz exten-sion of b R : 2 V  X  R . Then, b R is submodular if and only if R is convex. Furthermore, if b R is submodular, Thus submodular minimization problems reduce to convex minimization problems. A similar equivalence of continuous and combinatorial optimization prob-lems is the main topic of this paper. In the following we list some useful properties of the Lovasz extension (see Fujishige, 2005; Bach, 2011; Hein &amp; Setzer, 2011). Proposition 2 Let R : R V  X  R be the Lovasz exten-sion of b R : 2 V  X  R . Then,  X  R is positively one-homogeneous 4 ,  X  R ( f )  X  0 ,  X  f  X  R V and R ( 1 ) = 0 if and only if  X  Let S : R V  X  R be the Lovasz extension of Unconstrained fractional set programs. Using the property of the Lovasz extension that R ( 1 C ) = b R ( C ) for all C  X  V , one can directly observe that the following continuous fractional program is a relaxation of the unconstrained version of problem (3) The following theorem shows that the relaxation is in fact tight, in the sense that the optimal values agree and the solution of the set-valued problem can be com-puted from the solution of the continuous problem. Note that given a vector f  X  R n for the continuous problem, one can construct a set C 0 by computing where the sets C i are defined in (4). We refer to this process as optimal thresholding .
 Theorem 1 Let b R, b S : 2 V  X  R be non-negative set functions and R,S : R n  X  R their Lovasz extensions, respectively. Then, it holds that Moreover, it holds for all f  X  R n + , R ( f ) S ( f ) can be found by optimal thresholding. Let furthermore b R ( V ) = b S ( V ) = 0 , then all the above statements hold if one replaces R n + with R n .
 In practice it may sometimes by difficult to derive and/or work with explicit forms of the Lovasz exten-sions of b R and b S . However, the following more gen-eral version of Theorem 1 shows that, given a decom-position of b R and b S into a difference of submodular set functions, one needs the Lovasz extension only for the first term of b R and the second term of b The remaining terms can be replaced by any convex one-homogeneous functions that also extend the cor-responding set functions. Note that by Proposition 3 such a decomposition always exists.
 Theorem 1 (b) Let b R, b S : 2 V  X  R be non-negative set functions and b R := b R 1  X  b R 2 and b S := b S 1 be decompositions into differences of submodular set functions. Let the Lovasz extensions of b R 1 , b S 2 be given by R 1 ,S 2 and let R 0 2 ,S 0 1 be positively one-homogeneous convex functions with S 0 1 ( 1 A ) = b S 1 ( A ) and R 0 b R 2 ( A ) such that S 0 1  X  S 2 is non-negative. Define R := R 1  X  R 0 2 and S := S 0 1  X  S 2 . Then, Moreover, it holds for all f  X  R n + , R ( f ) S ( f ) can be found by optimal thresholding. Let furthermore b R ( V ) = b S ( V ) = 0 , then all the above statements hold if one replaces R n + with R n .
 Before we prove the above Theorem, we collect some useful results. Lemma 1 shows that the Lovasz exten-sion of a submodular set function b R is an upper bound on any one-homogeneous convex function R 0 which ex-tends the set function b R to the continuous space. Lemma 1 Let b R : 2 V  X  R be a submodular set homogeneous convex function with R 0 ( 1 A ) = b R ( A ) for all A  X  V . Then, it holds  X  f  X  R V + that Let furthermore b R ( V ) = 0 , then the above inequality holds for all f  X  R V .
 Proof: Let f be ordered in increasing order f 1  X  f 2  X  X  X  X  X  X  f n . Note that every convex, positively one-homogeneous function R 0 : R V  X  R can be written as R ( f ) = sup u  X  U  X  u,f  X  , where U is a convex set (see Hiriart-Urruty &amp; Lemar  X echal, 2001). Then, since for any u  X  U ,  X  u,f  X  X  X  R 0 ( f ), it holds that for any u  X  U and hence for all f  X  R V + , As this holds for all u  X  U we obtain for all f  X  R V + , X For the second statement we use the fact that with the condition b R ( V ) = 0 the lower bound in (5) holds for all f  X  R V .
 The main part of the proof of Theorem 1 (b) is the fol-lowing Lemma which implies that optimal threshold-ing of a vector f always leads to non-increasing values of R ( f ) /S ( f ).
 Lemma 2 Let b R, b S : 2 V  X  R and R,S : R n  X  R satisfy the assumptions of Theorem 1 (b). Then for all f  X  R V + , holds for all f  X  R V .
 Proof: Let R 1 ,S 2 and R 0 2 ,S 0 1 satisfy the conditions from Theorem 1 (b). Let furthermore R 2 and S 1 be the Lovasz extensions of b R 2 and b S 1 . With Lemma 1 and Def. 1, we get  X  f  X  R n + ,
R ( f ) = R 1 ( f )  X  R 0 2 ( f )  X  R 1 ( f )  X  R 2 ( f ) = =  X  min where we used the non-negativity of b R and b S as well as the fact that f  X  R n + . Again using Def. 1, the above is equal to By assumption, S 0 1  X  S 2 is non-negative and thus divi-sion gives the result. The second statement is shown analogously.
 Now we are ready to prove Theorem 1 (b).
 Proof of Theorem 1 (b): Lemma 2 implies that inf On the other hand we have which implies equality. The statement regarding op-timal thresholding has been shown in Lemma 2. The proof for the case where b R ( V ) = b S ( V ) = 0 works anal-ogously.
 Note that no assumptions except non-negativity are made on b R and b S -every non-negative fractional set program has a tight relaxation into a continuous frac-tional program. The efficient minimization of the con-tinuous objective will be the topic of Section 4. Constrained fractional set programs. To solve the constrained fractional set program (3) we make use of the concept of exact penalization (Di Pillo, 1994), where the main idea is to transform a given constrained optimization problem into an equivalent unconstrained one by adding a penalty term. We use the same idea for our constrained fractional set pro-grams and define the penalty set function for a con-straint c M i ( C )  X  k i as th constraint and otherwise increasing with increasing infeasibility. The special treatment of the empty set in the definition of b T i is a technicality required for the can now formulate a modified problem We will show that using a feasible set of (3) one can compute a  X  such that (7) is equivalent to the orig-inal constrained problem. Once we have established the equivalence, we can then apply Theorem 1, noting that b T is a non-negative set function. This leads to the main result of this paper showing a tight relaxation of all problems of form (3) where b R, b S are non-negative set functions. In the following, the constant  X  quanti-fies a  X  X inimum value X  of b T i on the infeasible sets: For example, if c M ( C ) = | C | , then  X  is equal to 1. If c M ( C ) = vol g ( C ) and all vertex weights g i are rational numbers which are multiples of a fraction 1  X  , X   X  N then  X   X  1  X  . Note that in practice, the constant  X  and the parameter  X  introduced in the following are never explicitly computed (see experimental section). Theorem 2 Let b R, b S : 2 V  X  R be non-negative set functions and R , S their Lovasz extensions. Let C 0  X  V be feasible and b S ( C 0 ) &gt; 0 . Denote by T the Lovasz extension of b T . Then, for  X  &gt; b R ( C 0 ) Moreover, for any f  X  R n + with Q  X  ( f ) &lt; b Q  X  ( C the given  X  , we have Q  X  ( f )  X  min i =1 ,...,n b Q  X  ( C the minimizing set on the right hand side is feasible. Proof: We will first show the equivalence between the constrained fractional set program (3) and the uncon-strained problem (7) for the given choice of  X  . Then the equivalence to the continuous problem will follow by Theorem 1.
 sible subset C , that is c M i ( C )  X  k i , i = 1 ,...,K , the objective Q  X  of problem (7) is equal to the objective Q of problem (3). Thus, if we show that all minimizers of the second problem satisfy the constraints then the equivalence follows. Suppose that C  X  6 =  X  is a mini-mizer of the second problem and that C  X  is infeasible. Then by definition we have b T ( C  X  )  X   X  . This yields b Q  X  ( C  X  ) = where we used the non-negativity of b R and b S . Hence which contradicts the fact that C  X  is optimal. Noting that b T is a non-negative function with b T (  X  ) = 0 and  X  &gt; 0, we have a ratio of non-negative set func-tions which attain the value zero on the empty set. Thus application of Theorem 1 yields the equivalence to the continuous problem.
 The second statement can be seen as follows. Suppose Q  X  ( f ) &lt; b Q  X  ( C 0 ). By Lemma 2 we obtain Now suppose that the minimizer C  X  of the right hand side is not feasible, then again by the derivation in (8) and the choice of  X  , which leads to a contradiction. Thus C  X  is feasible. Note that Theorem 2 implies that the set found by optimal thresholding of the solution of the continuous program is guaranteed to satisfy all constraints. We are not aware of any other method which can give the same guarantee for the problems (1) and (2). The continuous optimization problems in Theorems 1 and 2 have the form where R and S are non-negative. The fact that they are the Lovasz extensions of set functions b R, b S also im-plies that they are one-homogeneous, see Bach (2011). We now apply a slightly modified version of a result from Hein &amp; Setzer (2011).
 Proposition 3 Every set function b S with b S (  X  ) = 0 can be written as b S = b S 1  X  b S 2 , where S 1 and S submodular and b S 1 (  X  ) = b S 2 (  X  ) = 0 . The Lovasz exten-sion S can be written as difference of convex functions. The above result implies that (9) can be written as ratio of differences of convex functions (d.c.), i.e. R = R 1  X  R 2 with R 1 , R 2 convex, and similarly for S . As the proof of Proposition 3 is constructive, the explicit form of this decomposition can be calculated. We can now use a modification of the RatioDCA which has re-cently been proposed as an algorithm for minimizing a non-negative ratio of one-homogeneous d.c. functions (Hein &amp; Setzer, 2011). This modification is necessary as the problems in Theorem 1 and 2 require optimiza-tion over the positive orthant. We report the modified version in order to make the paper self-contained. RatioDCA Minimization of a non-negative ratio of one-homogeneous d.c functions over R n + 1: Initialization: f 0  X  R n + ,  X  0 = Q ( f 0 ) 2: repeat 3: f l +1 = arg min 5: until We will refer to the convex optimization problem solved at each step (line 3) as the inner problem . Proposition 4 The sequence f l produced by Ra-tioDCA satisfies Q ( f l +1 ) &lt; Q ( f l ) for all l  X  0 or the sequence terminates.
 Proof: Let  X  f l ( u ) := R 1 ( u )  X  u,r 2 ( f l ) +  X  l u,s 1 ( f l ) denote the objective of the inner problem. The optimal value of the inner problem is non-positive since  X  where we used the fact that f l ,r 2 ( f l ) = R 2 ( f l ) and f l ,s 1 ( f l ) = S 1 ( f l ). Since  X  f l is one-homogeneous, the minimum of  X  f l is always attained at the boundary of the constraint set. If the optimal value is zero, then f is a possible minimizer and the sequence terminates. Otherwise the optimal value is negative and at the optimal point we get where we used that for a positively one-homogeneous convex function one has for all f,g  X  R n + , Thus we obtain The norm constraint of the inner problem is neces-sary as otherwise the problem would be unbounded from below. However, the choice of the norm plays no role in the proof and any norm can be chosen. More-over, in the special case where the one-homogeneous function R is convex and S is concave, the RatioDCA reduces to Dinkelbach X  X  method from fractional pro-gramming (Dinkelbach, 1967) and therefore computes the global optimum. In the general case, convergence to the global optimum cannot be guaranteed. How-ever, we can provide a quality guarantee : RatioDCA either improves a given feasible set or stops after one iteration.
 Theorem 3 Let A be a feasible set and  X  &gt; b sult of RatioDCA after initializing with the vector 1 A , and let C f  X  denote the set found by optimal thresh-olding of f  X  . Either RatioDCA terminates after one Proof: Proposition 4 implies that the RatioDCA ei-ther directly terminates or produces a strictly mono-tonically decreasing sequence. In the latter case, using the strict monotonicity and the fact that thresholding does not increase the objective (Lemma 2), we obtain Assume now that C f  X  is infeasible. Then, one can derive analogously to the proof of Theorem 2 that b Q a contradiction to b Q  X  ( A ) &gt; b Q  X  ( C f  X  ). Hence, C has to be feasible and it holds that b Q ( A ) = b Q  X  ( A ) &gt; b Q The above theorem implies that all constraints of the original constrained fractional set program are fulfilled by the set C f  X  returned by RatioDCA. The framework introduced in this paper allows us to derive tight relaxations of all problems discussed in Section 2. In the following, we will derive a tight re-laxation of the local community detection problem For the constrained balanced graph cut problem, the tight relaxation can be found in a very similar way and is thus omitted here.
 First, we integrate the volume constraint via a penalty term, see (7), which yields the equivalent problem where b T k is given as b T k ( C ) = max { 0 , vol h that the penalty term is equal to b T k ( C ) = vol h ( C )  X  min { k, vol h ( C ) } , which is a difference of submodular functions.
 We could reformulate the seed constraint J  X  C as inequality constraint | J  X  C | X  X  J | X  0 and add a similar penalty function to the numerator of (11). However, using the structure of the problem, a more direct way to incorporate the seed constraint is possible. It holds that (11) has the equivalent form where k 0 = k  X  vol h ( J ). Solutions C  X  of (11) and A of (12) are related via C  X  = A  X   X  J . In order to derive the tight relaxation via Theorem 1, we need the Lovasz extension of the set functions in (12). For technical reasons, we replace the constant set functions vol g ( J ) and assoc( J ) by vol g ( J ) b P ( A ) and assoc( J ) spectively, where b P is defined as b P ( A ) = 1 for A 6 =  X  and b P (  X  ) = 0. This leads to the problem The only difference to (12) lies in the treatment of the empty set. Note that with 0 0 :=  X  the empty set can never be optimal for problem (13). Given an optimal solution A  X  of (13), one then either considers either A  X   X  J or J , depending on whichever has lower objective, which then implies equivalence to (12). The resulting tight relaxation will be a minimization problem over R m with m = | V \ J | and we assume wlog that the first m vertices of V are the ones in V \ J . Moreover, we use the notation f max = max i =1 ,...,m f i for f  X  R m , and d ( A ) i = P j  X  A w ij . The following Lo-vasz extensions are useful: For the sake of brevity, we do not specify the con-vex function T (2) k 0 . Recall from Section 4 that we need only an element of the subdifferential for T (2) k 0 which by Prop. 2.2 in Bach (2011) is given by t where j i denotes the index of the i -th smallest com-ponent of the vector f . The above Lovasz extensions lead to the following tight relaxation of (13): where R 1 ( f ) =  X  ( g i ) m i =1 +  X  ( h i ) m i =1 ,f  X  + vol S R 2 ( f ) =  X T Lower bound constraints. Constraints of the form vol h ( C )  X  k are rewritten as  X  vol h ( C )  X   X  k , which leads to the penalty term, see (6), The decomposition b T k ( C ) = k b P ( C )  X  min { k, vol then again yields a difference of submodular functions (noting k  X  0). The derivation then proceeds analo-gously to the case of upper bound constraints. Solution via RatioDCA. Observe that both nu-merator and denominator of the tight relaxation (14) are one-homogeneous d.c. functions and thus we can apply the RatioDCA of Section 4. The crucial step in the algorithm is solving the inner problem (line 3). For both (14) and the tight relaxation of the constrained balanced graph cut problem, it has the form for c 1  X  R and c 2  X  R m . We solve this problem via the following equivalent dual problem.
 Lemma 3 The inner problem (15) is equivalent to where ( A X  ) i := P j w ij (  X  ij  X   X  ji ) , P R m projection on the positive orthant and S m is the sim-plex S m = { v  X  R m | v i  X  0 , P m i =1 v i = 1 } . Proof: First we replace the inner problem (15) by the modified problem min Given a solution f  X  of (16), a solution of (15) can be obtained via f  X  / k f  X  k 2 , which can be shown using the 1-homogeneity of the objective (15). We then derive the dual problem as follows: min = min = max where ( A X  ) i := P j w ij (  X  ij  X   X  ji ). The optimization over f has the solution Plugging f into the objective and using that D This dual problem can be solved efficiently using FISTA (Beck &amp; Teboulle, 2009), a proximal gradi-ent method with guaranteed convergence rate O ( 1 k 2 where k is the number of steps. The resulting explicit steps in FISTA with B  X  (1) = { x  X  R || x |  X  1 } to solve the inner problem are given below.
 FISTA for the inner problem The most expensive part of each iteration of the algo-rithm is a sparse matrix multiplication, which scales linearly in the number of edges. To solve the first sub-problem in FISTA, we make use of the following fact: Lemma 4 Let x  X  R n and y := P R n arg min Proof: The proof is a straightforward but technical transformation of the KKT optimality conditions of the left problem into the ones of the right problem. Lemma 4 implies that the minimization problem can be solved via a standard projection onto the simplex, which can be computed in linear time (Kiwiel, 2007). Unconstrained version. In the unconstrained case of the maximum density problem, the tight relaxation (14) reduces to a convex-concave ratio. As remarked in Section 4 it can then be solved globally optimally with our method, which in this case is equivalent to Dinkelbach X  X  method (Dinkelbach, 1967). In every it-eration, we have to solve Note that here we used the fact that one can replace the L 2 norm constraint in the inner problem by a L  X  norm constraint, see the remark after Prop. 4. The following lemma shows that (17) can be rewritten as a s -t -min-cut-problem, which shows that the procedure is similar to the method of Goldberg (1984).
 Lemma 5 Problem (17) is equivalent to the problem with V 0 = V  X  X  s,t } , H := u  X  R n + , k u k  X   X  1 and some non-negative weights w 0 ij , i,j  X  V 0 . Proof: Note that adding constant terms to the ob-jective does not change the minimizer. We rewrite
X =
X where we have used that f  X  H , where H := u  X  R n + , k u k  X   X  1 . We define the graph as V 0 V  X  X  s,t } and the weight matrix W 0 with and can rewrite the problem as which is a s -t -mincut.
 The above problem can be efficiently solved, e.g., using the pseudo-flow algorithm of Hochbaum (1998). We empirically evaluate the performance of our ap-proach on local clustering and community detection problems. Our goal is to address the following ques-tions: (i) In terms of the original objective of the frac-tional set program, how does the locally optimal so-lution of our tight relaxation compare to the globally optimal solution of a loose relaxation? (ii) How good is our quality guarantee (Theorem 3), i.e. how often does our method improve a given sub-optimal solution obtained by another method? In all experiments we start the RatioDCA with 10 dif-ferent random initializations and report the result with smallest objective value. Regarding the parameter  X  from Theorem 2, it turns out that best results are ob-tained by first solving the unconstrained case (  X  = 0) and then increasing  X  sequentially, until all constraints are fulfilled. In principle, this strategy could also be used to deal with soft or noisy constraints, however we focus here on the case of hard constraints.
 Local clustering. We first consider the local nor-malized cut problem, where s  X  V is a given seed vertex. We evaluate our approach (denoted as CFSP) against the Local Spectral (LS) method by Mahoney et al. (2012) and the Lazy Random Walk (LRW) by Andersen &amp; Lang (2006) on large social networks of the Stanford Large Network Dataset Collection (Leskovec).
 In Mahoney et al. (2012), a spectral-type relaxation is derived for (18) that can be solved globally optimally. The resulting continuous solution is then transformed into a set via optimal thresholding. However, con-trary to our method this is not guaranteed to yield a set that satisfies both the seed and volume con-straints. Hence Mahoney et al. (2012) suggest, at the cost of losing their approximation guarantees, to per-form constrained optimal thresholding which consid-ers only thresholds that yield feasible sets. In a re-cent generalization of their work, Hansen &amp; Mahoney (2012) compute a sequence of locally-biased eigenvec-tors, the first of which corresponds to the solution of the spectral-type relaxation of Mahoney et al. (2012). We use the code of Hansen &amp; Mahoney (2012) to com-pute the solution of LS in our experiments. The local clustering technique of Andersen &amp; Lang (2006) ex-plores the graph locally by performing a lazy random walk with the transition matrix M = 1 2 I + WD  X  1 , where D is the degree matrix of the graph and the ini-tial distribution is concentrated on the seed set. Under some conditions on the seed set, it is shown that af-ter a specified number of steps optimal thresholding of the random walk vector yields a set with  X  X ood X  nor-malized Cheeger cut. However, they cannot guarantee that the resulting set contains the seed. For a fair com-parison, we compute the full sequence of random walk vectors until the stationary distribution is reached, and in each step perform constrained optimal thresholding according to the normalized cut objective.
 For each dataset we generate 10 random seeds. In or-der to ensure that meaningful intervals for the volume constraint are explored, we first solve the local clus-tering problem only with the seed constraint. Treat-ing this as the  X  X nconstrained X  solution C 0 , we then repeat the experiment with upper bounds of the form Table 1 shows mean and standard deviation of the nor-malized cut values averaged over the 10 different ran-dom trials (seeds) and average runtime over the dif-ferent runs and volume constraints. To demonstrate the quality guarantee (Theorem 3) we also initialize CFSP with the solution of LS and LRW. Our method CFSP consistently outperforms the competing meth-ods by large margins and always finds solutions that satisfy all constraints. In some cases CFSP initialized with LS or LRW outperforms CFSP with 10 random initializations. While LRW is very fast, the obtained normalized cuts are far from being competitive. Note that CFSP still performs better if one uses for the optimal thresholding the normalized Cheeger cut for which LRW has been designed. This is shown in Table 2 where we compare the normalized Cheeger cut of our solutions (note that we optimized the normalized cut) to the solution obtained by the Lazy Random Walk method where we threshold in each step according to the normalized Cheeger cut objective.
 Community detection. We evaluate our approach for local community detection according to (10). The task is to extract communities around given seed sets in a co-author network constructed from the DBLP publication database. Each node in the network rep-resents a researcher and an edge between two nodes indicates a common publication. The weights of the graph are defined as w ij = P l  X  P denotes the set of publications of authors i and j and A l denote the sets of authors for publication l , i.e. the weights represent the total contribution to shared pa-pers. This normalization avoids the problem of giving high weight to a researcher who has publications that have a large number of authors, which usually does not reflect close collaboration with all co-authors. To avoid finding a trivial densely connected group of researchers with few connections to the rest of the au-thors, we further restrict the graph by considering only authors with at least two publications and maximum distance two from the seed set. As volume function in (10), we use the volume of the original graph in order to further enforce densely connected components. We perform local community detection with the size constraint | C | X  20 and three different seed sets J 1 = { P. Bartlett , P. Long, G. Lugosi } , J 2 = { E. Candes , J. Tropp } and J 3 = { O. Bousquet } . J 1 consists of well-known researchers in learning theory, and all members of the detected community work in this area. To validate this, we counted the number of publica-tions in the two main theory conferences COLT and ALT. On average each author has 18.2 publications in these two conferences (see Table 3 for more details). The seeds J 2 yield a community of key scientists in the field of sparsity such as T. Tao, R. Baraniuk, J. Romberg, M. Wakin, R. Vershynin etc. The third community contains researchers who either are/were members of the group of B. Sch  X olkopf or have closely collaborated with his group.
 This work has been supported by DFG Excellence Cluster MMCI and ERC Starting Grant NOLEPRO.
 Andersen, R. and Lang, K. Communities from seed sets. In WWW , pp. 223 X 232, 2006.
 Andersen, R., Chung, F., and Lang, K. Local graph partitioning using pagerank vectors. In FOCS , pp. 475 X 486, 2006.
 Bach, F. Learning with submodular functions:
A convex optimization perspective. CoRR , abs/1111.6453, 2011.
 Beck, A. and Teboulle, M. Fast gradient-based algo-rithms for constrained total variation image denois-ing and deblurring problems. IEEE Trans. Image Processing , 18(11):2419 X 2434, 2009.
 Bresson, X., Laurent, T., Uminsky, D., and von
Brecht, J. H. Convergence and energy landscape for Cheeger cut clustering. In NIPS , pp. 1394 X 1402, 2012.
 Chung, F. A local graph partitioning algorithm using heat kernel pagerank. In WAW , pp. 62 X 75, 2009. Di Pillo, G. Exact penalty methods. In Spedicato, E. (ed.), Algorithms for Continuous Optimization , pp. 209 X 253. Kluwer, 1994.
 Dinkelbach, W. On nonlinear fractional programming. Management Science , 13(7):492 X 498, 1967.
 Fortunato, S. Community detection in graphs. Physics Reports , 486(3-5):75  X  174, 2010.
 Fujishige, S. Submodular Functions and Optimization . Elsevier, 2005.
 Gajewar, A. and Das Sarma, A. Multi-skill collabo-rative teams based on densest subgraphs. In SDM , pp. 165 X 176, 2012.
 Goldberg, A. V. Finding a maximum density sub-graph. Technical Report UCB/CSD-84-171, EECS Department, UC Berkeley, 1984.
 Hagen, L. and Kahng, A. B. Fast spectral methods for ratio cut partitioning and clustering. In ICCAD , pp. 10 X 13, 1991.
 Hansen, T. and Mahoney, M. Semi-supervised eigen-vectors for locally-biased learning. In NIPS , pp. 2537 X 2545, 2012.
 Hein, M. and B  X uhler, T. An inverse power method for nonlinear eigenproblems with applications in 1-spectral clustering and sparse PCA. In NIPS , pp. 847 X 855, 2010.
 Hein, M. and Setzer, S. Beyond spectral clustering -tight relaxations of balanced graph cuts. In NIPS , pp. 2366 X 2374, 2011.
 Hiriart-Urruty, J.-B. and Lemar  X echal, C. Fundamen-tals of Convex Analysis . Springer, 2001.
 Hochbaum, D. S. The pseudoflow algorithm and the pseudoflow-based simplex for the maximum flow problem. In IPCO , pp. 325 X 337, 1998.
 Khot, S. Ruling out PTAS for graph min-bisection, dense k-subgraph, and bipartite clique. SIAM J. Comput. , 36(4), 2006.
 Khuller, S. and Saha, B. On finding dense subgraphs. In ICALP , pp. 597 X 608, 2009.
 Kiwiel, K. On Linear-Time algorithms for the contin-uous quadratic knapsack problem. J. Opt. Theory Appl. , 134(3):549 X 554, 2007.
 Leskovec, J. Stanford large network dataset col-lection. URL http://snap.stanford.edu/data/ index.html .
 Mahoney, M. W., Orecchia, L., and Vishnoi, N. K. A local spectral method for graphs: With applications to improving graph partitions and exploring data graphs locally. JMLR , 13:2339 X 2365, 2012.
 Maji, S., Vishnoi, N. K., and Malik, J. Biased normal-ized cuts. In CVPR , pp. 2057 X 2064, 2011.
 Pothen, A., Simon, H. D., and Liou, K.-P. Partitioning sparse matrices with eigenvectors of graphs. SIAM J. Matrix Anal. Appl. , 11(3):430 X 452, 1990.
 Rangapuram, S. S. and Hein, M. Constrained 1-spectral clustering. In AISTATS , pp. 1143 X 1151, 2012.
 Saha, B., Hoch, A., Khuller, S., Raschid, L., and
Zhang, X.-N. Dense subgraphs with restrictions and applications to gene annotation graphs. In RE-COMB , pp. 456 X 472, 2010.
 Shi, J. and Malik, J. Normalized cuts and image seg-mentation. IEEE Trans. Patt. Anal. Mach. Intell. , 22(8):888 X 905, 2000.
 Spielman, D. A. and Teng, S.-H. Nearly-linear time algorithms for graph partitioning, graph sparsifica-tion, and solving linear systems. In STOC , pp. 81 X  90, 2004.
 Szlam, A. and Bresson, X. Total variation and Cheeger cuts. In ICML , pp. 1039 X 1046, 2010. von Luxburg, U. A tutorial on spectral clustering. Statistics and Computing , 17:395 X 416, 2007.
 Wagstaff, K., Cardie, C., Rogers, S., and Schroedl,
S. Constrained K-means clustering with background
 Thomas B  X uhler tb@cs.uni-saarland.de Saarland University, Saarbr  X ucken, Germany Syama Sundar Rangapuram srangapu@mpi-inf.mpg.de Simon Setzer setzer@mia.uni-saarland.de Matthias Hein hein@cs.uni-saarland.de Saarland University, Saarbr  X ucken, Germany Graph-based data appear in manifold ways in learning problems -either the data have already graph struc-ture as in the case of social networks and biological networks or a similarity graph is constructed using a similarity measure based on features of the data. Sev-eral graph-based problems in clustering and commu-nity detection can be modelled as the optimization of a ratio of set functions (referred to here as fractional set program). Prominent examples are the normalized cut problem, from which the popular spectral cluster-ing method is derived (Shi &amp; Malik, 2000), and the maximum density subgraph problem, which has appli-cations in community detection (Fortunato, 2010) and bioinformatics (Saha et al., 2010).
 It turns out that in practice often additional back-ground or domain knowledge about the learning prob-lem is available. Such prior knowledge can then be incorporated as constraints into the optimization prob-lem. In the case of clustering, Wagstaff et al. (2001) are the first to show how prior information given in the form of must-link and cannot-link constraints be-tween vertices can be integrated into the k -means al-gorithm. Recently, Rangapuram &amp; Hein (2012) pro-posed a generalization of the normalized cut problem that can handle must-link and cannot-link constraints. In the recent work of Mahoney et al. (2012), locality constraints in the form of a seed set and volume con-straint have been integrated into the normalized cut formulation. Furthermore, Khuller &amp; Saha (2009) and Saha et al. (2010) considered size and distance con-straints for the maximum density subgraph problem. Since the above-mentioned combinatorial problems are NP-hard, the standard approach is to consider con-vex or spectral relaxations which can be solved glob-ally optimally in polynomial time. Due to its prac-tical efficiency the spectral relaxation is very popular in machine learning, e.g. spectral clustering (Hagen &amp; Kahng, 1991; Shi &amp; Malik, 2000). However, it is of-ten quite loose and thus leads to a solution far away from the optimal one of the original problem. More-over, spectral-type relaxations (Mahoney et al., 2012) fail to guarantee that the constraints which encode the prior knowledge are satisfied.
 In another line of work (Hein &amp; B  X uhler, 2010; Szlam &amp; Bresson, 2010; Hein &amp; Setzer, 2011; Bresson et al., 2012), it has been shown that tight continuous relax-ations exist for all balanced graph cut problems and the normalized cut subject to must-link and cannot-link constraints (Rangapuram &amp; Hein, 2012). A tight relaxation means that the continuous and the com-binatorial optimization problem are equivalent in the sense that the optimal values agree and the optimal solution of the combinatorial problem can be obtained from the continuous solution. While the resulting al-gorithms provide no guarantee to yield the globally optimal solution, the standard loose relaxations are outperformed by a large margin in practice.
 In this paper we show that any constrained minimiza-tion problem of a ratio of non-negative set functions allows a tight relaxation into a continuous optimiza-tion problem. This result together with our efficient minimization techniques enables the easy integration of prior information in form of constraints into many problems in graph-based clustering and community de-tection. While the general framework introduced in this paper is applicable to all problems discussed so far, we will focus on two particular applications: lo-cal clustering by constrained balanced graph cuts, and community detection via constrained densest subgraph problems. Compared to previous work, the algorithms developed in this paper are the first to guarantee that all given constraints are fulfilled by the obtained solu-tion. Note that in principle our method could also be applied to a setting with soft or noisy constraints, how-ever we will focus here on the case of hard constraints. In the experimental section we will show the supe-rior performance compared to state of the art methods (Andersen &amp; Lang, 2006; Mahoney et al., 2012). All proofs can be found in the supplementary material. In the following, G = ( V,W ) denotes an undirected, weighted graph with a non-negative, symmetric weight matrix W  X  R n  X  n , where n = | V | . Moreover, by assigning a non-negative weight g i to each vertex i , we can define the general volume of a subset A  X  V as vol g ( A ) = P i  X  A g i . As special cases, we obtain for g = 1 the cardinality | A | and for g i equal to the degree d = P j  X  V w ij the classical volume vol( A ) = vol d ( A ). Furthermore, A = V \ A denotes the complement of A . The balanced graph cut problem is a well-known problem in computer science with applications rang-ing from parallel computing to image segmentation (Pothen et al., 1990; Shi &amp; Malik, 2000). A very popu-lar balanced graph cut criterion is the normalized cut 1 , where cut( C, C ) := P i  X  C,j  X  C w ij . The spectral re-laxation of the normalized cut leads to the popular spectral clustering method (von Luxburg, 2007). A related criterion is the normalized Cheeger cut,
NCC( C, C ) = More general balanced graph cuts were studied by Hein &amp; Setzer (2011). In practice, often additional in-formation about the desired solution is available which can be incorporated into the problem via constraints. This motivates us to consider a more general class of problems where one optimizes a ratio of set functions 2 subject to constraints. In the following, we discuss two examples of constrained problems in network analysis. Constrained balanced graph cuts for local clus-tering. Recently, there has been a strong interest in balanced graph cut methods for local clustering. Start-ing with the work of Spielman &amp; Teng (2004), initially, the goal was to develop an algorithm that finds a sub-set near a given seed vertex with small normalized cut or normalized Cheeger cut value with running time lin-ear in the size of the obtained cluster. The proposed algorithm and subsequent work (Andersen et al., 2006; Chung, 2009) use random walks to explore the graph locally, without considering the whole graph. Algo-rithms of this type have been applied for community detection in networks (Andersen &amp; Lang, 2006). In contrast, Mahoney et al. (2012) give up the runtime requirement and formulate the task as an explicit op-timization problem, where one aims at finding the op-timal normalized cut subject to a seed constraint and an upper bound on the volume of the set containing the seed set. Again, the idea is to find a local clus-ter around a given seed set. Motivated by the stan-dard spectral relaxation of the normalized cut prob-lem, they derive a spectral-type relaxation which is biased towards solutions fulfilling the seed constraint. Their method has been successfully applied in semi-supervised image segmentation (Maji et al., 2011) and for community detection around a given query set (Ma-honey et al., 2012). However, while they provide an ap-proximation guarantee for their relaxation, they can-not guarantee that the returned solution satisfies seed and volume constraints.
 In this paper we consider an extended version of the problem of Mahoney et al. (2012). Let J denote the set of seed vertices, b S a symmetric balancing function (e.g. b S ( C ) = vol d ( C ) vol d ( C ) for the normalized cut) and let vol g ( C ) be the general volume of set C , where g  X  R n + are vertex weights. The general local cluster-ing problem can then be formulated as The choice of the balancing function b S allows the user to influence the trade-off between getting a partition with small cut and a balanced partition. One could also combine this with must-and cannot-link con-straints (see Rangapuram &amp; Hein, 2012) or add even more complex constraints such as an upper bound on the diameter of C . However, in order to compare to the method of Mahoney et al. (2012), we restrict our-selves in this paper to the normalized cut with volume Constrained local community detection. A sec-ond related problem is constrained local community detection. In community detection it makes more sense to find a highly connected set instead of em-phasizing the separation to the remaining part of the graph by minimizing the cut. Thus, we are search-ing for a set C which has high association, defined as assoc( C ) = P i,j  X  C w ij . Dividing the association of C by its size yields the density of C . The subgraph of maximum density can be computed in polynomial time (Goldberg, 1984). However, the obtained commu-nities in the unconstrained problem are typically either too large or too small, which calls for size constraints. Note that the introduction of such constraints makes the problem NP-hard (Khuller &amp; Saha, 2009). A general class of (local) community detection prob-lems can thus be formulated as where g,h  X  R n + are vertex weights. This formu-lation generalizes the above-mentioned density-based approaches by replacing the denominator by a general volume function vol g . One can use the vertex weights g to bias the obtained community towards one with desired properties by assigning small weights to ver-tices which one would prefer to occur in the solution and larger weights to ones which are less preferred. The problem (2) with only lower bound constraints has been considered in team selection (Gajewar &amp; Das Sarma, 2012) and bioinformatics (Saha et al., 2010) where constant factor approximation algorithms were developed. However, in the case of equality and up-per bound constraints the problem is very hard even when using only cardinality constraints (i.e., h i = 1), and it has been shown that there is no polynomial time approximation scheme in these cases (Khot, 2006; Khuller &amp; Saha, 2009). Our method can handle such hard upper bound and equality constraints. In the ex-periments we show results for a community detection problem with a specified query set J and an upper bound on the size for a co-author network.
 Note that for the choice of vol g ( C ) = vol d ( C ) in (2) and b S ( C ) = vol d ( C ) in (1), the problem (2) is equiva-lent to (1) if we choose the same constraints. Contributions of this paper. We show that all constrained non-negative fractional set programs have an equivalent tight continuous relaxation. This gen-eral result enables the integration of prior information in form of constraints into clustering and community detection problems. In particular, it allows us to de-rive efficient algorithms for problems (1) and (2). Our algorithms consistently outperform competing meth-ods (Andersen &amp; Lang, 2006; Mahoney et al., 2012). Moreover, we are not aware of any other methods for the above problems which can guarantee that the so-lution always satisfies volume and seed constraints. Although the tight relaxation results in Hein &amp; Setzer (2011) and Rangapuram &amp; Hein (2012) encompass a large class of problems, they are not applicable to the problems considered in this paper because of the fol-lowing limitations: First, tight relaxations were shown by Hein &amp; Setzer (2011) only for a ratio of symmet-ric non-negative set functions, where the numerator is restricted to be submodular . We extend the results to arbitrary ratios of non-negative set functions without any restrictions concerning symmetry or submodular-ity. Second, only equality constraints for non-negative set functions restricted to be either submodular or su-permodular could be handled by Rangapuram &amp; Hein (2012). We generalize this to inequality constraints without any restrictions on the constraint set functions in order to handle the constraints in (1) and (2). The problems discussed in the last section can be writ-ten in the following general form: where b R, b S, c M i : 2 V  X  R are set functions on a set V = { 1 ,...,n } . We assume here that b R, b S are non-negative and that b R (  X  ) = b S (  X  ) = 0. No assumptions are made on the set functions c M i , in particular they are not required to be non-negative. Thus also lower bound constraints can be written in the above form. Moreover, the formulation in (3) also encompasses the subset constraint J  X  C in (1) and (2) as it can be written as equality constraint | J | X  X  J  X  C | = 0. Al-ternatively, we will discuss a direct integration of the subset constraint into the objective in Section 5. The connection between the set-valued and the contin-uous space is achieved via thresholding. Let f  X  R n , and we assume wlog that f is ordered in ascending order f 1  X  f 2  X  X  X  X  X  X  f n . One defines the sets We frequently make use of this notation in the fol-lowing. Furthermore, we use 1 C  X  R n to denote the indicator vector of the set C , i.e. the vector which is 1 at entry j if j  X  C and 0 otherwise. A key tool for the derivation of the results of this paper is the Lovasz extension as a way to extend a set function (seen as function on the hypercube) to a function on R n . Definition 1 Let b R : 2 V  X  R be a set function with b R (  X  ) = 0 , and f  X  R n in ascending order f 1  X  f 2  X  X  X   X  f n . The Lovasz extension R : R n  X  R of b R is Note that R ( 1 C ) = b R ( C ) for all C  X  V , i.e. R is indeed an extension of b R from 2 V to R n . In the following, we always use the hat-symbol ( and omit it for the corresponding Lovasz extension. A particular important class of set functions are sub-modular set functions since their Lovasz extension is convex (Bach, 2011).
 Definition 2 A set function b R : 2 V  X  R is submod-ular if for all A,B  X  V , b R ( A  X  B ) + b R ( A  X  B )  X  b equality holds true, and modular if we have equality. Unconstrained fractional set programs. Using the property of the Lovasz extension that R ( 1 C ) = b R ( C ) for all C  X  V , one can directly observe that the following continuous fractional program is a relaxation of the unconstrained version of problem (3) The following theorem shows that the relaxation is in fact tight, in the sense that the optimal values agree and the solution of the set-valued problem can be com-puted from the solution of the continuous problem. Note that given a vector f  X  R n for the continuous problem, one can construct a set C 0 by computing where the sets C i are defined in (4). We refer to this process as optimal thresholding .
 Theorem 1 Let b R, b S : 2 V  X  R be non-negative set functions and R,S : R n  X  R their Lovasz extensions, respectively. Then, it holds that Moreover, it holds for all f  X  R n + , R ( f ) S ( f ) can be found by optimal thresholding. Let furthermore b R ( V ) = b S ( V ) = 0 , then all the above statements hold if one replaces R n + with R n .
 Note that no assumptions except non-negativity are made on b R and b S -every non-negative fractional set program has a tight relaxation into a continuous frac-tional program. The efficient minimization of the con-tinuous objective will be the topic of Section 4. A slightly technical generalization of Theorem 1 can be found in the supplementary material.
 Constrained fractional set programs. To solve the constrained fractional set program (3) we make use of the concept of exact penalization (Di Pillo, 1994), where the main idea is to transform a given constrained optimization problem into an equivalent unconstrained one by adding a penalty term. We use the same idea for our constrained fractional set pro-grams and define the penalty set function for a con-straint c M i ( C )  X  k i as th constraint and otherwise increasing with increasing infeasibility. The special treatment of the empty set in the definition of b T i is a technicality required for the Lovasz extension. Defining b T ( C ) := P K i =1 b T i ( C ), we can now formulate a modified problem We will show that using a feasible set of (3) one can compute a  X  such that (6) is equivalent to the orig-inal constrained problem. Once we have established the equivalence, we can then apply Theorem 1, noting that b T is a non-negative set function. This leads to the main result of this paper showing a tight relaxation of all problems of form (3) where b R, b S are non-negative set functions. In the following, the constant  X  quanti-fies a  X  X inimum value X  of b T i on the infeasible sets: For example, if c M ( C ) = | C | , then  X  is equal to 1. If c M ( C ) = vol g ( C ) and all vertex weights g i are rational numbers which are multiples of a fraction 1  X  , X   X  N then  X   X  1  X  . Note that in practice, the constant  X  and the parameter  X  introduced in the following are never explicitly computed (see experimental section). Theorem 2 Let b R, b S : 2 V  X  R be non-negative set functions and R , S their Lovasz extensions. Let C 0  X  V be feasible and b S ( C 0 ) &gt; 0 . Denote by T the Lovasz extension of b T . Then, for  X  &gt; b R ( C 0 ) Moreover, for any f  X  R n + with Q  X  ( f ) &lt; b Q  X  ( C the given  X  , we have Q  X  ( f )  X  min i =1 ,...,n b Q  X  ( C the minimizing set on the right hand side is feasible. Note that Theorem 2 implies that the set found by optimal thresholding of the solution of the continuous program is guaranteed to satisfy all constraints. We are not aware of any other method which can give the same guarantee for the problems (1) and (2). The continuous optimization problems in Theorems 1 and 2 have the form where R and S are non-negative. The fact that they are the Lovasz extensions of set functions b R, b S also im-plies that they are one-homogeneous, see Bach (2011). We now apply a slightly modified version of a result from Hein &amp; Setzer (2011).
 Proposition 1 Every set function b S with b S (  X  ) = 0 can be written as b S = b S 1  X  b S 2 , where S 1 and S submodular and b S 1 (  X  ) = b S 2 (  X  ) = 0 . The Lovasz exten-sion S can be written as difference of convex functions. The above result implies that (7) can be written as ratio of differences of convex functions (d.c.), i.e. R = R 1  X  R 2 with R 1 , R 2 convex, and similarly for S . As the proof of Proposition 1 is constructive, the explicit form of this decomposition can be calculated. We can now use a modification of the RatioDCA which has re-cently been proposed as an algorithm for minimizing a non-negative ratio of one-homogeneous d.c. functions (Hein &amp; Setzer, 2011). This modification is necessary as the problems in Theorem 1 and 2 require optimiza-tion over the positive orthant. We report the modified version in order to make the paper self-contained. RatioDCA Minimization of a non-negative ratio of one-homogeneous d.c functions over R n + 1: Initialization: f 0  X  R n + ,  X  0 = Q ( f 0 ) 2: repeat 3: f l +1 = arg min 5: until It is shown in the supplement that properties such as the fact that the sequence Q ( f l ) is either strictly de-creasing or the sequence terminates carry over from Hein &amp; Setzer (2011). The norm constraint of the inner problem is necessary as otherwise the problem would be unbounded from below. However, the choice of the norm plays no role in the proof and any norm can be chosen. Moreover, in the special case where the one-homogeneous function R is convex and S is con-cave, the RatioDCA reduces to Dinkelbach X  X  method from fractional programming (Dinkelbach, 1967) and therefore computes the global optimum. In the gen-eral case, convergence to the global optimum cannot be guaranteed. However, we can provide a quality guar-antee : RatioDCA either improves a given feasible set or stops after one iteration.
 Theorem 3 Let A be a feasible set and  X  &gt; b sult of RatioDCA after initializing with the vector 1 A , and let C f  X  denote the set found by optimal thresh-olding of f  X  . Either RatioDCA terminates after one The above theorem implies that all constraints of the original constrained fractional set program are fulfilled by the set C f  X  returned by RatioDCA. The framework introduced in this paper allows us to derive tight relaxations of all problems discussed in Section 2. In the following, we will derive a tight re-laxation of the local community detection problem For the constrained balanced graph cut problem, the tight relaxation can be found in a very similar way and is thus omitted here. Moreover, incorporating con-straints of the form vol h ( C )  X  k in both problems is similar and outlined in the supplementary material. First, we integrate the volume constraint via a penalty term, see (6), which yields the equivalent problem where b T k is given as b T k ( C ) = max { 0 , vol h We could reformulate the seed constraint J  X  C as inequality constraint | J  X  C | X  X  J | X  0 and add a similar penalty function to the numerator of (9). However, using the structure of the problem, a more direct way to incorporate the seed constraint is possible. It holds that (9) has the equivalent form where k 0 = k  X  vol h ( J ). Solutions C  X  of (9) and A  X  (10) are related via C  X  = A  X   X  J . In order to derive the tight relaxation via Theorem 1, we need the Lovasz extension of the set functions in (10). For technical reasons, we replace the constant set functions vol g ( J ) and assoc( J ) by vol g ( J ) b P ( A ) and assoc( J ) spectively, where b P is defined as b P ( A ) = 1 for A 6 =  X  and b P (  X  ) = 0. This leads to the problem The only difference to (10) lies in the treatment of the empty set. Note that with 0 0 :=  X  the empty set can never be optimal for problem (11). Given an optimal solution A  X  of (11), one then either considers either A  X   X  J or J , depending on whichever has lower objective, which then implies equivalence to (10). The resulting tight relaxation will be a minimization problem over R m with m = | V \ J | and we assume wlog that the first m vertices of V are the ones in V \ J . Moreover, we use the notation f max = max i =1 ,...,m f i for f  X  R m , and d ( A ) i = P j  X  A w ij . The following Lo-vasz extensions are useful: For the sake of brevity, we do not specify the convex function T (2) k 0 . The formula for a subgradient of T (2) which we need in RatioDCA is given in the supple-mentary material. The above Lovasz extensions lead to the following tight relaxation of (11): where R 1 ( f ) =  X  ( g i ) m i =1 +  X  ( h i ) m i =1 ,f  X  + vol S R 2 ( f ) =  X T Solution via RatioDCA. Observe that both nu-merator and denominator of the tight relaxation (12) are one-homogeneous d.c. functions and thus we can apply the RatioDCA of Section 4. The crucial step in the algorithm is solving the inner problem (line 3). For both (12) and the tight relaxation of the constrained balanced graph cut problem, it has the form for c 1  X  R and c 2  X  R m . We solve this problem via the following equivalent dual problem.
 Lemma 1 The above inner problem is equivalent to where ( A X  ) i := P j w ij (  X  ij  X   X  ji ) , P R m projection on the positive orthant in R m and S m is the simplex S m = { v  X  R m | v i  X  0 , P m i =1 v i = 1 } . This dual problem can be solved efficiently using FISTA (Beck &amp; Teboulle, 2009), a proximal gradi-ent method with guaranteed convergence rate O ( 1 k 2 where k is the number of steps. The explicit steps of FISTA for the above problem can be found in the sup-plementary material. The most expensive part of each iteration of the algorithm is a sparse matrix multipli-cation, which scales linearly in the number of edges. We empirically evaluate the performance of our ap-proach on local clustering and community detection problems. Our goal is to address the following ques-tions: (i) In terms of the original objective of the frac-tional set program, how does the locally optimal so-lution of our tight relaxation compare to the globally optimal solution of a loose relaxation? (ii) How good is our quality guarantee (Theorem 3), i.e. how often does our method improve a given sub-optimal solution obtained by another method? In all experiments we start the RatioDCA with 10 dif-ferent random initializations and report the result with smallest objective value. Regarding the parameter  X  from Theorem 2, it turns out that best results are ob-tained by first solving the unconstrained case (  X  = 0) and then increasing  X  sequentially, until all constraints are fulfilled. In principle, this strategy could also be used to deal with soft or noisy constraints, however we focus here on the case of hard constraints.
 Local clustering. We first consider the local nor-malized cut problem, where s  X  V is a given seed vertex. We evaluate our approach (denoted as CFSP) against the Local Spectral (LS) method by Mahoney et al. (2012) and the Lazy Random Walk (LRW) by Andersen &amp; Lang (2006) on large social networks of the Stanford Large Network Dataset Collection (Leskovec).
 In Mahoney et al. (2012), a spectral-type relaxation is derived for (13) that can be solved globally optimally. The resulting continuous solution is then transformed into a set via optimal thresholding. However, con-trary to our method this is not guaranteed to yield a set that satisfies both the seed and volume con-straints. Hence Mahoney et al. (2012) suggest, at the cost of losing their approximation guarantees, to per-form constrained optimal thresholding which consid-ers only thresholds that yield feasible sets. In a re-cent generalization of their work, Hansen &amp; Mahoney (2012) compute a sequence of locally-biased eigenvec-tors, the first of which corresponds to the solution of the spectral-type relaxation of Mahoney et al. (2012). We use the code of Hansen &amp; Mahoney (2012) to com-pute the solution of LS in our experiments. The local clustering technique of Andersen &amp; Lang (2006) ex-plores the graph locally by performing a lazy random walk with the transition matrix M = 1 2 I + WD  X  1 , where D is the degree matrix of the graph and the ini-tial distribution is concentrated on the seed set. Under some conditions on the seed set, it is shown that af-ter a specified number of steps optimal thresholding of the random walk vector yields a set with  X  X ood X  nor-malized Cheeger cut. However, they cannot guarantee that the resulting set contains the seed. For a fair com-parison, we compute the full sequence of random walk vectors until the stationary distribution is reached, and in each step perform constrained optimal thresholding according to the normalized cut objective.
 For each dataset we generate 10 random seeds. In or-der to ensure that meaningful intervals for the volume constraint are explored, we first solve the local clus-tering problem only with the seed constraint. Treat-ing this as the  X  X nconstrained X  solution C 0 , we then repeat the experiment with upper bounds of the form Table 1 shows mean and standard deviation of the nor-malized cut values averaged over the 10 different ran-dom trials (seeds) and average runtime over the dif-ferent runs and volume constraints. To demonstrate the quality guarantee (Theorem 3) we also initialize CFSP with the solution of LS and LRW. Our method CFSP consistently outperforms the competing meth-ods by large margins and always finds solutions that satisfy all constraints. In some cases CFSP initialized with LS or LRW outperforms CFSP with 10 random initializations. While LRW is very fast, the obtained normalized cuts are far from being competitive. In the supplement we show that CFSP still performs better if one uses for the optimal thresholding the normalized Cheeger cut for which LRW has been designed.
 Community detection. We evaluate our approach for local community detection according to (8). The task is to extract communities around given seed sets in a co-author network constructed from the DBLP publication database. Each node in the network rep-resents a researcher and an edge between two nodes indicates a common publication. The weights of the graph are defined as w ij = P l  X  P denotes the set of publications of authors i and j and A l denote the sets of authors for publication l , i.e. the weights represent the total contribution to shared pa-pers. This normalization avoids the problem of giving high weight to a researcher who has publications that have a large number of authors, which usually does not reflect close collaboration with all co-authors. To avoid finding a trivial densely connected group of researchers with few connections to the rest of the au-thors, we further restrict the graph by considering only authors with at least two publications and maximum distance two from the seed set. As volume function in (8), we use the volume of the original graph in order to further enforce densely connected components. We perform local community detection with the size constraint | C | X  20 and three different seed sets J 1 = { P. Bartlett , P. Long, G. Lugosi } , J 2 = { E. Candes , J. Tropp } and J 3 = { O. Bousquet } . J 1 consists of well-known researchers in learning theory, and all members of the detected community work in this area. To validate this, we counted the number of publica-tions in the two main theory conferences COLT and ALT. On average each author has 18.2 publications in these two conferences (see Table 3 in the supplemen-tary material). The seeds J 2 yield a community of key scientists in the field of sparsity such as T. Tao, R. Baraniuk, J. Romberg, M. Wakin, R. Vershynin etc. The third community contains researchers who either are/were members of the group of B. Sch  X olkopf or have closely collaborated with his group.
 This work has been supported by DFG Excellence Cluster MMCI and ERC Starting Grant NOLEPRO.
 Andersen, R. and Lang, K. Communities from seed sets. In WWW , pp. 223 X 232, 2006.
 Andersen, R., Chung, F., and Lang, K. Local graph partitioning using pagerank vectors. In FOCS , pp. 475 X 486, 2006.
 Bach, F. Learning with submodular functions:
A convex optimization perspective. CoRR , abs/1111.6453, 2011.
 Beck, A. and Teboulle, M. Fast gradient-based algo-rithms for constrained total variation image denois-ing and deblurring problems. IEEE Trans. Image Processing , 18(11):2419 X 2434, 2009.
 Bresson, X., Laurent, T., Uminsky, D., and von
Brecht, J. H. Convergence and energy landscape for Cheeger cut clustering. In NIPS , pp. 1394 X 1402, 2012.
 Chung, F. A local graph partitioning algorithm using heat kernel pagerank. In WAW , pp. 62 X 75, 2009. Di Pillo, G. Exact penalty methods. In Spedicato, E. (ed.), Algorithms for Continuous Optimization , pp. 209 X 253. Kluwer, 1994.
 Dinkelbach, W. On nonlinear fractional programming. Management Science , 13(7):492 X 498, 1967.
 Fortunato, S. Community detection in graphs. Physics Reports , 486(3-5):75  X  174, 2010.
 Gajewar, A. and Das Sarma, A. Multi-skill collabo-rative teams based on densest subgraphs. In SDM , pp. 165 X 176, 2012.
 Goldberg, A. V. Finding a maximum density sub-graph. Technical Report UCB/CSD-84-171, EECS Department, UC Berkeley, 1984.
 Hagen, L. and Kahng, A. B. Fast spectral methods for ratio cut partitioning and clustering. In ICCAD , pp. 10 X 13, 1991.
 Hansen, T. and Mahoney, M. Semi-supervised eigen-vectors for locally-biased learning. In NIPS , pp. 2537 X 2545, 2012.
 Hein, M. and B  X uhler, T. An inverse power method for nonlinear eigenproblems with applications in 1-spectral clustering and sparse PCA. In NIPS , pp. 847 X 855, 2010.
 Hein, M. and Setzer, S. Beyond spectral clustering -tight relaxations of balanced graph cuts. In NIPS , pp. 2366 X 2374, 2011.
 Khot, S. Ruling out PTAS for graph min-bisection, dense k-subgraph, and bipartite clique. SIAM J. Comput. , 36(4), 2006.
 Khuller, S. and Saha, B. On finding dense subgraphs. In ICALP , pp. 597 X 608, 2009.
 Leskovec, J. Stanford large network dataset col-lection. URL http://snap.stanford.edu/data/ index.html .
 Mahoney, M. W., Orecchia, L., and Vishnoi, N. K. A local spectral method for graphs: With applications to improving graph partitions and exploring data graphs locally. JMLR , 13:2339 X 2365, 2012.
 Maji, S., Vishnoi, N. K., and Malik, J. Biased normal-ized cuts. In CVPR , pp. 2057 X 2064, 2011.
 Pothen, A., Simon, H. D., and Liou, K.-P. Partitioning sparse matrices with eigenvectors of graphs. SIAM J. Matrix Anal. Appl. , 11(3):430 X 452, 1990.
 Rangapuram, S. S. and Hein, M. Constrained 1-spectral clustering. In AISTATS , pp. 1143 X 1151, 2012.
 Saha, B., Hoch, A., Khuller, S., Raschid, L., and
Zhang, X.-N. Dense subgraphs with restrictions and applications to gene annotation graphs. In RE-COMB , pp. 456 X 472, 2010.
 Shi, J. and Malik, J. Normalized cuts and image seg-mentation. IEEE Trans. Patt. Anal. Mach. Intell. , 22(8):888 X 905, 2000.
 Spielman, D. A. and Teng, S.-H. Nearly-linear time algorithms for graph partitioning, graph sparsifica-tion, and solving linear systems. In STOC , pp. 81 X  90, 2004.
 Szlam, A. and Bresson, X. Total variation and Cheeger cuts. In ICML , pp. 1039 X 1046, 2010. von Luxburg, U. A tutorial on spectral clustering. Statistics and Computing , 17:395 X 416, 2007.
 Wagstaff, K., Cardie, C., Rogers, S., and Schroedl,
S. Constrained K-means clustering with background
