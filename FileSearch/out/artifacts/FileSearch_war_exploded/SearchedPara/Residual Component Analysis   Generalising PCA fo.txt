 Alfredo A. Kalaitzis A.Kalaitzis@sheffield.ac.uk Neil D. Lawrence N.Lawrence@sheffield.ac.uk Probabilistic principal component analysis (PPCA) decomposes the covariance of a data vector y in R p , into a low-rank term and a spherical noise term. The underlying probabilistic model assumes that each da-tum is Gaussian distributed, where we assume the data is centred such that their mean is zero and W  X  R p  X  q , such that q &lt; p  X  1, imposes a reduced rank structure on the covariance. The log-likelihood of the centered data set Y in R n  X  p with n data points and p features, can be maximized (Tipping &amp; Bishop, 1999) with the result that W ML = U q L q R &gt; , where U q are the q principal eigenvectors of the sample covariance,  X  S = n  X  1 Y &gt; Y , and L q is a diagonal matrix with elements ` the sample covariance and  X  2 is the noise variance. This maximum-likelihood solution is rotation invari-ant; that is, R is an arbitrary rotation matrix. As a result, the matrix W spans the principal subspace of the data and the model is known as probabilistic -PCA (PPCA). Underlying this model is an assumption that the data set is represented by Y = XW &gt; + E , where X in
R n  X  q is the matrix of q -dimensional latent variables and E is a matrix of noise variables e i,j  X  N (0 , X  2 ). The marginal log-likelihood above is obtained by plac-ing an isotropic prior independently on the elements of X , x i,j  X  X  (0 , 1).
 Lawrence (2005) showed that the PCA solution is also obtained for log-likelihoods of the form which is recovered when we marginalize the loadings W , instead of latent variables X , with a Gaussian isotropic prior. This is the dual form of probabilis-tic PCA, also termed as probabilistic principal coor-dinate analysis , as this maximum likelihood solution solves for the latent coordinates X ML = U 0 q L q R &gt; stead of the principal subspace basis. Here, U 0 q the first q principal eigenvectors of the inner product matrix p  X  1 YY &gt; with L q defined as before. Note that this Gaussian density is independent across data fea-tures rather than data points . So the correlation is expressed between data points. The underlying model is in fact a product of independent Gaussian processes (Rasmussen &amp; Williams, 2006) with linear covariance functions.
 Both primal and dual scenarios involve maximizing likelihoods of a similar covariance structure, namely when the covariance of the Gaussians is given by a low-rank term plus a spherical term, XX &gt; +  X  2 I (dual case). In this paper we consider a more general form of the covariance, given by XX &gt; +  X  , where  X  is a general positive definite matrix. We are motivated by scenarios where our data has already been partly ex-plained by the covariance matrix  X  and we wish to study the components of the residual variance. Our ideas can be applied in both primal and dual repre-sentations: the form to be used depends on what in-formation we wish to encode in  X  . 1.1. Motivating examples Consider the general functional form of a linear mixed-effects model with two factors and noise, that will serve as a conceptual reference point for the rest of the paper (see Figure 1(a)): where Z is a matrix of known covariates ( fixed effects) with some predictive power for Y , and X is a matrix of latent variables ( random effects). The loadings W and V can be marginalised with Gaussian isotropic priors, to recover the log-likelihood where  X  = ZZ &gt; +  X  2 I is positive-definite. Specifically, one might have Y in eq. (1) manifested as: (a) a set of protein activation signals under various ex-ternal stimuli (heterogeneous data), with V = I and Z as a set of covariates (equidimensional to Y ) sharing sparse conditional dependencies. Sparse dependencies are valuable for parsimonious modelling but might be confounded by latent effects X induced by the hetero-geneous experimental conditions on the measurements Y . This alludes to the instantiation  X  GL =  X   X  1 for sparse  X  , which recovers a low-rank plus sparse-inverse parameterisation of the covariance in eq. (2). A sparse precision amounts to a sparsely connected Gaussian Markov random field (GMRF) as the graphical model of Z ; that is, each z i, : is distributed from N ( 0 ,  X  where the precision matrix  X  is sparse (Lauritzen, 1996). (b) a set of n gene expression profiles, where each pro-file is a concatenated time-series of p 1 + p 2 timepoints, with p 1 timepoints sampled under control conditions and p 2 timepoints sampled under test conditions. In this scenario, the instantiation  X  Gram = K +  X  2 I , with K ij = k ( z i , z j ) as a general Gram matrix defined by some covariance function k : R D  X  R D  X  R , could help express temporal correlations in a time-series dataset. This gets close to the common practice of explicitly subtracting the result of a simpler model from the data and then analyzing the residual separately. (c) Y as a set of n patients X  gene expression mea-surements ( p genes), with Z as the genotype of each patient and X as the unobserved environmental effects (confounders), see Fusi et al. (2012).
 In all of these cases, one useful task would be to analyse the components of the residual XX &gt; for the dual case (or WW &gt; for the primal case; for brevity we refer mostly to the dual case), given  X  or some estimate thereof. This begs the question: Given  X  , how can we solve for X (respectively W )? And more importantly, for what instantiations of  X  can we formulate useful new algorithms for machine learning? . 1.2. Proposed approach The key theoretical result of this paper in Section 2, eq. (3) shows that the maximum-likelihood solution for X is simply based on a generalised eigenvalue problem (GEP) on the sample-covariance matrix. Hence, the low-rank term XX &gt; can be optimized for general  X  , with the only constraint being the positive-definiteness of  X  . We call this data analysis approach residual component analysis (RCA).
 Secondly, the RCA approach gives rise to a range of new algorithms suited for the aforementioned scenar-ios. For instance, for scenario (a) we propose an EM/RCA hybrid algorithm for estimating both the low-rank and sparse-inverse factors, see section 3. For scenario (b) we present a pure RCA treatment: the residual basis of interest is explored with a single ex-act estimate via the RCA GEP. We demonstrate the efficacy of the algorithms on biological and motion cap-ture datasets in Section 4. 1.3. Related work and connection to CCA The low-rank plus sparse-inverse parameterisation ex-tends the Graphical Lasso algorithm (GLASSO, Fried-man et al., 2008), which finds a MAP estimate of the covariance with an L1 regularization term on the pre-cision. Sparse-inverse structures capture relations be-tween variables that are not well characterized by low-rank forms. As such, the combination of sparse in-verse and low-rank can be a powerful one with appli-cations in computational biology and visualisation, as we demonstrate in Section 4. We also point to the work of Stegle et al. (2011) for a different approach based on a multiplicative (Kronecker product) structure in the covariance.
 In addition, we note a few more connections to well-studied algorithms for linear dimensionality reduc-tion: The obvious connection to PPCA is recovered by  X  pca =  X  2 I . Furthermore, a sparse 1  X  relates to the robust -PCA framework of Candes et al. (2009). Probabilistic CCA (Bach &amp; Jordan, 2005) is recovered (for a proof, see supp. material). Furthermore, if Y = standard shared latent space of Z found in CCA, the partitions of Y have their own associated private latent spaces of X 1 and X 2 , see Figure 1(b). This is in fact an instantiation of a general multi-view learning model (Ek et al., 2008), the linear case of which was more closely studied by Klami &amp; Kaski (2008) who termed it extended probabilistic-CCA . To optimise this model, an iterative treatment of RCA can be used: solve for V on one step by setting  X  cca + = in the GEP of the full data sample covariance; on the other step solve for each of W k ,k  X  { 1 , 2 } , by set-ting  X  cca + = V k V &gt; k +  X  2 I in the GEP of the sample covariance associated with Y k . This iterative-RCA al-gorithm is reminiscent of the expectation maximiza-tion (EM) algorithm for optimising extended PCCA , as both approaches maximize the likelihood by fitting components into the residual. More details on the CCA connection and iterative-RCA can be found in the supplementary material. We show the main results on the dual case, with no loss of generalisation on the primal case.
 Theorem . The maximum likelihood estimate of the parameter X in the likelihood model in eq. (2) , for positive-definite and invertible  X  , is where S is the solution to the GEP with its columns as the generalised eigenvectors and D is diagonal with the corresponding generalised eigen-values.
 Proof . The RCA log-likelihood is given by
L ( X ,  X  ) =  X  p 2 ln | K | X  1 2 tr( YY &gt; K  X  1 )  X  np where K = XX &gt; +  X  . Since  X  is positive-definite, we consider its eigen-decomposition  X  = U  X  U &gt; , (5) where U &gt; U = I and  X  is diagonal. The projection of the covariance onto this eigen-basis, scaled by the eigenvalues, is allows us to define  X  K =  X  X  X  X &gt; + I and its inverse Therefore from (7), we have | K | =  X  | K ||  X  | and Now we can rewrite the entire RCA log-likelihood in terms of the transformed variables  X  X and  X  Y , L (  X 
X ) =  X  p 2 ln(  X  | K ||  X  | )  X  1 2 tr(  X  Y  X  Y &gt;  X  K We know how to maximize this new form of the log-likelihood with respect to  X  X . Following a similar route to the maximum likelihood solution proof in Tipping &amp; Bishop (1999), the gradient gives the stationary point 1 p  X  Y  X  Y &gt;  X  K  X  1  X  X = By substituting the singular value decomposition  X  Woodbury matrix identity , we can see that maximi-sation relies on the regular eigenvalue problem Next we focus on relating the stationary point of to the solution for X and then we proceed by express-ing this eigenvalue problem in terms of YY &gt; : By the definition of  X  X , we obtain the factorisation where we have defined T = U X  1 2  X  V . Since  X  is in-vertible, we substitute for  X  Y and  X  V in (11) and use the inverse of (5) to recover the equivalent eigenvalue problem in the original dual-space To conclude the proof, we define S =  X   X  1 T to recover the desired symmetric form of the GEP Based on the factorisation of X in (12), now we can recover X up to an arbitrary rotation ( R , which for convenience is normally set to I ), via the first q gener-alised eigenvectors of p  X  1 YY &gt; , Commentary . Due to the algebraic symmetry be-tween the dual and primal formulations of the log-marginal likelihood in (2), one can easily extend the proof to the primal case. Specifically, the maximum likelihood solution of W in (1) has the same form Aside from  X  , we note a subtle difference from the PPCA solution for W : Whereas PPCA explicitly sub-tracts the noise variance from the q retained principal eigenvalues, RCA in (6) implicitly incorporates any noise terms into  X  and standardises them when it projects the total covariance onto the eigen-basis of  X  . Thus we get a reduction of unity from the retained generalised eigenvalues in (3). Again, for  X  = I the two solutions are identical.
 Finally, we state the posterior density for the RCA probabilistic model (primal case) and  X  y = 0 , In this section we show how to optimise the following generative model, summarised in Figure 2, where  X  is sampled from a Laplace prior density, Marginalising X , yields log p ( Y ,  X  ) = P n i =1 log {N ( y i, : | 0 , WW &gt; +  X  where q ( Z ) is the variational distribution and  X  =  X   X  1 +  X  2 I , which we wish to optimise for some known W . This is an intractable problem, so instead we op-timise the lower bound (15) in an EM fashion.
 E-step Replacing q ( Z ) with the posterior p ( Z | Y ,  X  for a current estimate  X  0 , amounts to the E-step for updating the posterior density of z n | y n with and z n z &gt; n = cov[ z | y ] +  X  z n  X  X  z n  X  &gt; . (18) M-step Then for fixed Z 0 , the only free parame-ter in the expected complete-data log-likelihood Q = E Z | Y (log p ( Z 0 ,  X  )) is  X  . Therefore, argmax argmax which amounts to standard GLASSO optimisation with the covariance matrix from (18).
 RCA-step After one iteration of EM, we update W via RCA based on the newly estimated  X  , Algorithm (1) summarises the EM and RCA steps which is collectively one iteration of EM/RCA: Algorithm 1 EM/RCA
Initialise  X  2 , W and  X  . repeat until the lower-bound (15) converges We describe three experiments with EM/RCA and one purely with RCA analysing the residual left from a Gaussian process (GP) in a time-series. For all the ex-periments that involve EM/RCA, the following apply: Initialisations are  X  2 = 1 2 p tr( C y ), where C y is the sam-ple covariance of the analysed data; W = U q ( L q  X   X 
I ) 1 2 as the q principal eigenvectors whose eigenval-ues are larger than  X  2 , and  X  = I . Note that since  X  2 is fixed to its initialized value, this implicitly fixes the number of latent variables. A more systematic approach would be a line search on  X  2 during the M-step or using the BIC criterion over a small range of q (number of latent variables).
 Each dataset was analysed with ` 1 -regularisation pa-rameters  X  = 5 x for x linearly interpolated in the in-terval [  X  8 , 3], thus creating a solution path as  X  in-creases exponentially. For lasso-based algorithms, in general, the solution paths tend to be unstable, so to smoothen the solution paths we applied stability selection (Meinshausen &amp; B  X uhlmann, 2010), i.e. for each dataset and for each  X  , results of all methods (GLASSO, Kronecker-GLASSO, EM/RCA) are stabi-lized by taking 100 repeats using 90% of the data-points for each repeat. Edges (corresponding to con-nections in the figure) are assumed to be present if they are called more than 50% of the time. 4.1. Simulation First we consider an artificial dataset sampled from the generative model in (equation 14, Figure 2) to il-lustrate the effects of confounders on the estimation of the sparse-inverse covariance. Specifically, Y = XW &gt; + Z + E , where Y  X  R 100  X  50 ; W  X  R N ( 0 ,  X   X  1 );  X  was generated with a sparsity level of 1% over all possible edges in the GRF and its non-zero entries were iid-sampled from a Gaussian with mean noise, e i, : iid  X  N ( 0 , X  2 I ). The variance  X  was set such that  X   X  1 and WW &gt; explained equal variance and  X  2 was set such that the signal-to-noise ratio was 10. Figure 3(a) shows the precision-recall curve for GLASSO and EM/RCA. The EM/RCA curve shows significantly better performance than GLASSO on the confounded data, while the dashed line shows the performance of GLASSO on similarly generated data without the confounding effects ( W = 0 ). We note that EM/RCA performs better on confounded data than GLASSO on non-confounded data, because of the lower signal-to-noise ratio in the non-confounded data. 4.2. Reconstruction of a biological network Next we applied EM/RCA on the protein-signaling data of Sachs et al. (2008). In this case, we also com-pare against the results reported by Stegle et al. (2011) on the same data, with the Kronecker-GLASSO algo-rithm. These data provide signal measurements from 11 proteins under various external stimuli. We com-bined measurement from the first 3 experiments, re-sulting in a heterogeneous dataset of 2,666 samples. The different conditions of these experiments induce the confounding effects in the data. For the sake of comparison, we also run the analysis on a random 10% subset of the 2,666 samples. All algorithm were com-pared based on the moralised-version of the ground truth directed network which was biologically vali-dated in the related study.
 In Figure 3(b), EM/RCA performs slightly better than all other methods. Figure 4 shows the reconstructed networks for recall 0.4. We note that EM/RCA is more conservative in calling edges.
 4.3. Reconstruction of the human form These data come from the CMU motion-capture database ( http://mocap.cs.cmu.edu ). The objective here is to reconstruct the underlying connectivity of a human being, given only the 3 dimensional locations of 31 sensors placed about the figures body. Each captured motion in the database involves the skele-ton (or stickman) data specific to the person under the trial (different heights, builds, etc.) and the 3-D sensor cloud data. Each trial involves 31 sensors, so the dataset for each trial is 93  X  the number of frames captured in the trial.
 Our aim is to construct a model which recovers connec-tivity between these points. This should be possible because we expect sensors that are connected in the underlying figure to be conditionally independent of other sensors in the figure. This motivates the under-lying sparse structure. Conversely, different motions exhibit much broader correlations across the figure. In particular walking exhibits anti-correlations between sensors on different legs and across the arms. These types of motion should be far better recovered through a low rank representation of the covariance.
 If, as expected, the raw data is confounded by low-rank properties associated with particular structured motions (as opposed to random poses, as might be adopted by a wooden artist X  X  doll) then our combina-tion of low-rank with sparse connectivity should out-perform a model based purely on sparse connectivity. We therefore compare EM/RCA and GLASSO on tri-als involving walking, running, jumping and dancing. The local connectivity between the sensors, i.e. the human skeleton, should be represented in the sparse matrix  X  (proscribing a Gaussian random field). To further motivate this idea we note that  X  can also be seen as the stiffness (or Laplacian) matrix of a physical system of a spring network, where the off-diagonal en-tries represent the negative stiffness of the spring. To detect a connection between two sensors we are only looking for negative entries in the estimated  X  . Figure 5 shows the results in the form of re-call/precision curves for both GLASSO and our EM/RCA implementation of a sparse-inverse plus low-rank model. The EM/RCA algorithm consistently outperforms standard GLASSO. Figure 6 illustrates the recovered stickmen of EM/RCA and GLASSO. We note that the connectivities and eigenposes are more faithful to the true human form, in comparison to GLASSO. For a small  X  setting (recall 1) the pre-cisions are similar; nonetheless the human form is ro-bust, with very weak (yellow) edges wherever they do not apply (e.g. elbow-waist, elbow-head). This signi-fies that the precision measure might be ill-suited for evaluating a stickman, where the network configura-tion has a spatial interpretation. The ground truth is also  X  X oisy X  in the sense that a shoulder-chest edge, for instance, must be called as the torso is a rigid part of the human body (high stiffness). Figure 5(b) illus-trates the confounding effects that X is supposed to capture. Specifically, in the first component, the legs are anti-correlated to the upper-half of the body, which can be attributed to jumping motions. The second and forth components capture anti-correlations across the different legs and arms, exhibited by walking and run-ning, as discussed earlier. The third component shows strong anti-correlation between the hands and the rest of the upper-body, which is more open to interpreta-tion. 4.4. Differences in gene-expression profiles A common challenge in data analysis is to summarize the difference between treatment and control samples. To illustrate how RCA can help, we consider two gene expression time-series of cell lines. The treatment cells are targeted by TP63 introduced into the nucleus by tamoxifen. The control cells are simply subject to tamoxifen alone. The data used for this case study come from Della Gatta et al. (2008, GEO accession GSE10562) The treatment group Y 1  X  R n 1  X  p con-tains n=13 time-points of p = 22 , 690 gene expression measurements, whilst the control group Y 2  X  R n 2  X  p contains only n 2 = 7 time-points. This complexity of data (with different numbers of time-points and non-uniform sampling) is typical of many bio-medical data sets. The challenge is to represent the differences be-tween the gene expression profiles for these two data sets. CCA could be applied but this would represent the similarities between the data, not the differences. Assuming that both time-series are identical, implies y cess (GP) with a temporal covariance function, y  X  N ( 0 , K ), where K  X  R n  X  n for n = n 1 + n 2 is structured such that both y 1 and y 2 are generated from the same function, K i,j = k ( t i ,t j ) = exp(  X  1 2 `  X  2 ( t squared-exponential covariance function (or RBF ker-nel, Figure 7(a)). Other kernels can be used and their hyperparameters can be optimized, but for this demonstration we simply set ` = 20 which provides a bandwidth roughly in line with the time-point sam-pling intervals. We also add a small noise term along the diagonal of K which was set to 1% of the data variance.
 A more general model (dual paradigm) of the form y  X  N ( 0 , XX &gt; + K ), should explain no variance in the low-rank component XX &gt; , as all the signal in the time-series is assumed to be explained by the underly-ing function sampled from the GP. If we solve for the residual components X via RCA, they will be forced to explain how the two time-series are actually different. We project the profiles onto the eigenbasis of the first q generalised eigenvectors  X  Y = S &gt; q Y and obtain a score of differential expression based on the norms of their projections. The number q of retained principal eigenvectors is decided on the number of correspond-ing eigenvalues d i being larger than one. Recall from PPCA, that as we increase the assumed noise variance  X  , more eigenvalues become negative and less eigen-vectors are retained in W ML (cf. page 1). Similarly, RCA standardises any noise (6), so we only have to retain for eigenvalues larger than 1. In this case, the assumed noise variance embedded in the kernel drives the effective number of eigenvectors in the projection basis. Ranking the scores and comparing to the noisy ground-truth list of binding targets 2 of TP63 from (Della Gatta et al., 2008), gives the ROC performance curve in Figure 7(b). We compare against BATS as a baseline method (Angelini et al., 2007). We note that RCA outperforms BATS in terms the area under the ROC curve for all of its noise models. We are often faced with data that can be partially ex-plained by a set of covariates and may wish to analyse the residual components of these data. This motivated the construction of RCA: an algorithm for describ-ing a low-dimensional representation of the residuals of a data set, given partial explanation by a covari-ance matrix  X  . The low-rank component of the model can be determined through a generalized eigenvalue problem. The special case of PCA being recovered for  X  =  X  2 I . Our algorithm also generalizes CCA, but with further imaginative application we can develop new approaches to data analysis.
 We illustrated how a treatment and a control time-series could have their differences highlighted through appropriate selection of  X  (in this case we used an RBF kernel). We also introduced an algorithm for fitting a variant of CCA where the private spaces are explained through low dimensional latent variables. Our final, and perhaps most important, new data analysis technique combined sparse-inverse covariance with low-rank. Full covariance matrix models of data are often problematic as their parameterization scales with D 2 . Two separate approaches to a reduced pa-rameterization of these matrices are to base them on low-rank matrices (as in probabilistic PCA) or on a sparse-inverse structure (as in GLASSO). These two approaches have very different characteristics: one in-volves specifying sparse conditional dependencies in the data, the other assumes that a reduced set of la-tent variables governs the data. Clearly, in any data set, both of these characteristics may be present. Our sparse-inverse plus low-rank approach is the first ap-proach to deal with both of these cases in the same model. It was demonstrated to good effect in a mo-tion capture and protein network example.

