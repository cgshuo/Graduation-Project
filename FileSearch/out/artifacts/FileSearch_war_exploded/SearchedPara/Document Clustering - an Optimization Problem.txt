 Clustering algorithms have been widely used in information retrieval applications. However, it is difficult to define an ob-jective  X  X est X  result. This article analyzes some document clustering algorithms and illustrates that they are equivalent to the optimization problem of some global functions. Ex-periments show their good performance, but there are still counter-examples where they fail to return the optimal so-lution. We argue that Monte-Carlo algorithms in the global optimization framework have the potential to find better solutions than traditional clustering, and they are able to handle more complex structures.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Search and Retrieval]: Clustering General Terms: Algorithms, Performance, Theory Keywords: Clustering algorithm, Global optimization, Sim-ulated annealing
For a text collection with a large number of unlabeled documents, the commonly-used analysis is to run a cluster-ing process based on the proximity among these elements. Documents in a cluster are very likely to match the same information need [2]. There are many clustering algorithms [1], and most evaluation methods involve manual annotation of  X  X ruth X  data. How to define  X  X ptimal X  clusters without any subjective relevance judgment is still an open problem.
Some clustering algorithms have been identified equiva-lent to the optimization of their cost functions [1]. In this article, we will show that it is true for most (if not all) clus-tering methods. Section 2 introduces a few algorithms and defines their corresponding functions. Section 3 compares the performance of clustering to simulated annealing and shows when it fails. Conclusions and further extension of the framework are introduced in Section 4.
There are mainly two types of clustering algorithms, hi-erarchical and partitional. In this section, we will select one representative method from each and show that it corre-sponds to a global optimization problem. We believe that other algorithms have their own global functions as well.
In the previous section, we have described two clustering algorithms and shown their global functions. The clustering process corresponds to the optimization (in a hill-climbing manner) of the function. A natural question is, are they guaranteed to return the best solution? There are two ways to evaluate the clustering output. With the global functions defined above, we can calculate the objective: values closer to the optimum mean better re-sults. The other is to compare system-generated clusters to some  X  X ruth X  data, where a better match gets higher score. The latter depends on the quality of the manual annotation.
Assume that we randomly select two objects x i and x j , each of them will be assigned to some cluster in the system output and in the truth data, respectively. If they have the same membership status in both cases, it is regarded as a successful case for the system. here C ( x i ) is the cluster x i belongs to in the truth data, and C ( x i ) is its cluster in system output.
The collection used in the experiments is part of TDT-3 . Six topics are selected from the same scenario (sci-ence/discovery), with a total of 280 news stories. They are divided into about 30 manually generated clusters, each de-scribing a news event. The similarity matrix is calculated with tf-idf, where the term vectors are built based on the body text of stories.

The only parameter in HAC is the clustering threshold, and we tune it to maximize F 1 defined in Equation 6. The optimal threshold is 0.09 from the experiment.

Simulated annealing (SA) is implemented to optimize the global score S in Equation 1. It also starts with a list of singleton clusters ( R is all -1 except for the diagonal elements that are 0). In each round, a story d i is randomly picked, and another story d j is selected based on the probability distribution of R i  X  .If R ij = 0, the cluster they are in will be broken into two, where d i and d j are in different parts. If R ij =  X  1, the clusters d i and d j are in will be merged. Whenever S increases after a round, the change is kept. Otherwise, it is kept with some probability.

Since SA does not have deterministic output, we run it 20 times, and the best runs are shown in Table 1. HAC achieves better performance on both F 1and S , but SA runs are very close to it.
 In the experiment, HAC gets higher S than all SA runs. Does that mean HAC always finds the optimal solution? Figure1showsacounter-example. HACcombines1and 2 first since they are the most similar pair, then no other nodes can be merged because the average similarity cannot exceed 0.4. However, the best clusters are { 1,3 } and { 2,4 } , which gets 2.6 instead of 2.55 for S . Monte-Carlo algorithms can easily find the ideal solution with their ability to get out of local maxima.
Available from the linguistic data consortium (LDC), cat-alog number LDC2001T58.

