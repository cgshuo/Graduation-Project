 Recently there has been considerable interest in us-ing active learning (AL) to reduce HLT annotation burdens. Actively sampled data can have differ-ent characteristics than passively sampled data and therefore, this paper proposes modifying algorithms used to infer models during AL. Since most AL re-search assumes the same learning algorithms will be this paper opens up a new thread of AL research that accounts for the differences between passively and actively sampled data.

The specific case focused on in this paper is that of AL with SVMs (AL-SVM) for imbalanced widespread class imbalance for many HLT tasks, in-terest in using SVMs, and PL research showing that SVM performance can be improved substantially by addressing imbalance, indicate the importance of the case of AL with SVMs with imbalanced data.

Extensive PL research has shown that learning algorithms X  performance degrades for imbalanced datasets and techniques have been developed that prevent this degradation. However, to date, rela-tively little work has addressed imbalance during AL (see Section 2). In contrast to previous work, this paper advocates that the AL scenario brings out the need to modify PL approaches to dealing with im-balance. In particular, a new method is developed for cost-weighted SVMs that estimates a cost model based on overall corpus imbalance rather than the imbalance in the so far labeled training data. Sec-tion 2 discusses related work, Section 3 discusses the experimental setup, Section 4 presents the new method called InitPA, Section 5 evaluates InitPA, and Section 6 concludes. A problem with imbalanced data is that the class boundary (hyperplane) learned by SVMs can be too close to the positive (pos) examples and then recall suffers. Many approaches have been presented for overcoming this problem in the PL setting . Many require substantially longer training times or ex-tra training data to tune parameters and thus are not ideal for use during AL. Cost-weighted SVMs (cwSVMs), on the other hand, are a promising ap-proach for use with AL: they impose no extra train-ing overhead. cwSVMs introduce unequal cost fac-tors so the optimization problem solved becomes:
Minimize:
Subject to: where ( ~w,b ) represents the learned hyperplane, ~x is the feature vector for example k , y for example k ,  X  is the slack variable for example k , and C are user-defined cost factors.

The most important part for this paper are the cost factors C importance of reducing slack error on pos train ex-amples relative to reducing slack error on negative (neg) train examples. The value of the ratio is cru-cial for balancing the precision recall tradeoff well. (Morik et al., 1999) showed that during PL, set-tive heuristic. Section 4 explores using this heuris-tic during AL and explains a modified heuristic that could work better during AL. (Ertekin et al., 2007) propose using the balancing of training data that occurs as a result of AL-SVM to handle imbalance and do not use any further mea-sures to address imbalance. (Zhu and Hovy, 2007) used resampling to address imbalance and based the amount of resampling, which is the analog of our cost model, on the amount of imbalance in the cur-rent set of labeled train data, as PL approaches do. In contrast, the InitPA approach in Section 4 bases its cost models on overall (unlabeled) corpus imbal-ance rather than the amount of imbalance in the cur-rent set of labeled data. We use relation extraction (RE) and text classifica-for training the SVMs. For RE, we use AImed, previously used to train protein interaction extrac-tion systems ((Giuliano et al., 2006)). As in previ-ous work, we cast RE as a binary classification task (14.94% of the examples in AImed are positive). We use the K of the highest-performing systems on AImed to date and perform 10-fold cross validation. For TC, we use the Reuters-21578 ModApte split. Since a doc-ument may belong to more than one category, each category is treated as a separate binary classification problem, as in (Joachims, 1998). As in (Joachims, 1998), we use the ten largest categories, which have imbalances ranging from 1.88% to 29.96%. The key question when using cwSVMs is how to set learned hyperplane so recall is increased and preci-sion is decreased (see Figure 1 for a hypothetical ex-during AL-SVM?
We propose two approaches: one sets the PA based on the level of imbalance in the labeled train-ing data and one aims to set the PA based on an es-timate of overall corpus imbalance, which can dras-tically differ from the level of imbalance in actively sampled training data . The first method is called CurrentPA, depicted in Figure 2. Note that in step 0 of the loop, PA is set based on the distribution of positive and negative examples in the current set of labeled data. However, observe that during AL the beled data gets skewed from the ratio in the entire Input: Loop until stopping criterion is met: End Loop corpus because AL systematically selects the exam-ples that are closest to the current model X  X  hyper-plane and this tends to select more positive exam-ples than random selection would select (see also (Ertekin et al., 2007)).

Empirical evidence of this distribution skew is il-lustrated in Figure 3. The trend toward balanced datasets during AL could mislead and cause us to underestimate the PA.

Therefore, our next algorithm aims to set the PA based on the ratio of neg to pos instances in the en-tire corpus. However, since we don X  X  have labels for the entire corpus, we don X  X  know this ratio. But by using a small initial sample of labeled data, we can estimate this ratio with high confidence. This esti-mate can then be used for setting the PA throughout the AL process. We call this method of setting the PA based on a small initial set of labeled data the InitPA method. It is like CurrentPA except we move Step 0 to be executed one time before the loop and then use that same PA value on each iteration of the AL loop.

To guide what size to make the initial set of la-beled data, one can determine the sample size re-quired to estimate the proportion of positives in a finite population to within sampling error e with a desired level of confidence using standard statisti-cal techniques found in many college-level statistics references such as (Berenson et al., 1988). For ex-ample, carrying out the computations on the AImed dataset shows that a size of 100 enables us to be 95% confident that our proportion estimate is within 0.0739 of the true proportion. In our experiments, we used an initial labeled set of size 100. In addition to InitPA and CurrentPA, we also imple-mented the methods from (Ertekin et al., 2007; Zhu and Hovy, 2007). We implemented oversampling by duplicating points and by BootOS (Zhu and Hovy, 2007). To avoid cluttering the graphs, we only show the highest-performing oversampling variant, which was by duplicating points. Learning curves are pre-sented in Figures 4 and 5.

Note InitPA is the highest-performing method for all datasets, especially in the practically important area of where the learning curves begin to plateau. This area is important because this is around where we would want to stop AL (Bloodgood and Vijay-Shanker, 2009).

Observe that the gains of InitPA over CurrentPA are smaller for Reuters. For some Reuters cate-gories, InitPA and CurrentPA have nearly identical performance. Applying the models learned by Cur-rentPA at each round of AL on the data used to train the model reveals that the recall on the train-ing data is nearly 100% for those categories where InitPA/CurrentPA perform similarly. Increasing the relative penalty for slack error on positive training points will not have much impact if (nearly) all of the pos train points are already classified correctly. Thus, in situations where models are already achiev-ing nearly 100% recall on their train data, InitPA is not expected to outperform CurrentPA.

The hyperplanes learned during AL-SVM serve two purposes: sampling -they govern which unla-beled points will be selected for human annotation, and predicting -when AL stops, the most recently learned hyperplane is used for classifying test data. Although all AL-SVM approaches we X  X e aware of use the same hyperplane at each round of AL for both of these purposes, this is not required. We com-pared InitPA with hybrid approaches where hyper-planes trained using an InitPA cost model are used for sampling and hyperplanes trained using a Cur-rentPA cost model are used for predicting, and vice-versa, and found that InitPA performed better than both of these hybrid approaches. This indicates that the InitPA cost model yields hyperplanes that are better for both sampling and predicting. We X  X e made the case for the importance of AL-SVM for imbalanced datasets and showed that the AL sce-nario calls for modifications to PL approaches to ad-dressing imbalance. For AL-SVM, the key idea be-hind InitPA is to base cost models on an estimate of overall corpus imbalance rather than the class imbal-ance in the so far labeled data. The practical utility of the InitPA method was demonstrated empirically; situations where InitPA won X  X  help that much were made clear; and analysis showed that the sources of InitPA X  X  gains were from both better sampling and better predictive models.

InitPA is an instantiation of a more general idea of not using the same inference algorithms during AL as during PL but instead modifying inference al-gorithms to suit esoteric characteristics of actively sampled data. This is an idea that has seen relatively little exploration and is ripe for further investigation.
