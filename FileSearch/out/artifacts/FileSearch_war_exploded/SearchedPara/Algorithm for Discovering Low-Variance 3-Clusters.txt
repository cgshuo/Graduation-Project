
The idea of Triclusters [1], [2], [3], [4], [5], [6] is being increasingly applied in data mining situations where two distinct datasets need to be mined simultaneously; for example, in two microarray gene-expression datasets. The general form of a gene-expression dataset is given by a two dimensional table with rows representing genes (data objects), columns representing samples (features), and cell-values representing expression levels. Triclusters are con-structed or discovered from two gene-expression datasets by selecting a subset of samples (features) from each dataset and one shared subset of genes (rows) from amongst all the genes. Triclusters provide many useful insights; for example, they can help in unveiling biologically important relationships between two sets of samples (features), given in the two different gene-expression datasets; and they also help in identifying similarly behaved genes (rows) under two different sets of features. The triclustering algorithms have been explored only recently and most of them have many limitations. The limitations include: (i) some algorithms [1], [2], [3], [4] require both datasets to contain identical data objects (rows); and (ii) some [1], [5], [6] require cell-values in each dataset to fall in the same value ranges and even the distributions of values in the two datasets to be identical. These limitations reduce the applicability of these algorithms. Our triclustering algorithm presented here discovers triclusters from two datasets without any of the above limitations. Each component bicluster (sub-matrix in a dataset) obeys the constraint of a low-variance distribution of cell values (with specified upper-bound on the variance) and the intersection set of the rows of two biclusters (sub-matrices) is the largest possible while permitting each bi-cluster to contain some non-shared rows. In this paper we name this type of tricluster as Low-Variance 3-Cluster and its formal definition is given in Section II below.
Low-Variance 3-Clusters play an important role in many data mining problems. For example, cross-species biomed-ical researchers [7], [8], [9] often examine gene-expression levels from mouse models and humans for the samples suffering from the same type of disease. Some of the genes across these two species are conserved (are shared) across species and some are not, thus the sets of genes (rows) in the two expression level datasets overlap but are not identical. Also, the expression levels of the two species are obtained by different types of tests and equipments and therefore the value ranges and distributions of values are generally very different. One major focus of the cross-species researchers is to discover the subsets of genes that are similarly expressed in the datasets for both of the species. A Low-Variance 3-Cluster will generate one gene list for each of the two expression datasets (species) such that the genes are similarly expressed for some subset of samples in each dataset, meaning the genes are following low variance distribution within a subspace of samples for each species. The two gene lists obtained for the two datasets contain the largest number of genes in common while each set may contain genes unique to each of the species. The genes in common (referred to as  X  X ommon genes X  in the following paragraphs) may be highly suspected to be related to the disease and provide useful hypotheses for furthering the biomedical research. The genes uniquely existing in the two gene lists (also referred to as unique genes) provide clues to the researchers about interesting relationships between them and the common genes. Very similar problems are also found in other domains such as credit evaluation, recommender systems, and GIS applications.
We illustrate the tricluster idea with the example dataset shown in Figure I; this example is also used for explaining the steps of our algorithm in Section III. The figure shows two data tables: the first table contains five rows and four columns and the second table contains four rows and five columns. The range for the cell-values in each data table is different. The column labels of the two data tables are distinct and row labels of the two tables overlap, sharing the labels { g 1 ,g 2 ,g 3 } . If we search for sub-matrices in these two data tables satisfying the constraints that each sub-matrix should contain at least two rows, two columns and the selected cell-values in each sub-matrix should have a standard deviation less than 0.7, we find many row-column combinations (local low-variance biclusters). How-ever, there are some biclusters in each dataset that have no corresponding (sharing some rows) bicluster in the other dataset. Figure 1(c) lists some combinations of local low-variance biclusters that have at least one row in common. For example, &lt; { g 1 ,g 2 } , { a, b, c }{ g 1 ,g 2 } , combination of local biclusters forming a Low-Variance 3-Cluster. The problem we solve, in this paper, is to discover all such maximal sized pairs of biclusters that have low variance within local biclusters and share a large subset of rows between them.

Searching for low-variance 3-Clusters in the two data tables is a non-trivial problem. There are many challenges that have not been addressed before in the context of this problem. One critical issue is that when we enforce low-variance as a criterion without specifying any value for the mean value, there will be numerous low-variance biclusters satisfying the constraints which, in turn, makes it difficult to enumerate all possible pairs of biclusters from the two datasets in order to construct the Low-Variance 3-Clusters. The other critical issue is encountered when we permit data tables containing non-identical sets of rows (data objects). Our problem becomes difficult because of allowing both the local low-variance biclusters to contain unique rows (objects) as well as contain the largest possible number of shared rows. The third critical issue is that each data table contains cell-values with its own data distribution, therefore, it is very difficult to specify the same variance upper bound for local biclusters from the two data tables.

Compared to previous Triclustering problem formulations our problem is very different in many ways and is more general. Our algorithm accommodates data distribution dis-parities between datasets and focuses on satisfying simple to understand standard deviation bounds on biclusters from real valued datasets. Even though there are many biclustering algorithms that could search for maximal sized low-variance biclusters within individual datasets [10], [11], [12], [13], [14], [15], the strategy of first finding biclusters in individual datasets and then finding good pairs across the two datasets is computationally explosive and therefore too inefficient. Another difficulty arises from the fact that individual biclus-ters, if found in local contexts only, would be maximal in the number of attributes and rows; but a Low-Variance 3-Cluster may be formed by using non-maximal local biclusters which contain only subsets of attributes forming the locally optimal biclusters, and also consequently, larger number of rows, increasing chances of larger sets of shared rows.
For a binary dataset the theory of Formal Concept Anal-ysis [16] calls all maximum sized sub-matrices containing only 1 X  X  as concepts and arranges them in a partially ordered lattice. We consider a formulation parallel to this and define as concepts all those sub-matrices of a dataset that are maximal-sized and satisfy the low variance constraint. Each node of the lattice represents a partially ordered subspace of attributes and stores at this node all those biclusters that satisfy the low-variance constraint. There may be many bi-clusters in a subspace each satisfying the variance constraint but having a different mean value. Our search algorithm for triclusters traverses the concept lattices implicit in the two data tables, enumerating the promising parts, and pruning away the non-promising parts, using information from both datasets to make the pruning decisions.

In the following sections we present the formal description of ideas underlying our formulation and algorithm. These include a monotonic property that is used to direct the search towards the Low-Variance 3-Clusters in the search space given by the lattice. In section II we introduce symbols and related definitions used in this paper. The algorithm, its supporting basis, and some pruning strategies used by us are described in section III. Section IV presents results of our algorithm with a synthetic data set and also with a real world genomic data set. The discussion of the results and the conclusions are presented in section V.

Definition 1 :A Bicluster B is a pair consisting of a subset of rows r and a subset of columns c taken from a dataset D with row-set R and column-set C .

We use the term B = { r, c } to denote a Bicluster and use s B to denote the standard deviation of the cell values in bicluster B ; here r  X  R and c  X  C .
 Definition 2 :A 3-Cluster T , represented as T = { B 1 || B 2 } is a pair of Biclusters B 1 , B 2 taken from two datasets D 1 and D 2 respectively; the two component biclusters are such that | r 1 r 2 | is the number of data objects shared between the two biclusters and it is greater than zero.

Definition 3 :A Low-Variance 3-Cluster is a 3-Cluster in which the standard deviation of each component bicluster is less than or equal to some pre-defined threshold for the dataset from which the bicluster is taken. That is, s B 1 and s B 2  X  s 2 . And the number of rows and columns contained in each component bicluster are equal to or more than the pre-defined thresholds. That is, | r 1 | X   X  | c | X   X  1 , | r 2 | X   X  2 , and | c 2 | X   X  2 .
 Definition 4 : The interestingness of a Low-Variance 3-Cluster T , represented as | T | is defined to be: where | r | is the number of rows in a bicluster and | R | the number of rows in the entire data set.

This definition of interestingness is one possibility and algorithm would work with other interestingness criteria as long as the monotonicity of the metric with respect to the number of rows in the biclusters is guaranteed. The interestingness metric increases monotonically with the number of rows and also with the number of shared rows because we are interested in 3-clusters that include as many rows, shared or unique, as possible. The number of columns must be larger than or equal to some specified threshold.
In this section we introduce our ideas underlying the design of standard deviation threshold and the accompanying search algorithm which accommodates datasets with differ-ent sizes and/or different distributions of values. A. Setting STD. Threshold
Consider for example a dataset containing 100 rows, 100 columns, and cell-values that are normally distributed with a mean value of 0 and a standard deviation equal to 4.0. A histogram plot of this dataset is shown in Figure 2, consisting of bars with black borders, some of which are fully hidden behind the color-shaded bars. The cell values are distributed within the range [-15,15]. We now consider another dataset consisting of 100 rows, 100 columns, and cell values that are uniformly distributed in the range [-15,15]. We superimpose the histogram of this new dataset on the earlier normal distribution shown in Figure 2, using shaded bars with blue borders.

In this figure we see two different types of regions; in region-1 the normal distribution bars are shorter than the uni-form distribution bars and in region-2 the normal distribution bars are taller than the uniform distribution bars (red circled portion). A bicluster includes cell values within a Range given by the difference between the max and min of the bicluster X  X  cell values. Let us consider the amount of increase needed in the Range of values to achieve some fixed increase in the size of a bicluster. For normally distributed values this amount of increase in Range is larger than that needed for the uniform distribution in region-1 because of lower data density. In region-2 the normally distributed values require smaller increases in Range compared to the uniform distributions to achieve same increase in bicluster size. Since size of a bicluster can always be increased by increasing the Range of values contained in it, we believe that a bicluster is  X  X nteresting X  only when the increase in Range needed is smaller than what may be needed for a uniform distribution of values. Therefore, only the data cells from region-2 are of interest, and we seek to grow larger biclusters primarily using this region of a normally distributed dataset. Our algorithm is formulated to incorporate this notion. As a corollary of the above, we can say that the large-sized biclusters found by our algorithm will have their cell-values in very small Ranges.

Based on the above argument comparing distributions, we can define a standard deviation threshold in order to filter out cell values corresponding to the region-1 bars (in normal distributions). The first step is to quantitatively define the uniformly distributed data cells.

Definition 5 : Given a dataset D with | R | rows and | C | columns, the Cellstep( C D ) of the dataset is: where max d ij is the maximum value in D and min d ij is its minimum value in D .

The Cellstep value indicates the average distance between two consecutive values when all the cell values following a uniform distribution are placed in a sorted order.
To further capture the distribution with different size of biclusters from different data tables, we define size ratio and corresponding minimum size requirements.

Definition 6 : A bicluster B = { r, c } contains minimum acceptable number of rows and columns in data table D = {
R, C } if: where  X  and  X  are pre-defined size ratio .

In our algorithm, for each candidate bicluster, we compare their Range values (difference between the maximum and minimum value of the biclusters) with a pre-specified Range threshold.
 Definition 7 : The Range of a component biclusters is: where max i,j  X  B d ij is the maximum value of the bicluster and min i,j  X  B d ij is the minimum value.

The background Range threshold value (reflecting the uni-form distribution) for a bicluster is obtained by multiplying Cellstep of its dataset with the minimum number of rows and the minimum number of columns required to be in the bicluster. Since the standard deviation of a bicluster is bounded from above when the Range value for the bicluster is bounded from above, definition 3 can be rewritten to show a new property of the Low-Variance 3-Clusters:
Definition 8 :A Low-Variance 3-Cluster is a 3-Cluster when each of its component bicluster X  X  range is less than or equal to the background Range threshold (uniform dis-tribution Range) and contains minimum required number of rows and columns. That is: Where  X  and  X  are pre-defined size ratios.
 B. Constructing Low Variance 3-Cluster
The process of constructing Low Variance 3-Clusters follows the main steps as described in Figure 3. Before starting these steps the user selects the sizes of the desirable biclusters by selecting appropriate values for the size ratios; a size ratio for a dataset represents the lowest acceptable fraction of rows and columns that should be included in biclusters. Using the selected size ratios we calculate the range thresholds for candidate biclusters from each dataset. Our search algorithm starts to examine bicluster candidates for Low-Variance 3-Clusters from one dataset and then finds corresponding biclusters in the second dataset. Figure 3 illustrates the broad sequence of steps in our algorithm. We illustrate each of these steps below.

Our search algorithm first enumerates the top layer of the lattice shaped search space of the first dataset by enumer-ating biclusters consisting of every single column and each containing all the rows. This is shown in Figure 4 where three different search trees are initiated, and the root node of each tree is a single column and all rows. In later iterations in each search tree we selectively add columns to the candidate biclusters while removing those rows which violate the range threshold limit when checked along each row considering all the columns included in the hypothesis. Each box in Figure 4 represents one candidate bicluster with column set listed in the first line and the row set listed in the second line. The order of adding columns, as we go down the search trees, follows the prefix-tree depth-first ordering. For example, the first dataset in Figure 1(a) contains 5 rows and 4 columns. The maximum value of all the data cells is 5.8 and the minimum value is -3.2; thus the Cellstep of the dataset is 0.45. If we set the smallest desirable bicluster size ratios as 0.4 and 0.5 (implying two rows and two columns minimum), then the Range threshold, as defined by Definition 8 above is 1.8.

The bicluster candidates with red borders are deleted from the search because they violate the size requirements, such as B = &lt; { g 1 } , { a, b, c, d } &gt; contains only one row. The bicluster hypotheses in green borders are also deleted from the search process due to there being bigger biclusters which totally subsume them and have already been enumerated. For example, the bicluster B = &lt; { g 1 ,g 2 } , { a, c subsumed by B = &lt; { g 1 ,g 2 } , { a, b, c } &gt; and therefore deleted. Only those bicluster hypotheses that have blue borders are retained and passed on to step 3 of the algorithm in Figure 3. It should be noted here that till now we only check the range of values along the rows without considering the Range constraints along the columns. Therefore, each subspace of columns now contains all possible rows that may be included in one or more biclusters formed in that subspace.

This information about the viable row combinations within each column subspace is used to prune the search space of the second dataset X  X  biclusters. That is, we ex-amine only those row combinations in the second dataset (or there supersets containing rows that are not present in dataset1) which are viable in dataset1 and survive in one of the blue boxes of Figure 4. For example, B 0 = &lt; { g 1 ,g 2 } , { a, b, c } &gt; was output by the step-2 for dataset1, we initiate two candidate bicluster searches for it in the second dataset, one search tree starting with g 1 and the other starting with g 2 and building the search trees to examine biclusters local to dataset2 but containing only the row combinations that were viable in dataset1.

The fourth step of the algorithm described in Figure 3 works on the second dataset and is a mirror image of the step-2 that was performed on the first dataset. The difference is that the step-2 enumerated the column subspaces of the first dataset and from each subspace removed the unviable rows, whereas the step-4 enumerates the row subspaces of the second dataset and from each subspace removes the individual columns that are unviable due to not satisfying the range constraints. For the example dataset, the step-4 calculates the Cellstep for the second dataset (0.265) and corresponding range threshold turns out to be 1.06. We then enumerate all row combinations and remove from each those columns which individually violate this range threshold.
We now have all viable columns of dataset2 that can potentially form low variance biclusters in the subspace defined by a set of rows. For example, as shown in Figure 5(a), the row subspace ( g 3 ,g 6 ) has columns e, f, h, and i as viable columns. But all the viable columns together may not belong to a single bicluster; various overlapping subsets of these columns may constitute low-variance biclusters. Step-5 of the algorithm seeks to find those column combinations that satisfy the range constraint and the size constraint and thus form low-variance biclusters. Having selected the biclusters in dataset2 we know the row combinations that constitute low-variance biclusters in dataset2. Since the rows are shared between the two datasets, we can prune the space of hypotheses generated for dataset1 by deleting those row combinations which do not have sufficient overlap with biclusters in dataset2. This further reduces the search space for dataset1. The Step-7 of the algorithm searches for row combinations in the pruned space of dataset1 that form low-variance biclusters. This is done by searching for those column sets along which the range constraint is satisfied.
For the example dataset, in the 6 th step, we remove all rows which are shared by both datasets but are not included in the biclusters of dataset2 in the 5 th step. After the step the bicluster B = &lt; { g 3 ,g 6 } , { e, f, h } &gt; is formed within dataset2, and the row set { g 3 } which is shared by both datasets is returned ( g 6 is unique to dataset2). Thus bicluster hypothesis B = &lt; { g 1 ,g 2 ,g 3 ,g 4 } , { c, d dataset1 is changed into B = &lt; { g 3 ,g 4 } , { c, d } removing { g 1 ,g 2 } which are shared with dataset2 but not returned as viable ones after step5. Then in the 7 th step we update the row set of bicluster B following similar procedure as the 5 th step and construct Low-Variance 3-Cluster T = &lt; { g 3 ,g 4 } , { c, d }{ g 3 ,g 6 } , { e, f, h is one row in common between the two biclusters of this 3-cluster and the corresponding interestingness measurements is 0.45.

The steps 2 through 7 are repeated until all branches of the search trees are exhausted or truncated. Our algorithm retains the top K most interesting Low-Variance 3-Clusters in memory. The pseudo code summarizing the above out-lined steps is listed in Procedure 1.

Procedure 1. Low Variance 3-Cluster C. Efficiency
Our algorithm employs a number of pruning strategies to make the algorithms more efficient. Generally our al-gorithm conducts heuristic beam search in two prefix-tree defined search spaces which are the same as the lattices of all possible biclusters in the two datasets. We use the interestingness measure given in Definition 4 as the primary heuristic function driving the search. The time cost of our algorithms depends on the cell values of the data tables and also on the sizes of data tables. A typical search through the synthetic dataset described in section IV takes about 6 seconds on desktop computers equipped with one Intel Core 2 Quad @ 2.66 GHz cpu.

The most distinguishing pruning strategy is the design of the row set update methods which is embodied in steps 3 and 6 of the algorithm given in Figure 3. In the 3 rd step we get maximum possible size of rowset in biclusters from first dataset B and construct candidate biclusters of second dataset B . After the 5 th step, B has the smallest possible rowset and satisfies all of our requirements. Then we use the rowset of B to update (prune) the rowset of B . Numerous redundant hypotheses are thus pruned away from the search space of the first dataset. For example, if we do not use the rowset of B to prune that of B , for bicluster B = &lt; { g 1 ,g 2 ,g 3 ,g 4 } , { c, d } &gt; we need to expand a large suffix-tree as illustrated in Figure 5(b). This pruning strategy will not influence the results, because for each 3-Cluster or candidate bicluster we maintain the interestingness measure and always retain the largest k candidates.

We also employ another pruning strategy to help obtain distinct 3-clusters from the datasets. Whenever a bicluster ranks very high on the interestingness metric we can also find many small variations of this bicluster that vary by a few rows or columns and have very similar high interestingness metric. In such cases we retain the bicluster candidates having the largest metric value and prune away all its minor variants from the search space.

In order to test the performance of our algorithm, we tested our algorithm on one synthetic dataset and one real-world cross-species dataset from [7]. The results from both datasets are very promising and demonstrate the value of our algorithm and its design.
 There are no other known algorithms that find the Low-Variance 3-Clusters in real-valued datasets. Therefore we cannot compare our results with any other proven 3-cluster methodology; but we do compare our results to some of the biclustering algorithms and demonstrate our overall results in the following discussion.
 A. Synthetic Dataset
We make two data tables with 100 rows and 100 columns in each; the heatmap of these data tables is shown in Figure 7(a) and 7(b) with gray scale. In the first data table, we planted several blocks in which the cell-values are distributed with a low variance and all cells in the background are normally distributed with a relatively higher variance. Data cells in the yellow box (30 rows x 30 columns) and the blue box (10 rows x 30 columns) in Figure 7(a) follow uniform distributions with the same Means and Ranges; and data cells in pink box (10 row x 60 columns) are uniformly distributed with a different mean and range. The cell-values X  ranges for the pink, yellow, and blue boxes overlap each other. The second dataset contains cell-values that are exactly the same as in the first dataset but the order of rows has been changed: 80 rows have the same IDs as in the first dataset and 20 rows have unique, non-shared, IDs. We show the heatmap of the second dataset and show the regions which are the same as in the first dataset in Figure 7(b). In order to get further insights into the synthetic datasets, we plot the histograms of the datasets in Figure 7(c) to examine the distributions of the cell values. Again the blue bars with shaded lines represent the distribution of the same number of data cells if they were to be uniformly distributed within the same value range as of synthetic datasets.
The best 3-clusters discovered by our algorithm are summarized and compared in Table-I. It consists of two biclusters -D 1 from dataset1 and D 2 from dataset2. Both biclusters, D 1 and D 2 , contain 53 rows each, 43 of these are shared, and 10 rows are unique to each dataset. The shared 43 rows have come from the yellow and the pink boxes in Figure 7(a) and the unique 10 rows of each bicluster have come from the blue boxes in Figures 7(a) and 7(b). We have compared the quality of biclusters found by us with that of the biclusters that may have been found by other well known bicluster finding algorithms. The variance of each component bicluster found by us is lower than 1.2. We compared the variance of biclusters found by Cheng et al. X  X  algorithm [10], SAMBA [11] and the Co-clustering algorithm [12], [13]. For each of these algorithms we keep the largest biclusters and use the default parameter sets. The results are listed in Table I and show that we have been able to find larger and lower variance biclusters compared to all the other approaches. B. Cross-Species Datasets
Mito et al. [7] report results of their studies on Sarcoma tissue genetic datasets containing both mouse and human genes and samples. They first identify top 100 differentially expressed genes between normal control mouse samples and Sarcoma diseased samples. And then they use the GSEA tool [18] to check the functional enrichment in human datasets [19], [20]. Their conclusion is that mouse sarcoma model and human Malignant Fibrous Histiocytoma (MFH) share common genomic features. All of their methods are based on the statistical methods and no consideration is given to examining subspaces of samples or to subspaces of genes, or to interesting clusters, or to maximizing shared genes while guaranteeing locally low variance on both datasets (our approach).

Our algorithms could discover some very precious infor-mation that gives a broad view in cross species analysis. The Low-Variance 3-Clusters clearly inform researchers about which shared genes are co-expressed in both of the species, and which genes uniquely exist in each species but are co-expressed with the genes shared with the other species. Compared with the methods used in Mito et al X  X  research we expect to discover more and important knowledge from the datasets.

We use the dataset GSE16779 [7] for mouse genetic data and GDS1209 [19], [20] for human genetic data; this is the same dataset that is used by Mito et al. Mouse gene expression dataset contains 8201 genes and 21 samples; human gene expression dataset contains 5358 genes and 54 samples; and 2926 genes are in common between the mouse dataset and the human dataset. We plot the heatmaps of both of the gene expression datasets and also their histograms in Figure 8. As we can see from Figure 8(b) and 8(d), the distribution of two datasets are different with mouse dataset containing much more noise (higher blue bars) and possibly two peaks. Again the bars with blue borders and shaded lines represent the same number of data cells uniformly distributed within the same range of the original datasets. Figure 8(a) and 8(c) are the heatmaps of the expressions of mouse and human datasets.
 In order to test the biological merits of the Low-Variance 3-Clusters, we employ the functional enrichment tool CLEAN [17] to search for functional enrichment knowl-edge from the database L2L [21]. The first validation test we perform is similar to the test done on the synthetic datasets; we examine the sizes of component biclusters and also their variance. We also list the functional enrichment scores (CLEAN scores) in Figure 8(e). Based on the def-inition of the CLEAN score, the higher score means a high coherence between the rows(genes in this case) in a component bicluster and the biological functional categories as defined in L2L. The CLEAN score uses Fisher Exact Test to quantify the coherence. The CLEAN score is finally given in the format of  X  log ( p.value ) . The way to understand the CLEAN score is that minor differences on the scores mean very different coherence levels because it is a log value. Our Low-Variance 3-Cluster contains component biclusters which are highly functionally enriched as listed and the variance of the biclusters is also the lowest one among the biclusters found by all the other biclustering algorithms.
The other interesting test is made by comparing the gene subsets (rows) within each component bicluster. For each component bicluster in the Low-Variance 3-Cluster, we divided all the rows (genes), that is, all the genes , into two groups: one group containing genes that are shared by both of the component biclusters (and species) ( common genes ) and the other is unique genes that only exist in one component bicluster( unique genes ). We also use the CLEAN score to determine the functional enrichments of the gene sets in the L2L database, and the results are listed in Table II. The results tell us that the common genes are biologically important (functionally enriched with non-zero CLEAN score) in both mouse and human functional categories. And the unique genes are also important in both mouse or human biological functional categories. This result gives a very clear clue to the biomedical practitioners for further research on the need to explore relationships between the common genes and the unique genes . The number of common genes found by us, 437 genes, is also very large and points to some shared molecular and genetic processes between human and mouse species that are responsible for this disease.

In this paper, we have presented the formulation of algorithm for discovering Low-Variance 3-Clusters from two different data tables. Our results shows the discovery of 3-clusters in which the variance is kept under control while the overlap between the rows shared between the two components of a 3-Clusters is maximized. Low-Variance 3-Clusters open the door for new direction of cross-domain research and knowledge discovery.

