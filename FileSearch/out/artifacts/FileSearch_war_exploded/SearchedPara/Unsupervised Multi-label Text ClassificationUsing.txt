 The increasing availability of documents in the past decades has greatly pro-moted the development of information re trieval and organising systems, such as search engines and digital libraries. The widespread use of digital documents has also increased these systems X  accessibility to textual information. A fundamen-tal theory supporting these information retrieval and organising systems is that information can be associated with semantically meaningful categories. Such a theory supports also ontology learning, text categorisation, information filtering, text mining, and text analysis, etc. Text classification aims at associating tex-tual documents with semantically mean ingful categorises, and has been studied in the past decades, along with the development of information retrieval and organising systems [11].

Text classification is the process of classifying an incoming stream of docu-ments into predefined categories. Text classification usually employs a supervised learning strategy with the classifiers learned from pre-classified sample docu-ments. The classifiers are then used to classify incoming documents. In terms of supervised text classification, the p erformance is determined by the accuracy of pre-classified training samples and the quality of the categorisation. The ac-curacy of classifiers determines their capability of differentiating the incoming stream of documents; the descriptive and discriminative capacity of categorisa-tion reduces noise in classification, which i s caused by sense ambiguities, sparsity, and high dimensionality of the documents [7]. Text classification performance is also affected by the topic coverage of categories. An inadequate category may be assigned to a document if an in-compre hensive set of categories is employed, because non-adequate categories can be found. The performance of text classifi-cation relies upon the descriptive and discriminative capacity of categories and the accuracy of classifiers le arned from training sets.

However, there exist situations that a qualified training document set may not be available (e.g., the  X  X old start X  problem in recommender systems); a set of categories with in-comprehensive topic coverage may be used for classifica-tion; sometimes although a set of categories with comprehensive topic coverage is available, the large number of classes would easily introduce noise in classifi-cation results [5]. Traditionally, text classification models are designed to handle only single-label problems. However, in some circumstances (e.g., categorizing documents in library catalogue into multi ple subjects), multi-label text clas-sification is required and automatic classification is necessary, especially when classifying a very large volume of documents [15]. To deal with these prob-lems, in this paper we propose an automatic unsupervised text classification approach to classify documents into multiple classes, without the requirement of pre-classified sample documents for training classifiers. The approach consists of three modules; pattern mining for document feature extraction; feature-subject mapping for initial classification; knowledge generalisation for optimal classifica-tion. The method incorporates comprehensive world knowledge stored in a large ontology and classifies documents into the classes in the ontology without any pre-classified training samples available. The world ontology is built from Library of Congress Subject Headings (LCSH), which represents the natural growth and distribution of human intellectual work [4]. The subject classes and semantic relationships in the ontology are investigated and exploited to improve the clas-sification results. The proposed method was experimentally evaluated using a large library catalogue, by compared with typical text classification approaches. The presented work makes three-fold contributions:  X  An unsupervised text classification method that classifies documents into  X  A knowledge generalisation method to optimise text classification by  X  An exploration of using the LCSH as a world knowledge to facilitate text The paper is organised as follows. Section 2 discusses the related work; Section 3 introduces the research problem and t he the conceptual model of proposed su-pervised text classification method; Sect ion 4 presents the technical detail of the proposed method. The experiment design is described in Section 5, whereas the results are discussed in Section 6. Finally, Section 7 makes conclusions. Unsupervised text classification aims to classify documents into the classes with absence of any labelled training documents. In many occasions the target classes may not have any labelled training documents available. One particular example is the  X  X old start X  problem in recommender systems and social tagging. Unsu-pervised classification can automatically learn an annotation model to make recommendations or label the tags when the products or tags are rare and do not have any useful information associated. Unsupervised classification has been studied by many groups and many successful models have been proposed. With-out associated training samples, Yang et al. [16] built a classification model for a target class by analysing the correlating auxiliary classes. Though as similar as theirs in investigating correlating classes, our work is different by exploiting a hi-erarchical world knowledge ontology for classification, instead of only auxiliary classes. Also exploiting a world know ledge base, Yan et al. [14] examined un-supervised relation extraction from Wikipedia articles and integrated linguistic analysis with web frequency information to improve unsupervised classification performance. However, our work has different aims from theirs; ours aims to exploit a world knowledge ontology to help unsupervised classification, whereas Yan et al. [14] aims to extract semantic relations for Wikipedia concepts by using unsupervised classification techniques. Cai et al. [2] and Houle and Grira [6] pro-posed unsupervised approaches to evaluate and improve the quality of selecting features. Given a set of data, their work is to find a subset containing the most informative, discriminative features. Though the work presented in this paper also relies on features selected from docume nts, the features are further investi-gated with their referring-to ontologica l concepts to improve the performance of classification.

Text classification models are originally designed to handle only single-label problems, where each document is classified into only one class. However, in many circumstances single-label text classification cannot satisfy the demand, for example, in social network multiple labels may need to be suggested for a tag [8]. Comparing with the work done by Katakis et al. [8], our work relies on the semantic content of documents, rather than the meta-data of documents used in [8]. As similar as the work conducted by Yang et al. [15], our work also targets on multi-label text classification. However, Yang et al. [15] X  work is different in adopting active learning algorithms for multi-label classification, whereas ours exploits concepts and their structure in world knowledge ontologies.

Ontologies have been studied and exploited by many works to facilitate text classification. Gabrilovich and Markovitch [5] enhanced text classification by generating features using domain-specific and common-sense knowledge in large ontologies with hundreds of thousands of concepts. Comparing with their work, our work moves beyond feature discovery and investigates the hierarchical ontol-ogy structure for knowledge generalisation to improve text classification. Camous et al. [3] also introduced a domain-independent method that uses the Medical Subject Headings (MeSH) ontology. The method observes the inter-concept rela-tionships and represents documents by MeSH subjects. Similarly, Camous X  work considers the semantic relations existin g in the ontological concepts. However, their work focuses on only the medical domain, whereas our approach works on general areas because exploiting the LCSH, a superior world knowledge ontol-ogy. Another world ontology commonly used in text classification is Wikipedia. Wang and Domeniconi [13] and Hu et al. [7] derived background knowledge from Wikipedia to represent documents and attempted to deal with the sparsity and high dimensionality problems in text classification. Instead of Wikipedia with free-contributed entries, our work uses the superior LCSH ontology, which has been under continuous development for a hundred years by knowledge engineers.
Many works utilise pattern mining techniques to help build classification mod-els, which is similar as the strategy employed in our work. Malik and Kender [10] proposed the  X  X emocratic Classifier X , which is a pattern-based classification al-gorithm using short patterns. Different from our work, their democratic classifier relies on the quality of training samples and cannot deal with the  X  X o training set available X  problem. Bekkerman and Matan [1] argued that most of informa-tion on documents can be captured in phrases and proposed a text classification method that employs lazy learning from l abelled phrases. The phrases in their work are in fact a special form of sequential patterns that are used in our work for feature extraction of documents. Let D = { d i  X  D ,i =1 ,...,m } be a set of text documents; S = { s 1 ,...,s K } be a large set of classes, where K is the number of classes. If there is available a training set D t = { d j  X  D ,j = m +1 ,...,n } with y k j = { 0 , 1 } ,k =1 ,...,K provided for describing the likelihood of d j belonging to class s k ,itiseasytolearnabinary prediction function p ( y k | d ) and use it to classify d i  X  X  . However, our objective is to learn a prediction function p ( y k | d ) to classify d i into { s k } X  X  without D t available. We refer to this problem as unsupervised multi-label text classification .
The proposed classification method consi sts of three steps: feature extraction, initial classification, and optimising classification, using a world ontology. 3.1 World Ontology The world knowledge ontology is constructed from the Library of Congress Sub-ject Headings (LCSH), which is a knowledge system developed for organising information in large library collections. It has been under continuous develop-ment for over a hundred years to describe and classify human knowledge. Because of the endeavours dedicated by the knowledge engineers from generation to gen-eration, the LCSH has become a de facto standard for concept cataloguing and indexing, superior to other knowledge bases. Tao et al. [12] once compared the LCSH with the Library of Congress Classification, the Dewey Decimal Classifica-tion, and Yahoo! categorisation, and reported that the LCSH has broader topic coverage, more meaningful structure, and more accurate semantic relations. The LCSH has been widely used as a means for many knowledge engineering and management works [4]. In this work, the class set S = { s 1 ,...,s K } is encoded from the LCSH subject headings.
 Definition 1. (SUBJECT) Let S be the set of subjects, an element s  X  X  is a 4-tuple s := label, neighbour, ancestor, descendant ,where  X  label is a set of sequential terms describing s ; lable ( s )= { t 1 ,t 2 ,...,t n } ;  X  neighbour refers to the set of subjects in the LCSH that directly link to s ,  X  ancestor refers to the set of subjects directly and indirectly link to s and  X  descendant refers to the set of subjects directly and indirectly link to s and The semantic relationships of subjects are encoded from the references defined in the LCSH for subject headings, including Broader Term , Used for ,and Related to .The ancestor ( s ) in Definition 1 returns the Broader Term subjects of s ;the descendant ( s ) is the reversed function of ancestor ( s ), with additional subjects Used for s ;the neighbour ( s ) returns the subjects Related to s .
 With Definition 1, the world knowledge ontology is defined: Definition 2. (ONTOLOGY) Let O be a world ontology. O contains a set of subjects linked by their semantic relations in a hierarchical structure. O is a 3-tuple O := S , R , H S R ,where  X  S is the set of subjects defined in Definition 1;  X  R is the set of relations linking any pair of subjects;  X  H S R is the hierarchical structure of O constructed by S X R . 3.2 Document Features Various representations have been studied to formally describe text documents. The lexicon-based representation is ba sed on the statistic of occurring terms. Such a representation is easy to understand by users and systems. However, along with meaningful, representative feature s, some noisy terms ar e also extracted, caused by sense ambiguity of terms. To deal with this problem, pattern-based representation is studied, which uses frequent sequential patterns (phrases) to represent document contents [9]. The pa ttern-based representation is superior to lexicon-based, as the context of term s co-occurred in phrases is considered. However, the pattern-based presentation suffers from a limitation caused by the length of patterns. Though a long pattern is wealthy with information and so more discriminative, it usually has low frequency and as a result, becomes inapplicable. To overcome the problem, we represent the content of documents by a set of weighted closed frequent sequential patterns discovered by pattern mining techniques.
 Definition 3. (FEATURES) Given a document d = { t 1 ,t 2 , ...,t n } as a se-quential set of repeatable terms, the feature set, denoted as F ( d ) ,isasetof weighted phrase patterns, { p, w ( p ) } ,extractedfrom d that satisfies the follow-ing constraints:  X   X  p  X  X  ( d ) ,p  X  d .  X   X  p 1 ,p 2  X  X  ( d )( p 1 = p 2 ) ,p 1  X  p 2  X  p 2  X  p 1 .  X   X  p  X  X  ( d ) ,w ( p )  X  , a threshold. 3.3 Initial Classification The initial classification of d to s k  X  X  is done through accessing a term-subject matrix created by the subject s and their labels. Adopting the features discovered previously, we use a feature-subject mapping approach to initially assign subject classes to the document.
 Definition 4. (TERM-SUBJECT MATRIX) Let T be the term space of S , T = { t  X  ping exists: and its reverse mapping also exists:
Adopting Definition 3 and 4, we can initially classify d i  X  X  into a set of subjects using the following prediction: where I ( z ) is an indicator function that outputs 1 if z is true and zero, otherwise; 3.4 Generalised Classification The initial classification process easily generates noisy subjects because of direct feature-subject mapping. Against the problem, we introduce a method to gener-alise the initial subjects to optimise the classification. We observed that in initial classification some subjects extracted from the ontology are overlapping in their semantic space. Thus, we can optimise the classification result by keeping only the dominating subjects and pruning away those being dominated. This can be done by investigating the semantic relations existing between subjects. Let s 1 and s 2 be two subjects and s 1  X  ancestor ( s 2 )( s 2  X  descendant ( s 1 )). s 1 refers to an broader semantic space than s 2 and thus, is more general. Vice versa, s 2 is more specific and focused than s 1 . Hence, if some subjects are covered by a common ancestor, they can b e replaced by the common a ncestor without infor-mation loss. The common ancestor is unnecessary to be chosen from the initial classification result, as choosing an ex ternal common ancestor also satisfies the above rule. After generalising the initial classification result, we have a smaller set of subject classes, with no information lost but some focus. (The handling of focus problem is presented in next section.) Definition 5. (GENERALISED CLASSIFICATION) Given a document d and its initial classification result, a subject set denoted by S I ( d ) , the generalised classification result, denoted as S G ( d ) , is the set of subjects satisfying: 1.  X  s  X  S I ( d ) ,  X  s  X  S G ( d ) ,s = s ,s  X  descendants ( s ) . In this section, we present the technical details for implementing the proposed approach of unsupervised multi-label text classification.

Algorithm 1 describes the process of ext racting features to represent a docu-ment. The output is F ( d ), a set of closed frequent sequential patterns discovered from d . Adopting the prediction in Eq. (1), with F ( d ) the initial set of subjects, S ( d ), can be assigned to classify d . Taking into account the weights of feature patterns, we can evaluate t  X  d : All s  X  S I ( d ) can then be re-evaluated for their likelihood of being assigning to d with consideration of term evaluation and term distribution in s  X  S I ( d ). A prediction function can then be used to assess initial classification subjects for the second run of classification: where I ( z  X  ) returns the value of z if z  X  is true and zero, otherwise; filtering out noisy subjects. In experiments different values were tested for  X  . The results revealed that setting  X  as the top fifth z in S I ( d i ), a variable rather than a static value, gave the best performance. (Refer to Section 6 for detail.)
In the generalisation pha se, descendant subjects are replaced by their common ancestor subject. However , the common ancestor should not be too far away from the replaced descendants in the ontology st ructure. The focus will be significantly lost, otherwise. In implementation, we use only the lowest common ancestor (shortened by LCA) to replace its descendant subjects. The LCA is the common ancestor of a set of subjects, with the sho rtest distance to these subjects in the ontology structure. The LCA replaces descendant subjects with full information kept and minimised focus lost.

Algorithm 2 describes the process of gener alising the initial subject classes to optimise classification. The function str ( i, s ) describes the likelihood of assign s to d i and returns the value of I ( z  X  ) in Prediction function 0 y  X  i in Eq. (2). The function  X  ( s 1  X  s 2 ) returns a positive real number indicating the distance between two subjects. Such a distance is measured by counting the number of returns 0 s ,theLCAof s 1 and s 2 .Notethat  X  ( s 1  X  s 2 )  X  3, which restricts LCAs to three edges in distance. Subject s further than that in distance are too general; whereas using a highly-general subject for generalisation would severely jeopardise the focus of original subjects. (In the experiments,  X  ( s 1  X  s 2 )  X  3 and  X  5 were tested under the same environment in order to find a valid distance for tracking the competent LCA. The test ing results revealed that as of three the distance was better.) The experiments were performed, usin g a large corpus collected from the cat-alogue of a library using the LCSH for information organising. The title and content of each catalogue item were used to form the content of a document. The subject headings associated with the catalogue items were manually assigned by specialist librarians who were traine d to specify subjects for documents with-out bias [4]. The documents and subjects provided an ideal ground truth in the experiments to evaluate the effectivenes s of the proposed classification method. This objective evaluation methodology assured the solidity and reliability of the experimental evaluation.

The testing set was crawled from the online catalogue of library of the Uni-versity of Melbourne 1 . General text pre-processing techniques, such as stopword removal and word stemming (Porter stemming algorithm), were applied to the preparation of testing set for experiment s. In the experiments, we used only doc-uments containing at least 30 terms, resulted in 31,902 documents in the testing set. Documents shorter than that could hardly provided substantial frequent patterns for feature extraction, as revealed in the preliminary experiments.
Given that the LCSH ontology contains 394,070 subjects in our implementa-tion, the problem actually became a K -class text classification problem where K = |S| = 394 , 070, a very large number. Hence, we chose two typical multi-class classification approaches, Rocchio and k NN, as the baseline models in the experiments.

The performance of experimental mode ls was measured by precision and re-call, the modern evaluation methods in information retrieval and organising. In terms of text classification, precision was to measure the ability of a method to assign a document with only focusing subjects, and recall the ability to assign a document with all dealing subjects.

Taking into account K = |S| = 394070, in respect with the testing document set and the ground truth featured by the LCSH, the classification performance was evaluated by: where FT ( S )= grt referred to the ground truth subjects.

F 1 Measure as another common method used in information organising sys-tems was also employed in evaluation. We used micro -F 1 , which evaluated each document X  X  classification result first and then averaged the results for the final F 1 value. Greater F 1 values indicate better performance. Naming our proposed unsupervised classification approach as the UTC model, the experiments were to compare the effectiveness performance of the UTC model to the baselines, Rocchio and k NN models. Their effectiveness perfor-mances are depicted in Fig. 1 for the nu mber of documents with valid effective-ness ( &gt; 0), where the value axis indicates the effectiveness rate between 0 and 1; the category axis indicates the number of documents whose classification meets the respective accuracy rate. As shown in the figure, the effectiveness rates were measured by precision, recall, and F 1 Measure, where P ( x ) refers to the preci-sion results of experimental model x , R ( x ) the recall results, and F ( x )the F 1 Measure results. Their overall average performances are shown in Table 1.
F 1 Measure equally considers bot h precision and recall. Thus the F 1 Measure results can be deemed as an overall eff ectiveness performance. The average F 1 Measure result shown in Table 1 reveals that the UTC model has achieved a much better overall performance (0.125) than other two models (0.020 and 0.016). Such a performance is also confirmed by the detailed results depicted in Fig. 1 -the F ( UTC ) line is located at much higher bound level compared to the F ( Rocchio ) and F ( kNN ) lines.

Precision measures how accurate the classification is. In terms of this, the UTC model once again has outperformed the baseline models. The average precision results shown in Table 1 demonstrates the achievement (UTC 0.158 vs. Rocchio 0.020 and k NN 0.021). The precision results depicted in Fig. 1 illustrate the same conclusion; the P ( UTC )outperformedothers.

Recall measures the performance of classification by considering all dealing classes. The recall performance in the experiments shows a slightly different result, compared with those from F 1 Measure and precision performance. The Rocchio model achieved the best recall performance (0.290 on average), com-pared to that of the UTC model (0.135) and the k NN model (0.054). The result is also illustrated in Fig. 1, where R ( UTC ) lies in the middle of R ( Rocchio )and R ( kNN ).
There was a gap between the recall performance of the UTC and the Rocchio models. From the observation of recall results, we found that the classes as-signed by the Rocchio model were usually a large set of subjects (935 on av-erage), whereas the UTC model assigned documents with a reasonable number of subjects (16 on average) and the k NN results had an average size of 106. Due to the natural of recall measuremen t, more feature term would be cover if the subject size became larger. As a result, the Rocchio classification with the largest size achieved the best recall performance. The subject sets assigned by the k NN model had larger size than those assigned by the UTC. However, when expanding the classification by neighbours, a large deal of nosey data was also brought into the neighbourhood -the average number of neighbours arisen was 336. This was caused by the very large set and short length of documents in consideration. As a result, the classification became inaccurate though only the documents with the top cosine values were chosen to expand and only the subjects with the top similarity values were chosen to classify a document.
Different number of levels were tested in sensitivity study for choosing a right number of levels to find the lowest common a ncestor when generalising subjects for optimal classification. Table 2 displays the testing results for finding such a right level number. In the same experiment al environment, if tracing three levels to find a LCA the UTC model X  X  overall performance including F 1 Measure, precision, and recall was better than that of tracing five levels. In addition, tracing three levels only would give us better complexity. Therefore, we chose three levels to restrict the extent of finding CLAs. Text classification has been widely ex ploited to improve the performance in information retrieval, information organising, text categorisation, and knowl-edge engineering. Traditionally, text classification relies on the quality of target categorises and the accuracy of classifiers learned from training samples. Some-times qualified training samples may be unavailable; the set of categories used for classification may be with inadequate topic coverage. Sometimes documents may be classified into noisy classes because of large dimension of categories. Aiming to deal with these problems, in this paper we have introduced an un-supervised multi-label text classification method. Using a world ontology built from the LCSH, the method consists of three modules; closed frequent sequen-tial pattern mining for feature extraction; extracting subjects from the ontology for initial classification; and generalising subjects for optimal classification. The method has been promisingly evaluated by compared with typical text classi-fication methods, using a large real-world corpus, based on the ground truth encoded by human experts.

