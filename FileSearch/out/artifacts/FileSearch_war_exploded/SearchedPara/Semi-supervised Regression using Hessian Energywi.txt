 Central to semi-supervised learning is the question how unlabeled data can help in either classifica-tion or regression. A large class of methods for semi-supervised learning is based on the manifold assumption , that is, the data points do not fill the whole feature space but they are concentrated around a low-dimensional submanifold. Under this assumption unlabeled data points can be used to build adaptive regularization functionals which penalize variation of the regression function only along the underlying manifold.
 One of the main goals of this paper is to propose an appropriate regularization functional on a man-ifold, the Hessian energy, and show that it has favourable properties for semi-supervised regression compared to the well known Laplacian regularization [2, 12]. Opposite to the Laplacian regularizer, the Hessian energy allows functions that extrapolate, i.e. functions whose values are not limited to the range of the training outputs. Particularly if only few labeled points are available, we show that this extrapolation capability leads to significant improvements. The second property of the proposed Hessian energy is that it favors functions which vary linearly along the manifold, so-called geodesic functions defined later. By linearity we mean that the output values of the functions change linearly along geodesics in the input manifold. This property makes it particularly useful as a tool for semi-supervised dimensionality reduction [13], where the task is to construct user-defined embeddings based on a given subset of labels. These user-guided embeddings are supposed to vary smoothly or even linearly along the manifold, where the later case corresponds to a setting where the user tries to recover a low-distortion parameterization of the manifold. Moreover, due to user defined labels the interpretability of the resulting parameterization is significantly improved over unsupervised meth-ods like Laplacian [1] or Hessian [3] eigenmaps. The proposed Hessian energy is motivated by the recently proposed Eells energy for mappings between manifolds [11], which contains as a special case the regularization of real-valued functions on a manifold. In flavour, it is also quite similar to the operator constructed in Hessian eigenmaps [3]. However, we will show that their operator due to problems in the estimation of the Hessian, leads to useless results when used as regularizer for re-gression. On the contrary, our novel estimation procedure turns out to be more stable for regression and as a side effects leads also to a better estimation of the eigenvectors used in Hessian eigenmaps. We present experimental results on several datasets, which show that our method for semi-supervised regression is often superior to other semi-supervised and supervised regression techniques. Our approach for regression is based on regularized empirical risk minimization. First, we will corresponding to the case where we have access to an unlimited number of unlabeled data. In the following we denote by M the m -dimensional data-submanifold in R d . The supervised regression problems for a training set of l points ( X i ,Y i ) l i =1 can then be formulated as, where C  X  ( M ) is the set of smooth functions on M , L : R  X  R  X  R is the loss function and S : C  X  ( M )  X  R is the regularization functional. For simplicity we use the squared loss L ( y,f ( x )) = ( y  X  f ( x )) 2 , but the framework can be easily extended to other convex loss functions. Naturally, we do not know the manifold M the data is lying on. However, we have unlabeled data which can be used to estimate it, or more precisely we can use the unlabeled data to build an estimate  X  For the moment we just want to discuss regularization functionals in the ideal case, where we know the manifold. However, we would like to stress already here that for our framework to work it does not matter if the data lies on or close to a low-dimensional manifold. Even the dimension can change from point to point. The only assumption we make is that the data generating process does not fill the whole space but is concentrated on a low-dimensional structure.
 Regularization on manifolds. Our main goal is to construct a regularization functional on mani-folds, which is particularly suited for semi-supervised regression and semi-supervised dimensional-ity reduction. We follow here the framework of [11] who discuss regularization of mappings between manifolds, where we are interested in the special case of real-valued output. They propose to use the so called Eells -energy S Eells ( f ) , which can be written for real-valued functions, f : M  X  R , as, where  X  a  X  b f is the second covariant derivative of f and dV ( x ) is the natural volume element, see [7]. Note, that the energy is by definition independent of the coordinate representation and depends only on the properties of M . For details we refer to [11]. This energy functional looks quite abstract. However, in a special coordinate system on M , the so called normal coordinates, one can evaluate it quite easily. In sloppy terms, normal coordinates at a given point p are coordinates on M such that the manifold looks as Euclidean as possible (up to second order) around p . Thus in normal coordinates x r centered at p , so that at p the norm of the second covariant derivative is just the Frobenius norm of the Hessian of f in normal coordinates. Therefore we call the resulting functional the Hessian regularizer S Hess ( f ) . Before we discuss the discretization, we would like to discuss some properties of this regularizer. In particular, its difference to the regularizer S  X  ( f ) using the Laplacian, proposed by Belkin and Niyogi [2] for semi-supervised classification and in the meantime also adopted for semi-supervised regression [12]. While this regularizer makes sense for classification, it is of limited use for regression. The problem is that the null space N S = { f  X  C  X  ( M ) | S ( f ) = 0 } of S  X  , that is the functions which are not penalized, are only the constant functions on M . The following adaptation of a result in [4] shows that the Hessian regularizer has a richer null-space. Proposition 1 (Eells, Lemaire [4]) A function f : M  X  R with f  X  C  X  ( M ) has zero second derivative,  X  a  X  b f ized by arc length s , there exists a constant c  X  depending only on  X  such that We call functions f which fulfill  X   X  X  f (  X  ( s )) = const. geodesic functions. They correspond to linear maps in Euclidean space and encode a constant variation with respect to the geodesic distance of the manifold. It is however possible that apart from the trivial case f = const. no other geodesic functions exist on M . What is the implication of these results for regression? First, the use of Lapla-cian regularization leads always to a bias towards the constant function and does not extrapolate beyond data points. On the contrary, Hessian regularization is not biased towards constant functions if geodesic functions exist and extrapolates  X  X inearly X  (if possible) beyond data points. These crucial differences are illustrated in Figure 1 where we compare Laplacian regularization using the graph Laplacian as in [2] to Hessian regularization as introduced in the next section for a densely sampled spiral. Since the spiral is isometric to a subset of R , it allows  X  X eodesic X  functions. As discussed in the last section unlabeled data provides us valuable information about the data man-ifold. We use this information to construct normal coordinates around each unlabeled point, which requires the estimation of the local structure of the manifold. Subsequently, we employ the normal coordinates to estimate the Hessian regularizer using the simple form of the second covariant deriva-tive provided in Equation (1). It turns out that these two parts of our construction are similar to the one done in Hessian eigenmaps [3]. However, their estimate of the regularizer has stability prob-lems when applied to semi-supervised regression as is discussed below. In contrast, the proposed method does not suffer from this short-coming and leads to significantly better performance. The solution of the semi-supervised regression problem is obtained by solving a sparse linear system. In the following, capital letters X i correspond to sample points and x r denote normal coordinates. Construction of local normal coordinates. The estimation of local normal coordinates can be done using the set of k nearest neighbors (NN) N k ( X i ) of point X i . The cardinality k will be chosen later on by cross-validation. In order to estimate the local tangent space T X i M (seen as an m -dimensional affine subspace of R d ), we perform PCA on the points in N k ( X i ) . The m leading eigenvectors then correspond to an orthogonal basis of T X i M . In the ideal case, where one has a densely sampled manifold, the number of dominating eigenvalues should be equal to the dimension m . However, for real-world datasets like images the sampling is usually not dense enough so that the dimension of the manifold can not be detected automatically. Therefore the number of dimensions has to be provided by the user using prior knowledge about the problem or alternatively, and this is the way we choose in this paper, by cross-validation.
 Having the exact tangent space T X i M one can determine the normal coordinates x r of a point X j  X  N k ( X i ) as follows. Let { u r } m r =1 be the m leading PCA eigenvectors, which have been normalized, then the normal coordinates { x r } m r =1 of X j are given as, where the first term is just the projection of the difference vector, X j  X  X i , on the basis vector u r  X  T
X i M and the second component is just a rescaling to fulfill the property of normal coordinates that the distance of a point X j  X  M to the origin (corresponding to X i ) is equal to the geodesic distance d
M ( X j ,X i ) of X j to X i on M , k x ( X j ) k sense only if local geodesic distances can be accurately estimated. In our experiments, this was only  X  u r ,X j  X  X i  X  as normal coordinates. In [11] it is shown that this replacement yields an error of order O ( k X  a f k 2  X  2 ) in the estimation of k X  a  X  b f k 2 , where  X  is the maximal principal curvature (the curvature of M with respect to the ambient space R d ).
 Estimation of the Hessian energy. The Hessian regularizer, the squared norm of the second co-variant derivative, k X  a  X  b f k 2 , corresponds to the Frobenius norm of the Hessian of f in normal coordinates, see Equation 1. Thus, given normal coordinates x r at X i we would like to have an operator H which given the function values f ( X j ) on N k ( X i ) estimates the Hessian of f at X i , where the zeroth-order term is fixed at f ( X i ) . In the limit as the neighborhood size tends to zero, p ( i ) ( x ) becomes the second-order Taylor expansion of f around X i , that is, with A rs = A sr . In order to fit the polynomial we use standard linear least squares, where  X   X  R k  X  P is the design matrix with P = m + m ( m +1) 2 . The corresponding basis functions  X  , at X i ) of X j  X  N k ( x i ) up to second order. The solution w  X  R P is w =  X  + f , where f  X  R k and f = f ( X j ) with X j  X  N k ( X i ) and  X  + denotes the pseudo-inverse of  X  .
 Note, that the last m ( m +1) 2 components of w correspond to the coefficients A rs of the polynomial (up to rescaling for the diagonal components) and thus with Equation (3) we obtain the desired form H rsj . An estimate of the Frobenius norm of the Hessian of f at X i is thus given as, over all data points, where n denotes the number of unlabeled and labeled points, where B is the accumulated matrix summing up all the matrices B ( i ) . Note, that B is sparse since each point X i has only contributions from its neighbors.
 Moreover, since we sum up the energy over all points, the squared norm of the Hessian is actu-ally weighted with the local density of the points leading to a stronger penalization of the Hessian in densely sampled regions. The same holds for the estimate  X  S  X  ( f ) of Laplacian regularization,  X  S  X  ( f ) = P rigorous connection between  X  S  X  ( f ) and S  X  ( f ) has been established in [2, 5]).
 The effect of non-uniform sampling can be observed in Figure 1. There the samples of the spiral are generated by uniform sampling of the angle leading to a more densely sampled  X  X nterior X  region, Hessian energy this phenomena cannot be seen in this example, since the Hessian of a  X  X eodesic X  function is zero everywhere and therefore it does not matter if it is weighted with the density. On the other hand for non-geodesic functions the weighting matters also for the Hessian energy. We did not try to enforce a weighting with respect to the uniform density. However, it would be no problem to compensate the effects of non-uniform sampling by using a weighted from of the Hessian energy. Final algorithm. Using the ideas of the previous paragraphs the final algorithmic scheme for semi-supervised regression can now be immediately stated. We have to solve, where for notational simplicity we assume that the data is ordered such that the first l points are labeled. The solution is obtained by solving the following sparse linear system, where I 0 is the diagonal matrix with I 0 ii = 1 if i is labeled and zero else and Y i = 0 if i is not labeled. The sparsity structure of B is mainly influencing the complexity to solve this linear system. However, the number of non-zeros entries of B is between O ( nk ) and O ( nk 2 ) depending on how well behaved the neighborhoods are (the later case corresponds basically to random neighbors) and thus grows linearly with the number of data points.
 Stability of estimation procedure of Hessian energy. Since we optimize the objective in Equa-tion (4) for any possible assignment of function values f on the data points, we have to ensure that the estimation of the Hessian is accurate for any possible function. However, the quality of the es-timate of the Hessian energy depends on the quality of the local fit of p ( i ) for each data point X i . Clearly, there are function assignments where the estimation goes wrong. If k &lt; P ( P is the number of parameters of the polynomial) p can overfit the function and if k &gt; P then p generally underfits. In both cases, the Hessian estimation is inaccurate. Most dangerous are the cases where the norm of the Hessian is underestimated in particular if the function is heavily oscillating. Note that during the estimation of local Hessian, we do not use the full second-order polynomial but fix its zeroth-order much more likely if one fits a full second-order polynomial since the additional flexibility in fitting the constant term always reduces the Hessian estimate. In the worst case a function which is heavily oscillating can even have zero Hessian energy, if it allows a linear fit at each point, see Figure 3. If such a function fits the data well we get useless regression results 1 see Fig. 3. While fixing the con-stant term does not completely rule out such undesired behavior, we did not observe such irregular solutions in any experiment. In the appendix we discuss a modification of (Eq. (4)) which rules out Figure 2: Fitting two points on the spiral revis-ited (see Fig. 1): Left image shows the regres-sion result f using the Hessian energy estimated by fitting a full polynomial in normal coordi-nates. The Hessian energy of this heavily oscil-lating function is 0, since every local fit is lin-ear (an example shown in the right image; green curve). However, fixing the zeroth-order term yields a high Hessian energy as desired (local fit is shown as the red curve in the right image).
 for sure irregular solutions, but since it did not lead to significantly better experimental results and requires an additional parameter to tune we do not recommend to use it.
 Our estimation procedure of the Hessian has similar motivation as the one done in Hessian eigen-maps [3]. However, in their approach they do not fix the zeroth-order term. This seems to be suitable for Hessian eigenmaps as they do not use the full Hessian, but only its m + 1 -dimensional null space (where m is the intrinsic dimension of the manifold). Apparently, this resolves the issues discussed above so that the null space can still be well estimated also with their procedure. However, us-ing their estimator for semi-supervised regression leads to useless results, see Fig. 3. Moreover, we would like to note that using our estimator not only the eigenvectors of the null space but also eigenvectors corresponding to higher eigenvalues can be well estimated, see Fig. 3. We test our semi-supervised regression method using Hessian regularization on one synthetic and two real-world data sets. We compare with the results obtained using Laplacian-based regulariza-tion and kernel ridge regression (KRR) trained only with the labeled examples. The free parameters for our method are the number of neighbors k for k-NN, the dimensionality of the PCA subspace, and the regularization parameter  X  while the parameters for the Laplacian regularization-based re-gression are: k for k-NN, the regularization parameter and the width of the Gaussian kernel. For KRR we used also the Gaussian kernel with the width as free parameter. These parameters were chosen for each method using 5-fold cross-validation on the labeled examples. For the digit and figure datasets, the experiments were repeated with 5 different assignments of labeled examples. Digit Dataset. In the first set of experiments, we generated 10000 random samples of artificially generated images (size 28  X  28 ) of the digit 1. There are four variations in the data: translation (two variations), rotation and line thickness. For this dataset we are doing semi-supervised dimensionality reduction since the task is to estimate the natural parameters which were used to generate the digits. This is done based on 50 and 100 labeled images. Each of the variation corresponds then to a separate regression problem which we finally stick together to get an embedding into four dimensions. Note, that this dataset is quite challenging since translation of the digit leads to huge Euclidean distances between digits although they look visually very similar. Fig. 2 and Table 1 summarize the results. As observed in the first row of Fig. 2, KRR ( K ) and Hessian ( H ) regularization recover well the two parameters of line width and rotation (all other embeddings can be found in the supplementary material). As discussed previously, the Laplacian ( L ) regularization tends to shrink the estimated parameters towards a constant as it penalizes the  X  X eodesic X  functions. This results in the poor estimation of parameters, especially the line-thickness parameter. 2 Although KRR estimates well the thickness parameter, it fails for the rotation parameter (cf. the second row of Fig. 2 where we Figure 2: Results on the digit 1 dataset. First row: the 2D-embedding of the digits obtained by regression for the rotation and thickness parameter with 100 labels. Second row: 21 digit images sampled at regular intervals in the estimated parameter spaces: two reference points (inverted im-ages) are sampled in the ground truth parameter space and then in the corresponding estimated embedding. Then, 19 points are sampled in the estimated parameter spaces based on linear in-ter/extrapolation of the parameters. The shown image samples are the ones which have parameters closest to the interpolated ones. In each parameter space the interpolated points, the corresponding closest data points and the reference points are marked with red dots, blue and cyan circles. show the images corresponding to equidistant inter/extrapolation in the estimated parameter space between two fixed digits (inverted image)). The Hessian regularization provided a moderate level of accuracy in recovering the thickness parameter and performed best on the remaining ones. Figure Dataset. The second dataset consists of 2500 views of a toy figure (see Fig. 3) sampled based on regular intervals in zenith and azimuth angles on the upper hemisphere around the centered the azimuth angle is transformed into Euclidean x,y coordinates. 3 Both Laplacian and Hessian regularizers provided significantly better estimation of the parameters in comparison to KRR, which demonstrates the effectiveness of semi-supervised regression. However, the Laplacian shows again contracting behavior which is observed in the top view of hemisphere. Note that for our method this does not occur and the spacing of the points in the parameter space is much more regular, which again stresses the effectiveness of our proposed regularizer.
 Image Colorization. Image colorization refers to the task of estimating the color components of a given gray level image. Often, this problem is approached based on the color information of a subset of pixels in the image, which is specified by a user (cf. [8] for more details). This is essentially a semi-supervised regression problem where the user-specified color components correspond to the labels. To facilitate quantitative evaluation, we adopted 20 color images, sampled a subset of pixels in each image as labels, and used the corresponding gray levels as inputs. The number of labeled points were 30 and 100 for each images, which we regard as a moderate level of user intervention. As error measure, we use the mean square distance between the original image and the corresponding Figure 3: Results of regression on the figure dataset. First row: embedding in the three dimensional spaces with 50 labels. Second row: Left: some example images of the dataset, Right: error plots for each regression variable for different number of labeled points.
 Original image KRR L H Levin et al. [8] Figure 4: Example of image colorization with 30 labels. KRR failed in reconstructing (the color of) the red pepper at the lower-right corner, while the Laplacian regularizer produced overall, a greenish image. Levin et al X  X  method well-recovered the lower central part however failed in reconstructing the upper central pepper. Despite the slight diffusion of red color at the upper-left corner, overall, the result of Hessian regularization looks best which is also confirmed by the reconstruction error. reconstruction in the RGB space. During the colorization, we go over to the YUV color model such that the Y components, containing the gray level values, are used as the input, based on which the U and V components are estimated. The estimated U-V components are then combined with the Y component and converted into RGB format. For the regression, for each pixel, we use as features the 3  X  3 -size image patch centered at the pixel of interest plus the 2-dimensional coordinate value of that pixel. The coordinate values are weighted by 10 such that the contribution of coordinate values and gray levels is balanced. For comparison, we performed experiments with the method of Levin et al. [8] as one of the state-of-the-art methods. 4 Figure 4 shows an example and Table 2 summarizes the results. The Hessian regularizer clearly outperformed the KRR and the Laplacian-based regression and produced slightly better results than those of Levin et al. [8]. We expect that the performance can be further improved by exploiting a priori knowledge on structure of natural images (e.g., by exploiting the segmentation information (cf. [9, 6]) in the NN structure).
Table 2: Results on colorization: mean squared error (standard deviation) (both in units 10  X  3 ). [1] M. Belkin and P. Niyogi. Laplacian eigenmaps for dimensionality reduction and data repre-[2] M. Belkin and P. Niyogi. Semi-supervised learning on manifolds. Machine Learning , 56:209 X  [3] D. Donoho and C. Grimes. Hessian eigenmaps: Locally linear embedding techniques for high-[4] J. Eells and L. Lemaire. Selected topics in harmonic maps . AMS, Providence, RI, 1983. 3 [5] M. Hein. Uniform convergence of adaptive graph-based regularization. In G. Lugosi and [6] R. Irony, D. Cohen-Or, and D. Lischinski. Colorization by example. In Proc. Eurographics [7] J. M. Lee. Riemannian Manifolds -An introduction to curvature . Springer, New York, 1997. 2 [8] A. Levin, D. Lischinski, and Y. Weiss. Colorization using optimization. In Proc. SIGGRAPH , [9] Q. Luan, F. Wen, D. Cohen-Or, L. Liang, Y.-Q. Xu, and H.-Y. Shum. Natural image coloriza-[10] G. Peters. Efficient pose estimation using view-based object representations. Machine Vision [11] F. Steinke and M. Hein. Non-parametric regression between Riemannian manifolds. In Ad-[12] J. J. Verbeek and N. Vlassis. Gaussian fields for semi-supervised regression and correspon-[13] X. Yang, H. Fu, H. Zha, and J. Barlow. Semi-supervised nonlinear dimensionality reduction.
