 { koray,sermanet,ylan,kgregor,yann } @cs.nyu.edu, mmathieu@clipper.ens.fr Over the last few years, a growing amount of research on visua l recognition has focused on learning low-level and mid-level features using unsupervised learn ing, supervised learning, or a combination of the two. The ability to learn multiple levels of good featu re representations in a hierarchical structure would enable the automatic construction of sophi sticated recognition systems operating, not just on natural images, but on a wide variety of modalitie s. This would be particularly useful for sensor modalities where our lack of intuition makes it diffic ult to engineer good feature extractors. The present paper introduces a new class of techniques for le arning features extracted though con-volutional filter banks . The techniques are applicable to Convolutional Networks a nd their variants, which use multiple stages of trainable convolutional filter banks, interspersed with non-linear oper-ations, and spatial feature pooling operations [1, 2]. While ConvNets have traditionally been trained in supervised mode, a number of recent systems have proposed to use unsupervised learning to pre-train the filters, followed by supervised fine-tuning. Some a uthors have used convolutional forms of Restricted Boltzmann Machines (RBM) trained with contrast ive divergence [3], but many of them have relied on sparse coding and sparse modeling [4, 5, 6]. In sparse coding, a sparse feature vector z is computed so as to best reconstruct the input x through a linear operation with a learned dictionary matrix D . The inference procedure produces a code z  X  by minimizing an energy function: Figure 1: Left: A dictionary with 128 elements, learned with patch based spa rse coding model. Right: A dictionary with 128 elements, learned with convolutional sparse coding model. The dic-tionary learned with the convolutional model spans the orie ntation space much more uniformly. In addition it can be seen that the diversity of filters obtained by convolutional sparse model is much richer compared to patch based one.
 The dictionary is obtained by minimizing the energy 1 wrt D : min training set of input samples. There are two problems with th e traditional sparse modeling method when training convolutional filter banks: 1: the representa tions of whole images are highly redun-dant because the training and the inference are performed at the patch level; 2: the inference for a whole image is computationally expensive.
 First problem. In most applications of sparse coding to image analysis [7, 8 ], the system is trained on single image patches whose dimensions match those of the filters. After training, patches in the image are processed separately. This procedure complet ely ignores the fact that the filters are eventually going to be used in a convolutional fashion. Lear ning will produce a dictionary of filters that are essentially shifted versions of each other over the patch, so as to reconstruct each patch in isolation. Inference is performed on all (overlapping) p atches independently, which produces a very highly redundant representation for the whole image. T o address this problem, we apply sparse coding to the entire image at once, and we view the dictionary as a convolutional filter bank: where D feature map of dimension ( w + s  X  1)  X  ( h + s  X  1) , and  X   X   X  denotes the discrete convolution operator. Convolutional Sparse Coding has been used by seve ral authors, notably [6].
 To address the second problem , we follow the idea of [4, 5], and use a trainable, feed-forwa rd, non-linear encoder module to produce a fast approximation of the sparse code. Th e new energy function includes a code prediction error term: where z  X  = arg min is a point-wise non-linear function. Two crucially importa nt questions are the form of the non-linear function f , and the optimization method to find z  X  . Both questions will be discussed at length below. The contribution of this paper is to address both issues simu ltaneously, thus allowing convolutional approaches to sparse coding to scale up, and opening the road to real-time applications. In this section, we analyze the benefits of convolutional spa rse coding for object recognition systems, and propose convolutional extensions to the coordinate des cent sparse coding (CoD) [9] algorithm and the dictionary learning procedure. 2.1 Learning Convolutional Dictionaries The key observation for modeling convolutional filter banks is that the convolution of a signal with a given kernel can be represented as a matrix-vector product by constructing a special Toeplitz-structured matrix for each dictionary element and concaten ating all such matrices to form a new incurs a cost, since the size of the dictionary then depends o n the size of the input signal. Therefore, it is advantageous to use a formulation based on convolution s rather than following the naive method outlined above. In this work, we use the coordinate descent s parse coding algorithm [9] as a starting convolutional dictionaries: 1. The boundary effects due to convolutions need to be properly handled. 2. The derivative of equation 2 should be computed efficientl y. Since the loss is not jointly convex in
D and z , but is convex in each variable when the other one is kept fixed , sparse dictionaries are usually learned by an approach similar to block coordinate d escent, which alternatively minimizes over z and D (e.g., see [10, 8, 4]). One can use either batch [7] (by accumu lating derivatives over many samples) or online updates [8, 6, 5] (updating the dicti onary after each sample). In this work, we use a stochastic online procedure for updating the dictio nary elements.
 The updates to the dictionary elements, calculated from equ ation 2, are sensitive to the boundary effects introduced by the convolution operator. The code un its that are at the boundary might grow much larger compared to the middle elements, since the outer most boundaries of the reconstruction take contributions from only a single code unit, compared to the middle ones that combine s  X  s units. Therefore the reconstruction error, and correspondingly t he derivatives, grow proportionally larger. One way to properly handle this situation is to apply a mask on the derivatives of the reconstruction error wrt z : D T  X  ( x  X  X  X  z ) is replaced by D T  X  ( mask ( x )  X  X  X  z ) , where mask is a term-by-term multiplier that either puts zeros or gradually scales down t he boundaries.
 Algorithm 1 Convolutional extension to coordinate descent sparse codi ng[9]. A subscript index (set) of a matrix represent a particular element. For slicin g the 4 D tensor S we adopt the MATLAB notation for simplicity of notation. function ConvCoD ( x, D ,  X  ) end function The second important point in training convolutional dicti onaries is the computation of the S = D T  X  D operator. For most algorithms like coordinate descent [9], FISTA [11] and matching pur-suit [12], it is advantageous to store the similarity matrix ( S ) explicitly and use a single column at a time for updating the corresponding component of code z . For convolutional modeling, the same approach can be followed with some additional care. In patch based sparse coding, each element ( i, j ) of S equals the dot product of dictionary elements i and j . Since the similarity of a pair of dictionary elements has to be also considered in spatial dim ensions, each term is expanded as  X  X ull X  convolution of two dictionary elements ( i, j ) , producing 2 s  X  1  X  2 s  X  1 matrix. It is more convenient to think about the resulting matrix as a 4 D tensor of size K  X  K  X  2 s  X  1  X  2 s  X  1 . One should note that, depending on the input image size, proper alignme nt of corresponding column of this tensor has to be applied in the z space. One can also use the steepest descent algorithm for fin ding the solution to convolutional sparse coding given in equati on 2, however using this method would be orders of magnitude slower compared to specialized algor ithms like CoD [9] and the solution would never contain exact zeros. In algorithm 1 we explain th e extension of the coordinate descent algorithm [9] for convolutional inputs. Having formulated convolutional sparse coding, the overall learning procedure is simple stochastic (online) gradient descent over dictionary D : The columns of D are normalized after each iteration. A convolutional dicti onary with 128 elements which was trained on images from Berkeley dataset [13] is sho wn in figure 1. Figure 2: Left: Smooth shrinkage function. Parameters  X  and b control the smoothness and location of the kink of the function. As  X   X   X  it converges more closely to soft thresholding operator. Center: Total loss as a function of number of iterations. The vertica l dotted line marks the iteration number when diagonal hessian approximation was updated. It is clear that for both encoder func-tions, hessian update improves the convergence significant ly. Right: 128 convolutional filters ( W ) learned in the encoder using smooth shrinkage function. The decoder of this system is shown in image 1. 2.2 Learning an Efficient Encoder In [4], [14] and [15] a feedforward regressor was trained for fast approximate inference. In this work, we extend their encoder module training to convolutio nal domain and also propose a new encoder function that approximates sparse codes more close ly. The encoder used in [14] is a simple feedforward function which can also be seen as a small convol utional neural network:  X  z = g k  X  tanh ( x  X  W k ) ( k = 1 ..K ) . This function has been shown to produce good features for ob ject recognition [14], however it does not include a shrinkage op erator, thus its ability to produce sparse representations is very limited. Therefore, we propose a di fferent encoding function with a shrinkage operator. The standard soft thresholding operator has the n ice property of producing exact zeros around the origin, however for a very wide region, the deriva tives are also zero. In order to be able to train a filter bank that is applied to the input before the sh rinkage operator, we propose to use an encoder with a smooth shrinkage operator  X  z = sh Note that each  X  k and b k is a singleton per each feature map k . The shape of the smooth shrinkage operator is given in figure 2 for several different values of  X  and b . It can be seen that  X  controls the smoothness of the kink of shrinkage operator and b controls the location of the kink. The function can be easily written and these parameters can be learned fro m data.
 Updating the parameters of the encoding function is perform ed by minimizing equation 3. The ad-ditional cost term penalizes the squared distance between o ptimal code z and prediction  X  z . In a sense, training the encoder module is similar to training a C onvNet. To aid faster convergence, we use stochastic diagonal Levenberg-Marquardt method [16] t o calculate a positive diagonal approx-imation to the hessian. We update the hessian approximation every 10000 samples and the effect of hessian updates on the total loss is shown in figure 2. It can be seen that especially for the tanh encoder function, the effect of using second order informat ion on the convergence is significant. 2.3 Patch Based vs Convolutional Sparse Modeling Natural images, sounds, and more generally, signals that di splay translation invariance in any di-mension, are better represented using convolutional dicti onaries. The convolution operator enables the system to model local structures that appear anywhere in the signal. For example, if k  X  k image patches are sampled from a set of natural images, an edge at a g iven orientation may appear at any location, forcing local models to allocate multiple dictio nary elements to represent a single underly-ing orientation. By contrast, a convolutional model only ne eds to record the oriented structure once, since dictionary elements can be used at all locations. Figu re 1 shows atoms from patch-based and convolutional dictionaries comprising the same number of e lements. The convolutional dictionary does not waste resources modeling similar filter structure a t multiple locations. Instead, it mod-els more orientations, frequencies, and different structu res including center-surround filters, double center-surround filters, and corner structures at various a ngles.
 In this work, we present two encoder architectures, 1. steep est descent sparse coding with tanh encoding function using g k  X  tanh ( x  X  W k ) , 2. convolutional CoD sparse coding with shrink encoding function using sh higher than for the second system due to steepest descent spa rse coding. However, the performance of the encoding functions are almost identical. 2.4 Multi-stage architecture Our convolutional encoder can be used to replace patch-base d sparse coding modules used in multi-stage object recognition architectures such as the one prop osed in our previous work [14]. Building on our previous findings, for each stage, the encoder is follo wed by and absolute value rectifica-tion, contrast normalization and average subsampling. Absolute Value Rectification is a simple pointwise absolute value function applied on the output of t he encoder. Contrast Normalization is the same operation used for pre-processing the images. Th is type of operation has been shown to reduce the dependencies between components [17, 18] (fea ture maps in our case). When used in between layers, the mean and standard deviation is calculat ed across all feature maps with a 9  X  9 neighborhood in spatial dimensions. The last operation, average pooling is simply a spatial pooling operation that is applied on each feature map independently .
 One or more additional stages can be stacked on top of the first one. Each stage then takes the different architectural parameters like size and connecti ons. When the input to a stage is a series of feature maps, each output feature map is formed by the summat ion of multiple filters.
 In the next sections, we present experiments showing that us ing convolutionally trained encoders in this architecture lead to better object recognition perfor mance. We closely follow the architecture proposed in [14] for obje ct recognition experiments. As stated above, in our experiments, we use two different systems: 1. Steepest descent sparse coding with tanh encoder: SD tanh . 2. Coordinate descent sparse coding with shrink encoder: CD shrink . In the following, we give details of the unsupervised training and supervised recognition experiments. 3.1 Object Recognition using Caltech 101 Dataset The Caltech-101 dataset [19] contains up to 30 training imag es per class and each image contains a single object. We process the images in the dataset as follo ws: 1. Each image is converted to gray-scale and resized so that the largest edge is 151 . 2. Images are contrast normalized to obtain locally zero mean and unit standard deviation input using a 9  X  9 neighborhood. 3. The short side of each image is zero padded to 143 pixels. We report the results in Table 1 and 2. All results in these tables are obtained using 30 training samples per clas s and 5 different choices of the training set. We use the background class during training and testing .
 Architecture : We use the unsupervised trained encoders in a multi-stage sy stem identical to the one proposed in [14]. At first layer 64 features are extracted from the input image, followed by a second layers that produces 256 features. Second layer feat ures are connected to fist layer features through a sparse connection table to break the symmetry and t o decrease the number of parameters. Unsupervised Training : The input to unsupervised training consists of contrast nor malized gray-scale images [20] obtained from the Berkeley segmentation d ataset [13]. Contrast normalization consists of processing each feature map value by removing th e mean and dividing by the standard deviation calculated around 9  X  9 region centered at that value over all feature maps.
 First Layer: We have trained both systems using 64 dictionary elements. Each dictionary item is a 9  X  9 convolution kernel. The resulting system to be solved is a 64 times overcomplete sparse coding problem. Both systems are trained for 10 different sp arsity values ranging between 0 . 1 and 3 . 0 .
 Second Layer: Using the 64 feature maps output from the first layer encoder on Berkeley i mages, we train a second layer convolutional sparse coding. At the s econd layer, the number of feature maps is 256 and each feature map is connected to 16 randomly selected input features out of 64 . Thus, we aim to learn 4096 convolutional kernels at the second layer. To the best of our knowledge, none of the previous convolutional RBM [3] and sparse coding [6] methods have learned such a large number of dictionary elements. Our aim is motivated by the fact that using such large number of elements and using a linear classifier [14] reports recogn ition results similar to [3] and [6]. In both of these studies a more powerful Pyramid Match Kernel SV M classifier [21] is used to match the same level of performance. Figure 3 shows 128 filters that connect to 8 first layer features. Each Figure 3: Second stage filters. Left: Encoder kernels that correspond to the dictionary elements . Right: 128 dictionary elements, each row shows 16 dictionary eleme nts, connecting to a single second layer feature map. It can be seen that each group extra cts similar type of features from their corresponding inputs. row of filters connect a particular second layer feature map. It is seen that each row of filters extract similar features since their output response is summed toge ther to form one output feature map. stage architecture. Each system is trained using 64 convolu tional filters. The recognition accuracy results shown are very similar for both systems.
 One Stage System: We train 64 convolutional unsupervised features using both SD tanh and lute value rectification, contrast normalization and avera ge pooling. The convolutional filters used are 9  X  9 . The average pooling is applied over a 10  X  10 area with 5 pixel stride. The output of first layer is then 64  X  26  X  26 and fed into a logistic regression classifier and Lazebnik X  X  PMK-SVM classifier [21] (that is, the spatial pyramid pipeline is use d, using our features to replace the SIFT features).
 Two Stage System: We train 4096 convolutional filters with SD tanh method using 64 input feature maps from first stage to produce 256 feature maps. The second l ayer features are also 9  X  9 , pro-ducing 256  X  18  X  18 features. After applying absolute value rectification, con trast normalization and average pooling (on a 6  X  6 area with stride 4 ), the output features are 256  X  4  X  4 ( 4096 ) dimensional. We only use multinomial logistic regression c lassifier after the second layer feature extraction stage.
 We denote unsupervised trained one stage systems with U , two stage unsupervised trained systems with U U and  X  +  X  represents supervised training is performed afterwards. R stands for randomly initialized systems with no unsupervised training.
 Table 2: Recognition accuracy on Caltech 101 dataset using a variety of different feature represen-tations using two stage systems and two different classifier s.
 Comparing our U system using both SD tanh and CD shrink ( 57 . 1% and 57 . 3% ) with the 52 . 2% re-ported in [14], we see that convolutional training results i n significant improvement. With two layers of purely unsupervised features ( U U , 65 . 3% ), we even achieve the same performance as the patch-based model of Jarrett et al. [14] after supervised fine-tuni ng ( 63 . 7% ). Moreover, with additional supervised fine-tuning ( U + U + ) we match or perform very close to ( 66 . 3% ) similar models [3, 6] Figure 4: Results on the INRIA dataset with per-image metric . Left: Comparing two best systems with unsupervised initialization ( U U ) vs random initialization ( RR ). Right: Effect of bootstrapping on final performance for unsupervised initialized system. with two layers of convolutional feature extraction, even t hough these models use the more complex spatial pyramid classifier (PMK-SVM) instead of the logisti c regression we have used; the spatial pyramid framework comprises a codeword extraction step and an SVM, thus effectively adding one layer to the system. We get 65 . 7% with a spatial pyramid on top of our single-layer U system (with 256 codewords jointly encoding 2  X  2 neighborhoods of our features by hard quantization, then ma x pooling in each cell of the pyramid, with a linear SVM, as prop osed by authors in [22]). Our experiments have shown that sparse features achieve sup erior recognition performance com-pared to features obtained using a dictionary trained by a pa tch-based procedure as shown in Ta-ble 2. It is interesting to note that the improvement is large r when using feature extractors trained in a purely unsupervised way, than when unsupervised traini ng is followed by a supervised training phase ( 57 . 1 to 57 . 6 ). Recalling that the supervised tuning is a convolutional procedure, this last training step might have the additional benefit of decreasin g the redundancy between patch-based dictionary elements. On the other hand, this contribution w ould be minor for dictionaries which have already been trained convolutionally in the unsupervi sed stage. 3.2 Pedestrian Detection We train and evaluate our architecture on the INRIA Pedestri an dataset [23] which contains 2416 positive examples (after mirroring) and 1218 negative full images. For training, we also augment the positive set with small translations and scale variations t o learn invariance to small transformations, yielding 11370 and 1000 positive examples for training and v alidation respectively. The negative set is obtained by sampling patches from negative full images at random scales and locations. Addition-ally, we include samples from the positive set with larger an d smaller scales to avoid false positives from very different scales. With these additions, the negat ive set is composed of 9001 training and 1000 validation samples.
 Architecture and Training A similar architecture as in the previous section was used, w ith 32 filters, each 7  X  7 for the first layer and 64 filters, also 7  X  7 for the second layer. We used 2  X  2 average pooling between each layer. A fully connected linear layer with 2 output scores (f or pedestrian and background) was used as the classifier. We trained this system on 78  X  38 inputs where pedestrians are approximately 60 pixels high. We have trained our system with and without unsu pervised initialization, followed by fine-tuning of the entire architecture in supervised mann er. Figure 5 shows comparisons of our system with other methods as well as the effect of unsupervis ed initialization.
 After one pass of unsupervised and/or supervised training, several bootstrapping passes were per-formed to augment the negative set with the 10 most offending samples on each full negative image and the bigger/smaller scaled positives. We select the most offending sample that has the biggest opposite score. We limit the number of extracted false posit ives to 3000 per bootstrapping pass. As [24] showed, the number of bootstrapping passes matters m ore than the initial training set. We find that the best results were obtained after four passes, as shown in figure 5 improving from 23 . 6% to 11 . 5% .
 Per-Image Evaluation Performance on the INRIA set is usually reported with the per -window methodology to avoid post-processing biases, assuming that better per-window perfor mance yields better per-image perfor-Figure 5: Results on the INRIA dataset with per-image metric . These curves are computed from the bounding boxes and confidences made available by [25]. Compa ring our two best systems labeled (
U + U + and R + R + )with all the other methods. mance. However [25] empirically showed that the per-window methodology fails to predict the performance per-image and therefore is not adequate for rea l applications. Thus, we evaluate the per-image accuracy using the source code available from [25 ], which matches bounding boxes with the better) at 1 false positive per image on average (1 FPPI). The value of 1 FPPI is meaningful for pedestrian detection because in real world applications, i t is desirable to limit the number of false alarms.
 It can be seen from figure 4 that unsupervised initialization significantly improves the performance ( 14 . 8% vs 11 . 5% ). The number of labeled images in INRIA dataset is relativel y small, which limits the capability of supervised learning algorithms. However , an unsupervised method can model large variations in pedestrian pose, scale and clutter with much b etter success.
 Top performing methods [26], [27], [28], [24] also contain s everal components that our simplis-tic model does not contain. Probably, the most important of a ll is color information, whereas we have trained our systems only on gray-scale images. Another important aspect is training on multi-resolution inputs [26], [27], [28]. Currently, we train our systems on fixed scale inputs with very small variation. Additionally, we have used much lower reso lution images than top performing sys-tems to train our models ( 78  X  38 vs 128  X  64 in [24]). Finally, some models [28] use deformable body parts models to improve their performance, whereas we r ely on a much simpler pipeline of feature extraction and linear classification.
 Our aim in this work was to show that an adaptable feature extr action system that learns its pa-rameters from available data can perform comparably to best systems for pedestrian detection. We believe by including color features and using multi-resolu tion input our system X  X  performance would increase. In this work we have presented a method for learning hierarch ical feature extractors. Two different methods were presented for convolutional sparse coding, it was shown that convolutional training of feature extractors reduces the redundancy among filters com pared with those obtained from patch based models. Additionally, we have introduced two differe nt convolutional encoder functions for performing efficient feature extraction which is crucial fo r using sparse coding in real world ap-plications. We have applied the proposed sparse modeling sy stems using a successful multi-stage architecture on object recognition and pedestrian detecti on problems and performed comparably to similar systems.
 In the pedestrian detection task, we have presented the adva ntage of using unsupervised learning for feature extraction. We believe unsupervised learning sign ificantly helps to properly model extensive variations in the dataset where a pure supervised learning a lgorithm fails. We aim to further improve our system by better modeling the input by including color an d multi-resolution information.
