 A data set is  X  X mbalanced X  if its dependent variable is categorical and the number of instances in one class is different from those in the other class. Learning from imbalanced data sets has been identified as one of the 10 most challenging problems in data mining research [1].

In the literature of solving class imbalance problems, data-oriented meth-ods use sampling techniques to over-sam ple instances in the minor class or under-sample those in the major class, so that the resulting data is balanced. A typical example is the SMOTE method [2] which increases the number of minor class instances by cr eating synthetic samples. It has been recently pro-posed that using different weight degrees on the synthetic samples (so-called safe-level-SMOTE [3]) produces be tter accuracy than SMOTE. The focus of algorithm-oriented methods has been on extensions and modifications of ex-isting classification algorithms so that they can be more effective in dealing with imbalanced data. For example, modifications of decision tree algorithms have been proposed to improve the standard C4.5, such as HDDT [4] and CCPDT [5].

K NN algorithms have been identified as one of the top ten most influential data mining algorithms [6] for their ability of producing simple but powerful classifiers. The k neighbors that are the closest to a test instances are conven-tionally called prototypes. In this paper we use the concepts of  X  X rototypes X  and  X  X nstances X  interchangeably.
 There are several advanced k NN methods proposed in t he recent literature. Weinberger et al. [7] learned Mahanalobis distance matrices for k NN classifica-tion by using semidefinite programming, a method which they call large margin nearest neighbor (LMNN) classification. Experimental results of LMNN show large improvements over conventional k NN and SVM. Min et al. [8] have pro-posed DNet which uses a non-linear feature mapping method pre-trained with Restricted Boltzmann Machines to achieve the goal of large-margin k NN classi-fication. Recently, a new method WD k NN was introduced in [9] which discovers optimal weights for each instance in training phase which are taken into ac-count during test phases. This method is demonstrated superior to other k NN algorithm including LPD [10], PW [11], A-NN [12] and WDNN [13].

In this paper, the model we propose is an algorithm-oriented method and we preserve all original information/distribution of the training data sets. More specifically, the contributions of this paper are as follows: 1. We express the mechanism of traditional k NN algorithms as equivalent to 2. We propose CCW (class confidence weights), the confidence (likelihood) of a 3. We propose two methods, mixture modeling and Bayesian networks, to effi-The rest of the paper is structured as f ollows. In Section 2 we review existing k NN algorithms and explain why they are flawed in learning from imbalanced data. We define CCW weighting strategy and justif y its effectiveness in Section 3. CCW is estimated in Section 4. Section 5 reports experiments and Section 6 concludes the paper. Given labeled training data ( x i , y i )( i = 1,...,n), where x i  X  R d are feature vectors, d is the number of features and y i  X  X  c 1 , c 2 } are binary class labels, k NN algorithm finds a group of k prototypes from the training set that are the closest to a test instance x t by a certain distance measure (e.g. Euclidean distances), and estimates the test instan ce X  X  label according t o the predominance of a class in this neighborhood. When there is no weighting (NW) strategy, this majority voting mechanism can be expressed as: where y t is a predicted label, I (  X  ) is an indicator function that returns 1 if its condition is true and 0 otherwise, and  X  ( x t ) denotes the set of k training instances (prototypes) closest to x t . When the k neighbors vary widely in their distances and closer neighbors are more reliable, the neighbors are weighted by the multiplicative-inverse (MI) or the additive-inverse (AI) of their distances: where dist ( x t , x i ) represents the distance between the test point x t and a pro-totype x i ,and dist max is the maximum possible distance between two training instances in the feature space which normalizes dist ( x t , x i ) dist
While MI and AI solve the problem of large distance variance among k neigh-bors, their effects become insignificant if the neighborhood of a test point is considerably dense, and one of the class (or both classes) is over-represented by its samples  X  since in this scenario all of the k neighbors are close to the test point and the difference among their distances is not discriminative [9]. 2.1 Handling Imbalanced Data Given the definition of the conventional k NN algorithm, we now explain its drawback in dealing with imbalanced data sets. The majority voting in Eq. 1 can be rewritten as the following eq uivalent maximization problem: where p t ( c 1 )and p t ( c 2 ) represent the proportion of class c 1 and c 2 appearing in  X  ( x t ) X  X he k -neighborhood of x t . If we integrate this k NN classification rule as priors 1 of two classes in this sample space, Eq. 4 intuitively illustrates that the classification mechanism of k NN is based on finding the class label that has ahigher prior value.

This suggests that traditional k NN uses only the prior information to estimate class labels, which has suboptimal classification performance on the minority class when the data set is highly imbalanced. Suppose c 1 is the dominating class label, it is expected that the inequality p t ( c 1 ) p t ( c 2 ) holds true in most regions of the feature space. Especially in the overlap regions of two class labels, k NN always tends to be biased towards c 1 . Moreover, because the dominating class is likely to be over-represented in t he overlap regions,  X  X istance weighting X  strategies such as WI and AI are in effective in correcting this bias.
Figure 1 shows an example where k NN is performed by using Euclidean dis-tance measure for k = 5. Samples of positive and negative classes are generated [3 , 6] respectively and a common standard deviation I (the identity matrix). The (blue) triangles are samples of the negative/majority class, the (red) un-filled circles are those of the positive/mi nority class, and the (green) filled circles indicate the positive samples incorr ectly classified by the conventional k NN al-gorithm. The straight line in the middle of two clusters suggests a classification boundary built by an ideal linear classifier. Figure 1(a) and 1(c) give global overall views of k NN classifications, while Figure 1(b) and 1(d) are their corre-sponding  X  X oom-in X  subspaces that focus on a particular misclassified positive sample. Imbalanced data is sampled under the class ratio of Pos:Neg = 1:10.
As we can see from Figure 1(a) and 1(b), when data is balanced all of the misclassified positive samples are on the upper left side of the classification boundary, and are always surrounded by only negative samples. But when data is imbalanced (Figure 1(c) and 1(d)), misclassifications of positives appear on both sides of the boundary. This is because the negative class is over-represented and dominates much larger regions than the positive class. The incorrectly classified positive point in Figure 1(d) is surrounded by 4 negative and 1 positive neighbors, with a negative neighbor being the closest prototype to the test point. In this scenario, distances weighting strateg ies (e.g. MI and AI) cannot be helpful to correct the bias to negative class. In the next section, we introduce CCW and explain how it can solve such problems and correct the bias. To improve the existing k NN rule, we introduce CCW to capture the probability (confidence) of attributes values given a class label. We define CCW on a training instance i as follows: where x i and y i represent the attribute vector and the class label of instances i . Then the resulting classification rule integrated with CCW is: and by applying it into distance weighting schemes MI and AI we obtain: With the integration of CCW , the maximization problem in Eq. 4 becomes: where p t ( c | x i ) x the attribute values of all prototypes in  X  ( x t ). Comparisons between Eq. 4 and Eq. 9 demonstrate that the use of CCW changes the bases of k NN rule from using priors to posteriors : while conventional k NN directly uses the proba-bilities (proportions) of class labels among the k prototypes, we use conditional probabilities of classes given the values of the k prototypes X  feature vectors . The change from priors to posteriors is easy to understand since CCW behaves just like the notion of likelihood in Bayes X  theorem. 3.1 Justification of CCW Since CCW is equivalent to the notion of likelihood in Bayes X  theorem, in this subsection we demonstrate how the rationale of using CCW -based k NN rule can be interpreted by likelihood ratio tests .

We assume c 1 is the majority class and define the null hypothesis ( H 0 )as X  x t belonging to c 1  X , and the alternative hypothesis ( H 1 )as X  x t belonging to c 2  X . Assume among  X  ( x t ), the first j neighbors are from c 1 and the other k  X  j ones are from c 2 . We obtain the likelihood of H 0 ( L 0 )and H 1 ( L 1 )from: Then the likelihood ratio test statistic can be written as: Note that the numerator and the denominator in the fraction of Eq. 10 corre-spond to the two terms of the maximization problem in Eq. 9. It is essential to ensure the majority class does not have higher priority than the minority in imbalanced data, so we choose  X   X  = 1 X  as the rejection threshold. Then the mechanism of using Eq. 9 as the k NN classification rule is equivalent to  X  X redict x t to be c 2 when  X  not reject H 0 ).
 Example 1. We reuse the example in Figure 1. The size of triangles/circles is proportional to their CCW weights: the larger the size of a triangle/cirle, the greater the weight of that instance; and the smaller the lower the weight. In Figure 1(d), the misclassified positive instance has four negative-class neighbors with CCW weights 0.0245, 0.0173, 0.0171 and 0.0139, and has one positive-class neighbor of weight 0.1691. Then the total negative-class weight is 0.0728 and the a label prediction to the positive (minority) class. So even though the closest prototype to the test instance comes from the wrong class which also dominates the test instance X  X  neighborhood, a CCW weighted k NN can still correctly classify this actual positive test instances. In this section we briefly introduce how we employ mixture modeling and Bayesian networks to estimate CCW weights.
 4.1 Mixture Models In the formulation of mixture models, the training data is assumed follow a q -component finite mixture distribution with probability density function ( pdf ): where x is a sample of training data whose pdf is demanded,  X  m represents mix-is the complete set of parameters specifying the mixture model. Given training data  X  , the log-likelihood of a q -component mixture distribution is: log p (  X  | 5  X  )= estimate 5  X  ML =argmax  X  log p (  X  |  X  ) can be found analytically. We use the expectation-maximization (EM) algorithm to solve ML and then apply the esti-mated 5  X  into Eq. 11 to find the pdf of all instances in training data set as their corresponding CCW weights.
 Example 2. We reuse the example in Figure 1, but now we assume the underlying distribution parameters (i.e. the mean and variance matrixes) that generate the two classes of data are unknown. We apply training samples into ML estimation, solve for 5  X  by EM algorithm, and then use Eq. 11 to estimate the pdf of training instances which are used as their CCW weights. The estimated weights (and their effects) of the neighbors of the originally misclassified positive sample in Figure 1(d) are shown in Example 1. 4.2 Bayesian Networks While mixture modeling deals with numeri cal features, Bayesian networks can be used to estimate CCW when feature values are categorical. The task of learning a Bayesian network is to ( i ) build a directed acyclic graph (DAG) over  X  ,and ( ii ) learn a set of (conditional) probability tables { p (  X  | pa (  X  )) , X   X   X  } where pa (  X  ) represents the set of parents of  X  in the DAG. From these conditional distributions one can recover the joint probability distribution over  X  by using
In brief, we learn and build the structure of the DAG by employing K2 al-gorithm [14] which in the worst case has an overall time complexity of O ( n 2 ), one  X  n  X  for the number of features and another  X  n  X  X orthenumberoftrain-ing instances. Then we estimate the conditional probability tables directly from training data. After obtaining the joint distributions p (  X  ), the CCW weight of a training instance i can be easily obtained from w CCW i = p ( x i | y i )  X  p (  X  ) p ( y p ( y i ) is the proportion of class y i among the entire training data. In this section, we analyze and compare the performance of CCW -based k NN against existing k NN algorithms, other algorithm-oriented state of the art approaches (i.e. WD k NN 2 ,LMNN 3 ,DNet 4 , CCPDT 5 and HDDT 6 )and data-oriented methods (i.e. safe-level-SMOTE). We note that since WD k NN has been demonstrated (in [9]) better than LPD, PW, A-NN and WDNN, in our exper-iments we include only the more superior WD k NN among them. CCPDT and HDDT are pruned by Fisher X  X  exact test (as recommended in [5]). All experi-ments are carried out using 5  X  2 folds cross-validations, and the final results are the average of the repeated runs. We select 31 data sets from KDDCup X 09 7 , agnostic vs. prior competition 8 , StatLib 9 , text mining [15], and UCI repository [16]. For multiple-label data sets, we keep the smallest label as the positive class, and combine all the other labels as the negative class. Details of the data sets are shown in Table 1. Besides the proportion of the minor class in a data set, we also present the coefficient of variation (CovVar) [18] to measure imbalance. CovVar is defined as the ratio of the standard deviation and the mean of the class counts in data sets.
The metric of AUC-PR (area under precision-recall curve) has been reported in [19] better than AUC-ROC (area under ROC curve) on imbalanced data. A curve dominates in ROC space if and only if it dominates in PR space, and clas-sifiers that are more superior in terms of AUC-PR are definitely more superior in terms of AUC-ROC, but not vice versa [19]. Hence we use the more informative metric of AUC-PR for classifier comparisons. 5.1 Comparisons among NN Algorithms In this experiment we compare CCW with existing k NN algorithm using Euclidean distance on k =1.When k = 1, apparently all k NNs that use the same distance measurehaveexactlythesamepredictio n on a test instances. However the effects of CCW weights generate different probabilities of being positive/negative for each test instance, and hence produce different AUC-PR values.

While there are various ways to compare classifiers across multiple data sets, we adopt the strategy proposed by [20] that evaluates classifiers by ranks. In Table 1 the k NN classifiers in comparison are ranked on each data set by the value of their AUC-PR, with ranking of 1 being the best. We perform Friedman tests on the sequences of ranks between di fferent classifiers. In Friedman tests, p  X  X alues that are lower than 0.05 reject the hypothesis with 95% confidence that the ranks of classifiers in comparison are not statistically different. Numbers in parentheses of Table 1 are the ranks of classifiers on each data set, and a sign in Friedman tests suggests classifiers in comparison are significantly different. As we can see, both CCW MI and CCW AI (the  X  X ase X  classifiers) are significantly better than existing methods of NW, MI, AI and WD k NN.
 5.2 Comparisons among k NN Algorithms In this experiment, we compare k NN algorithms on k &gt; 1. Without losing gen-erality, we set a common number k = 11 for all k NN classifiers. As shown in Ta-ble 2, both CCW MI and CCW AI significantly outperforms MI, AI, WD k NN, LMNN, DNet, CCPDT and HDDT.

In the comparison with over-sampling techniques, we focus on MI equipped with safe-level-SMOTE [3] method, shown as  X  X MOTE X  in Table 2. The results we obtained from CCW classifiers are comparable to (better but not significant than) the over-sampling technique under 95% confidence. This observation sug-gests that if one uses CCW he can obtain results comparable to the cutting-edge sampling technique, so the extra computational cost of data sampling before training can be saved. 5.3 Effects of Distance Metrics While in all previous experiments k NN classifiers are performed under Euclidean distance ( 2 norm), in this subsection we provide empirical results that demon-strate the superiority of CCW methods on other distance metrics such as Manhat-tan distance ( 1 norm) and Chebyshev distance (  X  norm). Due to page limits, here we only present the comparisons of  X  CCW MI vs.MI X .Aswecanseefrom Figure 2, CCW MI can improve MI on all three distance metrics. The main focus of this paper is on improving existing k NN algorithms and make them robust to imbalanced data sets. We have shown that conventional k NN algorithms are akin in using only prior probabilities of the neighborhood of a test instance to estimate its class labels, which leads to suboptimal performance when dealing with imbalanced data sets.

We have proposed CCW , the likelihood of attribute values given a class label, to weight prototypes before taking them into effect. The use of CCW transforms the original k NN rule of using prior probabilities to their corresponding posteriors . We have shown that this transformation has the ability of correcting the inherent bias towards majority class in existing k NN algorithms.

We have applied two methods (mixture modeling and Bayesian networks) to estimate training instances X  CCW weights, and their effectiveness is confirmed by synthetic examples and comprehensive experiments. When learning Bayesian networks, we construct network structures by applying the K2 algorithm which has an overall time complexity of O ( n 2 ).

In future our plan is to extend the idea of CCW to multiple-label classification problems. We also plan to explore the use of CCW on other supervised learning algorithms such as support vector machines etc.
