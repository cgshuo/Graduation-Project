 There are many models and learning algorithms where it becomes necessary, or is at least very useful, to com-pute entries of the Hessian of some complicated function. For functions that can be computed using a computational graph there are automatic methods available for computing Hessian-vector products exactly (e.g. Pearlmutter, 1994). These can be used to recover specific columns of the Hes-sian, but are inefficient at recovering other parts of the ma-trix such as large blocks, or the diagonal. For the diagonal of the Hessian of a neural network training objective, there are deterministic approximations available such as that of Becker and Le Cun (1988), but these are not guaranteed to be accurate.
 Recently Chapelle and Erhan (2011) showed how to com-pute an unbiased estimate of the diagonal of the Gauss-Newton matrix, and used this to perform precondition-ing within a Hessian-free Newton optimization algorithm (Martens, 2010). In this paper we build upon this idea and develop a family of algorithms, which we call Curvature Propagation (CP), for efficiently computing unbiased es-timators of the Hessians of arbitrary functions. Estimat-ing entries of the Hessian turns out to be strictly harder than doing the same for the Gauss-Newton matrix, and the resulting approach is necessarily more complex, requiring several additional ideas.
 As with the algorithm of Chapelle and Erhan (2011), CP involves reverse sweeps of the computational graph of the function, which can be repeated to obtain higher-rank esti-mates of arbitrary accuracy. And when applied to a func-tion which decomposes as the sum of M terms, such as typ-ical training objective functions, applying CP to the terms individually results in an estimate of rank M , at no addi-tional expense than than applying it to the sum.
 This is useful in several applications. The diagonal of the Hessian can be used as a preconditioner for first and second order nonlinear optimizers, which is the motivating appli-cation of Becker and Le Cun (1988) and Chapelle and Er-han (2011). Score Matching (Hyvarinen, 2006), a method for parameter estimation in Markov Random Fields, uses the diagonal of the Hessian within its objective, making it expensive to apply to all but the simplest models. As we will see, our work makes it possible to efficiently apply score matching to any model. In the following section we develop the Curvature Propaga-tion method (CP) for functions that are defined in terms of general computational graphs. We will present one version of the approach that relies on the use of complex arithmetic, and later also give a version that uses only real arithmetic. At a high level, we will define complex vector-valued lin-ear function on the computational graph of our target func-tion f , and then show through a series of lemmas that the expectation of the self outer-product of this function is in fact the Hessian matrix. This function can be computed by what amounts to a modification of reverse-mode automatic differentiation, where noise is injected at each node. 2.1. Setting and notation Let f : R n  X  X  X  R be a twice differentiable function. We will assume that f can be computed via a computation graph consisting of a set of nodes N = { i : 1  X  i  X  L } and directed edges E = ( i,j ) : i,j  X  N , where at each node i there is a vector valued output y i  X  R n i is com-puted via y i = f i ( x i ) for some twice-differentiable func-given by the concatenation of vectors y k for k  X  P i and P i = { k : k is a parent of i } = { k : ( k,i )  X  E } . We identify node 1 as input or  X  X ource X  node (so that P 1 =  X  ) and node L as the output or  X  X ink X  node, with y L = f ( y being the final output of the graph.
 Let J a b denote the Jacobian of a w.r.t. b where a and b are vectors, or in other words, note the Hessian of the scalar function c w.r.t. a and then w.r.t. b (the order matters since it determines the dimen-sion of the matrix). Note that if a and b are quantities as-sociated with nodes i and j (resp.) in the computational graph, J a b and H c a,b will only be well-defined when j does not depend directly or indirectly on i , i.e. i 6 X  A j , where A j = { k : k is an ancestor of j } . Also note that when there is no dependency on b of a it will be the case that J b = 0 . Under this notation, the Hessian of f w.r.t. its in-put is denoted by H f y for convenience.
 For k  X  P i , let R i,k denote the projection matrix which maps the output y k of node k to the their positions in node i  X  X  input vector x i , so that we have x i = P k  X  P Summarizing, we have the following set of recursive defi-nitions for computing y L = f ( y 1 ) which are iterated for i ranging from 2 to L : Note that R i,k need not appear explicitly as a matrix when implementing these recursions in actual code, but is merely the formal mathematical representation we will use to de-scribe the projective mapping which is performed when-ever outputs from a given computational node k are used as input to another node i . 2.2. Computing gradients and Hessians Reverse-mode automatic differentiation 1 is a well known method for computing the gradient of functions which are defined in terms of computation graphs. It works by start-ing at the final node L and going backwards through the graph, recursively computing the gradient of f w.r.t. the y i for each i once the same has been done for all of i  X  X  children. Using the vector-valued computational graph for-malism and notation we have established, the recursions for computing the gradient  X  f = J f y f  X  y L ) are given by where C i = { k : k is a child of i } and we have used the fact that J x k y For this method to yield a realizable algorithm, it is as-sumed that for each node i , the function f i is simple enough that direct computation of and/or multiplication by the  X  X o-cal X  Jacobian J y i x i this is not the case, then the usual procedure is to split i into several new nodes which effectively break f i into sev-eral computationally simpler pieces.
 By computing the vector derivative of both sides of each of the above equations w.r.t. y L , y j , and y j respectively (for j 6 X  A i ), the following recursions can be derived where and where y i,q denotes the q -th component of y i . In deriv-ing the above it is important to remember that R k,i is a con-stant, so that its Jacobian w.r.t. y j is the zero matrix. Also note that J f y local nonlinearity f i . The overall Hessian of f , H f y be obtained by applying these recursions in a backwards manner (assuming that the various Jacobians are already computed).
 The additional Jacobian terms of the form J x i y pear in eqn. 6 can be computed according to recursions analogous to those used to compute the gradient, which are given by the equations below: where, for convenience, we have defined J x i ,x j to be zero whenever i is an ancestor of j , whereas otherwise it would be undefined.
 In general, using these recursions for direct computation tion tree for f involves a small total number of nodes, each with small associated output and input dimensions n i and m i . The purpose in giving them is to reveal how the  X  X truc-ture X  of the Hessian follows the computation tree, which will become critically important in both motivating the CP algorithm and then proving its correctness. 2.3. The S function We now define an efficiently computable function S that will allow us to obtain rank-1 estimates of the Hessian. Its argument consists of an ordered list of vectors V  X  { v vector (which may be complex valued). It will be defined as S ( V )  X  S y 1 ( V ) , where S y i ( V )  X  C n i and S x are vector-valued functions of V defined recursively via the equations where each F i is a (not necessarily square) complex-valued guaranteed to exist because M i is symmetric, which fol-lows from the fact that it is a linear combination of Hessian matrices.
 Note that these recursions closely resemble those given pre-viously for computing the gradient (eqn. 1, 2, and 3). The multiplication by J y i x of the recursion is easy to perform since this is precisely what happens at each stage of reverse-mode automatic dif-ferentiation used to compute the gradient of f . In general, the cost of computing S is similar to that of computing the gradient, which itself is similar to that of evaluating f . The practical aspects computing S ( V ) will be discussed further in section 5. 2.4. Properties of the S function with stochastic inputs Suppose that the random variable V satisfies: For example, each v i could be drawn from a multivariate normal with mean 0 and covariance matrix I .
 We will now give a result which establishes the useful-ness of S ( V ) as a tool for approximating H . The proof of this theorem and others will be located in the ap-pendix/supplement.
 Theorem 2.1. S ( V ) S ( V ) &gt; is an unbiased estimator of H (  X  H f y In addition to being unbiased, the estimator S ( V ) S ( V ) is will be symmetric and possibly complex-valued. To achieve a real valued estimate we can instead use only the real component of S ( V ) S ( V ) &gt; , which itself will also be an unbiased estimator for H f y of S ( V ) S ( V ) &gt; is zero in expectation. 2.5. Avoiding complex numbers The factorization of the M i  X  X  and resulting complex arith-metic associated with using these factors can be avoided if we redefine V so that each v i is of dimension m i (in-stead of ` i ), and we define the real vector-valued functions T ( V )  X  T y 1 ( V ) and U ( V )  X  U y 1 ( V ) according to the following recursions:
Both these recursions for T and U are trivial modification of those given for S ( V ) , with the only difference being the matrix which multiplies v i (it X  X  F &gt; i for S , M i for T , and I for U ). And because they do not involve complex quantities at any point, they will be real-valued.
 Theorem 2.2. T ( V ) U ( V ) &gt; is an unbiased estimator of H Since H is symmetric, it follows directly from this re-sult that T ( V ) U ( V ) &gt; &gt; = U ( V ) T ( V ) &gt; unbiased estimator of H . Note however that while both T ( V ) U ( V ) &gt; and U ( V ) T ( V ) &gt; will be symmet-ric in expectation (since H f y lar choice of V they generally will not be. This is-sue can be addressed by instead using the estimator 1 2 ric for any V . However, despite the fact that S ( V ) S ( V ) and this alternative estimator are both symmetric for all V  X  X  and also unbiased, they will not, in general, be equal. While computing both T and U will require a total of 2 sweeps over the computational graph versus only the one required for S ( V ) , the total amount of work will be the same due to the doubly expensive complex-valued arithmetic required to evaluate S ( V ) . 2.6. Matrix interpretation of S , T and U Suppose we represent V as a large vector v  X  [ v &gt; 1 ...v with dimension m  X  P i m i . Then the functions S , T and U are linear in the v i  X  X  (a fact which follows from the re-cursive definitions of these functions) and hence v . Thus S , T , and U have an associated representation as matrices  X  S  X  C n  X  m ,  X  T  X  R n  X  m , and  X  U  X  R n  X  m w.r.t. the coordi-nate bases given by  X  v .
 Then noting that S ( V ) S ( V ) &gt; =  X  Svv &gt;  X  S &gt; tion (15) is equivalent to E [ vv &gt; ] = I , we obtain and thus we can see that  X  S has an interpretation as  X  X actor X  H At the cost of roughly two passes through the computa-tional graph it is possible to compute the Hessian-vector Hw for an arbitrary vector w  X  R n (e.g. Pearlmutter, 1994). This suggests the following simple approach to computing an unbiased rank-1 estimate of H : draw w from a distribution satisfying E [ ww &gt; ] = I and then take the outer product of Hw with w . It is easy to see that this is unbiased, since Computationally, this estimator is just as expensive as CP, but since there are several pre-existing methods comput-ing Hessian vector products, it may be easier to implement. However, we will prove in the next section that the CP es-timator will have much lower variance in most situations, and later confirm these findings experimentally. And in ad-dition to this, there are certain situations, which arise fre-quently in machine learning applications, where vectorized implementations of CP will consume far less memory than similar vectorized implementations of this simpler estima-tor ever could, and we will demonstrate this in the specific case when f is a neural network training objective function. It is also worth noting that this estimator underlies the Hes-sian norm estimation technique used in Rifai et al. (2011). That this is true is due to the equivalence between the stochastic finite-difference formulation used in that work and matrix-vector products with randomly drawn vectors. We will make this rigorous in the appendix/supplement. Let AB &gt; be an arbitrary matrix factorization of H , with A,B  X  C n  X  ` . Given a vector valued random variable u  X  R ` satisfying E [ uu &gt; ] = I , we can use this factoriza-tion to produce an unbiased rank-1 estimate of the Hessian, H A,B  X  ( Au )( Bu ) &gt; = Auu &gt; B &gt; . Note that the various CP estimators, as well as the simpler one discussed in the previous section are all of this form, and differ only in their choices of A and B .
 Expanding we have: E where here (and in the remainder of this section) the sub-scripts on u refer to scalar components of the vector u and not elements of a collection of vectors.
 If we assume u  X  G  X  Normal (0 ,I ) , we can use the well-know formula E G [ u a u b u c u d ] =  X  ab  X  cd +  X  ac  X  and simplify this further to: = X where A i is a vector consisting of the i -th row of A , and similarly for B i , and where we have used H ij = A &gt; i Consequently, the variance is given by:
Note that when A = B =  X  S , we have that ( A estimator H  X  S,  X  S has the following desirable property: its covariance depends only on H and not on the specific de-tails of the computational graph used to construct the S function.
 If on the other hand we assume that u  X  K  X  Bernoilli ( { X  1 , 1 } ) ` , i.e. K is a multivariate distribution of independent Bernoulli random variables on { X  1 , 1 } , we have E B [ u a u b u c u d ] =  X  ab  X  cd +  X  ac  X  bd +  X  2  X  ab  X  bc  X  cd , which when plugged into (18) gives: Of particular interest is the self-variance of H A,B Var h H A,B ij i = Cov h H A,B ij ,H A,B ij i ). In this case we have that: and we see that variance of estimator that uses K will always be strictly smaller than the one that uses G , unless P a ( B ia A ja ) 2 = 0 (which would imply that P Returning to the case that u  X  G , we can prove the follow-ing result, which shows that when it comes to estimating the diagonal entries H ii of H , the estimator which uses A = B =  X  S has the lowest variance among all possible estimators of the form H A,B : Theorem 4.1.  X  i and  X  A,B s.t. AB &gt; = H we have: Moreover, in the particular case of using the  X  X imple X  esti-mator (which is given by A = H,B = I ) the variance of the diagonal entries is given by: and so we can see that the CP estimator based on S always gives a lower variance, and is strictly lower in most cases. 5.1. Computing and factoring the M i  X  X  Computing the matrices M i for each node i is necessary in order to compute the S , T and U functions, and for S we must also be able to factor them. Fortunately, each M i can be computed straightforwardly according to eqn. 7 as long as the operations performed at node i are simple enough. And each H y i,q x i ,x i is determined completely by the local function f i computed at node i . The Jacobian term J just a scalar, and is the derivative of f w.r.t. y can be made cheaply and easily available by performing, in parallel with the computation of S ( V ) , the standard back-wards automatic differentiation pass for computing the gra-dient of f w.r.t. to y 1 , which will produce the gradient of f w.r.t. each y i along the way. Alternatively, this gradient information may be cached from a gradient computation which is performed ahead of time (which in many applica-tions is done anyway).
 In general, when M i is block diagonal or banded diagonal, so too will F i (with the same pattern), which will greatly reduce the associated computational and storage require-ments. For example, when f i corresponds to the element-wise nonlinearities computed in a particular layer of a neu-ral network, M i will be diagonal and hence so will F i , and these matrices can be stored as such. Also, if M i happens to be sparse or low rank, without any other obvious special structure, there are algorithms which can compute factors F i which will also be sparse or low-rank.
 Alternatively, in the most extreme case, the vector valued nodes in the graph can be sub-divided to produce a graph with the property that every node outputs only a scalar and has at most 2 inputs. In such a case, each M i will be no bigger than 2  X  2 . Such an approach is best avoided unless deemed necessary since the vector formalism allows for a much more vectorized and thus efficient implementation in most situations which arise in practice. Another option to consider if it turns out that M i is easy to work with but hard to factor, is to use the T,U based estimator instead of the S based one.
 It may also be the case that M i or F i will have a special sparse form which makes sampling the entire vector v i un-necessary. For example, if a node copies a large input vec-tor to its output and transforms a single entry by some non-linear function, M i will be all zeros with a single element on the diagonal (and hence so will its factor F i ), making it possible to sample only the component of v i that corre-sponds to that entry. 5.2. Increasing the rank As with any unbiased estimator, the estimate can be made more accurate by collecting multiple samples. Fortunately, sampling and computing S ( V ) for multiple V  X  X  is trivially parallelizeable. And it can be easily implemented in vec-torized code for k samples by taking the defining recur-sions for S (eqn. 12, 13, and 14) and redefining S y i ( V ) and S i ( V ) to be matrix valued functions (with k columns) and v to be a m i  X  k matrix of column vectors which are gen-erated from independent draws from the usual distribution for v i .
 In the case where f is a sum of B similarly structured terms, which occurs frequently in machine learning such as when f is sum of regression errors or log-likelihood terms over a collection of training cases, one can apply CP indi-vidually to each term in the sum at almost no extra cost as just applying it to f , thus obtaining a rank-k estimate of f instead of a rank-1 estimate. 5.3. Curvature Propagation for Diagonal Hessian In this section we will apply CP to the specific exam-ple of computing an unbiased estimate diag (  X  H ) of the diagonal of H (diag ( H ) ) of a feed-forward neural net-works with ` layers. The pseudocode below computes the objective f of our neural network for a batch of B cases. Here g ( x ) is a coordinate-wise nonlinearity, z i are matrices containing the outputs of the neuronal units at layer i for all the cases, and similarly the matrices u i contain their inputs. L b denotes the loss-function associated with case b (the de-pendency on b is necessary so we can include targets). For simplicity we will assume that L b is the standard squared t (where t will denote the matrix of these vectors). The special structure of this objective permits us to ef-ficiently apply CP to each scalar term of the average P b =1 L b ( z `,b ) /B , instead of to f directly. By summing the estimates of the diagonal Hessians for each L b ( z `,b we thus obtain a rank-B estimate of H instead of merely a rank-1 estimate. That this is just as efficient as applying CP directly to f is due to the fact that the computations of each z `,b are performed independently of each other. For ease of presentation, we will redefine V  X  { v i so that each v i is not a single vector, but a matrix of such vectors with B columns. We construct the compu-tational graph so that the element-wise nonlinearities and the weight matrix multiplications performed at each of the ` layers each correspond to a node in the graph. We define S computation of u i (from z i  X  1 and W i  X  1 ), S z i  X  S where k i is the node correspond to the computation of z i (from u i ), and S W i  X  [ S y 1 ( V )] W i where [  X  ] W traction of the rows in y 1 corresponding to the i -th weight-matrix ( W i ). Consistent with our mild redefinition/abuse of notation for V , each of S u i , S z i , S y 1 , and S be matrix-valued with a column for each of the B training cases. The variables d z i and d u i are the derivatives w.r.t. u and z i computed with backpropagation and also have B columns. Finally, let a b be the element-wise prod-uct, a 2 be the element-wise power, outer ( a,b )  X  ab outer2 ( a,b )  X  outer ( a 2 ,b 2 ) , and vec (  X  ) be the vector-ization operator. Under this notation, the algorithm below estimates the diagonal of the Hessian of f by estimating the sub-objective corresponding to each case, and then averag-ing the results. Like the pseudo-code for the neural network objective itself, it makes use of vectorization, which allows for an easily parallelized implementation. For i &lt; ` , each K i is a B -columned matrix of vectors con-taining the diagonals for each training case of the local ma-trices M k i for each case occurring at node k i . Because M j corresponds to an element-wise non-linear function, it is diagonal, and so K 1 / 2 i will be a matrix of vectors cor-responding to the diagonals of the factors F k i (which are themselves diagonal). Note that the above algorithm makes use of the fact that the local matrices M j i can be set to zero and the estimator of the diagonal will remain unbiased. At no point in the above implementation do we need to store any matrix the size of S W i , as the computation of [ diag (  X  H )] W i , which involves an element-wise square of S
W i and sum over cases (as accomplished by line 5), can be performed as soon as the one and only contribution to S W from other nodes in the graph is available. This is desir-able since S W i will usually be much larger than the various other intermediate quantities which we need to store, such as z i or S u i . In functions f where large groups of param-eters are accessed repeatedly throughout the computation graph, such as in the training objective of recurrent neural networks, we may have to temporally store some matrices the size S y 1 (or certain row-restrictions of this, like S as the contributions from different cases are collected and summed together, which can make CP less practical. No-tably, despite the structural similarities of back-prop (BP) to CP, this problem doesn X  X  exist with BP since one can store incomplete contributions from each case in the batch into a single n dimensional vector, which is impossible in CP due to the need to take the entry-wise square of S before summing over cases. An approach like CP wouldn X  X  be as useful if there was an efficient and exact algorithm for computing the diagonal of the Hessian of the function defined by an arbitrary com-putation graph. In this section we will argue why such an algorithm is unlikely to exist.
 To do this we will reduce the problem of multiplying two matrices to that of computing (exactly) the diagonal of the Hessian of a certain function f , and then appeal to a hardness due to Raz and Shpilka (2001) which shows that matrix multiplication will require asymptotically more computation than CP does when it is applied to f . This result assumes a limited computational model consisting of bounded depth arithmetic circuits with arbitrary fan-in gates. While not a fully general model of efficient compu-tation, it nonetheless captures most natural algebraic for-mulae and algorithms that one might try to use to compute the diagonal of f .
 The function f will be defined by: f ( y )  X  Note that f may be easily evaluated in O ( n 2 ) time by mul-tiplying y first by W , obtaining z , and then multiplying z by Z , obtaining Zz , and finally pre-multiplying by z obtaining z &gt; Zz = y &gt; W &gt; ZWy . Thus applying CP is rel-atively straight-forward, with the only potential difficulty being that the matrix Z , which is the local Hessian associ-ated with the node that computes z &gt; Zz , may not be easy to factorize. But using the T / U variant of CP gets around this issue, and achieves a O ( n 2 ) computational cost. More-over, it is easy to see how the required passes could be im-plemented by a fixed-depth arithmetic circuit (with gates of arbitrary fan-in) with O ( n 2 ) edge-cost since the critical operations required are just a few matrix-vector multiplica-tions. The goal of the next theorem is to show that there can be no such circuit of edge cost O ( n 2 ) for computing the exact Hessian of f .
 Theorem 6.1. Any family of bounded depth arithmetic cir-cuits with arbitrary fan-in gates which computes the diag-onal of f given inputs W and Z will have an edge count which is superlinear in n 2 .
 The basic idea of the proof is to use the existence of such a circuit family to construct a family of circuits with bounded depth and edge count O ( n 2 ) , that can multiply arbitrary n  X  n matrices (which will turn out to be the matrices P and Q that parameterize f ), contradicting a theorem of Raz and Shpilka (2001) which shows that any such circuit fam-ily must have edge count which is superlinear n 2 . The fol-lowing lemma accomplishes this construction: Lemma 6.2. If an arithmetic circuit with arbitrary fan-in gates computes the diagonal of the Hessian of f for ar-bitrary P , Q and Z then, there is also a circuit of twice the depth + O (1) , and three times the number of edges + O ( n 2 ) , which computes the product PQ for arbitrary input matrices P,Q  X  R n  X  n .
 The results presented in this section rule out, or make extremely unlikely, the possible existence of algorithms which could perform a constant number of backwards and forwards  X  X asses X  through the computational graph of f to find its exact Hessian. The simplest way of computing the entries of the Hes-sian, including the diagonal, is by using an algorithm for Hessian-vector multiplication and running through the vec-tors e i for i = 1 ...n , recovering each column of H in turn. Unfortunately this method is too expensive in most situ-ations, and in the example function f used in Section 6, would require O ( n 3 ) time.
 The method of Chapelle and Erhan (2011) can be viewed as a special case of CP, where all the M i  X  X  except for the M i associated with the final nonlinearity are set to zero. Because of this, all of the results proved in this paper also apply to this approach, but with the Hessian replaced by the Gauss-Newton matrix.
 Becker and Le Cun (1988) gave an approach for approx-imating the diagonal of the Hessian of a neural network training objective using a deterministic algorithm which does several passes through the computation tree. This method applies recursions similar to (4)-(6), except that all the  X  X ntermediate Hessians X  at each layer are approximated by their diagonals, thus producing a biased estimate (unless the intermediate Hessians really are diagonal). We numer-ically compare CP to this approach in Section 8.
 In Bishop (1992), a method for computing entries of the Hessian of a feedforward neural network was derived. This method, while being exact, and more efficient than the naive approach discussed at the start of this section, is not practical for large networks, since it requires a number of passes which will be at least as big as the total number of hidden and outputs units. CP by contrast requires only 1 pass to obtain a single unbiased rank-B estimate, where B is the number of training cases. 8.1. Accuracy Evaluation In this section we test the accuracy of CP on a small neural network as we vary the number of samples. The network consists of 3 hidden layers, each with 20 units. The input and output layers are of size 256 and 10 respectively giv-ing a total of 6190 parameters. We tested both a network with random weights set by Gaussian noise with a variance of 0 . 01 , and one trained to classify handwritten digits from the USPS dataset 2 . For the random vectors v , we tested both Gaussian and { X  1 , 1 } -Bernoulli noise using the CP estimators based on using S and T / U , and the simpler es-timator discussed in Section 3 based on using H / I . For the sake of comparison, we also included the deterministic method of (Becker and Le Cun, 1988). The experiments were carried out by picking a subset of 1000 data points from the USPS dataset and keeping it fixed. Note that sam-ple size refers to the number of random vectors generated per data case . This means that a sample size of 1 corre-sponds to an aggregation of 1000 rank-1 estimates. Our results in 8.1 show that the accuracy of each estimator improves by roughly an order of magnitude for every order of magnitude increase in samples. It also shows that the S -based estimator along with binary noise is by far the most efficient and the simple H / I based estimator is the least efficient by an order of magnitude. 8.2. Score-Matching Experiments To test the effectiveness of CP in a more practical sce-nario, we focus on estimating the parameters of a Markov random field using the score matching technique. Score matching is a simple alternative to maximum likelihood that has been widely used to train energy-based models (K  X  oster and Hyv  X  arinen, 2007; Swersky et al., 2011). One of its drawbacks is that the learning objective requires the diagonal Hessian of the log-likelihood with respect to the data, which can render it unreasonably slow for deep and otherwise complicated models.
 Our specific test involves learning the parameters of a covariance-restricted Boltzmann machine (cRBM; Ranzato et al., 2010). This can be seen as a two-layer network where the first layer uses the squared activation function followed by a second layer that uses the softplus activation function: log(1+exp( x )) . The details of applying score matching to this model can be found in Swersky et al. (2011).
 In this experiment, we attempted to train a cRBM using stochastic gradient descent on minibatches of size 100 . Our setup is identical to Ranzato et al. (2010). In particular, our cRBM contained 256 factors and hidden units. We trained the model on 11000 image patches of size 16  X  16 from the Berkeley dataset 3 . For our training procedure, we optimize the first layer for 100 epochs, then freeze those weights and train the second layer for another 25 epochs.
 Score-matching requires the derivatives (w.r.t. the model parameters) of the sum of the diagonal entries of the Hes-sian (w.r.t. the data). We can thus use CP to estimate the score-matching gradient by applying automatic differentia-tion to the CP estimator itself (sampling and then fixing the random noise V ), exploiting the facts that the linear sum over the diagonal respects expectation, and the derivative of the expectation over V is the expectation of the deriva-tive, and so this will indeed produce an unbiased estimate of the required gradient.
 A random subset of covariance filters from the trained model are shown in Figure 8.2. As expected the filters ap-pear Gabor-like, with various spatial locations, frequencies, and orientations. The second layer also reproduces the de-sired effect of pooling similar filters from the layer below. To demonstrate that learning can proceed with no loss in accuracy we trained two different versions of the model, one where we use the exact minibatch gradient, and one where we use approximate gradients via our estimator. We plot the training loss versus epoch, and our results in Fig-ure 1(c) show that the noise incurred from our unbiased ap-proximation does not affect accuracy during learning with minibatches. Unfortunately, it is difficult to train for many epochs in the second layer because evaluating the exact ob-jective is prohibitively expensive in this model.
 A We thank Olivier Chapelle for his helpful discussions. R
