 The aim of an Information Retrieval (IR) application is to support the user accessing relevant information effectively and efficiently. It is well known that system performance, in terms of finding relevant information is heavily depen-dent upon the IR application (i.e. the IR system exposed through the application X  X  interface), as well as how the ap-plication is used by the user (i.e. how the user interacts with the system through the interface). Thus, a very pragmatic evaluation question that arises at the application level is: what is the effectiveness experienced by the user during the usage of the application? To be able to answer this ques-tion, we represent the usage of an application by the stream of documents the user encounters while interacting with the application. This representation enables us to monitor and track the performance over time and usage. By taking a stream-based, time-centric view of the IR process, instead of a rank-list, topic/task centric view, the evaluation can be performed on any IR based application. To illustrate the difference and the utility of this approach, we demonstrate how a new suite of usage based effectiveness measures can be applied. This work provides the conceptual foundations for measuring , monitoring and modeling the performance of any IR application which needs to be evaluated over time and in context.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval:Search process Theory, Measurement, Experimentation Usage Performance, User Experience
An Information Retrieval (IR) application can comprise of one or more functions/systems exposed through a common interface. For example, a web search engine is typically ac-cessed within a web browser enabling both search, where the user interacts with a ranked list of documents, and brows-ing, where the user interacts with web documents through hyper-links, all within a single interface. Other examples of IR applications include: a news recommender which al-lows both the push and pull of relevant news; personal dig-ital libraries that enable indexing, tagging, browsing and retrieval of academic documents; and advanced intelligent information applications that presents a mash up of rele-vant information from numerous sources. A primary goal of all these applications is to enable efficient and effective access to relevant information. However, each application is different and allows many possible interactions along with various ways to access information. This makes evaluation particularly difficult and a grand challenge in Interactive In-formation Retrieval (IIR) [4, 13]. The evaluation difficulties arise because of three main reasons: 1. interaction is difficult to synthesize and replicate mak-2. standard IR evaluation [25] focuses on: (a) the system, 3. measurements are typically based on a ranked list with The evaluation of different interactive IR applications, there-fore, requires an evaluation paradigm that differs from the TREC/Cranfield convention [3]. One that is more general, at a higher level, and focuses on usage. It needs to be ( i )gen-eral in the sense that any IR application can be evaluated; ( ii ) at a higher level, i.e. the application level, to incorpo-rate interaction and the presentation/interface within the evaluation, and; ( iii ) focused on usage, because how an ap-plication is used determines how much relevant information is accessed and thus how effective it is. A paradigm that addresses these needs should enable the evaluation of any interactive IR application to be conducted using the same conceptual framework: in order to measure , monitor ,and model the performance experienced during usage.
 With recent technological developments (such as Lemur X  X  Query Log Project, Google X  X  Search API, Yahoo! X  X  Search-Monkey Toolkit, etc.), the feasibility of building instru-mented interactive IR applications is enabling research to be conducted outside the lab, and in the wild. Consequently, naturalistic and laboratory based observational studies such as [12, 21] are now more viable than ever. And so, this advancement in technology coupled with the move towards evaluating IR systems in context (as exemplified by initia-tives like IIiX and the many dedicated workshops on interac-tive retrieval) means that it is very timely, if not, necessary to consider alternative evaluation approaches that can un-derpin future IIR research.

To this aim, we adopt a stream based view as advocated by Bookstein [7] to form the basis of evaluation of any IR application: whereby the interaction between the user and the application produces a stream of documents which are assessed during the usage of the application. The sequence of interactions with the application determines the user X  X  perception of the application X  X  performance; which in turn defines their user experience [17]. So from the user X  X  point of view, it is this stream of documents that largely determines their experience of the application [1]. This is because their goal is to access relevant/useful information. Consequently, the application needs to be delivering a sufficient amount of relevant/useful information to the user to have a satisfac-tory user experience. From an application provider X  X  per-spective, it is important that the usage of the application results in a good user experience. As a poor user experience may lead to disengagement or worse abandonment of the application. Given that an application provider wants to retain and expand their user base, then it is important that the usage performance of the application is monitored over time so that such critical incidents can be identified (and minimized). In terms of evaluation at the application level, different questions arise, such as: 1. what is the effectiveness experienced by the user during 2. how long does it take before the user can effectively 3. how do changes to the application affect usage perfor-4. what level of effectiveness is required to retain users, Given the motivations for this much needed research, the remainder of this paper will be presented as follows: in the next section, we shall present Bookstein X  X  view of the IR process and use a generalization of the approach to repre-sent any IR application. The stream based view is then used as the basis of measuring performance. Then in Section 3, we formalize the stream based view and define a number of precision based measures along with a novel measure, called Relevance Frequency. Through a series of examples, in Sec-tion 4 we demonstrate how to employ these measures in a number of different ways, before summarizing this contribu-tion and outlining directions of future work in Section 5.
In this section we present the stream based view of the IR process that generalizes to any IR application.

Ranked List / Topic Centric View : First lets review the standard model of the IR process: a user submits a query to the system for a given topic, and the system responds by presenting the user with a ranked list of documents. The Figure 1: Sequence Diagram of the standard IR Pro-cess. user then assesses, in turn, each document in the ranked list. Figure 1 depicts the standard IR process. This ranked list / topic centric view makes several assumptions, regarding the interaction and usage of the application:
This abstraction of the IR process has been the basis of much of the evaluation performed in IR. Consequently, most evaluations perform measurements assuming a ranked list. However, an IR application, and the way it is used, may not necessarily produce a ranked list of documents; nor may the user inspect the documents in a linear fashion; and de-pending on the application, various functionality may be engaged by the user during the process (such as inspecting document clusters, using a find similar feature, browsing links or facets, etc.). This assumed user behavior is seldom the case, in practice: and so this view of the IR process does not generalize well to non-standard IR applications or non-deterministic usage.
 Evaluating interactive IR applications with Ranked lists : One direction that has been taken to eval-uate non-standard interaction is to transform the output of the interaction into a ranked list enabling the comparison against standard ranked based methods. For example, in [22], an ostensive browser application recommends similar documents, given the previous documents viewed. The trail of documents the user visits (or the simulated user visits), was used to form a  X  X anking X  which could be compared to a standard retrieval method X  X  ranked lists. Similarly in [15, 16, 20, 27] rankings are formed from the interaction with the envisioned application. While transforming the sequence of documents encountered into a ranked list is appropriate for some applications, it is not possible for all applications. For instance, a filtering application recommends documents, and so the notion that it is evaluated as a ranked list is not ap-propriate. Nor is it appropriate in an exploratory search application where the query is ill defined, and the informa-tion need is highly dynamic (e.g. [6]). While reverting to a ranked list enables comparison with standard retrieval mod-els, it does not consider evaluation at the application level, where it is important to consider the effectiveness that the user experiences during the usage of the application. We argue that adopting a stream based view provides a gen-eral way to represent the usage of an application, such that any IR application can be represented, and that it enables the measurement of the effectiveness experienced through-out the usage of the application.

Stream Based / Time Centric View : The origins of the stream based view stem from the early work of Book-stein [7]. Under Bookstein X  X  view the retrieval process is one where the user examines a sequence of retrieved documents, and where the system receives feedback and adjusts the doc-uments presented to the user. Depending on the feedback, the system presents different documents and the user se-lections create a particular sequence of retrieved/accessed documents. Bookstein argues that it is this sequence of documents that should be evaluated. In [1], this view is generalized to any IR system, such that the usage of the application results in a stream of documents presented and assessed by the user. For any given IR application, the in-teraction can be characterized as follows: a user performs an action given the application X  X  interface, the application en-gages the IR system(s) to produce a response, which is then presented to the user via the application X  X  interface, the user assesses the response and engages with the presented docu-ments, then the user performs a subsequent action, and so on. The order in which the user engages with the presented documents, therefore, defines a stream of documents. This stream based view does not make any assumptions about the user actions/input (i.e. it does not have to be a query), or how the documents are presented to the user (i.e. it does not have to be a ranked list). Without the standard assump-tions it is possible to easily generalize the stream based view to consider any type of IR application. Some example IR applications are shown in Figure 2 which are all represented using this view, where the interaction in the sequence dia-gram shows the stream of documents (denoted by 1 ,...,n ) built up over the course of interaction with the application.
The main difference between this view and the standard rank based view, is that it is temporal and usage specific: where the stream of documents encountered by the user de-pends upon on how the application was used. It is this stream that forms the basis of the evaluation of the effec-tiveness that the user experiences. While the stream based view imposes little restrictions on describing the IR process, it does require other assumptions to be engaged: 1. only one document can be accessed/viewed at a time, 2. each document is independently judged, each time,
The first assumption is that the user can only access one document at any particular point in time. There are, of course, instances when parallel streams of documents are en-countered during the usage of the IR application (this may be the case when dealing with images, or comparing multi-ple documents in multiple windows), but generally only one document is examined at any one point in time. So it is reasonable to engage this simplifying assumption in order to develop usage based measures for streams. Regarding the second assumption, in the standard view it is assumed that each document is judged (independently) with respect to a fixed topic. However, it has been widely acknowledged that through interaction, the information need changes as the user X  X  state of knowledge changes. Under the stream based view, it is assumed that each document is judged with re-spect to the user X  X  current information need/state. Each judgement is based on the current information need, and given the current ( perhaps altered ) state of the user. This is a very important point, because it means that when the same document appears in the stream, it may attract a dif-ferent relevance judgement by the user. These assumptions mean that it is the usage of the application that is evaluated and not whether the system is able to achieve total recall. Thus, usage based effectiveness measures will be predomi-nately precision oriented.

Summary Taking a stream based view allows the usage based effectiveness of any IR application to be evaluated us-ing the same conceptual basis. The same question can be asked of any IR application, such as web browsers, news recommendation applications, search engines, query-less os-tensive browsers, etc, that is: how effective is the usage of the IR application? While the standard rank-based view en-ables a thorough comparison of the different systems/models to be achieved under prescribed circumstances, the stream based view enables the measuring and monitoring of the per-formance. Invariably, the usage performance is going to be a result of the (potentially synergistic) interaction between the user and the application. This view means that either a observational or simulated study needs to be undertaken in order to evaluate the usage performance experienced over time by the user.
In this section, we shall describe a suite of measures for tracking and monitoring the performance resulting from the usage of an application under the stream based view. These measures are designed specifically to be used in a simulation or observational study: so that the effectiveness of the usage of various interactive IR applications can be analyzed and modeled.

First, we begin with the necessary definitions and nota-tion, before outlining these different measures. Then we present a number of precision based measure that provide an estimate of the usage performance at a particular point in the stream, or for a particular time period within the stream. Note that, since an application X  X  performance is monitored in the context of usage, it is not possible to de-velop recall based measures. This would require a post-hoc assessment of all documents. Such a task is problematic due to changes (or the evolution) of a user X  X  information need over time and context [11]. We then introduce a novel measure of the performance, called relevance frequency : which characterizes the rate at which relevant information is encountered during the usage of an IR application.
Before we outline the notation, it is necessary to clarify the definitions of streams and sub-streams. A stream is a sequence of objects ordered temporally. A sub-stream is a sub-sequence derived from the stream, where the order of the objects is preserved. For the purposes of measurement the stream is decomposed into sub-streams. This can be performed, logically, conceptually or practically depending on the specific unit of interest (i.e. topic, session, hour, day,etc).

To formalize the measurement of streams we first intro-duce some notation. Let s denote a (sub) stream which con-sists of a sequence of documents such s =( d 1 ,d 2 ,...,d with length N .Foreachdocument d i we assume that there is an associated judgement r i assigned to it forming a corre-sponding sequence r =( r 1 ,r 2 ,...,r N ). A stream s can be decomposed into sub-streams, such that s ij =( d i ,...,d we shall use stream and sub-stream interchangeably.
For the purpose of introducing the set of measures we focus on a dichotomous decision based on document rele-vance 1 ), where r i = { 0 , 1 } . However, the value of value r could also be a rating, grade or a continuous measurement. This judgement represents whether the document is relevant or useful to the user at that time point in the stream.
For a given stream s with the corresponding sequence of judgements r , it is possible to estimate the precision of the stream by treating the stream as a set and determining the proportion of relevant documents within the stream: where
Given a series of sub-streams ordered by time, it is then possible to obtain a series of precision measurements across the entire stream. While the individual sub-streams are treated like sets, the order of the measurements determines the usage performance experienced over the course of inter-action.
Depending on the type of application and the focus of the evaluation the stream is decomposed into sub-streams, accordingly. This defines the unit of measurement. While there are many possible ways to decompose the stream, here we only consider a few possible variations (and leave other variations for future work):
Regardless of the decomposition, these measures will pro-vide an indication of the application X  X  performance over time, enabling the monitoring of performance. The main distinction between the Block and Window measures and the other measures is that they do not consider the time between document interactions, but simply the order, while
However, other judgements based on utility, for instance, could be used instead [3]. the other measures consider the period of time in which the usage took place. As we shall see (in the next section) both provide interesting ways in which to track, monitor and an-alyze the usage performance.

Cumulative Average Precision: As the precision mea-sures provide point estimates of performance for the sub-streams, it is of interest to summarize the usage performance experienced over these sub-streams (i.e. a temporal aver-age). The cumulative (marco) averaged precision (CAP) can be obtained by averaging over the measurements taken on a stream s decomposed into M sub-streams s j , as follows: this represents the cumulative distribution of precision in the stream up to and including sub-stream s M . We refer to this as a macro-average since the average is taken based on the precision of the sub-streams, and differs from the micro-average which be estimated at the document level. Mea-surements of this nature indicate how the application X  X  usage performance convergences over time/usage. Other statistics, such as the standard deviation and standard error for the stream precision measurements can also be calculated.
In the previous subsection, we defined a suite of precision based measures, however, this only provides one possible way to evaluate streams. In this subsection, we propose a novel stream based measure which conveys a different view of the usage performance. Intuitively, the number of docu-ments a user must examine before encountering a relevant document will impact upon their user experience. If they en-counter many non-relevant documents successively this will detract from the user experience. And, many long periods of non-relevance before encountering relevant information is likely to lead to a negative user experience. The Relevance Frequency measure aims to quantify the rate at which rele-vant documents are encountered during the stream.
RFreq : Given a stream s , decompose s into sub-streams by partitioning the stream whenever a relevant document occurs. The length x of a sub-stream denotes how many documents are examined to find a relevant document. So the Relevance Frequency for a distance of x is the count of the number of sub-streams that are of length x ,( RF req ( x )). For example, if the judgements on a stream yields: where the | . | indicates the sub-streams. Then, the RF req (1) = 2 because there are two sub-streams of length one, and so on, such that: RF req (2) = 1, RF req (3) = 1, RF req (4) = 1, and RF req ( &gt; 5) = 0. By plotting the RF req ( x ) values the distribution of encountering relevant documents can be visualized (see Figures 3 and 6 for exam-ples of RFreq plots.)
Points of Failure ( pof ) : If it is crucial that the appli-cation delivers relevant information at least every y docu-ments, then it is easy to compute the number of points in the stream where this criteria is not satisfied, i.e. the num-ber of points of failure is equal to the: Table 3 provides an example, where the pof ( x&gt; 10) = 163. This means that during the usage of the application there are 163 critical instances where the user has to endure at least ten documents before encountering a relevant docu-ment. Whether these periods of prolonged non-relevance are tolerated depends upon the user and the application. Ob-viously, a stream s which contains all relevant documents will result in a RF req ( x =1)= n ,where n is the length of the stream, and RF req ( x&gt; 1) = 0. Whereas a stream which contains only non-relevant documents will result in an RF req ( x = i ) = 0 for all i  X  n . This is because no relevant document occurs in the stream, so no sub-streams which are terminated by a relevant document exist within s . EFreq : To summarize the distribution of the Relevance Frequencies, the maximum likelihood estimate of the dis-tribution can be taken to obtain the Expected Relevance Frequency as follows: where E [ RF req ] denotes the expected rate of relevant docu-ments in the stream. If E [ RF req ] = 10 the user could expect to encounter nine non-relevant documents before encounter-ing a relevant document at the tenth position, on average. It should be noted that the Expected Relevance Frequency is the stream based analogy to Cooper X  X  Expected Search Length [9] which is computed given a ranked list. An in-teresting direction for future work would be to explore the relationship between these measures.
To demonstrate the utility of the usage based effectiveness measures we shall illustrate the application of such measures using a number of different scenarios. During the course of this demonstration we shall show examples of: (i) how the stream based view can be used to monitor the (ii) how the measures can be used to describe the perfor-
For the purposes of this conceptual paper, we shall simu-late a number of different IR scenarios that use different IR applications in order to demonstrate the proposed measures.
Methods and Materials : The empirical setup of the demonstration uses a TREC test collection for which we have a number of topics and corresponding relevance judg-ments. While these topics and judgments have been created in a specific manner, given the more liberal modeling as-sumptions of the stream based view, this test collection will enable us to demonstrate the application of usage based ef-fectiveness measures in a simulated setting . It should be noted that simulation provides a powerful tool for hypoth-esizing about user behavior [16, 27]. The analysis is con-ducted using the AP 88-89 TREC test collection with topics 51-100. The collection was indexed in Lemur 2 , where stop words were removed and stemming applied. The applica-tions considered are as follows:
While filtering and retrieval are considered to be two sides of the same coin they are evaluated differently [5]. Retrieval uses a ranked list as the basis of measurement, while in fil-tering the set of filtered documents forms the basis of mea-surement (as done in the TREC Filtering Tracks, e.g. [18]). Under a stream based view, conceptually they are measured in the same way though the interpretation of the results is different. It should be noted that the usage of the filtering application occurs over many weeks, months, years, whereas http://www.lemurproject.org the usage of the retrieval application could occur over a much shorter time frame. Thus for this demonstration, we can only show the performance of the retrieval applications over blocks and windows, whereas for the filtering applica-tion we can show the performance over both blocks/windows and day/week/month/etc. For brevity and clarity, we shall only report the following measures: Block Precision (BP) and Week Precision (WP), along with Relevance Frequency (RFreq), and their associated averages.

Standard Retrieval Application :ForeachTREC topic, the title of the topic was used as a query, and each query was posed to the retrieval application. The top 200 documents retrieved for each query was then concatenated together to form the stream of documents that the user en-counters when searching the 50 TREC topics. Essentially, we assume the user proceeds to perform a search on each topic, sequentially. We further assumed that the user ex-amines the top 200 documents per topic (i.e. a fixed form of interaction), and that each topic is queried in numeric order. Given this scenario, it is interesting to note the dif-ference between BP and Precision at n documents, as they are very similar. Table 1 shows performance on a stream defined by a ranked list for P@n and BP at intervals/blocks of 25 documents, up to the first 125 documents. The BP reflects the current effectiveness the user experiences when they examine a given page of 25 results, whereas the P @ n reflects the average effectiveness over n . It is of note that the P@n and CAP provide the same value, in this specific case . Table 1: BP, P@n and CAP for a query stream aka ranked list. The BP reflects the current effective-ness experiences when they examine each page of results, where as the P @ n reflects the average effec-tiveness over n documents. The third column shows the no. of relevant documents seen in the current block and the bracketed figure is the total no. of relevant documents seen up to the current block.
 This is because the blocks are of equal size, so the macro averaging over blocks is the same as the micro average over n documents, and because we are evaluating the ranking in a stream based view. For Day/Week/Month/etc Precision measures where the length of the sub-streams is variable then the micro and macro averages will be different. The added advantage of BP (and the other stream based pre-cision measures) is that the variability of the performance, such as the standard deviation, can also be obtained be-cause the average is taken across a number of blocks (e.g. see Figure 5 for an example using WP).

Retrieval Application with Interaction :Tosimulate a different usage of the retrieval application, we shall assume that the user takes a  X  X erry picking X  like approach [2], where they only harvest the tops of ranked lists for relevant doc-uments (instead of trawling through every single document in the ranking). We shall further assume that after querying the user is presented with a page of 25 results (i.e. a block size = 25), the user assesses these documents. If the perfor-mance of the current page is above a user defined threshold (  X  ) then they continue to the next page of results, otherwise they curtail the search and proceed to issue the next query.
Filter Application : For the filtering application, we as-sume that a user sets up a number of filters within the news recommender filtering application. As news stories are re-ceived by the application, if they appear relevant to one of the filters then it is recommended to the user. The stories are presented to the user on a daily basis as the news breaks. In our example, we monitor the weekly performance and for the purposes of illustration, we used the same 200 documents re-trieved for each topic in retrieval application scenario. How-ever, we ordered the documents by their publication date to simulate the stories being recommended when the news breaks. This was done to show that different orderings of the same set of documents result in distinctly different usage performance experienced over time.
How does the performance of different IR applications vary during their usage? For retrieval, previous research suggests that as the user evaluates more of the ranked list, the precision will decrease (as seen when examining the pre-cision at k documents measurements). However, for other applications, like filtering, it is not known. For this demon-stration, we show the usage performance for the retrieval and a filtering application to illustrate the differences in the effectiveness experienced by the user.

Figure 3 shows the BP measured on blocks of 25 docu-ments within the streams of the retrieval application (top plot) and the filtering application (middle plot). While the precision given the entire stream produced by both applica-tions is the same in this constructed example, the precision experienced at different blocks in the stream is considerably different. Usage of the standard retrieval application in this way results in the user experiencing very high precision, usu-ally after they enter a query, followed by low precision as they examine the ranking provided. On the other hand, the usage of the filtering application results in the user experi-encing less variation in usage performance. The two addi-tional plots on the right show the distribution of the Block Precision over the observed period. The retrieval applica-tion X  X  usage results in performance following an exponential distribution, while the filtering application X  X  usage results in performance following a relatively normal distribution.
The bottom-left subplot, in Figure 3 shows the cumula-tive averaged BP up to the i th block. This figure clearly shows that the performance of a filtering application con-verges to the average performance much quicker than the performance of the retrieval application. This is due to the performance for filtering being sampled from a  X  X ormal X  dis-tribution. From these plots, it is quite clear that the perfor-mance experienced over usage varies dramatically depending on the application.

The bottom-right subplot shows the Relevance Frequency distribution for each application. The expected Relevance Frequency is 3 . 279 and 3 . 806 for the retrieval and filtering applications, respectively. Table 3 also provides statistics to show that while the retrieval application has a lower ex-pected RFreq, there are more times when the user will have to endure 20 or more non-relevant documents, than when us-ing the filtering application (34 versus 16). This is because the retrieval application delivers more relevant content at the beginning of the ranked list, and less relevant content at the end of the ranked list.
Another type of analysis that can be conducted using stream based measures is to examine how the application performance changes due to the user behavior. Here, we pro-vide a very simple demonstration, where we assume that a simulated user adopts a  X  X erry picking X  information seeking behavior [2]. Given the retrieval application (as described above), we assume that a user poses a query, and the system responds with a page of results (a block of 25 documents) which the user examines. If the page is sufficiently rich in relevant material, this motivates the user to continue their search and examine the following page. Otherwise, the user will stop examining the current result list and pose the next query given the set of topics. While we have artificially con-structed this example, it captures the essence of the berry picking approach, as the user only continues if the results are rich in relevant information.

For this experiment, we varied the threshold  X  between 0 and 1. The threshold indicates the decision criteria for our simulated user to continue to the next page (i.e. block), or not. If the current page X  X  precision is higher than the thresh-old  X  , the user continues, else they stop and move on to the next query in the topic set. A threshold of zero means the user examines all pages, while a threshold of one indicates that the user will examine only the first page of results for
Thres. Avg. BP Num. Blocks Avg. Num. Table 2: Summary of performance statistics of dif-ferent berry pickers given the same retrieval appli-cation. Each page contains 25 results with a max-imum of 8 pages per query: as the user X  X  thresh-old increases, less and less pages are viewed. The best BP is obtained at  X  =0 . 7 , where only 83 pages are viewed out of the possible 400, and not when their threshold is  X  =1 . 0 each query (which is equivalent to Precision at k = 25). Ta-ble 2 shows the performance experienced over the course of interaction with the retrieval application for different thresh-olds, along with reporting the number of blocks/pages of re-sults viewed during the usage, and the average number of relevant documents per block.

When the user examines all pages (  X  = 0), then overall usage performance experienced is 0.26 BP. Whereas if the user examines only the first page (  X  =1 . 0) then the over-all usage experienced is 0.45 BP. However, the best usage performance possible is obtained when the user X  X  decision criteria is in between these two extremes  X  and at  X  =0 . 7 the BP is 0.55! Thus, a savvy user can work the application in order to maximize the usage performance. To further il-lustrate the point, Figure 4 shows the distribution of Block Precision for three cases (  X  = { 0 , 0 . 2 , 0 . 5 } ). The results ap-pear to indicate that the berry picker strategy reduces the number of times the user encounters pages with only few rel-evant documents: resulting in the user experiencing higher effectiveness during the course of usage. Since the number of documents accessed is less because of their interaction, it is a open question whether a user could maintain or sus-tain this level of effectiveness if they continued information seeking.
In this example we shall use the filtering application ex-ample where we monitor the weekly usage performance. Fig-ure 6 shows the Weekly usage performance for the filtering application powered by either BM25 or LM. In this simu-lated environment it is possible to obtain the performance for both algorithms; however, if an application is deployed in the wild it is not possible to perform such a paired com-parison. Instead, some form of classical observational ex-periment needs to be conducted [8]: where for example, a pre-test/post test experiment is performed where the usage of the application is observed without the experimental con-dition, then the experimental condition is introduced, then the usage is again observed (i.e. O X O [8]). Figure 4: Distribution of Usage Performance (Block Precision) for the different thresholds (  X  ).

Here, we show an example where we monitor the filtering application with the control filter system (that uses a Lan-guage Model as the kernel), then change to the experimental filter system (that uses BM25), and monitor again. For the first 45 weeks, the application is configured using a poorly tuned Language Model (LM), then for the next 45 weeks BM25 (BM) is used in the filtering application. The weekly precision is obtained for each week, and the macro-averaged Weekly Precision is also computed for weeks 1-45 and 46-90, along with the standard deviation (see Figure 5). From the figure, we can see that the mean performance of the BM powered filter application tends to outperform the LM power application. By performing an appropriate statistical test it should be possible to determine whether there is any significant differences between the underlying methods.
To demonstrate the Relevance Frequency measure, we again use the filtering application. In this example, we use four filters, BM and LM, as above, and a TFIDF filter (TF), and also BM25 with pseudo relevance feedback (BMFB) fil-Figure 5: An observational experiment where the application uses a LM filter (Weeks 1-45), then a BM25 filter (Weeks 46-90). The top plot shows the Weekly Precision experienced during usage, and the bottom plot shows the CAP and standard deviation (dotted lines) for each 45 week period. ter. The IR application is configured with each of the dif-ferent filters, and then monitored across the 90 weeks of the AP88-89 collection given the 51-100 topic set.

Figure 6 shows a plot of the Weekly Precision for BM and LM filtering applications, along with the Relevance Fre-quency plot. In Table 3 the expected Relevance Frequency is shown, along with the number of times the application fails to deliver a relevant document after seeing 10+ and 20+  X  non-relevant documents, respectively. The Expected Rele-vance Frequency indicates that BMFB method delivers, on average, one relevant documents per 3.64 document, where as the LM method delivers, on average, one relevant doc-ument per 4.38. We can also see that there are 36 times when the LM method fails to deliver a relevant document after encountering 20 or more non-relevant documents. On the other hand, for BMFB this drops to only 9 such inci-dents. From an application provider X  X  point of view they want to maximize the frequency of relevance, and minimize the periods of prolonged non-relevance in order to engage and retain users: because by doing so this should translate into a better user experience. However, it is a matter of empirical investigation to determine whether such measures correlate/correspond to a good user experience, or not [19]. Table 3: Expected Relevance Frequency and points of failure statistics for the different filters.
In this paper, we have formalized an alternative approach to the evaluation of Interactive IR applications:: the stream-based / time-centric view that forms the basis of approach represents the usage of any IR application through a stream of documents. This stream enables the usage performance to be evaluated. Given the goal of an IR application, the proposed usage based effectiveness measures provide a novel way in which to monitor and model the performance expe-rienced by the user while interacting with the application. However, there are two main practical considerations that need to be addressed in order to monitor the usage perfor-mance of an application: For the purposes of introducing stream based measures, we have assumed that every document in the stream has a cor-responding judgement (i.e. the completeness assumption), however this does not mean that the stream can not con-tain un-assessed documents. But considering un-assessed documents would require further measures to be developed. These could reflect other aspects of the user experience, such as user engagement. The second consideration poses the greater challenge. However, there have been many advance-ments made towards developing mechanism to infer the rel-evance of documents implicitly [14, 12, 26]. And, as these mechanisms improve and the implicit judgements become more reliable, the quality and accuracy of the usage per-formance measures will also improve making this form of evaluation feasible and more reliable.

In this paper, we have only been able to explore some aspects of the stream-based view, however, there are many aspects that we have not discussed or explored here, but which motivate further research, such as: plot) for the filter application configured with BM and LM methods. These questions are left for future work.

In conclusion, the stream based view naturally focuses evaluation on the usage of an application, and thus requires observational experiments: either with simulations or user studies. With recent developments in technology to rapidly build and prototype practical and real world IR applications then the number of naturalistic studies is likely to increase. So, engaging a stream based view in order to measure, mon-itor and model the usage performance of IR applications is not only timely, but necessary to facilitate the evaluation of these future interactive IR applications. Finally, the stream based view entails a shift away from evaluation through col-lections (especially as this is considered impractical [24]), and towards evaluation through applications deployed, in the lab, and in the wild.

