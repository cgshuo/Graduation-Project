 Multi-instance Learning (MIL) was first proposed by Dietterich et.al. in [1] to predict the binding ability of a drug from its biochemical structure. A certain drug molecule corresponds to a set of conformations which cannot be differentiated via chemical experiments. A drug is labeled positive if any of its constituent conformations has the binding ability greater than the threshold, otherwise negative. Therefore, each sample (a drug) is a bag of instances (its constituent conformations). In multi-instance learning the label information for positive samples is incomplete in that the instances in a certain positive bag are all labeled positive. Generally, methods for multi-instance learning are modified versions of approaches for supervised learning by shifting the focus from discrimination on instances to discrimination on bags.
 The earliest exploration were the APR algorithms proposed in [1]. From then on, a number of approaches emerged. Examples include Diverse Density [2], Citation k  X  NN [3], MI-SVMs [4], MI-kernels [5], reg-SVM [6], MissSVM [7], sbMIL, stMIL [8], PPMM [9], MIGraphs [10], etc. Many real-world applications can be regarded as Multi-instance learning problems. Examples include image classification [11], document categorization [12], computer aided diagnosis [13], etc. As far as positive bags are concerned, current research usually treat them as labyrinths in which witnesses (responsible positive instances) are encaged, and consider nonwitnesses (other instances) therein to be useless or even distractive. The information carried by nonwitnesses is not well utilized. Factually, nonwitnesses are indispensable for characterizing the overall instance distribution, and thus help to improve the learner. Several researchers realized the importance of nonwitnesses and attempted to utilize them. In MI-kernels [5] and reg-SVM [6], nonwitnesses together with witnesses are squeezed into the kernel matrix. In mi-SVM [4], the labels of all nonwitnesses are treated as unknown integer variables to be optimized. mi-SVM tends to misclassify negative instances in positive bags since the resulted margin will be larger. And we will elaborate on this flaw in section 3.1. In MissSVM [7] and stMIL [8], multi-instance learning is addressed from the view of semi-supervised learning, and nonwitnesses are treated as unlabeled data, whose labels should be assigned to maximize the margin. sbMIL [8] attempt to estimate the ratio of positive instances inside positive bags and utilize this information in the subsequent classification. MissSVM, sbMIL and stMIL suffer from the same flaw as mi-SVM. Figure 1: Illustration of the False Positive Phenomenon: The top image is a positive training sample, and the bottom image is a negative testing sample. The symbol  X  and  X  respectively denote positive and negative instances. Enveloped points are instances in a positive bag. The Point not enveloped is a negative bag of just one instance. Separating plane F i corresponds to f ( x ) = i , and G i corre-sponds to g ( x ) = i . The learners f and g are obtained with and without the projection constraint, respectively. Instances are labeled according to f . For details, please refer to the passage below. The neglect of nonwitnesses in positive bags may lead to false positive and cause a model to misclas-sify unseen negative samples. For example, in natural scene classification, each image is segmented to a bag of instances beforehand, and each instance is a patch (ROI, Regions Of Interest) charac-terized by one feature vector describing its color. The task is to predict whether an image contains a waterfall or not (Figure 1). A positive image contains some positive instances corresponding to waterfall and some negative instances from other categories such as sky, stone, grass, etc., while a negative bag exclusively contains negative instances from other categories. Naturally, some neg-ative instances (patches) only exist in positive bags. For instance, the end of a waterfall is often surrounded by mist. The aforementioned approaches tend to misclassify negative instances in posi-tive bags. Therefore, the patch corresponding to mist is misclassified as positive. Given an unseen image with cirrus cloud and without waterfall, the obtained learner will misclassify this image as positive because cirrus cloud and mist are similar to each other.
 To avoid both false negative and false positive, we attempt to classify instances inside positive bags far from the separating hyperplane and place positive and negative instances at opposite sides. We achieve this by introducing projection constraints based on kernel principal component analysis into MI-SVM [4]. Each constraint is defined on a positive bag to encourage large variance of its con-stituent instances along the normal direction of the separating hyperplane. We apply the Constrained Concave-Convex Procedure (CCCP) to solve the resulted optimization problems.
 The remainder of the paper is organized as follows: Section 2 introduces notation convention and the CCCP. In Section 3 we bring out the projection constraint and the corresponding formulation for multi-instance learning. In Section 4, the algorithm is evaluated on real world data sets. Finally, conclusions are drawn in Section 5. 2.1 Notation Convention The origin of multi-instance learning [1] has been presented in section 1. Let X  X  R p be the a learner to predict the label of an unseen bag. Compared with traditional supervised learning, the learner is a mapping from 2 X to Y instead of from X to Y . Denote the index sets for positive and negative bags by I + and I  X  respectively. Without loss of generality, assume that the instances are I ( x ij ) = 2.2 Constrained Concave-Convex Procedure Non-convex optimizations are undesirable because few algorithms effectively converge even to a local optimum. However, if both objective function and constraints take the form of a difference be-tween two convex functions, then a non-convex problem can be solved efficiently by the constrained concave-convex procedure [14]. The fundamental is to eliminate the non-convexity by changing non-convex parts to their first-order Taylor expansions. The original problem is as follows: from a random x (0) , (1) is approximated by a sequence of successive convex optimization problems. first-order Taylor expansions, and the resulted optimization problem is as follows: convergence. In [14] it is proved that the CCCP converges to a local optimum of (1). 3.1 Support Vector Machine Formulation Our work is based on the support vector machine (SVM) formulations for multi-instance learning, to be exact, the MI-SVM [4] as follows: Compared with the conventional SVM, in MI-SVM the notion of slack variables for positive samples is extended from individual instances to bags while that for negative samples remains unchanged. As shown by the first set of max constraints, only the  X  X ost positive X  instance in a positive bag, or the witness, could affect the margin. And other instances, or nonwitnesses, become irrelevant for determining the position of the separating plane once the witness is specified.
 The max constraint at first sight seems to well embody the characteristic of multi-instance learning. Indeed, it helps to avoid the false negative, i,e., the misclassification of positive samples. However, it may incur false positive due to the following two reasons. Firstly, the max constraint aims at discovering the witness, and tends to skip nonwitnesses. Thus each positive bag is approximately oversimplified to a single pattern, i.e., the witness. Most information in positive bags is wasted. Secondly, due to the characteristic of the max function and the greediness of optimization methods, the predictions of nonwitnesses are often adjusted above zero in the learning process. Besides, there is no mechanism to draw the predictions of nonwitenesses below zero. Nevertheless, many nonwit-nesses in positive bags are factually negative instances. For example, in natural scene classification, many image patches in a positive bag are from the irrelevant background; in document categoriza-tion, many posts in a positive bag are not from the target category. Hence, many nonwitnesses are mislabeled as positive, and we obtain a falsely large margin.
 As shown in Figure 1, MI-SVM classifies half instances in the training sample as positive, and some negative instances are mislabeled. This false positive will impair the generalization performance. 3.2 Projection Constraint The above problem is not unique for MI-SVM. Any approach without properly utilizing nonwit-nesses has the same problem. In our preliminary work before this paper, we tried three solutions. Firstly, we treat the labels of all nonwitnesses as unknown integer variables to be optimized. In the SVM framework, it is exactly the mi-SVM [4] as follows: It seems that assigning labels over all nonwitnesses should lead to a reasonable model. Nevertheless, nonwitnesses are usually labeled positive since the consequent margin will be larger. Thus, many of nonwitnesses are misclassified. As far as the example in Figure 1 is concerned, the obtained learner is g ( x ) instead of f ( x ) . MissSVM [7] takes an unsupervised approach. For every instance in positive bags, two slack variables are introduced, measuring the distances from the instance to the positive boundary f ( x ) = +1 and the negative boundary f ( x ) =  X  1 respectively, and the label of the instance depends on the smaller slack variable. stMIL [8] takes a similar approach. As mi-SVM, MissSVM and stMIL also suffers from misclassification of nonwitnesses. sbMIL [8] tackles multi-instance learning in two steps. The first step is similar to MI-SVM, and the second step is a traditional SVM. Still, there is no mechanism in sbMIL to avoid false positive.
 In the second solution, we simultaneously seek for the  X  X ost positive X  instance and the  X  X ost neg-ative X  instance in a positive bag by adding the following constraints to (3): And the term sification of nonwitnesses is alleviated since at least the  X  X ost negative X  nonwitness is classified correctly, the information carried by most nonwitnesses are not fully utilized. As far as the example not appropriate for applications which involve positive bags only with positive instances. The third solution is the projection constraint proposed in this paper. In a maximum margin frame-work we want to classify instances in a positive bag far away from the separating hyperplane while place positive instances and negative instances at opposite sides. From another point of view, in the feature (kernel) space, we want to maximize the variance of instances in a positive bag along w , the normal vector of the separating hyperplane. Therefore, the principal component analysis (PCA) [15] is just the technique that we need. To tackle complicated real world datasets, we directly develop our approach in the Reproducing Kernel Hilbert Space (RKHS). Let X be the space of instances, and H be a RKHS of functions f : X  X  R with associated kernel function k (  X  ,  X  ) . Note that f is both a function on X and a vector in H . With an abuse of notation, we will not differentiate them unless necessary. Denote the RKHS norm of H by  X  f  X  H . Then MI-SVM can be rewritten as follows: Figure 2: Illustration of the Effect of the Projection Constraint: Please note that the projection constraint is effective for datasets with any geometric distribution once an appropriate kernel is selected. Enveloped points are instances in a positive bag. Points not enveloped are negative bags of The learner f and g are obtained with and without the projection constraint, respectively. Instances are labeled according to f .  X  and  X  denote positive instances and negative instance respectively. According to the representer theorem [16], each minimizer f  X  H of (6) has the following form: where all  X  i  X  R , and  X  (  X  ) induced by k (  X  ,  X  ) is the feature mapping from X to H . Next, we will propose our key contribution, i.e., the projection constraint. Given a positive bag B RKHS by f . According to the theory of PCA [15, 17], maximizing the variance of mapped instances { data points to their projections on the normalized vector f  X  f  X  where  X  ( m i ) = 1 n projection point of  X  ( x ij ) . After simple algebra, we get: Substituting (9) and (7) into (8), we arrive at: where K is a n  X  n kernel matrix defined on all the instances of both positive bags and negative bags, o i = trace ( K I ( B and L 2 i is the  X  X entralized X  L 2 i as follows: where 1 n is a matrix with all elements equal to 1 n , and L i is a n  X  n matrix formed by keeping the I ( B i ) rows of K and setting all the elements in other rows to 0 : Generally, the optimal normal vector f varies for different positive bags. Hence it is meaningless to solve (10) for its optimum. Instead, we average (10) by the bag size n i , and use a common threshold  X  to bound the averaged projection distance for different bags from above. We name the obtained inequality  X  X he projection constraint X , as follows: This is equivalent to bounding variance of instances in positive bags along f from below [15]. Substituting (7) into (6), and adding the projection constraint (12) for each positive bag to the re-sulted problem, we arrive at the following optimization problem: 3.3 Optimization via the CCCP In the problem (13), the objective function and the second set of constraints are convex. The first set of constraints are all in the form of difference of two convex functions since the max function is convex. According to the definition of J i ( f ) in (8), J ( ) in (10) is not less than 0 for any . are all in the form of difference of two convex functions. Therefore, we can apply the Constrained Concave-Convex Procedure (CCCP) introduced in section 2.2 to solve the problem (13).
 Since the function max in the first set of constraints is nonsmooth, we have to change gradients to subgradients to use the CCCP. The subgradient is usually not unique, and we adopt the definition used in [6] for the subgradient of max where where n a is the number of x ij that maximize k T I ( x estimate for and  X  ij by ( t ) and  X  ( t ) ij respectively. Then the first order Taylor expansion of max According to (15), we have Using (17), (16) reduces to  X  Replacing max straints by their first order Taylor expansions, finally we get: where S i = o i  X  K  X  L 2 i . The problem (19) is a quadratically constrained quadratic program (QCQP) with a convex objective function and convex constraints, and thus can be readily solved via interior point methods [18]. Following the CCCP, we can do the iteration until (19) converges. 4.1 Classification: Benchmark Benchmark data sets comes from two areas. Musk 1 and Musk 2 data sets [1] are two biochemical tasks which directly promoted the research of multi-instance learning. The aim is to predict activ-ity of drugs from structural information. Each drug molecule is a bag of potential conformations (instances). The Musk 1 data set consists of 47 positive bags, 45 negative bags, and totally 476 instances. The Musk 2 data set consists of 39 positive bags, 63 negative bags, and totally 6598 in-stances. Each instance is represented by a 166 dimensional vector. Elephant, tiger and fox are three data sets from image categorization. The aim is to differentiate images with elephant, tiger, and fox [4] from those without, respectively. A bag here is a group of ROIs (Region Of Interests) drawn from a certain image. Each data set contains 100 positive bags and 100 negative bags, and each ROI as an instance is a 230 dimensional vector. Related methods for comparison includes Diverse Table 1: Test Accuracy(%) On Benchmark: Rows and columns correspond to methods and datasets respectively.
 Density (DD,[2]), EM-DD [19], MI-SVM [4], MI-Kernel [5], stMIL [8], sbMIL [8], MIGraph and miGraph [10]. When applied for multi-instance classification, our approach involves three parame-ters, namely, the bias/variance trade-off factor C , the kernel parameter (e.g.:  X  in RBF kernel), and the bound parameter  X  in the projection constraint. In the experiment, C ,  X  , and  X  are selected from { MOSEK toolbox 1 to solve the resulted QCQP problem (19). The other experiment uses the same parameter setting.
 The ten-times 10-fold cross validation results (except Diverse Density) are shown in Table 1. The results for other methods are replicated from their original papers. The results not available are marked by N/A. The bolded figure indicates that result is better than all other methods. Table 1 shows that the performance of our approach (PC-SVM) is competitive. Recall that the difference between our approach and MI-SVM is just the projection constraint. Therefore, as discussed in section 3.2, the results in Table 1 demonstrates that the strength of nonwitnesses is well utilized via the projection constraint. 4.2 Classification: COREL Image Data Sets Table 2: Test Accuracy(%) On COREL: Rows and columns correspond to methods and datasets respectively.
 COREL is a collection of natural scene images which have been categorized according to the pres-ence of certain objects. Each image is regarded as a bag, and the nine dimensional ROIs (Region Of Interests) in it are regarded as its constituent instances. In experiments, we use the 1000-Image data set and the 2000-Image data set which contain ten and twenty categorizes, respectively. Following the methodology in [10], on both of the two data sets the related methods are compared by their five times 2-fold cross validation results. The algorithm for comparison include Diverse Density (DD), MI-SVM, MIGraph, miGraph , MI-Kernel and reg-SVM. In the last four algorithms one-against-all strategy is employed to tackle this multi-class task. In our approach this strategy is also used. Table 2 shows the overall accuracy as well as the 95% interval. As in benchmark data sets, our approach is competitive with the latest methods. The results again suggest that fully utilizing the nonwitnesses is important for multi-instance classification. We design a projection constraint to fully exploit nonwitnesses to avoid false positive. Since our approach is basically MI-SVM with projection constraints, the improved results on real world data sets validate the strength of nonwitnesses. We will introduce the universal projection constraint into other existing approaches for multi-instance learning, and related learning tasks, such as multi-instance regression, multi-label multi-instance learning, generalized multi-instance learning, etc. Acknowledgments We gratefully acknowledge reviewers for their insightful remarks and editors for their assiduous work. We also deeply appreciate Kuijun Ma X  X  careful proof-reading. Finally, we are extremely thankful to Runing Liu for the fascinating illustrations. This work was partially supported by Na-tional Basic Research Program of China under Grant No.2004CB318103 and National Natural Sci-ence Foundation of China under award No.60835002 and 60975040. References
