 Topic or feature extraction is often used as an important step in document classification and text mining. Topics are suc-cinct representation of content in a document collection and hence are very effective when used as content identifiers in peer-to-peer systems and other large scale distributed con-tent management systems. Effective topic extraction is de-pendent on the accuracy of term clustering that often has to deal with problems like synonymy and polysemy . Retrieval techniques based on spectral analysis like Latent Seman-tic Indexing (LSI) are often used to effectively solve these problems. Most of the spectral retrieval schemes produce term similarity measures that are symmetric and often, not an accurate characterization of term relationships. Another drawback of LSI is its running time that is polynomial in the dimensions of the m  X  n matrix, A . This can get pro-hibitively large for some IR applications. In this paper, we present efficient algorithms using the technique of Locality-Sensitive Hashing (LSH) to extract topics from a document collection based on the asymmetric relationships between terms in a collection. The relationship is characterized by the term co-occurrences and other higher-order similarity measures. Our LSH based scheme can be viewed as a sim-ple alternative to LSI. We show the efficacy of our algorithms via experiments on a set of large documents. An interesting feature of our algorithms is that it produces a natural hi-erarchical decomposition of the topic space instead of a flat clustering.
 H.3.3 [ Information Search and Retrieval ]: Clustering, Hashing Algorithms, Experimentation Latent Semantic Indexing, Locality-Sensitive Hashing work done while the author was at Ebrary, Inc.
 Copyright 2006 ACM 1-59593-433-2/06/0011 ... $ 5.00.
Topics are succinct content identifiers that can be effec-tively used in various applications such as document clas-sification and clustering. One of the desired characteristics of a term clustering scheme is the ability to group terms based on their semantic relationships. To this end, LSI uses singular value decomposition (SVD) to  X  X luster X  terms and documents based on their semantic content. An ex-cellent exposition of LSI and its applications in Informa-tion Retrieval can be found in [17] and references therein. The key feature of LSI is to map the documents from the m -dimensional term space to (often significantly) lower k -dimensional topic space. Each topic can be specified by a weighted set of terms and typically, corresponds to a left eigenvector of the original term-document matrix, A .When the term-document matrix is large, the exact computation of the SVD of the matrix and hence the decomposition of the original term space into topic space becomes prohibitively expensive ( min { mn 2 ,m 2 n, mnk } ). To overcome this prob-lem, most variants of LSI [6, 8] deal with the best rank k approximation D  X  of A such that where . F is the Frobenius norm of the matrix. The reader is referred to the theorem of Eckert and Young [1] that shows the existence of such a minimum. The r.h.s of the above equation is the sum of the smallest n  X  ( k + 1) singular values of A .

In this paper, we focus on using relationships between terms to compute the topic subspace. We consider similar words to be related to similar terms. An interesting feature of our algorithms is that it produces a natural hierarchical decomposition of the topic space instead of flat clustering. We compute the topic set by using Locality-Sensitive Hash-ing(LSH) [11, 14] to compute the similarity between terms rather than measures like cosine similarity commonly used in vector space models. A locality-sensitive hashing scheme is a distribution on a family H of hash functions when ap-plied to a collection C of entities yields where sim ( x, y )  X  [0 , 1] is some similarity function defined on the collection C [4]. The primary advantage of this ap-proach is its efficiency. The pairwise similarity between m terms can be computed in time proportional to the num-ber of non-zero entries in the matrix, O ( | M mn |  X  0 )versusthe O ( m 2 n ) it takes for computing the cosine similarity in vector space models. We will describe LSH in more detail in section 3. LSH lacks the advantage of SVD to compute similarity of two objects by not only comparing the objects themselves, but also recursing to include the similarity of sets of ob-jects similar to these objects. There is work to show that LSI exploits higher-order term co-occurrences [18]. We over-come the drawback by computing higher-order similarities between terms. Higher-order term similarity essentially de-fines similarity between two terms as a function of the num-ber of terms that are similar to both terms. This procedure is detailed in Section 4.1.

In this work, we adopt an asymmetric measure for similar-ity which, in our observations, is more appropriate for doc-ument collections. In information theoretic terms, a pair-wise symmetric measure of term similarity which implies the mutual information contained in both terms is roughly the same, i.e., A tells as much about B as much as B tells about A . In reality, this is not always true. Term relationships are usually characterized by generalization and specialization of a concept. For any term pair ( t a ,t b ), t a  X  t b if t a is a hyper-nym of t b ; t a t b if t a is a hyponym of t b . For example, for the term pair (network, protocol) , every occurrence of the term protocol might co-exist with the term network , but the inversemightnotbetrue. Theterm network might occur along with other terms such as computer .Weuseasim-ilarity function that supports the asymmetric relationship between terms in the collection. One such function is
This similarity measure can be interpreted as the natu-ral conditional probability of term B occurring in a random document given that it contains A . This measure of correlat-ing two sets has been used widely in several areas including data mining for mining association rules between database columns. We believe the methods we proposed for comput-ing term similarity using this measures is applicable to not only information retrieval, but also to other contexts such as recommendation systems and data mining. Such a metric rightfully allows for topics to be hierarchically related in-stead of a flat structure. Our algorithms naturally produce such a hierarchy.
In this study, we propose algorithms for hierarchical topic extraction from a collection of documents using the asym-metric relationships between terms. The algorithms are ef-ficient compared to schemes that are used to compute term similarities. The efficiencies are obtained by sampling only a small subset of the original term-document matrix and then computing the similarity between two terms using LSH in-stead of cosine similarities. We propose novel algorithms for bag similarities where the running time depends only on the number of distinct entries in the multi-set. Further-more, we extend the term similarity computation to include higher-order similarity measures to approximate computa-tionally intensive schemes like LSI. The running time of our algorithms is proportional to the number of non-zero en-tries in the truncated term-document space. The details of this scheme are presented in Sections 3 and 4. Finally, in Section 6, we show the efficacy of our algorithms via exper-iments on large document sets.
Current work on topic extraction has broadly focused on concept learning [22, 9], semantic similarity between terms [3], and symmetric similarity measures.

The most relevant work on LSH for document cluster-ing is that of Haveliwala, Gionis, and Indyk [12]. In that work, they propose an algorithm to cluster URLs based on LSH. Our algorithms differs from theirs in several respects: first, we use asymmetric similarity to compare terms; sec-ond, since our LSH is performed on bags with large counts, their algorithms become impractical. Their work looks at every element in the multi-set to compute bag similarity. This can get prohibitively large; third, their work focuses on document clustering and does not address topic extrac-tion.

Drineas and others [8] propose an approximation algo-rithm to efficiently compute the SVD on a random sub-matrix of a given matrix. However, it is only applicable to symmetric matrices.

Jeh and Widom [15] propose a fixed-point scheme to com-pute similarity between objects based on concepts that are similar to ours. Again, their analysis relies on symmetric relationship between objects and consider all term-pairs in their computations. In this study, we circumvent similarity computation between all term pairs. Dhillon, Mallela and Kumar [7] provide a feature clustering algorithm based on information-theoretic analysis while Kontostathis and Pot-tenger [18] show that higher-order co-occurrences, used by LSI and our algorithms, are important for term clustering.
In this section, we will present the model and definitions that form the basis of our algorithms to extract topics and from the document classification. We model the asymmet-ric relationship between terms by a weighted directed graph G =( V, E ) with each term representing a vertex in G and an edge between vertex u and vertex v exists if terms corre-sponding to vertices u and v co-occur in the collection. Note that the weight on the edge ( u, v ) is not necessarily equal to the weight on the edge ( v, u ). Clustering nodes in such graphs can be very hard since the graphs can be very large. We propose randomized algorithms to group nodes that are similar to each other.
We now present an overview of our algorithms, which we will detail in the following sections. Before we start process-ing a term-document matrix, we rank the terms in the collec-tion based on their average TFIDF scores and consider the top k terms in the analysis. Typically, it is the TFIDF score
The algorithm to compute the topics based on higher-order term similarities runs in two phases. In the first phase, we compute the term-term similarity matrix as follows: 1. Each term is represented as a document vector where 2. Next, we compute the sketch for each term as a con-3. We compute similarity between two given terms sim ( t 1 In the second phase of the algorithms, we consider the sim-ilarity between sets of terms that are similar to t 1 and t compute the higher-order similarity. 1. Each term is represented by a term vector whose length 2. Next we compute sketch of each term by concatenation
Based on the term-term similarity matrix, we generate the directed graph and compute strongly connected components (SCCs) in this graph and the resulting kernel-DAG (directed acyclic graph) with the strongly connected components as nodes. Each SCC is declared as a topic with the relationship between two topics encapsulated by the direction of the edge between edges in the kernel-DAG. A topic a that generalizes another topic b is represented by the edge ( a, b )inthekernel-DAG.

The details of the two phases are presented in Sections 3 and 4 respectively. We start with an exposition of the theory behind locality-sensitive hashing and show how it can be used to design fast algorithms for similarity estimation.
Definition 1. A locality-sensitive hashing scheme [4] is a distribution on a family H of hash functions operating on a collection of objects, such that for any two objects x, y , Here sim ( x, y ) is some similarity function between objects in the collection.
 A commonly used similarity measure for comparing sets A and B is
There is a well-known locality-sensitive hashing function  X  Min-wise independent permutation [2]  X  that can be used for this similarity measure. Clearly, sim ( A, B ) is symmet-ric. For clarity, we will denote this measure of similarity by sim S ( A, B ). Given a set A of elements from a universe U , its min-wise independent permutation denoted MH ( A ) is computed as follows: let R ( x ) ,x  X  U be a real valued (hash) function that maps elements from the universe U to a real number randomly and uniformly in the interval [0 , 1].
Definition 2. A Min-wise independent permutation is de-fined by MH ( A )= argmin x { R ( x ) | x  X  A } . Essentially, MH ( A ) is the element in A whose hash value into the in-terval [0 , 1] is minimum.

Observe that the hash function R used to map elements to real numbers must be performed consistently ;thatis, R applied on the same element must result in the same value each time regardless of which set(s) it appears in.
Alternately, instead of using the function that maps each element to a real number, we can use a random permuta-tion of all the elements in U , and define MH ( A )tobethe leftmost element of A in this permutation. As mentioned earlier, it is well known that Theorem 1. [2] For any two sets A and B , Pr[ MH ( A )=
A simple extension of this similarity measure and the hash function from sets to bags has been shown in [12]. Given a bag (multiset), create a new set with distinct elements for each copy of a given element in the bag. Essentially, if f x is the number of occurrences of the element x in a bag, replace x by the pairs { ( x, i ) | 1  X  i  X  f x } . It is easy to see that the size of the intersection and union of the sets thus obtained are same as the bag intersection and bag union. This gives a simple generalization of min wise independent permutation that works for bags. However, unfortunately the time com-plexity of computing such a hash function grows linearly in the total number of occurrences of the different elements. So, if the number of occurrences of a certain element is a million, then a million operations have to be performed for that element. Also, this method does not work if the num-ber of occurrences (or weight) of an element is allowed to be fractional. One method to deal with non-integral weights is to multiply these weights by a large number and round them to the nearest integer. However, as pointed before, multi-plying by large number makes the number of occurrences large thus resulting in a large running time.

We present two methods to provide a fast running time even if the number of occurrences of the different elements is large. Unlike the earlier algorithm, our running time is strictly polynomial in the size of the bit representations of the frequencies. The second method also works for weighted sets with fractional weights.
Since the bad elements have been ordered to pairs of the form ( x, i ), we will work with a real valued hash function R that maps such pairs to a random value in [0 , 1].
Given a bag A , where an element x has a frequency f x , we need to compute This computation can be viewed as first computing, copies of x .Let y = argmin x { MH e ( x, f x ) | x  X  A } then computing, instead of f x .

Visualize the process of hashing the different copies of x onto the real line sequentially. An important observation is that since we want to find the copy of x that hashes to the smallest real number, we can maintain the smallest hash value seen so far and ignore the computations on subsequent elements that hash to its right, i.e., larger values.
That is, given that MH e ( x, j )= r , with probability 1 each of ( x, j +1) , ( x, j +2) ,.. will hash to a value larger than r which will leave the MH e value unchanged. In fact, we can directly compute the distribution on the number of ele-ments that will hash to the right of (greater than) r before the next one hashes to its left. This is essentially a random integer with the inverse geometric distribution with param-eter 1  X  r .Denotethisby G  X  1 1  X  r .Let G  X  1 1  X  r ( x, i )denotethe distribution seeded by ( x, i ). This ensures the same value from the distribution for the same seed.
 We have the following algorithm to compute MH e ( x, f x ): Algorithm 1 ComputeMinHash, MH e ( x, f x ) 1: i  X  0 ,r  X  1 2: while i  X  f x do 3: skip  X  G  X  1 1  X  r ( x, i ) { number of elements that will hash 4: mh  X  i 5: i  X  i + skip 6: r  X  rU [0 , 1] ( x, i ) { uniform random number in the 7: end while 8: return MH e ( x, f x )= mh
Since algorithm 1 produces MH e ( x, f x )withthesame distribution as before, we have the following theorem.
Theorem 2. For any two bags A and B , for the hash functions defined above, Pr[ MH ( A )= MH ( B )] = | A  X  B |
Although Method 1 takes only log f x time for each ele-ment x , it still requires an iterative computation that picks random numbers from an inverse geometric distribution.
We present an alternate method that is much simpler but realizes the desired similarity measure with a small error. We will argue that if the bag sizes are large than the similar-ity measure it realizes is very close to the desired similarity measure. To achieve this we need to make use of an upper bound F on the frequency of any element in a bag. We then normalize the weight of each bag element to a value between zero and one by setting w x = f x /F .

Let  X  A and  X  B , denote the normalized bags where an ele-ment has frequency equal to its weight.
Instead of working with bag A , we produce a set A that includes each element x  X  A with probability w x , as follows. Let R 2 ( x ) being a random (consistent) function that maps elements to a real number in [0 , 1] such that A = { x  X  A | R 2 ( x )  X  w x } . To compare the two bags A and B ,we simply compute sim S ( A ,B ).

We summarize the algorithm for computing the min-hash value of a bag.
 Algorithm 2 ComputeMinHash, MH ( A ) 1: normalize the bag A to produce  X  A . 2: produce set A = { x  X  A | R 2 ( x )  X  w x } . 3: return MH ( A ) { Since A is a set, use min-wise inde-
Continuing the analysis to quantify the error in the simi-larity computation introduced by this method, it is easy to see that given two bags A and B ,
However, it is possible to show that if |  X  A | or |  X  B then the similarity computation holds within a small error.
Theorem 3. For any two bags A and B ,and 0 &lt;&lt; 1 , functions defined above, with probability at least 1  X  e  X (
Proof. We have Pr[ MH ( A )= MH ( B )] = | A  X  B | all coin tosses are independent, | A  X  B | is a sum of indepen-dent Bernoulli random variables, with expectation at least M . Sold by Chernoff bounds, the probability that it differs from the mean by more than an fraction is at most e  X   X ( M Similarly the probability that | A  X  B | differs from its mean by more than |  X  A  X   X  B | is that most e  X   X ( M ) , completing the proof
In this paper, we consider the relationship to be asymmet-ric . This is based on the number of co-occurrence of a term pair compared to the total number of occurrences of each term in a pair. As explained in Section 1, one of the terms could be a hyponym or hypernym w.r.t the other term.
We use sketching algorithms described in the previous sec-tion to estimate similarity wherein we use succinct sketches of objects in a collection to compute the similarity between objects. Objects x and y that are similar have sim ( x, y )=1 while completely dissimilar objects would have sim ( x, y )= 0, where sim ( x, y )  X  [0 , 1] denotes the similarity between objects x and y .

Min-Wise Independent Permutations are useful sketching functions that allow us to compute symmetric similarities between sets in a collection of sets as defined in Equation 1. This measure of similarity is exactly the Jaccard Coefficient in information retrieval. In this analysis, we view document sets and terms interchangeably. We want to use a similarity function that supports the asymmetric relationship between terms in the collection. One such function is
This similarity measure can be interpreted as the natural ability of term B occurring in a random document given that it contains A .Given sim S ( A, B ) (computed by Equa-tion 1), one can compute sim A ( A, B ) from basic set theory, by
Given a random collection of k locality-sensitive hash func-tions, MH 1 ,MH 2 ,...,MH k , we can compute the sketch of atermby where D t is the document set of term t .Now,giventwo terms t 1 and t 2 , we can compute the similarity between the terms by using Since each coordinate of this sketch of two terms agrees with probability sim A ( t 1 ,t 2 ), it follows from Chernoff bounds that the estimate is close to the desired similarity measure.
Theorem 4. With probability at least 1  X  e  X   X ( k ) , |
EstimateSim A ( t 1 ,t 2 )  X  sim A ( t 1 ,t 2 ) | X  .
Unsupervised clustering algorithms have successfully used methods like LSI to compute clusters. There has been work to show that LSI exploits higher-order term co-occurrences [18]. We adopt a similar approach by considering a second order similarity computation. Using the term-term matrix containing the similarity between any two terms, we iterate over the same procedure to compute the second order simi-larity, this time using the set of similar terms, N t for a given aterm t instead of its document set as
EstimatedSim 2 A ( t 1 ,t 2 )= where S 2 ( t )= { MH 1 ( N t ) ,MH 2 ( N t ) ,...,MH k ( N t ) we generate a term-term sim ilarity matrix and the corre-sponding directed graph G =( V, E )wheretheweightonan edge ( u, v )in G has weight w uv = EstimatedSim 2 A ( u, v ). An edge is added to G only if w uv  X   X  , for some threshold  X   X  [0 , 1]. In this study, we use  X   X  [0 , 0 . 25]. We now present our algorithm for generating hierarchical topics based on the concept that two objects are similar if they share a lot of similar objects.
Given an m  X  n term-document matrix A , we generate a hierarchical decomposition of the term space. Informally, our algorithm picks the top l terms in the collection of n documents. The reduced l  X  n term-document matrix A is used to generate the term-term similarities using LSH. The algorithm 3 takes a parameter, k , that specifies the number of hash functions to be used in the LSH scheme.
 Algorithm 3 GenerateTopics(  X  , l , A , N h ) 1: T  X  t 0 ,t 1 ,...,t l { top l ( &lt;&lt; m )terms } 2: Generate l  X  l term-term matrix B s.t. b ij = 3: Generate similarity graph G =( V, E ) based on the un-4: Compute connected components T 1 ,T 2 ,...,T p in G to
Computing the top l terms takes time O ( | M mn |  X  0 )where |
M mn |  X  0 ( &lt;&lt; mn ) is the number of non-zero entries in the m  X  n term-document matrix. Let | M ln |  X  ber of non-zero entries in the truncated l  X  n matrix. To compute term similarity, each MH computation takes time O ( similar, populating the term similarity matrix takes O ( l )per iteration with a total running time of O ( | M mn |  X  0 + k From the guarantees of Theorem 3, for a confidence proba-bility of 1  X   X  , we need to set k = 1 log( 1  X  ) giving a running O ( m 2 n ) running time for cosine similarity computation.
A simple analysis of the term-weight distribution yielded a heavy-tailed distribution with the number of terms with large weights being very small in the collection. Thus, reduc-ing the dimensionality of the term space helps in  X  X iltering X  spurious term relationships. Based on this observation, we compute the top l terms in a collection of documents as where N is the total number of documents in the collection and tf j is the term frequency of term t i in document j ,and D the average TFIDF score of the term. Thus, we compute a l  X  k truncated version of the original n  X  n term-term similarity matrix.

Low rank approximation -One practical application when dealing with large matrices is to find a low-rank approx-imation of the matrix and then work with the low-rank approximation to produce efficient algorithms. There is a lot of literature in the area of finding low-rank approxima-tions [1]. Our approach is analogous to sampling entries in the term-document matrix with probabilities proportional to the weight of an entry in the matrix to generate a ma-trix of lower rank. An interesting open problem would be to characterize this truncated matrix as a valid low-rank approximation of the original term-term matrix. Another view that can be adopted would be to verify the properties of the reduced term similarity graph are good approxima-tions to those in the original graph. Some examples of these properties could include a clustering of the original graph and commute times between any two nodes in the original graph.
Extracting topics in the term space is equivalent to com-puting the strongly connected components in the underlying reduced graph. In this study we use algorithms based on DFS to extract the SCCs in the directed graph. However, these algorithms can produce SCCs that represent topics that are not cohesive. By this we mean, the output could be weak in terms of the induced hierarchy of topics since the SCCs could be disconnected. To produce more granu-lar hierarchies, it might be required to analyze individual components to extract sub-topics from each component and define the relationships between various sub-topics. One could employ algorithms to find directed sparse cuts to bet-ter analyze each SCC in the graph.

Flake, Tarjan, and Tsioutsiouliklis [10] have proposed a hierarchical graph partitioning algorithm using min-cut trees which is well suited for web and citation graphs. In this work, we adopt a simple intuitive approach to recursively partition a SCC in a directed graph. To generate the hier-archy, we generate the graph G in step 3 of Algorithm 3 with the threshold for edge weight,  X  , set to a very small value, say 0 . 01. We use a straight-forward DFS based scheme to par-tition the term similarity graph into forest of DAGs. Each DAG expresses the hierarchical relationship between topics. Any SCC in a DAG can be decomposed into a DAG of SCCs by setting the threshold parameter  X  to a larger value and recursively running our partition algorithm.
One of the main advantages of using an asymmetric sim-ilarity measure is the resulting directed acyclic graph from any connected component analysis on the directed term sim-ilarity graph. Moreover, an asymmetric similarity measure better characterizes the relationship between any two terms. For example, if | A | = 100 and | B | =5,and | A  X  B | = 3, we have sim S ( A, B )=0 . 30, sim A ( A, B )=0 . 3, and sim A ( B, A )=0 . 6. If the threshold in a graph reduction step were set at 0 . 5, we would loose the relationship between A and B in the similarity graph when using sim S ( A, B ), whereas sim A ( B, A ) would survive the graph reduction step.
In this section, we detail the experiments we carried out to test the efficacy of our algorithms. We compare the term space decomposition to topic space produced by our algo-rithms with LSI. We report re sults that show that our al-gorithm, even with adopting simpler schemes for graph par-titioning, produce accurate topic clusters. We test the al-gorithms with various values of parameters, viz. number of hash functions and the number of top weighted words in the collection. For lack of space, we do not report all results.
We tested our algorithms on collection of 300 digital books with a resulting term-document matrix of size 20000  X  300. The resulting number of terms were arrived at by removing the stop words from the term set. The collection of books were picked to cover various topics like health, computer networks, life sciences, music, defense , etc. In addi-tion to our specific collection, we ran the algorithms on stan-dard collections like the open-source DMOZ project [13] to test the effectiveness of our similarity computations.
After generating the top k terms, we compute the term-document matrix where each entry in the matrix is the weight associated with a term, document pair. For the case of Method 1, we use the TFIDF score of each term in a document as the corresponding matrix entry. For Method 2, we normalize this value using a large value for F = 100.
In this set of experiments, we set the top k terms used in the analysis a parameter of our algorithm. We varied k between 100 and 5000. We report results for k = 300 and k = 1000 in two sets of experiments we report. All experiments were run using Method 2 described in Section 3.
The first set of experiments illustrate the hierarchical de-composition of the topic space given a term-document ma-trix. We ran the experiments on 300 documents represent-ing sports, life sciences, entertainment, economics , and communication networks . Some documents in the col-lection included topics that belonged to more than one sub-ject area. Table 1 shows the hierarchical decomposition of the topic space into the top 3 topics. For example, Topic 1 groups terms representing religion, Eastern Europe, economics, and sports . Sub-topics produced under topic 1are Eastern Europe , professional sports , Christian theology , Polish politics . One can intuit the relation-ship between religion and Eastern Europe when there is a talk of religious freedom and spread of religion in the erst-while Soviet Union, Eastern Europe and economics of the Euro region, sports and Eastern Europe vis-a-vis the pro-fessional sports such as NHL in the United States, sports and economics under the subject of salary caps and such. Our algorithms effectively capture these relationships when run with small values of  X  .Thevaluesof  X  were increased to decompose a given topic into sub-topics. The values chosen the sub-topics are orthogonal to each other, very much like the term space decomposition produced by other principal component analyses. Our algorithms produced specific and orthogonal decompostions of other components as well.
Effectiveness of our similarity computations -We measured the effectiveness of locality-sensitive hashing to extract similar terms. We use a  X  X lanted cluster X  model, similar to the one described in [20], to test the effective-ness of our algorithms. In this model, we assign documents to pre-defined clusters and the accuracy of the generated topic space is measured by how much of the defined docu-ment clustering is recovered using the term sets in the topic space. We generate the document set, { d i ,d 2 ,...,d k the similarity computation from the DMOZ tree [13] using random sampling such that the document set is composed of distinct clusters. The correlation between topic T k and the document clusters is computed by considering all term pairs in a topic, where d ( ., . ) is a normalized distance function on the clus-ters. In the case of DMOZ hierarchy, it is the normalized path length between the roots of the clusters in the tree; C ( t ) is a subset of the document clusters to which a term t belongs. One of the advantages of using such a correlation function is that it handles mis-classifications more grace-fully. A term pair that is grouped in the same topic and yet has its terms belong to clusters far apart should have a very low correlation with the clusters. This value of correlation increases as the distance between the respective clusters be-comes smaller and finally becomes 1 . 0whenboththeterms inatermpairbelongtoasamecluster.

We set the number of clusters to 10. The size of the document set was set to 1000. Table 2 shows the topic clusters from the document set for  X  =0 . 4. This value of the threshold produced 10 topic sets in the term space. A higher value of  X  =0 . 5 decomposed topic set 7 to produce content management and day commemorative ,and century british among other topics. The last column in Table 2 shows the correlation between the topics and the planted clusters in the collection. Clearly, the topics correlate very well with the planted document clusters with  X &gt; 0 . 75 for 80% of the topics.
In the second set of experiments, we compare the topic space obtained by our algorithms with term-space decom-position obtained by LSI. We use MATLAB to carry out the term-document matrix decomposition for LSI. The trun-cated term-term matrix can be generated from the truncated term-document matrix using US 2 U T ,where U and S are obtained by the singlular value decomposition of the term-document matrix A = USV T . To extract topics, we analyze the left-singular vectors, i.e., the columns of U .Someofthe earlier work [5, 19] has proposed the relevance of the columns of U in understanding information retrieval concepts such as feature/topic selection and term similarities. A truncated SVD of the matrix A with k set to 10 produced 10 singular vectors corresponding to the top 10 eigenvalues of A .Along each dimension, we represent each topic by  X  X rouping X  the top weighted terms. We set the threshold for term selection to 0 . 1. Table 3 shows the term clustering produced by this method .

Clearly, one significant difference between the two clus-terings is the presence of the same term (e.g., banknotes ) with large weight in multiple clusters produced by the LSI based approach. Whereas, our approach was able to iden-tify strongly correlated terms like banknote and currency ,
Topic Terms 10 animer software drug falukillar Table 3: Clustering topics in DMOZ tree using LSI
Cluster Keywords Table 4: Keywords for the topics in the DMOZ tree credit and card ,and commemorative and day , LSI did not extract these relationships effectively. The clustering pro-duced by LSI is more soft or continuous in nature as opposed to the discrete nature of the term clustering produced by our algorithms.
It is often useful to have a succinct representation of a topic in the truncated term space. Keywords are one way to represent the information contained in a topic. Given a strongly connected component that represents a topic, the keywords can be associated with the  X  X entral X  nodes in each SCC. These nodes are characterized by high out-degree. There are many ways one can compute the  X  X enters X  of such components. In our case where an edge between terms A and B implies A  X  B , it suffices to compute a vertex cover of the component. However, to ensure that the number of key-words is reasonably small compared to the number of terms in a topic, one simple approach would be to consider the top k terms in a component ranked by their weighted out degrees. Table 4 shows the keywords for each topic shown in Table 2. The analysis of a more formal approach to com-puting keywords in each topic is beyond the scope of this study. We point out that one could adopt approaches in-volving fixed-point solutions like the Pagerank and HITS algorithms [21, 16] to compute keywords in each topic.
We proposed algorithms based on LSH to efficiently com-
Topic Terms Cluster Corr,  X  pute hierarchical relationships between terms in a given col-lection of documents. The algorithms use an asymmetric similarity measure between terms in a collection to compute a decomposition of the term space into a lower dimensional topic space from a term-document matrix. We provide both theoretical and experimental evidence to show the effective-ness and time efficiency of our algorithms.

There are interesting open questions that arise from this work. One open question is whether LSH can be shown to compute an effective low-rank approximation of the orig-inal term-document matrix. Another interesting problem would be to study effective graph partitioning algorithms to decompose a strongly connected component to generate sub-topics for the topic represented by the SCC.
