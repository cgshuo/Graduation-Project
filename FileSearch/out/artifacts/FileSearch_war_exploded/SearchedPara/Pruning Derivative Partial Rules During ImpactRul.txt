 Exploratory rule discovery seeks to retrieve all implicit patterns and regularities that satisfy some user-defined set of constraints in a population, with respect to a set of available sample data. The best known such approach is association rule discovery [1]. Most approaches seeks rules A  X  C for which there is a correlation between the antecedent A and the consequent C . However, whenever one such rule is found, there is a risk that many derivative and potentially uninteresting rules A  X  C will also be found. These derivative rules are those for which there is a correlation between the antecedent and the consequent only by virtue of there being a correlation between A and C . For example, if A and C are correlated then for any term B that is unrelated to either A or C , AB will also turn out to be correlated with C .
 such derivative rules. The closed itemset techniques [12, 3, 16] can identify rules for which some elements can be removed without changing the support of the rule. Minimum improvement techniques [6] can to identify rules for which some elements can be removed without decreasing rule confidence. However, since ex-ploratory rule discovery seeks to discover rules characterizing the features in a population, with respect to a given sample, rules may happen to be interesting simply due to sampling fluctuation. Statistical tests are also applied to assess whether there is evidence that no elements can be removed without significantly altering the status of the rule with respect to the population from which the sample data is drawn [11, 5, 9]. However, all these techniques relate only to iden-tifying rules that are derivative due to the addition of irrelevant or unproductive elements.
 in many rules that are likely to be of little interest to the user. For any rule AB  X  C which is not derivative from another rule and for which there is a correlation between the antecedent and the consequent, both A and B may each be correlated with C solely due to correlation between AB and C . In this case A  X  C and B  X  C will both be potentially uninteresting derivative rules that may be discovered by an exploratory rule discovery system.
 teresting rule may be generated.
 Example 1. Suppose a retailer is trying to identify the groups of customers who is likely to buy some new products. After applying the impact rule discovery with the rule filters proposed by Huang and Webb [9, 10], two rules are identified as solutions: filter proposed by Huang and Webb [9], the first rule, which is an ancestor of the second one is misleading. Actually, no profit is produced by customers who belong to district A and are older than 50 years! The retailer X  X  attention should more sensibly focus on the group of customers who are under age 50 in district A, instead of on all those in district A. Keeping the first rule in the resulting solutions may confuse the decision makers.
 for which the consequent is an undiscretized quantitative variable, referred to as the target and is described using its distribution. This paper investigates the identification of the second type of derivative rules in the context of impact rule discovery [9, 14].
 rule discovery related concepts is presented in section 2. The definitions and no-tations of impact rule discovery is charaterized in section 3. Derivative impact rules are defined and relationship between different rules are clarified in section 4, together with the implementation of the derivative rule filter in section 3. Ex-perimental results are evaluated in section 5, which is followed by our conclusions in section 6.
 Many machine learning systems discover a single model from the available data that is expected to maximize some objective function of interestingness on un-known future data. Predictions or classifications are done on the basis of this single model [15]. However, alternative models may exist that perform equally well. Thus, it is not always sensible to choose only one of the X  X est X  models. Moreover the criteria for deciding whether a model is best or not also varies with the context of application. Exploratory rule discovery techniques overcome this problem by searching for multiple models which satisfy certain user-defined set of constraints and present all these models to the users to provide them with alternative choices. Greater flexibility is achieved in this way.
 discovery which seeks rules with qualitative attributes only and distributional-consequent rule discovery which seeks rules with undiscretized quantitative vari-ables as consequent. Propositional rules are composed of Boolean conditions only. While the status or performance of the undiscretized quantitative attributes in distributional-consequent rules are described with their distributions. Associa-tion rule discovery [1], contrast sets discovery [5] and correlation rule discovery [8] are examples of propositional exploratory rule discovery, while impact rule [14] or quantitative association rule discovery [2], as is variously known, be-longs to the class of distributional-consequent rule discovery. It is argued that distributional-consequent rules are able to provide better descriptions of the in-terrelationship between quantitative variables and qualitative attributes. distributional-consequent rule discovery, there are inherent differences between the techniques for propositional and distributional-consequent rule pruning and optimizations. Researchers have devoted extensive efforts to develop rule prun-ing and optimization techniques. Reviews of such work can be found in many related works [9].
 1. For propositional rule discovery, a record is an element to which we apply 2. Rule r 1 is a parent of r 2 if the body of r 1 is a subset of the body of r 2 .If 3. We use the notion coverset ( A ), where A is a conjunction of conditions, to We construct our impact rule discovery algorithm on the basis of OPUS [13] search algorithm, which enables successful discovery of the top k impact rules that satisfy a certain set of user-specified constraints.
 used in this paper as follows: 1. An impact rule takes the form of A  X  target , where the target is describe by 2. Impact is an interestingness measure suggested by Webb [14] 1 : impact ( A  X  3. An k-optimal impact rule discovery task is a 6-tuple: table 1. In this table, current is the set of conditions, whose supersets (children) are currently being explored. Available is the set of conditions that may be added to current . By adding the conditions in available to current one by one, the antecedent of the current rule : New  X  target , is produced. Rule list is an ordered list of the top-k interesting rules we have encountered by now. this search space is connected with a potential impact rule, whose antecedent is composed of the conditions between the braces. By performing a depth-first search through such a search space, the algorithm is guarantee to access every nodes and generate all potential impact rules. Based on the OPUS structure, powerful search space pruning is facilitated [13], making it suitable for discov-ering impact rules in vary large, dense databases. The completeness of OPUS based algorithms is proved by Webb [13]. Techniques for automatically discarding potentially uninteresting rules are exten-sively explored, examples are the constraint-based techniques, the non-redundant techniques and the techniques regarding the rule improvement and statistically significance. The first classes of techniques seek to identify whether a rule r fails to satisfy the constraints in M . The second class of techniques assess whether the resulting rules are redundant or not by reference to the sample data. Example of non-redundant rule discovery techniques are the closed set related techniques and the trivial rule filter. Each assessment of whether r is desirable is not al-ways free from the risk that the rule is not correct with respect to D due to the sampling fluctuation [15]. rules. Statistical tests have been utilized for discarding potentially uninteresting rules generated due to sampling fluctuation, both in context of propositional and distributional-consequent rule discovery. For propositional rule discovery, Brin et al. [8] proposed a pruning technique for removing insignificant correlation rules using a chi-square test; Liu et al. [11] also made use of the chi-square test to identify the significance of association rules with fixed consequents. Bay and Pazzani [5] applied a significance test to remove the insignificant contrast sets in STUCCO. Webb [15] sought to control the number of potentially uninteresting association rules which happen to be interesting due to the sampling by apply-ing a Fisher exact test. For distributional consequent rule discovery, Aumann and Lindell [2] applied a standard z test to quantitative association rule discov-ery and Huang and Webb [9] also developed an insignificance filter in impact rule discovery whose efficiency is considerably improved by introducing several efficiency improving techniques for rule discovery in very large, dense databases. subset of derivative rules. 4.1 Relationship Among Rules As is argued in the introduction, there are derivative rules other than the deriva-tive extended rules that the existing techniques cannot successfully remove. Even after both rules, A  X  target and A &amp; B  X  target , have been identified as non-derivative extended rules, there is still a risk that either or both of them are potentially uninteresting. For example, if the target mean of coverset ( A &amp;  X  B )is not significantly higher than the target mean of coverset (  X  A ), it can be asserted that the notably high target mean for coverset ( A ) derives solely from that of coverset ( A &amp; B ), which is only a subset of coverset ( A ). Such rules are defined as derivative partial rules , which are insignificant compared to fundamental rules which are their children.
 figure, fundamental rules can also be regarded as non-derivative rules. Derivative extended rules are those referred to as insignificant rules in previous research. Unproductive rules are those exhibit no improvement in target mean comparing with their parent rules. Trivial rules are rules whose antecedents cover exactly the same records as one of their parent rules. As was proved by Huang and Webb [9], trivial rules are a subset of unproductive rules. while those that are productive, with respect to the sample, but fail the significance test are all classified as statistically unproductive. 4.2 Derivative Partial Rules and Implementation We first define derivative partial impact rules: Definition 1. A non-derivative extended impact rule, A  X  target is an deriva-tive partial rule, iff there exists a condition x , not included in A , where the target mean for coverset ( A )  X  coverset ( A &amp; x ) is not higher than the target mean for coverset (  X  A ) at a user specified level of significance.
 Statistical Test. Since by performing the exploratory rule discovery, we are aiming at discovering rules that characterize the features of the population with reference to sample data, hypothesis tests must be done to identify whether an impact rule is derivative or not. A t test is applied to assess whether a partial rule is derivative with regard to its children.
 Implementation. The new algorithm with derivative partial rule filter is pro-vided in table 2. In this algorithm all the parent rules of the current rule are stored in the parent rule list while checking whether current rule is a derivative extended rule or not. After current rule is identified as perspectively fundamen-tal the derivative partial rule filter is then applied to check whether the parents are derivative with regard to current rule . Derivative parent rules are deleted from the rule list . Since all the parent rules of current rule has already been ex-plored before current rule (please refer to the search space of OPUS IR), every derivative rule is guaranteed to be removed. We study the effectiveness of the algorithm in table 2 for the derivative partial rule filter by applying it to 10 large databases chosen from KDD archives [4] and UCI machine learning repository [7], in which many attributes are quantitative. Great differences exist among these databases with the smallest database in size having less than 300 records and the greatest having 2000 times as many records as that of the smallest. Number of attributes vary from only 9 to almost 90. Since complex interrelationships exist among the data, there is a strong necessity for rule pruning. We choose a target attribute from among the quantitative attributes in each database, and discretize the rest using a 3-bin equal-frequency discretization. After discretization the numbers of available conditions turn out to be over 1500 for some of the databases. The significance level for the derivative rule filters is 0.05.
 ble 1 is run using the insignificance filter proposed by Huang and Webb [9] to find the top 1000 significance rules from each database, with maximum number of conditions on rule antecedent set to 3, 4 and 5. Then, the algorithm with derivative partial rule filter in table 2 is executed to remove derivative partial rules from the resulting solutions. Results are organized in table 4. The numbers of fundamental rules found after both filters are applied are those before the slashes. Integers after the slashes are those found using the insignificance filter only. Decreases in resulting rules are also presented in percentage.
 variance :0 . 049729 ,min :0 . 0065 ,max :1 . 351 ,sum : 661 . 542 , impact : 112 . 428) 1.0295 cannot have a very high shucked weight. The first rule is thus misleading! When the number of maximum conditions on rule antecedent increases, gen-erally, more derivative partial rules are produced by the impact rule discovery system. The greatest change for the numbers of resulting rules after the deriva-tive partial rule filter is applied is as much as 34%. Even the database with a slightest change saw a decrease of over 4%. This justify the argument that there are considerable amount of derivative partial rules still exist in the resulting rules even after the derivative extended rule filter (insignificance filter) is ap-plied. The derivative partial rules can be pruned using our proposed algorithm in reasonable period of time. Exploratory rule discovery searches for multiple models within a set of given data to represent the underlying patterns or regularities. However, it often re-sults in large numbers of rules. Sometimes, the resulting rules are too numerous for human to analysis. Research has investigated techniques for automatically discarding potentially uninteresting rules, thus reducing the number of rules and removing those that are unlikely to be of fundamental interest. One class of these techniques is to apply statistical tests to the resulting models, so as to alleviate the risk of accepting rules which appear to be interesting by reference to the given data which is only a sample, instead of the real world population. In this paper, we argued that there is a type of potentially uninteresting rules which ex-isting techniques fail to remove. We call these rules derivative rules. A derivative rule filter is developed in a impact rule discovery system. Experiments showed a considerable decrease in the number of resulting rules.

