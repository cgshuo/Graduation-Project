 1. Introduction
Recently, supervised machine learning methods have been applied to ranking function construction in document retrieval learning methods X .)
One question arises here: can we leverage the merits of the two approaches and develop a method that combines the uses vised learning. To the best of our knowledge, there has been no previous work focusing on this problem. amount of labeled data and by leveraging a traditional IR model.
 learning (e.g. BM25) methods for ranking model construction.
 to achieve our goal. In training, given a certain number of queries and the associated labeled documents, SSR With the labeled data, a new supervised learning model can be constructed. SSR terion is met. In this paper, we propose a stopping criterion on the basis of machine learning theory.
Experimental results on three benchmark datasets and one web search dataset show that the proposed method can sig-vised method).
 some clear differences between SSR ANK and relevance feedback (or pseudo-relevance feedback), however, as will be ex-plained in Section 2.
 work. 2. Related work 2.1. Learning for document retrieval
In Information Retrieval, traditionally ranking models are constructed in an unsupervised fashion, for example, BM25 query and document. There is no need of data labeling, which is no doubt an advantage. Many experimental results show that these models are very effective and they represent state-of-the-art methods for document retrieval.
In machine learning, the problem of  X  X earning to rank X  became a popular research topic recently and many methods have and a ranking model is created using the labeled data. For example, Herbrich, Graepel, and Obermayer (2000) proposed proposed a similar approach to learning to rank, but using the framework of boosting.
Learning to rank (supervised learning) can also be applied to document retrieval, as document retrieval is in nature a ranking problem. Recently, there have been many investigations in the IR community along this direction. For example, criminant model with features generated by a language model, and made use of the model in document retrieval. Burges
IR method. 2.2. Semi-supervised learning
Many semi-supervised learning methods have been proposed. Typical methods include those using the EM algorithm 2005a, 2007 ).
 for document retrieval tasks.
 (Huang et al., 2006 ).

Note that existing semi-supervised learning methods may not be directly applicable to learning of ranking functions in newly labeled training set. Duh and Kirchhoff (2008) exploited unlabeled data in a transductinve settings, where KPCA ing to rank machinery and traditional document retrieval model to address the semi-supervised document retrieval problem. 2.3. Relevance feedback retrieval system returns a number of documents and asks the user to make judgments on the relevance of the documents feedback takes the top k retrieved documents as  X  X  X elevant documents X .
 There are similarities between the settings of relevance feedback (or pseudo-relevance feedback) and that of SSR this paper. Specifically, both approaches attempt to leverage a certain number of relevance judgements to improve the the query , while SSR ANK makes use of the labeled documents to refine the ranking model. Secondly, relevance feedback documents are labeled, but the remaining are not). Thirdly, while relevance feedback aims to improve the retrieval re-relevance feedback).
 3. The proposed method: SSR ANK 3.1. General framework simplicity, we also refer a feature vector as a document (associated with a certain query).
Let x denote an instance (feature vector), x 2 X : X # R d y 2 Y : Y  X f r 1 ; r 2 ; ... ; r M g . There exists a total order between the labels in Y : r r respect to a query are sorted according to f such that x i formed by employing supervised learning methods such as Ranking SVM and RankNet.
In this paper we considerthe case inwhich foreach query inthe training data only a small numberof documents(instances) associatedwithit arelabeled andtheremainingdocuments(instances)are unlabeled. Notethat thisis commonlytrueinIR.Let
We propose a semi-supervised learning method to accomplish the learning task. For any unlabeled instance x with these newly labeled instances, we train a more accurate ranking model.
 labeling the unlabeled data, following the idea of co-training. Specifically, there are V base ranking functions f for each view v ; x u is assigned a score representing the likelihood of its being in rank r ranking function f v . We can then calculate the final score of x and choose the rank that has the highest score as the rank of x strategies for the combination can be considered. First, we can employ linear combination where w  X  v  X  is weight of view v and P v w  X  v  X  X  1. Here, we can define w  X  we can employ majority voting where d  X  B  X  takes 1 as value if B is true and 0 otherwise.
 for multi-class classification. 3.2. Score calculation We propose a way of calculating scores of unlabeled data for each view in the above semi-supervised learning method.
Using one of the base ranking functions f v , we can rank the instances (corresponding to documents) associated with a query. Note that some of the instances are labeled while the others are unlabeled. If f x has a higher rank than x j , i.e. y i y j .
 given by the base ranking function.

First, we define the probability of x i being ranked no lower than x following the proposal in Burges et al. (2005) . We next define the probability of instance x where r q denotes the labeled instances with respect to q and l calculate S v  X  y u  X  r m j x u  X  , the score of possible rank r Euclidean distance is used as the metric.
 ploy a method like k NN to make predictions on the ranks of unlabeled instances from all the labeled instances. We note that alternative ways for labeling unlabeled data may exist. For instance, one can make use of
P  X  r ing to our experiment. 3.3. Theoretical analysis of the learning. The following proposition provides such a condition.

Proposition 1. Let m 0 denote the number of labeled instance pairs in the training data and m instance pairs in the first iteration of semi-supervised learning. Let e pairs in the first iteration. If the following inequality holds in the first iteration of the semi-supervised learning.
 iteration and the  X  t 1  X  th iteration. If the following inequalities hold: age precision in the t th iteration ( t &gt; 1) of the semi-supervised learning. It is not difficult to verify that the proposition holds.
 Proof. We use two theoretical results obtained in previous work.

First, let us consider using a learning to rank method, for example RankNet (Burges et al., 2005 ) and Ranking SVM inversions (errors).
 of h .If m , g and satisfy the following condition: u  X  c = 2 , the equation can be re-formulated as the following utility function:
Let h 0 denoted the hypothesis learned from the labeled instance pairs and h ation. To make h 1 have smaller classification error rate than h where
Assume that there exists no noise in the original training set, and thus g follows that when Eq. (5) is satisfied, h 1 makes fewer pair inversions than h has higher average precision lower bound than f 0 .

It is also easy to verify that Eq. (6) holds for the  X  t 1  X  th and t th iterations in a similar way. h 3.4. Algorithm Now we can build the semi-supervised learning algorithm SSR pseudo-code of the algorithm. We can see that significant differences exist between SSR do-relevance feedback).

In this paper, we only consider the uses of two views (i.e. V  X  2). One ranking function is based on machine learning 3.3 to derive the stopping criterion. The algorithm iterates until the stopping criterion is met. tion learning method. For example, for RankNet, v  X  cWN 2 ranking function generated by the machine learning method, and hence is roughly O  X  ment of the performance.
 4. Experiments 4.1. Benchmark datasets We used three benchmark datasets on document retrieval in our experiments.
 discarded.
  X  X artially Relevant X , or  X  X rrelevant X  (to the queries).

IR (e.g. Cao et al., 2006; Nallapati, 2004 ). 4.2. Evaluation measures the performance of the ranking methods. Given a query q i function is defined as where r j is the rank of the j th document, and the normalization constant n positions of 1, 3, 5 and 10 are reported.
 query q i , Average Precision is defined as ted as relevance and the others irrelevant in calculation of MAP.
 4.3. Experiment 1: comparison with baselines documents were used, and in the other group the labels were withheld and the documents were viewed as unlabeled. The instances were selected into the labeled dataset, for each query, documents with BM25 scores lower than 0.01 were dis-carded and were not used in the experiment. The number of relevant instances was roughly one tenth in the experiments. We then applied SSR ANK to all the datasets. In our experiments, for Learning View of SSR k was fixed at 10 (cf. Section 3). For combination of the two views RankNet and BM25 at SSR linear combination (c.f. Eq. (1)) and agreement (a special case of Eq. (2) when V  X  2), denoted as SSR
Agr, respectively. For comparison, we also tested SSR ANK SSR ANK -RN, and the other one is referred to as SSR ANK -BM.

Here, we only experiment with two implementations of RankNet as baseline methods. The first one, RankNet-L, uses only real-world tasks. However, it is good to include it in the comparison since it might be the upper performance of SSR BM25 for the best performance. The detailed information of the compared algorithms is tabulated in Table 3 . For each dataset and each labeled rate, the proposed methods and baseline methods were evaluated in terms of NDCG and sets and by labeling rates.
 Fig. 2 shows the average performances of the methods on different datasets. We can see from the figure that SSR ment on OHSUMED is small. We can also see that SSR ANK -Lin and SSR Fig. 3 shows the average performances of the methods in labeling rates. We can see that SSR better than the baseline methods under all the four labeling rates. We can also see that SSR than SSR ANK with only one view.
 Statistical significance testing ( t -test) at significant level 0.05 shows that SSR and they outperform BM25 in 9 and 7 settings, respectively. SSR respectively. The highest numbers are highlighted in boldface. We can see that SSR baseline methods of RankNet-L and BM25 consistently . Furthermore, SSR RN and SSR ANK -BM. Additionally, SSR ANK -Lin performs slightly better than SSR We can conclude, therefore, that SSR ANK can perform better than the baseline methods, and SSR perform better than SSR ANK with one view.
 4.4. Experiment 2: learning curve
To investigate how the performance of SSR ANK improves as the labeling rate increases (10%, 20%, 30% and 40%), we con-ducted an additional experiment. Figs. 4 X 6 give the learning curves of SSR Anyway, in general, SSR ANK -Lin and SSR ANK -Agr perform better than SSR ing rate is low. 4.5. Experiment 3: stopping criterion in which we had a fixed number of iterations T in data labeling, T  X  10. (Recall that in SSR the stopping criterion is satisfied). We refer to the corresponding methods as SSR
Experimental results show that SSR ANK -Lin and SSR ANK -Agr usually perform better than SSR all the 12 settings (3 datasets by 4 labeling rates). For example, in terms of MAP, SSR value that using our proposed criterion. It is obvious from the figure that both SSR than SSR ANK -Lin and SSR ANK -Agr, respectively, which suggests the proposed stopping criterion is effective.
Furthermore, since the unlabeled data actually had labels (they were only withheld in the experiments), accuracies on SSR ANK . For example, Fig. 8 shows the accuracies of SSR WSJ under labeling rate of 10% (i.e. starting from 10% of data labeled). We can see from the figure that SSR the unlabeled instances. Thus, using only the semi-supervised process could hardly reach the maximum performance that labeled, semi-supervised learning is not needed.
 derived from number of inverse instance pairs and is more closely related to MAP (Joachims, 2002 ). 4.6. Discussions
The experimental results show that SSR ANK outperforms RankNet-L (using the same amount of labeled data). It indicates learning method. This is because the only extra information used by SSR RankNet-L.
 In addition, the performance of BM25 can be improved by using SSR The experimental results also show that SSR ANK performs better than SSR of two views are better than the uses of one view. For most of time, SSR not highly correlated, then co-training can work well (Balcan, Blum, &amp; Yang, 2005 ). majority voting when V  X  2). One possible explanation is that the weights used in linear combination can provide more information.
 semi-supervised learning noise will be inevitably involved, the use of the stopping criterion seems to be better. 4.7. Experiment 4: application to Web search
We also applied the proposed method SSR ANK to a real search system, in which the amount of labeled data was exactly labeled to represent the degree of relevance, while others were left unlabeled. SSR ANK -Lin and SSR ANK -Agr, as well as SSR ANK -RN and SSR ated, because not all the training data were actually labeled as in the other experiments. methods outperform the two baseline methods. Furthermore, SSR and SSR ANK -RN. For example, with the use of SSR ANK -Lin NDCG@1, NDCG@3, NDCG@5, NDCG@10 and MAP are improved by 19.6%, 16.9%, 17.5%, 11.1% and 26.0%, respectively, when compared with RankNet-L. 5. Conclusion and future work
This paper addresses the issue of ranking model construction in document retrieval, particularly when there are only a small amount of labeled data available. The paper proposes a semi-supervised learning method SSR drawn from the experimental results. First, SSR ANK can work better than the baseline methods of using BM25 or using a supervised learning model with only labeled data. It demonstrates that SSR view. This agrees with the findings in semi-supervised learning studies. Third, the stopping criterion used in SSR effective to control the quality of data labeling.
 cases in which some training queries have labeled documents while the others do not. How to extend our method to the future.
 Acknowledgement
We want to thank the anonymous reviewers for their helpful comments and suggestions. Part of the research was sup-ported by the National Science Foundation of China (60505013, 60635030, 60721002), the Jiangsu Science Foundation fessorship Award and the Microsoft Research Asia Internet Services Program.
 References
