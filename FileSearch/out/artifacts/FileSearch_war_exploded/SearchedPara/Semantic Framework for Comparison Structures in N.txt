 Representing the meaning of text has long been a focus in linguistics and deriving computational models of meaning has been pursued by various semantic tasks such as semantic parsing. Deep semantic parsing (as opposed to shallow seman-tic parsing, such as semantic role labeling) aims to map a sentence in natural language into its cor-responding formal meaning representation (Zelle and Mooney, 1996; Berant and Liang, 2014). There has been a renewed interest in deeper se-mantic representations of natural language (Ba-narescu et al., 2013) in NLP community. Open-domain semantic representations enable inference and reasoning, which is required for many lan-guage understanding tasks such as reading com-prehension tests and open-domain question an-swering. Comparison is a common way for ex-pressing differences in sentiment and other prop-erties towards some entity. Comparison can hap-pen in very simple structures such as  X  X ohn is taller than Sam X , or more complicated construc-tions such as  X  X he table is longer than the sofa is wide X . So far the computational semantics of com-paratives and how they affect the meaning of text has not been studied effectively. That is, the dif-ference between the existing semantic and syntac-tic representation of comparatives is not distinc-tive enough for enabling deeper understanding of a sentence. For instance, the general logical form representation of the sentence  X  X ohn is taller than Susan X  using the Boxer system (Bos, 2008) is the following: named ( x 0 ,john,per )
The above meaning representation does not fully capture the underlying semantics of the ad-jective  X  X all X  and what it means to be  X  X aller X . A hu-man reader can easily infer that actually the height of John is greater than the height of Susan. An-other example to consider is the sentence  X  X ohn is tall X , which basically has the typical logical form tall ( john )  X  X hich is a very superficial represen-tation for the meaning of the predicate  X  X all X . Like-wise, a human reader can infer that defining some-one as  X  X all X  in some domain of discourse entails that this person is somehow  X  X aller X  than some other population (say their average), however, the earlier typical logical form representation does not enable such inferences.

In this paper we introduce a novel framework for semantic representation and computational analysis of the structure of comparison in natural language. This framework enables deeper repre-sentation of semantics of comparatives, including all different types of comparison within compara-tives, superlatives, equatives, excessives, and as-setives , and the way they are related to their cor-responding semantic roles. Together with this pa-per, we provide a dataset of gold-annotated com-parative structures using our meaning representa-tion, which enables training models on compar-ison constructions. We propose a new approach for automatic extraction of comparison structures from a given text. A semantic representation of the comparison expressed by the sentence  X  X he equip-ment is too old to be much of use to us. X  aug-mented under our representation would be the fol-lowing:
Throughout this paper we define a comparison to be any statement comparing two or more enti-ties, expressing some kind of measurement on a scale, or indicating some degree of having a mea-surable property. The details of these variations will be discussed in Section 3. In this section we provide a linguistic background on comparison constructions in language, which provides the basis of our semantic framework (to be presented in Section 3). 2.1 Comparative Structures in Language Measurement in natural language is mainly ex-pressed in sentences having comparative mor-phemes such as more, less, -er, as, too, enough, adverbial, nominal, or verbal, i.e., the main com-ponent of the sentence carrying out the measure-ment can have either of these parts of speech.
Adjectival Comparatives: Canonical exam-ples of comparative sentences contain adjectives, e.g.,  X  X all X  or  X  X retty X . Even within adjectival com-paratives, there is a good deal of structural variety. Consider the following examples: (1) a. Mary is taller than Susan.
The comparative form of the adjective  X  X all X  in sentence 1a is viewed as an expression denoting a greater than ( ) relation between two individuals,  X  X ary X  and  X  X usan X , on the scale of  X  X allness X . The degree-theoretic analysis of such adjectives brings up the notion of Gradable Adjectives : many adjec-tives describe qualities that can be measured ac-cording to degrees on scales , such as the scale of  X  X ize X ,  X  X eauty X ,  X  X ge X , etc. These adjectives can be used with comparative morphemes, indicating less or more of a particular quality on a scale. Gradable adjectives can express specific relations between individuals on a scale, e.g., in sentence 1b Mary is taller than Susan by a measure of 3 inches.
Comparison on the scale does not always in-volve two individuals. For example consider sen-tence 1c which denotes a comparison being made between an individual and a specific point on the scale of  X  X allness X . All the earlier examples are among the simplest types of comparative struc-tures using adjectives. Consider the following ex-ample: (2) Mary is taller than the bed is long.
In sentence 2 we have a case of subcompara-tives , where we compare  X  X ary X  and  X  X ed X  accord-ing to two different dimensions: height and length. Each dimension provides a degree, and the degrees are ultimately related by the greater than ( ) re-lation. Scalability is known to be universal in lan-guage and a wide variety of linguistic phenomena can be explained in terms of degrees and scales (Solt, 2015).

The Semantics of Scales: A fairly common view (Kennedy, 2007) is that a scale S is a triple of the following form: where D is a set of degrees, is an ordering re-lation on D , and DIM is the dimension of mea-
Individuals are linked to degrees by measure functions. A measure function  X  S is the func-tion that maps an individual x to the degree on the scale S that represents x  X  X  measure with respect to the dimension DIM . For example, the  X  HEIGHT measure function is a function that maps individ-uals to their respective heights. Under this model, we represent the comparative structure of the sen-tences 1a-1c as follows:
A generic comparative interpretation of some degree of tallness under HEIGHT scale is as fol-lows: where d is the degree argument which is sup-plied by some form of degree morpheme: a degree modifier (e.g., too, very), a measure phrase (e.g. 1.7 inches), or simply comparative or superlative morphology. Under this model, we can also rep-resent the comparative structure of the sentence argument. A common assumption is that the de-gree role is played by a phonologically null degree morpheme called pos , which denotes a context-dependent threshold or standard of comparison (Kennedy, 2007; Heim, 2007). For instance, in a specific context of adult men in north America being  X  X all X  could be interpreted as being over 6 feet.

Non-canonical Comparatives: Comparative structures can also be verbal, nominal, and adver-bial. Consider the following verbal comparatives: (4) a. The women ate more than men did.
It has been proposed (Wellwood et al., 2012) that measure functions (  X  ) can be applied both to individuals and to events, in the latter case mea-suring either the event or an entity related to the event. The comparative interpretation for the two sentences 4a and 4b is as follows: where cool is a function that takes an event e and an object x (here  X  X ake X ) and returns a degree rep-resenting the amount to which x changes in cool-ness as a result of participating in e . The underly-ing scale of verbal comparatives is sometimes am-biguous, e.g., in sentence 5a it is not clear whether the women ate more in volume or in quantity. Comparative structures can also be nominal. Consider the following sentences: (6) a. More juniors than seniors came to the
The meaning of sentences presented above must be stated with reference to degrees as well (Solt, 2015). Hence, the scale for the comparison sen-tence 6a is the numerical counting by integers and the scale for sentence 6b is something correspond-ing to a mass dimension, here perhaps liquid vol-ume. Adverbial comparatives share many of their characteristics with the adjectival and verbal class, which we do not develop further for brevity. For example the sentence  X  X ary ran faster than Sam X  is an example of adverbial comparison, where the implicit  X  X peed X  attribute of the  X  X unning X  event as-sociated with Mary and Sam is being compared. 2.2 Categories of Comparison There are various ways for making comparisons, each indicating different degrees of difference or similarity. Following are the major categories for degrees of comparison together with example sen-(7) Comparative (8) Superlative (9) Equative (10) Excessive (11) Assetive As discussed earlier, having a deep meaning rep-resentation of comparison structures can help us build computational models of comparison in nat-ural language and perform inferential tasks in var-ious domains. Here we introduce a novel seman-tic framework of comparison. This framework is based on the linguistic interpretations presented in Section 2, but formalized and adapted to suit our semantic computational framework.

We model comparatives as inter-connected predicate-argument structures, where predicates are the main comparison operators (implicit and explicit comparison morphemes), and arguments are connected to the predicates via semantic roles (relations). Our framework includes not only ex-plicit comparisons, but also implicit ones in the form of an evaluation or a measurement on a scale, which will be explained throughout this section. More detailed and complete list of the predicates, semantic roles, and arguments can be found in the supplementary material.

Predicates : Table 1 lists all the predicate oper-ators under our framework. As the table shows, there are four main types of predicates: compara-tives, extremes, bases, and measurements . Most of these types can be associated with operators from any of our parts of speech: Adjective (JJ), Adverb (RB), Noun (NN), and Verb (VB). The predicate operator in each of the examples is ital-icized. The comparatives type also includes the operators &lt; and =&lt; , which are the opposite of the operators &gt; and &gt;= presented in the table. It is important to note that the  X  X ase positive X  predi-cate is actually the implicit pos operator (as men-tioned in Section 2.1; however, for easier repre-sentation we specify it by marking its correspond-ing adjective or adverb. The same thing happens for measurement predicates. Also, our framework captures the subtle difference between the mean-ing of  X  X ary is [ tall ] positive  X  and  X  X ary is 5 feet Mary is tall according to some standard of tallness in a context, while the latter means that Mary X  X  height equals the degree of 5 feet.

Semantic Roles : Each predicate is character-ized by its arguments and each argument is con-nected to the predicates by a relation (semantic role) type. Table 2 shows the possible semantic role types for a predicate. Figure is the core role for a comparison structure, i.e., any comparison should have a role indicating the main entity which is being evaluated/measured/compared on a scale. The simplest form of comparative predicate, e.g.,  X  X ohn is taller than Sam X , involves two main roles: Figure (John) and Ground (Sam). The non-core roles are mainly associated with non-comparative comparisons
Arguments : Last but not least, each role points to an argument, which can have various types, as listed in Table 3. The most frequent argument type is individual, as in  X  X ohn is taller than Sam X . The other notable role is Phrase-value, which repre-sents an interesting comparison phenomena. In the corresponding example in the table, the speed of John X  X  driving is explicitly being compared with some point on the scale of speed to which  X  X e was allowed X . Such ground roles are classified as phrase-value, where a verb phrase signifies a point of comparison on scale, not an individual entity. Figure 1 shows an example of predicate-argument structure under the described semantic framework. Given an input sentence, we want to predict the predicate operators, their semantic roles, and ar-guments. We decompose this problem into three sub-problems:  X  Labeling predicate candidates using a multi- X  For each predicate, considering the set of all
Our overall approach, to be described in this section, is similar to the works on joint inference with global constraints for learning event relations and process structures (Do et al., 2012; Berant et al., 2014).

Predicting Predicates: The first step in com-parison structure prediction is to identify and la-bel the predicates. For this purpose we train a multi-class classifier that labels all one-word con-stituents in the sentence with any of predicate types in Table 1 or None (indicating that the con-stituent is not a predicate). The set of all possible predicate labels is named P .

We used various features for training the predi-cate classifier: we extract the lemma and POS tag of the word, POS tag of children, siblings, par-ent and root of the sentence in the dependency work. tree, POS tag and lemma of two adjacent words, similarity features from WordNet (Miller, 1995), word polarity features, and most importantly  X  X t-tribute concepts X  for words which are adjectives (Bakhshandeh and Allen, 2015). The  X  X ttribute concepts X  are the different properties that an adjec-tive can describe, for instance  X  X eight X  and  X  X hick-ness X  are the attributes of the adjective  X  X angling X . Last but not least, we include the conjunction of all these features.

Predicting Roles and Arguments: Given the predicates, one should label the predicate-argument role and predict the argument type. Here we take an approach used for semantic role label-ing (Punyakanok et al., 2008): given a predicate, we collect all constituents in the sentence to build a set of plausible candidate arguments. As a re-sult, each predicate has a set of candidate argu-ments which should be labeled with their argu-ment types and be assigned with a semantic role edge. Here we jointly train two logistic regres-sion classifiers for predicting semantic role type and argument type of a predicate-argument pair, using argument identification features from (Pun-yakanok et al., 2008) and using the structured av-eraged Perceptron algorithm (Collins, 2002). The role types can be any of the roles from table 2 or None (set R ), and the argument types can be any of the ones from table 3 or None (set G ). At the end of this stage we have two scores: sc p,j,r = log Pr p,j,r where p  X  P is a predicate type, j is a candidate argument, r  X  R is a role type; and sc
Joint Inference: Given a sentence with its ex-p , the goal is the following: find the best assign-ment for the indicators y = { y p,j,r | p  X  P, 1 &lt; j  X  n,r  X  R } and x = { x p,j,g | p  X  P, 1 &lt; j  X  n,g  X  G } . Here n is the number of candidate arguments for the given predicate. We model the problem as an Integer Linear Programming (ILP). We formulate the problem as follows:
The hard constraints 4 b  X  4 c each indicate a re-striction on the structure of the predicate-argument relation and labels: each argument can have only one role and argument type ( 4 b  X  4 c ), each pred-icate can only have one of each role type ( 4 d ), a  X  X one X  role type should be matched with a  X  X one X  argument type ( 4 e ), and each predicate should have exactly one  X  X igure X  role ( 4 f ). There are also some other specific constraints such as the fact that a predicate labeled with  X  X omparative X  cannot have a  X  X omain X  role type and vice verse. 5.1 Dataset Creation In order to make our gold-annotated dataset we used OntoNotes (Pradhan et al., 2007) release 5.0 corpus. OntoNotes covers various genres such as conversations, news-wire, and Weblogs, which provides distinctive variations of compari-son structures in natural language. Furtheremore, we think our annotations can potentially provide augmentations on OntoNotes, so using the origi-nal OntoNotes sentences can be beneficial.

One approach for pinpointing comparison sen-tences is to mine for some known patterns and train a classifier for distinguishing comparison and non-comparison sentences (Jindal and Liu, 2006b). However, as demonstrated earlier, the va-riety of comparison structures is so vast that be-ing limited to some specific patterns or syntactic structures will not serve our purpose. In order to address this issue, we randomly selected 2000 sentences from OntoNotes which contained an ad-jective, an adverb, or any of the comparison mor-phemes. This set contained some non-comparison sentences, such as  X  X ohn admitted to the crime too X .

In order to make the final set of comparison sen-tences we performed the following task: we define a comparative sentence as a sentence that contains at least one predicate operator as defined in Sec-tion 3. Hence, we provided three human experts with a full predicate operator types table and asked each of them to annotate any predicate operator found in the given sentences. Then we retained any sentences with at least one predicate operator which was annotated by at least two of the three judges. We further refined the set to include equal number of predicate types. This resulted in 531 sentences.

After collecting the comparison sentences, we asked the annotators to provide gold-standard an-notation of predicate-argument structure of the sentences. This involves the annotator to read the annotation guideline and basically understand the semantic framework for comparison structures that we introduced in Section 3. Initially, we ran a pilot study on a set of 50 sentences where each sentence was annotated by two of the ex-perts. We used pilot results for iterating over the annotation schema and guideline and resolving is-sues regarding low agreement predicates and ar- X  = 0 . 80 . We split the dataset into 30% and 70% for testing and training respectively. 5.2 Evaluation Here we evaluate the performance of our pro-posed predicate-argument structure prediction. We present the following two methods:  X  ILP Method : Our full approach as described  X  Baseline : A simple pattern-based method
Here with compare their predictions on test set to the gold standard annotations and compute micro-averaged precision, recall and F1 score. For this analysis we remove the  X  X quative X  predicate type, given its very low frequency in our training set. Moreover, here we do not include the positive and negative predicate types, as these take only one role argument which is  X  X igure X , making the prediction task trivial.

Table 4 shows the results of predicate type pre-diction. The final reported average in this table ex-cludes the type  X  X one X . The best performing cate-gory in both methods is  X  X uperlative X , which is be-cause of its more typical structure which makes it easier to be predicted. In general, the precision of predicate prediction is very high in ILP method, which is due to the fact that our predicates are the comparison operators indicated by the com-parison morphemes. The baseline performs con-siderably weaker than ILP method for predicting less and greater predicates. This is because pre-dicting these types requires a more complicated analysis where simple morphological and syntac-tic patterns can result in many false positives.
Table 5 depicts the results of the role type pre-diction. The weighted average in this table is based on frequency, excluding the type  X  X one X . The precision on role prediction varies across different types. Overall, the baseline performs weakly on predicting role types, which is dues to the complicated structure of roles.

The best prediction of ILP method is on scales, which has benefited from the attribute concept fea-ture. The weaker performing types have been affected by the low-frequency occurrence in the training set. There are many cases of very long and complex sentences in our dataset. One major reason behind some of the false predictions is in-correct dependency parse for long sentences. One notable issue here is that for easier prediction and analysis, we had asked our annotators to mark only the head words for phrasal arguments. This had of-ten caused lower agreement among annotators and hence worse predictions on the system trained on the dataset. In future, we are going to switch to span-based argument identification.
 Table 4: The evaluation results on predicate type prediction. The syntax and semantics of comparison in lan-guage have been studied in linguistics for a long time (Bresnan, 1973; Cresswell, 1976; Von Ste-chow, 1984). However, so far, computational modeling of the semantics of comparison com-ponents of natural language has not been devel-Table 5: The evaluation result on role type predic-tion. oped as elaborately as needed. The main efforts on computational aspects of comparatives have been in the context of sentiment analysis. Jindal and Liu (2006b) introduced the first approach for the identification of sentences containing compar-isons. Their system trains a Naive Bayes classi-fier for labeling sentences as comparative or non-comparative.

Later works progressed into identifying the components of the comparisons: comparative predicates and arguments. For example for the sentence  X  X anon X  X  optics is better than those of Sony and Nikon. X , the extracted relation should be: (better, {optics}, {Canon}, {Sony, Nikon}). Jindal and Liu (2006a) detect such arguments by labeling sequential rules. Xu et al. (2011) use Conditional Random Fields (Lafferty et al., 2001) to extract relations between two entities, an attribute and a predicate phrase. These works all provide a rudi-mentary basis for computational analysis of com-paratives, however, they lack depth and breadth as they are limited to the limited comparison struc-ture (Entity1, Entity2, aspect) expressed within some sequential patterns. It is evident that the framework of comparison proposed in this paper goes beyond simple triplet annotation of compari-son structures and is more representative of the lin-guistics literature on comparatives and measure-ments.

The most recent related work on comparatives (Kessler, 2014) focuses on argument identification task: given a comparative predicate, they find the arguments corresponding to it. They train a classi-fier for this task emphasizing on syntax informa-tion. Most of the entities in their training data are products (cameras, cars, and phones). An-other recent work (Kessler and Kuhn, 2014) con-centrates on the annotation of what they call multi-word predicates (such as  X  X ore powerful X , where the comparison is not one-word such as  X  X almer X ). They show that annotating the modifier of com-paratives (i.e., the adjectives) gives better results in classification. Both these works share the major shortcoming of the earlier works, as they are very limited to their specific patterns and fail to enable deeper representation and analysis of various com-plex comparative structures. Systems that can understand and reason over com-paratives are crucial for various NLP applications ranging from open-domain question answering to product review analysis. Understanding compar-atives requires a semantic framework which can represent their underlying meaning. In this pa-per we presented a novel semantic framework for representing the meaning of various comparison constructions in natural language. We mainly modeled comparisons as predicate-argument pairs which are connected via semantic roles. Our framework supports all possible parts of speech and variety of measurements and comparisons, hence providing a unique computational represen-tation of the underlying semantics of comparison. Furthermore, we introduced an ILP-based method for predicting the predicate-argument structure of comparison sentences.

With this paper, we provide a novel dataset of gold-standard annotations based on our seman-tic framework. We are planning on expanding our gold-standard annotations under this frame-work for having more training data. Our semantic framework on comparison constructions enables us to do logical reasoning and inference over com-paratives. In the future, we are planning to de-sign a reading comprehension task where we use this framework for answering comparison ques-tions from a paragraph containing various inter-related comparisons.

Last but not least, the works on broad-coverage semantic parsing (Allen et al., 2008; Bos, 2008) can all benefit from our semantic framework. We will be extending the TRIPS logical form (Allen et al., 2008) according to this framework and will modify the grammar to generate the deeper repre-sentations.
 We would like to thank Alexis Welwood for her invaluable comments and guidelines on this work. Moreover, we thank Ritwik Bose for his help on annotations. This work was funded by the Office of Naval Research (grant N000141110417) and the DARPA Big Mechanism program under ARO contract W911NF-14-1-0391.

