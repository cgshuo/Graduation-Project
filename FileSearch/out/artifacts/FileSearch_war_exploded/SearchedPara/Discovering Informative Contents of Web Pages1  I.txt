 The World Wide Web has long become a huge container of information, which includes news and reports about politics, economics, culture, entertainment and others. Recently, developments of Web 2.0 have further greatly increased the magnitude of information. In the face of the large amount of information, many websites, especially commercial websi tes, provide web pages with much tem-plate content for many purposes, such as banner advertisements, navigation bars, copyright notices, decoration, etc. Gibson et al. [7] have found that 40-50% of the content on the Web is template content and the volume is still growing steadily. Although templates are helpful for users to browse the Web pages, they are harmful to many web mining and searching systems. They can decrease the precision of search, increase the size of index and impair the performance of applications that manipulate web pages. Thus it is very important to identify templates correctly and efficiently.

In this work, we research on discovering informative contents of Web pages based on the following observation: In a given Web site, templates usually share some common presentation styles. Moreov er, the contents of templates tend to be similar or almost identical.

Many previous extraction methods we found in literature extract informative contents of Web pages based on single Web page analysis. These  X  X age-level X  methods have some drawbacks. Generally ,  X  X age-level X  methods have lower ac-curacy than  X  X ite-level X  methods. Some heuristics page-level methods rely on manual rules and extract template effect ively only in specified sites. Our method is based on a site-oriented structure which is the alignment of DOM trees of pages within a site. Template detection methods proposed in [13] [16] are based on RTDM (Restricted Top-Down Mapping). While these methods are efficient, they are of limited use because of the fo llowing two reasons: First, in some Web sites, especially article-type sites, ma ny informative contents have almost iden-tical structures, and they tend to be detected as templates in these methods. Second, since these methods take no text contents of Web pages into considera-tion, they cannot utilize texts to distinguish informative contents from template ones. Yi et al. [19] and Li et al. [18] pr oposed a template d etection method based on SST (Site Style Tree) by aligning the DOM (Document Object Model) trees to extract structural information of Web sites. The method leads to a high precision. However, it is unable to d etect the informative contents in the multiple-template Web sites well, b ecause the document collection of one SST node may be the combination of templates and informative contents. Moreover, although a node in the SST is meaningful as a unit, it may still contain some noisy items.

We observe that generally templates appear in the form of segments. For example, segments for navigation, copyright and advertisements appear as tem-plates, and segments for main article appear as informative contents. In this paper, we use segment as the element node of the DOM tree. In addition, our approach set sentence as the granularity of informative content units, so it can effectively discover informative items within one content block.

The rest of this paper is organized as follows: In section 2, we give an overview of the problem of informative content extraction and review related work. Then we illustrate the representation of pages and Web sites in Section 3. In section 4, we describe the approach for informative contents extraction. Experiments are discussed in Section 5. Finally, we draw a conclusion of this study and outline directions in Section 6. The problem of detecting tem plates and extracting inf ormative contents of Web pages has been addressed in many previous research papers. Most of the proposed methods are based on machine learning or heuristics.

The problem was first discussed by Bar-Yossef and Rajagopalan in [1], propos-ing two template detection algorithms based on DOM tree segmentation and seg-ment selection. While their proposal was considered as a segmentation method, the goal was to detect noisy information. Their heuristics used the cues pro-vided by hyperlinks. If an element contains more than K links, it is considered as a segment, otherwise it is counted a s part of a segment containing it. Then template segments were selected by one of the above two template detection algorithms. A similar method was proposed by Ma et al. in [12], which segments pages by table text chunk . All table text chunks are identified as template table text chunks if their document frequency is over a determined threshold.
Wang et al. [17] proposed DSE (Data-rich Subtree Extraction) algorithm to recognize and extract the informative contents of Web pages by matching sim-plified DOM trees. Lin et al. [11] proposed a method to discover informative contents based on the heuristics that redundant blocks, opposite to informative blocks, appear more frequently. Reis et al. [13] presented a method based on the RTDM algorithm through generating patterns to identify templates. A similar solution was presented by Vieira et al. i n [16], which detects templates through the operation of each step on the Tree Edit Distance of page structures.
Some other methods using the DOM tree of pages to extract informative contents were proposed in articles [19] [18] [7].

Yi et al. [19] and Li et al. [18] studied noise elimination by aligning the DOM trees to extract structural information of Web sites. They present a data struc-ture called SST (Site Style Tree) to capt ure the actual contents and the common presentation styles of the Web pages i naWebsite.Basedontheconstructed SST, the importance of each node is evaluated through information theory (or entropy). For each node in the SST tree, th e more diversity of presentation styles and contents associated to it, the more likely the node is informative. On the contrary, less diversity indicates the node is likely to be noisy.
Gibson et al. [7] have conducted an extensive survey on the use of templates on the Web which revealed the rapid development of template usage. They also develop new randomized algorithms (DOM-based algorithm and Text-based al-gorithm) for template extraction. In DOM-based algorithm, for each node, a hash is computed by the content of the node and the start and end of offsets. The nodes are considered as templates if the occurrence counts of their hashes are within a specified threshold. In Text-based algorithm, the page is pre-processed to remove all HTML tags, comments, and text within &lt; script &gt; tags, and then a shingling procedure is performed on the result data through sliding a window of size W . Templates are identified by comp aring the occurrence count of the fragment contained in the window to a specified threshold.

Some page-level algorithms of templat e detection have been proposed in [3] [9] [5] [8] [15].

Kao et al. [9] proposed a greedy algorithm operating on features derived from the page to segment a given Web page. Debnath et al. [5] also proposed a page-level algorithm ( X  X -Extractor X ) that uses various block-features and trains a Support Vector (SV) based classifier to identify an informative block versus a non-informative block. Kao et al. [8] utilized entropy-based Link Analysis on Mining Web Informative Structures. Song et al. [15] used VIPS (Vision-based Page Segmentation) algorithm to segment a Web page into blocks which are then judged on their salience and qualit y. Chakrabarti et al. [3] developed a framework for the page-level template detection problem based on two main ideas: the automatic generation of training data for a classifier that assigns templateness score to every DOM node of a page, and the global smoothing of per-node classifier scores. There are also other methods discussed in [4] [10].
Many authors have explored Web page segmentation. Although segmentation of pages and content extraction may appear different, there are many similarities between these two tasks, such as models and algorithms. We can also extract informative contents through segmentation. One of the best solutions found in literature, called Vision-based Page Segmentation algorithm (VIPS), was pre-sented by Cai et al. in [2]. The method segments a page by simulating the visual perceptions of the users about that page, but it requires a considerable human ef-fort. In [6] David addressed a method to segment Web pages of a site by aligning the DOM trees, which gives us inspiration.

We here propose a novel method based on SST for discovering informative contents. This method leads to a significant improvement, compared to DOM Based Algorithm in [7], Text Based Algorithm in [7], SST Based Algorithm in [19] [18] and RTDM Based Algorithm in [16]. Experiments indicate that our method outperforms these four baselines at F score. 3.1 Representation of Web Pages Unlike plain text documents, Web pages consist of text contents and tags. Each page can be represented as a DOM (Document Object Model) tree, in which tags are internal nodes and the detailed texts, images or hyperlink are leaf nodes. Based on the DOM tree, a page is partitioned into several segments in line with the following rules:  X  Traverse the DOM tree through post-order.  X  If a node and all its sibling nodes are leaf nodes and they also have the same tag, merge them into one.  X  If a node has only one child, merge its child into it.

Thus, each node of the DOM tree can be considered as a segment. 3.2 Representation of Web Sites A DOM tree is sufficient for representing the content and style of a single HTML page, but it is insufficient to study the overall presentation style and contents of a set of HTML pages. To address this problem, we create a hierarchical structure named SST tree that summarizes the DOM trees of the pages in a given Web site. Now we define a site style tree.

Definition: Anode S in site style tree is the combination of DOM nodes having the identical tag and attr ; it has seven components, denoted by ( tag , attr , collection , parent , children , counter , threshold ), where  X  tag is the tagName of a DOM node.  X  attr is the set of display attributes of TAG .  X  collection is the document collection of segments containing it.  X  parent is the pointer to its parent.  X  children is the set of pointers to its children.  X  counter is the number of pages containing it.  X  threshold is the entropy-threshold which is used to detect templates. Figure 1 shows modules of our approach. Each module will be described in the following sub-sections.
 4.1 Constructing SST Tree The first step of our algorithm is constructing the SST tree. The Insert and Merge algorithm is described in Algorithm 1 and Algorithm 2. In Algorithm 3, we describe the Construct algorithm. With the above 3 algorithms, the SST tree of the site can be easily gained and here we omit the detailed description. Algorithm 1. Insert Algorithm 2. Merge Algorithm 3. Construct 4.2 Extracting Features and Calculating Entropy of Features Using SST tree, segments with the common presentation style can be aggregated together. Each node S of SST is a document collection, which contains many segments from different pages.

While constructing SST tree, features of all contents in SST are simultane-ously extracted. In this pap er, features correspond to m eaningful keywords. Stop words are not included. The motivation of our approach is that: Features dis-tributed in more pages in a Web site usually carry less information. In contrast, those appearing in fewer pages carry more information. Hence, we use entropy, which is determined from the probability distribution of features among the whole document sets, to represent the in formation strength of features. Shan-non X  X  information entropy [14] is applied on the feature-document matrix (F-D Matrix) which is generated from the feature extraction module to calculate the entropy. Thus, S.collection form the F-D matrix.
 The following is Shannon X  X  famous general formula for uncertainly: where p i is the probability of event i .
 By normalizing the weight of the feature to be [0, 1], the entropy of feature F i is: in which w ij is the weight of F i in document D i . w ij is an entity in the feature-document matrix to represen t the weight of a feature. where tf ij is the feature frequency of feature i in document j . To normalize the entropy value of a feature to be range [0, 1], the base of the logarithm is chosen to be the number of documents in the collection N . Equation (2) thus becomes: where n = | C | , C is the document collection. 4.3 Estimating Entropy of Sentences We have observed that: A lot of times, one content block contains not only informative contents but also template contents. Sentence is a better fit for template detection. Hence, we use sentence as the granularity of informative content units. Features and their entropies are gained in the previous subsection. By instinct, the entropy of a sentence is th e summation of its features X  entropies. We then define the entropy of sentence S i as the average entropy of all features in S i below: where F j is the jth feature among all k features of S i . 4.4 Determining Entropy Threshold Based on entropies, sentences can be divided into two categories: redundant and informative by a threshold T . If E(S) is higher than T , the sentence S is much more likely to be redundant, or else informative. It is difficult to determine the threshold since it would vary for diff erent clusters or sites. The higher the threshold is, the higher recall is and the lower precision is. In order to address this problem, we have labeled sentences of 100 sampled pages as redundant or informative in each site. Based on these labeled data, F score can be gained easily. For each node S of SST, we start the threshold T from 0 to 1.0 with an interval of 0.1. With some a threshold T ,ifFscoreisthelargest, T is determined as S.threshold .
 4.5 Classifying Sentences According to subsection 4.3, all senten ces X  entropies of Web pages can be cal-culated by features X  entropies acquired b efore. We can identify informative sen-tences by checking if the entropy is la rger than the specified threshold. Algorithm 4. Discoverer Algorithm 4 summarizes all the steps of our discovering approach. Given a Web site, the system first constructs the SST. For each node of the SST, entropies of features and sentences are calculate d. Then the threshold is determined by a labeled data set of the site. For all the pages in the site, informative contents can be discovered by comparing the entropy with threshold. This section evaluates our proposed informative contents discovering algorithm. Since the main purpose of Web content extraction is to improve Web data min-ing, we performed two data mining tasks, i.e., clustering and classification, to test our system. To validate our method, we compared our approach with these methods, i.e., Origin (results before cleaning), DOM Based Algorithm [7], Text Based Algorithm [7], SST Based Algorithm [18] [19] and RTDM Based Algorithm [16]. These methods have been introduced in the Related Work section.
In the following subsections, we first describe the data sets and evaluation measures used in our experiments. After that, we present the results of the experiments, and also give some discussions. 5.1 Data Sets and Evaluation Measures In this paper, we crawled six distinct commercial Web sites: PCConnection 1 , Amazon 2 ,CNet 3 ,J&amp;R 4 ,PCMag 5 nd ZDnet 6 . The six Web sites contain Web pages of many categories or classes of products. We choose the Web pages which focus on the following categories of products: Notebook, Camera, Mobile, Printer and TV. Table 1 lists the number of documents downloaded from each Web site, and their corresponding classes.

Since we test our method using clustering and classification, we use the popu-lar F score measure to evaluate clustering and classification performance of our method and the baselines. F score is defined as follows: where P is the precision and R is the recall. Fscore is the weighted harmonic mean of P and R , which reflects the average effect of both precision and recall. 5.2 Experimental Results We now present the experimental results of Web page clustering and classification based on our method and the baselines.

For the experiments, we implemented the following algorithms for comparison, the parameters of which are set to the best value in line with the corresponding experiments:  X  DOM Based Algorithm with parameters ( F =100);  X  Text Based Algorithm with parameters ( W =32, F =50, D =0, P =1000);  X  RTDM Based Algorithm;  X  SST Based Algorithm with parameters ( r =0.9, t =0.4);
In this paper, we construct the SST of each Web site by all the crawled pages for each site. After labeling sentences of sampled 100 Web pages, the entropy-threshold of each SST node is determined automatically.
 Clustering. We stress the experimental procedure we use is the same as in [11], but the sets of pages used are not the same. We put all the 5 categories of Web pages into a big set, and use the popular k-means clustering algorithm to cluster them into 5 clusters. Since the k-means algorithm selects the initial cluster seeds randomly, we performed a large number of experiments (800) to show the behaviors of k-means clustering on our method and the baselines. Figure 2 shows 10 bins of F score from 0 to 1 with an interval of 0.1 and gives the statistics of the number of experiments whose F scores fall into each bin. The average F scores are plotted in Table 2.

F(Origin) represents the F score of clustering based on original noisy Web pages; F(DOM), F(Text), F(RTDM) and F( SST) represent the F scores of clus-tering based on DOM-Based Algorithm [7], Text-Based Algorithm [7], RTDM Based Algorithm [16] and SST-Based Algorithm [11] respectively; While F(S) represents the F score of clustering based on our method.

We can clearly observe that clustering results based on all the cleaning meth-ods are dramatically better than results using the original noisy pages. Our method also performs better than the other baselines for Web page clustering. Classification. The evaluation of our method over classification also follows the experimental procedure proposed in [11]. For classification, we use the Naive Bayesian classifier (NB), which has been shown to perform very well in practice by many researches. We experiment with three different configurations for clas-sification tasks using all possible pairs of product categories. The configurations are summarized in Table 3. For each pair, we train the NB classifier using a Training Set and then run the classifier over the corresponding Test Set.
In Table 4, we can observe that classification results based on all the cleaning methods are dramatically better than the results using the original noisy Web pages. Our method performs better than SST and much better than the other three baselines for Web page classification. In this paper, we propose a novel method for information extraction based on SST. Given a Web site, SST tree is constr ucted from a collection of pages in the site. Then a method based on information theory is used to detect informative sentences in each node of SST.

Experiments conducted on 6 Web sites show that our approach produces quite strong result compared with other five typical algorithms (including original texts).

In future work, we will investigate the application of our method to the pro-cessing of short-text data, such as forum and micro blog dataset. Further, we plan to explore the improvement on how to discovery informative contents of a large scale of Web pages automatically.
 Acknowledgments. We thank the anonymous reviewers for their valuable and constructive comments. This work is partially supported by NSFC Grant No.61272340.

