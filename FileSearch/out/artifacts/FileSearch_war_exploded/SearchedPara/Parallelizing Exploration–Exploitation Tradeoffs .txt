 Thomas Desautels tadesaut@caltech.edu Andreas Krause krausea@ethz.ch Joel Burdick jwb@robotics.caltech.edu Many applications, from recommender systems to op-timal control to experimental design, require solving exploration X  X xploitation tradeoffs: one needs to make a sequence of decisions with uncertain outcomes and thus, based on noisy feedback, one wishes to simulta-neously learn a model and use that model to maximize the reward obtained.
 Often, the set of possible decisions is large or infinite, and therefore we must be able to generalize from par-tial observations to predict the likely reward associated with unexplored decisions. A second crucial challenge is that we wish to explore many possible decisions in parallel: in information retrieval, it may not be pos-sible to update the predictive model in real-time, but perhaps once per day, taking into account all the feed-back collected; in experimental design, we may wish to design batches of simultaneously running experiments, only incorporating feedback once all experiments ter-minate; and in complex control tasks, performance feedback may become available only after a delay. This paper tackles these two central challenges aris-ing when solving large-scale exploration X  X xploitation tradeoffs. We model the problem as a stochastic multi-armed bandit problem, where the unknown mean pay-off function is modeled as a Gaussian process (GP, Rasmussen &amp; Williams (2006)). As nonparametric statistical models, GPs can flexibly incorporate a vari-ety of assumptions about regularity of the payoff func-tion via its covariance (or kernel) function. We design an efficient algorithm, GP-BUCB , that is able to handle both the parallel exploration problem (where we pro-pose batches of B experiments executed concurrently) and delayed feedback (where each decision can only use feedback up to B rounds ago). Our approach gen-eralizes the GP-UCB approach (Srinivas et al., 2010) to the parallel setting. We prove bounds on the cu-mulative regret incurred by GP-BUCB . We show that, perhaps surprisingly, near-linear speedup is possible for many commonly used kernel functions: as long as the batch size B grows at most polylogarithmically in the number of rounds T , the GP-BUCB regret bounds only increase by a constant factor independent of B as compared to the known bounds for the sequential algorithm. We also demonstrate how the GP-BUCB algorithm can be drastically accelerated by using lazy evaluations . We evaluate our approach on several syn-thetic benchmark optimization tasks, as well as two real data sets, respectively related to automated vac-cine design and therapeutic spinal cord stimulation. Related Work Classical work on multi-armed ban-dit problems has focused on the case of a finite number of decisions (Robbins, 1952). Optimistic allocation ac-cording to upper-confidence bounds (UCB) on the pay-offs has proven to be particularly effective (Auer et al., 2002). Recently, approaches for coping with large (or infinite) sets of decisions have been developed. In these cases, dependence between the payoffs associated with different decisions must be modeled and exploited. Ex-amples include bandits with linear (Dani et al., 2008; Abernethy et al., 2008) or Lipschitz-continous payoffs (Kleinberg et al., 2008), or bandits on trees (Koc-sis &amp; Szepesv  X ari, 2006; Bubeck et al., 2008). The exploration-exploitation tradeoff has also been studied in Bayesian global optimization and response surface modeling, where Gaussian process models are often used due to their flexibility in incorporating prior as-sumptions about the payoff function (Brochu et al., 2009). Several heuristics, such as Maximum Expected Improvement (Jones et al., 1998), Maximum Prob-ability of Improvement (Mockus, 1989), and upper-confidence based methods (Cox &amp; John, 1997), have been developed to balance exploration with exploita-tion and successfully applied in learning problems (Li-zotte et al., 2007). Recently, Srinivas et al. (2010) an-alyzed GP-UCB , an upper-confidence bound sampling based algorithm for this setting, and proved bounds on its cumulative regret, and thus convergence rates for Bayesian global optimization. We build on this foundation and generalize it to the parallel setting. To enable parallel selection, one must account for the lag between decisions and observations. Most existing approaches that can deal with such delay result in a multiplicative increase in the cumulative regret as the delay grows. Only recently, Dudik et al. (2011) demon-strated that it is possible to obtain regret bounds that only increase additively with the delay (i.e., the penalty becomes negligible for large numbers of deci-sions). However, the approach of Dudik et al. only ap-plies to contextual bandit problems with finite decision sets, and thus not to settings with complex (even non-parametric) payoff functions. In contrast, there has been heuristic work in parallel Bayesian global opti-mization using GPs, e.g. by Ginsbourger et al. (2010). The state of the art is the simulation matching algo-rithm of Azimi et al. (2010). To our knowledge, no the-oretical results regarding the regret of this algorithm exist. We compare with this approach in Section 5. We wish to make a sequence of decisions x , x 2 ,..., x T  X  D , where D is called the deci-sion set , which is often (but not necessarily) a compact subset of R d . For each decision, we observe noisy scalar reward y 1 ,y 2 ,...,y T , where for any t , y t = f ( x t ) +  X  t and where f : D  X  R is in turn an unknown function modeling the expected payoff f ( x ) for each decision x . For now we assume that the noise variables  X  t are i.i.d. Gaussian with known assumption later. In the strictly sequential setting, we allow x t to depend on observations y 1: t  X  1 associated with x 1 ,..., x t  X  1 . Below, we will formalize the main problem tackled in this paper: the challenging setting where x t may only depend on y 1: t 0 , for some t 0 &lt; t  X  1. We wish to maximize the cumulative reward P t =1 f ( x t ), or equivalently minimize the cumulative regret R T = P T t =1 r t , where r t = [ f ( x  X  )  X  f ( x x  X   X  argmax sumed to exist, but not necessarily to be unique). In experimental design, D might be the set of possible stimuli that can be applied, and f ( x ) corresponds to the response to stimulus x  X  D . By minimizing the regret, we ensure progress towards the most effective stimulus uniformly over T . In fact, the average re-gret, R T /T , is a natural upper bound on the subop-timality of the best stimulus considered so far, i.e., R
T /T  X  min t [ f ( x  X  )  X  f ( x t )] (often called the simple regret , Bubeck et al. (2009)).
 The Problem: Parallel / Delayed Selection In many applications, we wish to select batches of deci-sions x 1 ,..., x B to be evaluated in parallel. One natu-ral application is the design of high-throughput exper-iments, where we perform several experiments in par-allel, but only receive feedback after the experiments have concluded. In other settings, we may only receive feedback after a delay. In both situations, decisions are selected sequentially, but when making the decision x t in round t , we can only make use of the feedback ob-tained in rounds 1 ,...,t 0 , for some t 0  X  t  X  1. Formally, we assume there is some mapping fb : N  X  N 0 such that fb[ t ]  X  t  X  1 ,  X  t  X  N , and when taking decision at time t , we can use feedback up to and including round fb[ t ]. If fb[ t ] = 0, no information is available. This framework can model a variety of realistic sce-narios. Setting B = 1 corresponds to the non-delayed, strictly sequential setting. If the feedback is delayed by exactly B rounds, we can simply set fb[ t ] = max { t  X  B, 0 } . To select batches of size B , we can simply fb[ B + 1] = ... fb[2 B ] = B , . . . . We may also be in-terested in executing several experiments in parallel, but the duration of an experiment may be variable, and we can start a new experiment as soon as one finishes. In this case, fb[ t ] may be a more complex mapping. Here, B is the bound on the duration of any single experiment. In the following, we only assume that t  X  fb[ t ]  X  B for some known constant B . Modeling f via Gaussian Processes (GPs) If we do not make any assumptions about the payoff func-tion f , for large (possibly infinite) decision sets D there is no hope to do well, i.e., incur little regret or even simply converge to an optimal decision. One effective formalism is to model f as a sample from a Gaussian process (GP) prior. A GP is a probability distribution across a class of  X  typically smooth  X  functions, which is parameterized by a kernel function k ( x , x 0 ), which characterizes the smoothness of f , and a mean func-tion  X  ( x ), which we assume to be  X  ( x ) = 0 w.l.o.g. We write f  X  GP (  X ,k ) to denote that we model f as sampled from such a GP. If we assume that the noise is i.i.d. Gaussian and we condition on a set of obser-vations y 1: t  X  1 = [ y 1 ,...,y t  X  1 ] corresponding to X = { x 1 ,..., x t  X  1 } , at any x  X  D , we obtain a Gaussian where k = k ( x ,X ) is the row vector of kernel evalua-tions between x and X and K = K ( X,X ) is the ma-trix of kernel evaluations between past observations. The GP-UCB approach Modeling f as a sample from a GP has the major advantage that the predic-tive uncertainty can be used to guide exploration and exploitation. Recently, Srinivas et al. (2010) analyzed the Gaussian process Upper Confidence Bound ( GP-UCB ) selection rule This decision rule uses  X  t , a domain-specific time-varying parameter, to trade off exploitation (sam-pling x with high mean) and exploration (sampling x with high standard deviation) by changing the rela-tive weighting of the posterior mean and standard de-viation, respectively  X  t  X  1 ( x ) and  X  t  X  1 ( x ) from Equa-tions (1) and (2). Srinivas et al. (2010) showed that, Algorithm 1 GP-BUCB
Input: Decision set D , GP prior  X  0 , X  0 , kernel func-tion k (  X  ,  X  ) for t = 1 , 2 ,...,T do end for with proper choice of  X  t , the cumulative regret of GP-UCB grows sublinearly for many commonly used kernel functions, providing the first regret bounds and con-vergence rates for GP optimization.
 Motivated by the strong theoretical and empirical per-formance of GP-UCB , we explore generalizations to batch / parallel selection (i.e., B &gt; 1). One na  X  X ve ap-proach would be to update the GP-UCB score (3) only once new feedback becomes available, but this algo-rithm would simply select the same observation up to B times, leading to limited exploration. To encourage more exploration, one may require that no decision is selected twice (i.e., simply rank decisions according to the GP-UCB score, and pick decisions in order of de-creasing score, until new feedback is available). How-ever, since f often varies smoothly, so does the GP-UCB score; this modification would also suffer from limited exploration. In the following, we introduce the Gaussian process -Batch Upper Confidence Bound ( GP-BUCB ) algorithm, which encourages diversity in exploration, and prove strong performance guarantees. A key property of GPs is that the predictive variance (2) only depends on where the observations are made, but not which values were actually observed. Thus, it is possible to compute the posterior variance used in the sequential GP-UCB score, even while previous observations are not yet available. A natural approach towards parallel exploration is therefore to alter (3) to sequentially choose decisions within the batch as Here, the role of  X  t is analogous to that of  X  t in the GP-UCB algorithm. This approach naturally encour-ages diversity in exploration by taking into account the change in predictive variance: since the payoffs of  X  X imilar X  decisions have similar predictive distribu-tions, exploring one decision will automatically reduce the predictive variance of similar decisions.
 The disadvantage of taking this approach, however, is that the decision selection late in the batch is predi-cated on having information from the early decisions in the batch, but we do not in fact currently have that information; we are being  X  X verconfident X  about our knowledge of the function at those locations. This overconfidence requires us to compensate in a principled manner. One conceptual approach to doing so is to increase the width of the confidence intervals (through proper choice of  X  t ), such that the confidence intervals used by GP-BUCB are conservative , i.e., contain the true function f ( x ) with high probability. Figure 1 illustrates this idea. In Section 4, we show how it is indeed possible to properly choose  X  t so that the regret only mildly increases, providing strong theoretical evidence about the potential for parallelizing GP optimization.
 Lazy Variance Calculation One major computa-tional bottleneck of applying GP-BUCB is calculating the posterior mean  X  t ( x ) and variance  X  2 t ( x ) for the candidate decisions. The mean is updated only when-ever feedback is obtained, and  X  upon computation of the Cholesky factorization of K ( X,X ) +  X  2 n I (which only needs to be done once whenever new feedback arrives)  X  predicting  X  t ( x ) takes O ( t ) additions and multiplications. On the other hand,  X  2 t must be re-computed for every x in D after every single round, and requires solving backsubstitution, which requires O ( t 2 ) computations. Therefore, the variance compu-tation dominates the computational cost of GP-BUCB . Fortunately, for any fixed decision x ,  X  2 t ( x ) is mono-tonically decreasing in t . This fact can be exploited to dramatically improve the running time of GP-BUCB , at least for finite (or when using discretizations of the) decision sets D . The key idea is that instead of re-computing  X  t  X  1 ( x ) for all decisions x in every round t , we can maintain an upper bound ized to the GP-BUCB rule with this upper bound, to identify We then recompute lies in the argmax of (5), we have identified the next de-cision to make, and set ing decisions x . This idea generalizes to the bandit setting a technique proposed by Minoux (1978), which concerns calculating the greedy action for submodu-lar maximization and leads to dramatically improved empirical computational speed, discussed in Section 5. Srinivas et al. (2010) prove that the cumulative regret of the strictly sequential GP-UCB can be bounded (up to logarithmic factors) as R T = O  X  ( is the maximum mutual information I ( f ; y A ) = H ( y A )  X  H ( y A | f ) = obtained through observations y A of any set A  X  D of T decisions evaluated. For many kernel functions commonly used in practice, they show that  X  T grows sublinearly and  X  T only needs to grow polylogarith-mically in T . Thus, R T /T  X  0, i.e., GP-UCB is a no-regret algorithm.
 The analysis of GP-UCB (and upper-confidence index policies in general) rests upon three major pillars: (1) The constructed confidence intervals contain the true payoff f ( x ) with high probability; (2) The width of the confidence interval at the se-lected decision bounds the instantaneous regret r t (i.e., r t  X  w t , where w t = 2  X  widths w 1 ,...,w T shrink sufficiently quickly to ensure sublinear regret.
 Our strategy for choosing  X  t in the GP-BUCB rule rests on a generalization of this analysis. We will choose  X  t such that the confidence intervals still contain the true expected payoff f ( x ) with high probability. Under this condition, a straightforward generalization of the arguments of Srinivas et al. (2010) leads to regret bounds of the form O  X  ( mal statement is given below).
 Avoiding Overconfidence. We seek to derive suf-ficient conditions on  X  t to ensure that the confidence intervals employed by GP-BUCB contain f with high probability. As we will see, a crucial role is played by the conditional mutual information , which for observa-tions y A and y S of two finite sets A,S  X  D is defined as Lemma 1 is the key technical result, which allows us to infer how much the confidence intervals must be enlarged to avoid overconfidence.
 Lemma 1. For f sampled from a known GP prior with known noise variance  X  2 n , the ratio of  X  fb [ t ]  X  t  X  1 ( x ) is bounded as Therefore, the relative amount by which the confi-dence intervals can shrink w.r.t. decision x is bounded by the worst-case (greatest) mutual information x fb[ t ]+1: t  X  1 , those decisions for which feedback is not available. Thus, if we have a constant bound C on the maximum conditional mutual information that can be accrued within a batch, we can use it to guide our choice of  X  t to ensure that the algorithm is not overconfident. We can then leverage the machinery of Srinivas et al. (2010) to derive our regret bound below. Regret Bounds Our main result bounds the regret of GP-BUCB in terms of a bound C on the maximum conditional mutual information. It holds under any of three different assumptions about the payoff function f , which may all be of practical interest. In particu-lar, it holds even if the assumption that f is sampled from a GP is replaced by the assumption that f has low norm in the Reproducing Kernel Hilbert Space (RKHS) associated with the kernel function.
 Theorem 1. Let  X   X  (0 , 1) . Suppose one of the fol-lowing assumptions holds: 1. D is finite, f is sampled from a known GP 2. D  X  [0 ,l ] d is compact and convex, d  X  N , l &gt; 0 . 3. D is arbitrary; f has RKHS norm || f || k  X  M . The Further suppose we have bound C &gt; 0 s.t., for all t , Then, the cumulative regret of GP-BUCB , using  X  t = exp(2 C )  X  fb [ t ] , is bounded by O  X  ( p T X  T exp(2 C )  X  w.h.p. Precisely, Pr n R T  X  p C 1 T exp(2 C )  X  T  X  T + 2  X  T  X  1 o  X  1  X   X  where C 1 = 8 / log(1 +  X   X  2 n ) .
 The key quantity that controls the regret in Theorem 1 is the bound C on the maximum conditional mutual information obtainable within a batch (10). In par-ticular, the cumulative regret bound of GP-BUCB is a factor exp( C ) larger than the regret bound for the se-quential ( B = 1) GP-UCB algorithm. Intuitively, one expects that C must grow monotonically with B : with greater delay, there is more potential for exploration (and thus to gain more information). An easy upper bound is obtained as follows: Due to the  X  X nforma-tion never hurts X  bound (Cover &amp; Thomas, 1991), the conditional mutual information I ( f ; y A | y S ) is mono-tonically decreasing in S (i.e., as elements are added to set S ). Therefore, I ( f ; y A | y S )  X  I ( f ; y  X  B  X  1 , whenever | A |  X  B  X  1. However, the choice C =  X  B  X  1 is not satisfying; usually,  X  B  X  1 grows at least as  X (log B ), suggesting that exp( C ) would have to grow at least linearly in B . In the following, we show that it is possible to slightly modify the GP-BUCB al-gorithm so that a constant choice of C independent of B suffices.
 Better Bounds Through  X  X nitialization X  The key idea that allows us to obtain regret bounds in-dependent of B is again to exploit monotonicity prop-erties of the conditional mutual information. Suppose that instead of GP-BUCB , we use a two-stage proce-dure, that first nonadaptively (i.e., without any feed-back) selects an initialization set D init of size | D init | T init . The algorithm then obtains feedback y init for all it then applies GP-BUCB on the posterior Gaussian process distribution, conditioned on y init .
 Notice that if we define then, under the assumptions of Theorem 1, using C =  X  init B  X  1 , the regret of the two-stage algorithm is bounded by R T = O ( T init + p T X  init T  X  T exp 2 C ). In the following, we show that it is indeed possible to construct an initialization set D init such that the size T init is dominated by p T X  init cially  X  that C =  X  init B  X  1 can be bounded independently of the batch size B .
 We will construct D init via uncertainty sampling: we start with D init 0 = {} , and for each t = 1 ,...,T greedily add the most uncertain decision key result about the residual information gain  X  init : Lemma 2. Suppose we use uncertainty sampling to generate an initialization set D init of size T init . Then Whenever  X  T is sublinear (i.e.,  X  T = o ( T )), then for any constant C &gt; 0, we can choose T init as a function of B such that  X  init B  X  1 &lt; C . In order to derive bounds on T init , we in turn need a concrete analytical bound on  X  . Fortunately, Srinivas et al. (2010) prove bounds on how the information gain  X  T grows for some of the most commonly used kernels. Table 1 provides suffi-cient conditions for how quickly T init must grow as a function of the batch size B . Finally, note that uncer-tainty sampling is a special case of the GP-BUCB algo-rithm with a constant prior mean of 0 and the require-ment that for all 1  X  t  X  T init , fb[ t ] = 0, i.e., no feed-back is taken into account for the first T init iterations. We summarize our analysis in the following theorem. For sake of notation, define R seq T to be the regret bound of Srinivas et al. (2010) associated with the sequential GP-UCB algorithm (i.e., Theorem 1 with B = 1). Theorem 2. Suppose one of the conditions of Theo-rem 1 is satisfied. Further suppose the kernel and T init are as listed in Table 1. Fix  X  &gt; 0 . Let R T be the regret of GP-BUCB , which ignores feedback for the first T init rounds. Then there exists a constant C 0 independent of B such that for any T  X  0 , it holds with probability at least 1  X   X  that where C 0 takes the value shown in Table 1.
 Notice that, whenever B = O (polylog( T )), T init = O (polylog( T )). Further note R seq T =  X ( long as the batch size does not grow too quickly, the term O ( T init ) is dominated by C 0 R seq T and thus the regret bounds of GP-BUCB are only a constant factor independently of B worse than those of GP-UCB . We empirically evaluate GP-BUCB on several synthetic benchmark problems as well as two real applications. We compare it with four alternatives: (1) The strictly sequential GP-UCB algorithm ( B = 1); (2) NRB-UCB , an approach that simply picks the maximizer of the GP-UCB score B times; (3) NTB-UCB , an approach that picks the top B scores according to the GP-UCB criterion; (4) A state of the art algorithm for Batch Bayesian optimization proposed by Azimi et al. (2010), which can use either a UCB or Maximum Expected Improvement (MEI) decision rule, herein SM-UCB and SM-MEI respectively. All batch selec-tion algorithms pick batches of B = 10 points and all experiments were repeated for 100 trials with independent observation noise for each trial.
 Synthetic Benchmark Problems We first test GP-BUCB in conditions where the true prior is known. A set of 100 example functions was drawn from a zero-mean GP with Mat  X ern kernel over the interval [0 , 1]. The kernel, its parameters, and the noise variance were known to each algorithm. The decision set D was the discretization of [0 , 1] into 1000 evenly spaced points. Figures 2(a) and 2(e) present the results of this experiment. GP-BUCB performed slightly better than SM-UCB and SM-MEI in terms of both average regret and minimum regret. GP-BUCB , SM-UCB , and SM-MEI were outperformed by GP-UCB early on, but after they received their first observations at the end of batch 1 (query 10), performance was comparable to GP-UCB . As expected, both of the na  X  X ve algorithms performed quite poorly. Figure 2(d) compares the algorithms in terms of their running time; lazy variance calculations led to dramatic running time im-provements. We also performed experiments on other synthetic benchmark domains, with qualitatively sim-ilar results (presented in the supplemental material). Automated Vaccine Design We also tested GP-BUCB on a database of Widmer et al. (2010), which describes the binding affinity of various peptides with a Major Histocompatibility Complex (MHC) Class I molecule, of importance when designing vaccines to exploit peptide binding properties. Each of the pep-tides is described by a set of chemical features in R 45 . The binding affinity of each peptide, which is treated as the reward or payoff, is described as an offset IC 50 value. The experiments used a linear ARD kernel fitted on a different MHC molecule from the same data set. Figures 2(b) and 2(f) present this experiment X  X  re-sults. GP-BUCB performs competitively with SM-MEI and SM-UCB , both in terms of average and minimum regret, and converges to the performance of GP-UCB . Spinal Cord Therapy Lastly, we compare the al-gorithms on a data set of leg muscle activity triggered by therapeutic spinal electrostimulation in spinal cord injured rats. The experimental objective is to choose the stimulus electrodes which maximize the resulting activity in lower limb muscles, as measured by elec-tromyography (EMG), in order to improve spinal reflex and locomotor function. We sought to maximize the peak-to-peak amplitude of the recorded EMG wave-forms from the right medial Gastrocnemius muscle in a time window corresponding to a single interneuronal delay. This objective function measures to what de-gree the selected stimulus activates the interneurons in the spinal gray matter which control reflex activity. Electrode configurations were represented in R 4 by the cathode and anode locations on the array. A squared-exponential ARD kernel was fitted for this space us-ing experimental data from 12 days post-injury. Al-gorithm testing was done on data from 116 electrode pairs tested on the 14th day post-injury. Experimen-tal results are presented in Figures 2(c) and 2(g). This problem setting was quite challenging for all algo-rithms, as the data was highly multi-modal. Conse-quently, GP-UCB often failed to find the optimum in the number of queries examined; out of 100 runs, only 18 had converged to the optimum, and out of the re-mainder, none had ever visited the optimum in 200 queries. Interestingly, the GP-BUCB , SM-UCB and SM-MEI algorithms were more robust to these diffi-culties; their superior initialization, born of the ex-ploratory behavior forced on them by their initial ig-norance, resulted in convergence likelihoods on the or-der of 40% for each. The superior performance of GP-BUCB to SM-UCB and SM-MEI with respect to average regret and the comparable likelihoods of convergence within the practical experimental window considered indicate that GP-BUCB is at least as effective as the current state of the art in this challenging experimental setting. Lazy variance calculations again led to dra-matic running time improvements, presented in Fig-ure 2(h). We have developed the GP-BUCB algorithm for paral-lelizing exploration and exploitation tradeoffs in Gaus-sian process bandit optimization. We showed how the regret of GP-BUCB can be bounded in terms of an in-tuitive conditional mutual information quantity. Using this analysis, we prove that GP-BUCB can be  X  X nitial-ized X  to obtain regret bounds which only additively depend on the batch size for many kernel functions commonly used. We further show how  X  X azy X  vari-ance evaluation can yield order-of-magnitude improve-ments in running time. In our experiments, GP-BUCB compares favorably to the state of the art in paral-lel Bayesian optimization, which is not equipped with theoretical guarantees. We believe that our results provide an important step towards solving complex, large-scale exploration-exploitation tradeoffs. Acknowledgments The authors thank Daniel Golovin for helpful discussions. This work was par-tially supported by NIH project R01 NS062009, SNSF grant 200021 137971, NSF IIS-0953413, DARPA MSEE FA8650-11-1-7156 and the ThinkSwiss Re-search Scholarship.
 Abernethy, J., Hazan, E., and Rakhlin, A. Competing in the dark: An efficient algorithm for bandit linear optimization. In COLT , 2008.
 Auer, P., Cesa-Bianchi, N., and Fischer, P. Finite-time analysis of the multiarmed bandit problem. Mach. Learn. , 47(2-3):235 X 256, 2002.
 Azimi, J., Fern, A., and X.Fern. Batch bayesian opti-mization via simulation matching. In NIPS , 2010. Brochu, E., Cora, M., and de Freitas, N. A tutorial on
Bayesian optimization of expensive cost functions, with application to active user modeling and hierar-chical reinforcement learning. In TR-2009-23, UBC , 2009.
 Bubeck, S., Munos, R., Stoltz, G., and Szepesv  X ari, C.
Online optimization in X-armed bandits. In NIPS , 2008.
 Bubeck, S., Munos, R., and Stoltz, G. Pure explo-ration in multi-armed bandits problems. In ALT , 2009.
 Cover, T. M. and Thomas, J. A. Elements of Infor-mation Theory . Wiley Interscience, 1991.
 Cox, D. D. and John, S. Sdo: A statistical method for global optimization. Multidisciplinary Design Opti-mization: State of the Art , 1997.
 Dani, V., Hayes, T. P., and Kakade, S. M. Stochas-tic linear optimization under bandit feedback. In COLT , 2008.
 Dudik, M., Hsu, D., Kale, S., Karampatziakis, N.,
Langford, J., Reyzin, L., and Zhang, T. Efficient optimal learning for contextual bandits. In UAI , 2011.
 Ginsbourger, D., Riche, R., and Carraro, L. Kriging is well-suited to parallelize optimization. In Tenne,
Yoel and Goh, Chi-Keong (eds.), Computational In-telligence in Expensive Optimization Problems , vol-ume 2 of Adaptation, Learning, and Optimization , pp. 131 X 162. Springer Berlin Heidelberg, 2010. Jones, D. R., Schonlau, M., and Welch, W. J. Ef-ficient global optimization of expensive black-box functions. J Glob. Opti. , 13:455 X 492, 1998.
 Kleinberg, R., Slivkins, A., and Upfal, E. Multi-armed bandits in metric spaces. In STOC , pp. 681 X 690, 2008.
 Kocsis, L. and Szepesv  X ari, C. Bandit based monte-carlo planning. In ECML , 2006.
 Lizotte, D., Wang, T., Bowling, M., and Schuurmans,
D. Automatic gait optimization with Gaussian pro-cess regression. In IJCAI , pp. 944 X 949, 2007. Minoux, M. Accelerated greedy algorithms for max-imizing submodular set functions. Optimization Techniques, LNCS , pp. 234 X 243, 1978.
 Mockus, J. Bayesian Approach to Global Optimization . Kluwer Academic Publishers, 1989.
 Rasmussen, C. E. and Williams, C. K. I. Gaussian Processes for Machine Learning . MIT Press, 2006. Robbins, H. Some aspects of the sequential design of experiments. Bul. Am. Math. Soc. , 55, 1952.
 Srinivas, N., Krause, A., Kakade, S., and Seeger, M. Gaussian process optimization in the bandit setting: No regret and experimental design. In ICML , 2010. Widmer, C., Toussaint, N., Altun, Y., and R  X atsch, G.
Inferring latent task structure for multitask learning by multiple kernel learning. BMC Bioinformatics ,
