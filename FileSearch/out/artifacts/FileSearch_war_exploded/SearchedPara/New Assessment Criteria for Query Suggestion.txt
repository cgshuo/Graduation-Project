 Query suggestion is a useful tool to help users express their infor-mation needs by supplying alternative queries. When evaluating the effectiveness of query suggestion algorithms, many previous studies focus on measuring whether a suggestion query is relevant or not to the input query. This assessment criterion is too simple to describe users X  requirements. In this paper, we introduce two scenarios of query suggestion. The first scenario represents cases where the search result of the input query is unsatisfactory. The sec-ond scenario represents cases where the search result is satisfactory but the user may be looking for alternative solutions. Based on the two scenarios, we propose two assessment criteria. Our labeling re-sults indicate that the new assessment criteria provide finer distinc-tions among query suggestions than the traditional relevance-based criterion.
 H.3.3 [ Information Search and Retrieval ]: query formulation Measurement Assessment criteria, query suggestion
Query suggestion is widely adopted in commercial search en-gines to supply alternative queries and assist users to achieve their search goals. When evaluating the effectiveness of query sugges-tion methods, some previous works [1, 4, 6] assign relevant or ir-relevant labels to the suggestion queries, and then compute metrics such as precision, recall, MAP.

This evaluation method, however, is too rudimentary to capture users X  requirements precisely, and various refinements have been proposed in the literature. For example, Jain et al. [5] require a good suggestion to be not only relevant but also useful. The sug-gestions return zero search results or are synonyms of the input query are treated as bad . Bhatia et al. [3] also regard suggestions that are almost duplicate to the input query as irrelevant. Yang et
The work was done when the first author and the fifth author were visiting Microsoft Research Asia.
 al. [7] adopt a five-point score to distinguish suggestions from  X  X re-cisely related X  to  X  X learly unrelated X . Baraglia et al. [2] define a suggestion as useful if it  X  X an interpret the possible intent of the user better than the original query X .

We argue that we should emphasize the usefulness instead of relevance because a relevant suggestion is not necessarily useful to the user. Therefore, we attempt to describe the usefulness of a suggestion by our proposed assessment criteria.
Whether a suggestion is useful or not depends on the situation that the user encounters in her/his search session. We believe there exists at least two different scenarios: Scenario 1 The search result of the input query is unsatisfactory. In this case, the user expects a suggestion that can return better search results to meet his/her information need. For example, for an input query q =  X  X ol instant mess X , two suggestion queries q instant messenger X  and q 2 = X  X ol aim X  (here,  X  X im X  is the acronym of  X  X OL Instant Messenger X ) are both labeled as relevant to q by the assessor. When we submit these three queries to a search engine and checked their results, we found that q 1  X  X  result matches q  X  X  intention better than the results of both q and q 2 . Therefore, q more useful than q 2 in terms of retrieving relevant documents. Scenario 2 The search result of the input query already contains desirable documents. In this case, the user expects a suggestion that is related to the input query, but has some difference in topics, in order to broaden his/her knowledge. For example, when the in-put query is  X  X ol instant messenger X , whose search result is good enough, returning  X  X indows live messenger X  and  X  X ahoo! messen-ger X  as alternative solutions might be more useful than returning  X  X ol aim X .

According to the two scenarios, we propose two judgment guide-lines for human assessors.
The utility judgment address scenario 1 by measuring whether a suggestion query q s can retrieve more relevant web documents than the input query q , based on the information need described by q . To decide the utility judgment of q s for q , an assessor must compare the search results of both q and q s . Then she can put q the following three categories, based on the comparison:
Although some of the existing works, e.g. [6], also require as-sessors to read the search results of q and q s , the search results are only for understanding the intention of q and q s in order to decide whether they are relevant. To the best of our knowledge, none of them compare the quality of the two search results.
The desirable suggestions in scenario 2 should be related to the input query and have some difference in topic. Instead of using this vague expression directly, we ask assessors to classify the relation-ship between q and q s into one of the following five categories:
Suggestions in generalization, specialization, and peer are the desirable ones, because they are related to the input queries and have some differences in topic. Suggestions in same intention are less useful as they have no difference in topic. Suggestions in no association are irrelevant to their input queries and hence are use-less.
Using our proposed criteria, we hired three assessors to label 17,053 suggestions for 200 queries, which are sampled from a re-cent one month search log of Bing search engine. We will show some examples to demonstrate the advantages of our new assess-ment criteria in this section.

No. Suggestion Utility Relationship 1 aol instant messenger better same intention 2 aim messenger same same intention 3 aim instant mess worse same intention 4 msn messenger 2011 beta worse peer 5 yahoo messenger express worse peer
Table 1 shows the suggestions of the query  X  X ol instant mess X  and their labeling results. Although suggestions 1 to 3 are all in the same intention category, 1 can retrieve better documents while 2 and 3 cannot. Therefore, users in scenario 1 would prefer sug-gestion 1 rather than 2 or 3. Traditional assessments cannot reflect their differences. Suggestion 4 and 5 are peers of the input query. Although their search results do not satisfy the input query X  X  inten-tion, users in scenario 2 would be interested in them because they help to broaden the knowledge.

No. Suggestion Utility Relationship 1 juicer blender machines same peer 2 juicer machine same same intention
There are also cases that suggestions not in the same intention category can return better results, such as suggestion 1 in Table 2 and all the suggestions in Table 3. In these cases, the peer sug-gestions retrieve better search results because they introduce better query keywords. In traditional assessments, they are likely to be labeled as irrelevant and query suggestions algorithms are discour-aged to return them. Therefore, our assessment criteria are more precise at describing the usefulness of query suggestions.
No. Suggestion Utility Relationship 1 consumer reports best teeth 2 best whitening toothpastes better peer 3 crest whitestrips directions better peer 4 how does baking soda work better peer Table 3: Suggestions  X  X op 5 teeth bleaching treatments X  and their labels
In this paper, we identify two user scenarios of query suggestion and introduce corresponding assessment criteria. The first criterion addresses the scenario that the input query fails to return satisfac-tory search result. It requires assessors to compare the search re-sults of the suggestions to that of the input query and awards those suggestions having better search results. To the best of our knowl-edge, this is the first criterion that compares the search result quality of the input query and its suggestions. The second criterion consid-ers different kinds of relationships between an input query and its suggestions. It assumes the input query already returns good search result, and awards the suggestions that help to broaden the user X  X  knowledge.

Our assessment can be applied to different types of evaluation by assigning a suitable score value to each category depending on the evaluation purpose. Then precision, recall, MAP, and even DCG/NDCG can be applied to measure the effectiveness of a sug-gestion list. [1] R. A. Baeza-Yates, C. A. Hurtado, and M. Mendoza. Query [2] R. Baraglia, C. Castillo, D. Donato, F. M. Nardini, R. Perego, [3] S. Bhatia, D. Majumdar, and P. Mitra. Query suggestions in [4] B. M. Fonseca, P. B. Golgher, E. S. de Moura, B. P X ssas, and [5] A. Jain, U. Ozertem, and E. Velipasaoglu. Synthesizing high [6] Y. Song and L. wei He. Optimal rare query suggestion with [7] J.-M. Yang, R. Cai, F. Jing, S. Wang, L. Zhang, and W.-Y. Ma.
