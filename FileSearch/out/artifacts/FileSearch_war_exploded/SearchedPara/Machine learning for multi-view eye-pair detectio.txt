  X  1. Introduction
Face alignment is an important requirement for a successful face recognition application. A human face in an image can be in a variety of scales, positions and poses. Without any alignment of the face entities in an image, recognition performance is very limited. An eye-pair is the image that contains a pair of eyes, and it easier than other parts of a face. Still to the best of our knowledge, there is currently only one eye-pair detection method based on the
Viola  X  Jones framework ( Castrill X n-Santana et al., 2008 ), but as we will show in this paper, this method is not very accurate. The aim of our work is to develop a system that can accurately detect eye-pairs. This will be useful to address in the future the problem of accurate face alignment and recognition.

Eye or eye-pair detection is a sub-fi eld of object detection in images. The approaches can be classi fi ed into three fundamental methods: shape-based models, feature based models and appear-ance-based models. Shape-based models depend on a geometrical model of the eyes and use this model to decide whether an image patch contains an eye. It extracts contour properties of the image patch and compares these to the model using a similarity measure.
In Kawaguchi et al. (2000) , a separability fi lter is used for feature extraction and the Hough transform is used for model fi tting.
Some researchers focus on color images in order to exploit skin color of faces. So, a color conversion algorithm is applied to the image containing a face so that the separation of skin color from the background becomes easier. After the conversion the face is detected by means of a face mask calculation. In Kalbkhani et al. (2013) a non-linear RGB to YCBCr color conversion is adopted, and an eye mapping algorithm is applied using an already created face mask to fi nd the eyes. In Huang et al. (2011) an algorithm which converts color pixels from the RGB color space to the HSL space is developed and used. Then, after some image enhancement opera-tions speci fi c to human skin, an object searching algorithm is used for fi nding eye candidates. Exploiting human-skin color as a discriminator can be very ef fi cient, provided that the background is relatively simple and different from human skin color. In another eye detection and tracking system ( Abdel-Kader et al., 2014 ), eyes are detected and tracked by a particle swarm optimi-zation based multiple template matching algorithm. In another paper, the Hough transform algorithm is used in combination with directional image fi lters previously proposed for face detection ( Maio and Maltoni, 2000 ). In Ilbeygi and Shah-Hosseini (2012) , luminance and chrominance values of colored image patches are extracted and given to a template matching algorithm to detect eyes. Shape-based eye detection models may be suitable for real-time eye-tracking applications if they require tracking only the iris and the pupil. However, they are sensitive to different rotation angles and image quality. Moreover, for obtaining more precise results, these models use more parameters to model the shape and this results in an extensive engineering effort and the application is computationally more demanding ( Hansen and Ji, 2010 ).
Feature-based methods focus on fi nding local features related to the eye. For instance, the eyebrow, the pupil and the iris are basic parts of an eye and locating these features can be helpful for locating the eye. In Kim and Dahyot (2008) , features of eyes and other facial parts (nose, mouth, etc.) of the face are extracted with the SURF algorithm ( Bay et al., 2008 ). Then these features are given to a support vector machine (SVM) ( Vapnik, 1998 ) to locate these facial parts. In Sirohey and Rosenfeld (2001) special linear and non-linear fi lters constructed from Gabor wavelets are used to detect the iris and corner features of the eyes. Then these features are further fi ltered to remove false features from the detected feature set. A voting mechanism is fi nally applied to compute the most accurate location of the iris. In Ando and Moshnyaga (2013) , integral images are utilized for face tracking, face detection, and eye detection. There, instead of eyes themselves, the area between the eyes is exploited as a discriminator from other parts of a face.
The face area is fi rst obtained by subtracting the adjacent frames of video data and then a seven segmented rectangle template is used to slide through the image which contains the face. The output of the sliding window algorithm is given and processed by an algorithm according to their integral image output values. Since that application is designed for energy-constrained environments, the algorithm it uses is relatively simple which might give inaccurate results for some environments. While feature-based methods are robust to illumination and pose changes, they usually require high-quality images ( Hansen and Ji, 2010 ).

Appearance-based models make a model from eye images by using the photometric appearance of the eyes. Since no speci priori information related to eyes is used, a suf fi cient number of training data to learn the parameters for eye detection are needed.
For the purpose of eliminating noise and reducing dimensionality, feature extraction and normalization operations to training data are usually applied. As for feature extraction techniques, principal component analysis (PCA), and edge detection methods are some of the techniques being used. After all these operations the output ( Freund and Schapire, 1995 ), neural networks and SVMs have been used. In Huang and Mariani (2000) , patches of example eye images are processed by principal component analysis (PCA) to reduce the dimensionality and make a model eye for classifying unseen image patches if they contain an eye. In Vijayalaxmi and Rao (2012) ,a
Gabor fi lter is used as a feature extractor and an SVM is used as a classi fi er. To make the fi nal detector more robust to rotations, the face images are populated by rotation, translation and mirroring operations before giving them to a Gabor fi lter to be processed and then fi nally the output of it is fed into the SVM to train the classi fi er. In the face and eye detection method in Lin et al. (1997) , some edge extraction techniques and a histogram equalization algorithm are applied to image patches before they are given to a probabilistic decision based neural network for detection. In
Motwani et al. (2004) , wavelet coef fi cients of image patches are given to a multilayer perceptron. In You-jia et al. (2010) , the output of an orthogonal wavelet analysis on image patches is given to an
SVM. The biggest advantage of the appearance-based methods is that they are applicable to all kinds of different objects, because they are based on machine learning algorithms to learn the model from training data. Therefore, they also often require almost no a priori knowledge and less engineering effort. A disadvantage is that they may need a lot of labeled data to learn a very good performing model.

In the Viola  X  Jones object detection framework ( Viola and Jones, 2004 ), the eye-pair detector ( Castrill X n-Santana et al., 2008 ) adopted an appearance-based method as well. The framework exploits Haar wavelets as object features and these features are calculated using integral images, which makes the computation still a de facto standard for general platforms where speed can be preferred over accuracy. The method is based on using a cascaded classi fi er structure using weak Haar features to build a classi To train the cascaded structure an adaptive boosting algorithm is used. In this scheme, if a training example is misclassi the detector, the weight of that example is increased so that the subsequent classi fi er is able to correct the errors made by the previous classi fi ers.

Primarily meant to be used for face detection, this framework has been extended for detecting facial parts such as eye, eye-pair, mouth, and nose. Nevertheless, this detector is not very accurate and may not be very suitable for platforms where source images are cluttered, noisy or have low-contrast. Since we aim to develop a very robust face recognition application useful for very different types of face images taken in challenging environments, we need high accuracy rather than high speed in order to minimize the recognition error caused by incorrectly aligned face images. Because of this reason, we will utilize a strong classi fi powerful feature extraction methods to increase the discrimina-tion power of the system.

Contributions : In this paper a novel eye-pair detection method, addressing the problem of face alignment, is proposed. Our aim is to build a robust application, which can deal with many variances in different images, that can also be useful for robots. The system is constructed by using a feature vector extraction method that converts an image patch to the input of a support vector machine classi fi er. We have compared fi ve different feature vector extrac-tion methods. The fi rst one is the linear restricted Boltzmann machine (RBM) ( Smolensky, 1986 ) that extracts activities of latent variables which model the data. The second one directly uses pixel-intensity values. The third method uses principal component analysis to extract eigenvalues from an image patch, and the last two feature extraction methods use the difference-of-Gaussians edge detector and the Gabor wavelength fi lter before the image patch is given as an input to a linear RBM. These fi ve feature extraction methods and the SVM classi fi er are implemented in a sliding window method to fi nd the best matching eye-pair region in a face image. The detector is trained on images we collected from the Internet for which we manually cropped the eye-pair regions. We have compared our methods to the Viola  X  Jones eye-pair detector on three different test face image datasets (with 240, 450 and 566 face images). The results show that our eye-pair detection systems consistently perform better than the state-of-the-art Viola  X  Jones eye-pair detector. For almost all test images, the eye-pair regions are located very accurately with our system. Besides, we compare our eye-pair detector application with a single eye detector that we constructed in a similar fashion to show the superiority of using one single wider rectangle which contains two eyes instead of two smaller ones.

Paper outline : This paper is organized as follows: In Section 2 , the classi fi er and feature extraction methods are described. In Section 3 , the whole eye-pair detection algorithm is explained. After that, the experimental setup and results are described in Section 4 . Section 5 discusses our fi ndings and describes some directions for future work.
 2. Classi fi er and feature extraction methods 2.1. Support vector machines
The support vector machine (SVM), invented by Vapnik and co-workers ( Vapnik, 1998; Boser et al., 1992 ), is a machine learning algorithm which is very useful for two-class pattern recognition problems ( Cristianini and Shawe-Taylor, 2000 ). The SVM algorithm assumes that the maximum margin between two classes makes the best separation. Although originally developed as a linear classi fi er, an SVM can be used with non-linear kernels to produce a non-linear classi fi er. We will shortly describe the SVM. Let D be a training dataset,
D  X f X  x i ; y i  X  ; 1 r i r n g where x i A R p are input vectors and y i A f 1 ; 1 g are binary labels.
Given an input vector x i the linear SVM outputs the following class output o i : o  X  g  X  x i  X  X  sign  X  w T x i  X  b  X  where w is the weight vector and b is the bias. To compute the weight vector w and the bias b , the SVM minimizes the cost function:
J  X  w ;  X   X  X  1 2 w T w  X  C  X  n subject to constraints: w x i  X  b Z  X  1  X  i for y i  X  X  1 and w x i  X  b r 1  X   X  i for y i  X  1 where C weighs the training error and  X  i Z 0 are slack variables. This is usually done by using the dual formulation, but because the
SVM is a famous machine learning method and not the main scope of this paper, we will not go into detail here. One possible disadvantage of this soft margin method is that it increases the number of support vectors and therefore it increases the chance of over fi tting. A recent algorithm proposes a solution to this, called separable case approximation ( Geebelen et al., 2012 ), which achieves the right separation with a decreased number of support vectors without using soft margins.

Non-linear case : Although linear separation is faster and less complex than non-linear models, it is not suitable for all kinds of data. Because of this problem, the non-linear SVM model was proposed by Boser et al. (1992) . In this case, the dot product between two input vectors that leads to a linear classi fi replaced with a non-linear kernel function that allows us to separate non-linearly separable data. Many kernel functions have been proposed ( Cristianini and Shawe-Taylor, 2000 ). The most often used kernel functions are the radial basis function (RBF):
RBF : K  X  x ; y  X  X  e  X   X   X  x y  X   X  2 ;  X  4 0 ; and the polynomial kernel: POLY : K  X  x ; y  X  X  X  x T y  X  c  X  d
Recently, to make bene fi t of both kernels' discrimination capabil-ities, a combination of both kernels given above is proposed ( A et al., 2013 ), where the kernel formula becomes POLY RBF : K  X  x ; y  X  X  X  e  X   X   X  x y  X   X  2  X  c  X  d
When using a kernel function, the decision function becomes o  X  g  X  x i  X  X  sign  X  n where the weight w j for an example x j is given by  X  j y computed by optimizing the dual objective problem of the SVM. In this research, SVM light ( Joachims, 1999 ) is used for training the
SVM classi fi er with the RBF kernel. 2.2. Linear restricted Boltzmann machines neural network model ( Smolensky, 1986 ), the purpose of which is noise elimination and dimensionality reduction of the input data. It is typically composed of an input vector v and a hidden layer h , which are connected to each other by a weight vector w .
This structure is called a bipartite graph. For the graphical depic-tion of an RBM, see Fig. 1 . The RBM learns to model the dataset by updating weights. Then the values of hidden units of the
RBM represent features, while the values of visible units represent input data.
 stochastic, in this research we use a linear implementation of the RBM. The reason we used the linear RBM is that it was able to model the data better than other implementations of the RBM in our experiments. We will now mathematically describe the work-ings of the linear RBM.
 that model the input data. The latent variables h j are extracted from the input using: h j  X  b j  X   X 
Here b j is the hidden bias value for unit j and w ij 's are weights connecting input and hidden units. The values of h j can then be given to a classi fi er to classify the input vector v . 2002 ). This method fi rst computes the h j values, as explained above. Then, it computes ^ v i values which denote the activities of reconstructed input units, and ^ h j which denotes activity values of the hidden units computed using ^ v i 's: ^ v i  X  a i  X   X  where a i 's are visual (or input) unit bias values. The learning rule for updating weights of the RBM is as follows:
 X  w where  X  is the learning rate. Visual and hidden biases are updated by
 X  a
Modeling the data is then done by training the RBM model for several epochs on the training data. 2.3. Difference-of-Gaussians fi lter algorithm that detects edges by subtraction of one blurred version of an original image from another, which is a less blurred version of the original. Let f be the image matrix, and let G 1 and G the fi rst and second Gaussian functions, which produce Gaussian matrices for convolving the image. The Gaussian function is
G  X  x ; y  X  X  1 The Gaussian blurred images are
O  X  X  f G i  X  x ; y  X  X  ; i A f 1 ; 2 g where is a convolution operation. Finally, the fi nal output image is computed by O  X  O 2 O 1 .

Blurring an image using a Gaussian convolution kernel sup-presses spatial information with high-frequency properties. Sub-tracting one blurred image from the other helps keeping spatial frequencies that are preserved in the two blurred images. So, the
DoG can be considered a low band-pass fi lter which discards all except some signi fi cant spatial frequencies that are present in the original image. A detailed analysis of this fi lter is given in Basu (2002) . 2.4. Gabor wavelets
A Gabor wavelet is a linear fi lter used for edge detection operations. It is a convolution product of a sinusoidal plane wave and a Gaussian function. The mathematical de fi nition of the is given below: g  X  x g where x 0  X  x cos  X   X  y sin  X  and y 0  X  x sin  X   X  y cos  X 
As can be seen above, it has fi ve parameters to affect the response of the fi lter. Here  X  represents the wavelength of the sinusoidal wave function,  X  represents the orientation,  X  is the phase offset, the standard deviation of the Gaussian function and  X  is the spatial aspect ratio which determines the ellipticity of the Gabor function. 3. The eye-pair detection system 3.1. The training method
The training method is divided into three parts: collecting necessary training data, creating feature vectors using a feature extraction method, and supervised training using an SVM for making a model to discriminate between eye-pair and non-eye-pair regions. 3.1.1. Image dataset
The image dataset was constructed manually at the beginning of the project by us, by collecting images containing a human face from the Internet and then by cropping the eye-pairs as positives and other parts as negatives in these images. The human faces in the images, from which eye-pair and negative image patches are cropped, are in varied positions like different yaw, pitch and roll angles, and a substantial amount of them are with spectacles, also in different sizes and colors. In addition, the faces in the images are in different zoom levels.

The fi nally used training dataset constructed from face images contains 1750 eye-pair and 5700 non-eye-pair image patches. The core number of eye-pairs to start with is 300. Then, we further populated this eye-pair dataset by adding the horizontally mirrored versions of the image patches. Second, we added cropped patches which are located one pixel away in the hori-zontal direction from the manually cropped eye-pair patches. For the non-eye-pair images, we fi rst cropped initially around 2300 image patches from non-eye-pair regions of the faces. After collecting this initial data we evaluated the system with the training method, which is shown in Fig. 2 on training images. In this process we collected the false positives and retrained the system to make it more robust. After repeating the process around 5 times, this resulted fi nally in 5700 non-eye-pair image patches. For examples of positive and negative samples, see Fig. 3 .
The cropping window that is used to pass the speci fi c part of an image to the detector is a rectangle as can be seen in the
We have chosen to use a single rectangle, because it also integrates some information from the upper part of the nose, which can make the detection more reliable. We have also compared this single rectangle to using two smaller rectangles that both sur-round a single eye. We also want to note that we use all faces in gray scales and do not make use of color information.

Since the eye-pair part of a face represents a small region of the whole face, the number of negative examples which contain non-eye-pairs should be much larger than the number of eye-pair images. Therefore the negative dataset is increased incrementally according to the detector's false positive outputs on the training images when fi nding eye-pairs in the training images as shown in Fig. 2 . 3.1.2. Creating feature vectors
In this research fi ve feature vector creation methods have been applied. The fi rst feature method uses the hidden activity values of the linear RBM, the second method uses normalized intensity values of image patches, the third method uses hidden activity values of the linear RBM using the output of the DoG fi lter as an input rather than intensity values, the fourth feature extraction method fi rst uses Gabor fi lters and then the linear RBM, and the last feature extraction method uses eigenvalues computed using principal component analysis (PCA). We will now describe how we have used these feature extraction methods on our datasets, and which parameters have been used that were found to perform best using preliminary experiments.

Hidden activities of linear RBM : In this scheme, the feature extractor is a two-layer linear RBM. The weights of the linear RBM are trained iteratively with training data (using positive and negative examples) to produce hidden activities as feature vectors.
For training the linear RBM, 60 hidden units are used and the input image resolution is set to 24 9 pixels. Some original eye-pairs and the reconstructed eye-pairs using the linear RBM on training images are shown in Fig. 4 (a) and (b), respectively. Although the reconstructed images are not perfect, they resemble the original ones quite well while reducing the dimensionality from 216 pixel values to 60 hidden unit activations. The learning rate is set to 0.035 from the start and is decreased by dividing by 1.05 for every 10 epochs. For the size of our training set, 50 epochs work well for training the linear RBM. In order to train the neural network faster, the pixel values of each gray-scale image are normalized.
Normalized pixel values : The second method is based on directly using pixel intensities. In this scheme, the feature extraction method uses a standard image resizing technique based on a linear interpolation algorithm. After resizing, the gray-scale pixel values of each image patch are normalized between 0 and 1. Since the resolution of 24 9 was shown to give noisy inputs and led to a slower detection performance, we changed the resolution to 16 6 pixels for this method.
 method is based on using the hidden activities of the linear RBM which uses the DoG fi lter output as an input. The output of the DoG fi lter is further smoothed with a noise reducer (despecling) before giving it to the linear RBM. For the DoG fi lter, the radius of the
Gaussian fi lter is set to 24 24, while the second fi lter uses a small radius of 2 2. The standard deviation of both Gaussian fi set to 1. The image resolution of the search window frame is set to 24 9. We also use 60 hidden units for the RBM in this case. is based on using the hidden activities of the linear RBM which uses the Gabor fi lter output as an input. For the Gabor fi lter, due to obtaining best performances, the wavelength and bandwidth are selected as 2 and 16 respectively. As for the orientations the angles of 0 ;  X  = 4 ;  X  = 2 ; 3  X  = 4 ;  X  ; 5  X  = 4 ; 3  X  = 2, 7  X  = selected as 1 (a square). Again 60 hidden units for the RBM are used. is based on PCA, which creates eigenvectors as a model of the training data and eigenvalues as feature vectors. For this method, to make a good comparison, we use the same parameters as the RBM, namely 60 eigenvalues and the resolution is set to 24 9. 3.1.3. Training the SVM with feature vectors
SVM classi fi er. The radial basis function kernel is used as the kernel of the SVM. The best regularization and gamma parameters, which are two important parameters to be tuned to obtain good classi fi cation results, were selected by testing the resulting eye-pair detector on images of the ORL dataset ( Samaria and Harter, 1994 ), which we used as a separate training set. The details about those parameters are given in the experimental section. 3.2. Eye-pair detection method resized to user selected prede fi ned scales by preserving the original width-to-height ratio. The algorithm is explained below. 3.2.1. Sliding windows technique resolution value is slided through the image from top to down and from left to right, to extract different regions. Then the feature vector constructed by a feature extractor on each region is given to the SVM to get a classi fi cation result (the discriminative value).
The region with the highest output of the SVM is assumed to contain the eye-pair.
 3.2.2. Using a scale of resolutions
Since the faces are not in standard scales in the images, the detector assumes a range of different scales. Also because the resolution of the eye-pair search window is fi xed, the detector changes the resolution of the image in which it looks for an eye-pair. For an illustration of scale changes of the eye-pair detector, see Fig. 5 . The resolution of the image containing an eye-pair is rescaled such that the ratio of width of the main image to width of the search window changes from 2 to 1 with steps of 0.125 while preserving the resolution of the main image. For a detailed explanation of the method, see Algorithm 1 . We move the sliding window with 2 pixels in horizontal and vertical directions. Going over a complete image with different scales and locating the eye-pair costs around 1.3 s with our method.

Algorithm 1. Eye pair detection ( w , inc w , inc x , inc 1: w is rescaled width of main image being scanned, w f h 2: Set x and y to zero; 3: while w r max w do 4: Calculate original aspect ratio: r  X  h org 5: Calculate rescaled height: h  X  wr 6: Rescale the original image to w h : I  X  w ; h  X   X  R  X  I 7: while y r max y do 8: while x r max x do 9: Get image patch at x and y from main image: 10: Process the patch with the feature extractor: v  X  F  X  I 11: Get classi fi cation value from the SVM: d  X  SVM  X  v  X  12: Store this value with x , y and w values in a list: 13: Increment x value: x  X  x  X  inc x 14: end while 15: Increment y value: y  X  y  X  inc y 16: end while 17: Update width size: w  X  w  X  inc w 18: end while 19: Return x , y and width with the highest discriminant value: 3.3. The single eye detector
In this section, the single eye detector for comparison with the eye-pair detector in terms of accuracy and speed performance is presented. Since we already explained our eye-pair detection method in detail and the detection method of the single eye detector is very similar, only the differences will be given below. 3.3.1. Training the single eye detector
Collecting the training data : Differently from the eye-pair detector a single eye detector searches and fi nds eyes separately. Because of this fact, we collected around 300 eye images from the main face dataset we constructed for the eye-pair detector. For some eye examples, see Fig. 6 . Then, similar to our eye-pair detection training method, we collected mirrored versions of the original eyes. Then we created an initial amount of non-eye images from the previously constructed non-eye-pair negative set. Next, we collected negatives by testing the detector on the face dataset we used for training the eye-pair detector and we kept on adding false positives to the negative dataset. In this way we fi aggregated 7800 training images for our single eye detector.
Image resolution and feature extraction method : As for cropping resolution we used 14 10 (hence 140 pixels) and for feature extraction we selected the linear RBM method for this task as it proved to perform best from our eye-pair detection experiments. For the linear RBM, 50 hidden units are used for training. 3.3.2. Detection method of single eye detector
Since the eye detector fi nds eyes separately, it returns always two values for the two best-matching eyes, which is different from the eye-pair detector that returns one detected eye-pair. To prevent ambiguity that the same eye with a slightly different resolution and position appears as the second best matching eye patch, we use a distance condition so that the second best matching eye which does not ful fi ll this is removed. This condition is given below: d  X  c where c eye 1 and c eye 2 are the center points of two eyes, d  X  X  is a function which computes the Euclidean distance between two points and l i is the maximum distance from the center to the border of the i th eye frame with the angle of the line which connects the center points of two eyes. This condition eliminates the second found eye when it is too close to the best matching eye patch, since they are almost surely the same eye detected twice. Here, l i is given by l  X  X  w eye i = 2  X  n sin  X  a  X  ; and the center points are computed as follows: c c In the equation above a , the angle of the slope between two center points is computed from m , where m is the slope of the line which connects two center points: m  X  c eye 1  X  y  X  c eye 2  X  y  X  c a  X  arctan  X  m  X  3.3.3. Evaluation method
To compare the single eye detector to the eye-pair detector, we created a bounding box using the information from two detected eyes. Some example bounding box pictures are given in Fig. 7 . 4. Experimental setup and results
In this section the datasets that are used in the experiments, the evaluation metrics, and the results obtained by the different methods are presented. We have also compared our methods with the Viola  X  Jones eye-pair detection algorithm. 4.1. Datasets ( Nordstr X m et al., 2004 ), the Caltech 1 and the Indian 2 are used. The images in these datasets (except for the ORL dataset) were not seen before for training our eye-pair detection system.
The ORL dataset, on the other hand, is used to evaluate training parameters of our method. We have used all face images in these datasets and manually cropped the eye-pair regions to be able to compute the system performances. The ORL face dataset was created at AT&amp; T labs of the University of Cambridge. It involves 400 faces obtained from 40 individuals. The IMM face database was created by the Technical University of Denmark. It contains 240 images obtained from 40 individuals. The Caltech face database was created by Markus Weber at the California Institute of Technology. It contains 450 images obtained from 27 individuals with different lighting/expressions/backgrounds. The Indian face database was created in the campus of the Indian Institute of
Technology Kanpur. It contains 566 images obtained from 40 individuals. Some example images of the ORL, IMM, Caltech and Indian datasets are provided in Figs. 8 and 9 . The images in the
IMM, Caltech and Indian datasets were cropped manually before giving them to the eye-pair detector. 4.2. Evaluation
To evaluate the results of automatically detected eye-pairs, an overlapping windows ratio (OWR) metric, which calculates the fraction of matching pixels between automatically detected and manually cropped eye-pair regions, is used. The detection perfor-mance (OWR) is de fi ned by OWR  X  r ffiffiffiffiffiffiffiffiffiffi where r is the matched pixel count, m is the pixel count of the manually annotated (  X  true  X  )eye-pairregionand a is the pixel count of the eye-pair region which is detected automatically by the system. The minimum OWR is 0 and the best obtainable performance is 1, when the windows have equal size and completely overlap. Some examples of face images in which detected eye-pairs have an OWR higher or lower than 0.75 are shown as rectangles in Fig. 10 (a) and (b), respectively.
 We will compute the percentage of test images that have an OWR above a speci fi c threshold. Finally, we will also report the average OWR on all test images and the standard error. 4.3. Results
The SVM needs two parameters to be set, which we optimized during detection mode on the training images. The parameters which worked best are  X   X  0 : 4 and C  X  6 for RBM, DoG, PCA and Gabor,  X   X  0 : 2 and C  X  4 for the pixel-based method, and C  X  7 and  X  0 : 5 for the single eye detection method.

The summary of results, as percentage of retrieved correct eye-pairs (recall) with a minimum OWR of 0.75 and average of OWR results are given in Tables 1 and 2 , respectively. Additionally, we made a comparison of speed performances of our eye-pair detector and the single eye detector. Our eye-pair detector detects the eye-pair within 1.3 s on average. The single eye detector can be used to detect the eye-pair within 3.6 s. Both methods are signi fi cantly slower than the Viola  X  Jones eye-pair detector, which is optimized for speed, rather than accuracy.

As can be seen from Table 1 , the linear RBM method and PCA give the best overall results. The PCA method performs very similar to the linear RBM method and the performance differences of RBM and PCA are statistically insigni fi cant. The pixel-based method closely follows the linear RBM and the PCA methods, except for the IMM dataset in which the pixel-based method performs signi fi cantly worse. The IMM dataset contains many rotated faces, with which the dimensionality reduction methods seem to cope better. It can also be seen that the method that uses the DoG fi lter performs much worse than PCA and the linear RBM method. On the Indian dataset the DoG method performs even worse than the Viola  X  Jones eye-pair detector. This indicates that the low-contrast and noisy nature of the images of the Indian dataset hinders the DoG fi lter to perform well. Furthermore, our images have a low resolution and the DoG fi lter cannot cope well with that. On the other hand, the method that uses a Gabor shows somehow varying results. It remarkably outperforms the other methods signi fi cantly for the Indian dataset. This is because the Gabor fi lter increases the contrast which is very helpful for this dataset. The Gabor fi lter with the SVM gives a close performance to the best feature methods for the IMM dataset. However, for the
Caltech dataset, where the illumination properties of the images vary a lot, the Gabor fi lter diminishes the detection performance of the system a lot. This seems to suggest that highly illuminated images processed by the Gabor fi lter lose some important infor-mation. Finally, our results clearly show that the use of a single eye detector for fi nding an eye-pair is outperformed by the eye-pair detector when both use the RBM feature extraction method. OWR results and the recall performance results show a correlation.
Table 2 shows that the average OWR on all datasets with our best methods is always larger than 0.78, which shows our methods reliably detect the eye-pairs. Especially on IMM and Caltech, the detection accuracies are very high.
 the results of RBM, pixel-based methods and Viola  X  Jones detector when we let the OWR threshold increase from 0.0 to 1.0 in
Figs. 11  X  13 , for the IMM, the Caltech and the Indian dataset, respectively.
 performs similar to the pixel-based method, except for the threshold area between 0.7 and 0.9. In this area, the RBM method outperforms the pixel-based method. The Viola  X  Jones eye-pair detector performs much worse than these two methods. The
Viola  X  Jones detector often fails to get close to an eye-pair at all with around 18% misses, which is shown by the percentage of retrieved eye-pairs with a low OWR threshold.

In Fig. 12 , it can be seen that the RBM performs better than the pixel-based method on the Caltech dataset between the thresh-olds of 0 and 0.9. After the 0.9 threshold, it performs a bit worse than the pixel-based method. The Viola  X  Jones eye-pair detector with a threshold larger than 0.9 performs similar to the RBM method and somewhat worse than the pixel-based method.
However, just as with the IMM dataset, the Viola  X  Jones eye-pair detector quite often fails to fi nd an eye-pair at all. The rough proportion of undetected eye-pairs of the Viola  X  Jones eye-pair detector is 20%.

In Fig. 13 , we can see that the RBM and the pixel-based method perform equally well throughout the whole threshold area on the
Indian dataset. After the threshold of 0.85, they perform some-what less well than the Viola  X  Jones eye-pair detector, but with lower OWR thresholds the Viola  X  Jones detector performs much worse than our methods. The difference of undetected eyepairs between the Viola  X  Jones eye-pair detector and our best method is around 27% until the OWR threshold of 0.4.

To summarize, all these results show that RBM and PCA are the best methods and perform much better than the Viola  X  Jones eye-pair detector. Visual inspection of images with the SVM discrimi-nant value on the red channel revealed that the midpoint between the eyes is well detected with very rare occasions of maximum values outside the eye-pair region. 5. Conclusion
Eye-pair detection is an important step to align different face images for improving a face recognition application. Because of illumination effects, non-rigidity of human eyes caused by ocular muscles and eyelids and also different poses of human faces, the problem is quite dif fi cult to solve. In this paper we presented and compared different eye-pair detection systems which consist of different feature extraction methods and a support vector machine classi fi er. We explained how the methods are trained and how they are combined with a sliding window to detect an eye-pair.
The experimental results showed that the linear RBM and the use of principal component analysis give the best results. These two methods generally give more reliable results and they seem to be less sensitive to noisy and low-contrasted inputs. The use of the pixel-based method gives sometimes better results than the other methods on particular images. However, its results have higher variance, indicating that it is less reliable than the RBM and PCA methods. The use of the difference-of-Gaussians fi lter decreases the performance and with low-contrast images (like in the Indian dataset), its performance can be even quite bad. The Gabor results in much better performance levels than the other methods for low contrast and low illuminated images (Indian dataset), but loses its strength signi fi cantly for highly illuminated images (Caltech dataset).

The comparison of our application with the Viola  X  Jones eye-pair detector showed that the Viola  X  Jones eye-pair detector per-forms much worse on all datasets compared to the RBM, PCA, Gabor fi lter, and pixel-based methods. This may be explained by the low information capacity of the Haar features, which are used in the Viola  X  Jones framework. Finally, the performance of the single eye detector is also much worse than the performance of the eye-pair detector. This con fi rms our hypothesis that eye-pair detection can be more accurate, because more pixel information can be used.

We will now summarize our main fi ndings. First, using a dimensionality reduction method such as the linear RBM or PCA improves the robustness of the system and lowers the false positive rate. Second, the size of the training dataset directly affects the system's performance. We noticed that increasing the training data with mirrored versions of non-frontal eye-pairs and shifted versions of the cropped eye-pairs was important to get to the very accurate approach proposed here. Furthermore, adding a substantial number of additional negative samples according to the false positive outputs of the system also makes the detector much more reliable and accurate. Third, we want to note that although we trained our detector for eye-pair detec-tion, our approach can be generalized for creating any object detection method, such as face detection and pedestrian detection since no eye-speci fi c modeling was applied in the algorithms presented here.

As future work, we are interested in improving our application even further. Our algorithm generates very few false positives, however, it is always tested on images including human faces. We also want to test the system on natural and indoor images without faces, and to tune the decision threshold in order to minimize false alarms. To increase the accuracy of our application, a combination of different detectors trained on different datasets could be useful. Furthermore, instead of using the 2-layered shallow RBM, a multi-layered deep RBM (deep architecture) might perform better. Finally, we want to use our detector to align faces in a data-mining effort and subsequently to develop a complete face recognition system.
 References
