 Matching pairs of objects is a fundamental operation of unsupervised learning. For instance, we might want to match a photo with a textual description of a person, a map with a satellite image, or a music score with a music performance. In those cases it is desirable to have a compatibility function which determines how one set may be translated into the other. For many such instances we may be able to design a compatibility score based on prior knowledge or to observe one based on the co-occurrence of such objects.
 In some cases, however, such a match may not exist or it may not be given to us beforehand. That is, while we may have a good understanding of two sources of observations, say X and Y , we may not understand the mapping between the two spaces. For instance, we might have two collections of documents purportedly covering the same content, written in two different languages. Here it should be our goal to determine the correspondence between both sets and to identify a mapping between the two domains. In the following we present a method which is able to perform such matching without the need of a cross-domain similarity measure.
 Our method relies on the fact that one may estimate the dependence between sets of random variables even without knowing the cross-domain mapping. Various criteria are available. We choose the Hilbert Schmidt Independence Criterion between two sets and we maximize over the permutation group to find a good match. As a side-effect we obtain an explicit representation of the covariance. We show that our method generalizes sorting. When using a different measure of dependence, namely an approximation of the mutual information, our method is related to an algorithm of [1]. Finally, we give a simple approximation algorithm for kernelized sorting. 1.1 Sorting and Matching The basic idea underlying our algorithm is simple. Denote by X = { x 1 ,...,x m }  X  X and Y = { y 1 ,...,y m } X  That is, we would like to find some permutation  X   X   X  m on m terms, that is such that the pairs Z (  X  ) := ( x i ,y  X  ( i ) ) for 1  X  i  X  m correspond to dependent random variables. Here 1 m  X  R m is the vector of all ones. We seek a permutation  X  such that the mapping x i  X  y  X  ( i ) and its converse mapping from y to x are simple. Denote by D ( Z (  X  )) a measure of the dependence between x and y . Then we define nonparametric sorting of X and Y as follows This paper is concerned with measures of D and approximate algorithms for (2). In particular we will investigate the Hilbert Schmidt Independence Criterion and the Mutual Information. Let sets of observations X and Y be drawn jointly from some probability distribution Pr xy . The Hilbert Schmidt Independence Criterion (HSIC) [2] measures the dependence between x and y by computing the norm of the cross-covariance operator over the domain X  X  Y in Hilbert Space. It can be shown, provided the Hilbert Space is universal, that this norm vanishes if and only if x and y are independent. A large value suggests strong dependence with respect to the choice of kernels. Formally, let F be the Reproducing Kernel Hilbert Space (RKHS) on X with associated kernel k : X  X  X  X  R and feature map  X  : X  X  F . Let G be the RKHS on Y with kernel l and feature map  X  . The cross-covariance operator C xy : G 7 X  F is defined by [3] as where  X  x = E [  X  ( x )] ,  X  y = E [  X  ( y )] , and  X  is the tensor product. HSIC, denoted as D , is then term of kernels HSIC can be expressed as where E xx 0 yy 0 is the expectation over both ( x,y )  X  Pr xy and an additional pair of vari-ables ( x 0 ,y 0 )  X  Pr xy drawn independently according to the same law. Given a sample Z = { ( x 1 ,y 1 ) ,..., ( x m ,y m ) } of size m drawn from Pr xy an empirical estimate of HSIC is where K,L  X  R m  X  m are the kernel matrices for the data and the labels respectively, i.e. K ij = space. Finally,  X  K := HKH and  X  L := HLH denote the centered versions of K and L respectively. Note that (5) is a biased estimate where the expectations with respect to x,x 0 ,y,y 0 have all been replaced by empirical averages over the set of observations. 2.1 Kernelized Sorting Previous work used HSIC to measure independence between given random variables [2]. Here we use it to construct a mapping between X and Y by permuting Y to maximize dependence. There are several advantages in using HSIC as a dependence criterion. First, HSIC satisfies concentration of measure conditions [2]. That is, for random draws of observation from Pr xy , HSIC provides values which are very similar. This is desirable, as we want our mapping to be robust to small changes. Second, HSIC is easy to compute, since only the kernel matrices are required and no knowledge into the dependence estimation process. The consequence is that we are able to generate a family of methods by simply choosing appropriate kernels for X and Y .
 Lemma 1 The nonparametric sorting problem is given by  X   X  = argmax  X   X   X  Proof We only need to establish that H X  =  X H since the rest follows from the definition of (5). Note that since H is a centering matrix, it has the eigenvalue 0 for the vector of all ones and the eigenvalue 1 for all vectors orthogonal to that. Next note that the vector of all ones is also an eigen-vector of any permutation matrix  X  with  X  1 = 1 . Hence H and  X  matrices commute.
 Next we show that the objective function is indeed reasonable: for this we need the following in-equality due to Polya, Littlewood and Hardy: Lemma 2 Let a,b  X  R m where a is sorted ascendingly. Then a &gt;  X b is maximized for  X  = argsort b . Lemma 3 Let X = Y = R and let k ( x,x 0 ) = xx 0 and l ( y,y 0 ) = yy 0 . Moreover, assume that x is sorted ascendingly. In this case (5) is maximized by either  X  = argsort y or by  X  = argsort  X  y . Proof Under the assumptions we have that  X  K = Hxx &gt; H and  X  L = Hyy &gt; H . Hence we may rewrite the objective as ( Hx ) &gt;  X  ( Hy ) 2 . This is maximized by sorting Hy ascendingly. Since the centering matrix H only changes the offset but not the order this is equivalent to sorting y . We have two alternatives, since the objective function is insensitive to sign reversal of y . This means that sorting is a special case of kernelized sorting, hence the name. In fact, when solving the general problem, it turns out that a projection onto the principal eigenvectors of  X  K and  X  L is a good initialization of an optimization procedure. 2.2 Diagonal Dominance particular in the case of document analysis. This is the case since kernel matrices on texts tend to be diagonally dominant: a document tends to be much more similar to itself than to others. In this case the O (1 /m ) bias of (5) is significant. Unfortunately, the minimum variance unbiased estimator [2] does not have a computationally appealing form. This can be addressed as follows at the expense of a slightly less efficient estimator with a considerably reduced bias: we replace the expectations (4) by sums where no pairwise summation indices are identical. This leads to the objective function This estimator still has a small degree of bias, albeit significantly reduced since it only arises from the product of expectations over (potentially) independent random variables. Using the shorthand  X  K been removed we arrive at the expression ( m  X  1)  X  2 tr H  X  LH  X  K . The advantage of this term is that it can be used as a drop-in replacement in Lemma 1. 2.3 Mutual Information An alternative, natural means of studying the dependence between random variables is to compute since it requires density estimation. However, if we assume that x and y are jointly normal in the Reproducing Kernel Hilbert Spaces spanned by the kernels k,l and k  X  l we can devise an effec-tive approximation of the mutual information. Our reasoning relies on the fact that the differential entropy of a normal distribution with covariance  X  is given by Since the mutual information between random variables X and Y is I ( X,Y ) = h ( X ) + h ( Y )  X  h ( X,Y ) we will obtain maximum mutual information by minimizing the joint entropy h ( X,Y ) . Using the Gaussian upper bound on the joint entropy we can maximize a lower bound on the mutual information by minimizing the joint entropy of J (  X  ) := h ( X,Y ) . By defining a joint kernel on X  X  Y via k (( x,y ) , ( x 0 ,y 0 )) = k ( x,x 0 ) l ( y,y 0 ) we arrive at the optimization problem Note that this is related to the optimization criterion proposed by Jebara [1] in the context of sorting via minimum volume PCA. What we have obtained here is an alternative derivation of Jebara X  X  criterion based on information theoretic considerations. The main difference is that [1] uses the setting to align bags of observations by optimizing log | HJ (  X  ) H | with respect to re-ordering within each of the bags. We will discuss multi-variable alignment at a later stage.
 In terms of computation (8) is considerably more expensive to optimize. As we shall see, for the optimization in Lemma 1 a simple iteration over linear assignment problems will lead to desirable solutions, whereas in (8) even computing derivatives is a computational challenge. DC Programming To find a local maximum of the matching problem we may take recourse to a well-known algorithm, namely DC Programming [4] which in machine learning is also known as the Concave Convex Procedure [5]. It works as follows: for a given function f ( x ) = g ( x )  X  h ( x ) , where g is convex and h is concave, a lower bound can be found by This lower bound is convex and it can be maximized effectively over a convex domain. Subsequently one finds a new location x 0 and the entire procedure is repeated.
 Lemma 4 The function tr  X  K X  &gt;  X  L X  is convex in  X  .
 function as V  X U &gt; 2 which is clearly a convex quadratic function in  X  .
 Note that the set of feasible permutations  X  is constrained in a unimodular fashion, that is, the set has only integral vertices, namely admissible permutation matrices. This means that the following procedure will generate a succession of permutation matrices which will yield a local maximum for the assignment problem: Here we may choose  X  = 1 in the last step to ensure integrality. This optimization problem is well known as a Linear Assignment Problem and effective solvers exist for it [6].
 Lemma 5 The algorithm described in (11) for  X  = 1 terminates in a finite number of steps. We know that the objective function may only increase for each step of (11). Moreover, the solution set of the linear assignment problem is finite. Hence the algorithm does not cycle.
 Nonconvex Maximization When using the bias corrected version of the objective function the problem is no longer guaranteed to be convex. In this case we need to add a line-search procedure is quadratic in  X  we only need to check whether the search direction remains convex in  X  ; otherwise we may maximize the term by solving a simple linear equation.
 Initialization Since quadratic assignment problems are in general NP hard we may obviously not hope to achieve an optimal solution. That said, a good initialization is critical for good estimation performance. This can be achieved by using Lemma 3. That is, if  X  K and  X  L only had rank-1, the problem could be solved by sorting X and Y in matching fashion. Instead, we use the projections onto the first principal vectors as initialization in our experiments.
 Relaxation to a constrained eigenvalue problem Yet another alternative is to find an approximate solution of the problem in Lemma 1 by solving Here the matrix M =  X  K  X   X  L  X  R m 2  X  m 2 is given by the outer product of the constituting kernel matrices,  X   X  R m 2 is a vectorized version of the permutation matrix  X  , and the constraints imposed by A and b amount to the polytope constraints imposed by  X  m . This is essentially the approach proposed by [7] in the context of balanced graph matching, albeit with a suboptimal optimization procedure. Instead, one may use the exact algorithm proposed by [8].
 The problem with the relaxation (12) is that it does not scale well to large estimation problems (the size of the optimization problem scales O ( m 4 ) ) and that the relaxation does not guarantee a feasible solution which means that subsequent projection heuristics need to be found. Hence we did not pursue this approach in our experiments. A natural extension is to align several sets of observations. For this purpose we need to introduce a multivariate version of the Hilbert Schmidt Independence Criterion. One way of achieving this goal is to compute the Hilbert Space norm of the difference between the expectation operator for the joint distribution and the expectation operator for the product of the marginal distributions. Formally, let there be T random variables x i  X  X i which are jointly drawn from some distribution p ( x 1 ,...,x m ) . Moreover, denote by k i : X i  X  X i  X  R the corresponding kernels. In this case we can define a kernel on X 1  X  ...  X  X T by k 1  X  ...k T . The expectation operator with respect to the joint distribution and with respect to the product of the marginals is given by [2] respectively. Both terms are equal if and only if all random variables are independent. The squared difference between both is given by which we refer to as multiway HSIC. A biased empirical estimate of the above is obtained by re-placing sums by empirical averages. Denote by K i the kernel matrix obtained from the kernel k i on where T t =1  X  denotes elementwise product of its arguments (the  X .* X  notation of Matlab). To apply this to sorting we only need to define T permutation matrices  X  i  X   X  m and replace the kernel matrices K i by  X  &gt; i K i  X  i .
 Without loss of generality we may set  X  1 = 1 , since we always have the freedom to fix the order of one of the T sets with respect to which the other sets are to be ordered. In terms of optimization the same considerations as presented in Section 3 apply. That is, the objective function is convex in the permutation matrices  X  i and we may apply DC programming to find a locally optimal solution. The experimental results for multiway HSIC can be found in the appendix. To investigate the performance of our algorithm (it is a fairly nonstandard unsupervised method) we applied it to a variety of different problems ranging from visualization to matching and estimation. In all our experiments, the maximum number of iterations used in the updates of  X  is 100 and we terminate early if progress is less than 0 . 001% of the objective function. 5.1 Data Visualization In many cases we may want to visualize data according to the metric structure inherent in it. In particular, we want to align it according to a given template, such as a grid, a torus, or any other fixed structure. Such problems occur when presenting images or documents to a user. While there is a large number of algorithms for low dimensional object layout (self organizing maps, maximum variance unfolding, local-linear embedding, generative topographic map, . . . ), most of them suffer from the problem that the low dimensional presentation is nonuniform. This has the advantage of revealing cluster structure but given limited screen size the presentation is undesirable. Instead, we may use kernelized sorting to align objects. Here the kernel matrix L is given by the similarity measure between the objects x i that are to be aligned. The kernel K , on the other hand, denotes the similarity between the locations where objects are to be aligned to. For the sake of simplicity we used a Gaussian RBF kernel between the objects to laid out and also between the inverse median of k x  X  x 0 k 2 such that the argument of the exponential is O (1) . Our choice of the Gaussian RBF kernel is likely not optimal for the specific set of observations (e.g. SIFT feature extraction followed by a set kernel would be much more appropriate for images). That said we want to emphasize that the gains arise from the algorithm rather than a specific choice of a function class . We obtained 284 images from http://www.flickr.com which were resized and downsampled to 40  X  40 pixels. We converted the images from RGB into Lab color space, yielding 40  X  40  X  3 dimensional objects. The grid, corresponding to X is a  X  X IPS 2008 X  letters on which the images are to be laid out. After sorting we display the images according to their matching coordinates (Figure 1). We can see images with similar color composition are found at proximal locations. We also lay out the images (we add 36 images to make the number 320) into a 2D grid of 16  X  20 mesh using kernelized sorting. For comparison we use a Self-Organizing Map (SOM) and a Generative Topographic Mapping (GTM) and the results are shown in the appendix. Although the images are also arranged according to the color grading, the drawback of SOM (and GTM) is that it creates blank spaces in the layout. This is because SOM maps several images into the same neuron. Hence some neurons may not have data associated with them. While SOM is excellent in grouping similar images together, it falls short in exactly arranging the images into 2D grid. 5.2 Matching To obtain more quantifiable results rather than just generally aesthetically pleasing pictures we apply our algorithm to matching problems where the correct match is known.
 Image matching: Our first test was to match image halves. For this purpose we used the data from the layout experiment and we cut the images into two 20  X  40 pixel patches. The aim was to find an alignment between both halves such that the dependence between them is maximized. In other words, given x i being the left half of the image and y i being the right half, we want to find a permutation  X  which lines up x i and y i .
 This would be a trivial undertaking when being able to compare the two image halves x i and y i . While such comparison is clearly feasible for images where we know the compatibility function, it may not be possible for generic objects. The figure is presented in the appendix. For a total of 320 images we recovered 140 pairs. This is quite respectable given that chance level would be 1 correct pair (a random permutation matrix has on expectation one nonzero diagonal entry).
 Estimation In a next experiment we aim to determine how well the overall quality of the matches is. That is, whether the objects matched share similar properties. For this purpose we used binary, multi-class, and regression datasets from the UCI repository http://archive.ics.uci.edu/ml and the LibSVM site http://www.csie.ntu.edu.tw/  X  cjlin/libsvmtools .
 In our setup we split the dimensions of the data into two sets and permute the data in the second set. The so-generated two datasets are then matched and we use the estimation error to quantify the quality of the match. That is, assume that y i is associated with the observation x i . In this case we compare y i and y  X  ( i ) using binary classification, multiclass, or regression loss accordingly. To ensure good dependence between the subsets of variables we choose a split which ensures corre-lation. This is achieved as follows: we pick the dimension with the largest correlation coefficient as a reference. We then choose the coordinates that have at least 0 . 5 correlation with the reference and split those equally into two sets, set A and set B. We also split the remainder coordinates equally into the two existing sets and finally put the reference coordinate into set A. This ensures that the set B of dimensions will have strong correlation with at least one dimension in the set A. The listing of the set members for different datasets can be found in the appendix.
 The results are summarized in Table 1. As before, we use a Gaussian RBF kernel with median adjustment of the kernel width. To obtain statistically meaningful results we subsample 80% of the data 10 times and compute the error of the match on the subset (this is done in lieu of cross-validation since the latter is meaningless for matching). As baseline we compute the expected performance of random permutations which can be done exactly. Finally, as reference we use SVM classification / regression with results obtained by 10-fold cross-validation. Matching is able to retrieve significant information about the labels of the corresponding classes, in some cases performing as well as a full classification approach.
 Multilingual Document Matching To illustrate that kernelized sorting is able to recover nontrivial similarity relations we applied our algorithm to the matching of multilingual documents. For this purpose we used the Europarl Parallel Corpus. It is a collection of the proceedings of the Euro-pean Parliament, dating back to 1996 [9]. We select the 300 longest documents of Danish (Da), Dutch (Nl), English (En), French (Fr), German (De), Italian (It), Portuguese (Pt), Spanish (Es), and Swedish (Sv). The purpose is to match the non-English documents (source languages) to its English translations (target language). Note that our algorithm does not require a cross-language dictionary. In fact, one could use kernelized sorting to generate a dictionary after initial matching has occurred. In keeping with the choice of a simple kernel we used standard TF-IDF (term frequency -inverse document frequency) features of a bag of words kernel. As preprocessing we remove stopwords (via NLTK) and perform stemming using http://snowball.tartarus.org . Finally, the feature vectors are normalized to unit length in term of ` 2 norm. Since kernel matrices on documents are notoriously diagonally dominant we use the bias-corrected version of our optimization problem. As baseline we used a fairly straightforward means of document matching via its length. That is, longer documents in one language will be most probably translated into longer documents in the other language. This observation has also been used in the widely adopted sentence alignment method [10]. As a dictionary-based alternative we translate the documents using Google X  X  trans-lation engine http://translate.google.com to find counterparts in the source language. Smallest distance matches in combination with a linear assignment solver are used for the matching. The experimental results are summarized in Table 2. We describe a line search procedure in Section 3. In practice we find that fixing  X  at a given step size and choosing the best solution in terms of appendix. Low matching performance for the document length-based method might be due to small variance in the document length after we choose the 300 longest documents. The dictionary-based method gives near-to-perfect matching performance. Further in forming the dictionary, we do not perform stemming on English words and thus the dictionary is highly customized to the problem at hand. Our method produces results consistent to the dictionary-based method with notably low performance for matching German documents to its English translations. We conclude that the difficulty of German-English document matching is inherent to this dataset [9]. Arguably the results are quite encouraging as our method uses only a within class similarity measure while still matches more than 2 / 3 of what is possible by a dictionary-based method. In this paper, we generalized sorting by maximizing the dependency between matched pairs or observations by means of the Hilbert Schmidt Independence Criterion. This way we are able to perform matching without the need of a cross-domain similarity measure. The proposed sorting visualization to image and multilingual document matching and estimation. Further examples of kernelized sorting and of reference algorithms are given in the appendix.
 Acknowledgments NICTA is funded through the Australian Government X  X  Backing Australia X  X  Ability initiative, in part through the ARC.This research was supported by the Pascal Network. Parts of this work were done while LS and AJS were working at NICTA.
 [1] T. Jebara. Kernelizing sorting, permutation, and alignment for minimum volume PCA. In [2] A.J. Smola, A. Gretton, L. Song, and B. Sch  X  olkopf. A hilbert space embedding for distri-[3] K. Fukumizu, F. R. Bach, and M. I. Jordan. Dimensionality reduction for supervised learning [4] T. Pham Dinh and L. Hoai An. A D.C. optimization algorithm for solving the trust-region [5] A.L. Yuille and A. Rangarajan. The concave-convex procedure. Neural Computation , 15:915 X  [6] R. Jonker and A. Volgenant. A shortest augmenting path algorithm for dense and sparse linear [7] T. Cour, P. Srinivasan, and J. Shi. Balanced graph matching. In B. Sch  X  olkopf, J. Platt, and [8] W. Gander, G.H. Golub, and U. von Matt. A constrained eigenvalue problem. In Linear [9] P. Koehn. Europarl: A parallel corpus for statistical machine translation. In Machine Transla-[10] W. A. Gale and K. W. Church. A program for aligning sentences in bilingual corpora. In
