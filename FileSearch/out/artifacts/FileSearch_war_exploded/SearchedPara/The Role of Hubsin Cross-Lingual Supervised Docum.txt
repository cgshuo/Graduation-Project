 Text mining has always been one of the core data mining tasks, not surprisingly, as we use language to express our understanding of the world around us, encode knowledge and ideas. Analyzing textual data across a variety of sources can lead to some deep and potentially useful insights.

The use of internet has spawned vast amounts of textual data, even more so now with the advent of Web 2.0 and the increased amount of user-generated content. This data, however, is expressed in a multitude of diff erent languages. There is a high demand for effective and efficient cross-language information retrieval tools, as they allow the users to access potentially relevant information that is written in languages they are not familiar with.

Nearest neighbor approaches are common both in text classification [1][2][3] and document retrieval [4][5][6], which is not surprising given both the simplicity and the effectiveness of most k NN methods. Nearest neighbor methods can be employed both at the document level or at the word level.

The curse of dimensionality is known to affect the k -nearest neighbor methods in clearly negative ways. The distances concen trate [7] and uncovering relevant examples becomes more difficult. Additionally, some examples have a tendency to become hubs , i.e. very frequent nearest neighbors [8]. Though this may not in itself sound like a se-vere limitation, it turns out to be quite detrimental in practice. Namely, the hubness of particular documents depends more on data preprocessing, feature selection, normal-ization and the similarity measure than on the actual perceived semantic correlation between the document and its reverse nearest neighbors. In other words, the semantics of similarity is often either completely broken or severely compromised around hubs.
Textual data is high-dimensional and the impact of hubness on various text mining tasks involving nearest neighbor reasoning needs to be closely evaluated.

In this paper we examined the hub structur e of an aligned bi-lingual document cor-pus, over a set of 14 different binary categorization p roblems. We will show that there is a high correlation between the hub structure in different language representations, but this correlation vanishes when using the common semantic representation. This similar-ity in the k NN graph topology can be exploited for improving the system performance and we demonstrate this by proposing a hubness-aware instance weighting scheme for the canonical correlation analysis [9]. 2.1 Emergence of Hubs The concept of hubs is probably most widely known from network analysis [10] and the hubs-and-authorities (HITS) algorithm [ 11] which was a precursor to PageRank in link analysis. However, hubs arise naturally in other domains as well, as for instance the protein interaction networks [12]. Hubness is a common property of high-dimensional data which has been correlated with the distance concentration phenomenon. Any in-trinsically high-dimensional data with meaningful distribution centers ought to exhibit some degree of hubness [8][13][14]. The phenomenon has been most thoroughly exam-ined in the music retrieval community [15][16][17]. The researchers had noticed that some songs were constantly being retrieved by the system, even though they were not really relevant for the queries. The hubness in audio data is still an unresolved issue. Similar phenomena in textual data have received comparatively little attention. Denote by N k ( x i ) the total number of occurrences of a neighbor point x i .Ifthe N k ( x i ) is very much higher than k , we will say that x i is a hub, and if it is much lower than k , we will say that x i is an orphan or anti-hub . In case of labeled data, we can further decompose the total occurrence frequency as follows: N k ( x i )= GN k ( x i )+ BN k ( x i ) ,where GN k ( x i ) and BN k ( x i ) represent the number of good and bad k -occurrences, respectively. An occurrence is said to be good if the labels of neighbor points match and bad if there is a mismatch. Bad hubness is, obviously, closely related to the misclassification rates in k NN methods.

In intrinsically high-dimensional data, the entire distribution of k -neighbor occur-rences changes and becomes highly skewed. 1 Not only does this result in some exam-ples being frequently retrieved in k NN sets, but also in that most examples never occur as neighbors and are in fact unintentionally ignored by the system. Only a subset of the original data actually participates in the learning process. This subset is not a carefully selected one, so such implicit data reduction usually induces an information loss.
Furthermore, it is advisable to consider not only bad hubness but also the detailed neighbor occurrence profiles, by taking int o account the class-specific neighbor occur-rences. The occurrence frequency of a neighbor point x i in neighborhoods of points from class c  X  C is denoted by N k,c ( x i ) and will be referred to as class hubness .
Several hubness-aware classification methods have recently been proposed in order to reduce the negative influence of bad hubs on k NN classification (hw-k NN [8], h-FNN [18], NHBNN [19], HIKNN [20]).

Apart from classification, data hubness has also been used in clustering [21], metric learning [22] and instance selection [23]. 2.2 Canonical Correlation Analysis (CCA) A common approach to analyzing multilingua l document collections is to find a com-mon feature representation, so that the docum ents that are written in different languages can more easily be compared. One way of achieving that is by using the canonical cor-relation analysis.

Canonical Correlation Analysis (CCA) [9] is a dimensionality reduction technique somewhat similar to Principal Component An alysis (PCA) [24]. It makes an additional assumption that the data comes from two sources or views that share some information, such as a bilingual document corpus [25] or a collection of images and captions [26]. Instead of looking for linear combinations of features that maximize the variance (PCA) it looks for a linear combination of feature vectors from the first view and a linear combination for the second view, that are maximally correlated.

Formally, let S =( x 1 ,y 1 ) ,..., ( x n ,y n ) be the sample of paired observations where x  X  R p and y spaces. Let X =[ x 1 ,...,x n ] and let Y =[ x 1 ,...,x n ] be the matrices with observa-tion vectors as columns, interpreted as being generated by two random vectors X and Y . The idea is to find two linear functionals (row vectors)  X   X  R p and  X   X  R q so that the random variables  X   X X and  X   X Y are maximally correlated. The  X  and  X  map the random vectors to random variables, by computing the weighted sums of vector components. This gives rise to the following optimization problem: where C XX and C YY are empirical estimates of the variances of X and Y respectively and C XY is an estimate of the covariance matr ix. Assuming that the observation vec-tors are centered, the matrices ar e computed in the following way: C XX = 1 n  X  1 XX , C
This optimization task can be reduced to an ei genvalue problem and includes inverting the variance matrices C XX and C YY . In case of non-invertible matrices, it is possible to use a regularization technique by replacing C XX with (1  X   X  ) C XX +  X I ,where  X   X  [0 , 1] is the regularization coefficient and I is the identity matrix.

A single canonical variable is usually inadequate in representing the original random vector and typically one looks for k projection pairs (  X  1 , X  1 ) ,..., (  X  k , X  k ) ,sothat  X  i and  X  i are highly correlated and  X  i is uncorrelated with  X  j for j = i and analogously for  X  .

The problem can be reformulated as a symme tric eigenvalue problem for which effi-cient solutions exist. If the data is high-dimensional and the feature vectors are sparse, iterative methods can be used, such as the well known Lanczos algorithm [27]). If the size of the corpus is not prohibitively large, it is also possible to work with the dual representation and use the  X  X ernel trick X  [28] to yield a nonlinear version of CCA. For the experiments, we examined the Acquis aligned corpus data (http://langtech.jrc.it/JRC-Acquis.html), which comprise a set of more than 20000 documents in many different languages. To si mplify the initial analysis, we focused on the bi-lingual case and compared the English and French aligned document sets. We will consider more language pairs in our future work. The documents were labeled and associated with 14 different binary classification problems.

The documents were analyzed in the standard bag-of-words representation after to-kenization, lemmatization and stop word removal. Only nouns, verbs, adjectives and adverbs were retained, based on the part-o f-speech tags. The inter-document similarity was measured by the cosine similarity measure.

Common semantic representation for the t wo aligned document sets was obtained by applying CCA. Both English and French documents were then mapped onto the common semantic space (CS:E, CS:F). The u sed common semantic representation was 300-dimensional, as we wanted to test our a ssumptions in the context of dimensionality reduction and slight information loss. Longe r representations would be preferable in practical applications.

The Acquis corpus exhibits high hubness. Th is is apparent from Figure 1. The data was normalized by applying TF-IDF, which is a standard preprocessing technique. The normalization only slightly reduces the overall hubness.

The common semantic projections exhibit significantly lower hubness than the origi-nal feature representations, which already suggests that there might be important differ-ences in the hub structure. The outline of the data is given in Table 1. The two languages exhibit somewhat different levels of hubness.

If the hubness information is to be used in t he multi-lingual context, it is necessary to understand how it maps from one language representation to another. Both the quantita-tive and the qualitative aspects of the mapping need to be considered. The quantitative aspect refers to the the correlation between the total document neighbor occurrence counts and provides the answer to the general question of whether the same documents become hubs in different languages. The qua litative aspect is concerned with charac-terizing the type of influence expressed by the hubs in correlating the good and bad hubness (label mismatch percentages) in both languages.

Let us consider one randomly chosen hub document from the corpus. Figure 2 shows its occurrence profiles in both English and French over all 14 binary classification prob-lems. The good/bad occurrence distributions for this particular document appear to be quite similar in both languages, even though the total hubness greatly differs. From this we can conclude that, even though the overall occurrence frequency depends on the language, the semantic nature of the document determines the type of influence it will exhibit if and when it becomes a hub. On the other hand, this particular document is an anti-hub in both projections onto the co mmon semantic space, i.e. it never occurs as a neighbor there. This illustrates how the CCA mapping changes the nature of the k -nearest neighbor structure, which is what Table 1 also confirms.

The observations from examining the influence profiles of a single document are easily generalized by considering the average Pearson correlation between bad hubness ratios over the 14 binary label assignments, as shown in Table 2(a). There is a quite strong positive correlation between document influence profiles in all considered repre-sentations and it is strongest between the p rojections onto the common semantic space, which was to be expected. As for the total number of neighbor occurrences (Table 2(b) and Table 2(c)), the Pearson product-moment gives positive correlation between the hubness of English and French texts, as well as between the projected representations. In all other cases there is no linear correlation. We measured the non-linear correlation by using the Spearman correlation coefficient (Table 2(c)). It seems that there is some positive non-linear correlation between hubness in all the representations.
The results of correlation comparisons can be summarized as follows: frequent neigh-bor documents among English texts are usually also frequent neighbors among the French texts and the nature of their influence is very similar. Good/bad neighbor documents in English texts are expected to be good/bad neighbor documents in French texts and vice-versa. We will exploit this apparent regularity for improving the neighbor structure of the common semantic space, as will be discussed in Section 4. In the canonical correlation analysis, all examples contribute equally to the process of building a common semantic space. However, due to hubness, not all documents are to be considered equally relevant or equa lly reliable. Documents that become bad hubs exhibit a highly negative influence. Furthermore, as shown in Figure 2, a single hub-document can act both as a bad hub and as a good hub at the same time, depending on the specific classification task at hand. Ther efore, instance selection doesn X  X  seem to be a good approach, as we cannot both accept and reject an example simultaneously.
What we propose instead is to introduce instance weights to the CCA procedure in order to control the influence of hubs on forming the common semantic representation in hope that this would in turn improve the cross-lingual retrieval and classification performance in the c ommon semantic space.

The weights introduce a bias in finding the canonical vectors: the search for canonical vectors is focused on the spaces spanne d by the instances with high weights.
Given a document sample S ,let u 1 ,...,u n be the positive weights for the examples x  X  X and v compute the modified covariance and variance matrices as follows: These matrices are input for the standard CCA optimization problem. By modifying them, we are able to directly influence the outcome of the process. The weighting ap-proach is equivalent to performing over-sampling of the instances based on their speci-fied weights and then computing t he covariances and variances.

Let h ( x i ,k ) and h B ( x i ,k ) be the standardized hubness and standardized bad hubness scores respectively, i.e. h ( x i ,k )= N k ( x i ) influential and relevant for classification and retrieval, while a high bad hubness score indicates that the document is unreliable.

We have experimented with several different weighting schemes. We will focus on two main approaches. The first approach would be to increase the influence of rele-vant points ( hubs ) in the CCA weighting. The second meaningful approach is to re-duce the influence of unreliable points ( bad hubs ). Additionally, for comparisons, we will also consider the opposite of what we propose, i.e. reducing the influence of hubs and increasing the influence of bad hubs. Therefore, the considered weighting schemes are given as follows: un-weighted, v i := 1 , emphasized hubs, v i := e h ( x i ,k ) ,de-In the experimental protocol, we randomly selected two disjoint subsets of the aligned corpus: 2000 documents were used for training ad 1000 for testing. For each of the 14 bi-nary classification problems we computed five common semantic spaces with CCA on the training set: the non-weighted variant (CS:N), emphasized hubs (CS:H), de-emphasized hubs (CS:h), emphasized bad hubs (CS:B) and de-emphasized bad hubs (CS:b). The training and test documents in both languages were then projected onto the common semantic space. In each case, we evaluated the quality of the common semantic space by measuring the performance of both classification and document retrieval. The whole procedure was repeated 10 times, hence yielding the repeated random sub-sampling val-idation. We have measured the average performance and its standard deviation.
Many of the binary label distributions were highly imbalanced. This is why the clas-sification performance was measured by cons idering the Matthews Correlation Coeffi-cient (MCC) [29].

Comparing the classification performance on the original (non-projected) documents with the performance on the common semantic space usually reveals a clear degradation in performance, unless the dimensionality of the projected space is high enough to capture all the relevant discriminative information.

The overview of the classification experiments is given in Table 3. We only report the result on the English texts and projections, as they are basically the same in the French part of the corpus. We have used the k NN classifier with k =5 , as we are primarily interested in capturing the change of the neighbor-structure in the data. It is immedi-ately apparent that the weights which emphasize document hubness (CS:H) achieve the best results among the common semantic document representations. Reducing the in-fluence of bad hubs (CS:b) is in itself not enough to positively affect the classification performance. This might be because many hubs reside in borderline regions, so they might carry some relevant disambiguating feature information. It seems that empha-sizing the relevance by increasing the preference for all hub-documents gives the best classification results.

In evaluating the document retriev al performance, we will focus on the k -neighbor set purity as the most relevant metric. The inverse mate rank is certainly also important, but the label matches are able to capture a certain level of semantic similarity among the fetched results. A higher purity among the neighbor sets ensures that, for instance, if your query is about the civil war, you will not get results about gardening, regardless of whether the aligned mate was retrieved or not. This is certainly quite useful. The comparisons are given in Table 4.

Once again, the CS:H weighting proves to be the best among the evaluated hubness-aware weighting approaches, as it retains the original purity of labels among the docu-ment k NNs. It is significantly better than the un-weighted baseline (CS:N).
The CS:H weighting produces results most similar to the ones in the original En-glish corpus and we hypothesized that it is b ecause this particular document weighting scheme best helps to preserve the k NN structure of the original document set. We ex-amined the relevant correlations and it tur ns out that this is indeed the case, as shown in Table 5. By preserving the original structure, it compensates for some of the infor-mation loss which would have resulted due to the dimensionality reduction during the CCA mapping. We have examined the impact of hubness on cross-lingual document retrieval and clas-sification, from the perspective of calcu lating the common semantic document repre-sentation. Hubness is an important aspect of the dimensionality curse which plagues the similarity-based learning methods.

Our analysis shows that the hub-structure of the data remains preserved across differ-ent languages, but is radically changed b y the canonical correlation analysis mapping onto the common semantic space. The dimensionality reduction also results in some information loss. We have proposed to overcome the information loss by introducing the hubness-aware instance weights into the CCA optimization problem, which have helped in preserving the original k NN structure of the data during the CCA mapping.
The experimental evaluation shows that increasing the influence of hubs on spanning the common semantic space results in an increased k NN classification performance and the higher neighbor set purity.

These initial experiments were perform ed on an aligned bi-lingual corpus and we intend to expand the analysis by comparing more languages. Additionally, we intend to examine the unsupervised aspects of the problem, like clustering.
 Acknowledgments. This work was supported by ICT Programme of the EC under XLike (ICT-STREP-288342) and LTWeb (ICT-CSA-287815).

