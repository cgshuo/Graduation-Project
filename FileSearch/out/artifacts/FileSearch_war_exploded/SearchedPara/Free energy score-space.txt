 The complementary nature of discriminative and generative approaches to machine learning [20] has motivated lots of research on the ways in which these can be combined [5, 12, 15, 18, 9, 24, 27]. One recipe for such integration uses  X  X enerative score-spaces. X  Using the notation of [24], such spaces can be built from data by considering for each observed sequence x = ( x 1 , . . . , x k , . . . , x K ) of by  X  i .
 The observed sequence x is mapped to the fixed-length score vector  X  f  X  where f is the function of the set of probability densities under the different models, and  X  F is some operator applied to it. For instance, in case of the Fisher score [9], f is the log likelihood, and the operator  X  F produces the first order derivatives with respect to parameters, whereas in [24] other derivatives are also included. Another example is the TOP kernel [27] for which the function f is the posterior log-odds and  X  F is again the gradient operator.
 In these cases, the generative score-space approaches help to distill the relationship between a model parameter  X  i and the particular data sample. After the mapping, a score-space metric must be defined in order to employ discriminative approaches.
 A number of nice properties for these mappings, and especially for Fisher score, can be derived under the assumption that the test data indeed follows the generative model used for the score computation. However, the generative score spaces build upon the choice of one (or few) out of many possible generative models, as well as the parameters fit to a limited amount of data. In practice, these models can therefore suffer from improper parametrization of the probability density function, local minima, over-fitting add under-training problems. Consider, for instance, the situation where the assumed model over high dimensional data is a mixture of n diagonal Gaussians with a given small and fixed variance, and a uniform prior over the components. The only free parameters are therefore the Gaussian centers, and let us assume that training data is best captured with these centers all lying on (or close to) a hypersphere with a radius sufficiently larger than the Gaussians X  deviation. An especially surprising and inconvenient outlier in this case would be a test data point that falls close to the center of the hypersphere, as the derivatives of its log likelihood with respect to these parameters (Gaussian centers) evaluated at the estimate could be very low when the number of components n in the mixture is large, because the derivatives are scaled by the uniform posterior 1 /n . But, this makes such a test point insufficiently distinguishable Gaussian centers. If the model parameters are extended to include the prior distribution over mixture components, then derivatives with respect to these parameters would help disambiguate these points. In this paper, we propose a novel score space which focuses on how well the data point fits different parts of the generative model, rather than on derivatives with respect to the model parameters. We start with the variational free energy as a lower bound on the negative log-likelihood of the data, as this affords us with two advantages. First of all, the variational free energy can be computed for an arbitrary structure of the posterior distribution, allowing us to deal with generative models with many latent variables and complex structure without compromising tractability, as was previously done for inference in generative models. Second, a variational approximation of the posterior typically provides an additive decomposition of the free energy, providing many terms that can be used as features. These terms/features are divided into two categories: the  X  X ntropy set X  of terms that express uncertainty in the posterior distribution, and the  X  X ross-entropy set X  describing the quality of the fit of the data to different parts of the model according to the posterior distribution.
 We find the resulting score space to be highly informative for discriminative learning. In partic-ular, we tested our approach on three computational biology problems (promoter recognition, ex-ons/introns classification, and homology detection), as well as vision problems (scene/object recog-nition). The results compare favorably with the state-of-the-art from recent literature. The rest of the paper is organized as follows. The next section describes the proposed framework in more detail. In Sec. 3, we show that the proposed generative score space leads to better classification performances than the related generative counterpart. Some simple extensions are described in Sec. 4, and used in the experiments in Sec. 5. A generative model defines the distribution P ( h, x |  X  ) =  X  shared across all observations. In addition, to model the posterior distribution P ( h | x ) , we also define a family of distributions Q from which we need to select a variational distribution Q ( h ) that best fits the model and the data. Assuming i.i.d data, the family Q can be simplified to include only distributions of the form Q ( h ) = parameters of the posterior Q ( h ) , and the parameters of the model P , defined as The free energy bounds the log likelihood, F Q  X   X  log P ( x ) and the equality is attained only if Q is expressive enough to capture the true posterior distribution, as the free energy is minimized when Q ( h ) = P ( h | x ) . Constraining Q to belong to a simplified family of distributions Q , however, provides computational advantages for dealing with intractable models P . Examples of distribution families used for approximation are the fully-factorized mean field form [13], or the structured vari-ational approximation [7], where some dependencies among the hidden variables are kept.
 Minimization of F Q as a proxy for negative log likelihood is usually achieved by alternating opti-mization of with respect to Q and  X  , a special case of which  X  when Q is fully expressive  X  is the EM algorithm. Different choices of Q provide different types of compromise between the accuracy and computational complexity. For some models, accurate inference of some of the latent variables may require excessive computation even though the results of the inference can be correctly reinterpreted by studying the posterior Q from a simpler family and observing the symmetries of the model, or by reparametrizing the model (see for example [1]). In what follows, we will develop a technique that uses the parts of the free energy to infer the mapping of the data to a class variable with an increased accuracy despite possible imperfections of the data fit, whether this imperfection is due to the approximations and errors in the model or the posterior.
 Having obtained an estimate of parameters  X   X  that fit the given i.i.d. data we can rearrange the free energy (Eq.2) as The second term in the equation above is the cross-entropy term and it quantifies how well the data point fits the model, assuming that hidden variables follow the estimated posterior distribution. This posterior distribution is fit to minimize the free energy; the first term in 3 is the entropy and quantifies the uncertainty in this fit.
 If Q and P factorize, then each of these two terms further breaks into a sum of individual terms, each quantifying the aspects of the fit of the data point with respect to different parts of the model. For example, if the generative model is described by a Bayesian network, the joint distribution can (hidden or visible) and PA n are the parents of the n  X  th of these variables, i.e., v ( t ) n . The cross-entropy term in the equation above further decomposes into
For each discrete hidden variable v ( t ) n , the appropriate terms above can be further broken down into individual terms in the summation over the D n possible configurations of the variable, e.g., In a similar fashion, the entropy term can also be decomposed further into a sum of terms as dictated by the factorization of the family Q . Therefore, the free energy for a single sample t can be expressed as the sum different parts of the model. Such information can be encapsulated in a score space that we call free energy score space or simply FESS .
 For example, in the case of a binary classification problem, given the generative models for the two particular model with its estimated parameters, and a particular choice of the posterior family Q for each of the classes, and then concatenate the scores. Therefore, using the notation from [24] the free  X  If the posterior families are fully expressive, then the MAP estimate based on the generative models for the two classes can be obtained from this mapping by simply summing the appropriate terms to obtain the log likelihood difference, as the free energy equals the negative log likelihood. However, the mapping also allows for the parts of the model fit to play uneven roles in classification after an additional step of discriminative training. In this case the data points do not have to fit either model well in order to be correctly classified. Furthermore, even in the extreme case where one model provides a higher likelihood than the other for the data from both classes (e.g., because the models are not nested, and likelihoods cannot be directly compared), the mapping may still provide an abstraction from which another step of discriminative training can benefit. The additional step of training a discriminative model allows for mining the similarities among the data points in terms of the path through different hidden variables that has to be followed in their generation. These similarities may be informative even if the generative process is imperfect.
 Obviously, (7) can be generalized to include multiple models (or the use of a single model) and/or multiple posterior approximations, either for two-class or multi-class classification problems. We use here the terminology introduced in [27], under which FESS would be considered a model-dependent feature extractor , as different generative models lead to different feature vectors [25]. The family of feature extractors  X   X  F : X  X  &lt; d maps the input data x  X  X in a space of fixed dimension derived from a plug-in estimate  X  , in our case the generative model with parameters  X   X  from which the features are extracted.
 Given some observations x and the corresponding class labels y  X  { X  1 , +1 } following the joint probability P ( x, y |  X   X  ) , a generative model can be trained to provide an estimate  X   X  6 =  X   X  , where  X   X  are the true parameters. As most kernels (e.g. Fisher and TOP) are commonly used in combination performance of a feature extractor the classification error of a linear classifier w T  X   X   X  F ( x ) + b in the feature space &lt; d , where w  X  &lt; d and b  X  &lt; . Assuming that w and b are chosen by an optimal learning algorithm on a sufficiently large training dataset, and that the test set follows the same distribution with parameter  X   X  , the classification error R (  X   X  F ) can be shown to tend to where  X [ a ] is an indicator function which is 1 when a &gt; 0 , and 0 otherwise, and E x,y denotes the expectation with respect to the true distribution P ( x, y |  X   X  ) .
 The Fisher kernel (FK) classifier can perform at least as well as its plug-in estimate if the parameters of a linear classifier are properly determined [9, 27], where  X  represents the generative model used as plug-in estimate.
 This property also trivially holds for our method, where  X   X  F ( x ( t ) ) =  X  FESS  X  energy can be expressed as a linear combination of the elements of  X  .
 In fact, the minimum free energy test (and the maximum likelihood rule when Q is fully expressive) can be defined on  X  derived from the generative models with parameters  X   X  +1 for one class and  X   X   X  1 for another as The extension to a multiclass classification is straightforward. When the family Q is expressive enough to capture the true posterior distribution, then free energy reduces to negative log likelihood, and the free energy test reduces to ML classification. In other cases, likelihood computation is intractable, and free energy test is used instead of the likelihood ratio test. It is straightforward to prove that a kernel classifier that works in FESS is asymptotically at least as good as the MAP labelling based on the generative models for the two classes since generative classification is a special case of our framework.
 Lemma 3.1 For  X  FESS  X  the free energy for one model, and the remaining M 2 for the second, a linear classifier employing as R Q (  X  ) achieved using the free energy test above.
 Proof Furthermore, when the family Q is expressive enough to capture the true posterior distribution, the free energy test is equivalent to maximum likelihood (ML) classification, R Q (  X  ) = R (  X  ) . The dominance of the Fisher and Top kernels [9, 27] over their plug-in holds for FESS too, and the same plug-in (the likelihood under a generative model) may be used when this is tractable. However, if the computation of the likelihood (and the kernels derived from it) is intractable, then the free energy test as well as the kernel methods based on FESS that will outperform this test, can both still be used with an appropriate family of variational distributions Q . In some generative models, especially sequence models, the number of hidden variables may change from one data point to the next. In speech processing, for instance, hidden Markov models (HMM) are also of variable lengths. The parameters  X  of this model include the prior state distribution  X  , the state transition probability matrix A = a { ij } , and the emission probabilities B = b { iv } . Exact inference is tractable in HMMs and so we can use the exact posterior (EX) distribution to formulate the free energy and the free energy minimization is equivalent to the usual Baum-Welch training algorithm [17] and F EX =  X  log P ( x ) . The free energy of each sample x t is F Depending on how this is broken into terms f i , we could get feature vectors whose dimension de-pends on the length of the sample K ( t ) . To solve this problem, we first note that a standard approach to dealing with utterances of different lengths is to normalize the likelihood by the sequence length, and this approach is also used for defining other score spaces. If, before the application of the score operator, we simply evaluate the sums over k in the free energy and divide each by K ( t ) , we obtain a fixed number of terms independent of the sequence length. This results in a length-normalized score space nFESS , where the granularity of the decomposition of the free energy is dramatically reduced. Figure 1: A) SVM error rates for nFESS and probability product kernels [10] using Markov models (we reported only their best result) and hidden Markov models as plug-ins. T represents the param-eters used in the kernel of [10], and K is the order of the Markov chain. The results are arranged along the x axis by the regularization constant used in SVM training. B) Comparison with results obtained using FK and TK score spaces. C) Comparison of the five homology detection methods in Experiment 3. Y axis represents the total number of families for which a given method exceeds a median RFP score on the X axis.
 In general, even for fixed-length data points and arbitrary generative models, we do not need to cre-ate large feature vectors corresponding to the finest level of granularity described in (5), or for that matter the slightly coarser level of granularity in (4). Some of the terms in these equations can be grouped and summed up to ensure for shorter feature vectors, if this is warranted by the application. The longer the feature vector, the finer is the level of detail with which the generative process for the data sample is represented, but more data is needed for the training of the discriminative classifier. Domain knowledge can often be used to reduce the complexity of the representation by summing appropriate terms without sacrificing the amount of useful information packed in the feature vectors. Such control of the feature vector length does not negate the previously discussed advantages of the classification in the free energy score space compared with the straightforward application of free energy, likelihood, or in case of sequence models, length-normalized likelihood tests. We evaluated our approach on four standard datasets and compared its performance with the clas-sification results provided by the datasets X  creators, those estimated using the plug-in estimate  X  , and those obtained using the Fisher ( FK ) and TOP ( TK ) kernel [9, 27] derived from the plug-ins. Support vector machines (SVMs) with RBF kernel were used as discriminative classifiers in all the score spaces, as this technique was previously identified as most potent for dealing with variable-length sequences [25]. As plug-ins, or generative models/likelihoods  X  , for the three score spaces we compare across experiments, we used hidden Markov models (HMMs)[23] in Experiments 1-3 and latent Dirichlet allocation (LDA)[4] in Experiment 4. For each experiment, comparisons are based on the same validation procedure used in the appropriate original papers that introduced the datasets. For both FK and FESS , in each experiment we trained a single generative model (HMM or LDA, depending on the experiment). For all HMM models, the length-normalization with associ-ated summation over the sequence as described in the previous section was used in the construction of the free energy score space. The model complexity, e.g., the number of states for the HMM were chosen by cross-validation on the training set.
 Experiment 1: E. coli promoter gene sequences. The first analyzed dataset consists of the E. coli promoter gene sequences (DNA) with associated imperfect domain theory [26]. The standard task on this dataset is to recognize promoters in strings of nucleotides (A, G, T, or C). A promoter is a genetic region which facilitates the transcription of gene located nearby. The input features are 57 sequential DNA nucleotides. Results, obtained using leave-one-out (LOO) validation, are reported in Table 1 and illustrate that FESS represents well the fixed size genetic sequences, leading to superior performance over other score spaces as well as over the plug-in  X  HMM .
 Experiment 2: Introns/Exons classification in HS 3 D data set. The HS 3 D data set 1 [10] con-tains labelled intron and exon sequences of nucleotides. The task here is to distinguish between the two types of gene sequences that can both vary in length (from dozens of nucleotides to tens of thousands of nucleotides). For the sake of comparison, we adopted the same experimental setting of [10]. In Fig.1-A (top right), we reported the results obtained in [10] (overall error rate, OER, 7,5%), the results obtained using the HMM model (  X  HMM , OER 27,59%) together with the results obtained by our method (OER 6,12%). In Fig. 1-B (bottom right), we compared our method also with FK (OER 10,06%) and TK (OER 12,82%) kernels.
 Experiment 3: Homology detection in SCOP 1.53. We tested the ability of FESS to classify The sequences in the database were selected from the Astral database, based on the E-value thresh-grouped into families and superfamilies. For each family, the protein domains within the family are considered positive test examples, and the protein domains outside the family, but within the same superfamily, are taken as positive training examples. The data set yields 54 families containing at least 10 family members (positive test) and 5 superfamily members outside of the family (positive train) for a total of 54 One-Vs-All problems. The experimental setup is similar to that used in [8], except for one important difference: in the current experiments, the positive training sets do not include additional protein sequences extracted from a large, unlabelled database. Therefore, the recognition tasks performed here are more difficult than those in [8]. In order to measure the quality of the ranking, we used the median RFP score [8] which is the fraction of negative test sequences that score as high as or better than the median-scoring positive sequence. We used SVM decision values as score. We find that FESS outperforms task-specific algorithms (PSI-Blast [2] and SAM [14]) as well as the Fisher score ( FK ,[8]) with statistical significance with p-values of 5.1e-9, 8.3e-7, 1.1e-5, respectively. There is no statistical difference between our results FESS and those based on FPS [3]. In particular, the poor performance of [8] is explained by the under-training of HMMs [6]. The FESS representation proved to be much less sensitive to the training problems. We repeated the test using two different choices of Q : the approximate mean field factorization and the exact posterior ( FESS -MF and FESS -EX, respectively, in Fig.1-C). Interestingly, the performance was also robust with respect to these choices.
 Experiment 4: Scene/object recognition. Our final set of experiments used the data from the cation (LDA) [4] as the generative model. The free energy for LDA is derived in [4]. To serve as words in the model, we extracted SIFT features from 16x16 pixel windows computed over a grid with spacing of 8 pixels. These features were mapped to 175 codewords ( W = 175 ). We varied the number of topics to explore the effectiveness of different techniques.
 Graz dataset has two object classes, bikes (373 images) and persons (460 images), in addition to a is highly diverse, e.g., a  X  X erson X  image may show a pedestrian at a certain distance, a side view of a complete body, or just a closeup of a head. We performed two-class detection (object vs. background) using an experimental setup consistent with [16, 22]. We generated ROC curves by thresholding raw SVM output, and report here the ROC equal error rate averaged over ten runs. The results are shown in Table 2. The standard deviation of the classification rate is quite high as the images in the database have very different complexities, and the performance for any single run is Table 2: Classification rates for object/scene recognition tasks. The deviation is shown in brackets. Our approach tends to be robust to the choice of the number Z of topics, and so in scene recognition experiments, we report only the result for Z=40. highly dependent on the composition of the training set.
 We also tested our approach on the scene recognition task using the datasets of [21], composed of two (Natural and Artificial scenes) datasets, each with 4 different classes. The results are reported in Table 2 where for the first time we employed Fisher-LDA in a vision application. Although this new technique outperformed state of the art, once again, FESS outperforms both this result and other state-of-the-art discriminative methods [21, 16]. In this paper, we present a novel generative score space, FESS , exploiting variational free energy terms as features. The additive free energy terms arise naturally as a consequence of the factorization of the model P and the posterior Q . We show that the use of these terms as features in discriminative classification leads to more robust results than the use of the Fisher scores, which are based on the derivatives of the log likelihood of the data with respect to the model parameters. As was previously observed, we find that the Fisher score space suffers from the so called  X  X rap-around X  problem, where very different data points may map to the same derivative, an example of which was dis-cussed in the introduction. The free energy terms, on the other hand, quantify the data fit in different parts of the model, and seem to be informative even when the model is imperfect. This indicates that the re-scaling of these terms, which the subsequent discriminative training provides, leads to improved modelling of the data in some way. Scaling a term in the free energy composition, e.g., the term distribution to the power w . This is indeed reminiscent of some previous approaches to correcting generative modelling problems. In speech applications, for example, it is a standard practice to raise the observation likelihood in HMMs to a power less than 1, before inference is performed on the test sample, as the acoustic signal would otherwise overwhelm the hidden process modelling the language constraints [28]. This problem arises from the approximations in the acoustic model. For instance, a high-dimensional acoustic observation is often modelled as following a diagonal Gaus-sian distribution, thus assuming independent noise in the elements of the signal, even though the true acoustics of speech is far more constrained. This results in over-accounting for the variation in the observed acoustic signal, and to correct for this in practice, the log probability of the observation given the hidden variable is scaled down. The technique described here proposes a way to automati-cally infer the best scaling, but it also goes a step further in allowing for such corrections at all levels of the model hierarchy, and even for specific configurations of hidden variables. Furthermore, the use of kernel methods provides for nonlinear corrections, as well. This extremely simple technique was shown here to work remarkably well, outperforming previous score space approaches as well as the state of the art in multiple applications.
 It is possible to extend the ideas here to other types of model/data energy. For example, the free energy approximated in different ways is used in [1] to construct various inference algorithms for a single scene parsing task. It may also be effective, for example, to use the terms in the Bethe free energy linked to different belief propagation messages to construct the feature vectors. Finally, although we find that FESS outperforms the previously studied score spaces that depend on the derivatives, i.e. where  X  F is a derivative with respect to  X  , the use of this derivative in (7) is, of course, possible. This allows for the construction of kernels similar to FK and TK , but derived from intractable generative models as we show in Experiment 4 ( FK in Table 2) on latent Dirichlet allocation.
 We acknowledge financial support from the FET programme within the EU FP7, under the SIMBAD project (contract 213250).
