 Pairwise learning-to-rank algorithms have been shown to allow rec-ommender systems to leverage unary user feedback. We propose Multi-feedback Bayesian Personalized Ranking (MF-BPR), a pair-wise method that exploits different types of feedback with an ex-tended sampling method. The feedback types are drawn from dif-ferent  X  X hannels X , in which users interact with items (e.g., clicks, likes, listens, follows, and purchases). We build on the insight that different kinds of feedback, e.g., a click versus a like, reflect differ-ent levels of commitment or preference. Our approach differs from previous work in that it exploits multiple sources of feedback si-multaneously during the training process. The novelty of MF-BPR is an extended sampling method that equates feedback sources with  X  X evels X  that reflect the expected contribution of the signal. We demonstrate the effectiveness of our approach with a series of ex-periments carried out on three datasets containing multiple types of feedback. Our experimental results demonstrate that with a right sampling method, MF-BPR outperforms BPR in terms of accuracy. We find that the advantage of MF-BPR lies in its ability to leverage level information when sampling negative items.
In many domains, users provide unary feedback via a number of different  X  X hannels X . For example, online marketplaces collect view, add-to-cart and buy signals, and music platforms log listen, favorite and add-to-playlist events. Bayesian Personalized Rank-ing [8], a pair-wise learning-to-rank method that learns the prefer-ences of the users by sampling pairs, allows recommender systems to learn effectively from unary feedback. However, current instan-tiations of BPR fall short of taking full advantage of the entire range of different types of feedback that are available in certain domains. .
 In this paper, we propose an approach called Multi-Feedback Bayesian Personalized Ranking (MF-BPR). The innovation of MF-BRP is a sampling method designed to simultaneously exploit unary feedback from multiple channels during training. New sampling methods have proven effective in improving BPR [7]. However, previous attempts to leverage different sources of unary feedback have focused on channels individually, e.g., [5].

The key to our approach is to map different feedback channels to different  X  X evels X  that reflect the contribution that each type of feedback can have in the training phase. BPR samples pairs such that the first item in the pair is preferred to the second item. The levels of MF-BPR help to automatically direct sampling to focus on the most informative pairs. The appeal of the approach is that it takes advantage of the available information consistently with the intuition that some user feedback signals are more reliable or meaningful than others.

The paper is organized as follows. In the next section, we cover relevant related work. Then, we present MF-BPR in detail, and introduce the three data sets on which we test it. Next, we report experimental results, which demonstrate the ability of MF-BPR to improve over BPR. Finally, we close with a discussion of the results, which sheds light on improvement delivered by judicious sampling of the  X  X egative X  item of the item pair.
Since the BPR was introduced [8], many improvements have been proposed. MF-BPR belongs to that class of approaches that propose sampling refinements. The initial contribution in this di-rection was made by [3] who weights item sampling by popularity. Recently, [7] has proposed adaptive sampling, which over-samples informative pairs based on the current model. MF-BPR is similar to this approach in its focus on informative pairs, but its sampling is driven by feedback channels. In the remainder of this section, we cover a selection of the various techniques that have been proposed to exploit feedback from different sources, providing key examples of each, and noting the differences with MF-BPR.

Matrix factorization approaches have attempted to simultane-ously factorize multiple matrices. The authors of [11] take into account different signals from social networks (comment, re-share, or create-post) with separate factorizations, which are used to make predictions that are subsequently combined. The work of [4] ex-ploits multiple types of relations, and is directed at the cold start problem. In contrast, the  X  X ulti X  of MF-BPR refers to multiple types of feedback reflecting the same user-item relation.
Other approaches have leveraged the fact that ratings represent different levels of user feedback. A list-wise optimization criterion is proposed in [9], which allowed learning-to-rank to take advan-tage of graded feedback. Most closely related to our work is [5], which modified the BPR algorithm using graded implicit feedback derived from ratings and interaction counts, and also explores the contribution of temporal information. The outlook of the paper points out the promise of hybrid approaches. Our MF-BPR can be considered hybrid, since it simultaneously uses different sources of feedback. However, it differs from [5] in that it avoids hand-chosen weights, and, more significantly, uses multiple feedback channels simultaneously.
BPR-MF is based on the insight that user feedback collected via various channels reflects different strengths of user preference, and the sampling method of BPR can exploit these differences. In short, we allow BPR to learn from the fact that, e.g., a click represents a different level of commitment or preference than a  X  X ike X .
For a given training set S consisting of user-item pairs ( u,i ) , standard BPR creates tuples ( u,i,j ) by sampling an observed feed-back pair ( u,i ) from S and a negative item j not observed with u . For each sampled tuple, the BPR algorithm updates the parameters with Stochastic Gradient Descent in such a way that i is ranked higher than j . MF-BPR maps different types of feedback onto lev-els , which allow us to constrain sampling to reflect the preference strengths that we associate with the feedback types. Figure 1 com-pares the standard BPR sampling method (A) with the extended sampling method used by MF-BPR (B), which imposes an order on the types of positive feedback, that makes finer-grained differ-ences between feedback available during the learning phase.
We now express formally how MF-BPR takes advantage of feed-back levels. Let L = ( L 1 ,...,L p ) represent a given ordered set of levels in a dataset such that a feedback in L i is a stronger signal of interest compared to a feedback in L i +1 , that is L For generality, the unobserved feedback is also considered to be-long to a level L uo such that for each positive feedback level L and negative feedback level L j , L i L uo and L uo L j . The set of training feedback in level L is denoted by S L . We further define L + and L  X  as positive and negative feedback levels. In the standard BPR, | L | = 2 , | L + | = 1 and | L  X  | = 0 since there is only one level for positive and one level for unobserved feedback and there is no explicit negative feedback. We also define I the items in level L that user u interacted with and I u as all items that u interacted with (in all levels). The set of all combinations of preferences in Multi-feedback BPR D MF can be defined as: D MF = { ( u,i,j ) | i  X  I L,u  X  j  X  I N,u  X  L  X  L +  X  L N } (1)
In the standard BPR the tuples ( u,i,j ) are sampled uniformly but in MF-BPR we introduce a non-uniform sampler that takes into account the level (importance) of the feedback channel. To use multiple levels, MF-BPR requires sampling L and N from and then the tuples ( u,i,j ) can be sampled with respect to the given levels. The positive item is sampled by sampling an observed feedback from S using sampling distribution p ( u,i,L ) , which also samples the positive level L . The probability distribution p ( u,i,L ) can be further expanded as p ( u,i,L ) = p ( u,i | L ) p ( L ) such that p ( u,i | L ) is a uniform distribution over S L and p ( L ) is the sam-pling distribution of level L . A trivial choice for p ( L ) would be a uniform distribution over all levels. However, with a uniform dis-tribution, levels with small cardinality will be oversampled, which can result in being a poor sampling. We propose a non-uniform distribution for p ( L ) where the cardinality of feedback as well as the importance of a level is taken into account with a weight factor. The probability distribution p ( L ) is defined by: where w L is the weight of level L . With equal weights, positive items are sampled uniformly from all levels. This would be equiv-alent to the positive sampler of standard BPR. The weight values can be defined non-uniformly to reflect the importance of levels. The weight parameters can be influenced by the available context or other properties of the levels. In our experiments, we found that the inverse rank of positive levels are good candidates for weights (i.e., w 1 = 1 ,w 2 = 1 2 ,... ). If the order of levels is not known a priori (for example whether a like should be considered more Figure 1: Sampling item pairs in BPR(A) and MF-BPR (B). The arrows show preferences. In MF-BPR any item at the higher level is preferred over all items in the lower levels. In the standard BPR, the only way of sampling item pairs is to sample from observed positive feedback and unobserved feedback. important that a share or vice versa), the optimal weights can be approximated by using a hyper-parameter search algorithm.
To sample negative item j , the level of the positive item should be given so that the sampler samples an item from one of the lev-els below it. Given the positive level L and a positive user-item pair ( u,i ) , we denote p ( j,N | u,L ) as the sampling distribution of and negative sample j and its corresponding level N . Similar to the positive sampler, the negative sampler can be expanded as p ( j,N | u,L ) = p ( j | u,L,N ) p ( N | u,L ) where p ( j | u,L,N ) is the negative item sampler and p ( N | u,L ) is the conditional negative level sampler. We also propose a non-uniform negative item sam-pler, similar to (2), that takes into account the cardinality and the weights of the levels. The negative level sampler p ( N | u,L ) is de-fined as: such that 0  X   X   X  1 is a parameter that controls the ratio of unob-served feedback in the sampling. For the standard BPR  X  = 1 , as all negative items are sampled from the unobserved feedback. The right value for  X  can be found experimentally. In our experiments, we found that with high values of  X  the model is more accurate. Our observation about the right value of  X  is discussed in more detail in Section 4.

Similar to the standard BPR, the negative sampler p ( j | u,L,N ) can sample negative item uniformly from I N,u if N 6 = L from I \ I u if N = L uo . We denote p uni ( j | u,L,N ) as the uniform negative item sampler which can be defined as:
In addition, we also propose a multi-level item sampler with a non-uniform sampler when N is the unobserved level L define the multi-level negative item sampler p ml ( j | u,L,N ) as: p ml ( j | u,L,N ) = where u 0 6 = u and L 0  X  L + . Here, if the given level L is an observed level, the negative item is sampled uniformly the same as the uniform-item sampler. However, if the the given level N = 1: procedure L EARN MF-BPR( S ,  X  , W , L ) 2: initialize  X  3: repeat 4: draw ( u,i,L ) from p ( u,i,L ) 5: draw N from p ( N | u,L ) 6: draw j from p ( j | u,L,N ) 7: update  X  with BPR update rule [8] 8: until convergence 9: return  X  10: end procedure Figure 2: Learning MF-BPR with Stochastic Gradient Descent. L uo , the negative item is sampled non-uniformly with respect to depends on its feedback from other users. Items that are more popular and have enjoyed user interactions from stronger levels, have a higher chance of being sampled. In other words, if N = L the sampler selects feedback by sampling a tuple ( u which an interaction between u and j is not observed. Note that u and L 0 for the tuple are subsequently discarded.

By sampling the tuple ( u,i,j ) the parameters of the model can be updated via Stochastic Gradient Descent (SGD) similarly to standard BPR. Note that here the parameters  X  , the weights of the levels W and the order of levels in L are given a priori. Figure 2 lists the learning algorithm of BPR with Multi-feedback sampler.
The MF-BPR is evaluated on three datasets and its performance is compared with different methods using 4-fold cross validation. The ground truth and relevant recommendations however, are dif-ferent in the three datasets depending on the problem. The datasets, the splitting strategy and the feedback signals of the datasets are de-scribed bellow. Dataset statistics are presented in Table 1. Kollekt.fm Kollekt.fm is an online music discovery platform. For Kollekt.fm the recommender task is to recommend playlists to users. Kollekt.fm is interested in increasing the number of follow-ers for their playlists, so we predict follows. This dataset contains three different levels of feedback relevant to playlists: follows , which is considered the highest level of feedback. listening which refers to the ratio of the listening time to a playlist to the total time a user listened to any playlist. favorites is the ratio of the songs in a playlist which are favorited to the total number of songs in the playlist. For both listening and favorites we used only the occurrences with a value greater than a specific threshold. Due to space constraints, we report results setting both thresholds to 0.1.

XING XING 1 is a social network and a job discovery frame-work. This dataset is related to the job postings of in XING. This is the data used for the RecSys 2016 challenge. It contains four differ-ent levels of feedback: click , bookmark , reply and remove . To be compliant with XING X  X  needs, we used  X  X lick X ,  X  X ookmark X  and  X  X eply X  as positive feedback for both training and ground truth and  X  X emove X  as negative. Adopting XING X  X  own valuation of user feedback, we used reply as the highest level, then bookmark and click as the lowest level of positive feedback. In order to reduce data sparsity and the size of the dataset we filtered out all the users and jobs that have less than five instances of feedback.
MovieLens-1M For the sake of completeness, we also report the results for the MovieLens dataset 2 , containing movie ratings on a http://xing.com/ http://grouplens.org/datasets/movielens/ Dataset #user #item #feedback %density #levels Kollect 15972 34910 195k 0.0350 3 ML1M 6040 3706 1000k 4.4684 5
XING 11324 9857 569k 0.5097 4 1-5 scale. We considered each discrete rating as a different level of feedback. For each user, we took the levels above the user X  X  aver-age rating as positive feedback, both for training and ground truth, and the ones below as negative. We consider a recommendation relevant if user has rated it above his average.

The experiments are implemented with an open source toolkit developed by the authors and available online [6]. To measure per-formance we calculated Mean Reciprocal Rank (MRR), recall, pre-cision, all at 10, as well as and mean average precision and nDCG. For calculating these metrics, we applied the approach known as One-plus-random [2, 1]. Here we report MRR@10. In the interest of space, we simply state that the other metrics were comparable. The three datasets are evaluated with four different algorithms:
MF-BPR-UNI This is Multi-Feedback BPR with uniform item sampling. The positive item is sampled with the level-based sam-pler and the negative item is sampled according to (4).

MF-BPR-ML This is another variation of Multi-Feedback BPR with multi-level negative feedback sampler where negative feed-back are sampled non-uniformly according to (5).

BPR This is the standard BPR [7] where all positive feedback are assumed to be equally strong.
 BPR-Dynamic This is BPR with dynamic sampling method [7]. The items are re-scored after every few iterations and they are sam-pled proportional to their current score.

We also compared our results with random and popularity base-line. The popularity baseline exploits the positive feedback from all channels without differentiating levels. In all our experiments, we used 50 latent features and 100 iterations. For the SGD algorithm, learning-rate is set to 0 . 05 and the regularization parameter  X  is set to 0 . 002 for all the four algorithms.

Figure 3 compares the performance of the two sampling models of MF-BPR with previous BPR methods. We report on the y-axis MRR@10 and on the x-axis the ratio of unobserved sampled items (i.e., parameter  X  ), to see the influence of  X  on performance of MF-BPR. As we can see from the figure, increasing  X  improves the ac-curacy of the model. This observation implies that a combination of an observed and an unobserved item in a pair is a better candi-date to train the model. This validates the  X  X issing not at random X  hypothesis which is highlighted by [10]. That is, the missing (un-observed) items are most likely missed as negative items. Another implication is that the presence of feedback in lower levels indi-cates that all positive levels contribute in defining what is preferred by the user w.r.t to the unobserved feedback.

The performance improvement of MF-BPR-ML can be attributed to the fact that it selects unobserved feedback as negative sam-ples using a non-uniform sampling strategy that is sensitive to the strength of the feedback given by other users. However an inter-esting observation is that the MF-BPR-ML method performs better than the standard BPR and MF-BPR-UNI in the XING and Kol-lect.fm datasets, while it performs worse in the Movielens dataset. This can be explained by the fact that MovieLens is rather a dense dataset and a uniform-item sampler can sample a good enough rep-resentative of items.

Another interesting result is that in the Kollekt.fm dataset the popularity-based method outperforms BPR. An explanation can be found in Figure 4 where the popularity-skewness of the three datasets Figure 3: Comparison of results for our multi-level methods and the original and dynamic BPR algorithm. The horizontal axis rep-resents the unobserved sampling ratio (  X  ). are plotted. The y axis represents the ratio of interactions for a given ratio of items on the x axis, sorted by decreasing popularity. Kollekt.fm is the most popularity skewed dataset, where 1% of the items accounts for 40% of the interactions. A consistent behav-ior of the popularity baseline can be found in the other datasets: the less the dataset is popularity skewed, i.e. the feedback is well distributed among the items, the worst are the popularity-based per-formances.
In this work we introduced MF-BPR, an extension to BPR that can be used when multiple types of feedback are available. MF-BPR extends the standard BPR sampling model by exploiting the difference in strength among user feedback  X  X hannels X . From our experiments we can draw two insights. The first is that the data is  X  X ot missing at random X , but rather the missing data can be regarded as negative feedback. This supports the underlying assumptions in the BPR model and it is further validated by the fact that sampling negative items from the lower levels of observed feedback decrease the overall quality of the recommendation algorithm. Second, we showed that sampling the negative item with a level-based method has a substantial impact on the quality of recommendations. In fu-ture work, we will learn the optimal relative ordering of the levels automatically, and also explore the contribution of additional con-text information.
 This research is supported by an EU-FP7 project under grant agree-ments no. 610594 (CrowdRec), and Dutch national e-infrastructure with the support of SURF Cooperative.
 [1] Alejandro Bellogin, Pablo Castells, and Ivan Cantador. [2] Paolo Cremonesi, Yehuda Koren, and Roberto Turrin.
 [3] Zeno Gantner, Lucas Drumond, Christoph Freudenthaler, [4] Artus Krohn-Grimberghe, Lucas Drumond, Christoph [5] Lukas Lerche and Dietmar Jannach. Using graded implicit [6] Babak Loni and Alan Said. Wraprec: An easy extension of [7] Steffen Rendle and Christoph Freudenthaler. Improving [8] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and [9] Yue Shi, Alexandros Karatzoglou, Linas Baltrunas, Martha [10] Harald Steck. Training and testing of recommender systems [11] Zhe Zhao, Zhiyuan Cheng, Lichan Hong, and Ed H Chi.
