 1. Introduction
The use of Latent Semantic Indexing (LSI) has been proposed for text retrieval in several recent works to project very high dimensional document and query vectors into a low dimensional space. In this new performance. Furthermore, LSI can be alternatively reviewed as a query expansion method (see Sections categorization (Baker &amp; McCallum, 1998; Dumais, 1995; Yang, 1999) and word sense disambiguation
Vempala, 1998; Zha et al., 1998) have also provided some understanding of the effectiveness of LSI.
These LSI studies have, however, mostly used relatively small text collections and simplified document models. In this work we investigate the use of LSI on a larger document collection (TREC). Our initial norm (the magnitudes) of terms and study the term norm distribution in detail. We propose a term nor-malization scheme for LSI which improves retrieval precision on the TREC and NPL text collections.
In Section 2 we introduce the concepts of text retrieval and LSI necessary for our work. A short variability affects the SVD and then shows how the decomposition influences retrieval performance. A preliminary version of this report appeared in (Husbands, Simon, &amp; Ding, 2001). 2. The vector space model and LSI treatments of some of the issues), a simple way to represent a collection of documents is with a term-document matrix X with incorrectly dominated by frequent terms. 2.1. Term weighting
Perhaps the most commonly used term weighting scheme is the tf.idf weighting scheme. This scheme scheme is specified by where n is total number of documents, and df  X  i  X  is the document frequency of term i , the number of documents in which term i occurs. This scheme gives very frequent terms low weight and assigns large weight for infrequent (and hopefully more discriminating) terms.

For comparison purposes we also study the log.entropy weighting scheme (Dumais, 1991). In this weighting, the local term weight is the logarithm of the term frequency. The global weighting uses the entropy E  X  i  X  of term i . This scheme is specified by
Queries (over the same set of terms) are similarly represented. The similarity between document vectors (the columns of term-document matrices) can be found by their inner product. This corresponds to determining the number of term matches (weighted by frequency) in the respective documents. Another commonly used similarity measure is the cosine of the angle between the document vectors. This can be achieved computationally by first normalizing (to 1) the columns of the term-document matrices before computing inner products.
 the relevance scores for each document form a vector r and is computed as r  X  X T q . For  X  queries 2.2. LSI and a query expansion interpretation a truncated singular value decomposition (SVD) of the term-document matrix X .
 If X is an m n matrix, then the SVD of X is where U is m m with orthonormal columns, V is n n with orthonormal columns, and S is diagonal with the main diagonal entries sorted in decreasing order. LSI uses a truncated SVD of the term-document matrix where X is approximated by upper left k by k part of S ). This gives the best rank k approximation to the original matrix. Now the relevance score matrix becomes
U is the projection operator that projects an m -dimensional document or query into k -dimensional LSI subspace. Note that even if the columns of X are normalized to 1, the columns of U T k X are not auto-matically normalized, and so we compute cosines between the projected documents and projected queries. matching, no query expansion.) Thus LSI can be alternatively viewed as a query expansion method. This word co-occurrence is important for our modification of LSI in Section 5.
 U simple linear combinations of the projected terms (see Section 5).
 The truncated SVD is usually computed by an iterative technique such as the Lanczos method. The
SVDs in this report were computed with the PARPACK software package (Maschhoff &amp; Sorensen, 1996) (as well as TRLAN (Wu &amp; Simon, 1999) for verification). Another popular software package for com-puting SVDs is SVDPACK (Berry, 1992). 2.3. Evaluation MEDLINE test set (with 8847 terms and 1033 documents) using term matching and LSI is shown in Fig. 1. retrieval based on their precision/recall performance. 2.4. LSI performance
Experiments with LSI have primarily used small data sets. The primary reason for this is the complexity (in both time and space) of computing the SVD of large, sparse term-document matrices. Nevertheless, early results were encouraging. Fig. 1 compares LSI using with k  X  200 to term matching for the small MEDLINE collection. Here IDF weighting was used and the term-document matrix was normalized prior to decomposition. The cosine similarity measure was used in both cases.

Performance on very large collections is not as good. Fig. 2 shows LSI using k  X  300 on TREC6 and showed similar performance. Note that the computational resources needed for using more than 1000 factors make this impractical for all but the largest supercomputers. projected terms, discussed in Section 4. 3. Software used For the experiments in this paper we used the MATLAB*P system (Husbands, Isbell, &amp; Edelman, 1999). MATLAB*P enables users of supercomputers to transparently work on large data sets within Matlab.
Through the use of an external server (that stores and operates on data) and Matlab X  X  object oriented ments in parallel on NERSC X  X  Cray T3E and IBM SP and no changes had to be made when moving from investigate LSI we can type, [U,S,V]  X  svds(X,k); % Perform a truncated SVD newTerms  X  U*diag(S); % Compute the projected terms newA  X  V 0 ; newQ  X  newTerms 0 *Q; % Get new representation for queries % Use normcols for cosine measure and find the similarities Scores  X  normcols(newA) 0 *normcols(newQ); Computing and graphing precision/recall curves from pre-judged queries also takes place in MAT-
LAB*P using simple m-file scripts. 4. Term norms vs. inverse document frequency
The norms (lengths) of the rows of U k S k (in addition to their directions) have great influence on mance.
 terms, terms with low norm contribute very little to the representations of documents and queries. The small component of that term and will be dominated by other terms making it difficult for retrieval.
Fig. 3 shows a histogram of term norms for the TREC collection and Fig. 4 plots IDF vs. term norm for the NPL (7491 terms and 11429 documents) and TREC text collections. We see the wide range of norms with the lowest frequencies in the collection.

A term with low frequency is often important to discriminate among relevant documents. Therefore, its the correction power of IDF is not sufficient.
 disease polio , the small norm of polio is not helpful for this goal.

The popular tf.idf global weighting scheme appears to be inadequate to mitigate the effect of low term norm. Table 1 shows the range of norms and IDFs for a few test collections. The lowest term norms are typically orders of magnitude away from the highest IDF, hinting at IDF X  X  inadequacy. We can similar. The entropy weights have an even smaller range than the IDF weights and so also are unable to compensate for large variations in term norm.

Because the columns of U k are scaled by the singular values, these have a contributing effect on term norm distribution and the projected documents. Fig. 5plots the singular values of the NPL and TREC6 displayed range. 5. Normalized LSI In this section we attempt to remedy the situation by (1) examining how documents are represented in LSI space and (2) proposing a normalization to compensate for infrequent terms.

In LSI space, the documents are represented by columns of U T k X and terms are represented by rows S the j th document is c  X  X  X  i ; j  X  .

As discussed in previous sections, idf and entropy are inadequate in compensating for the effects of of U k S k as compute the relevance scores using instead of using Eq. (3) in standard LSI.

The normalized LSI can be alternatively viewed as an improvement to the query expansion interpre-word co-occurrence matrix is term in LSI space to 1 so that terms with small norm are promoted to be equals.

The complete normalized LSI is described below:  X  Compute the SVD with k factors U k , S k , V k .  X  Normalize the rows of the projected terms T  X  D 1 U k S k .  X  Compute the relevance score using Eq. (5) (in practice, we compute cosine between rows of X T T and columns of T T Q ).

The results of using NLSI on TREC6 are shown in Fig. 6 using both tf.idf and log.entropy effect on LSI performance for the NPL and TREC6 collections. For NPL, NLSI outperforms both term matching and LSI. For TREC6 NLSI improves upon standard LSI, but still falls short of term matching.
We also applied NLSI on MED, but NLSI performance is not as good as LSI. 6. Conclusions
LSI projects the documents of a collection into a lower dimensional space in order to improve retrieval performance. This work examines the properties of SVD-based projections in order to determine whether they agree with our intuition about IR concepts. The lower dimensionality of the space is intuitively also see that in LSI the representation of rare terms (with low norm) are not adequately weighted, sometimes resulting in poor retrieval performance. We proposed an LSI normalization scheme based on the query expansion interpretation of LSI. Normalized LSI partially compensates for this inadequacy and resulted in better retrieval precision on NPL and TREC.
 Acknowledgements This work is supported by the U.S. Department of Energy (Office of Science, Office of Laboratory Policy and Infrastructure Management, and Office of Advanced Scientific Computing Research, Divi-sion of Mathematical, Information, and Computational Sciences) under Contract No. DE-AC03-76SF00098.
 References
