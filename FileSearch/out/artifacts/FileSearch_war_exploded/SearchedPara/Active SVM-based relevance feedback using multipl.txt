 1. Introduction
With the explosive growth in image records and the rapid increase of computer power, retrieving images from a large-scale image database becomes one of the most active research fields ( Datta et al., 2008 ; Hu et al., 2011 ). To give all images text annotations manually is tedious and impractical and to automa-tically annotate an image is beyond current technology. Content-based image retrieval (CBIR) is a technique to retrieve images semantically relevant to the user X  X  query from an image database, which has received much attention in the last decade.
Many CBIR systems have been developed, including QBIC, Photo-book, MARS, NeTra, PicHunter, Blobworld, VisualSEEK, SIMPLIcity, system, low-level visual image features (e.g., color, texture, and shape) are automatically extracted for image descriptions and indexing purposes. To search for d esirable images, a user presents similar images based on the ext racted features. However, the paramount challenge in CBIR is the so-called semantic gap between the low-level visual features and the high-level semantic concepts.
To bridge the semantic gap, relevance feedback (RF) methods were proposed to learn the user X  X  intentions ( Wang and Hua, 2011 ).
RF, originally developed for text retrieval systems, has found wide spread acceptance in many practical CBIR applications ( Wang and Hua, 2011 ; Auer and Leung, 2011 ; Azimi-Sadjadi et al., 2009 ). The main idea of RF is to let the user guide the system. The conventional process of RF is as follows: (1) from the retrieved images, the user labels a number of relevant samples as positive feedbacks, and a number of irrelevant samples as negative feedbacks; (2) the CBIR system then refines its retrieval procedure based on these labeled feedback samples to improve retrieval performance. These two steps can be carried out itera-tively. As a result, the performance of the system can be enhanced is an unprecedented development in the RF CBIR field, and many
RF methods have been introduced. These RF schemes can be roughly divided into subspace learning, random sampling, feature selection, and support vector machine (SVM) based methods ( Wang and Hua, 2011 ; Auer and Leung, 2011 ; Azimi-Sadjadi et al., 2009 ; Bian and Tao, 2010 ). 1.1. Subspace learning
In essence, the subspace learning based RF methods either find a low-dimensional subspace of the feature space, such that the positive and negative samples are well separated after projection to this subspace, or define a (1  X  x ) class problem (biased discriminant analysis or BDA for short) and find a subspace within which to separate the one positive class from the unknown number of negative classes. He and Cai (2009) proposed a novel active subspace learning algorithm which selects the most infor-mative data points and used them for learning an optimal sub-space. Using techniques from experimental design, they discussed how to perform data selection in supervised or semi-supervised subspace learning by minimizing the expected error. Lu and He (2005) proposed a semi-supervised subspace learning algorithm for image retrieval, and it is fundamentally based on locality-preserving projection (LPP) which can incorporate user X  X  RFs. As the user X  X  feedbacks are accumulated, a semantic subspace can be ultimately obtained in which different semantic classes can be best separated and the retrieval performance can be enhanced. By in semantic and geometric (image) domains, Yu and Tian (2008) proposed an optimal semantic subspace projection (SSP) that captures the most important properties of the subspaces with respect to classification. Si et al. (2010) presented a family of subspace learning algorithms based on a new form of regulariza-tion, which transfers the knowledge gained in training samples to testing samples. In particular, the new regularization minimizes the Bregman divergence between the distribution of training samples and that of testing samples in the selected subspace, so it boosts the performance when training and testing samples are not independent and identically distributed. Mehdizadeh et al. (2011) proposed a subspace learning method based on semi-supervised neighborhood preserving discriminant learning, which was called semi-supervised neighborhood preserving discrimi-nant embedding (SNPDE). The method preserves the local neigh-borhood structure of face manifold and maximizes the (LDA). The subspace learning can reduce the dimensionality of task with C classes, if the dimension of the projected subspace is strictly lower than C-1, the projection to a subspace tends to merge those classes, which are close together in the original feature space. In addition, the subspace learning FR tends to give undesired results in the reduced subspace if samples in a class are multimodal. 1.2. Random sampling
Random sampling is an essential component of bagging, which is a variance reduction technique. Random sampling based RF approaches can apply statistical sampling techniques to reduce some particular problems in RF. For example, the classifier unstable problem caused by the insufficient number of labeled feedback samples, the classification hyperplane biased problem caused by the imbalanced distributions of positive and negative classes, and the over-fitting problem caused by the very high-dimensional features used for image representation etc. ( Bian and
Tao, 2010 ). Sirikunya et al. (2010) proposed a new RF approach based on a lazy processing framework, in which the random sampling, data clustering, and ensembles of classifiers are com-bined. Zhou and Antonio Robles-Kelly (2008) presented a method for image retrieval that employs a maximum likelihood method to organize the dataset and a quasi-random stratified sampling for the query operation. The method is quite general in nature and allows the use of a variety of metrics between image pairs and descriptors. Tao et al. (2007) proposed an asymmetric bagging-based SVM (AB-SVM) and a random subspace SVM (RS-SVM), and built an asymmetric bagging and random subspace SVM (ABRS-
SVM) by integrating AB-SVM and RS-SVM. For random sampling based RF approaches, it is a difficult issue how to combine multiple classifiers to form a classification. 1.3. Feature selection
Generally, the feature selection based FR methods can adjust weights associated with various dimensions of the feature space to enhance the importance of those dimensions that help in retrieving the relevant images and to reduce the importance of those dimensions that hinder the retrieval performance. Hota et al. (2010) proposed a novel approach which incorporates local feature representation for retrieval of gray and color images from an archive with user intervention. They used histogram features, which are computationally efficient, hence resulting in quick image retrieval. The computed image feature vectors are used for similarity matching with weighted feed-backed image retrie-val. Kherfi and Ziou (2006) presented a new RF framework based on a feature selection algorithm that nicely combines the advan-tages of a probabilistic formulation with those of using both the positive example (PE) and the negative example (NE). Ziou et al. (2009) proposed a probabilistic framework for efficient retrieval and indexing of image collections. This framework uncovers the hierarchical structure underlying the collection from image fea-tures based on a hybrid model that combines both generative and discriminative learning. Piras and Giacinto (2009) proposed a weighted similarity measure based on the nearest-neighbor RF technique. Each image is ranked according to a relevance score depending on nearest-neighbor distances from relevant and non-relevant images. Distances are computed by a weighted measure, the weights being related to the capability of feature spaces of representing relevant images as nearest-neighbors. 1.4. SVM based method
SVM based FR methods either estimate the density of positive instances or regard RF as a classification problem with the positive and negative samples as training sets ( Zhang et al., 2001 ). Zagoris et al. (2011) proposed an MPEG-like descriptor that contains conventional contour and region shape features with a wide applicability from any arbitrary shape to document retrieval through word spotting. Its size and storage requirements are kept to minimum without limiting its discriminating ability. In addition to that, a RF technique based on SVMs is provided that employs the proposed descriptor with the purpose to measure how well it performs with it. Rahman et al. (2011) presented a classification-driven biomedical image retrieval framework based on image filtering and similarity fusion by employing supervised learning techniques. In this framework, the probabilistic outputs of a multiclass SVM classifier as category prediction of query and database images are exploited at first to filter out irrelevant images, thereby reducing the search space for similarity match-ing. Images are classified at a global level according to their modalities based on different low-level, concept, and keypoint-based features. SVM active learning ( Tong and Chang, 2001 ), which plays an important role in CBIR RF research, selects the samples near the SVM boundary and queries the user for labels. After training, the points near the SVM boundary are regarded as the most informative images while the most-positive images are the farthest ones from the boundary on the positive side. Wu et al. (2010) proposed a novel scheme that combines semi-supervised learning, ensemble learning and active learning in a uniform framework. Concretely, unlabeled data is exploited to facilitate ensemble learning by helping augment the diversity among the base SVM classifiers, and then the learned SVM ensemble model is used to identify the most informative examples for active learning. In particular, a bias-weighting mechanism is developed to guide the ensemble model to pay more attention on the positive examples than the negative ones. Marakakis et al. (2009) proposed a RF approach for content based image retrieval, which combines SVM with Gaussian Mixture (GM) models.
Specifically, it constructs GM models of the image features distribution to describe the image content and trains an SVM classifier to distinguish between the relevant and irrelevant images according to the preferences of the user. Huang et al. (2006) proposed a novel paired feature AdaBoost learning system for relevance feedback-based image retrieval. To facilitate density estimation in the feature learning, the author proposed an ID3-like balance tree quantization method to preserve most discrimi-native information. By using paired feature combination, they mapped all training samples obtained in the relevance feedback process onto paired feature spaces and employed the AdaBoost algorithm to select a few feature pairs with best discrimination capabilities in the corresponding paired feature spaces. Liu and
Wang (2008) proposed a SVM-based active feedback in image retrieval using clustering and unlabeled data, in which a new active selection criterion to select images for user X  X  feedback is designed, and unlabeled images are incorporated within the co-training framework. Ye et al. (2010) proposed and evaluated a regression-based document re-ranking approach for image retrie-val, in which the SVM regression model is used to learn a re-ranking function automatically. Under this regression-based fra-mework, rich features can be utilized to re-rank the first-pass retrieved documents by traditional weighting models. Min and
Cheng (2009) presented a fuzzy SVM (FSVM) that is more robust to the problems, such as small size of samples, biased hyperplane, over-fitting and real-time. However, it is difficult to select an effective fuzzy membership function, because different people have different methods to select the fuzzy membership function.
The SVM based FR methods have been popular because they can outperform many other classifiers when the size of the training set is small. However, the conventional SVM-based RF treats the positive and negative feedbacks equivalently although this assumption is not always appropriate, since the two groups of training feedbacks have very different properties. That is, all positive feedbacks share a homogeneous concept while negative feedbacks do not. In addition, the conventional SVM-based RF usually ignores the unlabeled samples, which are very helpful in constructing a good classifier.

In this paper, we present an active SVM-based RF using multiple classifiers ensemble and features reweighting. Firstly, we select the most informative images by using active learning method for user to label, and quickly learn a boundary that separates the images that satisfy the user X  X  query concept from the rest of the dataset. Secondly, the feature space is modified dynamically by appropriately weighting the descriptive features according to a set of statistical characteristics. Then, a set of moderate accurate one-class SVM classifiers are trained sepa-rately by using different sub-features vectors. Finally, we com-pute the weight vector of component SVM classifiers dynamically by using the parameters for positive and negative samples, and combine the results of the component classifiers to form an output code as a hypothesized solution to the overall image retrieval problem.

The rest of this paper is organized as follows. Section 2 presents the basic theory about SVM active leaning. Section 3 describes the SVM ensemble method. In Section 4 , the features reweighting is discussed. Section 5 contains the description of our
SVM-based active feedback system. Simulation results in Section 6 will show the performance of our scheme. Finally, Section 7 concludes this presentation. 2. SVM active leaning
Recently, the SVM based RF has shown promising results owing to its good generalization ability. SVM has a very good performance for pattern classification problems by minimizing the Vapnik X  X hervonenkis dimensions and achieving a minimal structural risk. SVM active learning ( Tong and Chang, 2001 ;
Gosselin and Cord, 2008 ) halves the image space each time in which the most positive samples are selected farthest from the classifier boundary on the positive side and the samples close to the boundary are deemed as the most informative ones for the user to label. Active learning is close to supervised learning, except that training data are not independent and identically distributed variables. Some of them are added to the training set thanks to a dedicated process. Active learning methods have been introduced to perform good classifications with few training data in comparison to the standard supervised scheme. 2.1. Example of active strategies
Fig. 1 shows the interest of a selection step. In this example, the images are represented by 2-D feature vectors, the white provided two labels, represented in figures by larger circles (see Fig. 1 (a)). These two labels allow the system to compute a first classification. In classical RF systems, a common way of selection was to label the most relevant pictures returned by the system. As new examples may be considered. For instance, in Fig. 1 (c), the active learning selection working on uncertainty is proposed to the boundary, resulting in an enhanced classification in that case ( Fig. 1 (c)). 2.2. Optimization scheme
A determined in order to rank the whole database. New notations are 1, the indexes of the labeled images I , and the unlabeled ones I .
The active learning aims at selecting the unlabeled data x will enhance the most relevance function f trained with the label s  X  x tion problem, a cost function gA y is introduced. According to any is x i n minimizing gA y ( x ) over the pool of unlabeled images i n  X  argmin 2.3. SVM active learning
In this paper, we use a SVM active learning method. Our SVM active learning system takes the simple approach of choosing the pool-queries to be the fifteen images closest to its separating hyperplane. Compared with other active learning methods which are unstable during the first round of querying. our system always randomly chooses fifteen images for the first RF round. Then it uses the simple active method on the second and subsequent rounds.

To summarize, our SVM active learning system performs the following for each round of RF: Learn an SVM on the current labeled data.

If this is the first feedback round, ask the user to label fifteen randomly selected images. Otherwise, ask the user to label the fifteen pool images closest to the SVM boundary.

After the RF rounds have been performed, SVM active learning system retrieves the top-k most relevant images: Learn a final SVM on the labeled data.

The final SVM boundary separates  X  X  X elevant X  X  images from irrelevant ones. Display the k  X  X  X elevant X  X  images that are farthest from the SVM boundary. 3. SVM ensemble
An ensemble of classifiers is a set of classifiers whose individual decisions are combined in some way to classify new examples. is method for constructing good ensemble of classifiers. The main discovery is that ensembles are often much more accurate than the 2003 ). Assume that there is an ensemble of n classifiers: { f wrong at the same data, where an ensemble will show the same performance as individual classi fiers. However, if classifiers are result of majority voting can be correct.

In recent years, a lot of researchers pay much attention to SVM the SVM ensemble. During the training phase, each individual SVM is trained independently by its own replicated training data set. All constituent SVMs will be aggregated by various combina-to all SVMs simultaneously and a collective decision is obtained based on the aggregation strategy.

Many methods for constructing an ensemble of classifiers have been developed. The most important thing in constructing the SVM ensemble is that each individual SVM becomes different with another SVM as much as possible. This requirement can be met by using the training samples are bagging, boosting, randomization, stacking and dagging etc. Among them, we put focus on the representative methods  X  Adaboosting and Majority voting.

Adaboosting is the representative boosting algorithm, which is based on the principle of error minimization. Adaboosting can obtain the best classification results by repeatedly adjusting the Another method is based on Majority voting mechanism to obtain the scores of individual classifier, and then, choose the one whose score is high. The disadvantage of Majority voting method is that the scores of individual classifier may be very similar when users focus on many classifiers at the same time. The common feature of the above two methods is that the individual classifier treats the classification ability of positive samples and negative samples equally. In this paper, we adjust the parameters of the positive and negative samples to improve the classification performance.
Suppose there are n visual feature spaces, we construct a SVM classifier for each visual feature space in each feedback, the SVM classifier is represented as P the image database uses X as its corresponding eigenvector, and P i th feature space. The probability of the image which is relevant after the feedback is represented as P  X  x  X  X  o i P t i  X  x  X  X  3  X  where o i is the weight of the corresponding classifier. The following describes how to compute the weight.

Based on the visual feature i for each classifier, the greater the training error, the higher the level of such features which users set, the value of the probability of images which are relevant images (or irrelevant images) reflects the correct level of the classifier. The weight of the classifier is represented as  X   X  b i  X  1
When the image X belongs to the relevant image, y  X  1; otherwise, y  X  1. a i and b i are the importance of the relevant and the irrelevant image for the i th classifier. The greater a more important the classifier to the relevant images is, the more weight of the corresponding classifier is. Extremely, if a shows the classifier only considers the relevant images. 4. The features reweighting 4.1. Features reweighting and user feedback
The aim of weight updating is to emphasize the most dis-criminating parameters. In practice, the idea is to perform a dynamic feature selection, driven by the user feedbacks. The feature reweighting algorithm used in this work is similar to the one proposed in Mattia and Francesco (2010) and Wu and Zhang (2002) and is based on a set of statistical characteristics.
For a given query image with the optimized query points q
Each image i k n A I k is represented by m features f k n
Let the set of relevant and irrelevant images after the k th RF iteration be R k and U k , respectively, where I k  X  R k [ U images in R k , we stack their s th component of feature m into component of feature m is defined as
F with f f where Min and Max are the functions giving the minimum and axis of the s th component of feature m spanned by the relevant images after the k th iteration.

The confusion set of the s th component of feature m after the k th iteration is given by
C dominant range F k m , s after the k th iteration. The discriminant ratio of the s th component of feature m is defined as d  X  1
The discriminant ratio indicates the ratio of irrelevant images located outside of the dominant range over all irrelevant images, and it shows the ability of component s of feature m in separating irrelevant images from relevant ones. Denote the standard devia-w  X  4.2. Disturbing factor
The RF may converge with only few or even just one relevant image. This is because the feature weights are trapped in some suboptimal state, which can be detected by the following conditions
When either one of the above conditions holds, we think the feature weights is trapped in a suboptimal state ( Mattia and Francesco, 2010 ; Wu and Zhang, 2002 ). To push the feature weights out of the sub-optimum, we use a disturbing factor measured from the scatter of the classes of relevant images and irrelevant ones. In reality, irrelevant images tend to be multi-model, but we simplify the situation by regarding them as one class, since we just wish to resume the feature reweighting process when it is stuck.

Denote the mean values of F k , R m , s and F k , U m , s disturbing factor is given by: l
The above formula is the Fisher criterion ( Wu and Zhang, 2002 ; Fukunage, 1990 ), which has been extensively used in measuring the scatter between two classes. The weight w k then updated by w
After each feedback iteration, the feature vectors of the query are set to the average values of all relevant feature vectors. 5. Image retrieval system
For CBIR the search engine is required to feedback the most semantically relevant images after each previous RF iteration.
The user will not label many images for each iteration and will usually only do a few iterations. Fig. 3 describes our image retrieval system framework with RF. From Fig. 3 , we can see that our SVM feedback system mechanism has four main components: query, retrieval, labeling, and learning. Compared with the tradi-tional CBIR without feedback, there are two more components which are labeling and learning, and they are the key contribu-tions in the RF system.

Query unit : extract three features of every image in the database, then, store these features into the feature database.
The three features are color, texture, and shape respectively. The details of the three features are given as follows: (1) Color feature . Global color histogram (GCH) features are used as the color features. First, we convert the color space from
RGB into HSV. Then, we quantify the HSV color space into 64 bins, hue, saturation, and value are quantized into 8 bins, 4 bins, and 2 bins respectively. (2) Texture feature . Pyramidal wavelet transform (PWT) features are employed as the texture features. An image is transferred into the YCrCb color space, and Haar Wavelet transform is applied to the component of Y. The mean and standard deviation are calculated in terms of the subbands at each decomposed level. The vector dimension is 18. (3) Shape feature . Edge direction histogram (EDH) features are used as the shape features. The edge of the images is extracted using the Sobel edge detection algorithm. An image is transferred into the YCrCb color space, and the statistical calculated on the component of Y. The vector dimension of EDH is 5.

Retrieval unit : selecting randomly an image from the image database as the query image, and then, compute the similarity between the query image and the images in the database by
Euclidean distance and return 10 most similar images as the retrieval result.

Labeling unit : labeling the top 10 returned images as the positive feedback sample or the negative feedback samples at the first feedback. Moreover, select other 290 most informative samples from the unlabeled image database as the negative feedback sample to improve the performance of system with the large number of samples. At the next feedback iterations, the labeled samples are selected from the feedback pool ( N  X  15) and the unlabeled image database, the total number of the labeled samples is 300.

Learning unit : training a RF model based on the SVM machine learning algorithm. In this unit, we first modify dynamically the feature space by appropriately weighting the descriptive features (i.e. color, texture, and edge) according to a set of statistical characteristics. Then, we construct the SVM classifiers separately for three new sub-feature vectors. Finally, we use SVM ensemble method (see Section 3 ) to fuse the individual SVM classifier. The ensemble SVM classifier resort the remaining images and return the feedback results, if the user is satisfied with the current retrieval result, the feedback process will be stopped; otherwise user label new samples and go to the next feedback iteration. The specific algorithm is shown in Table 1 . Fig. 4 shows our image retrieval system interface.

As shown in Table 1 , the proposed algorithm in this paper initial retrieval, we compute the visual features of images in the database and store them into the feature database, then, the user select randomly an image from the image database as the query image and compute the similarity between the query image and every image in the image database and, finally, sort the similarity and return the top 10 images as the retrieval results. After go to samples or the negative samples at the first feedback, then, select more negative samples from the unlabeled images which is shown in Table 1 . After that, we modify dynamically the feature space by appropriately weighting the descriptive features accord-to train three individual SVM classifiers. Finally, we compute the weights for each of individual SVM classifiers, and fuse the three individual SVM classifiers. The integrated SVM classifier returns 10 images as the feedback results and 15 images in the pool which are to be labeled in the next feedback iteration. After the first feedback, the user labels the training samples from the pool, the other processes are the same as the above. When the user is satisfy with the feedback result, the feedback iterations stop.
It can be seen from above flow that no additional efforts are required in the feedback process, such as asking the user to label more examples, what the user to do is to label the returned images to be the positive or the negative samples. This algorithm puts minimum burden onto the user, so it comply with the requirements of the algorithm.
 6. Experimental results
To evaluate the performance of the proposed algorithm, we conduct an extensive set of CBIR experiments by comparing the proposed algorithm to several state-of-the-art feedback methods ( Zhang et al., 2001 ; Tong and Chang, 2001 ;
Huang et al., 2006 ; Liu and Wang, 2008 ) that have been used in image retrieval.
 6.1. Image database
We perform experiments over 15,000 images from 150 cate-gories of the COREL photo gallery, in which each category contains 100 images. Every database image is of size 256 384 or 384 256. To evaluate the retrieval performance, we need a ground truth to assess the relevance of the test query images. We follow the previous work to construct the ground truth by merging semantically similar categories. The reorganized data-base consists of 71  X  X  X emantic categories X  X . In our experiments, 10,000 images are randomly selected out of the whole databases as test queries. We define images within the same semantic category as relevant. Fig. 5 (a) shows some image examples in this dataset.

We also perform experiments over 10,000 images from another image dataset, which is constructed by collecting images from the Internet. Fig. 5 (b) shows some image examples in our constructed image dataset. 6.2. Comparative performance evaluation
We report experimental results that show the feasibility and utility of the proposed algorithm and compare its performance with four state-of-the-art feedback methods ( Zhang et al., 2001 ; Tong and Chang, 2001 ; Huang et al., 2006 ; Liu and Wang, 2008 ). query images used in all the experiments is generated at random.
At the beginning of image retrieval, the images in the database are ranked according to their Euclidean distances to the query, and top 15 images are labeled as the set of initially labeled data for learning system. In many interactive CBIR systems, the user is required to label more than 20 images in each round of RF, which is not practical because few users are patient to label so many images. In our system, only 15 images judged as the most informative ones are put into a pool in each round of RF, which is shown in Fig. 4 . In particular, only the positive images are required to be marked by user and all the other images are automatically marked as negative by the system.

Fig. 6 shows the retrieval results by using the proposed algorithm relevant image, and  X  X  N  X  X  denotes nonrelevant image.
Figs. 7 X 14 show the retrieval results using different RF after the as the number of the feedback iteration increases, the performance of our SVM RF retrieval system becomes better and better, and it is more effective than four state-of-the-art feedback methods. We use the Average Precision (AP) measure defined by NISTTREC video (TRECVID) as our retrieval performance metric ( Huiskes and Lew, 2008 ). At each query session our system refines the retrievals by executing the proposed SVM RF framework for several iterations. The AP value that can be obtained at each Average AP 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Average AP Average AP 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 Average AP iteration is defined as the average of precision value obtained ratio between the retrieved relevant pictures and the number of pictures currently retrieved. Let P be the AP obtained at the current feedback iteration and it is computed by P  X  P D where P i denotes the precision value obtained after the system the query, and 9 R 9 denotes the cardinality of R . As an example, assume that one of the classes consists of six relevant pictures second, fourth, seventh, thirteenth, and eighteenth places. Thus, the precision value obtained when each relevant picture is retrieved is 1, 1, 0.75, 0.57, 0.38, and 0.33, respectively. The AP computes the average of these precision values and it is 0.67. The
AP calculated over all relevant pictures can avoid precision fluctuation that is usually encountered by the traditional preci-sion measure. Fig. 15 shows the variations of the average AP as the number of feedback iterations increases. Note that the average AP obtained at the zero feedback iteration indicates the average AP calculated at the first retrieval result of each query session before activating the RF process. Fig. 16 shows the variations of the average AP as the number of returned images increases. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 Average AP 0 10 20 30 40 50 60 70 80 Time (second) 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 Average AP 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Average AP
Fig. 17 shows the relationship between average AP and number of the training samples. Fig. 18 shows the variations of the time as the number of feedback iterations increases. 7. Conclusion
Recently, SVM has been widely applied in RF, which plays an essential role in improving the performance of CBIR. The main advantage of SVM is that its good generalization ability, without restrictive assumptions regarding the data, fast learning and evaluation for RF, and flexibility, etc. SVM active learning, which plays an important role in CBIR RF research, selects the samples near the SVM boundary and queries the user for labels. After training, the points near the SVM boundary are regarded as the most informative images while the most-positive images are the farthest ones from the boundary on the positive side. However, the conventional active SVM-based RFs are often very complex and some unsatisfactory relevance of results occurs frequently. To overcome the above limitations, we propose an active SVM-based RF using multiple classifiers ensemble and features reweighting.
The novelty of our work lies in the use of an improved SVM classifier, and in which: (1) the feature space is modified dyna-mically by weighting the descriptive features according to a set of statistical characteristics, (2) the individual SVM classifier is trained separately by using different new sub-features vectors, and (3) the individual SVM classifier is fused by utilizing the SVM ensemble method. Through experiments on a subset of Corel
Photo Gallery with 15,000 images and the constructed image dataset with 10,000 images, we show that our new method can improve the conventional SVM active learning based RF consistently.

Future work includes the extension of the proposed frame-work to video retrieval. Another important issue that will be investigated is the use of indexing structures to speed up the overall retrieval performance.
 Acknowledgment This work was supported by the National Natural Science
Foundation of China under Grant nos. 60773031 and 60873222, the Open Foundation of State Key Laboratory of Information
Security of China under Grant no. 04-06-1, the Open Foundation of Network and Data Security Key Laboratory of Sichuan Province, the Open Foundation of Key Laboratory of Modern Acoustics Nanjing University under Grant no. 08-02, and Liaoning Research
Project for Institutions of Higher Education of China under Grant nos. 2008351 and L2010230.
 References
