 To date, many unsupervised WSD systems rely on a sense similarity module that returns a similar-ity score given two senses. Many similarity mea-sures use the taxonomy structure of WordNet [WN] (Fellbaum, 1998), which allows only noun-noun and verb-verb pair similarity computation since the other parts of speech (adjectives and adverbs) do not have a taxonomic representation structure. For example, the jcn similarity measure (Jiang and Conrath, 1997) computes the sense pair similarity score based on the information content of three senses: the two senses and their least common subsumer in the noun/verb hierarchy.
 The most popular sense similarity measure is the Extended Lesk [ elesk ] measure (Banerjee and Peder-sen, 2003). In elesk , the similarity score is computed based on the length of overlapping words/phrases between two extended dictionary definitions. The definitions are extended by definitions of neighbor senses to discover more overlapping words. How-ever, exact word matching is lossy. Below are two definitions from WN: Despite the high semantic relatedness of the two senses, the overlapping words in the two definitions are only a, the , leading to a very low similarity score.
Accordingly we are interested in extracting latent semantics from sense definitions to improve elesk . However, the challenge lies in that sense defini-tions are typically too short/sparse for latent vari-able models to learn accurate semantics, since these models are designed for long documents. For exam-ple, topic models such as LDA (Blei et al., 2003), can only find the dominant topic based on the ob-served words in a definition ( financial topic in bank # n #1 and stock # n #1 ) without further dis-cernibility. In this case, many senses will share the same latent semantics profile, as long as they are in the same topic/domain.

To solve the sparsity issue we use missing words as negative evidence of latent semantics, as in (Guo and Diab, 2012). We define missing words of a sense definition as the whole vocabulary in a corpus minus the observed words in the sense definition. Since observed words in definitions are too few to reveal the semantics of senses, missing words can be used to tell the model what the definition is not about. Therefore, we want to find a latent semantics pro-file that is related to observed words in a definition, but also not related to missing words, so that the in-duced latent semantics is unique for the sense.
Finally we also show how to use WN neighbor sense definitions to construct a nuanced sense simi-larity wmfvec , based on the inferred latent semantic vectors of senses. We show that wmfvec outperforms elesk and LDA based approaches in four All-words WSD data sets. To our best knowledge, wmfvec is the first sense similarity measure based on latent se-mantics of sense definitions. Table 1: Three possible hypotheses of latent vectors for 2.1 Intuition Given only a few observed words in a definition, there are many hypotheses of latent vectors that are highly related to the observed words. Therefore, missing words can be used to prune the hypotheses that are also highly related to the missing words.
Consider the hypotheses of latent vectors in ta-ble 1 for bank # n #1 . Assume there are 3 dimen-sions in our latent model: financial, sport, institu-tion . We use R v o to denote the sum of relatedness between latent vector v and all observed words; sim-ilarly, R v m is the sum of relatedness between the vector v and all missing words. Hypothesis v 1 is given by topic models, where only the financial dimension is found, and it has the maximum relat-edness to observed words in bank # n #1 definition R o = 20 . v 2 is the ideal latent vector, since it also detects that bank # n #1 is related to institution . It has a slightly smaller R v 2 o = 18 , but more impor-tantly, its relatedness to missing words, R v 2 m = 300 , is substantially smaller than R v 1 m = 600 .
However, we cannot simply choose a hypothesis with the maximum R o  X  R m value, since v 3 , which is clearly not related to bank # n #1 but with a min-imum R m = 100 , will therefore be (erroneously) returned as the answer. The solution is straightfor-ward: give a smaller weight to missing words, e.g., so that the algorithm tries to select a hypothesis with maximum value of R o  X  0 . 01  X  R m . We choose weighted matrix factorization [WMF] (Srebro and Jaakkola, 2003) to implement this idea. 2.2 Modeling Missing Words by Weighted We represent the corpus of WN definitions as an M  X  N matrix X , where row entries are M unique words existing in WN definitions, and columns rep-resent N WN sense ids. The cell X ij records the TF-IDF value of word w i appearing in definition of sense s j .

In WMF, the original matrix X is factorized into two matrices such that X  X  P &gt; Q , where P is a K  X  M matrix, and Q is a K  X  N matrix. In this scenario, the latent semantics of each word w i or sense s j is represented as a K -dimension vector P  X  ,i or Q  X  ,j respectively. Note that the inner product of P  X  ,i and Q  X  ,j is used to approximate the seman-tic relatedness of word w i and definition of sense s j : X
In WMF each cell is associated with a weight, so missing words cells ( X ij =0) can have a much less contribution than observed words. Assume w m is the weight for missing words cells. The latent vec-tors of words P and senses Q are estimated by min-imizing the objective function: 1
Equation 1 explicitly requires the latent vector of sense Q  X  ,j to be not related to missing words ( P  X  ,i  X  Q  X  ,j should be close to 0 for missing words X ij = 0 ). Also weight w m for missing words is very small to make sure latent vectors such as v 3 in table 1 will not be chosen. In experiments we set w m = 0 . 01 .
After we run WMF on the definitions corpus, the similarity of two senses s j and s k can be computed by the inner product of Q  X  ,j and Q  X  ,k . 2.3 A Nuanced Sense Similarity: wmfvec We can further use the features in WordNet to con-struct a better sense similarity measure. The most important feature of WN is senses are connected by relations such as hypernymy, meronymy, similar at-tributes , etc. We observe that neighbor senses are usually similar, hence they could be a good indica-tor for the latent semantics of the target sense. We use WN neighbors in a way similar to elesk . Note that in elesk each definition is extended by in-cluding definitions of its neighbor senses. Also, they do not normalize the length. In our case, we also adopt these two ideas: (1) a sense is represented by the sum of its original latent vector and its neigh-bors X  latent vectors. Let N ( j ) be the set of neigh-bor senses of sense j . then new latent vector is: Q stead of cosine similarity) of the two resulting sense vectors is treated as the sense pair similarity. We refer to our sense similarity measure as wmfvec . Task: We choose the fine-grained All-Words Sense Disambiguation task, where systems are required to disambiguate all the content words (noun, adjective, adverb and verb) in documents. The data sets we use are all-words tasks in SENSEVAL2 [SE2], SENSE-VAL3 [SE3], SEMEVAL-2007 [SE07], and Semcor. We tune the parameters in wmfvec and other base-lines based on SE2, and then directly apply the tuned models on other three data sets.
 Data: The sense inventory is WN3.0 for the four WSD data sets. WMF and LDA are built on the cor-pus of sense definitions of two dictionaries: WN and Wiktionary [Wik]. 2 We do not link the senses across dictionaries, hence Wik is only used as augmented data for WMF to better learn the semantics of words. All data is tokenized, POS tagged (Toutanova et al., 2003) and lemmatized, resulting in 341,557 sense definitions and 3,563,649 words.
 WSD Algorithm: To perform WSD we need two components: (1) a sense similarity measure that re-turns a similarity score given two senses; (2) a dis-ambiguation algorithm that determines which senses to choose as final answers based on the sense pair similarity scores. We choose the Indegree algorithm used in (Sinha and Mihalcea, 2007; Guo and Diab, 2010) as our disambiguation algorithm. It is a graph-based algorithm, where nodes are senses, and edge weight equals to the sense pair similarity. The final answer is chosen as the sense with maximum inde-gree. Using the Indegree algorithm allows us to eas-ily replace the sense similarity with wmfvec . In In-degree, two senses are connected if their words are within a local window. We use the optimal window size of 6 tested in (Sinha and Mihalcea, 2007; Guo and Diab, 2010).
 Baselines: We compare with (1) elesk , the most widely used sense similarity. We use the implemen-tation in (Pedersen et al., 2004).

We believe WMF is a better approach to model latent semantics than LDA, hence the second base-line (2) LDA using Gibbs sampling (Griffiths and Steyvers, 2004). However, we cannot directly use estimated topic distribution P ( z | d ) to represent the definition since it only has non-zero values on one or two topics. Instead, we calculate the latent vec-tor of a definition by summing up the P ( z | w ) of all constituent words weighted by X ij , which gives much better WSD results. 3 We produce LDA vec-tors [ ldavec ] in the same setting as wmfvec , which means it is trained on the same corpus, uses WN neighbors, and is tuned on SE2.

At last, we compare wmfvec with a mature WSD system based on sense similarities, (3) (Sinha and Mihalcea, 2007) [ jcn + elesk ], where they evaluate six sense similarities, select the best of them and com-bine them into one system. Specifically, in their im-plementation they use jcn for noun-noun and verb-verb pairs, and elesk for other pairs. (Sinha and Mi-halcea, 2007) used to be the state-of-the-art system on SE2 and SE3. The disambiguation results ( K = 100 ) are summa-rized in Table 2. We also present in Table 3 results using other values of dimensions K for wmfvec and ldavec . There are very few words that are not cov-ered due to failure of lemmatization or POS tag mis-matches, thereby F-measure is reported.

Based on SE2, wmfvec  X  X  parameters are tuned as  X  = 20 ,w m = 0 . 01 ; ldavec  X  X  parameters are tuned as  X  = 0 . 05 , X  = 0 . 05 . We run WMF on WN+Wik for 30 iterations, and LDA for 2000 iterations. For LDA, more robust P ( w | z ) is generated by averag-ing over the last 10 sampling iterations. We also set a threshold to elesk similarity values, which yields better performance. Same as (Sinha and Mihalcea, 2007), values of elesk larger than 240 are set to 1, and the rest are mapped to [0,1]. elesk vs wmfvec : wmfvec outperforms elesk consis-tently in all POS cases (noun, adjective, adverb and verb) on four datasets by a large margin ( 2 . 9%  X  4 . 5% in total case). Observing the results yielded per POS, we find a large improvement comes from nouns. Same trend has been reported in other distri-butional methods based on word co-occurrence (Cai et al., 2007; Li et al., 2010; Guo and Diab, 2011). More interestingly, wmfvec also improves verbs ac-curacy significantly. ldavec vs wmfvec : ldavec also performs very well, again proving the superiority of latent semantics over surface words matching. However, wmfvec also outperforms ldavec in every POS case except Sem-cor adverbs (at least +1% in total case). We observe the trend is consistent in Table 3 where different di-mensions are used for ldavec and wmfvec . These results show that given the same text data, WMF outperforms LDA on modeling latent semantics of senses by exploiting missing words. jcn + elesk vs jcn + wmfvec : jcn + elesk is a very ma-ture WSD system that takes advantage of the great performance of jcn on noun-noun and verb-verb pairs. Although wmfvec does much better than elesk , using wmfvec solely is sometimes outperformed by jcn + elesk on nouns and verbs. Therefore to beat jcn + elesk , we replace the elesk in jcn + elesk with wmfvec (hence jcn + wmfvec ). Similar to (Sinha and Mihalcea, 2007), we normalize wmfvec similarity such that values greater than 400 are set to 1, and the rest values are mapped to [0,1]. We choose the value 400 based on the WSD performance on tun-ing set SE2. As expected, the resulting jcn + wmfvec can further improve jcn + elesk for all cases. More-over, jcn + wmfvec produces similar results to state-of-the-art unsupervised systems on SE02, 61 . 92 % F-mearure in (Guo and Diab, 2010) using WN1.7.1, and SE03, 57 . 4 % in (Agirre and Soroa, 2009) us-ing WN1.7. It shows wmfvec is robust that it not only performs very well individually, but also can be easily incorporated with existing evidence as rep-resented using jcn . 4.1 Discussion We look closely into WSD results to obtain an in-tuitive feel for what is captured by wmfvec . For ex-ample, the target word mouse in the context: ... in experiments with mice that a gene called p53 could transform normal cells into cancerous ones... elesk returns the wrong sense computer device , due to the sparsity of overlapping words between definitions of animal mouse and the context words. wmfvec chooses the correct sense animal mouse , by recog-nizing the biology element of animal mouse and re-lated context words gene, cell, cancerous . Sense similarity measures have been the core com-ponent in many unsupervised WSD systems and lexical semantics research/applications. To date, elesk is the most popular such measure (McCarthy et al., 2004; Mihalcea, 2005; Brody et al., 2006). Sometimes people use jcn to obtain similarity of noun-noun and verb-verb pairs (Sinha and Mihalcea, 2007; Guo and Diab, 2010). Our similarity measure wmfvec exploits the same information (sense defini-tions) elesk and ldavec use, and outperforms them significantly on four standardized data sets. To our best knowledge, we are the first to construct a sense similarity by latent semantics of sense definitions. We construct a sense similarity wmfvec from the la-tent semantics of sense definitions. Experiment re-sults show wmfvec significantly outperforms previ-ous definition-based similarity measures and LDA vectors on four all-words WSD data sets.

This research was funded by the Office of the Di-rector of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), through the U.S. Army Research Lab. All state-ments of fact, opinion or conclusions contained herein are those of the authors and should not be construed as representing the official views or poli-cies of IARPA, the ODNI or the U.S. Government.
