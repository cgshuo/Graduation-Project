 In the information age, people can easily collect information about the same set of entities from multiple sources, among which conflicts are inevitable. This leads to an important task, truth discovery , i.e., to identify true facts (truths) via iteratively updating truths and source reliability. However, the convergence to the truths is never discussed in existing work, and thus there is no theoretical guarantee in the re-sults of these truth discovery approaches. In contrast, in this paper we propose a truth discovery approach with the-oretical guarantee. We propose a randomized gaussian mix-ture model (RGMM) to represent multi-source data, where truths are model parameters. We incorporate source bias which captures its reliability degree into RGMM formula-tion. The truth discovery task is then modeled as seek-ing the maximum likelihood estimate (MLE) of the truth-s. Based on expectation-maximization (EM) techniques, we propose population-based (i.e., on the limit of infinite data) and sample-based (i.e., on a finite set of samples) solutions for the MLE. Theoretically, we prove that both solutions are contractive to an  X  -ball around the MLE, under certain condi-tions. Experimentally, we evaluate our method on both sim-ulated and real-world datasets. Experimental results show that our method achieves high accuracy in identifying truths with convergence guarantee.
  X  Information systems  X  Data mining; Truth Discovery; Mixture Model; Asymptotic Consistency
With the increase in our capabilities in collecting data from the physical world, an important feature of the data collection is its wide variety, i.e., data about the same objec-t can be obtained from various sources. For example, cus-tomer information can be found from multiple databases in a company, a patient X  X  medical records may be scattered at dif-ferent hospitals, and product specifications are typically list-ed at different websits (e.g., Amazon.com, ShopZilla.com). Conflicts among information from different sources are com-monly observed. Therefore, it is an important task to dis-cover the truth (i.e., correct information) out of conflicting multi-source data, which is referred to as truth discovery .
A trivial approach of accomplishing the truth discovery task is to treat the average of the data as the truth. The drawback of this simple averaging approach is that it treats each source equally reliable, which is often violated in real practice. Usually there exist sources with low-quality infor-mation, such as faulty sensors that keep emanating erroneous information, or spammers who propagate false information on the Web. To address this challenge, researchers propose a variety of approaches that infer both source reliability and truths from multi-source data. These approaches, referred to as truth discovery approaches, are developed based on heuris-tic principles [3,5 X 8,13,16,24], optimization [9,10,14,27], or probabilistic models [2, 4, 12, 15, 17, 19 X 21, 25, 26]. Despite the difference in the techniques, the underlying principle is the same: The sources which often provide truths should be reliable, and the information from reliable sources usually represent the truths. Based on this principle, existing truth discovery approaches usually start with an initialization of source reliability, and then conduct the following two steps iteratively until convergence: (i) Based on source reliability, obtain the truth by assign-(ii) Based on the truths, calculate the reliability degree of Although these algorithms differ in the specific ways to com-pute truths or source reliability, they typically follow this iterative procedure. However, the convergence of such an iterative procedure has not been discussed in existing work, and there is no theoretical guarantee in the results of existing truth discovery approaches.

In contrast, in this paper we propose an effective truth dis-covery approach with theoretical guarantee in convergence. Compared with existing approaches, the proposed approach follows the same principle that reliable sources provide truth-s and truths are stated by reliable sources, but the major ad-vantage of the proposed approach is its theoretical guarantee. Specifically, we introduce the bias of each source which is a ran dom variable measuring its reliability degree. To repre-sent multi-source data with various reliability degrees, we propose a R andomized G aussian M ixture M odel (RGMM) formulation, which consists of a gaussian mixture model [18] with sources X  biases incorporated. We cast the truth dis-covery problem as inferring a maximum likelihood estima-tor (MLE) of the unknown parameters in RGMM. To solve the problem, we derive both population-based and sample-based Expectation-Maximization (EM) solutions [4] for the limit of infinite data and a finite set of samples respective-ly. We name the two approaches as population -and sample -EMrgmm (i.e., EM for RGMM), respectively.
 Theoretically, we prove that the output of the proposed EMrgmm approaches converge to a MLE of the truth as the number of sources increases. The proof is first derived for population -EMrgmm . We first bound the distance between the population -EMrgmm and the MLE. In Property 1, we present the conditions under which population -EMrgmm es-timate is contractive to the MLE, in an  X  -ball around the MLE. Then to prove that the output of sample -EMrgmm converges to an  X  -ball around MLE, we bound the deviation from sample -EMrgmm to population -EMrgmm . To achieve this goal, we introduce the definition of the covering num-ber [22] of a metric space, and show that the distance be-tween population -EMrgmm and sample -EMrgmm is upper bounded by the number of sources (Corollary 1). Combining these theoretical results and applying triangle inequality, we prove that the distance between sample -EMrgmm and the MLE is bounded, under certain conditions.

Experimentally, we evaluate the effectiveness of the pro-posed sample -EMrgmm and verify the theoretical results on several simulated data as well as three real-world application datasets: Weather Forecast, Indoor Floorplan, and Stock data. Experimental results show that the proposed sample -EMrgmm approach are able to estimate truths from conflict-ing multi-source data. Compared with the state-of-the-art truth discovery algorithms, the proposed sample -EMrgmm can achieve comparable performance. Moreover, the esti-mate is asymptotically consistent to an  X  -ball around the MLE under the stated conditions.,
The remainder of the paper is organized as follows. We discuss related work in Section 2. In Section 3, we mathemat-ically formulate the task setting, introduce the randomized Gaussian mixture model, and derive the closed form EM al-gorithms for MLE. Then we present two important theoret-ical results for the convergence of the proposed approaches (Peoperties 1 and 2) in Section 4. The detailed proofs are presented in Appendix. In Section 5, we report experimen-tal results on both simulated and real-world datasets. We conclude the paper in Section 6.
The problem of truth discovery has attracted much atten-tion recently. People develop various approaches to extract true information from multiple sources of conflicting data. Initial studies [6,24] were motivated by the observation that source reliability and truths are highly relevant X  X ruths are often stated by many reliable sources and reliable sources tend to tell truths more often. Based on this principle, ap-proaches were developed to iteratively update source relia-bility and true facts. Later, various approaches were further developed to capture various factors that affect truth discov-ery [3,5,7,8,13,16].

In recent work, this principle is formulated as an optimiza-tion framework [10]. The objective is to minimize the overal-l distance between source observations and truths in which sources are weighted by their reliability degrees. Then this framework was extended to handle data with long-tail dis-tributions by calculating the confidence of source reliability estimates [9]. In [27], an optimization framework was devel-oped based on min-max entropy. The solutions to these op-timization formulations usually involve an iterative update of truths and source reliability as well.

In addition, probabilistic approaches [2,4,15,19 X 21,25,26] were developed to tackle the truth discovery task. The basic idea is to formulate multi-source data as certain mixture of distributions and incorporate source reliability as some ran-dom variable into the probabilistic models. The approaches differ in the way of selecting proper distributions and cap-ture source reliability to handle various scenarios in truth discovery, such as the existence of multiple truths, various d-ifficulty levels of the task, and various data types. To obtain the truths, these approaches try to maximize likelihood or posterior distributions, which leads to iteratively updates in model parameters and truth inference. In particular, Dawid &amp; Skene were the first to develop a maximum likelihood for-mulation and an expectation-maximization based approach to solve the problem. This approach was then adapted to social sensing scenarios in [19]. In [20], the authors develope-d a maximum likelihood estimator for source reliability and approximately quantified confidence in its estimation based on an asymptotic Cramer-Rao lower bound, but the conver-gence to the truth is not analyzed and cannot be guaranteed.
In summary, although the topic of truth discovery has been widely studied and most approaches achieve success in real-world applications, there is no theoretical guarantee that the results of these approaches converge to the truths. In this paper, we propose a novel truth discovery approach which can not only achieve comparable effectiveness in identifying the truths but also has theoretical guaranteed in the conver-gency to the truths.
In this section, we mathematically formulate the truth discovery task, propose a randomized gaussian mixture model to represent multi-source data with various relia-bility degrees, derive both population-based and sample-based expectation-maximization solutions (i.e., population-EMrgmm and sample-EMrgmm ), and finally present the complete algorithm of sample-EMrgmm .
We first introduce the notations that will be used through-out the paper and then state the target problem.

Input. Consider a set of entities N := { n } N n =1 that we are interested in, and there are S := { s } S 1 sources which provide information about all N entities. For the s -th source, denote its claims on N entities as X s = ( x s 1 ,  X   X   X  , x s N represents its claim on the n -th entity. Then, X = { X s } S represents the whole set of claims over the sources S .
Output. The truths for all entities are denoted as = (  X  1 ,  X   X   X  ,  X  N ), which is unknown a priori. Let X  X  also denote the estimator of the truth obtained from a truth discovery approach as b .

Truth Discovery Task. The truth discovery task is for-mally defined as follows: Given the data collection X , the go al of a truth discovery method is to obtain an estimate for all entities X  truths as close to as possible.
We summarize the notations in Table 1. Some of the no-tations will be introduced later in the paper.
 No tation Definition
Now we propose a probabilistic model with parameters whose estimate will be inferred via maximum likelihood es-timator technique. In truth discovery, different sources are treated differently depending on the quality of their data. To do this, we assume that the reliability of the s -th source is captured by a random variable s that measures bias (i.e., deviation from the truth). The smaller absolute value the bias s , the more reliable the s -th source. Considering all the S sources together, their reliability degrees { s } S assumed to follow certain distributions, for example, a uni-form distribution s  X  Uniform(  X  C, C ) N . This distribution models the overall quality of the collection of sources.
For the s -th source, we assume that the claims made by this source follow a multi-variate gaussian distribution with variance  X  2 I N , where I N is an identity matrix of size n . This assumption is typically adopted in many existing truth dis-covery work [9,10,27]. The mean of the gaussian distribution is  X  s in which is the truth and s is the source bias. Therefore, a more reliable source X  X  mean is closer to the truth and thus its claims are more likely to be close to the truth. With these notations, we can write the distribution of the claims made by the s -th source as: We further model the whole data collection using the follow-ing mixture model [23]: where we assume equal weights among components. Differ-ent from traditional gaussian mixture model with fixed mean in each component, the proposed model X  X  mean is random be-cause we incorporate the random variables  X  s as the source bias. Thus, we name the proposed model as R andomized G aussian M ixture M odel (RGMM). We introduce a latent variable Y  X  { s } S 1 as a source indicator, which is also an in-dicator of the underlying mixture component. Namely, the claims from the s -th component of RGMM is denoted as:
We assume that the pair ( X , Y ) are random variables in the sample spaces X  X Y . Based on the RGMM, we formulate the problem of truth discovery as the task of estimating the model parameter . Specifically, the objective is to obtain the estimate of which maximizes the likelihood of observ-ing the multi-source input. We define  X  as the maximum likelihood estimator of in the proposed RGMM formula-tion. In the following sections, we propose effective solutions to estimate  X  and then demonstrate the approaches X  con-vergence guarantee in Section 4.
The objective of the proposed approach is to obtain an es-timate of the unknown parameter  X  which maximizes the likelihood of RGMM on the data. An effective approach for deriving MLE is expectation-maximization (EM) method. We develop EM solutions for two versions of MLE estimates in this task: population-based MLE (assuming the limit in-finite data) and sample-based MLE (assuming a finite set of samples). We name these solutions as population -and sam-ple -EMrgmm , respectively. Data is finite in real practice, so we should use sample -EMrgmm to identify entities X  truths, but the introduction of population -EMrgmm enables us to conduct the convergence analysis for both solutions. In this subsection, we first introduce the general EM procedure, and then derive the EM updates of both population -and sample -EMrgmm .

First, let us briefly review the EM algorithm. Given the lower bounds on the log likelihood Q (  X | X  ), EM algorithm suc-cessively maximizes the lower bound and then reevaluates the lower bound at the new parameter value. The update procedure is as follows.

EM updates: Given t  X  1 obtained at the ( t  X  1)-th iter-ation, the t -th iteration of EM algorithm can be summarized in the following two steps: An EM-based method iteratively conduct these steps until some conditions are satisfied. Next, we introduce both the population -and sample -EMrgmm . population-EMrgmm . When deriving the E-step and M-step for population-based MLE of RGMM, we assume that there is an infinite set of samples. The population-based Q -function Q (  X | ) takes the form that
Q (  X  | ) = where f ( y | X ) denotes the conditional density of Y given X and g ( X ) is the density function of the observed variable X . The population-based EM operator M : R N  X  R N is defined as follows: M (  X  ) is to find the maximizer of the Q -function given the parameters obtained in previous step. sample-EMrgmm . sample -EMrgmm is derived on a finite set of the claims, i.e., X = { X s } S 1 . We assume that each sample given by every source is drawn i.i.d. from the mix-ture density Eq. (2). Under this assumption, we define the sample-based Q -function as Q S , which is shown as follows: Q P [ y = s  X  | X s ] is the probability that a sample X belongs to source s  X  , and its value is defined by the following function:
P [ y = s | X ] := e  X  To simplify the notation, we denote Eq. (7) as  X  ( X , s ). Substituting  X  ( X , s ) (Eq. (7)) into Eq. (6) and ignoring terms that do not contain , we show that the sample-based function Q S (  X  | ) takes the form: Q We denote the sample-based EM operator as M n : R N  X  R N , which is to maximize the sample-based Q -function. Namely, M S ( ) := arg max  X   X  R N Q S (  X  | ). According to Eq. (8), we have that Moreover, Eq. (9) implies that M ( ) = E [  X  ( X , )( X  X  where the expectation is taken over X  X  .
As discussed, sample -EMrgmm which deals with finite samples, is typically adopted in real practice. We summarize this algorithm in Algorithm 1. We will show its performance on both simulated and real-world datasets in Section 5. A lgorithm 1 sample -EMrgmm
In put: Entities N = { n } N 1 , Sources S = { s } S 1 , and data
Output: Truth estimates b 1: for entity n ( n = 1 ,  X   X   X  , N ) do 2: Calculate its variance  X   X  2 n over S sources; 3: Initialize  X  old n using the mean of claims over sources; 4: end for 5: Estimate model variance:  X   X  2 = 1 N 6: Est imate upper bound of biases C using the maximum 7: while convergence criterion is not satisfied do 8: For each source s , generate s  X  Uniform(  X  C, C ) N 9: Update truth estimator according to Eq. (9): 10: end while 11: return b = new
In this section, we theoretically present convergence analy-sis for the proposed solutions: population -and sample -rgmm . The outline of this section is: (1) In Property 1, we first provide conditions under which the distance between the population -EMrgmm  X  X  result and the MLE is bounded; (2) Based on the concept of covering number of a metric space, we bound the distance between the result of popula-tion -and that of sample -EMrgmm (Corollary 1); (3) Based on Property 1 and Corollary 1, applying triangle inequality, the error between the sample -EMrgmm  X  X  result and MLE is upper bounded by the number of samples (Property 2). Let X  X  first introduce the convergence property of the popula-tion -EMrgmm . Recall that  X  represents the maximizer of the population likelihood. [1] introduces the self-consistency property for the maximum likelihood estimator, that is, Eq. (10) implies that the maximum likelihood estimator should maximize the population-based Q-function. Combin-ing with Eq. (5), it is obvious that  X  = M (  X  ). For the proposed RGMM (Eq. (2)), we have the following property.
Property 1. Given the RGMM with a sufficiently small bias-to-mean ratio C  X   X   X  noise ratio  X   X   X  2 2 , there is a universal constant c &gt; 0 and a constant  X   X  (0 , 1) with  X   X  exp(  X  c  X   X   X  2 2 ) , such that holds for all if  X   X   X   X  2  X   X   X   X  2 4 .
 Pr oof. Please refer to  X  A for a detailed proof. The idea of the proof for Property 1 is adopted from [1]. However, two major differences are: (1) we incorporate a random variable for each latent component whose mean share the same sign, and (2) we consider arbitrary number of latent components more than 2. Both differences make the proof more complicated comparing with Corollary 1 in [1]. The detailed proof is deferred in Appendix A.

Property 1 establishes the conditions under which the con-vergence of the population -EMrgmm M ( ) is guaranteed. Namely, the proposed M ( ) is contractive over a small ball around  X  , a maximum likelihood estimator of the truths. Given an initial 0 , an immediate result from Property 1 is: Eq. (12) implies that M ( ) is linear convergence. Moreover, It shows that given any initialization, the proposed M ( ) is able to modify it as the iteration increases.
 To prove the convergence property of the sample -EMrgmm , we first measure the deviation of its result from that of pop-ulation -EMrgmm . As the covering number is used in the proof, we formally introduce its definition from [22].
Definition 1. Let G be a subset of a metric space.  X   X  &gt; 0 , the covering number N ( G ,  X  ) is defined to be the minimal integer n  X  N such that these n balls with radius  X  cover Based on Definition 1, the difference between population -EMrgmm ( M ( )) and sample -EMrgmm ( M n ( )) is upper bounded as follows.
 Corollary 1. Given the population-and sample-based EM operator M n and M , there exists a constant c such that holds with probability at least 1  X   X  , where  X  is the parameter space.

Proof. Please refer to  X  B for a detailed proof. Ba sed on Property 1 and Corollary 1, we can bound the distance between the result of sample -EMrgmm M n ( ) and the MLE  X  , which is stated in the following property.
Property 2. Under the conditions of Property 1,  X  0 such that  X  0  X   X   X  2  X  r , if there are enough sources, then holds with probability of at least 1  X   X  , where  X   X  (0 , 1) . Proof. Please refer to  X  C for a detailed proof.
 Aft er conduct enough iterations, Eq. (14) shows that the main component of the upper bound is the second terms (i.e., formance of the proposed sample -EMrgmm is upper bound-ed by  X ( S  X  1 = 2 ). Intuitively, in truth discovery tasks the more the sources, the better the performance of methods. In Property 2, we theoretically present that the convergence rate of the sample -EMrgmm is  X ( S  X  1 = 2 ).
Note that, only finite number of samples can be obtained in real-world applications, which fits the sample -Emrgmm setting. Therefore, all experiments are conducted using sample -EMrgmm 1 . In this section, we test the sample -EMrgmm on both simulated and real-world data sets. The experimental results show the effectiveness of the proposed sample -EMrgmm in identifying truths as well as its conver-gence. We first introduce baselines and performance mea-sures in Subsections 5.1 and 5.2, respectively. Experimental results on simulated data are presented in Subsection 5.3. In Subsection 5.4, we show experimental results on three real-world application datasets: Weather Forecast, Indoor Floorplan, and Stock Data.
A variety of truth discovery methods have been developed to identify each object X  X  truth. As we consider applications of continuous data in this paper, we compare the sample -EMrgmm with three state-of-the-art truth discovery meth-ods CRH, CATD, GTM, and two naive methods: Mean and Median. Details of the baselines are shown as follows:
W e will use RGMM and sample -EMrgmm interchangeably in the experiment section
In the experiments, we have continuous input obtained from multiple sources. Although the ground truths are avail-able, we conduct all methods in an unsupervised manner and the ground truths will only be used in evaluation. To evalu-ate the performance of sample-EMrgmm as well as baselines, we adopt the following measures: Note that a lower measure value means that the truth esti-mates are closer to the ground truths. Thus, for all measures, the lower the value, the better the method X  X  performance.
The advantage of using simulated data is that we can sim-ulate different truth discovery scenarios to compare the per-formance of the proposed sample -EMrgmm with that of the baselines. In this section, we first introduce the procedure of generating simulated data. Then, we show the performance of the sample -EMrgmm as well as the comparison with base-lines in terms of MAE , RMSE , and ErrorRate (  X  ). Data Generation. In each experiment, we generate N = 200 entities and S = 100 sources. For each source, its bias ( s ) is drawn i.i.d. from a distribution F . We as-sume that the ground truth for all entities are 0s. Thus, the s -th component (i.e., source) of the mixture model fol-lows a multivariate normal distribution, Normal(  X  s ,  X  2 To generate the sample of claims { X s } S 1 , we first random-ly generate a source index s from [1 ,  X   X   X  , S ], and then X is drawn from Normal(  X  s ,  X  2 I N ), where  X  2 = 1. We use MAE and RMSE , and Error Rate (0 . 1) for evaluation. We simulate three different scenarios involving different distri-butions of source biases: Uniform, Normal, and Student  X  s t-distribution, and then evaluate the performance of all truth discovery methods.

Scenario 1 :  X  s  X  Uniform(  X  c, c ). In this scenario, sources X  biases are drawn from a uniform distribution with c = 2. The source reliability degrees are uniformly distribut-ed. We report the results on experiments with different source number S = { 10 , 20 ,  X   X   X  , 100 } in terms of all methods in Figure 1. In Figure 1, the solid and dark line represents the value of function f ( S ) = S  X  1 2 , which is the dominat-ed term in upper bound of the proposed sample -EMrgmm (Eq. (14)). From Figures 1(a), 1(b) and 1(c), we can see that the convergence of the proposed sample -EMrgmm is similar to the S  X  1 2 , which confirms the result in Property 2. Moreover, we can see that the performance of the proposed RGMM is better when comparing with baselines in terms of RMSE and Error Rate . It means that most of truth esti-mates from the sample -EMrgmm have smaller errors com-pared with that of baselines. For MAE , all truth discovery methods have the same performance.

Scenario 2 :  X  s  X  Normal(0 ,  X  2 ). In this scenario, sources X  biases are drawn from a Normal distribution with variance  X  is chosen based on the scale of the multi-source data.  X  = 0 . 5. Based on three-sigma rule of thumb, sources biases Meanwhile, as the variance of the normal distribution is s-mall, there are many samples closer to the mean 0. There-fore, there are many reliable sources than unreliable sources. The results are shown in Figure 2. The convergence perfor-mance of the sample -EMrgmm with respect to the number of sources is similar to that in Scenario 1.

Scenario 3 :  X  s  X  Student  X  s t -(  X  ). In this scenario, sources X  biases are drawn from a student X  X  t-distribution with freedom  X  = 2. Compared with previous scenarios, Student  X  t-distribution has heavier tails, i.e., it is more prone to pro-ducing values that fall far from its mean 0. Consequently, there are more unreliable sources. The results in terms of MAE , RMSE , and Error Rate ( . 1) are shown in Figure 3.
Result Analysis. Comparing the performance on Sce-narios 1  X  3, we can see that all methods perform best in Scenario 3 and worst in Scenario 3. In Property 1, the condi-tions show that the performance is better if the upper bound of data C is smaller and the variance  X  2 is larger. In Sce- X   X  2 = 2 . 0886 in Scenario 2. As student t-distribution is long-tail, there are some sources which have very large bias. Thus, in Scenario 3, the upper bound of original simulated data is 161 . 4553. The claims provided these sources can be treat-ed as outliers. After removing them, we have b C = 19 . 9335 and  X   X  2 = 5 . 8458. Based on Property 11, the performance in Scenario 2 should be the best, as shown in Figure 2.
The convergence rate of the sample -EMrgmm is  X ( S  X  1 2 a s shown in the Section 4. Experimentally, the convergence rate in Scenarios 1 and 2 is nicely fit to  X ( S  X  1 2 ), as shown in Figures 1 and 2. In Scenario 3, the convergence is not as clear as that in Scenarios 1 and 2, as there are more outliers. Data Description. We test the proposed sample -RGMM and baselines on real-world data. The detailed de-scription of each dataset and their tasks are shown as follows: h ttp://www.hamweather.com http://www.wunderground.com http://www.worldweatheronline.com
Note that we have a different task setting compared with [9,10] on the real-world datasets reported in this paper. We consider a scenario where all entities are claimed by all sources while CATD [9] and CRH [10] were applied to en-tities that are observed by a subset of sources. To fit the full observation scenario, we preprocess the data used in [9,10] by deleting those entities which have not been claimed by all sources. In addition, our model in this paper tackles contin-uous data only. Therefore, we select continuous attributes in the Weather Forecast dataset, and the  X  X rice X  attribute (i.e., a continuous attribute) in the Stock dataset ([11]), which al-so differs from the setting in [10] (i.e., using both categorical and continuous attributes). The statistics of three real-world datasets are summarized in Table 2.

R esult Analysis. On the Weather Forecast data, we re-duce the scale of the observations in preprocess step. For example, the original 77 Fahrenheit is changed to . 77. We evaluate all methods on different scenarios in which the num-ber of sources increases from 4 to 10 by the stepsize of 2. In Figure 4, we report the experimental results in terms of MAE and RMSE . We can see that the performance of the proposed RGMM improves as the number of sources increases. When the number of sources is relatively small, the performance of the proposed RGMM is worse than baselines. however, giv-en a plenty of sources, RGMM converges to other baselines. The experimental results with respect to Error Rate (  X  ) is p-resented in Table 3. Table 3 shows that the performance of RGMM is comparable to that of baselines. Figure 4: Weather Forecast dataset: Performance with respect to the Number of Sources ( S ).

On Indoor Floorplan dataset, we test all truth discovery methods on different scenarios where the number of sources increases from 6 to 44 by the stepsize of 1. In each scenario, we randomly choose the pre-fixed number of sources. To Me thod Error Rate (  X  ) .05 .06 .07 .08 .09 .10 R GMM .2159 .1591 .1477 .1023 .0795 .0455 CATD .4205 .3523 .2955 .2273 .1705 .1364
Median .2045 .1477 .1023 .0682 .0455 .0341 Figure 5: Indoor Floorplan dataset: Performance with respect to the Number of Sources ( S ). reduce the randomness, we repeat experiments 20 times and report the average of evaluation measures. The performance comparison between the proposed RGMM and baselines in terms of MAE and RMSE are presented in Figures 5(a) and 5(b), respectively. Note that the decrease rate is not exactly fit to the exponential function with power to  X  1 2 . For both measures, although the performance of all methods decrease as the number of sources increases, the proposed RGMM is the best. Namely, the truth estimates obtained by RGMM is closer to the truths. We also report experimental results in terms of Error Rate (  X  ). We change  X  from . 5 to 1 . 0 and show the results in Table 4. We can see that the performance of the proposed RGMM is the best on all scenarios exept  X  = . 6. Me thod Error Rate (  X  ) .5 .6 .7 .8 0.9 1.0 R GMM .5714 .4286 .2857 .2857 .1429 .1429 CATD .4286 .4286 .2857 .2857 .2857 .2857 Median .7143 .2857 .2857 .2857 .1429 .1429 On Stock data, we follow the similar experiment design. We report the results in terms of MAE and RMSE measures in Table 5. Table 5 shows that the performance of RGMM comparable with baselines. We also test the performance with respect to the number of sources. Due to the page limit, we only show the results on the data collected on day 1 in Figure 6. To better confirm the theoretical results obtained in Section 4, we report the convergence of the RGMM with respect to the number of sources on all days X  Stock data in Figure 7. Each blue line represents a experiment conducted in a single day Stock data. We also fit each line into a function a  X  S  X  1 2 + c wh ere a and c are coefficients. The red line is plot using the average of a and c over 19 days. From Figures 6 and 7, we can see that the convergence of the RGMM X  X  performance is indeed  X ( S  X  1 2 ), which perfectly confirms our theoretical results. Figure 6: Stock data on Day 1 : Performance with respect to the number of sources ( S ). Figure 7: Convergence w.r.t. the number of sources on all Stock data
In summary, the experimental results on both simulat-ed and real-world datasets demonstrate that: (1) the per-formance of the proposed sample -EMrgmm is comparable compared with several state-of-the-art methods, and (2) the convergence rate of its performance is  X ( S  X  1 2 ) as proved in Section 4 (Property 2).
With the increasing possibilities to collect data from mul-tiple sources in the real world, it is critical to identify true facts from conflicting data. To solve this problem, many al-gorithms were developed based on heuristic principles, opti-mizations, or probabilistic model. However, in existing liter-ature on truth discovery, the convergence analysis is missing, and thus there is no theoretical guarantee that the results of these algorithms converge to the truths. In this paper, we proposed an effective truth discovery approach with the-oretical guarantee. We first introduced the randomized bi-ases of sources to measure their reliability degrees. Then we proposed a novel model (RGMM) to represent multi-source data with various reliability degrees, which consists of a Gaussian mixture model with the randomized biases in-corporated. The parameters of interests in this model are the truths to be identified. We then derive both population -and sample -EMrgmm to the MLE of the truth parameter of RGMM. Theoretically, we prove that population -EMrgmm converges in probability to an  X  -ball around the MLE with the increasing number of sources, under certain condition-s. Moreover, we prove that sample -EMrgmm also converges to the  X  -ball around the MLE with more iterations. In the experiments, we evaluate the effectiveness of the proposed sample -EMrgmm on both simulated and real-world dataset-s. Experimental results demonstrate that the proposed sam-ple -EMrgmm is able to identify reliable information from multi-source data, and the estimator converges to an  X  -ball around the MLE under the stated conditions. This work was sponsored in part by US National Science Foundation under grant IIS-1319973, IIS-1553411 and CNS-1566374. The views and conclusions contained in this paper are those of the authors and should not be interpreted as representing any funding agency. A. PROOF OF PROPERTY 1 We follow the similar procedure of the proof in [1]: Decom-pose M ( )  X   X  into two separate functions: E [ X  1 ( X )] and  X  ( X ), and then bound  X  1 ( X ) and  X  2 ( X ).
 Define u =  X  +  X   X  where  X  :=  X   X  ,  X   X   X  [0 , 1]. Before applying Taylor X  X  property to the function  X   X  ( X , ), we first take a look at its derivative and upper  X   X   X  ( X , ) and take the expectation over X . Combining with the upper bound, we have that
E [(  X  ( X , )  X   X   X  ( X , ))( X  X  )]  X  where  X ( X , ) = exp(  X  E [(  X  ( X , )  X   X   X  ( X , ))( X  X  )]  X  2  X  The following two Lemmas provide the upper bounds for E [ X  2 ( X )] and E [ X  2 ( X )], respectively. The proofs are de-ferred in A.1 and A.2.
 Lemma 1. There exist  X  1 ,  X  2 ,  X  3 and  X  4 , such that
 X  1 ( X )  X   X  Le mma 2. There exist  X  1 ,  X  2 and  X  3 , such that Ap plying Lemmas 1 and 2, it is easy to prove Theorem 1. Specifically, substituting Equations (20), (21), (23), and (24) into (16), we have that whenever  X  holds provided that the single-to-noise ratio  X   X   X  2 2 is suffi-ciently large, and the bias-to-mean ratio C  X   X   X  small. So far, we have finished the proof for Property 1. A.1 Proof of Lemma 1 We first apply Taylor X  X  property to the function X  X 
 X  1 ( X ) = E Based on (18), it is easy to obtain that sup The remainder of the proof is to show a sufficient uniform upper bound of  X  E (  X  1 1 ( X ))  X  op and  X  E (  X  1 2 ( X )) [0 , 1]. Based on the discussion before, the distribution of X is symmetric around  X   X   X  2 . Let us define A = { X  X  therefore, we have that
E (  X  1 1 ( X )) Based on the fact that  X   X  2 =  X   X  +  X  (  X   X  )  X  2  X  P [ A ]  X  exp(  X   X  further:
E Similarly, we have that
E Therefore, Lemma 1 holds based on (20) and (21), where  X  ,  X  2 ,  X  3 and  X  4 are chosen properly.
 A.2 Proof of Lemma 2 Similar to the proof in Corollaries 1, we have that
 X  2 ( X ) = E To derive the uniform upper bounds of  X  E (  X  2 1 ( X ))  X   X  E (  X  2 2 ( X ))  X  op , let  X   X  [0 , 1] be arbitrarily given and e denotes the first canonical basis vector. We can construct an orthonormal matrix, Q , such that Q =  X   X  2 e 1 . Assume that Y = Q X , which makes Y  X  N ( Q  X  ,  X  2 I N ). Note that  X   X  2  X  (1 + r )  X   X   X  2 . Thus, we have  X  E (  X  2 E  X  E (  X  2  X   X  (1 + (1 + r )  X   X   X  2 2  X   X  (1 + (1 + r )  X   X   X  2 2 Based on the constructed orthonormal projection matrix, the operator norm of the matrix is shown as follows E (  X  2 2 ( X )) = E whe never  X   X   X  2 2  X  16  X  2 / 3. Moreover, for any index j we have E jj = E [ tion of the uniform upper bound of  X  E (  X  2 1 ( X ))  X  op E jj  X  2  X  exp(  X  E jj , we have that  X  E (  X  2 Therefore, the corollary holds based on Equations (23) and (24), where  X  1 ,  X  2 and  X  3 are chosen properly. B. PROOF OF COROLLARY 1 Define g ( X , ) =  X  ( X , )( X  X  ). Therefore, M n ( ) =  X   X  } . Define X , sup s  X  [ S ]  X  X s  X  2 . Based on Theorem 1, we can show that  X  g  X  E ( g )  X  2  X  X + C +  X r  X   X   X  2 , and ( X + C ) 2 . As for the special case with  X  = 0 in Lemma 5 . 1 in [22], for  X  &gt; 0 we have that where c is a positive constant. Define the right hand side as  X  . Then we can derive that  X  = c ;N ( X  ; ) S  X  1 2 for some C. PROOF OF PROPERTY 2 Define event A t = { X  M n ( t )  X  M ( t )  X  2  X  c ;N ( X  ; ) a nd A = that P [ A t  X  c  X  we have that
P = P [  X  M n ( t )  X  M ( t )  X  2 +  X  M ( t )  X   X   X  2 &gt;  X  +
P [  X  M n ( t )  X  M ( t )  X  2 +  X  M ( t )  X   X   X  2 &gt;  X  |A c  X  P [  X  M n ( t )  X  M ( t )  X  2 +  X  M ( t )  X   X   X  2 &gt;  X  The first inequality holds because P ( A )  X  1 ,  X  A . To obtain the second inequality, we first show that As A t  X  Applying the same procedure on  X  M ( t )  X   X   X  2 , we can obtain the first part. The second part is easy to obtain by using the Boole X  X  Inequality. The last inequality holds
Th erefore, in probability at least 1  X   X  , we have that which completes the proof.
