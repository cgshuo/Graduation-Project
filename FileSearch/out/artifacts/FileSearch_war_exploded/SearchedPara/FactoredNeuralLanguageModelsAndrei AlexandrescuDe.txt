 Neur al langua ge models (NLMs) (Bengio et al., 2000) map words into a continuous representation space and then predict the probability of a word given the continuous representations of the preced-ing words in the history . The y have pre viously been sho wn to outperform standard back-of f models in terms of perple xity and word error rate on medium and lar ge speech recognition tasks (Xu et al., 2003; Emami and Jelinek, 2004; Schwenk and Gauv ain, 2004; Schwenk, 2005). Their main dra wbacks are computational comple xity and the fact that only dis-trib utional information (w ord conte xt) is used to generalize over words, whereas other word prop-erties (e.g. spelling, morphology etc.) are ignored for this purpose. Thus, there is also no principled way of handling out-of-v ocab ulary (OO V) words. Though this may be suf cient for applications that use a closed vocab ulary , the current trend of porting systems to a wider range of languages (esp. highly-inected languages such as Arabic) calls for dy-namic dictionary expansion and the capability of as-signing probabilities to newly added words without having seen them in the training data. Here, we in-troduce a novel type of NLM that impro ves gener -alization by using vectors of word features (stems, afx es, etc.) as input, and we investigate deri ving continuous representations for unkno wn words from those of kno wn words.
A standard NLM (Fig. 1) tak es as input the pre vi-ous n  X  1 words, which select rows from a continu-ous word representation matrix M . The next layer' s input i is the concatenation of the rows in M cor -responding to the input words. From here, the net-work is a standard multi-layer perceptron with hid-den layer h = tanh( i  X  W o = h  X  W ho + b o . where b h,o are the biases on the respecti ve layers. The vector o is normalized by the softmax function f propagation (BKP) is used to learn model parame-ters, including the M matrix, which is shared across input words. The training criterion maximizes the regularized log-lik elihood of the training data. An important task in language modeling is to pro-vide reasonable probability estimates for n-grams that were not observ ed in the training data. This generalization capability is becoming increasingly rele vant in current lar ge-scale speech and NLP sys-tems that need to handle unlimited vocab ularies and domain mismatches. The smooth predictor func-tion learned by NLMs can pro vide good generaliza-tion if the test set contains n-grams whose indi vid-ual words have been seen in similar conte xt in the training data. Ho we ver, NLMs only have a simplis-tic mechanism for dealing with words that were not observ ed at all: OO Vs in the test data are mapped to a dedicated class and are assigned the singleton probability when predicted (i.e. at the output layer) and the features of a randomly selected singleton word when occurring in the input. In standard back-off n-gram models, OO Vs are handled by reserv-ing a small x ed amount of the discount probabil-ity mass for the generic OO V word and treating it as a standard vocab ulary item. A more powerful back off strate gy is used in factored language models (FLMs) (Bilmes and Kirchhof f, 2003), which vie w a word as a vector of word features or  X factors X : w =  X  f 1 , f 2 , . . . , f k  X  and predict a word jointly from pre vious words and their factors: A general-ized back off procedure uses the factors to pro vide probability estimates for unseen n-grams, combin-ing estimates deri ved from dif ferent back off paths. This can also be interpreted as a generalization of standard class-based models (Bro wn et al., 1992). FLMs have been sho wn to yield impro vements in perple xity and word error rate in speech recogni-tion, particularly on sparse-data tasks (Vergyri et al., 2004) and have also outperformed back off mod-els using a linear decomposition of OO Vs into se-quences of morphemes. In this study we use factors in the input encoding for NLMs. NLMs dene word similarity solely in terms of their conte xt: words are assumed to be close in the contin-uous space if the y co-occur with the same (subset of) words. But similarity can also be deri ved from word shape features (af x es, capitalization, hyphenation etc.) or other annotations (e.g. POS classes). These allo w a model to generalize across classes of words bearing the same feature. We thus dene a factor ed neur al langua ge model (FNLM) (Fig. 2) which tak es as input the pre vious n  X  1 vectors of factors. Dif-ferent factors map to disjoint row sets of the ma-trix. The h and o layers are identical to the standard NLM' s. Instead of predicting the probabilities for all words at the output layer directly , we rst group words into classes (obtained by Bro wn clustering) and then compute the conditional probability of each word given its class: P ( w This is a speed-up technique similar to the hierarchi-cal structuring of output units used by (Morin and Bengio, 2005), except that we use a  X at X  hierar -chy . Lik e the standard NLM, the netw ork is trained to maximize the log-lik elihood of the data. We use BKP with cross-v alidation on the development set and L2 regularization (the sum of squared weight values penalized by a parameter  X  ) in the objecti ve function. In an FNLM setting, a subset of a word' s factors may be kno wn or can be reliably inferred from its shape although the word itself never occurred in the train-ing data. The FNLM can use the continuous repre-sentation for these kno wn factors directly in the in-put. If unkno wn factors are still present, new contin-uous representations are deri ved for them from those of kno wn factors of the same type. This is done by averaging over the continuous vectors of a selected subset of the words in the training data, which places the new item in the center of the region occupied by the subset. For example, proper nouns constitute a lar ge fraction of OO Vs, and using the mean of the rows in M associated with words with a proper noun tag yields the  X average proper noun X  representation for the unkno wn word. We have experimented with the follo wing strate gies for subset selection: NULL (the null subset, i.e. the feature vector components for unkno wn factors are 0), ALL (average of all kno wn factors of the same type); TAIL (averaging over the least frequently encountered factors of that type up to a threshold of 10%); and LEAST , i.e. the representation of the single least frequent factors of the same type. The prediction of OO Vs themselv es is unaf fected since we use a factored encoding only for the input, not for the output (though this is a pos-sibility for future work). We evaluate our approach by measuring perple x-ity on two dif ferent language modeling tasks. The rst is the LDC CallHome Egyptian Colloquial Ara-bic (ECA) Corpus, consisting of transcriptions of phone con versations. ECA is a morphologically rich language that is almost exclusi vely used in in-formal spok en communication. Data must be ob-tained by transcribing con versations and is therefore very sparse. The present corpus has 170K words for training ( | V | = 16026 ), 32K for development (de v), 17K for evaluation (eval97). The data was preprocessed by collapsing hesitations, fragments, and foreign words into one class each. The corpus was further annotated with morphological informa-tion (stems, morphological tags) obtained from the LDC ECA lexicon. The OO V rates are 8.5% (de-velopment set) and 7.7% (eval97 set), respecti vely .
The second corpus consists of Turkish newspa-per text that has been morphologically annotated and disambiguated (Hakkani-T  X  ur et al., 2002), thus pro-viding information about the word root, POS tag, number and case. The vocab ulary size is 67510 (relati vely lar ge because Turkish is highly aggluti-nati ve). 400K words are used for training, 100K for development (11.8% OO Vs), and 87K for test-ing (11.6% OO Vs). The corpus was preprocessed by remo ving segmentation marks (titles and paragraph boundaries). We rst investigated how the dif ferent OO V han-dling methods affect the average probability as-signed to words with OO Vs in their conte xt. Ta-ble 1 sho ws that average probabilities increase com-pared to the strate gy described in Section 3 as well as other baseline models (standard back off tri-grams and FLM, further described belo w), with the strongest increase observ ed for the scheme using the least frequent factor as an OO V factor model. This strate gy is used for the models in the follo wing per -ple xity experiments.

We compare the perple xity of word-based and factor -based NLMs with standard back off trigrams, class-based trigrams, FLMs, and interpolated mod-els. Ev aluation was done with (the  X w/unk X  column in Table 2) and without (the  X no unk X  column) scor -ing of OO Vs, in order to assess the usefulness of our approach to applications using closed vs. open vo-cab ularies. The baseline Model 1 is a standard back-off 3-gram using modied Kneser -Ne y smoothing (model orders beyond 3 did not impro ve perple x-ity). Model 2 is a class-based trigram model with Bro wn clustering (256 classes), which, when inter -polated with the baseline 3-gram, reduces the per -ple xity (see row 3). Model 3 is a 3-gram word-based NLM (with output unit clustering). For NLMs, higher model orders gave impro vements, demon-strating their better scalability: for ECA, a 6-gram (w/o unk) and a 5-gram (w/unk) were used; for Turk-ish, a 7-gram (w/o unk) and a 5-gram (w/unk) were used. Though worse in isolation, the word-based NLMs reduce perple xity considerably when interpo-lated with Model 1. The FLM baseline is a hand-optimized 3-gram FLM (Model 5); we also tested an FLM optimized with a genetic algorithm as de-10 genetic FLM 190 188 181 188 761 1181 776 1179 11 1) &amp; 10) 183 166 175 164 706 488 720 498 12 factored NLM 189 173 190 175 1216 808 1249 832 13 1) &amp; 12) 169 155 168 155 724 487 744 500 14 1) &amp; 10) &amp; 12) 165 155 165 154 652 452 664 461 scribed in (Duh and Kirchhof f, 2004) (Model 6). Ro ws 7-10 of Table 2 display the results. Finally , we trained FNLMs with various combinations of fac-tors and model orders. The combination was opti-mized by hand on the dev set and is therefore most comparable to the hand-optimized FLM in row 8. The best factored NLM (Model 7) has order 6 for both ECA and Turkish. It is interesting to note that the best Turkish FNLM uses only word factors such as morphological tag, stem, case, etc. but not the actual words themselv es in the input. The FNLM outperforms all other models in isolation except the FLM; its interpolation with the baseline (Model 1) yields the best result compared to all pre vious inter -polated models, for both tasks and both the unk and no/unk condition. Interpolation of Model 1, FLM and FNLM yields a further impro vement. The pa-rameter values of the (F)NLMs range between 32 and 64 for d , 45-64 for the number of hidden units, and 362-1024 for C (number of word classes at the output layer). We have introduced FNLMs, which combine neu-ral probability estimation with factored word repre-sentations and dif ferent ways of inferring continuous word features for unkno wn factors. On sparse-data Arabic and Turkish language modeling task FNLMs were sho wn to outperform all comparable models (standard back off 3-gram, word-based NLMs) ex-cept FLMs in isolation, and all models when inter -polated with the baseline. These conclusions apply to both open and closed vocab ularies.
 Ackno wledgments This work was funded by NSF under grant no. IIS-0326276 and DARP A under Contract No. HR0011-06-C-0023. An y opinions, ndings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reect the vie ws of these agencies.

