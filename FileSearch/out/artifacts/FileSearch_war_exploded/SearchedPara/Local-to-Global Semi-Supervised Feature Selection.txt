 Variable-weighting approaches are well-known in the context of embedded feature selection. Generally, this task is performed in a global way, when the algorithm selects a single cluster-independent subset of features (global feature selection). However, there ex-ist other approaches that aim to select cluster-specific subsets of features (local feature selection). Global and local feature selec-tion have different objectives, nevertheless, in this paper we pro-pose a novel embedded approach which locally weights the vari-ables towards a global feature selection. The proposed approach is presented in the semi-supervised paradigm. Experiments on some known data sets are presented to validate our model and compare it with some representative methods.
 H.2 [ Database Management ]: Database Applications X  Data Min-ing ; I.5 [ Pattern Recognition ]: Design Methodology X  Feature evaluation and selection Feature Selection, Semi-supervised learning, Variable weighting, Constraints
The main goal of feature selection is to remove both irrelevant and redundant features in order to improve performance, decrease the complexity and improve the interpretability of learning algo-rithms [12]. The feature selection scenarios can be grouped into global and local approaches. In the first scenario, feature selection algorithms choose one  X  X lobal X  subset of features for all data ex-amples. Generally, most feature selection algorithms are global. In the second scenario, some algorithms produce "local" subset of features for each group of data instances, as it is the case in co-clustering or bi-clustering [14] and subspace clustering [9]. These methods try to maximize the coherence exhibited by a subset of instances on a subset of features.

Regarding the ordering of feature selection during the learning process, it could roughly break down into three parts. The first part concerns filter methods, which select variables by ranking them with correlation coefficients, or other scores that are independent of the learning algorithm. In the other hand, some "wrapper X  methods assess subsets of variables according to their usefulness to a given learning system. Finally, the embedded methods implement the same idea, but proceed more efficiently by directly optimizing the relevance of features during the learning process.

Feature selection is well studied in both supervised and unsu-pervised contexts [13, 7]. In the semi-supervised context, this task is recently developed with some interesting researches. The early works were introduced by [27] with a method called sSelect, based on spectral analysis. The authors exploited intrinsic properties un-derlying supervised and unsupervised feature selection algorithms, and proposed a unified framework for semi-supervised feature se-lection based on spectral graph theory [6]. Several other works have attempted to exploit pairwise constraints or other prior infor-mation for semi-supervised feature selection. The authors in [26] proposed an efficient algorithm, called SSDR, which can simul-taneously preserve the structure of original high-dimensional data and the pairwise constraints specified by the user. The main prob-lem with this method is that although the use of variance is very important to preserve the feature locality, the proposed objective function is independent from this parameter. In addition, the simi-larity matrix used in the objective function uses the same value for all pairs of data which are not related by any constraint. The same authors proposed a constraint score based method [25] which eval-uates the relevance of features according to constraints only. The method carries out with little supervision information in labeled data ignoring the unlabeled data regardless how large it is. The authors in [18] proposed to solve the problem of semi-supervised feature selection by a simple combination of scores computed on both labeled and unlabeled data. This method (called C4) tries to find a consensus between an unsupervised score and a super-vised one (by multiplying both scores). The combination is simple, but can dramatically bias the selection of the features having best scores for labeled part of data and bad scores for the unlabeled part and vice-versa. More recently, the authors in [3] proposed a constrained laplacian score (CLS) for semi-supervised feature se-lection by a more developed combination between laplacian and constraint scores [15, 25] . Then, they improved CLS in [16] by an exploitation of a constraint selection procedure during the feature selection process (CSFS). However, the approach discards feature interdependencies.

One of the techniques in which we are interested is the well-known K-means clustering [19]. The objective of K-means is to group the unsupervised instances depending on some notion of sim-ilarity between them. Similarity could be defined by a metric or probabilistic model, which is a function of the features or attributes describing the data instances. An interesting step over the original K-means is to integrate the background information presented by domain knowledge into the clustering process. Background infor-mation can be found under the form of instance-level constraints [23]. The authors in [24] presented an approach in which the al-gorithm of K-means clustering is modified in such a way that it assesses the non-violation of constraints during the clustering pro-cess. The results showed a better clustering when making profit of constraints over the original "unsupervised X  K-means. The authors in [2] proposed two variants of K-means algorithms : the first one is called "Seeded K-means" which employs the label information in K-means initialization only. The other algorithm called "Con-strained K-means X  assesses the respect of label information during the clustering process, then the attribution based on nearest cluster-centroid concerns the unlabeled examples only. The authors in [4] proposed to incorporate the background information into traditional partitional clustering algorithms by adapting the objective function to include penalties for violated constraints. The objective function has been shown to have a probabilistic basis related to the assign-ment of labels in Hidden Markov Random Fields.

Furthermore, one of the limitations of K-means algorithm is that it deals with all variables in the same way. It is proved that the non-relevant variables may mislead the clustering, and results in  X  X oggy X  clusters more than well distinct ones. Moreover, it is proved that the appropriate weighting of variables during the clustering process may lead to far better clustering results [17]. This weight-ing can be set manually, or calculated automatically during the learning of clusters. Unfortunately, manual variable weighting is not possible in data-mining applications where data instances are described by hundreds of thousands of features. Therefore, the au-tomatic variable weighting became the feasible method for feature selection in this case.

Feature weighting is well discussed in several works [11, 10, 20, 21]. They could basically be classified into two types. The first type concerns the weighting approaches that produce subsets of weights, and then they try to select the optimal one during the clustering depending on the partitioning result in each phase. The drawback of such methods is the supposition of having an optimal set of predefined weights which could not be verified. In addition, such predefined weights may not be feasible when dealing with high-dimensional data. The other type of variable weighting tech-niques try to assign a weight for each feature. These techniques tend to give bigger weights to relevant variables and very less to the noisy (or irrelevant) ones, which may serve as a feature selec-tion technique.

In this paper, we provide a locally weighting metric model, based on constrained clustering in order to perform a global semi-super-vised feature selection. The rest of the paper is organized as fol-lows: Section 2 introduces the basic K-means and semi-supervised K-means algorithms. In section 3, we describe the weighting ap-proach for local to global semi-supervised feature selection. Em-pirical study is given in section 4, over several known data sets. Finally, we conclude in section 5 by a summary and an insight for future research based on our proposed approach. The K-means clustering algorithm can be defined as follows: Let X = { x 1 ,x 2 ,...,x n } be a set of n instances. An instance x i = ( x i 1 ,x i 2 ,...,x im ) is characterized by a set of m features. The K-means algorithm searches for a partition of X into k clus-ters { c 1 ,c 2 ,...,c k } such that the sum of the dissimilarities between each instance x i and its nearest cluster centroid c l is minimized. subject to where At iteration t = 0 , the centroids { c l } are chosen randomly, then the above optimization problem can be solved by iterating the cal-culation of the unknown variables A and C in the following two equations:
At iteration t &gt; 0 : and c lj ( t ) =
The algorithm iterates the two equations eq(2) and eq(3) until convergence. This occurs when there is no further changes in as-signment of instances to the clusters (i.e. a il ( t ) = a 1  X  i  X  n and 1  X  l  X  k and then c i ( t ) = c i ( t  X  1) ), or when a limited number of iterations is reached. The choice of k (the num-ber of clusters) is an important issue in K-means type algorithms, and could imply an important modification over the clustering re-sults. However, in semi-supervised clustering, it is often assumed that the number of classes of the labeled instances represent the optimal number of clusters.

The integration of background knowledge in clustering process was practically proven to enhance performance. In semi-supervised clustering, background knowledge about application or data set could exist in several forms like labels, pairwise constraints or any other form. Several works in [23, 24, 2, 4] presented approaches to make use of the available supervision information in the semi supervised clustering. The main goal of incorporating such infor-mation in clustering is to have a closer-to-reality partitioning and to reduce the chance of getting stuck in local optima.

The first work to integrate the instance-level constraints in K-means algorithm is the COP-Kmeans [24]. It proposes to incorpo-rate two types of constraints in clustering process:
The proposed algorithm of COP-Kmeans tries to satisfy all con-straints in each iteration. This is done by assigning each instance to the nearest cluster (as it is done in the basic K-means), but with a condition that the assignment would not violate any constraint from  X 
ML (set of ML constraints) or  X  CL (set of CL constraints).
Weighting-based clustering has been an important research topic in data analysis [11, 20, 21, 17]. In this work, we propose a mod-ification of the objective function of the constrained version of K-means [24] such that we add a new unknown variable to the func-tion, the weights W , which would be used to weight the variables at each iteration, and then reduce the effects of irrelevant ones. We propose a local-to-global semi-supervised feature selection ap-proach, termed L2GFS as a shorthand. In the following, we start with a detailed local semi-supervised weighted extension to the ob-jective function of K-means described in eq(1).

In detail, we first weight the variables regarding the clusters, this means that each feature would have as much weights as the number of clusters. We believe that this local feature weighting would help at finding the relevant variables which well describe each cluster, instead of having the same features rating over all the clusters. This method results in selecting an appropriate feature subset for each cluster (local feature selection). The cluster in its turn regroups a homogeneous instances (instance selection). The application of such technique (as in co-clustering) could be done for example in order to study the effects of important factors (features) which in-fluences specific subset of a given population (instances). However, the aim of this work is to use the local feature selection in the goal of having a better global selection. Finally, it is obvious that a vari-able which well describes a given cluster might not well describes the other ones.
 We propose to minimize the new following objective function: subject to the pairwise constraints (  X  ML ,  X  CL ) and the follow-ing constraints: where  X  is a parameter for feature weights. Note that this param-eter cannot be equal neither to zero nor to one. Indeed, if  X  = 0 , the weighting is removed and so the feature selection cannot be performed. If  X  = 1 , w would disappear because of the following derivations for solving the problem.

In semi-supervised learning, a data set of n instances X consists of two subsets depending on the label availability: X L for which the labels are provided, and X U whose labels are not given. The pairwise constraint sets,  X  ML and  X  CL are constructed directly from the labeled instances ( X L ).  X  ML contains pairs of instances that have the same label and  X  CL contains those with different la-bels. Thus, the constraint sets would form a complete graph be-tween all instances in X L and so, |  X  ML  X   X  CL | = L ( L  X  1) L is the size of X L . In eq (4), we assign k initial random weights to each feature, and then optimize the associated problem by itera-tively solving the following three problems:
Problem1 : Minimizing eq(4) by fixing C and W for finding the solution for A (Assignment). The optimization leads to: where (for any iteration t &gt; 0 ):
Problem 2 : Minimizing eq(4) by fixing A and W for finding the solution for C (Centroids):
The calculation of the new cluster centroids, remains the same as in eq(3).
 Problem 3 : Minimizing eq(4) by fixing A and C for finding the solution for W (Weights). The optimization leads to: where h is the number of features where P n i =1 a il d ( x We rewrite the objective function eq(4) as follows where P n i =1 a il d ( x ij ,c lj ) are constants for fixed A and C . If P n i =1 a il d ( x ij ,c lj ) = 0 , this means that the j the same value for all instances in the cluster l . This is a de-generate solution, so we assign w lj = 0 to any feature where P i =1 a il d ( x ij ,c lj ) = 0 . Note that a zero weight to a feature in this case is only related with certain cluster in which the variable have identical value for all instances in this cluster.

For the ( h  X  m ) feature weights where P n i =1 a il d ( x 0 (we consider the reasoning for one cluster for simplification pur-pose), we minimize the function via the Lagrangian multiplier. Let  X  be the multiplier and  X ( W, X  ) be the Lagrangian.
 The both sets of variables derivative ( W, X  ) must vanish and then we would have From eq(9) we obtain Substituting eq(11) into eq(10), we have From eq(12), we derive
Substituting eq(13) into eq(11), we obtain
With eq(14) the objective function eq(4) is minimized locally over each cluster, and then the variables are ranked in each cluster by the local weights w lj , these weights express the relevance of each variable j regarding each corresponding cluster l . In addition, if 0 &lt;  X  &lt; 1 , a high value of P n i =1 a il d ( x ij leads to a high value of w lj , so does w  X  lj . This would be paradoxical with the principle of feature weighting. Thus, in order to use the weights as measures for evaluating feature relevance, the value of  X  must be either negative or greater than 1.

Finally, in order to have a global variable weighting, we aggre-gate the weights of each variable over all clusters: The variables are then selected regarding to their global weights.
Subsequently, we can summarize all the above mathematical de-velopments in Algorithm 1.
 Algorithm 1 L2GFS Input: Data set X of dimension n  X  m Output: Weighted and ranked features 1: Give the parameter  X  2: Construct the constraint sets (  X  ML and  X  CL ) from X 3: Randomly choose initial centroids c 1 ,c 2 ,...,c k from X where 4: Randomly generate initial local weights for each variable in 5: Calculate the cluster-memberships using eq(5) 6: Update the cluster-centroids using eq(3) 7: Update the local variable weights using eq(6) 8: Iterate between steps 5: and 7: until convergence 9: Calculate the global variable weights using eq(15) 10: Rank the features { j } according to their weights { w
L EMMA 1. L2GFS is computed in O ( m  X  max ( ntk,log m )) , where t is the number of iterations.
 Proof . Step 2 of the algorithm requires L 2 operations. L is gen-erally very small in the semi-supervised context. Step 5 calculates the cluster-membership values by nk operations. Step 6 updates the centroids by mk operations and step 7 provides the local vari-able weights after nk operations. Step 9 provides the global vari-able weights after m operations and the last step ranks the features according to their weights with m log ( m ) operations.
In this section, we evaluate the performance of the algorithm derived from L2GFS on several data sets. The empirical study is made in the semi-supervised context.
The empirical study is done on seven data sets, the first one can be found in [8]. This data set is not high-dimensional (only 40 features), but is interesting to evaluate the algorithm X  X  perfor-mance since the part of relevant features is known a priori. The noise. The other data sets are available at [28]. They are of dif-ferent types, Text data ("PCMAC", "RELATHE"); Microarray data ("TOCX-171", "CLL-SUB-111") and Face-image data ("PIE10P", "PIX10P"). The whole data sets information is detailed in Table 1 in which ( n ) represents the number of instances, ( m ) the number of features, ( k ) the number of clusters in each data set, and the last column ( S ) represents the percentage of supervision.

The used data sets are high dimensional, with different number of classes and few supervision; for evaluating the performance of L2GFS and comparing it with other methods. All the considered methods are semi-supervised and most of them are based on con-straints:
Since L2GFS provides a partition with a hard satisfaction of con-straints, other methods are considered for comparison over con-strained clustering. These methods are particularly chosen because they are based on semi-supervised K-means :
To simulate the small labeled sample context, we set L , the num-ber of labeled data, by randomly selecting 3 instances per class and the remaining instances are used as unlabeled data. The portion of the supervised information is very small for each data set (see the last column ( S = 3 k n ) in Table 1). The parameter  X  should be carefully tuned according to the problem at hand. If underes-timated, the optimization of the objective function could be non-convex, this would be against our optimization procedure to have a local minimizer. On the other hand, if overestimated, the effect of feature weighting would disappear. For that reason, we set in the experiment: where  X  is the centroid of the used data set. This usually results in good learning performance.

The obtained feature weights are averaged over 20 runs with dif-ferent initializations of centroids and ranked in a descendant order for selecting the relevant ones.

Each data set is split (in a stratified way) into a training partition with 50% of the instances and a test partition with the remaining 50% of instances. After feature selection, a linear SVM classifier (using LIBSVM package [5]) is employed for classification accu-racy. The classifier is tuned via 3-fold cross-validation on training data set by repeating the process for 10 times on 10 different parti-tions of the data. Note that any other classifier could be employed instead of SVM since the performance concerns here the quality of selected features. In fact, when employed for classification, the method is not involved in the learning process, but is part of any pre-processing step. Thus, L2GFS can be employed for providing an optimal subset of features for any learning process.

In addition, we evaluate the clustering accuracy by comparing the obtained label of each instance with that provided by the data corpus. For that, we use Rand index [22] to provide the cluster-ing quality, by measuring the correspondence between the correct partition produced by the labels and the partition obtained from the clustering. This external index is widely used to evaluate the clus-tering approaches. We used it for comparing our approach with the most known approaches that used the same measure.
In this section, we are particularly interested in the waveform of the Brieman ("WAVE") data set which is widely used in machine learning and data mining tasks. This data set consists of 5000 in-stances divided into 3 classes, and composed of 21 relevant features (the first ones) and 19 noise features with a mean of 0 and a vari-ance of 1. Each class is generated from a combination of 2/3 "base" waves (Figure1). This figure plots the different values of the data set matrix in 3D way (X-axis for features, Y-axis for instances and Z-axis for the matrix values). The different colors represents the graduation between different values of the data matrix, where the red color is used for the higher ones and the blue color for the lower ones.

After applying L2GFS, we obtain the results presented in Figure 2. In the left side of the figure we present the feature weights from according to the definition of the weighting function. The red color represents the irrelevant features, and the blue color represents the relevant ones. We can see that the features (22 to 40) have low values on their weights, so the noise represented by these features is clearly detected.

In the right side of the figure, we show the classification accu-racy vs. different number of selected features with three curves. The black curve plots the accuracy using all the features in their original order in the data set (no selection), while the red one repre-sents the accuracy with the irrelevant features detected by L2GFS. We can see that the performance is very weak when the learning is undertaken using noise features only. The last curve (blue) out-performs the black one. It increases steadily over the first twenty features whose weights are the high ones on the left side of the figure. features. This shows the performance on each used subset of features.
In this section, we assess the performance of our approach and compare it with the above cited methods over high-dimensional data sets. The comparison is conducted by measuring both clas-sification and clustering analysis. Indeed, in the semi-supervised context, the aim can concern supervised learning according to the labeling of data; and the unsupervised learning according to the geometrical structure of data.
In this scenario, we compare the performance of our proposal with other semi-supervised feature selection methods. This com-parison concerns the classification accuracy that we present in both, Figure 3 and Table 2.

Figure 3 plots the curves of the whole algorithms (except cFLD and SSDR which do not do feature selection) for classification ac-curacy vs. different number of selected features. This figure in-dicates that in most cases L2GFS outperforms the other methods, especially for "PCMAC" and "RELATHE" in which the noise is im-portant. L2GFS seems to combine efficiently the labeled and unla-beled parts of data than the other constraint based semi-supervised methods.

Table 2 compares the averaged accuracy under different numbers of selected features. From this table and Figure 3, we can find that, the performance of L2GFS is almost always better than that of s-Select, C4 and CLS, and is comparable with CSFS. In general, we can see that L2GFS is superior to the other methods on all data sets. This observation suggests that the compromise between the label information and the geometrical structure of data is more adopted with an embedded approach than a filter one.

Furthermore, we compare the performance between all methods when different levels of supervision are used. Figure 4 shows the plots for accuracy under desired number of selected features versus different number of constraints. The desired number of selected features is fixed to 10. Here, cFLD and SSDR are included in the comparisons as they are dimensionality reduction methods. A par-ticular study on Figure 4 reveals that only a limited supervision is required for L2GFS to provide high performance. This corresponds exactly to our initial problem concerning "small-labeled-sample" data.
To show how the dimensionality of the projected space affects the performance, we compare the clustering accuracies with a fixed number of selected features (the same as above). The percentage of supervision is the same as indicated in Table 1. Since our pro-posal is based on K-means paradigm, we can obviously calculate its clustering accuracy. For the other methods, we also perform K-means algorithm in the selected feature subspace. The clustering is repeated 10 times with different initializations and the best result in terms of the objective function is recorded in Table 3. This ta-ble illustrates the accuracy on the hold-out test sets (subsets of data composed of instances that are not directly or transitively affected by the constraints).

As can be noted, L2GFS is very competitive with the other algo-rithms. For example, it performs much better than cFLD and SSDR for dimensionality reduction, when the number of constraints is limited. This indicates that the semi-supervised feature selection achieved by L2GFS is capable of enhancing clustering performance, which is provided by the same algorithm.
In this section, we present some comparisons of L2GFS vs. two known constrained clustering algorithms, COP-Kmeans [24] and MPC-Kmeans [4]. These comparisons were done without feature selection in order to show the performance that can provide the proposal on constrained clustering when the features are weighted as explained previously. We compare the results for each algorithm in terms of its unconstrained and constrained performance, when provided with the constraints extracted from the labeled part of data according to the last column of Table 1. Table 4 shows the accuracy (Rand index) on the hold-out test sets.

On the one hand, L2GFS provides a clear improvement to clus-tering accuracy, despite the violation of some constraints. On the other hand, the results obtained by L2GFS are similar and some-times better than the other constrained clustering methods. The most important remark is that with our proposal the clustering per-formance increases significantly with a few number of constraints comparing to other ones. For example, in Table 4 , for "PIE10P", L2GFS (80.39%) is not better than MPC-Kmeans (81.62%) but L2GFS yields an improvement of 9.46% over the baseline while MPC-Kmeans achieves 3.22% increase in accuracy.
 Table 4: Performance of L2GFS vs. two known constrained clustering algorithms.

In this paper, we proposed an embedded approach for semi-su-pervised feature selection. We presented L2GFS, a new method based on constrained K-means algorithm that calculates local fea-ture weights automatically for the goal of producing a general fea-ture selection. We achieved feature selection by combining the lo-cal feature weights after the convergence of the algorithm. Experi-mental results on several high dimensional data sets are performed by measuring both classification and clustering accuracies. We showed that with only a small number of constraints, the proposed algorithm significantly outperforms other semi-supervised features selection methods.

There are several avenues that could be planned for future re-search. It would be interesting to extend the proposed method to deal with regression problems in which the supervised part of data contains continuous values instead of labels. A possible avenue is to integrate the proposal in an ensemble-based framework, then the bagging of constraints can create diversity and thus improving the performance of learning algorithms. In addition, more efforts are needed to adapt the method to very high-dimensional data. [1] A. Bar-Hillel, T. Hertz, N. Shental, and D. Weinshall. [2] S. Basu, A. Banerjee, and R. J. Mooney. Semi-supervised [3] K. Benabdeslem and M. Hindawi. Constrained laplacian [4] M. Bilenko., S. Basu, and R. Mooney. Integrating constraints [5] C.-C. Chang and C.-J. Lin. LIBSVM: A library for support [6] F. Chung. Spectral graph theory. AMS , 1997. [7] J. Dy and C. E. Brodley. Feature selection for unsupervised [8] A. Frank and A. Asuncion. UCI machine learning repository, [9] Q. Fu and A. Banerjee. Bayesian overlapping subspace [10] R. Gnanadesikan, J. R. Kettenring, and S. L. Tsao. Weighting [11] P. Green, J. Kim, and F. Carmone. A preliminary study of [12] Y. Guan, J. Dy, and M. Jordan. A unified probabilistic model [13] I. Guyon and A. Elisseeff. An introduction to variable and [14] J. A. Hartigan. Direct Clustering of a Data Matrix. Journal of [15] X. He, D. Cai, and P. Niyogi. Laplacian score for feature [16] M. Hindawi, K. Allab, and K. Benabdeslem. Constraint [17] J. Z. Huang, M. K. Ng, H. Rong, and Z. Li. Automated [18] M. Kalakech, P. Biela, L. Macaire, and D. Hamad. Constraint [19] J. MacQueen. Some methods for classification and analysis [20] V. Makarenkov and P. Legendre. Optimal variable weighting [21] D. Modha and W. Spangler. Feature weighting in k-means [22] W. Rand. Objective criteria for the evaluation of clustering [23] K. Wagstaff and C. Cardie. Clustering with instance level [24] K. Wagstaff, C. Cardie, S. Rogers, and S. Schroedl. [25] D. Zhang, S. Chen, and Z. Zhou. Constraint score: A new [26] D. Zhang, Z. Zhou, and S. Chen. Semi-supervised [27] Z. Zhao and H. Liu. Semi-supervised feature selection via [28] Z. Zhao, F. Morstatter, S. Sharma, S. Alelyani, A. Anaud,
