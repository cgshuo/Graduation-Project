 Event schema is a high-level representation of a bunch of similar events. It is very useful for the tra-ditional information extraction ( IE )(Sagayam et al., 2012) task. An example of event schema is shown in Table 1. Given the bombing schema, we only need to find proper words to fill the slots when extracting a bombing event.
 There are two main approaches for AESI task. Both of them use the idea of clustering the poten-tial event arguments to find the event schema. One of them is probabilistic graphical model (Chamber-s, 2013; Cheung, 2013). By incorporating tem-plates and slots as latent topics, probabilistic graphi-cal models learns those templates and slots that best explains the text. However, the graphical models considers the entities independently and do not take the interrelationship between entities into account. Another method relies on ad-hoc clustering algo-rithms (Filatova et al., 2006; Sekine, 2006; Cham-bers and Jurafsky, 2011). (Chambers and Jurafsky, 2011) is a pipelined approach. In the first step, it us-es pointwise mutual information(PMI) between any two clauses in the same document to learn events, and then learns syntactic patterns as fillers. How-ever, the pipelined approach suffers from the error propagation problem, which means the errors in the template clustering can lead to more errors in the s-lot clustering.

This paper proposes an entity-driven model which jointly learns templates and slots for event schema induction. The main contribution of this paper are as follows:  X  To better model the inner connectivity between  X  We use constraints between templates and be-Our model is an entity-driven model. This mod-el represents a document d as a series of entities E d = { e i | i = 1 , 2 ,  X  X  X } . Each entity is a quadruple e = ( h, p, d, f ) . Here, h represents the head word of an entity, p represents its predicate, and d repre-sents the dependency path between the predicate and the head word, f contains the features of the entity (such as the direct hypernyms of the head word), the sentence id where e occurred and the document id where e occurred. A simple example is Fig 1.
Our ultimate goal is to assign two labels, a slot variable s and a template variable t , to each entity. After that, we can summarize all of them to get event schemas. 3.1 Inner Connectivity Between Entities We focus on two types of inner connectivity: (1) the likelihood of two entities to belong to the same tem-plate; (2) the likelihood of two entities to belong to the same slot; 3.1.1 Template Level Connectivity
It is easy to understand that entities occurred n-ear each other are more likely to belong to the same template. Therefore, (Chambers and Jurafsky, 2011) uses PMI to measure the correlation of two words in the same document, but it cannot put two words from different documents together. In the Bayesian model of (Chambers, 2013), p(predicate) is the key factor to decide the template, but it ignores the fact that entities occurring nearby should belong to the same template. In this paper, we try to put two mea-sures together. That is, if two entities occurred n-earby, they can belong to the same template; if they have similar meaning, they can also belong to the same template. We use PMI to measure the distance similarity and use word vector (Mikolov et al., 2013) to calculate the semantic similarity.

A word vector can well represent the meaning of a word. So we concatenate the word vector of the j -th entity X  X  head word and its predicate, denoted as vec hp ( i ) . We use the cosine distance cos hp ( i, j ) to measure the difference of two vectors.

Then we can get the template level connectivity formula as shown in Eq 1. The P M I ( i, j ) is cal-culated by the head words of entity mention i and j .
 3.1.2 Slot Level Connectivity
If two entities can play similar role in an event, they are likely to fill the same slot. We know that if two entities can play similar role, their head words may have the same hypernyms. We only consider the direct hypernyms here. Also, their predicates may have similar meaning and the entities may have the same dependency path to their predicate. There-fore, we give the factors equal weights and add them together to get the slot level similarity.

W S ( i, j ) = cos p ( i, j ) +  X  ( depend i = depend j ) Here, the  X  (  X  ) has value 1 when the inner expression is true and 0 otherwise. The  X  X ypernym X  is derived from Wordnet(Miller, 1995), so it is a set of direct hypernyms. If two entities X  head words have at least one common direct hypernym, then they may belong to the same slot. And again cos p ( i, j ) represents the cosine distance between the predicates X  word vector of entity i and entity j . 3.2 Template and Slot Clustering Using Normalized cut intend to maximize the intra-class similarity while minimize the inter class similarity, which deals well with the connectivity between en-tities.

We represent each entity as a point in a high-dimension space. The edge weight between two points is their template level similarity / slot level similarity. Then the larger the similarity value is, the more likely the two entities (point) belong to the same template / slot, which is also our basis intu-ition.

For simplicity, denote the entity set as E = { e 1 ,  X  X  X  , e | E | } , and the template set as T . We use the | E | X | T | partition matrix X T to repre-sent the template clustering result. Let X T = [ X T for template l ( T l ).
 Usually, we define the degree matrix D T as: D
T ( i, i ) = ously, D T is a diagonal matrix. It contains infor-mation about the weight sum of edges attached to each vertex. Then we have the template clustering optimization as shown in Eq 4 according to (Shi and Malik, 2000). where 1 | E | represents the | E | X  1 vector of all 1 X  X .
For the slot clustering, we have a similar opti-mization as shown in Eq 5. where S represents the slot set, X S is the slot clus-tering result with X S = [ X S X S l is a binary indicator for slot l ( S l ). 3.3 Joint Model With Sentence Constraints For event schema induction, we find an important property and we name it  X  X entence constraint X . The entities in one sentence often belong to one template but different slots.

The sentence constraint contains two types of constraint,  X  X emplate constraint X  and  X  X lot constrain-t X . 1. Template constraint : Entities in the same sen-2. Slot constraint : Entities in the same sentence Based on these consideration, we can add an extra item to the optimization object. Let N sentence be the number of sentences. Define N sentence  X | E | matrix J as the sentence constraint matrix, the entries of J is as following: Easy to show, the product G T = J T X T represents the relation between sentences and templates. In ma-trix G T , the ( i, j ) -th entry represents how many en-tities in sentence i are belong to T j .

Using G T , we can construct our objective. To rep-resent the two constraints, the best objective we have found is the trace value: tr ( G T G T the diagonal of matrix G T G T all the entries in the corresponding line in G T , and the larger the trace value is, the less templates the sentence would taken. Since tr ( G T G T of the diagonal elements, we only need to maximize the value tr ( G T G T For the same reason, we need to minimize the value tr ( G S G T
Generally, we have the following optimization ob-jective: The whole joint model is shown in Eq 9. The de-X T , X S = argmax ) 4.1 Dataset In this paper, we use MUC-4(Sundheim, 1991) as our dataset, which is the same as previous works (Chambers and Jurafsky, 2011; Chambers, 2013). MUC-4 corpus contains 1300 documents in the training set, 200 in development set (TS1, TS2) and 200 in testing set (TS3, TS4) about Latin American news of terrorism events. We ran several times on the 1500 documents (training/dev set) and choose the best | T | and | S | as | T | = 6 , | S | = 4 . Then we report the performance of test set. For each doc-ument, it provides a series of hand-constructed even-t schemas, which are called gold schemas. With these gold schemas we can evaluate our results. The MUC-4 corpus contains six template types: Attack, Kidnapping, Bombing, Arson, Robbery, and Forced Work Stoppage , and for each template, there are 25 slots. Since most previous works do not evaluate their performance on all the 25 slots, they instead focus on 4 main slots like Table 1, we will also focus on these four slots. We use the Stanford CoreNLP toolkit to parse the MUC-4 corpus. 4.2 Performance Fig 2 shows two examples of our learned schemas: Bombing and Attacking. The five words in each s-lot are the five randomly picked entities from the mapped slots. The templates and slots that were joint learned seem reasonable.

Induced schemas need to map to gold schemas before evaluation. Previous works used two meth-ods of mapping. The first ignores the schema type, and simply finds the best performing slot for each gold template slot. For instance, a perpetrator of a bombing and a perpetrator of an attack are treated the same. We call this the slot-only mapping eval-uation. The second approach is to map each tem-plate t to the best gold template g , and limit the slot mapping so that only the slots under t can map to slots under g . We call this the strict template map-ping evaluation. The slot-only mapping can result in higher scores since it is not constrained to preserve schema structure in the mapping.

We compare our results with four works (Cham-bers and Jurafsky, 2011; Cheung, 2013; Chambers, 2013; Nguyen et al., 2015) as is shown in Table 2 and Table 3. Our model has outperformed all of the previous methods. The improvement of recall is due to the normalized cut criteria, which can better use the inner connectivity between entities. The sen-tence constraint improves the result one step further.
Note that after adding the sentence constraint, the slot-only performance has increased a little, but the strict template mapping performance has increased a lot as is shown in Table 3. This phenomenon can be explained by the following facts: We count the amount of entities which has been assigned different templates or different slots in  X  X ur Model-SC X  and  X  X ur Model X . Of all the 11465 entities, 2305 enti-ties has been assigned different templates in the two methods while only 108 entities has different slots. This fact illustrates that the sentence constraint can affect the assignment of templates much more than the slots. Therefore, the sentence constraint lead-s largely improvement to the strict mapping perfor-mance and very little increase to the slot-only per-formance. The traditional information extraction task is to fill the event schema slots. Many slot filling algorithms requires the full information of the event schemas and the labeled corpus. Among them, there are rule-based method (Rau et al., 1992; Chinchor et al., 1993), supervised learning method (Baker et al., 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009; Maslennikov and Chua, 2007), bootstrapping method (Yangarber et al., 2000) and cross-document inference method (Ji and Grishman, 2008). Also there are many semi-supervised solutions, which begin with unlabeled, but clustered event-specific documents, and extrac-t common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Patwardhan and Riloff, 2007; Filatova et al., 2006; Surdeanu et al., 2006)
Other traditional information extraction task learns binary relations and atomic facts. Models can learn relations like  X  X enny is married to Bob X  with unlabeled data (Banko et al., 2007; Etzioni et al., 2008; Yates et al., 2007; Fader et al., 2011), or ontology induction (dog is an animal) and at-tribute extraction (dogs have tails) (Carlson et al., 2010a; Carlson et al., 2010b; Huang and Riloff, 2010; Van Durme and Pasca, 2008), or rely on pre-defined patterns (Hearst, 1992).

Shinyama and Sekine (2006) proposed an ap-proach to learn templates with unlabeled corpus. They use unrestricted relation discovery to discover relations in unlabeled corpus as well as extract their fillers. Their constraints are that they need redun-dant documents and their relations are binary over repeated named entities. (Chen et al., 2011) also ex-tract binary relations using generative model.
Kasch and Oates (2010), Chambers and Jurafsky (2008), Chambers and Jurafsky (2009), Balasubra-manian et al. (2013) captures template-like knowl-edge from unlabeled text by large-scale learning of scripts and narrative schemas. However, their struc-tures are limited to frequent topics in a large corpus. Chambers and Jurafsky (2011) uses their idea, and their goal is to characterize a specific domain with limited data using a three-stage clustering algorith-m.

Also, there are some state-of-the-art works using probabilistic graphic model (Chambers, 2013; Che-ung, 2013; Nguyen et al., 2015). They use the Gibbs sampling and get good results. This paper presented a joint entity-driven model to induct event schemas automatically.

This model uses word embedding as well as PMI to measure the inner connection of entities and us-es normalized cut for more accurate clustering. Fi-nally, our model uses sentence constraint to extract templates and slots simultaneously. The experiment has proved the effectiveness of our model.
 This research is supported by National Key Basic Research Program of China (No.2014CB340504) and National Natural Science Foundation of China (No.61375074,61273318). The contact authors of this paper are Sujian Li and Baobao Chang.

