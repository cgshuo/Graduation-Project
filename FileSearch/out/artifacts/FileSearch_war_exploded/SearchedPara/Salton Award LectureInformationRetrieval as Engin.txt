 H.3.0 [ Information Storage and Retrieval ]: General Theory, Experimentation theoretic foundations, explanations, standardized testing First, let me say how pleased I am to receive the Gerard Salton Award from SIGIR. His pioneering work strongly in-fluenced our field for several decades. I had lively discussions with him at several occasions, especially during his stay with our research group in Darmstadt in 1988. We were following different theoretic approaches X  X e as inventor of the vector space model, myself as a young researcher being enthusi-astic about probabilistic models X  X ut the discussions with him improved my understanding of the commonalities and differences of both approaches, such as the role of represen-tations, underlying assumptions and theoretic justifications.
Like my predecessors, here I want to give my personal view on information retrieval, by first presenting my own definition of the field, and then pointing out some areas for future research.

For me, IR deals with vagueness and imprecision in in-formation systems . The imprecision is usually caused by the imperfection in the representation of the semantics and pragmatics of the objects stored, which are typically (mul-timedia) documents. Vagueness, on the other hand, is due to the fact that the user is not able to formulate her in-formation need in a precise way; this leads to an iterative search process (which might even occur with databases for formatted data, like e. g. in product search in a Web shop).
This definition does not make the usual distinction be-tween database (DB) and IR systems. In fact, in a cer-tain aspect, I view IR as a kind of generalization of the former: The logical DB view interprets query processing as the task of finding those objects o for a query q for which the implication o  X  q holds. Rijsbergen defined IR as being based on uncertain inference, i.e. determining the probabil-ity P ( d  X  q ) that document d implies the query. Thus, from a querying point of view, classic DBs can be regarded as a special case of IR (though probabilistic DBs have become a major research topic in the DB field in recent years).
However, the major difference between DB and IR lies in the consideration of the pragmatic level: The major goal of an IR system is to support the user in solving her current problem or fulfilling her task at hand. Thus, standard IR evaluation is based on the concept of relevance or, in the case of user-based experimentation, on task-related criteria like task time or success rate. In contrast, the DB field del-egates the pragmatic aspects to the application layer, which is beyond the DB scope.

Now let me turn to the major topic of this paper: IR as an engineering discipline. Certainly, many people work-ing in this field would feel themselves as IR  X  X ngineers X , i.e. they apply and extend known methods for solving new prob-lems. However, a civil engineer planning a new building, a mechanical engineer constructing a new machine or an elec-trical engineer designing a new device X  X hey all build upon a rich portfolio of basic findings and theoretic models, which not only allow them to solve the problem, but also make pre-dictions about the result of their efforts (as well as knowing the limits of their methods). In analogy, assume an IR  X  X n-gineer X  being confronted with the task of designing a system for a new collection of documents and a new type of in-formation needs X  X ould she be able to guarantee a certain MAP or an average search task completion time? Rather not X  X nstead, the standard practice is to try out some of the existing methods and tune them for the specific case X  then an evaluation will show if and how well this system works.
 So we might wonder what is required for constructing an IR system like in other engineering disciplines? I think we need two things that allow us to build upon, namely 1) the-oretic models, and 2) solid empirical evidence.

On the theory side, the basis is rather small. E.g., for the case of ad-hoc retrieval, we have the probability rank-ing principle, various models based on it (like the classi-cal probabilistic models, the principle of uncertain infer-ence, and language models / divergence from randomness), and the term weighting axioms. These frameworks tell us how to achieve good or even optimum retrieval performance, provided that the underlying assumptions hold. However, we hardly have theories that tell us why certain methods work and others don X  X . From an engineering perspective, we would like to know how document features like length, language, domain, genre, structure or the term definition (single words, phrases, named entities etc.) affect quality (and why) X  X nd in a similar way, also for the topic char-acter istics as well as the relevance definition. Alas, we can hardly answer any of these questions.

On the empirical side, we have a vast amount of papers presenting numerous variants of existing models and meth-ods, along with experimental results. This is partly a conse-quence of favoring experimental over theoretic work in our field. However, in the light of our general theme of IR as engineering science, this development was not helpful, for two major reasons: 1. We are not able to give a fully satisfying explanations 2. We can not predict to what extent these findings can
In order to draw any useful conclusions from experimen-tal results, we need standardized test environments and test procedures for being able to aggregate knowledge from larger numbers of experimental studies. I know only of a sin-gle meta-study [1] that looked at papers proposing specific methods for improving the quality of ad-hoc retrieval, which used some of the official TREC and CLEF collections. Al-though all of the considered papers claimed some perfor-mance improvements over previous work, hardly any of them was actually able to beat the best official run for the corre-sponding collection X  X he authors had been cheating by us-ing poor baselines for comparison. This experience calls for more standardized experimentation, so that experimental results can be compared immediately.

Unfortunately, in recent years, we have seen a trend in the opposite direction: A large fraction of the papers at the top IR venues publish experimental results using propri-etary collections. Thus, the experiments presented cannot be repeated by other researchers. This procedure does not satisfy the basic requirements for empirical scientific work, and it is kind of surprising how the IR community tolerates this violation of scientific standards. Proprietary data not only eases cheating (as we are no longer able to find out, this might even increase the frequency of this kind of mis-conduct), the major problem is the impediment of scientific progress: If someone has an idea on how to improve over the method presented in a paper (and the vast majority of our research is of that kind), then there is no possibility to perform this research, since the data used for the previous work is inaccessible. Some people claim that it would be sufficient to describe the major characteristics of the data used, so that other researchers could set up a similar test collection to perform that work. However, we are far away from such an approach X  X e just don X  X  know the relevant fea-tures that determine retrieval quality and thus would allow for repeating IR experiments with different collections.
However, even with the best experimental standards, it is mainly theoretic work that will move our field forward towards an engineering discipline: A good theoretic model has a high explanatory power, and it is based on explicit as-sumptions. The former allows us to make predictions, and the latter specifies the conditions under which the model is applicable. A more detailed discussion of experimental vs. theoretic work in computer science can be found in [2], where the main conclusion can be summarized in the some-what paradox statement  X  X here is empirical evidence that the most important contributions to our field are not the experimental ones X .

The above discussion has focused on classical, system ori-ented IR approaches, but it also holds for user-oriented ap-proaches ([3, p. 105] calls for engineering in information seeking) as well as for new research areas in IR [ ? ].
Empirical research is necessary, but it must be accompa-nied by strong theoretic models. IR evaluations should focus more on answering the why questions, and less on how to achieve good performance for the test collections at hand. Only this kind of research will put an IR engineer into the position to judge whether or not a specific model is applica-ble in a given situation, and then use it for making predic-tions about the properties of the system she is designing. [1] T. G. Armstrong, A. Moffat, W. Webber, and J. Zobel. [2] G. G  X enova. Is computer science truly scientific? [3] P. Ingwersen and K. J  X  arvelin. The turn: integration of
