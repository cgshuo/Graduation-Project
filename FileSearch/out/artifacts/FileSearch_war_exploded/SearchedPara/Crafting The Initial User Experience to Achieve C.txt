 Recommender systems try to address the  X  X ew user problem X  by quickly and painlessly learning user preferences so that users can begin receiving recommendations as soon as possible. We take an expanded perspective on the new user experience, seeing it as an opportunity to elicit valuable contributions to the community and shape subsequent user behavior. We conducted a field experiment in MovieLens where we imposed additional work on new users: not only did they have to rate movies, they also had to enter vary-ing numbers of tags. While requiring more work led to fewer users completing the entry process, the benefits were significant: the re-maining users produced a large volume of tags initially, and con-tinued to enter tags at a much higher rate than a control group. Further, their rating behavior was not depressed. Our results sug-gest that careful design of the initial user experience can lead to significant benefits for an online community.
 H.5.2 [ Information Interfaces and Presentation ]: User Inter-faces Design, Experimentation recommender system, entry barrier, user experience, online com-munities, tagging, user interfaces, new user problem
First impressions matter. In online communities, potential mem-bers quickly evaluate the community and decide if they want to participate in it. And community designers want to present poten-tial members an accurate picture of the community, including what is expected from members.

For recommender systems, the initial user experience has fo-cused on the  X  X ew user problem X  [15, 18]. The system must learn enough about a user X  X  preferences to make accurate recommenda-tions. The goal has been to do this with minimal user effort, thus preventing users from getting discouraged and giving up.
We suggest a different perspective: rather than a new user prob-lem , we see a new user opportunity . By shaping new users X  experi-ence, a system may influence the amount and type of contributions users make to the community, thus advancing community goals. Our work has three pillars:
We applied these ideas in MovieLens (MovieLens.org). We re-designed the initial user experience, measuring the effect of our changes on work accomplished and user behavior patterns. Specif-ically, we modified the existing process  X  which required new users to rate a number of movies  X  to also require new users to tag some movies. We experimented with several different amounts of tags, thus constructing relatively higher and lower entry barriers. We then measured the proportion of users who made it through the en-try barrier in each condition (as well as a control group), and the amount and types of contributions (ratings and tags) they subse-quently made to MovieLens.
We organized our research around four main questions. We sum-marize them and briefly preview our results. 1. How many new users do we lose by imposing higher entry 2. How much work (of the requested type) will users do? Most 3. To what extent can we  X  X hape X  ongoing user behavior? Re-4. Is user activity a  X  X ero sum game X ? No. Gains in tagging Thus, while asking new users to do more work leads some to  X  X rop out X , we found that the benefits outweighed the costs: the fewer users who completed the entry process did much more work and became more active members of the community.

In the remainder of the paper we survey related work, describe our research platform and methods for quantifying user contribu-tions, outline our experimental design, present our findings and dis-cuss their implications, and close with a brief summary.
The New User Problem. Until a new user X  X  preferences are discovered, it is difficult for a recommender system to make per-sonalized recommendations. There are two main ways a system can learn new user preferences: explicit and implicit . Explicit tech-niques require users to state preferences for items by rating them on some scale (1 to 5 stars, thumbs-up vs. thumbs-down, etc.). Pre-vious research [15, 18] has focused on clever methods for selecting items for a user to rate. Factors considered include: how likely a user is to be able to rate an item, how much rating a particular item will tell the system about the user X  X  taste, and how useful a rating would be for the community. However, while community utility has been identified as a factor of interest, algorithms have not yet incorporated it.

Implicit techniques infer user preferences from observed user behavior. Amazon.com treats the items users purchase and view implicit interest indicators, including mouse clicks and movement, time spent, and scrolling behavior, and correlated these with ex-plicit ratings to assess their accuracy. They found that time spent on a page and scrolling behaviors correlated strongly with explicit ratings.
 To emphasize, we are not trying to solve the new user problem. Instead of streamlining the process of learning new user prefer-ences, we are exploring what happens when we ask new users to do more work.

The Group Entry Process. Off-line groups traditionally have ways to control group membership. They create entry barriers to filter out people who  X  X on X  X  belong X  and enhance the desirability of the group. Entry barriers take many forms, including required de-mographic characteristics (American Association of Retired Peo-ple: age 50 and older), intelligence test scores (Mensa), or income level (costly membership fees). They can require more or less com-mitment from potential members, ranging from undergoing college fraternity hazing to simply signing up for a campus student group. Online communities and other web sites have entry barriers, too. Content sites may require users to pay a fee (Angie X  X  List, Flickr Pro), register for a free account (New York Times), or view ad-vertisements (Salon Premium). Many social networking sites like Facebook and LinkedIn make new users complete a profile con-taining personal information, preferences, and a list of friends or colleagues.

Social psychology research has shown the power of entry barri-ers. Aronson and Mills [1] conducted a lab experiment that found a correlation between the difficulty of an entry barrier and people X  X  level of commitment to a group once they passed the barrier. In-tuitively, if you  X  X o through hell X  to get into a group, you X  X e likely to convince yourself that it must be worth it. While this research inspired our idea to experiment with a more challenging entry pro-cess, we must be careful in how we apply it to online communities. In particular, subjects in a lab study are unlikely to leave the ex-periment, and they know it is just an experiment. However, people are used to checking out various web sites and quickly abandoning them if they X  X e not of interest. Thus, we did not expect Aronson and Mill X  X  results to transfer directly.
 Motivating Contributions to Open Content Communities.
 Open Content Communities rely on their users to produce valu-able content. Wikipedia is the most well-known example, but other sites like Slashdot [12] and Flickr also follow this approach. Rec-ommender systems like MovieLens always have relied on users to enter ratings, and they have moved even further in the direction of Open Content. For example, Cosley [5, 6] experimented with let-ting MovieLens users enter movies into the database and edit movie information (actors, director, etc.).

When content comes from users, getting users to produce enough  X  and high quality  X  content is a key challenge. Cosley developed different algorithms to match movies that needed editing with users who might be interested in doing the editing. He found that intel-ligent matching techniques led to significantly more editing work being done than random matching. Cosley also showed these ideas could be extended to Wikipedia [7]. Beenen et al [2] took another approach, showing that setting specific, challenging goals (e.g., numbers of movies to rate) gets users to do more work than simply asking them to  X  X o their best X .

Other work has explored at a general level what motivates peo-ple to participate in voluntary activities. Building on seminal stud-ies of off-line volunteerism [3] and motivations for participating in open source software projects [10], Nov [14] surveyed Wikipedia editors about why they edited the encyclopedia. The most com-mon responses were because editing was fun and because editors believed that information should be free.

In summary, where prior work on the initial user experience in recommender systems saw a  X  X ew user problem X , we see an op-portunity. Research on entry barriers and goal setting leads us to consider whether asking more of potential users can increase the amount of valuable work they accomplish and shape their future behavior along useful paths. Figure 1: Prediction accuracy using initial ratings to predict subsequent 50 ratings
We conducted our research in the MovieLens film recommen-dation web site, run by the GroupLens research group at the Uni-versity of Minnesota. MovieLens enables users rate movies and receive recommendations, tag movies with descriptive keywords, and participate in movie-oriented discussions.

Current Entry Process. MovieLens learns user preferences by requiring each new user to rate 15 movies. Once users have done this, they are free to use the system as they choose.

Redesigning the Entry Process. Recall that the aim of our re-search is to investigate how different sizes of entry barriers affect how much  X  X ork X  users do during the entry process and what their subsequent usage is like. Entering movie ratings already is an entry barrier, so an obvious path would be to manipulate the number of ratings new users enter.

However, we chose not to do this because  X  as detailed below  X  the number of initial ratings users enter affects the accuracy of the recommendations they receive. Since MovieLens is a real web site that supports an active user community (over 150 active users on any given day), we want all users to receive equally accurate recommendations. Therefore, we instead chose to introduce an ad-ditional entry barrier: entering tags for movies. Further, we didn X  X  want to make the entry process too onerous, since this could result in too many new users getting frustrated and abandoning the site. Therefore, we sought to minimize the number of ratings required while still preserving recommendations accuracy.

To find a lower number of ratings that would retain an acceptable level of predictive accuracy, we  X  X eplayed X  the history of a sample of users from the MovieLens database, while asking the question: how accurately could we recommend to this user if he/she only rated 1 (2, 3, 4, ..., 15) movies? We selected a random sample of 5000 users with more than 65 ratings (about 30% of all users). For each user, we created a chronological history of ratings. We con-structed an initial test set beginning with the user X  X  first rating, then measured the difference between predicted and actual rating for the next 50 movies in the user X  X  rating history. This was repeated for all 5000 users to determine the mean absolute error (MAE). The first point in Figure 1 shows the result of the first iteration. The
Figure 3: Thumbs-up, thumbs-down tag rating interface. remaining points represent the MAE as the test set size increased from 2 to 15 ratings.

From these results, we decided that asking new users to enter just 5 ratings would be acceptable. It is low enough that we felt com-fortable experimenting with a relatively high tagging requirement. Yet the prediction accuracy is high enough that we did not believe that user satisfaction with the system would suffer. MAE increased by 0 . 073 , from 0 . 687 for 15 ratings to 0 . 760 for 5 ratings.
Overview of tagging in MovieLens. We chose tagging as the additional entry task for several reasons. Tags are familiar to most users, being featured in popular social websites like Flickr and del.icio.us, as well as major e-commerce sites like Amazon.com. Tags are useful: they help users evaluate movies, aid navigation, and support browsing to discover movies with common themes. Therefore, we believed that users would be willing and able to tag, and that tags they entered would be a valuable asset to the system. Finally, each  X  X nit X  of work  X  tagging one movie with one tag  X  is easy to accomplish. Therefore, we could create a very low en-try barrier  X  tagging just a few movies  X  as well as an arbitrarily high one  X  simply asking users to tag many movies. We detail the barriers below.

Tags are shown in several places throughout MovieLens. When a user does a search, the results page shows movies and associated information including title, year of release genre, and tags (Figure 2). Tags are divided into those added by the user ( X  X our tags X ) and popular tags added by other users. Tags also appear on movie  X  X etails X  pages; Figure 3 shows the tags applied to the movie Juno .
In order to measure the effectiveness of entry barriers, we must establish metrics that reflect the quantity and quality of work com-pleted by a user. We first focused on examining what types of be-havior should be included in our metrics. In MovieLens, the possi-ble types of contributions include include ratings, tags, discussion forum posts, and movie information edits. We chose to include ratings because they were critical to recommender system perfor-mance. We also included tags to measure the effects of entry barrier conditions. We considered counting forum posts and movie edits, but concluded that the level of these activities was too low to see significant differences 1 .
 Second, we explored how we should count tags and ratings. While a simple count is useful, it isn X  X  sufficient, since not all con-tributions are equal. Therefore, we introduced a notion of value . For instance, if a user rates a movie with 8000 ratings, this may have less value to the community than a rating for a movie with 10 ratings [15]. Furthermore, if if a user enters a purely personal tag like  X  X ant to rent X , this might not be as valuable as an objective tag such as  X  X scar-winner X .

We distinguish between two aspects of value: intrinsic quality and influence on the community. The quality of a tag is subjective: one user may consider  X  X heap Star Wars ripoff X  a perfect description of a movie, while another vehemently disagrees. Some tags are useful only to the person who entered them, like  X  X n netflix queue X . We measured tag quality us-ing collective opinion of the user community. We were able to do this because MovieLens includes an interface that lets users rate tags as  X  X humbs up X  (good tag, I agree) or  X  X humbs down X  (poor tag, I don X  X  agree) for a tag [16]. Over 90% of tags in MovieLens have been rated. Tag ratings are independent from movie ratings, and currently have no impact on movie recommendations and pre-dictions.

Figure 3 shows the tags associated with the movie Juno . Two icons appear to the right of each community tag. These icons let users rate tags thumbs up or thumbs down. A tag rated thumbs-up is moved to the top of the list of community tags, thus increasing its visibility. A tag rated thumbs-down is moved to the bottom of the list of community tags. More than 75% of tag ratings in MovieLens are thumbs-down 2 . When enough users rate a tag thumbs-down, it will no longer be shown in the short list of tags in search results (Figure 2).
For ratings in a recommender system, influence is easy to define (at least intuitively): how much does entering a rating for a movie affect how that movie and other movies are recommended in the future? Defining rating quality is harder.
The very point of a recommender system is to accommodate sub-jective preferences: one user may hate  X  X o Country for Old Men X , while another loves it. An algorithm finds the proper neighborhood for each user, and like-minded users in each neighborhood become a source of accurate personalized recommendations.

Although it seems counterintuitive to objectively measure the quality of a subjective rating, researchers who study malicious at-tacks on recommender systems address similar issues. Shilling is the practice of entering random or insincere ratings to try to ma-nipulate the recommendations of a given item. Shilling typically appears in e-commerce settings, where a company might try to
To be safe, we did look for differences in our analysis. As we expected, we did not find any.
We speculate that this is due to the high subjectivity of movie preferences and because the set of tags has not reached maturity yet: i.e., the good tags have not yet  X  X rowded out X  the poor ones. get manipulate their products X  ratings (up) and their competitors X  products X  ratings (down). Although this is an active area of re-search [11], there are no standard metrics for us to apply. Further-more, it is not obvious that metrics developed for shilling detection would be appropriate for our purposes. Due to these considerations, we do not attempt to measure rating quality in this research.
In a collaborative filtering system, ratings are used to create pre-dictions for other users or items. Thus, a rating X  X  impact can be measured by how much it influences predictions.

To quantify the impact of rating a movie, we created a model from a subset of the MovieLens ratings data. We calculated the av-erage change in prediction for the target movie and all other movies each time a rating was made. We binned movies by the number of ratings they had, and then calculated an average influence for each bin. Each time a user rated a movie, they were allocated a number of credits according to how many ratings the movie had. Users were given more credits for movies with fewer ratings, as rarely rated movies are greatly influenced by any ratings they receive. Movies with many ratings earned less credit, as rating a frequently rated movie has little impact on other movies, and even less impact on the target movie.
Figure 4 gives an overview of our experimental design. We as-signed new MovieLens users to four groups, a control group and three experimental groups. All users were required to rate five movies. After doing this, the control group was immediately di-rected to MovieLens, while the experimental groups were taken to the tagging interface (Figure 5). Users were able to navigate through the list of movies in order to find movies they were able to tag. Users could tag any movie, whether or not they had rated it. Tags created by users in this experiment immediately became Figure 5: Screen shot of the tagging interface shown during the entry barrier  X  X ive X  in MovieLens, so the experimental users created real value for the community.

Research on  X  X ognitive dissonance X  [1, 9] predicts that requiring more work will make users who do complete the entry barrier value MovieLens more, and thus (we hope) contribute more. However, it isn X  X  clear what the proper size of the barrier is: too small, and the effect may be minimal; too large, and the added value produced by the users who complete the entry process may not make up for those who do not complete. Therefore, we experimented with several different  X  X izes X  of entry barriers to try to find an effective setting.
The high barrier group was required to enter 25 tags; we refer to this group as TAG25 throughout the rest of this paper. The low barrier group was required to add five tags (TAG5). Note that the requirement was for tags, not movies. In other words, one TAG25 user could apply 25 tags to one movie, and another could apply one tag to each of 25 different movies.

We also created an exposure only group. This group was shown the tagging interface, but was not required to enter any tags; we refer to this group as TAG0. This condition let us distinguish two possible influences on user behavior: requiring them to do a task (e.g., enter tags) and simply exposing them to the possibility of do-ing that task. TAG0 users could use the tag rating interface as much as they wanted or simply click a link to go directly to MovieLens.
New MovieLens users were enrolled in the experiment over a period of 45 days, and were randomly assigned to a condition. In the first week, we experimented with several different values for the low and high entry barriers, and we only realized the need for the TAG0 condition after running the experiment for about a week. Therefore, the groups did not end up with equal numbers of users. Of the 583 users enrolled in the experiment, 177 were assigned to the control group, 117 were assigned to TAG0, 161 were assigned to TAG5, and 128 were assigned to TAG25. Our analysis all scale for the difference in group size, and the statistics we use are appro-priate for groups of different sizes.
For each of our four research questions, we computed several metrics to answer that question. 1. How many new users do we lose by imposing higher entry Figure 6: Percentage of users who completed the entry barrier 2. How much work (of the requested type) will users do? For 3. To what extent can we  X  X hape X  ongoing user behavior? This 4. Is user activity a  X  X ero sum game X ? Suppose we can get
Who counts as  X  X n X  an experimental condition? There are two possible answers to this question: (1) all users who were assigned to a condition, and (2) all users who completed that condition X  X  entry process. Each answer is useful for some purposes. Generally, for pre-entry activities we prefer the first answer, and for post-entry activities we prefer the second answer. Put another way, the first approach is more useful for measuring total work accomplished, and the second is more useful for understanding the kind of user  X  X reated X  by each entry condition. When we report results, we state clearly which of these two definitions we are using.
Figure 6 shows the percentage of people who completed the en-try process for each condition. As one would expect, the higher the entry barrier, the lower the percentage of users who completed the entry process. The control group had the highest completion rate at 90% (160/177). The completion rates for the experimental groups are as follows: TAG0 85% (99/117), TAG5 80% (128/161), and TAG25 69% (89/128). A likelihood ratio Chi-Squared test shows that the differences between conditions are statistically significant (  X  2 =22.7, DF=3, p &lt; .0001).
As we mentioned, there are two sources of tags that must be considered in answering this question. The first source is the entry process  X  when (some) users are required (or given the opportunity) to enter tags  X  and the second is subsequent normal system usage. We focus on the first source now, and deal with the second source next.

Tag Count. Figure 7 summarizes the number of tags entered during the entry process in each condition. We can draw several interesting conclusions from these data. 1. The entry barriers  X  X orked X . Since most users completed 2. Instituting any of the TAG conditions as the normal Movie-3. TAG0 looks pretty competitive with TAG5: even though
There is one caution to our conclusion that the entry barriers worked: what if users simply rushed through the tagging process, entering poor quality tags to complete as quickly as possible? Our next two analyses investigate this issue.

Tag Quality. We measure the quality of a tag as the percentage of  X  X humbs up X  votes the tag receives. Figure 8 shows average tag quality for each condition. We user-normalized tag averages by first calculating each user X  X  average rating for a tag, and then averaging Figure 8: Tag quality by condition, measured by percentage of thumbs-up tag ratings each user X  X  result. User-normalization reduced the effects of power users such as the user who rated the tag  X  X omedy X  thumbs-down on 206 different movies.

None of the differences between conditions were statistically significant. Further, the experimental observations (the tags them-selves) are not truly independent since a single user creates multiple tags. This further reduces the apparent differences between condi-tions. Therefore, the potential concern that users in TAG5 and (es-pecially) TAG25 entered poor quality tags during the entry process were unfounded. The vast increase in quantity was not purchased with a decrease in quality.
The TAG conditions led to a burst of tags during the entry pro-cess. However, a different issue was whether the initial exposure to tagging affected subsequent (post-entry) user behavior. Put simply, did we create  X  X aggers X ?
We did two analyses to answer this question. For each condition, we computed the percentage of users who tagged at all after the entry process and the average number of post-entry tags entered per user per day.

There are significant differences between the conditions in the number of users who tagged during the post-entry phase (Figure 9). Only 5.6% of the control group and and 5.1% of the TAG0 group entered any tags after the entry process. Contrast this with users who were required to tag during the entry process: 19.5% of TAG5 users, and 24.7% of TAG25 users entered at least one tag during the post-entry phase. A likelihood ratio Chi-Squared test again reveals that TAG5 and TAG25 are significantly different from TAG0 and control ( n = 476 ,  X  2 = 28 . 1 , DF = 3 , p &lt; 0 . 0001 ).
This analysis was done only for users who completed the en-try barrier. However, we can also do it for all users assigned to each condition. Again, the differences between TAG5 (15.5%) and TAG25 (16.4%) on the one hand, and TAG0 (4.3%) and control (5.1%) on the other were significant (likelihood ratio chi-squared test, n = 583 ,  X  2 = 20 . 7 , DF = 3 , p &lt; 0 . 0001 ). In other words, being assigned to tag movies during a user X  X  initial experi-ences made him or her more likely to tag later. This is important, because it shows that the entry process can shape subsequent user Figure 9: Percentage of users who added at least one tag post-entry Figure 10: Average number of tags added post-entry, per user, per day behavior, not just filter people who already were more likely to ex-hibit a particular behavior.

Figure 10 summarizes the amount of post-entry tagging for users in each condition. While all the numbers were small, TAG25 and TAG5 users entered an order of magnitude more tags than control and TAG0 users (  X  2 = 29 . 1 , DF = 3 , p &lt; . 0001 ).
While we have shown that we were able to induce new post-entry behavior in users, a final question is whether we did so at the cost of existing behavior. In particular, did users who tagged more rate less? Was it a zero-sum game?
Figure 11 shows the average number of post-entry ratings per user per day. It shows two averages per condition. The darker bars represent an average across only those users who completed their entry barrier, and the lighter bars show the average across all users who were assigned to that condition. In neither case were there any significant differences (for users who completed the entry barrier: Wilcoxon test,  X  2 = 2 . 1 , DF = 3 , p = . 56 ; for all users assigned to a condition: Wilcoxon test,  X  2 = 3 . 3 , DF = 3 , p = . 34 ). We also applied the ratings influence metric to post-entry ratings. Figure 11: Average number of post-entry ratings per user, per day Again, there were no significant differences between conditions. Thus, the increase in tags was not purchased with a decrease in (quantity or quality of) ratings.
Our findings are encouraging. They suggest that designers of a recommender based community  X  and perhaps other types of online communities  X  can tailor the entry process to achieve community goals. We showed that introducing a tagging requirement to the MovieLens entry process resulted in a huge increase in the num-ber of tags entered into the system with a relatively small increase in user attrition. Further, requiring users to tag during their initial experience significantly increased the probability they would tag in subsequent visits: thus, we appear to have shaped the long-term be-havior of MovieLens users. Interestingly, this was true only when new users were required to tag, not when they were simply given the opportunity to do so. Finally, when users tagged more, they did not rate any less, so user activity did not appear to be a zero-sum game.

Our results suggest much interesting future research. First, try-ing our approach in other online communities would help estab-lish generality of our results. Second, we would like to expand our focus beyond the entry process, seeking to shape user behav-ior throughout the user life cycle by suggesting tasks for users to perform. Finally, we would like to develop a unified metric for quantifying different types of user contribution to an online com-munity.
We gratefully acknowledge our experimental subjects and mem-bers of the GroupLens research group at the University of Min-nesota. This research was supported by the National Science Foun-dation, grant IIS 03-2485. [1] E. Aronson and J. Mills. The effect of severity of initiation [2] G. Beenen, K. Ling, K. Ling, X. Wang, K. Chang, [3] E. Clary, M. Snyder, R. Ridge, J. Copeland, A. Stukas, [4] M. Claypool, P. Le, M. Wased, and D. Brown. Implicit [5] D. Cosley, D. Frankowski, S. Kiesler, L. Terveen, and [6] D. Cosley, D. Frankowski, L. Terveen, and J. Riedl. Using [7] D. Cosley, D. Frankowski, L. Terveen, and J. Riedl. [8] S. Drenner, M. Harper, D. Frankowski, J. Riedl, and [9] L. Festinger and J. Carlsmith. Cognitive consequences of [10] A. Hars and S. Ou. Working for free? -motivations of [11] S. K. Lam and J. Riedl. Shilling recommender systems for [12] C. A. Lampe, E. Johnston, E. Johnston, and P. Resnick. [13] E. A. Locke and G. P. Latham. Building a practically useful [14] O. Nov. What motivates wikipedians? Commun. ACM , [15] A. M. Rashid, I. Albert, D. Cosley, S. K. Lam, S. M. McNee, [16] S. Sen, F. M. Harper, A. LaPitz, and J. Riedl. The quest for [17] S. Sen, S. K. Lam, A. M. Rashid, D. Cosley, D. Frankowski, [18] P. Viappiani, P. Pu, and B. Faltings. Conversational
