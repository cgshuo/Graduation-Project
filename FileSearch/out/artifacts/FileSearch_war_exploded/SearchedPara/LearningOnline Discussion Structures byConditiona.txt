 Online forum discussions are emerging as valuable infor-mation repository, where knowledge is accumulated by the interaction among users, leading to multiple threads with structures. Such replying structure in each thread conveys important information about the discussion content. Un-fortunately, not all the online forum sites would explicitly record such replying relationship, making it hard for both users and computers to digest the information buried in a discussion thread.

In this paper, we propose a probabilistic model in the Con-ditional Random Fields framework to predict the replying structure for a threaded online discussion. Different from previous replying relation reconstruction methods, most of which fail to consider dependency between the posts, we cast the problem as a supervised structure learning problem to incorporate the features capturing the structural depen-dency and learn their relationship. Experiment results on three different online forums show that the proposed method can well capture the replying structures in online discussion threads, and multiple tasks such as forum search and ques-tion answering can benefit from the reconstructed replying structures.
 I.5.1 [ Pattern Recognition ]: Models -Statistical Algorithms, Measurement, Experimentation Threaded Discussion, Replying Relation Reconstruction, Struc-ture Learning
As the development of Web 2.0, more and more people take the advantage of online forum discussions to freely share and exchange their mind and knowledge. Valuable knowledge and information on various topics, e.g., sports, health, entertainment and etc., have been accumulated by this collaborative content contribution. More importantly, such knowledge can hardly be found in general web sites and encyclopedia, making forums a unique and valuable resource for extracting useful knowledge to facilitate other informa-tion seeking tasks, including forum search [2, 13], question answering [4, 6] and expert finding [21, 8]. Fi gure 1: A sample threaded discussion from Apple Discussions
A typical online forum discussion originates from a root post, which initializes a topic for the following discussions, e.g., system failure in Mac OS X in Apple discussions as shown in Figure 1. The followers read existing messages and reply to the post they are informed of or most interested in. From temporal perspective, those replying posts form a chain structure, or thread (as shown in the time line in Figure 1). Replying posts can reply to any preceding post, forming branches of discussion as more users are joining in and making comments. As a result, the discussion thread grows and forms a tree structure from semantic perspective: one post has only one  X  X eply-to X  post, while one post can be replied to by multiple posts.

The semantic tree structure is helpful for both human to digest the discussion content and automatic method to ex-tra ct useful information from it. From user X  X  perspective, it would save a lot of effort to track and get involved in the discussion if the site provides a tree view of the current discussion structure. [13, 5] demonstrate that incorporating the replying relationship boosts the forum retrieval perfor-mance; Zhang et al. [21] show that ranking users by their replying relationship would further help to identify the ex-perts in online communities than solely by their post counts.
Nevertheless, such tree structure is not always present in most of online discussion sites. Because phpBB ( http: //www.phpbb.com ) and vBulletin ( http://www.vbulletin. com ), which are two most popular softwares to build the online discussion site, do not provide a threaded view as de-fault [13], flat-view online community pages are much more prevalent. In the flat-view systems, the replying relation is not explicitly recorded (some of them contain partial con-tent from the replied post as quotation), and all the posts in one thread are positioned by the chronological order.
In this circumstance, reconstructing the replying relation-ship from the flat-view discussion forums would be useful. However, this reconstruction task is quite challenging: 1) posts in a discussion thread are temporally dependent upon each other -users always read some of the previous messages before posting; 2) most of the posts are short and highly context dependent, solely examining the post content is in-adequate to understand the replying relationship between the posts; 3) rich features are needed to perform better pre-diction -user X  X  replying habits, friendship and even their online periods are all potential features helpful for analyz-ing the threaded discussions.

Several approaches have been proposed to address this kind of reconstruction task. A topic modeling based ap-proach is proposed in [11], where the temporal relationship is incorporated into semantic modeling of threaded discus-sions. In [13], the reconstruction task is converted into a retrieval problem, in which the authors proposed various features to train a Ranking SVM model. Wang et al. [19] utilize a variant of Latent Semantic Analysis (LSA) method to overcome the sparseness in online discussion contents and identify the replying post pairs.

Previous work mainly focused on applying content analy-sis techniques to identify the replying relation between the posts. However, they do not take other factors, e.g. users in-teractions and structural dependency, into consideration. In this paper, we cast this replying relationship reconstruction problem as a structural learning task and propose a thread-CRF model to solve it in the probabilistic model frame-work. Various features are incorporated into the proposed threadCRF to capture both long-range and short-range de-pendencies among the posts. To estimate the relative im-portance of those features, a supervised learning framework is employed. A set of novel evaluation metrics is intro-duced to access the quality of structural prediction. Ex-periment results on three different forum collections, Ap-ple Discussion ( http://discussions.apple.com/ ), Google Earth Community ( http://bbs.keyhole.com/ ) and CNET forums ( http://forums.cnet.com/ ), confirm the effective-ness of the proposed method: structure reconstruction qual-ity gets encouraging improvement over the baseline methods; tasks like forum retrieval and community question answering benefit from the reconstructed replying structure. More im-portantly, the proposed method is adaptive and generalizes well when trained on different domains.
Much work has been done recently on extracting informa-tion from online forums. Ding et al. [4] extract contexts and answers for the questions from online forums; Shi et al. [15] analyze the user grouping behaviors in online forums; Cong et al. [3] detect the question-answer pairs in the threaded discussions.

Different from the traditional document repository, on-line forum discussions have their unique internal replying structures, which are crucial for correctly understanding the discussion contents. Xu et al. [20] illustrate as much as 75% of the links in a typical forum page points to noise pages like user profiles, login pages etc., and hence link-based algorithms like PageRank and HITS cannot be used effectively. Besides, they also demonstrate content-induced links are more helpful for ranking forum pages. Seo et al. [13] state that using thread structures to estimate language models improves forum search performance. In [8], Jurczyk et al. show that the linkage pattern can be used to help identify the quality of answers in the online community en-vironment.

However, the replying structure of a threaded discussion is not always available, and some work has been proposed to reconstruct the conversional structures. Shen et al. [14] treat the reconstruction task as a content-based clustering problem and propose to use the clusters of posts as the sub-threads in a threaded discussion. However, this method does not identify the explicit parent-child relations within each cluster. Wang et al. [18] use a graph-based connectivity matrix, in which both content similarity and temporal in-formation are utilized, to recover thread structure. But the parameters are manually tuned in the proposed method. In [11], Lin et al. incorporate the temporal relationship into semantic modeling of threaded discussions based on the sta-tistical topic model. Unfortunately, their model cannot di-rectly predict the replying relationship, and they depend on an additional distance measure in the projected topic space to find the  X  X losest X  replying post pairs. In [13], Seo et al. converted this structure prediction task into a ranking prob-lem: each post is considered as a query, and parent candidate posts are considered as the documents to be retrieved. Mul-tiple features are utilized to learn a Ranking SVM model. But because Ranking SVM can only take the post X  X  pairwise interaction into consideration, it fails to directly model the whole discussion structure.

In this paper, various kind of features, from content sim-ilarity and posting time gap to user interactions, are intro-duced in the proposed probabilistic model to capture both the long-range and short-range dependencies within a dis-cussion thread. In addition, to judge the quality of the struc-ture prediction, we design a set of new evaluation metrics, in which we emphasize more about the structure preservability in the good predicted result.
Before introducing the proposed method in detail, we would first define some concepts that would be referred to in later discussions.

Formally, let T = { X 0 ; X 1 ; : : : ; X N } be a set of thread dis-cussions from a particular online forum site; each thread X consists of m individual posts { p 0 ; p 1 ; : : : ; p m  X  in the chronological order. In each post p i of thread X n th ere are 5 attributes: 1) post ID, ranging from [0, m); 2) post content c i , modeled as a bag of words; 3) author name u , the displayed author name for the post; 4) author ID a i a unique user identifier in the whole forum; and 5) posting time t i , the system time stamp when the post is created. In this work, we only assume the above five general post attributes, which should be found in most of the online dis-cussion forum sites; other site-specific attributes, such as post quotations and tree views, are not considered. Definition (Root Post) Root post is the first post in one discussion thread according to its posting time. It initiates the topic of discussion in this thread.
 Definition (Previous Post) Previous post of p i is the post posted closest in prior to p i in the chronological order. Definition (Parent Post) Post p i is said to be the par-ent post of p j if and only if p j is posted later than p contains an immediate follow-up discussion of p i . In the threaded-view forum site, this information is indicated by  X  X n response to X  or a reply tree. Such pairwise relationship is called  X  X eplying to X  relation, which is asymmetric. Definition (Thread Structure) The thread structure de-fined by the  X  X eplying to X  relation between posts is strictly a tree: posts in X n , except for the root post, can only and must have one parent post in X n . Each path in this tree forms a self-consistent sub-threaded discussion. Such a structure is intuitively demonstrated in Figure 1.
Based on the above definitions, the task of reply rela-tion reconstruction is equivalent to assigning every post p in thread X n a parent post p y i , which makes every sub-threaded discussion consistent and coherent. We call such parent labeling sequence as Y n for each X n .
Although the  X  X eplying to X  relation between two posts is defined according to their semantic relations in Section 3, the interaction patterns among the replying posts are far beyond what can be observed merely from the post content. Because most of the posts are very short, content similar-ity is inadequate to infer the correct replying to relation-ship. Besides, the involvement of user interactions makes it even harder to predict the replying relation. For example, users are more likely to reply to the post which has replied to him/her before; root post tends to receive more replies than other posts. Another important character of threaded discussion is that the post contents are highly context de-pendent: the topic focus is evolving during the discussion, so that in order to understand the intension of a certain post it is often necessary to find the conversational informa-tion hidden in the related posts. Nevertheless, due to the intra-dependencies among the posts in one thread, such cor-respondence is not known until we recover the whole replying structure. Thus the difficulty arises from the intuition that every local prediction depends on all the other posts X  reply-to assignment in the same thread. On the other hand, the internal dependencies within the replying structure provide regularization over each local prediction, which helps the overall prediction in turn: when we have high confidence on recovering some parts of the thread, the prediction on the less certain parts will become easier with the assumption that the local predictions can be propagated. Intuitively, the assumption holds if we can properly identify the dependency among the predictions. Besides, as there are various types of factors interacting and suggesting different level of depen-dency, including users, post contents and time, it is hard to determine their relative importance without learning from the data. Therefore, we need an appropriate mechanism to encode the structural dependency among such factors in a systematic way. To achieve this goal, we cast the thread replying relation reconstruction task as a structure learning problem, and propose a thread Conditional Random Field (threadCRF) model to solve it.
Our main focus in this paper is to exploit the dependencies among the posts, which can facilitate us to better identify the correct structures from the threaded discussion. Proba-bilistic graphical models provide a systematic methodology to describe and manipulate the short-range and long-range dependencies in both observed data and unknown predic-tions. By properly defining the features (or potential func-tions) in the joint/conditional probability, arbitrary depen-dencies could be encoded and their relative importance can be captured by the feature X  X  weight.

In our reconstruction task, we are more interested in mod-eling the conditional probability of the replying relationship Y n given the posts { p 0 ; p 1 ; : : : ; p m  X  1 } in thread X p ( Y n | X n ), than their joint probability p ( X n ; Y Conditional Random Fields (CRFs) [10], which is a family of probabilistic models for sequence segmentation and labeling problems, is the nature choice for solving this problem. By directly modeling the conditional probability, we can flexibly introduce any meaningful features, particularly dependent features of the observed sequence without having to model the distribution of those dependencies.

Following the basic concepts in traditional CRFs, we pro-pose a threadCRF model to capture the dependency among the posts within one thread based on various kinds of fea-tures describing the interactions in both posts and authors, and estimate the weights for the designed features in a su-pervised manner.

In threadCRF, we define the conditional distribution over replying relationship given post sequence X n as: where { f k ( Y n ; X n ) } K k =1 is a set of features defined on the given thread X n and its parent labeling sequence Y n ; { are the weights for the corresponding features.

Thus, given a model  X  = { k } K k =1 , the replying relation reconstruction task in our threadCRF could be formulated as a Maximum a Posteriori (MAP) inference problem: for each given thread X , we aim to find the optimal replying structure Y  X  , such that, where Y is the set of all the possible replying structures for the given thread X.

Within such a framework, the key challenge is to define a proper set of features by which the dependencies among the posts could be explicitly captured. In this paper, we design ) ) ) ,y ) by dotted line; edge features are represented by solid rectangles. features to model both posts X  replying patterns and users X  interactions in the threaded discussion environment.
We define two kinds of features, each of which can be formalized by some particular feature functions. The first kind of features depicts the local potential of replying rela-tions, i.e., how likely post p i replies to p j if they are close in posting time. Such features can be defined by a func-tion on p i and its own parent assignment p y i , and outputs a certain confidence measure of this replying pair based on the observed attributes from p i and p y i . The second kind of features captures the long-range dependency among the predictions, e.g., users prefer to reply to the post which has replied to him/her before. Such kind of feature functions for posts p i and p j not only depends on the observed attributes, but also on the unknown parent assignments for both p i and p , since the parent prediction on one of these two posts would affect the parent assignment for the other. Using the language of probabilistic graphical models [9], we treat each post parent assignment as a random variable, and depict the dependency among the features by a factor graph, where the edges are connected by the defined features (as shown in Fig-ure 2). In a factor graph, any complicated dependency can be decomposed into factor functions defined on edges. We can then propagate local predictions along the edges within the graph to find the global optimal prediction.

Previous methods [13] mostly operate on node features, and therefore they are performing isolated predictions for each post. The edge features proposed in this paper are new and can capture more complex dependency than those handled by the node features. Altogether, we propose six node features and seven edge features for our threadCRF. The proposed threadCRF incorporates both kind of features and learns the weights from the given training corpus.
Node features only depend on the observed attributes in post p i and p j , i.e., solely from the observed attributes of p and p j , to determine how likely p i is replying to p j .
Content Similarity Sim( y i ) : the content similarity be-tween post p i and p y i . One post tends to reply to another if they are talking about similar topics. We use the standard TF-IDF weighted cosine similarity [12], where IDF is cal-culated based on the number of posts containing the word in the given thread.

First Post First( y i ) : a binary feature to test if post p is replying to the root post. In the threaded discussions, root post initiates the topic for the followers to discuss; as a result, the following posts tend to reply to the root post.
Last Post Last( y i ) : a binary feature to test whether post p is replying to its previous post. Since in the conversational discussions, authors exchange their ideas in turn. One post in the dialog tends to directly reply to its previous post.
Author Reference AuthorRef( y i ) : a binary feature to test whether post p y i  X  X  author is mentioned in the content of post p i . In the threaded discussions, especially in the flat-view forum site, to clearly point to the receiver, some posts would directly mention the name of the author of the post he/she is now replying to.

Time Recency Recency( y i ) : the time proximity be-tween posting time of post p i and p y i . A small time differ-ence between two posts could be an evidence of the replying relationship. We use the time difference between post p i the root post to normalize the time difference between the root post and p y i . We should note the feature Recency( y is different from Last( y i ) , since Recency( y i ) is more sensi-tive to the actual time gap. When Recency( y i ) approaches 0, it indicates there is a long period of time when no one has followed the previous discussion; as a result, the chance th at p i still replies to p j would get lower though p j previous post.

Reply to Oneself Self( y i ) : a binary feature to test if post p i and p y i are posted by the same author. Since the dis-cussion forum is a place to share and exchange opinions, we assume one author should seldom reply to his/her own post. We should admit that there exist such situations, where the authors want to clarify or expand his/her previous claim. But we believe those cases happen with an arguably small probability and thus assign negative credit for this feature.
Edge features are defined over two parent assignments y i and y j (i &gt; j) together with the observed attributes in p p , i.e., ( X; Y ), to emphasize the dependencies between y and y j . We can either boost or penalize their interactions to reflect our assumptions on the replying structure.
Repeat Reply Repeat( y i ; y j ) : A binary feature to pe-nalize the replying pattern where p i and p j reply to the same post, when p i and p j are written by the same author. This feature is designed to capture the intuition that users seldom reply to their own posts.

Jumping Reply Jump( y i ; y j ) : A binary feature to pe-nalize the replying pattern where p i replies to an earlier post p k by p j  X  X  author a j rather than its closest preceding post p . The intuition behind this feature is that in a conversa-tional discussion, once people get replies from a recent post, he/she is unlikely to jump to an earlier post by the same author to start another conversation.

Author Response AuthorRes( y i ; y j ) : A binary feature to reward the replying from p i to p j , if p j  X  X  author a replied to p i  X  X  author a i before. As the nature of threaded discussion, people discuss with each other in turn and make the conversation evolving. To track this reply/response pat-tern, we check when p i replies to the author of p j , whether post p j has replied to the author of p i before.
Author Preference AuthorPref( y i ; y j ) : a binary fea-ture to reward the pattern that post p i and p j reply to the same author when p i and p j are from one same author. The intention of this feature is to capture the preference of re-plying pattern between friends against other unknown users, who have not replied to each other before.

Content Propagation ConProp( y i ; y j ) : cosine simi-larity between post p i and p j  X  X  parent post p y j , if p plying to p j . As we mentioned before, post contents are usually short and highly context dependent. In some cases, people have to borrow some content from one post X  X  parent or even grandparents to understand its topic. To simulate this process, we assume that when p i is replying to p j , its content should be somehow similar to p j  X  X  parent X  X  content; in other words, we propagate information from p j  X  X  parent to identify the possibility of p i is replying to p j .
Parent Propagation ParentProp( y i ; y j ) : cosine sim-ilarity between post p i and p j , if they are replying to the same post. Similarity is a symmetric measurement, while replying relation is asymmetric. When two posts are similar to each other, they might be talking about similar topics in a replying relationship, or replying to the same post dis-cussing the parallel aspects. The former is encoded in fea-ture Sim( y i ) , and ParentProp( y i ; y j ) is designed to catch the latter possibility.

Conversation Recency ConRecency( y i ; y j ) : similar to Recency( y i ) by assuming a later post has a higher chance to be replied to. But in this feature, we calculate the normal-ized time gap between three consecutive posts in a conversa-tion: if p i replies to p j while p j replies to p k , the time gap be-tween p j and p k normalized by the time gap between p i and p k is used to measure the recency between y i and y j in the conversation. The intuition is that Recency( y i ) only con-siders the time gap in the whole thread (normalized by par-ent p i  X  X  posting time to the root), while ConRecency( y is designed to capture the recency in each possible sub-thread discussion. In other words, a newer post tends to join a recently hot discussion in the whole thread. Table 1: Features for thread reconstruction task T ype F eature No de Ed ge
T able 1 summarizes the definitions of these 13 different features, and Figure 2 gives an intuitive illustration of the effects introduced by those features.

The proposed edge features introduce many large-size cliques (in some extreme case the whole thread forms a fully con-nected graph), which make threadCRF more complex than the commonly used linear chain CRFs[10], skip-chain [16] and even 2D [22] CRFs. Exact inference is intractable in this situation, and we appeal to approximation methods to per-form the Maximum a Posteriori inference and model learn-ing.
We perform approximate Maximum a Posteriori (MAP) inference for Eq(2) based on a schedule for loopy belief prop-agation named Tree Reparameterizaton (TRP) [17], which breaks the loops in the induced graph into its spanning trees, and perform exact inference on such generated trees. TRP typically provides quick convergence and more accurate ap-proximation results.

To learn the relative importance of the proposed features, we estimate the weights for each feature in a supervised man-ner. Given a training set of threads T = { X 1 ; X 2 ; : : : ; X with ground-truth replying relationship R = { Y 1 ; Y 2 ; : : : ; Y we need to estimate the optimal model setting  X  = { k } K wh ich maximizes the conditional likelihood defined in Eq(1) over the training set.
 The log-likelihood function could be represented as: where F ( Y n ; X n ) are the accumulated feature values defined in Table 1, and Z ( X n ) =
To avoid overfitting, we penalize the likelihood with a spherical Gaussian weight prior  X  N (0 ; 2 ). By taking the derivative of this object function, we get:  X  L =
The first part in Eq(4) is the accumulated empirical fea-ture values in the training set, the second part is the expec-tation of the features X  occurrences in the given training data, which could be efficiently calculated by TRP inferencer, and the last part is the regularization term from the Gaussian prior. This derivative is easy to understand: the maximum likelihood of the training data is reached when the empirical average of the global feature vector equals to its model ex-pectation. L-BFGS algorithm is employed to optimize the object function Eq(3) by the gradient.
We collect 51,716 threads from three different online dis-cussion forums with ground-truth replying relations: Apple Discussion ( http://discussions.apple.com ), Google Earth Community ( http://bbs.keyhole.com ) and CNET ( http: //forums.cnet.com ) [5], for evaluation. Apple Discussion and CNET are two computer support forums, where people seek helps for the technical problems they encountered in hardware and software; while Google Earth Community is an entertainment focused forum, where people share inter-esting findings in the Google Earth software. Apple Discus-sion and Google Earth threads collections are available at http://timan.cs.uiuc.edu/downloads.html .

We perform simple preprocessing on these three collec-tions to refine the evaluation corpus: 1) remove the threads with less than 3 posts, because we don X  X  need to reconstruct the structure for such threads; 2) remove the threads with inconsistent posting time, i.e., earlier post replies to a later one; 3) remove a standard list of stop words [1] and punctua-tion from each post X  X  content attribute. The basic statistics of the evaluation corpus are shown in Table 2.

F rom Table 2, we can get a brief sense of the differences between these three collections: 1) in Apple Discussion and CNET, people use more words to explain/answer the prob-lems in their posts, while the posts in Google Earth are much shorter, where people only post links to the attrac-tions they found in Google Earth with very few word de-scriptions; 2) users discuss more actively in Google Earth (1.25 thread/user) than other forums (0.66 thread/user in Apple Discussion and 1.10 thread/user in CNET).
Previous studies on replying relationship reconstruction only employ accuracy on the predicted edges (Acc edge ) as the evaluation criterion [11, 19]. However, we argue that such measurement is not sufficient to judge the goodness of the predicted structure: it only evaluates the point-wise predictions on each node, but loses sight on the predicted structure as a whole. We can use the example thread shown in Figure 1 to illustrate the defect in edge accuracy mea-surement. Fi gure 3: Example reconstruction results for thread shown in Figure 1
Figure 3 lists three different reconstruction results with the ground-truth structure. Both (b) and (d) achieved the same Acc edge of 0.75 (3 out of 4 predictions are correct), which is better than (c) X  X  Acc edge of 0.5. However, from the point of structural preservation, result (b) (a chain) is quite different from the original structure (a tree), and result (d) is arguably better from this perspective. Besides, when we take the actual discussion content into consideration, struc-ture (b) misleads us: Frank Miller2 in post 3 is actually replying to dessto in post 0 to give a possible solution in his/her first question, rather than to explain dessto  X  X  fur-ther concern in post 2. Result (b) mistakenly connects post 3 to post 2, which changes the whole context. Therefore, to correctly maintain the discussion context in the scenario of conversational replying reconstruction, a mistake at higher level of the replying tree should incur more serious penalty than the one at a lower level.

To address this problem and more appropriately evaluate the quality of the predicted structure with respect to the ground-truth, we define a set of novel metrics. Formally, suppose Y = { y 0 ; : : : ; y m  X  1 } is the ground-truth structure for thread X, and Y  X  = { y  X  0 ; : : : ; y  X  m  X  1 } is the prediction. To evaluate how well the discussion context is preserved for each node in the tree, we define path accuracy Acc path as the proportion of correct path from each node to root: where path Y ( i ) and path Y  X  ( i ) are the set of nodes lying in the path from node i to the root node in ground truth Y a nd prediction Y  X  respectively. For example, path Y  X  (4) in Figure 3.(b) is { 4,3,2,1,0 } .

Acc path measures if we can read from root to a particular post without missing a post, nor meeting an irrelevant one from other branch. To relax the strict whole path matching requirement, we compute the overlap between the path from one node to the root in ground-truth versus in the predicted path, and define path precision P path and recall R path as: where || S || is the size of set S.

Above path-based metrics emphasize the correct predic-tion of the nodes with more decedents at higher level of a tree: since we are counting path from every node to the root, higher level nodes will be involved multiple times by its decedents, in that case a mistake at higher levels would incur a more serious penalty.

Another important aspect that should be evaluated is how well a node X  X  local structure is preserved. When one node is a branching node, it X  X  crucial to recover all its child nodes to get the correct track of its sub-trees. For the same sake of allowing inexact match and distinguishing the missing case versus the incorrect-inserting case, we define node precision P node and recall R node as the overlap between the child node set of each node in ground-truth and prediction: where child Y ( i ) and child Y  X  ( i ) are node i  X  X  child node set in the corresponding structures. The summation is taken from 0 to m-2 since the last post does not have a child node.
As a commonly used compromise between precision and recall, we also define F-value for the proposed P path , R and P node , R node as the harmonic mean of these two metrics. Now, we can take the example in Figure 3 to justify the proposed metrics: result (b) achieves Acc path of 0.5, F1 of 0.66 and F1 node of 0.75, while result (d) receives Acc of 0.75, F1 path of 0.75 and F1 node of 0.75, which is much closer to our intuitive expectation.

When we are evaluating in the corpus level, we could av-erage all the above metrics in the thread level, known as Micro average, or in the whole corpus level, known as Macro average. In the following experiments, we employ both Mi-cro/Marco performance as the evaluation metrics.
To compare the effectiveness of the proposed method, we employ several baseline methods: 1) reply to Root Post (FIRST); 2) reply to Previous Post (LAST); 3) ranking by similarity (SIM), in which we use the same cosine similar-ity measure as sim( y i ) feature in threadCRF to select the precede most similar post as the parent post. Beside the unsupervised methods, we also use Seo et al. X  X  supervised Ranking SVM [13] as the baseline, where we use all our node features to train the Ranking SVM model, since Rank-ing SVM cannot deal with edge features. We use the default parameter setting as in Ranking SVM [7].

Because Apple Discussion and Google Earth are two types of online discussion sites, it would be interesting to compare the methods X  performance on these two different domains. We use 75% threads in each collection as the training set and the rest threads for the testing purpose. The reported performance is calculated based on the evaluation criteria proposed in Section 5.2.

F rom the results in Table 3 and Table 4, we can find that the proposed threadCRF model outperforms both the unsu-pervised and supervised baseline methods in these two dif-ferent collections. The reason heuristic rule-based method FIRST has perfect P path is that all the posts are directly connected to the root post and no other posts can lie in be-tween. The similar reasoning works for LAST X  X  R path perfor-mance. However, these two simple heuristics could not cap-ture the real replying structure in the data; as a result, their F1 performance is worse than the supervised learning meth-ods. Besides, we can also find that similarity alone ( SIM ) is far from enough to predict the correct replying structures. The proposed threadCRF works better than Ranking SVM under most of performance metrics: threadCRF improves over 17.65% in Micro Acc path and 9.44% in Marco Acc path against Ranking SVM, which two are the strictest criteria to judge the goodness of the predicted structures. In addi-tion, we perform the paired randomization test with p &lt; 0.05, which shows the statistical significance of improvements over the Ranking SVM. The comparison results with both the unsupervised heuristic rules and supervised Ranking SVM confirm the benefits of modeling the dependency among the posts in one thread, and the proposed threadCRF model correctly exploits the interactions in both posts and users by the introduced edge features.

When the thread size gets larger, i.e., more than 10 posts, it becomes harder for human to digest and understand the discussion content, and therefore, it is more necessary to per-form automatically replying relation reconstruction in this situation. From another perspective, the evaluation per-formance might be biased by too many short threads, the comparison over long threads will better distinguish differ-ent methods X  capability. In this experiment, we compare threadCRF with Ranking SVM on the long threads (more than 10 posts) to further investigate the effectiveness of the proposed method. We apply the models trained in previous experiment to analyze their performance only on the testing threads with more than 10 posts. Micro average is used over the selected performance metrics.
 Table 5: Prediction performance on long threads
In this testing set, we have 167 threads from Apple Discus-sion and 226 from Google Earth. Both methods X  path-based performance drops compared with the results in Table 3 and Table 4, which is understandable, because the structures are getting more complex. Compared with Ranking SVM X  X  per-formance, we can discover that threadCRF X  X  performance improves more than that in all the testing cases: 25.71% and 23.67% in terms of Micro Acc path in Apple Discussion and Google Earth accordingly.
Although the proposed threadCRF works in a supervised manner, we want to test its adaptability when we are lack of such labeled training data. There are two scenarios in real situation: 1) we have a small set of labeled thread set from the target forum; 2) we do not any labeled threads from the target forum, but some labeled threads from other forums. In this experiment, we want to investigate our method X  X  ca-pability when applied in both of these two situations.
First, to test the method X  X  dependency on the training data, we variate the training sets during the training phase. We select both Apple Discussion and Google Earth as the evaluation collection and use the same train/test separation as in Section 5.3. The volume of training set is gradually increased from empty to all the training threads. We select Ranking SVM model as the baseline method and compare their Micro Acc path and Micro F 1 node performance in this experiment. When we have no training samples, we manu-
F igure 4: Performance on different training size ally set all the features X  weights to be 1 for both threadCRF and Ranking SVM.

From Figure 4, we find with a small training set, 100 labeled threads, threadCRF achieves encouraging perfor-mance improvement against the default weight setting. As more training data is available, the performance converges for both methods. Another phenomenon we observe is that threadCRF X  X  performance vibrates over different training sets. The reason is two folds: 1) the employed TRP infer-encer in threadCRF is an approximation inference method, the estimated posterior probability varies as starting from different initial points; 2) the variances within the training set introduce noise into the learning phase, similar tendency could also be observed in Ranking SVM X  X  performance.
Second, we perform a cross domain train/test to validate the learning method X  X  generality. In this setting, we eval-uate on all the three collections: we randomly select 2,000 threads for training and 2,000 threads for testing from the three data sets accordingly to make the comparison compa-rable. Besides evaluating the performance on the testing set from the same collections, we also apply the learned model on the other two testing sets from the different domains. Micro Acc path is employed as the evaluation metric and we compare threadCRF with Ranking SVM in this experiment. In Table 6 and Table 7, columns indicate the testing set and rows indicate the training set.
 Table 6: Cross domain evaluation on Ranking SVM T able 7: Cross domain evaluation on threadCRFs
F rom the comparison results in Table 6 and Table 7, we find that threadCRF generalizes better than Ranking SVM in all the three data sets. Because Apple Discussion and CN ET are both computer technical forums, they share some properties in common. Therefore, the threadCRF model trained on Apple Discussion achieves promising performance on CNET, and vice versa. Instead, Google Earth is an enter-tainment focused forum, where the topics and users X  interac-tion patterns are quite different from the other two forums. Ranking SVM trained on Google Earth doesn X  X  perform as well as the threadCRF on the other two forums: 0.4880 of threadCRF v.s. 0.4103 of Ranking SVM on Apple Dis-cussion and 0.4313 of threadCRF v.s. 0.3724 of Ranking SVM X  X  on CNET. Besides, all the threadCRF X  X  off-diagonal entries are better than Ranking SVM X  X , which indicates the proposed threadCRF possesses better generality capability than Ranking SVM.

It would be interesting to investigate the weights learned by threadCRF in different collections, from where we can discover interesting patterns in those domains. We normal-ize the weights from different domains by their correspond-ing largest weights to make them comparable, and illustrate part of the weights in Table 8.
 Table 8: Normalized weights learned by threadCRF
F rom the learned feature weights, we can find Sim( y i ) is the most prominent feature in Apple Discussion. Because the topics in Apple Discussion are relatively more focused, mainly on Apple X  X  products, replying post pairs tend to over-lap more in their contents. In CNET, Self( y i ) (reply to oneself), Repeat( y i ; y j ) (repeatedly reply to the same post) and AuthorRes( y i ; y j ) (author response) are all relatively large. This indicates the conversational pattern in CNET is more significant than the other two forums. Another in-teresting observation is that the weight of Repeat( y i ; y in Google is negative, which means users will reply to the same post they have replied to before. We manually checked the discussions in Google Earth Community and discovered that people repeatedly reply to the same post to provide more links and background of the findings in that post.
The replying relationship reconstructed by the proposed threadCRF model can be potentially useful for many further applications. Here we present two sample applications in forum search and community question answering to demon-strate the benefit of reconstructing the replying structure.
Seo et al. [13] demonstrate that using thread structures can lead to significant performance improvement in forum search compared to standard IR methods. In detail, they estimate a post language model by smoothing with the dia-logue structure within the ground-truth replying structures.
Following the same line of retrieval model proposed in [13], we compare the retrieval performance on 1) post lan-guage model estimated on each individual post; 2) smooth-ing the post language model by the dialogue structure recon-structed by threadCRF and Ranking SVM; 3) smoothing by the whole thread, in order to justify whether the predicted structures could help the retrieval task.

In the CNET data set, Duan et al. manually selected 30 result documents for 30 different queries under the topic of  X  X omputer &amp; Internet X , and annotated them as X  X elevant X  X r  X  X rrelevant X  [5]. 5-fold cross validation is employed to com-pare the retrieval performance, where the smoothing param-eters in all the language models are exhaustively searched to maximize the MAP metric. MAP, P@1 and P@10 are em-ployed as the evaluation criteria.
 Table 9: Forum Search Performance on CNET
W e can observe that the retrieval model estimated on the reconstructed thread structure generally improves the rank-ing performance over non-structure smoothing. The reason is that if we directly use all the posts in the thread to smooth the language model without carefully analyzing the seman-tic relationship between the posts, noise might be introduced into the ranking model and thus affect the retrieval perfor-mance, i.e., a degenerated P@1 for Post+Thread.
Online forum discussion is a valuable repository for ques-tion answering mining tasks. Previous studies on question answering mainly depend on content analysis techniques to find the most probable question-answer pairs [4, 3].
To confirm that the constructed conversational structure can help to detect the answers in the threaded discussion, we choose Apple Discussion as the evaluation collection, in which answer posts are explicitly labeled as  X  X olved X  or  X  X elpful X  by the users who raised the question in the discus-sion. We use the same train/test split as in Section 5.3.
The first step in question-answer pair extraction task is to identify the questions from the threaded discussion. There are many available techniques to perform such task [3, 6]. To get a clear sense of how the reconstructed structure can help us retrieve the answer posts in a threaded discussion, in this experiment, we assume the question posts are already iden-tified before hand. To achieve it, we filter out the threads without the  X  X olved X  or  X  X elpful X  annotation in the testing set and assume the first post in the left 246 threads are the question posts.

In those question-answering oriented threads, we propose the following criterion to rank all the rest posts in the same thread by: s ( p i ) = Intuitively, we assume a good answer should be the post which the question raiser has replied to, e.g., thanks for the replier or asks for further details, and receives many comments from others.

From Table 10, we can find that the naive content-based method, i.e., rank by similarity, doesn X  X  work in this threaded discussion question answering environment, while replying structure based method achieves promising performance. Be-cause most posts in the threads are short, even though they are answering the questions in the previous posts, the con-tent between question post and answer post overlaps little. The structure-based extraction method captures a strong in-dication for a good answer post from the perspective of user interactions, so that the precision is relatively high.
In this paper, we proposed a probabilistic graphical model based method, threadCRF, to solve the problem of replying relationship reconstruction in the online threaded discus-sion forums. By introducing various kind of features, the proposed threadCRF well captures both the long-range and short-range dependency among the posts and better pre-dicts the replying relationship. To judge the quality of the predicted structures, we design a set of novel evaluation met-rics. Experiment results on three different forum collections confirm the effectiveness of the proposed method, and the improvement over the baseline methods demonstrates the benefit of modeling the user interaction and long-range de-pendency in the threaded discussions.

Because the features employed in our method are not strictly limited to forum discussions, and they could be easily adopted to other domains, e.g., replying relationship mining in micro-weblogs, such as Twitter, and user wall posts and comments in social websites, such as Facebook. In addition, other advanced content analysis techniques, e.g., entity re-conization/resolution and syntax parsing structure match-ing, can also be incorporated into our model as features to further enhance the performance.
This material is based upon work supported by the Na-tional Science Foundation under Grant Numbers IIS-0713581, IIS-0905215, CNS 1028381, and by an AFOSR MURI Grant FA9550-08-1-0265, and the U.S. Army Research Laboratory under Cooperative Agreement No. W911NF-09-2-0053 (NS-CTA). We thank Huizhong Duan for providing the CNet data set and retrieval codes. [1] Onix text retrieval toolkit stopword list. http: [2] S. Bhatia and P. Mitra. Adopting Inference Networks [3] G. Cong, L. Wang, C. Lin, Y. Song, and Y. Sun. [4] S. Ding, G. Cong, C. Lin, and X. Zhu. Using [5] H. Duan and C. Zhai. Exploiting Thread Structure to [6] L. Hong and B. Davison. A classification-based [7] T. Joachims. Optimizing search engines using [8] P. Jurczyk and E. Agichtein. Discovering authorities in [9] D. Koller and N. Friedman. Probabilistic graphical [10] J. Lafferty, A. McCallum, and F. Pereira. Conditional [11] C. Lin, J. Yang, R. Cai, X. Wang, and W. Wang. [12] G. Salton and C. Buckley. Term-weighting approaches [13] J. Seo, W. Croft, and D. Smith. Online community [14] D. Shen, Q. Yang, J. Sun, and Z. Chen. Thread [15] X. Shi, J. Zhu, R. Cai, and L. Zhang. User grouping [16] C. Sutton and A. McCallum. Collective segmentation [17] M. Wainwright, T. Jaakkola, and A. Willsky. MAP [18] Y. Wang, M. Joshi, W. Cohen, and C. Ros  X e.
 [19] Y. Wang and C. Ros  X e. Making conversational [20] G. Xu and W. Ma. Building implicit links from [21] J. Zhang, M. Ackerman, and L. Adamic. Expertise [22] J. Zhu, Z. Nie, J. Wen, B. Zhang, and W. Ma. 2d
