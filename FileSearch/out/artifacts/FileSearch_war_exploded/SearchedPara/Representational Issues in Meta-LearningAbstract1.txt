 Alexandros Kalousis kalousis@cui.unige.ch Melanie Hilario hilario@cui.unige.ch Classification algorithm selection can be seen as one of the  X  X oly grails X  of the machine learning field, it is com-mon knowledge that there is no algorithm uniformly superior over all possible problems.
 The most common and widely accepted methods for performing algorithm selection require substantial ex-pertise on machine learning and involve systematic ex-perimentation and evaluation. In recent years there have been some efforts to automate the selection and lift the experimental burden by relying on some form of learning, most often called meta-learning. The whole idea dates back to the work of (Rendell et al., 1987), it became more concrete within the European project STATLOG, (Michie et al., 1994), to finally find its full expression in another European project, METAL, which produced a considerable amount of publica-tions (Brazdil et al., 2003; Pfahringer et al., 2000). The main concept is very simple and views classifica-tion algorithm selection as just another learning prob-lem. The training examples consist of descriptions of complete classification datasets and the target; the lat-ter is usually defined from a performance based prefer-ence order over the set of available algorithms for the given dataset. While there has been significant work on the definition of preference order over learning al-gorithms and the construction of the target, little at-tention has been given to representational issues that arise when one tries to describe datasets.
 The heart of the representational issues can be traced to the one-to-many relationships that appear in the descriptions of classification datasets. A training in-stance in a meta-learning setup is a dataset descrip-tion, which comprises characterizations of each of the attributes that constitute the dataset 1 . In the propo-sitional framework it is not possible to retain the complete information about the individual attributes, since this would clearly result in meta-instances of vari-able length, depending on the number of the attributes of a given dataset.
 The bulk of the work in meta-learning naively ad-dressed the representational problem by resorting to descriptions of properties that are computed for each one of the attributes of a dataset using averages or at most the min and max statistics. Two exceptions are, the work of (Todorovski &amp; Dzeroski, 1999) where the problem was treated as a multi-relational problem, as it actually is, and handled via first-order learners; and the work of (Kalousis &amp; Theoharis, 1999) where the distributions of the properties are described us-ing histograms. The former suffers from a well known problem of first-order learning; the high number of de-grees of freedom of the search space, which is deter-mined by the number of properties used to describe datasets and the actual number of attributes of the dataset. This results in an extremely time consuming learning process and at the same time increases the chances of overfitting by accidental discovery of in-valid patterns, especially when the number of training instances is small as is typical in applications of meta-learning. The latter attacks the high dimensionality of the search space adequately, as its dimensions are only determined by the examined properties. However the semantic power of the distribution based representa-tion is not fully exploited in the framework of classical propositional learners.
 In a previous paper, (Hilario &amp; Kalousis, 2001), we described the case representation of a relational case-based system which served as a repository of classifi-cation experiments. Thorough records were kept on every aspect of these experiments. The goal was to use the case-base as a predictive tool, that would pro-vide support in classification algorithm selection, but also as an explanatory tool determining the areas of expertise of classification algorithms.
 Although case-based systems do not induce first-order theories they can make use of multi-relational rep-resentations addressing thus the representational re-quirements of the meta-learning problem. The collec-tive treatment of dataset attributes and their proper-ties in the similarity measures can make case based systems less susceptible to overfitting phenomena. In a first order meta-learning scenario we would be look-ing for rules of the form:  X  attribute whose property I satisfies condition X . As the number of attributes increases chances that we would, accidentally, discover invalid rules of that form which hold on the training set also increase. The appropriate selection of similarity measures, which would be defined over all attributes and their properties in contrast to the existential ap-proach, can alleviate that problem.
 In this paper we continue the work started in (Hi-lario &amp; Kalousis, 2001). We equip the relational case-base with precise similarity measures, that can cope with the multi-relational structure of the information describing the classification datasets, Section 3, and undertake a systematic evaluation of the predictive power of the relational case-based classification, Sec-tion 5. The evaluation takes place within a specific meta-learning framework described in Section 4. The next section gives a brief overview of the cases. In Figure 1 we give the overall description of a case in the relational case-based system. We now give a brief description of the main components of a case.  X  Mo dTool : A general class that provides the pro- X  EvalStrategy : A general class that defines the  X  Dataset : The class of all datasets that have been  X  Variable : The class of variables or attributes of a A Modeling Tool can be applied and evaluated on a Dataset using a specific Evaluation Strategy. The com-plete results of the evaluation are stored in a ModPro-cess object, which links together the Dataset, ModTool and EvalStrategy objects. A case in the case base is a learning episode, i.e. the application and evaluation of a learning algorithm to a specific dataset. We have defined the representation of cases in what is essentially a relational case base. In order to fully exploit that representation we have to define similarity measures that are able to cope with it.
 The most important component of the relational case base is the Dataset object. Each instance of the Dataset class is associated with a set of instances of the Variable class. Any similarity measure defined over a Dataset object should also take into account the elements of the Variable class associated with that Dataset object. Before addressing the problem of sim-ilarity definition between sets of variables we have to define a similarity measure among the individual ele-ments of the sets, i.e. among the variables.
 A dataset I includes a set of variables S I . Each vari-able v i  X  S I is described by a vector u of n dimensions describing various properties of the variable. The sim-ilarity between any two variables v i ,v j , whether from the same or from different datasets, will be given by : The given similarity measure is based on the normal-ized Manhattan distance between the two vectors that describe the corresponding variables. Any measure of similarity between two sets of variables will make use of the similarity measure between individual variables defined by Formula 1.
 The definition of similarity measures between sets of objects, in our case the sets of variables that consti-tute the datasets, is not trivial. There is no unique-best similarity measure and the appropriate selection always depends on the semantics of the problem at hand. For example in the scope of similarity-based multi-relational learning (Kirsten et al., 2001), the similarity between two sets is defined as the sum of the maximum similarities of the elements of the set with the lower cardinality with the elements of the set with the greater cardinality, normalized by the cardi-nality of the greater set. The normalization with the cardinality of the larger set results in very low sim-ilarities when the two sets have very different cardi-nalities. More formally for two sets S I and S J with possible different cardinalities n i and n j , where the similarity between elements v i  X  S I ,v j  X  S J , is given by Formula 1, the similarity according to Kirsten et al. (2001) is: sim K ( S I ,S J ) = Very similar ideas are quite developed in clustering algorithms and more precisely for defining the similar-ities between sets in agglomerative hierarchical clus-tering (Duda et al., 2001). Exploiting the exper-tise developed there, we have chosen to implement and evaluate two different similarity measures based on measures commonly used by the clustering com-munity. The first one is based on the similarity used in the single linkage clustering algorithm, where the similarity between two sets is defined as the maximum similarity observed between all pairs of elements of the two sets, while the second one is based on the similar-ity used in the average linkage clustering algorithm, where the similarity between two sets is defined as the average similarity between all pairs of elements from the two sets. More formally we have:  X  Single Linkage Based Similarity  X  Average Linkage Based Similarity For all three measures similarity is determined af-ter the computation of all the pair-based similarities, sim ( v i ,v j ) . The difference comes from the way it is computed from them. In sim SL the similarity be-tween two sets is determined by the most similar el-ements of the two sets. As a result this method is more sensitive to outliers. On the other hand sim AL reduces the effect of outliers by averaging over all el-ement pairs. Sim K emphasizes the best matches be-tween the elements of the two sets, and penalizes the similarity when there is a large difference in cardinali-ties. Of course there are many possible refinements of the above similarity measures, some of which may be worthy of investigation in a more detailed study. For example, in computing the final similarity with sim AL one could remove similarity outliers, i.e. extremely high or low similarities that do not conform with the general similarity distribution. However, this should be done with caution because for the algorithm selec-tion application it might be exactly these cases that determine the relative performance of classification al-gorithms. In the general case the appropriate measure depends heavily on the semantics of the problem. Apart from these detailed descriptions of individual variables there are also higher level characteristics that are part of the description of a Dataset object. These characteristics do not make sense for all datasets. For example summary statistics of the properties of con-tinuous variables do not make sense on datasets that contain only discrete variables and vice versa. In or-der to be able to handle such cases the description of a dataset is divided into two groups of characteris-tics; the first one contains summary statistics for the discrete variables ( Disc. Properties object), while the second summary statistics for the continuous ( Cont. Properties object). Each group is treated as a single variable of the dataset object when computing the sim-ilarity of datasets. When the computation of the char-acteristics of one group does not make sense then the corresponding variable takes the value non-applicable, na . The problem is known in the CBR community as the heterogeneity problem. Aha et al. (2001) dealt with the problem in the automatic construction of tree-structured representations of case libraries in a similar way.
 Integrating the notion of non-applicability into the def-inition of similarity, the similarity of values A i ,A j of variable A , A  X  [ A min ,A max ]  X  na becomes: sim ( A i ,A j ) = 0, if A i = na  X  A j = na The similarity of two datasets A,B is given by: where N is the number of variables that constitute the description of a Dataset object. The variables A i ,B i are :  X  any of the high level characteristics that are part  X  the set of variables associated with the datasets  X  and the grouped descriptions of continuous and One of the goals of the system is to act as an assis-tant for algorithm selection. The analyst wishes to identify the most suitable classification algorithm for a new dataset based on past learning episodes and the description of the dataset. Given a new classi-fication task, the analyst uses a Data Characteriza-tion Tool (Lindner &amp; Studer, 1999) to extract dataset meta-attributes which are stored in a new instance of the Dataset class. This dataset is then posed as a query to the case-based system. Using the similarity mea-sures presented in section 3, the case-base returns the k most similar datasets. For each j of these k datasets there are j i associated cases; these are instances of the ModProcess class which describe past learning ex-periments performed on this dataset (i.e. past appli-cations of ModTool instances to the dataset using a specific EvalStrategy . The number of instances j i of the ModProcess object which are associated with each dataset j of the k most similar datasets depends on the number of the classification algorithms that have been evaluated on the j dataset and the evaluation strategies that have been used; it is not necessary to have complete results for every dataset registered in the case base, so j i can be different for different j . For each retrieved dataset the  X  X est X  classification al-gorithm is identified using the error rates recorded in the j i ModProcess instances associated with it. The algorithm that most frequently obtained the lowest er-ror on the k most similar datasets is recommended by the system for the new dataset. A more systematic approach for defining the  X  X est X  algorithm should be based on the notions of pairwise comparisons and sig-nificant wins between classification algorithms given in (Kalousis &amp; Theoharis, 1999). We have chosen to work with the more simplistic approach simply be-cause our main goal was the exploration of the repre-sentational and predictive power of the relational case-base system and it was more straightforward to use the simplistic scenario. The meta-learning problem is a typical classification problem where each instance corresponds to the de-scription of a dataset; the class label that we are try-ing to predict is simply the algorithm that achieves the lowest error on the specific dataset. We use a sim-ple 0/1 loss function to compute what we will call the Strict Error .
 Algorithm selection was performed amongst the ten following: c50boost, c50rules, c50tree, mlcib1 (1-nearest neighbor), mlcnb (naive bayes), ripper (rule inducer (Cohen, 1995)), ltree (multivariate decision tree (Gama &amp; Brazdil, 1999)), lindiscr (linear discrim-inants), clemMLP (multi-layer perceptron) and clem-RBFN (radial basis function network). Table 1 gives the distribution of the class labels; the majority class corresponds to c50boost which is the best performing algorithm in 35.92% of the 103 datasets used in the study. The default error of the meta-learning prob-lem  X  X hat of the default learner which simply predicts the majority class X  X s 64.08%. To be at all useful, the case-based system X  X  algorithm recommendation should exhibit an error rate lower than this default error. We used 10-fold stratified cross validation in order to estimate the classification errors of the algorithms. All algorithms were applied using their default parameter setting. Normally this should give rise to 10  X  103 cases in the case base, but some learning algorithms could not be successfully applied on all the datasets. In addition based on the results of the evaluation of the algorithms we determined the group of the best algorithms for each one of the 103 datasets using Mc-Nemar X  X  test of significance. This consists of learn-ing algorithms whose performance did not vary signif-icantly within the group but was significantly better than that of any algorithm outside the group. The sig-nificance level was set to 0.05. Table 2 gives the most frequent groups of significantly better performing al-gorithms. Each column corresponds to a given group whose members are indicated via The establishment of the significantly better group of algorithms will provide the basis of one more eval-uation scenario. Here the suggestion of the system will be considered successful if the recommended al-gorithm belongs to the set of best algorithms for the given dataset. This method reflects the idea that when dealing with a classification task, for which the main goal is low classification error, we will be satisfied if the algorithm that we select belongs to the group of the significantly better algorithms for the given dataset. We will call the error estimated via this method Loose Error . In this case the default Loose Error is the error that we get when we predict the algorithm that most often appears in the top groups. In the datasets ex-amined here this is c50boost, which in 52.43% of the datasets is part of the group of significantly better al-gorithms. This default strategy corresponds thus to an error of 100%-52.43%=47.57%.
 For evaluation we used leave-one-out cross validation. We experimented with three different values of k , k = 1 , 3 , 10 and the two different similarity measures defined over the set of variables, i.e. sim AL ,sim SL We also report results for sim K , and for a simple flat attribute-value version of the case-base 2 , ( AV ), where the information on the individual variables of each dataset was simply ignored, (remember though that the high level description of a dataset contains sum-mary statistics of the information on the individual variables mostly in the form of averages). The later is done in order to examine whether the synergy of the relational case-based representation and of the similar-ity measures can bring an improvement over the flat AV representation. We report the estimated error and the results of a Wilcoxon test on the significance of the difference of the estimated error with the default error. 5.1. Results In Table 3 we present the estimated Strict Error for all the experimental setups. Both methods that use the clustering based similarity measures perform consis-tently better than the AV version over all the values of k , though not always at a statistically significant level. The only two experimental setups in which a signifi-cant improvement over the default error is observed are for k = 3 ,sim AL ,sim SL . For sim AL that difference is statistically significant. Sim K exhibits an error which is worst than the error of AV . One explanation for the low performance of sim K could be the penalization of similarity for datasets pairs with very different number of attributes, in which case sim K becomes very small. In the algorithm selection problem it is probable that the most important factor in determining relative per-formance is the highest similarity observed between any pair of attributes independently of any differences in the number of attributes, a fact that is supported by the good performance of the sim SL which takes into account only the most similar pair of attributes. The results with respect to the Loose Error are fairly similar. Again sim AL and sim SL perform better than AV , which in this case is worse than the default for all the values of k . Nevertheless the difference between sim AL ,sim SL , and the default loose error is not sta-tistically significant. Like before sim K and AV have similar levels of performance.
 The results provide evidence that the exploitation of the representational power of the relational case base can indeed improve the predictive performance over the flat attribute-value version. Nevertheless this by itself is not sufficient, it should be coupled by the selec-tion of the appropriate similarity measure, otherwise we can even have performance deterioration. 5.2. Recommendations The recommendations of the system are mainly guided by the nature of the dataset that is given as a query, Table 5 presents some examples of the recommenda-tions of the case-base. For datasets composed of nu-meric variables, ( typewriter fonts ), the most simi-lar datasets are also composed of numeric variables, the same holds for datasets with discrete ( slanted fonts ) or a mixture of discrete and continuous variables ( sans serif fonts ). However this intuitively appealing behavior does not guarantee the best performance. For example for dataset allrep the algorithm exhibit-ing the lowest error in its three most similar datasets is c50boost, but for allrep itself the algorithm with the lowest error is c50tree, a fact that results in an erro-neous recommendation. Taking a closer look at the performance of the algorithms for allrep we can see that c50tree, c50rules and c50boost have very similar errors, 0.9%, 0.8% and 0.7% respectively. Moreover the differences between the three algorithms are not statistically significant. All these make the prediction of the algorithm that achieves the lowest error on the specific dataset a very hard task. The problem could have been alleviated by adopting the notion of signifi-cant wins as in (Kalousis &amp; Theoharis, 1999). Another interesting observation is that the case-based system seems to group together datasets that come from similar application domains. For example for dataset byzantine , a dataset related to the recogni-tion of byzantine printed notes, almost all its sim-ilar datasets come from the pattern recognition do-main and even more specifically from character recog-nition, with the exceptions of vowel (acoustic vowel recognition), lrs (low resolution spectrometer dataset) and abalone (predicting the age of gastropod mollusks from some of its physical characteristics). The same holds for the allrep and allbp datasets where their six most similar datasets are the well known thyroid based datasets. The seventh dataset is the hepatitis dataset another medical diagnosis dataset; only the last three most similar datasets do not involve medical appli-cations. In a sense the relational case-based system seems to group datasets into clusters that correspond to specific application regions. The use of CBR systems to support the exploration, comparison, categorization and application of tools from a given domain is not new. Althoff et al. (2000) use a CBR system to build an experience base which does exactly that for Knowledge Managment tools. One of the major learning paradigms in machine learn-ing is lazy learning. Central to this paradigm is the retrieval, based on some notion of similarity, of past in-stances and their reuse in order to determine the class of an unseen instance. On the other hand the standard Case Based Reasoning cycle consists of four phases: re-trieval, reuse, revision and retention (Aamodt &amp; Plaza, 1994). Past cases are retrieved, using similarity mea-sures, reused and possibly revised in order to solve a new unseen problem, with the final solution being re-tained as part of a new case. Lazy learning can profit from the experience developed in CBR systems, and more specifically relational CBR, in order to adapt its instance representation, and the retrieval and reuse phases to tackle classification problems that cannot be adequately addressed within the propositional frame-work. One example of this synergy is the work of Ar-mengol and Plaza (2001), where they used a relational case-based representation and defined similarity mea-sures to perform classification. Their approach is not based on pairwise similarity comparisons, they view similarity as a symbolic description of what the cases present in the case base and the case to be classi-fied have in common. Of direct utility is the work on similarities over complex structures, for example Bergmann and Stahl (1998) discuss the definition of similarities over object oriented structures.
 The meta-learning task serves as an opportunity to set forth a number of representational issues that can-not be tackled adequately via propositional learners. The main issue that arises is the ability to handle one to many relationships that appear when one tries to describe properties of datasets which consist of many variables. Relational case based systems offer a natu-ral solution. In the current paper we continue previous work on meta-learning using such a system. We de-fine precise similarity measures between sets exploiting well established results from clustering and present the results of a thorough evaluation of the system. The re-lational case base allows us to overcome naturally the representational limitations inherent in propositional learning algorithms since it makes use of the relational representations of the training instances.
 The evaluation of the system with respect to Strict Er-ror showed that it provides suggestions which are sta-tistically significant better than the default strategy for sim AL ,k = 3. Comparing the relational versions with the attribute-value version showed a consistent advantage of the former, albeit not statistically signif-icant, for the two cluster inspired similarity measures. The sim K measure had a performance very similar to the attribute-value version, thus providing support for the hypothesis that the two first are more appropriate for the algorithm selection problem. The Loose Error results, i.e. how often the recommendation is a part of the truly best set of algorithms for a given dataset, were similar though the differences observed this time were not statistically significant.
 Another interesting dimension of the present work are the groups of datasets that the case-base was form-ing whenever it was presented with a query. These seem to comprise datasets that come from very sim-ilar application domains. So we observed groups of datasets from pattern recognition problems, or groups of datasets from medical diagnosis problems. An inter-esting further research direction would be the applica-tion of clustering algorithms making use of the multi-relational representation capabilities of the case-base. The proposed relational case-based representation of datasets can be used together with any of the various meta-learning frameworks for algorithm selection, e.g. algorithms ranking, direct error prediction etc. Overall the performance results are encouraging but there is still space for significant improvement. We plan to focus our efforts on the representational issues that are set forth here by providing more elaborate representation schemas for the description of various properties of the set of attributes of a dataset, but also more general of sets of objects. More precisely we want to use histogram representations, as they were introduced in (Kalousis &amp; Theoharis, 1999), to de-scribe the distributions of properties of sets of objects in a compact form and use them to extend the classical propositional learning schema. This will give rise to a new representational schema that will lie between the propositional and the multi-relational paradigms. The authors would like to thank all reviewers for their most helpful comments, that considerably improved the paper.
 Aamodt, A., &amp; Plaza, E. (1994). Case-based reason-ing: Foundational issues, methodological variations, and system approaches. AI Communications , 7 , 39 X  59.
 Aha, D., Breslow, L., &amp; Munoz-Avila, H. (2001). Con-versational case-based reasoning. Applied Intelli-gence , 14 , 9 X 32.
 Althoff, K.-D., Muller, W., Nick, M., &amp; Snoek, B. (2000). KM-PEB: An online experience base on knowledge management technology. Advances in
Case-Based Reasoning, 5th European Workshop (pp. 335 X 347). Springer.
 Armengol, E., &amp; Plaza, E. (2001). Lazy induction of descriptions for relational case-based learning. Pro-ceedings of the 12th European Conference on Ma-chine Learning (pp. 13 X 24). Springer.
 Bergmann, R., &amp; Stahl, A. (1998). Similarity measures for object-oriented case representations. Advances in Case-Based Reasoning, 4th European Workshop (pp. 25 X 36). Springer.
 Brazdil, P., Carlos, S., &amp; Costa, J. (2003). Ranking learning algorithms. Machine Learning .
 Cohen, W. (1995). Fast effective rule induction. Pro-ceedings of the 12th International Conference on Machine Learning (pp. 115 X 123). Morgan Kaufman. Duda, R., Hart, P., &amp; Stork, D. (2001). Pattern clas-sification and scene analysis , chapter Unsupervised Learning and Clustering. John Willey and Sons. Gama, J., &amp; Brazdil, P. (1999). Linear tree. Intelligent Data Analysis , 3 , 1 X 22.
 Hilario, M., &amp; Kalousis, A. (2001). Fusion of meta-knowledge and meta-data for case-based model se-lection. Proceedings of the 5th European Conference on Principles and Practice of Knowledge Discovery in Databases . Springer.
 Kalousis, A., &amp; Theoharis, T. (1999). Noemon: De-sign, implementation and performance results of an intelligent assistant for classifier selection. Intelli-gent Data Analysis , 3 , 319 X 337.
 Kirsten, M., Wrobel, S., &amp; Horvath, T. (2001). Re-lational data mining , chapter Distance Based Ap-proaches to Relational Learning and Clustering, 212 X 232. Springer.
 Lindner, C., &amp; Studer, R. (1999). AST: Support for algorithm selection with a CBR approach. Proceed-ings of the 16th International Conference on Ma-chine Learning, Workshop on Recent Advances in Meta-Learning and Future Work .
 Michie, D., Spiegelhalter, D., &amp; Taylor, C. (1994). Ma-chine learning, neural and statistical classification . Ellis Horwood Series in Artificial Intelligence. Peng, Y., Flach, P., Soares, C., &amp; Brazdil, P. (2002). Improved dataset characterisation for meta-learning. Proceedings of the 5th International Con-ference on Discover Science 2002 . Springer-Verlag. Pfahringer, B., Bensusan, H., &amp; Giraud-Carrier, C. (2000). Tell me who can learn you and I can tell you who you are: Landmarking various learning algo-rithms. Proceedings of the 17th International Con-ference on Machine Learning (pp. 743 X 750). Morgan Kaufman.
 Rendell, L., Seshu, R., &amp; Tcheng, D. (1987). Layered concept learning and dynamically variable bias man-agement. Proceedings of the 10th International Joint Conference on AI (pp. 366 X 372). Morgan Kaufman. Todorovski, L., &amp; Dzeroski, S. (1999). Experiments in meta-level learning with ILP. Proceedings of the 3rd
European Conference on Principles of Data Mining
