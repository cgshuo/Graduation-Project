 Universit  X  e de Caen Basse-Normandie, GREYC, CNRS, UMR 6072 Universit  X  e de Caen Basse-Normandie, GREYC, CNRS, UMR 6072 Universit  X  e de Caen Basse-Normandie, GREYC, CNRS, UMR 6072
Agreement measures have been widely used in computational linguistics for more than 15 years concerning categorization , fewer studies address unitizing , and when both paradigms are combined even fewer methods are available and discussed. The aim of this article is threefold.
First, we advocate that to deal with unitizing , alignment and agreement measures should be the units from different annotators, and this alignment should be computed according to the principles of the measure. Second, we propose the new versatile measure  X  , which fulfills this requirement and copes with both paradigms, and we introduce its implementation. Third, we methods devoted to categorization or segmentation, while combining the two paradigms at the same time. 1. Introduction
A growing body of work in computational linguistics (CL hereafter) or natural language processing manifests an interest in corpus studies, and requires reference annotations for system evaluation or machine learning purposes. The question is how to ensure that an annotation can be considered, if not as the  X  X ruth, X  than at least as a suitable reference. For some simple and systematic tasks, domain experts may be able to annotate texts with almost total confidence, but this is generally not the case when no expert is available, or when the tasks become harder. The very notion of  X  X ruth X  may even be utopian when the annotation process includes a certain degree of interpretation, and we should in such cases look for a consensus, also called the  X  X old standard, X  rather than for the  X  X ruth. X  confidence is to give the same annotation task to several annotators, and to analyze to what extent they agree in order to assess the reliability of their annotations. This is the aim of inter-annotator agreement measures. It is important to point out that most of these measures do not evaluate the distance from annotations to the  X  X ruth, X  but rather the distance across annotators. Of course, the hope is that the annotators will agree as far as possible, and it is usually considered that a good inter-annotator agreement ensures the constancy and the reproducibility of the annotations: When agreement is high, then the task is consistent and correctly defined, and the annotators can be expected to agree on another part of the corpus, or at another time, and their annotations therefore con-stitute a consensual reference (even if, as shown for example by Reidsma and Carletta [2008], such an agreement is not necessarily informative for machine learning purposes).
Moreover, once several annotators reach good agreement on a given part of a corpus, then each of them can annotate alone other parts of the corpus with great confidence in the reproducibility (see the preface to Gwet [2012, page 6] for illuminating consid-erations). Consequently, inter-annotator agreement measurement is an important point for all annotation efforts because it is often considered that a given agreement value provided by a given method validates or invalidates the consistency of an annotation effort.
 of the problem. There is no universal answer, because how to measure depends on the nature of the task, hence on the kind of annotations.
 namely, when annotators have to choose a category for previously identified entities.
This approach, which we will call pure categorization , has led to several well-known and widely discussed coefficients such as  X  ,  X  , or  X  , since the 1950s. Some more recent efforts have been made in the domain of unitizing , following Krippendorff X  X  termi-nology (Krippendorff 2013), where annotators have to identify by themselves what scarce, however, as Krippendorff pointed out:  X  X easuring the reliability of unitizing has been largely ignored in favor of coding predefined units X  (Krippendorff 2013, page 310). This scarcity concerns either segmentation , where annotators simply have to mark boundaries in texts to separate contiguous segments, or more generally unitizing , where gaps may exist between units. Moreover, some even more complex configura-tions may occur (overlapping or embedding units), which are more rarely taken into account.
 are proposed and discussed. That is the main problem we focus on in this article and to which  X  provides solutions. 438 concerning the joint tasks of unit locating ( unitizing ) and unit labeling ( categorization ).
It relies on an alignment of units between different annotators, with penalties associated with each positional and categorial discrepancy. The alignment itself is chosen to mini-mize the overall discrepancy in a holistic way, considering the full continuum to make choices, rather than making local choices. The proposed method is unified because the computation of  X  and the selection of the best alignment are interdependent: The computed measure depends on the chosen alignment, whose selection depends on the measure.
 2010, and were first presented to the French community in a very early version in
Mathet and Widl  X  ocher (2011). The initial motivation for their development was the lack of dedicated agreement measures for annotations at the discourse level, and more specifically for annotation tasks related to T OPIC T RANSITION ing the important notions that are necessary to characterize annotation tasks and by introducing the examples of linguistic objects and annotation tasks used in this article to compare available metrics. Second, we analyze the state of the art and identify the weaknesses of current methods. Then, we introduce our method, called  X  . As this method is new, we compare it to the ones already in use, even in their specialized fields (pure categorization, or pure segmentation), and show that it has better properties overall for CL purposes. 2. Motivations, Scope, and Illustrations
We focus in the present work on both categorizing and unitizing, and consider therefore annotation tasks where annotators are not provided with preselected units, but have to locate them and to categorize them at the same time. An example of a multi-annotated continuum (this continuum may be a text or, for example, an audio or a video recording) is provided in Figure 1, where each line represents the annotations of a given annotator, from left to right, respecting the continuum order.
 2.1 Properties of Annotation Tasks and Annotated Items we consider the following properties, illustrated in Figure 1.
 Categorization occurs when the annotator is required to label (predefined or not) units. Unitizing occurs when the annotator is asked to identify the units in the continuum:
Embedding (hierarchical overlap) may occur if units may be embedded in larger ones
Free overlap may occur when guidelines tolerate the partial overlap of elements
Full-covering (vs. sporadicity) applies when all parts of the continuum are to be anno-
Aggregatable types or instances correspond to the fact that several adjacent elements
Two specific cases. We call hereafter pure segmentation (illustrated by Figure 3) the special case of unitizing with full-covering and without categorization, and we call pure categorization categorization without unitizing. 2.2 Examples of Annotation Tasks
To present the state of the art as well as our own propositions, and to make all of them more concrete, it is useful to mention examples of linguistic objects and annotation tasks for which agreement measures may be required. The following sections will then refer to these examples as often as possible, in order to illustrate discussions on abstract problems or configurations. Small caps are used to refer to the names of these tasks. 440 mentioned in this article to illustrate and compare methods and metrics. These objects and tasks are briefly described for convenience in Appendix A. This table shows that annotation of T OPIC T RANSITIONS is the most demanding of the tasks regarding the number of necessary criteria a suitable agreement metric should assess, but most of the tasks listed here require assessment of both unitizing and categorization. 3. State of the Art
As we saw in the previous section, different studies in linguistics or CL involve quite different structures, which may lead to annotation guidelines having very different properties. They require suitable metrics in order to assess agreement among annota-tors. As we will see, some of the needs for which  X  is suitable are not satisfied by other available metrics.
 which are of most importance for this work, in particular, chance correction and unitiz-ing. For a thorough introduction to the most popular measures that concern categoriz-ing, we refer the reader to the excellent survey by Artstein and Poesio (2008). measures, then we give an overview of available measures in three domains: pure categorization, pure segmentation, and unitizing. 3.1 Agreement Measures and Chance Correction
We begin the state of the art with the question of chance correction, because it is a cross-cutting issue in all agreement measure domains, and because it influences the final value provided by most agreement measures.
 the output of an annotating system is compared to a valid reference , and (2) inter-annotator agreement measures, which try to quantify the degree of similarity between what different annotators say about the same data, and which are the ones we are really concerned with in this article.
 valid answers of a system: We know exactly how far the evaluated system is from the gold standard, and we can compare this system to others just by comparing their results. exists), but they compare annotations from different annotators. As such, they are clearly not direct distances to the  X  X ruth. X  So, the question is: Above what amount of agreement can we reasonably trust the annotators? The answer is not straightforward, and this is where chance correction is involved.
 categories. If they annotate at random (with the 10 categories having equal prevalence), they will have an agreement of 10%. If we consider another task involving two cate-gories only, still at random, the agreement expected by chance rises to 50%. Based on this observation, most agreement measures try to remove chance from the observed measure, that is to say, to provide the amount of agreement that is above chance. More precisely, most agreement measures (for about 60 years, with well-known measures  X  , S ,  X  ) rely on the same formula: If we note A o the observed agreement (i.e., the agreement directly observed between annotators) and A e the so-called expected agreement (i.e. the agreement which should be obtained by chance), the final agreement A is defined by Equation (1).

A = 0 . 7, A = 0 . 2 / 0 . 3 = 0 . 67, which is not that good, and if A annotators did not perform better than chance, then A = 0.
 in this article, are computed from observed and expected disagreements (instead of agreements), denoted here respectively D o and D e , and they define the final agreement by Equation (2).
 many coefficients (  X  , S ,  X  , and their generalizations), and is a controversial question. As precisely described in Artstein and Poesio (2008), there are three main ways to model chance in an annotation effort: 1. By considering a uniform distribution . For instance, in a categorization 442 2. By considering the mean distribution of the different annotators hence 3. By considering the individual distributions of annotators. Here, annotators very high, depending on the prevalence of categories . In some annotation tasks, expected agreement becomes critically high, and any disagreements on the minor category have huge consequences on the chance-corrected agreement, as hotly debated by Berry (1992) and Goldman (1992), and criticized in CL by Di Eugenio and Glass (2004). However, we follow Krippendorff (2013, page 320), who argues that disagreements on rare categories are more serious than on frequent ones. For instance, let us consider the reliability of medical diagnostics concerning a rare disease that affects one person out of 1,000. There are 5,000 patients, 4,995 being healthy, 5 being affected. If doctors fail to agree on the 5 affected patients, their diagnostics cannot be trusted, even if they agree on the 4,995 healthy ones.
 because most coefficients address these tasks, but they are more general and may also concern segmentation and, as we will see further, unitizing. 3.2 Measures for Pure Categorization
The simplest measure of agreement for categorization is percentage of agreement (see for example Scott 1955, page 323). Because it does not feature chance correction, it should be used carefully for the reasons we have just seen.

Goldstein 1954) relies on a uniform distribution model of chance,  X  (Scott 1955) and  X  (Krippendorff 1980) on the mean distribution, and  X  (Cohen 1960) on individual distributions. Generalizations to three or more annotators have been provided, such as  X  (Fleiss 1971), also known as K (Siegel and Castellan 1988). Moreover, weighted coefficients such as  X  and  X  w (Cohen 1968) are designed to take into account the fact that disagreements between two categories are not necessarily all of the same nominal categories), a mistake between categories 3 and 4 should be less penalized than a mistake between categories 1 and 10.
 egorization tasks X  X or example, in the domains of P ART -OF R already been discussed and compared in the perspective of CL and we will not do so here. 3.3 Measures for Segmentation
In the domain of T OPIC S EGMENTATION , several measures have been proposed, es-pecially to evaluate the quality of automatic segmentation systems. In most cases, this evaluation consists in comparing the output of these systems with a reference annotation. We mention them here because their use tends to be extended to inter-annotator agreement because of the lack of dedicated agreement measures, as illustrated by Artstein and Poesio (2008), who mention these metrics in a survey related to inter-annotator agreement, or by Kazantseva and Szpakowicz (2012).
 the penalty must depend on the distance from a true boundary. Thus, dedicated mea-sures have been proposed, such as WindowDiff (WD hereafter; Pevzner and Hearst 2002), based on Pk (Beeferman, Berger, and Lafferty 1997). WD relies on the following principle: A fixed-sized window slides over the text and the numbers of boundaries in the system output and reference are compared. Several limitations of this method have been demonstrated and adjustments proposed, for example, by Lamprier et al. (2007) or by Bestgen (2009), who recommends the use of the Generalized Hamming Distance (GHD hereafter; Bookstein, Kulyukin, and Raita 2002), in order to improve the stability of the measure, especially when the variance of segment size increases.
 systems, their most serious weakness for assessing agreement is that they are not chance-corrected, but they present another limitation: They are dedicated to segmen-tation and assume a full-covering and linear tiling of the continuum and only one category of objects (topic segments). This strong constraint makes them unsuitable for unitizing tasks using several categories (A RGUMENTATIVE 3.4 Measures for Unitizing 3.4.1 Using Measures for Categorization to Measure Agreement on Unitizing. Because of the lack of dedicated measures, some attempts have been made to transform the task as  X  .
 quence of atoms, thereby reducing a unitizing problem to a categorization problem.
This is illustrated by Figure 4, where real unitizing annotations are on the left (with two annotators), and the transformed annotations are on the right. To do so, an atom granularity is chosen X  X or instance, in the case of texts, it may be character, word, sentence, or paragraph atoms. Then, each unit is transformed into a set of items labeled with the category of this unit, and a new  X  X lank X  category is added in order to emulate gaps between units.
 1. Two contiguous units seen as one. In zone (1) of the left part of Figure 4, 444 2. False positive/negative disagreement and slight positional disagreement 3. Agreement on gaps. Because of the discretization, artificial blank items are 4. Overlapping and embedding units are not possible. This results because of B
EHAVIOR in video recordings by Reidsma (2008), where unitizing is, on the contrary, clearly required: The time-line is discretized ( atomized ), then  X  and  X  are computed using discretized time spans as units. It should be noted that Reidsman, Heylen, and
Ordelman (2006, page 1119) and Reidsma (2008) claim that this  X  X airly standard X  method (which we call discretizing measure henceforth) has certain drawbacks, such as the fact that it  X  X oes not compensate for differences in length of segments, X  whereas  X  X hort segments are as important as long segments X  in their corpus (which is an addi-tional limitation to the ones we have just mentioned). They propose a second approach relying on an alignment, as we mention in Section 4.2.1.
 the perspective of D ISCOURSE F RAMING , two adjacent temporal frames should not be aggregated in a larger one. In the same manner, for T OPIC makes no sense to aggregate two consecutive segments. 3.4.2 A Measure for Unitizing Without Chance Correction. Another approach, derived from SER below, was more specifically used in the context of evaluation of N recognition systems.
 (i.e., position) with cost 0.5, error  X  X B X  on both type and boundaries with cost 1, error  X  X  X  of insertion (i.e., false positive) with cost 1, and error  X  X  X  of deletion (false negative) with cost 1. The overall cost relies on an alignment of objects from reference and hypothesis, which is chosen to minimize this cost. The final value provided by SER is the average cost of the aligned pairs of units X 0 meaning perfect agreement, 1 roughly meaning systematic disagreement. An example is given in Figure 5.
 particular, all positioning and categorizing errors have the same penalty, which may be a serious drawback for annotation tasks where some fuzziness in boundary positions is
Moreover, it is difficult to interpret because its output is surprisingly not upper bounded designed to compare an output to a reference, and so requires some adjustments to cope with more than 2 annotators. Last but not least, it is not chance corrected. 3.4.3 Specific Measures for Unitizing. To our knowledge, the family of  X  measures pro-posed by Krippendorff is by far the broadest attempt to provide suitable metrics for various annotation tasks, involving both categorization and unitizing.
 answer to unitizing is formulated as follows:  X  X e suspect that the methods proposed by Krippendorff (1995) for measuring agreement on unitizing may be appropriate for the purpose of measuring agreement on discourse segmentation. X  Unfortunately, as far as we know, its usage in CL is rare, despite the fact that it is the first coefficient that copes both with unitizing and categorizing at the same time, while taking chance into account. The family of  X  measures would then be suitable for annotation tasks related, cause it constitutes a very interesting reference to compare with, both in terms of the-oretical choices and of results. Let us briefly recap Krippendorff X  X  studies on unitizing from 1995 to 2013 and introduce some of the  X  measures, which will be discussed in this article. The  X  coefficient (Krippendorff 1980, 2004, 2013), dedicated to agreement measures on categorization tasks, generalizes several other broadly used statistics and known  X  measure, which copes with categorizing, a new coefficient called  X  U has been proposed since 1995 in Krippendorff (1995) and then Krippendorff (2004), which can apply to unitizing. Recently, Krippendorff (2013, pages 310, 315) proposed a new version of this coefficient, called u  X  ,  X  X ith major simplifications and improvements over 446 previous proposals, X  and which is meant to  X  X ssess the reliability of distinctions within a continuum X  X ow well units and gaps coincide and whether units are of the same or of a different kind. X  To supplement u  X  , which mainly focuses on positioning, Krippendorff has proposed c | u  X  (Krippendorff 2013), which ignores positioning disagreement and focuses mainly on categories.
 noted that u  X  and c | u  X  are not currently designed to cope with embedding or free overlapping between the units of the same annotator. These metrics are then unsuitable for annotation tasks such as, for instance, T OPIC T RANSITION S 3.5 Overview Table
To conclude the state of the art, we draw up a final overview of the coverage of the requirements by the different measures in Table 2. The  X  measure, introduced in the next section, aims at satisfying all these needs. 4. The Proposed Method: Introducing  X   X   X  4.1 Our Proposal disorders ) between units from different annotators are averaged to compute an overall disorder. However, these local disorders can be computed only if we know for each unit of a given annotator, which units, if any, from the other annotators it should be compared with (via what is called unitary alignment ) X  X hat is to say, if we can rely on a suitable alignment of the whole (called alignment ). Because it is not possible to get a reliable preconceived alignment (as explained in Section 4.2.1),  X  considers all possible ones, and computes for each of them the associated overall disorder. Then,  X  retains as the best alignment the one that minimizes the overall disorder, and the latter value is retained as the correct disorder. To obtain the final agreement, as with the familiar kappa and alpha coefficients, this disorder is then chance-corrected by a so-called expected disorder, which is calculated by randomly resampling existing annotations.
 in Section 4.3 the basic definitions. The comparison of two units (depending on their relative positions and categories) relies on the concept of dissimilarity (Section 4.4).
A unitary alignment groups at most one unit of each annotator, and a set of unitary alignments covering all units of all annotators is called an alignment (Section 4.5). The disorder associated with a unitary alignment results from dissimilarities between all its pairs of units, and the disorder associated with an alignment depends on those of its unitary alignments (Section 4.6). The alignment having the minimal disorder (Section 4.7) is used to compute the agreement value, taking chance correction into account (Section 4.8). 4.2 Main Principles of  X   X   X  4.2.1 Measuring and Aligning at the Same Time:  X  is Unified. For a given phenomenon iden-tified by several annotators, it is necessary to provide an agreement measure permissive enough to cope with a double discrepancy concerning its position in the continuum, and the category attributed to the phenomenon.
 sure with an inter-annotator alignment, which shows which unit of a given annotator corresponds, if any, to which unit of another annotator. If such an alignment is provided, it becomes possible, for each phenomenon identified by annotators, to determine to what extent the annotators agree both on its categorization and its positioning. This quantification relies on a certain measure (called dissimilarity hereafter) between an-notated units: The more the units are considered as similar, the lesser the dissimilarity.
A 1 of annotator A with unit B 1 of annotator B consists in considering that their proper-ties are similar enough to be associated: annotator A and annotator B have accounted for the same phenomenon, even if in a slightly different manner. Consequently, to operate, the alignment method should rely on a measure of distance (in location, in category assignment, or both) between units.
 to correctly measure without aligning, and it is not possible to align units without measuring their distances. In that respect, measuring and aligning cannot constitute two successive stages, but must be considered as a whole process. This interdependence 448 reflects the unity of the objective: Establishing to what extent some elements, possibly different, may be considered as similar enough either to quantify their differences (when measuring agreement), or to associate them (when aligning).
 by the use of the discretizing measure as already mentioned,  X  X ave developed an extra method of comparison in which [they] try to align the various segments. X  This attempt highlights the necessity to rely on an alignment. Unfortunately, the way the alignment is computed, adapted from Kuper et al. (2003), is disconnected from the measure itself, being an ad hoc procedure to which other measures are applied. 4.2.2 Aligning Globally:  X  is Holistic. Let us consider two annotators A and B having respectively produced unit A 5, and units B 4 and B 5, as shown in Figure 7. When consid-ering this configuration at a local level, we may consider, based on the overlapping area for instance, that A 5 fits B 5 slightly better than B 4. However, this local consideration may be misleading. Indeed, Figure 8 shows two larger configurations, where A 5, B 4, and B 5 are unchanged from Figure 7. With a larger view, the choice of alignment of A 5 may be driven by the whole configuration, possibly leading to an alignment with B 4 in
Figure 8a, and with B 5 in Figure 8b: Alignment choices depend on the whole system and the method should consequently be holistic . 4.2.3 Accounting for Different Severity Rates of Errors: Positional and Categorial Permissive-ness of  X  . As far as positional discrepancies between annotators are concerned, it is important for a measure to rely on a progressive error count, not on a binary one: Two positions from two annotators may be more or less close to each other but still concern the same phenomenon (partial agreement), or may be too far to be considered as related to the same phenomenon (no possible alignment). For instance, for segmentation, spe-cific measures such as GHD or WD rely on a progressive error count for positions, with an upper limit being half the average size of the segments. For unitizing, Krippendorff considers with u  X  that units can be compared as long as they overlap. However,  X  considers that in some cases, units by different annotators may correspond to the same phenomenon though they do not intersect. We base this claim on two grounds. First, if we observe the configuration given in Figure 9, annotators 2 and 3 have both annotated part of the N AMED E NTITY that has been annotated by annotator 1. Consequently, though they do not overlap, their units refer to the same phenomenon. In addition, we find a direct echo of this assumption in Reidsma (2008, pages 16 X 17) where, in a video corpus concerning C OMMUNICATIVE B EHAVIOR ,  X  X ifferent timing (non-overlapping) [of the same episode] was assigned by [...] two annotators. X  Regarding categorization, some available measures consider all disagreements between all pairs of categories as equal. Other coefficients, called weighted coefficients (see Artstein and Poesio 2008), as well as  X  , consider on the contrary that mismatches may not all have the same weight, some pairs of categories being closer than others. This closeness is often referred to as overlap .
 and overlap means positional overlap . For example, within annotation efforts related to W ORD S ENSE or D IALOG A CTS , it is clear that disagreements on labels are not all alike. 4.3 Definitions: Unit, Annotator, Annotation Set
Given a multi-annotated continuum t : and produced by a given set of annotators.
 is composed of a set of continua, and of the set of annotations related to these continua. 450 boundaries, each of them corresponding to a position in the continuum, respectively denoted start ( u ) and end ( u ), start and end being functions from U to 4.4 Dissimilarity Between Two Units
We introduce here the first brick to build the notion of disorder, which works at a very local level, between two units. A dissimilarity tells to what degree two units should categories, or a combination of the two.
 in particular because triangular inequality is not mandatory (for instance, in Figure 10, d ( A 1, B 2) &gt; d ( A 1, C 1) + d ( C 1, B 2)). 4.4.1 Empty Unit u  X  , Empty Dissimilarity  X   X  . As we will see,  X  relies on an alignment of units by different annotators. In particular, this alignment indicates for unit u annotator a 1 , to which unit u a 2 j of annotator a 2 it corresponds, in order to compute the associated dissimilarity. In some cases, though, the method will choose not to align u with any unit of annotator a 2 (none corresponds sufficiently). We define the empty pseudo unit, denoted u  X  , which corresponds to the realization of this phenomenon: ultimately, a pseudo unit u  X  is added to the annotations of a pared units are considered critically different. Consequently, it constitutes a reference, and dissimilarities will be expressed in this article as multiples of  X 
It is not a parameter of gamma , but a constant (which is set to 1 in our implementation). 4.4.2 Positional Dissimilarity d pos . Different positional dissimilarities may be created, in order to deal with different annotation tasks. In this article, we use the dissimilarity shown in Equation 3, which is very versatile. units in its numerator. Its denominator sums the lengths of both units, so that this dis-similarity is not scale-dependent. Squaring the value is an option used here to accelerate dissimilarity when differences of positions increase. It is illustrated in Figure 10 with different configurations and their associated values, from 0 for the perfectly aligned pair of units ( A 1, B 1) to 22 . 2  X   X   X  for the worst pair ( A 1, C 2). 4.4.3 Categorial Dissimilarity d cat . Let K be the set of categories. For a given annotation effort, | K | different categories are defined. For more convenience, we first define catego-rial distance between categories dist cat via a square matrix of size | K | , with each category appearing both in row titles and column titles. Each cell gives the distance between two categories through a value in [0, 1]. Value 0 means perfect equality, whereas the maximum value 1 means that the categories are considered as totally different. As dist is symmetric, such a matrix is necessarily symmetric, and bears 0 in each diagonal cell.
Table 3 gives an example for three categories, and shows that an association between a unit in category cat 1 with one in category cat 3 is the worst possible (distance = 1), whereas it is half as much between cat 1 with cat 2 (distance = 0.5). This makes it possible to take into account so-called category-overlapping (in our example, cat said to overlap, which means they are not completely different), as weighted coefficients such as  X  w or  X  already do. Note that in the case of so-called  X  X ominal categories, X  the categories are considered as not matching at all).
 taking into account the  X   X  value. We define categorial dissimilarity between two units by: 452 the categorial distance values. The standard option 2 (used in this article) is to simply consider f cat ( x ) = x , with which d cat naturally increases gradually from zero when cate-gories match, to  X   X  when categories are totally different ( dist d cat ( u , v ) =  X   X  ). 4.4.4 Combined Dissimilarity d combi . Because in some annotation tasks units may differ both in position and in category, it is necessary to combine the associated dissimilarities so that all costs are cumulated. This is provided by a combined dissimilarity. dissimilarity (if (  X  ,  X  ) 6 = (0, 0)). It enables the same weight to be assigned to positions with any other one, or to be aligned with a unit in the same configuration as ( A 1, C 1) of Figure 10 (if they have the same category), or to be aligned with a unit having an incompatible category (if they occupy the same position). 4.5 Unitary Alignment, Alignment Unitary alignment  X  a. A unitary alignment  X  a is an i -tuple, i belonging to number of annotators), containing at most one unit by each annotator: It represents the hypothesis that i annotators agree to some extent on a given phenomenon to be unitized.
In order to make all unitary alignments homogenous, we eventually complete any unitary alignment that is an i -tuple with n  X  i empty units u ments are ultimately n -tuples. Figure 11 illustrates unitary alignments with some u units.

Alignment  X  a. For a given annotation set, an alignment  X  a is defined as a set of unitary alignments such that each unit of each annotator belongs to one and only one of its unitary alignments. Mathematically, it constitutes a partition of the set of units (if we do not take u  X  into account). 4.6 Alignment and Disorder 4.6.1 Disorder of a Unitary Alignment. The disorder of a unitary alignment  X  a , denoted is defined for a given dissimilarity d as the average of the one-to-one dissimilarities of its units: of the number of annotators. 4.6.2 Disorder of an Alignment. The disorder of an alignment  X  a , denoted of the disorders of all its unitary alignments divided by the mean number of units per annotator: does not depend on the size of the continuum. 4.7 Best Alignment, Disorder of an Annotation Set
Best alignment  X  a . An alignment  X  a of the annotation set s is considered as the best (with respect to a dissimilarity) if it minimizes its disorder among all possible alignments of s . It is denoted  X  a . The proposed method is holistic in that it is necessary to take into account the whole set of annotations in order to determine each unitary alignment.
Disorder of an annotation set  X  ( s ). The disorder of the annotation set s , denoted  X  ( s ), is defined as the disorder of its best alignment(s) that several alignments produce the lowest disorder.

We have just presented the two crucial definitions of our new method, which make it  X  X nified. X  Indeed, the best alignment is chosen with respect to the disorder, 454 therefore with respect to what computes the agreement measure; and, conversely and simultaneously, the resulting agreement value (see below) is given by the best alignment: agreement computation and alignment are fully intertwined, whereas in most agreement metrics, the alignment is fixed a priori or no alignment is used. 4.8 Expected Disorder for Chance Correction 4.8.1 The Model of Chance of  X  . As we have already mentioned in the state of the art, it is necessary for an inter-annotator agreement measure to provide chance correction. We have also seen that there are several chance correction models, and that it is a contro-versial question. However, for  X  , we follow Krippendorff, who claims that annotators should be interchangeable , because, as stressed by Krippendorff (2011) and Zwick (1988),
Cohen X  X  definition of expected agreement (using individual distributions) numerically rewards annotators for not agreeing on their use of values, that is to say when they have different prevalences of categories, and punishes those that do agree. Therefore, expected values of  X  are computed on the basis of the average distribution of observed annotations of the several annotators .
 a multi randomly annotated continuum where: whereas other studies systematically compute the expected value on the data also used to compute the observed value (see Section 3.1), we consider that it should be computed, when possible (that is to say, when several continua have been annotated with the same set of categories and the same instructions), from the distribution ob-served in all continua of the annotation effort the evaluated continuum comes from:
If distribution changes from one continuum to another one, it is more because of the content of each continuum than because of chance. Let us illustrate this by a simple example, where two annotators have to annotate several texts from a sentiment analysis point of view, using three available categories: positive, negative, and neutral. On average, on the whole corpus, we assume that the prevalence is
The expected agreement on the whole corpus is thus 0.33. We also assume that for one negative one. The expected agreement for this particular text is 0.5, which means that this particular text is considered to facilitate agreement by chance, and which has the consequence that the final agreement will be more conservative than for the rest of the corpus. Why does the third category,  X  X egative, X  not appear in this expected agreement computation? This conception of chance considers that when an annotator begins to annotate this particular text, which she does not already know, the third category no they are not supposed to cooperate. It cannot be by chance that all annotators use one category in some texts, and not in another one, but because of the content, and of the take into account the data observed on a whole annotation effort rather than on each individual continuum. The complete data tell more about the mean behavior of annota-tors, whereas data of a given continuum may depend more on the particularities of its content.
 considers only the data of the continuum being evaluated, as does every other coeffi-cient; and a second one, which considers the data from all continua of the annotation effort the evaluated continuum comes from. When available, we recommend using the second one, for the reasons already expressed. 4.8.3 Using Sampling to Compute the Expected Value. Expected agreement (or disagree-ment) is the expected value of a random variable. But which random variable? For coefficients like kappa and alpha, observed agreement (or disagreement) is the mean agreement (or disagreement) on all pairs of instances, so the random variable can be as simple as a random pair of instances (however we interpret  X  X andom X ). This value can be readily computed. For gamma, however, observed disagreement is determined on a whole annotation, so the random variable needs to be a whole random annotation.
The expected value of such a complicated variable is much more difficult to determine analytically. Instead, gamma uses sampling, as introduced in Section 5. 4.9 Agreement Measure  X   X   X 
Now that the disorder and the expected disorder have been introduced, we can define the agreement measure (of annotation set s belonging to corpus c , with c = { s } if s is a sole annotation set) with Equation 8, which is derived from Equation 2: worst case, where the annotators are worse than annotating at random, with  X  &lt; 0.
Figure 11b shows an intermediate situation. 5. Implementation
In this section, we first propose an efficient solution to compute the disorder of an an-notated continuum, which relies on linear programming. Second, we propose two ways to generate random annotated continua (with respect to the observed distributions) to compute the expected disorder, one relying on a single continuum, the other one relying on a corpus (i.e., several continua). Third, we determine the number of random data sets 456 that we must generate (and compute the disorder of) to obtain an accurate value of the expected disorder. 5.1 Computing the Disorder
In order to simplify the discussion and the demonstrations, we consider in this section that n annotators all made the same number of annotations p .
 being holistic, its software implementation leads to a major problem of complexity. One can demonstrate that there are theoretically ( p !) n  X  1 will (1) show how to reduce the initial complexity, and (2) provide an efficient linear programming solution. 5.1.1 Reducing the Initial Complexity. The initial number of possible unitary alignments (which are used to build a possible alignment) is p as Equation (9) states that any unitary alignment with a cost beyond the value n  X   X  cannot belong to the best alignment, and so can be discarded. Indeed, any unitary alignment with a cost above  X   X  can be replaced by creating a separate unitary alignment for each unit (of cost  X   X  per unitary alignment, so of total cost n  X   X  alignments. For convenience, we attribute to it the index 1 (  X  a =  X  a indexed from 2 to m . This unitary alignment  X  a contains n units (either real or u each of these units u i (1  X  i  X  n ), we create the unitary alignment  X  a of cardinality n . It is possible to create an alignment  X  a made up of the set of unitary alignments of  X  a \{  X  a } , to which we add the unitary alignments  X  a have just created. 3 It is of cardinality m + n  X  1. Because  X  a minimizes the disorder, we obtain: unitary alignments. 5.1.2 Finding the Best Alignment: A Linear Programming Solution. Finding the best align-ment consists of minimizing the global disorder. Such a problem may be described as a linear programming problem, so that the solution can be computed by a linear programming solver. For convenience, we introduce two new definitions:
Boolean variable X  X  a alignment: tator) should belong to one and only one unitary alignment of the alignment  X  a , that is to say that among all unitary alignments containing u , exactly one X others equal 0: possible alignments  X  a : annotators and p = 100 annotations per annotator on a current laptop (once the initial complexity has been reduced thanks to the previous theorem), which is fast enough to be practical. 5.2 Implementation of Expected Disorder
The next two subsections detail two strategies to generate randomly annotated continua with respect to the definition of the expected disorder of  X  , and the third subsection explains how to choose the number of expected disorder samples to generate so that their average is an accurate enough value of the theoretical expected value. The two strategies correspond to the need expressed in Section 4.8.1 to compute the expected value on the largest set of available data, either a single continuum, or, when available, several continua from the same corpus. 458 5.2.1 A Strategy to Compute the Expected Disorder Using a Single Continuum. When the annotation effort is limited to a single continuum, we can only rely on the annotated continuum itself to compute the expected value. To create random annotations that fulfill the observed distributions, the implemented strategy is as follows: We take the real annotated continuum of an annotator (such as the example shown on the left in
Figure 12), choose at random a position on this continuum, split the continuum at this position, and permute the two parts of the split continuum. Three examples of split and permutation are shown in the right part of the figure, for split positions of, respectively, 15, 24, and 38, all coming from the same real continuum, with units that are no longer aligned (except by chance). However, we have to address the fact that some units may intersect with themselves, generating some part of agreement beyond because the length of the unit, 12, is higher than the difference of shifts 24  X  15 = 9.
To limit this phenomenon, we do not allow the distance between two shifts to be less than the average length of units. 5.2.2 A Strategy to Compute the Expected Disorder Using Several Continua (from the Same
Corpus). This strategy consists of mixing annotations coming from different continua, so that their units may align only by chance. To create a random annotation of n annotators, we randomly choose n different continua of the corpus, and pick the annotations of one annotator (randomly chosen) of each of these continua. When different texts are of different lengths, each of them is adjusted to the longest one by duplicating as many times as necessary (like a mosaic).
 eight continua, each annotated by three annotators. To generate a random set of three annotations, we have randomly selected a combination of three values between 1 and 8, here (2, 4, 7), to select three different continua among the eight available ones of the cor-pus. Then, for each of these selected continua, we choose one annotator, here annotator 2 for continuum 2, annotator 3 for continuum 4, and annotator 1 for continuum 7. We combine the associated annotations as shown in the right part of the figure, and obtain a set of random annotations that fulfill (on average) the observed distributions. The (very limited) extent of the resulting agreement we can see in this example (only two units have matching categories, but with discrepancies in position) is only by chance because the compared annotations come from different continua.
 with this strategy: With n annotators and m continua ( m  X  n ), it is possible to generate up to C n m  X  n n different combinations. For instance, in our example, which assumes n = 3 and m = 8, there are 56  X  3 3 = 1512 combinations to create random annotations. 5.3 Computing an Average Value: A Sampling Question
Because the expected disorder is by definition randomly obtained on average , and because there is virtually an infinite number of possible random annotations (with a discrete and finite continuum, it is not really infinite, but still too big to be computed), we can only compute a reduced but sufficient number of experiments and obtain an ap-proximate value of the expected disorder. This is a sampling problem as described, for example, in Israel (1992). What statistics provide is a way to determine the minimal number n 0 of experiments to do (and to average) so that we get an approximate result of a given precision with a given confidence level . It consists in: First, taking a small sample to estimate the mean and standard deviation; then, using these estimates to determine the sample size n 0 that is needed.
 that differs less than e = 2% from the real value with a (1  X   X  ) = 95% confidence (the software distribution we provide is set by default with these values).
 sample mean, and  X  0 be its standard deviation.  X  is directly an unbiased estimator of the population mean, and  X  = p ( n n  X  1 )  X   X  0 is an unbiased estimator of the real standard deviation.

This value is provided in statistical tables. We get n 0 by the following equation:
Let us consider a real example. We generate a sample of random disorders of size n = 30. We compute its mean  X  = 3 . 49, its standard deviation  X  hence  X  = 0 . 1403, and C v = 0 . 040188. We get U 1  X  0 . 05 values gives 2% of precision with 95% confidence. The mean we have already computed with 30 values fulfills this condition, and is a good approximation of the real expected disorder. If we wish to obtain a high precision of 1%, we need n 460 initial size of our sample (which is 30), and we will have to generate an additional set of 32 values in order to obtain the required number. 6. Comparing and Benchmarking  X   X   X 
As  X  is an entirely new agreement measure method, it is necessary to analyze how it compares with some well known and much studied methods. First, we carry out a thorough comparison between  X  and the two dedicated alphas, the most specific measures in the domain. Second, we benchmark  X  by comparing it with other main measures, thanks to a special tool that is briefly introduced. 6.1 Krippendorff X  X  Alphas: Introducing u  X  u  X  u  X  and c | u
As already mentioned, Krippendorff X  X  u  X  and c | u  X  are clearly the most suitable coef-ficients for combined unitizing and categorizing. To better understand the pros and cons as well as the behavior of these measures compared with  X  , we first explain how they are designed in Section 6.1.1, and then make thorough comparisons with  X  from
Section 6.1.2 to 6.1.6 including: (1) how they react to slight categorial disagreements, (2) interlacement of positional and categorial disagreements, (3) the impact of the size of the units on positional disagreement, (4) split disagreements, and (5) the impact of scale (e.g., if the size of all units is multiplied by 2). We finish by showing a paradox of  X  in Section 6.1.7. 6.1.1 Introducing u  X  and c | u  X  . To introduce how these two coefficients work, let us consider the example taken from Krippendorff (2013), shown in Figure 14. The length of the continuum is 76, there are two annotators, and there are four possible categories, numbered 1 to 4.
 annotators, a section being either a categorized unit or a gap. To get the observed disagreement value u D o , squared lengths of the unmatching intersections are summed, and this sum is then divided by the product of the length of the continuum and m ( m  X  1), m being the number of annotators. In the example, mismatches occur around (by symmetry) to the sum 10 2 + 5 2 + 8 2 + 5 2 + 5 2 , and so the observed disagreement the possible positional combinations of each pair, and not only the observed ones. This means that for a given pair, one of the two units is virtually slid in front of the other in all possible ways, and the corresponding values are averaged. In this example, sums of the lengths of all intersections of units for each given pair of categories. For instance, in the example, the observed coincidence between category 1 and category 3 is 5, and so on. A metric matrix is chosen for these categories, for instance, an interval metric (for numerical categories), which says that the distance between category i and category j is ( i  X  j ) 2 . Hence, the cost for a unitary intersection between categories 1 and 2 is (1  X  2) 2 = 1, but is 2 2 = 4 between categories 1 and 3, and so on. Then, the observed disagreement is computed according to these two matrices. To finish, an expected matrix is filled (in a way which cannot be detailed here due to space constraints), and the expected value is computed the same way. In the example, annotators. In the example, u  X  = 0 . 405 indicates that the unitizing is not so good, but also that the categorizing is much better, with c | u these two values are not independent, since unitizing and categorizing coexist here by nature).
 extent they differ from  X  . 6.1.2 Slight Categorial Disagreements: Alphas Versus  X   X   X  . When annotators have slight cate-gorial disagreements (with overlapping categories), c | u  X  does not take categorial overlapping into account, but has a binary response to such disagreements, and is lowered as much as if they were severe categorial disagreements.
A consequence of this approach is illustrated in Figure 15, where two annotators per-perfectly agree on position but slightly diverge concerning categories in the experiment on the right (1/2, 6/7, and 8/9 are assumed to be close categories). However, from 1 in the left experiment to  X 0.34 (a negative value means worse than random) in the right experiment, despite, in the latter, the positions being all correct, and the categories being quite good, since c | u  X  = 0 . 85. On such data,  X  considers that there is no positional disagreement, and c | u  X  and  X  both consider that there are slight categorial disagreements. 6.1.3 Positional Disagreements Impacting Categorial Agreement: tions of how to account for categorial disagreement have, respectively, led to  X  relies on intersections between the units of different annotators, which is basically equivalent to an observation at the atom level, whereas  X  relies on alignments between units (any unit being finally attached and compared to, at most, only one other) based both on positional and categorial observation. Hence, in a configuration such as the one given in Figure 16, where two annotators annotated three units with the same categories 462 1, 4, and 2, but not exactly at the same locations; categorial disagreements, whereas  X  does not. According to the principles of part of the continuum (even at the atom level) with an intersection between different categories means some confusion between them, whereas  X  considers here that the annotators fully agree on categories (they both observed a  X 1 X  then a  X 4 X  then a  X 2 X  with no confusion), and disagree only on where phenomena exactly start and finish.
The crucial difference between the two methods is probably whether we consider units to be non-atomizable (and therefore consider alignments, as  X  does), or atomizable (in which case two different parts of a given unit may be simultaneously and respectively compared to two different units from another annotator). 6.1.4 Disagreements on Long versus Short Spans of Texts. Here again, the way disagree-ments are accounted for may differ markedly between u  X  and  X  : when a unit does not match with any other, u  X  takes into account the length of the corresponding span of text to assess a disagreement. As shown in Figure 17, an orphan unit of size 10 will cost 100 times as much as an orphan unit of size 1, whereas for  X  , they will have the same cost. In the whole example in Figure 17, to compute the observed disagreements, says the first case is 50 times worse than the second, whereas  X  says on the contrary mentioned) expressed by Reidsma, Heylen, and Ordelman (2006, page 3) to consider that  X  X hort segments are as important as long segments. X  This phenomenon is the same for categories between c | u  X  and  X  , the size of the units having consequences only for  X  . 6.1.5 Split Disagreements. Sometimes, an annotator may divide a given span of text into several contiguous units of the same type, or may annotate the same span with one whole unit. In these cases, c | u  X  computes its observed disagreement the same in both configurations, and u  X  assigns decreasing disagreement when splitting increases, as shown in Figure 18, whereas  X  assigns increasing disagreements.
 is still responsive. 6.1.6 Scale Effects. The way u  X  computes dissimilarities is directly proportional to squared lengths, as shown in Figure 20. On the other hand,  X  may use any positional dissimilarity, and usually uses ones that are not scale-dependent for CL applications, one at word level, the other one at paragraph level, we may prefer to account for relative disagreements so that a missing word will be more heavily penalized in the first case than in the second. In Figure 20, the observed disagreement of for B units than for A units, but would be the same for  X  with d 6.1.7 A Paradox: When Agreement on Positioning Reinforces Disagreement on Categorizing. In
Figure 21a, the annotators disagree on categorization, and have a moderate agreement on unitizing. This configuration leads to u  X  = 0 . 107. In Figure 21b, the configuration is quite similar, but now annotators fully agree on unitizing: Each of them puts units in the same positions. Paradoxically, u  X  drops to  X  0 . 287, which is less than in the first configuration. In brief, the reason for this behavior is that in the first case, the computed disagreement regarding a given pair of units is virtually distributed into shorter parts of the whole (an intersection of length 80 between them, and an intersection of length 464 20 with a gap for each of them, which leads to 80 disagreement is maximum in the second case (an intersection of length 100 with a unit of another category, which leads to 100 2 = 10, 000). Contrarily, with similar data,  X  provides a better agreement in the second case than in the first one. With its design, it considers that there is the same categorial agreement in both cases, but better positional agreement in the second case, which seems to better correspond to the CL tasks we have considered. 6.1.8 Overlapping Units (Embedding or Free Overlap). Both alpha coefficients are currently designed to cope only with non-overlapping units (the term overlapping also stands whether they could be generalized to handle overlapping units. It seems that it would involve a major change in the strategy, which currently necessitates comparing the intersections of all pairs of units. In the example shown in Figure 22, even though annotators fully agree on their two units, the alphas will inherently compare A1 with
B2 and A2 with B1 (in addition to the normal comparisons between A1 with B1 and A2 with B2), and will count the resulting intersections as disagreements. It is necessary here to choose once and for all what unit to compare to what other, rather than to perform all the comparisons. But making such a choice precisely consists in making an alignment, which is a fundamental feature of  X  . Consequently, it seems that the alphas would need a structural modification to cope with overlapping.
 6.2 Discretizing Measure
As explained by Reidsma, Heylen, and Ordelman (2006), because of the lack of special-ized coefficients coping with unitizing, a fairly standard practice is to use categorization coefficients on a discretized (i.e., atomized) version of the continuum: For instance, each character (or each word, or each paragraph) of a text is considered as an item; and a standard categorization coefficient such as  X  is used to compute agreement. Such a mea-sure is called  X  d (for discretized  X  ) hereafter. Several weaknesses of this approach have been already mentioned in the state-of-the-art section. It is interesting to compare such a measure to the specialized one c | u  X  : even if they both bear the aggregatable hypothesis , they have, however, significant differences (as confirmed by the experiments presented in the next section). The main one is that c | u  X  does not use an artificial atomization of the continuum, and only compares units with units. In doing so, it is not prone to agreement on blanks, in contrast to  X  d . Another difference is that, for the same reason, inherently limited to non-overlapping units: Even if it is not currently designed to cope with them, as we have already seen, it is possible to submit overlapping units to this measure (some results are shown in the next section). 6.3 Benchmarking Using the Corpus Shuffling Tool In this section on benchmarking, we use the Corpus Shuffling Tool (CST) introduced by Mathet et al. (2012) to compare  X  concretely and accurately to the other measures. mistakes may occur), position (the boundaries may be shifted), false positives (the annotators add units to the reference units), false negatives (the annotators miss some of the reference units), and splits (the annotators put two or more contiguous units instead of a reference unit, which occupy the same span of text).
 types, and the metrics are compared with each other according to how they react to these disagreements. For a given error type, for each magnitude between 0 and 1 (with a step of 0.05), the tool creates 40 artificial, multi-annotator shuffled annotation sets, and computes the different measures for them. Hence, we obtain a full graph showing the behavior of each measure for this error type, with the magnitude on the x -axis, and the average agreement (over the 40 annotation sets) on the y -axis. This provides a sort of  X  X -ray X  of the capabilities of the measures with respect to this error type, which should be evaluated against the following desiderata:
Indeed, in most real annotated corpora, even when the overall agreement is high, errors 466 corresponding to all magnitudes may occur. For instance, an agreement of 0.8 does not necessarily correspond to the fact that all annotations are affected by slight errors (which correspond to magnitudes close to 0), but may for instance correspond to the fact that a few units are affected by severe errors (which may correspond to magnitudes close or equal to 1).
 disagreements concerning small units are as important as those concerning large ones.
However, it is provided as open-source (see Conclusions section) so that anyone can test and modify it, and propose new experiments to test  X  and other measures in the future. 6.3.1 Introducing the CST. The main principle of this tool is as follows. A reference corpus their prevalence, the minimum and maximum length for each category, and so forth.
Then, this reference is used by the shuffling tool to generate a multi-annotator corpus, simulating the fact that each annotator makes mistakes of a certain type, and of a certain magnitude. It is important to remark that the generated corpus does not include the reference it is built from.
 mistakes annotators make compared to the reference. It can be set from 0, which means no damage is applied (and the annotators are perfect) to the extreme value 1, which means annotators are assumed to behave in the worst possible way (but still being independent of each other) X  X amely, at random.
 some categorized units, three new sets of annotations are built, simulating three an-notators who are assumed to have the same annotating skill level, which is set in this example at magnitude 0.1. The applied error type is position only, that is to say that each annotator makes mistakes only when positioning boundaries, but does not make any other mistake (the units are reproduced in the same order, with the correct category, and in the same number). At this low magnitude, the positions are still close to those of the reference, but often vary a little. Hence, we obtain here a slightly shuffled multi-annotator corpus. Let us sum up the way error types are currently designed in the CST.
Position. At magnitude m , for a given unit, we define a value shift to m and to the length of the unit, and each boundary of the unit is shifted by a value randomly chosen between  X  shift max and shift max (note: at magnitude 0, because shift max = 0, units are not shifted).

Category. This shuffling cannot be described in a few words (see Mathet et al. [2012] for details). It uses special matrices to simulate, using conditional probabilities, progressive confusion between categories, and can be configured to take into account overlapping of categories. The higher the magnitude, the more frequent and severe the confusion.
False negatives. At magnitude m , each unit has the probability m to be forgotten. For instance, at magnitude m = 0 . 5, each annotator misses (on average) half of the units from the reference (but not necessarily the same units as the other annotators).
False positives. At magnitude m , each annotator adds a certain number of units (propor-tional to m ) to the ones of the reference.

Splits. At magnitude m , each annotator splits a certain number of units (proportional to m ). A split unit may be re-split, and so on. 6.3.2 Pure Segmentation:  X  , WD, GHD. Even if  X  was created to cope with error types that are poorly or not at all dealt with by other methods, and, moreover, to cope simultaneously with all of them (unitizing of categorized and overlapping categories), it is illuminating to observe how it behaves in more specific error types, to which specialized and well known methods are dedicated. We start with pure segmentation. Figure 24 shows the behavior of WD, GHD, and  X  for two error types.
 until magnitude 0 . 6. Their drawback is that their responses are limited by an asymptote, because of the absence of chance correction, while  X  shows a full range of agreements; for shifts , WD and GHD show an asymptote at about agreement = 0 . 4, while  X  shows values from 1 to 0. This experiment confirms the advantage of using  X  instead of these distances for inter-annotator agreement measure. 468 categories with given prevalences. The units are all of the same size, positioned in fixed, predefined positions, so that the focus is on categorizing only. It should be noted that, with such a configuration,  X  and  X  behave exactly in the same way as striking in Figure 25 that  X  behaves in almost the same way as values of these measures are exactly the same, the only difference coming from a slight difference in the expected values, due to sampling. Other tests carried out with the pure categorizing coefficient  X  yielded the same results on this particular error type, which means that  X  performs as well as recognized measures as far as categorizing is concerned, with two or more annotators. The u  X  curve goes below zero at magnitude 0.5 (probably for the reasons seen in Section 6.1.7). Moreover, its behavior depends on the size of the gaps: Indeed, with other settings of the shuffling, the curve may, on the contrary, be stuck over zero.  X  d fails to reach 0 because of the virtual agreement on gaps (but it would if there were no gaps). Lastly, SER (averaging the results of each pair of annotators) is bounded below by 0.6, which results from not taking chance into account. 6.3.4 Almost General Case: Unitizing + Categorizing. This section concerns the more gen-eral uses of  X  , combining both unitizing and categorizing. However, in order to be compliant with u  X  , c | u  X  , and  X  d , we limit the configurations here so that the units do not overlap at all. In particular, the reference was built with no overlapping units, and we have used a modified version of the shifting shuffling procedure so that the non-overlapping constraint is fully satisfied, even at high magnitudes.

Positional errors (Figure 26a). An important point is that this shuffling error type, which is based only on moving positions, has a paradoxical consequence on category agreement, since units of different categories align when sufficient shifting is applied. Conse-
Additionally, it starts to decrease from the very first shifts, as soon as units from different annotators start overlapping. This is a concrete consequence of what has been formally studied in Section 6.1.3.  X  has a most progressive response, reaches 0.1 at magnitude 1, and is the only measure to be strictly decreasing. SER immediately drops to agreement 0.5 at magnitude 0.05. As it relies on a binary positional distance, it fails to distinguish between small and large errors. This is a serious drawback of such a measure for most
CL tasks. Then it goes below zero and is not strictly decreasing. decreasing, but has some increasing parts, and, even more problematic, negative values from 0.6 to 0.9, probably because of the reason explained in Section 6.1.7.  X  responsive at the very first magnitudes, and is not strictly decreasing, probably because it  X  X oes not compensate for differences in length of segments X  (Reidsma, Heylen, and Ordelman 2006, page 3).
 alphas are not strictly decreasing, and once again u  X  drops below 0 from magnitude 0.6 onwards.  X  d is not strictly decreasing (again, probably because it  X  X oes not compensate for differences in length of segments X ), but its general shape is not that far from  X  .
Split errors (Figure 27). The split error type would need to create an infinite number of splits to mean pure chaos at magnitude 1. As this is computationally not possible, we restricted the number of splits to five times the number of units of the reference. We should therefore not expect measures to reach 0. In this context,  X  shows a good range of responses, from 1 to 0.2, in an almost linear curve. SER is also quite linear, but gives very confusing values for this error type because it reaches negative values above as expected, and remain blocked at 1 (which is normal for categorizing).

False positives and false negatives (Figure 28). In the current version of the CST, the false positive error type creates some overlapping (new units may overlap), and it is the reason why u  X  and  X  d were discarded from this experiment. However, we have kept  X  because it behaves quite well despite overlapping units. 470 shown in Figure 28a, even if the shape of c | u  X  is delayed compared with the others, but it should be pointed out that SER has a curious and unfortunate final increasing section (not visible in the figure because this section is below 0).
 is still strictly decreasing and almost reaches 0 (0.025), but and is at 0 or below from m = 0 . 3; SER quickly drops below 0 from m = 0 . 4,  X  strictly decreasing, and c | u  X  , as for splits, does not react at all but remains stuck at 1, which is desired for this coefficient focused on categories (values of are missing since there are not enough intersections between units for this measure to work).

Overview of each measure for the almost general case. In order to summarize the behavior of each measure in response to the different error types for the almost general case (with-out overlap), we pick all curves relative to a given measure out of the previous plots and draw them in the same graph, as shown in Figure 29. Briefly,  X  shows a steady behavior for all error types, almost strictly decreasing from 1 to 0. and negative values and is sometimes not responsive. c | u error types, is less responsive for some other types, and is sometimes not responsive at all (which is desired, as already said). SER has unreliable responses, being either too responsive (reaching negative values) or not responsive enough. Finally,  X  always responsive, is most of the time not strictly decreasing, but is sometimes quite progressive. general case, where overlapping of units within an annotator is allowed. In this ex-periment, we took a reference corpus with no overlap, but the errors applied (combi-nation of positioning and false positives) progressively lead to overlapping units. The results are shown in Figure 30. As expected,  X  behaves quite the same as it does with non-overlapping configurations. Admittedly, c | u  X  was not designed to handle these configurations (and so should not be included in this experiment), but surprisingly it 472 seems to perform in rather the same way as it does with no overlapping; this must be investigated further, but judging from this preliminary observation, it seems this coefficient could still be operational and useful in such cases. On the contrary, not handle correctly this experiment and so was not included in the graph. 7. Conclusion
The present work addresses an aspect of inter-annotator agreement that is rarely studied in other approaches: the combination of unitizing and categorizing. Nevertheless, the use of methods that have been transposed from other domains (such as  X  , which was originally dedicated to pure categorizing) in CL, for example at the discourse level, leads to severe biases, and manifests the need for specialized coefficients, fair and meaningful, suitable for annotation tasks focusing on complex objects.
 we expressed in the introduction, with the restriction that they are natively limited to non-overlapping units.
 enlarge Krippendorff X  X  coefficients to overlapping units, probably results from the fact that we are facing here a major difficulty: the simultaneous double discrepancy between annotators, with annotations possibly differing both in positioning relevant units any-where on a continuum, and in categorizing each of these free units. Consequently, it is difficult for a method to choose precisely which features to compare between different annotators (unlike pure categorizing, where we know exactly what each annotator says for each predefined element to be categorized); and this problem is exacerbated when overlapping units (within an annotator) occur.
 expresses which unit from one annotator should be compared to which unit, if any, from another one, and consequently makes it natural and easier to compute the agreement.
Moreover, we have shown that this alignment cannot be done in an independent way, but is part of the measure method itself. This is the  X  X nified X  aspect of our approach.
We have also shown that in order to be relevant, this alignment cannot be done at a local level (unit by unit), but should consider the whole set of annotations at the same time, which is the  X  X olistic X  aspect.
 is highly configurable to cope with different annotation tasks (in particular, boundary errors should not necessarily be considered the same for all kinds of annotations), and it provides the alignment that emerges from an agreement measurement. Not only quickly from a multi-annotated corpus (by listing all unitary alignments, and for each of them showing the corresponding frontiers and category proposed by each annotator), alignments gives crucial information on the choices the measure makes and whether it needs to be adjusted, unlike other methods which only provide a sole  X  X ut of the box X  value.
 specific domains (pure categorization, pure segmentation), through a specific bench-mark tool (namely, CST) which scans the responsivity of the measures to different kinds of errors and at all degrees of severity. Overall,  X  provides broader and more progressive responsivity than the others in the experiments shown here. Concerning pure categorizing,  X  does not have an edge over the well-known coefficients, such as  X  , but it is interesting to see that it behaves in much the same way as others in this specific field. Concerning segmentation,  X  outperforms WD and GHD, by taking chance into account, but also by not depending on the heterogeneity of the segment sizes.
Concerning unitizing with categorizing, as theoretically expected and confirmed by the benchmarking, SER shows severe limitations, such as a binary response to various (small or severe) positional or categorial errors, the fact that it does not make chance correction, or its limitation to two annotators only. Krippendorff X  X  coefficients  X  present very interesting properties, such as chance correction. However, as we have shown with thorough comparisons, they rely on quite different hypotheses to ours, since they consider intersections between units whereas we advocate considering aligned units. We have identified several situations in CL where considering alignments is advantageous, for instance, when contiguous segments of the same type may occur, or when errors on several short units should be considered as more serious than one error on a long unit, but we do not posit these situtations as a universal rule. In conclusion, when unitizing and categorizing involve internal overlapping of units, only  X  is cur-rently available, and, even if it cannot be compared to any other method at the moment for this reason, benchmarking reveals very similar responses to overlapping configura-tions and to non-overlapping ones, which already demonstrates its consistency and its relevance.
 eties of unitizing, combines unitizing and categorizing simultaneously, enables any number of annotators, provides chance correction, processes an alignment while it measures agreement, and provides progressive responsivity to errors both for uni-tizing and for categorizing. This makes  X  suitable for annotation tasks such as rela-S http://gamma.greyc.fr Web site. It is already compatible with annotations created with the Glozz Annotation Platform (Widl  X  ocher and Mathet 2012), and with annotations generated by the Corpus Shuffling Tool. 474 Appendix A. Examples of Linguistic Objects and Possible Annotation Tasks Terms emphasized hereafter refer to the terminology defined in Section 2.
 P
ART -OF -S PEECH . Part-of-speech (POS) tagging (see, for example, G  X  ung  X  or [2010] for a recent state of the art) gives a well-known illustration of a pure categorization without annotators have to select a category, belonging to quite a small set of exclusive elements. POS units (words) having the same label are obviously not aggregatable .
 G
ENE R ENAMING . In a study on gene renaming presented in Fort et al. (2012), all the default value),  X  X ormer X  (the original name of a gene) or  X  X ew X  (its new name). This work at word level considers sparser ( sporadic ) phenomena than POS tagging. However, the presence of the  X  X othing X  category also reveals here the reduction of a unitizing problem (detection of renaming) to a pure coding system ( categorization ). These units are not aggregatable .
 W
ORD S ENSE . For the annotation task described in Passonneau et al. (2012), annotators were asked to assign sense labels ( categorization without unitizing ) to preselected moder-ately polysemous words ( sporadicity , predefined units , no overlap ) in preselected sentences where they occur. Adjacent words are not aggregatable with sense preservation. N
AMED E NTITIY . Well-established named entity (NE) recognition tasks (see, for exam-ple, Nadeau and Sekine [2007]) led to many annotation efforts. In such tasks, the anno-tator is often asked to identify the units in the text X  X  continuum ( unitizing , sporadicity ) and to select a NE type from an inventory ( categorization ). It is well known that some difficulties of NE annotation relate to the delimitation of NE boundaries. For example, for a phrase such as  X  X r X, the President of Y, X  it makes sense to annotate subparts ( X  X , X   X  X r X, X   X  X he President of Y X ) and/or the whole.  X  X  X  is also a NE of another type. This may result in hierarchical or free overlapping structures. Adjacent NE are not aggregatable . A RGUMENTATIVE Z ONING . Studies concerned by argumentative zoning (Teufel 1999;
Teufel, Carletta, and Moens 1999; Teufel and Moens 2002) consider the argumenative structure of texts, and identify text spans having specific roles. For each sentence ( full-sentences of the same type are aggregated into larger spans (argumentative zones). This reveals an underlying question of unitizing . However, it has to be noted that the catego-rization mainly concerns predefined sentences: argumentative types are aggregatable . D
ISCOURSE F RAMING . In Charolles et al. X  X  discourse framing hypothesis (Charolles number of propositions which are linked by the fact that they must be interpreted with reference to a specific criterion, realized in a frame-initial introducing expression. X  Thus, temporal or spatial introducing expressions lead, for example, to temporal or spatial discourse frames in the text continuum ( unitizing , sporadicity , categorization ). Discourse frames are not aggregatable . Subordination is possible, leading to possibly hierarchical overlap , where frames (of the same type or of different types) are embedded. C
OMMUNICATIVE B EHAVIOR . The multimodal AMI Meeting corpus (Carletta 2007) covers a wide range of phenomena, and contains many different layers of annotation describing the communicative behavior of the participants of meetings. For example, in Reidsma (2008), annotators are required to identify fragments in a video recording task, one can easily imagine instruction manuals allowing annotators to use multiple labels and to identify embedded ( hierarchical overlap ) or free overlapping units, even if the example provided by Reidsma (2008) does not.
 D
IALOG A CT . Annotating dialog act conforming to a standard as defined, for example, in Bunt et al. (2010), leads annotators to assign communicative function labels and types of semantic content ( categorization ) to stretches of dialogue called functional segments.
The possible mulitfunctionality of segments (one functional segment is related to one or more dialog acts), and the fact that annotations may be attached directly to the primary data such as stretches of speech defined by begin and end points, or attached to structures at other levels of analysis, seems to allow different kinds of configurations and annotation instructions: unitizing or pure categorization of pre-existing structures, sporadicity or full-covering , hierarchical , overlapping or linear segmentation . T OPIC S EGMENTATION . Topic segmentation (see, for example, the seminal work by
Hearst [1997] or Bestgen [2006] for a more recent state of the art), which aims at detecting the most important thematic breaks in the text X  X  continuum, gives an illuminating ex-ample of pure segmentation. This unitizing problem of linear segmentation is full-covering and restricted to the detection of breaks (the right boundary of a unit corresponds to the left boundary of the following segment) ( no overlap ). If we consider the resulting segments, there is just one category (topic segment) and then no categorization . Adjacent topic segments are obviously not aggregatable without a shift in meaning.
 H
IERARCHICAL T OPIC S EGMENTATION . In order to take better into account the fact that lexical cohesion is a multiscale phenomenon and that discourse displays a hierarchical structure, hierarchical topic segmentation proposed, for example, by Eisenstein (2009) covering , not aggregatable segments ), but allows a topic segment to be subsegmented into sub-topic segments ( hierarchical [but not free] overlap ).
 T
OPIC T RANSITION . The topic zoning annotation model presented in Labadi  X  e et al. (2010) is based on the hypothesis that, in a well constructed text, abrupt topic bound-aries are more the exception than the rule. This model introduces transition zones ( uni-tizing ) between topics, zones that help the reader to move from one topic to another. troduction, conclusion, and transition zones. Hierarchical overlap is possible (embedded elements of the same type or of different types are allowed). Free overlapping structures are frequent, by virtue of the nature of transitions. Adjacent topic zones and adjacent transition zones are not aggregatable .
 E
NUMERATIVE S TRUCTURES . A study on complex discourse objects such as enumer-ative structures (Afantenos et al. 2012) illustrates both the need for sporadic unitizing and the need for categorization . The enumerative structures have a complex internal organization, which is composed of various types of subelements ( hierarchical overlap ) (a trigger of the enumeration, items composing its body, etc.) which are not aggregatable . 476 Acknowledgments References 478
