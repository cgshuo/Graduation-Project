 Query suggestion is a useful tool to help users formulate bet-ter queries. Although this has been found highly useful glob-ally, its effect on different queries may vary. In this paper, we examine the impact of query suggestion on queries of differ-ent degrees of difficulty. It turns out that query suggestion is much more useful for difficult queries than easy queries. In addition, the suggestions for difficult queries should rely less on their similarity to the original query. In this paper, we use a learning-to-rank approach to select query suggestions, based on several types of features including a query perfor-mance prediction. As query suggestion has different impacts on different queries, we propose an adaptive suggestion ap-proach that makes suggestions only for difficult queries. We carry out experiments on real data from a search engine. Our results clearly indicate that an approach targeting diffi-cult queries can bring higher gain than a uniform suggestion approach.
 H.3.3 [ Information Search and Retrieval ]: query for-mulation Algorithms, Experimentation, Measurement Difficult queries, adaptive query suggestion, query sugges-tion evaluation
It is often difficult for users to compose appropriate queries in Web search. Even if a query expresses well the right in-formation need from the point of view of human beings,  X 
The work was done when the first author was visiting Mi-crosoft Research Asia.
 the query can still fail to retrieve the desirable documents. This is the case for the query  X  X hat X  X  in fashion X , which clearly describes the information need on learning the cur-rent fashion trends. However, most top search results are irrelevant, as shown in the top results from Google ( http: //www.google.com ) in Table 1: the second result is irrele-vant while the third one is not authoritative enough. The query is difficult because the current search methods can-not find the desired documents effectively. To be general, we consider all the queries with a low NDCG score difficult queries as in [17]. The low NDCG scores could be due to several reasons: the query X  X  key terms mismatch those in the desired documents; the query is too general, the query is too narrow, and so on. No matter what the underlying reason is, one can alleviate the problem by suggesting appropriate alternative queries to the user, which can perform better in search.

Query suggestion is a technique which can assist users to interactively refine queries. However, most previous work on query suggestion, such as [29, 23, 7, 8], tries to identify alternative queries that bear a strong similarity or relevance to the original query. For easy queries, the suggested queries can often result in good search results, as is also the case for the original query. In such cases, query suggestion is less crucial, and can even be annoying sometimes (imagine, for example, the case when a user submits a very good query, but becomes uncertain when a number of alternatives are suggested). However, for difficult queries, it is critical but much harder to suggest queries that perform well. For ex-ample, for the same example query  X  X hat X  X  in fashion X , the suggestions  X  X hat X  X  in fashion 2010 X  and  X  X hat X  X  in fashion for men X  are both relevant , but they are not more effective than the original query, as will be shown in Table 4. To be useful, a suggested query should not only be relevant to the original query, but also allow to better retrieve the desired documents.

Query reformulation is another technique aiming at im-proving the relevance of search results. Different from query suggestion, query reformulation is performed automatically on the original query without explicit interactions with the user. In such a setting, one should make sure that the au-tomatic reformulation has a high precision. In practice, it is often limited to replacing some original query terms by correcting misspelled words, or suggesting some more fre-quently used terms that are slightly different from the origi-nal terms in morphology. However, such a replacement does not apply to the difficult queries we target in this paper such Google in November 4, 2011.
 as  X  X hat X  X  in fashion X , in which all the words are spelled cor-rectly and are quite frequent.

In this paper, we do not intend to design a system that solves the problem of difficult queries alone; rather we pro-pose an interactive suggestion method that adaptively sug-gests alternative queries when this is necessary or useful, namely for difficult queries. The goal is to suggest more ef-fective queries for them. As an example, a good suggestion for the original query  X  X hat X  X  in fashion X  is  X  X atest fashion trends X , which both corresponds well to the original search intent and leads to better search results, as shown in Table 1.
As we hinted earlier, query suggestion can be useful in some cases (especially difficult queries), but annoying in some others (especially easy queries). It is then natural to determine when it is useful to perform query suggestion and to use it only for the useful cases. To do this, we use a regres-sion model to predict the retrieval performance of queries and then return suggestions according to the query difficulty. Experimental results show that our adaptive query sugges-tion approach significantly improves the uniform query sug-gestion.

In addition to proposing an effective query suggestion method that targets difficult queries, we also propose some new measures to evaluate the quality of query suggestion, which we call Max@n (the maximal NDCG using the first n suggestions) and SDCG@n (the discounted cumulative gain using the first n suggestions). Compared to the measures used in previous studies, these measures can better reflect the use of query suggestions by end users.

The contributions of this work are twofold. First, to our best knowledge, it is the first time that query suggestion is studied specifically to help difficult queries. Second, we successfully demonstrate that our adaptive approach signifi-cantly improves the retrieval effectiveness of original queries and outperforms the current state of the art.

The remainder of this paper is organized as follows. We briefly review related work in Section 2. We propose two new evaluation measures in Section 3. Then, we describe our proposed approach for difficult queries in Section 4 and evaluate the approach in Section 5. The adaptive query suggestion approach is described and tested in Section 6. Finally, we present concluding remarks and future work in Section 7.
Our work is related to query suggestion, query reformula-tion and query performance prediction. We will review some work in these three areas.
In the past decade, many approaches have been proposed to perform query suggestion. Much work takes advantage of click-through information from query logs and leverages co-clicked URLs to identify related queries. For example, [6] constructed a bipartite graph based on click-through and clusters similar queries by assuming that queries for which the same documents are clicked on are similar. Here, clicked documents are considered as representing the user X  X  search intent. [32] further extended the approach by also taking into account the content words of the queries. Therefore, queries in the same cluster are both similar to each other and share the same search intent. Baeza-Yates et al. [2] proposed to cluster similar queries by considering queries alongwiththetextoftheirclickedURLs. Givenanini-tial query, the queries from its cluster can be considered as possible suggestions.

In recent years, other types of useful information have been included into the bipartite graph. Mei et al. [24] proposed to compute hitting time on a large-scale bipar-tite graph mined from click-through data. Then candidate queries are ranked by the hitting time. Cao et al. [8] clus-tered queries into concepts and then by mapping user query sessions to concept sessions, they enhanced query sugges-tion by considering enriched contextual information. In [23], the authors proposed a query suggestion framework includ-ing two parts: The offline part builds a query similarity graph by user-query and query-click bipartite graphs; The online part applies a ranking algorithm to the query simi-larity graph and then suggests latent semantically relevant queries to users. To tackle the problem of rare query sugges-tion, Song et al. [28] built two bipartite graphs by leveraging both click and skip information from query logs and used an optimal random walk and combination model to determine query correlations.

Most of these approaches focus on enhancing user search experiences by providing related queries to expand searches [29]. The evaluation measure commonly used reflects whether the suggested queries are relevant to the original query. As we argued, a useful query suggestion is not only relevant, but also more effective. This is particularly important for diffi-cult queries. To cope with this requirement, we will propose a new evaluation methodology in this paper.
Query reformulation techniques are widely used to modify user queries in order to improve retrieval effectiveness. Tra-ditional methods include pseudo relevance feedback, which adds some terms extracted from the top ranked documents into the query [22, 30, 33]. Recent work exploits query logs to collect reformulation candidates. In [20], the authors identify query-level and phrase-level candidate substitutions for user queries by mining session data. Candidates are ranked according to two relevance measurements, i.e.,  X  X re-cise rewriting X  and  X  X road rewriting X . To address the prob-lem of term mis-specification and under-specification within a query, Wang and Zhai [31] define two novel term associ-ation patterns, i.e., context-sensitive term substitution and term additions, and propose a method to discover these pat-terns by analyzing term co-occurrences in query logs. Gao et al. [15] propose a ranker-based search query speller that gathers correction candidates from query logs and then use a ranker trained from manually annotated data to rank the candidates. Guo et al. [16] propose a unified CRF model for query refinement by incorporating four independent tech-niques for query correction.

Anchor text is an alternative data source for query refor-mulation. It has been observed that there is a similarity between search queries and anchor texts [13]. Therefore, Kraft and Zien [21] propose a method to generate refine-ments for queries by mining anchor texts. They also employ a ranking algorithm for combining multiple factors to se-lect query refinements. By using anchor texts to simulate click-through data of query log, Dang and Croft [12] employ the approaches described in [25, 31] to reformulate queries. Their results show that anchor texts are at least as effective as a real query log for this purpose.

The refinement work usually outputs alternative queries that will most likely change the user query X  X  search results. To ensure high precision, the previous work often performs changes that are safe and hesitates to do risky refinements. The same technique can be hardly applied to difficult queries, for which more different suggestions are needed.
Query performance prediction is the task of estimating the quality of the search results for a query. Recently, a number of predictors have been proposed for this task [11, 1, 34, 10, 35, 36]. Carmel et al. [9] conducted a comprehen-sive comparison among these predictors over several TREC benchmarks and further discussed several methods for comb-ing different predictors to obtain performance enhancement. Different from previous predictors evaluated on TREC-like collections, Balasubramanian et al. [5] propose an effective and efficient query performance prediction technique for real Web search that uses aggregates of retrieval scores and re-trieval features. Our work is in the same setting. Therefore, we will use a similar approach to determine difficult queries in Section 6.
In this section, we first formulate the problem of query suggestion for difficult queries. Then we define measures to evaluate how good a suggestion list is for a difficult query.
Query suggestion can be formulated as a two-step pro-cess: First, given a query q , a set of candidate queries C = { c 1 ,c 2 ,...,c m } for suggestion are identified; then the candidates are ranked according to some quality criterion. Similar to the Probability Ranking Principle (PRP) [27], the best suggestion list can be seen as the one in which the suggested queries are ranked in decreasing order of their rel-evance probability P ( rel =1 | q,c ), in which rel =1means relevance, q is the original query and c refers to a candidate. We then choose at most n ( n&lt;m ) top candidate as sugges-tions to q , denoted by C s = &lt;c s 1 ,c s 2 ,...,c s n problem is to find an optimal function r ( q,c i )toestimate P ( rel =1 | q,c i ).
 This is the standpoint taken in many previous studies. The measurements proposed are based on direct human judg-ments [29, 23, 7, 28, 8] or indirect human judged resources like Open Directory Project (ODP) data [23, 3]. For exam-ple, for each pair of the original query and a suggestion, as-sessors are given the queries and their corresponding search result pages side-by-side, and asked to make a binary deci-sion of whether two queries are related [29].

As we discussed previously, a useful suggestion should be the one that improves the search effectiveness. Therefore, we change the previous relevance probability to the usefulness probability P ( useful =1 | q,c ). For example, although both  X  X hat X  X  in fashion for men X  and  X  X atest fashion trends X  are relevant to the query  X  X hat X  X  in fashion X , the latter is more effective, thus its usefulness is higher. We will define two measures to reflect the usefulness in the next subsection.
Some measures in a similar vein have been used in query reformulation. For example, [12] used the metric preci-sion@n, which is the fraction of top n retrieved documents that are relevant, to measure the quality of a refined query. In this paper, we will assume that our goal is to improve NDCG@k. Accordingly, we propose two measurements Max@n and SDCG@n to measure the effectiveness of a suggestion list.
We use the judged document set of an original query q , which is pooled from several popular search engines, to eval-uate a query suggestion c s i . Instead of precision@n, we choose NDCG to evaluate an individual suggestion, which is commonly used for Web search. Given an original query q , we have a set of documents, denoted as D , judged by human assessors. Then, NDCG@k is defined as DCG @ k IDCG where DCG@k is calculated by as follows:
Here rating ( i ) is the relevance rating of the document at position i . In our experiments, we have five grades of relevance, i.e., perfect, excellent, good, fair, and bad, cor-responding to the ratings 4, 3, 2, 1, and 0, respectively. IDCG@k is the ideal discounted cumulated gain which is produced by the perfect ordering of D .

We evaluate the retrieval performance of a suggestion c s i by calculating NDCG@k measure for the list of retrieved documents S ( c s i ) based on the judgments of D .Notethat although on average there are more than 100 documents judged for each original query, there are still some un-judged documents retrieved by suggested queries. We regard these un-judged documents as irrelevant, or bad, as is commonly done in IR evaluation.
Based on the NDCG measure of a single suggestion, we can measure the quality of a suggestion list. Usually search engines show at most ten suggestions for a query in rows at the bottom of search results or a list on the left side bar. The suggestion list is usually short so that the users can easily go through it and choose a suggestion that looks promising. Given a list of n suggestions, it is impossible to tell which of them will be selected by the user. Therefore, we use the maximum NDCG@k achievable by these n suggestions, denoted by Max@n, as a quality measure of the list.
For example, for an original query, the NDCG@3 values of its top five suggestions are: Then Max@1 is 0.4; Max@2 is 0.6; Max@3 is 0.6; Max@4 is 0.7; and Max@5 is 0.7. By connecting these Max@n points, we can use a monotonically increasing curve to describe these values.

We also use the metric SDCG@n to measure the over-all quality of a suggestion list, which assumes that the user scans the suggestion list from top to bottom. This is sim-ilar to the assumption used in the general DCG measure. SDCG@n is defined as follows:
Here n is the total number of suggestions in a suggestion list and NDCG@k( i ) is the quality of the suggestion at po-sition i .

As search engines often return less than ten suggestions for a query, it is unfair to compare them with our methods if they return less suggestions. Thus, we choose Max@n and SDCG@n with n  X  5 as our main metrics and use the queries with at least five suggestions in our experiments.
In this section, we describe our approach to address the problem of query suggestion for difficult queries. Figure 1 shows the work flow of our approach.

Given a query q , we first retrieve candidates based on clicks and query terms. Then we use a search system to ob-tain search result pages for the original query q and candi-dates { c 1 ,c 2 ,...,c m } 1 . For each pair of q and c i features to predict how well the candidate will perform. Our proposed features are extracted from queries, candidates, and their search result pages. Next, we apply a ranking model f ( x c i )toestimate P ( useful =1 | q,c i ). The model is learned on a training set composed of difficult queries. Fi-nally, we sort the candidates by their estimated scores and return top n candidates as suggestions.

We will describe the above processes in detail in the fol-lowing subsections.
Candidates can be collected from many data sources. In our work, we mine query clusters from click-through data and retrieve candidates based on query clusters.

First, we build a click-through bipartite graph from the search logs collected on Bing ( http://www.bing.com )from
Note that the search results of the candidates can be prefetched offline. q
S(q) Search Figure 1: Flowchart of generating suggestions for a query January 1st to March 25th, 2010. An edge e ij is created between a query node q i and a URL node u j if u j has been clicked when users issued q i .Theweight w ij of edge e ij the aggregated click number. The query q i is then repre-sented as an L 2 -normalized vector, in which each dimension corresponds to a URL. If edge e ij exists, the value of the dimension is normalized w ij ;otherwise,itiszero. Second, we apply the clustering method proposed in [8]. The algorithm creates a set of clusters as it scans through the queries. For each query q , the algorithm first finds the closest cluster C , and then tests the diameter of C  X  X  q If the diameter is not larger than D max , q is merged into C . Otherwise, a new cluster is created with q as its first member.

Third, we divide queries in each query cluster into intent groups. When examining query clusters, it is observed that the search intents of some queries in a query cluster are al-most the same. Despite the slight difference in their forms, they can be safely treated as duplicates to one another. This phenomenon is mainly caused by misspellings, with or with-out stop words, different tenses, equivalent syntax, and so on. For example,  X  X et bleu X ,  X  X etbue X  and  X  X etb X  share the same intent with  X  X etblue X . It is annoying if we show these misspelled or duplicate queries as suggestions. To solve this problem, we apply a sequence of transformation operations, such as spelling correction, stop words removal, stemming, and term sorting, to all queries in each cluster and then group two queries together if their edit distance after the transformation operations is less than a threshold. Then, we select the most frequent query in each group to be the group leader, which will be returned as a candidate on behalf of the whole group.

Givenanoriginalquery q , we identify the query-cluster map to find the cluster containing q . All the group leaders in the cluster are returned as candidates for q .
We extract some features to measure how well candidate c performs. These features will be used within a learning-to-rank framework (see the next section). The features are extracted from q,S ( q ) ,c i ,S ( c i ) ,where S ( q )and S ( c search results returned by Bing for q and c i respectively. In our experiments, we use the first page containing top ten search results for each query.

Our proposed features can be divided into four categories: match features, cross match features, similarity features and an estimated NDCG feature.
The match features aim to measure how well a candidate matches its own search result. Our intuition is that only well-matched candidates are qualified to be suggestions.
Match features are extracted from c i and S ( c i ). As each result in S ( c i ) is usually composed of three parts: title, snip-pet, and URL, we calculate match features for each of them. Take title as an example. Given the candidate c i and a title T i,j , which is the title of the j -th result in S ( c i ), we first re-move stop words by Fox X  X  list [14] and stem terms by Porter Stemmer [26]. Then we count how many times a term t k of c occurs in the title and normalize the term frequency (TF) by the length of title in words:
Finally, we calculate the feature of TitleMatch by ag-gregating all titles from the top N results and discounting MatchScore by the position of the title as follows: Similar to TitleMatch, we also calculate the features of SnippetMatch and URLMatch. The three features compose our match features.
The cross match features aim to measure how well a candi-date X  X  search result S ( c i ) matches the original query q .The intuition is that a search result S ( c i )thatmatchesbetter the original query is more likely to correspond the original query X  X  information need.

Cross match features are extracted from q and S ( c i ). Sim-ilar to the match features, we calculate three features based on title, snippet, and URL. For example, MatchScore ( q,T represents the cross match feature on the title of the j -th re-sult in S ( c i ). TitleCrossMatch can be computed as:
SnippetCrossMatch and URLCrossMatch can be computed similarly.
The similarity features are other features trying to mea-sure the similarity between a candidate and the original query. This similarity is measured on the candidate X  X  search result S ( c i ) and the original query X  X  search result S ( q ). On the one hand, as we want S ( c i ) to satisfy the original query X  X  information need, the topic of S ( c i ) should not drift too much away from S ( q ). On the other hand, S ( c i ) would fail to provide new and better results if it is too similar to S ( q ).
To cover the two aspects, we extract three similarity fea-tures on the result pages, URLs and domains. PageSimilar-ity tries to prevent from a possible topic drift. It is calcu-lated by the cosine similarity between the vectors of S ( q )and S ( c i ). The vectors are formed by terms in the search results after stopword removal and stemming. TF-IDF formula is used to weigh the terms. URLSimilarity is estimated by the number of common URLs between S ( q )and S ( c i )andDo-mainSimilarity is based on the number of common domains, which are derived from URLs, between the two search re-sults.
As we discussed earlier, difficult queries are more in need of suggestions. The estimated NDCG feature tries to cap-ture, to some extent, how relevant a set of search results is to the original query, by which we want to reflect the difficulty of the query. This estimate is made using the sets of search results from different query formulations. The intuition is that, if a document appears in the top search results of many candidates, a.k.a. voted by many candidates, it is very likely to be relevant. Therefore, an estimate of relevance can be obtained for each search result.

Given q ,thetopsearchresultsof q  X  X  candidates form a search result collection m i =1 S ( c i ). For each unique docu-ment in the collection, we count the number of times that the candidates return it among top ten as the document X  X  estimated relevance ratings. For a candidate c i ,everydocu-ment in S ( c i ) will have an estimated rating. Consequently, we can calculate the estimated NDCG for the candidate us-ing Equation 1, in which rating ( i ) is the estimated relevance rating instead.
We use a pairwise learning-to-rank method, RankSVM [19] to rank the candidates. RankSVM focuses on the rela-tive order between two items in a ranking list and its ob-jective of learning is to directly minimize the number of item pairs with reverse order. Given a list of candidates, RankSVM outputs the prediction score for each candidate, which can be used to rank candidates by sorting them on the prediction score in descending order.

Formally, suppose a candidate set C = { ( x i ,y i ) | x i ,y i  X  R 1 } and let y i be the retrieval performance, e.g., NDCG@3, of a candidate x i . The ranking function has the form: where K (  X  ) is the kernel function. In our experiments, we choose the RBF (radial basis function) kernel. Notice that RankSVM tries to rank queries according to its search effectiveness ( y i ) rather than their relevance or relatedness to the original query. A higher-ranked candidate is the one that is more effective in search. This allows us to capture the desired usefulness.

We define the error function for incorrect pairwise order-ing as follows [4]:
 X 
Then, the optimal ranking function f  X  can be learned by minimizing the overall ranking errors:
Note that we only use the difficult queries, which perform worse than a threshold (e.g., 0.4 in this paper) in terms of NDCG@3, in a training set to learn ranking models. In this section, we conduct experiments on a set of real Web search queries to evaluate the effectiveness of our pro-posed approach and compare it with several baseline ap-proaches.
The dataset collected contains about 10,000 real Web queries from the search logs of Bing. On average, each query has more than 100 documents judged in five relevance grades. As our evaluation will use the top 5 suggestions, to be fair, we discard the queries for which the baseline approaches or our approach returns less than five suggestions. Finally we have 4,068 queries for experimentation.

We fetch top three search results from Bing for each of the original 4,068 queries, and calculate NDCG@3 to mea-sure how difficult each original query is. We divide the original queries into 10 bins according to their NDCG@3 in each bin is shown in Figure 2. As we can see, there are a quite large number of queries (1,019) in the four lowest bins below 0.4. These queries account for around 25% of the total queries. Their low NDCG values indicate that it is difficult to retrieve relevant documents for them with the original queries, and they need better suggestions to improve search. Figure 2: Distribution of original queries in different bins of retrieval performance
For all the 4,068 original queries, our approach identifies 638,391 suggestion candidates from the three-month search log. On average, each original query has about 157 candi-dates. Again, we fetch the top three search results from the search engine for each candidate and compute their NDCG@3 values based on the relevance judgments of the original query. If a candidate has a higher NDCG@3 value than its origi-nal query, it is an improved candidate . Figure 3 shows the average number of candidates and improved candidates per original query in each NDCG bin. As we can see, there are a larger number of improved queries in the lower bins than in the higher bins. For the [0 , 0 . 1) bin, each query has 33 improved candidates on average. The number of improved candidates decreases progressively as the original queries be-Figure 3: Number of candidates per query and num-ber of improved candidates per query in different bins come easier. For the original queries in the bins of [0 . 8 , 0 . 9) and [0 . 9 , 1], almost none of their candidates is better than the original ones. This is not a surprise because search en-gines already perform very well in returning relevant docu-ments for the easy queries, and hence there is less space left for further improvement. The above observation supports our conjecture that query suggestion is more necessary for difficult queries. It is on difficult queries that we can obtain large improvements on search effectiveness.

As additional baselines, we also evaluate the suggestions from two commercial search engines, SE1 and SE2, for the original queries. For a suggestion from SE1 or SE2, we fetch the top three search results from them to evaluate its effec-tiveness in NDCG@3 .
We first conduct experiments to investigate the ranking models that are learnt from different types of features. The 4,068 original queries are randomly divided into ten subsets, and we conduct ten-fold cross validation for all learning ex-periments in this paper. In each trial, the queries in nine subsets are used as the training set, and the remaining sub-set as the testing set. In order to train a RankSVM model specifically for difficult queries, we only preserve the queries whose NDCG values are below 0.4 in the training set. For testing, however, we use all the original queries in order to reflect the fact that we cannot know their NDCG values in advance. The reported performance measures are averaged on the ten trials.

Figure 4 shows the results measured by Max@1 for the first six bins. Due to limited space, we do not show the results for original queries in other bins and other Max@n, but the trend is similar and has been clearly presented in Figure 4. First, we find that cross match features perform consistently better than match features. This observation is intuitive because the cross match features capture how well a candidate matches the original query. Second, we observe that the estimated NDCG feature is the most sig-nificant type in the bin of [0 , 0 . 1), more effective than the similarity measures. This is interesting and consistent with our assumption that for difficult queries we need to suggest queries that are more different from the original queries. In contrast, when queries become easier, the similarity mea-sures stand out as the most effective criteria. This is also intuitive  X  easy queries do not need suggestions that are very different from the original ones; otherwise, there is a high risk of topic drift. However, we also observe that the combined approach that uses all types of feature ( All )does not always outperform the one using the similarity measures only, namely in the bins [0 . 3 , 1]. This can be explained by the fact that the models are trained on the difficult queries only (from bins below 0.4). During the training process, some of the features may be over-used and their possible negative impact on the queries in other bins may not be ob-served during the training. It would be more appropriate to use all the queries in the training in order to separate a model for difficult queries from the one for easy queries. We will leave this to future work.

The very good performance of similarity measures for easy queries does provide a good indication that these measures can be used as a good model for easy queries. It is then intuitive to combine the model trained on all the features (which performs well on difficult queries) and the one with similarity features only. Figure 4: Evaluating ranking models based on differ-ent type of features over original queries in different bins
We propose using a variant of Borda X  X  ranking fusion method [18] to combine the two models as follows: where r a is the position of candidate c in the list of sug-gestions returned by the model based on all features, and r is the position of c in the list returned by the model based on similarity features.  X  is a trade-off parameter to balance the two models. The experimental results show that for a wide range of  X  , from 0.3 to 0.7, the fusion outperforms both models. In our remaining experiments, we empirically set  X  to 0.5.

The fusion method is consistently the best among our pro-posed methods as shown in Figure 4. This indicates that the simple fusion method is capable of taking advantage of both models for difficult and easy queries. We also calculate the difference between the fusion method and the two separate models in terms of Max@1-5 for all queries and SDCG@5 for queries in different bins. The results are shown in Ta-ble 2 and Table 3 respectively. In all the cases except one, the fusing approach improves the two separate models, and the improvements are statistically significant. Therefore, we use the fusion method as our query suggestion approach in remaining experiments.
We compare our fusion approach with several baseline ap-proaches. We collect suggestions from SE1 and SE2 as two additional baselines representing the current state of the art for query suggestion in commercial search engines. Also we randomly choose five queries from candidates to form sug-gestions. We denote this baseline as Random .Thecom-parison results are evaluated by Max@1-5 and SDCG@5, shown in Table 2 and Table 3. As our method contains a candidate identification step followed by a candidate selec-tion step, in order to see the impact of each of them, we also report the case when 5 suggestions are randomly cho-sen from the identified candidates (Random), i.e. we only use the first step.

Asshowninthetwotables,ourfusionapproachisdra-matically better than any baseline in terms of all Max@1-5 and SDCG@5. The differences are statistically significant. This indicates that the existing query suggestion technolo-gies used in commercial search engines have not well solved the problem of difficult queries. A possible reason may be that too much emphasis is put on the similarity between the suggestions and the original query. As we observed, the similarity criterion does not work well for difficult queries. To illustrate this, we show two examples in Table 4. In each example, we show an original query and top five suggestions returned by three approaches along with their NDCG@3 val-ues. Given the query  X  X hat X  X  in fashion X , SE1 and SE2 pro-vide some related queries that are subtopics of the query by adding a few key terms. Unfortunately, they fail to find better results on fashion because of the ineffective original query terms. Our approach suggests queries that are more different, such as  X  X atest fashion trends X  and  X  X ashion now X , which remove some ineffective key terms, like  X  X hat X  X  in X , and add a few better ones, like  X  X rends X ,  X  X atest X , and  X  X ow X . In terms of NDCG@3, our suggestions can improve the re-trieval performance from 0.1564 to 0.5307. For the query  X  X allery furniture X , SE1 and SE2 provide specific furniture brands that are related to the original query. However, there is a clear topic drift. Our fusion approach is more effective and the suggestions are more specific and within the scope of the original query.

In Table 2, we observe that although our approach is the best one among all suggestion approaches, it performs worse than original queries in terms of Max@1 and Max@2, mean-ing that we have to use at least 3 suggestions in order to catch up and improve the original queries. In order to bet-ter understand the situation, we have a closer look into the six bins in Figure 5. It turns out that our fusion approach works better for queries that are more difficult. For the most difficult queries (in the bins of [0 , 0 . 2)) there is already im-provements on Max@1, i.e. the first suggestion is already better than the original queries. When we extend to larger bins in [0 , 0 . 3), it still achieves improvement starting from Max@2. For easier queries, it becomes more difficult to im-prove the result by one or two suggestions only. However, it consistently improves the original queries from Max@3. This means that the approach is capable of proposing better queries for these bins when we allow at least 3 suggestions.
Nevertheless, we observe a clear trend: the easier the orig-inal query, the more difficult to propose a better suggestion. It is then intuitive to suggest queries according to the diffi-culty of the query. We investigate such an approach in the next section. with p&lt; 0 . 01 .
 their NDCG@3. All suggestions are changed to lowercase.
To make good suggestions for all the queries, it is impor-tant to distinguish difficult queries from easy ones. A key problem is to predict how difficult a query is. Although we have used a simple feature (Estimated NDCG) to reflect the query difficulty in the learning-to-rank model, there is still the need to determine query difficulty with a more sophis-ticated method and explicitly use it to vary the strategy of query suggestion. We will carry out experiments using such a prediction of query difficulty.
Various methods have been proposed to predict query dif-ficulty. Any effective predictor can be used in our exper-iments. We choose to use the RAPP method proposed in [5]. The key idea behind this approach is to use the rank-ing scores, as well as the features that are used for ranking documents (e.g. BM25, click and PageRank), to predict the quality of the results. We re-implement the approach of [5].
We conduct three-fold cross validation to train a regressor using all queries, including easy queries and difficult queries. The predicted difficulty value for a query q is denoted by g ( q ). If the query q is judged as difficult according to g ( q ), we return suggestions that are generated by our fusion ap-proach to users. This leads to an adaptive approach for query suggestion, i.e. more query suggestions for difficult queries than for easy queries.
To quantitatively test the idea, we assume that a certain suggestion budget (the number of suggestions) is allowed for the entire query set, and our goal is to distribute the budget on different queries according to their difficulty. The budget is defined by m suggestion slots per query on average. The total suggestion slots available for all our test queries is then defined by 4 , 068  X  m . To make our test easier, we will assume that whenever a query is considered to be worthy for suggestions, we propose 5 suggestions. This is in line with our evaluation method that uses the top 5 suggestions. Therefore, we will select to perform query suggestion on n = 4 , 068  X  m  X  5 most difficult queries. For the other queries, no suggestion will be provided. This evaluation schema is of course a simplification of the real situation; but it does reflect the latter to some extent.

For a budget of m slots per query, we evaluate our adap-tive query suggestion approach and the fusion approach on the Max@ ns metric. Here ns is the total number of the suggestions assigned to a query. For the queries without suggestion, we use NDCG@3 of the original query to calcu-late Max@ ns . Figure 6 shows the average Max@ ns scores of the adaptive approach ( Adaptive ) and the fusion ap-proach ( Fusion ), along with different number of sugges-tions: m =1 ... 5. We also plot the average NDCG@3 values of all original queries ( Original Query ) in the figure. Figure 6: Comparing adaptive query suggestion with the fusion approach and the original queries with budget control
As shown in Figure 6, our adaptive approach consistently outperforms the fusion approach and the original queries. Specifically, when m is small (one or two), the fusion ap-proach cannot suggest queries better than the original ones. One can only find better queries when there are 3 slots or more. This is because the slots are uniformly distributed over all the queries, including on easy queries which do not need suggestions. The adaptive approach successfully avoids this problem. It can target more difficult queries for which query suggestion is more useful. Its improvements over the original queries are statistically significant for all budget set-tings.

The above simulation provides a good indication that one can gain more in targeting difficult queries for query sugges-tion. However, we assumed an equal number (5) of sugges-tions for all the queries whose g ( q ) is lower than a threshold. In practice, this strategy can be improved by proposing dif-ferent numbers of suggestions according to g ( q ). We will leave this to future work.
Query suggestion is widely used by search engines. Al-though it is found useful, the method is used uniformly on all the queries. In this paper, we show that query suggestion is more beneficial for difficult queries than for easy queries. To the best of our knowledge, this is the first investigation of query suggestion according to query difficulty.
In this paper, we used a learning-to-rank method to learn to rank suggestion candidates by using different features. Then an adaptive approach is proposed to provide sugges-tions according to the estimation on query difficulty. The experiments reported in this paper show that: 1. There is much more to gain in proposing query sug-2. To be useful, the suggestions for more difficult queries 3. An adaptive suggestion according to query difficulty is
In addition to the above conclusions, we also proposed two new evaluation measures that take into account the possible improvements in NDCG by the suggested queries, as well as their ranking in the suggestion list. These measures can better reflect the use of suggestions by end users than the previous measures.
This work is a first step towards adaptive query sugges-tion. There is much room for future improvements. First, in order to gain more insights on query suggestion for the whole spectrum of queries, we may need to train different, difficulty-dependent, query suggestion models. That is, for each category of queries, a specific model is trained to pre-dict whether it is useful to suggest queries and what type of query is the most useful. Second, the true usefulness of dif-ferent query suggestion methods should be tested with true users. Third, the proposed method can be used in differ-ent tasks. For example, as we have shown, some suggested queries lead to better search results. It is then possible to utilize our suggestions to automatically re-rank the search results of difficult queries. [1] G. Amati, C. Carpineto, and G. Romano. Query [2] R. A. Baeza-Yates, C. A. Hurtado, and M. Mendoza. [3] R. A. Baeza-Yates and A. Tiberi. Extracting semantic [4] N. Balasubramanian, G. Kumaran, and V. R.
 [5] N. Balasubramanian, G. Kumaran, and V. R.
 [6] D. Beeferman and A. L. Berger. Agglomerative [7] S. Bhatia, D. Majumdar, and P. Mitra. Query [8] H. Cao, D. Jiang, J. Pei, Q. He, Z. Liao, E. Chen, and [9] D. Carmel and E. Yom-To v. Estimating the query [10] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg. [11] S. Cronen-Townsend, Y. Zhou, and W. B. Croft. [12] V. Dang and W. B. Croft. Query reformulation using [13] N. Eiron and K. S. McCurley. Analysis of anchor text [14] C. Fox. Lexical analysis and stoplists. Information [15] J. Gao, X. Li, D. Micol, C. Quirk, and X. Sun. A large [16] J. Guo, G. Xu, H. Li, and X. Cheng. A unified and [17] K. J  X  arvelin and J. Kek  X  al  X  ainen. Cumulated gain-based [18] J.C.Borda. M  X  emoire sur les  X  elections au scrution. [19] T. Joachims. Optimizing search engines using [20] R. Jones, B. Rey, O. Madani, and W. Greiner. [21] R. Kraft and J. Y. Zien. Mining anchor text for query [22] V. Lavrenko and W. B. Croft. Relevance-based [23] H. Ma, H. Yang, I. King, and M. R. Lyu. Learning [24] Q. Mei, D. Zhou, and K. W. Church. Query suggestion [25] F.Peng,N.Ahmed,X.Li,andY.Lu.Context [26] M. Porter. An algorithm for suffix stripping. Program: [27] S. E. Robertson. The probability ranking principle in [28] Y. Song and L. wei He. Optimal rare query suggestion [29] Y. Song, D. Zhou, and L. wei He. Post-ranking query [30] T. Tao and C. Zhai. Regularized estimation of mixture [31] X. Wang and C. Zhai. Mining term association [32] J.-R. Wen, J.-Y. Nie, and H. Zhang. Clustering user [33] J. Xu and W. B. Croft. Improving the effectiveness of [34] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow. [35] Y. Zhou and W. B. Croft. Ranking robustness: a [36] Y. Zhou and W. B. Croft. Query performance
