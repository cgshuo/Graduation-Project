
Mining for frequent itemsets can generate an overwhelm-ing number of patterns, often exceeding the size of the original transactional database. One way to deal with this issue is to set filters and interestingness measures. Others advocate the use of constraints to apply to the patterns, either on the form of the patterns or on descriptors of the items in the patterns. However, typically the filtering of patterns based on these constraints is done as a post-processing phase. Filtering the patterns post-mining adds a significant overhead, still suffers from the sheer size of the pattern set and loses the opportunity to exploit those constraints.

In this paper we propose an approach that allows the effi-cient mining of frequent itemsets patterns, while pushing simul-taneously both monotone and anti-monotone constraints dur-ing and at different strategic stages of the mining process. Our implementation shows a significant improvement when con-sidering the constraints early and a better performance over Dualminer which also considers both types of constraints.
Frequent Itemset Mining (FIM) is a key component of many algorithms which extract patterns from transactional databases. For example, FIM can be leveraged to produce association rules, clusters, classifiers or contrast sets. This capability pro-vides a strategic resource for decision support, and is most commonly used for market basket analysis. One challenge for frequent itemset mining is the potentially huge number of ex-tracted patterns, which can eclipse the original database in size. In addition to increasing the cost of mining, this makes it more difficult for users to find the valuable patterns. Introducing con-straints to the mining process helps mitigate both issues. Deci-sion makers can restrict discovered patterns according to spec-ified rules. By applying these restrictions as early as possible, the cost of mining can be constrained. For example, users may be interested in purchases whose total price exceeds $100, or whose items cost between $50 and $100.
 Constraint based mining is an ongoing area of research. Two important categories of constraints are monotone and anti-monotone [13]. Anti-monotone constraints are constraints that when valid for a pattern, they are consequentially valid for any subset subsumed by the pattern. Monotone constraints when valid for a pattern are inevitably valid for any superset sub-suming that pattern. The straightforward way to deal with con-straints is to use them as a filter post-mining. However it is more efficient to consider the constraints during the mining pro-cess. This is what is refereed to as  X  pushing the constraints  X  [14]. Most existing algorithms leverage (or push) one of these types during mining and postpone the other to a post-processing phase. New algorithms such as Dualminer apply both types of constraints at the same time. [7]. It considers these two types of constraints in a double process, one mirroring the other for each type of constraint, hence its name. However, monotone and anti-monotone constraints do not necessarily apply in du-ality. Especially when considering the mining process as a set of distinct phases, such as the building of structures to compress the data and the mining of these structures, the application of these constraints differ by type. Moreover, some constraints have different properties and should be considered separately. For instance, minimum support and maximum support are in-tricately tied to the mining process itself while constraints on item characteristics, such as price, are not. There is no ex-isting algorithm that pushes both types of constraints early in the mining process and neither traverses the lattice of patterns top-down nor bottom-up. We introduce herein an algorithm that pushes both monotone and anti-monotone constraints by wisely jumping several levels in the pattern lattice from the bot-tom and top, and cleverly reducing the unnecessary constraint checking while considering the intricacies and properties of the constraints and the patterns sought after.
The problem of mining association rules over market bas-ket analysis was introduced in [1, 2]. The problem consists of finding associations between items or itemsets in transac-tional data. Formally, the problem is stated as follows: Let an object with some predefined attributes such as price, weight, etc. and is considered the dimensionality of the problem. Let be a set of transactions, where each transaction is a set of items such that . A transaction is said to contain , a set of items in ,if . A constraint is a predicate on itemset that yields either true or false . An itemset satis-fies a constraint if and only if is true . An itemset has a support s in the transaction set if s% of the transac-tions in contain . Two particular constraints pertain to the support of an itemset, namely the minimum support constraint and the maximum support constraint. An itemset is said to be infrequent if its support is smaller than a given minimum support threshold ; is said to be too frequent if its support is greater than a given maximum support ; and is said to be large or frequent if its support is greater or equal than and less or equal than .
The problem of discovering all frequent itemsets that satisfy constraints is a difficult one. The difficulty stems from the fact that, firstly, testing for minimum support and maximum sup-port can not be done simultaneously, since when valid, one is always true for subsets while the other is always true for super-sets. Secondly, despite their selective power, some constraints cannot be checked to filter candidate itemsets until a very late stage of the mining process depending upon the type of con-straint and the search space traversal strategy used.
We introduce a frequent itemset mining algorithm with the following properties:
The remainder of the paper is organized as follows: The rel-evant types of constraints, monotone and anti-monotone, are discussed in Section 2 with illustrative examples. Section 3 presents a new algorithm for frequent itemset mining using the COFI-tree idea but instead of a bottom-up or top-down approach, selectively jumps within the pattern lattice to find those patterns that satisfy the minimum support threshold [17]. How to push both monotone and anti-monotone constraints and where these constraints are evaluated in this new approach is presented in Section 4. A selection from a battery of tests for performance evaluation is presented in Section 5. In particular, we compare our approach to Dualminer [7]. Section 6 presents related work. Finally, Section 7 concludes the paper.
Table 1. Commonly used monotone and anti-monotone constraints
It is known that algorithms for discovering association rules generate an overwhelming number of those rules. While many new efficient algorithms were recently proposed to allow the mining of extremely large datasets, the problem due to the sheer number of rules discovered still remains. The set of discov-ered rules is often so large that it becomes useless. Different measures of interestingness and filters have been proposed to reduce the discovered rules, but one of the most realistic ways to find only those interesting patterns is to express constraints on the rules we want to discover. However, filtering the rules post-mining adds a significant overhead and misses the oppor-tunity to reduce the search space using the constraints. Ideally, dealing with the constraints should be done as early as possible during the mining process.
A number of types of constraints have been identified in the literature [13]. In this work, we discuss two important cate-gories of constraints  X  monotone and anti-monotone . Definition 1 ( Anti-monotone constraints ) A constraint is anti-monotone if and only if an itemset violates , so does any superset of . That is, if holds for an itemset then it holds for any subset of .
 Many constraints fall within the anti-monotone category. The minimum support threshold is a typical anti-monotone constraint. As an example, is an anti-monotone constraint. Assume that items , , and have prices $100, $150, and $200 respectively. Given the constraint =( ), then since itemset , with a total price of $250, violates the constraint, there is no need to test any of its supersets (e.g. ) as they also violate the constraint.
 Definition 2 ( Monotone constraints ) A constraint is monotone if and only if an itemset holds for , so does any superset of . That is, if is violated for an itemset then it is violated for any subset of .

An example of a monotone constraint is with constraint =( ), then knowing that violates the constraint is sufficient to know that all subsets of ABC will violate as well.
Table 1 presents commonly used constraints that are either anti-monotone or monotone . From the definition of both types of constraints we can conclude that anti-monotone constraints can be pushed when the mining-algorithm uses the bottom-up approach, as we can prune any candidate superset if its subset violates the constraint. Conversely, the monotone constraints can be pushed efficiently when we are using algorithms that follow the top-down approach as we can prune any subset of patterns from the answer set once we find that its superset vio-lates the monotone constraint.
 Algorithm 1 COFILeap: Leap-Traversal with COFI-tree Input: (transactional database); (Support threshold). Output: Type patterns with their respective supports. for each item in Header( ) in increasing support do end for
Output
Pushing constraints early means considering constraints while mining for patterns rather than postponing the checking of constraints until after the mining process. Given the intrin-sic characteristics of existing algorithms for mining frequent itemsets, either going over the lattice of candidate itemsets top-down or bottom-up, considering all constraints while mining is difficult. Most algorithms attempt to push either type of con-straints during the mining process hoping to reduce the search space in one direction: from subsets to supersets or from su-persets to subsets. Dualminer [7] pushes both types of con-straints but at the expense of efficiency. Focusing solely on reducing the search space by pruning the lattice of itemsets is not necessarily a winning strategy. While pushing constraints early seems conceptually beneficial, in practice the testing of the constraints can add significant overhead. If the constraints are not selective enough, checking the constraint predicates for each candidate can be onerous. It is thus important that we also reduce the checking frequency. While the primary bene-fit of early constraint checking is the elimination of candidates which can not pass the constraint, it can also be used to iden-tify candidates which are guaranteed to pass the constraint and therefore do not need to be re-checked. In summary, the goal of pushing constraints early is to reduce the itemset search space, eliminating unnecessary processing and memory consumption, while at the same time limiting the amount of constraint check-ing performed.
Most existing algorithms traverse the itemset lattice top-down or bottom-up, and search using a depth first or breadth first strategy. In contrast, we propose a leap traversal strat-egy that finds a superset of pertinent itemsets by  X  X eaping X  be-tween promising nodes in the itemset lattice. In addition to finding these relevant candidate itemsets, sufficient information is gathered to produce the frequent itemset patterns, along with their supports. Here, we use leap traversal in conjunction with the complementary COFI idea [10], where the locally frequent itemsets of each frequent item are explored separately. This creates additional opportunities for pruning. What is the COFI idea and what is this set of pertinent itemsets with their addi-tional information? This set of pertinent itemsets is the set of maximals. We will first present the COFI tree structure, then we will introduce our algorithm COFILeap which mines for fre-quent itemsets using COFI trees and jumping in the pattern lat-tice. In the next section, this same algorithm will be enhanced with constraint checking to produce our algorithm BifoldLeap .
The COFI-tree idea was introduced in [10] as a means to reduce the memory requirement and speed up the mining strat-egy of FP-growth [12]. Rather than recursively building condi-tional trees from the FP-tree, the COFI strategy was to create COFI-trees for each frequent item and mine them separately. Conditional trees are FP-trees conditioned on the existence of a given frequent item. The FP-tree [12] is a compact prefix-tree representation of the sub-transactions in the input data. Here, sub-transactions are the original transactions with infrequent items removed. The FP-tree contains a header and inter-node links which facilitate downward traversal (forward in the item-set pattern) as well as lateral traversal (next node representing a specific item).

Building COFI-trees based on the FP-tree. For each fre-quent item in the FP-tree, in order of increasing support, one COFI-tree is built [10]. This tree is based on sub-transactions which contain the root item and are composed only of items locally frequent with the root item that have not already been used as root items in earlier COFI-trees. The COFI-tree is sim-ilar to the FP-tree, but includes extra links for upward traversal (earlier in the itemset pattern), a new participation counter in each node, and a data structure to allow traversal of all leaves in the tree. This participation counter is used during the min-ing process to count up the participation of each node in the generation of a frequent pattern.
 Algorithm 2 BifoldLeap: Pushing P() and Q() Input: (transactional database); ; P() ; Q() .
 Output: Frequent patterns satisfying P(), Q() for each item in Header( ) do end for PQ-Patterns GPatternsQ( )
Output PQ-Patterns
COFILeap is different than the algorithm presented in [10] in the sense that it generates maximal patterns, where a pattern is said to be maximal if there is no other frequent pattern that subsumes it. COFILeap rather than traversing the pattern lattice top-down it leaps from one node to the other in search of the support border where maximals sit. Once maximals are found, with the extra information collected, all other patterns can be generated with their respective support.
 Following is a brief summary of the COFILeap algorithm. First, a frequent pattern FP-tree [12] is created, using two scans of the database. Second, for each frequent item, a COFI-tree is created including all co-occurant frequent items to the right (i.e. in order of decreasing support). Each COFI-tree is gen-erated from the FP-tree without returning to the transactional database for scans. Unique sub-transactions in the COFI-tree along with their count (called branch support ) are obtained from the COFI-tree. These unique sub-transactions are called frequent path bases (FPB). These can be obtained by traversing upward from each leaf node in the COFI-tree, updating the par-ticipation counter to avoid over-counting nodes. Clearly, there is at most one FPB for each sub-transaction in the COFI-tree. Frequent FPBs are declared candidate maximals. Infrequent FPBs are intersected iteratively, producing subsets which may be frequent. When an intersection of infrequent FPBs results in a frequent itemset, that itemset is declared as a candidate maximal and is not subject to further intersections. When an intersection is infrequent, it participates in further intersections looking for maximals. This is indeed how the leaps in the lat-tice are done. The result of the intersection of FPBs indicates the next node to explore. How is the support of a pattern cal-culated? Given the set of frequent path bases along with their branch supports, it is possible to count the support of any item-set. This is done by finding all FPBs which are supersets of the target itemset, and summing their branch supports. For exam-ple, if there are two FPBs, and , each with branch support 1, has support 2, and has support 1.
 Algorithm 1 shows the main steps of COFILeap . Notice that COFI-trees are not generated systematically for all frequent 1-itemsets. There is no need to look for maximals locally with respect to an item ,if and its locally frequent items are al-ready subset of known global maximals. Finally, in the func-tion the set of candidate maximal patterns is used along with the frequent path bases to produce the set of all frequent itemsets that satisfy the constraints along with their supports. Maximal itemsets can be found by filtering the can-didate maximals to remove subsets. Supports for the candidate maximal patterns were computed as part of the intersection pro-cess (to discover that they were frequent), and therefore do not need to be recomputed. Once the maximal itemsets have been found, the all frequent itemsets can be found by iterating over all subsets of the maximals, suppressing duplicates resulting from overlap with other maximal patterns. Support counting for the frequent itemsets is done as for the FPBs, i.e. by sum-ming the branch supports of all FPBs which are supersets of the pattern.
The conjunction of all anti-monotone constraints comprises a predicate that we call . A second predicate contains the conjunction of the monotone constraints. A common ap-proach is to include the ubiquitous minimum support constraint as part of . Similarly, the monotone maximum support con-straint can be included as part of . In this way, a frequent itemset mining algorithm can be extended to push deeply by replacing checks for minimum support with checks for . In our algorithm, we separate the constraints on the support from other constraints. Thus, the minimum support constraint and the maximum support constraint are extracted from and respectively. This is because checking for support is an integral part of the frequent itemset enumeration, while other constraints on item attributes are used for search space pruning. The algorithm COFILeap offers a number of opportunities to push the monotone and anti-monotone predicates, and respectively. We start this process by defining two terms which are head ( ) and tail ( ) where is a frequent path base or any subset generated from the intersection of frequent path bases, and is the itemset generated from intersecting all remaining frequent path bases not used in the intersection of . The in-tersection of and , , is the smallest subset of that may yet be considered. Thus Leap focuses on finding frequent that can be declared as local maximals and candidate global maximals. BifoldLeap extends this idea to find local maximals that satisfy . We call these P-maximals.

Although we further constrain the P-maximals to itemsets which satisfy , not all subsets of these P-maximals are guar-anteed to satisfy . To find the itemsets which satisfy both constraints, the subsets of each P-maximal are generated in or-der from long patterns to short. When a subset is found to fail set, as they are guaranteed to fail also.

There are three significant places where constraints can be pushed: (a) while building the FP-tree, (b) while building the COFI-trees, and (c) while intersecting the frequent path bases, which is the main phase where both types of constraints are pushed at the same time (Algorithm 2).
 Constraint pushing opportunities during FP-tree construction. First, is applied to each 1-itemset. Items which fail this test are not included in FP-tree construction. Second, we use the idea from FP-Bonsai [5] where sub-transactions which do not satisfy are not used in the second phase of the FP-tree building process. The supports for the items within these trans-actions are decremented. This may result in some previously frequent items becoming infrequent. Such items will not be used to construct COFI-trees in the following phase. Constraint pushing opportunities during COFI-tree construc-tion. Let be the set of all items that will be used to build the COFI-tree, i.e. the items which satisfy individually but have not been used as the root of a previous COFI-tree. If fails , there is no need to build the COFI-tree, as no subset of can satisfy . Alternatively, if satisfies , there is also no need to build the COFI-tree, as is a candidate P-maximal.
 Constraint pushing opportunities during intersection of Frequent-Path-Bases. There are two high-level strategies for pushing constraints during the intersection phase. First, and can be used to eliminate an itemset or remove the need to evaluate its intersections with additional frequent path bases. Second, and can be applied to the  X  X ead intersect tail X  ( ), which is the smallest subset of the current itemset that can be produced by further intersections. These strategies are detailed in the following four theorems.

Theorem 1 : If an intersection of frequent path bases ( ) fails , it can be discarded, and there is no need to evaluate further intersections with .
 Proof : If an itemset fails , all of its subsets are guaranteed to fail based on the definition of monotone constraints. Fur-ther intersecting will produce subsets, all of which are guar-anteed to violate .
 Theorem 2 : If an intersection of frequent path bases ( ) passes ate further intersections with . Proof : Further intersecting will produce subsets of . By definition, no P-maximal is sub-sumed by another itemset which also satisfies . Therefore, none of these subsets of are potential new P-maximals. Theorem 3 : If a node X  X  fails , the node can be discarded, and there is no need to evaluate further intersections with . Proof : If an itemset fails , then all of its supersets will also violate from the definition of anti-monotone con-straints. Since a node X  X  represents the subset of that results from intersecting with all remaining frequent path bases, and all combinations of intersections between and remaining frequent path bases are supersets of and there-fore guaranteed to fail also.
 Theorem 4 :If a node X  X  passes , is guaranteed to pass for any itemset resulting from the intersection of a subset of the frequent path bases used to generate plus the remain-ing frequent path bases yet to be intersected with . does not need to be checked in these cases. Proof : is guaranteed to pass for all of these itemsets because they are generated from a subset of the intersections used to produce the and are therefore supersets of the .
 The following example, shown in Figure 1, illustrates how BifoldLeap works. An A-COFI-tree is made from five items, , , , , and , with prices $60, $450, $200, $150, and $100 respectively. In our example, this COFI-tree generates 5 fre-quent path bases, , , , , and predicate, ,is , and the monotone predicate, ,is . Intersecting the first FPB with the second produces which has a price of $510, and therefore violates and passes . Next, we ex-amine the , the intersection of this node with the remain-ing three FPBs, which yields with price $60, passing and failing . None of these constraint checks provide an oppor-tunity for pruning, so we continue intersecting this itemset with the remaining frequent path bases. The first intersection is with the third FPB, producing with price $410, which sat-isfies both the anti-monotone and monotone constraints. The second intersection produces , which also satisfies both constraints. The same thing occurs with the last intersection, which produces . Going back to the second frequent path base, ABCDE, we find that the , , violates the anti-monotone constraint with price $510. Therefore, we do not need to consider or any further intersections with it. The remaining nodes are eliminated in the same manner. In to-tal, three candidate P-maximals were discovered. We can gen-erate all of their subsets while testing only against . Finally, the support for these generated subsets can be computed from the existing frequent path bases.
To evaluate our BifoldLeap algorithm, we conducted a set of experiments to test the effect of pushing monotone and anti-monotone constraints separately, and then both in combination for the same datasets. To quantify scalability, we experimented with datasets of varying size. We also measured the impact of pushing versus post-processing constraints on the number of evaluations of and . Like in [7], we assigned prices to items using both uniform and zipf distributions. Our constraints consisted of conjunctions of tests for aggregate, minimum, and maximum price in relation to specific threshholds.

We compared our algorithm with Dualminer [7]. Based on its authors X  recommendations, we built the Dualminer frame-work on top of the MAFIA [8] implementation provided by its original authors. Our experiments were conducted on a 3GHz Intel P4 with 2 GB of memory running Linux 2.4.25, Red Hat Linux release 9. The times reported also include the time to output all matching itemsets. We have tested these algorithms using both real datasets provided by [11] and synthetic datasets generated using [3]; we used  X  X etail X  as our primary real dataset reported here. A dataset with the same characteristics as the one reported in [7] was also generated.

We received an FP-Bonsai code (base on FP-Growth) from its original authors [5]. Unfortunately, not all pruning and clever constraint considerations suggested in their FP-Bonsai paper were implemented in this code. Moreover, the imple-mentation as received produced some false positives and false negatives. This is why we opted not to add it to our comparison study. Although, with simple and only monotone constraints, the received FP-Bonsai implementation was indeed fast. FP-Bonsai as described in the paper has merit but because of lack of time we could not implement it ourselves (albeit adding im-plementation bias) or fix the received code.
To differentiate between our novel BifoldLeap algorithm and Dualminer, we experimented against the retail dataset. In the first experiment (Figure 2.A), we pushed , then , and finally . We used the zipf distribution to assign prices to items. Both and consisted of constraints on the sum of the prices. The constraint thresholds were chosen to not be very selective. Figure 2.B presents the same experiment with more selective constraints.

Figure 2.C presents pushing extremely selective constraints, using anti-monotone and monotone constraints on the sum of the prices, and on the minimum and maximum item price. In this experiment, we found that BifoldLeap in most cases out-performs Dualminer and in some cases by more than one order of magnitude. The most interesting observation we found from this experiment was that if we push one type of constraint, e.g. both constraints together will take seconds, where is al-ways between and . In contrast, BifoldLeap always takes less time with the conjunction of the constraints than with ei-ther constraint in isolation. Monotone and anti-monotone con-straints can indeed be mutually assisting each other in the se-lectivity. BifoldLeap took better advantage of this reciprocal assistance in the pruning.
Scalability is an important issue for frequent itemset min-ing algorithms. Synthetic datasets were generated with 50K, 100K, 250K, and 500K transactions, with 5K or 10K distinct items. In this experiment, BifoldLeap demonstrated extremely good scalability versus increasing dataset size. In contrast, Dualminer reached a point where it consumed almost three or-ders of magnitude more time than that needed by BifoldLeap. Figure 3.A depicts one of these results while mining datasets with only 5K unique items. As another experiment exam-ple, we tested both algorithms on datasets with up to 50 Mil-lion transactions and 100K items. Dualminer finished the 1M dataset in 8534 seconds while BifoldLeap finished in 186s, 190s, 987s and 2034s for the 1M, 5M, 25M and 50M trans-actions datasets respectively.
One of the major challenging issues for constraint mining is reducing the number of evaluations of and . In the following experiment, we generated a synthetic dataset with the same characteristics as the one reported in [7]. Specifically, it was generated with 10,000 transactions, an average transaction length of 15, an average maximal pattern length of 10, 1000 unique items, and 10,000 patterns. We found that Dualminer was indeed good on this dataset as reported in [7]. However, BiFoldLeap outperformed it with the same order of magnitude as the tests on timing. This shows that the predicate checking is indeed a significant overhead and BiFoldLeap outperforms Dualminer in time primarily because it does significantly less predicate checks.

The goal of these experiments was to test the number of evaluations and the effect of pushing constraints early ver-sus post-processing them. We ran our experiments using this constraints dataset with absolute support equal to 25, 50, and 75 using the two different distributions. We used a modified version of MAFIA with post-processing as the post-processing counter-part to Dualminer. Our implementation of Dualminer always tests minimum support and P() together, while BifoldLeap X  X  minimum support checks occur at different times and do not contribute to the count for . Figure 4 depicts the results of these experiments. Our first observation is that Dualminer performs a huge number of constraint evaluations as compared to BifoldLeap. Even in cases where we only generated 255 patterns, Dualminer needed more than 50 thousand evalua-tions for both and , compared to almost 6 thousand needed by BifoldLeap. Our second observation is that MAFIA with post-processing requires fewer constraint evaluations than Dualminer.
Figure 3. (A) Scalability test. (B) Effect of chang-ing the price distribution
All of our experiments were conducted using uniform and/or zipf price distributions. In most of the experiments, we found that the effect of changing the distribution on Dualminer was greater than for BifoldLeap. This can be justified by the effec-tiveness of the pruning techniques used by BifoldLeap that also reduce the number of candidate checks which consequently af-fected its performance. Figure 3.B depicts one of these results for the retail dataset.
Mining frequent patterns with constraints has been studied in [13] where the concept of monotone and anti-monotone and
Figure 4. No. of and evaluations, using constraint pushing vs. post-processing succinct were introduced to prune the search space. Jian Pei et al. [14, 15] have also generalized these two classes of con-straints and introduced a new convertible constraint class. In their work they proposed a new algorithm called which is an FP-Growth based algorithm [12]. This algorithm gener-ates most frequent patterns before pruning them. Its main con-tribution is that it checks for monotone constraints early and once a frequent itemset is found to satisfy the monotone con-straint, then all itemsets having this item as a prefix are sure to satisfy the constraint and consequently there is no need to ap-ply further checks. Dualminer [7] is the first algorithm to mine both types of constraints at the same time. Nonetheless, it suf-fers from many practical limitations and performance issues. First, it is built on the top of the MAFIA [8] algorithm which produces the set of maximal patterns, and consequently all fre-quent patterns generated using this model do not have their sup-port attached. Second, it assumes that the whole dataset can fit in main memory which is not always the case. FP-Growth and our approach use a very condensed representation, namely FP-Tree, which uses significantly less memory [12]. Third, their top-down computation exploiting the monotone constraint of-ten performs many useless tests for relatively large datasets, which raises doubts about the performance gained by pushing constraints in the Dualminer algorithm. In a recent study of par-allelizing Dualminer [16], the authors showed that by mining relatively small sparse datasets consisting of 10K transactions and 100K items, the sequential version of Dualminer took an excessive amount of time. Unfortunately, the original authors of Dualminer did not show any single experiment to depict the execution time of their algorithm but only the reduction in pred-icate executions [7]. A recent strategy dealing with monotone and anti-monotone constraints suggests reducing the transac-tional database input via pre-processing by successively elimi-nating transactions that violate the constraints and then apply-ing any frequent itemset mining algorithm on the reduced trans-action set [4, 6]. The main drawback of this approach is that it is highly I/O bound due to the iterative process needed in re-writing the reduced dataset to disk. This algorithm is also sen-sitive to the results of the initial monotone constraint checking which is applied to full transactions. In other words, if a whole transaction satisfies the monotone constraint, then no pruning is applied and consequently no gains are achieved even if parts of this transaction do not satisfy the same monotone constraint. To overcome some of the issues in [4], the same approach has been tested against the FP-Growth approach in [5] with new effective pruning heuristics.
Since the introduction of association rules a decade ago and the launch of the research in efficient frequent itemset mining, the development of effective approaches for mining large trans-actional databases has been the focus of many research studies. Furthermore, it is widely recognized that mining for frequent items or association rules, regardless of its efficiency, usually yields an overwhelming, crushing number of patterns. This is one of the reasons it is argued that the integration of data mining and database management technologies is required [9]. These large sets of discovered patterns could be queried. Express-ing constraints using a query language could indeed help sift through the large pattern set to identify the useful ones.
We argue that pushing the consideration of these constraints at the mining process before discovering the patterns is an effi-cient and effective way to solve the problem. This does not ex-clude the integration of data mining and database systems, but suggests the need for data mining query languages intricately integrated with the data mining process.

In this paper we address the issue of early consideration of monotone and anti-monotone constraints in the case of fre-quent itemset mining. We propose a leap traversal approach, BifoldLeap , that traverses the search space by jumping from relevant node to relevant node and simultaneously checking for constraint violations. The approach we propose uses exist-ing data structures, FP-tree and COFI-tree, but introduces new pruning techniques to reduce the search costs. We conducted a battery of experiments to evaluate our constraint-based search and report a fraction of them herein for lack of space. The ex-periments show the advantages of pushing both monotone and anti-monotone constraints as early as possible in the mining process despite the overhead of constraint checking. We also compared our algorithm to Dualminer, a state-of-the-art algo-rithm in constraint-based frequent itemset mining, and showed how our algorithm outperforms it and can find all frequent item-sets, the closed and the maximal patterns that satisfy constraints along with their exact supports.

