 Mario Boley  X  Henrik Grosskreutz Abstract We investigate the problem of counting the number of frequent (item)sets X  X  problem known to be intractable in terms of an exact polynomial time computation. In this paper, we show that it is in general also hard to approximate. Subsequently, a randomized counting algorithm is developed using the Markov chain Monte Carlo method. While for general inputs an exponential running time is needed in order to guarantee a certain approxi-mation bound, we show that the algorithm still has the desired accuracy on several real-world datasets when its running time is capped polynomially.
 Keywords Data mining  X  Frequent itemsets  X  Approximate counting  X  Markov chain Monte Carlo 1 Introduction Frequent pattern mining is considered one of the most influential methods in data mining as compiled by Wu et al. [ 27 ], and recently it has even made the step into commercial database tion domains. A few examples for such combinations are association rules for market basket data, frequent subgraphs for molecule prediction, and sequential patterns for time series data (see [ 11 ] for an overview). Their unifying property is that patterns are only considered inter-esting if they satisfy a (minimum) frequency constraint, i.e., a certain number of records of the input dataset have to  X  X ontain X  the pattern. In contrast to most other works devoted to frequent pattern mining which deals with the development and evaluation of algorithms that list all frequent patterns, in this study, we are interested in counting them quickly. Knowing the relationship between frequency threshold and the resulting number of frequent patterns can, for instance, be used for computing a frequency-plot , i.e., a plot showing all possible thresholds ( x -axis) against the corresponding number of frequent sets ( y -axis). Having such a plot prior to the actual mining process can, for instance, be used within intelligent discov-ery assistants (IDAs) [ 2 ] in order to either provide user guidance for setting the frequency threshold or tune it automatically.

For this purpose it is essential that the involved computation is performed quickly.  X  X ndeed, the problem is precisely to predict a combinatorial explosion without suffering from it [...] X , as Geerts et al. [ 7 ] have put it in their related study of bounding the number of candidate patterns that have to be processed within a BFS-listing of all frequent patterns. In particular, for dense datasets, this requirement prohibits the use of any of the known exhaustive data-mining algorithms that list all frequent patterns. Even though these algorithms have been optimized to an impressive level in recent years, they have the number of frequent sets as an inherent lower bound of their time complexity. While this does not pose a problem for sparse datasets, in dense ones this number behaves in essence exponentially (for low frequency thresholds). For that reason we are aiming for an algorithm counting the number of frequent sets in a time that does not depend on that number. Since it is well known that no exact deterministic algorithm with this property can exist (unless P = NP ), we are aiming for a randomized approximation algorithm using the Markov chain Monte Carlo method. More-over, we restrict ourselves to the case of plain frequent (item)sets, which are used, e.g., for generating association rules. In summary, we are aiming for an input polynomial algorithm for solving the following computational problem: Problem 1 ( #-FREQUENT SETS ) Given a transactional dataset and a frequency threshold, compute the cardinality of the corresponding frequent set family.
 We discuss the question of why all this is a worthwhile venture in some more detail in Sect. 2 . After some formal definitions (Sect. 3 ) that are needed for the subsequent technical content we analyze theoretical limits in Sect. 4 . As the main result of this section we show that the number of frequent sets is hard to approximate. In addition, we interpret some known complexity results in the context of our problem. We then develop a randomized approxima-tion scheme in Sect. 5 that makes use of a sampling procedure presented in Sect. 6 . Details and speed-ups are presented in Sect. 7 resulting in a hybrid algorithm with an exhaustive and an approximative counting phase. As indicated by the hardness result, we show that for general inputs the algorithm X  X  correctness cannot be guaranteed or its time complexity is not bounded polynomially. However, experiments we present in Sect. 8 constitute its applicability on several real-world and synthetic datasets. A concluding discussion is given in Sect. 9 . 2 Motivation In order to raise business value of data mining or generally to make it more accessible to non-expert users, approaches like intelligent discovery assistants (IDAs) [ 2 ] or Mining Mart [ 18 ] have been proposed. Both assist a user in selecting a valid data-mining process for their data. Still, once a user has decided on a valid process the next problem is to find good parameter settings, and in case a frequent pattern mining step is part of the process this usually involves a minimum frequency threshold. Knowledge about the relation between frequency threshold and the corresponding number of frequent sets is very helpful in this context, as it allows to control the output size and thus indirectly the output time, because frequent set mining algo-rithms usually exhibit a time complexity that is roughly linear in the output. Consequently, quickly counting the frequent sets helps to make optimal use of the available time budget. Moreover, as we will argue below, a computed frequency plot can also help semantically to setup and interpret the whole process. It should be noted that the following discussion is illustrative and motivating. A thorough investigation of how to choose a minimum frequency threshold justifies a complete study in its own right. 2.1 Frequency plots In order to explain the possible use of a frequency plot, let us assume we have a dataset generated by the following illustrative and absolutely idealized underlying process. Example 2 (Beginner X  X  Guide Process) There is a hobby shop carrying items for k different hobbies. In particular there is a  X  X eginner X  X  guide X  a i for every hobby and a corresponding chases every item independent from one another with probability p c except for the beginner X  X  guides, which he will never purchase. However, with probability p a (per hobby independent from one another) a customer will pick up a new hobby and buy the corresponding beginner X  X  customer otherwise.
 Clearly, the most (if not the only) interesting association rules for this underlying process are the rules for i  X  X  1 ,..., k } . All of these rules will have the maximum confidence 1 of 1, which mea-sures the actual semantical value of a rule, whereas their expected support is equal to the and p c = 0 . 6. Each of the datasets was generated with k = 5 implicant items, 10 different consequence items for each implicant, and 5,000 transactions. Figure 1 shows their resulting frequency plots. Depending on initial assumption on the generative process there can be to clarify the motivation of introducing a minimum frequency threshold at all. 2.2 Frequency thresholds The reasons for introducing a frequency constraint can roughly be subsumed under three purposes: (1) suppress statistically insignificant results that are a mere random fluctuation of the (2) raise output pattern value in the sense that they are applicable to more data instances, (3) allow additional pruning of search space thus making otherwise intractable tasks trac-In particular for the last purpose, the frequency threshold is often set to higher values than it would be necessary for reasons (1) and (2) alone. Clearly, this can harm the analytical value of the resulting patterns, because patterns that are interesting according to a primary criterion (e.g., confidence) might be pruned without any statistical reason purely for the sake of performance. Thus, it would be desirable to set the frequency threshold to the minimum value that is reasonable with respect to reasons (1) and (2).

For the above example, observe that for all three datasets the interesting rules have an expected support that lies before that point of when the plot finally turns to a purely exponen-tial behavior. Thus, a user setting the frequency threshold to this point is expected not to miss most of them. Although this may not always be the case, this strategy generally aims to find the most conservative threshold that preserves most interesting rules while it suppresses most of the statistically insignificant sets. Moreover, the plots lead to a small set of reasonable candidate frequency points, namely the sockets before steep exponential slopes. Clearly, this is an improvement when compared to the uninformed approach of trial-and-error parameter twiddling. 2.3 Related work and objections An important related approach to raise user control in frequent set mining is listing only the top-K frequent sets proposed by Wang et al. [ 26 ]. While this approach takes care of output size control and scheduling, it fails to provide the global overview of computing a frequency plot for all possible frequency values X  X  task that cannot be done using a top-K miner (in particular on dense datasets) because the same restrictions apply to them as stated in the introduction for ordinary exhaustive miners. Moreover, the most frequent sets are not necessarily the most interesting ones [ 6 , 23 ].

Still, this may inspire a general objection to the above motivation: Why is it useful to compute regions of the plot that correspond to frequent set families so large that they are impossible to list in a subsequent mining step? The answer to this lies in the fact that fre-quency is usually only a subsidiary interestingness criterion. While the frequency plot shows constraints, e.g., confidence or lift, many of which can be used for pruning already during threshold, can in fact be listed effectively. Moreover, if effective knowledge post-processing is employed (e.g., based on visual analytics as in [ 3 ]) it is often reasonable to produce the maximum number of frequent sets that can possibly be computed within a given time budget.
A different approach for replacing the minimum frequency threshold by a more intui-tive parameter has been proposed by [ 29 ]. In their work, user-specified  X  X uzzy X  thresholds like  X  X ore or less frequent X  or  X  X ighly frequent X  are translated into traditional frequency thresholds. This translation is based on an approximation to the average support that, in turn, relies on an independence assumption for the distribution of single items. While this method simplifies parameter setting for the user, it does not provide control over the output size and the corresponding computation time. 3 Preliminaries In this section, we recall and fix notions from frequent set mining, randomized approximation algorithms, and Markov chains that are needed in the subsequent discussion. 3.1 Frequent set mining and prefix trees Let E be a finite set. A dataset D over E is a finite multiset with D  X  E for all D  X  D .Inthe context of frequent pattern mining the elements of E are often called items and the elements of D transactions .Foraset F  X  E we define its support multiset as D [ F ] ={ D  X  D : D  X  F } . Frequency thresholds can be specified as absolute or relative thresholds depending on what F ( D , f ) or just as F when D and f are clear from the context.

Unless stated differently we denote the number of items by n and identify E with the set { 1 ,..., n } throughout this article. This allows use to use the natural order on { 1 ,..., n } and particularly to use the symbols max F and min F for F  X  E . In addition we assume without loss of generality that the items are numbered in descending order of their frequency, i.e, 1  X  i &lt; j  X  n  X  | D [ { i } ] |  X  | D [ { j } ] | .Foraset F  X  E and i  X  X  0 ,..., n } we denote by F i = F  X  X  1 ,..., i } the i-prefix of F .The prefix tree of D is a labeled directed tree (arborescence) T = ( V , E , X ) with nodes V ={ D i : D  X  D , 0  X  i  X  n } , i.e., the set of all prefixes occurring in D , edges E ={ ( D i , D j ) : D  X  D , D j = D i  X  X  j }} , node labels  X (
X ) = | { D  X  D : X  i , D i = X } | , and edge labels  X (( D i , D j )) = j . 3.2 Probabilistic approximation algorithms A bounded probability (BP) algorithm for a problem with instances X and possible solutions Y specified by a correctness relation R  X  X  X  Y is a probabilistic algorithm A such that it holds that P [ ( x , A ( x ))  X  R ] X  3 / 4where A ( x ) denotes the output of A on input x .Now we consider the case when Y is the set of natural numbers N .A randomized approximation scheme for a mapping g : X  X  N is a BP-algorithm A taking arguments x  X  X and  X  ( 0 , 1 ) (this is sometimes just described by the phrase  X  y is -close to g ( x )  X ), i.e., A satisfies Such an algorithm is called fully polynomial if its time complexity is bounded in a polynomial in size ( x ) and 1 / . The constant 3 / 4 appearing in the definition has no significance other than being strictly between 1 / 2 and 1. Any two success probabilities from this interval can be reached from one another by a small number of repetitions of the corresponding algorithm and returning the median of the results (see [ 12 ]).

A weaker notion of approximation is given by the following definition: An algorithm A is called an  X  -factor approximation of g (or said to approximate g within  X  )ifitsatisfies function that may grow in the size of x . Clearly, an efficient approximation scheme can act as an efficient c -factor approximation algorithm for all constants c &gt; 1. 3.3 Markov chains A(discrete) Markov chain on state space is a sequence of discrete random variables M = X 1 , X 2 ,... with domain satisfying the Markov condition, i.e., for all n  X  N and x , x 1 ,..., x n  X  satisfying P [ X 1 = x 1 ,..., X n = x n ] &gt; 0. In this article we only consider finite state spaces . Thus, given a probability distribution on the initial state, M is completely specified by the state transition probabilities P ( x , y ) = P
X n + 1 = y | X n = x of all x , y  X  that do not depend on n .The ( | |  X  | | ) -matrix containing P ( x , y ) in column x and row y is a stochastic matrix we denote by P .The t th power of this matrix contains the probability of going from x to y in t steps P t ( x , y ) = P
X n + t = y | X n = x . We call a state y  X  reachable from a state x  X  if there is a t  X  N such that P t ( x , y )&gt; 0. A Markov chain M is called aperiodic if for all x , y  X  with x is it is called irreducible if any two states are reachable from one another. Finally, M is called ergodic if it is irreducible and aperiodic.

Any ergodic Markov chain has a unique limiting stationary distribution  X  :  X  X  0 , 1 ] , tion  X  :  X  X  0 , 1 ] satisfying the detailed balance condition  X  x , y  X  ,  X  ( x ) P ( x , y ) = symmetric transition probabilities always converge to the uniform distribution . The distance from the t -step distribution of a Markov chain with X 0 = x to its stationary distribution can Using this definition we can define the mixing time of M by as the minimum number of steps one has to simulate M until the resulting distribution is guaranteed to be -close to its stationary distribution. For more details and results about Markov chains and their mixing time we refer to Randall X  X  survey [ 20 ]. 4 Problem complexity Gunopulos et al. [ 9 ] P = NP proved #P -hardness 2 of #-FREQUENT SETS implying that there is no exact algorithm for that problem unless. They did this using a reduction from the #P -complete problem of computing the number of satisfying truth assignments of a given monotone 2-CNF formula, i.e., a conjunctive normal form formula containing only two positive literals per clause. It was shown by Zuckerman [ 30 ] that this number and in fact even its logarithm is hard to approximate within a factor of n for instances of size n .
The reduction in [ 9 ], however, transforms a 2-CNF formula into a transaction dataset with n items such that the number of satisfying truth assignments corresponds to the number of sets that are not 1-frequent, and then it uses the fact that the number of infrequent sets is equal to 2 n minus the number of frequent sets. Hence, the construction is highly non-parsimonious, i.e., the numbers usually change drastically without any reasonable bound. As a consequence, relative approximation guarantees are not preserved by that reduction and it does not lead to a hardness result for approximating #-FREQUENT SETS. Still, it is an important side note that the two aforementioned theorems together do imply the strong result that there is no efficient approximation algorithm for counting the number of infrequent sets even if the (absolute) frequency threshold is fixed to 1. This is an interesting difference to the same restriction for #-FREQUENT SETS: When restricted to frequency threshold 1 approximating the number of frequent sets becomes equivalent to approximating the num-ber of satisfying assignments of a given DNF-formula, and for this problem there is a fully polynomial randomized approximation scheme [ 15 ].

Now, for acquiring a hardness result for the number of frequent sets, we have to choose a different starting point, namely the hardness of approximating a frequent set of maximum cardinality. With this approach we can show the following result: Theorem 3 Unless for all &gt; 0 and for all problems in NP there is a BP-algorithm that runs in time 2 n for instances of size n, the following holds: There is a constant  X  #F such that there is no polynomial time BP-algorithm that, given a dataset D over n items and a frequency threshold f , approximates log | F ( D , f ) | within n  X  #F .
 Proof It was shown in [ 5 ] that under the same assumption as in the claim there is no polyno-mial time algorithm approximating a frequent set of maximum cardinality within n  X  BC .That result was based on Khot X  X  seminal inapproximability result for Bipartite Clique, which in fact ruled out BP-algorithms under the above assumption (see [ 16 ] where you can also find more information about the magnitude of  X  BC ). Furthermore, it is easy to prove that approxi-mating only the maximum number k such that there is a frequent set of size k is polynomially equivalent to the actual construction of a corresponding set. Thus, it is sufficient to show that an algorithm for approximating the logarithm of | F | can be used to approximate this number k .

Since all subsets of a maximizing frequent set F  X  F with | F | = k are also frequent, it (the other case, only a little more complicated, is omitted here for the sake of simplicity). Now suppose a BP-algorithm A approximates log | F | within n  X  . It follows that Now observe that for any  X &lt; X  BC , the expression n  X  log n is asymptotically dominated by n
BC ; say starting from the constant n ( X ) . Choose  X  #F to be any number strictly between 0 and  X  BC . Then modify A such that it looks up the true result of all (finitely many) instances of size less than n ( X  #F ) in a hard-coded table. Then A is a BP-algorithm approximating the maximum cardinality of a frequent set within n  X  BC as required.
 Although the complexity assumption of this theorem is stronger than P = NP it is still a widely believed standard assumption. Moreover, non-existence of an  X  -approximation of the logarithm of a number implies non-existence of an 2  X  -approximation to the actual number. Thus, we have strong evidence that there is no reasonable approximation algorithm for the general #-FREQUENT SETS problem and in particular no fully polynomial approximation scheme.

That said, there may still be algorithms allowing a good approximation for a wide range of practical relevant datasets. With this in mind, we are going to construct a randomized approximation algorithm in the next section. 5 Monte Carlo estimation The perhaps simplest Monte Carlo approach for counting the number of frequent set would be the following: Uniformly generate an element F  X  E ,return1if F  X  F , and return 0 otherwise. The expected value of this experiment is | F | / 2 | E | . Thus, taking the mean of sufficiently many independent repetitions and multiplying it by 2 | E | is a correct randomized can be as small as 1 / 2 n for an instance of size n . For such instances, the expected number of trials before the first 1 turn-out appears is not bounded by a polynomial in n . But as long as the returned result is 0, the solution does not satisfy any relative approximation guarantee and in particular not Eq. ( 1 ). 5.1 Estimator The standard solution to the problem above is to partition the result into a number of factors, each of which having a reasonable lower bound (see [ 13 ]). In our case such a partitioning can simply be done as follows. For i  X  X  1 ,..., n } let be the family of frequent sets containing only elements from the first i items (recall that we denote the i -prefix of a set F  X  E by F i ). With this we can rewrite the quantity to compute | F | = | F n | as the product i A constant non-zero lower bound for the ratios r i is implied by the following observation: For F and thus it holds that With this we can design a Monte Carlo algorithm as follows: Approximate each of the ratios r , count | F s | for an appropriate s exhaustively, and then compute | F | through Eq. ( 2 ). | { X  X  | = 1. There are, however, better choices in practice as we will discuss in Sect. 7.1 .The ratios r i are approximated as follows: assume we can sample a set F from F i according to a distribution D i satisfying D , U ( F i ) tv  X  b . The latter approach is commonly referred to as almost uniform sampling . Let Z i denote the random variable that takes on value 1 if F  X  F i  X  1 and 0 otherwise. Then with t  X  1and Z ( j ) i independent copies of Z i is a (biased) estimator of r i satisfying E  X  Z i  X  r i  X  b . With this we can write our final estimator for | F | as | F s | Z  X  1 where Z denotes the product of all ratio estimators
It is easy to see that if Z is -close to the product of all ratios r i then this implies the randomized approximation scheme. Moreover, as the maximum bias b approaches 0 and the number of independent trials t of each Bernoulli experiment Z i approaches infinity that guarantee will eventually hold. 5.2 Performance In this subsection, we discuss for what values of b and t an algorithm that simulates the estimator | F | Z  X  1 is a correct randomized approximation scheme for | F | as specified by Eq. ( 1 ).

We start with the bias b and the relative deviation it causes for the mean of the ratio esti-of r i from Eq. ( 3 )that This can be used to bound the deviation of Z  X  X  mean from the product r = n i = s + 1 r i of all reciprocal ratios from Eq. ( 2 ). Suppose we enforce a maximum absolute bias Then using the independence of the random variables Z i and basic bounds of the exponential function we can deduce: If in addition Z  X  ( 1 + 2 / 3 ) E [ Z ] we arrive at the required upper bound for the deviation of Z from r and the lower bound follows similarly.

It remains to assure that Z is 2 / 3-close to its mean with probability at least 3 / 4. The required number of trials t can be calculated by instantiating Chebycheff X  X  inequality as follows So a bound on this probability can be established by appropriately bounding the ratio of Z  X  X  variance to the square of its expectation. For the estimator of each factor  X  Z i we know that the last inequality follows from Eqs.( 3 )and( 4 ). Thus, if we set the number of trials t to we can deduce for the product: Plugging this bound into Eq.( 5 ) it follows that Z is 2 / 3-close to its mean with probability at least 3 / 4 as required.

Note that there is a tradeoff between the constant appearing in the minimum number of trials and that in the bias bound. Thus, in case one can assure a stricter bias bound for a similar cost it is worthwhile to recompute the corresponding trial number. 6 Frequent set sampling In the naive Monte Carlo algorithm sketched in the beginning of Sect. 5 the necessary number of trials was prohibitive, while the required uniform sampling from the power set did not pose a problem. Now the situation is different: The required number of trials is polynomi-ally bounded, but it is unclear how to sample from the frequent set families F i ( D , f ) for i = s + 1 ,..., n according to a distribution D i satisfying Eq.( 4 )for b = / 12 ( n  X  s ) as required for estimating the factors r i . In fact a general sampling algorithm satisfying this condition X  X n particular uniform or almost uniform sampling algorithms X  X ith a worst-case polynomial running time cannot be expected to exist in the light of Theorem 3 and the reduc-tion of counting frequent sets to sampling them from the previous subsection. Therefore, we design a sampling algorithm that performs well in practice while its worst-case convergence time is exponential. 6.1 Markov chain We approach this problem by setting F 0 = X  and then repeatedly applying the following randomized perturbation procedure F j  X  F j + 1 : 1. set F to F j 2. uniformly draw an k  X  X  1 ,..., i } 3. if k  X  F set F to ( F \{ k } ) ; otherwise: 4. if ( F  X  X  k } )  X  F then set F to ( F  X  X  k } ) . 5. with probability 1 / 2set F j + 1 to F j ; otherwise: set F j + 1 to F This procedure simulates one step of a Markov chain M i on F i with state transition probabilities where denotes symmetric difference. All  X  X emaining X  probability is assigned to the self-loops, i.e., P i ( F , F ) = 1  X  { F  X  F : F F = 1 } / 2 i . So the transition probabilities and thus the corresponding state transition matrix P i as well as the reachability relation of M i are symmetric.

Together with the fact that F is closed under taking subsets, this implies that M i is irreducible because all states are reachable from  X  . Moreover, there are non-zero self-loop probabilities for every state. This implies that M i is also aperiodic and together with irre-ducibility this means that M i is ergodic.

For an ergodic Markov chain we know that there is a unique distribution  X  that it con-verges to and that is stationary, i.e., P i  X  =  X  .Since P i is symmetric,  X  must be the uniform distribution on F i . Hence, simulating M i for sufficiently many steps can be used to sample a frequent set from F i uniformly at random as sufficient for satisfying Condition ( 4 ).
The question is, however, for how many steps l we have to simulate M i until P l i (  X  ,  X  ) is  X  X lose enough X  to the uniform distribution. The standard approach in approximate counting (by almost uniform sampling) is to derive an upper bound on the mixing time  X (/ 12 ( n  X  s )) and then use this upper bound to compute the necessary number of simulation steps. The from the uniform distribution is an upper bound to the maximum bias b . As stated earlier, for our problem this approach is infeasible: in line with Theorem 3 , a good worst-case bound to the mixing time  X  cannot be expected. Indeed, we can observe the following: Proposition 4 Fo r n  X  N the Markov chain M 2 n with frequency threshold f = 1 and at least 2 n  X  1 log ( 1 / 2 ) . Proof Let P ( X ) denote the power set of a set X .For n  X  N the 1-frequent sets of the dataset givenintheclaimare tance of a Markov chain induces a lower bound on its mixing time. The conductance of M 2 n is defined as ary distribution of leaving S given the current state is an element of S . The mixing time is then bounded from below as follows: Plugging this bound into Eq.( 6 ) yields the claim.

Intuitively, the reason for the slow mixing time on these instances is that the probability the other is very low compared to their sizes. 6.2 Heuristic step bound In order to circumvent the negative implications of Proposition 4 , we will simulate the Markov chain only for a heuristic number of steps X  X  number much smaller than the best theoreti-cal worst-case bound on the mixing time would yield. The justification for this approach is twofold:  X  The situation constructed in the proof of Proposition 4 is obviously rather artificial and  X  The mixing time is equal to a guaranteed number of steps such that the total variation The heuristic for the number of steps we simulate the chain M i is order as the expected number of steps until each item has been drawn at least once (coupon collector X  X  theorem) X  X  reasonable minimum requirement. Clearly, there are other possible choices for steps ( i , ) ; in particular when there is prior knowledge of the input dataset.
We evaluated this heuristic using = 0 . 5 on several test datasets (see Sect. 8 for a descrip-tion of these datasets). The relative thresholds used and the resulting numbers of frequent sets ( pc60pa10 ), respectively. These moderate state space sizes allowed us to explicitly com-crosses) are compared to the required maximum bias b = 1 / 24 n (blue line) for each of the ratios r i . Also, these figures show the total variation distance from the uniform distribution (red crosses). Indeed we can observe a significant gap between the quantities for some of the factors. Consequently, the total variation distances sometimes violate the maximum bias requirement while the actual biases do always satisfy this condition.

To give an idea of how conservative the heuristic is, we created another diagram, which is presented in Fig. 3 . Here, the minimum number of steps that is resulting in order to satisfy the bias requirement (red crosses) is compared to the number of steps computed by the heuristic (green line). We can observe that the heuristic is much less conservative for the first factors depend on the overall total number of factors while the maximum allowed bias does. Instead the heuristic only depend on the index of the ratio to be approximated, which is approaching the number of factors only for the later ratios. As we will see below this is no problem for our method, because finally we will end up with a hybrid counting algorithm that does not approximate the first factors at all. 7 Algorithmic details and improvements The basic algorithm assembled from the partitioning from Sect. 5 and the sampling from Sect. 6 has on overall time complexity of O  X  3 mn 3 ln n if we denote by m the number of transactions | D | and account O ( mn ) for one frequency check. While this time complexity does not depend on the number of frequents sets as desired, it performs rather poorly with performance improvement in practice. That said, they do not affect the overall asymptotic behavior.

A pseudocode incorporating all ideas is given with Algorithm 1 . Note that the requirement stated here is a little stronger than necessary in that it demands the bias bound to hold for i.e., for i &gt; s , and usually s is chosen larger than 0 (see Sect. 7.1 ). 7.1 Hybrid counting which having a time complexity that depends on the complete number of items n . In contrast exhaustive counting of F s only runs on a reduced dataset and has to enumerate at most 2 s frequent sets. This is more efficient as long as s is sufficiently small.

Deciding what choice for s will result in the minimum total running time requires some analysis of the involved time complexities. For that we regard one frequency check as unit step. For ease of notation we will drop all symbols that are constant throughout one call of the algorithm from the parameter lists of trials (  X  ) and steps (  X  ) .

Estimating r i requires at most trials ( s ) steps ( i ) frequency checks. On the other hand, counting | F s | exhaustively can take up to 2 s frequency checks in case all subsets of E s are frequent. This rough bound can be used to make an initial choice for s by choosing it such that it minimizes the estimated overall running time 2 s + T apx ( s ) with Algorithm 1 Frequent set counting denoting the expected time for approximating all remaining factors. Clearly, more knowledge about the used implementations X  X n particular that of the exhaustive miner X  X s likely to lead to an improvement of this choice.

Additionally, we can further improve our choice of the starting index s . Denote the index found by the considerations above as s  X  . The loose a priori bound used there can be improved long as the above condition remains true for the new s it amortizes to repeat this step. The resulting algorithm counts F s for the final s  X  X hunk-wise X  as For the exhaustive counting tasks it is desirable to use one of the existing highly optimized frequent set listing algorithm. Let D | i denote the dataset in which all transactions have been pute the latter quantity. 7.2 Basic chain simulation speedups The central part of the approximative phase of Algorithm 1 are the Markov chain simulations. naively. Instead, we first buffer the required number steps ( i , ) of random items for a single random walk. Since the result F of a random walk is only used to evaluate the Bernoulli experiment Z i of whether item i is an element of F , we can simply stop the Markov chain simulation after the last occurrence of i in the buffer, because afterwards the outcome of the experiment cannot change anymore. Similarly, if i was put into F due to the next to last occurrence of i we can also stop the chain simulation at that point. Since in this case the last occurrence of i will surely cause the item to be removed from F again, we can directly report that result.

The really dominant operation, however, is the test of whether a set I = F  X  X  e } that is an augmentation of a frequent set F with a single item e remains frequent. This corresponds to step 4 of the Markov chain (see Sect. 6 ). Since this test has to be performed roughly every second step of each random walk, it is crucial for the overall performance of our algorithm. In order to decrease the cost of this operation, our implementation makes use of the data compression that is enabled by representing the dataset D as an fp-tree [ 11 ]. An fp-tree is aprefixtree T = ( V , E , X ) where, in addition, there are node lists L i containing all nodes corresponding to prefixes X  X  V with max X = i . Since overlapping transactions share a prefix-branch, this usually achieves a significant compression.

The frequency test for a set is done by selecting its least frequent item max I , following its node list in the fp-tree, and adding the counts  X ( X ) of all nodes X  X  L max I = L with X  X  I , i.e., those corresponding to a prefix that contains I . The traversal of the node list can be stopped already after an initial part L  X  L in case the frequency state of I can already be determined by L , that is either if l = {  X ( X ) : X  X  L , X  X  I } X  f returning  X  X rue X , or if l + {  X ( X ) : X  X  L \ L } &lt; f returning  X  X alse X  where the last sum can be computed by | D [ max I ] |  X  {  X ( X ) : X  X  L } . 7.3 Infrequent set cache Even with the representation of D by an fp-tree, frequency checks remain an expensive operation. In order to reduce the computational effort for these operations one can store the infrequent sets that are visited throughout the various random walks. The thus created cache can then be used to quickly check whether I = F  X  X  e } is a superset of an already visited infrequent set I  X  X ossibly avoiding the expensive frequency check. This test is invoked in line 2 of the frequency check procedure of Algorithm 1 . Below we discuss how this step can be implemented.

Let us denote the family of visited infrequent sets by I . Again it is useful to represent this family by a prefix tree. A superset test can then be implemented as a depth first search traversal of the tree that recurses only into child nodes via edges corresponding to items that are contained in I . Moreover, we can ignore edges corresponding to items e &gt; e as long as the current node does not contain item e . Below this edge we can only find sets not containing e , none of which can be a subset of I . This is because we know that I \{ e }= F is frequent and, thereby, can never be superset of an infrequent set.

It is, however, not always reasonable to perform this infrequency check. There are candi-date sets I that require only very few nodes in the fp-tree to be checked using the standard frequency check; those with a relatively small max I . As we assume the items to be ordered according to their frequency, these candidates are in addition rather unlikely to be infrequent. In order to decide for what candidate sets infrequency should be checked against I it is necessary to estimate the expected look-up time. For that observe that the recursion depth of the DFS is bounded by min { | I | , d } where d = max I  X  I I is the depth of the prefix tree. Moreover, in a node X with outgoing edges  X  + ( X ) at level i the number of recursive calls can then be estimated as follows: where is the average outdegree of the prefix tree representing I . This quantity is compared to the number of nodes corresponding to max I in the fp-tree of D in line 2 of the frequency check procedure of Algorithm 1 . 8 Evaluation In this section, we present experiments contrasting the Markov chain Monte Carlo algorithm with counting via exhaustive enumeration. The experiments are performed with respect to performance as well as accuracy ( = 0 . 5 was used throughout all experiments). As a rep-resentative exhaustive miner, we used the modified FPgrowth [ 11 ] algorithm by Grahne and Zhu [ 8 ], whose C++ implementation is publicly available. This implementation has shown to rank among the fastest exhaustive miners in the competitive workshop FIMI [ 1 ]. In the following, we will refer to this implementation as  X  FpZhu  X . The benchmark datasets are also taken from the FIMI repository supplemented by synthetic datasets generated according to the beginner X  X  guide process (Example 2 ) with different choices for the probabilities p a and p c each of which with 5,000 transactions. We refer to these dataset as pc60pa10 and pc90pa10 , because they are generated by instantiations of this general process with prob-abilities p a = 0 . 1and p c = 0 . 6, respectively, p c = 0 . 9. For Algorithm 1 we used a Java implementation that is available online together with the artificial datasets ( http://www-kd. iai.uni-bonn.de/index.php?page=people_details&amp;id=16 or corresponding main page). Dur-ing the exhaustive phase the external miner called is again FpZhu . The experiments were performed on an Intel Core 2 Duo E8400 with 3 GB of RAM running Windows XP. All figures below use relative frequency thresholds. 8.1 Accuracy We now report a series of accuracy experiments that are summarized in Fig. 4 . Our algorithm was applied 100 times to each combination of one of eight test datasets with one of four frequency thresholds, resulting in a total of 400 runs per dataset. A run  X  X ails X  if the reported result deviates from the true number of frequent sets by more than = 0 . 5, which we used as accuracy parameter throughout all experiments. If the overall approximation guarantee holds we expect the fraction of failed runs to be below 1 / 4(Eq. 1 ). As desired, this was the case on all of the eight test datasets. In fact, we had no failed runs among all experiments. Thus, the observed success rate was consistent with the probabilistic approximation guarantee for all datasets, and in fact we experienced much better error bounds. The table also shows the upper quartile ( X  X p. qrt. X ), median ( X  X edian X ), lower quartile ( X  X w. qrt. X ), and maximum ( X  X ax. X ) of the experienced relative deviation.

These results may lead to the impression that the upper absolute bias bound b and the number of trials t have been chosen to conservative in Sect. 5.1 and less restrictive numbers would still yield the overall approximation guarantee. But in fact this good performance is partially caused by mutual cancellation of the actual bias terms, i.e., some of them are nega-tive and some are positive. Moreover, the number of trials t takes into account the worst-case r = 1 / 2. For the test datasets, however, many of the ratios r i are close 1.

Figure 5 shows accuracy results on chess in more detail. For different frequency thresh-olds, it shows on a logarithmic scale the exact number of sets ( X  X xact count X ) computed exhaustively, the upper and lower 0 . 5 deviation limits ( X  X pper limit X  and  X  X ower limit X ), as well as the result of the randomized algorithm in a series of 15 runs ( X  X cmc count X ). The figure shows that in all randomized runs, the approximated result lies in the desired devia-tion interval. The figure does not contain the exact number of sets for the lowest thresholds, because these could not be computed by the exhaustive miner.

We close this subsection by presenting approximated frequency plots computed for four of the datasets. The results are shown in Fig. 6 together with the exact curve. They illustrate that the randomized algorithm provides an adequate approximation of the exact plot, which was one motivating goal of this study (see Sect. 2 ). 8.2 Runtime Next, we compare the runtime of our randomized algorithm with that of FpZhu on sev-eral datasets. We start with the results on chess , which are presented in Fig. 7 a. In order to give an idea of the quality of FpZhu we also added the performance curve of an Apriori implementation [ 4 ], which was another contender in the FIMI workshop. But most important the diagram shows that while on higher thresholds FpZhu is faster than our randomized algorithm, the latter outperforms the exact algorithm when the threshold becomes smaller. A similar behavior can be observed for connect (Fig. 7 b), pc60pa10 (Fig. 7 c), and pumsb (Fig. 7 e). On pc90pa10 (Fig. 7 d), which is extremely dense and as a result has a very high number of frequent sets, the randomized approach outperforms the exhaustive miner on all threshold. In fact, for all but the highest thresholds the runtime of the exhaustive miner becomes unacceptable.

However, the sparser the dataset is the later the randomized algorithm catches up to the exhaustive miner. On mushroom (Fig. 7 f) it is even dominated on all thresholds. Hence, it heavily depends on the dataset X  X  density (the number of frequent sets) whether the randomized or the exhaustive approach performs better. This can roughly be explained and summarized as follows: while the complexity of the Monte Carlo algorithm, in contrast to any exhaustive miner, does not depend on the output size, it does not scale as well as the latter in the input size (see also the discussion in Sect. 9 ).
We close the performance study with the comparison of Algorithm 1 to implementations that (a) do not use the infrequent set cache and (b) that do neither use hybrid counting nor the infrequent set cache. Figure 8 shows the impact of these two speedups. 9Conclusion In this section, we give a summarizing discussion of our study that emphasizes both: its contributions as well as its current limitations. Thereafter, we conclude by presenting ideas for future research. In this course, we outline promising directions to overcome the said limitations. 9.1 Summary and discussion In this paper, we developed a randomized approximation scheme for counting the number of frequent sets using the Markov chain Monte Carlo method. It relies on sampling frequent sets in a way that satisfies a certain bias bound. We gave a corresponding sampling procedure that, albeit meeting this bound, in worst case does so only after an exponential number of iterations. For that reason, we switched to a heuristic but polynomially bounded number of steps that we validated on several test datasets. Although this heuristic approach works well on these datasets, by applying it, the overall counting algorithm loses its worst-case approximation guarantee. We have shown, however, by giving a negative complexity result that a general polynomial algorithm with a good approximation guarantee is unlikely to exists. Moreover, we experienced very good approximation rates on real world and artificial dataset. In order to improve the performance of our method we described several techniques that can significantly speed up its running time when compared to a naive implementation. We demonstrated that for dense datasets/low frequency thresholds the randomized algorithm remains well applicable while exhaustive counting is infeasible.

Altogether, our study should be regarded as an initial work on a computational problem general preprocessing method to frequent set mining, still suffers from serious limitations: (1) The method scales very badly in the number of items, i.e., super-cubic, which makes it (2) Even on datasets with a small number of items it is often outperformed by exhaustive While the first point appears to be inherent to the method and its circumvention is likely to require fundamental additional ideas, the second can probably be solved by some straight-forward additions (see the paragraph about integrated exhaustive counting below). Beside larger constant factors in its complexity the relative weakness of the current implementation on some datasets/thresholds can be explained as follows: exhaustive mining is a systematic process that can optimally make use of reduced data portions. In contrast, for randomized counting it is inherent that the part of the data that is needed next is unpredictable. Con-sequently, the randomized method suffers more from the size of the input data (here, in particular from the number of transactions) than the exhaustive method does. 9.2 Directions for future research 9.2.1 Integrated exhaustive counting In principle issue (2) above is addressed by the hybrid counting technique presented in Sect. 7.1 . But the approach of  X  X lugging in X  any exhaustive miner in our current implemen-tation is causing a lot of unnecessary overhead by external calls and disk accesses. Clearly, this can be tackled by an integrated and well implemented exhaustive counting algorithm that, in addition, should have an internal enumeration order compatible to the subfamilies F , F 2 ,... . Such an algorithm could pass on control to the approximative algorithm as soon as the cost of the next search level would outweigh the cost of approximating the next factor, thus finding an optimal starting index s . It is likely that this improvement alone leads to a hybrid counting algorithm that is never significantly outperformed by any exhaustive miner. 9.2.2 Improved lower bounds Finding better lower bounds for the means of the random variables  X  Z i , respectively, for the ratios of their variance to their squared expectation would allow to reduce the required number of trials, and thus to reduce the constants. This can either be done by monitoring these quantities empirically in a sequential sampling fashion (cp. [ 21 ]) or by an exhaustive analysis of certain parts of the input dataset. Specifically for the task of computing the com-plete frequency plot, i.e., solving #-FREQUENT SETS for all thresholds, one can acquire a for threshold f  X  1. 9.2.3 Better scaling in the number of items One approach to achieve a better scaling in the number of items is to reduce the data in a way that (approximately) preserves the number of frequent sets. An example for this is the sketch matrix technique of Jin et al. [ 14 ]. Essentially, this method performs a bi-clustering on the items and transactions, and then approximates the number of frequent sets only based on statistics that are computed for each bi-cluster.

Another approach is to redefine the partitioning such that not only one but also a block of k items is introduced at each level, i.e., F i ={ 1 ,..., ik } . While this results in weaker lower bounds per factor, which ideally should be addressed by empirical estimates (see the paragraph above), it divides the number of factors by k and consequently, also reduces the requirement on the maximum bias. 9.2.4 Using more structural properties It is important to point out that both, the Monte Carlo framework as well as the Markov chain used for sampling, do not specifically rely on the fact that F is the family of frequent sets of a transactional dataset. Instead the top-level analysis only uses the property that F is closed  X  X ownward X , i.e., under taking subsets. This is also true for most of the speedups presented in Sect. 7 . Thus, the major part of Algorithm 1 is applicable to other pattern mining tasks that use an anti-monotone interestingness predicate. On the other hand this is a strictly more general problem as there are anti-monotone set families (independence systems) that cannot concisely be represented as the family of frequent sets of a transactional dataset [ 22 ]. Conse-quently, for the specific task of counting frequent sets a major route for potential improvement is to investigate whether there are partitionings as well as sampling methods that exploit more structural properties of the problem. 9.2.5 Beyond frequent sets On a more global scale, an important next step is to investigate whether the randomized counting approach can be extended to other pattern classes like closed sets or graph mining. The first one induces set systems that are not closed downward, and the second is not even representable as sets at all. Thus, a straightforward application of the techniques discussed in this paper is impossible. This extension, however, promises to be both, challenging and beneficial, as the cost of exhaustive mining generally increases with the pattern complexity. References Author Biographies
