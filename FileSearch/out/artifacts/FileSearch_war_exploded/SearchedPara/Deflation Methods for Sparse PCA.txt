 the covariance matrix to eliminate the influence of that eige nvector. eigenvectors X , sparse loadings that explain a maximal amou nt variance in the data. datasets. deflation techniques on real-world datasets.
 Notation I is the identity matrix. S p Card ( x ) represents the cardinality of or number of non-zero entries in the vector x . discuss deflation in the context of PCA and then consider its e xtension to sparse PCA. 2.1 Hotelling X  X  deflation and PCA A 0  X  S of and we then use Hotelling X  X  deflation to annihilate x The deflation step ensures that the t + 1 -st leading eigenvector of A A . The following proposition explains why.
 Proposition 2.1. If  X  eigenvectors, and  X  A = A  X  x with corresponding eigenvalues  X  P vector of A 2.2 Hotelling X  X  deflation and sparse PCA cardinality-constrained version of Eq. (1): eigenvector.
 following example.
 Example. Let C = 2 1 deflated matrix. Then  X  C = 0 1 eigenvector.
 That S p x pseudo-eigenvectors. 2.3 Alternative deflation techniques techniques discussed below. 2.3.1 Projection deflation  X  Note that when x Hotelling X  X  deflation: However, in the general case, when x is preserved: where z = ( I  X  x orthogonal to x annihilates all covariances with x 2.3.2 Schur complement deflation matrix V =  X   X  x we arrive at a new deflation technique: see this, suppose A as Furthermore, Schur complement deflation renders x symmetric and A Additionally, Schur complement deflation reduces to Hotell ing X  X  deflation when x of A t  X  1 with eigenvalue  X  t 6 = 0 : is a mean-centered data matrix, x  X  R p has unit norm, and  X  Y = ( I  X  Y xx T Y T  X  2.3.3 Orthogonalized deflation additional variance explained by the t -th pseudo-eigenvector, x plained by the component of x q hilating the full vector x parallel to previously annihilated vectors. Consider the f ollowing example: Example. Let C C x be easily expressed in terms of a running Gram-Schmidt decom position for t &gt; 1 : where q basis for the space spanned by x orthogonal projection.
 That is, if a vector v is orthogonal to A A deflation.
 Proof. Consider the t -th round of Schur complement deflation. We may write x p is in the subspace spanned by all previously extracted pseud o-eigenvectors and o to this subspace. Then we know that A and A Further, A A mization objective on each round.
 maximizing the additional variance of each new vector naturally suggests itself. On round t , the additional variance of a vector x is given by q T A 0 q ance matrix, q = ( I  X  P pseudo-eigenvectors x eigenvalue problem, If we let q spanned by x technique into the following algorithm for sparse PCA: Algorithm 1 Generalized Deflation Method for Sparse PCA Given: A Execute: Return: { x cardinality-constrained generalized eigenvalue problem . thogonalized Hotelling X  X  (OHD), orthogonalized projecti on (OPD), and generalized (GD). 4.1 Pit props dataset sparse loadings, each constrained to have cardinality less than or equal to k the pit props dataset capture 87% of the variance. Props dataset.
 Hotelling X  X  underperform the remaining techniques.
 Pit Props dataset. 4.2 Gene expression data and report the results in Table 4.
 loadings of GSLDA on the BDTNP VirtualEmbryo.
 deflations, both of which explain less than 76% of variance af ter 8 rounds. Acknowledgments This work was supported by AT&amp;T through the AT&amp;T Labs Fellows hip Program. [7] I.T. Jolliffe, Principal component analysis, Springer Verlag, New York, 1986.
