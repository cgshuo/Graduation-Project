 Traditionally, searching for patterns in graphs has been almost exclusively lim-ited to searching for frequent subgra phs. A frequent subgraph is a structure within a graph where nodes with certain labels are fre quently interconnected in exactly the same way. We propose to rel ax this requirement. We claim that a pattern, a set of node labels, is interesting if these labels often occur in the graph near each other, not necessarily with exactly the same edges between them.
Consider the graph given in Fig. 1. It can be very easily observed that pattern abcd clearly stands out. However, traditio nal approaches, even with a frequency threshold as low as 2, will not find a single subgraph of size larger than 2, since nodes labelled a , b , c and d are always interconnected differently. This demonstrates the need for a new approach.

On top of being more flexible, and thus capable of finding previously unde-tected patterns, our method also greatly reduces the search space by looking for itemsets alone, rather than considerin g all possible combinations of edges and paths connecting them. However, if the dataset does contain a frequent sub-graph, our method will find the equivalent itemset consisting of the same labels, but without the edges connecting them.

We consider two different problem setti ngs, as the dataset can consist either of a (very large) single graph, or of a set of (smaller) graphs. In the single graph setting, the goal is to find itemsets that reoccur within the graph. We propose two methods to achieve this goal. The first is based on the traditional approaches to mining frequent itemsets in transactio n databases. Our main contribution here consists of a way to transform a graph into a transaction database, after which any existing itemset mining algorithm can be used to find the frequent itemsets. In our second approach, we look for cohesive itemsets, whereby we insist that for an itemset to be considered interest ing, it should not only appear often, but its items should also never appear far from each other. In the multiple graph setting, the goal is to find itemsets that occur in many of the input graphs. Here, the dataset cannot be transformed into a typical transaction database. However, the cohesive itemset approach proves perfectly adaptable to this setting.
An interesting property of the cohesive itemset approach is that the proposed interestingness measure is not anti-mono tonic, which, at first glance, might rep-resent an obstacle when generating large candidate itemsets. However, in both the single and the multiple graph setting we managed to come up with an effi-cient pruning method, which enables us to mine cohesive itemsets in a reasonable amount of time.

Even though the problem setting of subgraph mining is very different to that of mining itemsets in graphs, and therefore mostly incomparable, we do note that our algorithm results in a massive reduction in output.

The paper is organised as follows. In Section 2, we present the main related work. In Section 3 we propose two methods of identifying interesting itemsets in the single graph setting. In Section 4, we extend these approaches to be able to handle datasets consisting of multiple graphs. In Section 5, we give a sketch of our algorithms, before presenting the r esults of our experiments in Section 6. We end the paper with our conclusions in Section 7. The problem of discovering subgraphs that occur frequently in a dataset con-sisting of either one or multiple graphs has been very popular in data mining research. A good survey of the early graph based data mining methods is given by Washio and Motoda [18]. The first studies to find subgraph patterns were conducted by Cook and Holder [3] for a single graph, and by Motoda and In-durkhya [21] for multiple graphs, both using a greedy scheme to find some of the most prevalent subgraphs. Although this greedy search may miss some significant subgraphs, it avoids the high complexity of the graph isomorphism problem. Inokuchi et al. [9] and Kuramochi and Karypis [13] proposed the Agm and Fsg algorithms, respectively, for mining all frequent subgraphs, using a level-wise approach, similar to that of Apriori [1]. They suffer from two additional draw-backs, however: costly subgraph isomorphism testing and an enormous number of candidates that are generated along the way (due to the fact that both edges and nodes must be added to the pattern). Yan and Han [19] proposed gspan , which performs a depth-first search, based on a pattern growth principle similar to the one used in Fp-growth [7] for mining itemsets. Nijssen et al. proposed a more efficient frequent subgraph mining tool, called Gaston , which finds the frequent substructures in a number of phases of increasing complexity [15]. More specifically, it first searches for frequent paths, then frequent free trees and finally cyclic graphs. Further attempts at mi ning frequent subgraphs, both in one or many input graphs, have been made by Bringmann and Nijssen [2], Kuramochi and Karypis [14], Huan et al. [8], Yan et al.[20] and Inokuchi et al. [10].
Up to now, however, most of the research in graph mining has gone into finding frequent subgraphs. We focus on mi ning interesting itemsets in graphs, thus relaxing the underlying structure of the pattern. By doing so, we avoid the costly isomorphism testing, and by using a depth-first search algorithm, we avoid the pitfalls of Apriori -like algorithms. A similar approach has been proposed by Khan et al. [12], where nodes propagate their labels to other nodes in the neighbourhood, according to given prob abilities. Labels are thus aggregated, and can be mined as itemsets in the resulting graph. Khan et al. also propose to solve the problem of query-based search in a graph using a similar method [11]. Silva et al. [16, 17] and Guan et al. [6] introduce methods to identify correlation between node labels and graph structure, whereby the subgraph constraint has been loosened, but some structural info rmation is still present in the output. Formally, we define a graph G as the set of nodes V ( G ) and the set of edges E ( G ), where each node v  X  V ( G ) has a label l ( v ). We assume that the graph is connected, and that each node carries at mo st one label. In this setting, a pattern is an itemset X , a set of node labels that frequently occur in graph G in each other X  X  neighbourhood. In Sections 3.1 and 3.2 we propose two ways of defining and finding interesting patterns in a graph. Note that we can also handle input graphs that contain nodes with multiple labels, by transforming each such node into a clique of nodes, each carrying one label. 3.1 Frequent Itemsets Approach One way of defining an interesting itemset is by simply looking at its frequency. An interesting itemset is one that occurs often in the dataset. Our challenge therefore consists of taking a graph and converting it into a transaction database. In essence, we are looking for items that often appear near each other in the graph, so we propose to create a transaction database in which each transaction would correspond to the neighbourhood of a single node in the graph. For a node v , we define the corresponding transaction as t ( v )= { l ( w ) | w  X  G,d ( v,w )  X  n } , where d ( v,w ) is the distance from v to w and n is the neighbourhood size, a user-defined parameter, indicating how far w can be from v and still be considered a part of v  X  X  neighbourhood. Typically, n should be relatively small if meaningful results are to be obtained. In a we ll-connected graph, with a large n , all nodes will be in each other X  X  neighbourhoods.

We define a transaction database of graph G as T ( G )= { t ( v ) | v  X  G } . The frequency of an itemset X in graph G , fr G ( X ), is defined as the number of transactions in T ( G )thatcontain X . An itemset is considered frequent if its frequency is greater than or equal to a user-defined frequency threshold min freq . Once we have converted our original dataset in this way, we can apply any one of a number of existing frequent itemset algorithms to obtain the desired results.
Let us return to our example in Fig. 1. Given a neighbourhood size of 1, the resulting transaction database is given in Table 1. We see that itemset abcd will now be discovered as interesting. In fact, only its subsets of size 1 or 2 will score higher.
 3.2 Cohesive Itemset Approach Another possible approach is to look for cohesive, rather than simply frequent, itemsets. This idea was inspired by the approach Cule et al. applied in order to find interesting itemsets in event sequences [5]. The idea is not to find items that only occur near each other often, but items that imply the occurrence of each other nearby with a high enough probability.

Consider, for example, the graph gi ven in Fig. 2. We see that patterns de and bc are both frequent, though de will score higher than bc , both in subgraph mining and in frequent itemset mining as defined in Section 3.1. However, it can be argued that the value of itemset de should diminish due to the fact that a d appears in the graph without an e nearby, while each b has a c rightnexttoit, and vice versa. Here, we present an a pproach that takes this into account.
To start with, we introduce some notations and definitions. In a graph G , the set of nodes is denoted V ( G ). The number of nodes in G is denoted | V ( G ) | . For an itemset X , we denote the set of nodes labelled by an item in X as N ( X )= { v  X  G | l ( v )  X  X } . The number of such nodes is denoted | N ( X ) | . Finally, we define the probability of randomly encountering a node in G labelled by an item in X as the coverage of X in G ,or P ( X )= | N ( X ) | | G | .
For each occurrence of an item of X , we must now look for the nearest oc-currence of all other items in X .Foranode v , we define the sum of all these smallest distances as W ( v,X )= x  X  X min w  X  N ( { x } ) d ( v,w ) . We then compute the average of such sums for all occu rrences of items making up itemset X , in G as C ( X )= | X | X  1 the items in X are in G on average. If they are always right next to each other, the sum of these distances for each occurrence of an item in X will be equal to |
X | X  1, as will the average of such sums, and the cohesion of X will therefore be equal to 1.

Finally, the interestingness of an itemset X is defined as the product of its coverage and its cohesion, I ( X )= P ( X ) C ( X ). An itemset is considered interest-ing if its interestingness is greater than or equal to a user-defined interestingness threshold, min int . Unlike with frequent itemsets, where we could tell nothing about the cohesion of the itemsets, we are now able to say that having en-countered an item from an interesting itemset, there is a high probability of encountering the rest of the itemset nearby.

Let us now return to the example given in Fig. 2. If we apply these measures to itemsets bc and de ,wefirstnotethat P ( bc )=5 / 13 and P ( de )=7 / 13. In order to compute the cohesion of the two itemsets, we first have to compute W ( bc )and W ( de ), which are respectively 1 and 10 7 (note that all relevant minimal windows are of size 1, except W ( v 5 ,de ) = 4). Therefore, C ( bc )=1and C ( de )= 7 10 ,and I ( bc )=0 . 385 and I ( de )=0 . 377. We see that the value of itemset de has indeed diminished due to a d occurring far from any e .

Finally, note that while our method allows us to find more flexible patterns than subgraph mining, we do not miss out on any pattern subgraph mining can discover. If a graph contains many occurrences of a subgraph consisting of nodes labelled a , b and c , then we will find abc as an interesting itemset. In many applications, the dataset does not consist of a single graph, but of a collection of graphs. Formally, as before, we define a graph G as a set of nodes V ( G ) and a set of edges E ( G ), where each node v  X  G has a label l ( v ). We also define G as a set of graphs { G 1 ,...,G n } , and assume that each graph in G is connected. A pattern is now an itemset X , a set of node labels that frequently occur in the set of graphs G in each other X  X  neighbourhood. In other words, for a pattern to be interesting, it needs to appear in a cohesive form in many graphs. Unlike the single graph setting, the multiple graph setting does not allow us to transform the dataset into a transaction database. However, the cohesive itemset approach, presented in Section 3.2, can be generalised to the multiple graph setting in a relatively straightforward manner.
 We first revisit the notations introduced in Section 3.2. Given a set of graphs G , the number of graphs in G is denoted |G| . We denote the set of all graphs that contain itemset X as N m ( X )= { G  X  X | X  x  X  X  X  v  X  V ( G ) with l ( v )= x } . The number of such graphs is denoted as | N m ( X ) | . Finally, we define the probability of encountering a graph in G containing the whole of itemset X as the coverage
Given a graph G j in N m ( X ), we must now look for the most cohesive occur-rence of X . To find such an occurrence, we will look for a node in the graph, labelled by an item in X , from which the sum of the distances to all other items in X is the smallest. Given a graph G j containing an itemset X , we define this lowest sum as W ( X,j )=min v  X  N ( X,G nodes in G j labelled by an item in X , while W ( v,X ) is defined as in Section 3.2.
We now compute the average of such smallest sums for all graphs in G con-the cohesion of an itemset X in G as C m ( X )= | X | X  1 cohesive itemset will have cohesion equal to 1.

Finally, the interestingness of an itemset X in G is defined as the product of its coverage and its cohesion, I m ( X )= P m ( X ) C m ( X ). Mining frequent itemsets in graphs can be done by transforming a graph into a transaction database, and then using an existing itemset miner to generate the output. However, the cohesive itemset approach is less straightforward, and we now present our algorithms, Grit and Mug , for solving this problem in the single graph setting and the multiple graph setting, respectively. 5.1 Single Graph Setting The fact that our interestingness measure is not anti-monotonic clearly rep-resents a problem. We will sometimes need to search deeper even when we encounter an uninteresting itemset, as one of its supersets could still prove in-teresting. Traversing the complete sea rch space is unfeasible, so we will need a different pruning technique to speed up our algorithm. We adapt the approach introduced by Cule et al. for mining itemsets in sequences [5] to our setting.
We approach the problem using depth-first search, and the pseudocode of the main Grit algorithm is provided in Algorithm 1. The first call to the algorithm is made with X empty and Y containing all possible items. At the heart of the algorithm is the UBI pruning function, used to decide when to prune a complete branch of the search tree, and when to p roceed deeper. Essentially, we can prune a complete branch if we are certain that no itemset generated within this branch can be interesting. To be able to ascertain this, we compute an upper bound for the interestingness of all these itemsets, and prune the branch if this upper bound is smaller than the interestingness threshold. We begin by noting that, for each Z , such that X  X  Z  X  X  X  Y , it holds that | N ( Z ) | X | N ( X  X  Y ) | , | Z | X | X  X  Y | and v  X  N ( X ) W ( v,X )  X  v  X  N ( Z ) W ( v,Z ). Expanding the definition of the for the interestingness of all itemsets Z , that can be generated in a branch of the search tree starting off with itemset X , and reaching as deep as itemset X  X  Y .
However, while this upper bound is theoretically sound, it is also computa-tionally very expensive. Note that we would need to compute v  X  N ( X ) W ( v,X ) at each node in our search tree. This would require traversing the whole graph searching for the minimal distances between all items in X for all relevant nodes. This, too, would be infeasible. Luckily, if we express these sums differently, we can avoid these computationally expensive database scans. Adapting the ap-proach introduced by Cule and Goethals [4] to the graph setting, we first note that the sum of the minimal distances between items making up an itemset X and the remaining items in X can also be expressed as a sum of sepa-rate sums of such distances for each item individually, v  X  N ( X ) W ( v,X )= x  X  X v  X  N ( { x } ) W ( v,X ). We then note that each such sum for an occurrence of an item x  X  X is equal to the sum of individual minimal distances between the same occurrence of x and any other item y  X  X . For the sum of such distances, plify our notation, from now on we will denote v  X  N ( { x } ) W ( v,xy )by s ( x, y ). Finally, we see that v  X  N ( X ) W ( v,X )= x  X  X y  X  X \{ x } ( s ( x, y )), giving us a much more elegant way to compute the sum of distances between an occur-rence of an item in X and the rest of X for all nodes labelled by an item in X . Finally, we are ready to define our upper-bound-based pruning function:
This pruning function is easily evaluated, as all it requires is that we store s ( x, y ), the sum of minimal distances between x and y over all occurrences of x , for each pair of items ( x, y ), so we can look them up when necessary. This can be done as soon as the dataset has been read, and all entries can be computed efficiently. Note that if Y is empty, then UBI ( X,Y )= I ( X ), so if we reach a leaf node in the search tree, we can immediately output the itemset. 5.2 Multiple Graph Setting As in the single graph setting, the interestingness measure based on cohesive itemsets in multiple graphs is also not anti-monotonic. Here, however, the cov-erage alone is anti-monotonic. Given itemsets X and Y , such that X  X  Y ,itis clear that any graph that contains Y will also contain X . Keeping in mind that the interestingness of an itemset is never greater than its coverage, this allows us to prune even more candidates from our search space. For any itemset Z ,such that X  X  Z , it holds that I ( Z )  X  P ( Z )  X  P ( X ). Therefore, if we encounter an itemset X , such that its coverage is lower than the interestingness threshold min int , we can safely discard all its supersets from the search space.
On top of this pruning criterion, we can develop a pruning function MUBI in much the same way as we did in the single graph setting above. Once again, it would be infeasible to compute all necessary sums of minimal windows at each node in our search tree. However, this time we cannot express this sum using similar sums for pairs of items as we did in Section 5.1. The reason for this is the fact that we now define W ( X,j ) as the minimal occurrence of X in graph G j , while in Section 3.2 we defined W ( v,X ) as a sum of individual minimal distances between items. Given a graph G j andanitemset X , knowing the individual minimal distances between the items of X in G j would bring us no closer to knowing the size of the minimal occurrence of X in G j .Wehave therefore decided to perform all our experiments without MUBI  X  pruning less, but faster. As a result, our Mug algorithm for mining interesting itemsets in multiple graphs is exactly the same as the one given in Algorithm 1, with line if P ( X )  X  min int then replacing line 1. For our single graph setting experiments, we generated a number of synthetic datasets. To start with, we generated a graph with 10 000 nodes, randomly allocating labels ranging from 1 to 20. We made sure that labels 0 and 1 were more probable than all the others, followed by labels 2 and 3, while the remain-ing labels were all equally probable. We build the graph by, at each step, adding either a new node and connecting it to a random existing node, or adding a new edge between two existing nodes. In our first set of experiments, we set the prob-ability of adding a new node to 60%, and the probability of adding a new edge to 40%, resulting in a relatively sparse graph, with around 1.7 edges per node on average. The characteristics of the input graph and the results of the experi-ments for both the frequent itemset approach and the cohesive itemset approach are presented in the top quarter of Table 2. For the frequent itemset approach, the reported runtime is the sum of the time needed to transform the dataset into a transaction database added to the time needed to run an implementation 1 of the classical Fp-growth algorithm on the transformed dataset.

To examine how our methods react to different types of input, we created three more graphs, each time changing one of the settings. In the second set of experiments, we introduced some noise into the dataset, by randomly choosing 100 nodes in the original graph and changing their labels to a random item ranging from 21 to 30. For the third set of experiments, we created a denser graph, by setting the probability of adding a new node to 15%, and the proba-bility of adding a new edge to 85%. In the fourth set of experiments, we used a larger graph, creating 100 000 nodes, keeping the other settings as in the original dataset. The characteristics of these three input graphs and the results of our experiments can be seen in the bottom three quarters of Table 2.

In all four sets of experiments, we not e that frequent itemset mining works very fast once the transformation of the dataset has been done, while the co-hesive itemset approach suffers from having to compute s ( x, y ) for each pair of items x and y , regardless of the threshold. Grit struggles with noisy data, as expensive computations need to be done even for very rare items and itemsets, while transforming the graph into a transaction database takes a lot more time when dealing with both a denser or a larger input graph. Finally, all experiments on the four datasets produ ced expected results in terms of the discovered item-sets  X  items 0 and 1 were ranked top amongst items, while itemset { 0 , 1 } was the highest ranked pair of items. Both methods discovered the same itemsets of size 3 and 4 (itemsets made up of items 0, 1, 2 and 3), while results differed for itemsets of size 5 or more. However, due to the anti-monotonicity of the traditional frequency measure, Fp-growth ranked singletons higher than their supersets, while Grit considered itemset { 0 , 1 , 2 , 3 } , for example, more inter-esting than each individual item alone. We conclude that both approaches have their respective merits, with the frequen t itemset approach being faster, and the cohesive itemset approach more intuitive. Grit will first find large, informative itemsets, while Fp-growth first finds singletons, and the really interesting large itemsets can be buried beneath a pile of smaller itemsets.

In the multiple graph setting, we experimented on a dataset consisting of 340 chemical compounds, made up of 66 different atom types. However, we discov-ered five unconnected graphs in the dat aset and removed them, as the cohesive itemset approach can only be applied to connected graphs, leaving us with 335 input graphs. We compare our algorithm for finding cohesive itemsets in multi-ple graphs with the Gaston tool developed by Nijssen and Kok [15]. Both the dataset and the implementation are available online 2 . We present the results of our experiments in Tables 3 and 4.

By comparing our Mug algorithm to a frequent subgraph miner, we are in fact comparing apples and oranges. While Mug uses an interestingness measure, Gaston uses a frequency threshold. Mug searches for itemsets, Gaston dis-covers subgraphs. Mug discards structure and focuses on items in each other X  X  neighbourhoods, while Gaston focuses on structure. Clearly, Mug results in a massive reduction in output, as, for a typical interesting itemset discovered by Mug , Gaston will find a number of combinations of edges (and their subsets) that are frequent. Furthermore, the actual patterns discovered by the two algo-rithms differ greatly. To name but one example, the most interesting itemset of size 4 that we discovered was { 0 , 1 , 4 , 5 } , while the most frequent subgraph of size 4 was 0  X  0  X  0  X  0. On the other hand, if we compare only patterns of size 2,
Gaston finds graph 1  X  9 as the most frequent, while we also found { 1 , 9 } as highest ranked itemset of size 2. However, it is also important to note that itemset { 0 , 1 , 4 , 5 } ranked as one of the best itemsets overall in our output, while, due to the nature of frequent subgraph mining, graph 0  X  0  X  0  X  0wasranked below all its subgraphs. As with Grit , Mug has the advantage of ranking the larger interesting itemsets above all other, while Gaston will always rank a large pattern below a number of smaller patterns. In this paper we presented a number of new methods to identify interesting itemsets in one or many input graphs. For one graph, such itemsets consist of items that often occur close to each other. Unlike previous approaches that typically look for structures connecting these items, we only look at the distances between the items themselves. This enables us to avoid the typical pitfalls of subgraph mining  X  costly isomorphism checks and a huge number of candidates. On top of the classical frequent itemset approach that we adapted to mining itemsets in a large graph, we propose a second method, mining cohesive itemsets, consisting of items that appear close to each other frequently enough. This second approach proved perfectly adaptable to the multiple graph setting, too.
