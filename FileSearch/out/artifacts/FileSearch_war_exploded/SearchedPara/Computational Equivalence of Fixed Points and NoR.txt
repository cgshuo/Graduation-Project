 We consider a setting where a number of agents need to repeatedly make decisions in the face of uncertainty. In each round, the agent obtains a payoff based on the decision she chose. Each agent would like to be able to maximize her payoff. While this might seem like a natural objective, it may be impossible to achieve without placing restrictions on the kind of payoffs that can arise. For instance, if the payoffs were adversarially chosen, then the agent X  X  task would become essentially hopeless.
 In such a situation, one way for the agent to cope with the uncertainty is to aim for a relative benchmark rather an absolute one. The notion of regret minimization captures this intuition. We imagine that the agent has a choice of several well-defined ways to change her decision, and now the agent aims to maximize her payoff relative to what she could have obtained had she changed her decisions in a consistent manner. As an example of what we mean by consistent changes, a possible objective could be to maximize her payoff relative to the most she could have achieved by choosing some fixed decision in all the rounds. The difference between these payoffs is known as external regret in the game theory literature. Another notion is that of internal regret , which arises when the possible ways to change are the ones that switch from some decision i to another, j , whenever the agent chose decision i , leaving all other decisions unchanged.
 modifiers (also called deviations)  X  if the average payoff of an agent using the algorithm converges to the largest average payoff she would have achieved had she changed her decisions using a fixed decision modifier in all the rounds. Based on what set of decision modifiers are under consideration, various no regret algorithms are known (for e.g. Hannan [10] gave algorithms to minimize external regret, and Hart and Mas-Collel [11] give algorithms to minimize internal regret). The reason no regret algorithms are so appealing, apart from the fact that they model rational behav-ior of agents in the face of uncertainty, is that in various cases it can be shown that using no regret algorithms guides the overall play towards a game theoretic equilibrium. For example, Freund and Schapire [7] show that in a zero-sum game, if all agents use a no external regret algorithm, then the empirical distribution of the play converges to the set of minimax equilibria. Similarly, Hart and Mas-Collel [11] show that if all agents use a no internal regret algorithm, then the empirical distribution of the play converges to the set of correlated equilibria.
 In general, given a set of decision modifiers  X  , we can define a notion of game theoretic equilibrium that is based on the property of being stable under deviations specified by  X  . This is a joint distri-bution on the agents X  decisions that ensures that the expected payoff to any agent is no less than the most she could achieve if she decided to unilaterally (and consistently) decided to deviate from her suggested action using any decision modifier in  X  . One can then show that if all agents use a  X  -no regret algorithm, then the empirical distribution of the play converges to the set of  X  -equilibria. This brings us to the question of whether it is possible to design no regret algorithms for various sets of decision modifiers  X  . In this paper, we design algorithms which achieve no regret with respect to  X  for a very general setting of arbitrary convex compact decision spaces, arbitrary concave payoff functions, and arbitrary continuous decision modifiers. Our method works as long as it is possible to compute approximate fixed points for (convex combinations) of decision modifiers in  X  . Our algorithms are based on a connection to the framework of Online Convex Optimization (see, e.g. [18]) and we show how to apply known learning algorithms to obtain  X  -no regret algorithms. The generality of our connection allows us to use various sophisticated Online Convex Optimization algorithms which can exploit various structural properties of the utility functions and guarantee a faster rate of convergence to the equilibrium.
 Previous work by Greenwald and Jafari [9] gave algorithms for the case when the decision space is the simplex of probability distributions over the agents X  decisions, the payoff functions are linear, and the decision modifiers are also linear. Their algorithm, based on the work of Hart and Mas-Collel [11], uses a version of Blackwell X  X  Approachability Theorem, and also needs to computes fixed points of the decision modifiers. Since these modifiers are linear, it is possible to compute fixed points for them by computing the stationary distribution of an appropriate stochastic matrix (say, by computing its top eigenvector).
 Computing Brouwer fixed points of continuous functions is in general a very hard problem (it is PPAD -complete, as shown by Papadimitriou [15]). Fixed points are ubiquitous in game theory. Most common notions of equilibria in game theory are defined as the set of fixed points of a certain mapping. For example, Nash Equilibria (NE) are the set of fixed points of the best response mapping (appropriately defined to avoid ambiguity). The fact that Brouwer fixed points are hard to compute in general is no reason why computing specific fixed points should be hard (for instance, as mentioned earlier, computing fixed points of linear functions is easy via eigenvector computations). More specifically, could it be the case that the NE, being a fixed point of some well-specified mapping, is easy to compute? These hopes were dashed by the work of [6, 3] who showed that computing NE is as computationally difficult as finding fixed points in a general mapping: they show that computing NE in a two-player game is PPAD -complete. Further work showed that even computing an approximate NE is PPAD -complete [4].
 Since our algorithms (and all previous ones as well) depend on computing (approximate) fixed points of various decision modifiers, the above discussion leads us to question whether this is necessary. We show in this paper that indeed it is: a  X  -no-regret algorithm can be efficiently used to compute approximate fixed points of any convex combination of decision modifiers. This establishes an equivalence theorem, which is the main contribution of this paper: there exist efficient  X  -no-regret algorithms if and only it is possible to efficiently compute fixed points of convex combinations of decision modifiers in  X  . This equivalence theorem allows us to translate complexity theoretic lower bounds on computing fixed points to designing no regret algorithms. For instance, a Nash equilibrium can be obtained by applying Brouwer X  X  fixed point theorem to an appropriately defined continuous mapping from the compact convex set of pairs of the players X  mixed strategies to itself. Thus, if  X  contains this mapping, then it is PPAD -hard to design  X  -no-regret algorithms. It was recently brought to our attention that Stolz and Lugosi [17], building on the work of Hart and Schmeidler [12], have also considered  X  -no-regret algorithms. They also show how to design them from fixed-point oracles, and proved convergence to equilibria under even more general conditions than we consider. Gordon, Greenwald, Marks, and Zinkevich [8] have also considered similar no-tions of regret and showed convergence to equilibria, in the special case when the deviations in  X  can be represented as the composition of a fixed embedding into a higher dimensional space and an adjustable linear transformation. The focus of our results is on the computational aspect of such reductions, and the equivalence of fixed-points computation and no-regret algorithms. 2.1 Games and Equilibria We consider the following kinds of games. First, the set of strategies for the players of the game is a convex compact set. Second, the utility functions for the players are concave over their strategy sets. To avoid cumbersome notation, we restrict ourselves to two player games, although all of our results naturally extend to multi-player games.
 Formally, for i = 1 , 2 , player i plays points from a convex compact set K i  X  R n i . Her payoff is given by function u i : K 1  X  K 2  X  R , i.e. if x 1 , x 2 is the pair of strategies played by the two of x 1 for any fixed x 2 , and similarly u 2 is a concave function of x 2 for any fixed x 1 . We now define a notion of game theoretic equilibrium based on the property of being stable with respect to consistent deviations. By this, we mean an online game-playing strategy for the players from their suggested moves.
 To model this, assume that each player i has a set of possible deviations  X  i which is a finite 1 set of If it is the case that for any deviation  X  1  X   X  1 , player 1 X  X  expected payoff obtained by sampling x 1 using  X  is always larger than her expected payoff obtained by deviating to  X  1 ( x 1 ) , then we call  X  stable under deviations in  X  1 . The distribution  X  is said to be a  X  -equilibrium if  X  is stable under deviations in  X  1 and  X  2 . A similar definition appears in [12] and [17].
 Definition 1 (  X  -equilibrium) . A joint distribution  X  over K 1  X  K 2 is called a  X  -equilibrium if the following holds, for any  X  1  X   X  1 , and for any  X  2  X   X  2 : We say that  X  is a  X  -approximate  X  -equilibrium if the inequalities above are satisfied up to an additive error of  X  .
 Intuitively, we imagine a repeated game between the two players, where at equilibrium, the players X  moves are correlated by a signal, which could be the past history of the play, and various external factors. This signal samples a pair of moves from an equilibrium joint distribution over all pairs of moves, and suggests to each player individually only the move she is supposed to play. If no player stands to gain if she unilaterally, but consistently, used a deviation from her suggested move, then the distribution of the correlating signal is stable under the set of deviations, and is hence an equilibrium.
 Example 1: Correlated Equilibria. A standard 2-player game is obtained when the K i are the simplices of distributions over some base sets of actions A i and the utility functions u i are bilinear then it can be shown that any  X  -equilibrium can be equivalently viewed as a correlated equilibrium of the game, and vice-versa .
 Example 2: The Stock Market game. Consider the following setting: there are two investors (the generalization to many investors is straightforward), who invest their wealth in n stocks. In each period, they choose portfolios x 1 and x 2 over the n stocks, and observe the stock returns. We model the stock returns as a function r of the portfolios x 1 , x 2 chosen by the investors, and it maps the portfolios to the vector of stock returns. We make the assumption that each player has a small influence on the market, and thus the function r is insensitive to the small perturbations in the input. which the market prices are affected by the investments of the players.
 A natural goal for a good investment strategy would be to compare the wealth gain to that of the best fixed portfolio, i.e.  X  i is the set of all constant maps. This was considered by Cover in his Universal Portfolio Framework [5]. Another possible goal would be to compare the wealth gained to that achievable by modifying the portfolios using the  X  a,b maps above, as considered by [16]. In Section 3, we show that the stock market game admits algorithms that converge to an  X  -equilibrium in O ( 1  X  log 1  X  ) rounds, whereas all previous algorithms need O ( 1  X  2 ) rounds. 2.2 No regret algorithms The online learning framework we consider is called online convex optimization [18], in which there is a fixed convex compact feasible set K  X  R n and an arbitrary, unknown sequence of concave payoff functions f (1) , f (2) , . . . : K  X  R . The decision maker must make a sequence of decisions, functions f (1) , . . . , f ( t  X  1) to choose the point x ( t ) .
 The performance measure we use to evaluate online algorithms is regret, defined as follows. The decision maker has a finite set of N decision modifiers  X  which, as before, is a set of continuous mappings from K  X  K . Then the regret for not using some deviation  X   X   X  is the excess payoff the decision maker could have obtained if she had changed her points in each round by applying  X  . Definition 2 (  X  -Regret) . Let  X  be a set of continuous functions from K  X  K . Given a set of T concave utility functions f 1 , ..., f T , define the  X  -regret as Two specific examples of  X  -regret deserve mention. The first one is  X  X xternal regret X , which is defined when  X  is the set of all constant mappings from K to itself. The second one is  X  X nternal regret X , which is defined when K is the simplex of distributions over some base set of actions A , and  X  is the set of the  X  a,b functions (defined in (1)) for all pairs a, b  X  A .
 A desirable property of an algorithm for Online Convex Optimization is Hannan consistency : the regret, as a function of the number of rounds T , is sublinear. This implies that the average per iteration payoff of the algorithm converges to the average payoff of a clairvoyant algorithm that uses the best deviation in hindsight to change the point in every round. For the purpose of this paper, we require a slightly stronger property for an algorithm, viz. that the regret is polynomially sublinear as a function of T .
 Definition 3 (No  X  -regret algorithm) . A no  X  -regret algorithm is one which, given any sequence of called efficient if it computes x ( t ) in poly ( n, N, t, L ) time.
 In the above definition, L is a description length parameter for K , defined appropriately depending on how the set K is represented. For instance, if K is the n -dimensional probability simplex, then L = n . If K is specified by means of a separation oracle and inner and outer radii r and R , then L = log( R/r ) , and we allow poly ( n, N, t, L ) calls to the separation oracle in each iteration. The relatively new framework of Online Convex Optimization (OCO) has received much attention recently in the machine learning community. Our no  X  -regret algorithms can use any of wide variety of algorithms for OCO. In this paper, we will use Exponentiated Gradient (EG) algorithm ([14], [1]), which has the following (external) regret bound: Theorem 1. Let the domain K be the simplex of distributions over a base set of size n . Let G  X  be an upper bound on the L  X  norm of the gradients of the payoff functions, i.e. G  X   X  If the utility functions are strictly concave rather than linear, even stronger regret bounds, which depend on log( T ) rather than While most of the literature on online convex optimization focuses on external regret, it was ob-served that any Online Convex Optimization algorithm for external regret can be converted to an internal regret algorithm (for example, see [2], [16]). 2.3 Fixed Points As mentioned in the introduction, our no regret algorithms depend on computing fixed points of the relevant mappings. For a given set of deviations  X  , denote by CH ( X ) the set of all convex combinations of deviations in  X  , i.e.
 Since each map  X   X  CH ( X ) is a continuous function from K  X  K , and K is a convex compact domain, by Brouwer X  X  fixed theorem,  X  has a fixed point in K , i.e. there exists a point x  X  K such that  X  ( x ) = x . We consider algorithms which approximate fixed points for a given map in the following sense.
 Definition 4 (FPTAS for fixed points of deviations) . Let  X  be a set of N continuous functions from K  X  K . A fully polynomial time approximation scheme (FPTAS) for fixed points of  X  is an algorithm, which, given any function  X   X  CH ( X ) and an error parameter  X  &gt; 0 , computes a point x  X  K such that k  X  ( x )  X  x k X   X  in poly ( n, N, L, 1  X  ) time. In this section we prove that if the players use no  X  -regret algorithms, then the empirical distribu-tion of the moves converges to a  X  -equilibrium. [11] shows that if players use no internal regret algorithms, then the empirical distribution of the moves converges to a correlated equilibrium. This was generalized by [9] to any set of linear transformations  X  . The more general setting of this paper also follows easily from the definitions. A similar theorem was also proved in [17]. The advantage of this general setting is that the connection to online convex optimization allows for faster rates of convergence using recent online learning techniques. We give an example of a natural game theoretic setting with faster convergence rate below.
 Theorem 2. If each player i chooses moves using a no  X  i -regret algorithms, then the empirical game distribution of the players X  moves converges to a  X  -equilibrium. Further, an  X  -approximate  X  -equilibrium is reached after T iterations for the first T which satisfies 1 T Regret  X  ( T )  X   X  . played by the two players. From player 1 X  X  point of view, the payoff function she obtains, f ( t ) , is the following: Note that this function is concave by assumption. Then we have, by definition 3, Rewriting this in terms of the original utility function, and scaling by the number of iterations we get inequality can be rewritten as A similar inequality holds for player 2 as well. Now assume that both players use no regret algo-rithms, which ensure that Regret  X  have 1 T Regret  X  equilibrium as soon as T is large enough so that 1 T Regret  X  A corollary of Theorem 2 is that we can obtain faster rates of convergence using recent online learning techniques, when the payoff functions are non-linear. This is natural in many situations, since risk aversion is associated with the concavity of utility functions.
 Corollary 3. For the stock market game as defined in section 2.1, there exists no regret algorithms which guarantee convergence to an  X  -equilibrium in O ( 1  X  log 1  X  ) iterations.
 Proof sketch. The utility functions observed by the investor i in the stock market game are of the assumption on the insensitivity of the function r to small perturbations in the input. Thus the online algorithm of [5], or the more efficient algorithms of [13] can be applied. In the full version of this paper, we show that Lemma 6 can be modified to obtain algorithms with Regret  X  By the Theorem 2 above, the investors reach  X  -equilibrium in O ( 1  X  log 1  X  ) iterations. In this section we prove our main result on the computational equivalence of computing fixed points and designing no regret algorithms. By the result of the previous section, players using no regret algorithms converge to equilibria.
 We assume that the payoff functions f ( t ) are scaled so that the ( L 2 ) norm of their gradients is bounded by 1, i.e. k X  f ( t ) k X  1 . Our main theorem is the following: Theorem 4. Let  X  be a given finite set of deviations. Then there is a FPTAS for fixed points of  X  if and only if there exists an efficient no  X  -regret algorithm.
 The first direction of the theorem is proved by designing utility functions for which the no regret property will imply convergence to an approximate fixed point of the corresponding transformations. The proof crucially depends on the fact that no regret algorithms have the stringent requirement that their worst case regret, against arbitrary adversarially chosen payoff functions, is sublinear as a function of the number of the rounds.
 Lemma 5. If there exists a no  X  -regret algorithm then there exists an FPTAS for fixed points of  X  . Proof. Let  X  0  X  CH ( X ) be a given mapping whose fixed point we wish to compute. Let  X  be a given error parameter. have found an approximate fixed point. Else, supply A with the following payoff function: k  X  and since all the f ( t ) are linear functions, we have Thus, Since A is a no-regret algorithm, assume that A ensures that Regret  X  ( T ) = O ( T 1  X  c ) for some constant c &gt; 0 . Thus, when T =  X ( 1  X  1 /c ) the lower bound (2) on the regret cannot hold unless we have already found an  X  -approximate fixed point of  X  0 .
 The second direction is on the lines of the algorithms of [2] and [16] which use fixed point compu-tations to obtain no internal regret algorithms.
 Lemma 6. If there is an FPTAS for fixed points of  X  , then there is an efficient no  X  -regret algorithm. In fact, the algorithm guarantees that Regret  X  ( T ) = O ( Proof. We reduce the given OCO problem to an  X  X nner X  OCO problem. The  X  X uter X  OCO problem is the original one. We use a no external regret algorithm for the inner OCO problem to generate points in K for the outer one, and use the payoff functions obtained in the outer OCO problem to generate appropriate payoff functions for the inner one.
 tions on  X  , denoted  X  N . For a distribution  X   X   X  N , let  X  i be the probability measure assigned to  X  i in the distribution  X  . There is a natural mapping from  X  N  X  CH ( X ) : for any  X   X   X  N , denote by  X   X  the function P N i =1  X  i  X  i  X  CH ( X ) .
 Let x ( t )  X  K be the point used in the outer OCO problem in the t th round, and let f ( t ) be the obtained payoff function. Then the payoff functions for the inner OCO problem is the function g ( t ) :  X  N  X  R defined as follows: We now apply the Exponentiated Gradient (EG) algorithm (see Section 2.2) to the inner OCO prob-lem. To analyze the algorithm, we bound k X  g ( t ) k  X  as follows. Let x 0 be an arbitrary point in K . Thus, The last inequality follows because we assumed that the diameter of K is bounded by 1, and the norm of the gradient of f ( t ) is also bounded by 1.
 Let  X  ( t ) be the distribution on  X  produced by the EG algorithm at time t . Now, the point x ( t ) is computed by running the FPTAS for computing an 1  X  Now, using the definition of the g ( t ) functions, and by the regret bound for the EG algorithm, we have that for any fixed distribution  X   X   X  N ,
X Since k X  f ( t ) k X  1 , Summing (4) from t = 1 to T , and adding to (3), we get that for any distribution  X  over  X  , In particular, by concentrating  X  on any given  X  i , the above inequality implies that P gorithm.

