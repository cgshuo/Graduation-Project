 Large knowledge bases consisting of entities and relationships be-tween them have become vital sources of information for many applications. Most of these knowledge bases adopt the Semantic-Web data model RDF as a representation model. Querying these knowledge bases is typically done using structured queries utilizing graph-pattern languages such as SPARQL. However, such struc-tured queries require some expertise from users which limits the ac-cessibility to such data sources. To overcome this, keyword search must be supported. In this paper, we propose a retrieval model for keyword queries over RDF graphs. Our model retrieves a set of subgraphs that match the query keywords, and ranks them based on statistical language models. We show that our retrieval model outperforms the-state-of-the-art IR and DB models for keyword search over structured data using experiments over two real-world datasets.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  retrieval models, search process Algorithms, Design, Experimentation The continuous growth of knowledge-sharing communities like Wikipedia and the advances in automated information-extraction from Web pages [5, 23] have made it possible to build large-scale knowledge bases. Examples of such knowledge bases include YAGO [26], DBpedia [1] and Freebase [7]. These repositories contain en-tities such as people, movies, books, etc. and the relationships be-tween them such as bornIn , actedIn , isAuthorOf and so on. Such data is typically represented in the form of subject-predicate-object triples of the Semantic-Web data model RDF [22], where
W ork performed while intern at Yahoo! Research. This work is partially supported by the EU Large Scale Integrated Project LivingKnowledge (contract no. 231126).

Table 1: A set of RDF triples from a movie knowledge base s ubjects and objects are entities and predicates are relationships between pairs of entities. An RDF collection conceptually forms a large graph, which we refer to as an RDF graph, with nodes cor-responding to subjects and objects and edges denoting predicates. Table 1 shows a set of RDF triples from a movie knowledge base.
RDF data can be queried using a conjunction of triple patterns , where a triple pattern is a triple with variables and the same variable in different patterns denotes a join condition. For example, the information need of finding comedies that have won the Academy Award can be expressed using the following 2 triple-patterns: &lt;?x hasGenre Comedy; ?x hasWonPrize Academy_Award&gt; .

Structured queries like the one above are very expressive, yet very restrictive. They require the users to be familiar with the un-derlying data and a structured-query language like SPARQL. Em-powering users to search RDF graphs using keywords only can in-crease the usability of such data sources. In addition, it enables adapting the-state-of-the-art IR searching and ranking techniques.
In this paper, we develop a retrieval model that enables users to search RDF graphs using keywords . Our model takes as an in-put a keyword query and returns a ranked list of RDF subgraphs . By retrieving subgraphs instead of just entities, we treat triples in a holistic manner and we explicitly take into account the relation-ships between the entities. This can be particularly beneficial for both result retrieval and result representation. As an example, con-sider running the query "comedy academy award" against the RDF collection in Table 1. One possible result to such a query retrieved by our model is the 2-triple subgraph &lt;Innerspace hasGenre Comedy; Innerspace hasWonPrize Academy_Award&gt; .

To be able to process keyword queries over RDF graphs, we as-sociate each triple with a set of keywords derived from the subject and object of the triple, as well as representative keywords for the predicate. We explain how we do this in Section 3. To retrieve a ll subgraphs that match a given keyword query, we utilize a back-tracking algorithm over graphs, which is described in Section 4.
Once candidate subgraphs have been retrieved, we need to rank them. We try to identify the structured-information need intended by the keyword query. For instance, consider the query "comedy academy award". It is likely that the user is looking for movies of genre comedy that have won the Academy Award. Thus, we need to rank the subgraphs that match this information need higher. Our ranking model is based on statistical language-models and it uti-lizes the distribution of terms in the whole knowledge base as a means of inferring the structured-information need of a user key-word query. For instance, given that the term  X  X omedy X  appears of-ten in the object of triples with predicate hasGenre and the terms  X  X cademy X  and  X  X ward X  appear often in the object of triples with predicate hasWonPrize , we can infer that the most likely struc-tured triple-pattern query intended by the keyword query "comedy academy award" is &lt;?x hasGenre Comedy; ?x hasWonPrize Academy_Award&gt; . We thus rank subgraphs that match this im-plicit structured-query higher. We explain our ranking model in Section 5.

To show the effectiveness of our retrieval model, we create a benchmark for keyword queries over two real-world RDF datasets and use it to compare our ranking model to well-known IR and DB techniques for keyword search over structured data. Our evaluation results are highlighted in Section 6.
The work on keyword search over structured data can be classi-fied into two classes. The first class aims at mapping the keyword query into one or more structured query. For instance, the authors in [27] assume that the user keyword-query is an implicit represen-tation of a structured triple-pattern query. They try to infer such structured query using the RDF graph and retrieve the top-k most relevant structured queries. They then provide the user with the re-trieved queries and let her choose the most appropriate structured query to be evaluated. Their approach involves user interaction, and in addition suffers from a loss-of-information phenomenon since typically k is set to a small number.

The work on query inference from a user X  X  natural language ques-tion in [18] is also closely related. It utilizes natural-language pro-cessing tools and try to parse a user X  X  question in order to infer the most-likely structured query. Their technique however relies heav-ily on the quality of the parsing process and it also suffers from the information-loss problem highlighted above.

The second class of work on keyword search over structured data overcomes the aforementioned issues by directly retrieving results of the keyword query. The work on keyword search over XML data for instance falls into this category. XKSearch [29] returns a set of nodes that contain the query keywords either in their labels or in the labels of their descendant nodes and have no descendant node that also contains all keywords. Similarly, XRank [9] returns the set of elements that contain at least one occurrence of all of the query keywords, after excluding the occurrences of the keywords in sub-elements that already contain all of the query keywords. However, all these techniques assume a tree-structure and thus can not be directly applied to graph-structured data such as RDF graphs.
Also, closely related to our work is the language-modeling ap-proach for keyword search over XML data proposed in [14]. The authors assume that a keyword query has an implicit mapping of each keyword into XML element(s). Their ranking is based on the hierarchal language-models proposed in [20] and they utilize the distribution of terms in the elements of the XML collection to weight the different component of the LMs. However, the setting of XML data is quite different from that of RDF since in XML the retrieval unit is an XML document (or a subtree). In an RDF set-ting, we are interested in ranking subgraphs that match the user X  X  query. These subgraphs are not known in advance and are com-puted on the fly during retrieval time, and thus most of the prior work on XML IR would not apply.

Keyword search on graphs which returns a ranked list of Steiner trees [2, 12, 10, 8] (the exception is [17] which returns graphs) deals with the latter problem of having a predefined retrieval unit. However, the result ranking in each of the above is based on the structure of the results [2, 13] (usually based on aggregating the number or weights of nodes and edges), or on a combination of these properties with content-based measures such as tf-idf [4, 10, 17] or language models [19].

A closely related work that combines structure and content for ranking is the LM-based ranking model in [19] for ranking ob-jects (entities in an RDF setting). This model however assumes that the retrieval unit is entities only, while our ranking model goes beyond this to treat triples in a holistic manner by taking into ac-count the relationships between the entities. In addition, it assumes the presence of a document associated with each Web Object or entity, something that we lack in the case of RDF data in general.
The Semantic Search Challenge provided a benchmark for key-word queries over RDF data, however the judgments were made over entities built by assembling all the triples that shared the same subject. The best performing approach [3] ranked the entities using a combination of BM25F and additional hand-crafted information about some predicates, properties and sites. In contrast, we retrieve the set of subgraphs that match the query keywords and rank them. We believe that the graph representation provides more concise an-swers to the user information need than a set of entities.
Our knowledge base consists of a set of SPO-triples such as the one shown in Table 1. To be able to process keyword queries, we construct a virtual document for each triple t i which we refer to as D i . D i contains a set of keywords that are extracted from the subject and object of the triple, and representative keywords for the predicates (these can be generated from extraction patterns for in-stance [25]). For example, the triple &lt;Innerspace hasWonPrize Academy_Award&gt; would be associated with the following docu-ment: { innerspace, won, prize, academy, award }. In case there are any textual information associated with the triple t i contextual text from which the triple was extracted [6], we can also extract all the keywords there and add them to D i . We stem the terms in document D i using any standard stemmer and store them in an inverted index. In addition, we also store the term frequency of term w in D i , which we refer to as c ( w, D i ) .

Given a keyword query, we utilize our inverted index to retrieve, for each query keyword, a list of matching triples. We then join the triples from different lists based on their subjects and objects to retrieve subgraphs with one or more triples. However, we only construct subgraphs that contain triples from different lists, corre-sponding to matches to different (sets of) keywords. The intuition behind this is that we assume that the user has a precise information need in mind that can be precisely represented using a set of triple patterns. However, since the user cannot express her information need using triple patterns, she represents each triple pattern using a set of keywords. Without any knowledge about which keywords map to which triple pattern, we need to consider all extremes: from all keywords representing a single triple pattern up to each single keyword representing an individual triple pattern. Thus, the results Table 2: All subgraphs retrieved for the query "comedy a cademy award" to the user information need would be subgraphs with one or more triples up to the number of keywords in the user query. Our algo-rithm for retrieving subgraphs is described in Section 4.
For example, consider the information need of finding come-dies that have won the Academy Award. Furthermore assume that the user expressed this information need using the keyword query "comedy academy award". The results for such a query would then be single triples matching one or more query keywords, subgraphs with 2 triples matching at least 2 query keywords and so on. Table 2 shows all subgraphs retrieved given the query from our example RDF collection in Table 1. The second column shows the set of matched keywords by each triple in the corresponding subgraph.
Since keyword queries introduce additional ambiguity that is not present in the case of structured triple-pattern queries, result rank-ing becomes very crucial. For instance, the subgraphs in Table 2 differ in their size (i.e., how many triples they contain) as well as how many keywords they match. In addition, they also differ in their semantics. For instance, consider the last subgraph in Ta-ble 2. It states the fact that the movies Police_Academy and The_Darwin_Awards are both comedy movies. The rest of the subgraphs in Table 2 describe movies that have genre comedy and have won the Academy Award. Recall our earlier observation that the user keyword query is a representation of an implicit structured triple-pattern query. Thus, in order to provide an effective ranking, the system must infer what is the most likely structured query the user has in mind, and rank the subgraphs based on how well they match this implicit structured query. Our ranking model, described in Section 5, does this by combining the structure and the contents of the triples in the ranking function.
As mentioned in the previous section, the first step is to retrieve the set of subgraphs that match the user keyword query. In order to avoid retrieving subgraphs that are arbitrarily long, we restrict the subgraphs retrieved to have the following two properties: 1. The subgraphs should be unique and maximal. That is, each 2. The subgraphs should contain triples matching different sets
Our subgraph-retrieval algorithm starts by retrieving the lists of all triples matching the query keywords. That is, given a query Algorithm 1 R ETRIEVESUBGRAPHS( E ) 1: f or each edge t  X  E do 2: X  X  X  t  X  A ( t ) } 3: EXTENDSUBGRAPH( { t } , X ) 4: end for Algorithm 2 E XTENDSUBGRAPH( G , X ) 1: w hile X 6 =  X  do 2: Remove an arbitrary chosen edge t from X 3: if L ( { t } ) * L ( G ) and L ( G ) * L ( { t } ) then 4: X  X   X  X  X  X  t  X   X  NEIGHBORS ( t, G ) } 5: EXTENDSUBGRAPH( G  X  X  t } , X  X  ) 6: end if 7: end while 8: if MAXIMAL( G ) 6 = true then 9: print G 10: return 11: end if q = { q 1 , q 2 , ..., q m } where q i is a single keyword, we utilize our inverted index to retrieve lists { L 1 , L 2 , ..., L m } where L of all triples that match the keyword q i (see Table ?? ). Let the set of all unique triples in all the lists be E . This set E can be viewed as a disconnected graph which we refer to as the query graph . Recall that each triple can be viewed as an edge where its subject and object are nodes.

We adapt the backtracking algorithm for network-motif detec-tion in [28] to retrieve the subgraphs from the query graph. The modified algorithm utilizes adjacency lists for edges. Given an edge t i from list L i , its adjacency list A ( t i ) would contain all neighbor edges t j from all other lists L j . Two edges are consid-ered neighbors if they share a common node. That is, given an edge t i = &lt;s i p i o i &gt; , an edge t j = &lt;s j p neighbor of t i if s i = s j , s i = o j , o i = s j or o to retrieve only unique subgraphs, we associate with each edge t an id and we only add a neighbor to the adjacency list of t is greater than that of t i . Also, to ensure that we do not consider joining triples that match the same set of keywords, we only add a neighbor t j to the adjacency list A ( t i ) of triple t t /  X  L j and t j /  X  L i . Finally, we loop over all edges and generate all unique subgraphs using the following two algorithms.
Algorithm 1 loops over all the edges and for each edge t extracts its neighbors from its adjacency list A ( t ) . Algorithm 2 takes as an input a subgraph and a list of neighbors and recursively tries to add edges to this subgraph. The condition in line 3 of Algorithm 2 ensures that only edges that belong to at least one different list other than the lists the edges of the current subgraph G belong to are considered. This ensures that we construct only subgraphs whose edges match different sets of keywords. The function L ( G ) returns the set of lists the edges of a subgraph G belong to. Once an edge is added to the current subgraph, we also add its neighbors that are not neighbors of edges in G to the current list of neighbors and continue. The function NEIGHBORS ( t, G ) retrieves all neighbors of an edge t that are not neighbors to edges in G . Finally, the function MAXIMAL ( G ) ensures that the retrieved subgraph is unique and maximal.
In the previous section, we presented a graph-searching algo-rithm that retrieves a set of subgraphs matching a given keyword query. We now explain how we rank these subgraphs. Our rank-ing model is based on statistical language-models (LMs) [21] and w orks as follows. Given a query Q = { q 1 , q 2 , ..., q is a single term and a subgraph G = { t 1 , t 2 , ..., t n a triple, we rank the subgraph G based on the query likelihood or the probability of generating the query Q given the subgraph G  X  X  LM. Assuming independence between the query terms, the query likelihood P ( Q | G ) is computed as follows: where P ( q i | G ) is the probability of the term q i in the LM of G . The LM of subgraph G is computed as a mixture model of the LMs of its constituent triples as follows:
That is, the probability of a term q i in the subgraph LM is the average of its probability in the triples LMs. Note that more than one triple in subgraph G can match the same keyword q i and thus averaging over all the triples is a natural choice. The LM of a triple t can then be directly computed using the document of the triple . Recall from Section 3 that each triple t j is associated with a virtual document D j which is composed of all the terms associated with the triple. However, this approach completely ignores the struc-ture of the triples and treats every triple as a bag-of-words. For in-stance, consider the query "comedy academy award" to find come-dies that have won the Academy award. Now, consider the 2 sub-graphs G 1 = &lt;Innerspace hasGenre Comedy; Innerspace hasWonPrize Academy_Award&gt; and G 2 = &lt;The_Darwin_Awards matching the query. Given the intended information need of the query, we should rank the subgraph G 1 higher.

In our ranking model, we try to take into consideration the struc-ture of the triples as an additional evidence of how well they match the structured-information need intended by the keyword query. This is motivated by our earlier remark that we built our retrieval model on: a user keyword query is a representation of an implicit structured triple-pattern query. Considering our example query "com-edy academy award", the term  X  X omedy X  most likely refer to the triple pattern &lt;?x hasGenre Comedy&gt; given the fact that the term  X  X omedy X  appears more often in the documents of triples of the form &lt;s hasGenre Comedy&gt; . Similarly, the terms  X  X cademy X  and  X  X ward X  would likely refer to the pattern &lt;?x hasWonPrize Academy_Award&gt; . Thus, it would be desirable to assign higher probability mass to triples that match these patterns (i.e., triples with predicate hasGenre for the keyword  X  X omedy X  and hasWonPrize for the keywords  X  X cademy X  and  X  X ward X ).

To this end, we set the probability of a term q i in the triple t LM or P ( q i | t j ) in equation 2 to P ( q i | D j , r j bility of a term in a triple LM does not only depend on the docu-ment of the triple t j , but also on its predicate which we denote by r . Applying Bayes X  rule, we have:
Furthermore, we set P ( r j | q i , D j ) as a linear combination of the following two components [24, 16, 15]:
The first component in equation 4 is the probability that the pred-icate r j is relevant to the term q i whereas the second component is the probability that the predicate of triple t j is r j . The latter can be set to the extraction accuracy of triple t j for instance. Since this value is not generally present in RDF knowledge bases, we assume that we are always fully confident in the extraction quality of any triple, and set P ( r j | D j ) to 1. The parameter  X  is a weighting pa-rameter that controls the effect of each component on the ranking and can be set using training queries.
 Substituting equation 4 in equation 3 and simplifying, we have:
P ( q i | D j , r j ) =  X P ( q i | D j ) P ( r j | q i ) + (1  X   X  ) P ( q where P ( q i | D j ) is the probability of generating the term q the triple document D j which can be estimated using a maximum likelihood estimator after smoothing with the collection probability as follows: where c ( w, D j ) is the term frequency of term w in document D | D j | is the length of document D j (i.e., the sum of the term fre-quencies of all terms in D j ), Col is the whole collection con-structed by concatenating all the documents of all the triples in the knowledge base and | Col | is the length of the whole collec-tion. Finally, the parameter  X  is a smoothing parameter and is set according to Dirichlet prior smoothing [30].

The only remaining component to estimate in equation 5 is the probability of relevance of the predicate r j to the query term q order to estimate this probability, we first construct a document R for each predicate r j in the knowledge base concatenating the doc-uments of all the triples with such predicate. For instance, given the predicate hasGenre , we construct a document which is a concate-nation of all the documents of all triples of the form &lt;s hasGenre o&gt; . Once we have constructed a document R j for each predicate r , we set the probability of relevance of r j to a term q i in equation 5 to the P ( R j | q i ) (i.e., the probability of relevance of the document R j to the term q i ). Applying Bayes X  rule, the proba-bility P ( R j | q i ) can be estimated as follows: where P ( w | R j ) is the probability of generating the term w given the document R j which is estimated using a maximum-likelihood estimator as in equation 6 and P ( R j ) is the prior probability of the document R j being relevant to any term, which we set uni-formly. For example, using the above technique, the probability P ( hasGenre | comedy ) would be higher than P ( actedIn | comedy ) given the fact that the keyword  X  X omedy X  appears much more often in the documents of triples that have predicate hasGenre than those that have predicate actedIn .
We evaluated our retrieval model using a comprehensive user-study over two RDF datasets. The first dataset was derived from the LibaryThing community, which is an online catalog about books. The second dataset was derived from the Internet Movie Database (IMDB). The data from both sources was automatically parsed and converted into RDF triples.

Recall that our retrieval model assumes that each triple t sociated with a document D i . The triples documents were con-structed by using keywords derived from the triple entities, and representative words for the relations. For example, the relation isMarriedTo was represented using the terms {marry, wife, hus-band, spouse, etc} . This was done manually since we did not h ave that many relations in our datasets, but for bigger datasets, the representations of relations can be generated automatically us-ing a dictionary, or by utilizing the textual extraction-patterns in case the triples were extracted using some IE technique from free-text[25]. Once each triple was associated with a set of keywords, we stemmed all the keywords using the Stanford stemmer, removed stop words and created an inverted index over the triples. Table 3 gives an overview of the datasets.

Due to the lack of an appropriate query benchmark for keyword search over RDF data, we had to create a benchmark and gather rel-evance assessments ourselves. The queries we used for evaluation were a subset of the query benchmark in [6]. The benchmark there contains a set of structured queries, possibly augmented with key-words, along with their descriptions. We extracted 30 queries from there, 15 for each dataset and represented each query using a set of keywords. We opted for 30 queries only since we pooled 50 results per each query, and gathered relevance assessment for each result using at least 4 different human judges. Overall, we had about 15,000 unique relevance assessments for the 30 queries. All eval-uation queries, results and relevance assessments are available at http://www.mpii.de/~elbass/demo/rdftext.txt .

We compared our ranking model, which we refer to as the Struc-tured LM approach, to 3 competitors: 1) a baseline language-modeling approach (Baseline LM), 2) the Web Object Retrieval Model (WOR) [19] and 3) the BANKS system [2]. We chose these 3 competitors since they represent the family of approaches applicable to our set-ting, namely: keyword search over structured data. The rest of the approaches sketched in Section 2 do not directly apply to our set-ting and thus were omitted from our preliminary evaluation.
For each evaluation query, we retrieved the top-50 results re-trieved using each one of the 4 models described above. We then pooled all the results together and presented the set of all unique re-sults from the pool to 13 human judges in no particular order, along with the query description. The judges were all computer-scientists in two different research institutes. For the case of results retrieved using WOR, we presented the entity name as a result and provided the judges with a link to the Wikipedia article for that entity (in case there was one) in order to help them decide whether a result is relevant or not. For the rest of the results, we just presented the subgraphs for judgment.

We asked the judges to assess the results on a 4-levels scale: 3 corresponding to results that completely match the information need as given by the query description, 2 corresponding to results that do not completely match the information need but are still highly related to it, 1 corresponding to results that do not really match the information need of the query, but the results still make sense and add valuable information to the user and finally 0 corre-sponding to trivial, or nonsense results. Each result was evaluated by 4 different judges. The levels of agreement between the judges as measured by the Kappa coefficient were 0.449 for the Library-Thing dataset and 0.542 for the IMDB dataset. We also computed th e agreement for relevant and irrelevant results only (i.e., assum-ing that levels 3,2,1 are relevant and 0 is irrelevant). We obtained a Kappa coefficient of 0.397 for LibraryThing and 0.671 for IMDB which are in line with the numbers reported for standard TREC evaluation campaigns. For instance, the TREC legal track for 2006 reports a Kappa value of 0.49 on 40 queries, the opinion detection task in 2009 reports a Kappa value of 0.34, and the TREC 2004 Novelty track reports a value of 0.54 for sentence relevance.
We conducted 3 experiments: an overall evaluation using all evaluation queries, a training experiment to set the parameters of the models that involve ones, and a cross-validation to predict how well our parameter-learning approach would generalize.

Overall Evaluation. In the first experiment, we report the aver-age NDCG values (Normalized Discounted Cumulative Gain [11]) over all 30 evaluation queries at levels 20, 10 and 5 using all 4 different models in Table 4. The values for the Structured LM ap-proach and the BANKS system reported in the table are the ones achieved when the models parameters were set to their optimum values (i.e.,  X  = 0 . 9 for the Structured LM approach and  X  = 1 for BANKS).

As can be seen from Table 4, the Structured LM approach sig-nificantly outperforms ( p  X  value &lt; 0 . 05 with a one-tailed t-test) all other methods in terms of NDCG values at all levels. In the next experiment, we explain how to set the models parameters using training queries.

Training Results. In the second experiment, we used one dataset for training and the other for testing. That is, the 15 queries for the IMDB dataset were used as a training set to learn the optimal pa-rameter setting for the Structured LM approach and BANKS. The 15 queries for LibraryThing were then used to test the performance of the different methods. We did the same thing using the Library-Thing queries for training and the IMDB queries for testing. The learning procedure was as follows. For the Structured LM ap-p roach, we computed the average NDCG at level 50 over the 15 training queries, setting the parameter  X  to a value between 0 and 1 . We achieved the highest average NDCG @50 for both datasets when  X  was set to 0 . 9 . For BANKS, we did the same thing us-ing the same set of training queries and setting the parameter  X  to a value between 0 and 1 , and we achieved the highest average NDCG at level 50 when  X  was set to 1 . Table 5 shows the average NDCG values over the test queries at levels 20 , 10 and 5 .

Similar to the first experiment, the Structured LM approach sig-nificantly outperforms ( p  X  value &lt; 0 . 05 with a one-tailed t-test) all other methods in terms of NDCG values at all levels for both datasets. In order to test how well our training strategy generalizes, we performed a cross-validation experiment which we report next.
Cross-Validation Results. The third experiment was a cross-validation experiment to show how well the parameter learning pro-cedure we described above generalizes over unseen datasets. We performed a leave-one-out cross validation, where 14 out of the 15 queries for each dataset were used as a training set to determine the the value of the parameter  X  , and then the left-out query was used for testing. We repeated the same process such that each evalua-tion query is used for validation once, and we averaged the NDCGs over all the validation queries. For BANKS, we also performed a cross-validation to validate the learning of its parameter  X  , and again averaged the NDCGs over all the queries. For the IMDB dataset, the results were identical to those reported in Table 5 for all approaches, and for the LibraryThing dataset, the results were also the same as in the training experiment, except for a slight change in the case of the Structured LM approach (with NDCG values of 0.814 , 0.833 and 0.841 at levels 20,10 and 5, respectively). That is, similar to the results of the first two experiments, the Structured LM approach outperforms all other methods for both datasets.
We proposed a retrieval model for keyword queries over RDF graphs. Our retrieval model adopts backtracking algorithms to re-trieve subgraphs matching the query keywords. Our model pro-vides a result ranking based on a novel structure-aware language-modeling approach. We have shown through a preliminary, yet comprehensive user-study that our retrieval model outperforms well-known techniques for keyword search over structured data. [1] S. Auer, C. Bizer, G. Kobilarov, J. Lehmann, R. Cyganiak, [2] G. Bhalotia, A. Hulgeri, C. Nakhe, S. Chakrabarti, and [3] R. Blanco, P. Mika, and H. Zaragoza. Entity search track [4] T. Cheng, X. Yan, and K. Chen-Chuan Chang. Entityrank: [5] A. Doan, L. Gravano, R. Ramakrishnan, and [6] S. Elbassuoni, M. Ramanath, R. Schenkel, M. Sydow, and [7] Freebase: A social database about things you know and love. [8] K. Golenberg, B. Kimelfeld, and Y. Sagiv. Keyword [9] L. Guo, F. Shao, C. Botev, and J. Shanmugasundaram. [10] V. Hristidis, H. Hwang, and Y. Papakonstantinou.
 [11] K. J X rvelin and J. Kek X l X inen. Ir evaluation methods for [12] Varun Kacholia, Shashank Pandit, Soumen Chakrabarti, [13] G. Kasneci, M. Ramanath, M. Sozio, F. M. Suchanek, and [14] J. Kim, X. Xue, and W. B. Croft. A probabilistic retrieval [15] O. Kurland. The opposite of smoothing: a language model [16] O. Kurland and L. Lee. Corpus structure, language models, [17] G. Li, B.C. Ooi, J. Feng, J. Wang, and L. Zhou. Ease: an [18] V. Lopez, V. Uren, E. Motta, and M. Pasin. Aqualog: An [19] Z. Nie, Y. Ma, S. Shi, J. Wen, and W. Ma. Web object [20] P. Ogilvie and J. Callan. Hierarchical language models for [21] J. Ponte and W. B. Croft. A language modeling approach to [22] W3c: Resource description framework (rdf). [23] S. Sarawagi. Information extraction. Foundations and Trends [24] L. Si, R. Jin, J. Callan, and P. Ogilvie. A language modeling [25] F. Suchanek, M. Sozio, and G. Weikum. SOFIE: A [26] F. M. Suchanek, G. Kasneci, and G. Weikum. Yago: A large [27] T. Tran, H. Wang, S. Rudolph, and P. Cimiano. Top-k [28] S. Wernicke. A faster algorithm for detecting network motifs. [29] Y. Xu and Y. Papakonstantinou. Efficient keyword search for [30] C. Zhai and J. Lafferty. A study of smoothing methods for
