 Inspired by the PageRank and HITS (hubs and authorities) algorithms for Web search, we propose a structural re-rank-ing approach to ad hoc information retrieval: we reorder the documents in an initially retrieved set by exploiting asym-metric relationships between them. Specifically, we consider generation links , which indicate that the language model in-duced from one document assigns high probability to the text of another; in doing so, we take care to prevent bias against long documents. We study a number of re-ranking criteria based on measures of centrality in the graphs formed by generation links, and show that integrating centrality into standard language-model-based retrieval is quite effective at improving precision at top ranks.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Search and Retrieval]: Retrieval models General Terms: Algorithms, Experimentation Keywords: language modeling, PageRank, HITS, hubs, authorities, social networks, high-accuracy retrieval, graph-based retrieval, structural re-ranking
Information retrieval systems capable of achieving high precision at the top ranks of the returned results would be of obvious benefit to human users, and could also aid pseudo-feedback approaches, question-answering systems, and other applications that use IR engines for pre-processing purposes [31, 35, 32]. But crafting such systems remains a key re-search challenge.

The PageRank Web-search algorithm [1] uses explicitly-indicated inter-document relationships as an additional source of information beyond textual content, computing which documents are the most central . Here, we consider adapting this idea to corpora in which explicit links between docu-ments do not exist.
 How should we form links in a non-hypertext setting? While previous work in summarization has applied Page-Rank to cosine-based links [4], we draw on research demon-strating the success of using language models to improve IR performance in general [30, 2] and to model inter-document relationships in particular [16]. Specifically, we employ gen-eration links , which are based on the probability assigned by the language model induced from one document to the term sequence comprising another. 1 Our use of such links echoes the standard language-model-based ranking principle, first introduced in [30], that a document is relevant to the extent that its corresponding language model assigns high proba-bility to the query. However, given that we are working with multiple documents rather than a single query, we employ a technique that compensates for length bias in estimating generation probabilities.

We note that the analogy between hyperlinks and gener-ation links is not perfect. In particular, one can attribute much of the success of link-based Web-search algorithms to the fact that hyperlinks are (often) human-provided certi-fications that two pages are truly related [13]. In contrast, automatically-induced generation links are surely a noisier source of information. To compensate, we advocate an ap-proach (used elsewhere as well [39, 10, 13, 20, 37, 22]) that we term structural re-ranking : we use inter-document rela-tionships to compute an ordering not of the entire corpus, but of a (possibly unranked) set of documents produced by an initial retrieval method. This set should provide a reasonable ratio of relevant to non-relevant documents, and thus form a good foundation for our algorithms. Note that our approach differs in spirit from pseudo-feedback-based methods [31], which define a model based on the initially retrieved documents expressly in order to re-rank the entire corpus. Indeed, since the quality of the initially retrieved results plays a major role in determining the effectiveness of pseudo-feedback-based algorithms [35], our methods can potentially serve to greatly enhance the input to them.
To compute centrality values for a given generation graph, we propose a number of methods, including variants of Page-Rank [1] and HITS (a.k.a. hubs and authorities) [13]. Com-parisons on various TREC datasets against numerous base-lines (including use of cosine-based links and re-ranking em-
While the term  X  X enerate X  is convenient, we do not think of a  X  X enerator X  document or language model as literally  X  X reating X  others. Other work further discusses this issue and proposes alternate terminology (e.g.,  X  X ender X ) [17]. ploying only document-specific characteristics) show that language-model-based re-ranking using centrality as a form of  X  X ocument prior X  is indeed successful at moving relevant documents in the initial retrieval results higher up in the list.
Throughout this section, we assume that the following have been fixed: the corpus C (in which each document has been assigned a unique numerical ID); the query q ;theset D init  X  X  of top documents returned by some initial re-trieval algorithm in response to q (this is the set upon which re-ranking is performed); and the value of an ancestry pa-rameter  X  that pertains to our graph construction process.
For each do cument d  X  X  , p d (  X  ) denotes the smoothed unigram language model induced from d (estimation details appear in Section 2.4). We use g and o to distinguish be-tween a document treated as a  X  X enerator X  and a document treated as  X  X ffspring X , that is, something that is generated (details below).
 We use the notation ( V, w t ) for weighted directed graphs: V is the set of vertices and w t : V  X  V  X  X  y  X  : y  X  0 } is the edge-weight function . Thus, there is a directed edge between every ordered pair of vertices, but w t may assign zero weight to some edges. We write w t ( v 1  X  v 2 )todenote the value of w t on edge ( v 1 ,v 2 ).
Our use of language models to form links can be moti-vated by considering the following two documents: Knowing that d 2 is important (i.e., central or relevant) would provide strong evidence that d 1 is at least somewhat impor-tant. However, knowing that d 1 is very important does not allow us to conclude that d 2 is, since the importance of d might stem from its first two terms. Using language models induced from documents enables us to capture this asymme-try in how centrality is propagated: we allow a document d to receive support for centrality status from a document o only to the extent that p d ( o ) is relatively large. (If o is not in fact important, the support it provides may not be significant.) Note that ranking documents by p d ( q ), as first proposed by Ponte and Croft [30], can be considered a vari-ation of this principle.
 We are thus led to the following definitions.
 Definition 1. The top  X  generators of a document d  X  D init ,denotedT opGen ( d ) ,isthesetof  X  documents g  X  D init  X  X  d } that yield the highest p g ( d ) , where ties are broken by document ID. (We suppress  X  in our notation for clarity.)
Definition 2. The offspring of a document d  X  X  init are those documents that d is a top generator of, i.e., the set { o  X  X  init : d  X  T opGen ( o ) } .
 Note that multiple documents can share offspring, and that it is possible for a document to have no offspring.
We can encode top-generation relationships using either of two generation graphs G U =( D init , w t U )and G W = (
D init , w t W ), where for o, g  X  X  init , Thus, in both graphs, positive-weight edges lead only from offspring to their respective top  X  generators; but G U treats (edges to) the top generators of o uniformly ,whereas G W differentially weights them by the probability their induced language models assign to o .

Some of our algorithms require  X  X moothed X  versions of these graphs, in which all edges (including self-loops) have non-zero weight, to work correctly. To be specific, we employ PageRank X  X  [1] smoothing technique.

Definition 3. Given an edge-weighted directed graph G = (
D init , w t ) and smoothing parameter  X   X  [0 , 1) ,the smoothed graph G [  X  ] =( D init , w t [  X  ] ) has edge weights defined as fol-lows: for every o, g  X  X  init . w t [  X  ] ( o  X  g )=(1  X   X  )  X  1 |D The weights of all edges leading out of any given node in G sum to 1 and thus may be treated as transition probabilities .
With these concepts in hand, we can now phrase our centrality-determination task as follows: given a generation graph, compute for each node (i.e., document) how much centrality is  X  X ransferred X  to it from other nodes  X  by our edge-weight definitions, cent rality therefore corresponds to the degree to which a document is responsible for  X  X enerat-ing X  (perhaps indirectly) the other documents in the initially retrieved set. We now consider different ways to formalize this notion of transferrence of centrality.
A straightforward way to define the centrality of a docu-ment d with respect to a given graph G =( D init , w t )isto setitto d  X  X  weighted in-degree, which we call its influx : The Uniform Influx algorithm sets G = G U , so that the only thing that matters is how many offspring d has; it is thus reminiscent of the journal impact factor function from bibliometrics [5], which computes normalized counts of ex-plicit citation links. The Weighted Influx algorithm sets G = G W , so that the generation probabilities that d assigns to its offspring are factored in as well.

As previously noted by Pinski and Narin in their work on influence weights [29], one intuition not accounted for by weighted in-degree methods is that a document with even a great many offspring should not be considered central (or relevant) if those offspring are themselves very non-central. We can easily modify Equation 1 to model this intuition; we simply scale the evidence from a particular offspring doc-ument by that offspring X  X  centrality, thus arriving at the following recursive equation: wherewealsorequirethat d  X  X  init Cen RI ( d ; G )=1. Un-fortunately, for arbitrary G U and G W , Equation 2 may not have a unique solution or even any solution at all under the normalization constraint just given; however, a unique so-lution is guaranteed to exist for their PageRank-smoothed versions. 2 By analogy with the two influx algorithms given above, then, we have the Recursive Uniform Influx al-gorithm, which sets G = G [  X  ] U and is a direct analog of PageRank, and the Recursive Weighted Influx algorithm, which sets G = G [  X  ] W .
The centrality scores presented above can be used in iso-lation as criteria by which to rank the documents in D init However, if available, it might be useful to incorporate more information from the initial retrieval engine to help handle cases where centrality and rele vance are not strongly corre-lated. (Recall that it participates in any case by specifying the set D init .) In our experiments, we explore one concrete instantiation of this approach: we apply language-model-based retrieval [30, 2] to determine D init , and consider the following family of re-ranking criteria: where d  X  X  init and C en is one of the centrality functions defined in the previous section. This gives rise to the al-gorithms Uniform Influx + LM , Weighted Influx + LM , Recursive Uniform Influx + LM ,and Recursive Weighted Influx + LM .

Incidentally, our choosing p d ( q ) as initial score function has the interesting consequence that it suggests interpreting C en ( d ; G ) as a document  X  X rior X   X  in fact, Lafferty and Zhai write,  X  X ith hypertext, [a document prior] might be the distribution calculated using the  X  X ageRank X  scheme X  [18]. We will return to this idea later.
Generation probabilities form the basis for the graphs on which our algorithms are defined. This section describes our method for estimating these probabilities.

Let tf( w  X  x ) denote the number of times the term w occurs in the text or text collection x . What is often called the maximum-likelihood estimate (MLE) of w with respect to x is defined as Some prior work in language-model-based retrieval [22, 40] employs a Dirichlet-smoothed version: the smoothing parameter  X  controls the degree of reliance on relative frequencies in the corpus rather than on the counts in x . Both estimates just described are typically extended
The edge weights correspond to the transition probabili-ties for a Markov chain that is aperiodic and irreducible, and hence has a unique stationary distribution [8] that can be computed by a variety of means [34, 6, 7]. In our exper-iments, power iteration converged very quickly. to distributions over term sequences by assuming that terms are independent: for an n -term text sequence w 1 w 2  X  X  X  Another estimation approach, which we adopt, incorporates the Kullback-Leibler divergence D between document lan-guage models [16, 17] (see also previously proposed ranking principles [26, 18]): unless otherwise specified, for document d and word sequence s (in our setting, either a document or the query), we set p d ( s )to
Equation 4 has some useful properties. We can show that where H is the entropy function. Now, observe that for both p MLE x (  X  )and p [  X  ] x (  X  ), longer text sequences tend to be assigned lower probabilities; this would correspond to an unmotivated reduction of weights for edges out of long docu-ments in the graph G W . However, Term A length-normalizes p d ( s )viathe geometric mean , which has helped amelio-rate numerical problems in previous work [19]. Addition-ally, term B raises the generation probability for texts with high-entropy MLE term distributions. High entropy may be correlated with a larger number of unique terms  X  for exam-ple, we get an entropy of 0 for the document  X  X alvador Sal-vador Salvador X  but log 3 for  X  X oronto Sheffield Salvador X   X  which, in turn, has previously been suggested as a cue for relevance [33, 11]. Hence, generators of documents inducing high-entropy language models may be good candidates for centrality status. (We hasten to point out, though, that for the algorithms based on smoothed graphs (Definition 3), the entropy term cancels out due to our normalization of edge weights.)
Work on structural re-ranking in traditional ad hoc in-formation retrieval has mainly focused on query-dependent clustering , wherein one seeks to compute and exploit a clus-tering of the initial retrieval results [39, 10, 20, 37, 22]. Clusters represent structure within a document set, but do not directly induce an obvious single criterion or principle by which to rank documents; for instance, they have been used to improve rankings indirectly by serving as smooth-ing mechanisms [22]. Interestingly, some centrality measures have been previously employed to produce clusterings [36].
There has been increasing use of techniques based on graphs induced by implicit relationships between documents or other linguistic items [9, 3, 12, 4, 24, 28, 38]. The work in the domain of text summarization [4, 24] most resembles ours, in that it also computes centrality on graphs (although the nodes correspond to sentences or terms instead of doc-uments). Perhaps the main contrast with our work is that links were not induced by generation probabilities; Section 4.2 presents the results of experiments studying the relative merits of our particular choice of link definition.
Our centrality scores constitute a relationship-based re-ranking criterion that can serve as a bias affecting the initial retrieval engine X  X  scores, as in Equation 3. Alternative biases that are based on individual documents alone have also been investigated. Functions incorporating document or average word length [11, 14, 25] are applicable in our setting; we report on experiments with (variants of) document length in Section 4.2. Other previously suggested biases that may be somewhat less appropriate for general domains include document source [25] and creation time [21], and webpage hyperlink in-degree and URL form [15].
The objective of structural re-ranking is to (re-)order an initially-retrieved document set D init so as to improve preci-sion at the very top ranks of the final results. Therefore, we employed the following three evaluation metrics: the preci-sion of the top 5 documents (prec@5), the precision of the top 10 documents (prec@10), and the mean reciprocal rank of the first relevant document (MRR) [32].

We are interested in the general validity of the various structural re-ranking methods we have proposed. We be-lieve that a good way to emphasize the effectiveness (or lack thereof) of the underlying principles is to downplay the role of parameter tuning. Therefore, we made the following de-sign decisions, with the effect that the performance numbers we report are purposely not necessarily the best achievable by exhaustive parameter search :
The search ranges for the latter two parameters were: As it turned out, for many instances (except for the Weighted Influx algorithm), the optimal value of  X  with respect to precision at 5 was either 4 or 9, suggesting that a relatively small number of generators per document should be consid-ered when constructing the graph. In contrast,  X  exhibited substantial variance in optimal value for precision at 5 in some of our datasets. We set |D init | , the number of initially-retrieved documents, to 50 in all results reported below (sim-ilar performance patterns were obtained when |D init | = 100).
The remaining details are as follows. We conducted our experiments on the following four TREC corpora: (AP89 is a subset of AP containing articles just from the year 1989). All documents and queries (in our case, TREC-topic titles) were stemmed using the Porter stemmer and to-kenized, but no other pre-processing steps were applied. We used the Lemur toolkit [27] for language-model estimation. Statistically-significant differences in performance were de-termined using the two-sided Wilcoxon test at a confidence level of 95%.
In the tables that follow, we use the following abbrevia-tions for algorithm names.
 Our main experimental results are presented in Table 1. The first three rows specify reference-comparison data. The initial ranking was, as described above, produced using p with  X  chosen to optimize for non-interpolated precision at 1000. The empirical upper bound on structural re-ranking , which applies to any algorithm that re-ranks D init , indicates the performance that would be achieved if all the relevant documents within the initial fifty were placed at the top of the retrieval list: note that these bounds indicate that the initial rankings for AP89 are quite worse than those for the other three corpora. We also computed an optimized baseline for each metric m and test corpus C ; this consists of ranking all the documents (not just those in D init )by p d ( q ), with  X  chosen to yield the best m -results on a sanity check, we observe that the performance of the initial retrieval method is always below that of the corresponding optimized baseline (though not statistically distinguishable from it).

The first question we are interested in is how our struc-tural re-ranking algorithms taken as a whole do. As shown in Table 1, our methods improve upon the initial ranking in many cases, specifically, roughly 2/3 of the 96 relevant comparisons (8 centrality-based algorithms  X  4corpora  X  3 evaluation metrics). An even more gratifying observation is that Table 1 shows (via italics and boldface) that in many cases, our algorithms, even though optimized for precision at 5, can outperform a language model optimized for a dif-ferent (albeit related) metric m even when performance is measured with respect to m ; see, for example, the results for precision at 10 on the AP corpus.

Closer examination of the results in Table 1 reveals that in about 60% of the 48 relevant comparisons, our algorithms not only are at least as effective when applied to the graph G
W as when applied to G U , but often yield better perfor-mance results; the comparison between Recursive Weighted
U-In 29 . 6 27 . 8 39 . 5 o 50 . 9 49 . 0 i o 66 3 50 . 0 46 . 6 66 . 7 50 . 0 45 . 0 62 . 0
W-In 31 . 3 29 . 6 46 . 8 51 . 3 48 . 7 i 64 . 4 52 . 0 47 . 8 63 . 3 o 49 . 2 43 . 4 63 . 7
U-In+LM 33 5 27 . 0 46 . 5 51 . 3 i 49 4 i o 63 . 2 56 . 4 49 . 2 73 . 6 52 . 8 52 0 i o 66 . 6
W-In+LM 31 . 7 27 . 6 48 . 4 51 . 1 i 48 . 4 i o 63 . 0 57 . 2 50 . 0 77 . 2 51 . 6 49 . 6 i 64 . 5
R-U-In 31 . 3 28 . 9 46 . 4 51 . 5 48 . 9 i 63 . 4 53 . 6 49 . 6 68 . 5 52 . 0 44 . 6 66 . 5
R-W-In 32 . 2 29 . 6 40 . 5 o 52 . 1 i 49 . 1 i o 63 . 9 54 . 0 49 . 2 70 . 2 52 . 4 44 . 6 66 . 5
R-U-In+LM 33 . 0 29 . 3 45 . 8 52 . 1 i o 49 . 2 i o 64 . 3 58 8 i 51 0 i 78 6 55 . 6 46 . 0 68 . 4
R-W-In+LM 33 5 29 8 46 . 0 52 9 i o 49 . 0 i o 62 . 6 58 8 i 50 . 6 78 6 56 0 45 . 8 67 . 6 results over all ten algorithms.
 Influx (R-W-In) and Recursive Uniform Influx (R-U-In) is a good example. These results imply that it is a bit better to explicitly incorporate generation probabilities into the edge weights of our generation graphs than to treat all the top generators of a document equally.

Another observation we can draw from Table 1 is that adding in query-generation probabilities as weights on the centrality scores (see Equation 3) tends to enhance perfor-mance. This can be seen by comparing rows labeled with some algorithm abbreviation  X  X  X  against the correspond-ing rows labeled  X  X +LM X : about 80% of the 48 relevant comparisons exhibit this improvement. Most of the coun-terexamples occur in settings involving precision at 10 and MRR, which we did not optimize our algorithms for.
Similarly, by comparing  X  X  X -labeled rows with  X  X -Y X -labeled ones, we see that in about 70% of the 48 relevant comparisons, it is better to use the recursive formulation of Equation 2, where the centrality of a document is affected by the centrality of its offspring, than to ignore offspring centrality as is done by Equation 1.

Perhaps not surpri singly, then, the Recursive Uniform In-flux + LM and Recursive Weighted Influx + LM algorithms, which combine the two preferred features just described (re-cursive centrality computation and use of the initial search engine X  X  score function) appear to be our best performing algorithms: working from a starting point below the op-timized baselines, they improve the initial retrieval set to yield results that even at their worst, are not only clearly better than the initial ranking for precision at 5 and 10, but are also merely statistically indistinguishable from the opti-mized baselines. Moreover, in one setting (AP, precision at 10) they actually produce statistically significant improve-ments over the optimized baseline even though they were not optimized for that evaluation metric.

It is interesting to note that the relative performance of our algorithms does not seem to depend strongly on the quality of the initial ranking, in the following sense. The average percentage of relevant documents among the 50 that are initially retrieved is 21%, 35 . 5%, 33 . 3% and 30 . 3% for AP89, AP, WSJ and TREC8, respectively, but the relative improvements for precision at 5 and 10 that our algorithms achieve with respect to the initial ranking are almost always higher on AP89 than on WSJ or TREC8.
We have advocated the use of generation relationships to define centrality, where these asymmetric relationships are based on language-model probabilities. However, other inter-document relationships have been previously exploited in information retrieval. Perhaps the most well-known is vector-space proximity , with the cosine frequently used as (symmetric) closeness metric; indeed, as mentioned above, previous work in summarization [4] has used the cosine to determine centrality in ways very similar to the ones we have considered. It is thus important to examine whether the performance improvements we have achieved can be repro-duced, or even surpassed, by the use of vector-space-based links rather than language-model-based generation links.
To run this evaluation, we simply modified Definition 1 and all eight of our structural re-ranking algorithms to use the cosine of the angle between log tf.idf document vectors, rather than language-model probabilities, to form the ba-sis for determining the edge weights of our graphs. (Note that the fact that the cosine is symmetric does not imply that edges ( v 1 ,v 2 )and( v 2 ,v 1 ) get the same weight even in our non-smoothed graphs  X  document d 1 being a top  X  X enerator X  of d 2 with respect to the cosine does not imply the reverse.) It should be observed that the language-model weights on centrality scores (i.e., the p d ( q )terminEquation 3, on which the  X + LM  X  algorithms are based) were not re-placed with cosine values, which makes sense since we want our comparison to focus on the effect of different means of computing graph-based centrality.

Table 2 depicts the relative performance differences be-tween using our language-model-based graphs and graphs induced using vector-space proximity in the manner just described. For each choice of algorithm, evaluation mea-sure, and dataset, we indicate which formulation, if any, 5% with either a  X   X (LMsuperior)ora X   X (VECsuperior).
W-In 31 . 7 27 . 6 48 . 4 51 . 1  X  48 . 4  X  63 0 57 . 2 50 . 0 77 . 2 51 . 6 49 6 64 . 5 length 29 . 1 24 . 3 50 . 8 41 . 6 41 . 4 55 . 3 44 . 4  X  entropy 30 . 0 26 . 5 52 6 46 . 1 42 . 5 60 . 8 56 . 8 48 . 6 71 . 1 baseline, and bold highlights the best results over all eight algorithms. resulted in at least 5% relative improvement with respect to the other. As can be seen, in at least three of our four cor-pora, our language-modeling approach seems to be a more effective basis for determining document centrality than the vector-space/cosine. We hasten to point out, though, that in most instances, vector-space proximity yielded better per-formance than the corresponding baselines (the results are omitted since the precise numerical comparison does not yield additional information); this finding provides further support to the idea that the overall structural re-ranking approach is a flexible and effective paradigm that can incor-porate different types of inter-document relationships when appropriate.
One well-known alternative method for computing cen-trality in a graph is the HITS algorithm [13], originally pro-posed for Web search. There has been some work utilizing it for text summarization in non-Web domains as well [23]. Thereasonwehavenotyetdiscusseditindetailisthat it differs conceptually from our proposed algorithms in an important way: two different notions of centrality are iden-tified, represented by hub and authority scores. While the concepts of hubs and authorities are highly suitable for Web-search scenarios, it is less clear whether it is useful in our setting to distinguish between the two.

As a preliminary investigation, we experimented with us-ing hub and authority scores as measures of centrality on the generation graphs we built. Space constraints preclude a detailed discussion, but the results may be summarized as follows. We found that authority scores yielded better performance than hub scores, and that the results were gen-erally at least as good as or better than those for the opti-mized baselines. However, they were slightly inferior in sev-eral cases to those of the corresponding influx algorithms. Thus, it seems that our method for graph construction can support a variety of different algorithms, but that the HITS-style hubs/authorities distinction may not be effective for the task we have addressed.
So far, we have discussed the use of graph-based centrality as a re-ranking criterion, the idea being that relationships between documents can serve as an additional source of in-formation. Our best empirical results seem to be produced by using the weighted formulation given in Equation 3 from Section 2.3: Since, as noted above, in this equation C en ( d ; G )canbe regarded as a  X  X rior X  on documents, it is natural to ask whether other previously-proposed biases on generation prob-abilities might prove similarly useful. The comparison is es-pecially interesting because these biases have tended to be isolated-document heuristics; we thus refer to their use as a replacement for C en ( d ; G ) as  X  X on-structural re-ranking X .
Document length has been employed several times in the past to model the intuition that longer texts contain more information [11, 14, 25]. We refine this hypothesis to disen-tangle several distinct notions of information: the number of tokens in a document, the distribution of these tokens, and the number of types ( X  X alvador Salvador Salvador X  contains three tokens but only one type). Thus, as substitutions for centrality in the above expression, we consider not only doc-ument length, but also the entropy of the term distribution and the number of unique terms (used as the basis for piv-oted unique normalization in [33]). As baseline, we took the initial retrieval results; note that doing so corresponds to using a uniform bias, or, equivalently, using no bias at all.
As can be seen in Table 3, taking the log of token or type count is an improvement over using the raw frequencies, often yielding above-baseline performance. The entropy is more effective than raw frequency of either tokens or types, and in two cases leads to the best performance overall. How-ever, in the majority of settings, structural re-ranking gives the highest accuracies.
We posed our centrality-computation techniques as meth-ods for improving the results returned by an initial retrieval engine, and showed that they are successful at accomplish-ing this goal. But one can ask whether it is necessary to restrict our attention to an initial pool D init ; that is, would we expect similarly good results if we based our generation graphs on the entire corpus? As it happens, preliminary ex-periments with the Recursive Uniform Influx + LM and Re-cursive Weighted Influx + LM algorithms on two full corpora (AP89 and LA combined with FR) showed that one would be better off sticking with the standard language-modeling approach if no pre-filtering of documents is available.
We do not see this finding as surprising, for our intuition is that in the re-ranking case, there is a more direct connection between centrality and relevance since we can assume that relevant documents comprise a reasonable fraction of the initial retrieval results.
We have proposed and evaluated a number of methods for structural re-ranking using inter-document generation rela-tionships based on language models. Our main experiments showed that even non-optimized instantiations of our over-all approach yield results rivaling those of optimized base-lines. Further analysis revealed that generation relation-ships seem more effective within our centrality-computation framework than relationships based on vector-space proxim-ity do, and that using inter-document relationships seems to be a promising alternative to employing the isolated-document heuristics we implemented (several of which were novel to this study). Based on our results, we believe that exploring other methods for combining statistical language models and explicitly graph-based techniques is a fruitful line for future research.
 Acknowledgments. We thank James Allan, Bruce Croft, Carmel Domshlak, Jon Kleinberg, Fernando Pereira and the anonymous reviewers for valuable discussions and com-ments. We also thank CMU for its hospitality during the year. This paper is based upon work supported in part by the National Science Foundation under grant no. IIS-0329064 and CCR-0122581; SRI Int ernational under sub-contract no. 03-000211 on thei r project funded by the De-partment of the Interior X  X  National Business Center; and an Alfred P. Sloan Research Fellowship. Any opinions, findings, and conclusions or recommendations expressed are those of the authors and do not necessarily reflect the views or offi-cial policies, either expressed or implied, of any sponsoring institutions, the U.S. government, or any other entity. [1] Sergey Brin and Lawrence Page. The anatomy of a [2] W. Bruce Croft and John Lafferty, editors. Language [3] Inderjit Dhillon. Co-clustering documents and words [4] G  X  une  X  s Erkan and Dragomir R. Radev. LexRank: [5] Eugene Garfield. Citation analysis as a tool in journal [6] Gene H. Golub and Charles F. Van Loan. Matrix [7] Winfried K. Grassmann, Michael I. Taksar, and [8] Geoffrey R. Grimmett and David R. Stirzaker. [9] Vasileios Hatzivassiloglou and Kathleen McKeown. [10] Marti A. Hearst and Jan O. Pedersen. Reexamining [11] Djoerd Hiemstra and Wessel Kraaij. Twenty-One at [12] Thorsten Joachims. Transductive learning via spectral [13] Jon Kleinberg. Authoritative sources in a hyperlinked [14] Wessel Kraaij and Thijs Westerveld. TNO-UT at [15] Wessel Kraaij, Thijs Westerveld, and Djoerd Hiemstra. [16] Oren Kurland and Lillian Lee. Corpus structure, [17] Oren Kurland, Lillian Lee, and Carmel Domshlak. [18] John D. Lafferty and Chengxiang Zhai. Document [19] Victor Lavrenko, James Allan, Edward DeGuzman, [20] Anton Leuski. Evaluating document clustering for [21] Xiaoyan Li and W. Bruce Croft. Time-based language [22] Xiaoyong Liu and W. Bruce Croft. Cluster-based [23] Rada Mihalcea. Graph-based ranking algorithms for [24] Rada Mihalcea and Paul Tarau. TextRank: Bringing [25] David R. H. Miller, Tim Leek, and Richard M. [26] Kenney Ng. A maximum likelihood ratio information [27] Paul Ogilvie and Jamie Callan. Experiments using the [28] Bo Pang and Lillian Lee. A sentimental education: [29] Gabriel Pinski and Francis Narin. Citation influence [30] Jay M. Ponte and W. Bruce Croft. A language [31] Ian Ruthven and Mounia Lalmas. A survey on the use [32] Chirag Shah and W. Bruce Croft. Evaluating high [33] Amit Singhal, Chris Buckley, and Mandar Mitra. [34] William J. Stewart. Introduction to the numerical [35] Tao Tao and ChengXiang Zhai. A two-stage mixture [36] Naftali Tishby and Noam Slonim. Data clustering by [37] Anastasios Tombros, Robert Villa, and C.J. van [38] Kristina Toutanova, Christopher D. Manning, and [39] Peter Willett. Query specific automatic document [40] Chengxiang Zhai and John D. Lafferty. A study of
