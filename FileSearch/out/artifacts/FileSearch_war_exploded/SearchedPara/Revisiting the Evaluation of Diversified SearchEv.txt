 Evaluation metrics have always been one of the most important and challenging topics in information retrieval research because of the part they play in tuning and optimizing retrieval systems [3]. For diversified search tasks, many evaluation methods, such as  X   X  nDCG [7] and D #  X  measures [14], have been proposed. These metrics more or less simplify the assumptions about user behaviors. For example, with the assumption that users always view search results from top to bottom, most metrics leverage a ranking-based discount. These assumptions may help to simplify the evaluation process but also make the evaluation deviate from user X  X  actual experience and satisfaction [17].

In this paper, we propose to take user preferences as the ground truth to evalu-ate diversity metrics. We first compare u ser preferences collected at the subtopic level, user preferences at the topic leve l, and the weighted user preferences with each other. Diversity metrics are the n discussed in terms of the performance differences of run pairs det ected by the metric, which is similar with Sanderson X  X  work [16] (more recent than Turpin X  X  wor k [18,19]) except that we involve more diversity metrics. Other differences be tween Sanderson X  X  work and ours are that we collect user preferences in a g raded strategy and leverage  X  b to evaluate the correlations between diversity metrics and the graded user preferences, whereas Sanderson et al. use agreement/disagreement bet ween metrics and binary user preferences to discuss metrics X  properties. And on the other hand, we further discuss the same metrics in terms of the performance differences of run pairs detected by the users. Based on the grade d user preferences, we then propose a preference-weighted correlation, nam ely Multi-grade User Preference ( MUP ), to evaluate the diversity metrics. Finally, three widely-used methods for evaluating diversity metrics, namely Kendall X  X   X  , Discriminative Power and the Intuitive-ness Test are compared with MUP .

The major contributions of this paper are as follows: 1. We construct a test coll ection that contains 6,000 graded preferences col-2. The correlations (  X  b ) between a large number of diversity metrics and the 3. We propose a preference-weighted corre lation, namely Multi-grade User Pref-The remainder of this paper is organized as follows. In Section 2, we review re-lated work regarding the ways to evaluate diversity metrics. Section 3 compares the user preferences collected at the subt opic level with the user preferences at the topic level. Next, in Section 4 we comp are the correlations between several widely-used diversity metrics and user preferences. Section 5 presents the pro-posed method to evaluate diversity metr ics. In Section 6, we provide our exper-iments and corresponding analyses. Finally, Section 7 presents our conclusions and directions for future work. It is difficult to evaluate a diversity met ric because different metrics make differ-ent assumptions to simplify the process o f diversity evaluation. To present the possible effectiveness of a diversity metric, several methods have been developed.
Sakai et al. [15] propose to leverage Discriminative Power [11] to assess the effectiveness of diversity metrics. The m ethod computes a significance test be-tween every pair of the system runs and r eports the percentage of pairs that a metric can significantly distinguish at some fixed significance level.
Kendall X  X   X  [9] is another method used to co mpare different metrics. It is defined as the value proportional to the number of pairwise adjacent swaps needed to convert one ranking into the other ranking. Many previous works related to evaluation leverage it to compare their proposed metrics with other widely-used metrics [4,5].

The Intuitiveness Test [12] is developed by Sakai to compare the intuitiveness of different diversity metrics. In this method, a metric that is simple but can effectively represent the intuitiveness, e.g., the most important property that the diversity metrics should satisfy, is taken as the gold standard metric. The relative intuitiveness of two diversity metrics is measured in terms of preference agreement with its gold standard metric.

Moffat [10] proposes to characterize met rics by seven numeric properties, i.e. boundedness, monotonicity, convergence, top-weightedness, localization, com-pleteness, and realizability. These properties are used to partition the evaluation metrics and help the metrics to be better understood.

Amig  X  o et al. [2] propose reliability and sensitivity to compare evaluation met-rics. Reliability is the probability of finding the document relationships of a system run in a chosen gold standard run, whereas sensitivity is the probability of finding the document relationships of a chosen gold standard run in a system run.
 In general, these methods are lack of consideration about user preferences. After all, the ultimate aim of diversified search is satisfying the diverse informa-tion needs of users. In this paper, we high light the possible effectiveness of user preferences in the evaluat ion of diversity metrics. We first select 5 of the 12 runs created b y different methods in NTCIR 10 Intent-2 task [13] (Table 1). Each two of the 5 runs are then presented to users in a paralleled way to collect user preference. To decrease th e total work of preference collection, we only choose 50 of the 200 queries that contain as fewer (but at least 3) subtopics as possible. This is because in the experiments, only the top 3 subtopics ordered by weight are reserved for every query (for small workloads). We need to possibly decrease the bias of the subtopic reservation. The weights of the three reserved subtopics are then re-normalized by their sum.
 3.1 Graded User Preferences Collection Because in diversified search a query t opic is considered to contain several subtopics, we collect user preferences at both the subtopic and the topic lev-els. We present a subtopic with search re sults from two different runs to collect user preference at the subtopic level, wher eas simultaneously present all the three subtopics underlying a query with the search results to collect user preference at the topic level. The annotator is required to assess his preference by a number between 0 and 4. Because we have selected 50 queries with 150 subtopics, there are 200 presentations for each run pair. Considering 5 runs can generate 10 run pairs and each is confirmed to be presented to 3 annotators in the experiment, 110 annotators participate in this experiment with one person finishing 60 an-notations. 10 annotators are filtered b ecause the number of their decisions made within 10 seconds are larger than 30. Ther efore, we collect 6,000 user preferences. 3.2 Graded User Preferences Comparisons User preferences at the subtopic level a re collected by presenting one subtopic at a time without giving the subtopic weight. However, the weights of subtopics underlying a query may always differ from each other. To compare the bias, we linearly combine the user preferences at the subtopic level with corresponding subtopic weights to form the weighted preferences at topic level. We can then compare the preferences at the topic le vel with the weighted preferences and even with the user preferences at the subtopic level.

We average the user preferences and present results in Table 1. The average preferences for each run pair at the subtopic level are computed based on the 150 subtopics, whereas the average weighted preferences and the average preferences at the topic level are based on the 50 queries. Table 1 shows that in our experi-ment no matter what type of user preferences is considered, the relative orders between each pair are identical. For exam ple, users prefer BASELINE-D-C-1 to THUIR-D-C-1A in terms of all the three types of preferences according to the first row of Table 1.

Table 1 shows that the three types of user preferences have the same assess results for all the run pairs, although the significant results at the subtopic level contain the most items, which completely includes all of the significant results of the other two types of user preferences . This may be caused by a larger number of instances at the subtopic level considered in the significance test. Because user preferences at the subtopic level are coll ected without giving subtopic weights, it is more reasonable to consider the res ults of weighted preferences or the use preferences at the topic level. On the other hand, Table 1 shows that both the assess results and the significant results of weighted preferences are similar with the results of user preferences at the topic level. If we only collect user preferences at the topic level, 1,500 rather than 6,000 user preferences need to be collected. The corresponding cost would decrease to one-fourth. In the following sections of this paper, we only consider the user preferences at the topic level. There are many works about methods to evaluate a diversified search result. These works have proposed diversity metrics such as  X   X  nDCG , IA  X  measures , D #  X  measures . These metrics more or less simplify the assumptions about user behaviors, which prevent the metrics fr om reflecting aspects of the search pro-cesses that are experienced by the user. In this section, we ta ke user preferences collected in Section 3 as the ground trut h to present the user behavior related properties of diversity metrics in details. We consider a large range of diversity metrics such as IA  X  nDCG [1], IA  X  ERR [6],  X   X  nDCG [7], NRBP [8], I  X  rec [11], D  X  nDCG , D #  X  nDCG , D  X  Q , D #  X  Q [14], DIN #  X  nDCG , DIN #  X  Q , P + Q , P + Q #[12]. 4.1 Comparing Correlations on Run Pairs with Different For a certain query, we leverage diversity m etrics to evaluate the retrieval results of every two different runs. According t o the evaluation score, we can obtains the run preferred by the metric within each pair. On the other hand, we have also collected user preferences on the s ame pairs. The correlation between the metric and user preferences can then be computed for these pairs. Since there may be a tie in both the evaluation scor es and the user preferences, we compute the  X  b coefficient.  X  b is similar with the Kendall X  X   X  [9] except that the former explicitly excludes the influences of the tie in the rankings.

We first demonstrate the changes of correlations from run pairs with small differences to run pairs with large differen ces by classifying the run pairs into two bins. The run pairs whose difference in terms of a metric is greater than the aver-age difference of all the pairs are assigned into a large  X  bin and the other pairs areintoasmall  X  bin. The correlations between metrics and user preferences are computed within each bin, respectively. From another dimension, we also classify the run pairs into different cate gories according to user preferences. As described in Section 3, we collect user pr eferences in a graded strategy(between 0 and 4). We equally split this range into 4 different subranges. Within each sub-range, we compute the correlations betw een the metrics and user preferences.
Fig. 1 presents all the results. In these heatmaps, a rectangle with color near the red indicates a strong positive correlation, whereas a rectangle with color near the blue indicates a strong negative correlation. From these heatmaps, we can find: 1. Comparing Fig. 1(a) with Fig. 1(b), we can find  X  b values in the large  X  bin 2. From the dimension of user preferences ( x axis), we also find that the metrics 3. The  X  b values of DIN #  X  Q , DIN #  X  nDCG , D #  X  Q , P + Q #, and D #  X  The discussions above show that when the differences of run pairs are large, the metrics are more likely to achieve agreemen ts with user preferences, whereas the agreements are difficult to achieve when the differences are small. This conclusion keeps true no matter the differences ar e detected by the metrics or the users. This inspires us to penalize the metric more in evaluating diversity metrics if it disagrees with user preferences on the r un pairs with large differences. That is because the metric makes mistakes on run pairs where other metrics seldom make a mistake. The  X  b itself is not aware of this, although we have discussed the metrics based on the  X  b value. We first define some symbols in use. We denote a run pair as c . All of the 500 run pairs mentioned above compose a pair set C . Then, we define an indicator J ( c ) satisfying J ( c ) = 1 if the metric agrees with the user preference on run pair c ,whereas J ( c )=  X  1 if the metric disagrees with the user preference. The user preference of c is denoted as u c (where 0  X  u c  X  4). We propose the Multi-grade User Preference ( MUP ) to evaluate diversity metrics as follows: In Formula 1, if the metric agrees with the user preference on a pair c ,then both the numerator and the denominator increase by u c . However, if the metric disagrees with the user preference, then J ( c )=  X  1 and the sum of the corre-sponding u c  X  J ( c ) in the numerator is indeed equal to subtracting the user preference u c from the sum. In contrast, the denominator always increases by u . This is taken as the penalization of the disagreement. If u c is larger, MUP punishes the disagreement more. This is meaningful because the experiments and corresponding discussions in Section 4 show that the metrics achieve a bet-ter agreement with user preferences when the differences of run pairs are larger, whereas they perform worse on the run pai rs with smaller differences. If a metric makes a mistake (which means the metri c disagrees with the user preference) on a run pair whose difference can be easily de tected, it should be heavily penalized when we evaluate the metric. However, if the mistake is made on the run pair whose differences are small and difficult to detect, the corresponding punishment may be slight. Especially, MUP does not consider the run pairs with u c =0.
We can also discuss Formula 1 from the user X  X  perspective. A large user pref-erence means the user considers the diffe rence between the run pair to be large. It is reasonable to consider that the user can, on average, detect a large differ-ence with more confidence than when detect ing a small difference. Therefore, if a metric makes a mistake on the run pair whose difference is detected by the user with much confidence, the metric should be penalized more. In contrast, a small u c indicates a small difference detect ed by the user. We can also consider that the user is more confused when he is required to decide which one of two similar runs is better. Therefore, a smaller u c would indicate less user confidence on the user X  X  preference decision. If a metric makes a mistake on the run pair with a small u c , the penalization may be slight. 5.1 Relationships between MUP and Kendall X  X   X  The MUP defined in Formula 1 is similar with the  X  value. The difference is that MUP leverages the user preference u c to weight the agreements and the disagreements considered in  X  . As we discussed above, this weighted agreements (disagreements) would make MUP to penalize the mistakes more on the run pairs with large differences while to weaken the penalization to the mistakes on the run pairs with small differences.

We also can define MUP b based on MUP , just like the extension from  X  to  X  .
 Where T 0 ( c ) is an indicator satisfying T 0 ( c ) = 1 when the run pair c is tied only in terms of the metric, otherwise T 0 ( c )=0. T 0 ( c ) is a similar indicator satisfying T 0 ( c ) = 1 when the run pair c is tied only in terms of user preferences, otherwise T 0 ( c ) = 0. Note that if T 0 ( c ) = 1, then we obtain u c = 0 according to the definition of T 0 ( c ). This means u c  X  T 0 ( c ) is always equal to 0. Formula 2 can then be simplified as: As discussed in Section 5, MUP ( MUP b )isaweighted  X  (  X  b ). In this section, we first construct experiments to discuss the consistency of them. As most of the existing studies in the literature usually leverage Discriminative Power and the Intuitiveness Test to investigate the different aspects of the diversity metric, the discussions about them are also included in the experiments.

We compute the MUP , MUP b ,  X  ,  X  b , and Discriminative Power values on the selected 500 run pairs, respectively. We leverage the two-tailed paired boot-strap test with 1,000 bootstrap samples [11] for the Discriminative Power. The significance level in use is  X  =0 . 05. The results are presented in Fig. 2.
Fig. 2 shows: 1. D-Q gets higher MUP value than  X   X  nDCG , NRBP ,and IA  X  ERR .This 2. The MUP b values of most metrics are nearly the same with (indeed, slightly 3. The  X  (  X  b ) value decreases from I  X  rec to P + Q # while The MUP ( MUP b ) 4. The Discriminative Power is not correlative with the MUP or MUP b .Met-6.1 Comparison between MUP and the Intuitiveness Test The Intuitiveness Test is proposed by Sakai [12] to quantify the intuitiveness of a metric. It requires a gold standard metr ic to represent the intuitiveness that the diversity metric should satisfy. Following the work of Sakai, we take I  X  rec and Ef  X  P as the gold standard metrics and list the intuitiveness computed in Table 2. The metrics considered here are distributed in the top, middle and bottom positions of the ranking ordered by the MUP value.
 From Table 2, we can find: When Ef  X  P is taken as the gold standard, the Intuitiveness Test agrees better with MUP than it does when I  X  rec is taken as the gold standard. The  X  b value between MUP and the Intuitiveness Test in the former case is 0.333, whereas the  X  b value in the latter case is -0.407. Since both I  X  rec and Ef  X  P are set-based metrics based on bi nary relevance, the possible bias of this type may be weakened. Considering that I  X  rec is the gold standard of the diversity property and Ef  X  P is the gold standard of the relevance property, the larger  X  b value in the former case indicates i n the user X  X  opinion, the relevance of the search result is more important than the diversity of the search result in the diversified search evaluation of our experiments. This result may direct the design of new diversity metrics and help us tune the trade-off parameters between relevance and diversity in diversity metrics such as D #  X  nDCG . We will do a further research for this in future work. It is difficult to evaluate the effectiveness of diversity metrics. Most of the ex-isting studies leverage Discriminative Power, Kendall X  X   X  , or the Intuitiveness Test to evaluate the possible effectiveness of a diversity metric. However, they are lack consideration about the behaviors of user preferences. In this paper, we first collect 6,000 effective user preferen ces for 500 difference run pairs. A com-parison between the weighted user prefe rences and the user preferences collected at the topic level shows they share similar characters, which means we only need to collect user preferences at the topic le vel with much less efforts. Then we in-vestigate the correlations between the div ersity metrics and us er preferences. We find that diversity metrics agree better wit h user preferences when the difference of a run pair is larger, no matter the difference is detected in terms of the metric or user preferences. Based on these findings, we propose a preference-weighted correlation, namely MUP to evaluate diversity metrics. In the experiments, we first present the effort of the  X  X referen ce-weighted X  correlation by comparing MUP ( MUP b ) with  X  (  X  b ). The results also show that MUP method evaluates diversity metrics from the aspects that m ay differ from Discriminative Power. In addition, we construct experiments to compare MUP with the Intuitiveness Test and find that when Ef  X  P is taken as the gold standard of relevance evaluation, the Intuitiveness Test agrees better with MUP than it does when I  X  rec is taken as the gold standard of diversity evaluation. Since the MUP method evaluates diversity metrics from r eal users X  perspective, then this larger agreement reveals that in the user X  X  opinion, the relevance of the search result is more important than the diversity of the search result in the diversified search evaluation of our experiments. This res ult may direct the design of new diver-sity metrics and help us tune the trade-off parameters between relevance and diversity in diversity metrics. In future work, we will base on the conclusions in this paper to develop new diversity metrics. We will also do a further research for tuning trade-off parameters in diversity metrics such as D #  X  nDCG . Acknowledgement. This work was supported by Natural Science Founda-tion (61073071) and a Research Fund FY14-RES-SPONSOR-111 from Microsoft Research Asia.

