 Freek Stulp freek.stulp@ensta-paristech.fr FLOWERS Research Team, INRIA Bordeaux Sud-Ouest, Talence, France Olivier Sigaud olivier.sigaud@upmc.fr Scaling reinforcement learning (RL) methods to con-tinuous state-action problems, such as humanoid robotics tasks, has been the focus of numerous re-cent studies (Kober &amp; Peters, 2011; Theodorou et al., 2010). Most of the progress in the domain comes from direct policy search methods based on trajectory roll-outs. The recently proposed direct  X  X olicy Improve-ment with Path Integrals X  algorithm (PI 2 ) is derived from first principles of stochastic optimal control, and is able to outperform gradient-based RL algorithms such as REINFORCE (Williams, 1992) and Natural Actor-Critic (Peters &amp; Schaal, 2008) by an order of magnitude in terms of convergence speed and quality of the final solution (Theodorou et al., 2010). What sets PI 2 apart from other direct policy im-provement algorithms is its use of probability-weighted averaging to perform a parameter update, rather than using an estimate of the gradient. Interestingly enough,  X  X ovariance Matrix Adaptation  X  Evolution-ary Strategy ( CMAES ) X  and  X  X ross-Entropy Meth-ods ( CEM ) X  are also based on this concept. It is strik-ing that these algorithms, despite having been derived from very different principles, have converged to al-most identical parameter update rules. To the best of our knowledge, this paper is the first to make this re-lationship between the three algorithms explicit (Sec-tion 2). This hinges on 1) re-interpreting CEM as performing probability-weighted averaging; 2) demon-strating that CEM is a special case of CMAES , by setting certain CMAES parameters to extreme values. A further contribution of this paper is that we concep-tually and empirically investigate the differences and similarities between PI 2 , CEM , and CMAES (Sec-tion 3). These comparisons suggest a new algorithm, PI 2 -CMA, which has the algorithm structure of PI 2 , but uses covariance matrix adaptation as found in CEM and CMAES . A practical contribution of this paper is that we show how PI 2 -CMA automatically determines the exploration magnitude, the only pa-rameter which is not straightforward to tune in PI 2 . We now describe the CEM , CMAES and PI 2 algo-rithms, and their application to policy improvement. 2.1. Cross-Entropy Method (CEM) Given a n -dimensional parameter vector  X  and a cost function J : R n 7 X  R , the Cross-Entropy Method (
CEM ) for optimization searches for the global min-imum with the following steps: Sample  X  Take K samples  X  k =1 ...K from a distribution. Sort  X  Sort the samples in ascending order with respect to the evalua-tion of the cost function J (  X  k ). Update  X  Recompute the distribution parameters, based only on the first K e  X  X lite X  samples in the sorted list. Iterate  X  return to the first step with the new distribution, until costs converge, or up to a certain number of iterations. A commonly used distribution is a multi-variate Gaus-sian distribution N (  X  ,  X ) with parameters  X  (mean) and  X  (covariance matrix), such that these three steps are implemented as in (1)-(5). An example of one iter-ation of CEM is visualized in Figure 1, with a multi-variate Gaussian distribution in a 2D search space 1 . Throughout this paper, it is useful to think of CEM as performing probability-weighted averaging , where the elite samples have probability 1 /K e , and the non-elite have probability 0. With these values of P k , (1)-(5) can be rewritten as in the left algorithm in Table 1. Here we use Q K the distribution J k =1 ...K . This notation is chosen for brevity; it simply means that in the sorted array of ascending J k , P k is 1 /K e if K  X  K e , and 0 otherwise, as in (4). The resulting parameter updates are equiv-alent to those in (4) and (5), but this representation makes the relation to PI 2 more obvious.
 CEM for Policy Improvement. Because CEM is a very general algorithm, it is used in many differ-ent contexts in robot planning and control. CEM for policy optimization was introduced by Mannor et al. (2003). Although their focus is on solving fi-nite small Markov Decision Processes (MDPs), they also propose to use CEM with parameterized poli-cies to solve MDPs with large state spaces. Buso-niu et al. (2011) extend this work, and use CEM to learn a mapping from continuous states to discrete ac-tions, where the centers and widths of the basis func-tions are automatically adapted. The main difference with our work is that we use continuous action spaces of higher dimensionality, and compare CEM to PI 2 and CMAES . CEM has also been used in combina-tion with sampling-based motion planning (Kobilarov, 2011). An interesting aspect of this work is that it uses a mixture of Gaussians rather than a single distribu-tion to avoid premature convergence to a local mini-mum. In (Marin et al., 2011), a CEM is extended to optimize a controller that generates trajectories to any point of the reachable space of the system. 2.2. Covariance Matrix Adaptation -The Covariance Matrix Adaptation -Evolution Strat-egy (Hansen &amp; Ostermeier, 2001) algorithm is very similar to CEM , but uses a more sophisticated method to update the covariance matrix, as listed in Table 2. There are three differences to CEM :  X  The probabil-ities in CMAES do not have to be P k = 1 /K e as for CEM , but can be chosen by the user, as long as the constraints P K e k =1 P k = 1 and P 1  X   X  X  X   X  P K e are met. Here, we use the default suggested by Hansen &amp; Ostermeier (2001), i.e. P k = ln (0 . 5( K + 1))  X  ln ( k ).  X  Sampling is done from a distribution N (  X  , X  2  X ), i.e. the covariance matrix of the normal distribution is multiplied with a scalar step-size  X  . These compo-nents govern the magnitude (  X  ) and shape ( X ) of the exploration, and are updated separately.  X  For both step-size and covariance matrix an  X  X volution path X  is maintained ( p  X  and p  X  respectively), which stores in-formation about previous updates to  X  . Using the in-formation in the evolution path leads to significant im-provements in terms of convergence speed, because it enables the algorithm to exploit correlations between consecutive steps. For a full explanation of the algo-rithm we refer to Hansen &amp; Ostermeier (2001). Reducing CMAES to CEM. This is done by set-ting certain parameters to extreme values: 1) set the time horizon c  X  = 0. This makes (21) collapse to  X  new =  X   X  exp(0), which means the step-size stays equal over time. Initially setting the step-size  X  init = 1 means  X  will always be 1, thus having no effect during sampling. 2) For the covariance matrix update, we set c 1 = 0 and c  X  = 1. The first two terms of (23) then drop, and what remains is P K e k =1 P k (  X  k  X   X  )(  X  k which is equivalent to (16) in CEM , if P k is chosen as in (12).
 CMAES for Policy Improvement. Heidrich-Meisner and Igel (2008) use CMAES to directly learn a policy for a double pole-balancing task. R  X uckstiess et al. (2010) use Natural Evolution Strategies (NES), which has comparable results with CMAES , to di-rectly learn policies for pole balancing, robust stand-ing, and ball catching. The results above are compared with various gradient-based methods, such as REIN-FORCE (Williams, 1992) and NAC (Peters &amp; Schaal, 2008). To the best of our knowledge, our paper is the first to directly compare CMAES with CEM and PI 2 . Also, we use Dynamic Movement Primitives as the un-derlying policy representation, which 1) enables us to scale to higher-dimensional problems, as demonstrated by (Theodorou et al., 2010); 2) requires us to perform temporal averaging, cf. (18) and (19). 2.3. Policy Improvement with Path Integrals A recent trend in reinforcement learning is to use pa-rameterized policies in combination with probability-weighted averaging ; the PI 2 algorithm is a recent ex-ample of this approach. Using parameterized poli-cies avoids the curse of dimensionality associated with (discrete) state-action spaces, and using probability-weighted averaging avoids having to estimate a gradi-ent, which can be difficult for noisy and discontinuous cost functions.
 PI 2 is derived from first principles of optimal con-trol, and gets its name from the application of the Feynman-Kac lemma to transform the Hamilton-Jacobi-Bellman equations into a so-called path inte-gral, which can be approximated with Monte Carlo methods (Theodorou et al., 2010). The PI 2 algorithm is listed to the right in Table 1. As in CEM , K sam-ples  X  k =1 ...K are taken from a Gaussian distribution. In PI 2 , the vector  X  represents the parameters of a pol-icy, which, when executed, yields a trajectory  X  i =1 ...N with N time steps. This multi-dimensional trajectory may represent the joint angles of a n -DOF arm, or the 3-D position of an end-effector.
 So far, PI 2 has mainly been applied to poli-cies represented as Dynamic Movement Primitives (DMPs) (Ijspeert et al., 2002), where  X  determines the shape of the movement. Although PI 2 searches in the space of  X  , the costs are defined in terms of the tra-jectory  X  generated by the DMP when it is integrated over time. The cost of a trajectory is determined by evaluating J for every time step i , where the cost-to-go of a trajectory at time step i is defined as the sum over all future costs S (  X  i,k ) = P N j = i J (  X  j,k ), as in (11) Analogously, the parameter update is applied to ev-ery time step i with respect to the cost-to-go S (  X  i ). The probability of a trajectory at i is computed by exponentiating the cost, as in (13). This assigns high probability to low-cost trials, and vice versa. In prac-tice,  X  1  X  S i,k is implemented with optimal baselining As can be seen in (15), a different parameter update  X  i is computed for each time step i . To acquire the single parameter update  X  new , the final step is there-fore to average over all time steps (18). This average is weighted such that earlier parameter updates in the trajectory contribute more than later updates, i.e. the weight at time step i is T i = ( N  X  1) / P N j =1 ( N  X  1). The intuition is that earlier updates affect a larger time horizon and have more influence on the trajectory cost. PoWeR is another recent policy improvement algo-rithm that uses probability-weighted averaging (Kober &amp; Peters, 2011). In PoWeR , the immediate costs must behave like an improper probability, i.e. sum to a constant number and always be positive. This can make the design of cost functions difficult in practice; (24) for instance cannot be used with PoWeR . PI 2 places no such constraint on the cost function, which may be discontinuous. When a cost function is com-patible with both PoWeR and PI 2 , they perform es-sentially identical (Theodorou et al., 2010). When comparing CEM , CMAES and PI 2 , there are some interesting similarities and differences. All sam-ple from a Gaussian to explore parameter space  X  (6) and (7) are identical  X  and both use probability-weighted averaging to update the parameters  X  (14) and (15). It is striking that these algorithms, which have been derived within very different frame-works, have converged towards the same principle of probability-weighted averaging.
 We would like to emphasize that PI 2  X  X  properties fol-low directly from first principles of stochastic optimal control. For instance, the eliteness mapping follows from the application of the Feymann-Kac lemma to the (linearized) Hamilton Jacobi Bellmann equations, as does the concept of probability-weighted averag-ing. Whereas in other works the motivation for using CEM / CMAES for policy improvement is based on its empirical performance (Busoniu et al., 2011; Heidrich-Meisner and Igel, 2008; R  X uckstiess et al., 2010) (e.g. it is shown to outperform a particular gradient-based method), the PI 2 derivation (Theodorou et al., 2010) demonstrates that there is a theoretically sound motivation for using methods based on probability-weighted averaging, as this principle follows directly from first principles of stochastic optimal control. Whereas Section 2 has mainly highlighted the similar-ities between the algorithms, this section focuses on the differences. Note that any differences between PI 2 and CEM / CMAES in general also apply to the spe-cific application of CEM / CMAES to policy improve-ment, as done for instance by Busoniu et al. (2011) or Heidrich-Meisner and Igel (2008). Before compar-ing the algorithms, we first present the evaluation task used in the paper. 3.1. Evaluation Task For evaluation purposes, we use a viapoint task with a 10-DOF arm. The task is visualized and described in Figure 2. This viapoint task is taken from (Theodorou et al., 2010), where it is used to compare PI 2 with PoWeR (Kober &amp; Peters, 2011), NAC (Peters &amp; Schaal, 2008), and REINFORCE (Williams, 1992). The goal of this task is expressed with the cost func-tion in (24), where a represents the joint angles, x and y the coordinates of the end-effector, and D = 10 the number of DOF. The weighting term ( D + 1  X  d ) penalizes DOFs closer to the origin, the underlying motivation being that wrist movements are less costly than shoulder movements for humans, cf. (Theodorou et al., 2010).

The 10 joint angles trajectories are generated by a 10-dimensional DMP, where each dimension has B = 5 basis functions. The parameter vectors  X  (one 1  X  5 vector for each of the 10 dimensions), are initialized by training the DMP with a minimum-jerk movement. During learning, we run 10 trials per update K = 10, where the first of these 10 trials is a noise-free trial used for evaluation purposes. For PI 2 , the eliteness parameter is h = 10, and for CEM and CMAES it is K e = K/ 2 = 5. The initial exploration noise is set to  X  = 10 4 I B =5 for each dimension of the DMP. 3.2. Exploration Noise A first difference between CEM / CMAES and PI 2 is the way exploration noise is generated. In CEM and CMAES , time does not play a role, so only one explo-ration vector  X  k is generated per trial. In stochastic optimal control, from which PI 2 is derived,  X  i repre-sents a motor command at time i , and the stochasticity  X  + i is caused by executing command in the environ-ment. When applying PI 2 to DMPs, this stochasticity rather represents controlled noise to foster exploration, which the algorithm samples from  X  i  X  N (  X  ,  X ). We call this time-varying exploration noise. Since this ex-ploration noise is under our control, we need not vary it at every time step. In the work by Theodorou et al. (2010) for instance, only one exploration vector  X  k is generated at the beginning of a trial, and exploration is only applied to the DMP basis function that has the highest activation. We call this per-basis exploration noise. In the most simple version, called constant ex-ploration noise, we sample  X  k,i =0 once at the begin-ning for i = 0, and leave it unchanged throughout the execution of the movement, i.e.  X  k,i =  X  k,i =0 . The learning curves for these different variants are de-picted in Figure 3. We conclude that time-varying ex-ploration convergences substantially slower. Because constant exploration gives the fastest convergence, we use it throughout the rest of the paper.
 3.3. Definition of Eliteness In each of the algorithms, the mapping from costs to probabilities is different. CEM implements a cut-off value for  X  X liteness X : you are either elite ( P k = 1 /K or not ( P k = 0). PI 2 rather considers eliteness to be a continuous value that is inversely proportional to the cost of a trajectory. CMAES uses a hybrid eliteness measure where samples have zero probability if they are not elite, and a continuous value which is inverse proportional to the cost if they are elite. The probabilities in CMAES do not have to be P k = 1 /K e as for CEM , but can be chosen by the user, as long as the constraints P K e k =1 P k = 1 and P 1  X   X  X  X   X  P K e are met. Here, we use the defaults suggested by Hansen &amp; Ostermeier (2001), i.e. P k = ln (0 . 5( K + 1))  X  ln ( k ). These different mappings are visualized in Fig-ure 4. An interesting similarity between the algo-rithms is that they each have a parameter  X  K e in CEM / CMAES , and h in PI 2  X  that determines how  X  X litist X  the mapping from cost to probability is. Typi-cal values are h = 10 and K e = K/ 2. These and other values of h and K e are depicted in Figure 4.
 The average learning curves in Figure 5 are all very similar except for CEM with K e = 5 / 7. This verifies the conclusion by Hansen &amp; Ostermeier (2001) that choosing these weights is  X  X elatively uncritical and can be chosen in a wide range without disturbing the adap-tation procedure. X  and choosing the optimal weights for a particular problem  X  X nly achieves speed-up fac-tors of less than two X  when compared with CEM -style weighting where all the weights are P k = 1 /K e . Be-cause choosing the weights is uncritical, we use the PI 2 weighting scheme with h = 10, the default suggested by Theodorou et al. (2010), throughout the rest of this paper.
 3.4. Covariance Matrix Adaptation We now turn to the most interesting and relevant dif-ference between the algorithms. In CEM / CMAES , both the mean and covariance of the distribution are updated, whereas PI 2 only updates the mean. This is because in PI 2 the shape of the covariance matrix is constrained by the relation  X  =  X  R  X  1 , where R is the (fixed) command cost matrix, and  X  is a param-eter inversely proportional to the parameter h . This constraint is necessary to perform the derivation of PI 2 (Theodorou et al., 2010).
 In this paper, we choose to ignore the constraint  X  =  X  R  X  1 , and apply covariance matrix updating to PI 2 . Because a covariance matrix update is com-puted for each time step i (17), we need to perform temporal averaging for the covariance matrix (19), just as we do for the mean  X  . Temporal averaging over covariance matrices is possible, because 1) ev-ery positive-semidefinite matrix is a covariance matrix and vice versa 2) a weighted averaging over positive-semidefinite matrices yields a positive-semidefinite ma-trix (Dattorro, 2011).
 Thus, rather than having a fixed covariance matrix, PI 2 now adapts  X  based on the observed costs for the trials, as depicted in Figure 4. This novel al-gorithm, which we call PI 2 -CMA, for  X  X ath Integral Policy Improvement with Covariance Matrix Adap-tation X , is listed in Table 1 (excluding the red in-dices i = 1 ...N in (7), and including the green equations (17) and (19)). A second algorithm, PI 2 CMAES, is readily acquired by using the more sophis-ticated covariance matrix updating rule of CMAES . Our next evaluation highlights the main advantage of these algorithms, and compares their performance. In Figure 6, we compare PI 2 (where the covariance matrix is constant 3 ) with PI 2 -CMA ( CEM -style co-variance matrix updating) and PI 2 -CMAES (covari-ance matrix updating with CMAES ). Initially, the covariance matrix for each of the 10 DOFs is set to  X  init =  X  init I 5 , where 5 is the number of basis func-tions, and  X  init = { 10 2 , 10 4 , 10 6 } determines the ini-tial exploration magnitude. All experiments are run for 200 updates, with K = 20 trials per update. We chose a higher K because we are now not only com-puting an update of the mean of the parameters (a 1  X  5 vector for each DOFs), but also its covariance matrix (a 5  X  5 matrix), and thus more information is needed per trial to get a robust update (Hansen &amp; Os-termeier, 2001). After each update, a small amount of base level exploration noise is added to the covariance matrix ( X  new  X   X  new + 10 2 I 5 ) to avoid premature convergence, as suggested by Kobilarov (2011). When the covariance matrices are not updated, the ex-ploration magnitude remains the same during learning, i.e.  X  =  X  init (labels A in Figure 6), and the conver-gence behavior is different for the different exploration magnitudes  X  init = { 10 2 , 10 4 , 10 6 } . For  X  init = 10 have nice convergence behavior B , which is not a coin-cidence  X  this value has been specifically tuned for this task, and it is the default we have used so far. How-ever, when we set the exploration magnitude very low (  X  init = 10 2 ) convergence is much slower C . When the exploration magnitude is set very high  X  init = 10 6 , we get quick convergence D . But due to the high stochas-ticity in sampling, we still have a lot of stochasticity in the cost after convergence in comparison to lower  X  init . This can be seen in the inset, where the y -axis has been scaled  X  20 for detail E .
 For PI 2 -CMA, i.e. with covariance matrix updating, we see that the exploration magnitude  X  changes over time (bottom graph), whereby  X  is computed as the mean of the eigenvalues of the covariance matrix. For  X  init = 10 2 ,  X  rapidly increases F until a maximum value is reached, after which it decreases and con-verges to a value of 10 2 . 8 G . The same holds for  X  init = 10 4 , but the initial increase is not so rapid H . For  X  init = 10 6 ,  X  only decreases I , but converges to 10 2 . 8 as the others.
 From these results we derive three conclusions: 1) with PI 2 -CMA, the convergence speed does not depend as much on the initial exploration magnitude  X  init , i.e. after 500 updates the  X   X   X  cost for PI 2 -CMA over all  X  init is 10 5  X  (8  X  7), whereas for PI 2 without covariance matrix updating it is 10 5  X  (35  X  43) J . 2) PI 2 -CMA automatically increases  X  if more exploration leads to quicker convergence F H . 3) PI 2 -CMA automatically decreases  X  once the task has been learned G K . Note that 2) and 3) are emergent properties of covariance matrix updating, and has not been explicitly encoded in the algorithm. In summary, PI 2 -CMA is able to find a good exploration/exploitation trade-off, independent of the initial exploration magnitude.
 This is an important property, because setting the ex-ploration magnitude by hand is not straightforward, because it is highly task-dependent, and might require several evaluations to tune. One of the main con-tributions of this paper is that we demonstrate how using probability-weighted averaging to update the co-variance matrix (as is done in CEM ) allows PI 2 to au-tonomously tune the exploration magnitude  X  the user thus no longer needs to tune this parameter. The only remaining parameters of PI 2 are K (number of trials per update) and h (eliteness parameter), but choosing them is not critical. Although an initial  X  must be given, Figure 6 shows that with an initial exploration magnitude two orders of magnitude higher/lower than a tuned value, PI 2 -CMA still converges to the same cost and exploration magnitude, with only slight dif-ferences in the initial speed of convergence.
 When comparing PI 2 -CMA and PI 2 -CMAES, we only see a very small difference in terms of convergence when the initial exploration is low  X  init = 10 2 L . This is because the covariance update rule of CMAES is damped, (21) and (23), and it makes more conserva-tive updates than CEM , cf. I and N . In our exper-iments, PI 2 -CMAES uses the default parameters sug-gested by Hansen &amp; Ostermeier (2001). We have tried different parameters for PI 2 -CMAES, the conclusion being that the best parameters are those that reduce CMAES to CEM , cf. Section 2.2. In general, we do not claim that PI 2 -CMAES outperforms PI 2 , and Hansen &amp; Ostermeier (2001) also conclude that there are tasks where CMAES has identical performance to simpler algorithms. Our results on comparing PI 2 -CMAES and PI 2 -CMA are therefore not conclusive. An interesting question is whether typical cost func-tions found in robotics problems have properties that do not allow CMAES to leverage the advantages it has on benchmark problems used in optimization. In this paper, we have scrutinized the recent state-of-the-art direct policy improvement algorithm PI 2 from the specific perspective of belonging to a fam-ily of methods based on the concept of probability-weighted averaging. We have discussed similarities and differences between three algorithms in this fam-ily, being PI 2 , CMAES and CEM . In particular, we have demonstrated that using probability-weighted av-eraging to update the covariance matrix, as is done in CEM and CMAES , allows PI 2 to autonomously tune the exploration magnitude. The resulting algorithm PI 2 -CMA shows more consistent convergence under varying initial conditions, and alleviates the user from having to tune the exploration magnitude parameter by hand. We are currently applying PI 2 -CMA to chal-lenging tasks on a physical humanoid robot. Given the ability of PI 2 to learn complex, high-dimensional tasks on real robots (Stulp et al., 2011), we are con-fident that PI 2 -CMA can also successfully be applied to such tasks.
 We thank the reviewers for their constructive sugges-tions for improvement of the paper. This work is sup-ported by the French ANR program (ANR 2010 BLAN 0216 01), more at http://macsi.isir.upmc.fr
