 Community-based Question Answering (CQA) services have become a major venue for people X  X  information seeking on the Web. However, many studies on CQA have focused on the prediction of the best answers for a given question. This paper looks into the formulation of effective questions in the context of CQA. In particular, we looked at effect of contextual factors appended to a basic question on the performance of submitted answers. This study analysed a total of 930 answers returned in response to 266 questions that were formulated by 46 participants. The results show that adding a questionnaire X  X  personal and social attribute to the question helped improve the perceptions of answers both in information seeking questions and opinion seeking questions.
 H.3.3 [ Information Search and Retrieval ]: Query for-mulation; H.3.5 [ On-line Information Services ]: Web-based services Experimentation, Human Factors Question formulation, context, CQA
Taylor X  X  1968 model is perhaps one of the earliest work to point out that people X  X  expression of their information needs can be compromised by a given environment [9]. Short queries frequently submitted to search engines show that Taylor X  X  model is also evident on the Web. One goal of Information Retrieval (IR) research is to retrieve relevant information in response to such compromised expressions of people X  X  information needs. In this regard, community-based Question Answering (CQA) sites are an interesting venue for information seeking on the Web, since the expres-sion of information needs does not have to be compromised as much as typical search engine queries. People use natural language to formulate a query and it is common to use mul-tiple sentences to elaborate the information needs in CQA services. Meanwhile, Yahoo! Answers alone are reported to an important research area in IR. However, existing studies on CQA have been focused on the prediction of the best an-swers for a given question [1, 2, 8]. Investigations on how to formulate effective questions have been limited.
To address the problem, this paper examined effect of contextual factors in question formulation in the context of CQA. Appending a contextual factor has been shown to be effective in query formulation [7] in IR. Therefore, we hy-pothesised that appending a contextual factor to a basic question could improve the performance of submitted an-swers. To examine the hypothesis, we asked participants to formulate genuine questions with different contextual fac-tors, submitted them to a CQA site, and assessed the an-swers responded to the questions.
The first task was to identify the contextual factors to be examined in our investigation. Our approach was two-fold. First, we revisited exiting context models proposed in IR. More specifically, we synthesised the context models proposed in Interactive IR [6], QE [7], and personalisation [3]. Second, we manually investigated a CQA archive to identify contextual factors that have been used in questions. We used the CQA data of Yahoo Japan Corporation which contains over 3 million questions posted during 2004 and 2005. 1,400 questions were randomly chosen from the data collection and manually analysed by one of the authors.
As a result of this process, we have identified five groups of contextual factors, namely, task (reasons, motivations, aims, and goals), thought (own thought, answer prediction), situation (circumstances, environment, experience, knowl-edge, and familiarity), attribute (personal attribute, social status), and limit (conditions on answers and answerers). The category of task, situation, and attribute had overlap with existing models in IR, while the category of thought and limit were found to be distinctive in CQA.

It should be noted that we appended only one contextual factor to basic questions once at a time in this study.
We were also interested in how effect of contextual factors differed across the types of questions. We decided to look ht t p://yanswersblog.com/index.php/archives/2009/ 12/14/yahoo-answers-hits-200-million-visitors-worldwide/ at two types of questions based on the investigations by [1, 5, 4 ]. One type was called information seeking questions while another type was called opinion seeking questions. As the name suggests, the former type was generally looking for information such as facts and methods, while the latter type was looking for opinions and suggestions. 46 participants (27 females and 19 males) were recruited for the study. The majority of participants were the under-graduate and postgraduate students of our institution, with varied backgrounds (e.g., Management, Physics, Mathemat-ics, Linguistics, Engineering, Medicine, Cultural Study, Me-dia Study, Computer Science, Information Science, Chem-istry, Art, Robotics, Biology, etc).

Participants were asked to formulate six genuine ques-tions. Three of them were information seeking questions while others were opinion seeking questions. Participants were asked to include one of the five contextual factors in their question formulation. The remaining question was for-mulated without any contextual factor. We rotated the or-der of contextual factors to append across participants.
The formulated questions were then submitted to one of the major CQA sites in Japan by the experimenter, and recorded any answers given to the questions for 7 days. Par-ticipants were called again and asked to assess the answers from eight perspectives. They were Overall satisfaction , Topicality , Novelty , Situational relevance , Attitude , Trusti-ness , Promptness , and Length . A 5-point Likert scale was used to capture participants X  assessment of answers. 266 questions were formulated and 930 answers were col-lected and assessed by participants.
We examined effect of five contextual factors used in ques-tion formulation on the perception of answers, by comparing to the questions formulated without using contextual fac-tors. This section reports only on a subset of analysis we carried out during this study.

First, we examined the number of answers submitted to the questions. Although opinion seeking questions tended to receive more answers than information seeking questions (3.9 vs. 2.9, p  X  . 005 by T-test), there was no significant effect of contextual factors on the number of answers.
Second, we examined participants X  perceptions on the an-swers. The result pattern differed across the question types. In information seeking questions, the questions with At-tribute and Thought had a higher average score of Attitude of answers (Attribute: p  X  . 001, Thought: p  X  . 003, by T-test). In opinion seeking questions, the questions with Attribute had a higher average score of Novelty of answers ( p  X  . 003, by T-test). On the other hand, the factors such as Task and Situation had a negative impact on the perceptions of their answers.

Overall, we gained some evidence suggesting that append-ing a contextual factor is a promising way to formulate effec-tive questions in a CQA site. At the same time, the results suggest that some contextual factors should be avoided in a CQA site. Also, our results suggest that effective contextual factors can differ across the question types. A comprehen-sive description of this study and analysis of the results will be reported elsewhere.
This paper explored effect of contextual factors as a means of formulating effective questions in the context of community-based question answering services. Genuine questions were formulated by our participants and some questions were pre-sented with a contextual factor and others without. Based on the analysis of over 900 answers submitted to partici-pants X  questions, we gained evidence to support the following findings. First, including a contextual factor to the question can improve a questioner X  X  assessment of the answers. Sec-ond, affective contextual factors can differ across the ques-tion type (information seeking and opinion seeking type). These findings can be a good starting point in guiding how beginners use CQA services to formulate their questions.
The authors thank to Yahoo JAPAN Corporation and Na-tional Institute of Informatics who gave us access to Yahoo! Chiebukuro Data (Version 1). [1] L. A. Adamic, J. Zhang, E. Bakshy, and M. S. [2] E. Agichtein, Y. Liu, and J. Bian. Modeling [3] A. G  X  oker and H. I. Myrhaug. User context and [4] F. M. Harper, D. Moy, and J. A. Konstan. Facts or [5] F. M. Harper, D. Raban, S. Rafaeli, and J. A. Konstan. [6] P. Ingwersen and K. J  X  arvelin. The turn: integration of [7] D. Kelly, V. D. Dollu, and X. Fu. The loquacious user: [8] Y. Liu, J. Bian, and E. Agichtein. Predicting [9] R. S. Taylor. Question-negotiation and information
