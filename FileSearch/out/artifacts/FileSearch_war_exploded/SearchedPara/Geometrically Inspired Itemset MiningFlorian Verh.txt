
In our geometric view, an itemset is a vector ( itemvector ) in the space of transactions. Linear and potentially non-linear transformations can be applied to the itemvectors before mining patterns . Aggregation functions and interestingness measures can be applied to the transformed vectors and pushed inside the mining process. We show that interest-ing itemset mining can be carried out by instanti-ating four abstract functions: a transformation ( g ), an algebraic aggregation operator (  X  ) and measures ( f and F ). For Frequent Itemset Mining (FIM), g and F are identity transformations,  X  is intersection and f is the cardinality. Based on this geometric view we present a novel algorithm that uses space linear in the number of 1-itemsets to mine all inter-esting itemsets in a single pass over the data, with no candidate generation. It scales (roughly) linearly in running time with the number of interesting item-sets. FIM experiments show that it outperforms FP-Growth on realistic datasets above a small support threshold (0.29% and 1.2% in our experiments) 1 .
Traditional Association Rule Mining (ARM) con-siders a set of transactions T containing items I . Each transaction t  X  T is a subset of the items, t  X  I . The most time-consuming task of ARM is Frequent Itemset Mining (FIM), whereby all itemsets I  X  I that occur in a sufficient number of transactions are generated. Specifically, if  X  ( I )  X  minSup , where  X  (
I )= |{ t : I  X  t }| is the number of transactions containing I (known as the support of I ).

For item enumeration type algorithms, each trans-action has generally been recorded as a row in the dataset. These algorithms make two or more passes, reading it one transaction at a time.
 We consider the data in its transposed format: Each row, x { i } , (corresponding to an item i  X  I ) contains the set of transaction identifiers ( tid s) of the transactions containing i . Specifically, x { i } = { t.tid : t  X  T  X  i  X  t } . We call x { i } an itemvector because it represents an item in the space spanned by the transactions 2 . An example is provided in Figure 1(a).

We can also represent an itemset I  X  I as an itemvector: x I = { t.tid : t  X  T  X  I  X  t } . Figure 1(b) shows all itemsets that have support greater than one, represented as vectors in transaction space. For example, consider x { 4 } = { t 2 ,t 3 } located at g and x is located at g . It should be clear that  X  ( I )= | x I | = | X 
There are a three important things to note from the above: (1) We can represent an item by a vector (we used a set representation to encode its location in transaction space, but we could have equally well used an alternate representation in some other space). (2) We can create itemvectors that represent itemsets by performing a simple operation on the itemvectors (in the case above, set intersection). (3) We can eval-uate a measure using a function on the itemvector (in the above case, we used set size and the support measure). These fundamental operations are all that are required for a mining algorithm. In Section 3 we generalise them to function g (  X  ) , operator  X  and func-tion f (  X  ) respectively. We add an additional family of functions F (  X  ) for more complicated measures. So far we have considered itemvectors as binary. There is no reason for this restriction. Provided that we have functions f (  X  ) and F (  X  ) and an operator  X  that obey the requirements set out in Section 3, we can map the original itemvector into some other space via g (  X  ) . This has potential for dimensionality and noise reduction or sketch algorithms. Suppose it is possible to perform Singular Value Decomposition (SVD) or Random Projections [1] to reduce the noise and dimensionality (number of transactions) before mining itemsets. This has previously been impossi-ble since the transformation creates real valued vec-tors, and hence cannot be mined using exiting algo-rithms. Using our framework, all that is required are suitable  X  , f (  X  ) and F (  X  ) . We can also use measures other than support. Any anti-monotonic (or prefix or weakly anti-monotonic) measure that fits into our framework can be used.

We briefly illustrate some of the ideas used in our algorithm using Figure 1(a). We simply wish to convey the importance of the transpose view to our technique, and introduce some of the challenges we solved. Too keep things simple, we use the instantia-FIM. Our algorithm scans the transposed dataset row by row. Suppose we scan it bottom up 3 so we first read x { 5 } = { t 1 ,t 3 } . Assume minSup =1 .We can immediately say that  X  ( { 5 } )=2  X  minSup and so itemset { 5 } is frequent. We then read the next row, x { 4 } = { t 2 ,t 3 } , and find that { 4 } is frequent. Since we now have both x { 5 } and x { 4 } , we can create x all possible itemsets containing items 4 and 5 .To progress, we read x { 3 } = { t 2 } and find that { 3 } is frequent. We can also check more itemsets: x { 3 , 5 } = x so { 3 , 4 } is frequent. Since { 3 , 5 } is not frequent, neither is { 3 , 4 , 5 } by the anti-monotonic property of support [2]. We next read x { 2 } and continue the pro-cess. It should be clear from the above that (1) a sin-gle pass over the dataset is sufficient to mine all fre-quent itemsets, (2) having processed any n itemvec-tors corresponding to items in J = { 1 , ..., n } ,we can generate all itemsets L  X  J and (3) having the dataset in transpose format and using the itemvector concept allows this to work.

Each itemvector could take up significant space, we may need many of them, and operations on them could be expensive. We generate at least as many itemvectors as there are frequent itemsets 4 . Since the number of itemsets is at worst 2 | I |  X  1 , clearly it is not feasible to keep all these in memory, nor do we need to. On the other hand, we do not want to recom-pute them as this is expensive. If there are n items we could use n itemvectors of space and create all item-sets, but we must recompute most itemvectors mul-tiple times, so the time is not linear in the number of frequent itemsets  X  it will be exponential. For ex-ample, suppose we have created x { 1 , 2 , 3 } . When we would like to use the previously calculated x { 1 , 2 , 3 } x challenge is to use as little space as necessary, while avoiding re-computations.

We present an algorithm that uses time roughly linear in the number of interesting itemsets and at worst n + l/ 2 itemvectors of space, where n  X  n is the number of interesting 1-itemsets and l is the size of the largest interesting itemset. This worst case scenario is only reached with extremely low support, and most practical situations require only a small fraction of n . Based on these facts and the geomet-ric inspiration provided by the itemvectors, we call it Geometrically inspired Linear Itemset Mining In the Transpose ,or GLIMIT .

It is widely recognised that FP-Growth type algo-rithms are the fastest know algorithms. We show ex-perimentally that GLIMIT outperforms FP-Growth [5] when the support is above a small threshold.
GLIMIT is more than  X  X ust another FIM/ARM al-gorithm X  and support is just one of many possible interestingness measures it can use.

We make the following contributions :  X  We show interesting consequences of viewing  X  We present GLIMIT, a new, efficient and fast
In Section 2 we put our framework and GLIMIT in context of previous work. Section 3 presents our itemvector framework. Section 4 gives the the two data structures that can be used by GLIMIT. In Section 5 we first give the main facts exploited by GLIMIT and follow up with a comprehensive exam-ple. We prove the space complexity and give the pseudo-code. Section 6 contains our experiments. We conclude in Section 7.
Many itemset mining algorithms have been pro-posed since association rules were introduced [2]. Recent advances can be found in [3] and [4]. Most algorithms can be broadly classified into two groups, the item enumeration (such as [2, 5, 9]) and the row enumeration (such as [7, 12]) techniques. Broadly speaking, item enumeration algorithms are most ef-fective for datasets where | T | &gt;&gt; | I | , while row enu-meration algorithms are effective for datasets where |
T | &lt;&lt; | I | , such as for microarray data [7].
Item enumeration algorithms mine subsets of an itemset I before mining I . Only those itemsets for which all subsets are frequent are generated  X  mak-ing use of the anti-monotonic property of support. Apriori-like algorithms [2] do this in a breadth first manner and use a candidate generation step. They use multiple passes, at most equal to the length of the longest frequent itemset. Our algorithm does not perform candidate generation, and generates associ-ation rules in a depth first fashion using a single pass over the transposed dataset.

FP-Growth type algorithms [5, 9] generates a compressed summary of the dataset using two passes in a highly cross referenced tree, the FP-tree, before mining itemsets by traversing the tree. Like our al-gorithm it does not perform candidate generation and mines the itemsets in a depth first manner while still mining all subsets of an itemset I before mining I . It is very fast at reading from the FP-tree, but the downside is that the FP-tree can become very large and is expensive to generate, so this investment does not always pay off. Our algorithm uses only as much space as is required.

Row enumeration techniques effectively intersect transactions and generate supersets of I before min-ing I . Although it is much more difficult for these al-gorithms to make use of the anti-monotonic property for pruning, they exploit the fact that searching the row space in data with | T | &lt;&lt; | I | becomes cheaper than searching the itemset-space. GLIMIT is simi-lar to row enumeration algorithms since both search using the transpose of the dataset. However, where row enumeration intersects transactions (rows), we effectively intersect itemvectors (columns). But this similarity is tenuous at best. Furthermore, exist-ing algorithms use the transpose for counting con-venience rather than for any insight into the data, as we do in our itemvector framework. Since GLIMIT searches through the itemset space, it is classified as an item enumeration technique and is suited to the same types of data. However, it scans the original data column-wise (by scanning the transpose row-wise), while all other item enumeration techniques scan it row-wise. The transpose has never , to our best knowledge, been used in an item enumeration algo-rithm. In summary, we think it is about as similar to other item enumeration techniques as FP-Growth is to Apriori.
 Efforts to create a framework for support exist. Steinbach et al. [11] present one such generalisation, but their goal is to extend support to cover continuous data. This is very different to transforming the orig-inal (non-continuous) data into a real vector-space (which is one of our motivations). Their work is geared toward existing item enumeration algorithms and so their  X  X attern evaluation vector  X  summarises transactions (that is, rows ). Our framework operates on columns of the original data matrix. Furthermore, rather than generalising the support measures so as to cover more types of datasets, we generalise the oper-ations on itemvector and the transformations on the same dataset that can be used to enable a wide range of measures, not just support.

To our best knowledge, Ratio Rules are the clos-est attempt at combining SVD (or similar techniques such as Principal Component Analysis) and rule min-ing. Korn et al. [6] consider transaction data where items have continuous values associated with them, such as price. A transaction is considered a point in the space spanned by the items. By performing SVD on such datasets, they observe that the axes (orthogo-nal basis vectors) produced define ratios between sin-gle items. We consider items ( and itemsets )in trans-action space (not the other way around) so when we talk of performing SVD, the new axes are linear com-binations of transactions  X  not items. Hence I is un-changed. Secondly, we talk about mining itemsets, not just ratios between single items. Finally, SVD is just one possible instantiation of g (  X  ) .
By considering items as vectors in transaction space, we can interpret itemsets geometrically, which we do not believe has been considered previously. As well as inspiring our algorithm, this geometric view has the potential to lead to very useful preprocess-ing techniques, such as dimensionality reduction of the transactions space. Since GLIMIT uses only this framework, it will enable us to use such techniques  X  which are impossible using existing FIM algorithms.
In Section 1 we used the example of frequent itemset mining (FIM) to introduce our ideas. But our work is much more general than this  X  the instan-tiations of g (  X  ) ,  X  and f (  X  ) are trivial for FIM. The functions and operator we formally describe in this section define the form of interestingness measures and dataset transformations that are supported by our algorithm. Not only can many existing measures be mapped to this framework, but we hope the geomet-ric interpretation will inspire new ones.

Recall that x I is the set of transaction identifiers of the transactions containing the itemset I  X  I . Call X the space spanned by all possible x I . Specif-ically, X = P ( { t.tid : t  X  T } ) .
 Definition 1 g : X  X  Y is a transformation on the original itemvector to a different representation y I = g ( x I ) in a new space Y .
 Even though g (  X  ) is a transformation, it X  X  output still  X  X epresents X  the itemvector. To avoid too many terms, we also refer to y I as an itemvector.
 Definition 2  X  is an operator on the transformed That is,  X  is a commutative operator for combining itemvectors to create itemvectors representing larger itemsets. We do not require that y I = y I  X  y I 5 . Definition 3 f : Y  X  R k is a set of k measures on itemsets, evaluated on transformed itemvectors. We write m I = f ( y I ) . k is fixed.
 Definition 4 interestingness : P ( I )  X  R is an interestingness measure (order) on all itemsets. Suppose we have a measure of interestingness of an itemset that depends only on that itemset (eg: support). We can represent this as follows, where I = { i 1 , ..., i q } and k =1 : interestingness ( I )= f ( g ( x { i 1 } )  X  ...  X  g ( x { i
So the challenge is, given an interestingness measure, find suitable and useful g ,  X  and f so that the above holds. For support , we know we can use  X  =  X  , f = | X | and g as the identity function.
We now return to our motivation. First assume that g (  X  ) trivially maps x I to a binary vector. Us-Figure 1(a) we have y { 1 } = g ( x { 1 } ) = 110 and y { 5 } = g ( x { 5 } ) = 101 . It should be clear that we can use bitwise AN D as  X  and f = sum () , the number of set bits. But notice that sum ( y { 1 } AN D y { 2 } )= is the element-wise product 6 ). That is, the dot prod-uct of two itemvectors is the support of the the 2-itemset. What makes this interesting is that this holds for any rotation about the origin. Suppose we have an arbitrary 3  X  3 matrix R defining a rotation about the origin. This means we can define g ( x )= Rx T because the dot product is preserved by R (hence g (  X  ) ). For example,  X  ( { 1 , 5 } )= y { 1 }  X  y { 5 } (
Rx T { 1 } )  X  ( Rx T { 5 } ) . So we can perform an arbitrary rotation of our itemvectors before mining 2-itemsets. Of course this is much more expensive than bitwise AN D , so why would we want to do this? Consider Singular Value Decomposition. If normalisation is skipped, it becomes a rotation about the origin, pro-jecting the original data onto a new set of basis vec-tors pointing in the direction of greatest variance (in-cidentally, the covariance matrix calculated in SVD also defines the support of all 2-itemsets 7 ). If we ad-ditionally use it for dimensionality reduction, it has the property that it roughly preserves the dot product. This means we should be able to use SVD for dimen-sionality reduction and or noise reduction prior to mining frequent 2-itemsets without introducing too much error. The drawback is that the dot product applies only to two vectors. That is, we cannot use it for larger itemsets because the  X  X eneralised dot prod-uct X  satisfies sum ( Rx T { 1 } .  X  Rx T { 2 } .  X  ... .  X  ever, this does not mean that there are not other use-ful  X  , f (  X  ) , F (  X  ) and interestingness measures that satisfy Equation 1 and use g (  X  )= SV D , some that perhaps will be motivated by this observation. Note that the transpose operation is crucial in applying di-mensionality or noise reduction because it keeps the items intact. If we did not transpose the data, the itemspace would be reduced, and the results would be in terms of linear combinations of the original items, which cannot be interpreted. It also makes more sense to reduce noise in the transactions.
We could also choose g (  X  ) as a set compres-sion function or use approximate techniques, such as sketches, to give estimates rather than exact values of support or other measures. However, we think new geometrically inspired measures will be the most promising. For example, angles between itemvectors are linked to the correlation between itemsets. Of course, we can also translate existing measures into our framework.

To complete our framework we now define the family of functions F (  X  ) and give an example. Definition 5 F : R k  X |P ( I ) |  X  R is a measure on an itemset I that supports any composition of mea-sures (provided by f (  X  ) ) on any number of subsets of I . We write M I = F ( m I 1 ,m I 2 , ..., m I where m I i = f ( y I i ) and all I i  X  X  ( I ) . We can now support more complicated interesting-ness functions that require more than a simple ( k = 1 ) measure on one itemset: interestingness ( I )= F ( m I 1 ,m I 2 , ..., m I where the m I i are evaluated by f (  X  ) as before. That is, M I = F (  X  ) is evaluated over measures m I i where all I i  X  I .If F (  X  ) does not depend on an m
I i , we leave it out of the notation. In that sense we call F (  X  ) trivial if M I = F ( m I ) . In this case the function of F (  X  ) can be performed by f (  X  ) alone, as was the case in the examples we considered before introducing F (  X  ) .
 Example 1 The minP I of an itemset I = { 1 , ..., q } is minP I ( I ) = min i {  X  ( I ) / X  ( { i } ) } . This mea-sure is anti-monotonic and gives high value to item-sets where each member predicts the itemset with high probability. It is used in part for spatial co-location mining [10]. Using the data in Figure 1(a), minP I ( { 1 , 2 , 3 } ) = min { 1 / 2 , 1 / 3 , 1 / 1 } =1 terms of our framework g (  X  ) is the identity function,  X  =  X  , f = | X | so that m I =  X  ( I ) and M I = F (
Our algorithm uses only the framework described above for computations on itemvectors. It also pro-vides the arguments for the operators and functions very efficiently so it is flexible and fast. Because GLIMIT generates all subsets of an itemset I  X  before it generates the itemset I , an anti-monotonic prop-erty enables it to prune the search space. Therefore, to avoid exhaustive searches, our algorithm requires 8 that the function F (  X  ) be anti-monotonic 9 in the un-derlying itemsets over which it operate (in conjunc-tion with  X  , g (  X  ) and f (  X  ) 10 ).
 Definition 6 F (  X  ) is anti-monotonic if M I  X  M
I  X   X  X  X  I  X  I  X  , where M I = F (  X  ) is evalu-ated as per Definition 5.
 In the spirit this restriction, an itemset I is consid-ered interesting if M I  X  minM easure , a thresh-old. We call such itemsets F-itemsets .
In this section we outline two data structures that our algorithm (optionally) generates and uses.
We use the PrefixTree to efficiently store and build frequent itemsets. We represent an itemset I = { i 1 , ..., i k } as a sequence i 1 , ..., i k by choosing a global ordering of the items (in this case i 1 &lt; ... &lt; i ), and store the sequence in the tree. An exam-ple of a PrefixTree storing all subsets of { 1 , 2 , 3 , 4 } is shown in Figure 2(a). Since each node represents a sequence (ordered itemset) we can use the terms prefix-node , itemset and sequence interchangeably. The prefix tree is built of PrefixNodes. Each PrefixN-ode is a tuple ( parent, depth, m, M, item ) where parent points to the parent of the node (so n.parent represents the prefix of n ), depth is its depth of the node and therefore the length of the itemset at that node, m ( M ) is the measure(s) of the itemset evaluated by f (  X  ) ( F (  X  ) ) and item is the last item in the sequence represented by the node. is the empty item so that { } X   X  =  X  where  X  is an itemset. The sequence (in reverse) represented by any node can be recovered by traversing toward the root. To make the link with our itemvector frame-work clear, suppose the itemset represented at a Pre-fixNode p is I = { i 1 ,i 2 , .., i k } . Then p.m = m I = f ( where F is potentially a function of the m  X  X  of Pre-fixNodes corresponding to subsets of I .
 The tree has the property that if s is in the Prefix-Tree, then so are all subsequences s s by the anti-monotonic property of F (  X  ) . Hence we save a lot of space because the tree never duplicates prefixes. In fact, it contains exactly one node per F-itemset .
The PrefixTree is designed for efficient storage, and not for lookup purposes. This is why there are no references to child nodes. To facilitate the generation of association rules, a set of all nodes that have no children is maintained. We call this the fringe .
The fringe is useful for because (1) it contains all maximal itemsets 11 , and (2) it can be used to effi-ciently generate all association rules: Lemma 1 Let s = i 1 , ..., i k =  X  X  X  be the sequence corresponding to a prefix node n where  X ,  X  =  X  . All association rules can be generated by creating all rules  X   X   X  and  X   X   X  for each fringe node.
 Proof: (Sketch) We don X  X  generate all possible as-sociation rules that can be generated from itemset { i 1 , ..., i k } by considering only n . Specifically, we miss (1) any rules  X   X   X  or  X   X   X  where  X  is not a prefix of s , and (2) any such rules where there is a gap between  X  and  X  . However, by the construction of the tree there exists another node n corresponding to the sequence s =  X  , X  (since s s ). If n is not in the fringe, then by definition s s  X  where s  X =  X  , X  , X  for some  X  =  X  and n  X  (the node for s  X  ) is in the fringe. Hence  X   X   X  and  X   X   X  will be generated from node(s) other than n . Finally, the longest sequences are guaran-teed to be in the fringe, hence all rules will be gener-ated by induction.
 We use a SequenceMap to index the nodes in the PrefixTree so we can retrieve them for the follow-ing purposes: (1) to check all subsequences of a po-tential itemset for pruning (we automatically check two subsets without using the sequence map 12 ), (2) to find the measures ( m , M )for  X  in Lemma 1 when generating association rules 13 , and (3) to find the the m s when we evaluate a nontrivial F (  X  ) .

First we introduce an abstract type  X  Sequence  X  which represents a sequence of items. Equality test-ing and hash-code calculation is done using the most efficient iteration direction. PrefixNode can be con-sidered a Sequence 14 and reverse iteration is the most efficient. By performing equality comparisons in re-verse sequence order, we can map any Sequence to a PrefixNode by using a hash-table that stores the Pre-fixNode both as the key and the value. Hence we can search using any Sequence implementation (in-cluding a list of items), and the hash-table maps Se-quences to the PrefixNodes that represents them. The space required is only that of the hash-table X  X  bucket array, so it is very space efficient.

Finally, we can avoid the use of both the Pre-fixTree and SequenceMap without alteration to GLIMIT if (1) we output frequent itemsets when they are mined, (2) do not care if not all subsets are checked before we calculate a new itemvector, and (3) have a trivial F (  X  ) .
In this section we outline the main principles we use in GLIMIT and follow up with an illustrative ex-ample. We prove space complexity bounds before giving the algorithm in pseudo-code.

We exploit the following facts in order to use min-imum space while avoiding any re-computations: 1. We can construct all itemvectors y I by incre-2. It also means we use least space if we perform a 3. We only ever check a new sequence by  X  X oining X  4. Suppose the items are I = { i 1 ,i 2 , ..., i n } . 5. When a PrefixNode p with p.depth &gt; 1 (or 6. When a topmost sibling (the topmost child of 7. When we create a PrefixNode p on the topmost
We now present an example of our algorithm to illustrate some of these facts.

Suppose we have the items { 1 , 2 , 3 , 4 } and the minM easure (we will use minSup ) threshold is such that all itemsets are interesting (frequent). Fig-ure 2 shows the target prefix tree and the steps in min-ing it. This example serves to show how we manage the memory while avoiding any re-computations. For now, consider the frontier list in the figure as a list of PrefixNodes that have not been completed. It should be clear that we use a bottom up and depth first pro-cedure to mine the itemsets, as motivated by Facts 2 and 4. We complete all subtrees before moving to the per Fact 1. Note we are also making use of Fact 3  X  { 3 } and { 4 } are siblings. Once we have created the node for { 3 , 4 } in (d), we can delete y { 3 , 4 } by Fact 5. It has no possible children because of the ordering of the sequences. The same holds for { 2 , 4 } in (f). In (g), the node for { 2 , 3 } is the topmost sibling (child). Hence we can apply Fact 6 in (h). Note that by Fact Note also that because we need the itemvectors of the single items in memory we have not been able to use Fact 7 yet. Similarly, Fact 6 is also applied in (l), (m), (o) and (p). However, note that in (m), (o) and (p) we also use Fact 7 to delete y { 2 } , y { 3 } , and y { 4 } . In (l) we deleted y { 1 } for two reasons: Fact 6 and 7 (it is a special case in Fact 6). Finally, to better illustrate Fact 3, suppose { 2 , 4 } is not frequent. This means that { 2 , 3 } will have no siblings anymore, so we do not even need to consider { 2 , 3 , 4 } by Fact 3.
We know already that the time complexity is roughly linear in the number of frequent itemsets, be-cause we avoid re-computations of itemvectors. So the question now is, what is the maximum number of itemvectors that we have in memory at any time? There are two main factors that influence this. First, we need to keep the itemvectors for individual items in memory until we we have completed the node for the top-most item (Fact 1 and 7). Hence, the  X  X igher X 
Figure 3. Maximum number of itemvec-tors. Two cases: n even or odd. up in the tree we are, the more this contributes. Sec-ondly, we need to keep itemvectors in memory until we complete their respective nodes. That is, check all their children (Fact 6) or if they can X  X  have chil-dren (Fact 5). Now, the further we are up in the tree, or any subtree for that matter, without complet-ing the node, the longer the sequence of incomplete nodes is and hence the the more itemvectors we need to keep. Considering both these factors leads to the situation in Figure 3  X  that is, we are up to the top item and the topmost path from that item so that no node along the path is completed. If we have n items, the worst case itemvector usage is just the number of coloured nodes in Figure 3. There are n itemvec-tors y { i } : i  X  X  1 , ..., n } corresponding to the single items (children of the root). There are a further n/ 2 itemvectors along the path from node { 1 } (inclusive) to the last coloured node (these are the uncompleted nodes) 16 . Therefore the total space required is just n + n/ 2  X  1 , where the  X  1 is so that we do not double count the itemvector for { 1 } .

This is for the worst case when all itemsets are frequent. Clearly, a closer bound is if we let n  X  n be the number of frequent items. Hence, we need space linear in the number of frequent items. The multiplicative constant ( 1 . 5 ) is low, and in practice (with non-pathological support thresholds), we use far fewer than n itemvectors. If we know that the longest frequent itemset has size l , then we can ad-ditionally bound the space by n + l/ 2  X  1 . Fur-thermore, since the frontier contains all uncompleted nodes, we know from the above that its upper bound is l/ 2 . We have sketched the proof of: Lemma 2 Let n be the number of items, and n  X  n be the number of frequent items. Let l  X  n be the largest itemset. GLIMIT uses at most n + l/ 2  X  1 itemvectors of space. Furthermore, | frontier | X  l/ 2 .
 As an aside, note we could perform the transpose operation in memory before mining while still re-maining within the worst case space complexity. However, on average and for practical levels of minM easure (eg: minSup ), this would require more memory.

The algorithm is a depth first traversal through the PrefixTree. Any search can be implemented either recursively or using the frontier method, whereby a list (priority queue) of states (each con-taining a node that has yet to be completely ex-panded) is maintained. The general construct is to retrieve the first state, evaluate it for the search cri-teria, expand it (create some child nodes), and add states corresponding to the child nodes to the frontier. Using different criteria and frontier orderings leads to different search techniques. Our frontier contains any nodes that have not yet been completed, wrapped in State objects. Algorithm 1 describes 17 the addi-tional types we use (such as State ) and shows the ini-tialisation and the main loop  X  which calls step (  X  ) .It also describes the check (  X  ) and calculateF (  X  ) meth-ods, used by step (  X  ) .
We evaluated our algorithm on two publicly available datasets from the FIMI repository 18  X  T10I4D100K and T40I10D100K. These datasets have 100 , 000 transactions and a realistic skewed his-togram of items. They have 870 and 942 items re-spectively. To apply GLIMIT we first transpose the dataset as a preprocessing step 19 .

We compared GLIMIT to a publicly available im-plementation of FP-Growth and Apriori. We used the algorithms from ARtool 20 as it is written in Algorithm 1 Data-types, initialisation, main loop and methods. Java, like our implementation, and it has been avail-able for some time. In this section we really only want to show that GLIMIT is quite fast and efficient when compared to existing algorithms on the tradi-tional FIM problem. Our contribution is the itemvec-tor framework that allows operations that previously could not be considered, and a flexible and new class of algorithm that uses this framework to efficiently mine data cast into different and useful spaces. The fact that it is also fast when applied to traditional FIM is secondary . To represent itemvectors for tra-ditional FIM, we used bit-vectors 21 so that each bit is set if the corresponding transaction contains the item(set). Therefore g creates the bit-vector,  X  = AN D , f (  X  )= sum (  X  ) and F ( m )= m .
 Figure 4(a) shows the runtime 22 of FP-Growth, GLIMIT and Apriori 23 on T10I4D100K, as well as the number of frequent items. The analogous graph for T40I10D100K is shown in Figure 4(b)  X  we did not run Apriori as it is too slow. These graphs clearly show that when the support thresh-old is below a small value (about 0.29% and 1.2% for the respective datasets), FP-Growth is superior to GLIMIT. However, above this threshold GLIMIT outperforms FP-Growth significantly. Figure 5(a) shows this more explicitly by presenting the runtime ratios for T40I10D100K. FP-Growth takes at worst 19 times as long as GLIMIT. We think it is clear that GLIMIT is superior above the threshold. Further-more, this threshold is very small and practical ap-plications usually mine with much larger thresholds than this.

GLIMIT scales roughly linearly in the number of frequent itemsets. Figure 5(b) demonstrates this ex-perimentally by showing the average time to mine a single frequent itemset. The value for GLIMIT is quite stable, rising slowly toward the end (as there we still need to check itemsets, but very few of these turn out to be frequent). FP-Growth on the other hand, clearly does not scale linearly. The reason be-hind these differences is that FP-Growth first builds an FP-tree. This effectively stores the entire Dataset (minus infrequent single items) in memory. The FP-tree is also highly cross-referenced so that searches are fast. The downside is that this takes significant time and a lot of space. This pays off extremely well when the support threshold is very low, as the fre-quent itemsets can read from the tree very quickly. However, when minSup is larger, much of the time and space is wasted. GLIMIT uses time and space as needed, so it does not waste as many resources, mak-ing it fast. The downside is that the operations on bit-vectors (in our experiments, of length 100 , 000 ) can be time consuming when compared to the search on the FP-tree, which is why GLIMIT cannot keep up when minSup is very small. Figure 5(c) shows the maximum and average 24 number of itemvectors our algorithm uses as a percentage of the number of items. At worst, this can be interpreted as the percentage of the dataset in memory. Although the worst case space is 1 . 5 times the number of items, n (Lemma 2), the figure clearly shows this is never reached in our experiments. Our maximum was ap-proximately 0 . 82 n . By the time it gets close to 1 . 5 minSup would be so small that the runtime would be unfeasibly large anyhow. Furthermore, the space required drops quite quickly as minSup is increased (and hence the number of frequent items decreases). Figure 5(c) also shows that the maximum frontier size is very small.

Finally, we reiterate that we can avoid using the prefix tree and sequence map, so the only space re-quired are the itemvectors and the frontier . That is, the space required is truly linear.
We showed interesting consequences of view-ing transaction data as itemvectors in transaction-space, and developed a framework for operating on itemvectors. This abstraction gives great flexibility in the measures used and opens up the potential for useful transformations on the data. Our future work will focus on finding useful geometric measures and transformations for itemset mining. One problem is to find a way to use SVD prior to mining for itemsets larger than 2 . We also presented GLIMIT, a novel algorithm that uses our framework and significantly departs from existing algorithms. GLIMIT mines itemsets in one pass without candidate generation, in linear space and time linear in the number of in-teresting itemsets. Experiments showed that it beats FP-Growth above small support thresholds. Most im-portantly, it allows the use of transformations on the data that were previously impossible.

