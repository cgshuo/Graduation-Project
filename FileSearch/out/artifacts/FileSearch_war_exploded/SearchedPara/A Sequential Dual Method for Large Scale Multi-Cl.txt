 Efficient training of direct multi-class formulations of linear Support Vector Machines is very useful in applications such as text classification with a huge number examples as well as features. This paper presents a fast dual method for this training. The main idea is to sequentially traverse through the training set and optimize the dual variables associated with one example at a time. The speed of training is en-hanced further by shrinking and cooling heuristics. Experi-ments indicate that our method is much faster than state of the art solvers such as bundle, cutting plane and exponen-tiated gradient methods.
 I.5.2 [ Pattern Recognition ]: Design Methodology X  Clas-sifier design and evaluation Algorithms, Performance, Experimentation
Support vector machines (SVM) [2] are popular methods for solving classification problems. An SVM employing a nonlinear kernel maps the input x to a high dimensional feature space via a nonlinear function  X  ( x ) and forms a lin-ear classifier in that space to solve the problem. Inference as well as design are carried out using the kernel function, tions the input vector x itself has rich features and so it is sufficient to take  X  as the identity map, i.e.  X  ( x )= x .SVMs developed in this setting are referred to as Linear SVMs.
In applications such as text classification the input vector x resides in a high dimensional space (e.g. a bag-of-words representation), the training set has a large number of la-beled examples, and the training data matrix is sparse, i.e. the average number of non-zero elements in one x is small. Most classification problems arising in such domains involve many classes. Efficient lea rning of the parameters of linear SVMs for such problems is important. The One-Versus-Rest (OVR) method which consists of developing, for each class, a binary classifier that separates that class from the rest of the classes, and then combining the classifiers for multi-class inference, is one of the popular schemes to solve a multi-class problem. OVR is easy and efficient to design and also gives good performance [21].

Crammer and Singer [5, 6] and Weston and Watkins [25] gave direct multi-class SVM formulations. With nonlinear kernels in mind, Crammer and Singer [5, 6] and Hsu and Lin [10] gave efficient decomposition methods for these formula-tions. Recently, some good methods have been proposed for SVMs with structured outputs, that also specialize nicely for Crammer-Singer multi-class linear SVMs. Teo et al. [23] suggested a bundle method and Joachims et al. [13] gave a cutting plane method that is very close to it; these meth-ods can be viewed as extensions of the method given by Joachims [12] for binary linear SVMs. Collins et al. [4] gave the exponentiated gradient method.
 In this paper we present fast dual methods for Crammer-Singer and Weston-Watkins formulations. Like the methods of Crammer and Singer [5, 6] and Hsu and Lin [10] our methods optimize the dual variables associated with one ex-ample at a time. However, while those previous methods always choose the example that violates optimality the most for updating, our methods sequentially traverse through the examples. For the linear SVM case this leads to significant differences. In particular, for this case our methods are ex-tremely efficient while those methods are not so efficient. We borrow and modify the shrinking and cooling heuristics of Crammer and Singer [6] to make our methods even faster. Experiments on several datasets indicate that our method is also much faster than cutting plane, bundle and expo-nentiated gradient methods. For the special case of binary classification our methods turn out to be equivalent to doing coordinate descent. We covered this method in detail in [9].
Notations. The following notations are used in the pa-per. We use l to denote the number of training examples, l the number of classes. Throughout the paper, the index i will denote a training example and the index m will denote a class. Unless otherwise mentioned, i will run from 1 to l and m will run from 1 to k . x i  X  R n is the input vector ( n is the number of input features) associated with the i -th ex-ample and y i  X  X  1 ,...,k } is the corresponding target class. We will use 1 to denote a vector of all 1 X  X  whose dimension will be clear from the context. Superscript T will denote transpose for vectors.

The following multi-class datasets are used in the paper for doing various experiments: NEWS20 [14], SECTOR [20, 19], MNIST [15], RCV1 [16] and COVER [13]. RCV1 is a taxonomy based multi-label dataset and so we modified it to a multi-class dataset by removing all examples which had mismatched labels in the taxonomy tree; in the process of doing this, two classes do not have any training exam-ples. For NEWS20, MNIST and RCV1 we used the standard train-test split; for SECTOR we used the second train/test split used in [20]; and, for COVER we used the train/test split used in [13]. Table 1.1 gives key properties of these datasets.

The paper is organized as follows. In section 2 we give the details of our method (we call it as the Sequential Dual Method (SDM)) for the Crammer-Singer formulation and evaluate it against other methods. In section 3 we describe SDM for the Weston-Watkins formulation. In section 4 we compare these methods with the One-Versus-Rest method of solving multi-class problems. We conclude the paper in section 5. An implementation of a variant of the SDM al-gorithm for Crammer-Singer formulation is available in the LIBLINEAR library (version 1.3 or higher) with the option -s 4 ( http://www.csie.ntu.edu.tw/~cjlin/liblinear ).
In [5, 6], Crammer and Singer proposed an approach for the multi-class problem by formulating the following primal problem: where C&gt; 0 is the regulariza tion parameter, w m is the weight vector associated with class m , e m i =1  X   X  y i ,m  X  y i ,m =1if y i = m ,  X  y i ,m =0if y i = m . Note that, in (1) the constraint for m = y i corresponds to the non-negativity constraint,  X  i  X  0. The decision function is The dual problem of (1), developed along the lines of [5, 6], involves a vector  X  having dual variables  X  m i  X  m, i .The w get defined via  X  as At most places below, we simply write w m (  X  )as w m .Let i =0if y i = m , C m i = C if y i = m . The dual problem is The gradient of f plays an important role and is given by If  X  n is the average number of nonzero elements per training example, then on average the evaluation of each g m i takes O (  X  n ) effort. Optimality of  X  for (4) can be checked using the quantity, For a given i ,thevaluesof m that attain the max and min in (6) play a useful role and are denoted as follows: From (6) it is clear that v i is non-negative. Dual optimality holds when: For practical termination we can approximately check opti-mality using a tolerance parameter, &gt; 0: We will also refer to this as -optimality. The value =0 . 1 is a good choice for a practical implementation of SDM.
SDM consists of sequentially picking one i at a time and solving the restricted problem of optimizing only  X  m i  X  keeping all other variables fixed.  X  To do this, we let  X  X  denote the additive change to be applied to the current  X  and optimize  X  X  m i  X  m .Let  X  i ,  X  X  i , g i and C i be vectors that respectively gather the elements  X  m i ,  X  X  m i , g m over all m .With A i = x i 2 the sub-problem of optimizing  X  X  i is given by This can be derived by noting the following: (i) changing i to  X  m i +  X  X  m i causes w m to change to w m +  X  X  m i x
When specialized to binary classification ( k =2),foreach i the equality constraint in (4) can be used to eliminate the variable  X  m i , m = y i . With this done and the dual solu-tion viewed as optimization for the variables {  X  y i i } is equivalent to doing coordinate descent. This is what we did in [9]. Algorithm 2.1 SDM for Crammer-Singer Formulation of g m i in (5); and (v) using the above in (4) and leaving out all constants that do not depend on  X  X  i . The sub-problem in (10) has a nice simple form. Let us discuss methods for solving it. Suppose A i =0forsome i ,whichcanhappen only when x i = 0 . In the primal problem (1) such examples contribute a fixed cost of C to the objective function, they have no effect on the w m , and so they can be effectively ignored. So, hereafter we will assume A i &gt; 0  X  i . Crammer and Singer [5, 6] suggest two methods for solving (10): (i) an exact O ( k log k ) algorithm (see section 6.2 of [5]) and (ii) an approximate iterative fixed-point algorithm (see section 6 of [6]). Alternatively, one could also employ an active set method meant for convex quadratic programming problems [8], starting from  X  X  i = 0 as the initial point. Since the Hes-sian of the objective function in (10) is A i times the identity matrix and the constraints of (10) are in a simple form, var-ious linear equation solving steps of the active set method can be done analytically and cheaply. Our implementation uses this method.

A description of SDM is given in Algorithm 2.1. If good seed values are unavailable, a simple way to initialize is to set  X  = 0 ; this corresponds to w m = 0  X  m . Let us now discuss the complexity of the algorithm. Each execution of step (a) requires O ( k  X  n )effort;recallthat  X  n is the average number of nonzero elements per training example. The cost of step (b) is at most O ( k  X  n ), and it could be much less depending on the number of  X  m i that are actually changed during the solution of (10). Since the solution of (10) itself is usually not as high as these costs if k is not very large, it is reasonable to take the cost of an update for one i as O ( k  X  n ). Thus the overall cost of a loop, i.e. a full sequence of updates over all examples is O ( lk  X  n ). The main memory requirement of the algorithm consists of storing the data, x i  X  i ( O ( l  X  n )), the weights, w m  X  m ( O ( kn )) and the  X  m  X  i, m (at most O ( kl )). For problems with a large number of classes but having only a small number of non-zero  X  m i ,it is prudent to use a linked list data structure and store only those  X  m i which are non-zero.

The following convergence theorem can be proved for SDM, employing ideas similar to those in [17].
 Theorem 2.1 Let  X  t denote the  X  at the end of the t -th loop of Algorithm 2.1. Any limit point of a convergent sub-sequence of {  X  t } is a global minimum of (4).
 Because of space limitation we omit the proof. In [9] we showed that SDM for binary classification has a linear rate of convergence, i.e., there exists 0  X   X &lt; 1anda t 0 such that where f is the dual objective function, t is the loop count in the algorithm,  X  t is the  X  at the end of the t -th loop, and  X   X  is the dual optimal solution. On the other hand Theorem 2.1 only implies a weaker form of convergence for Algorithm 2.1. As we remarked earlier, for binary classification SDM becomes coordinate descent, a standard method for which good convergence results exist in the optimization literature [18]. The presence of the equality constraint in (4) makes the multi-class version somewhat non-standard. It is an in-teresting open problem to establish linear convergence for Algorithm 2.1.

Let us now discuss various ways of making Algorithm 2.1 more efficient. In the algorithm the examples are considered in the given order i =1 , 2 ,...,l for updating. Any system-atic regularities in the given order (for instance, all examples of one class coming together) may lead to slow convergence. So it is a good idea to randomly permute the examples. In fact it is useful to do this random permutation before each loop through the examples. We do this in our implemen-tation of SDM. In [9] we found the same idea to be useful for binary classification. Theorem 2.1 is valid even in the presence of such random permutations.

In the solution of the sub-problem (10), the variables,  X  corresponding to the  X  X ost violating X  indices m  X  X  M i ,m (see (7)) are particularly important. When the number of classes is large, for efficiency one could consider solving the sub-problem only for these two variables. An extended ver-sion of this idea is the following. Since the inactive variables, i.e., m :  X  m i &lt;C m i are important, we can optimize on the variables  X  m i for m = M i and m :  X  m i &lt;C m i .Inmostprob-lems the number of inactive variables per example is quite small  X  (even when the number of classes is large) and so we use this method in our implementation.
Crammer and Singer [6] employed two heuristics to speed up their algorithm. We employ these heuristics for SDM too. We now discuss these heuristics.

Shrinking. Shrinking [11], a technique for removing vari-ables that are not expected to change, is known to be a very good way to speed up decomposition methods for binary SVMs. In the multi-class case, it may turn out that for many examples  X  m i =0  X  m . So, for efficiency Crammer and Singer [6] suggested the idea of concentrating the algo-rithm X  X  effort on examples i for which there is at least one non-zero  X  m i .(If,foragiven i there is at least one non-zero  X  m i , then it is useful to observe the following using the special nature of the constraints in the dual problem: (i) there are at least two non-zero  X  m i ; and (ii)  X  y i i &gt; 0.) We can get further improved efficiency by doing two modifica-tions. The first modification is to also remove those i for which there is exactly one m = y i with  X  m i =  X   X  y i i (In other words, apart from removing those i with  X  m i =0  X  m , we are also removing those i with exactly two non-zero i each having magnitude C .) At optimality this gener-ically corresponds to arg max m = y i w T m x i being a singleton and  X  i = w T m x i  X  w T y i x i +1 &gt; 0. As we approach optimal-ity, for the m that attains the arg max above we expect  X  to stay at  X  C ,  X  y i i to stay at C and  X  m i =0forallother m . If there are many such examples, then this modification will give good benefits. The second modification is the follow-
This corresponds to the observation that for any given ex-ample the confusion in classification is only amongst a few classes. ing: even within an example i that does not get removed, consider for updating only those  X  m i which are non-zero. Efficiency improvement due to this modification can be sub-stantial when the average number of non-zero  X  m i for each example is much smaller than the number of classes. With these modifications included in shrinking SDM is changed as follows. We alternate between the following two steps. 1. First do a full sequential loop over all examples. 2. Do several loops over only those examples i that do
Cooling of accuracy parameter. Suppose we desire to solve the dual problem quite accurately using a small value of . When shrinking is employed there is a tendency for the algorithm to spend a lot of effort in the  X  X hrunk X  loops. The algorithm has to wait for the next full loop (where other optimality-violating  X  m i get included) to make a decent im-provement in the dual objective function. To avoid a stair-case behavior of the objective function with computing time, it is a good idea to start with a large value for and decrease it in several graded steps until the final desired is reached. In our implementation we start from = 1 and decrease it byafactorof10ineachgradedstep.

Figure 1 demonstrates the effectiveness of including the heuristics on two datasets. Clearly, the heuristics are valu-able when (4) needs to be solved quite accurately.
In [5, 6], Crammer and Singer gave a method for solving (4) with nonlinear kernels in mind. Their method also uses the basic idea of choosing an i and solving (10) to update  X  . Unlike our method which does a sequential traversal of the i , Crammer and Singer X  X  method always chooses the i for which the optimality violation v i in (6) is largest at the current  X  . To appreciate Crammer and Singer X  X  motivation for doing this, first note that the g m i are key quantities for calculating the v i as well as for setting up (10). So let us look at the computational complexity associated with g m i some detail.

For problems with a nonlinear kernel K ( x j , x i ), the g need to be computed using Let us recall the notations g i ,  X  i and  X  X  i given above (10). Direct computation of g i for one given i via (12) takes O ( lk  X  n ) effort. On the other hand, suppose we maintain g  X  j . In a typical step of Crammer and Singer X  X  method, an i is chosen and  X  i is changed as  X  i  X   X  i +  X  X  i ;insucha case the g j can be updated using
In our implementation we count the total number of g m i evaluations, divide it by kl to get the number of effective loops and terminate the  X  X hrunk X  loops when the number of effective loops exceeds 5. Figure 1: Comparison of SDM with (Red, dashdot) and without (Blue, solid) the shrinking and cooling heuristics on MNIST (top) and COVER (bottom) datasets. The horizontal axis is Training time in seconds. fore maintaining g j  X  j does not cost more than the direct computation of a single g i using (12). As using g j  X  j implies fewer iterations (i.e., faster convergence due to the ability to choose for updating the i that violates optimality most), of course one should take this advantage. This clearly moti-vates the scheme for choosing i in Crammer and Singer X  X  method.
 However, the situation for linear SVM is very different. With the cheap O ( k  X  n )way( g m i = w T m x i + e m i puting g i and the cheap O ( k  X  n )way( w m  X  w m +  X  X  m  X  m )ofupdatingthe w m , each update of  X  i takes only O ( k  X  n ) effort, and so, if l is large, SDM can do many more ( O ( l )) updates of the w m for the same effort as needed by Cram-mer and Singer X  X  method for one update of the w m .Hence, for linear SVM, always choosing the i that yields the maxi-mum v i is unwise and sequential traversal of the i is a much better choice.
We now compare our method with other methods for solv-ing (1). The cutting plane method of Joachims et al. [13] and the bundle method of Teo et al. [23] are given for the more general setting of SVMs with structured outputs, but they also specialize to multi-class linear SVMs using the Crammer-Singer formulation. These methods combine the loss term C convex, piecewise linear lower bounding approximations for this function by using sub-differentials of it from various points encountered during the solution. A negative aspect 2: C =0 . 1 of these algorithms is their need to scan the entire training set in order to form one sub-differential mentioned above; as we will see below in the experiments, this causes these algorithms to be slow to converge. Barring some differences in implementation the algorithms in [13] and [23] are the same and we will simply refer to them jointly as the Cut-ting Plane (CP) method. For comparison we use the im-plementation in http://www.cs.cornell.edu/People/tj/ svm_light/svm_multiclass.html .

The Exponentiated Gradient (EG) method [4] also ap-plies to structured output problems and specializes to the Crammer-Singer multi-class linear SVM formulation. It uses a dual form that is slightly different from (4), but is equiv-alent to it by a simple translation and scaling of variables. Like our method the EG method updates the dual variables associated with one example at a time, but the selection of examples is done in an online mode where an i is picked ran-domly each time. The EG method does an approximate de-scent via a constraint-obeying exponentiated gradient step. By its very design all dual inequality constraints remain inactive (i.e.,  X  m i &lt;C m i  X  i, m ) throughout the algorithm, though, as optimality is reached some of them may asymp-totically become active. The EG method is very straightfor-ward to implement and we use our own implementation for the experiments given below.

We now compare our method (SDM) against CP and EG on the five datasets used in this paper, both with respect to the ability to converge well to the optimum of (4) as well as the ability to reach steady state test set performance fast. Weevaluatetheformerbythe Relative Function Value Difference given by ( f  X  f ) /f where f is the optimal objective function value of (4), and the latter by Test Set Accuracy which is the percentage of test examples that are classified correctly. For simplicity we set C =1forthisex-periment. Figure 3 (see the last page of the paper) gives plots of these two quantities as a function of training time for the five datasets. Overall, SDM is much faster than EG, which in turn is much faster than CP. This is true irrespec-tive of whether the aim is to efficiently reach the optimum dual objective very closely or reach the steady state test ac-curacy fast. On COVER (see the bottom left row of Figure 3) EG has difficulty reaching the optimal objective function value, which is possibly due to numerical issues associated with large exponential operations involved in its updates.
When the regularization parameter C needs to be tuned, say via cross-validation techniques, it is necessary to solve (4) for a range of C values. It is important for an algorithm to be efficient for widely varying C values. For one dataset, MNIST, Figure 2 compares the three methods for large and small C values ( C =10and C =0 . 1); for the intermediate value of C = 1, see also the plots for MNIST in Figure 3. When actually finding solutions for an ordered set of several C values, the optimal  X  and the w m obtained from the algorithm for one C value can be used to form initializations of corresponding quantities of the algorithm for the next C value. This is a standard seeding technique that improves efficiency, and is also used in the EG method [4].
Stochastic gradient descent methods [22, 3] can be applied to the online version of (1) and they are known to achieve steady state test performance quite fast. We have not eval-uated SDM against these methods. However, our evaluation in [9] shows that, for binary classification SDM is faster than stochastic gradient methods. So we expect the same result to hold for the multi-class case too. The modified MIRA algorithm proposed by Crammer and Singer [7] has some similarities with our sequential dual method, but it is given in an online setting and does not solve the batch problem. Also, ideas such as shrinking do not apply to it. Bordes et al. [1] apply a coordinate descent method for multi-class SVM, but they focus on nonlinear kernels.
Let e m i and  X  m i be as defined in section 2. In [25], We-ston and Watkins proposed an approach for the multi-class problem by formulating the following primal problem: The decision function remains the same as in (2). The dual problem of (14) involves a vector  X  having dual variables and define The dual of (14) can now be written as The gradient of f is given by g Optimality can be checked using v m i , m = y i , defined as: Clearly v m i  X  0. Optimality holds when: For practical termination we can approximately check this using a tolerance parameter, &gt; 0: The value =0 . 1 is a good choice for a practical implemen-tation.

Like in section 2, the Sequential Dual Method (SDM) for the Weston-Watkins formulation consists of sequentially picking one i at a time and solving the restricted problem of optimizing only  X  m i  X  m = y i . Todothis,welet  X  X  m i denote the additive change to be applied to the current  X  m i ,and optimize  X  X  m i  X  m = y i .Let  X  i ,  X  X  i and g i be vectors that respectively gather the elements  X  m i ,  X  X  m i and g m i m = y i .With A i = x i 2 the sub-problem of optimizing  X  X  i is given by Like (10), the sub-problem (21) too has a simple form. We solve it also using the active set method. We note that the Hessian of the objective function in (21) is A i ( I + 11 (where I is the identity matrix), and so, any of its block diag-onal sub-matrices can be analytically and cheaply inverted. Therefore the active-set method is very efficient and we used Algorithm 3.1 SDM for Weston-Watkins Formulation Figure 4: Comparison of SDM (Weston-Watkins) with (Red, dashdot) and without (Blue, solid) the shrinking and cooling heuristics on MNIST (top) and COVER (bottom) datasets. The horizontal axis is Training time in seconds. this method in our implementation. A complete description of SDM for the Weston-Watkins formulation is given in Al-gorithm 3.1. The initialization, as well as time and memory complexity analysis given earlier for Crammer-Singer SDM also hold for this algorithm. Turning to convergence, let us assume, like in section 2, that A i &gt; 0  X  i . Unlike (4), the dual (16) does not have any equality constraints. Algorithm 3.1 is a block coordinate descent method for which conver-gence results from the optimization literature (see [18] and in particular section 6 there) can be applied to show that Algorithm 3.1 has linear convergence. Finally, similar to section 2.2 we can argue that, the method of Hsu and Lin [10] is suited for nonlinear kernels, but SDM is better for the linear SVM case.

For efficiency, (21) can be solved only for some restricted variables, say only the  X  X  m i for which v m i &gt; 0. The heuris-tics given for Crammer-Singer SDM can be extended to the Weston-Watkins version too. In the shrinking phase, for each i , among all m = i we only compute g m i and update i for those m which satisfy 0 &lt; X  m i &lt;C .Figure4shows the effectiveness of including the heuristics on MNIST and COVER datasets.
The One-Versus-Rest (OVR) method which consists of de-veloping, for each class m , a binary classifier ( w m )thatsep-arates class m from the rest of the classes, and then using (2) for inference, is one of the popular schemes to solve a multi-class problem. For nonlinear SVMs, Rifkin and Klatau [21] argue that OVR is as good as any other approach if the binary classifiers are well-tuned regularized classifiers like SVMs. The main points in defense of OVR (and against direct multi-class methods) are: (1) complicated implemen-tation of direct multi-class methods, (2) their slow training, and (3) OVR yields accuracy similar to that given by di-rect multi-class methods. These points should be viewed differently when considering linear SVMs. Firstly, a method such as SDM is easy to implement. Secondly, with respect to training speed, the following simplistic analysis gives in-sight. It is reasonable to take the cost of solution of a dual problem (whether it is a binary or a direct multi-class one) to be a function T ( l d )where l d is the number of dual vari-ables in that problem.  X  In OVR we solve k binary problems each of which has l dual variables and so its cost is kT ( l ). For a direct multi-class solution, e.g. Crammer-Singer, the number of dual variables is kl and therefore its cost is T ( kl ). For nonlinear SVMs T (  X  ) is a nonlinear function (some em-pirical studies have shown it to be close to a quadratic) and so OVR solution is much faster than direct multi-class solu-tions. On the other hand, for linear SVMs, T (  X  ) tends to be a linear function and so the speeds of OVR and direct multi-class solutions are close. We observe this in an experiment given below. Thirdly, performance differences between OVR and direct multi-class methods is dependent on the dataset. The experiments below offer more insight on this aspect.
First we do an implementation of OVR in which each SVM binary classifier employing the standard hinge loss is solved using the binary version of SDM, as developed in [9]. By the results in [9] this is probably the fastest solver for OVR for the types of datasets considered in this paper. We be-gin with a simple experiment where we fix C = 1 and solve the multi-class problems using OVR and SDM for Crammer-Singer and Weston-Watkins formulations. For MNIST and COVER datasets Figure 5 gives the behavior of test set per-formance as the solutions progress in time. For the OVR method test set performance was evaluated after each loop of the training set in each binary classifier solution, whereas, for the direct multi-class SDM the evaluation was done on a finer scale, four times within one loop. Clearly, all three methods take nearly the same amount of computing time to reach their respective steady state values. This is in agreement with our earlier mention that, for linear SVMs OVR and direct multi-class solutions have similar complex-ity. Also, in these datasets the final test set accuracies of Crammer-Singer and Weston-Watkins formulations are bet-ter than the accuracy of OVR.

Since accuracy depends on the choice of C ,wedoamore detailed study on the accuracy aspects further. For each of
This can be seen from the complexity analysis of section 2 coupled with the assumption that a fixed small number of loops gives good enough convergence. Figure 5: Comparison of One-Versus-Rest (Green, solid), Crammer-Singer (Red, dashdot) and Weston-Watkins (Blue, dashed) on MNIST (top) and COVER (bottom) datasets. The horizontal axis is Training time in seconds. the 5 datasets, we combined the training and test datasets and make 10 random 60%/20%/20% train/validation/test class-stratified partitions. For each of the three multi-class methods the C parameter is tuned using the train and val-idation sets by considering the following set of C values: { the largest validation set accuracy, a re-training is done on the train+validation set and the resulting classifier is eval-uated on the test set. The mean and standard deviation values of test set accuracies are reported in Table 4.2. We conduct Wilcoxon sign-rank test with a significance level of 0.01 to compare OVR against the direct multi-class meth-ods on each dataset; see Table 4.3. Comparison of OVR and Crammer-Singer indicates that the observed differences are statistically significant in favor of Crammer-Singer for SECTOR, MNIST, RCV1 and COVER datasets. Compar-ison of OVR and Weston-Watkins indicates that the re-sults are statistically significant for all the datasets. How-ever, on NEWS20 and SECTOR the results are in favor of OVR while on the other three datasets they are in favor of Weston-Watkins. Though statistical significance analy-sis may not mean practically significant improvements, we do note that the improvements shown by the direct multi-class methods over OVR on MNIST and COVER datasets are quite good. Overall, for linear SVMs it may be bet-ter to employ the direct multi-class methods (in particular, Crammer-Singer) whenever fast solvers for them (such as ones based on SDM) are available. It is worth noting that, of the five datasets under consideration only MNIST and COVER are un-normalized, i.e. x i is not the same for all i . It is possible that OVR X  X  performance is more affected by Table 4.2: Test set accuracy (Mean and Standard deviation) comparison of One-Versus-Rest (OVR), and Multi-class SDMs for Crammer-Singer (CS) and Weston-Watkins (WW) formulations with C tuned for each method.
 Table 4.3: p-values from Wilcoxon sign-rank test. the lack of data normalization and therefore direct multi-class solutions may be more preferred in such situations. A more detailed analysis is needed to check this.
In this paper we have presented sequential descent meth-ods for the Crammer-Singer and the Weston-Watkins multi-class linear SVM formulations that are well suited for solving large scale problems. The basic idea of sequentially looking at one example at a time and optimizing the dual variables associated with it can be extended to more general SVM problems with structured outputs, such as taxonomy, multi-label and sequence labeling problems. We note that, for the structured outputs setting, Tsochantaridis et al. [24] mentioned the sequential dual method as a possibility (see Variant (b) in Algorithm 1 of that paper), but did not pur-sue it as a potentially fast method. In such extensions the quadratic programming sub-problem associated with each example does not have a simple form such as those in (10) and (21), and some care is needed to solve this sub-problem efficiently. We are currently conducting experiments on such extensions and will report results in a future paper. [1] A. Bordes, L. Bottou, P. Gallinari, and J. Weston. [2] B. E. Boser, I. Guyon, and V. Vapnik. A training [3] L. Bottou. Stochastic gradient descent examples, 2007. [4] M. Collins, A. Globerson, T. Koo, X. Carreras, and [5] K. Crammer and Y. Singer. On the learnability and [6] K. Crammer and Y. Singer. On the algorithmic [7] K. Crammer and Y. Singer. Ultraconservative online [8] R. Fletcher. Practical Methods of Optimization .John [9] C.-J. Hsieh, K.-W. Chang, C.-J. Lin, S. S. Keerthi, [10] C.-W. Hsu and C.-J. Lin. A comparison of methods [11] T. Joachims. Making large-scale SVM learning [12] T. Joachims. Training linear SVMs in linear time. In [13] T. Joachims, T. Finley, and C.-N. J. Yu. Cutting [14] K. Lang. Newsweeder: Learning to filter netnews. In [15] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. [16] D. D. Lewis, Y. Yang, T. G. Rose, and F. Li. RCV1: [17] C.-J. Lin. A formal analysis of stopping criteria of [18] Z.-Q. Luo and P. Tseng. On the convergence of [19] A. McCallum and K. Nigam. A comparison of event [20] J. D. M. Rennie and R. Rifkin. Improving multiclass [21] R. Rifkin and A. Klautau. In defense of one-vs-all [22] S. Shalev-Shwartz, Y. Singer, and N. Srebro. Pegasos: [23] C. H. Teo, A. Smola, S. V. Vishwanathan, and Q. V. [24] I. Tsochantaridis, T. Joachims, T. Hofmann, and [25] J. Weston and C. Watkins. Multi-class support vector NEWS20; Row 2: SECTOR; Row 3: MNIST; Row 4: RCV1; and Row 5: COVER.
