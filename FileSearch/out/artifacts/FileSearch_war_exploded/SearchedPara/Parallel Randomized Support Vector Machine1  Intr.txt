 Sampling theory has a long successful history in optimization [6, 1]. The appli-cation to the SVM training problem is first proposed by Balcazar et al. in 2001 [2]. However, Balcazar assumed that the SVM training problem is a separable problem or a problem that can be transformed to an equivalent separable prob-lem by assuming an arbitrary small regularization factor  X  ( D and 1 /k in [2] and [3]). They also stated that there were number of implementation difficulties so that no relevant results could be provided [3].

We model a LP-type problem such that the general linear nonseparable prob-lem can be covered by our randomized support vector machine (RSVM). In order to take advantage of distributed computing facilities, we proposed a novel par-allel randomized SVM (PRSVM) in which multiple working sets can be worked on simultaneously. The basic idea of the PRSVM is to randomly shuffle the training vectors among a network based on a carefully designed priority and weighting mechanism and to solve the multiple local problems simultaneously. Unlike the previous works on parallel SVM [7, 10] that lacks of a convergence bound, our algorithm, the PRSVM, on average, converges to the global opti- X  denotes the underlying combinatorial dimension, N denotes the total number of training vector, C denotes the number of working sites, and r denotes the size for a working set. Since the RSV M is a special case of PRSVM, our proof naturally works for the RSVM. Note that, when C = 1, our result reduces to Balcazar X  X  bound [3].

This paper is organized as follows. The support vector machine is introduced and formulated in the next section. Then, we present the parallel randomized support vector machine algorithm. The th eoretical global convergence is given in the fourth section followed by a presentation of a successful application. We conclude our result in Section 6.
 We prepare fundamentals and basic notations on SVM and randomized sampling technique in this section. 2.1 Support Vector Machine hyperplane is defined by w T x i  X  b = 0 with parameter w  X  R m and b  X  R such the distance of the two parallel hyperplanes w T x  X  b =1and w T x  X  b =  X  1, The support vector machine (SVM) is in fact a quadratic programming prob-lem, which maximizes the margin over the parameters of the linear classifier. For general nonseparable problems, a set of slack variables  X  i ,i =1 ,...,N are introduced. The SVM problem is defined as follows: The dual of problem (1) is shown as follows:
A nonlinear kernel function can be used for nonlinear separation of the train-ing data. In that case, the gram matrix ZZ T is replaced by a kernel matrix k ( x,  X  x )  X  R N  X  N . Our PRSVM that is described in the following section can be kernelized and therefore is able to keep the full advantages of the SVM. 2.2 The Sampling Lemma, LP-Type Problem and KKT Condition An abstract problem is denoted by ( S ,  X  ). Let X be the set of training vector. That is, each element of X is a row vector of the matrix X . Throughout this paper, we use CALLIGRAPHIC style letters to denote sets of the row vectors of a matrix denoted by the same letter with italian style. Here,  X  is a mapping corresponding to X R and S is of size N . Define
The elements of V ( R ) are called violators of R and the elements of E ( R )are called extremes in R .Bydefinition,wehave For a random sample R of size r , we consider the expected values Gartner proved the following sampling lemma [9]: Lemma 1. (Sampling Lemma). For 0  X  r&lt;N , Proof. By definitions, we have where [ . ] is the indicator variable for the event in brackets and the last row followsthefactthattheset Q has r + 1 elements. The Lemma immediately follows.
 The problem ( S ,  X  )issaidtobeaLP-typeproblemif  X  is monotone and local (see Definition 3.1 in [9]). Balcazar proved that the problem (1) is a LP-type problem [2]. So is the problem (2). We use the same definitions given by [9] to define the basis and combinatorial dimension as follows. For any R X  X  ,a basis of
R is a inclusion-minimal subset B X  X  with  X  ( B )=  X  ( R ). The combinatorial problem ( S ,  X  ) with combinatorial dimension  X  , the sampling lemma yields This follows that |E ( R ) | X   X  .

Then, we are able to relate the definitions of the extremes, violators and the basis to our general SVM training problem (1) or (2). For any local solution  X  p or of the local solutions will be the vectors that violate the Karush-Kuhn-Tucker (KKT) necessary and sufficient optimality conditions. The KKT conditions for the problem (1) and (2) are listed as follows: Since the  X  i and  X  i for the training vector x i is always 0 for x i  X  X \X p ,the only condition needed to be tested is or  X  lems,wedonotknowtheboundfor  X  before we actually solve the problem. What we can do is to set a sufficiently large number to bound  X  from above. We consider the following problem: the training data are distributed in C +1 sites, where there are C working sets and 1 nonworking set. Each working site site contains r training vectors, where r  X  6  X  2 and  X  denotes the combinatorial dimension of the SVM problem.

Define a function u ( . )torecordthenumberofcopiesofelementsofatraining set. For training set X , we define a set W such that W contains the virtually the virtual set W p corresponding to training set X p at site p .
 Our parallel randomized support vector machine (PRSVM) works as follows. Initialization Training vectors X are randomly distributed to C + 1 sites. Assign priorities to all sites such that each site gets a unique priority number. Iteration Each iteration consists of the following steps.

Repeat for t =1 , 2 , ... 1. Randomly distribute the training vectors over the working sites according 2. Each site with priority p , p  X  C solves the local partial problem and record until  X  q = p V q,p =  X  for some p .
 Return the solution  X  p .
 The priority setting of working sets actually defines the order of sampling. The highest priority server gets the first sampled batch of data, lower one gets the second batch and so on. This kind of sequential behavior is designed to help toaworkingset W p , only one copy of x i is included in the optimization problem (
X p ,  X  ) that we are solving, while we record this number of copies as a weight of this training vector.
 The merging procedure has two properties: working set X p .Thatis, x i /  X  X  ( X p ), if x i  X  X  p .
 Property 2. If multiple copies of a vector x i are sampled to a working set X p , none of those of vectors can be the extreme of the problem ( X p ,  X  ). That is, x i /  X  X  ( X p )if u ( { x i } ) &gt; 1atsite p .
 The above two properties follow immediately by definitions of violators and extremes.

One may note that the merging procedure actually constructs an abstract type problem and has the same combinatorial dimension,  X  , as the problem ( X p ,  X  u ( V p ).

Step 4 plays the key role in this algorithm. It says that if the number of since the violators already have enough weights to be sampled to a working site.
One may note when C = 1, the PRSVM is reduced to the RSVM. However, our RSVM is different from the randomized support vector machine training algorithm in [2] in several ways. First, our RSVM is capable of solving general nonseparable problems, while Balcazar X  X  method has to transfer nonseparable problems to an equivalent separable problems by assuming an arbitrarily small  X  . Second, our RSVM merges examples after sampling them. Duplicated examples are not allowed in the optimization steps. Third, we test the KKT conditions to identify a violator instead of identifying a misclassified point. In our RSVM, a correctly classified example may also be a violator if this example violates the KKT condition. We prove the average number of iterations executed in our algorithm, PRSVM, alization of the one given in [2]. The result of the tradition RSVM becomes a special case of our PRSVM.
 Theorem 1. For general SVM training problem the average number of itera-Proof. We consider an update to be successful if the if-condition in the step 4 holds in an iteration. One iteration has C updates, successful or not.
We first show the bound of the number of successful updates. Let V p denote the set of violators from site with priority q  X  p for the solution  X  p .Bythis definition, we have Since the if-condition holds, we have By noting that the total number of training vectors including duplicated ones in each working sites is always r for any iterations, we have and Therefore, at each successful update, we have where k denotes the number of successful updates. Since u 0 ( X )= N ,after k successful updates, we have
Let X 0 be the set of support vectors of the original problem (1) or (2). At doubled. Since, |X 0 | X   X  ,thereissome x i in X 0 that gets doubled at least once
Therefore, we have By simple algebra, we have updates.

The rest is to prove that the probability of a successful update is higher than one half. By sampling lemma, the bound (3), we have By Markov equality, we have This implies that the expected number of updates is at most twice as large as the number of successful updates, i.e. , K  X  6  X  ln( N +6 r ( C  X  1)  X  ), where K denotes the total number of updates. Note that, at the end of each iteration, we have Therefore, the PRSVM algorithm guarantees, on average, within (6  X /C )ln( N + 6 r ( C  X  1)  X  ) steps, that all the support vectors are contained by one of the C working sites. For separable problems, we have  X   X  n + 1. For general nonsepa-rable problems, we have  X  is bounded by the number of support vectors. is very limited. We analysis our PRSVM by using synthesized data and a real-world geographic information system (GIS) database.

Through out this section, the machine we used has a Pentium IV 2.26G CPU and 512M RAM. The operation system is Windows XP. The SVM light [11] ver-sion 6.01 was used as the local SVM solver. Parallel computing is virtually sim-ulated in a single machine. Therefore, we ignore any communication overhead. 5.1 Synthesized Demonstration We demonstrate our RSVM (reduced PRSVM when C = 1) training procedure by using a synthesized two-dimensional training data set. This data set consists of 1000 data points: 500 positive and 500 negative. Each class is generated from an independent Gaussian distribution. Random noise is added. We set the sample size r to be 100 and the regularization factor  X  to be 0.2. The RSVM converges in 13 iteration. In order to demonstrate the weighting pro-higher weight the training sample has. Fig. 1 shows that how those  X  X mportant X  points stand out and get higher and higher probability to be sampled. 5.2 Application in a Geographic I nformation Sy stem Database We select covtype, a geographic information system database, from the UCI Repository of machine learning databases as our PRSVM applications [5]. The covtype database consists of 581,012 instances. There are 12 measures but 54 columns of data: 10 quantitative variables, 4 binary wilderness areas and 40 variables to [0,1] and keep binary variable unchanged. We select 287831 training vectors and use our PRSVM to classify class 4 against the rest. This is a very suitable database for testing PRSVM since the database has huge number of training data and the number of SVs is limited.

We set the size of working size r to be 60000, the regularization factor  X  to be 0.2. We try three cases with C =1, C =2and C = 4 and compare the learning time with the SVM light in Table 1. The results show that our implementation of RSVM and PRSVM achieves comparable result with the reported fastest algorithm SVM light , though they cannot beat SVM light in terms of computing speed for now. However, the lack of a the oretical convergence bound makes SVM light not always preferable.

We plot the number of violators and support vectors (extremes) in each itera-tions in Fig. 2 to compare the performance of different number of working sites. The results show the scalability of our method. The numerical results match the theoretical result very well.
This figure shows the effect of adding more servers. The system with more servers will find the support vectors much faster than that with less servers. The proposed PRSVM has the following advantages over previous works. It is able to solve general nonseparable SVM training problems. This is achieved by using KKT condition as the criterion of identifying violators and extremes. Second, our algorithm supports multiple working sets that may work parallel. Multiple working sets have more freedom than normal gradient based paral-lel algorithms since no synchronization and no special solver is required. Our PRSVM also has a provable and fast average convergence bound. Last, our nu-merical results show that multiple working sets have scalable computing advan-tage. The provable convergence bound and scalable results make our algorithm more preferable in some applications.
Further research is going to be conducted to accelerate the performance of the PRSVM. Intuitively, the weighting mechanism may be able to be improved so that the initial iterations play a more determinant role.

