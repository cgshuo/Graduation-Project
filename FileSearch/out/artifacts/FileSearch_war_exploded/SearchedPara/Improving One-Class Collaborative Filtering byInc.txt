 One-Class Collaborative Filtering (OCCF) is an emerging setup in collaborative filtering in which only positive exam-ples or implicit feedback can be observed. Compared with the traditional collaborative filtering setting where the data has ratings, OCCF is more realistic in many scenarios when no ratings are available. In this paper, we propose to im-prove OCCF accuracy by exploiting the rich user informa-tion that is often naturally available in community-based interactive information systems, including a user X  X  search query history, purchasing and browsing activities. We pro-pose two ways to incorporate such user information into the OCCF models: one is to linearly combine scores from dif-ferent sources and the other is to embed user information into collaborative filtering. Experimental results on a large-scale retail data set from a major e-commerce company show that the proposed methods are effective and can improve the performance of the One-Class Collaborative Filtering over baseline methods through leveraging rich user information. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval -Information Filtering Algorithms, Performance, Experimentation  X  Work done as a summer intern at eBay Research Labs Recommender Systems, One-Class Collaborative Filtering, Rich User Information
A Recommender System analyzes users X  past behavior and predicts user X  X  preference t o improve user X  X  satisfaction. Originally introduced by Goldberg et al. [8], Collaborative Filtering (CF) approaches are the most popular methods in recommender systems and have been extensively studied [1]. One of the most famous examples is the Netflix Prize prob-lem, in which the most successful methods reported are CF models. However, most CF methods are focused on data sets with explicit ratings. Such explicit ratings are hard to collect in many applications because of the intensive user involvement. Recently, One-Class Collaborative Filtering (OCCF) has emerged as a very in teresting problem setup where only binary data of the user X  X  interaction can be ob-served through implicit fee dback [16]. OCCF reflects a more realistic scenario. In fact, most of the real life recommen-dations with implicit user feedback can be considered as an OCCF problem. Famous examples include Amazon X  X  prod-uct recommendation, web page bookmarking and the KDD cup 2007  X  X ho rated What X  problem on movie recommen-dation.

In the OCCF problem setup, data is usually extremely sparse and unbalanced: only a small part of data is labeled as positive examples. So far, research on OCCF has focused on how to best model the missing examples [16, 15, 9, 20]. However, the information about users in these studies has been restricted to implicit judgments on items only. In many applications, we naturally have much more user information that can be leveraged. For example, in most interactive systems where users can search for items, we would natu-rally have a search log with users X  queries and clickthroughs on search result. In a modern online market-place, there are several types of data reflecting the user-item interac-tion which can be utilized, such as search logs, item click-throughs, user X  X  transaction history and so on. To the best of our knowledge, in the setting of One-Class Collaborative Filtering, little has been studied in exploring how to exploit the rich user information to overcome the sparsity problem of the data and improve the recommendation performance.
In this paper we propose two strategies of incorporating rich user information to imp rove the OCCF performance. The first strategy is to use different sources of user infor-mation as independent evidence to score items, and linearly combine the scores with regular collaborative filtering scores to make the final decision. The second is to tightly embed the user information into a collaborative filtering model. Specifically, we extend the User-based CF baseline model, by replacing the user-user similarity function with a content-based similarity function or replacing the sparse transaction matrix with a denser clickthrough data matrix. We extend the Matrix Factorization baseline models by replacing the global weighting scheme with the content-based dissimilar-ity function. Experiments on a large-scale dataset from a major online market-place show that the proposed methods can effectively improve the recommendation performance. And the analysis on extreme cases indicates that the com-plementary nature of the content-based features and collab-orative filtering could help to ease the cold-start problem in recommendation.

The paper is organized as follows: We first summarize the related work in Section 2. In section 3 and 4 we define our problem setup, and introduce types of user information we could exploit. In Section 5 we propose major strategies for incorporating rich user information into the Neighbor-based CF. In Section 6 and 7 we present experimental results on a large-scale data set, and discuss which user information performs better and when the best result can be achieved.
Recommendation methods can be classified as Content-based recommendation [3]; Collaborative filtering [8], and Hybrid approaches [6, 17, 18]. The Content-based approaches make the prediction based on the similarity between the item and the user X  X  content profile. However, it needs efforts to collect and extract knowledge from the content. Collabo-rative Filtering (CF) approaches have been successfully ap-plied to several real world problems, such as Amazon X  X  prod-uct recommendation [14], Netflix X  X  movie recommendation [13]. CF methods are popular because it does not require domain knowledge, and can discover interesting associations that the Content-based methods couldn X  X . However, CF suf-fers from the Cold Start problem, in which few ratings can be obtained when a new item enters to the system. The Hybrid approaches try to combine the Content-based and CF approaches to overcome their limitations. For example, Claypool et al. [6] compute CF and Content-based compo-nents separately and combine their ratings linearly for the online news recommendation. Popescul, Schein et al. [17, 18] propose to unify CF and Content-based evidence by proba-bilistic mixture of aspects. But this kind of mixture mod-els is prone to overfitting. Most of the methods mentioned above deal with explicit user ratings, whereas the focus of this paper is on implicit user feedback, which is easier to collect, and common in modern online information systems. In addition, although methods proposed in this paper have similarity with the Hybrid methods, most of the previous Hybrid approaches do not exploit the user information as rich as ours.

In the One-Class Collaborative Filtering setup, we only have positive examples, and usually the portion of un-labeled examples is large, which leads to an extremely sparse data matrix. Instead of simply ignoring all missing examples, recent researches focus on how to take advantage of the missing examples. Pan et al. [16, 15] propose to employ weighted matrix factorization approximation and negative example sampling to improve the result. The basic idea in that work is to treat all unknown examples as negative examples, and assign weights to quantify the relative contri-bution of these examples. Sindhwani et al. [20] jointly learn the non-negative matrix factorization model while searching for the best classification of the labels in missing data. Our work emphasizes on improving OCCF by incorporating rich user information. Despite its importance, none of the ex-isting literature has addressed this problem seriously, partly due to the difficulty of collecting large-scale user informa-tion.

On the other hand, rich user information, such as search logs and clickthrough data has been reported to be able to improve the performance of personalized search in the field of information retrieval [10, 2, 19, 22]. Joachims [10] pro-poses to improve the retrieval quality of search engines by learning from the clickthrough data. Shen et al. [19] propose a decision-theoretic framework for optimizing search perfor-mance via user feedback. Tan et al. [22] integrate user search history into the query language model to improve the perfor-mance of language modeling approach. Agichtein et al. [2] examine the effectiveness of user behavior using a popular search engine, and demonstrate that web search ranking can be improved by incorporating such behavior. Personalized search and recommendation are similar but different in that search focuses on satisfying users X  momentary information needs with regard to specific queries, while recommendation systems try to provide users with interesting contents based on their preferences.
In this section, we first define the problem of OCCF and then introduce two representative approaches to it, which we will extend later by incorporating user information.
The goal of One-Class Collaborative Filtering is to predict the preference of a user on available items given that there are only positive implicit feedback examples in the data set. In this paper, we are interested in predicting users X  future purchase preference based on the historical behavior of the users. Formally, let The value of an element in R is 1 in a positive purchase record between some u and i , otherwise its corresponding element value in R is either 0 or 1. We aim at predict-ing a sorted list of top-k items L =( L 1 ,L 2 , ...L k ) ,L which match the user X  X  actual purchased items. We mea-sure the prediction performance by Mean Average Precision andMeanPercentageRanking,whichwillbedescribedin detail in Section 6.2.
Neighbor-based collaborative filtering systems have been introduced one decade ago and still are the most popular models for recommender systems. Neighbor-based CF sys-tems can be classified in user-based and item-based systems depending on the way past preference judgments are used. A user-based system makes new predictions by first finding users with similar ratings to an active user and then takes a weighted combination of their ratings. More formally let u betheactiveuserand i an item which is not rated by u . Then the predicted rating of u to i , R ( u, i ) is obtained by where r a,i is the rating of user a for item i ,  X  r u and  X  r the mean ratings of users u and a and w u,a is the similarity weight between users u and a . On the other hand, in an item-based system predictions are made by finding similarly rated items and then calculating a weighted combination of their ratings: where now  X  r i is the mean rating of item i and w i,k is the similarity weight between item i and k . Finding neighbors by using item-based similarity has computational advantage since in a typical CF setting, item space is more stable and bounded than user space. However, in a long-tail and highly dynamic market-place, this difference becomes less obvious. For example, the company from which we collect the data has 200M active users and 20M new listings added every month. Moreover, meta-data usually is available for users. Therefore, we decided to choose the User-based CF as our baseline model, which we call the UCF model.
Matrix Factorization Models have been successfully ap-plied to the Netflix movie recommendation [13]. It is su-perior to the neighbor-based models in reducing the Root Mean Squared Error (RMSE) where the explicit ratings of items are available. The basic idea of Matrix Factorization for explicit ratings is that the rating matrix R can be ap-proximated by decomposing R into two low-rank matrices X and Y .Foreach R ij :
Where X = { X 1 ,X 2 , ..., X m } T is a m by k matrix in which the i -th row X i is a k -dimension vector representing a user X  X  preference for latent factors. And Y = { Y 1 ,Y 2 , ..., Y a n by k matrix where the j -th row Y j is a k -dimension vector representing an item X  X  affiliation with latent factors. To avoid overfitting, we usually find X and Y with smaller values by Tikhonov-regularization [23], that is equivalent to solving the following unconstrained optimization problem: where || X || F is the Frobenius Norm of a matrix (Euclidean Norm) and  X  is a coefficient controlling how much regular-ization is needed. The user-item matrix R is usually very sparse with a lot of missing value. In this case, there is no exact solution for X and Y . Alternating Least Squares (ALS)[7] is an effective iterative algorithm to solve the opti-mization problem. The ALS algorithm works as follows: it first assigns random values to matrix Y , and updates X by minimizing the Loss function defined by Eq. (4): then fix X ,update Y according to the algorithm iterates these steps until X and Y converge to a local optimum.

In the One-Class Collaborative Filtering setting, there are several strategies to adopt the Matrix Factorization frame-work. One way is to simply treat all missing values in R as negative examples (AMAN). This method transforms the OCCF to the regular Matrix Factorization setting where the response variables only take 0 or 1 in value. In this case, the above algorithm can be directly applied to the OCCF prob-lem. However, this strategy doesn X  X  fully utilize the missing examples that would be positive ones. A better method pro-posed in [16] is to treat all missing values as negative, but with weights controlling their relative contribution to the loss function (wAMAN): L w ( X, Y )=
By minimizing L x ( X, Y ), X and Y can be solved via weighted low-rank approximation with ALS [21]. In this paper we include the AMAN method and wAMAN method as our two baselines in the Matrix Factorization methods. In wAMAN, we adopt the global weighting method for assign-ing the weights. We will elaborate two strategies of incorpo-rating rich user informat ion into the AMAN and wAMAN methods in Section 5.
The online market-place records a large amount of user information in the interaction of its users. Such informa-tion, usually as a kind of implicit feedback, can be exploited to infer the user X  X  purchase preference. This information includes search query logs, item clickthroughs, and trans-action history. In this paper we sought to explore different ways to utilize these kinds of user information to improve the One-Class Collaborative Filtering problem.
Search keywords strongly imply a user X  X  purchase pref-erence. However, the search queries are usually short and compact, and vary by time period as the user X  X  purchase in-terest shifts by time. To construct the search profile for user u ,let S u be the set of keywords issued by u over a fixed pe-riod of time. It is very natural to represent the user X  X  search profile as a vector space model as follows: Where w ui are keywords, c ui are TFIDF weightings, and c 1  X  c u 2  X  c u 3 ... . By truncating S s ( u )afterthefirst n words, we create a signature of the n most popular words in the vocabulary of u . A limitation of this signature is that common words (e.g. NEW and NWT in the cellphone cate-gory) tend to appear in many profiles without contributing to the descriptive power of any of them. We use a standard weighting strategy to promote words that are descriptive of a particular user. For a given word w ui we let where U is the set of all users and | w ui  X  V j | is the number of users whose vocabularies V j contain w ui .Wethenlet where tf ( w ui )= | w ui  X  S u | / | S u | is the term frequency of word w ui in the concatenated search queries of S u .
Besides the search query history, we can also assign pro-files to a user according to his purchasing and item browsing history. The purchase history is recorded in the transaction logs and browing history is captured in the web behavioral logs. We argue that the items purchased by a user provide evidence for the user X  X  preference for these items. In the context of our purchasing and clickthrough data, the most reliable and content-rich descriptors of an item lie in its title written by the seller. The search engine indexes items solely using these attributes, and since the majority of the pur-chased items are found using content search, the title of the item provides a significant source of data about the users X  preference. Similar to the search keywords, we use word vec-tor to represent a user, with an important difference that we just use the word count in computing  X  c ui instead of using term frequency:
S p ( u ) ,S t ( u )=(( w u 1 ,  X  c u 1 ) , ( w u 2 ,  X  c where S p ( u ) is calculated by the user X  X  purchase history while S t ( u ) by the browsing history, and
The reasons for this treatment are as follows: 1. We want the signature to reflect the number of times 2. Since the titles are written by sellers rather than buy-
Our main idea of leveraging rich user information is to use the profiles of users as extra evidence for computing similar-ity of users as well as similarity between users and items. We can then combine these similarity values with a collab-orative filtering algorithm to improve prediction accuracy. Here we define two similarity metrics: similarity between two users, and similarity between a user and an item.
Our goal is to produce a ranked list of items with high to low user preference. There are many strategies for combin-ing all sources of information and output a ranking. Here we propose two methods.
The first strategy we employ is to treat all kinds of user information as independent evidence, and linearly combine their scores to produce a final ranking score. Given a user u and an item i ,let F ucf ui be the ranking score obtained by the User-based Collaborative Filtering (UCF) method de-different information source (e.g. X ui = sim ( u, i ), the con-tent similarity between S u and i ,where S u  X  X  S s u ,S p Then the final score for item i is:
Y the wights w 0 ui ,w 1 ui , ... is trained using Gradient Descent un-der least square error to the ground truth in training dataset. We name this type of methods as UCF+Features
The same strategy can be applied to the Matrix Factoriza-tion baselines (AMAN and wAMAN). Once we obtain the X and Y matrix (please refer to Section 3.2), we can assign ascoreofuser u to item i according to Eq. (3): and then use Eq. (15) to output a final score. We refer this type of methods to MF+Features
Besides treating the user information as independent evi-dence, we can also directly embed this information into the User-based Collaborative Filtering or the Matrix Factoriza-tion framework. For the UCF framework, remember in Eq. (1), the user-user similarity ca n be an arbitrary function. So we can replace it with the similarity between users X  profiles defined in Eq. (13). In addition, the user-item matrix itself can be replaced by a denser matrix in the clickthrough data. We call this kind of embedding methods UCFWithFeatures. For the MF framework, AMAN and wAMAN are two base-line methods representing the state of the art of the MF approach. Similar to wAMAN, we also treat all missing ex-amples as negative examples. But instead of using a global weighting scheme, a better way for assigning the weight for each negative example is to look at the similarity between the user and the item: the more similar they are, the less weight we should assign to that negative example. And this similarity is measured by the content features. Hence we use for assigning the weights to the negative examples, and re-maining the weights for positive examples to be 1. Where sim ( i, j ) is defined by Eq. (13). With this important sub-stitution, we then aim to find a solution by minimizing the following loss function: L ( X, Y )=
We refer to these methods as MFWithFeatures. The low-rank matrices X , Y canbesolvedbyweightedALSasfol-lows: In order to solve X ,wefirstfix Y , and take derivatives of L d ( X, Y ) with respect to each entry of X , Then for the i -th row in X ,wehave 1 2 where  X  D i  X  R n  X  n is a diagonal matrix with entries of i -th row in D on the diagonal, I is a k  X  k identity matrix. Let X i = R i  X  D i Y ( Y T  X  D i Y +  X  (
Similarly, by fixing X and taking derivative of L d ( X, Y ) with respect to each entry of Y ,wehave
Y j = R T j D j X ( X T D j X +  X  ( where D j  X  R m  X  m is a diagonal matrix with entries of j -th column in D on the diagonal. The algorithm for estimating the low-rank matrices X , Y is described in Algorithm 1. For the runtime performance, since in each iteration it takes time O ( k 2 mn )toupdatethe X (or Y ), the computational complexity of running Algorithm 1is O ( N itr k 2 mn ). Where m , n is the number of users and items, repectively, k is the rank of matrix X and Y ,and N itr is the number of iterations.
 Algorithm 1 Weighted ALS for AMAN Require: user-item matrix R , weight matrix D , Ensure: low rank matrices X and Y . 1: Initialize Y jr : 2: Y jr := RandNum,  X  1  X  j  X  n, 1  X  r  X  k 3: Initialize D ij ,  X  1  X  i  X  m, 1  X  j  X  n : 4: if R ij =0 then 5: D ij := 1  X  sim ( i, j ) 6: else 7: D ij =1 8: end if 9: repeat 10: X i := R i  X  D i Y ( Y T  X  D i Y +  X  ( 11: Y j := R T j D j X ( X T D j X +  X  ( 12: until X , Y converge 13: return X , Y
Here we fully describe the baseline models and the ad-vanced models incorporating rich user information. We first introduce 6 baseline models, which generate recommenda-tions from different modeling aspects. 4 of these models are considered as weak baselines (PopRank, SchKW, CT, AMAN), and the remaining 2 are considered as strong base-lines (UCF and wAMAN).
The following models with rich user information can be classified by two categories: one is called UCF+Features, in which the models are associated with the User-based Col-laborative Filtering. The other one is called MF+ Features, in which the models are related to Matrix Factorization. We first list the UCF+Features models. Methods Baseline Combining User UCF+SchKW UCF Linear Query UCF+CT UCF Linear CT UCF+SchKW+CT UCF Linear Query, UCFWithSchKW UCF Embed Query UCFWithCT UCF Embed CT
UCFWithCT+SchKW UCF Embed Query, +CT and CT wAMAN+SchKW wAMAN Linear Query wAMAN+CT wAMAN Linear CT wAMAN+SchKW wAMAN Linear Query, +CT CT wAMANWithSchKW wAMAN Embed Query wAMANWithCT wAMAN Embed CT On the other hand, the MF+ Features category has 5 related models as listed below. Since they are defined in a similar way as the UCF+Features, we only briefly list them and point out some important differences.
We summarize important aspects of all advanced models in Table 1.
We test our proposed method on a large-scale dataset from the market-place of a major e-commerce company. The data set includes transactions, user search logs and clickthrough records in the CELLPHONE &amp; ACCESSORIES catalog in a period of 9 weeks. We split the data into three parts, the first 7 weeks of data is used for training, the 8-th week X  X  data a validation set, and the 9-th week X  X  data is for test-ing. We first remove the users who have no transactions in either of the data sets, and remove the users who have no search queries in the training set. Finally there are 1.29 million users and 1.85 million items in the training and val-idation set, and 17,135 users and 274,830 items in the test set. Please note that the data sets are very sparse, with about 49.3% of the users only bought one item in the train-ing set. The items in the datasets are transformed to unique items by a generative clustering method, resulting in 20,409 unique items (we use the term item briefly in the rest of the paper). Figure 1 shows some statistics of the dataset. The reasons and process of doing the unique item mapping are as follows: for an online marketplace featuring long-tail items, the life span of the item is ephemeral, so is the user-item relationship. This makes the user-item matrix of transactional counts extremely sparse (order of magnitude sparser than Netflix data), thus precluding the application of some traditional CF approaches such as SVD-based low-rank projection. To address this issue, Chen and Canny [5] have proposed a generative clustering algorithm mapping ephemeral items to more persistent latent product concepts. Specifically, the clustering algorithm uses product-to-item likelihood as the distance metric, where an item title is as-sumed to be generated from a latent product, word-by-word or property-by-property, following appropriate parametric distributions.
We use two standard measures to evaluate the prediction accuracy: Mean Average Precision, and Mean Percentage Ranking. 1. Mean Average Precision (MAP) : Mean Average 2. Mean Percentage Ranking (MPR) : Because of
We first analyze the effect of treating user information as independent evidence to the User-based CF model. Table 2 indicates that Search Keywords as independent information (UCF+SchKW) improves the result significantly, the abso-lute MAP gain over the UCF baseline reaches 0.0243, and the MPR gain over UCF is 6.5%. Compared to the Search Keywords, the Clickthrough history (UCF+CT) only out-performs the UCF slightly, because the content in the item title might not be as informative as the Search Keywords, or people tend to not purchase duplicated items. In addition, combining three sources of information (UCF+SchKW+CT) gets the best result, suggesting the effectiveness of the in-clusion of rich user information.

Table 3 summarizes the results for embedding user infor-mation into the UCF model. Interestingly, by only replacing the user-item matrix with a denser Clickthrough data ma-trix, the MAP improves by 0.0343 and the MPR by 10.4% Table 2: Performance of UCF+Features methods Table 3: Performance of UCFWithFeatures meth-ods Methods MAP Gain in MPR Gain in PopRank 0.0120 -35.8 -UCF 0.0513 -28.5 -UCFWithSchKW 0.0501 -0.0012 31.5 -3.0
UCFWithCT 0.0856 0.0343 18.1 10.4 over the UCF model. It suggests that the extremely sparse dataset is the major obstacle for Collaborative Filtering. On the other hand, the way for handling other sources of infor-mation will significantly affect the result. For example, while the Search Keywords effectively improve the result when us-ing as independent information, it drops by 0.0012 in MAP and3.0%inMPRoverthebaselinewhenactingasanem-bedded component of the UCF model: it does not alleviate the sparsity problem of the data.

We can also see in Table 4 that adding all user information achieves the best result over UCF baseline. But the best MAP (0.1037) is still very low compared to other datasets. It indicates that recommending items in a large-scale online market-place is a very difficult problem.
Table 5 and Table 6 show the MAP and MPR results on the Matrix Factorization-based models. We try different number of factors k from 40, 60 to 100, and find that the performance increases as the k increases. k = 100 is chosen in reporting the results. Firstl y, the un-weighted version of MF model AMAN performs badly in this case, suggesting simply treating all missing examples as negative examples without weighting does not fit for highly sparse OCCF prob-lem. Secondly, previous studies from [16] [9] show that the weighted MF methods perform well in the OCCF problem; Table 4: Comparison of the well-performing models Table 5: Performance of MF+Features methods
Methods MAP Gain in MPR Gain in
AMAN 0.0085 -38.2 -wAMAN 0.0314 -33.4 -SchKW 0.0486 -31.6 -
CT 0.0302 -30.9 -wAMAN 0.0583 0.0269 27.3 6.1 +SchKW wAMAN+CT 0.0350 0.0036 29.0 4.4 wAMAN 0.0621 0.0307 23.3 10.1 +SchKW+CT Table 6: Performance of MFWithFeatures methods
Methods MAP Gain in MPR Gain in
AMAN 0.0085 -38.2 -wAMAN 0.0314 -33.4 -wAMAN-0.0607 0.0293 25.5 7.9
WithSchKW wAMANWithCT 0.0577 0.0263 28.1 5.3 however, results in this study show that the wAMAN model with global weighting scheme performs not as well as the UCF model in the same test set (0.0314 vs 0.0513 in MAP and 33.4% vs 28.5% in MPR). This result suggests that in the MF model, it is not guaranteed to capture the relevant information between users and items by simply assigning a uniform weight in the negative examples; it might vary from dataset to dataset. Whereas the UCF model could perform more robust across datasets since the relevant in-formation from neighborhoods is easier to explain. On the other hand, in Table 5, Search Keywords and Clickthrough again demonstrate the effectiveness as additive evidence to the baseline model. Still, Search Keyword is more effective than Clickthrough, delivering significantly higher gain over the baseline (0.0269 vs 0.0036 in MAP gain). In addition, adding up all information (wAMAN+SchKW+CT) still gets the best result.
The analysis of results above supports the idea that rich user information is effective in solving the OCCF problem. We want to further investigate in what scenarios the rich user information is most effective. As the results show, MF-based models are not as good as UCF-based models in this case, therefore we put our focus on the UCF-based models. The typical transactional user-items matrix in e-commerce is extremely sparse. There are 1.3 million users and 17 thou-sand unique items, but only 1.8 million items purchased. The histogram in Figure 1 shows the statistics in the training dataset: it is highly skewed, with 49.3% of people only buy-ing one item in 7 weeks. It is known that the performance of the Neighbor-based CF drops drastically when there is no neighborhood supports on items. In that scenario, the con-tent similarity between the item and the user search profile might promote the ranking score from 0 to a reasonable level.
To test this hypothesis, we stratify the users in the test set into three sub-groups according to the scores which the UCF model outputs. These scores indicate the support from neighbors. We observe that the range of that score is from 0 to 1, and we group them into [0, 0.3), [0.3, 0.5) and [0.5, 1] empirically, representing low, middle and high supports. Then, we re-run two well-performing models: UCF+SchKW and UCFWithCT+SchKw+CT, as well as the UCF model. Figure 2 summarizes the expe rimental results. We can see clearly in Figure 2 that while the two additive models gain over UCF in both groups of middle and high neighbor sup-port. They gain significantly in the group with low neighbor support. This result indicates the complementary nature of the neighbor-based CF model and the information in Search Keywords and Clickthroughs. As an extreme case, if the score from UCF model is 0, other sources of information of the user-item association would help overcoming the prob-lem.
If the rich user information is effective, an interesting ques-tion is how much user information is needed? This question is difficult to answer if all kinds of user information are in-volved. So we control all other variables, and only select the Search Keywords, which is an important type of user in-formation, to access how the performance changes with the amount of user information varies. In this experiment, we divide the users to different groups according to the number of Search Keywords they have in the training set. A step of 50 keywords is used, which results in 30 user groups. And we use the UCF+SchKW to compute the average MAP of each group. Results are plotted in Figure 3. This experi-ment shows very interesting results: the model performs well on people who have small amount of Search Keywords, and performance drops in the groups of people who have rela-tively more Search history; and interestingly, performance grows again in the groups who have a lot of search history. The last groups of people represent the frequent buyers; they tend to buy similar things repeatedly so that the accumu-Figure 3: Number of Keywords VS Performance lated Search history would help in recommending items to them.
Research work in [12] shows the importance of time dy-namics in Collaborative Filtering. The above results show the importance of the search keywords in overcoming the limitation of the User-based CF. We further want to know the time effects on the search keyword information. We select search keywords from 4 periods of time (7 weeks, 4 weeks, 2 weeks and one week), and randomly select 2000 users who have at least one search keyword in his search his-tory. Table 7 shows the performance on the UCR+SchKW model. Interestingly, long search history doesn X  X  produce better results. This result suggests that we can only keep the most recent search logs of the users and can still obtain reasonable performance in recommending items to users.
In this paper we propose two major strategies of incor-porating rich user informat ion to improve the OCCF per-formance: one is treating the information as independent evidence and combining the scores from all sources of user information to produce the final recommendation. The other is to tightly include the rich user information into the mod-els. After careful analysis of the results, we have found the following conclusions: (1) Rich user information, such as Search Keywords and Clickthrough data, is very effective to overcome the sparsity in One-Class Collaborative Filtering. (2) Rich user information helps the most when the neighbor-based methods have very low support from its neighbors. (3) The short term search query history tends to perform as well as, if not better than, the long term history; so we can leverage users X  recent search query history to make recom-mendations.
In the future, we plan to investigate more sophisticated probabilistic models for unifying all available resources. Ad-ditional user information is shown to be effective, and more sophisticated models have the potential to further improve the recommendation performance. We are also interested in designing efficient and scalable algorithms for combining rich user information, since modern large-scale information systems desire scalable recommender solutions. This paper is based upon work supported in part by the National Science Founda tion under gra nt CNS-0834709. We also thank Huizhong Duan from the University of Illinois at Urbana-Champaign and Maks Ovsjanikov from Stanford University for their help in the data analysis and discussion. [1] G. Adomavicius and A. Tuzhilin. Toward the next [2] E. Agichtein, E. Brill, and S. Dumais. Improving web [3] M. Balabanov  X   X c and Y. Shoham. Fab: Content-based, [4] W. Chen, J.-C. Chu, J. Luan, H. Bai, Y. Wang, and [5] Y. Chen and J. F. Canny. Probabilistic clustering of [6] M. Claypool, A. Gokhale, T. Miranda, P. Murnikov, [7] K. R. Gabriel and S. Zamir. Lower rank [8] D. Goldberg, D. Nichols, B. M. Oki, and D. Terry. [9] Y. Hu, Y. Koren, and C. Volinsky. Collaborative [10] T. Joachims. Optimizing search engines using [11] Y. Koren. Factorization meets the neighborhood: a [12] Y. Koren. Collaborative filtering with temporal [13] Y. Koren, R. Bell, and C. Volinsky. Matrix [14] G. Linden, B. Smith, and J. York. Amazon.com [15] R. Pan and M. Scholz. Mind the gaps: weighting the [16] R. Pan, Y. Zhou, B. Cao, N. N. Liu, R. M. Lukose, [17] A. Popescul, L. Ungar, D. Pennock, and S. Lawrence. [18] A. I. Schein, A. Popescul, L. H. Ungar, and D. M. [19] X. Shen, B. Tan, and C. Zhai. Implicit user modeling [20] V. Sindhwani, S. S. Bucak, J. Hu, and A. Mojsilovic. [21] N. Srebro and T. Jaakkola. Weighted low-rank [22] B. Tan, X. Shen, and C. Zhai. Mining long-term search [23] A. N. Tikhonov and V. Y. Arsenin. Solution of
