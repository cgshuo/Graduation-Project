 Most applications for outlier mining are high dimensional domains in which the data may contain hundreds of dimensions. In this paper, we propose a unified framework for outlier detection in high dimensional spaces from an ensemble-learning viewpoint. 
In our new framework, the outlying-ness of each data object is measured by fusing demonstrate the usefulness of the ensemble-learning based outlier detection framework, we developed a very simple and fast algorithm, namely SOE1 ( S ubspace O utlier E nsemble using 1 -dimensional Subspaces) in which only subspaces with one dimension is used for mining outliers from large categorical datasets. The SOE1 data mining applications. Experimental results on real datasets show that SOE1 has comparable performance with respect to those state-of-art outlier detection algorithms on identifying true outliers. S  X  A , is called a subspace. The cardinality of S (| S |) is called the dimensionality of S . The power set of A , denoted by Pow ( A ), is defined as Pow ( A ) = Hence, each subspace is an element of Pow ( A ). The projection of an object P into a subspace S is denoted by )) ( ( P OF S  X  . 
The problem of outlier detection in high dimensional space and the unified ensemble learning based algorithmic framework are described in Fig.1.The input for outlier detection in high dimensional space includes the target database, the number of desired outliers, the set of subspaces considered in the mining process and the combination function. Among all these input parameters, the set of subspaces and combination function are of primary importance. Compute the outlier factor of P in S , i.e., )) ( ( P OF S  X 
The unified algorithmic framework (subspace outlier ensemble (SOE) framework) consists of two steps: subspace outlier mining and subspace outlier ensemble.

In the subspace outlier-mining step, the SOE framework uses existing outlier mining algorithms to compute the outlier factors of data objects in all the input subspaces.

In subspace outlier ensemble step, we borrow some ideas from ensemble learning by fusing outlier factors in different subspaces using a combination function. Hence, the choice of combination function (or combining operator) is at the core of the outlier ensemble stage. 
Suppose the outlier factors of an object P in D in different subspaces are denoted as v denoted as  X  . By fusing all the subspace outlier factors, the final outlier factor of P 
Our potential choices for  X  are the followings (which are also used in [7] for class outlier mining). z The product operator  X  :  X  ( v 1 , v 2 , ... , v m ) = v 1 v 2 ... v m . z The addition operator +:  X  ( v 1 , v 2 , ... , v m ) = v 1 + v 2 +...+ v m . z A generalization of addition operator-it is called the S q combining rule, where q P  X  D with P . A ( v , entry. The histogram of the dataset D is defined as: H = { h 1 , h 2 , ..., h d }. 
The proposed SOE1 algorithm needs only tw o scans over the dataset. The first scan of SOE1 is the subspace outlier-mining step, in which we construct the histogram of determined by the occurrences of its co rresponding attribute value, i.e., higher frequency implies more normal the object is. Hence, the outlier factor of each object P  X  D in subspace A store the histogram of the dataset D , we need d hash tables as our basic data structures materialization of a histogram. Therefore, we will use histogram and hash table interchangeably in the remaining parts of the paper. 
The second scan of SOE1 is the subspace outlier-ensemble step, in which we outlier factors to get final outlying-ness. To report the top-k outliers, we maintain a k -length array for this purpose. We used three real life datasets from UCI [5] to demonstrate the effectiveness of our algorithm against FindFPOF algorithm [1], FindCBLOF algorithm [2] and KNN algorithm [3]. 
For all the experiments, the two parameters needed by FindCBLOF algorithm are were obtained using the 5-nearest-neighbour ; For FindFPOF algorithm [1], the parameter mini-support for mining frequent patterns is fixed to 10%, and the maximal number of items in an itemset is set to 5. Since the SOE1 algorithm is parameter-free, we don X  X  need to set any parameters. Furthermore, we empirically study the impact of different combining operators on SOE1. That is, in the experiments, we report the results of SOE1 with different combining operators. For S q operator, we set q to 2, 5 and 7 separately. 
Since we know the true class of each object in the test datasets, we define objects in utilized as the assessment basis for comparing our algorithm with other algorithms. 
The first dataset used is the Lymphography dataset, which has 148 instances with largest number of instances. The remained classes are regarded as rare class labels for they are small in size. The corresponding class distribution is illustrated in Table 1. dataset. For example, we let SOE1 (+) algorithm find the top 16 outliers with the top ratio of 11%. By examining these 16 points, we found that 6 of them belonged to the rare classes. 
One important observation from Table 2 was that, among all the potential choices of  X  we are considered in SOE1, the + operator and  X  operator are the clear winners in this experiment. That is, SOE1 with the + operator and  X  operator outperform S q and  X  S in all cases. This observation suggests that the + operator and  X  operator will be better choices in practice for users. Consequent experiments also performance of SOE1 will deteriorate. 
Furthermore, in this experiment, the SOE1 algorithm with  X  operator performed top ratio at 10%, which is almost the twice for that of our algorithm. 
The second dataset used is the Wisconsin breast cancer data set, which has 699 instances with 9 attributes. Each record is labeled as benign (458 or 65.5%) or malignant (241 or 34.5%). We follow the experimental technique of Harkins, et al. [6] (http://research.cmis.csiro.au/rohanb/outlie rs/breast-cancer/). The corresponding class distribution is illustrated in Table 3. 
For this dataset, we also consider the RNN algorithm [6]. The results of RNN algorithm on this dataset are reproduced from [6]. Table 4 shows the results produced by the different algorithms. Clearly, SOE1 with + operator and  X  operator also outperform SOE1 with S q and  X  S in all cases on this dataset. Furthermore, among all of these algorithms, RNN performed the worst in most cases. Compared to other algorithms, SOE1 (with + operator and  X  operator) achieves roughly the same average performance with respect to the number of outliers identified. 
Arrhythmia data is the third dataset used in our experiments, which has 279 corresponding class distribution is illustrated in Table 5. discretization of the data. Each attribute is divided into 2 equal-width bins. produced by the different algorithms. 
From Table 6, we can see that the algorithm in [4] produced the best result with the cost of From an ensemble-learning viewpoint, a unified subspace outlier ensemble framework for outlier detection in high dimensional spaces is proposed in this paper. Empirical evidence verified the feasibility and advantage of our method. 
