 describing the contributions of this paper.
 of successive weak classifiers grows smaller and smaller as b oosting proceeds. precisely, if the accuracy of the t -th weak classifier is 1 has error at most Q T  X  1 and indeed (1) is a good approximation if no  X  model, see e.g. [AL88, Kea98, AD98, BKW03, KS05, RDM06] and m any other references.) final hypothesis constructed by martingale boosting has err or at most  X  t = 1 / 0 . 5 (which is trivial), and in fact (2) approaches 1 as t  X  X  X  . is both adaptive enough to satisfy a bound of exp  X   X  P T  X  1 hypotheses to be Boolean-valued).
 weak hypothesis h at that node assigns to example x .
 opposed to the 1 / X  4 nodes of [KS05, LS05]). write D  X  to denote D restricted to the negative examples { x  X  X : c ( x ) =  X  1 } . if D distribution D call h mapping from X to [  X  1 , 1] . Once the hypothesis h for a confidence-rated base classifier, have as many as eight) . Our algorithm. To fully specify our new boosting algorithm we must describe : it passes through the branching program. satisfies both E construct a weak hypothesis h from D We call  X  We do this for all nodes at level t . Now we define the advantage at level t to be Branching. Intuitively, we would like to use  X  the weak classifier h h bound the number of such nodes).
 We simulate the effect of having an edge from (  X , t ) to (  X  +  X  from (  X , t ) to ( i  X  at which x ends up is (  X  +  X  Since | h a randomized variant of each weak hypothesis h outgoing edges. two-sided weak learners: ues  X  learner on distribution D constructed by the booster satisfies The algorithm makes at most M  X  O (1) P T  X  1 structs a branching program with at most M nodes).
 Proof: We will show that Pr argument shows a similar bound for negative examples, which gives (4). For t = 1 , . . . , T we define the random variable A original distribution D restricted to positive examples), the value of A A step that brings it to level t.
 We define B such that i (  X  We have that E [ B B ) equals (( i +  X  ) (  X  the value of X Fix 1  X  t  X  T and let us consider the conditional random variable ( X X x is distributed according to ( D where the first inequality follows from the two-sided advant age of h For t = 0 , . . . , T , define the random variable Y conditioning on the value of Y get
E [ Y t | Y t  X  1 ] = E so the sequence of random variables Y has bounded differences, note that we have The value of B multiple of  X  Now recall Azuma X  X  inequality for sub-martingales: We apply this with each c positive examples, Pr program. Let us write M The t -th level of boosting can cause the rightmost (leftmost) nod e to be at most 2  X  that at level t , every node is at a position (  X , t ) with |  X  |  X  2 P t  X  1 integer multiples of  X  Remark. Consider the case in which each advantage  X   X  4.1 Improving efficiency by freezing extreme nodes program with fewer nodes.
 t have been created, each node (  X , t ) with |  X  | &gt; We have the following theorem about the performance of this a lgorithm: values  X  distribution D the booster satisfies The algorithm makes O Proof: As in the previous proof it suffices to bound Pr gives us that if we never did any freezing, then Pr let us analyze the effect of freezing in a given stage t &lt; T . Let A past which examples are frozen in round t ; i.e. A in round t , it must be the case X of A t gives us that Pr x  X  X  + [ x incorrectly frozen in round t ] is at most so consequently we have Pr [LS05]: we have that Pr fact that there are O ( A q (8 It is easy to check that if  X  5.1 Standard weak learners on average, we can borrow ideas from [LS05].
 of example.
 D + [ S ] + 1 2 D  X  [ S ] .
 follows: (a) if E 0 , then, for all x  X  X ,  X  h ( x ) = h ( x )  X  1 E x  X  X  [ h ( x ) c ( x )]  X   X  Proof. Assume without loss of generality that E symmetrically). By linearity of expectation Since D is balanced we have E lemma follows from the fact that E g , and return h =  X  g . Our next lemma analyzes this transformation. Lemma 7 If E Proof : Lemma 6 implies that E Since h balanced g with respect to b D and c , we have E the definition of b D , we get that E tive LHS in (8) completes the proof.
 two-sided weak learner is weakened to require only standard weak learning, but each  X  with  X  5.2 Tolerating random classification noise repaired exactly as in [KS05, LS05]; because of space constr aints we omit the details.
