 To take the first step beyond keyword-based search toward entity-based search, suitable token spans ( X  X pots X ) on docu-ments must be identified as references to real-world entities from an entity catalog. Several systems have been proposed to link spots on Web pages to entities in Wikipedia. They are largely based on local compatibility between the text around the spot and textual metadata associated with the entity. Two recent systems exploit inter-label dependencies, but in limited ways. We propose a general collective dis-ambiguation approach. Our premise is that coherent docu-ments refer to entities from one or a few related topics or do-mains. We give formulations for the trade-off between local spot-to-entity compatibility and measures of global coher-ence between entities. Optimizing the overall entity assign-ment is NP-hard. We investigate practical solutions based on local hill-climbing, rounding integer linear programs, and pre-clustering entities followed by local optimization within clusters. In experiments involving over a hundred manually-annotated Web pages and tens of thousands of spots, our approaches significantly outperform recently-proposed algo-rithms.
 Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Information Systems  X  Information Storage And Retrieval General Terms: Algorithms, Experimentation Keywords: Entity annotation/disambiguation, Wikipedia, collective inference
A critical step in bridging between unstructured Web text and semistructured search and mining applications is to iden-tify textual references (called  X  X pots X ) to named entities and annotate the spots with unambiguous entity IDs (called  X  X a-bels X ) from a catalog. These entity ID annotations enable powerful join operations that can combine information across pages and sites. Named entity recognition and tagging have seen widespread success [17]. Here we are concerned with the second step: entity disambiguation from a given catalog, such as Wikipedia. (The availability of a catalog makes this a supervised setting, unlike unsupervised coreference resolu-tion.)
The success of semantic annotation is greatly determined by widespread adoption of the entity catalog. For common English words, WordNet [14] provides an authoritative lex-ical network designed by linguists, and widely used for dis-ambiguation of common words [1]. CYC and OpenCYC [12] are partly commercial efforts to maintain entity catalogs, rules and reasoning engines. To understand and maintain TAP [8], WordNet , or OpenCYC, substantial training is needed in knowledge representation and linguistics.
In contrast, the  X  X eb 2.0 X  trend is to throw open tagging and cataloging of knowledge to the masses. Wikipedia is a stunning example of the success of this strategy: it has over 0.34 million categories and over 2.6 million cataloged entities, and keeps up with world events on an hourly or daily basis. The flip side is that Wikipedia lacks the rigorous  X  X nowledge base X  quality of TAP or OpenCYC. There is little by way of schema, quality of authorship is diverse, and the category hierarchy is haphazard. The challenge of Web mining systems is to harness the chaotic  X  X isdom X  of the crowds into relatively clean knowledge.
Most existing systems annotate only salient entity refer-ences. In some prototypes, only entities of specific recog-nized types (most often people and locations) are disam-biguated. The goal is to emulate Wikipedia X  X  restrained, informative, editorial links on ordinary Web pages. SemTag. The first Web-scale entity disambiguation system was SemTag [5]. SemTag annotated about 250 million Web pages with IDs from the Stanford TAP entity catalog [8]. The basic technique was to compare the surrounding con-text of a spot s with text metadata associated with candi-date entity  X  in TAP. SemTag preferred high precision over recall, proposing only about 450 million annotations, i.e., fewer than two annotations per page on average.
 Wikify!. Wikify! [13] has two components. The first, key-word extraction, decides if a phrase should be linked to Wiki-pedia. This is based on how often a word or phrase is found to be in the anchor text of some link internal to Wikipedia. The second step is disambiguation. Wikify!, too, is conser-vative in flagging keywords, so much so that even random disambiguation results in an F 1 score of 0.82. Suppose Wik-ify! is considering linking spot s to entity  X  . Wikipedia X  X  page describing  X  is explicitly referred from other Wikipedia pages. The context of these known citations is compared with the context of s to decide on a compatibility score. This may be regarded as generalizing SemTag, where known references to  X  form part of the metadata of  X  . Bunescu and Pasca [3] further improved the compatibility function using SVMs with tree kernels. However, none of these systems attempt collective disambiguation across spots.
 M&amp;W. A limited form of collective disambiguation pro-posed by Milne and Witten [15] yields considerable improve-ment beyond Wikify!. M&amp;W propose a relatedness score r (  X ,  X  ) between two entities. From the set of all spots S they identify the subset S ! of so-called context spots that can refer to exactly one entity each (let this entity set be  X  ). They define a notion of coherence of a context spot  X   X   X  ! based on its relatedness to other context spots. For an ambiguous spot s  X  S ! , the score of a candidate entity  X   X   X  ! is strongly influenced by its mention-independent on the page, their coherence, and a measure of overall qual-ity of context entities. M&amp;W also propose a link detector (a function similar to keyword extraction in Wikify!) that, like SemTag and Wikify!, sacrifices recall for high precision. For the spots picked by M&amp;W for labeling, even random disambiguation achieves an F 1 score of 0.53.
 Cucerzan X  X  algorithm. To our knowledge, Cucerzan [4] was the first to recognize general interdependence between entity labels in the context of Wikipedia annotations. He represents each entity  X  as a high-dimensional feature vec-tor g (  X  ), and expressed r (  X ,  X  ) as the inner product (or cosine, if g (  X  ) are normalized) g (  X  ) g (  X  ), also written as g (  X  )  X  g (  X  ). Let  X  0 be all possible entity disambiguations for all spots on a page. He precomputes the average vec-tor g ( X  0 )= s depends on two factors. The first, like SemTag or Wik-ify!, is a local context compatibility score. The second is g (  X  ) g ( X  0 \{  X  } ), reminiscent of leave-one-out cross valida-tion. Cucerzan also annotates very sparingly: only about 4.5% of all tokens are annotated.

A problem with this approach is that  X  0 is contaminated with all possible disambiguations of all spots, so this check for  X  X greement with the majority X  may be misleading. Note that both M&amp;W and Cucerzan avoid direct joint optimiza-tion of all spot labels, which is precisely what we undertake.
The above line of work has some similarity to identify-ing mentions of entities in databases (e.g. product catalogs) amidst unstructured text (e.g., blogs) [2], but, in such appli-cations, the  X  X ntity catalog X  is a clean relational database, and, to our knowledge, no collective labeling is employed.
Our goal in this paper is aggressive open-domain annota-tion of Web pages with entity IDs from an entity catalog such as Wikipedia. We contrast this with a more restricted disambiguation of entities which achieves high precision by sacrificing recall. The central purpose of our annotation is not direct human consumption, but downstream indexing, search and mining.

For example, we may gather from one page that m is a mathematician, and from another, that m plays the violin. Such data can be aggregated to explore whether scientists tend to play music significantly more or less often than other people mentioned on the Web.

Our guiding premise is that documents largely refer to topically coherent entities, and this  X  X oherence prior X  can be exploited for disambiguation. While Michael Jordan and Stuart Russell can refer to seven (basketball player, foot-baller, actor, machine learning researcher, etc.) and three (politician, AI researcher, DJ) persons respectively in Wiki-pedia (as of early 2009), a page where both Michael Jordan and Stuart Russell are mentioned is almost certainly about computer science, disambiguating them completely.
We propose a collective optimization problem that pre-cisely models the combination of evidence from local spot-to-entity compatibility ( X  X ode potential X ) and global page-level topical coherence ( X  X lique potential X ) of the entities chosen to disambiguate all spots. Our optimization is equivalent to searching for the maximum probability annotation configu-ration (inference) in a probabilistic graphical model where each page is a clique. Inference is NP-hard. We propose practical and effective heuristics based on local hill-climbing and linear program relaxations. Our framework also applies to word sense disambiguation [1], and therefore, may be of independent interest.

We describe our experiments with two data sets. Cucerzan X  X  ground-truth data [4] has annotations only for persons, places and organization and is limited to only 700 spots on non-Wikipedia data. While SemTag was run on 264 million pages and produced 434 million annotations, human judge-ment was collected on only about 1300 spot labelings (and this data is not publicly available). We built a browser-based annotation UI that six volunteers used to collect over 19,000 spot annotations on more than 100 pages; largest among known prior work 1 . Experiments show that we can significantly push the recall envelope without hurting pre-cision. Our trained node potential alone can improve F 1 accuracy considerably compared to all of Wikify!, Cucerzan and M&amp;W X  X  algorithms. Taking clique potentials into con-sideration using LP rounding or greedy hill-climbing gives further accuracy gains.
Because we process one page at a time, we will elide the page in our notation. A spot s is a (short) token (sequence) that is potentially a direct reference to an entity in Wiki-pedia. We do not consider indirect references like pronouns, but do aim to resolve imperfect matches such as Michael for Michael I. Jordan .The context of s is the text in a suit-able window around s .An entity is expressed as a URN in Wikipedia, and denoted  X  . na is a special label denoting  X  X o attachment X , i.e. an algorithm can avoid labeling a spot to increase precision at the cost of recall.  X  na  X  0 is a tuned parameter to guide this tradeoff. See Figure 1. y s is a variable denoting the entity label, taking a value from  X  0  X  X  na } . y is the vector of all spot labels. f s is a feature vector whose elements express various measures of compatibility between s and  X  .The context of a spot is a bag of words collected from a suitable window around the candidate entity reference.
The data is in the public domain, see http: // soumen.cse. iitb.ac.in / ~soumen / doc / CSAW / or http: // www.cse.iitb. ac.in / ~soumen / doc / CSAW /
Wikipedia is preprocessed so that each page corresponding to an entity  X  is represented by four fields . Each field is turned into a bag (multiset) of words. Three text match scores are computed between a field of  X  and s : So in all, we get 4  X  3 = 12 features.
 and rare; i.e., Pr(  X  | s ) is very low. E.g., Intel is (also) a fictional cartel in a 1961 BBC TV serial, but this sense is much rarer than the semiconductor giant. We can easily count from intra-Wikipedia links the fraction of times a link with Intel in the anchor text points to every sense of Intel , and use this as a prior estimate of Pr 0 (  X  | s ). The last el-This feature is somewhat different from local compatibility features, so we will often study its effect separately.
The local compatibility score between s and  X  is modeled as w f s (  X  ) where w  X  R d is a model vector. For a locally optimal choice, we would pick arg max  X   X   X  s w f s (  X  )asthe label for s . If we had to normalize this to a probability, we would use a logistic model Pr(  X  | s )=exp( w f s (  X  )) /Z where Z s = (hence the log in the sense probability feature above). We call exp( w f s (  X  )) the node potential of s , using graphical model [9] terminology.

We train w using a max-margin technique. Given ground truth assignment  X   X  s  X   X  s ,wewant w f s (  X   X  s )tobelarger than any other w f s (  X  ), with a margin; this gives us the usual SVM linear constraints (for spots with  X   X  s = na only): and we minimize over  X   X  0and w the objective w 2 2 + C
P training data had tens of thousands of spots, and the  X  s more than 10 elements, leading to  X  10 6 constraints. Rather than use w 2 2 and a QP solver, we used w 1 which let us use a more scalable LP solver (Mosek). More notation is summarized in Figure 2.
The last and most important piece in our model is an em-bedding of labels  X  in a suitable (usually high-dimensional) feature space: g : X  0  X  R c . This embedding is used to define relatedness between two entities.
The Wikipedia page for  X  lists a set of categories that  X  belongs to. E.g.,  X  = Michael_jordan is a Sportspeople of multiple sports while  X  = Michael_I._Jordan is a Machine learning researcher .Ifthereare c categories in Wikipedia, the categories that  X  belongs to can be represented by a c -long bit vector, which is designated as g (  X  ). Cucerzan defined the relatedness between two entities  X ,  X  as a standard cosine measure.

Wikipedia X  X  categorization is organic and uncontrolled:  X  = Michael_I._Jordan also belongs to categories called Living people and Year of birth missing , which are not topi-cal. We tried to mitigate this with various weighting schemes, but Cucerzan X  X  algorithm nevertheless performed worse than each of our algorithms, which used the relatedness definition described next.
Cocitation has been used to detect relatedness for a long time [11]. Milne and Witten [15] represented g (  X  ) as the set of Wikipedia pages that link to  X  , with size | g (  X  ) | be the total number of Wikipedia pages. M&amp;W defined a relatedness measure (larger value implies more related) as: The numerator is a slight variation on Jaccard similarity, and the denominator is inversely related to min {| g (  X  ) Unless otherwise specified this is the measure we use.
To robustly balance between local and global signals hav-ing diverse dynamic ranges, we apply a range compressor function R (  X  ) to all elements of vectors f s (  X  )and g ( ically, This limits the numeric output range without clipping it at To keep notation uncluttered, we will hide R and this pre-processing step. Note that R (  X  ) is not applied to  X  na reward for assigning label na toaspot. We now describe our main model and inference approaches. The key is to define, over and above node potentials, a col-lective score based on pairwise topical coherence of all  X  s used for labeling.
For the moment, disallow y s = na . Consider Figure 3. If  X ,  X  are used as labels for s, s , their agreement is defined as r (  X ,  X  ). For the whole page, the overall agreement is Figure 3: Labels  X   X   X  s , X   X   X  s have to be chosen for spots s, s to maximize a combination of spot-to-label compatibility scores NP s (  X  ) , NP s (  X  ) as well as topical similarity between  X  and  X  ,say, g (  X  ) g (  X  ) . aggregated as graphical models style [9], we can turn this into a clique potential and the overall probability of a label assignment y is writ-ten as Pr( y )=(1 /Z )CP( y ) P y CP( y ) probabilities add up to 1 over all possible y .

Evaluating Z is difficult because an exponential number of terms need to be added up. For predicting the most likely label vector, finding Z is not needed; we just need arg max The two sums have different number of terms, which also vary from page to page. To be able to use a single consis-tent w across all pages, we need to scale the two parts to a compatible magnitude. So our objective, barring na s, is
Almost a third of spots in our ground truth data are marked  X  na  X  by volunteers, meaning that no suitable en-tity was found in Wikipedia. This is a reality on the open-domain Web, and many systems [13, 3, 15] can back off from annotation (indeed, back off aggressively).

To implement a recall-precision balance, we use one tuned parameter  X  na  X  0, the reward for not assigning a spot any label. Let N 0  X  S 0 be the spots assigned na ,and A 0 = S \ N 0 the remaining spots. We thus get our final objective: The reader may demand that (CP1). This creates difficulty for at least one of our infer-ence approaches, because A 0 depends on y and the resulting optimization can no longer be written as an integer linear program. One possible rationalization is that na has zero topical coherence with any other label, including another instance of na : therefore, the edge potential sum can be rewritten over s = s  X  S 0 ,not s = s  X  A 0 ,sothatthe acceptable.

A reasonable way to tune  X  na would be to first compute the typical value of w f s ( y s ) across all pages and spots in the training set, then sweep  X  na between 0 . 1  X  to 10  X  typical value. We use this approach in our experiments.
Figure 3, (NP) and (CP1) get to the heart of the collective disambiguation problem, so it is of interest to understand the complexity of inference.
 Proposition 1. Inference problem max y (NP) + (CP1) is NP-hard, even when  X  na =  X  X  X  and therefore A 0 = S 0 .
The reduction is from the maximal clique problem [7]. We also note that other natural definitions of CP do not make the problem easier.
 Proposition 2. The inference problem remains NP-hard with the following alternative definitions of CP :
Hardness using (3) is shown using a reduction from exact cover by 3-sets [7]. Hardness using (4) is shown using a reduction from 3SAT. Proofs are omitted to save space. Guided by approaches to Quadratic Assignment Problems (QAPs) [16] we can turn our optimization into a 0/1 integer linear program, and then relax it to an LP. First disallow y = na . The ILP is designed with up to |  X  0 | + |  X  0 | 2 The node potential part is written as and the clique potential part is written as where we assume (2). So the goal is to Constraints (6) enforce what we need, because, if z s X  = z s X  = 1, the objective will push u  X  X  = 1. The formulation generalizes readily to the na case using one more variable z s na per spot, changing constraint (7) to and adding a term 1 | S 0 |
The relaxed LPs replace constraints (5) with 0  X  z s X   X  1 and 0  X  u  X  X   X  1. The optimal LP objective will be an upper bound on the optimal ILP objective. To understand how loose the upper bound can be in the worst case, consider the following  X  X utterfly graph X  example. (Disallow na using  X  and  X  s 2 = {  X  3 , X  4 } . Assume all node potentials are zero, and all r (  X ,  X  )=1. Theoptimalintegralsolutioncanhaveat most one u  X  X  = 1, leading to an objective value of 1 / 1. The fractional solution will find it best to assign all z u  X , X  =1 / 2, with an objective of 4 increased arbitrarily by increasing the bipartite clique size, i.e., |  X  s | .
In our experiments, we found about 70% of pages to give completely integral (hence, optimal) solutions. The obvious rounding strategy for fractional solutions is arg max  X   X   X  We found that this tended to label na as some  X  = na . In-sisting that z s X  &gt; 1 / 2 was more reticent and gave slightly better F 1 . 1: initialize some assignment y (0) 2: for k =1 , 2 ,... do 3: select a small spot set S  X  4: for each s  X  S  X  do 5: find new  X  that improves objective 6: change y ( k  X  1) s to y ( k ) s =  X  greedily 7: if objective could not be improved then 8: return latest solution y ( k )
Figure 4: Dominant cluster hill-climbing (Hill1)
Another approach is to avoid math programming and de-ploy a direct greedy hill-climbing approach. Hill climbing has the advantage that it can be easily stopped and inter-preted at any time, and may achieve acceptable accuracy faster than solving and rounding an LP. The generic tem-plate is shown in Figure 4. It remains to specify the initial-ization, and how to make label modifications.
Some initializations suggest themselves: In our experiments we did not find significant differences between accuracies obtained using the above initializations.
We tried perturbing sets S  X  of sizes 1 and 2. The ratio-nale for trying to perturb a pair of spots was that any single spot perturbation may appear unattractive while at a local optimum. However, | S  X  | = 2 was already too slow to im-prove upon LP speeds. So we concentrate on single moves. If the label of s is changed from  X  1 to  X  2 , node score (NP) changes by  X  j and edge score (CP1) changes by
We downloaded the August 2008 version of Wikipedia, and prepared a dictionary of entity IDs, their labels and mentions, as follows:
Earlier work has used Wikipedia text itself as ground truth annotations. This is not suited to our aggressive recall target, so we looked for other data. SemTag collected only about 1300 manually labeled spots for quality checking, and these are not publicly available. Data used by Bunescu and Pasca [3] was not publicly available. Cucerzan X  X  data [4] (which we abbreviate to  X  X Z X ) is available and we do use it, but annotations are sparse and limited to a few entity types. Several URN labels in CZ data no longer exist in Wikipedia. Moreover, there is no na annotation.

Therefore we undertook to build a ground truth collection (which we call  X  X ITB X ) using a browser-based annotation sys-tem. Documents for manual annotation were collected from the links within homepages of popular sites belonging to a http: // webgraph.dsi.unimi.it / handful of domains that included sports, entertainment, sci-ence and technology, and health (sources: http: // news.go ogle.com / and http: // www.espnstar.com / ). Figure 6 sum-marizes some important corpus statistics. The annotations are available in the public domain. Both IITB and CZ data have high average ambiguity. CZ X  X  is higher because the spots are limited to common person and place names. Ob-viously, random assignment would get very poor accuracy, unlike M&amp;W.
CZ data came pre-annotated, but for the IITB corpus, we built a browser-based annotation tool. As illustrated in Figure 5, candidate spots are highlighted to differentiate be-tween pending and already annotated spots. Clicking on a spot drops down a list of possible disambiguations. Hover-ing on a specific Wikipedia label shows an excerpt from the definition paragraph of the corresponding entity.
In the IITB data, we collected a total of about 19,000 annotations by 6 volunteers. Unlike in previous work, vol-unteers were told to be as exhaustive as possible and tag all possible segments, even if to mark them as na .Thenumber of distinct Wikipedia entities that were linked to was about 3,800. About 40% of the spots was labeled na , highlight-ing the importance of backoffs. However, this also says that 60% of the spots were attached by volunteers, which by far exceeds the token rate of attachment in earlier work. While its absolute scale is impressive, SemTag produced only 434 million annotations from 264 million Web pages, or fewer than two per page. From Figure 6, we see that the CZ data identifies only about 15 spots per page. We thus highlight that we are in a completely different recall regime .
The annotation module allows each document to be tagged by two volunteers. Figure 7 summarizes some statistics on inter-annotator agreement. Clearly, a considerable number of disagreements are over na vs.  X  X ot-na  X . Accuracy. A simple option would be to count the fraction of spots s (that have manually associated labels  X   X  s )which get assigned y s =  X   X  s ,over N  X  0 and A  X  0 alike. However, typ-ical applications will be asymmetric in how they react to these labels. E.g., an indexing engine that incorporates ob-ject IDs will simply ignore na labels. Therefore, we need to also focus on A  X  0 separately.
 Recall, precision, F 1 . Suppose, in ground truth, the set of spots marked na is N  X  0 ,and A  X  0 = S 0 \ N  X  0 is the set of spots marked some label other than na . We will be largely concerned about the precision, recall, and F 1 scores of spots in A  X  0 . The fate of such a spot can be one of the following: Precision and recall are (macro-) averaged across documents and overall F 1 computed from average precision and recall. Note that the presence of na makes these definitions differ-ent from what Cucerzan and M&amp;W measured as spot label-ing accuracy after spot detection.

All parameters were tuned using 2-fold cross validation.
As a first step, we are interested in evaluating the effect of training w , isolated from the influence of clique poten-tials. For this, we ran a very simple system that we will call Local . Local used the trained w to choose a label for each spot independent of others, without any collective information: 1:  X  0  X  arg max  X   X   X  s w f s (  X  ) 2: if w f s (  X  0 ) &gt; X  na then return  X  0 else return na
If f s (  X  ) does not include the sense probability prior, we call the above strategy Local , otherwise we call it Lo-cal +Prior.
M&amp;W use two important signals, relatedness and com-monness, in their disambiguator. In Figure 8 we present ablation studies showing the relative effectiveness of various features, together with the benefits of using all features with a learnt model w . Figure 8: Training a model w is better than using any single spot-label compatibility feature.
Rather surprisingly, Local already produced significantly better F 1 scores than the two state-of-the-art annotations systems by M&amp;W and Cucerzan.

The M&amp;W algorithm can be directly executed on any page text using a Web service API 3 . The API includes a knob to control the recall-precision balance. We implemented Cucerzan X  X  algorithm locally. Cucerzan X  X  algorithm does not have a recall-precision knob. In Local , we used  X  na as the knob. Figure 9: Even a non-collective Local approach that only uses trained node potential dominates both Cucerzan and M&amp;W X  X  algorithms wrt both recall and precision (IITB data).

Figure 9 shows recall-precision plots. Cucerzan X  X  algo-rithm is shown by a single point. M&amp;W X  X  precision is very high, consistent with their claims. However, the R/P knob cannot increase recall beyond 20%. Meanwhile, the  X  na knob http: // www.nzdl.org / pohutukawa / wikifier / index.jsp in Local can be used to push it to 70% recall while remain-ing comparable to M&amp;W precision. If we dial down our re-call to levels comparable with M&amp;W, our precision becomes visibly larger than M&amp;W. Cucerzan X  X  recall and precision are both dominated by Local , like M&amp;W. Local +Prior is substantially better than Local , and is a formidable F level to beat.

Cucerzan did not learn the node potential but hardwired it. We gave Cucerzan X  X  algorithm the benefit of our learned node potentials. The F 1 score improved to 51.8%, which was still short of Local and far short of Local +Prior.
In Figure 10 we consider the trajectory of several doc-uments (one line per document) as Hill1 optimizes their labels. Specifically, we plot the objective minus the  X  na tribution on the x-axis, and correspondingly, the F 1 score for spots that are marked some non-na label in ground truth on the y-axis. Although there are occasional expected setbacks and oscillations, increasing the objective is generally good for F 1 too. This lends credibility to our basic dominant-cluster model. Figure 10: As Hill1 improves our proposed objec-tive, it usually improves F 1 as well (IITB data). Figure 11: Hill1 can attain objectives comparable to relaxed LP1 (IITB data).
For over 70% of the documents, LP1 gives fully integral solutions, which are therefore optimal for our integer pro-grams. Even otherwise, LP1 gives an efficiently computable, yet reliable upper bound to the objective that Hill1 is trying to attain. Figure 11 shows that in practice, the integrality gap is small, that Hill1 gets reasonably close to the upper bound, and that rounding makes LP1 slightly worse than Hill1 . 15 25 35 45 55 65 F1, % Local Figure 12: Hill1 attains almost the same F 1 score as LP1; both are better than Local (IITB data).

More directly useful is Figure 12, which compares F 1 scores of Hill1 and LP1 (rounded). They are very close, but Hill1 is slightly better at high recall levels of our inter-est. Both Hill1 and LP1 are robust to  X  na , whereas Local suffers if  X  na is chosen poorly.

Hill1 and LP1 scale mildly quadratically wrt | S 0 | ,as shown in Figure 13. For most documents, Hill1 takes about 2 X 3 seconds and LP1 takes around 4 X 6 seconds, much of which is fixed overhead. 0 2 4 6 8 10 Time, s Figure 13: Scalability of Hill1 and LP1, IITB data.
Having established that Local alone can significantly im-prove upon prior work, we investigate whether collective in-ference gives additional accuracy gains compared to Local . In Figures 14 and 15 we plot precision against recall for the Local , Hill1 ,and LP1 , for our two data sets.

In case of the IITB data set, we see that collective infer-ence has distinct precision advantage (almost 9%), especially as we push recall aggressively beyond 70 X 75%. Summa-rized below are F 1 scores obtained by 2-fold cross-validation of  X  na : From Figure 6, we see that the CZ data is much smaller, sparse in ground truth annotations, but has more potential ambiguity. Here LP1 led to more fractional solutions and overall worse accuracy than Hill1 , which still beat M&amp;W X  X  F 1 = 63% with our score of 69%, although M&amp;W attained larger precision than us at lower recall.
Figure 15: Recall/precision on Cucerzan X  X  data.
Our clique potentials (expressions 1, 3, 4) implicitly en-courage a single cluster model, because the clique poten-tials are largest when all g (  X  ) are close to each other. Let  X  ment. Is it true that a clustering of  X   X  in g (  X  )-space will show one giant component cluster?
Figure 16 shows a dendrogram formed by agglomeratively clustering  X   X  , the ground truth entities on a page. The  X  X ingle cluster hypothesis X  is only somewhat true. There is a cluster corresponding to the broad topic of the page, but this typically covers fewer than a third of the spots. The rest belong to smaller clusters, or are singletons, in which case they bear no collective information.

Might our implicitly single-cluster model losing out on re-call by not modeling and covering multiple clusters? Here is how this could happen. Say Hill1 is considering changing y from na to  X  , where s is a member of a non-dominant clus-ter. Therefore, (1 / to overcome the  X  na barrier. The large could share the blame. The end result is that a small but tight cluster of such spots cannot  X  X ecede X  from the domi-nant cluster.

Accordingly, we retained the node potential (NP), but demanded that an inference algorithm produce not only a label vector y but also a partitioning C = X  1 ,...,  X  K of the labels used. We modified the clique potential to By using denominator rewards smaller coherent clusters as desired, but it is no longer possible to express a simple linear objective as in Figure 16: Hierarchical clustering of  X   X  using the g embedding shows more than one clusters. (CP1 ), because |  X  k | themselves depend on y and C .
We extended the LP1 framework to optimize (NP)+(CPK) approximately. In experiments, (CPK) did not perform sig-nificantly better than LP1, giving less than 0.5% F 1 boost. We conjecture that this is because of the extreme sparsity of r (  X ,  X  ), which had only 5% fill. Basically, if r (  X ,  X  ) were used as edges in a graph, the graph is easy to partition, and LP1 finds it easy to make correct decisions within each partition, even if the LP1 objective tries to account for cross-cluster edges. However, this may change with denser sources of relatedness information.
We proposed new models and algorithms for a highly mo-tivated problem: annotating unstructured (Web) text with entity IDs from an entity catalog (Wikipedia). Unlike prior work that is biased toward specific entity types like persons and places, with low recall and high precision, our intention is aggressive, high-recall open-domain annotation for index-ing and mining tasks downstream.

Our main contribution is a formulation that captures a tradeoff between local spot-to-label compatibility and a global, document-level topical coherence between entity labels. In-ference in this model is intractable in theory, but we show that LP relaxations often give optimal integral solutions or achieve close to the optimal objective. We also give a sim-ple local hill-climbing algorithm that is comparable in speed and quality to LP relaxation. Both these algorithms are significantly better than two recently-proposed annotation algorithms.

In continuing work, we are trying to cast the annotation problem as special cases of quadratic assignment that can be approximated well [6, 16] or show that even approximation is difficult [10, 16]. We are trying to combine the generally low-recall, high-precision nature of M&amp;W X  X  r (  X ,  X  ) based on inlinks with the converse properties of Cucerzan X  X  r (  X ,  X  ) based on categories. This involves extending the training process from NP to the whole objective. We are also in-vestigating why the multi-cluster extensions of our model obtained no significant accuracy gains. Finally, we are con-sidering collective decisions beyond page boundaries. [1] E. Agirre and G. Rigau. Word sense disambiguation [2] S. Agrawal, K. Chakrabarti, S. Chaudhuri, V. Ganti, [3] R. Bunescu and M. Pasca. Using encyclopedic [4] S. Cucerzan. Large-scale named entity disambiguation [5] S. Dill et al. SemTag and Seeker: Bootstrapping the [6] U. Feige, D. Peleg, and G. Kortsaz. The dense [7] M. Garey and D. Johnson. Computers and [8] R. V. Guha and R. McCool. TAP: A semantic web [9] M. I. Jordan, editor. Learning in Graphical Models . [10] S. Khot. Ruling out PTAS for graph min-bisection, [11] R. Larson. Bibliometrics of the world wide web: An [12] D. B. Lenat. Cyc: A large-scale investment in [13] R. Mihalcea and A. Csomai. Wikify!: linking [14] G. Miller, R. Beckwith, C. FellBaum, D. Gross, [15] D. Milne and I. H. Witten. Learning to link with [16] V. Nagarajan and M. Sviridenko. On the maximum [17] S. Sarawagi. Information extraction. FnT Databases ,
