 Attribute interactions may increase the complexity of a classification task by dis-persing the instances that belong to the same class across the attribute space. In such cases, the initial attributes, when taken individually, appear to be only remotely related to the class attribute. To uncover the predictive power of such data, the learning systems need to analyze the interacting attributes simultane-ously and then build a model that takes int o account the interactions observed. As explained by several researchers, this is a complex task that surpasses the ability of many existing machine learning systems.

In particular, Rendell &amp; Seshu [12] emphasizes the fact that current machine-learning techniques rely on the assumption of simple attribute interactions which make them sub-optimal in domains with important attribute interactions. Fo-cusing on the attribute evaluation process, Kononenko &amp; Hong [6] and Bloedorn &amp; Michalski [1] explain that all learning approaches that evaluate the usefulness of each attribute individually using quality measures such as the information gain, the gini-index, the distance measure, or the j-measure are likely to gener-ate inaccurate or too complex models wh enever there are important attribute interactions. There have been numerous works on trying to improve the ability of the naive-Bayes with respect to attribute dependencies (e.g., [7]). Specific issues such as the replication and the fragmentation problems with decision trees are also directly related to the lack of capa city of current techniques to deal with attribute interactions [13]. From an applied perspective, it has been argued that attribute interactions are becoming the norm in KDD applications and failing to address this problem adequately has important consequences on the perfor-mance obtained [2]. All of these observations call for novel practical techniques that can facilitate learning in domains with important attribute interactions.
This paper proposes such a technique. It is a constructive induction tech-nique that augments the initial representation with new features which make explicit the important information hidden in the interactions among the initial attributes. The new features are self-contained globally relevant features that are suitable for learning algorithms assuming independence. As it will be shown experimentally, the new features can al so increase the performance of more com-plex learning algorithms.

After presenting motivation and related work, the paper introduces the method to derive the new globally releva nt features. Sect. 5 offers a large-scale experiment illustrating the usefulness of the approach and the last section con-cludes the paper. In this research, the concept of relevance designates the usefulness of a given attribute to predict the values of the class attribute. We assume that relevance is computed through a univariate measure such as the gain ratio [11]. Moreover, we use the term globally relevant attribute to designate an attribute that is relevant over the full training set.

To illustrate the potential effects of attribute interactions on relevance and the usefulness of globally relevant features, let us consider a simple binary clas-sification task with three attributes X 1 ,X 2 , and X 3 that follow a multivariate normal distribution defined by the following class-conditioned mean vectors and variance-covariance matrix (same for both class values):
From the mean vectors ( u 0 and u 1 ), we conclude that X 3 is the only relevant attribute for this task while  X  indicates that X 1 and X 2 interact with X 3 . Fig. 1 (a) shows a simple dataset generated from the above distribution. As seen from the scatter plots of X 3 versus X 1 and X 3 versus X 2 , it is difficult to separate the positive from the negative instances. This difficulty is further illustrated by the class-conditional density curves for X 3 ; the great overlap between the two curves clearly indicates t hat any decisions based on X 3 will be highly error-proned. The null gain ratio and  X  2 values confirm that, from a univariate global perspective, X 3 appears powerless in predicting the class values.
To uncover the power of the data, we propose a constructive induction method capable of generating a new globally relevant feature Z that cancels the negative effects of X 1 and X 2 on X 3 . The new feature is shown in Fig. 1 (b). We observe that the transformation removed a great proportion of the initial dispersion since the instances of the same class are now grouped together. As illustrated by the class-conditional density curves for Z , the new feature is highly relevant and its power is observable across the full dataset independently of the other attributes.
The data transformation approach proposed in this paper can automatically generate globally relevant features from complex interactions between any con-tinuous attribute and an arbitrary large number of influencing attributes of pos-sibly different types (continuous, nominal, binary). No information about the underlying distribution of the data or the nature of the interactions is required. Related research has been conducted in constructive induction and statistics. A large proportion of the constructive induction techniques are designed to be integrated with existing learning approaches and are not producing a new rep-resentation (e.g.:FRINGE[10], AQ17-DCI[1], and OCI[9]). With these systems, the focus is on the improvement of the accuracy of existing methods by oppo-sition to be on the assessment and removal of the negative effects of attribute interactions. Hu [4] noticed the lack of general data pre-processing methods that are independent of specific learning algorithms. Their solution was to propose the GALA systems. These systems generat e highly comprehensible features but the types of interactions that it can handle are limited to either prototypical re-lationships or boolean expressions. Th e GALA systems do not directly assess the interactions observed in the data and do not produce a model that describes the effects of these interactions. Recent work b y Jakulin &amp; Bratko [5] introduced the notion of interaction gain to analyse attribute interactions along with visualiza-tion methods. They proposed an experiment showing the benefits of Cartesian product as an approach to resolve the most important interactions.

The topic of interactions has been extens ively studied in statistics. PCA, ICA, and contextual normalization methods (e.g., [8]) are examples of methods that have been used in machine learning to help a ssess the structure of the interactions and produce new features that keep the most important information (according to some criteria). On the other hand, these methods do not rely on the class information, which limit their usefulness for classification tasks [3]. We also notice that most of them can only handle continuous attributes.

In summary, we observe a lack of paradigm-independent supervised construc-tive induction techniques that directly address the issues of attribute interac-tions while being capable of handling both continuous and discrete attributes. The GLOREF approach we propose in this paper is an attempt to fulfill this need. We now describe the GLOREF (GLObally RElevant Features) approach which we propose for the construction of globally relevant features that account for the initial interactions among the attributes. GLOREF works as a pre-processor and can be used with any standard learning algorithm. The input is a training dataset which contains at least one numerical attribute. The GLOREF approach has two phases: the analysis of relevance and the generation of globally relevant features. The analysis phase computes information to characterize the interactions among the attributes along with their impact on learning. The results of this analysis are stored in data st ructures named relevance matrices . The feature generation phase uses the relevance matrices to search fo r transformation models. Finally, these transformation models are applied to augment the initial data representation and the learning can proceed as usual with the augmented data rep resentation. The following subsections describe the analysis of relevance, the automatic generation of globally relevant features, and application considerations. 4.1 Analysis of Relevance The analysis of relevance takes as input the training dataset and, optionally, two lists defining the explanatory and the response attributes. If these lists are not provided, we simply generate default lists of explanatory and response at-tributes containing all initial attributes and all initial continuous attributes, respectively 1 . As output, the analysis of relevance returns a set of relevance matrices. These matr ices provide information on the relevance of the response attributes over partitions based on the explanatory attributes.

The analysis starts by creating a partition of the training dataset S = { s 1 ,s 2 ,...,s N based on a nominal explanatory attribute X with m possible values, noted {
X (1) ,X (2) ,...,X ( m ) } 2 , generates m subsets S 1 ,S 2 ,...,S m where each S i = { s  X  S | val X ( s )= X ( i ) } for i  X  X  1 , 2 ,...,m } . If the explanatory attribute is continuous, we first discret ize it and then partition based on the discretized val-ues instead of the original ones. Since the discretized attributes produced are not going to be used for classification, there is no need to use a supervised discretiza-tion technique in this step. A simple unsupervised method such as equal-width or equal-frequency is more appropriate. By default, we use three intervalls for discretization. As shown by the experim ental results in Sect. 5, this seems to be an adequate choice accross a variety of domains although it is likely that even better performance could be obtained by increasing the number of intervals.
The next step computes the relevance in formation. This step considers one explanatory and one response attributes at a time. To evaluate the effect of the explanatory attribute, we evaluate the relevance of the response attribute in each of the subsets ( S i ) and in the full training dataset ( S ). Following the standard approach to characterize relevan ce of continuous attributes in decision tree building, we first sort the instances along the response attribute. We then define a split for each observed value of the response attribute in the given set and compute how many examples of each class would fall on each side of the split. Using these numbers, we compute the gain ratio for each possible split. Finally, we define two additional values noted  X  1 and  X  2 that identify the majority class on each side of the split. We name these two values compatibility characteristics since they will be used to determine whether the subsets of the partitions interact in a compatible manner or not (i.e., if they reduce the global relevance or not). All information computed during this step is stored in a set of relevance matrices noted RM 1 , RM 2 ,..., RM m , and RM, where RM i contains the information computed using subset S i , and RM the information from S .
To illustrate, let us consider the analysis of the effects of X 2 on the relevance of X 3 for the domain introduced above. First, the partitioning step needs to discretize X 2 . Let us suppose that this discretization did lead to a new attribute X 2 discretized with 5 possible values (0, 1, 2, 3, and 4). In this case, 6 relevance matrices would be generated (one for each subset and one for the global dataset). The table on the left hand side in Fig. 2 shows part of the relevance matrix for the subset S 1 , which includes all instances s such that val X 2 discretized ( s )=0. There are 18 entries in this relevance ma trix which corresponds to the number of distinct values observed for the response attribute X 3 in the given subset. For each cut point, the relevance matrix shows the threshold value, the number of instances per class in each side of the split (columns  X  X umul. X  and  X  X al. X ), the compatibility characteristics  X  1 and  X  2 3 , and the relevance in terms of gain ratio. The best cut point for this subset (denoted by ) is at threshold 76 . 32 which splits the dataset into two subsets of 8 (7 from 1 st class and 1 from 2 nd class) and 10 (all from 2 st class) instances, respectively.
 Visualizing Relevance Matrices and Detecting Harmful Interactions.
 The information contained in the relevance matrices for a given pair of attributes can be effeciently v isualized through a Relevance Graph . For example, let us consider the graph in Fig. 2 which shows the effects of X 2 on the relevance of X 3 for the same example. This relevance graph is composed of 6 curves, one for each relevance matrix. The one on the left (named global relevance curve ) accounts for the global relevance matrix (i.e., RM) while the following ones (named local relevance curves ) are for the relevance matrices corresponding to the subsets of the partition based on X 2 discretized (i.e., RM 1 , RM 2 ,..., RM 5 ). In particular, the first local relevance curve (labeled  X 0 X ) corresponds to th e relevance matrix shown on the left side. Each point on a given curve represents one entry in the corresponding relevance matrix. The threshold values for the response attribute are shown along the vertical axis. The color (or gray scale) and symbol (e.g., square, cross, plus) of each point designate the compatibility characteristics  X  1 and  X  2 , respectively. There is one color ( symbol) for each possible value of  X  1 (  X  2 ). The relevance of a given point is sho wn by the horizontal distance that separates it from the vertical reference line located on the left side of each curve. The larger the distance; the better is the cut point in producing pure partitions.
The effect of a given interaction on the global relevance is directly assessed by comparing the relevance of the best cut points (the ones that are the farthest away from their vertical reference line) in the local relevance curves with the relevance of the best cut point in the global relevance curve. If one or more best cut points in local curves are more relevant than the best global cut point, then the interaction has a negative effect on the global relevance of the response attribute. The relevance graph in Fig. 2 illustrates this situation since several of the most relevant cut points in the local relevance curves (indicated on the graph by ) are more relevant than the best global cut point. 4.2 Automatic Generation of Globally Relevant Features A key idea behind the GLOREF approach comes from the observation that the global relevance of the response attribute can be modified by altering the aligne-ment of the local relevance matrices. Such a re-alignement can be accomplished by modifying the values of the response attribute within each local relevance matrix by a value  X  i for i =1 ,...,m . The result is a new feature Z defined as where Y is the response attribute, X (1) ,X (2) ,...,X ( m ) are the distinct values for the explanatory attribute X ,and {  X  1 , X  2 ,..., X  m } are the parameter values of the model. The objective is to set the  X  i values in a way that maximizes the global relevance of the new feature. We first present the algorithm developed to resolve this optimization problem and then introduce the approach to cope with interactions involving several explanatory attributes.
 Univariate Transformations. A brute force solution to select the parameter values {  X  1 , X  2 ,..., X  m } is to evaluate all possible alignments of the local rele-vance curves and select the alignment with the best global relevance. Recognizing that the number of possible alignments is exponential in the number of relevance curves, this solution would not be practical in most real world applications. We therefore introduce the heuristic approach described in Fig. 3.

The algorithm starts by handling a special case that happens when all the most relevant cut points in the various rel evance matrices are c ompatible (equal values for both compatibility characteristics  X  1 and  X  2 ). In this case, the algo-rithm directly returns the optimal solution which aligns these most relevant cut points on an arbitrary threshold noted T  X  4 . The relevance graph in Fig. 2 illus-trates this situation since all maximally relevant cut points are compatible (same color and same symbol). When the most relevant cut points are not all compat-ible, the algorithm proceeds with a grea dy search. This search gradually builds the complete solution by combining local solutions. It starts by finding the best alignment between the first two releva nce matrices and store the result into a temporary relevance matrix noted RM cum . In the following iteration, it combines RM cum with the third relevance matrix and so on until all local relevance ma-trices have been processed. There are th ree steps in each iteration of the search procedure: reduction of the two relevance matrices to be considered ( ReduceRM ), search for the best local alignment ( UnivExhaustiveSearch ), and update of the current solution ( ComputeGlobalRM ). In the first step, ReduceRM removes many of the cut points from the two relevance matrices considered in order to reduce the number of potential alignments to eval uate. Precisely, it removes all entries except the most relevant cut point for each observed combination of  X  1 and  X  2 and the two points with minimal and maximal thresholds. In the second step, UnivExhaustiveSearch evaluates all potential alignments of the two reduced relevance matrices and returns the two  X  values that maximize the global rele-vance of a new feature that would be created by combining the subsets consid-ered. Finally, ComputeGlobalRM updates the current solution by adding the new  X  values to the previous global solution. Once all relevance matrices have been considered, the heuristic returns the set of parameter values {  X  1 , X  2 ,..., X  m } selected for the generation of a new globally relevant feature (Eq. 1). Multivariate Transformations. The direct extension of the univariate solu-tion to handle the multivariate case would require a multivariate partionning of the initial dataset along with the analysis of the resulting combinatorial number of subsets. Efficiency concerns and the ri sk of having to proceed with insufficient data in the various subsets call for an alternative method. Accordingly, we pro-pose an inductive process where each phase has two steps: Feature Generation and Feature Selection .

The generation step constructs features in progressive order of complexity by combining pairs of features from the previous phase. In the first phase, it uses the univariate transformations to create multivariate features with two ex-planatory attributes. For instance, if there are two univariate transformations Z 1 =  X  ( X 1 ,Y )and Z 2 =  X  ( X 2 ,Y ), then the generation step in the inital phase would create a new multivariate feature Z =  X  ( { X 1 ,X 2 } ,Y ). In the second phase, the generation step uses the select ed features from the first phase to cre-ate features involving either three or four explanatory attributes, and so forth. Each multivariate feature is constructed through an iterative optimization pro-cess. Precisely, to constru ct a multivariate feature Z involving l explanatory attributes X 1 ,...,X l and a response attribute Y we repeat the following steps 1. Using the univariate procedure d escribed above, compute for each X i aset 2. Update the values of the new feature using values specified by the set  X  t i . The repeated summation allows us to jointly realign the univariate relevance curves in a way that maximize the relevance of the new feature. The process stops when there is no significant improvements in the global relevance of Z between two iterations or when a maximal number of iterations has been performed. In practice, only a few iterations are required to converge (between two and five in most c ases). This process ensures that the number of parameters to estimate grows linearly with the number of explanatory attributes and avoids the multivariate partitioning issues mentioned above. The reuse of the efficient univariate heurist ic presented above further improve the performance of the approach.

The feature selection determines whi ch features are allowed to proceed to the next phase of the inductive process. To be selected, a new multivariate feature must have a higher global relevance than any of the attributes involved in its creation. To control the risk of overfitting, we use only 70% of the training data during the creation of the features and keep the remaining part for the feature selection step. The inductive process stops when less than two new features are selected for the following iteration. Finally, all univariate transformation models and all selected multivariate ones are applied to augment the initial represen-tation with globally relevant features. We notice that the overall computational complexity of the approach is polynomial in the number of features provided as input to each iteration. By applying feature selection prior to each iteration, we ensure that the approach stays practical regardless of the number of initial attributes. 4.3 Application Issues and Smoothing of Transformations When computing the values for the new features, two issues may arise: missing values and unseen values. Missing valu es might be observed for one or more of the explanatory attributes or for the response attribute. The former case does not cause any problem as our implementation treats this situation explicitly by including the missing value as one of the potential values for all explanatory attributes. However, if the response attribute has a missing value then the new feature would also need to have a missing value. The problem of unseen val-ues arises when the model tries to process an instance for which the observed explanatory attribute value has not been seen during the generation of the trans-formation model. Since the given value was not part of the training dataset, the models do not include an entry for this value and therefore there is no corre-sponding  X  parameter value. In this case, the value of the new feature equals the value of the response attribute (i.e., no transformation).

The discretization of continuous explanatory attributes may introduce unnec-essary discontinuities in the new features. We avoid this problem by smoothing the  X  values when applying transformations that involve one or more contin-uous explanatory attributes. We use the inverse distance weighting smoothing method to adjust the  X  values based on the observed values of the explanatory attribute(s). To evaluate the feasibility of the GLOREF approach, we propose a large-scale experiment involving 24 datasets (12 artificial and 12 from the UCI repository) and 13 classifiers implemented in the WEKA package. The artificial datasets contain numerical attributes only with pre-defined univariate and simple mul-tivariate interactions. Several of the UCI datasets contain a mix of continuous and discrete attributes. The maximal number of attributes is 35. We followed the 10-fold cross-validation methodology. In each fold, we performed the fol-lowing tasks: (1) apply GLOREF on the training data to learn univariate and multivariate transformation models, (2) use these models to augment the ini-tial representation with GLOREF features, (3) for each learning system, learn a model using only the initial attributes and another model using the augmented representation, and (4) evaluate the accuracy of the two models on test data.
We first consider the ability of GLOREF to produce new globally relevant features by comparing the expected gain-ratio of the best initial attribute and thebestGLOREFfeature.Wecomputethe expected gain-ratio of the best ini-tial (resp. GLOREF) attribute by averaging the gain-ratios of the best initial (resp. GLOREF) attribute based on test data from the various folds of the cross-validation procedure. Table 1 presents the results. The gain ratio for the best GLOREF feature is systematically higher that the one for the best initial at-tribute. The standard t-test to compare group means reveals that all increases are statistically significant at the 0 . 05 level. The relatively large variation in per-centage of increase (from 12% to 488%) suggests that the datasets are not all equally affected by the problem of attribut e interactions. We repeated the analy-sis using the  X  2 measure and obtained consistent results. Therefore, we conclude that the GLOREF approach succeeded in pro ducing new highly globally relevant features.

The graph on the right side of Fig. 4 offers a quick view of the usefulness of the new features for learning. There is one point for each combination of learning system and dataset for which the use of GLOREF features significantly changed the accuracy. All points located above the diagonal line indicate positive results and inversely for the points located below. The table on the left side details the results by classifier. The first four columns provide the statistics on increase in accuracy due to the GLO REF features while the last two columns count the number of better and worse results with statistically significantly results in parentheses (the number of datasets for which the addition of the GLOREF fea-tures did not change the results equals the difference between 24 and the sum of the  X  X etter X  and  X  X orse X  columns). Out of the 312 experiments (13 classifiers * 24 datasets), 127 lead to a significant difference in accuracy and only 2 of these are on the negative side. As expected, lea rning systems which are powerless with respect to attribute interactions such a s HyperPipes, OneR, and DecisionStump profited the most from the GLOREF featu res with average increase in accuracy of 24%, 10.5% and 9.8%, respectively. Focusing on statistically significant results, we notice that all classifiers have been positively affected by the GLOREF fea-tures, with the number of statistically significant wins varying from 2 to 22 over 24. Moreover, the column  X  X ax X  clearly shows that complex approaches such as bagging, boosting and support vector machine (SMO) can also greatly benefit from highly globally relevant features. The relatively important standard devia-tions tend to confirm the heterogeneousness of the selected datasets. Finally, by analyzing the results by datasets, we observe that the levels of increase in accu-racy tend to match the increase of glob al relevance between the best GLOREF and best initial feature. In other words, large improvements in global relevance generally result in high increa ses in accuracy and inversely. This paper links the problem of attribute interactions to the concept of attribute relevance. After discussing the potenti al effects of interactions on relevance, we introduce the GLOREF method to model interactions and construct new glob-ally relevant features. The autonomous solution is evaluated through a large-scale experimentation involving 24 datasets and 13 learning systems. The analysis of the relevance of the new features show s that the GLOREF system generates highly globally relevant features for all datasets, with some increases in gain ra-tio that are close to 500%. Adding the GLOREF features to the initial represen-tation significantly improved the accura cy in more than 40% of the experiments, while reducing it in only less than 1%. Although these results are strongly posi-tive, it is possible that the heuristics proposed are not optimal. Future work will investigate alternative heuristics to further improve performance.

