 Bioinformatics provides a rich source for the application of techniques from machine learning. Es-pecially the elucidation of regulatory networks underlying gene expression has lead to a cornucopia of approaches: see [1] for review. Here we focus on one aspect of network elucidation, the identi-fi cation of the regulators of the causative agent of severe malaria, Plasmodium falciparum . Several properties of the parasite necessitate a tailored algorithm for regulator identi fi cation: Together, these properties point to a vector autoregressive model making use of the gene expression time series. The model should not rely on sequence homology information but it should be fl exible enough to integrate sequence information in the fu ture. This points to a Bayesian model as favored approach. We start with a semi-realistic model of transcription based on Michaelis-Menten kinetics [1] and subsequently simplify to obtain a linear model. Denoting the concentration of a certain mRNA transcript at time t by z ( t ) we write: The activator is thought to bind to DNA motifs upstream of the transcription start site and binds RNA polymerase which reads the DNA template to produce an mRNA transcript. M j can be thought of as the multiplicity of the motif,  X  z captures the characteristic life time of the transcript. While reasonably realistic, this equation harbors too many unknowns for reliable inference: 3 N +1 with N  X  1000 . We proceed with several simpli fi cations: Counting time in units of  X  and taking logarithms on both sides, Equation (1) then simpli fi es to the expression levels of a set of activators. With a similar derivation one can include repressors [1]. 2.1 A Bayesian model for sparse linear regression Let y be a vector with the log expression of the target gene and X =( x 1 ,..., x N ) a matrix whose columns contain the log expression of the candidate regulators. Assuming that the measurements are corrupted with additive Gaussian noise, we get y  X  X  ( X  X  , X  2 I ) where  X  =(  X  1 ,..., X  N ) T is a vector of regression coef fi cients and  X  2 is the variance of the noise. Such a linear model is commonly used [7, 8, 9]. Both y and x 1 ,..., x N are mean-centered vectors with T measurements. We specify an inverse gamma (IG) prior for  X  2 so that P (  X  2 )= IG (  X  2 , X / 2 , X  X / 2) ,where  X  is a prior estimate of  X  2 and  X  is the sample size associated w ith that estimate. We assume that apriori all components  X  i are independent and take a so-called  X  X pike and slab prior X  [10] for each of them. and  X  i =0 otherwise. Given  X  , the prior on  X  then reads where N ( x,  X ,  X  2 ) denotes a Gaussian density with mean  X  and variance  X  2 evaluated at x .Inorder to enforce sparsity, the variance v 1 of the slab should be larger than the variance v 0 of the spike. Instead of picking the hyperparameters v 1 and v 0 directly, it is convenient to pick a threshold of Finally, we assign independent Bernoulli priors to the components of the latent vector  X  : so that each of the x 1 ,..., x N can independently take part in the regression with probability w .We can identify the candidate genes whose expression is more likely to be correlated with the target gene by means of the posterior distribution of  X  : where
P (  X  ,  X  , X  2 , y | X )= N ( y , X  X  , X  2 I ) P (  X  |  X  ) P (  X  ) P (  X  2 ) Unfortunately, this posterior distribution cannot be computed exactly if the number N of candidate genes is larger than 25. An approximation bas ed on Markov Chain Monte Carlo (MCMC) methods has been proposed in [11]. 2.2 A hierarchical model for gene regulation In the section above we made use of the prior information that a target gene is typically regulated by a small number of regulators. We have not yet made use of the prior information that a regulator typically regulates more than one gene. We incorporate this information by a hierarchical extension of our previous model. We introduce a vector  X  of binary latent variables where  X  i =1 if gene i is a regulator and  X  i =0 otherwise. The following joint distribution captures this idea: In this hierarchical model,  X  is a matrix of binary latent variables where  X  j,i =1 if gene i takes part in the regression of gene j and  X  j,i =0 otherwise. The relationship between regulators and regulatees suggests that P (  X  j,i =1 |  X  i =1) should be bigger than P (  X  j,i =1 |  X  i =0) and thus w the expression of gene i and the delayed expression of gene j . Hyperparameter w represents the prior probability of any gene being a regulator and the elements  X  2 j of the vector  X  2 contain the variance of the noise in each of the N regressions. Hyperparameters  X  j and  X  j have the same meaning as in the model for sparse linear regression. The corre sponding plate model is illustrated in Figure 1. Compared with the sparse linear regression model we expanded the number of latent variables from O ( N ) to O ( N 2 ) . In order to keep inference feasible we turn to an approximate inference technique. Figure 1: The hierarchical model for gene regulation. The Expectation Propagation (EP) algorithm [12] allows to perform approximate Bayesian infer-ence. In all Bayesian problems, the joint distribution of the model parameters  X  and a data set D = { ( x i ,y i ): i =1 ,...,n } with i.i.d. elements can be expressed as a product of terms Expectation propagation proceeds to approxi mate (5) with a product of simpler terms where all the term approximations  X  t i are restricted to belong to the same family F of exponential distributions, but they do not have to integrate 1. Note that Q will also be in F because F is closed under multiplication. Each term approximation  X  t i is chosen so that is as close as possible to in terms of the direct Kullback-Leibler (K-L) divergence. The pseudocode of the EP algorithm is: The optimization problem in step (b) is solved by matching suf fi cient statistics between a distribu-exponential family it is generally trivial to calculate its normalization constant. Once Q is normal-ized it can approximate P (  X  |D ) . Finally, EP is not guaranteed to converge, although convergence can be improved by means of damped updates or double-loop algorithms [13]. 3.1 EP for sparse linear regression The application of EP to the models of Sectio n 2 introduces some nontrivial technicalities. Furthermore, we describe several techniques to speed up the EP algorithm. We approximate P (  X  ,  X  , X  2 , y | X ) for sparse linear regression by means of a factorized exponential distribution: where { q i , X  i ,s i : i =1 ,...,N } , a and b are free parameters. Note that in the approximation Q (  X  ,  X  , X  2 ) all the components of the vectors  X  and  X  and the variable  X  2 are considered to be density appears in (3) as a product of T + N terms (not counting the priors) which correspond to the t terms in (5). This way, we have T + N term approximations with the same form as (7) and which correspond to the term approximations  X  t i in (6). The complexity is O ( TN ) per iteration, because updating any of the fi rst T term approximations requires N operations. However, some of the EP update operations require to compute integrals whic h do not have a closed form expression. To avoid that, we employ the following simpli fi cations when we update the fi rst T term approximations: In order to improve convergence, we re-update all the N last term approximations each time one of the fi rst T term approximations is updated. Computa tional complexity does not get worse than O ( TN ) and the resulting algorithm turns out to be faster. By comparison, the MCMC method in [11] takes O ( N 2 ) steps to generate a single sample from P (  X  | y , X ) . On problems of much smaller size than we will consider in our experi ments, one typically requires on the order of 10000 samples to obtain reasonably accurate estimates [10]. 3.2 EP for gene regulation We approximate P (  X  ,  X  ,  X  ,  X  2 | X ) by the factorized exponential distribution posterior probability P (  X  | X ) that indicates which genes are mor e likely to be regulators can then linear regression model to this new case: the terms to be approximated are the same as before except for the new N ( N  X  1) terms for the prior on  X  . As in the previous section and in order to improve convergence, we re-update all the N ( N  X  1) term approximations corresponding to the prior on  X  each time N of the N ( T  X  1) term approximations corresponding to regressions are updated. In order to reduce memory requirements, we associate all the N ( N  X  1) terms for the prior on  X  into a single term, which we can do because they ar e independent so that we only store in memory one term approximation instead of N ( N  X  1) . We also group the N ( N  X  1) terms for the prior on  X  into N independent terms and the N ( T  X  1) terms for the regressions into T  X  1 independent terms. Assuming a constant number of iterations (in our experiments, we need at most 20 iterations for EP to converge), the computational complexity and the memory requirements of the resulting algorithm are O ( TN 2 ) . This indicates that it is feasible to analyz e data sets which contain the expression pattern of thousands of genes. An MCMC algorithm would require O ( N 3 ) to generate just a single sample. We carried out experiments with arti fi cially generated data in order to validate the EP algorithms. In the experiments for sparse linear regression we fi xed the hyperparameters in (3) so that  X  =3 ,  X  is the sample variance of the target vector y , v 1 =1 ,  X  = N  X  1 , v 0 is chosen according to (2) and w = N  X  1 . In the experiment for gene regulation we fi xed the hyperparameters in (4) so that w =( N  X  1)  X  1 ,  X  i =3 and  X  i is the sample variance of the vector x i , w 1 =10  X  1 ( N  X  1)  X  1 , w 0 =10  X  2 ( N  X  1)  X  1 , v 1 =1 ,  X  =0 . 2 and v 0 is chosen according to (2). Although the to determine the most likely regulators, are robust to even large changes. 4.1 Sparse linear regression In the fi rst experiment we set T =50 and generated x 1 ,..., x 6000  X  X  (0 , 3 2 I ) candidate vectors and a target vector y = x 1  X  x 2 +0 . 5 x 3  X  0 . 5 x 4 +  X  ,where  X   X  X  (0 , I ) . The EP algorithm times (each time using new data) and obtained similar results on each run.
 In the second experiment we set T =50 and generated a target vector y  X  X  (0 , 3 2 I ) and x ,..., x 500 candidate vectors so that x i = y +  X  i for i =2 ,..., 500 ,where  X  i  X  X  (0 , I ) . The candidate vector x 1 is generated as x 1 = y +0 . 5  X  1 where  X  1  X  X  (0 , I ) . This way, the noise in x 1 is twice as small as the noise in the other candidate vectors. Note that all the candidate vec-tors are highly correlated with each other and with the target vector. This is what happens in gene expression data sets where many genes show similar expression patterns. We ran the EP algorithm 100 times (each time using new data) and it always assigned to all the w 1 ,...,w 500 more or less the same value of 6  X  10  X  4 .However, w 1 obtained the highest value on 54 of the runs and it was among the three w s with highest value on 87 of the runs.
 Finally, we repeated these experiments setting N = 100 , using the MCMC method of [11] and the EP algorithm for sparse linear regression. Both techniques produced results that are statistically indistinguishable (the approximations obtained through EP fall within the variation of the MCMC method), for EP within a fraction of the time of MCMC. 4.2 Gene regulation In this experiment we set T =50 and generated a vector z with T +1 values from a sinusoid. We where  X  i,t  X  X  (0 , X  2 ) and  X  is one fourth of the sample standard deviation of z . We also generated The algorithm assigned t 1 the highest value on 33 of the runs and x 1 was ranked among the top fi ve on 74 of the runs. This indicates that the EP algorithm can successfully detect small differences in correlations and should be able to fi nd new regulators in real microarray data. We applied our algorithm to four data sets. The fi rst is a yeast cell-cycle data set from [5] which is commonly used as a benchmark for regulator discovery. Data sets two through four are from three different Plasmodium strains [6]. Missing values were imputed by nearest neighbors [14] and the hyperparameters were fi xed at the same values as in Section 4. The yeast cdc15 data set contains 23 measurements of 6178 genes. We singled out 751 genes which met a minimum criterion for cell cycle regulation [5]. The top ten genes with the highest values for  X  along with their annotation from factors and IOC2 is associated with transcriptio n regulation. As 4% of the yeast genome is associated with transcription the probability of this occurring by chance is 0.0062. However, although the result ACE2, FKH* or SWI*) among the top ten. Figure 2: Left: Plot of the vectors x 2 , ..., x 50 in grey and the vector x 1 in black. The vector x 1 Expressions of gene PF11 321 (black) and the 100 genes which are more likely to be regulated by it (light and dark grey). Two clusters of positively an d negatively regulated ge nes can be appreciated. The three data sets for the malaria parasite [6] c ontain 53 measurements (3D7), 50 measurements (Dd2) and 48 measurements (HB3). We focus on 3D7 as this is the sequenced reference strain. We expression measurements. The top ten genes with the highest values for  X  along with their annotation from PlasmoDB are listed in table 5. Recalling the motivation for our approach, the paucity of known transcription factors, we cannot expect to fi nd many annotated regulators in PlasmoDB version 5.4. Thus, we list the BLASTP hits provided by PlasmoDB instead of the absent annotation. These hits were the highest scoring ones outside of the genus Plasmodium. We fi nd four genes with a large identity to transcription factors in Dictyostelium (a recently sequenced social amoebe) and one annotated helicase which typically functions in post-transcriptional regulation. Interestingly three genes have no known function and could be regulators. Results for the HB3 strain were similar in that fi ve putative regulators were found. Somewhat disappointing, we found only one putative regulator (a helicase) among the top ten genes for Dd2. Our approach enters a fi eld full of methods enforcing sparsity ([15, 8, 7, 16, 9]). Our main contri-butions are: a hierarchical model to discover regulators, a tractable algorithm for fast approximate inference in models with many interacting variables, and the application to malaria.
 Arguably most related is the hierarchical model in [15]. The covariates in this model are a dozen external variables, coding experimental conditions, instead of the hundreds of expression levels of other genes as in our model. Furthermore, the prior in [15] enforces sparsity on the  X  X olumns X  of  X  to implement the idea that some genes are not in fl uenced by any of the experimental conditions. Our prior, on the other hand, enforces sparsity on the  X  X ows X  in order to fi nd regulators. Future work could include more involved priors, e.g., enforcing sparsity on both  X  X ows X  and  X  X olumns X  or incorporating information from DNA sequence data. The approximate inference tech-niques described in this paper make it feasible to evaluate such extensions in a fraction of the time required by MCMC methods.
 [1] T.S. Gardner and J.J. Faith. Reverse-engineering transcription control networks. Physics of [2] R. Coulson, N. Hall, and C. Ouzounis. Comparative genomics of transcriptional control in the [3] S. Balaji, M.M. Babu, L.M. Iyer, and L. Aravind. Discovery of the principal speci fi ctranscrip-[4] T. Sakata and E.A. Winzeler. Genomics, systems biology and drug development for infectuous [5] P.T. Spellman, G. Sherlock, V.R. Iyer, K. Anders, M.B. Eisen, P.O. Brown, and D. Botstein. [6] M. LLinas, Z. Bozdech, E. D. Wong, A.T. Adai, and J. L. DeRisi. Comparative whole [7] M. Beal. Variational Algorithms for Approximate Bayesian Inference . PhD thesis, UCL, 2003. [8] C. Sabatti and G.M. James. Bayesian sparse hidden components analysis for transcription [9] S.T. Jensen, G. Chen, and C.J. Stoeckert. Bayesian variable selection and data integration for [10] E.I. George and R.E. McCulloch. Appr oaches for Bayesian variable selection. Statistica [11] E.I. George and R.E. McCulloch. Variable selection via Gibbs sampling. Journal of the Amer-[12] T. Minka. A family of algorithms for approximate Bayesian inference . PhD thesis, MIT, 2001. [13] T. Heskes and O. Zoeter. Expectation pr opagation for approximate inference in dynamic [14] O. Troyanskaya, M. Cantor, P. Brown, T. Hastie , R. Tibshirani, and D. Botstein. Missing value [15] J. Lucas, C. Carvalho, Q. Wang, A. Bild, J. Nevins, and M. West. Sparse statistical modelling [16] M.Y. Park, T. Hastie, and R. Tibshirani. Averaged gene expressions for regression. Biostatis-

We introduce a hierarchical Bayesian model for the discovery of regulators from gene expression data only . The hierarchy incorporates the knowledge that only a few regulators regulate most of the genes. Running the mo del on a malaria parasite data set, we found four genes with significant homology to transcription facto rs in an amoebe, one RNA regulator and three genes of unknown function (out of the top ten genes considered).

The expression of each gene is linearly regressed against the expression of all other genes. A hierarchical prior for the regression coefficients enforces sparsity. The posterior of a latent parameter determines the probability of each gene being a regulator.
 We approximate the posterior distribution of the mo del parameters b y m eans of expectation propagation . We tested the algorithm on the cdc15 dataset from yeast and on the 3D7 , Dd2 and HB3 datasets from Plamodium . We took the 10 genes with highest probability of being regulators and looked up their annotations and sequence homo logies at public databases: SGD and PlasmoDB .
 Results for yeast are sta tistically significant. Results for 3D7 and HB3 are also satisfactory. However, only one 
