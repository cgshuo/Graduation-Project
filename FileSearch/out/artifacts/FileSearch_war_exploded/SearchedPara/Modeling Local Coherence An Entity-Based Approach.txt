 MassachusettsInstituteofTechnology UniversityofEdinburgh information about discourse entities. We re-conceptualize coherence assessment as a learning text ordering, summary coherence evaluation, and readability assessment. 1. Introduction A key requirement for any system that produces text is the coherence of its output.
Not surprisingly, a variety of coherence theories have been developed over the years (e.g., Mann and Thomson 1988; Grosz et al. 1995) and their principles have found application in many symbolic text generation systems (e.g., Scott and de Souza 1990;
Kibble and Power 2004). The ability of these systems to generate high quality text, almost indistinguishable from human writing, makes the incorporation of coherence theories in robust large-scale systems particularly appealing. The task is, however, challenging considering that most previous efforts have relied on handcrafted rules, valid only for limited domains, with no guarantee of scalability or portability (Reiter and Dale 2000). Furthermore, coherence constraints are often embedded in complex representations (e.g., Asher and Lascarides 2003) which are hard to implement in a robustapplication.
 ofsentence-to-sentencetransitions.Localcoherenceisundoubtedlynecessaryfor global coherence and has received considerable attention in computational linguistics (Foltz,
Kintsch, and Landauer 1998; Marcu 2000; Lapata 2003; Althaus, Karamanis, and Koller 2004;Karamanisetal.2004).Itisalsosupportedbymuchpsycholinguisticevidence.For instance,McKoonandRatcliff(1992)arguethatlocalcoherenceistheprimarysourceof inference-makingduringreading.
 ent texts exhibits certain regularities. This assumption is not arbitrary X  X ome of these regularities have been recognized in Centering Theory (Grosz, Joshi, and Weinstein 1995) and other entity-based theories of discourse (e.g., Givon 1987; Prince 1981). The algorithmintroducedinthearticleautomaticallyabstractsatextintoasetofentitytran-sition sequences, a representation that reflects distributional, syntactic, and referential informationaboutdiscourseentities.
 to learn the properties of coherent texts fro ma corpus, without recourse to manual annotationorapredefinedknowledgebase.Wedemonstratetheusefulnessofthisrep-resentationbytestingitspredictivepowerinthreeapplications:textordering,automatic evaluationofsummarycoherence,andreadabilityassessment.
 rankingproblems,andpresentanefficientlylearnablemodelthatranksalternativeren-deringsofthesameinformationbasedontheirdegreeoflocalcoherence.Suchamecha-nismisparticularlyappropriateforgenerationandsummarizationsystemsastheycan producemultipletextrealizationsofthesameunderlyingcontent,eitherbyvaryingpa-rametervalues,orbyrelaxingconstraintsthatcontrolthegenerationprocess.Asystem equipped with a ranking mechanism could compare the quality of the candidate outputs, in much the same way speech recognizers employ language models at the sentencelevel.
 tence order from a set of candidate permutations. In the summary evaluation task, we compare the rankings produced by the model against human coherence judgments elicitedforautomaticallygeneratedsummaries.Inbothexperiments,ourmethodyields improvements over state-of-the-art models. We also show the benefits of the entity-based representation in a readability assessment task, where the goal is to predict the comprehensiondifficultyofagiventext.Incontrasttoexistingsystemswhichfocuson intra-sentential features, we explore the contribution of discourse-level features to this task. By incorporating coherence features stemming from the proposed entity-based representation,weimprovetheperformanceofastate-of-the-artreadabilityassessment system(SchwarmandOstendorf2005).
 cal coherence and outline previous work on its computational treatment. Then, we introduce our entity-based representation, and define its linguistic properties. In the subsequentsections,wepresentourthreeevaluationtasks,andreporttheresultsofour experiments.Discussionoftheresultsconcludesthearticle. 2. Related Work
Ourapproachisinspiredbyentity-basedtheoriesoflocalcoherence,andiswell-suited for developing a coherence metric in the context of a ranking-based text generation system. We first summarize entity-based theories of discourse, and overview previous attemptsfortranslatingtheirunderlyingprinciplesintocomputationalcoherencemod-els.Next,wedescriberankingapproachestonaturallanguagegenerationandfocuson coherencemetricsusedincurrenttextplanners. 2 2.1 Entity-Based Approaches to Local Coherence
Linguistic Modeling. Entity-based accounts of local coherence have a long tradition within the linguistic and cognitive science literature (Kuno 1972; Chafe 1976; Halliday and Hasan 1976; Karttunen 1976; Clark and Haviland 1977; Prince 1981; Grosz, Joshi, and Weinstein 1995). A unifying assumption underlying different approaches is that discourse coherence is achieved in view of the way discourse entities are introduced anddiscussed.Thisobservationiscommonlyformalizedbydevisingconstraintsonthe linguisticrealizationanddistributionofdiscourseentitiesincoherenttexts. others, and consequently are expected to exhibit different properties. In Centering Theory (Grosz, Joshi, and Weinstein 1995; Walker, Joshi, and Prince 1998; Strube and
Hahn1999;Poesioetal.2004),salienceconcernshowentitiesarerealizedinanutterance (e.g.,whethertheyaretheypronominalizedornot).Inothertheories,salienceisdefined intermsoftopicality(Chafe1976;Prince1978),predictability(Kuno1972;Hallidayand
Hasan 1976), and cognitive accessibility (Gundel, Hedberg, and Zacharski 1993). More refinedaccountsexpandthenotionofsaliencefromabinarydistinctiontoascalarone; examplesincludePrince X  X (1981)familiarityscale,andGivon X  X (1987)andAriel X  X (1988) givenness-continuum.
 the linguistic form of its subsequent mentions. Salient entities are more likely to ap-pear in prominent syntactic positions (such as subject or object), and to be introduced in a main clause. The linguistic realization of subsequent mentions X  X n particular, pronominalization X  X s so tightly linked to salience that in some theories (e.g., Givon 1987) it provides the sole basis for defining a salience hierarchy. The hypothesis is that thedegreeofunderspecification inareferringexpression indicatesthetopicalstatusof itsantecedent(e.g.,pronounsrefertoverysaliententities,whereasfullNPsrefertoless salient ones). In Centering Theory, this phenomenon is captured in the Pronoun Rule , and Givon X  X  Scale of Topicality and Ariel X  X  Accessibility Marking Scale propose a graded hierarchy of underspecification that ranges fro mzero anaphora to full noun phrases, and includes stressed and unstressed pronouns, demonstratives with modifiers, and definitedescriptions.
 tities across discourse utterances, distinguishing between salient entities and the rest.
The intuition here is that texts about the same discourse entity are perceived to be more coherent than texts fraught with abrupt switches from one topic to the next. The patterneddistribution ofdiscourse entitiesisanatural consequence oftopiccontinuity observedinacoherenttext.CenteringTheoryformalizesfluctuationsintopiccontinuity in terms of transitions between adjacent utterances. The transitions are ranked, that is,textsdemonstratingcertaintypesoftransitionsaredeemedmorecoherentthantexts where such transitions are absent or infrequent. For example, CONTINUE transitions require that two utterances have at least one entity in common and are preferred over transitions that repeatedly SHIFT fro mone entity to the other. Givon X  X  (1987) and
Hoey X  X  (1991) accounts of discourse continuity complement local measurements by consideringglobalcharacteristicsofentitydistribution,suchasthelifetimeofanentity indiscourseandthereferentialdistancebetweensubsequentmentions.

Computational Modeling. An important practical question is how to translate principles of these linguistic theories into a robust coherence metric. A great deal of research has been devoted to this issue, primarily in Centering Theory (Miltsakaki and Kukich 2000; Hasler 2004; Karamanis et al. 2004). Such translation is challenging in several respects:onehastodeterminewaysofcombiningtheeffectsofvariousconstraintsand to instantiate parameters of the theory that are often left underspecified. Poesio et al. (2004)notethatevenforfundamentalconceptsofCenteringTheorysuchas X  X tterance, X   X  X ealization, X  and  X  X anking, X  multiple X  X nd often contradictory X  X nterpretations have been developed over the years, because in the original theory these concepts are not explicitly fleshed out. For instance, in some Centering papers, entities are ranked with respect to their grammatical function (Brennan, Friedman, and Pollard 1987; Walker,
Iida,andCote1994;Grosz,Joshi,andWeinstein1995),andinotherswithrespecttotheir positioninPrince X  X (1981)givennesshierarchy(StrubeandHahn1999)ortheirthematic role (Sidner 1979). As a result, two  X  X nstantiations X  of the same theory make different predictions for the same input. Poesio et al. (2004) explore alternative specifications proposed in the literature, and demonstrate that the predictive power of the theory is highlysensitivetoitsparameterdefinitions.
 models is to evaluate alternative specifications on manually annotated corpora. Some studiesaimtofindaninstantiationofparametersthatismostconsistentwithobservable data (Strube and Hahn 1999; Karamanis et al. 2004; Poesio et al. 2004). Other studies adoptaspecificinstantiationwiththegoalofimprovingtheperformanceofametricon a task. For instance, Miltsakaki and Kukich (2000) annotate a corpus of student essays with entity transition information, and show that the distribution of transitions corre-lates with human grades. Analogously, Hasler (2004) investigates whether Centering
Theorycanbeusedinevaluatingthereadabilityofautomaticsummariesbyannotating humanandmachinegeneratedextractswithentitytransitioninformation.
 ourworkbuildsuponexistinglinguistic theories, wedonot aimtodirectly implement or refine any of the min particular. We provide our model with sources of knowledge identified as essential by these theories, and leave it to the inference procedure to determine the parameter values and an optimal way to combine them. From a design viewpoint, we emphasize automatic computation for both the underlying discourse representation and the inference procedure. Thus, our work is complementary to com-putationalmodelsdevelopedonmanuallyannotateddata(MiltsakakiandKukich2000;
Hasler 2004; Poesio et al. 2004). Automatic, albeit noisy, feature extraction allows us to perfor ma large scale evaluation of differently instantiated coherence models across genresandapplications. 2.2 Ranking Approaches in Natural Language Generation
Ranking approaches have enjoyed an increasing popularity at all stages in the generation pipeline, ranging fro mtext planning to surface realization (Knight and
Hatzivassiloglou1995;LangkildeandKnight1998;Mellishetal.1998;Walker,Rambow, and Rogati 2001; Karamanis 2003; Kibble and Power 2004). In this framework, an underlying syste mproduces a potentially large set of candidate outputs, with respect to various text generation rules encoded as hard constraints. Not all of the resulting alternativeswillcorrespondtowell-formedtexts,andofthosewhichmaybejudgedac-ceptable,somewillbepreferabletoothers.Thecandidategenerationphaseisfollowed by an assessment phase in which the candidates are ranked based on a set of desirable properties encoded in a ranking function. The top-ranked candidate is selected for presentation. A two-stage generate-and-rank architecture circumvents the complexity 4 of traditional generation systems, where numerous, often conflicting constraints, have tobeencodedduringdevelopmentinordertoproduceasinglehigh-qualityoutput.
 ing approaches applied to text planning (see Walker et al. [2001] and Knight and
Hatzivassiloglou [1995] for ranking approaches to sentence planning and surface re-alization, respectively). The goal of text planning is to determine the content of a text byselectingasetofinformation-bearingunitsandarrangingthemintoastructurethat yieldswell-formedoutput.Dependingonthesystem,textplansarerepresentedasdis-course trees (Mellish et al. 1998) or linear sequences of propositions (Karamanis 2003).
Candidatetextstructuresmaydifferintermsoftheselectedpropositions,thesequence in which facts are presented, the topology of the tree, or the order in which entities are introduced. A set of plausible candidates can be created via stochastic search (Mellish etal.1998)orbyasymbolictextplannerfollowingdifferenttext-formationrules(Kibble andPower2004).Thebestcandidateischosenusinganevaluationorrankingfunction oftenencodingcoherenceconstraints.Althoughthetypeandcomplexityofconstraints varygreatlyacrosssystems,theyarecommonlyinspiredbyRhetoricalStructureTheory or entity-based constraints similar to the ones captured by our method. For instance, therankingfunctionusedbyMellishetal.givespreferencetoplanswhereconsecutive facts mention the same entities and is sensitive to the syntactic environment in which the entity is first introduced (e.g., in a subject or object position). Karamanis finds that a ranking function based solely on the principle of continuity achieves competi-tive performance against more sophisticated alternatives when applied to ordering short descriptions of museum artifacts. 1 In other applications, the ranking function is morecomplex,integratingrulesfromCenteringTheoryalongwithstylisticconstraints (KibbleandPower2004).
 ing function X  X eature selection and weighting X  X s performed manually based on the intuition of the system developer. However, even in a limited domain this task has proven difficult. Mellish et al. (1998; page 100) note:  X  X he proble mis far too co mplex and our knowledge of the issues involved so meager that only a token gesture can be made at this point. X  Moreover, these ranking functions operate over semantically rich input representations that cannot be created automatically without extensive knowl-edgeengineering.Theneedformanualcodingimpairstheportabilityofexistingmeth-ods for coherence ranking to new applications, most notably to text-to-text generation applications,suchassummarization.
 theselimitations:Weintroduceanentity-basedrepresentationofdiscoursethatisauto-matically computed from raw text; we argue that the proposed representation reveals entity transition patterns characteristic of coherent texts. The latter can be easily trans-latedintoalargefeaturespacewhichlendsitselfnaturallytotheeffectivelearningofa rankingfunction,withoutexplicitmanualinvolvement. 3. The Coherence Model
Inthissectionwedescribeourentity-basedrepresentationofdiscourse.Weexplainhow it is computed and how entity transition patterns are extracted. We also discuss how thesepatternscanbeencodedasfeaturevectorsappropriateforperformingcoherence-relatedrankingandclassificationtasks. 3.1 The Entity-Grid Discourse Representation
Each text is represented by an entity grid , a two-dimensional array that captures the distribution of discourse entities across text sentences. We follow Miltsakaki and
Kukich (2000) in assuming that our unit of analysis is the traditional sentence (i.e., a main clause with accompanying subordinate and adjunct clauses). The rows of the grid correspond to sentences, and the columns correspond to discourse entities. By discourse entity we mean a class of coreferent noun phrases (we explain in Section 3.3 how coreferent entities are identified). For each occurrence of a discourse entity in the text, the corresponding grid cell contains information about its presence or absence in a sequence of sentences. In addition, for entities present in a given sentence, grid cells contain information about their syntactic role. Such information can be expressed in many ways (e.g., using constituent labels or thematic role information). Because grammaticalrelationsfigureprominentlyinentity-basedtheoriesoflocalcoherence(see
Section2),theyserveasalogicalpointofdeparture.Eachgridcellthuscorrespondsto astringfromasetofcategoriesreflectingwhethertheentityinquestionisasubject( object ( O ), or neither ( X ). Entities absent fro ma sentence are signaled by gaps (  X  ).
Grammatical role information can be extracted from the output of a broad-coverage dependencyparser(Lin2001;BriscoeandCarroll2002)oranystate-of-theartstatistical parser (Collins 1997; Charniak 2000). We discuss how this information was computed forourexperimentsinSection3.3.

Because the text contains six sentences, the grid columns are of length six. Consider for instance the grid column for the entity trial , [ O  X  X  X  X  X ] present insentences 1and6(as O and X ,respectively) butisabsent fromtherestofthe sentences. Also note that the grid in Table 1 takes coreference resolution into account. Eventhoughthesameentityappearsindifferentlinguisticforms,forexample, Microsoft
Corp. , Microsoft ,and the company , it is mapped to a single entry in the grid (see the columnintroducedby Microsoft inTable1).
 6 samesentence,wedefaulttotherolewiththehighestgrammaticalranking:subjectsare rankedhigherthanobjects,whichinturnarerankedhigherthantherest.Forexample, the entity Microsoft is mentioned twice in Sentence 1 with the grammatical roles x (for Microsoft Corp. )and s (for the company ), but is represented only by s in the grid (see
Tables1and2). 3.2 Entity Grids as Feature Vectors
A fundamental assumption underlying our approach is that the distribution of entities in coherent texts exhibits certain regularities reflected in grid topology. Some of these regularities are formalized in Centering Theory as constraints on transitions of the local focus in adjacent sentences. Grids of coherent texts are likely to have some dense columns (i.e., columns with just a few gaps, such as Microsoft in Table 1) and many sparse columns which will consist mostly of gaps (see markets and earnings in Table 1).
One would further expect that entities corresponding to dense columns are more often subjectsorobjects.Thesecharacteristicswillbelesspronouncedinlow-coherencetexts. transitions. A local entity transition is a sequence { S , occurrences and their syntactic roles in n adjacent sentences. Local transitions can be easilyobtainedfromagridascontinuoussubsequencesofeachcolumn.Eachtransition will have a certain probability in a given grid. For instance, the probability of the transition [ S  X  X  in the grid fro mTable 1 is 0 . 08 (computed as a ratio of its frequency [i.e., six] divided by the total number of transitions of length two [i.e., 75]). Each text canthusbeviewedasadistributiondefinedovertransitiontypes.
 sequencesusingastandardfeaturevectornotation.Eachgridrendering j ofadocument d corresponds to a feature vector  X  ( x ij ) = ( p 1 ( x ij number of all predefined entity transitions, and p t ( x ij in grid x ij . This feature vector representation is usefully amenable to machine learning algorithms (see our experiments in Sections 4 X 6). Furthermore, it allows the consid-eration of large numbers of transitions which could potentially uncover novel entity distributionpatternsrelevantforcoherenceassessmentorothercoherence-relatedtasks. be included in a feature vector. These can be all transitions of a given length (e.g., two or three) or the most frequent transitions within a document collection. An example of a feature space with transitions of length two is illustrated in Table 3. The second row (introducedby d 1 )isthefeaturevectorrepresentationofthegridinTable1. 3.3 Grid Construction: Linguistic Dimensions
One of the central research issues in developing entity-based models of coherence is determiningwhatsourcesoflinguisticknowledgeareessentialforaccurateprediction, and howtoencode themsuccinctly inadiscourse representation. Previous approaches tend to agree on the features of entity distribution related to local coherence X  X he disagreementliesinthewaythesefeaturesaremodeled.
 forts(Poesioetal.2004)thatfocusonlinguisticaspectsofparameterization.Becausewe areinterestedinanautomaticallyconstructedmodel,wehavetotakeintoaccountcom-putationalandlearningissueswhenconsideringalternativerepresentations.Therefore, ourexplorationoftheparameterspaceisguidedbythreeconsiderations:thelinguistic importanceofaparameter,theaccuracyofitsautomaticcomputation,andthesizeofthe resultingfeaturespace.Fromthelinguisticside,wefocusonpropertiesofentitydistri-butionthataretightlylinkedtolocalcoherence,andatthesametimeallowformultiple interpretations during the encoding process. Computational considerations prevent us fro mconsidering discourse representations that cannot be co mputed reliably by exist-ingtools.Forinstance,wecouldnotexperiment withthegranularity ofanutterance X  sentenceversusclause X  X ecauseavailableclauseseparatorsintroducesubstantialnoise intoagridconstruction.Finally,weexcluderepresentationsthatwillexplodethesizeof thefeaturespace,therebyincreasingtheamountofdatarequiredfortrainingthemodel.
Entity Extraction. Theaccuratecomputationofentityclassesiskeytocomputingmean-ingfulentitygrids.Inpreviousimplementationsofentity-basedmodels,classesofcoref-erent nouns have been extracted manually (Miltsakaki and Kukich 2000; Karamanis et al. 2004; Poesio et al. 2004), but this is not an option for our model. An obvious solution for identifying entity classes is to employ an automatic coreference resolution toolthatdetermineswhichnounphrasesrefertothesameentityinadocument.
 of NPs is classified as coreferring or not based on constraints that are learned from an annotated corpus. A separate clustering mechanism then coordinates the possibly contradictory pairwise classifications and constructs a partition on the set of NPs. In our experiments, we employ Ng and Cardie X  X  (2002) coreference resolution system.
The syste mdecides whether two NPs are coreferent by exploiting a wealth of lexical, grammatical,semantic,andpositionalfeatures.ItistrainedontheMUC(6 X 7)datasets andyieldsstate-of-the-artperformance(70.4F-measureonMUC-6and63.4onMUC-7). 8 sonably successful X  X tate-of-the-art coreference tools today reach an F-measure 70% when trained on newspaper texts X  X t is unrealistic to assume that such tools will be readily available for different domains and languages. We therefore consider an additionalapproachtoentityextractionwhereentityclassesareconstructedsimplyby clustering nouns on the basis of their identity. In other words, each noun in a text cor-respondstoadifferententityinagrid,andtwonounsareconsideredcoreferentonlyif theyareidentical.Underthisview Microsoft Corp. fromTable2(Sentence1)corresponds to two entities, Microsoft and Corp. , which are in turn distinct from the company .This approach is only a rough approximation to fully fledged coreference resolution, but it is simple from an implementational perspective and produces consistent results across domainsandlanguages.

Grammatical Function. Severalentity-basedapproachesassertthatgrammaticalfunction isindicativeofanentity X  X prominenceindiscourse(Hudson,Tanenhaus,andDell1986;
Kameyama 1986; Brennan, Friedman, and Pollard 1987; Grosz, Joshi, and Weinstein 1995).Mosttheoriesdiscriminatebetweensubject,object,andtheremaininggrammati-calroles:subjectsarerankedhigherthanobjects,andthesearerankedhigherthanother grammaticalfunctions.
 modifying how transitions are represented in the entity grid. In syntactically aware grids, transitions are expressed by four categories: s , o , x and  X  , whereas in simplified grids,weonlyrecordwhetheranentityispresent( x )orabsent(  X  )inasentence. structureforeachsentence,fromwhichsubjects( s ),objects( o ),andrelationsotherthan subject or object ( x ) are identified. The phrase-structure output of Collins X  X  parser is transformed into a dependency tree from which grammatical relations are extracted.
Passive verbs are recognized using a small set of patterns, and the underlying deep grammatical role for arguments involved in the passive construction is entered in the grid(seethegridcell o for Microsoft ,Sentence2,Table2).Formoredetailsonthegram-maticalrelationsextractioncomponentwerefertheinterestedreadertoBarzilay(2003).
Salience. Centering and other discourse theories conjecture that the way an entity is introducedandmentioneddependsonitsglobalroleinagivendiscourse.Weevaluate theimpactofsalienceinformationbyconsideringtwotypesofmodels:Thefirstmodel treats all entities uniformly, whereas the second one discriminates between transitions ofsaliententitiesandtherest.Weidentifysaliententitiesbasedontheirfrequency, lowingthewidelyacceptedviewthatfrequencyofoccurrencecorrelateswithdiscourse prominence(Givon1987;Ariel1988;Hoey1991;MorrisandHirst1991).
 durebycomputingtransitionprobabilitiesforeachsaliencegroupseparately,andthen combining them into a single feature vector. For n transitions with k salience classes, thefeaturespacewillbeofsize n  X  k .Whilewecaneasilybuildamodelwithmultiple salience classes, we opt for a binary distinction (i.e., k = 2). This is more in line with theoretical accounts of salience (Chafe 1976; Grosz, Joshi, and Weinstein 1995) and results in a moderate feature space for which reliable parameter estimation is possible.
Consideringalargenumberofsalienceclasseswouldunavoidablyincreasethenumber of features. Parameter estimation in such a space requires a large sample of training examplesthatisunavailableformostdomainsandapplications.
 cussed.Ourexperimentswillconsiderseveralmodelswithvaryingdegreesoflinguistic complexity,whileattemptingtostrikeabalancebetweenexpressivityofrepresentation and ease of computation. In the following sections we evaluate their performance on threetasks:sentenceordering,summarycoherencerating,andreadabilityassessment. 3.4 Learning
Equipped with the feature vector representation introduced herein, we can view co-herence assessment as a machine learning problem. When considering text generation applications,itisdesirabletorankratherthanclassifyinstances:Thereisoftennosingle coherent rendering of a given text but many different possibilities that can be partially ordered. It is therefore not surprising that systems often employ scoring functions to select the most coherent output among alternative renderings (see the discussion in
Section 2.2). In this article we argue that encoding texts as entity transition sequences constitutes an appropriate feature set for learning (rather than manually specifying) such a ranking function (see Section 4 for details). We present two task-based exper-iments that put this hypothesis to the test: information ordering (Experiment 1) and summary coherence rating (Experiment 2). Both tasks can be naturally formulated as ranking problems; the learner takes as input a set of alternative renderings of the same document and ranks them based on their degree of local coherence. Examples of such renderings are a set of different sentence orderings of the same text and a set of summaries produced by different systems for the same document. Note that in both ranking experiments we assume that the algorithm is provided with a limited number ofalternatives.Inpractice,thespaceofcandidatescanbevast,andfindingtheoptimal candidatemayrequirepairingourrankingalgorithmwithadecodersimilartotheones usedinmachinetranslation(Germannetal.2004).
 workpreviouslysketched,nothingpreventstheuseofourfeaturevectorrepresentation for conventional classification tasks. We offer an illustration in Experiment 3, where featuresextractedfromentitygridsareusedtoenhancetheperformanceofareadability assessment system. Here, the learner takes as input a set of documents labeled with discrete classes (e.g., denoting whether a text is difficult or easy to read) and learns to makepredictionsforunseeninstances(seeSection6fordetailsonthemachinelearning paradigmweemploy). 4. Experiment 1: Sentence Ordering
Textstructuringalgorithms(Lapata2003;BarzilayandLee2004;Karamanisetal.2004) are commonly evaluated by their performance at information-ordering. The task con-cerns determining a sequence in which to present a pre-selected set of information-10 bearing items; this is an essential step in concept-to-text generation, multi-document summarization, and other text-synthesis problems. The information bearing items can be database entries (Karamanis et al. 2004), propositions (Mellish et al. 1998) or sen-tences(Lapata2003;BarzilayandLee2004).Insentenceordering,adocumentisviewed as a bag of sentences and the algorithm X  X  task is to try to find the ordering which maximizescoherenceaccordingtosomecriterion(e.g.,theprobabilityofanorder). orderings instead of trying to find an optimal ordering. We do not assume that local coherence is sufficient to uniquely determine a maximally coherent ordering X  X ther constraints clearly play a role here. It is nevertheless a key property of well-formed text(documentslackinglocalcoherencearenaturallygloballyincoherent),andamodel which takes it into account should be able to discriminate coherent from incoher-ent texts. In our sentence-ordering task we generate rando mper mutations of a test document and measure how often a permutation is ranked higher than the original document. A non-deficient model should prefer the original text more frequently than itspermutations(seeSection4.2fordetails).
 ordering task. Next, we give details regarding the corpus used for our experiments, describe the methods used for comparison with our approach, and note the evaluation metric employed for assessing model performance. Our results are presented in Sec-tion4.3. 4.1 Modeling
Our training set consists of ordered pairs of alternative renderings ( x document d i , where x ij exhibits a higher degree of coherence than x
Section 4.2 how such training instances are obtained). Without loss of generality, we assume j &gt; k . The goal of the training procedure is to find a parameter vector w that yieldsa X  X ankingscore X  X unctionwhichminimizesthenumberofviolationsofpairwise rankingsprovidedinthetrainingset where ( x ij , x ik )  X  r  X  if x ij is ranked higher than x training data), and  X  ( x ij )and  X  ( x ik ) are a mapping onto features representing the coherence properties of renderings x ij and x ik . In our case the features correspond to the entity transition probabilities introduced in Section 3.2. Thus, the ideal ranking function,representedbytheweightvector w wouldsatisfythecondition
The proble mis typically treated as a Support Vector Machine constraint opti mization problem, and can be solved using the search technique described in Joachims (2002).
This approach has been shown to be highly effective in various tasks ranging from collaborative filtering (Joachims 2002) to parsing (Toutanova, Markova, and Manning 2004). Other discriminative formulations of the ranking problem are possible (Collins 2002;Freundetal.2003);however,weleavethistofuturework. canberankedsimplybycomputingthevalues w  X   X  ( x ij )and w accordingly.Here, w  X  istheoptimizedparametervectorresultingfromtraining. 4.2 Method
Data. To acquire a large collection for training and testing, we create synthetic data, wherein the candidate set consists of a source document and permutations of its sen-tences. This framework for data acquisition enables large-scale automatic evaluation and is widely used in assessing ordering algorithms (Karamanis 2003; Lapata 2003;
Althaus, Karamanis, and Koller 2004; Barzilay and Lee 2004). The underlying assump-tion is that the original sentence order in the source document must be coherent, and so we should prefer models that rank it higher than other permutations. Because we do not know the relative quality of different permutations, our corpus includes only pairwise rankings that comprise the original document and one of its permutations.
Given k original documents, each with n randomly generated permutations, we obtain k  X  n (trivially)annotatedpairwiserankingsfortrainingandtesting.
 newspaper articles and accident reports written by government officials. The first col-lection consists of Associated Press articles fro mthe North A merican News Corpus on the topic of earthquakes (Earthquakes). The second includes narratives fro mthe
National Transportation Safety Board X  X  aviation accident database (Accidents). Both corporahavedocumentsofcomparablelength X  X heaveragenumberofsentencesis10.4 and11.5,respectively.Foreachset,weused100sourcearticleswithupto20randomly generatedpermutationsfortraining. 5 Asimilarmethodwasusedtoobtainthetestdata.
Table4showsthesizeofthetrainingandtestcorporausedinourexperiments.Weheld out 10 documents (i.e., 200 pairwise rankings) from the training data for development purposes.

Features and Parameter Settings. In order to investigate the contribution of linguistic knowledge on model performance we experimented with a variety of grid representa-tionsresultingindifferentparameterizationsofthefeaturespacefromwhichourmodel is learned. We focused on three sources of linguistic knowledge X  X yntax, coreference resolution, and salience X  X hich play a prominent role in entity-based analyses of dis-12 course coherence (see Section 3.3 for details). An additional motivation for our study was to explore the trade-off between robustness and richness of linguistic annotations.
NLP tools are typically trained on human-authored texts, and may deteriorate in per-formancewhenappliedtoautomaticallygeneratedtextswithcoherenceviolations. poverished representations. More concretely, our full model ( Coreference + Syntax +
Salience + ) uses coreference resolution, denotes entity transition sequences via gram-matical roles, and differentiates between salient and non-salient entities. Our less-expressive models (seven in total) use only a subset of these linguistic features during the grid construction process. We evaluated the effect of syntactic knowl-edge by eliminating the identification of grammatical relations and recording solely whether an entity is present or absent in a sentence. This process created a class of four models of the form Coreference[ + /  X  ]Syntax  X  Salience[ + / fully fledged coreference resolution was assessed by creating models where entity classes were constructed simply by clustering nouns on the basis of their identity measuredbycomparingthefullmodelwhichaccountsseparatelyforpatternsofsalient and non-salient entities against models that do not attempt to discriminate between them( Coreference[ + /  X  ]Syntax[ + /  X  ]Salience  X  ).
 to the original text and then generate permutations for the pairwise ranking task. An alternativedesignistoapplycoreferenceresolutiontopermutedtexts.Becauseexisting methodsforcoreferenceresolutiontakeintoconsiderationtheorderofnounphrasesin a text, the accuracy of these tools on permuted sentence sequences is close to random.
Therefore, we opt to resolve coreference within the original text. Although this design hasanoraclefeeltoit,itisnotuncommoninpracticalapplications.Forinstance,intext generation systems, content planners often operate over fully specified semantic rep-resentations, and can thus take advantage of coreference information during sentence ordering.
 specified by two free parameters: the frequency threshold used to identify salient en-titiesandthelengthofthetransitionsequence.Theseparametersweretunedseparately for each data set on the corresponding held-out development set. Optimal salience-based models were obtained for entities with frequency  X  2. The optimal transition lengthwas  X  3. 6 ingandtestingwithallparameterssettotheirdefaultvalues.

Comparison with State-of-the-Art Methods. We compared the performance of our algo-rith magainst two state-of-the-art models proposed by Foltz, Kintsch, and Landauer (1998) and Barzilay and Lee (2004). These models rely largely on lexical information for assessing document coherence, contrary to our models which are in essence un-lexicalized. Recall fro mSection 3 that our approach captures local coherence by mod-eling patterns of entity distribution in discourse, without taking note of their lexical instantiations.Inthefollowingwebrieflydescribethelexicalizedmodelsweemployed inourcomparativestudyandmotivatetheirselection. semantic relatedness between adjacent sentences. The underlying intuition here is that coherent texts will contain a high number of semantically related words. Semantic relatednessiscomputedautomaticallyusingLatentSemanticAnalysis(LSA;Landauer and Dumais 1997) from raw text without employing syntactic or other annotations. In thisframework,aword X  X meaningiscapturedinamulti-dimensionalspacebyavector representing its co-occurrence with neighboring words. Co-occurrence information is collectedinafrequencymatrix,whereeachrowcorrespondstoauniqueword,andeach column represents a given linguistic context (e.g., sentence, document, or paragraph). Foltz, Kintsch, and Landauer X  X  model use singular value decomposition (SVD; Berry,
Dumais, and O X  X rien 1994) to reduce the dimensionality of the space. The transforma-tion renders sparse matrices more informative and can be thought of as a means of uncovering latent structure in distributional data. The meaning of a sentence is next represented as a vector by taking the mean of the vectors of its words. The similarity betweentwosentencesisdeterminedbymeasuringthecosineoftheirmeans: where  X  ( S i ) = 1 | S measure can be easily obtained by averaging the cosines for all pairs of adjacent sen-tences S i and S i + 1 : andhasrelativelyfewparameters(i.e.,thedimensionalityofthespaceandthechoiceof similarityfunction),(b)itcorrelatesreliablywithhumanjudgmentsandhasbeenused to analyze discourse structure, and (c) it models an aspect of local coherence which is orthogonaltoours.TheLSAmodelislexicalized:coherenceamountstoquantifyingthe degreeofsemanticsimilaritybetweensentences.Incontrast,ourmodeldoesnotincor-porate any notion of similarity: coherence is encoded in terms of transition sequences thataredocument-specificratherthansentence-specific.

Landauer (1998). We constructed vector-based representations for individual words from a lemmatized version of the North American News Corpus using a term X  X ocument matrix. We used SVD to reduce the semantic space to 100 dimensionsobtainingthusaspacesimilartoLSA.Weestimatedthecoherenceofadoc-umentusingEquations(1)and(2).Arankingcanbetriviallyinferredbycomparingthe 14 coherencescoreassignedtotheoriginaldocumentagainsteachofitspermutations.Ties areresolvedrandomly.
 transitions without being aware of global document structure. In contrast, the content models developed by Barzilay and Lee (2004) learn to represent more global text prop-erties by capturing topics and the order in which these topics appear in texts fro mthe samedomain.Forinstance,atypicalearthquakenewspaperreportcontainsinformation about the quake X  X  epicenter, how much it measured, the time it was felt, and whether there were any victims or damage. By encoding constraints on the ordering of these topics, content models have a pronounced advantage in modeling document structure because they can learn to represent how documents begin and end, but also how the discourseshiftsfromonetopictothenext.LikeLSA,thecontentmodelsarelexicalized; however, unlike LSA, they are domain-specific, and would expectedly yield inferior performanceonout-of-domaintexts.
 correspondtodistincttopics(forinstance,theepicenterofanearthquakeorthenumber of victims), and state transitions represent the probability of changing from one topic to another, thereby capturing possible topic-presentation orderings within a domain.
Topics refer to text spans of varying granularity and length. Barzilay and Lee used sentencesintheirexperiments,butclausesorparagraphswouldalsobepossible.
 ordering for a document whose sentences had been randomly shuffled. Here, we use content models for the simpler coherence ranking task. Given two text permutations, weestimatetheirlikelihoodaccordingtotheirHMMmodelandselectthetextwiththe highest probability. Because the two candidates contain the same set of sentences, the assumptionisthatamoreprobabletextcorrespondstoanorderingthatismoretypical forthedomainofinterest.
 one for the Earthquake corpus. Although these models are trained in an unsupervised fashion, a number of parameters related to the model topology (i.e., number of states and smoothing parameters) affect their performance. These parameters were tuned on the development set and chosen so as to optimize the models X  performance on the pairwiserankingtask.

Evaluation Metric. Given a set of pairwise rankings (an original document and one of its permutations), we measure accuracy as the ratio of correct predictions made by the modeloverthesizeofthetestset.Inthissetup,randompredictionresultsinanaccuracy of50%. 4.3 Results
Impact of Linguistic Representation. We first investigate how different types of linguistic knowledge influence our model X  X  performance. Table 5 shows the accuracy on the or-deringtaskwhenthemodelistrainedondifferentgridrepresentations.Ascanbeseen, in both domains, the full model Coreference + Syntax + Salience + significantly outper-forms a linguistically naive model which simply records the presence (and absence) of entities in discourse ( Coreference  X  Syntax  X  Salience linguistically impoverished models consistently perform worse than their linguisti-cally elaborate counterparts. We assess whether differences in accuracy are statistically significantusingaFisherSignTest.Specifically,wecomparethefullmodelagainsteach ofthelessexpressivemodels(seeTable5).
 edge sources varies across domains. On the Earthquakes corpus every model that does not use coreference information ( Coreference  X  Syntax[ + / forms significantly worse than models augmented with coreference ( Coreference +
Syntax[ + /  X  ]Salience[ + /  X  ] ). This effect is less pronounced on the Accidents corpus, especially for model Coreference  X  Syntax + Salience + whose accuracy drops only by 0.5% (the difference between Coreference  X  Syntax + Salience + and Coreference +
Syntax + Salience + is not statistically significant). The same model X  X  performance de-creases by 4.2% on the Earthquakes corpus. This variation can be explained by differ-encesinentityrealizationbetweenthetwodomains.Inparticular,thetwocorporavary in the amount of coreference they employ; texts from the Earthquakes corpus contain manyexamplesofreferringexpressionsthatoursimpleidentity-basedapproachcannot possibly resolve. Consider for instance the text in Table 6. Here, the expressions the fromtheAccidentscorpuscontainsfewerreferringexpressions,infactentitiesareoften repeated verbati macross several sentences, and therefore could be straightforwardly resolvedwithashallowapproach(see the pilot , the pilot , the pilot inTable6). to the Accidents corpus. This effect is less noticeable on the Earthquakes corpus (com-pare the performance of model Coreference + Syntax  X  Salience + on the two corpora).
We explain this variation by the substantial difference in the type/token ratio between the two domains X 12.1 for Earthquakes versus 5.0 for Accidents. The low type/token ratio for Accidents means that most sentences in a text have some words in common.
For example, the entities pilot , airplane ,and airport appear in multiple sentences in the text from Table 6. Because there is so much repetition in this domain, the syntax-free gridswillberelativelysimilarforbothcoherent(original)andincoherenttexts(permu-tations).Infact,inspectionofthegridsfromtheAccidentscorpusrevealsthattheyhave many sequences of the form [ XXX ] , [ X  X  X  X  X ] , [ XX  X  X  X  16 whereas such sequences are more common in coherent Earthquakes documents and more sparse in their permutations. This indicates that syntax-free analysis can suffi-ciently discriminate coherent from incoherent texts in the Earthquakes domain, while a more refined representation of entity transition types is required for the Accidents domain.
 ence in performance between the full model ( Coreference + Syntax + Salience + )and its salience-agnostic counterpart ( Coreference + Syntax + Salience + ) is not statisti-cally significant. Salience-based models do deliver some benefits for linguistically impoverished models X  X or instance, Coreference  X  Syntax  X  Salience + improves over
Coreference  X  Syntax  X  Salience  X  (p &lt; 0.06)ontheEarthquakescorpus.Wehypothesize that the small contribution of salience is related to the way it is currently represented.
Addition of this knowledge source to our grid representation, doubles the number of features that serve as input to the learning algorithm. In other words, salience-aware models need to learn twice as many parameters as salience-free models, while havingaccesstothesameamountoftrainingdata.Achievinganyimprovementinthese conditionsischallenging.

Comparison with State-of-the-Art Methods. WenextdiscusstheperformanceoftheHMM-based content models (Barzilay and Lee 2004) and LSA (Foltz, Kintsch, and Landauer 1998)incomparisontoourmodel( Coreference + Syntax + Salience + ). mains ( p &lt; .01 using a Sign test, see Table 5). In contrast to our model, LSA is nei-ther entity-based nor unlexicalized: It measures the degree of semantic overlap across successive sentences, without handling discourse entities in a special way (all content wordsinasentencecontributetowardsitsmeaning).Weattributeourmodel X  X superior performance, despite the lack of lexicalization, to three factors: (a) the use of more elaborate linguistic knowledge (coreference and grammatical role information); (b) a moreholisticrepresentationofcoherence(recallthatourentitygridsoperateovertexts rather than individual sentences; furthermore, entity transitions can span more than two consecutive sentences, something which is not possible with the LSA model); and (c)exposuretodomainrelevanttexts(theLSAmodelusedinourexperimentswasnot particularly tuned to the Earthquakes or Accidents corpus). Our semantic space was created fro ma large news corpus (see Section 4.2) covering a wide variety of topics andwritingstyles.Thisisnecessaryforconstructingrobustvectorrepresentationsthat are not extremely sparse. We thus expect the grid models to be more sensitive to the discourseconventionsofthetraining/testdata.
 theEarthquakescorpus(thedifferenceisnotstatisticallysignificant)butissignificantly lower on the Accidents texts (see Table 5). Although the grid model yields similar performanceonthetwodomains,contentmodelsexhibithighvariability.Theseresults are not surprising. The analysis presented in Barzilay and Lee (2004) shows that the
Earthquakes texts are quite formulaic in their structure, following the editorial style of the Associated Press. In contrast, the Accidents texts are more challenging for content models X  X eports in this set do not undergo centralized editing and therefore exhibit more variability in lexical choice and style. The LSA model also significantly outper-formsthecontentmodelontheEarthquakesdomain( p &lt; .01usingaSigntest).Beinga local model, LSA is less sensitive to the way documents are structured and is therefore morelikelytodeliverconsistentperformanceacrossdomains.
 end of the spectru mis LSA, a lexicalized model of local discourse coherence which is fairly robust and domain independent. In the middle of the spectrum lies our entity-grid model, which is unlexicalized but linguistically informed and goes beyond sim-ple sentence-to-sentence transitions without, however, fully modeling global discourse structure. AttheotherendofthespectrumaretheHMM-basedcontentmodels,which are both global and lexicalized. Our results indicate that these models are complemen-tary and that their combination could yield improved results. For example, we could lexicalizeourentitygridsorsupplythecontentmodelswithlocalinformationeitherin thestyleofLSAorasentitytransitions.However,weleavethistofuturework.

Training Requirements. Wenowexamineinmoredetailthetrainingrequirementsforthe entity-grid models. Although for our ordering experiments we obtained training data cheaply, this will not generally be the case and some effort will have to be invested in collecting appropriate data with coherence ratings. We thus address two questions: (1)Howmuchtrainingdataisrequiredforachievingsatisfactoryperformance?(2)How domain sensitive are the entity-grid models? In other words, does their performance degradegracefullywhenappliedtoout-of-domaintexts?
Syntax + Salience + ) on the Earthquakes and Accidents corpora. We observe that the amountofdatarequireddependsonthedomainathand.TheAccidentstextsaremore repetitiveandthereforelesstrainingdataisrequiredtoachievegoodperformance.The 18 learning curve is steeper for the Earthquakes documents. Irrespective of the domain differences, the model reaches good accuracies when half of the data set is used (1,000 pairwise rankings). This is encouraging, because for some applications (e.g., summa-rization)largeamountsoftrainingdatamaybenotreadilyavailable.

Syntax + Salience + when trained on the Earthquakes corpus and tested on Accidents texts and reversely when trained on the Accident corpus and tested on Earthquakes documents. We also illustrate how this model performs when trained and tested on a data set that contains texts fro mboth do mains. For the latter experi ment the train-ing data set was created by randomly sampling 50 Earthquakes and 50 Accidents documents.
 (approximately by 20%) when tested on out-of-domain texts. On the positive side, the model X  X  out-of-domain performance is better than chance (i.e., 50%). Furthermore, once the model is trained on data representative of both domains, it performs almost as well as a model which has been trained exclusively on in-domain texts (see the row EarthAccid in Table 7). To put these results into context, we also considered the cross-domain performance of the content models. As Table 7 shows, the decrease in performance is more dramatic for the content models. In fact, the model trained on the Earthquakes domain plummets below the random baseline when applied to the
Accidents domain. These results are expected for content models X  X he two domains have little overlap in topics and do not share structural constraints. Note that the LSA model is not sensitive to cross-domain issues. The semantic space is constructed over manydifferentdomainswithouttakingintoaccountstyleorwritingconventions.
 these models are not lexicalized, and one would expect that valid entity transitions are preserved across domains. Although transition types are not domain-specific, their distribution could vary from one domain to another. To give a simple example, some domains will have more entities than others (e.g., descriptive texts). In other words, entity transitions capture not only text coherence properties, but also reflect stylistic and genre-specific discourse properties. This hypothesis is indirectly confirmed by the observed differences in the contribution of various linguistic features across the two domainsdiscussedabove.Cross-domaindifferencesinthedistributionandoccurrence of entities have been also observed in other empirical studies of local coherence. For instance, Poesio et al. (2004) show differences in transition types between instructional texts and descriptions of museum texts. In Section 6, we show that features derived fro mthe entity grid help deter mine the readability level for a given text, thereby verifying more directly the hypothesis that the grid representation captures stylistic discoursefactors.
 domain would involve some effort in collecting representative texts with associated coherence ratings. Thankfully, the entity grids are constructed in a fully automatic fashion, without requiring manual annotation. This contrasts with traditional imple-mentations of Centering Theory that operate over linguistically richer representations thataretypicallyhand-coded. 5. Experiment 2: Summary Coherence Rating
We further test the ability of our method to assess coherence by comparing model inducedrankingsagainstrankingselicitedbyhumanjudges.Admittedly,thesynthetic data used in the ordering task only partially approximates coherence violations that human readers encounter in machine generated texts. A representative example of such textsare automatically generated summaries whichoftencontain sentences taken out of context and thus display problems with respect to local coherence (e.g., dan-gling anaphors, thematically unrelated sentences). A model that exhibits high agree-ment with human judges not only accurately captures the coherence properties of the summaries in question, but ultimately holds promise for the automatic evaluation of machine-generated texts. Existing automatic evaluation measures such as BLEU (Papineni et al. 2002) and ROUGE (Lin and Hovy 2003) are not designed for the coherence assessment task, because they focus on content similarity between system outputandreferencetexts. 20 5.1 Modeling
Summary coherence rating can be also formulated as a ranking learning task. We are assuming that the learner has access to several summaries corresponding to the same document or document cluster. Such summaries can be produced by several systems thatoperateoveridenticalinputsorbyasinglesystem(e.g.,byvaryingthecompression length or by switching on or off individual system modules, for example a sentence compression or anaphora resolution module). Similarly to the sentence ordering task, our training data includes pairs of summaries ( x ij , x ik where x ij is more coherent than x ik . An optimal learner should return a ranking r orders the summaries according to their coherence. As in Experiment 1 we adopt an optimizationapproachandfollowthetrainingregimeputforwardbyJoachims(2002). 5.2 Method
Data. Our evaluation was based on materials from the Document Understanding Con-ference (DUC 2003), which include multi-document summaries produced by human writers and by automatic summarization systems. In order to learn a ranking, we require a set of summaries, each of which has been rated in terms of coherence. One stumbling block to performing this kind of evaluation is the coherence ratings them-selves,whicharenotroutinelyprovidedbyDUCsummaryevaluators.InDUC2003,the quality of automatically generated summaries was assessed along several dimensions ranging from grammatically, to content selection, fluency, and readability. Coherence was indirectly evaluated by noting the number of sentences indicating an awkward time sequence, suggesting a wrong cause X  X ffect relationship, or being semantically incongruent with their neighboring sentences. 8 Unfortunately, the observed coherence violations were not fine-grained enough to be of use in our rating experiments. In the majority of cases DUC evaluators noted either 0 or 1 violations; however, without judging the coherence of the summary as a whole, we cannot know whether a single violationdisruptscoherenceseverelyornot.
 mansubjects. 9 Werandomly selected16inputdocument clusters andfivesystemsthat had produced summaries for these sets, along with reference summaries composed by humans. Coherence ratings were collected during an elicitation study by 177 unpaid volunteers, all native speakers of English. The study was conducted remotely over the
Internet. Participants firstsawaset ofinstructions that explained thetask, anddefined the notion of coherence using multiple examples. The summaries were randomized in lists following a Latin square design ensuring that no two summaries in a given list weregeneratedfromthesamedocumentcluster.Participantswereaskedtouseaseven-point-scale to rate how coherent the summaries were without having seen the source texts.Theratings(approximately23persummary)givenbyoursubjectswereaveraged toprovidearatingbetween1and7foreachsummary.
 performed several tests to validate the quality of the annotations. First, we measured how well humans agree in their coherence assessment. We employed leave-one-out resampling 10 (Weiss and Kulikowski 1991), by correlating the data obtained fro meach participant with the mean coherence ratings obtained from all other participants. The inter-subjectagreementwas r = .768( p &lt; .01.)Second,weexaminedtheeffectofdiffer-enttypesofsummaries(human-vs.machine-generated.)AnA NOVA revealedareliable effect of summary type: F (1;15) = 20 . 38, p &lt; .01 indicating that human summaries are perceived as significantly more coherent than system-generated ones. Finally, we also compared the elicited ratings against the DUC evaluations using correlation analysis.
Thehumanjudgmentswerediscretizedtotwoclasses(i.e.,0or1)usingentropy-based discretization (Witten and Frank 2000). We found a moderate correlation between the human ratings and DUC coherence violations ( r = .41, p &lt; .01). This is expected given that DUC evaluators were using a different scale and and were not explicitly assessing summarycoherence.
 used for the development of our entity-based coherence models. To increase the size of our training and test sets, we augmented the materials used in the elicitation study with additional DUC summaries generated by humans for the same input sets. We assumedthatthesesummariesweremaximallycoherent.Asmentionedpreviously,our participants tend to rate human-authored summaries higher than machine-generated ones.Toensurethatwedonottuneamodeltoaparticularsystem,weusedtheoutput summaries of distinct systems for training and testing. Our set of training materials contained6  X  16summaries(averagelength4.8),yielding 6 2  X  ings.Becausehumansummariesoftenhaveidentical(high)scores,weeliminatedpairs of such summaries from the training set. Consequently, the resulting training corpus consisted of 144 summaries. In a similar fashion, we obtained 80 pairwise rankings for thetestset.Sixdocumentsfromthetrainingdatawereusedasadevelopmentset.

Features, Parameter Settings, and Training Requirements. We examine the influence of lin-guistic knowledge on model performance by comparing models with varying degrees oflinguisticcomplexity.Tobeabletoassesstheperformanceofourmodelsacrosstasks (e.g., sentence ordering vs. summarization), we experimented with the same model types introduced in the previous experiment (see Section 4.3). We also investigate the trainingrequirementsforthesemodelsonthesummarycoherencetask.
 was obtained. In Experiment 1, a coreference resolution tool was applied to human-written texts, which are grammatical and coherent. Here, we apply a coreference tool to automatically generated summaries. Because many summaries in our corpus are fraught with coherence violations, the performance of a coreference resolution tool is likely to drop. Unfortunately, resolving coreference in the input documents would requireamulti-documentcoreferencetool,whichiscurrentlyunavailabletous.
 quence were optimized on the development set. Optimal salience-based models were obtained for entities with frequency  X  2. The optimal transition length was modelsweretrainedandtestedusingSVM light (Joachims2002).

Comparison with State-of-the-Art Methods. Our results were compared to the LSA model introduced in Experiment 1 (see Section 4.2 for details). Unfortunately, we could not 22 employ Barzilay and Lee X  X  (2004) content models for the summary ranking task. Being domain-dependent,thesemodelsrequireaccesstodomainrepresentativetextsfortrain-ing.Oursummarycorpus,however,containstextsfrommultipledomainsanddoesnot provideanappropriatesampleforreliablytrainingcontentmodels. 5.3 Results
Impact of Linguistic Representation. Our results are summarized in Table 8. Similarly to the sentence ordering task, we observe that the linguistically impoverished model
Coreference  X  Syntax  X  Salience  X  exhibits decreased accuracy when compared against modelsthatoperateovermoresophisticatedrepresentations.However,thecontribution of individual knowledge sources differs in this task. For instance, coreference resolu-tion improved model performance in ordering, but it causes a decrease in accuracy in summary evaluation (compare the models Coreference + Syntax + Salience + and
Coreference  X  Syntax + Salience + in Tables 5 and 8). This drop in performance can be attributed to two factors both related to the fact that our summary corpus contains many machine-generated texts. First, an automatic coreference resolution tool will be expected to be less accurate on our corpus, because it was trained on well-formed human-authoredtexts.Second,automaticsummarizationsystemsdonotuseanaphoric expressionsasoftenashumansdo.Therefore,asimpleentityclusteringmethodismore suitableforautomaticsummaries.
 model. The impact of each of these knowledge sources in isolation is not dramatic X  dropping either of the myields so me decrease in accuracy, but the difference is not sta-tistically significant. However, eliminating both salience and syntactic information sig-nificantly decreases performance (compare Coreference  X  Syntax + Salience + against
Coreference + Syntax  X  Salience  X  and Coreference  X  Syntax
Salience + .Althoughthemodelperformspoorlywhentrainedonasmallfractionofthe data,itstabilizesrelativelyfast(with80pairwiserankings),anddoesnotimproveafter a certain point. These results suggest that further improvements to summary ranking areunlikelytocomefromaddingmoreannotateddata.

Comparison with the State-of-the-Art. As in Experiment 1, we compared the best per-forming grid model ( Coreference  X  Syntax + Salience + ) against LSA (see Table 8). The formermodelsignificantlyoutperformsthelatter( p &lt; .01)byawidemargin.LSAisper-hapsatadisadvantageherebecauseithasbeenexposedonlytohuman-authoredtexts.
Machine-generated summaries are markedly distinct from human texts even when these are incoherent (as in the case of our ordering experiment). For example, manual inspection of our summary corpus revealed that low-quality summaries often contain repetitiveinformation.Insuchcases,simplyknowingabouthighcross-sententialover-lapisnotsufficienttodistinguisharepetitivesummaryfromawell-formedone.
 being ranked here differ in lexical choice. Some are written by humans (and are thus abstracts), whereas others have been produced by systems following different summa-rization paradigms (some systems perform rewriting whereas others extract sentences verbatim from the source documents). This means that LSA may consider a summary coherent simply because its vocabulary is familiar (i.e., it contains words for which reliable vectors have been obtained). Analogously, a summary with a large number of out-of-vocabulary lexical items will be given low similarity scores, irrespective of whether it is coherent or not. This is not uncommon in summaries with many proper names.TheseoftendonotoverlapwiththepropernamesfoundintheNorthAmerican
News Corpus used for training the LSA model. Lexical differences exert much less influenceontheentity-gridmodelwhichabstractsawayfromalternativeverbalizations ofthesamecontentandcapturescoherencesolelyonthebasisofgridtopology. 6. Experiment 3: Readability Assessment
So far, our experiments have explored the potential of the proposed discourse repre-sentation for coherence modeling. We have presented several classes of grid models 24 achieving good performance in discerning coherent from incoherent texts. Our experi-ments also reveal a surprising property of grid models: Even though these models are notlexicalized,theyaredomain-andstyle-dependent.Inthissection,weinvestigatein detail this feature of grid models. Here, we move away from the coherence rating task and put the entity-grid representation further to the test by examining whether it can beusefullyemployedinstyleclassification.Specifically,weembedourentitygridsinto a syste mthat assesses docu ment readability. The ter mdescribes the ease with which a document can be read and understood. The quantitative measurement of readability hasattractedconsiderableinterestanddebateoverthelast70years(seeMitchell[1985] andChall[1958]fordetailedoverviews)andhasrecentlybenefitedfromtheuseofNLP technology(SchwarmandOstendorf2005).
 assessing whether texts or books are suitable for students at particular grade levels or ages. Many readability methods focus on simple approximations of semantic factors concerning the words used and syntactic factors concerning the length or structure of sentences (Gunning 1952; Kincaid et al. 1975; Chall and Dale 1995; Stenner 1996; Katz and Bauer 2001). Despite their widespread applicability in education and technical writing (Kincaid et al. 1981), readability formulas are often criticized for being too simplistic;theysystematicallyignoremanyimportantfactorsthataffectreadabilitysuch as discourse coherence and cohesion, layout and formatting, use of illustrations, the natureofthetopic,thecharacteristicsofthereaders,andsoforth.
 addresses some of the shortcomings of previous approaches. By recasting readability assessment asaclassification task, theyare abletocombine several knowledge sources ranging from traditional reading level measures, to statistical language models, and syntactic analysis. Evaluation results show that their system outperforms two com-monlyusedreadinglevelmeasures(theFlesch-KincaidGradeLevelindexandLexile).
In the following we build on their approach and examine whether the entity-grid rep-resentationintroducedinthisarticlecontributestothereadabilityassessmenttask.The incorporationofcoherence-basedinformationinthemeasurementoftextreadabilityis, toourknowledge,novel. 6.1 Modeling
WefollowSchwarmandOstendorf(2005)intreatingreadabilityassessmentasaclassifi-cationtask.Theunitofclassificationisasinglearticleandthelearner X  X taskistopredict whether it is easy or difficult to read. A variety of machine learning techniques are amenable tothisproblem. Because our goal wastoreplicate Schwarm and Ostendorf X  X  syste mas closely as possible, we followed their choice of support vector machines (SVMs)(Joachims 1998b) forour classification experiments. Our training sample there-foreconsistedof n documentssuchthat where x i is a feature vector for the i th document in the training sample and y (positive or negative) class label. In the basic SVM framework, we try to separate the positive and negative instances by a hyperplane. This means that there is a weight vector w and a threshold b , so that all positive training examples are on one side of the hyperplane,whileallnegativeoneslieontheotherside.Thisisequivalenttorequiring efficiently using the procedure described in Vapnik (1998). SVMs have been widely usedformanyNLPtasksrangingfromtextclassification(Joachims1998b),tosyntactic chunking (Kudo and Matsumoto 2001), and shallow semantic parsing (Pradhan et al. 2005). 6.2 Method
Data. For our experiments we used a corpus collected by Barzilay and Elhadad (2003) fro mthe Encyclopedia Britannica and Britannica Elementary . The latter is a new version targetedatchildren.Thecorpuscontains107articlesfromthefullversionoftheencyclo-pediaandtheircorrespondingsimplifiedarticlesfrom Britannica Elementary (214articles in total). Although these texts are not explicitly annotated with grade levels, they still represent two broad readability categories, namely, easy and difficult. thesetwocategoriesaregiveninTable9. 26 Features and Parameter Settings. Wecreatedtwosystemversions:thefirstoneusedsolely
Schwar mand Ostendorf (2005) features; 12 the second one employed a richer feature space X  X eaddedtheentity-basedrepresentationproposedheretotheiroriginalfeature set. We will briefly describe the readability-related features used in our systems and directtheinterestedreadertoSchwarmandOstendorfforamoredetaileddiscussion. mantic,andtheircombination.Theirsyntacticfeaturesareaveragesentencelengthand featuresextractedfromparsetreescomputedusingCharniak X  X (2000)parser.Thelatter includeaverageparsetreeheight,averagenumberofNPs,averagenumberofVPs,and averagenumberofsubordinateclauses(SBARs).Wecomputedaveragesentencelength bymeasuringthenumberoftokenspersentence.
 language model perplexity scores. A unigram, bigram, and trigram model was esti-mated for each class, and perplexity scores were used to assess their performance on test data. Following Schwar mand Ostendorf (2005) we used infor mation gain to select words that were good class discriminants. All remaining words were replaced by their parts of speech. The vocabulary thus consisted of 300 words with high information gain and 36 Penn Treebank part-of-speech tags. The language models were estimated using maximum likelihood estimation and smoothed with Witten-Bell discounting.
The language models described in this article were all built using the CMU statistical language modeling toolkit (Clarkson and Rosenfeld 1997). Our perplexity scores were sixintotal(2classes  X  3languagemodels).
 turesbothsyntacticandsemantictextproperties.TheFlesch-Kincaidformulaestimates readability as a combination of the the average number of syllables per word and the averagenumberofwordspersentence: basedfeatures.Eachdocumentwasrepresentedasafeaturevectorusingtheentitytran-sitionnotationintroducedinSection3.Weexperimentedwithtwomodelsthatyielded goodperformancesinourpreviousexperiments: Coreference + Syntax + Salience + (see
Experiment1)and Coreference  X  Syntax + Salience + (seeExperiment2).Thetransition lengthwas  X  2andentitieswereconsideredsalientiftheyoccurred previous experiments, we compared the entity-based representation against LSA. The latter is a measure of the semantic relatedness across pairs of sentences. We could not apply the HMM-based content models (Barzilay and Lee 2004) to the readability data set. The encyclopedia lemmas are written by different authors and consequently vary considerably in structure and vocabulary choice. Recall that these models are suitable formorerestricteddomainsandtextsthataremoreformulaicinnature. foldcross-validation. 13 Thelanguagemodelswerecreatedanewforeveryfoldusingthe documents inthetrainingdata. WeuseJoachims X  (1998a) SVM light andtestingwithallparameterssettotheirdefaultvalues.

Evaluation Metric. We measure classification accuracy (i.e., the number of classes as-signed correctly by the SVM over the size of the test set). We report accuracy averaged overfolds.Achancebaseline(selectingoneclassatrandom)yieldsanaccuracyof50%.
Our training and test sets have the same number of documents for the two readability categories. 6.3 Results
Table 10 summarizes our results on the readability assessment task. We first com-pared Schwar mand Ostendorf X  X  (2005) syste magainst a syste mthat incorporates entity-based coherence features (see rows 3 X 4 in Table 10). As can be seen, the sys-tem X  X  accuracy significantly increases by 10% when the full feature set is included ( Coreference + Syntax + Salience + ).Entity-gridfeaturesthatdonotincorporatecorefer-ence information ( Coreference  X  Syntax + Salience + ) perform numerically better (com-parerow1and3inTable10);however,thedifferenceisnotstatisticallysignificant. entirelyunexpected.Inspectionofourcorpusrevealedthateasyanddifficulttextsdiffer in their distribution of pronouns and coreference chains in general. Easy texts tend to employ less coreference and the use of personal pronouns is relatively sparse. To give a concrete example, the pronoun they is attested 173 times in the difficult corpus and only 73 in the easy corpus. This observation suggests that coreference information is a goodindicatorofthelevelofreadingdifficultyandexplainswhyitsomissionfromthe entity-basedfeaturespaceyieldsinferiorperformance. 28
Ostendorf X  X  (2005) original model. The latter employs a large number of lexical and syntactic features which capture sentential differences among documents. Our entity-basedrepresentationsupplementstheirfeaturespacewithinformationspanningtwoor moresuccessivesentences.Wethusareabletomodelstylisticdifferencesinreadability that go beyond syntax and lexical choice. Besides coreference, our feature representa-tion captures important information about the presence and distribution of entities in discourse. For example, difficult texts tendtohave twice as many entities as easy ones.
Consequently, easy and difficult texts are represented by entity transition sequences with different probabilities (e.g., the sequences [ SS ] and [ difficult texts). Interestingly, when coherence is quantified using LSA, we observe no improvement to the classification task. The LSA scores capture lexical or semantic text properties similar to those expressed by the Flesch Kincaid index and the perplexity scores (e.g., word repetition). It is therefore not surprising that their inclusion in the featuresetdoesnotincreaseperformance.
 herein. Figure 3 shows the learning curve for Schwar mand Ostendorf X  X  (2005) model enhanced with the Coreference + Syntax + Salience + feature space and on its own. As can be seen, both models perform relatively well when trained on small data sets (e.g., 20 X 40 documents) and reach peak accuracy with half of the training data. The inclusionofdiscourse-basedfeaturesconsistentlyincreasesaccuracyirrespectiveofthe amountoftrainingdataavailable.Figure3thussuggeststhatbetterfeatureengineering islikelytobringfurtherperformanceimprovementsonthereadabilitytask.
 tures aspects of text readability and can be successfully incorporated into a practical system. Coherence is by no means the sole predictor of readability. In fact, on its own, it performs poorly on this task as demonstrated when using either LSA or the entity-based feature space without Schwar mand Ostendorf X  X  (2005) features (see rows 5 X 7 in
Table 10). Rather, we claim that coherence is one among many factors contributing to text readability and that our entity-grid representation is well-suited for text classifica-tiontaskssuchasreadinglevelassessment.
 7. Discussion and Conclusions
In this article we proposed a novel framework for representing and measuring text co-herence.Centraltothisframeworkistheentity-gridrepresentationofdiscourse,which we argue captures important patterns of sentence transitions. We re-conceptualize co-herence assessment as a learning task and show that our entity-based representation is well-suited for ranking-based generation and text classification tasks. Using the proposed representation, we achieve good performance on text ordering, summary coherenceevaluation,andreadabilityassessment.
 investigated three important parameters for grid construction: the computation of coreferring entity classes, the inclusion of syntactic knowledge, and the influence of salience. All these knowledge sources figure prominently in theories of discourse (see Section 2) and are considered important in determining coherence. Our results empirically validate the importance of salience and syntactic information (expressed by
S , O , X ,and  X  ) for coherence-based models. The combination of both knowledge sources (Syntax + Salience) yields models with consistently good performance for all ourtasks.
 mismatches between training and testing conditions. The system we employ (Ng and
Cardie 2002) was trained on human-authored newspaper texts. The corpora we used inoursentenceorderingandreadabilityassessmentexperimentsaresomewhatsimilar (i.e., human-authored narratives), whereas our summary coherence rating experiment employed machine generated texts. It is therefore not surprising that coreference reso-lutiondeliversperformancegainsonthefirsttwotasksbutnotonthelatter(seeTable5 in Section 4 and Table 10 in Section 6.3). Our results further show that in lieu of an automatic coreference resolution system, entity classes can be approximated simply by string matching. The latter is a good indicator of nominal coreference; it is often included as a feature in machine learning approaches to coreference resolution (Soon,
Ng, and Li m2001; Ng and Cardie 2002) and is relatively robust (i.e., likely to deliver consistentresultsinthefaceofdifferentdomainsandgenres).
 coherence, our approach is not a direct implementation of any theory in particular.
Rather, we sacrifice linguistic faithfulness for automatic computation and breadth of coverage.Despiteapproximationsandunavoidableerrors(e.g.,intheparser X  X output), our results indicate that entity grids are a useful representational framework across tasks and text genres. In agreement with Poesio et al. (2004) we find that pronomi-nalization is a good indicator of document coherence. We also find that coherent texts are characterized by transitions with particular properties which do not hold for all discourses. Contrary to Centering Theory, we remain agnostic to the type of transi-tions that our models capture (e.g., CONTINUE , SHIFT ). We simply record whether an entity is mentioned in the discourse and in what grammatical role. Our experiments quantitatively measured the predictive power of various linguistic features for several coherence-relatedtasks.Crucially,wefindthatourmodelsaresensitivetothedomainat handandthetypeoftextsunderconsideration(human-authoredvs.machinegenerated texts). This is an unavoidable consequence of the grid representation, which is entity-specific.Differencesinentitydistributionindicatenotonlydifferencesincoherence,but also in writing conventions and style. Similar observations have been made in other workwhichiscloserinspirittoCentering X  X claims(Hasler2004;Karamanisetal.2004;
Poesioetal.2004). 30 with more fine-grained lexico-semantic knowledge. One way to achieve this goal is to cluster entities based on their semantic relatedness, thereby creating a grid represen-tation over lexical chains (Morris and Hirst 1991). An entirely different approach is to develop fully lexicalized models, akin to traditional language models. Cache language models (Kuhn and De Mori 1990) seem particularly promising in this context. The granularityofsyntacticinformationisanothertopicthatwarrantsfurtherinvestigation.
So far we have only considered the contribution of  X  X ore X  grammatical relations to thegridconstruction.Expandingourgrammaticalcategoriestomodifiersandadjuncts mayprovideadditionalinformation,inparticularwhenconsideringmachinegenerated texts. We also plan to investigate whether the proposed discourse representation and modeling approaches generalize across different languages. For instance the identifi-cation and extraction of entities poses additional challenges in grid construction for
Chinese where word boundaries are not denoted orthographically (by space). Similar challenges arise in German, a language with a large number of inflected forms and productivederivationalprocesses(e.g.,compounding)notindicatedbyorthography. of local coherence, while relational models, such as Rhetorical Structure Theory (Mann and Thomson 1988; Marcu 2000), are used to model the global structure of discourse.
We plan to investigate how to combine the two for improved prediction on both local andgloballevels,withtheultimategoalofhandlinglongertexts.
 Acknowledgments References 32
