 We propose a new model for the guided text summariza-tion task. In this task, it is required that a generated sum-mary covers all the aspects , which are predefined for the topic of the given document cluster; for example, aspects for the topic  X  X ccidents and Natural Disasters X  include WHAT, WHEN, WHERE, WHY, WHO AFFECTED, DAMAGES and COUNTERMEASURES. We use as a scorer for an as-pect, the maximum entropy classifier that predicts whether each sentence reflects the aspect or not. We formalize the coverage of the aspects as a max-min problem, which enables a summary to cover aspects in a well-balanced manner. In the max-min problem, the minimum of the aspect scores is going to be maximized so that the summary contains all the aspects as much as possible. Furthermore, we integrate the model based on the max-min problem with the maximum coverage summarization model, which generates a summary containing as many conceptual units as possible. Through the experiments on benchmark datasets for the guided sum-marization, we show that our model outperforms other ap-proaches in terms of ROUGE-2.
 I.2.7 [ Artifical Intelligence ]: Natural Language Processing X  Text analysis Experimentation, Performance Guided Summarization, Multi-Document Summarization, Op-timization Problem
This work was done while the author was at Tokyo Institute of Technology
The automatic text summarization task is to create a sum-mary, or a short and concise document that describes the content of a given set of documents [4]. The text summariza-tion is an important technique when there is a large amount of text about an event that the reader wants to know about. The task of generating a summary simply from a given set of documents is called the generic summarization .How-ever, there are usually certain pieces of information that we would like to know with respect to the event. If the event is a natural disaster, for example, we would probably like to know when it happened, what damages it caused, who was a ff ected, and so on. Such pieces of information are called aspects . The task of generating a summary containing the aspects is called the guided summarization task , which has been introduced in Text Analysis Conference (TAC) 2010 and 2011 [7, 8].

The purpose of this paper is to propose a guided sum-marization model formalized as an optimization problem. In our model, the coverage of aspects is represented by a max-min problem, which is formulated as an integer lin-ear programming (ILP) problem. The Max-Min model is combined with the maximum coverage model [2], to form a guided summarization model. In the Max-Min model, the minimum score of the aspects is going to be maximized so that the aspects are covered in a well-balanced manner.
In order to see the importance of the guided summariza-tion task, we note that the maximum coverage model may fail to generate an appropriate summary in some situations. In the maximum coverage model, the frequency of each word in the document is often used to determine the weight of word (or conceptual unit). However, the frequency is some-times not su ffi cient for determining its importance when we would like the summary to contain aspects. Consider, for example, when the topic of a document cluster is  X  X ccidents and Natural Disasters X  and one of the aspects is COUN-TERMEASURES, which corresponds to rescue e ff orts. If few documents describe rescue e ff orts, a simple maximum coverage model would fail to cover this aspect since it relies on the word frequency in the document cluster. We there-fore propose a model which is based not solely on the max-imum coverage problem, but also on the max-min problem, in which the balanced coverage of aspects is implemented. To capture the aspects, we use as an aspect scorer the max-imum entropy classifier.

Through the experiments on TAC datasets, we show that our model outperforms other models that ignore the balance of aspects coverage in terms of ROUGE-2.
Aspects are the important guidelines to understand the source documents and they are predefined. For example, when the topic of the source documents is  X  X ccidents and Natural Disasters X , the aspects are listed as below, which are predefined by TAC 1 .
Throughout this paper, we use the aspects which are pre-defined by TAC.
Carbonell and Goldstein [1] used sequential sentence selec-tion in combination with maximal marginal relevance (MMR), which gives penalty to sentences that are similar to the al-ready selected sentences. Since their method is a greedy procedure of selecting sentences and does not measure the goodness of the entire summary, global summarization mod-els based on ILP have recently been studied intensively. ILP is a kind of linear programming problems, in which the val-ues of the variables are constrained to integers. McDonald [5] formulated the text summarization task as an ILP and applied an approximate dynamic programming decoding.
Another well-studied ILP summarization model is the max-imum coverage model, in which a sentence is represented as asetofconceptualunits;forexample,thesentence X  X he man bought a book and read it. X  is represented by the set of words X  X an X ,  X  X uy X , X  X ook X , X  X ead X . The maximum cover-age model covers as many conceptual units as possible using only a small number of sentences. Filatova and Hatzivas-siloglou [2] first formulated the text summarization task as a maximum coverage problem. Takamura and Okumura [9] proposed the augmented maximum coverage model, in which the objective function is a combination of the coverage score and the relevance score to the subject of the document clus-ter.
In order to cover the aspects, Varma et al. [11] manually collected articles from Wikipedia that are relevant to the topic and manually labeled aspects to the first paragraph of each article, because the first paragraphs can usually be regarded as rough summaries of the articles. They then trained a naive Bayes classifier with the labeled data for the http://www.nist.gov/tac/2011/Summarization/Guided-Summ.2011.guidelines.html multi-label classification problem, where the classifier pre-dicts whether each sentence describes the given aspect or not. The importance of the sentence measured by the bi-gram overlap with the reference summary is predicted by the regression model [10]. The features are generic includ-ing the position of the sentence and the document frequency of bigrams included in the sentence. From the top ranked sentences, which are scored by the regression model, they selected the sentence that has the maximum score of the aspect, and is not covered yet. The multi-label classifier predicts the conditional probability for each label and the sum of the conditional probability over classes is always 1. This means that the predicted probability indicates the rel-ative tendency of the sentence being in the class compared with the other classes, but not the absolute tendency with-out regard to the other classes.

Ng et al. [6] used not only the generic features, but also the topic specific features such as the document frequency in the topic, when training the regression model [10]. They then selected sentences with MMR. Although their model does not explicitly take the aspects into consideration, their model performed best in terms of ROUGE-2 at guided sum-marization in TAC 2011.
Takamura and Okumura [9] modeled text summarization as the maximum coverage problem whose objective function is the combination of coverage and relevance. In fact, their model is based on the intuition that the summary should not contain redundant contents, and the sentences to be selected have to be relevant to the main content of the document cluster. On the basis of their model, we propose a couple of models that take aspects into consideration.

In order to calculate the score indicating the aspect cover-age, we use the scorer with maximum entropy method that predicts the conditional probability p aj ,whichindicateshow much the sentence j reflects the aspect a .
 Max-Min Model We added a new term to the objective function of the pre-vious work to cover aspects. Our model is formalized as below: max . (1  X   X  )
Our model is going to maximize the objective function under some constraints. Here, let c i denote 1 if the sum-mary contains conceptual unit i , otherwise 0, s j denote 1 if the summary contains conceptual unit j ,otherwise0, o ij denote 1 if the sentence j contains conceptual unit i ,and w denote the weight of conceptual unit i .Thefirsttermand the second term in Equation (1) correspond to coverage and relevance with the subject of the document cluster, respec-tively. These two terms are linearly combined by parameter  X  . A larger value of  X  indicates more weight on coverage, rather than on relevance. The third term is the proposed term to cover aspects. z denotes the minimum of the scores of aspects in the summary. The score of the aspect a in the summary is expressed as the summation of the scores of the aspect a for sentence j in the summary;
Thus, maximizing z means that the summary contains all the aspects as much as possible. The summary length is at most L and l j is the length of the sentence j where the length means the number of words. We use only sentences with high scores for each aspect as described in Section 4.1. Max-Total Model The second model is formalized as Equation (2). In this model, the sum of all the aspect scores in the summary is maximized. This model takes aspects into consideration, but not their balanced coverage. max . (1  X   X  ) For experiments, we used the dataset of TAC 2010 and TAC 2011, which are the guided summarization task. The datasets consist of 46 and 44 document clusters respectively. Each document cluster is a set of 10 documents. The topics of all document clusters are given. Reference summaries are created with balanced coverage of predefined aspects by hu-man annotators. We set the length of the summary to 100 words following the o ffi cial setting of TAC. ROUGE version 1.5.5 was used for evaluation 2 .Wilcoxonsignedranktest for paired samples with significance level 0.05 was used for the significance test of the di ff erence in ROUGE-2. Since the numbers of the document clusters are small in the two datasets respectively, we only conduct a significance test on the system and baseline using ROUGE-2 scores of all the topics 3 .Weused pyramid file s as the training data for the aspect scorer. Pyramid files are the set of the reference sum-With options: -l 100 -n 4 -w 1.2 -m -2 4 -u -r 1000 -c 95 -f A-p0.5-t0-a-d
In TAC proceedings, there are no papers in which signifi-cance test on each topic is conducted maries labeled with aspects. In this paper, we evaluate the systems in terms of ROUGE-2. ROUGE is usually used for evaluation of the generic summarization. It can also be used for the guided summarization as long as reference summaries are created so that they contain aspects. Therefore the im-provement in terms of ROUGE indicates the better coverage of aspects.

In order to determine the values of  X  and  X  for a document cluster, we used as development data all the other document clusters with the same topic from TAC 2010 and TAC 2011 and  X  and set the parameter values to the pair that maximize the ROUGE-2 score on the development data.

We used the dataset of TAC 2010 for calculating aspect scores for the dataset of TAC 2011, and vice versa. 0.089 0.125 0.078 0.101 0.154 0.109 0.131 0.160 0.120 0.080 0.116 0.123
We show the experimental results in Tables 1 and 2. The values in the tables are ROUGE-2 scores for each system and each dataset. Underlined ROUGE-2 scores are signif-icantly di ff erent from the score of baseline. The values in the parentheses are the 95% confidence interval. Baseline refers to the maximum coverage model proposed by Taka-mura and Okumura [9]. Max-Min refers to the proposed model formalized as Equation (1) and Max-Total refers to the model formalized as Equation (2). Peer 22, the system of Varma et al. [11], and peer 43, the system of Ng et al. [6], performed best in terms of ROUGE-2 respectively in TAC 2010 and TAC 2011. Since peer 22 and peer 43 did not have access to the reference summaries as training data, it is not fair to compare those models with our proposed models.
For both datasets of TAC 2010 and TAC 2011, ROUGE-2 scores of Max-Min are significantly di ff erent from baseline. ROUGE-2 score of Max-Total on the dataset of TAC 2010 is also significantly di ff erent from baseline, but is worse than baseline on the dataset of TAC 2011. Since Max-Total tends to select sentences with high aspect scores, regardless of their aspect types, some summaries generated by Max-Total do not cover all aspects, but only a few aspects. For example, Max-Total selects too many sentences with date expressions, because such sentences are reliably judged to have WHEN aspects. Thus, some summaries of Max-Total have lower ROUGE scores than those of baseline.

In Table 2, the result of our system is worse than the result of peer 43. Peer 43 utilizes the regression model that predicts the bigram overlap with the reference summaries. Although peer 43 does not explicitly take aspects into consideration, this model can predict the importance of bigrams in the topic. We expect that using the regression model of Ng et al. [6] to predict the importance of bigrams in the topic improves the performance of Max-Min.
We show the performance of the scorers in Figure 1. Us-ing cross-validation, the scorers are trained and tested on each dataset. The values in the results are the average. The accuracy of the scorer that performed best is higher than 90% (DAMAGES in  X  X ttacks X ). The reason of this high ac-curacy for DAMAGES is that the sentences in DAMAGES usually include an apparent clue such as the presense of word  X  X amage X . On the other hand, the accuracy of the scorer that performed worst is lower than 60% (COUNTERMEA-SURES in X  X ealth and Safety X ). In fact, no aspect scorers in the topics  X  X ealth and Safety X  and  X  X ndangered Resources X  achieved the accuracy higher than 80%. This di ffi culty in predicting aspects may lead to the low performance in sum-marization for the topics  X  X ealth and Safety X  and  X  X ndan-gered Resources X .
In order to examine the behavior of the proposed summa-rization model, we further conducted experiments using the oracle aspect scorers. However, since the available labeled dataset is pyramid files, which are actually the set of refer-ence summaries labeled with aspects, we cannot directly give the correct aspects to the sentences in the document clus-ters. We therefore train the scorer with the pyramid files (i.e., the reference summaries labeled with aspects) of the document clusters to be summarized; for example, pyramid files of TAC 2010 are used as training data for the scorer of TAC 2010 itself. It is expected that this oracle aspect scorers are very accurate, though not perfect. We show the results in Figure 2. The values in the results are the average.
The results of Max-Min are comparable to those of the oracle as shown in the figures. This is somewhat counterin-tuitive. A possible explanation is that although the oracle scorer accurately calculates the aspect scores, it is not guar-anteed that this aspect in the input sentence is related to the main event of the document cluster. An additional frame-work that guarantees the aspect-event relatedness would fur-ther improve the overall performance, which is left for future work. We also examine the summarization performance of the Max-Min model with parameter  X  being fixed to 1.0. The model therefore does not depend on the maximum coverage part of the objective function. We show the experimental result in Table 3. As shown in the table, ROUGE-2 scores of the Max-Min model with  X  =1 . 0areverylow. The result shows that it is required that the max-min problem should be combined with the maximum coverage problem, as long as the current scorer is used. However, an additional framework that guarantees the aspect-event relatedness in the previous subsection may improve the performance of the max-min problem.
We proposed a new text summarization model that aims at the balanced coverage of aspects. Our model is based on the max-min problem and the maximum coverage model. We showed that the proposed model works better than other related models in terms of ROUGE-2. Our future work in-cludes the following. The proposed model is based on the assumption that the document cluster contains all the as-pects. The proposed model becomes more robust if it can deal with the situation where the document cluster does not contain some aspects.

As discussed in Sections 4.4 and 4.5, an additional frame-work that guarantees the aspect-event relatedness would im-prove the summarization performance. The development of such a framework is an important piece of future work. Al-though we regarded aspects as predefined, there are some studies that focus on the automatic generation of aspects, for example, from Wikipedia [3]. The integration of such methods into our model will broaden the applicability of the proposed approach. [1] J. Carbonell and J. Goldstein. The use of MMR, [2] E. Filatova and V. Hatzivassiloglou. A Formal Model [3] A. Fujii. Modeling Wikipedia Articles to Enhance [4] I. Mani. Automatic Summarization .NaturalLanguage [5] R. McDonald. A study of global inference algorithms [6] J.-P. Ng, P. Bysani, Z. Lin, Y. Kan, Min, and C.-L. [7] K. Owczarzak and H. T. Dang. Overview of TAC 2010 [8] K. Owczarzak and H. T. Dang. Overview of the TAC [9] H. Takamura and M. Okumura. Text Summarization [10] V. Varma, V. Bharath Reddy, S. Bysani, K, P. Bysani, [11] V. Varma, P. Bysani, K. Reddy, V. Bharath Reddy,
