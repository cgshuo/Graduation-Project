 This paper presents a new active learning query strategy for Informativeness (DKI). Active learning is often used to reduce the amount of annotation effort required to obtain training data for machine learning algorithms. A key component of an active iteratively select samples for a nnotation. Knowledge resources have been used in information extraction as a means to derive additional features for sample representation . DKI is, however, the first query strategy that exploits such resources to inform sample selection . To evaluate the merits of DKI, in particular with respect to the reduction in annotation effort that the new query strategy allows to achieve, we conduct a comprehensive empirical comparison of active learning query strategies for information extraction within the clinical domain. The clinical domain was chosen for this work because of the availability of extensive structured knowledge resources which have often been exploited for feature generation. In additi on, the clinical domain offers a compelling use case for active learning because of the necessary high costs and hurdles associated with obtaining annotations in this domain. Our experimental findings demonstrated that (1) amongst existing query strategies, the ones based on the classification model X  X  confidence are a better choice for clinical data as they perform equally well with a much lighter computational load, and (2) signif icant reductions in annotation effort are achievable by exploi ting knowledge resources within active learning query strategies, w ith up to 14% less tokens and concepts to manually annotate than with state-of-the-art query strategies. [ Artificial Intelligence ]: Natural Language Processing  X  text analysis . General Terms Algorithms, Performance, Experimentation. Keywords Domain Knowledge; Active Learning; Concept Extraction; Clinical Free Text; Conditional Random Fields. Supervised machine learning (ML) algorithms can train powerful information from clinical free-text resources [19]. However, these algorithms require a large amount of annotated training data. Preparing high quality annotated data is a major obstacle to effective data analysis in many domains; this is even further exacerbated in the clinical domain as it requires significant efforts from highly trained, and costly , clinical professionals. Active learning (AL) has been sugge sted as an effective solution for reducing the burden of manual annotations in automatic information extraction tasks [26]. In standard supervised learning, training samples are annotated at once. Active learning differs from this because the annotation is performed as an iterative process in which informative samples (i.e., those that contain useful information for the model) are selected from a pool of unlabeled data and provided to experts for annotation. The selection of informative samples takes place according to a query strategy, which thus represents a key component within AL approaches. In each iteration, after samples are selected and refined supervised model is built [26]. It is possible to either fully retrain the model at each step, or incrementally update the model learnt in the preceding loop (see Figure 1). AL approaches vary in the query strategies that are adopted, as well as in the supervised mach ine learning algorithm that is employed throughout the iterati ons; as a result, AL has shown varying performance across different datasets [6]. Despite its practical benefits, active learning is not fully explored in clinical information extraction [19], in particular for the task of clinical concept extraction, which represents the focus of this paper. This task involves captur ing natural language sequences (words or multi-words expressions) belonging to pre-determined semantic categories from unstruc tured free texts. These terms express meaningful concepts with in a given domain (e.g. clinical problems, tests, and treatments) [8, 9]. Extracting clinical concepts is thus an important primary step in identifying meaningful information from clinical free texts. The clinical domain is rich in terms of information resources. MedEx [35], cTAKES [24], and Medtex [18]) have been widely leveraged in combination with dictionary-, rule-and machine learning-based approaches in order to improve clinical concept extraction. The strong need for effective information extraction methods in the clinical domain has encouraged the development of shared datasets such as th e i2b2 challenges [32-34] and the ShARe/CLEF eHealth Evaluation Lab [10, 31], which in turn have sparked the development of novel, more effective clinical information extraction methods. Sp ecifically, a wide attention has been given to the engineering of powerful features obtained from domain knowledge resources to strengthen the supervised machine learning models and increase their effectiveness [21, 34]. While these resources have been used for data representation (i.e., features) both in supervised and active learning approaches [11], there is no study investigating th e contributions these resources could have to improve current st ate-of-the-art query strategies within active learning methods for clinical informa tion extraction. In this paper, we address this gap and propose a novel active Informativeness (DKI), which leverages domain knowledge together with informativeness measures to select those samples that most strengthen the model obtained at the previous active learning iteration. We investigate the comparative performance of DKI and a wide range of state-of-t he-art query strategies in terms of annotation effort savings in th e clinical domain. The clinical domain is chosen for this work because of the already discussed availability of extensive structur ed knowledge resources. It also offers a compelling use case for active learning because of its intrinsic high costs and hurdles associated with obtaining annotations. To the best of our knowledge, DKI is the first query strategy which incorporates domain knowledge when querying samples within the active learning framework. The research questions we seek to tease out in this work are: 1. Which non-knowledge based query strategy is better suited 2. Can domain knowledge support a more effective query We find that, when comparing the current state-of-the-art query strategies, as well as a few novel strategies derived from existing ones for active learning, the confidence about the label of a sample as estimated by the classification model is a key factor for discovering informative samples in the clinical domain. Our findings also suggest that domain knowledge can play an important role for enhancing active learning X  X  performance by further reducing annotation effort. We also bring forward insights about how other machine learni ng approaches, such as semi-supervised learning, can be us ed to augment active learning. the problem definition. Section 3 introduces the query strategies. Section 4 describes our experime ntal and evaluation settings. Results are reported in Section 5. Section 6 briefly reviews related work and Section 7 concludes the paper by outlining directions of future investigation. Concept extraction (often referred to as entity extraction) can be sequence  X (= X  X   X   X ,...,  X  ) needs to be assigned to each observed sequence  X (= X  X   X   X ,...,  X  ) in the dataset. Supervised machine learning models are applied to this task by casting the entity recognition problem to that of es timating the posterior probability of  X  X  given  X  X  under the model parameters  X  . The output sequence with the highest posterior probabilit y is the one that is chosen by the supervised models to a nnotate the input sequence. Active learning approaches use supervised machine learning dataset are successively selected to be annotated by an expert informative if they contain more useful information for the model compared to the rest of the samples in the unlabeled set. The intuition is that identifying and adding informative samples to the labeled set would lead to training a model that would achieve the highest effectiveness. One of the main scenarios in active learning is the pool-based approach (see Figure 2). Under this paradigm, the active learning system has access to a pool of unlabeled data and, based on a query strategy, the system selects a batch of samples within successive interaction loops to training set and used to retrain the model [26]. A core issue when Input:  X  : set of labelled samples  X  : set of unlabelled samples  X  : classifier model  X  X  X   X   X  , X  X  X  : query strategy where  X  X   X   X  X  X   X  : number of samples to be selected in each iteration (batch size) Procedure: 1-Randomly select an initial labeled set  X  2-Train a model  X  on  X  ; 4-Select a batch of samples (  X  ) where  X  X  X  X  X  X   X   X   X   X   X  X  X  5-Add samples from step 4 to labeled set  X  and remove them 6-Repeat step 2 to 5. tokens or a sentence. designing an active learning framewo rk is: what query strategy should be used to estimate the info rmativeness of the samples that will be used to retrain or update the classification model? We discuss query strategies for active learning next. Active learning query strategies fo r sequence labeling tasks can be categorized in 3 groups of approaches: informativeness based, informativeness-similarity based, and model-independent. Within these categories we included a nu mber of variations to commonly used query strategies (ID iv, IDD, MRD, and ALC). In addition, we argue that a fourth group is formed by external knowledge-informed approaches, like DKI. This group of approaches consid ers the uncertainty of a model about the label of a sample as a measure of informativeness. These approaches query samples where the learnt model is most uncertain about their label. Least confidence is a common query strategy for measuring informativeness [5]. This query strategy considers the confidence This confidence is estimated base d on the posterior probability:  X   X  X  (  X   X  , X  )  X 1 X  =  X  (  X   X   X  |  X   X  ) (1) The Viterbi algorithm is used to compute the most likely predicted label sequence  X  X   X  . the most likely (  X  X   X   X  ) and the second most likely (  X  X  sequences [25]. To calculate the margin, the posterior probability of the two most likely labe l sequences is subtracted:  X   X  X  X  X  X  X  X  X  (  X   X  , X  )  X (  X  =  X  (  X   X   X   X  |  X   X  )  X  X   X  (  X   X   X  predict the label of a sample. Hence, according to this query strategy, that sample is an informative sample for the model. The first negative sign in Equation (2) ensures that  X  maximizer to be used within the AL algorithm (Figure 2). Another way to estimate the informativeness is entropy [30]. Entropy is a measure of uncertain ty that indicates the amount of specialization of entropy for sequen ce labeling; this is used to find informative samples:  X   X  X  (  X   X  , X  )  X  X  X  X  =  X  (  X   X  |  X   X  )  X  X  X  X   X  (  X   X  |  X   X  ) The higher the entropy, the more informative the sample is. As described in Section 3.1.1, th e least confidence (LC) approach uses the model X  X  confidence about a sample X  X  label to find informative samples, i.e., sample selection is based on those that posterior probability of the model for all samples in the unlabeled set is computed and then samp les are ranked accordingly. The LC approach selects unlabeled samples that are characterized by low classification confidence. However, there are other samples that are characterized by high posterior probabilities, meaning that the model is confid ent about the samples X  label. These samples can be automatically labeled by the model and added to the labeled set in each iteration, so as to provide further learning examples to the classifier. This approach is often referred to as semi-supervised learning [37]. To investigate whether this intuition can positively affect the active learning process, we modified step 4 in the active learning algorithm of Figure 2 as follows: We call this approach Augmented Least Confidence (ALC). The intuition behind informativene ss-similarity based approaches representative and diverse samples, in addition to informative ones, aiming to achieve a better coverage of the dataset characteristics. Information Density [27] consid ers the representativeness of samples, along with their informativeness, to prevent outliers to be selected by the active learning process. IDen is computed according to:  X  where  X   X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  (  X  X  ) corresponds to the representativeness of samples:  X  The average similarity between sample  X  X  and all other samples in the set of unlabeled samples (  X  X  ( X ) representativeness of sample  X  X  : the higher the similarity, the more representative the sample. Similarity is measured according to the cosine distance:  X  X  X   X  X  X   X  X   X   X ,  X  ( X )  X = informativeness of samples (  X   X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  (  X , X  X  ) ), we use least confidence (Equation (1)). Theref ore, according to this query strategy, those samples characterized by the least confidence and the highest similarity are those that are useful to the model. Differently from IDen, information diversity aims to take into account the diversity of the data within the process of querying samples; IDiv is formalized as follows:  X   X   X  X  X  X  X  X  X  X  X  X  X  X  (  X  X  ) is calculated based on how dissimilar a sample  X  X  is compared to already selected samples within the labeled set  X  X   X  Similarity is measured accordin g to Equation (6). LC (Equation (1)) is also used as a measure of informativeness. One way to find both representa tive and diverse informative samples is to combine IDen and IDiv approaches as an IDD approach:  X   X  X  X  (  X   X  ) = Both informativeness and informativeness-similarity based approaches are dependent on the model. In a real world active learning scenario, an expert should wait until a model is learnt on the current labeled set and then the next batch of samples are selected using one of the above-mentioned query strategies for labeling. This could introduce la rge time delays between expert annotations cycles due to the time required to train models. One way to prevent such a problem is to propose approaches that are independent from the model output. Maximum representativeness-diversity is an approach which only in the labeled (  X  X  (  X  ) ) and unlabeled (  X  X  ( X ) ) sets:  X   X  Equation (5) and (8), respectivel y. The most representative and diverse samples are labeled in the current batch and then added to the training set. External resources and ontologies are useful tools for extracting domain-specific features to repres ent samples and they have been shown to often enhance superv ised machine learning models. However, there are no active learning query strategies that use external knowledge resources to drive the process of sample selection. Here, we tackle th is gap and propose a novel query strategy called Domain Knowledge Informativeness. DKI is a query strategy to  X  X nf orm X  the model about unlabeled samples using external knowledge. We combine an informativeness based approach w ith domain knowledge to select samples for active learning:  X  Where |  X  X  | is the length of sequence  X  X  ,  X  (  X  domain knowledge contained in the sequence (i.e., importance), and  X   X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  is the informativeness measure.  X   X  X  X  X  the average importance of the tokens contained in a sequence based on their informativeness for the model as well as the importance of the domain knowledge they carry. A useful domain characteristic that can be extracted from domain knowledge resources is the semantic type th at each token belongs to. This information has often been exploite d to generate semantic features for sample representation in superv ised models. Specifically, here we consider the distribution of se mantic types as they appear in information to compute the Semantic Value (SV) of a semantic type as follows: ) X  X  X  X  X  X  X  X  X  X  X  X ( X  X  X  X  X  X  X  X  X  X  X  X  X  Semantic Value is calculated based on the semantic types neighboring tokens. However, it is po ssible for a token to be a part of a longer concept span. For example,  X  acute  X  could be an  X  acute headache  X  which is a disease, and should therefore be considered under this type inst ead. Since we model the concept considering the semantic types of each token separately, it is more appropriate to calculate the distribution of the semantic type assigned to the longest span that each token belongs to. In this paper we consider a specific instantiation of DKI, where the domain knowledge contributes to the query strategy by means of the Longest Span Semantic Value (LSSV). Here, LSSV is used to find sequences with the maximum number of possible target concepts: =) X  X  X  X  X  X  X  X  X  X  X  X ( X  X  X  X  (13) We use Equation (13) to calculate  X  (  X   X  ) in  X   X  X  X  X   X  (  X   X  ) is calculated for each token separately (  X  (  X   X   X  ,...,0=  X | |  X  X  |  X  ); informativeness is also measured per token:  X   X  X  X  X  X  (  X   X  , X  )  X 1 X =  X   X  X   X   X =  X   X   X  X   X   X  (14) Where  X   X   X  X   X   X =  X   X   X  X   X   X  is the marginal probability,  X  the label at the corresponding position of the most likely label sequence  X  X   X  . Note that  X   X  X  X  X  ( 1/ |  X  X  | in Equation (11) is the normalization factor). To encourage the selection of longer sequences as they usually contain more target concepts [27], we also propose a denormalized instantiation of DKI, termed the total token-based Domain Knowledge Informativeness ( DKI  X  X  ):  X  In this section we introduce the datasets and their preparation steps. We then elaborate on our supervised approach and active explained. We use the annotated training and testing sets for the concept extraction task in the i2b2/VA 2010 NLP challenge [34] and ShARe/CLEF 2013 eHealth Evaluation Lab (task 1) [20]. The i2b2/VA 2010 task requires the extraction of clinical problems, tests and treatments from clinical reports. These reports are a combination of discharg e summaries and progress notes. They are organized as a collection of phrases and sentences. The ShARe/CLEF 2013 eHealth Eval uation Lab (task 1) requires to extract and identify mentions of disorders from clinical free-text notes. The dataset consists of discharge summaries, electrocardiogram, echocardiogr am, and radiology reports. Table 1 reports the number of documents in the training and sequences obtained after pre-processing. 
Table 1. Document (#doc) and sequence (#seq) count in the i2b2/VA 2010 349 30,673 477 45,025 
ShARe/CLEF 2013 200 10,171 100 9,273 In this paper, a sample (in both supervised and active learning approaches) corresponds to a sequen ce of tokens or a sentence. In the i2b2/VA 2010 dataset, sentence boundaries are already However, this is not the case for the ShARe/CLEF 2013 dataset. We then used the Leaman X  X  sent ence segmentation method [14] to derive sentences for this dataset. After detecting sentence boundaries , the data was encoded into a representation format that clearly indicates the span of a concept within a sentence. The  X  X IO X  format was leveraged to specify the beginning (B), inside (I), and outside (O) of a concept [23]. We employed a typical feature se t, including linguistic (part-of-speech tags), orthographical (regular expression patterns), lexical and morphological (suffixes/prefixes and character n-grams), contextual (window of k words), and semant ic features. The Medtex system [18] was leverage d to extract semantic features. Specifically, the UMLS and SNOMED CT (SCT) semantic types for each token were used as th e domain knowledge. We leveraged this knowledge to distinguish ta rget from non-target semantic types. A non-quantized value was a ssigned to each semantic type based on the distribution of each semantic type in the training set according to Equation (12). We use a tuned linear-chain Conditional Random Fields (CRFs) [12] as benchmark supervised ML algorithm as well as our base ML algorithm in the AL framework. CRFs [13] are the state-of-the-art supervised machine learning approach in clinical concept extraction tasks [20, 34]. chain CRFs model with a set of parameters  X  :  X  (  X   X  |  X   X  ) = Here  X   X  ) X  X ( is the normalization factor. Each  X  feature function between label state  X 1 X  and  X  on the sequence  X  at position  X  . The  X ( = X   X   X ,...,  X  ) represent the corresponding feature weights. Our implementations of CRFs for supervised learning (Sup), both baselines, and all active learning (AL) approaches are based on the MALLET toolkit [17]. Within this paper, we use an incremental, pool-based, active learning framework. In the incremental approach, a model is not updated in successive iterations [ 12]. As we aim to study how AL can contribute towards reducing the annotation effort compared to a supervised approach, we use supervised effectiveness as our knowledge for each sample  X  X  (Equation (13)) in the DKI query strategy. Random sampling (RS) and longe st sequence (LS) are two common baselines for analyzing the benefits of the AL framework. RS randomly selects samples; LS selects samples with the longest length (number of tokens). For AL and the baseline approaches, the initial labeled set is formed by randomly selecting 1% of the training data. The batch size (  X  ) is set to 200 for i2b2/VA 2010 and 100 for ShARe/CLEF 2013 across all experiments, lead ing to a total of 153 and 101 batches, respectively. While in real settings AL woul d use human annotators to label informative samples at each iteration, here we simulate this process by using the annotations pr ovided in the training set of the two datasets. In our evaluation, concept extraction effectiveness is measured by Precision, Recall and F1-measure. Evaluation metrics are computed on test data using MALLET X  X  multi-segmentation each other and against the fully supervised method based on their learning curves across batches. Learning curves allow to associate model performance and annotation effort. A core aspect of our evaluation is determining the first iteration at which AL strategies achieve an effectiveness comparable (i.e., identify the minimum amount of annotated data the AL strategies method (target effectiveness): this corresponds to the point of intersection between the AL learning curve and the target effectiveness. We further analyze the results by considering the Annotation Rate comparable to those reported using the i2b2 or ShARe/CLEF evaluation scripts. (AR), which measures how much a nnotation effort is saved by an AL approach. AR is the number of labeled annotation units in terms of sequences (SAR), tokens (TAR) and concepts (CAR) used by AL to reach the target effectiveness, over the number of labeled annotation un its used by the supervised method. A R = The lower the AR, the less annotation effort is required. Note that here every annotation unit (sequence, token, and concept) is considered as having the same annotation cost (uniform annotation cost). While this setting may not be fully representative of real-world use cases [28], no annotation cost is distributed along with the considered datase ts and the literature lacks of specific studies that consider annotation costs for clinical concept extraction. We first study to what extent various active learning query strategies reduce the annotation e ffort for clinical data compared to a fully supervised approach: this serves to determine the state-of-the-art strategy for clinic al concept extraction among those proposed in the literature. We then compare the best results from the state-of-the-art with our proposed query strategy to assess if domain knowledge based query st rategies further reduce the burden of manual annotation in clinical settings. Table 2 presents the effectiveness of the fully supervised CRF model. 
Table 2. Effectiveness of the fully supervised approach (Sup) These results are used as the target effectiveness for determining the level of annotation effort savings contributed by different active learning approaches. Figure 3 shows the learning curves obtained with different active learning query strategies in th e first 40 batches of the i2b2/VA 2010 and ShARe/CLEF 2013 datasets. always well above the RS baseline, as expected. However, MRD always performs worse than th e longest sequence baseline, suggesting that only relying on the similarity between samples to select subsequent batches in the AL loop is not effective. This further highlights that the simila rity measure on its own is not enough to find useful samples for active learning. We hypothesize repetition of fairly similar pa tterns [11]. Although the diversity element prevents the MRD approach from selecting similar samples, it still fails to pick the most informative ones. The fact that the learning curves of approaches that leverage informativeness in addition to similarity measures are higher than the learning curve of MRD dem onstrates the importance of informativeness in selecting useful samples. In i2b2/VA 2010, the LS baseline effectiveness is higher than performs poorly compared to the LS baseline in early batches, but outperforms LS after ten batche s in i2b2/VA 2010 (F1 = 0.74) and six batches in ShARe/CLEF 2013 (F1 = 0.63). We hypothesize contain more concepts. Hence, by choosing the longest sequences, the effectiveness of LS increases quite sharply in early batches as 5 i2b2/VA 2010, F1  X  0.73 and ShARe/CLEF 2013, F1  X  0.63. i2b2/VA Informativeness-based Approaches Informativeness-Similarity Based Approaches Model-Independent Approaches target effectiveness (Table 2) twice sooner than RS, and it is is because the selected long sequences contain no more useful samples to train the model. Annotation rates for all active learning query strategies and baselines are reported in Table 3. The top three approaches are highlighted. Among the informativeness-based approaches, Least Confidence and Sequence Entropy are very close and outperform Margin. By adding a similarity element to informativeness, annotation rates in terms of sequences, tokens, and concepts are reduced. However, the informativeness-sim ilarity based approaches exhibit different behaviors across the tw o datasets. IDiv is the best method in ShARe/CLEF 2013, while IDD performs better than IDiv in i2b2/VA 2010, although differences are not significant. Since token (TAR) and concept (CAR) annotation rates are more appropriate to measur e the actual savings of annotation effort rather than the number of annot ated sequences [11], we can conclude that IDD and LC are the most promising approaches for i2b2/VA 2010 and ShARe/CLEF 2013, respectively. The advantage of LC compared to IDiv and IDD, is that LC is computationally more efficient. Indeed, informativeness-similarity based approaches generally require a considerably larger amount of computations. This is because  X   X  X  X  X  X  X  X  X  X  X  X  X  X  X  X   X  unlabeled and labeled sets, respec tively. Although these similarity computations for  X   X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  (  X  X  ) can be performed prior to starting the active learning loop (as the unlabeled dataset is available upfront in pool -based active learning),  X   X  X  X  X  X  X  X  X  X  X  X  X  to be computed at each iteration of active learning. We now study how iteratively including samples to the labeled set  X  for which the model is very conf ident about their automatically assigned labels (Section 3.1.4) could lead to further annotation effort reduction. To identify these high confidence samples, we experiment with two thresholds,  X  = 0.998 and 0.999=  X  . ALC approach with 0.999=  X  improved the results of LC (Table 3) in both datasets. However, when  X  was set to 0.998 , in the Table 4. Annotation rates for ALC using two thresholds.
 i2b2/VA 2010 SAR(%) 24.5% 22.5% ShARe/CLEF 2013 ShARE/CLEF 2013 dataset ALC required more annotated data to reach the target effectiveness compared to LC. While the longest sequence approach (LS) did not reach the target effectiveness much before having trained on all available batches, it is interesting to note that its effectiveness was comparable to the five batches for both datasets. This suggests that long sequences in clinical reports often include useful information to train the process so as to select longer sequences first. A possible way for achieving this is selecting long sequences as reports the results for LC, IDiv, IDD, and ALC ( 0.999=  X  ) approaches using the longest sequence approach (LS) for selecting the initial labeled set across both datasets. 
Table 5. Annotation rates for LC, IDiv, IDD, and ALC (  X = i2b2/VA 2010 SAR(%) 23% 24% 24% 22.5% ShARe/CLEF 2013 i2b2/VA 2010 External Knowledge-based Approaches Using the longest sequences as the initial labeled set did not show consistent results across datasets and query strategies. In fact, the annotation rates for IDD in Sh ARe/CLEF 2013, LC and ALC in i2b2/VA 2010 slightly decreased; but other query strategies do not exhibit improvements. the concepts in the training set are required to be manually annotated before reaching the targ et effectiveness, even if the most effective AL approaches were used. In this section, we compare the identified state-of-the-art approaches with the DKI query strategies to investigate how domain knowledge acquired from external resources can help to reduce the annotation effort on clinical data. The results in Table 6 show that DKI based methods lead to a lower annotation rate compared to baselines and state-of-the-art AL methods. Token-ba sed domain knowledge informativeness ( DKI  X  , Equation (11)) outperforms most baselines and benchmarks so far). The results on the ShARe/CLEF 2013 dataset provide however less conclusive findings. If the length of the sequences is used when querying samples with DKI  X  X  , then significant reductions in token and concept annotation rates are achieved on both datasets. In the i2b2/VA 2010 dataset, DKI  X  X  concept and token annotation rates by 14% compared to IDD (the best state-of-the-art approach for this dataset, see Table 3). In the ShARe/CLEF 2013 dataset, DKI  X  X  reaches the target effectiveness using 6% less annotated concepts and 7% less annotated tokens than LC. When the performance of DKI  X  and DKI  X  X  are compared, it can be observed that the length of sequences has a significant contribution in the annotation effo rt savings. However, to achieve the best results, sequence length should be considered along with informativeness and domain knowledge, as it does not provide significant AR reductions when considered alone (i.e., LS). An interesting result is the small difference in sequence annotation rate (SAR) between IDD and DKI  X  X  for the i2b2/VA 2010 dataset. The two approaches select almost the same number of sequences to reach the target effectiveness but with different characteristics (tokens and concepts); this leads to very different results when considering the num ber of tokens and concepts required to be annotated. This observation demonstrates that domain knowledge combined with a simple informativeness measure (marginal probabi lity) can lead to a better selection of samples; and this is achieved wit hout resorting to using similarity measures which are computationally costlier. DKI varied but significant range of savings in terms of required number of concepts to be annota ted across the two datasets (37%-57%). comparable to that of superv ised methods), while reducing annotation costs by minimizing the amount of data that is required to be annotated. Domains like the clinical one are characterized by high costs for obtaining (expert) annotations. In such domains, it ensuring no loss of effectiveness for automated techniques. One of the main challenges in active learning is identifying and querying samples that can better inform classification models [26]. Different strategies for selecting informative samples have been proposed, including: uncertainty sampling [15], query-by-committee [29], and information density [27]. Settles and Craven [27] performed an extensive empirical evaluation of different query selection strategies within active learning, using different corpora for sequence labeling tasks. They found that information density and sequence vote entropy outperform the state-of-the-art in active learning in most corpor a. However, in this paper we sample selection for concept extraction in the clinical domain. We speculate that this is because of the high similarity between sequences in clinical narratives. Hence, samples selected by information density are not useful for training classification model within the active learning process, while diversity allows to select more representative samples. While the effectiveness of active learning methods has been conclusively demonstrated in many domains such as text classification, information extrac tion and speech recognition [26], processing tasks have seen only limited use of active learning [19]. To demonstrate the key role of AL in reducing the burden of manual annotation, in previous work we built a preliminary active learning-based system and investigate the state-of-the-art AL methods (LC and IDen) for extracting medical concepts from clinical free text. The annotation e ffort saved by AL to achieve the target effectiveness (supervised effectiveness) was up to 54% of the total number of concepts (CAR) [11]. We also investigated the factors that influence the robustness and the effectiveness of models learnt within the active learning frameworks [12]. It was found that well selected feature set, the incremental learning setting, and the tuning of the supervised classifier parameters lead to more robust active learning mode ls. Here we consider the same settings and parameters, and observe that the models are generally robust, as demonstrated by th e learning curves (Figure 3). Within the clinical domain, acti ve learning has been used for Chen, et al. [3] introduced a  X  X odel change X  sampling-based algorithm and found it performed better than random sampling, uncertainty sampling-based algor ithms, and information density-based algorithm. Active learning has also been used for de-identifying Swedish clinical records [2]. There, the information extraction problem was treated as a classification task in which words belong to one of eight pers onal clinical information types or to the non-personal clinical information type. The most uncertain and the most certain sampling strategies were evaluated methods outperformed a random sa mpling baseline. Figueroa, et al. [6] analyzed the effectiveness of different active learning methods, including distance-based (DIST), diversity-based (DIV), and a combination of both (CMB ), based on clinical data characteristics. However, this study focused on a clinical classification task and only app lied a limited number of query strategies. They found that the effectiveness of DIV and DIST is strongly dependent on the dataset diversity and uncertainty, respectively. Rosales, et al. [22] presented a semi-supervised active learning framework based on query by committee and evaluated its effectiveness in a binary classification task, where concepts extracted from clinical free text are classified into two classes: present or absent/negated. Their framework is able to select both informative and representative samples. This paper has introduced a new active learning query strategy, called domain knowledge informa tiveness (DKI). This novel query strategy leverages domain knowledge resources, such as ontologies and terminologies, for extracting concepts from text. DKI is instantiated and evaluated within the clinical domain for the task of clinical concept extraction. Our instantiation considers the longest span semantic value, as obtained from a clinical NLP by a classification model. These are then used to query samples useful concepts based on the external domain knowledge. Our empirical evaluation highlight s the promise of integrating domain knowledge within active learning query strategies, as indicated by the gains in annotation effort reduction achieved by DKI over state-of-the-art approaches. We also found that, when anal yzing the performance of active learning on clinical data, the confidence about samples X  labels estimated by the classification model is a deciding factor in selecting effective samples. Our study has two main limitations. First, all evaluation metrics used in this study cannot be directly translated into actual cost reduction. However, CA R and TAR values can be used to design the full training data is not avai lable in advance for estimating the target effectiveness, thus making the decision of where to stop the active learning process is a cha llenging problem, which requires to consider a trade-off between cost and effectiveness. In order to overcome these limitations, our future work will consider designing a cost model th at uses the required number of annotated tokens and concepts. This cost model would constitute a step forward for precisely asse ssing the actual contribution of independently from the s upervised effectiveness. [1] A. R. Aronson. Effective mapp ing of biomedical text to the [2] H. Bostr X m and H. Dalianis. De-identifying health records by [3] Y. Chen, S. Mani, and H. Xu. Applying active learning to [4] R. A. Cote and S. Robboy. Progress in medical information [5] A. Culotta and A. McCallum. Reducing labeling effort for [6] R. L. Figueroa, Q. Zeng-Trei tler, L. H. Ngo, S. Goryachev, [7] C. Friedman, L. Shagina, Y. Lussier, and G. Hripcsak. [8] H. Gurulingappa. Mining the medi cal and patent literature to [9] M. Jiang, Y. Chen, M. Liu, S. T. Rosenbloom, S. Mani, J. C. [10] L. Kelly, L. Goeuriot, H. Suominen, T. Schreck, G. Leroy, D. [11] M. Kholghi, L. Sitbon, G. Zuccon, and A. Nguyen. Active [12] M. Kholghi, L. Sitbon, G. Zuccon, and A. Nguyen. Factors [13] J. D. Lafferty, A. McCa llum, and F. C. N. Pereira. [14] R. Leaman, R. Khare, and Z. Lu. NCBI at 2013 [15] D. D. Lewis and J. Catl ett. Heterogenous Uncertainty [16] D. A. Lindberg, B. L. Hu mphreys, and A. T. McCray. The [17] A. K. McCallum. MALLE T: A Machine Learning for [18] A. N. Nguyen, M. J. Lawley, D. P. Hansen, and S. Colquist. [19] L. Ohno-Machado, P. Nadkarni, and K. Johnson. Natural [20] S. Pradhan, N. Elhadad, B. South, D. Martinez, L. [21] S. Pradhan, N. Elhadad, B. South, D. Martinez, L. [22] R. Rosales, P. Krishnamu rthy, and R. B. Rao. Semi-[23] E. F. T. K. Sang and J. Veenstra. Representing text chunks. [24] G. K. Savova, J. J. Masanz, P. V. Ogren, J. Zheng, S. Sohn, [25] T. Scheffer, C. Decomain, and S. Wrobel. Active hidden [26] B. Settles. Active learning, (Vol. 6): Morgan &amp; Claypool [27] B. Settles and M. Craven. An analysis of ac tive learning [28] B. Settles, M. Craven, and L. Friedland. Active learning with [29] H. S. Seung, M. Opper, and H. Sompolinsky. Query by [30] C. E. Shannon. A mathematical theory of communication. [31] H. Suominen, S. Salanter X , S. Velupillai, W. W. Chapman, [32]  X . Uzuner, I. Goldstein, Y. Luo, and I. Kohane. Identifying [33]  X . Uzuner, I. Solti, and E. Cadag. Extracting medication [34]  X . Uzuner, B. R. South, S. Shen, and S. L. DuVall. 2010 [35] H. Xu, S. P. Stenner, S. Do an, K. B. Johnson, L. R. Waitman, [36] H.-T. Zhang, M.-L. Huang, and X.-Y. Zhu. A unified active [37] X. Zhu. Semi-supervised learning literature survey. Technical 
