 Multi-document summarisation is the process of producing a single summary of a collection of related documents. Much of the current work on multi-document text summarisation is concerned with the English language; relevant resources are numerous and readily available. Arabic multi-document summarisation is still in its infancy. One of the obstacles to progress is the limited availability of Arabic resources to support this research. We are not aware of any publicly available Arabic multi-document gold-standard summaries apart from what we have created ourselves in earlier work, translating the Document Understanding Conference (DUC) 2002 English dataset [5] into Arabic [7]. 1 The original dataset provided gold-standard extractive and abstractive summaries in English, both human and machine generated.

Our overall aim is to improve the state of the art in Arabic multi-document summarisation. This requires advances in at least two areas. First, appropriate Arabic test collections are needed. Second, experiments need to be conducted with different approaches to the summarisation process to find techniques that produce good quality Arabic summaries. We have addressed the first issue in earlier work by creating a parallel English-Arabic version of the commonly used DUC-2002 dataset [7]. The dateset allows Arabic summarisers to be compared with English summarisers. The current pa-per is primarily concerned with the summarisation process itself, in particular the use of clustering. We explore different ways of using clustering, and compare our results to other systems.

The paper is structured as follows. We will start with a discussion of related work in Section 2. We will then reprise how we created an Arabic summarisation test collection in Section 3. Section 4 will describe the clustering approaches we are working with together with a description of our summarisers and the exper-imental setup. Results are discussed in Section 5, and we conclude in Section 6. First we will describe multi-document summarisation, the application of cluster-ing to summarisation, our test collection, and the evaluation methodology. 2.1 Multi-document Summarisation The analysis for multi-document summarisation is usually performed at the level of sentences or documents. Multi-document summarisation systems follow two approaches: extractive or abstractive . There has been only limited work on ab-stractive summarisation; it requires natural language analysis and generation techniques that do not yet appear to be sufficiently robust for unconstrained data. For this reason, most of the current summarisation systems rely on the extractive approach.

One early multi-document summariser used information extraction (IE) to identify similarities and differences be tween documents [16]. Later systems com-bined IE with a process tha t regenerates the extract ed units in order to improve the quality of the summarisation [1]. Zhao et. al [29] describe a method for query-focused multi-doc ument summarisation.

In our own work we will not consider query-based summarisation as we are focusing on query-independent summarisation.

Summarisation of Arabic documents has not advanced as fast as work in other languages such as English. The summariser  X  X akhas X  [4] was developed using extraction techniques to produce ten-words summaries of news articles. Turchi et. al [26] presented a method for evaluating multilingual, multi-document, extractive summarisation, using a parallel corpus of seven languages. In their approach, the most important sentences in a document collection were manually selected in one language. This gold-standard summary was then projected into the other languages in the parallel corpus.

Our work aims at making progress in Arabic multi-document summarisation which has started to attract more attention in the research community. An example for that trend is the inclusion of Arabic as one of the languages in the new TAC MultiLing pilot track 2 this year. 2.2 Clustering for Summarisation Data clustering is the assignment of a set of observations into subsets, so called clusters. As a method of unsupervised lea rning, clustering has received a lot of attention in past years to improve information retrieval (IR) or to enhance the quality of multi-document summaries [6]. Clustering has been applied to many document levels starting from the document itself down to sentences and words. Clustering can broadly be grouped into hierarchical clustering and partitional clustering.

In our work we focus on the partitional clustering technique, in particular  X  X entroid-based X  clustering. In centroid-based multi-document summarisation, similarity to the cluster centroid is u sed as a measure to rank sentences. The centroid is defined as a pseudo-document consisting of words with TF*IDF scores greater than a predefined threshold [19,20]. It is this centroid-based clustering that we will also focus on here.

Liu et al. [15] proposed a Chinese multi-document summariser which is based on clustering paragraphs of the input articles, rather than sentences. The number of clusters used changes automatically depending on the number of paragraphs.
Wan et al. [27], proposed a multi-document summarisation technique using cluster-based link analysis. In their work they used three clustering detection algorithms including k-means [12], agglomerative and divisive clustering. Their system seeks to cluster the sentences in to different themes (subtopics), the num-ber of clusters is defined by taking the absolute square root of the number of all sentences in the document set. In contrast, we use a fixed number of clusters for each run, although we do explore the impact of using different numbers of clusters.

Sarkar [23] presented a multi-docu ment summariser which used sentence-based clustering. The system adopted the incremental clustering method that has been used for web clustering in [11] . In Sarkar X  X  work the clusters are re-ordered according to the number of sentences they contain. This is based on the assumption that size of a cluster correlates with its importance.

To the best of our knowledge, little work has been reported on applying clus-tering for Arabic multi-document summarisation. Schlesinger et al. [24] present CLASSY, an Arabic/English query-based multi-document summariser system. They use an unsupervised modified k-means method to iteratively cluster multi-ple documents into different topics (stories). They rely on the automatic trans-lation of an Arabic corpus into English. At the time of their experiments, the quality of machine translation was not high. This led to difficulties in reading and understanding the translated dataset. The translation resulted in inconsis-tent sentences; core keywords may have been dropped when translating. Errors in tokenisation and sentence-splitting were among the main challenges. 2.3 Test Collections In our work we also rely on machine translation. This is just to overcome the lack of Arabic multi-document datasets for multi-document summarisation. The DUC X 2002 English dataset [5] provides articles and multi-document gold stan-dards for extractive summaries. We perfo rmed sentence-by-sentence translation of this dataset into Arabic [7]. The translation is not part of the summarisa-tion process; it is merely intended to allow us to evaluate Arabic summaries against English gold standards. The technique involved translating the DUC-2002 dataset into Arabic using Google Translate 3 .

One reason we created our own parallel translated dataset is because the well-known freely available parallel corpora such as Europarl [13] and JRC-Acquis [25] do not include Arabic. Although Salhi [21] provides an Arabic/English par-allel corpus (The English-Arabic Parallel Corpus Of United Nations Texts  X  EAPCOUNT), we decided not to use it for a number of reasons. The corpus con-tains 341 texts aligned by paragraphs rather than sentences. We also required a corpus that was divided into groups of related articles, which is not the case with EAPCOUNT.

We also wished to be able to compare the performance of our Arabic multi-document summariser against the results of top performing (English) summaris-ers as published in conferences such as DUC and TAC. Having an Arabic-only corpus would make it extremely difficult to compare our summariser against the best performing English summarisers. Our parallel English X  X rabic dataset pro-vides a solution to this problem, and allows us to compare Arabic summaries with gold-standard English summaries. 2.4 Evaluation Evaluating the quality and consistency of a generated summary has proven to be a difficult problem [9]. This is mainly because there is no obvious ideal, objective summary. Two classes of metrics have been developed: form metrics and content metrics. Form metrics focus on grammati cality, overall text coherence, and or-ganisation. They are usually measured on a point scale [3]. Content metrics are more difficult to measure. Typically, system output is compared sentence by sen-tence or unit by unit to one or more human-generated ideal summaries. As with information retrieval, the percentage of information presented in the system X  X  summary (precision) and the percentage of important information omitted from the summary (recall) can be assessed.

There are various models for system evaluation that may help in solving this problem. Automatic evaluation metrics such as ROUGE [14] and BLEU [18] have been shown to correlate well with human evaluations for content match in text summarisation and machine translation. Other commonly used evaluations include assessing readers X  understanding of automatically generated summaries. Human-performed evaluation may be preferable to automatic methods, but the cost is high. As indicated earlier, we chose to use the DUC-2002 dataset [5] to create an Arabic test collection. 4 This dataset contains 59 collections of related newswire and newspaper articles. On average, each collection contains ten related articles.
The dataset includes fully-automatic, multi-document gold-standard sum-maries, each on a single subject. The su mmaries were of approximately either 200 or 400 words (white-space delimited tokens), or less.

All the summaries were extractive, con sisting of some subset of the sentences predefined by NIST in the sentence-separated document set. Each predefined sentence was used in its entirety or no t at all in constructing an extract 5 .
The main objective of experimenting with DUC-2002 is to be able to com-pare our Arabic multi-document summariser with the currently available En-glish summarisers. In order to do this, we first had to translate the sentences in each article into Arabic. This was done using the Java version of the Google Translate API. A total of 17,340 sentences were translated. The public, online version of Google Translate imposes limits on the size of text that can be trans-lated, and on the number of translations that can be requested within a given period. Our software had to work within these limits, making just one translation request every half second. The above pro cess resulted in a parallel sentence-by-sentence Arabic/English version of the DUC-2002 dataset. The summaries for the corpus were created using the applicable English or Arabic version of our multi-document summariser system. Fo r our purposes, it was not necessary to translate the gold-standard summaries themselves into Arabic. Given that we had a sentence-by-sentence translation, we could compare the results of our Ara-bic summariser (under different settings such as cluster sizes and summarisation approaches) with the top performing systems in the DUC-2002 multi-document English summarisation competition as well as against each other and sensible baselines simply by using the sentence identifiers. For all our experiments we are using our generic multi-document summarisers that have been implemented for both Arabic and English (using identical pro-cessing pipelines for both languages). T hese summarisers take a set of related articles and generate a summa ry by selecting sentences from these articles. Hav-ing summarisers for both English and Arabic allows us to confirm that the sum-marisation technique is working and that the impact of using Google Translate to create the parallel dataset is minimal. In the future, it will also allows us to contrast the impact of related techniques, such as stemming, when applied to different languages.

We will now describe the clustering methods employed in our experiments, the actual summarisation process and the exp erimental setup. We explored cluster-ing in two different ways, first of all by clustering all sentences prior to selecting a summary. In the second experiment we applied clustering as part of the redun-dancy elimination step following an initial selection of sentences. 4.1 K-means Clustering K-means clustering [12] is a partitional centroid-based clustering algorithm. The algorithm randomly selects a number of se ntences as the initial centroids, the number of sentences is dependent on the c luster size assigned. The algorithm then iteratively assigns all sentences to th e closest cluster, and recalculates the centroid of each cluster, until the centroids no longer change. For our exper-iments, the similarity between a sentenc e and a cluster centroid is calculated using the standard cosine measure applied to tokens within the sentence. 4.2 Experiment 1: Clustering All Sentences In this experiment we treat all documents to be summarised as a single bag of sentences. The sentences are clustered us ing different cluster sizes and we then select a summary using two approaches: 1. Select sentences from the biggest cluster only 2. Select sentences from all clusters.
 The intuition for the first approach is the assumption that a single cluster will give a coherent summary all centered around a single theme, whereas the second approach is expected to result in summa ries that contain more aspects of the topics discussed in the documents and therefore a summary that gives a broader picture. 4.3 Experiment 2: Clustering for Redundancy Elimination Redundancy elimination is an important part of automatic summarisation. A summary that contains very similar sen tences drawn from different documents is not ideal. Previously we used redundancy elimination algorithms applied to an initial set of selected sentences. Experim ents demonstrated that this has worked effectively [7]. We used the vector spac e model (VSM), latent semantic analy-sis (LSA) and Dice X  X  coefficient. We exp ect that clustering of the pre-selected sentences has the potential of improving the summarisation quality further.
In contrast to query-based summarisa tion [2,10], in gen eric summarisation there is no query that can be used to identify relevant sentences. Instead, we use the first sentence of each article to provide the relevant selection criteria. This is based on the assumption, born out in practice, that the first sentence of a well-written article provides a good starting point for generic summarisation [8,28].
Together with the first sentence from each document we select from each document a second sentence, the one most similar to the first one using the VSM [22]. We are effectively performing a n initial filtering step, as a result of which we have a similar (but smaller) bag of sentences compare to those of Experiment 1. All the subsequent steps a re then the same as in that experiment. 4.4 Experimental Setup We run k-means clustering using different numbers of clusters, the number of clusters used is 1, 2, 5, 10, 15 and 20. Clustering using a single cluster essentially results in a list of sentences which can be ranked according to the centroid of all sentences.

Two selection methods are used to pick sentences for inclusion in the summary: 1. Select the first sentence of each cluster; 2. Select all the sentences in the biggest cluster.
 The ranking of sentences is done accord ing to similarity to the centroid. The biggest cluster is defined as the one comprising the largest number of sentences. In the resulting summary we keep the ord er of sentences as they appear in the clusters (i.e. a sentence very similar t o the centroid appears earlier on in the summary than one that is less similar).

There were two parts to each of the summarisation experiments. First, we created summaries for the DUC-2002 dataset using the English version of our multi-document summariser. The results were evaluated against the English gold-standard summaries, using ROUGE. Second, we summarised the Arabic paral-lel translation version of using our Arabic multi-document summariser. These Arabic summaries were evaluated by re-constructing the corresponding English translations using the sentence identifiers. We were then able to evaluate the  X  X ranslated summaries X  using ROUGE. Running both English and Arabic exper-iments acts as a check on the automatic translation process, and provides some assurance that the underlying summarisation algorithm itself is sensible.
Note that in our experiments we do not trim the resulting summary to a particular length. ROUGE will do this automatically for summaries that are too long. Alternative ly, we could produce a summary that does not exceed a fixed maximum length.

ROUGE-1 was used as the evaluation metric. This is for two reasons. First, it was the metric used in evaluating the top performing summarisers in DUC-2002, with which we wish to compare our approach. Second, it has been shown to work well in multi-document summarisation tasks [14]. The ROUGE evaluation was performed having N-gram 1:1 and confidence interval of 95%, which was believed to give results close to those of human evaluation.

Where appropriate, we determined significant differences by performing pair-wise t -tests ( p&lt; 0 . 05) using the R statistics package. 6 Tables 1 and 2 illustrate the results of our Arabic summarisers applying k-means clustering to all sentences using the two different selection methods. The highest ROUGE scores are displayed in bold font. Similarly, Tables 3 and 4 represent the results of clustering for redundancy elimination.
We observe the following results. 1. The best overall ROUGE score is obtained by clustering all sentences and 2. An alternative way to obtain top ROUGE scores is to use clustering for 3. We also observe that for certain experimental settings (as just described) the 4. Throughout the experiments we observe that high precision goes hand in 5. We also found that none of the pairwise comparisons of ROUGE scores The ROUGE-1 results of the five best performing summarisers in DUC-2002 are given in [17], where the authors of that paper compared their summariser to those systems. The results are reproduced in Table 6. These results were produced using an earlier version of ROUGE. For this reason, we sought to replicate their baseline experiment to ensure comparability of the results. The baseline summariser simply selects the fir st sentence of each article of the set of related articles in the dataset and combines them. The baseline results we reproduced for the DUC-2002 dataset w ere comparable to those published in [17], with just a slight increase in the ROUGE-1 evaluation.

Comparing the ROUGE-1 results of our systems with those of the top five summarisers in DUC-2002 competition, we observe that our multi-document summarisers achieve slightly higher scores than the top systems reported at DUC-2002. This is true for the English system (top ROUGE score 0.3856), but more importantly for this paper, also for our Arabic multi-document summariser (working on the Arabic translation of the dataset).

The main finding of our experiments appears to be the fact that a simple centroid-based similarity clustering with a single  X  X luster X  when performing sum-marisation could be considered an alterna tive to using several cluster. The work by Sarkar and by Radev et al. [19,20,23] had a variable number of clusters when performing clustering, depending on the number of sentences, whereas our exper-iments demonstrate that for the given test collection the closeness to the centroid to identify the important sentences can p roduce summaries with similar quality. In this paper we explored clustering for multi-document Arabic summarisation. We investigated how clustering can be applied to multi-document summarisation as well as for redundancy elimination within this process. We used different parameter settings including the cluster size and the selection model applied in the extractive summarisation proce ss. Using ROUGE, precision and recall we could measure the different effects objectively. One of our main findings is that selecting sentences similar to the centr oid of all sentences in the collection of related documents gives the highest ROUGE scores. We also showed that the Arabic (as well as English) summarisation system we developed has comparable performance with the top performing (English summarisation) systems at DUC-2002.
 Running our summarisation experiments in parallel on Arabic as well as an English version of our summariser has allowed us to confirm that the automatic translation process does not affect the quality of the result. In future it will allow us to compare the impact of similar techniques on the two languages.
We intend to explore the application of more fine-tuned clustering to improve results further. We also aim to experiment with more language-specific features (such as light stemming) which we suspect will be more effective for Arabic than for English.

