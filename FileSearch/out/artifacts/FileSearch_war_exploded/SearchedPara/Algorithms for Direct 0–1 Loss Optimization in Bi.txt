 Tan T. Nguyen t258.nguyen@qut.edu.au QUT, Brisbane, QLD 4001, Australia Scott Sanner ssanner@nicta.com.au NICTA &amp; ANU, Canberra, ACT 2601, Australia The 0 X 1 loss objective for binary classifiers  X  mini-mize the number of misclassifications  X  is known to be robust to outliers. Unfortunately, it is NP X  X ard to optimize directly (Feldman et al., 2009; Ben-David et al., 2003) and thus most work has sought alternative losses with better computational guarantees. While hinge loss used in SVMs (Cortes &amp; Vapnik, 1995) and log loss used in logistic regression may be viewed as convex surrogates of the 0 X 1 loss that are computa-tionally efficient to globally optimize (Bartlett et al., 2003), such convex surrogate losses are not robust to outliers (Wu &amp; Liu, 2007; Long &amp; Servedio, 2010; Ding &amp; Vishwanathan, 2010) as shown in Figure 1. Given the outlier robustness properties of 0 X 1 loss, we explore a variety of practical algorithms for directly optimizing it and contribute the following novel opti-mization techniques targeted specifically for 0 X 1 loss: Branch and bound: We show this staple of search in the optimization literature proves effective with good initialization plus informed decision ordering heuristics and a forward-checking technique for pruning implied decisions within the convex hull of existing decisions; this yields an anytime approximation scheme with op-timal convergence guarantees.
 Combinatorial search: We exploit the fact that there are only a finite number of equivalence classes of sep-arating hyperplanes that can have different loss values and propose both prioritized systematic and heuristic search through combinations of data points that define these equivalence classes. While systematic combina-torial search yields efficient optimal solutions on low dimensional problems, heuristic combinatorial search offers excellent approximations and scalability. Smooth, differentiable relaxations of 0 X 1 loss: We re-lax the 0 X 1 loss to a smooth, differentiable function that can arbitrarily approximate the original 0 X 1 loss via a smoothness constant. We then provide an itera-tively unrelaxed coordinate descent approach for gra-dient optimization of this smoothed loss along with techniques for escaping local optima. This yields solu-tions comparable to the combinatorial search approxi-mation, while running two orders of magnitude faster. Empirically, we compare our proposed algorithms to logistic regression, SVM, and the Bayes point machine (a approximate Bayesian approach with connections to the 0 X 1 loss) showing that the proposed 0 X 1 loss optimization algorithms perform at least comparably and offer a clear advantage in the presence of outliers. We assume a D -dimensional data input vector x  X  R D ( D for dimension), where the goal of binary classifica-tion is to predict the target class  X  t  X  { X  1 , 1 } for a given x . Linear binary classification which underlies many popular classification approaches such as SVMs and logistic regression defines a predictor function: where w j  X  R and w 0  X  R is a bias. Then Thus, the equation of the decision boundary that sep-arates the two classes is f w ( x ) = w T x + w 0 = 0, which is a D -dimensional hyperplane.
 We use two notations for the weight vector w : in the homogeneous notation, we assume w = ( w 0 ,w 1 ,...,w D ) T and x = (1 ,x 1 ,...,x D ) so that f w ( x ) = w T x . In the non-homogeneous notation, we assume w = ( w 1 ,...,w D ) T and x = ( x 1 ,...,x D ) so that f w ( x ) = w T x + w 0 .
 The training dataset contains N data vectors X = { x 1 , x 2 ,..., x N } and their corresponding target class t = { t 1 ,t 2 ,...,t N } . To measure the confidence of a class prediction for an observation x i  X  X , the so X  called margin is defined as m i ( w ) = t i f w ( x i ). A margin m i ( w ) &lt; 0 indicates x i is misclassified, while m i ( w )  X  0 indicates x i is correctly classified and m i represents the  X  X argin of safety X  by which the predic-tion for x i is correct (McAllester, 2007).
 The learning objective in classification is to find the best (homogenous) w to minimize some loss over the training data ( X , t ), i.e., where loss L ( m i ( w )) is defined as a function of the margin for each data point x i , R ( w ) is a regularizer which prevents overfitting (typically k w k 2 2 or k w k and  X  &gt; 0 is the regularization strength parameter . Some popular losses as a function of the margin are where I [  X  ] is the indicator function taking the value 1 when its argument is true and 0 when false. These losses are plotted in Figure 2. 0 X 1 loss is robust to out-liers since it is not affected by a misclassified point X  X  distance from the margin, but this property also makes it non-convex; the convex squared, hinge, and log losses are not robust to outliers in this way since their penalty does scale with the margin of misclassification. Squared loss is not an ideal loss for classification since it harshly penalizes a classifier for correct margin pre-dictions 1, unlike the other losses. This leaves us with hinge loss as optimized in the SVM and log loss as optimized in logistic regression as two convex sur-rogates of 0 X 1 loss for later empirical comparison. Branch and bound techniques (BnB) (Land &amp; Doig, 1960) are a staple of the literature on discrete opti-mization via search methods. This section shows how to formulate the 0 X 1 loss problem in a BnB setting along with heuristics and pruning techniques that al-low it to efficiently find an optimal 0 X 1 loss solution. Following from the definition of the training objective from (3) w.r.t. 0 X 1 loss in (4), the 0 X 1 loss optimization problem can be written as where l i = I [ t i w T x i  X  0] denotes the individual 0 X 1 loss of point x i . Then we have Furthermore, let l = ( l 1 ,l 2 ,...,l N ) be the loss vector consisting of all individual losses. It can be seen that a specific assignment of the loss vector corresponds to a system of linear inequalities. For example, the loss vector l = (0 , 1 , 0) for training data of N = 3 points provides { t 1 w T x 1 &gt; 0 ,t 2 w T x 2  X  0 ,t 3 w T x data, resulting in a system of linear inequalities. Thus, the 0 X 1 loss optimization problem is now equiv-alent to finding a feasible loss vector l  X  with a minimal sum of its components (which is the sum of 0 X 1 losses): Once we have obtained l  X  , we need to recover optimal hyperplane parameters w  X  , which are insensitive to scaling. Hence, we fix w 0 at either 1 or  X  1 1 (whichever yields a feasible solution) and call a linear program (LP) solver to minimize P D i =1 w i (to obtain a unique solution) subject to the constraints induced by l  X  . In Algorithm 1 we provide the full BnB algorithm that searches for the optimal l  X  from (9). The key idea of the algorithm is to build a search tree of assignments while tracking the minimum loss solution w  X  and value loss min to prune provably suboptimal branches of the search. To make this algorithm efficient, we propose the following heuristic improvements: Algorithm 1 Branch and Bound Search (BnB) Initial Bound Approximation: In line 2, we run a fast SVM solver on the full data to obtain an initial best solution w  X  and loss min . Clearly this should prune a large portion of the search space and guarantees that BnB will do at least as well as the SVM.
 Decision Ordering: It is well-known that the ordering of decision and value assignments in BnB can dras-tically affect search efficiency. Fortunately, having an approximated decision hyperplane from the initial SVM solution helps to determine the assignment and value ordering. Under the assumption that the opti-mal decision hyperplane is somewhat near the approx-imated hyperplane, the loss values of points that lie closer to the approximated hyperplane are more likely to be changed than those that are far away. Hence in line 14, the points x i farthest from the initial approx-imated decision hyperplane  X  w are assigned first, and their inital value is assigned to be consistent with the assignment  X  l i of the initial SVM solution. Loss Propagation: In any partial assignment to l , points within the convex hull defined by the set of x i assigned true in l are implied to be true and simi-larly for the false case. Hence at every branch of BnB we utilize the technique of forward checking by calling Propagate-Loss (lines 15 and 19) to augment l with any implied assigments that prune future search steps or to detect whether an assignment conflict has been found leading to infeasibility. We also use the sum of forced and assigned components of the loss vector as a lower bound for purposes of pruning suboptimal branches (lines 16 and 19).
 To detect whether a point p  X  R D is an interior point of the convex hull created by other points p , p 2 ,..., p k  X  R D , we simply check whether the fol-lowing linear constraints over u are feasible (e.g., by calling an LP solver): u i  X  0 for i = 1 , 2 ,...,k  X  If a feasible u exists, then point p is a convex combination of points p 1 , p 2 ,..., p k (with coefficients u ,...,u k ), therefore p lies in the convex hull of points p , p 2 ,..., p k , otherwise it does not. This section introduces the idea behind the combi-natorial search approach, which is illustrated by Fig-ure 3. In short, we observe that the hyperplane pass-ing through a set of D points maps to an equivalance class of hyperplanes all with the same 0 X 1 loss. This suggests a simple combinatorial search algorithm to enumerate all N D hyperplane equivalence class repre-sentatives to find the one with minimal 0 X 1 loss. To formulate the approach discussed above, we write the 0 X 1 loss for x i in non-homogenous notation as: As noted previously, if both w 0 and w are scaled by 1 / | w 0 | , the solution is unchanged (assuming w 0 6 = 0). Specifically, there are two cases: first, if w 0 &gt; 0, then the loss function is: where we have defined w 0 = 1 w Second, if w 0 &lt; 0, the loss function is L ( w 0 , w ) = The equation of the decision hyperplane is the same in both cases ( w 0 + w T x = 0  X  1 + w 0 T x = 0) and the loss function is either L (1 , w 0 ) or L (  X  1 ,  X  w the bias term is now either 1 or  X  1. As shall be seen next, this fact is critically important for the purpose of the combinatorial search approach. As the initial discussion pointed out, to find the optimal solution, it suffices to check all hyperplanes that go through D points of the training dataset and find the one that has minimum 0 X 1 loss. So, assuming x 1 ,..., x D are (any) D distinct data points from the training dataset, then for the combinatorial search to work, the two follow-ing tasks must be solved: (1) Find the weight vector w 0 = ( w 0 goes through these D selected points. (2) Calculate the 0 X 1 loss value corresponding to w 0 .
 For the first task, because the hyperplane goes through the given D data points, at each point the hyperplane equation must be satisfied. So, which is written in matrix form as A w 0 =  X  1 , where A = ( x 1 x 2 ... x D ) T and 1 is the unit vector in R D This linear matrix equation can be easily solved to get a particular solution w 0 using LU decomposition. Here, one sees that if the bias term w 0 was still present, the above equation would be underdetermined.
 Now, with w 0 specified, the second task becomes easy, as the 0 X 1 loss value is obviously the smaller value of L (1 , w 0 ) and L (  X  1 ,  X  w 0 ). Thus, if L (1 , w L (  X  1 ,  X  w 0 ), the 0 X 1 loss value corresponding to de-cision hyperplane 1 + w 0 T x = 0 is L (1 , w 0 ), and the solution vector (including bias and weights) is (1 , w 0 ), otherwise, the 0 X 1 loss value is L (  X  1 ,  X  w 0 ), and the solution vector is (  X  1 ,  X  w 0 ). Note that generally N &gt;&gt; D , so the class of the D selected points can be assigned to the most frequent class (one could use a separating hyperplane method to do better but the gain is insignificant).
 The above discussion represents all necessary knowl-edge for the combinatorial search approach and it is now possible to build algorithms based on the foun-dation presented here. We present two variants: one provably optimal and one approximate.
 Prioritized Combinatorial Search (PCS) : This algo-rithm exploits the fact that combinations of data points lying closer to an initial approximated decision hyperplane (e.g., given by an SVM) are more likely to produce the optimal hyperplane than combinations of points lying far away. Algorithm 2 captures this idea by considering combinations of D points in the increas-ing order of their distance to the approximated deci-sion hyperplane, where the distance of a set of points to a hyperplane is the minimal distance of points in the set. PCS can find an optimal solution in O ( D 3 N time ( N D iterations, each requires D 3 time to solve the linear matrix equation), which can be much more efficient than BnB for small D .
 Algorithm 2 Prioritized Combinatorial Search (PCS) Combinatorial Search Approximation (CSA) : Rather than systematically enumerating all combinations as in prioritized search, we start from an initial  X  X est X  combination of D points near an approximated deci-sion hyperplane (e.g., given by an SVM), then at each iteration, we swap two points ( x k , x j ) in/out of the current combination. The algorithm stops when it can-not find any more points to swap. We do not present the full algorithm here due to space limitations but note that it is a slight variation on Algorithm 2. The non-smooth, non-differentiable 0 X 1 loss can be ap-proximated by a smooth differentiable loss (a modifi-cation of sigmoidal loss) This approximation is illustrated by Figure 4 (top) for different values of the precision constant K  X  R + that modulates smoothness. Assuming the x i do not lie on the decision hyperplane, then lim K  X  +  X   X  l K i = l i . Figure 4 (bottom) illustrates how an objective based on L K ( w  X  ) = P N i =1  X  l K i changes with different values of the precision constant K w.r.t. the actual 0 X 1 loss P i =1 l i . Clearly, small K yields smoother objectives with few local minima but provide only a rough ap-proximation of the true objective, while the opposite is true for large K .
 This suggests the following iterative unrelaxed opti-mization approach for zeroing in on the optimum 0 X 1 loss value: start with a small value of K and iteratively increase it; at each iteration with a fixed K , initialize with the solution from the previous iteration and per-form coordinate descent to locally optimize each w i (0  X  i  X  D ) in turn. This is summarized in the SLA algorithm (Algorithm 3). Of key importance here is the local search Grad-Desc-in-Range algorithm de-fined in Algorithm 4. Because of local optima (that increase with K ) this is a mixture of gradient descent, pattern search methods (Hooke &amp; Jeeves, 1961), and a hill-climbing heuristic. In short, this hybrid local search algorithm iterates between gradient descent to find a local optima followed by a probed search at uni-form intervals to find potentially better positions, this process repeating until an improved solution is found. Constants that work well in practice for SLA are the following: r R = 2  X  1 ,R 0 = 8 ,r = 2  X  1 , S 0 = 0 . 2 ,r 10 ,K MIN = 2 , K MAX = 200. In this section we evaluate the four previously defined direct 0 X 1 loss optimization algorithms (BnB, PCS, Algorithm 3 Smooth 0 X 1 Loss Approximation (SLA) Algorithm 4 Range Optimization for SLA CSA, and SLA) on a variety of real and synthetic data. We compare to SVMs and logistic regression using LibLinear (Fan et al., 2008) 2 . We also compare to the Bayes point machine (Minka, 2001) 3 which is a Bayesian approach with connections to the 0 X 1 loss. All tests are implemented in MATLAB 7.12.0 run-ning on a 1.7GHz dual-core i5 Intel processor and 4GB RAM. Each dataset was normalized to have all data with zero mean and unit variance in each dimension of x . Some tests use real-world datasets taken from the UCI machine learning repository (Frank &amp; Asuncion, 2010) and are listed in Table 1 along with the relevant number of data N and dimensionality D .
 Synthetic testing datasets are generated as follows: (1) A common data range R is randomly selected between 5 and 10. (2) Two center points C 1 ,C 2 , each cor-responding to one class, are randomly chosen in the range  X  ( R/ 2) from the origin. (3) Diagonal covari-ances in each dimension of C 1 ,C 2 are specified ran-domly in range [1 ,R 2 ]. (4) The desired number of data points for each class are then generated by a normal distribution with the generated mean and covariance. Noise Generation: We added noise to some datasets to test robustness properties of the classifiers. Noise is generated randomly and uniformly in the minimum and maximum range [ min  X  0 . 5( max  X  min ) ,max + 0 . 5( max  X  min )] of each dimension of the dataset and assigned to the first class of the dataset. Since the range of noise is twice the range of data, this process produces both background noise and outliers.
 Optimality of Solutions on Real-World Datasets: We evaluated various algorithms on the UCI datasets given in Table 1. This test compares the optimality of the returned solutions of all algorithms: BnB, PCS, CSA, SLA, SVM, Bayes point machine (BPM), and lo-gistic regression (LR). The size of the testing datasets do not allow an exhaustive search, hence a time thresh-old of 300 seconds is set for BnB, PCS.
 Table 1 shows the 0 X 1 loss values returned by compar-ing algorithms for the original training data. Here we see that all of the direct 0 X 1 loss approximation algo-rithms the BPM and other solutions based on convex surrogates of 0 X 1 loss. Table 2 shows results similar to Table 1 except in the scenario where 10% noise is added. Here we see an even strong performance from the direct 0 X 1 loss optimization algorithms indicating their robustness to noise. Table 3 shows the running time (T1) of BPM, SVM, LR, SLA, CSA and the time to reach the given solution (T0) of BnB and PCS. Since SLA has performed with the best optimization results and runs two orders of magnitude faster than other di-rect 0 X 1 optimizers, in Table 4 we compare it with the BMP, SVM, and LR for prediction error on held-out test data for each UCI dataset augmented with noise. (Without explicit outlier noise, SLA performed on par with the BPM, SVM, and LR; not shown due to space restrictions.) Here, we see that SLA outperforms the BPM, SVM, and LR on five of the seven data sets indicating its ability to find good solutions and the robustness of 0 X 1 loss in the presence of outliers. Optimality of Solutions on Synthetic Data: Since SLA performed best overall, we perform one further analysis to understand how consistently it outperforms the baselines. 200 synthetic datasets have been gen-erated with size N = 500, dimensionality D = 5, and optimal 0 X 1 loss in the range from 30 to 100. Figure 5 (left) shows the results of this test and (right) with 15% noise added. Clearly, noise and outliers adversely affected other algorithms to a greater extent than SLA. While other works have investigated classification methods that are robust to outliers, including t-logistic regression (Ding &amp; Vishwanathan, 2010), ro-bust truncated hinge loss (Wu &amp; Liu, 2007), Sav-ageBoost (Masnadi-Shirazi &amp; Vasconcelos, 2008), and (Collobert et al., 2006) (which shows that a non-convex piecewise linear approximation of 0 X 1 loss can be effi-ciently optimized), in this work we have chosen to di-rectly optimize 0 X 1 loss using search-based techniques (some with strong finite time guarantees) in contrast to the randomized descent approach of (Li &amp; Lin, 2007). Empirical results demonstrate the importance of 0 X 1 loss for its robustness properties while providing evi-dence that it can be effectively optimized in practice. Bartlett, Peter L., Jordan, Michael I., and McAuliffe, Jon D. Convexity, classification, and risk bounds. Technical Report 638, University of California, Berkeley, November 2003.
 Ben-David, S., Eiron, N., and Long, P.M. On the diffi-culty of approximately maximizing agree-ments. J. Comput. System Sci. , (66(3)):496 X 514, 2003.
 Collobert, R., Sinz, F., Weston, J., and Bottou, L.
Trading convexity for scalability. In In Proceed-ings of the Twenty-third International Conference on Machine Learning (ICML 2006) , pp. 201 X 208. ACM Press, 2006.
 Cortes, Corinna and Vapnik, Vladimir. Support-vector networks. Machine Learning , 20, 1995.
 Ding, Nan and Vishwanathan, S.V.N. t-logistic regres-sion. In Advances in Neural Information Processing Systems . NIPS, 2010.
 Fan, R.-E., Chang, K.-W., Hsieh, C.-J., Wang, X.-
R., and Lin, C.-J. Liblinear: A library for large linear classification. Journal of Machine Learning Research , pp. 1871 X 1874, 2008.
 Feldman, V., Guruswami, V., Raghavendra, P., and
Wu, Yi. Agnostic learning of monomials by half-spaces is hard. Preliminary version appeared in FOCS , 2009.
 Frank, A. and Asuncion, A. Uci machine learning repository. Irvine, CA: University of California, School of Information and Computer Science., 2010. URL http://archive.ics.uci.edu/ml .
 Hooke, R. and Jeeves, T.A. Direct search solution of numerical and statistical problems. Journal of the
Association for Computing Machinery , 8 (2):212 X  229, 1961.
 Land, A. H. and Doig, A. G. An Automatic Method for Solving Discrete Programming Problems. Econo-metrica , 28:497 X 520, 1960.
 Li, Ling and Lin, Hsuan-Tien. Optimizing 0-1 loss for perceptrons by random coordinate descent. In Pro-ceedings of the 2007 International Joint Conference on Neural Networks , 2007.
 Long, Phil and Servedio, Rocco. Random classification noise defeats all convex potential boosters. Machine Learning Journal , (78(3)):287 X 304, 2010.
 Masnadi-Shirazi, Hamed and Vasconcelos, Nuno. On the design of loss functions for classification: theory, robustness to outliers, and savageboost. In NIPS , 2008.
 McAllester, David. Statistical methods for artificial in-telligence. Lecture Notes for TTIC103, Toyota Tech-nological Institute at Chicago, 8 2007.
 Minka, Thomas. Expectation propagation for approx-imate bayesian inference. UAI , pp. 362 X 369, 2001. Wu, Y. and Liu, Y. Robust truncated hinge loss sup-
