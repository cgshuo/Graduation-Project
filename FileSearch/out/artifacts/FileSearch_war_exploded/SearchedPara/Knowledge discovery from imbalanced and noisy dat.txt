 Jason Van Hulse, Taghi Khoshgoftaar * 1. Introduction
Building data mining models using unreliable or abnormal datasets presents a significant challenge to classifier construc-tion. Regardless of the strength of a particular classification algorithm, learning from poor quality data will result in sub-opti-mal performance. In a classification problem, for example, numerous studies have shown that the presence of errors in the training dataset lowers the predictive accuracy of a learner on test data [27,46,50] . There are many different dimensions of data quality, including class noise or labeling errors [7,29] , attribute noise [42,50] , and missing values [32].
Another commonly-encountered challenge in data mining applications is the occurrence of class imbalance. A dataset used for binary classification tasks is imbalanced if the number of positive examples is unequal (and typically less than) the number of negative examples (this work only considers binary classification problems). Positive examples are typically defined as the minority class, while negative examples belong to the majority class. While class imbalance is not necessarily a data quality issue, it can at times be related. For example, when constructing datasets for use in data mining analysis, it is common in some application domains for the observations to be given the negative class by default unless some event oc-curs, in which case the example is labeled as positive. If the recording mechanism is faulty or if human error is involved in recording the occurrence of the positive class, there may be very few records of the event, and hence few examples in the training dataset are labeled as positive (even though other positive class examples exist, they may go unrecognized and are mistakenly labeled as negative). In other words, data quality issues may in some situations actually cause class imbalance (or at least make the problem more severe).

Consider, for example, the domain of software quality classification, where data mining techniques have been success-fully applied to the problem of classifying faulty program modules [24,26] . The observations in a software quality classifi-cation dataset are program modules, and the attributes are measurements of the program module characteristics. These measurements consist of code metrics and process metrics. Code metrics, which measure features related to the program module such as the number of lines of code , number of unique operands ,or number of branches , are generally recorded by soft-ware tools. Process metrics, on the other hand, may be reported by people working on the project and therefore tend to be error prone. In software quality classification, the dependent variable indicates if the program module contained more than c faults or bugs, where c is set by a practitioner or domain expert depending on the objective of the analysis. A module is given a class label of fault-prone if it contained more than c faults, while it is labeled not fault-prone if it contained no more than c faults. The class label is a process metric, with the responsibility of recording the faults in the modules often falling on the software engineer responsible for writing the original code, and is highly likely to be inaccurate and undependable. There is often a strong disincentive to accurately report software faults, since a large occurrence of software faults may highlight poor performance of the software development team. In particular, it is more likely that examples that should be labeled fault-prone are (accidentally or intentionally) mislabeled not fault-prone .

Inaccurate reporting of software fault data has two important impacts on the training dataset used for data mining in software quality classification. First, there are fewer fault-prone examples in the dataset, contributing to the class imbalance (software quality classification datasets typically favor the not fault-prone class). Second, labeling errors may not be uni-formly distributed in both classes -in fact, it is more likely that examples are mistakenly labeled as not fault-prone when they should have been fault-prone , and it is relatively rare that fault-prone examples were mistakenly labeled. A software engineer is unlikely to mistakenly report a module with no faults as fault-prone , and further, it is possible that a module con-tains a bug but the error has not yet been recognized. Consideration of the distribution of labeling errors among the two clas-ses is an important component of this work that to our knowledge is unique.

Although this example is specific to software engineering, this phenomenon is common in many other application do-mains, such as network intrusion detection or fraud detection. We further contend that the issues of data quality [13] and class imbalance [52] significantly impact the reliability of data mining models in real-world applications, and therefore deserve careful consideration and experimental evaluation. The primary objective of our experiments is to address these two issues, specifically examining class noise in the context of imbalanced data. 1.1. Research objectives
The motivation behind the experiments conducted in this work is as follows. Class imbalance is an issue encountered by data mining practitioners in a wide variety of fields, and the use of sampling techniques to improve classification perfor-mance has received significant attention in related work [11,20,46] . At the same time, real-world data often suffers from data quality issues, specifically class noise. While the practitioner may suspect that there are data quality issues, the extent and severity of the issues are typically unknown. Without full awareness of the extent of class noise in the data, the practitioner may attempt to sample the imbalanced data prior to constructing classification models for the purposes of obtaining an al-tered class distribution (and often without regard to the issue of class noise).

Another perspective for our motivation comes from considering related work regarding class noise. A number of studies have concluded that classifiers will be adversely impacted by class noise [6,50] , however, the impact of class noise in imbal-anced datasets has received relatively little attention. Although some research has discussed this issue [46,30,2] , comprehen-sive empirical analysis is still lacking.

Given these motivations, a number of interesting research questions arise that are of great interest to both researchers and practitioners: Q 1 : What is the impact of class noise on learners constructed using skewed datasets? Q 2 : Is the class in which the noise is located significant? Q 3 : Are the effects of class noise in imbalanced datasets uniform across different learning algorithms?
Q 4 : How do sampling procedures, which are often used to alleviate class imbalance, perform in the presence of class
Q 5 : Do different sampling techniques work better with different levels of noise and with noise represented in different
Q 6 : What guidance can be given to improve the commonly-used classification filters
Therefore, the primary objectives of this work are to explore the impact of class noise when learning from data with a skewed class distribution and to investigate the effectiveness of commonly-used data sampling techniques. 1.2. Experimental dimensions
In addressing these research questions, this work provides significant contributions to the disciplines of data quality and learning from imbalanced data. Unique and significant aspects of these experiments are:
We utilize a distinctive noise simulation methodology as described in Section 3. Starting with five real-world datasets from the software engineering domain exhibiting skewed class distributions, a learnable subconcept is extracted and class noise is injected relative to this subconcept. In addition, two larger, publicly available datasets from the UCI repository [1] are also injected with class noise.

Class noise is injected into the extracted datasets using two parameters, the overall level of class noise  X  K  X  and the per-centage of class noise resulting from the corruption of instances from the true the minority class  X  W  X  . Noise injection occurs by corrupting pre-existing instances in the datasets as opposed to adding new, noisy instances. Essentially, our noise model consists of two parameters  X  K ; W  X  where K determines how many instances are corrupted with noise and
W determines the relative proportions of noise in each of the two classes. As will be shown, W is the critical parameter of our simulation that has, to our knowledge, never been explored in previous work.

We utilize 11 different classifiers (see the Appendix Section A for more details) of the types commonly-used in data min-ing applications, including decision trees (denoted C4.5D and C4.5N), neural networks (denoted MLP and RBF), logistic regression (LR), rule-based learners (RIPPER), nearest neighbor learners (2NN and 5NN), support vector machines (SVM), na X ve Bayes (NB), and random forests (RF).

Seven different sampling techniques-random undersampling (RUS), random oversampling (ROS), SMOTE (SM), border-line-SMOTE (BSM), one-sided selection (OSS), cluster-based oversampling (CBOS), and Wilson X  X  Editing (WE)  X  are consid-ered in our experimentation. All of these techniques have been used in previous research to alleviate class imbalance. Additional details regarding these sampling techniques is provided in the Appendix Section B.

The noise injection process is repeated multiple times for each dataset, leading to the construction of over two million classifiers in these experiments.
 Results are reported using two different performance measures: the area under the ROC curve (AUC) and Kolmogorov X 
Smirnov statistic (KS). Both measure the general ability of the learner to separate positive and negative class examples, and in particular do not suffer from the well-known issues associated with evaluating learners using the overall misclassi-fication rate [35].

All results include an analysis of their statistical significance using analysis of variance (ANOVA) techniques. ANOVA is useful in settings where numerous experimental factors are being compared, and a researcher would like to understand the impacts of the factors on the performance measure. Without using ANOVA techniques, it is more difficult to rigorously evaluate the impact of different factors on the outcome.

Our research group strongly advocates the use of robust, well-designed, and statistically valid experimentation to evaluate the relative effectiveness of various techniques and the impacts of different experimental factors on the procedures of interest. There has been a significant amount of work in data mining related to the proposal of different techniques, but the relative lack of systematic comparisons of these techniques has provided data mining researchers and practitio-ners little guidance on what does and does not work. We believe that this work represents a significant step forward to empirically understanding class noise and imbalance, and their impacts on learning.

This work significantly expands upon previously presented results [44] that relate to learning from noisy and imbalanced data. For example, this previous work did not consider the impacts of data sampling, considered only a single performance metric (AUC), and due to severe space restrictions, only limited results were presented. In addition, this work considers two larger datasets from the UCI repository. 1.3. Paper organization
The remainder of this work is organized as follows. Related work on imbalance and noise handling is provided in Section 2. Section 3 gives a detailed description of our experimental design. The experimental results are presented in Sections 4 and 5. A thorough discussion of the empirical conclusions, specifically addressing our research questions, and threats to empirical validity, are provided in Section 6. Conclusions and future work are provided in Section 7. The Appendix includes a brief description of the learning algorithms and sampling techniques used in this work. 2. Related work
Previously published work relevant to our study is presented in this section. There has been much previous work related to both class imbalance and class noise, however limited attention has been given to the problem of learning from imbal-anced and noisy data. Section 2.1 considers noise in classification problems while Section 2.2 covers the impact of class imbalance on learning and methods for handling imbalance, a topic which has seen substantial attention. Finally Section 2.3 discusses the unique contributions of this work.
 2.1. Noise handling
An investigation of both class and attribute noise was conducted by Zhu and Wu [50] which concluded that the presence of both class and attribute noise can be harmful to a classifier, with the former having the more severe impact. The effect of noisy data in the context of cost sensitive learning was also analyzed by Zhu and Wu [51]. The authors concluded that a clas-sifier built on noisy data will increase the average cost of misclassification when the classifier is applied to previously unseen instances (i.e., test data). A recent study [15] compared the performance of numerous classifiers in the context of noisy data and found that robustness to noise varies greatly depending on the learning algorithm. Li et al. [31] proposed a fuzzy rele-vance vector machine to deal with the problems of imbalance and noise.

Some studies have considered noise handling as part of the data preprocessing steps undertaken before the construction of a classifier. One of the most common procedures, the classification filter [6,16] , uses an n -fold cross validation procedure, where a learner built on n 1 of the partitions is used to classify the data in the holdout partition. Instances in the holdout partition that are misclassified by a base classifier are determined to contain class noise. The procedure is repeated n times so that each partition is used as holdout data once.

It is possible that the learner selected for use in the classification filter does not have an appropriate bias and therefore misclassifies instances that are correctly labeled but are simply difficult for the learner to classify. To counteract this possi-bility, multiple classifiers can be combined into an ensemble filter . Those instances that are misclassified by at least a min-imum number of learners are considered mislabeled. A consensus filter, for example, requires that all of the classifiers mislabel an instance for it to be considered noise, while a majority filter requires misclassification by a majority of the learn-ers. Our research group recently provided a detailed examination of ensemble filtering [29] using 25 different classification techniques.

The rule-based classification model (RBCM) [28] was proposed as a noise filter in our recent work [27]. RBCM uses Bool-ean rules derived from the most significant attributes as determined by the Kolmogorov X  X mirnov (KS) test. Critical values are calculated for each significant attribute using the KS statistic. Boolean rules are formed using the critical values for each significant attribute, and rules are labeled as belonging to one class or another based on the number of  X 1 X  and  X 0 X  bits in the binary encoding of the rules. Instances that satisfy a rule of the opposite class are labeled as noise. Numerous other works have investigated data noise in data mining and classification problems [41,36,39] , however none specifically consider noise in the context of class imbalance. 2.2. Methods for handling class imbalance
Sampling techniques have also been studied in previous work, but not from the perspective of class noise. Since they are simple to implement and have existed for the longest time, random undersampling and random oversampling seem to have received the most attention. Drummond and Holte [11] found using C4.5 that majority undersampling is more effective at dealing with the imbalance problem and that minority oversampling often produces little or no change in performance. Mal-oof [33] performed experiments using both na X ve Bayes and C5.0 (the commercial successor to C4.5) and found that under-sampling and oversampling produce classifiers that are roughly equivalent to each other. Japkowicz [19] used neural networks and artificially generated one-dimensional data and found that oversampling and undersampling perform simi-larly on smaller datasets, while undersampling performs better on larger datasets. A later study by Japkowicz and Stephan [21] found that for C5.0, oversampling performs better than undersampling. Weiss and Provost [47] do not compare sam-pling techniques, but use random undersampling and C4.5 to evaluate the effect of class distribution on classifier perfor-mance. They concluded that there is no universally optimal distribution for ideal performance, and that it depends on the domain.

Chawla et al. [10] used C4.5, na X ve Bayes, and RIPPER to test their proposed SMOTE technique against random undersam-pling, one-sided selection, and varying the classifier X  X  decision threshold. They found SMOTE to generally outperform all of the other methods. Han et al. [17] evaluated borderline-SMOTE against regular SMOTE and random oversampling using C4.5 and found borderline-SMOTE to perform the best, followed by SMOTE and then random oversampling. Jo and Japkowicz [22] used both C4.5 and neural networks to compare cluster-based oversampling, random oversampling, random undersampling, pruning small disjuncts from the training data, and an oversampling technique that generates new artificial examples by adding random Gaussian noise to the data. Their experiments found cluster-based oversampling to perform the best out of all the methods. Prati et al. [34] used C4.5 to perform experiments comparing random oversampling, SMOTE, the combi-nation of oversampling with SMOTE and undersampling using Tomek links, and the combination oversampling with SMOTE and undersampling with Wilson X  X  editing. They found the two combinations to work well, though often random oversam-pling was able to achieve comparable results. They also assert that oversampling generally performs better than undersam-pling. Batista et al. [3] compares ten different sampling techniques using 13 UCI datasets, concluding that oversampling outperform s undersampling, and two hybrid procedures SMOTE + Tomek and SMOTE + ENN, are particularly useful for data-sets with a small number of positive examples. Given all of the previous research and conflicting results, there has been little consensus reached on which sampling techniques work best and under what conditions. Our recent work [43] compared seven sampling techniques using 35 benchmark datasets, and demonstrated that random undersampling performs very well.
A number of studies have dealt with class imbalance from the perspective of noise reduction. Kubat and Matwin [30] pro-posed a technique called one-sided selection , which attempts to intelligently undersample the majority class by removing majority class examples that are considered either redundant or noisy. Wilson X  X  editing [2] uses the kNN technique with k  X  3 to classify each example in the training set using all the remaining examples, and removes those majority class examples that are misclassified. Other procedures for handling class imbalance, such as SMOTE, cluster-based oversampling, and ran-dom sampling do not address the issue of noise.

Cost sensitive learning is closely related to the problem of class imbalance. Weiss et al. [45] compare the performance of oversampling, undersampling and cost sensitive learning when dealing with data that has both an imbalanced class distri-bution and unequal error costs. A wrapper-based sampling approach to minimizing misclassification cost is evaluated by
Chawla et al. [9]. Boosting algorithms are also commonly-used to alleviate the impacts of class imbalance [12,38,23] . Cao et al. [8] proposed techniques for discovering rare but significant impact-targeted activity patterns in imbalanced activity data. Future research could consider detailed experimentation on the impact of noise on cost sensitive learning, boosting, and unsupervised learning tasks such as association rule mining, as this work focuses on data sampling for supervised learn-ing applications. 2.3. Summary
In one of the few works that address noise in an imbalanced data environment, Weiss [46] argues that  X  X  X oise has a greater impact on rare cases than on common cases X  and that relatively high levels of class noise were required to cause rare cases and small disjuncts to be error prone. Weiss, however, utilized simulated data with binary attribute values, a single learner,
C4.5, and did not consider the very important factor of which class contains noise (consideration of this factor is unique to our work). Further, the objective of Weiss X  work [46] was to analyze various factors (e.g., attribute noise, missing values, training set size, and class noise), whereas this work concentrates solely on class noise and its relation to imbalanced data.
Another differentiating factor is that we consider many different learners and examine the utilization of sampling techniques in the context of noisy and imbalanced data. Our previous, preliminary work [44] did consider the problem of learning from noisy and imbalanced data, however, sampling techniques were not considered and fewer datasets were used. In addition, this work dramatically expands upon the results we have presented previously [44].

Therefore, while class noise has received attention in data mining literature, to our knowledge no previous work has ana-lyzed labeling errors in imbalanced data as systematically as in this work. Similarly, though numerous strategies for coping with skewed class distributions have been proposed and analyzed, the impact of class noise in an imbalanced dataset has not been adequately addressed. Noise handling in the presence of class imbalance has therefore received relatively little atten-tion in related work. Most experiments with the classification and ensemble filters have paid no attention to class distribu-tions and the need for special attention when one class is relatively rare. The important connections between this and previous work by researchers in the domain of imbalance is discussed in Section 5. A contention of numerous researchers disjuncts, overlapping classes, absolute rarity (i.e., a too few positive cases), or concept complexity, in combination with imbalance, cause the poor performance obtained by classifiers. This work shows that a related concept, class noise, can also harm classifiers trained from imbalanced data. Further, class noise can be one of the causes of all of the previously mentioned factors. In this respect, our work has a significant connection with previous work in the domain of imbalance. 3. Experimental design 3.1. Datasets
The software measurement data used for our study are from five NASA software projects, CM1, MW1, PC1, KC1, and KC3 [28]. The data was made available through the Metrics Data Program at NASA, and included software measurement data and associated error (fault) data collected at the function, subroutine, or method level. Hence, for the software systems, a func-tion, subroutine, or method is considered as a software module or an instance in the dataset.

The CM1 project, written in C, is a science instrument system used for mission measurements. The MW1 project, written in C, is the software from a zero gravity experiment related to combustion. The PC1 project, written in C, is flight software from an earth orbiting satellite. The KC1 and KC3 projects are different components of the same mission project, however they constitute different personnel and have no software overlap other than minimal third-party software libraries. The
KC1 system, written in C++, is a software component of a large ground system. The KC3 system, written in JAVA, is software developed for collection, processing, and delivery of satellite metadata.

The fault data collected for the software systems represent faults detected during software development. Each module was characterized by 13 software measurements [14] including basic measures such as the number of lines of code, number of branches, and the number of operators and complexity measures such as the cyclomatic complexity, essential complexity, and design complexity. The quality of the modules is described by their class labels. A program module was considered fault-prone if it had one or more software faults, and not fault-prone otherwise. The positive (or minority) class in software mea-surement datasets is the fault-prone class, while the not fault-prone class is the negative or majority class.
Two datasets from the UCI repository, Optdigits and Nursery, were also utilized in our experiments. The Optdigits dataset, which has 64 independent variables, can be used to train models to recognize handwritten digits from zero to nine. The digit  X 8 X  was selected as the positive class, and the remaining digits constitute the negative class. The Nursery dataset, which has eight attributes, contains information for ranking nursery schools. The dataset originally contained five classes, one of which ( X  X ery recommended X ) was selected in this work to be the positive class, while the remaining class values were grouped to-gether to form the negative class. Both Optdigits and Nursery are larger than the five software measurement datasets, and
Nursery is the most imbalanced of the seven datasets considered in this work. These datasets therefore provide a wide range of sizes, imbalance levels, and application domains. Throughout this work, the five software engineering datasets will be de-noted as the SE datasets, while Optdigits and Nursery will collectively be referred to as the UCI datasets. Note that all of the datasets considered in this work have a binary class, and consideration of multi-class domains is left to future work. 3.2. Data preprocessing
One way to conduct the required simulation is to inject class noise into unprocessed, real-world datasets. This can be problematic, however, because data from real-world application domains may already contain class noise. approach with the SE datasets was to first create a subset D encapsulated in D c is learnable. In other words, learners constructed using D tion accuracy. Once D c is derived, noise injection can be performed relative to this new dataset. In other words, class noise is injected relative to the target concept in D c .

To create the datasets D c , the RBCM-based filter [28], proposed in previous work by our research group and described briefly in Section 2, was used. Numerous classifiers were constructed using the filtered datasets D sified examples. Therefore, a set of transformed datasets have been created with a learnable target concept, with the exam-ples in the dataset conforming to that concept. Table 1 shows the characteristics of the original datasets in columns 2, 3, and 4, and the characteristics of the transformed datasets in columns 5, 6, and 7. The SE datasets used in our study have the per-centage of minority examples ranging from 6.43% to 19.87%, and are therefore imbalanced with respect to the class. For the Nursery and Optdigits datasets, unlike the five SE datasets, cleansing was not undertaken prior to noise injection.
Our previous experience has shown that these two datasets are relatively free from class noise, and hence preprocessing was unnecessary. The different treatment prior to noise injection for the UCI datasets compared to the SE datasets greatly ex-pands the scope and applicability of our results. Further, since the characteristics of these two sets of data are so different, empirical results are presented separately in later sections of this work. 3.3. Noise injection
After the datasets have been prepared, class noise must be injected based on two considerations. First, how much noise should be injected into the dataset? Second, in what proportions should noise be injected into each class? The first consid-eration is easily justified  X  based on the data generation process, datasets from different domains may have different levels of noise. The second idea is more subtle and may require some justification. We contend that noise may be inherent in a dataset in varying degrees among the two classes and that it is too simplistic to assume, for example, that in all situations, half of the noisy examples are from the positive class and half are from the negative class. The introduction of this work provided an example specific to the software engineering domain where noise is not uniformly inherent in both classes, and we believe this situation is common in other domains as well. Therefore, this work considers the impact of varying the class distribution of noise, which has been overlooked in previous studies.

Our experiments consider two noise simulation parameters that address these ideas. The first governs the level of class noise in the dataset and is denoted K  X  K  X  10 ; 20 ; 30 ; 40 ; and 50 %  X  . The second parameter is the percentage of class noise resulting from the corruption of instances from the true minority class, denoted W  X  W  X  0 ; 25 ; 50 ; 75 ; and 100 %  X  . Let D note D c after the injection of class noise. Define In our simulation, the total number of noisy examples corrupted in each dataset is given by 2 K S
KC1 dataset has 271 P and 1093 N examples. At a 30% noise level  X  K  X  30 %  X  , KC1 will be corrupted in 2 30 % 271  X  163 instances. Which 163 instances should be selected for corruption? This is controlled by the second parameter W : randomly select 2 K S P W examples from P and corrupt the class to N , and randomly select 2 K S and corrupt to class P . Continuing the example with KC1 and 30% noise, suppose W is set to 75%. Then 163 75 %  X  122 P examples are corrupted to class N , and 163  X  1 75 %  X  X  41 N examples are corrupted to class P .

Given this construction, S E  X  2 K S P . With W  X  50 % , the injection of class noise is identical to the proportional random corruption (PRC) procedure [51]. With W  X  50 % , the percentage of positive class examples before and after noise injection is equal. By varying the parameter W , we are able to analyze the impact of varying the degree of noise coming from the positive class on the learners. Note that to the best of our knowledge, previous work has not considered varying the parameter W , which as will be shown, is critical to understanding the impact of class noise.

Let P ! N be the set of examples whose class is corrupted from positive to negative, while N ! P are the examples where the class is corrupted from negative to positive:
Then E  X  X  P ! N [ X  N ! P , i.e., the set of noisy examples consists of those whose class is corrupted from positive to negative, and those whose class is corrupted from negative to positive. Note that the parameter W defines the relative proportion of
P ! N and N ! P examples in E . With W  X  50 % ; j P ! N j X j N ! P j X  K j N ! P j X  2 K 0 S P .

The parameter K controls the level of noise in the dataset, although K  X  10 % , for example, does not mean that 10% of the total examples are corrupted. Since the data is highly imbalanced, this level of corruption would dramatically impact the minority class. Instead K is the level of noise relative to the size of the minority class. 3.4. Dataset noise characteristics
Table 2 presents the noise corruption statistics for the KC1 dataset, which prior to noise injection has 271 P and 1093 N examples. With K  X  10 % ; 54  X  X  2 10 % 271  X  examples are injected with noise. Since KC1 has 1364 total examples, 54 noisy instances is about 4.0% of the total dataset (columns 3 and 4 in Table 2 ). With W  X  0 % ; P ! N  X ; (column 5) while N ! P has 54 examples (column 8). Note that 100% of the positive examples were retained in columns 6 and 7 (since
P ! N  X ; ), while approximately 5% of the negative class was corrupted (columns 9 and 10). In the final dataset after the corruption process was completed (the last five columns of Table 2 ), there are 325 positive examples, of which 16.7% or 54 are noisy (the true class of these instances is negative, even though they are labeled positive).

The last row of the table, K  X  50 % and W  X  100 % , results in a dataset with no positive examples, and hence was removed from the experiments. Note that for a fixed value of K , increasing W results in more noise coming from the positive class, i.e.,
P ! N increasingly dominates N ! P . Fewer examples from the original positive class remain after corruption (the values in the columns labeled  X  X lean P Left X  are decreasing for a fixed value of K ), and the positive class concept is obscured by exam-ples that are mislabeled as negative but should be positive. With K  X  40 % and W  X  100 % , for example, of the 271 examples from the original positive class, 217 are corrupted to the negative class. Note that after corruption of KC1, the percentage of positive examples ranges from 4.0% to 39.7%. 3.5. Performance measures
Two measures of classifier performance are presented in this work, the area under the ROC curve (AUC) and the Kolmogo-rov X  X mirnov (KS) [18] statistic. KS measures the maximum difference between the cumulative distribution functions of the predicted probabilities of examples in each class. In other words, KS measures how far apart the distribution functions are, and hence how well the learner is able to separate positive and negative class examples. Denote the classes as N (negative) and P (positive). Each instance x is assigned by the learner a probability of class P membership p  X  P j x  X 2 X  0 ; 1 (assuming the learner outputs on a probability scale, which is true of all learners in this study). The true class P and N distribution functions F  X  t  X  and F N  X  t  X  are unknown, but are estimated by the proportion of class P or N examples with p  X  P j x  X  b F defined as:
KS can also be calculated as the maximum difference between the curves generated by the true positive and false positive rates as t changes from 0 to 1. F P  X  t  X  can be further understood as follows: decision threshold t . Similarly, it can be shown that b F
The larger the distance between the two distribution functions, the better the learner is able to separate the two classes. The maximum possible value for KS is one (perfect separation), with a minimum of zero (no separation). The KS statistic is a com-monly-used performance measure in the domain of credit scoring [40].

The receiver operating characteristic (ROC) curve graphs the true positive rate on the y -axis versus the false positive rate on the x -axis. The ROC curve illustrates the performance of a classifier across the entire range of decision thresholds, and accordingly does not assume any particular misclassification costs or class prior probabilities. For a single numeric measure, the AUC is often used, which gives a general idea of the predictive potential of the classifier. A higher AUC is better, as it indicates that the ROC curve obtains higher true positive and lower false negative rates. We chose to use these two metrics (AUC and KS) because they measure the general ability of the learner to separate the two classes and are independent of the choice of decision threshold. 3.6. Design
This section provides a detailed description of our experimental design. Seven datasets were used in our experiments, and all learners were constructed and evaluated using 10-fold cross validation (CV). When building a learner, nine folds were used as a training dataset with the remaining fold acting as a test dataset (using CV). Class noise was injected into the train-ing dataset only, using the five different values for K and five different values for W , representing the overall level of noise and percentage of class noise resulting from the corruption of instances from the true minority class, respectively. For each of the five SE datasets 10-fold CV was repeated a total of ten times, each time with an independently chosen set of examples for noise corruption according to the different values of K and W . For Nursery and Optdigits, 10-fold CV was repeated three times due to the significantly larger size of these datasets and the extensive execution time that ten repetitions would have re-quired. One caveat to this design is that K  X  50 % and W  X  100 % could not be accommodated, because this scenario resulted in a dataset with no positive examples. Therefore, this case is excluded.

In total, for the SE datasets, 5 datasets 10 runs of 10-fold CV results in 500 uncorrupted training datasets. With K = 10, 20, 30, and 40% all five different values of W (0, 25, 50, 75, and 100%) were applied, while for K  X  50 % , only W = 0, 25, 50, and 75% were used. Therefore, for each of the 500 uncorrupted datasets, 24 different corruption schemes were used, resulting in 500 24  X  12 ; 000 corrupted datasets. For each of these training datasets, 17 combinations of sampling technique plus parameter (which includes no sampling) were applied, and 11 learners were constructed on each of these datasets (Sections
A and B provide more information on the sampling techniques and learners). Therefore in total, 17 11 12 ; 000  X  2 ; 244 ; 000 classifiers 3 were built in these experiments for the SE datasets.

For the UCI datasets, 2 datasets 3 runs of 10-fold CV results in 60 uncorrupted datasets. 24 corruption schemes were used, resulting in 60 24  X  1440 corrupted training datasets. Nine combinations were used in conjuction with 11 learners, resulting in the construction of 1440 9 11  X  142 ; 560 classifiers for the two
UCI datasets. Experiments were conducted using several high performance computing clusters and a supercomputer with very large memory. Even with these significant hardware resources available, many months of execution time were required to com-plete the experiments presented in this work, and hence judicious choices regarding parameter selections were required. AUC and KS measures are presented to assess classifier performance because they measure the general ability of the learners to sep-arate the two classes and are independent of the choice of the decision threshold. Table 3 provides a summary of the notation used throughout this work.

Note that all reported results for sampling techniques are optimized over the parameter values for each dataset and lear-ner. For example, for dataset KC1 with W  X  50 % and K  X  20 % with classifiers built using SVM, RUS with 35% generated ten
AUC values (because there are ten repetitions of 10-fold CV), RUS with 50% generated ten AUC values, and likewise for RUS 65%. If RUS with 35% generated the highest average AUC for KC1, W  X  50 % and K  X  20 % , then RUS with 35% is reported in the results for RUS for that particular dataset (and in particular, RUS 50% and RUS 65% are not reported). It is possible that given the same dataset with a different learner, for example C4.5D, RUS with 50% might generate the highest average AUC.
Generally speaking, 35% often produced the best results in our experiments, which matches with the conclusions of a pre-vious study [25]. Note, however, that analyzing different undersampling or oversampling percentages is not one of the objec-tives of this study.

The results presented in the remainder of this work generally represent averages of the AUC and KS performance metrics over the five SE datasets or two UCI datasets. Averaging across datasets, as opposed to presenting results for each dataset separately, was performed to enhance readability, reduce redundancy, and for space considerations. The conclusions dis-cussed in this work generally hold for all of the datasets individually. Further, ANOVA analysis, which is used to determine the statistical significance of the findings, does take into account cross-dataset variance. We do, however, present the results separately for the five SE datasets and the two UCI datasets, which gives the reader confidence that the results are generally valid across datasets and application domains. 4. Experimental evaluation I: learning from imbalanced and noisy data
This section explores the performance of the 11 classifiers when learning from imbalanced and noisy data, and varying the levels of the two factors K and W . All results in Section 4 do not consider sampling. The objective of this section is to analyze the impact of noise and class imbalance on learning, and the effects of sampling are considered in Section 5. We only present the results for the AUC performance measure, as the empirical conclusions obtained by considering the KS are similar.

Fig. 1 displays the AUC for the different values of K and W for each learner built without using sampling, averaged over five software engineering (SE) datasets. Fig. 2 displays the same information for the two UCI datasets. The x -axis in both fig-ures is W , the percentage of noise coming from the positive class (labeled  X  X in0 X , ... ,  X  X in100 X ), with the y -axis representing the AUC. Each level of noise K is represented by a different line (labeled  X  X 10 X , ... ,  X  X 50 X ). Note that each learner is presented relative to a different scale to make the trends more visible, so care must be taken when comparing trends for different learners.

Many of the learners display a similar  X  X an X  pattern. With W  X  0 % , regardless of the level of noise K , the performance of the is relatively horizontal, meaning that the AUC does not dramatically decrease. As K increases, the slope of the line becomes steeper as W increases for almost all learners. For K  X  40 % or 50 % and W P 50 % , the deterioration in AUC is significant for most learners. As anticipated, K  X  50 % results in the worst performance for all learners. There is a strong inverse correlation between K and AUC across W  X  X s K increases, the performance of the learner decreases, though this impact is more signif-icant with higher values of W . In addition, increasing the percentage of minority class noise generally decreased classifier performance.

For all of the learners, 50% class noise with W  X  0 % (i.e., 0% of the noise resulting from the corruption of instances from the true minority class) does not significantly harm classifier performance, whereas W  X  75 % with 50% class noise dramatically tates the impact of labeling errors on classification using imbalanced data. This observation is discussed further in Section 6.
Table 4 presents for each learner (without sampling), the average AUC over five measurement datasets (left-hand side of the table) and two UCI datasets (right-hand-side of the table) for different levels of noise K (for each noise level, the AUC is averaged over all values of W ). For all learners, the AUC decreases (often dramatically) as the level of noise increases from 10% to 50%. Much of the reduction occurs as K increases from 30% to 50%. There is substantial variation in the impact of class noise on the learning algorithms. C4.5 performs better with Laplace smoothing and without pruning compared to the stan-dard C4.5 decision tree algorithm (i.e., C4.5D), even at higher noise levels, contrary to expectations. Ideally, pruning should help avoid overfitting in noisy data, however our results clearly demonstrate that pruning and/or non-smoothed probability estimates may in fact hurt the predictive capability of the decision tree learner. 5NN generally outperforms 2NN, especially at higher noise levels, which might be expected since 5NN considers more nearest neighbors and hence is less likely to be impacted by noise. NB is the most stable learner of the 11 considered in this study. Two of the newer and more sophisticated techniques, RF and SVM, are impacted more by class noise than some of the simpler algorithms such as NB and 5NN.
Table 5 presents the average AUC over the five SE datasets and two UCI datasets for different levels of W (i.e., P ! N -type noise) for each learner (for each level of W , the AUC is averaged over all levels of K ). The performance of many learners dete-riorates significantly as W increases. C4.5D and RIPPER are the two learners which exhibit the most significant and consistent deterioration in performance, while other learners such as NB, and to a lesser extent 2NN, 5NN, and MLP, are relatively unaffected.

Fig. 3 shows the AUC, averaged over all five SE datasets, for each learner (without sampling) for the different levels of W (leftmost figure) and K (rightmost figure). For a given learner, if the points are clustered closely together, there is less of an impact on that learner when varying either W or K . For example, the AUC for NB is very similar regardless of W or K . C4.5N and MLP also perform well as K and W vary, while at the opposite extreme, RBF, RIPPER, and C4.5D are all severely impacted.
SVM displays an interesting trend, being a very good learner when K or W are small, but as either of these parameters in-creases, the performance of SVM deteriorates.
 4.1. Analysis of variance: learning from mislabeled and imbalanced data
In order to analyze the statistical significance of the factors used in our experiments, analysis of variance (ANOVA) tech-niques are utilized. ANOVA models are useful because the variability in performance can be divided among the different experimental factors, allowing the factors which significantly contribute to changes in the performance to be determined. A three factor, full factorial ANOVA model [4] can be written as where:
H ; K ; W are the three main factors or effects in the experiment, the learner, percentage of overall noise, and percentage of minority class noise, respectively (sampling is not considered in this ANOVA).
 minority class noise, k  X  1 ; ... ; 5.  X  H K  X  ij ;  X  HW  X  ik ;  X  K W  X  jk are two-way interaction terms between the main effects.  X  H K W  X  ijk is a three-way interaction term between the main effects.
 five datasets, each with ten different cross validation runs, l  X  1 ; ... ; 50. If the experiments are considered in an 11 5 5 design matrix, with each of the main factors representing a different dimension in the matrix, then each cell of the matrix has 50 observations. ijkl is the random error.

The ANOVA model can be used to test the hypothesis that the AUC for the main factors H is accepted, numerous procedures can be used to determine which of the means are significantly different from the others.
This involves the comparison of two means, with the null hypothesis that the means are equal. Multiple comparison tests in this work utilized Tukey X  X  Studentized Range (Honestly Significant Difference or HSD) test. All tests of statistical significance utilize a significance level a of 5%. A second ANOVA model was constructed using the KS as the dependent variable. The re-sults were similar to that of the AUC, and hence the details are omitted.

Table 6 presents the ANOVA table for our experiment. The first column lists the seven experimental factors, three main effects, three two-way interactions, and one three-way interaction, followed by the degrees of freedom (DF), sum of squares (SS), mean square (MS), F-statistic, and the p -value of the F-statistic. Two ANOVA models were constructed, one for the five
SE datasets and another for the two UCI datasets. All of the effects are statistically significant, with a p -value much less than 5%. The three main effects are analyzed in Tables 7 X 9 for the SE datasets and Tables 10 X 12 for the UCI datasets, all of which contribute significantly to variability in the AUC based on the p -values in Table 6 . Note that for each of these tables, the AUC is averaged over all values of the other variables. For example, in Table 7 , the average AUC for all 11 learners, 5 datasets, 10 runs of cross validation, and 5 levels of W for K  X  10 % is 0.981 (note N = 2750, which is 11 learners 5 datasets 10 runs of cross validation 5 values of W ).

Tables 7 and 10 show the overall level of noise K . The column entitled  X  N  X  represents the number of observations, the third column is the mean AUC, and the last column is the HSD grouping. If two or more levels of a factor have the same block letter, then they are not significantly different, with an a  X  5 % confidence level. Tables 8 and 11 show the percentage of noise from the minority class W . In general, as W increases, the AUC decreases. The only deviation from this trend is that W  X  75 % has a lower AUC than W  X  100 % . This is explained, however, by the fact that K  X  50 % could not be implemented with W  X  100 % .
By considering Figs. 1 and 2 and in particular the subfigure entitled  X  X ll Learners X , it is clear that for K forms worse than W  X  75 % . The third main effect, the learner H , is given in Tables 9 and 12 .

The ANOVA model included three two-way interactions and one three-way interaction between the main effects. An interaction between factors is significant if changes in one of the factors has a significant influence on the other. A graph of the interaction between the learners and level of noise (the interaction term H K ), is given in Fig. 3 (for space consid-erations, only the five SE datasets are included here). If there was no significant relationship between the learners and level of noise, then the lines in Fig. 3 would be approximately parallel to one another. In other words, changing the learner would result in the same change in AUC for different levels of class noise. As observed earlier, this is not the case, since for example the NB learner results in AUC values closely clustered for the five levels of noise, but other learners like RBF, C4.5D, and RIP-
PER exhibit more variability in the AUC as K changes. The interaction between the learners and noise from the minority class is also presented in Fig. 3 . This interaction was also significant in the ANOVA model, and it can be seen that there is a strong interaction between H and W . The power of applying ANOVA analysis is that it can be objectively determined if the relation-ship between factors is statistically significant. The interaction term K W is given in the bottom-right of Figs. 1 and 2 (in the panel labeled  X  X ll Learners X ). Significance of the three-way interaction term H K W implies that the two-way interaction
K W depends on the learner H . Two-dimensional graphs of the interaction term H K W were provided in Figs. 1 and 2 , and as discussed previously the relationship between K and W does depend on the learner being used. In other words, the
ANOVA model has validated the statistical significance of the remarks made in the previous section that some learners are more or less sensitive to different levels of noise and to the interaction between the level and distribution of noise. Indeed, the statistical tests of this section confirm that these behaviors are not anomalies. 5. Experimental evaluation II: using sampling to alleviate imbalance in noisy data
This section considers the utilization of sampling in the context of noisy and imbalanced data. For space considerations, we often include only the results for the AUC, however, the results using the KS statistic are typically similar. Since sampling techniques are often used to alleviate class imbalance, we would like to understand the effect of class noise on sampling. Tables 13 and 14 present the AUC and KS for each sampling technique and learner, averaged over all five SE datasets, while
Tables 15 and 16 display the same information for the two UCI datasets (the AUC and KS values are averaged over all values of K and W in these four tables). The value of the performance measure which results in the best performance for a given learner is underlined, with the worst performance in bold. WE and RUS generally perform very well (RUS in particular sub-stantially improves both C45D and RIPPER), while CBOS and OSS exhibit relatively weak performance for all learners. These results suggest that when constructing learners from noisy and imbalanced data, the application of sampling techniques, and in particular RUS and WE, will positively impact the classifier X  X  performance. Additional evaluation of this conclusion follows throughout this section as we explore the performance of sampling technique/learner combinations for different levels of K and W .

Tables 17 and 18 display the AUC for each learner and sampling technique by the overall level of class noise  X  K  X  , averaged over all values of W . RUS and WE are particularly strong with increasing levels of noise. In other words, RUS and WE, in par-ticular, perform very well when the data is both noisy and imbalanced. When the data is imbalanced but relatively clean (i.e., with K  X  10 % ), the improvement gained by the application of sampling is often small. Further, the impact of sampling de-pends on the learner  X  even at the 50% noise level, sampling does not significantly improve NB or LR, while at lower levels of noise, learners such as C4.5D and RIPPER can be improved dramatically by sampling. This suggests a significant three-way interaction between the learner, sampling technique, and level of noise.

Tables 19 and 20 present the AUC for each learner and sampling by the level of noise corrupted from the minority class  X  W  X  , averaged over all values of K . With W  X  0 % , sampling rarely provided significant benefit as far as increased AUC. As W increases, the effectiveness of RUS compared to the other sampling techniques increases dramatically. With W  X  100 % , RUS is the best sampling technique for seven learners in both Tables 19 and 20 . WE performs very well as a sampling technique with a lower level of minority class noise ( W  X  25 % and W  X  50 % ), but with more noise coming from the minority class, RUS is generally the best sampling technique.

One of the important conclusions that can be derived from Tables 17 X 20 is that imbalance by itself may not harm the classifier, but some other problem in combination with class imbalance is the actual cause. Most of the learners perform very well when the noise is less impactful (i.e., K and W are small), and applying sampling often does not improve performance.
Interestingly, this pattern is most evident in Tables 19 and 20 when W is small, i.e., when most of the noise results from the corruption of instances from the true majority class. For almost all learners, the improvements in AUC obtained by sampling occur primarily when the data contains a high percentage of impactful noise, i.e., higher levels of K and/or higher levels of W .
Other researchers have argued that class imbalance is not the primary problem, for example Japkowicz [20] argues that it is actually small disjuncts which degrade classifier performance. In a related study, Japkowicz and Stephen [21] show that con-cept complexity can also hinder classifier performance in imbalanced data. Batista et al. [3], by experimenting with 13 imbal-anced UCI datasets using the C4.5 learner, conclude that the problem is often related to learning with too few positive examples with confounding effects such as overlapping classes. Interestingly, class noise is related to all of these issues, since labeling errors can create small disjuncts (which are sometimes noisy when W  X  0 % , or the minority class concept can be-come disjointed when a large portion of the minority class is corrupted with W  X  100 %  X  see Section 6), make the target concept more complex to learn, and cause the classes to overlap with one another. We contend that class noise, which com-monly-occurs in many different application domains, can be the root cause of all of these issues, and hence detailed study of the impact of class noise when learning from imbalanced data is critical.

As either W or K increases, the relative performances of the sampling techniques change dramatically. With W  X  0 % , the baseline learner often performs better without any sampling technique. The relatively weaker performance of WE with
W  X  0 % is intuitive, because all of the class noise is in the positive class, and WE only removes examples from the negative class. Therefore WE with W  X  0 % will not reduce the level of noise in the dataset. With W  X  25 % , WE is clearly the best sam-pling technique as measured by the AUC, while the baseline learner without sampling is rarely optimal. As W increases fur-ther, the ability of WE begins to diminish, while RUS becomes the best technique most often with W  X  100 % . WE is most effective when there is a relatively small, non-zero percentage of noise corrupted from the minority class, while with more noise corrupted from the minority class (high values of W ), a more dramatic reduction in the majority class may be required.
As was discussed previously, learners are most harmed by high levels of P ! N noise, i.e., high values of W . This results in a large number of examples that are labeled negative, but should have been labeled positive. As more of the noise is in the negative class, it becomes increasingly important to handle. WE, which is targeting mislabeled examples from the negative class, may be insufficient to reduce noise as W increases, because the user has no direct control over the amount of noise that is eliminated. Therefore, a technique such as RUS may become more effective when the negative class has more noise. In this scenario, RUS has the effect of both balancing the class distribution and reducing noise from the negative class. For example, suppose the negative class has 1000 examples, 50 of which are noisy. Suppose RUS is performed to reduce the 1000 negative examples to 100, a 90% reduction of the negative class. Since the selection of examples are random, then it is expected that 45 of the 50 noisy examples (90%) may also be eliminated. WE may not be able to detect 90% of the noisy examples, and hence more noise remains in the dataset. When undersampling the majority class at very high levels, RUS may be more likely to remove a significant portion of the P ! N -type noise.

Fig. 4 shows the improvement in AUC obtained by the best sampling technique (on the y -axis) for each of the 11 learners for the different levels of W (on the x -axis). The line with the boxes is the AUC obtained by applying the best sampling tech-nique, while the line with  X   X  is the AUC obtained without sampling. Fig. 5 presents the improvement in AUC over the baseline learner for the different levels of K . These figures present the results for the five SE datasets only; similar results were ob-served for the two UCI datasets, and are hence omitted. The improvements obtained by using sampling as either W or K var-ied is dramatically different depending on the learner used. For some learners such as RIPPER, SVM, and C4.5D, sampling significantly improves the performance of the learner at higher levels of K or W . For other learners, for example NB, 2NN, and 5NN, there is little performance improvement obtained by sampling even at higher levels of noise. One explanation for this phenomenon is that some learners, after the application of sampling, may be better able to handle the effects of noise  X  increased concept complexity, small disjuncts, absolute rarity, and class overlap  X  than others. 5.1. Analysis of variance: sampling with skewed and noisy data In this section, we construct two additional ANOVA models with an extra factor, the sampling technique C . Therefore, the
ANOVA model in this section considers four main effects, learner H , the percentage of noise K , the percentage of noise from the minority class W , and the sampling technique C . We also include the six two-way interactions between the four main effects (three-way and four-way interactions were not used because the factors included capture the primary relationships we were interested in analyzing). Table 21 is the ANOVA table with the significance levels of the ten factors, all of which are significant with a p -value less than 5%.

Tables 22 X 25 include an analysis of the four main effects. The values presented in these tables are averaged over all of the other experimental factors. In Table 23 , RUS is the best overall sampling technique (averaged over all learners, levels of noise, and noise distributions), and is significantly better than WE. Both OSS and CBOS perform significantly worse than NONE.
Six interaction terms for the five SE datasets are graphed in Figs. 6 X 11 .In Fig. 6 , the average AUC by learner for each level of K is plotted. Fig. 7 shows the relationship between K and W , while Fig. 8 shows the interaction term H W . There is a strong interaction between K and W , which is confirmed by the F-statistic (which is relatively large for this interaction, 1047) from Table 21 (recall that non-parallel lines would indicate an interacting relationship between factors). Therefore, the impacts of class noise K are closely related to the level of W . For example, with W  X  75 % , 30% class noise will dramatically reduce the performance, while with W  X  0 % , 30% class noise has little impact on most classifiers. The statistical significance of this relationship as quantified by the ANOVA model confirms the importance of this interaction.
 The interaction term H K (Fig. 6 ) is relatively weak, as the lines are somewhat close to parallel. This is confirmed by the
F-statistics (70 and 15) from Table 21 , which though significant is not large relative to the other factors. Contrast this with the F-statistics for the interaction H W (356 and 21). Since these factors have the same number of degrees of freedom (40), they can be directly compared, and clearly H W is a more important experimental factor. This implies that W , the percent-age of minority class noise, has a more significant role in differentiating between learner performance that the overall level of noise K (these results are also evident from Table 6 , when we did not consider the effects of sampling). We have seen sup-port for this result in prior analysis, because some learners (2NN and 5NN in particular) performed well with W  X  100 % , while most others performed very poorly. These trends contribute to the significance of the interaction H W .
Similar to the comments made comparing the interactions H W and H K , Fig. 9 shows C K (F = 78 and 9) as a less significant interaction compared to C W ( F = 240 and 27) in Fig. 11 (which is once again confirmed by the F-statistics). In other words, W has a greater effect on differences in the performance of sampling techniques than K . The interaction H C , which is also a significant factor in the ANOVA analysis, is given in Fig. 10 .
 6. Discussion of results
The empirical results clearly demonstrate that P ! N noise has a significant impact on almost all of the classifiers when learning from imbalanced data. While some researchers have preliminarily considered this idea (for example, Wilson X  X  edit-ing [2] and Zhu and Wu [51]), noise handling literature has not sufficiently addressed this important issue. An example illustrating the impact of class noise in skewed data and the differences between W  X  0 % and W  X  100 % is given in
Fig. 12 , which contains a scatterplot of a simulated, two-dimensional dataset. The dataset contains 20 positive (squares) and 50 negative (triangles) examples. In the original dataset, on the left-hand-side of the figure, the data is easily separated by a linear decision boundary. The middle figure shows the case where K  X  30 % and W  X  0 % (in other words, all noisy exam-ples are of the type N ! P ). The noisy examples are filled-in squares. The original cluster of positive examples remains, while the cluster of negative examples has become more noisy (with mislabeled positive examples). The negative class consists of 38 correctly labeled examples, while the positive class consists of 32 examples, 12 of which are mislabeled.
The right-hand-side of Fig. 12 shows the same dataset with K  X  30 % and W  X  100 % . The negative class consists of 62 examples, 12 of which are mislabeled, while the positive class contains only eight examples. In this situation, the cluster of positive examples is significantly obscured by the presence of incorrectly labeled negative examples. It is therefore more difficult to learn the  X  X rue X  minority class because it is significantly fragmented and obscured. With W  X  0 % , the true minority class is uncorrupted, and there are just some noisy examples in the negative class. With W  X  100 % , the positive class now appears to consist of at least two small disjuncts, while with W  X  0 % , the noisy positive examples are relatively isolated and hence may not appear as disjuncts. In addition, from the perspective of concept complexity, the W  X  100 % case appears to be much more complex than W  X  0 % , because in the later scenario the frequency of noisy positive examples is small relative to the correctly labeled examples in that section of the feature-space. Finally, W  X  100 % results in absolute rarity in the positive class  X  only eight positive class examples remain after noise corruption.

Two lines were added to the figure with W  X  100 % and are labeled  X  X  X  and  X  X  X , indicating two possible decision boundaries that might be derived from a learner using this noisy data. One possible decision boundary learned is labeled  X  X  X , since a number of negative examples near the true class boundary are mislabeled, causing a large portion of the positive class region near the true boundary to be missed. Boundary  X  X  X  also captures a large percentage of mislabeled negative examples which truly belong to the positive region of the decision space.

All of the learners utilized in this study were adversely impacted by noise, however some learners (NB, 2NN, 5NN, MLP, and RF) were more stable at higher noise levels than others. In a relative sense, RIPPER, C4.5D, SVM, and RBF demonstrated a more dramatic decrease in performance compared to the other learners at higher noise levels. NB in particular performed very well in our experiments, as confirmed by the ANOVA analysis in Section 5.1.

Tables 17 X 20 show the effectiveness of the sampling techniques for varying levels of K and W . The relative effectiveness of RUS generally increases as either K or W increases. In other words, for many of the learners, utilizing RUS (or any other sampling technique, for that matter) at lower noise levels does not improve the performance of learners on imbalanced data.
This observation implies, as mentioned previously, that imbalance by itself may not be the issue  X  instead, imbalance cou-pled with another factor, in this case class noise, is the true problem. Note that this observation is not true for all learners.
Indeed, even at 10% class noise, the performance of both C4.5D and RBF were improved after sampling was applied, while for other learners such as 2NN, 5NN, and NB, sampling even at higher noise levels did not improve performance.

In addition to the strong performance of random undersampling, WE also works very well. The impact of the best sam-pling technique, when compared to the baseline learner without sampling, varies depending on the learner, level of noise, and distribution of class noise. Consider Figs. 4 and 5 , which compare the AUC obtained by the learner without sampling to that of the best sampling technique for W and K , respectively. For SVM, C4.5D, RBF, and RIPPER (and to a lesser extent RF, LR, and MLP), there is a divergence between the AUC for the baseline learner and the best sampling technique. In other words, for these learners, sampling provides a substantial benefit at higher levels of minority class noise or overall noise. One pos-sible explanation of this phenomenon is that sampling is alleviating the side-effects (e.g., disjuncts or overlapping classes) of class noise. However, not all learners benefit from the application of sampling, for example 5NN, 2NN, and NB.
Therefore, we conclude that in general, sampling improves the performance of many different classifiers when confronted with imbalanced data with class noise, and WE and RUS are generally the two best techniques. Further, RUS performed very well with higher levels of minority class noise W . As shown previously, this type of class noise is the most harmful, and RUS is generally the most effective at eliminating these examples from the dataset. This is particularly interesting because RUS is not designed to specifically address class noise. An unintended side effect of undersampling is to remove a significant portion of the mislabeled negative class examples, so in addition to balancing the class distribution, noisy examples are also elim-inated. WE, which was designed to address instance mislabeling, among other things, also performs well, which would be expected. The poor performance of OSS in our experiments is surprising, however it still may be useful for different types of noise not considered in this study. Further experimentation regarding the utility of OSS is advised. 6.1. Addressing our research questions
Section 1.1 included six research objectives to be addressed in this work. Here we summarize our findings in relation to each of these questions: Q 1 : What is the impact of class noise on learners constructed using skewed datasets?
A 1 : Class noise severely impacts the performance of all of the learners utilized in this work, although some learners were
Q 2 : Is the class in which the noise is located significant?
A 2 : The class in which the noise is located is absolutely critical. As we have shown, high levels of noise with all of that Q 3 : Are the effects of class noise in imbalanced datasets uniform across different learning algorithms?
A 3 : Different learners react differently to the presence of noise, and there is wide variation in the relative performances
Q 4 : How do sampling procedures, which are often used to alleviate class imbalance, perform in the presence of class
A 4 : There was substantial variability in the effectiveness of sampling techniques for different learners. Sampling does help
Q 5 : Do different sampling techniques work better with different levels of noise and with noise represented in different
A 5 : The relatively simple undersampling techniques (RUS and WE) generally performed the best, though results varied Q 6 : What guidance can be given to improve the commonly-used classification filters in the presence of class imbalance?
A 6 : Classification filters are inappropriate for noise handling with imbalanced data. In particular, it may not be sensible 6.2. Threats to validity
Experimental research commonly includes a discussion of two different types of threats to validity [49]. Threats to internal validity relate to unaccounted influences that may impact the empirical results. Throughout all of our experiments, great care was taken to ensure the authenticity and validity of our results. The benchmark WEKA data mining tool [48] was used for the constructionof all classifiers, andwe have includedall of the parameter settings to ensure repeatability.The parameters for the learners were chosen to ensure good performance in many different circumstances and to be reasonable for the datasets. The SASGLMprocedure [37]wasusedtoconstructtheANOVAmodelspresentedinthiswork,andtheassumptionsforconstructing
ANOVAmodelswerevalidated.Inparticular,thetargetvariablewastransformed et al. [4] to normalize the data and stabilize the variances before constructing the ANOVA model. All output was validated for accuracy by members of our research group, giving us confidence in the legitimacy of the results. We have included seven dif-ferent sampling techniques in this work, which includes all of those most commonly-used in related work. Since our research group developed software to implement the sampling techniques, all programs were checked for validity and accuracy.
Threats to external validity consider the generalization of the results outside the experimental setting, and what limits, if any, should be applied. Seven different datasets were used in our experiments, and we believe that the results presented in this work are valid for other datasets and application domains. In particular, these datasets represent a wide variety of sizes and imbalance levels and come from three different application domains. Future work is required to confirm our conclusions, but the inclusion of five real-world software measurement and two UCI datasets provides strong support for our empirical results. One of the strengths of our work is the use of datasets derived from real-world data, which as mentioned is in con-trast to many of the previous studies on data quality, where only simulated data is often used. Constructing over 2 million learners provides a high degree of reliability in our results, and we have considerable confidence in the validity of our exper-imentation. Future work can also consider different methodologies for injecting class noise. One possibility is to use simu-lated datasets injected with noise, although such a procedure has its drawbacks. 7. Conclusions
This work presented a comprehensive experimental investigation into knowledge discovery using noisy and imbalanced data, a topic of great interest to practitioners and researchers in many different application domains. In this work, we have included experimentation with a number of experimental factors (7 datasets, 11 learners, 24 combinations of class noise, and 7 sampling techniques) which have resulted in the construction of over 2 million learners. We have also included statistical analysis of the experimental results, an important dimension of reliable empirical work.

The results of our experiments conclusively demonstrate the adverse impacts of class noise on learning from imbalanced data, and in particular noise resulting from the corruption of instances from the true minority class (i.e., P ! N -type noise, or examples with a correct label of P that are mistakenly labeled as N in the training dataset) is seen to be the most severe type. As discussed, this result, which to our knowledge has not been previously analyzed so extensively, has very important conse-quences to the domain of noise handling. There was substantial variability in the impact of class noise on learner performance depending on the algorithm considered. Simple algorithms such as na X ve Bayes and nearest neighbor learners were often more robust (and performed better, in an absolute sense) than newer, more complex learners such as support vector machines or random forests. In addition, seven different and commonly-used sampling techniques were analyzed, and random under-sampling, a relatively simple procedure which randomly reduces the size of the majority class, was often found to be the most effective technique, especially at more impactful noise levels. Wilson X  X  editing, another undersampling technique which spe-cifically targets mislabeled examples, also performed very well while more complex oversampling techniques like cluster-based oversampling, SMOTE, and Borderline-SMOTE were either generally moderate or weak. One-sided selection, another sampling technique proposed to handle class noise (among other things), performed surprisingly poorly. Indeed, there was substantial variability in the performance of the sampling techniques across learners and levels of noise.

As with any empirical study, future work can include further studies with additional (real-world and simulated) datasets to confirm our results. Variations on our experimental design can also be considered, for example using different noise injec-tion procedures or different learning algorithms. Other performance measures can also be considered, as we focus solely on the AUC and KS. Other methodologies for handling imbalanced data such as cost sensitive learning can also be evaluated.
Future work can also consider the important problem of attribute noise. An exciting and useful area for research is further exploration into the relative sensitivities of learning algorithms in the presence of noise. Multi-class problems are increas-ingly important in a wide variety of domains, and a detailed study of class noise in such an environment would be very inter-esting. Our noise injection methodology would need to be extended to reflect different noise mixing distributions among the classes similar to our experimental parameter W . Finally, we recommend the development of a procedure that is better able to simultaneously deal with noise and class imbalance.
 Appendix A. Learners
The eleven learners used in our experiments are described briefly in this section, along with an explanation of the param-eters. These classifiers were considered since they are commonly-used in the data mining community. All learners were built using the WEKA data mining tool, version 3.5.2 [48]. Changes to default parameter values in WEKA were made only when experimentation showed a general improvement in classifier performance based on preliminary analysis.

Na  X  ve Bayes (NB) is a simple classifier that utilizes Bayes X  rule of conditional probability which assumes independence of the default parameter values of NB or LR in WEKA were made.

For a multilayer perceptrons (MLP) learner, two parameters were changed from the default values. The  X  X iddenLayers X  parameter was changed to  X 3 X  to define a network with one hidden layer containing three nodes, and the  X  X alidationSetSize X  parameter was changed to  X 10 X  to cause the classifier to leave 10% of the training data aside to be used as a validation set to determine when to stop the iterative training process. Another type of artificial neural network, Radial basis function networks (RBF), was utilized in our experiments. The only parameter change for RBF (called  X  X BF Network X  in WEKA) was to set the parameter  X  X umClusters X  to 10.

Two different K nearest neighbors classifiers (IBk in WEKA) were constructed, using k  X  2 and k  X  5, and denoted 2NN and 5NN. The  X  X istanceWeighting X  parameter was set to  X  X eight by 1/distance X  to use inverse distance weighting in determining how to classify an instance. Repeated incremental pruning to produce error reduction (RIPPER) is a rule-based learner. The WEKA implementation of RIPPER is called JRip, and our experiments used the default parameters for this learner.
The random forest (RF) classifier [5] utilizes bagging and the  X  X andom subspace method X  to construct randomized decision trees. The outputs of ensembles of these randomized, unpruned decision trees are combined to produce the final prediction.
No changes to the default parameters were made in our experiments. The support vector machine (SVM) learner in WEKA is called SMO. For our experiments, the complexity constant  X  X  X  in WEKA was changed from 1.0 to 5.0, and the  X  X uildLogistic-
Models X  parameter, which allows proper probability estimates to be obtained, was set to  X  X rue X  [48]. A linear kernel was used for the SVM learner in this work, and use of non-linear kernels is left for future exploration.
 Two different versions of the C4.5 classifier were used in this study, denoted C4.5D and C4.5N. C4.5D uses the default
WEKA parameter settings, while C4.5N uses no pruning and Laplace smoothing [47]. The WEKA implementation of C4.5 is called J48.
 Appendix B. Data sampling techniques
Seven sampling techniques proposed for dealing with imbalanced data are considered in this work: random undersam-pling (RUS), random oversampling (ROS), one-sided selection (OSS), cluster-based oversampling (CBOS), Wilson X  X  editing (WE), SMOTE (SM), and borderline-SMOTE (BSM).

The two most common preprocessing techniques are random minority oversampling (ROS) and random majority under-sampling (RUS). In ROS, instances of the minority class are randomly duplicated in the dataset. In RUS, instances of the major-ity class are randomly discarded from the dataset.

In one of the earliest attempts to improve upon the performance of random resampling, Kubat and Matwin [30] proposed a technique called one-sided selection (OSS). One-sided selection attempts to intelligently undersample the majority class by removing majority class examples that are considered either redundant or  X  X oisy. X 
The technique called Wilson X  X  editing [2] (WE) uses the kNN technique with k  X  3 to classify each example in the training set using all the remaining examples, and removes those majority class examples that are misclassified. Barandela et al. also propose a modified distance calculation, which causes an example to be biased more towards being identified with positive examples than negative ones.

Chawla et al. [10] proposed an intelligent oversampling method called Synthetic Minority Oversampling Technique (SMOTE). SMOTE (SM) adds new, artificial minority examples by extrapolating between preexisting minority instances rather than simply duplicating original examples. The technique first finds the k nearest neighbors of the minority class for each minority example (the paper recommends k  X  5). The artificial examples are then generated in the direction of some or all of the nearest neighbors, depending on the amount of oversampling desired.
Han et al. presented a modification of Chawla et al. X  X  SMOTE technique which they call borderline-SMOTE [17] (BSM). BSM selects minority examples which are considered to be on the border of the minority decision region in the feature-space and only performs SMOTE to oversample those instances, rather than oversampling them all or a random subset.

Cluster-based oversampling [22] (CBOS) attempts to even out the between-class imbalance, as well as the within-class imbalance. There may be subsets of the examples of one class that are isolated in the feature-space from other examples of the same class, creating a within-class imbalance. Small subsets of isolated examples like this are called small disjuncts .
Small disjuncts often cause degraded classifier performance, and this technique aims to eliminate them without removing data.

RUS, ROS, SM, and BSM require a user-defined parameter d . All four sampling techniques were used with three parame-ters, d  X  35 % ; 50 % , and 65%, where the dataset after application of the sampling technique with parameter d has d positive class examples. For example, if a dataset has 100 positive examples and 500 negative examples, RUS with d  X  35 % results in a tranformed dataset with 100 positive examples and 187 negative examples (since 100 = 287 d  X  35 % ). ROS at 50% would result in a dataset with 500 positive and 500 negative examples. When performing Wilson X  X  editing, we utilized both the weighted and unweighted (standard Euclidean) versions, and denote them WE-W and WE-E. In addition, classifiers were built with no sampling, which we denote  X  X ONE X . To summarize, a total of 16 sampling techniques and no sampling were done for each dataset. Our research group has put substantial effort into implementing software tools for all of the sampling techniques used in this experimentation.

References
