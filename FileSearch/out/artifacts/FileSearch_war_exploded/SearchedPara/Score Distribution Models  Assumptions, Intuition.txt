 Inferring the score distribution of relevant and non-relevant documents is an essential task for many IR applications (e.g. information filtering, recall-oriented IR, meta-search, dis-tributed IR). Modeling score distributions in an accurate manner is the basis of any inference. Thus, numerous score distribution models have been proposed in the literature. Most of the models were proposed on the basis of empirical evidence and goodness-of-fit. In this work, we model score distributions in a rather different, systematic manner. We start with a basic assumption on the distribution of terms in a document. Following the transformations applied on term frequencies by two basic ranking functions, BM25 and Lan-guage Models, we derive the distribution of the produced scores for all documents. Then we focus on the relevant documents. We detach our analysis from particular ranking functions. Instead, we consider a model for precision-recall curves, and given this model, we present a general mathe-matical framework which, given any score distribution for all retrieved documents, produces an analytical formula for the score distribution of relevant documents that is consistent with the precision-recall curves that follow the aforemen-tioned model. In particular, assuming a Gamma distribu-tion for all retrieved documents, we show that the derived distribution for the relevant documents resembles a Gaus-sian distribution with a heavy right tail.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Search and Retrieval] Retrieval models General Terms: Theory, Measurement Keywords: information retrieval, score distribution, den-sity functions, recall-precision curve
Given a user request an information retrieval system as-signs scores to each document in the underlying collection according to some definition of relevance of each document to the user X  X  request and returns a ranked list of documents to the user. In reality, this ranked list of documents is a mixture of both relevant and non-relevant documents. For a wide range of retrieval applications (e.g. information filter-ing, topic detection, meta-search, distributed IR), modeling and inferring the distribution of relevant and non-relevant documents over scores in a reasonable way can be highly beneficial. For instance, in information filtering, topic detec-tion and recall-oriented retrieval, modeling the score distri-butions of relevant and non-relevant documents can be uti-lized to find the appropriate threshold between relevant and non-relevant documents [16, 17, 2, 19, 9, 15]. In distributed IR and meta-search it can be used to normalize document scores and combine different collections or the outputs of several search engines [5, 12].

Inferring the score distribution for relevant and non-relevant documents in the absence of any relevance information is an extremely difficult task, if at all possible. Modeling score dis-tributions is often the basis of any possible inference. Due to this, numerous combinations of statistical distributions have been proposed in the literature to model score distributions of relevant and non-relevant documents. In the 1960s and 70s, Swets attempted to model the score distributions of non-relevant and relevant documents with two Gaussians of equal variance [16], two Gaussians of unequal variance, and two exponentials [17]. Bookstein instead proposed a two Poisson model [7] and Baumgarten a two Gamma model [5]. A negative exponential and a Gamma distribution [12] has also been proposed in the literature. The dominant model has been a negative exponential for the non-relevant docu-ments and a Gaussian for the relevant ones [2, 12, 19]. Ben-nett [6] observed that when using a two-Gaussians model for text classification, document scores outside the modes of the two Gaussians (corresponding to  X  X xtremely irrelevant X  and  X  X bviously relevant X  documents) demonstrated different em-pirical behavior than the scores between the two modes (cor-responding to  X  X ard to discriminate X  documents). This mo-tivated him to introduce several asymmetric distributions to capture these differences. Kanoulas et al. [11] recently pro-posed a Gamma distribution for the non-relevant documents and a mixture of Gaussians for the relevant documents.
The complexity of the underlying process that generates document scores makes it hard to theoretically argue about the actual distribution of document scores. Most of the aforementioned models were proposed on the basis of empir-ical fits to scores produced over different document corpora. There have also been several attempts to intuitively argue about the shape of the different distributions. The starting point for most of these attempts has been some basic as-sumptions about the frequency of query term occurrences in documents (e.g. in Manmatha et al. [12]). Harter [10] and Bookstein and Swanson [8] used a mixture of Poisson distri-butions to model the distribution of words in a document, with one Poisson corresponding to the distribution of words in relevant documents and the other to the distribution of words in non-relevant documents.

In a different line of work, Arampatzis and van Hameren [2] showed that the distribution of relevant document scores rapidly converges to a Gaussian via the Central Limit The-orem as the number of query terms increases, under some basic assumptions. Further, they claimed that this is not true in the case of non-relevant documents.

Finally, Robertson [14] considered various combinations of distributions and examined whether these combinations exhibit anomalous behavior with respect to theoretical prop-erties of precision and recall. Arampatzis et al. [1] proposed two truncated versions of the exponential-Gaussian model to overcome the theoretical problems associated with the original exponential-Gaussian model.

In this work, we model score distributions in a rather dif-ferent, systematic manner. We start with a basic assumption on the distribution of terms in a document. Following the transformations applied on term frequencies by two basic ranking functions, BM25 and Language Models, we derive the distribution of the produced scores for all documents in an analytical form and illustrate that the derived distribu-tion can be well approximated by a Gamma distribution.
Further, we also consider th e score distribution for rel-evant documents. We detach our analysis from particular ranking functions. Instead, we consider a simple model for precision-recall curves proposed by Aslam and Yilmaz [3], which makes some very basic assumptions about the shapes of precision-recall curves that are produced by reasonable retrieval system on average. Given this model, we present a general mathematical framework which, given any score dis-tribution for all retrieved documents, produces an analytical formula for the score distribution of relevant documents that is consistent with the precision-recall curves that follow the aforementioned model. In particular, assuming a Gamma distribution for all retrieved documents, we show that the derived distribution for the relevant documents resembles a Gaussian distribution with a heavy right tail.
Traditional retrieval models score documents based on how well their language matches the language of the user X  X  request. Thus, the essential component of all traditional scoring functions is the number of occurrences of query terms within a document (term frequency, TF). Different retrieval models apply different transformations over the term fre-quencies to produce a score per query term. The final score of a document is usually an aggregate of the document scores for each individual term.

Before we consider the distribution of term frequencies and the transformation applied by ranking functions over them in an analytical manner we illustrate the evolution of the term frequency distribution for all retrieved documents (documents that contain at least one of the query terms) for a sample query from the TREC 8 ad hoc collection ( Ireland Peace Talks ) and for two different retrieval models, BM25 and Language Models, in Figure 1.

The left panel corresponds to the transformation of TF distribution by BM25, while the right panel corresponds to the transformation by the Jelinek-Mercer Language Model. 1 Each column then, in both panels, corresponds to an indi-vidual query term and each row to progressively more com-plex transformations of the term frequency. The bottom row plots illustrate the final score distribution by the two retrieval models.

As can be observed, for both retrieval models, there is a critical step in the term frequency transformation (from Row 2 to Row 3) after which the score distribution radically changes and appears to be closer to the final score distribu-tion. Furthermore, the shape of the final score distribution appears to be dominated by the most frequent query term in the collection (as expected)  X  for the sample query this is the term talk  X  and thus our main goal will be to derive the score distribution for each individual query term.
For a fixed query, consider a partition of the collection into relevance classes, such that D Q is the class of docu-ments that satisfy the information need to a certain degree Q&gt; 0. Depending on several factors like the user, the infor-mation need, the collection of documents etc, Q can take a range of values from  X  X ompletely irrelevant X  (the lowest Q ) to  X  X xtremely relevant X  (the highest Q ). Note that in test collections (such as TREC) for simplicity only two or three classes are considered. The discussion in this section assumes a fixed quality/relevance class Q , and assumes all documents in the class contain all query terms at least once.
A query term t has a certain contribution to the document quality in response to the user query. For a given document quality Q , we assume an approximately constant probability of seeing the term t at any position in a document in class D
Q ; hence we can model term t occurrences in documents in class D Q with a Poisson process with rate  X  =  X  t = f ( g, Q ), where g = g t relates to the general rarity of the term in the language. Such a model is memoryless and implies that the query term appears equally likely at any moment. We do not model the dependence f  X  any monotonic function can be used, depending on the class model.

Counting the occurrences of a term t when reading a ran-dom document d  X  D Q is analogous to counting buses at a bus station: arrive at the station, wait for the first bus, for the second bus, etc., and leave at some point (when the document ends). It is well known that the waiting times w ,w 2 ,w 3 ,... among Poisson generated events are exponen-tially distributed i.i.d. random variables
The average waiting time is  X  =1 / X  , the mean of the ex-ponential distribution. Intuitively,  X  corresponds to a notion of the expected ratio of docume nt length to term frequency, i.e., DL/TF.
The parameter values used for BM25 are k 1 =1.2 and b =0.75, and  X  = 0.2 for the Jelenik-Mercer Language Model. TN is the number of terms in the collection.

Our purpose is to model the distribution of the random variable DL/TF for documents in class D Q . We will do so separately for each frequency andthenexpressthegeneral distribution as a mixture.
 Let us now fix a term frequency k =1 , 2 , 3 ,... and denote D Qk = { d  X  D Q | TF ( t, d )= k } the set of documents in D that contain term t exactly k times. Here, we make the ap-proximation that the document ends exactly after the k -th occurrence, and so we can write the document length DL as the sum of k waiting times plies that DL is Gamma distributed (and more specifically Erlang-distributed), with shape k and scale  X  =1 / X  : Since k is a constant for the subclass D Qk , the waiting time X Qk is also Gamma distributed: X Qk = DocLength Since the quality class D Q is partitioned into the classes D
Qk for k =1 , 2 , 3 ,... , the waiting time X on D Q follows a mixture of Gamma distributions with a constant mean  X  , while DL on D Q follows a mixture of Gamma distributions with a constant scale  X  : where P Q ( k )= Pr [ TF ( d, t )= k | d  X  D Q ] denotes the prob-ability that a document in class D Q contains the term t exactly k times.

Assuming a constant probability p that a term occurrence gives quality Q , P Q ( k ) can be expressed as probability of k  X  1 failures (term occurrences that do not imply quality Q ) followed by one success (term occurrence when quality Q is reached); therefore we model the mixture probabilities P
Q ( k ) with a geometric distribution (equivalent to a nega-tive binomial distribution with  X  =1), where p = p t =  X /ADL Q expresses the correlation between the term and the information need on the class D Q (the average document length, the general rarity of the term t , and the quality Q ). For example, p =0 . 5 implies that there are twice as many documents containing k terms than doc-uments containing k +1 terms in the class D Q .Intuitively p can be thought as a notion of inverse term frequency:
Note that a number of different mixtures could be used, perhaps based on the query type. For instance, an infor-mational query could use a negative binomial or a Poisson mixture. For the particular case of a geometric mixture how-ever, an interesting result follows: Neuts and Zachs [13] show that under certain conditions similar to ours, a negative bi-nomial mixture of Gamma distributions with constant scale is actually itself a Gamma distribution. With a different notation, their result is
X p k = N egBinomial ( p,  X  )= k +  X 
Applying this on DL (with  X  =1) implies that DL is expo-nentially distributed on D Q with mean  X /p .Ofcoursethis must hold for all query terms, not only for t ,whichrequires a proportionality  X /p = constant = ADL Q .Inpractice, for a given quality class, the document length variable will not be exactly exponentially distributed for two reasons: (1) relevance judgments cover a range of qualities inducing an average effect, (2) our Poisson process model for query term occurrence works reasonably well for frequent terms, but can fail on rare terms. However, this model is fairly accurate in that DL can be modeled well by a Gamma distribution with a small shape parameter (the exponential distribution is Gamma with shape = 1.)
Figure 2 illustrates the empirical histogram of DL/TF for the query term system . As can be observed, a Gamma dis-tribution appears to be a good approximation of the empir-ical score distribution, offering empirical evidence that the assumptions and approximations in our theory are reason-able. 2 Figure 2: The empirical histogram and the Gamma density function fit over the DL TF scores for term sys-tem in TREC8.
In this section, we derive the score distribution of the re-trieved documents in a systematic manner. We consider the transformation applied on the distribution of the elemen-tary statistics described in the previous section by two scor-ing functions, BM25 and Jelinek-Mercer Language Model. The derivations presented here can be applied in the case of other retrieval models, such as TF-IDF and Divergence From Randomness (DFR).
Consider a transformation of the random variable X by a monotonic, differentiable function r , Y = r ( X ). The proba-bility density function (pdf) of Y , f Y ( y ), can then be com-puted as a function of the pdf of X , f X ( x )[4]. Let F and F X ( x ) be the cumulative density function (cdf) of Y and X , respectively. Without loss of generality let r be a non-decreasing function. Then, f ( y )= d
In the general case of a monotonic function r ,
Some fits will be better than others, depending on the ex-ample. No theoretical model will fit all empirical examples, of course.

A rudimentary transformation of interest is just the in-verse of X=DL/TF , which gives the normalized term fre-quency TF/DL . According to the previous section, X = DL/T F  X  f X = that a mixture of Gamma can approximate any smooth func-tion [18]. By approximating P Q ( k ) with a geometric distri-bution inverting TF/DL has the effect displayed in Figure 3. A relevant class of documents (high Q ) implies:
Conversely, a lower quality Q implies a mixture with ef-fectively significant coefficients only for the lower k values, and also that the components of the mixture are less skewed towards the left-side, overall producing a more exponential-like distribution (after inversion).
 Figure 3: Mixture of gamma before and after the inversion, for different quality classes
Note that in practice fitting a Gamma, an inverse Gamma or an inverse Gaussian distribution in the TF/DL scores of existing collections/judgments (like TREC) are likely to differ in goodness-of-fit mostly due to random effects than other theoretical reasons -this is primarily due to complex score manipulations, and due to the sparsity and inaccuracy of the judgment process.
Assuming that query terms appear only once within a query the BM25 for a single query term can be calculated as: where TF is the term frequency, IDF is the BM25 inverse document frequency, DL is the document length, and ADL is the average document length in the collection. By setting the parameter b equal to 1 (fixing the document length nor-malization) and defining the variable X = DL / TF ,BM25 can be approximated by, where C = k 1 /ADL . Given Equation 10 it can be shown that r  X  1 ( Y )= IDF ( k 1 +1) of X and f Y ( y )thepdfof Y . Since function r is a monotonic and differentiable when X is positive, based on the principle of function transformations of random variables [4], we can calculate the pdf of Y as a function of the pdf of X , when 0 &lt;y&lt; IDF ( k 1 +1) and 0 otherwise.

In other words we can model the pdf of an approxima-tion of BM25 as a function of the density function of the reverse relative term frequency. Essentially, one can plug in the above formula any distribution for the relative term frequency and get an analytical form distribution of BM25.
Based on the previous section DL/T F approximately fol-lows a Gamma distribution. Let  X  k and  X   X  are estimated pa-rameters of the Gamma distribution from X via maximum likelihood estimation (MLE) for all retrieved documents (see Figure 2). Then, the approximated pdf of BM25 score for a single term can be reached as follows, f Y ( y )=
We repeat the exact same derivation in the case of lan-guage models with Jelinek-Mercer smoothing. The score for each term is computed as, where C = CTF / TN . CTF is collection term frequency and TN is the number of unique terms in the collection. As before, we let X = DL / TF , then the LM score can be written as,
Using the previous assumption that DL / TF is modeled by a Gamma distribution and since the function r is a mono-tonic and differentiable, after the random variable transform over X we get the pdf of the LM scores as a function of the Gamma distribution that models the reverse relative term frequency. f Y ( y )=
Figure 4 shows the comparison among the empirical his-togram, the analytical model derived from the distribution of DL/T F , and the Gamma distribution obtained by MLE over BM25 and JM language model scores all retrieved docu-ments for query system in TREC8 collection. As illustrated on the plots, the analytical model has more freedom than the Gamma distribution, but the Gamma is still a reason-able approximation to the term score distribution. Further, the mixture model presented in the previous section with the best-fit  X  is also shown on Figure 4 (black line denoted as  X  X odel (theory) X  in the legend).
 Most term frequency weighting functions are nonlinear mono-tonically increasing functions of the raw term frequency. In BM25 Roberston X  X  TF grows fast when the raw term fre-quency is small and gets gradually saturated. The parameter k controls the speed of the saturation. The logarithm func-tion in Language Models also has this saturation property but without the power of controlling the saturating speed. Therefore, the JM language model scoring function has a similar to BM25 impact on transforming the distribution of low level statistics, such as DL/TF or normalized TF to the final score distribution.

As it is illustrated in Figure 2 the typical shape of the distribution for the DL/TF tends to have a long right tail but a fast rising-up left tail. After applying a transforma-tion function with the saturation property, the imbalance between two tails of the original distribution is alleviated, so the peak of the new distribution is right shifted, and with a shorter right tail compared to the original one. The amount of difference is dominated by the parameter control-ling the saturating speed. This can be viewed in Figure 5. As k 1 becomes larger and the weighting function more lin-ear the empirical histograms of BM25 looks more similar to the distribution of DL/TF in Figure 2. This implies that the term score distribution can also be approximated by a Gamma distribution by adjusting the shape and the scale parameters.
In this paper we have considered scoring functions with the following property: score(d,query) = where X t = DL/tf ( t, d ). This class of scoring functions in-cludes BM25, TF-IDF, some Language Models etc, but does not include scores like PageRank. Assuming term indepen-dence, the intuition for the summation score = as follows:
Thus, the distribution of the summation of several term scores could also be modeled using a Gamma distribution if we use a Gamma distribution to model the term score distribution. Figure 6 shows this summation process.
In this section, we relate the score distributions for rele-vant and non-relevant documents with precision-recall curves. That the score distributions for relevant and non-relevant documents are related to precisi on-recall curves is well known and unsurprising: Given the two score distributions, one can easily infer a precision-recall curve [14], and we shall do so below as part of the treatment that follows. More interest-ingly, we demonstrate that one can infer the score distri-bution for relevant documents given a score distribution for non-relevant documents and a precision-recall curve, and we use the technique described to show that the score distribu-tions for relevant documents will tend to have a Gaussian-like form, with a heavy right tail.

Let f R ( s )and f N ( s ) be the score distributions for rel-evant and non-relevant documents, respectively. For any score threshold t , consider the set of documents whose scores are t or higher. The recal l and fallout associated with this document set are easily defined in terms of f R ( s )and f as follows: Frequency Figure 5: Roberston X  X  TF and empirical histograms of BM25 scores with different k 1 for term system in TREC8 Figure 6: MLE Gamma fitting over scores of all re-trieved documents for all query terms and query  X  X reland Peace Talks X 
Now let C be the size of the collection and let  X  be the fraction of the collection that is relevant to a given query. Then there are R =  X C total relevant documents and N = (1  X   X  ) C total non-relevant documents. At score t or above, there are relevant documents and non-relevant documents. Thus, the precision associated with this document set is simply p ( t )=  X C where O =(1  X   X  ) / X  is the odds of non-relevance in the collection. Equations 16 and 18 are parametric equations defining a precision-recall curve: Given the score distribu-tions f R ( s )and f N ( s )(and  X  ), one can vary the score thresh-old t in Equations 16 and 18 to obtain the precision-recall curve. (A substantially similar treatment can be found in Robertson [14].)
Now suppose that one has a candidate score distribution for either relevant or non-relevant documents and one has a candidate form for a precision-recall curve: Can one derive a form for the other score distribution? In what follows, we show how this can be accomplished, and using the score distributions described in Section 4 and a simple form for precision-recall curves, we infer a form for the score distri-butions of relevant documents.

Consider the simple model for precision-recall curves de-scribed by Aslam and Yilmaz [3] and shown in Figure 7.
This family of precision-recall curves is defined by the fol-lowing equation, implicitly parameterized by the value of R-precision rp : (Here  X  =(1 / rp  X  1) 2  X  1 governs the  X  X hape X  of the curve.) While it is certainly the case that X  X eal X  precision-recall curves are never this  X  X lean X , this simple model captures many properties found in real precision-recall curves, such as high precisions at low recall levels, low precisions at high recall levels, and so on. Furthermore, Aslam and Yilmaz show that this simple model allows one to explicitly and accurately re-late average precision, R-precision, precision-at-cutoff, and other seemingly disparate measures of retrieval performance.
Using such a model for precision-recall curves, we can re-late the score distributions for relevant and non-relevant doc-uments as follows. We first parameterize Equation 19 by the score threshold t , obtaining We now equate Equations 18 and 20 and solve for r ( t ) as a function of fo ( t ) r ( t )= Differentiating Equation 21 by t immediately establishes a closed-form relationship between the score distributions for relevant and non-relevant documents, since by Equations 16 and 17 and the Fundamental Theorem of Calculus, we have
As an example of this methodology, let us assume that the score distribution for all documents follows a Gamma distri-bution, as we argued in Section 4. Since the overwhelming majority of documents are non-relevant, the score distribu-tion for non-relevant documents will then tend to follow a Gamma distribution as well. Now consider the Gamma that fits the non-relevant documents for the TREC8 query  X  X sto-nia Economy X . Using this Gamma distribution for the non-relevant documents, together with a precision-recall curve
We set  X  and  X  =(1 / rp  X  1) 2  X  1 to match those parameters from the BM25 run on that query. from the family show in Equation 19, and employing the method described above, we obtain the score distribution for relevant documents shown in Figure 8.

While Figure 8 gives just one such example, the form of this curve is quite consistent across all tested input distri-butions from the Gamma family (which includes the nega-tive exponential distribution) and all tested precision-recall curves from the family defined by Equation 19: The distri-bution is roughly Gaussian in form, but with a heavy right tail. That the score distribution is  X  X aussian-like X  is much assumed (as discussed in the introduction), but the heavy right tail is also necessary to avoid problems with a simple Gaussian, such as those described by Manmatha et al. [12] and others. Figure 9 shows the typical form of the rele-vant document score distribution we obtained in TREC 8. We here for the first time derive such a form, given reason-able forms for non-relevant score distributions and precision-recall curves.

Our results in this section are descriptive rather than pre-scriptive, and as such, we conclude the following:
In this work, we attempt to model score distributions in a rather systematic manner. We start with a basic assumption that query terms are generated via a Poisson process and induced that the distribution the relative term frequency in a document is a inverse Gamma distribution. Following the mathematical transformations applied on the relative term frequencies by two basic ranking functions, BM25 and Language Models, we derived the distribution of the pro-duced scores, in an analytical form and illustrate that the derived distribution can be well approximated by a Gamma distribution. Further, we also considered the score distribu-tion for relevant documents by relating score distributions with precision-recall curves. In particular, we adopted a precision-recall curve model that has previously been pro-posed and given this model we presented a general math-ematical framework under which given any score distribu-tion for all retrieved documents we can derive an analyti-cal formula for the score distribution of relevant documents. The framework is general enough such that the same deriva-tions can be repeated for different models of precision recall curves. Finally, under the assumption that non-relevant doc-uments follow a Gamma distribution for all retrieved docu-ments, we show that there is a tendency of the derived dis-tribution for the relevant documents to look Gaussian with a heavy right-hand tail. We gratefully acknowledge the support provided by the NSF grants IIS-0533625 and IIS-0534482 and by the Euro-pean Commission grant FP7-ICT-248347 (Accurat project) and the Marie Curie Fellowship FP7-PEOPLE-2009-IIF-254562.
Figure 7: A family of precision-recall curves fit through the points { (0 , 1) , ( rp , rp ) , (1 , 0) } for rp =0 . 1 , 0 . 2 ,..., 0 . 9 .
