 We propose a framework which can perform Web page seg-mentation with a structured prediction approach. It formu-lates the segmentation task as a structured labeling prob-lem on a transformed Web page segmentation graph (WPS-graph). WPS-graph models the candidate segmentation bound-aries of a page and the dependency relation among the ad-jacent segmentation boundaries. Each labeling scheme on the WPS-graph corresponds to a possible segmentation of the page. The task of finding the optimal labeling of the WPS-graph is transformed into a binary Integer Linear Pro-gramming problem, which considers the entire WPS-graph as a whole to conduct structured prediction. A learning algorithm based on the structured output Support Vector Machine framework is developed to determine the feature weights, which is capable to consider the inter-dependency among candidate segmentation boundaries. Furthermore, we investigate its efficacy in supporting the development of automatic Web page classification.
 H.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Web page segmentation, Web page classification, structured prediction, integer linear programming  X 
Web pages are typically designed to facilitate visual in-teraction with the human readers. The designer normally organizes the information of a page into different units or functional types [22], which are arranged in coherent vi-sual segments in the page, such as header, footer, navigation menu, major content, etc. It is a trivial step to recognize those visual segments for readers. However, it is still a chal-lenging problem for computer since the source code of Web pages is not encoded in such a way to differentiate the se-mantic blocks. To overcome the gap between the manner of the pages designed/read by human and the manner of the pages operated by the computer, Web page segmentation is regarded as an essential task in Web information mining. The aim of Web page segmentation is to decompose a Web page into sections that reveal the information presentation logic of the page designer and appear coherent to the readers. For example, if we consider the page in Figure 1, Web page segmentation should recognize those segments separated by the red boundary lines. Identifying such segments is useful for different downstream applications. One application is to re-organize a Web page so that it can be properly displayed or redecorated for devices with small-sized screen [1, 17, 20]. Then, people with visual impairment can easily digest them with their screen readers [21]. Link analysis, Web document indexing, and pseudo-relevance feedback can be more effec-tive with the appropriate segments detected [12, 22, 29]. Duplicate content detection, Web page classification, and content change detection can be conducted on finer infor-mation units so as to obtain better performance [7, 19, 27].
One group of previous works on Web page segmentation is heuristics-based [6, 19]. Cai et al. employed heuristic rules to capture structure features and visual properties [6]. Their method recursively segments the larger blocks into smaller ones in a top-down manner. However, this greedy manner is myopic and it may be trapped locally and stop to search better solutions. In addition, the patterns used in page de-sign are unlimited and cannot be covered by a finite set of rules. Kohlsch  X  utter and Nejdl employed text density in dif-ferent portions of a Web page as a clue to conduct segmen-tation [19]. Their method ignores other types of information such as image, frames and whitespace which provide useful clues for page segmentation. Chakrabarti et al. proposed a graph-theoretic approach to deal with Web page segmenta-tion [7]. They cast the problem as a minimum cut problem on a weighted graph with the nodes as the DOM tree nodes and the edge weights as the cost of placing the end nodes in
Figure 1: An example of Web page segmentation. the same segment or different segments. A learning based method was developed to determine the weights of edges. This method sometimes reports non-rectangular segments which are generally inaccurate. Different from [6], both [19] and [7] only obtain a flat segmentation of a Web page with-out knowing the hierarchical structure of the segmentation.
In this paper, we propose a framework to solve the above shortcomings of the existing works. Our framework per-forms Web page segmentation with a structured prediction approach by formulating the segmentation task as a struc-tured labeling problem on a transformed Web page segmen-tation graph (WPS-graph). WPS-graph is a directed acyclic graph, as exemplified in Figure 2(b), and its vertices are composed of the candidate segmentation boundaries of the corresponding page, as depicted in Figure 2(a). Each vertex, i.e. boundary, is able to split the current segment into two sub-segments. The directed edges capture the dependency relation between the associated boundaries. Each labeling scheme of the WPS-graph, which assigns a binary label for each vertex to indicate whether or not the boundary should split the current segment, corresponds to a possible segmen-tation of the page. Different from the heuristic rule-based top-down search in [6], our framework performs the label prediction simultaneously for all vertices of the WPS-graph with a trained statistical model . The hierarchical structure of the intermediate segmentation steps is recorded by the layers in the WPS-graph. Thus, the output of our frame-work provides a good structural analysis of pages enabling better utilization for different Web mining tasks such as page classification. Furthermore, as exemplified with Figure 2, our framework can automatically avoid non-rectangular seg-ments via the horizontal and vertical boundaries.
DOM structure, visual property, and text content features of a Web page are jointly considered in our framework. DOM structure features capture the s tructure characteristics of the segments, such as the structure similarity of the neigh-boring segments, the regularity of the DOM structure, etc. Visual property features capture visual clues of segments, such as background color, whitespace, font size, etc. The text content features capture some important semantic char-acteristics of the segments, such as the text similarity of the neighboring segments, the title keywords in a segment, etc. To allow different impacts of the features, a weight is associ-ated with each feature. A machine learning algorithm based on the structured output Support Vector Machine frame-work [32] is developed to determine the feature weights. Figure 2: An example page and its WPS-graph.
 The learning algorithm can consider the inter-dependency among the vertices of a WPS-graph during the training pro-cess. Therefore, the feature weights are determined with a global view on WPS-graphs but not merely on individ-ual segmentation boundaries. To infer the optimal labeling, we consider the entire WPS-graph as a whole to conduct a structured prediction in which the labeling determination of one vertex, i.e., boundary, is coordinated with the others. To do so, we transform the labeling task into a binary In-teger Linear Programming (ILP) problem [35], whose linear programming relaxation can be efficiently solved with the simplex algorithm [11].

Extensive experiments have been conducted on a large data set. The results demonstrate that our framework achieves better performance compared with two state-of-the-art meth-ods. To investigate the efficacy of our framework in support-ing other Web mining tasks, another experiment, namely Web page classification, is conducted. In this task, we pro-pose a novel method to assemble the segmentation output of a page for constructing a more effective feature vector.
Web page structure analysis has been one hot research area for the past decade. There are several directions in this area, including single page oriented segmentation [6, 7, 19], site-oriented page segmentation [14], informative con-tent extraction [13, 25, 31, 34], template (or boilerplate) detection [2, 33, 37], data record detection and entity ex-traction [3, 4, 5, 16, 24], etc. These directions are closely in-terwoven. Some directions share similar methodologies but aim at different products, such as site-oriented page segmen-tation and template detection. Some can be regarded as a preparation step for another purpose.

Single page oriented segmentation aims at segmenting an input Web page into distinct coherent segments or blocks (such as main content, navigation bars, etc.) based on its own content. Besides the works [6, 7, 19] discussed above, Chen et al. [9] distinguish five block types, namely, header, footer, left side bar, right side bar, and main content. Fer-nandes et al. [14] proposed a site-oriented method for Web page segmentation. Hattori et al. proposed a segmentation method based on calculating the distance between content elements within the HTML tag hierarchy [17].

Aiming at improving the performance of Web page clus-tering and classification, Yi et al. proposed a site style tree (SST) structure that labels DOM nodes with similar styles across pages as uninformative [37]. In [15], the URL fea-tures were shown to be effective in homepage identification, which can be regarded as a special case of page functional type classification.
Web page segmentation is the task of breaking a page into sections that reveal the information presentation structure of the page designer and appear coherent to the readers. To facilitate the description of our framework, one illustration page example is given in Figure 2(a). The lines in the page are candidate boundaries for splitting the page into visual segments. The solid lines are the boundaries that should split the segments where the boundaries locate into sub-segments. For example, b 1 and b 2 split the entire page into the upper, middle, and lower parts. While the dashed lines are the boundaries that do not perform splitting operation, such as b 8 , b 9 ,and b 10 . Therefore, the final segmentation result is as depicted by the solid boundaries. Performing page segmentation is equivalent to determining a label as-signment which assigns the label Y (splitting operation) or the label N (not a splitting operation) to each boundary. For the page in Figure 2(a), the solid boundaries take the label Y , while the dashed boundaries take the label N .Asegmen-tation should follow the structure constraints of the page. For the above example, only after the boundaries b 1 , b 2 and b 4 simultaneously decide to split, it becomes meaningful to consider which label b 6 and b 7 should have. If any one of b , b 2 , b 3 and b 4 is labeled with N , b 6 and b 7 are automat-ically labeled with N . This kind of constraints compose of a topology graph G , named Web page segmentation graph (WPS-graph) in this paper. The WPS-graph of the page in Figure 2(a) is given in Figure 2(b).
 Let C denote a label assignment by taking a label from { Y , N } for each vertex, i.e., boundary, in G . The segmentation task is formulated as the following optimization problem: where F is an objective function that evaluates the fitness of C for G .Thevariable w givestheweightsoftheDOM structure, visual property and text content features. Such design globally evaluates the fitness of a label assignment C for G so that the determined segmentation is more accurate. Note that each legal label assignment must satisfy all the dependency constraints depicted by the edges of G . Funda-mentally, some existing methods such as [6, 7, 19] can also be represented in the form of F ( G , C ; w ). For example, the visual block tree approach in VIPS [6] can be transformed into our WPS-graph. And recursively segmenting the larger blocks can be transformed into determining the labels of the corresponding boundaries. The DoC criteria in VIPS can
Algorithm 1: Construction of WPS-graph. be regarded as simple DOM structure and visual property features. Different from these methods, our model conducts a global evaluation on the fitness of a segmentation for a page. Definition 1 (WPS-graph). For a Web page p ,its WPS-graph G = { B , E } is an acyclic directed graph. Each vertex b in B is a candidate segmentation boundary in p . Each directed edge e : b i ,b j in E indicates the constraint ( C ( b j )= Y )  X  X  X  ( C ( b i )= Y ) ,where C is a label assignment for the vertex set B of G .
 The construction of WPS-graph for a page is described in Algorithm 1. Initially, the entire page is regarded as one segment and denoted as s 0 . Meanwhile, the WPS-graph is  X  . Refer to Line 1 in Algorithm 1 and part (a) in Figure 3. While the segment queue Q is not empty, the segment at the front of Q is processed, as depicted in Line 3. Take s 0 as an example as depicted in part (b) in Figure 3. The candidate horizontal boundaries b 1 and b 2 split s 0 into three sub-segments and they are added as vertices into G ,asgiven in Line 5 in Algorithm 1. For the new vertices, namely, b and b 2 , no new edges need to be added since s 0 is the entire page and b 1 and b 2 do not depend on any previous boundary. The current G is as depicted in the lower section of part (b) in Figure 3. The sub-segments, namely, s o 1 (on the boundary b ), s o 2 (also known as s b 1 ), and s b 2 (beneath the boundary b ) are enqueued, as shown in Line 7 in Algorithm 1. To continue, after s o 1 is found inseparable, s o 2 is processed as depicted in part (c) in Figure 3. The boundaries b 3 and b are added as new vertices in G . b 3 and b 4 depend on b 1 b 2 so that the edges are added accordingly as depicted in the lower section of part (c). The sub-segments, namely, s s 4 and s b 4 , in the upper section of part (c) are enqueued. For the sake of simplicity, we also use s o i to denote the sub-segment on the left of b i and s b i to denote the sub-segment on the right of b i . Then, s b 2 isprocessedasdepictedinpart (d) in Figure 3. The boundary b 5 and the edge b 2 ,b 5 are added into the graph. The construction process terminates until Q is empty.

Several issues should be noted in the construction. First, a candidate boundary cannot cut across any sub DOM tree. For example, there is no subtree whose one part is in s o the other part is in s o 2 . Second, when judging the separability of a segment, the horizontal boundaries inside it are exam-ined first since they are more commonly used than vertical boundaries. Third, in separability judgment, if a segment is composed of a single DOM tree, we recursively use the lower level subtrees of it instead. For example, if s 0 is composed will find the boundaries between each neighboring pair of &lt; tr &gt; , we will find the boundaries between each neighboring directly depends on two other vertices. Theoretically, a can-didate boundary b inside the segment s depends on all four boundaries of s , among which b indirectly depends on at least two and at most three of them. In part (e) in Figure 3, we can see that b 6 and b 7 indirectly depend on b 1 and b
The segments with essential information are normally ar-ranged in conspicuous position of a page, such as the mid-dle of the first screen. Such segments are known as in-formative segment [13, 23]. Incorrectly segmenting infor-mative segments causes larger loss. Taking the news con-tent segment in a news page as an example, any segmen-tation that mistakenly segments the paragraphs of the con-tent into different segments is not a favorable result. This is called over-segmentation mistake. On the other hand, if the news content segment is combined with other segments and becomes a subpart of the combined segment, insufficient-segmentation mistake occurs. To make the informative seg-ments accurately segmented, we define the related bound-aries as informative boundaries and give them some special treatment.

Definition 2 (Informative Boundary). If a bound-ary is mistakenly labeled, the related informative segment will be overly or insufficiently segmented. Such boundary is an informative boundary.
 Suppose s o 4 is an informative segment in the demo page in Since b 3 and b 4 depend on b 1 and b 2 , we only need to ensure that b 3 and b 4 are correctly labeled as Y . Similarly, after b and b 7 are correctly labeled as N , the boundaries that depend on b 6 and b 7 will be labeled as N automatically. Therefore, we define proper informative boundary as follows. Definition 3 (Proper Informative Boundary).
 Suppose b i is an informative boundary and outside the in-formative segment, if there is no b j which directly depends on b i and is also outside the informative segment, b i is a proper informative boundary. Suppose b k is an informative boundary and inside the informative segment, if b k directly depends on a proper informative boundary outside the infor-mative segment, b k is a proper informative boundary. In the above example, b 3 , b 4 , b 6 ,and b 7 are the proper in-formative boundaries.
Let  X  ( G , C ) denote the combined feature representation of G and its label assignment C . Thus, the objective function F in Equation 1 is formulated as: which is the linear combination of the features in  X  ( G , C ) with their corresponding weights given in w . For each bound-ary b in B of G , we define a group of features from its sur-rounding segments to assist the determination of its label. For the example in Figure 2(a), if the segments s o 5 and s have different background colors, the label Y is probably more suitable than the label N for the boundary b 5 .Ifone text segment contains very similar terms as in the title of a news page, we probably should not split the boundary beneath this segment since it will separate the title and the main content of the news. According to the sources, the fea-tures are categorized into two types, namely, local feature, and context feature,
Local features of a boundary b i are computed based on the characteristics of its surrounding sub-segments, i.e., s and s b i . We design two types of local features, namely, local segment features, and local segment relation features.
Local segment features are designed to capture the char-acteristics of a single segment s o i or s b i .Let  X  ( s the feature vector related to s o i . The combined feature map of s o i and the label c i of b i is denoted as: where  X  is the operator of tensor multiplication,  X  c ( c the canonical representation of the label c i : where  X  ( Y ,c i ) is an indicator function and has the value 1 if c = Y and the value 0 otherwise. As revealed by Equation 3, each single feature is mapped to a dimension according to the label c i . Similarly, the combined feature map of the segment s b i and the label c i is denoted as:
Local segment features include basic features, geographic features, color features, content features, text appearance features, text richness features, tag richness features, font size features, etc. Some examples are the number of links in the segment, the background color of the segment, the number of terms in a segment that also appear in the page title, the token based text density over the segment size, etc.
Local segment relation features reveal the relation of the two segments. Let  X  ( s o i ,s b i ) represent the features summa-rized for capturing the relations between s o i and s b i combined feature map is denoted as: Such features include height difference in the DOM, text length difference, color difference, size difference, font size difference, typeface difference, text similarity, etc.
Human readers also consider the context information in identifying page segments. Take the page given in Fig-ure 2(a) as an example. In addition to the local features from s o 10 and s b 10 , the sibling segments s o 8 and s vide useful hints to determine the label of b 10 . Suppose the DOM structures of these four segments are similar, it is very likely that they present four records of the same type of information. Thus, it is probably not preferred to split the boundary b 10 . We design two types of context features, namely, context segment features, and context segment re-lation features, for each b i to capture the characteristics of the segments in the sibling sequence. Context segment fea-tures include the average DOM structure similarity with the sibling segments, occurrence frequency based on DOM struc-ture similarity, mean and standard deviation of occurrence intervals, etc. Context segment relation features reveal the relation of the two segments X  context features, such as the difference of occurrence frequency, occurrence characteris-tics of the forest of them, etc. The combined context feature maps are denoted as: where S is the sequence of the corresponding sibling seg-ments,  X  is the features extracted according to the charac-teristics of the segments in S .
By aggregating the above features, the combined feature map of a boundary b i and its label c i is presented as:
The combined feature representation of a WPS-graph G and its label assignment C is the combination of the feature map from each boundary: As shown above, different features are combined and the difference of their impacts will be captured by the corre-sponding weights in w .
To infer a label assignment satisfying the dependency con-straints, one strategy is to consider the vertices one by one in the topological order as depicted by the WPS-graph. Only when all the ancestors of a vertex are labeled with Y ,we will evaluate Y and N for it. Otherwise, we assign the label N to this vertex. However, this manner is myopic and can-not achieve an optimal solution. To overcome this problem, we formulate the label inference on G as a binary Integer Linear Programming (ILP) problem with the label depen-dency constraints transferred as the constraints of the binary ILP. The source code of the inference is publicly available at http://www.se.cuhk.edu.hk/~textmine/ .

Let F  X  i denote the partial objective value achieved by the vertex b i with the label c  X  i in the optimal label assignment C  X  . F  X  i can be represented as: where x i =  X  ( Y ,c  X  i ). F i and F i are calculated as follows: If x i =0,wehave F  X  i = F i and c  X  i = N .Otherwise,wehave F i = F i and c  X  i = Y . Thus, the optimal value of F can be computed as F  X  = n i =1 F  X  i ,where n = | B | .Thetaskof finding the optimal label assignment as given in Equation 1 can be transformed as solving a binary ILP problem:
The second constraint ensures that if there is an edge from b to b j in E of G and b i has the label N , i.e., x i =0, b also have the label N , i.e., x j =0;if b j has the label Y , i.e., x j =1, b i must also have the label Y , i.e., x i =1. After removing the unchanged term F i in F  X  i , Formula 15 can be equivalently written as the following compacted form: where f is the coefficient vector and f i = F i  X  F i , A is the constraint matrix, where m = | E | . Each constraint x  X  x j corresponds to one row in A whose j -th element is +1, i -th element is -1 and other elements are 0. However, binary ILP has been proved to be NP-hard [35]. To solve it, we first relax the binary ILP in Equation 16 to a linear programming (LP) problem which has efficient and widely used solvers such as simplex [11]. Then, it can be proved that the linear relaxation has an integral optimal so-lution, which is thus also the optimal solution of the original binary ILP problem. The constraint x  X  X  0 , 1 } n is written as x  X  Z n + and x  X { 1 } n . x  X { 1 } n and A x  X { 0 } m are jointly written as B x  X  b ,where B = A I Thus, we get an ILP problem: The linear relaxation of the problem in Formula 17 is: One nice property of the LP problem in Formula 18 is that the constraint matrix B is totally unimodular [35], which can be proved straightforwardly and is omitted due to the tight space. This property guarantees that the LP problem has an integral optimal solution for any integer vector b for which it has a finite optimal value [35]. Therefore, the integral optimal solution of the LP in Formula 18 is also an optimal solution of the ILP in Formula 17. Obviously, it is also an optimal solution of the binary ILP in Formula 16, from which the ILP is transformed.
 To speed up the inference algorithm, the leaf vertex b i in G whose value satisfies F i  X  F i can be safely labeled as N and removed from the inference procedure. The reason is that the labeling of its ancestor vertices does not affect b Obviously, this removal preprocessing is recursive until each of the remaining leaf vertex b j has F j &lt;F j .
We develop a machine learning algorithm based on the structured output Support Vector Machine framework [32] to determine the feature weights. Our learning algorithm considers the inter-dependency among the vertices of a WPS-graph during the training process. Therefore, the feature weights are determined with a global view on WPS-graphs but not merely on individual segmentation boundaries. The source code of this learning framework is publicly available at http://www.se.cuhk.edu.hk/~textmine/ . Let { ( G i , C i ) } N i =1 denote a set of training data instances. The quadratic program form of the SVM model with slack re-scaled by the loss is: where  X  i  X  0 is the slack variable of G i , C&gt; 0isatrade-off constant of the two parts and takes value 1 in this pa-per, Y is the set of all possible label assignments of G i tween the objective values of C i and C ,and X ( C i , C )de-notes the loss caused by the label assignment C . Similarly, the quadratic program form of the SVM model with margin re-scaled by the loss is:
Tsochantaridis et al. proposed a cutting plane based al-gorithm to solve this optimization problem in its dual for-mulation [32]. It selects a subset of constraints from the exponentially large set Y to ensure a sufficiently accurate solution. The procedure of finding the feature weights is briefly summarized in Algorithm 2. S i is the working set of selected constraints for the instance G i ,  X   X  X  are the Lagrange multipliers, and  X  is the precision parameter. The algorithm proceeds by finding the most violated constraint for G i in-volving  X  C (refer to Line 5). If the margin violation of this constraint exceeds the current  X  i by more than  X  (refer to Line 7), the working set S i of G i is updated.  X   X  X  and w are also updated with the updated working set accordingly. We refer the reader to [32] for more details of the algorithm.
In the learning procedure as depicted in Algorithm 2, it is required to optimize the cost function in Line 4 for finding the most violated constraint corresponding to  X  C : The upper and the lower forms of H ( C ) in Line 4 correspond to the slack re-scaling and margin re-scaling definitions as given in Formulae 19 and 20 respectively.
Recall that the missing of the informative segments of a page causes larger loss. Our slack re-scaling formulation as given in Formula 19 is able to take this into consideration with a loss function defined based on informative segments: where B info denotes the set of proper informative bound-aries, C ( b )isthelabelof b in C ,  X   X  is an indicator function which takes the value 0 if C i ( b )= C ( b ) and takes the value 1otherwise.If B info =  X  ,weset X ( C i , C )=1.
 To optimize the following cost function: we enumerate a subset of possible loss value levels as de-fined in Equation 22. The derivation of the optimal  X  C for Equation 23 is summarized as Proposition 1.

Proposition 1. Let C  X  denote the label that achieves the optimal value for F and B  X  info denote the proper informative
Algorithm 2: Finding feature weights via structured output SVM learning. 10: end if 11: end for boundaries wrongly labeled in C  X  .Let { B info } be all subsets of B info having more elements than B  X  info ,andlet C be the label assignment that achieves the largest F value when all boundaries in B info are wrongly labeled and all boundaries in B info \ B info are correctly labeled. The label  X  C maximizing H ( C ) is from  X  B info { C } X  X  C  X  } .
 The proof of Proposition 1 is straightforward and omitted due to the space limitation.
The margin re-scaling formulation in Formula 20 is de-signed to perform a general segmentation of pages. For this design, we define a hamming distance based loss function: where each boundary is treated equally. To optimize the second cost function as given in Equation 25: it is equivalent to optimize: Let  X  C denote the label that achieves the largest value of H ,and  X  H i denote the partial value of H achieved by the vertex b i having the label  X  C ( b i ), denoted as  X  c i represented as: where x i =  X  ( Y ,  X  c i ). H i and H i are calculated as follows: Referring to Equations 12, 13, and 14, the task of finding the label assignment  X  C maximizing H can also be solved in the same way as given in Section 5.
Data Preparation . We categorize the pages on the Web into 10 broad types as given in the first column of Table 1. 1,000 pages of the first 9 types were randomly picked from a Table 1: Different types of pages on the Web and the number of each type in our data set.
 large page repository, since adult pages are normally omit-ted by applications and wap pages are designed differently compared with normal Web pages. The number of pages in different types is given in the third column of Table 1. The total number of pages in our data set is almost 10 times of that used in some previous works [7, 19]. In addition, the smallest type contains 46 pages, which is a reasonable number for conducting type-specific experiment. To perform data annotation, we developed a browser-based user-friendly tool for annotators to specify the label of each boundary. Af-ter the annotators finish annotating one page, they label the informative segment in the page. If the page is not an index page, the segment that presents the major information of the page is annotated as informative segment, such as the news content segment of a news page and the result segment of a Web search page.

Evaluation Metrics . The segmentation result gener-ated by a Web page segmentation method groups the visual elements of a Web page into cohesive regions visually and se-mantically. Similar to the previous works [7, 19], we regard each generated segment as a cluster of visual elements and employ cluster correlation metrics to conduct the evalua-tion. The first metric is the Adjusted Rand Index (ARI) [18]. Rand Index is defined to measure the agreement between an output clustering and the ground truth clustering by count-ing the pairs of elements on which two clusterings agree [28]. The Rand Index lies between 0 and 1, with 0 indicating that the two clusters do not agree on any pair of elements and 1 indicating that the two clusters are exactly the same. ARI is a corrected-for-chance version of the Rand Index, which equals 0 on average for random partitions, and 1 for two identical partitions. Therefore, the larger the ARI value is, the better the performance is. Mutual Information (MI) is a symmetric measure to quantify the statistical information shared between two distributions [10]. It can provide an indication of the shared information between a pair of clus-terings. The second metric employed in this paper is the Normalized Mutual Information (NMI) introduced in [30], which is the MI between two clusterings normalized with the geometric mean of their entropies. NMI ranges from 0 to 1 and larger value indicates better performance.
Comparison Methods .Kohlsch  X  utter and Nejdl ob-served that the number of tokens in a text fragment, i.e. Table 2: Comparison of segmentation results on the entire data set.
 text density, is a valuable feature for segmentation deci-sions [19]. Therefore, they proposed a block fusion model that utilizes the text density ratios of subsequent blocks to identify segments, where the Web page segmentation prob-lem is reduced to solving a 1D-partitioning task. Among the variants of their model, B F-RULEBASED achieves the best performance. B F-RULEBASED constrains the density-based fu-sion operation between subsequent blocks with a set of gap-enforcing tags and a set of gap-avoiding tags. We imple-mented B F-RULEBASED for conducting comparison. The opti-mal fusing threshold  X  max is tuned with the training set of our data. Chakrabarti et al. proposed a graph-theoretic ap-proach to deal with Web page segmentation [7]. They cast the problem as a minimization problem on a weighted graph with the nodes as the DOM tree nodes and the edge weights as the cost of placing the end nodes in the same segment or different segments. They presented a learning framework to learn these weights from manually labeled data. The proposed CC LUS algorithm solves this problem with corre-lation clustering on a graph that only contains leaf DOM nodes of a page as the nodes of the weighted graph. The proposed GC UTS algorithm solves this problem with energy-minimizing cuts on a graph that regards each DOM node as a node of the graph. GC UTS involves a rendering constraint to ensure that, if the root node of a subtree is in a partic-ular segment, all the nodes in the entire subtree are in the same segment. We implemented both CC LUS and GC UTS to conduct comparison. Our training data is employed to learn the feature weights as well as the trade-off parameter  X  of two counterbalancing costs in the objective functions.
Recall that, in Section 6.1, we employ two quadratic forms of SVM model, namely, slack re-scaling and margin re-scaling, which incorporate informative segment oriented loss and Hamming loss respectively. Accordingly, we have two vari-ants of our model, named WPS Slack and WPS Margin re-spectively. The learning precision  X  for the feature weight estimation in Algorithm 2 is set to 0.1.

We first conduct experiment on the entire data set con-taining 1,000 pages. 4-fold cross-validation is employed and the average performance evaluated with ARI and NMI is reported in Table 2. Both variants of our model can outper-form the comparison methods significantly. The improve-25%. In addition, paired t-tests (with P &lt; 0.01) comparing the variants of our model with the comparison methods show that the performance of our variants is significantly bet-ter. Among different variants of our model, WPS Margin with Hamming loss can achieve better performance than WPS Slack with informative segment oriented loss. It is because WPS Slack favors the segmentations that generate more accurate informative segments, which makes it perform less accurately on the uninformative segments. While the Figure 4: Percentage of pages below an ARI value. Table 3: Type-specific segmentation results in ARI. evaluation metrics ARI and NMI do not consider the impor-tance difference among the segments, which gives WPS Margin more advantage in the reported performance.
 .
 And CC LUS achieved the lowest accuracy, because it re-ports many non-rectangular segments since the built graph of it only contains the leaf DOM nodes. Generally, non-rectangular segment should not exist according to common sense. We observe that the page designers now prefer us-Sheets (CSS) in page design. This makes the heuristics based on gap-enforcing tags in B F-RULEBASED less effective. Similar to our method, GC UTS is more adaptable since its feature weights are tuned with training examples. It also solves the problem of reporting non-rectangular segments to some extent with the rendering constraint. The cumulative percentage of Web pages for which the segmentation perfor-mance of a particular method is less than a certain ARI value is plotted in Figure 4. The slower the curve goes up from left to right, the better the corresponding method is. For value lower than 0.6 are about 82%, 56% and 52% respec-tively. For the variants of our framework, such percentages are between 20% to 30%.
To evaluate the type-specific performance of different seg-mentation methods, we employ the page set of each type as an individual experimental data set. Also 4-fold cross-validation is conducted on each of the nine page sets. The results evaluated with ARI are reported in Table 3. We find that all methods can achieve better performance on an individual page type compared with on the entire data set. This is because the trained or tuned parameters in dif-ferent methods are more tailor-made for a particular type so as to achieve better accuracy. Among different types of pages, Search Result and Blog are relatively easier to han-dle. The main reason is that these two types of pages have relatively simple structures. Index and News are the most difficult types. The reason is that these two types of pages have more heterogenous structures and various information topics. In addition, the loss function in WPS Slack is set to 1 for the index pages since no informative segments are Figure 5: Stacked percentage in different ARI value intervals by WPS Margin.
 Table 4: Results of informative block extraction. annotated for them, which makes WPS Slack less effective in tackling index pages compared with tackling the other types. Note that our framework does not have type-specific features, since we assume that the type of the pages is un-known. The stacked percentage in different ARI value inter-vals for individual page types is given in Figure 5. Besides the types of Product and News, the percentage of ARI value lower than 0.6 is no more than 20%.
Recall that besides the index pages, we annotated infor-mative segments in the pages of other types. We conduct evaluation on the performance of our method for segment-ing these informative segments. If the informative segment of a page is accurately segmented, we regard this page suc-cessfully handled. If the informative segment is regarded as a subpart of any other segment or it is separated into several sub-segments, this page is not successfully handled. We calculate the percentage of the pages whose informative segments are successfully segmented.

In addition to the eight individual sets of pages, we have another data set that aggregates the pages of these eight types and it is called ALL \ Index. The average results from 4-fold cross-validation are reported in Table 4. The WPS Slack with informative segment oriented loss achieved the best performance and dominated other methods significantly. It demonstrates that the design of informative segment ori-ented loss is helpful for capturing informative segments for different types. It also shows that our variants with different loss designs have their own advantages to handle segmen-tation tasks with different focuses, making our framework more adaptable compared with previous works. The trained segmentation models on the individual types are more tailor-made so as to achieve better results compared with that on ALL \ Index. After some manual checking, we found that most unsuccessful cases include some noise elements as part of the informative segments, such as the comments in the news and blog pages. In B F-RULEBASED , one heuristic rule is that a segmentation gap should be enforced after the tags and the main content of the segment, which results in over-segmentation mistakes.
Algorithm 3: In-segment position finding. 10: else 11: if n =2 then 13: else 16: end if 21: else 25: end if 26: end if
As discussed above, the output of our segmentation model provides a good structural analysis of Web pages enabling better utilization for different Web page mining tasks. Such efficacy of our model is examined in the task of page func-tional type classification [26]. Different from topical type, functional type describes the role that a Web page plays, such as image page mainly presenting an image, video page mainly presenting a video, etc. The identification of func-tional type is very useful for different Web mining problems. For example, search result ranking normally considers func-tional type as one factor. Page crawler can also trigger a better crawling strategy given the type of crawled pages.
The functional type of a page is closely related to the page structure and the functional terms appearing in differ-ent positions of the page. For example, the functional terms  X  X eply X  and  X  X ost X  in the informative segment of a forum page are indicative features. The term X  X orum X  X n the header and bottom sections is also an important feature. To utilize the output of our segmentation model in this classification task, five different in-segment positions are defined, namely, top, bottom, left, right, and middle. For a segment s ,these spectively. Let b 1 ..n denote the boundaries having the label Y inside s . The procedure of finding the in-segment positions of s is given in Algorithm 3. Note that an in-segment posi-tion can contain more than one subsegments when n&gt; 2.
To construct the feature vector of each input page, we consider the in-segment positions in two major segments of the page obtained from different layers of the segmentation procedure. The first major segment is the entire page and the second major segment is the largest segment obtained by segmenting the entire page. In Figure 3, the major seg-ments are s 0 in (a) and s o 2 in (b). Suppose b 1 , b 2 , b b 4 are labeled as Y by our segmentation model. Thus, s 0 is segmented into s 0 ( t ) containing s o 1 , s 0 ( m ) containing s and s 0 ( b ) containing s b 2 as shown in Figure 3(b). s mented into s o 2 ( l ) containing s o 3 , s o 2 ( m ) containing s s ( r ) containing s b 4 as shown in Figure 3(c). After a page is segmented by our model and the in-segment positions of the major segments are determined by Algorithm 3, we calculate the in-segment position based TF-ISF value for each term t in the individual positions, where TF is the frequency of t in this position of the page and ISF is the inverted segment frequency of t calculated based on the same position across the corpus. Therefore, a single term is decomposed into 10 different dimensions in the feature vector according to its in-segment positions in the two major segments. After some basic preprocessing such as stop word removal, we perform feature selection with information gain (IG) [36]. Only the top 10% of terms are retained in the construction of the feature vector so as to control the dimensionality.
We prepare another collection of 4,000 pages from the eight types having informative segments as indicated in Ta-ble 4. Each type contains about 500 pages. LibSVM [8] with linear kernel is employed to train eight classifiers under one-against-the-rest strategy for multi-class classification. To conduct comparison, three baseline methods are designed. The first baseline, named Unstructured Text, is a purely text-based method without considering page structure. It employs all terms after preprocessing to form the feature vectors. We design the other two comparison methods based on the informative segments detected by B F-RULEBASED and GC UTS . The informative segment of each page is identified with the following rule. The largest segment that appears (maybe partially) in the first screen of a page is regarded as its informative segment. The first screen of a page is defined as the top fraction of the page with the height of 1,000 pixels. Then, the feature vector of a page employs the terms appear-ing in its informative segment as the dimensions. These two respectively. For our method, the employed segmentation model is the variant of WPS with margin re-scaling. All the segmentation models are tuned or trained with the entire data set in Table 1.

The average results, evaluated with macro-averaged Pre-cision, Recall and F1 measure, of 4-fold cross-validation are reported in Table 5. Our method achieves significantly bet-ter performance compared with the other methods. The per-centages of improvements in F1 are about 23% to 37%. This demonstrates that the page structure information, revealed by the in-segment positions in our method, is very useful in the classification of page functional types. The F1 values of different page types are given in Figure 6. We observe difficulties in handling three types of pages, namely, Blog, Figure 6: Classification performance for different page types.
 News, and Search Result. It is because these types of pages have very few functional terms in their main content. The two informative-segment-based methods cannot well distin-guish the main content of a news page and that of a blog page. Although the baseline Unstructured Text keeps the functional terms outside the informative segments, it does not have the structure information to differentiate the ap-pearances of the same term as functional term in a specific position and as a normal term in the main content. There-fore, its performance is even degraded by the negative effect of the noise.
We propose a framework which can perform page segmen-tation with a structured prediction approach. The segmen-tation task is formulated as a structured labeling problem on the WPS-graph. Each labeling scheme on the WPS-graph corresponds to a possible segmentation of the page. The feature weight learning algorithm is developed based on the structured output Support Vector Machine framework so that it is able to consider the inter-dependency among the vertices of a WPS-graph. Extensive experiments demon-strate that our framework achieves better performance com-pared with state-of-the-art methods.
