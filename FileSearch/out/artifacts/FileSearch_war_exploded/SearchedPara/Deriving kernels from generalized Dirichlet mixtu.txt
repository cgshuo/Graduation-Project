 1. Introduction and flexible due to its favorable properties widely discussed in Bouguila and Ziou (2006); Bouguila, Ziou, and Hammoud each mixture component namely the parameters of each generalized Dirichlet and the importance of each component (i.e specific problem of gender classification.

Recent studies have shown that a compromise is to take advantage of both approaches via hybrid generative/discrimina-endeavor. Although some approaches have been proposed, most research have been made toward general approaches that capabilities.
 integration of color and spatial information. Finally, Section 5 presents our conclusions. 1.1. Relevant related works the generative model from which is developed. The Fisher kernel has been used, for instance, in Wan and Renals (2005) posed in Jebara, Kondor, and Howard (2004) where two main special cases have been developed namely Bhattacharyya ker-Chan, Vasconcelos, and Moreno (2004) , also. Examples include the Kullback X  X eibler divergence kernel investigated in
Moreno, Ho, and Vasconcelos (2003) , R X nyi and Jensen X  X hannon Kernels. Many other kernels have been proposed also for to G X rtner (2003) for interesting and in depth discussions about kernels on structured data. 2. Fitting generalized Dirichlet mixture models 2.1. The generalized Dirichlet mixture model space, characterized by its 2 D positive parameters a 1 , b for d =1 ... D 1 and c D = b D 1. It is note worthy that the GDD is reduced to a Dirichlet distribution with parameters ( a
A M component GDM is then defined as where the mixing weights p j , j =1, ... , M are positive and satisfy the following mixture, and H  X f ~ P  X  X  p 1 ; ... ; p M  X  ; ~ h  X  X  h 1 ; ... ; h probabilities along with the unknown parameters of interest related to the mixture distributions. other well-known distributions. In Bouguila and Ziou (2006) , we have shown that an important feature of GDM that en-sional ones. Indeed, if a vector ~ X has a GDD with parameters ( a  X  Y ; ... ; Y D  X  using the following geometric transformation ( Connor &amp; Mosimann, 1969 ): Y
Y and thus p  X  ~ Y j a 1 ; b 1 ; ... ; a D ; b D  X  X  Q D d  X  1 can be done via the clustering of Y X  X  ~ Y 1 ; ... ; ~ Y N Many other interesting properties of the GDD and GDM can be found in Bouguila and Ziou (2006); Bouguila et al. (2009);
Boutemedjet et al. (2009); Connor and Mosimann (1969) . 2.2. Model learning problem of learning finite mixture models can be treated as a maximum likelihood estimation problem which is the most putes the expectation of the complete data log-likelihood using the posterior probabilities p  X  j j ~
Y belongs to the j th component) based on the current parameter H where The M-step maximizes the Q function with respect to H in order to obtain the new parameters values at iteration t +1,
H ( t +1) . The maximization of Eq. (5) with respect to p j out using Newton X  X aphson method as shown in Bouguila and Ziou (2006) .
 ber of clusters which is time consuming. Here we propose another learning approach based on both an agglomerative EM as follows:
Algorithm 1. We have tested several merge criteria such as: where ~ P j 1  X  X  p  X  j 1 j ~ Y 1 ; H  X  t  X   X  ; ... ; p  X  j
Nakano, Ghahramani, and Hinton (1998) we merge the j 1 th and j the following to choose j 1 and j 2 ( Figueiredo et al., 1999 ) where D p  X  ~ X j h k  X  ; p  X  ~ X j h j  X  by (see Appendix A ) The Chernoff distance ( Chernoff, 1952 ) is given by (see Appendix B ) The Matusita distance (called also the Hellinger distance) is given by Matusita and Fit (1955) and is related to the Bhattacharyya distance as following ( Aherne, Thacker, &amp; Rockett, 1998 ) taining all the observations previously allocated to j 1 and j 3. GDM-based SVM kernels kernels from generative models.
 (KL) divergence between Gaussian mixtures. Symmetric KL divergence (or J-divergence) based kernel replaces the compu-et al., 2003 ) ical approximations methods ( Moreno et al., 2003 ): where ~ X 1 ; ... ; ~ X L is a sample generated from p  X  ~ given by the following in the case of the GDD: Jebara (2003) .
 The JS kernel, generated according to the Jensen X  X hannon divergence ( Lin, 1991 ), is given by Chan et al. (2004) where x is a parameter and H  X  p  X  ~ X j H  X  X  R X p  X  ~ X j H  X  log p  X  lowing in the case of the GDD (see Appendix C ) It is noteworthy that, when x = 2, the JS divergence is reduced to Lin (1991) nels and then we have to use Monte Carlo numerical approximations methods. 4. Experimental results
In this section we present, analyze and discuss the performance of the proposed approach through a set of experiments based on (KL) divergence (KL-GDM), R X nyi divergence (R-GDM), JS divergence (JS-GDM) and Bhattacharyya distance (B-mixture (DM) namely KL-DM, R-DM, JS-DM and B-DM; and the classic Gaussian mixture (GM) namely KL-GM, R-GM, JS-and the generalized Dirichlet (FK-GD) mixtures. In all our experiments we have considered the case where x = 2 for the ues for all design parameters were obtained by performing 10-fold cross-validation. 4.1. Object detection via example-based learning negative examples ( Emde, 1994 ). We focus here on two main detection problems which have relatively received a lot of ered a database of 5000 face images collected from different sources commonly used as benchmarks, such as Yale half for testing.

An important step when dealing with object detection is the extraction of low-level features, such as color, shape and matrices to feed SVM classifier.

Table 1 shows the average error rates for both detection problems using kernels generated from the GDM, DM and GM. better than what we have reached when using directly the generative models namely the GDM, DM and GM by evaluating the were 14.83% and 19.52% for face and vehicle detection, respectively. 4.2. Color spatial information modeling for content-based image classification
With the advent of large image collections with complex images, efficient approaches to model and categorize this con-spatial distance. Let I be an L C image composed of pixels p ( x , y ). The colors in I are quantized into m colors c
For a pixel p , let I X  p  X  denotes its color. Let I c  X f p jI X  p  X  X  c g and D ={ d measured using the L 1 norm. The correlogram of image I is defined for color pair ( c
Which gives the probability that given any pixel p 1 of color c to compute the correlogram it suffices to compute the following count
GDM mixture by observing that for each color pair ( c i , c ( Bouguila, 2008 ) is then represented by m 2 D -dimensional vectors of counts which after their normalization can be modeled by a GDM.
An important issue when processing color images is the choice of the color space. The RGB representation is the most one contains 1920 images and was derived from the Vistex color texture database obtained from the MIT Media Lab. formed by dividing each of the 512 512 images into 64 64 images. Since each 512 512  X  X  X other image X  X  contributes 64 92.96%, 92.65% and 92.34% using GDM-KL, GDM-R, GDM-B, GDM-JS and GDM-FK, respectively.
In the case of DM-based kernels the average classification accuracies were equal to 91.61%, 91.02%, 90.91%, 89.79% and 89.60% using DM-KL, DM-R, DM-B, DM-JS and DM-FK, respectively. As for the GM-based kernels the average classification accuracies were 86.22%, 86.16%, 86.08%, 86.02% and 85.94% using GM-KL, GM-R, GM-B, GM-JS and GM-FK, respectively. second data set (see Table 7 ).

GM-based kernels. 5. Conclusion tion of GDM models based on both an agglomerative EM and MML criterion. The merits of the generated kernels have been ences, data mining and information processing where non-Gaussian data are largely present. Acknowledgment The completion of this research was made possible thanks to the Natural Sciences and Engineering Research Council of Canada (NSERC).
 Appendix A. Proof of Eq. (10)
If a S -parameter density p belongs to the exponential family, then we can write it as the following ( Brown, 1986 ) where G ( h )= ( G 1 ( h ), ... , G S ( h )), T  X  ~ X  X  X  X  T ( Brown, 1986 ) where E h is the expectation with respect to p  X  ~ X j h  X  . The K X  X  divergence between two distributions p  X  belong to the exponential family is given by Kupperman (1958)
It is straightforward to show that the GDD belongs to the exponential family, since it can be written as the following ( Bouguila et al., 2009 )
Then, G ( h )= ( a 1 , ... , a D , b 1 , ... , b D ), T  X  log  X  1 X 1  X  ; log  X  1 X 1 X 2  X  log  X  1 X 1  X  ; ... ; log 1 where W (.) is the digamma function. By substituting the previous four equations into Eq. (29) , we obtain Appendix B. Proof of Eq. (11) We can show that
Let a 00 d  X  r a d  X  a 0 d r a 0 d and b 00 d  X  r b d  X  b
Appendix C. Proof of Eq. (22) Moreover, we have the following according to Eqs. (30) X (33) By substituting Eqs. (30) X (33), (35) and (36) into Eq. (34) , we obtain the following References
