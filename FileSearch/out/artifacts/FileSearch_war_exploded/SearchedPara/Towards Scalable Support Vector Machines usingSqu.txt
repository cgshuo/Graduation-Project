 Supp ort vector machines (SVMs) pro vide classi cation mo d-els with strong theoretical foundations as well as excellent empirical p erformance on a variety of applications. One of the ma jor drawbacks of SVMs is the necessit y to solve a large-scale quadratic programming problem. This pa-p er combines likeliho o d-based squashing with a probabilis -tic formulation of SVMs, enabling fast training on squashed data sets. W e reduce the problem of training the SVMs on the weighted \squashed" data to a quadratic programming problem and show that it can b e solved using Platt's sequen-tial minimal optimization (SMO) algorithm. W e compare p erformance of the SMO algorithm on the squashed and the full data, as well as on simple random and b o osted samples of the data. Exp eriments on a numb er of datasets show that squashing allows one to sp eed-up training, decrease memory requirements, and obtain parameter estimates close to that of the full data. More imp ortan tly, squashing pro duces close to optimal classi cation accuracies.
 H.2.8 [ Information Systems ]: Database Managemen t| Database Applications,Data Mining ;I.2.6 [ Computing Metho d-ologies ]: Arti cial Intelligence| Learning supp ort vector machines (SVMs), scalabilit y, squashing, b o ost-ing Following the pioneering work of Vladimir Vapnik [15], sup-p ort vector machines (SVMs) are steadily gaining p opularit y in the machine learning community. SVMs ha v ebeen pro v en to exhibit several attractive theoretical prop erties including maximum margin separation b etween the classes. In addi-tion, SVMs ha ve b een empirically shown to outp erform con-ventional classi ers on a variet yofbenc hmarks [13]. How-ev er, the applicabil ity of SVMs to large datasets is limited b ecause of the high computational cost involved in solving quadratic programming problem arising in their training. Various training algorithms have b een prop osed to sp eed up the training, includin g chunking [13], Osuna's decom-p osition metho d [9], and Sequential Minimal Optimization (SMO) [11]. Although these algorithms accelerate the train-ing, they do not scale well with the size of the training data. Another approach to reducing the computational cost is to use approximation metho ds. The simplest metho d is sam-pling from the original dataset and using the sample to train the SVM. An extension of this idea w as explored b yP a vlo v et. al. [10] who rep orted results on the application of b o ost-ing to training SVMs. Bo ost-SMO algorithm trains a se-quence of SVM classi ers on samples of data so that eac h subsequent classi er concentrates mostly on the errors made by the previous ones [12, 10]. While this metho d is not opti-mal in general, it allows for fast training of SVMs, has sub-stantially lower memory cost and yields p erformance close to that of SMO on the full data.
 In this pap er we suggest another metho d for scaling SVMs up based on squashing . DuMouc hel et. al. [5] and Madigan et. al. [7] recently intro duced squashing as a tec hnique that allows one to scale a dataset down while preserving its statis-tical prop erties. In particular, likeliho o d-ba sed squashing [7] assumes a particular statistical mo del and tries to preserve the b ehavior of the likeliho o d function of the original data (referred to as the \mother-data") in the neighb orho o d of the maximum likeliho o d solution. To apply likelih o o d-b ased squashing to SVMs it is necessary to ha ve a probabili stic in terpretation (see, e.g.,[6, 16]). W euseanin terpretation of the SVM training pro cedure [14] as a problem of nding maximum ap osteriori values for the parameters of the SVM. W esho w that the probabili sti c interpretation of SVM train-ing in conjunction with likeliho o d-based squashing allows one to scale the training up. W e reduce the problem of training the SVMs on the w eighted \squashed" data to a quadratic programming problem and show that it can b e solved using the SMO algorithm. W e compare the p erfor-mance of the SMO algorithm on the squashed and the full data, as well as on simple random and b o osted samples of the data. Exp erimen ts on a n um b er of datasets sho w that squashing allows one to sp eed-up training, decrease mem-permission and/or a fee.

KDD 2000, Boston, MA USA  X  ACM 2000 1 -58113 -233 -6/00/0 8 ...$5.00 ory requiremen ts, and obtain parameter estimates close to that of the full data. More imp ortan tly , squashing pro duces classi cation accuracy that is close to optimal. Note that although our results can b e directly generalized to large-scale datasets that do not t in the main memory ,w eare using mo derately sized data that resides in memory in our exp erimen ts.
 The rest of the pap er is organized as follo ws. W e giv e a brief o v erview of SVMs in section 2. W e then describ e squash-ing and its application to SVMs in more detail in section 3. Section 4 describ es results of empirical ev aluation of the al-gorithms on 4 datasets. In section 5 w edra w conclusions and discuss p ossible directions for the future w ork. W e giv e a brief o v erview of linear SVMs here. Supp ose that w eha v e a total of N lab eled patterns in the d -dimensional space D = f ( x i ;y i ) g belonging to t w o classes, so that lab els are either 1 or 1.In linear SVMs the line separating the classes is sough tintheform where w is the normal v ector and b is the in tercept of the h yp erplane. The problem of nding the maxim um margin separation b et w een the classes is equiv alen t to the minimiza-+ b ) 1 to ensure that all the patterns in the training set are classi ed correctly [4]. Ho w ev er, in most cases p erfect separation is imp ossible and w e need to trade errors on the individu al training patterns for the maxim um margin. The optimization problem b ecomes sub ject to where C is a tradeo constan t and i are the non-negativ e errors. Th us, learning the parameters of the SVM is reduced to a linearly constrained quadratic programming problem. The standard SMO algorithm solv es the dual of problem 2 and 3 b y decomp osing it in to the smaller problems that can b e solv ed analyticall y . SMO is pro v ably optimal [8]. Sev eral authors (e.g., [16, 14]) ha v e sho wn that the righ t side of Equation 2 can b e treated as a log-p osterior on the parameters w and b , with the rst term corresp onding to a prior on w and the second one to the lik eliho o d. The prior on w will ha v e a normal distribution with zero mean: while the log-lik eli ho o d of the data will (up to a constan t additiv e factor indep enden tof w and b ) b e prop ortional to: where I ( z )is z times the indicator function of z . By solving the dual optimization problem via SMO one es-sen tially nds the maxim um ap osteriori v alues for w and b . W e will use this probabilisti c in terpretation of the training pro cedure to dev elop a squashing pro cedure for SVMs. T o illustrate the idea of squashing let us assume that w eha v e selected a probabilisti c mo del p ( z j ) for the data D and our ob jectiv eisto ndthemaxim um lik eliho o d estimate ML for the v ector of parameters : F urther, supp ose that the terms in equation 6 can b e group ed (or clustered) so that the lik elih o o d s of the individu al p oin ts within eac h group do not v ary signi can tly in the neigh bor-ho o d of ML . The squashing pro cedure eliminates p oin ts that con tribute similar amoun ts to the lik eliho o d b y replac-ing eac h cluster with a single p oin t placed at its cen ter of mass and a w eigh t equal to the n um berofpoin ts in the clus-ter. The maxim um lik elih o o d estimate can no wbe found from expression 7 whic h appro ximates equation 6: Here N c is the n um b er of clusters, c is the w eigh t (or equiv-alen tly the n um berofpoin ts in the original cluster c ) and ( x c ;y c ) sq is the squashed data p oin t placed at the cen ter of the cluster c . Note that w et ypically lo ok for at least a 50-100 times reduction in the amoun t of data so that con-v en tional algorithms can b e applied to the squashed data directly .
 Notice that clustering at this step should not b e computa-tionally more in tensiv e than solving the original maxim um lik eliho o d problem on the mother-data. In the form ulation giv en in [7] squashing mak es only t w o passes through the mother-data. It ev aluates the lik elih o o d s of the training poin ts for certain c hoices of and forms the so-called likeli-ho o dpr o les in the rst pass. After this N c p oin ts are ran-domly selected to form initial cen ters of clusters. Finally ,on the second pass eac h training p oin t is assigned to the cluster whose cen ter's lik eliho o d pro le is closest to the lik eliho o d pro le of the data p oin t.
 Imp ortan t design issues for the squashing algorithm include the c hoice of the n um b er and lo cation of the p oin ts in the parameter space to ev aluate the lik eliho o d s at, and the n um-b er of squashed data p oin ts to ensure a sucien tly go o d ap-pro ximation. V arious factorial designs [3] in the parameter space are suggested in [7]. While this metho d is univ ersal, as w e noted ab o v e, for SVMs w e can sample the v alues of w from the prior distribution in equation 4. The c hoice of the in tercept term b can b e made from the condition that the h yp erplane go es through the cloud of training p oin ts. W e ev aluate the lik eliho o d of eac h training p oin t for the xed pair of w and b and rep eat the selection pro cedure L times, where L is the length of lik elih o o d pro le. Our exp erimen ts sho w that for SVMs the p erformance of the squash-SMO metho d is almost insensitiv etothec hoice of L as long as L is at least on the order of 10 2 .
 Supp ose that w eha v e p erformed squashing of the original data set using the lik elih o o d de ned in equation 5. W eno w need to classify the squashed dataset D sq = f ( x c ;y c con taining N c w eigh ted p oin ts. An expression for the lik e-liho o d of the squashed dataset (up to a constan t additiv e factor indep enden tof w and b ) is prop ortional to since the squashed dataset con tains c p oin ts coinciding with ( x c ;y c ) sq . Going bac k to the non-probabili sti c form u-lation w e will ha v e the follo wing optimization problem for the squashed data: with the constrain ts where the slac kv ariables i are non-negativ e as b efore. It is easy to see that the substitution of slac kv ariables i = transforms the ob jectiv e function for the squashed problem in to exactly the same form as w e had for the mother-data, while the inequalities for constrain ts will ha v e slac kv ariables i divided b y the corresp onding w eigh ts. Th us, the only di erence b et w een the dual problems for the mother-and the squashed data will b e in the allo w able regions for the Lagrange m ultiplie rs: the i th m ultiplier will b e b ounded b y sq i in the squashed data as opp osed to C in the mother-data. Once the SMO co de is up dated to accoun t for the new b ounds on Lagrange m ultipliers, it can b e used directly on the w eighed data from squashing. W eha v e run exp erimen ts on four datasets, one of whic his syn thetic and the rest are publicly a v ailable at either the UCI mac hine learning [2] or UCI KDD rep ositories [1]. W e ev aluated the p erformance of the full-SMO (SMO on the full training data), srs-SMO (SMO on a simple random sample), squash-SMO (SMO on the squashed data) and b o ost-SMO (SMO on the b o osted samples) with resp ect to the misclas-si cation rate, the time to learn the parameters of the mo del and the memory used during the training.
 W e p erformed all exp erimen ts on Sun Solaris 2.7 UNIX w orkstation with t w o 168MHz Sparc CPUs and 256Mb of main memory .W ea v eraged results for squash-SMO, b o ost-SMO and srs-SMO o v er 100 runs to accoun t for v ariabilit y in the data resulting from sampling. Our implemen tation of the SMO algorithm used cac hing [11] to store frequen tly used dot pro ducts. Syn thetic data w as obtained b y sampling from the sp ecially designed piece-wise gaussian distributions in 2D. Eac h dis-tribution is comp osed of t w o parts as follo ws. Let g and g means =( x ; y )=( 0 : 75 ; 0) and diagonal co v ariance matrices 1 and 2 , suc hthat( 1 ) ( 1 ) 22 =( 2 ) 22 = 9. Let the probabili t y densit y for one of the classes b e g that there is a discon tin ui t yat x = x . Another distribution is a mirror symmetrical image of the rst one relativ e to the line x =0. W e generated 5000 examples of eac h class for b oth training and test sets. This example w as sp eci cally tailored to ha v e high densit y of data from b oth classes in the neigh b orho o d of the optimal decision b oundary x =0. Recall that w e seek the b oundary in the form w 0, here w =( w T able 1 illustrates ho w dramatically di eren tw as the p er-formance of squash-SMO from srs-SMO on this dataset. In the rst column w e rep ort the a v erage relativ e error in pa-rameter estimates o v er 100 runs, where the error is de ned as follo ws: Here ( w gorithm on the full data and ( w ther the squashed or sampled data. Squash-SMO obtains T able 1: Av erage Relativ e Error and St. Dev. in P arameter Estimates on Syn thetic Dataset.
 parameter estimates that are almost 4 times more accurate than srs-SMO. The next 3 columns con tain the standard de-viation of the individual estimates for w runs of srs-SMO and squash-SMO. Notice that the parame-ters estimated b y the squash-SMO are 5 times more stable than estimates obtained b y srs-SMO.
 W e analyzed the structure of the errors made b y di eren t mo dels and concluded that srs-SMO made most errors in the neigh b orho o d of the optimal decision b oundary . Since Squash-SMO w as able to obtain more stable and accurate parameter estimates, it exhibited m uc h b etter p erformance in that region.
 T able 2 sho ws the error on the test set for v arious mo dels on the syn thetic data. Clearly , srs-SMO pro vides the w orst impro v emen to v er the baseline mo del, full-SMO is the b est with an error close to the Ba y es error rate, and squash-SMO is a close runner-up.
 T able 2: T est Set Error Rates On Syn thetic Dataset. The public datasets w ere \The Microsoft Anon ymous W eb" and the "F orest Co v er T yp e" datasets a v ailable at UCI KDD arc hiv e [1] and Adult dataset a v ailable at UCI mac hine learn-ing rep ository [2]. The classi cation task for the Web Data , as w e p ose it, is to predict whether a user will visit the most p opular site S based on his/her visiting pattern of all other sites. This dataset con tains binary data and a v erage record has only 3% of its attributes set to 1. F or A dult data the task is to predict if the income of a p erson is greater than $50K based on sev eral census parameters, suc h as age, education, marital status and so forth. Finally , for the F orest data the task is to distinguish b et w een sev eral forest co v er t yp es based on the information ab out the lo cation of the place, t yp e of soil, distance to the w ater etc. F or this dataset w e exp erimen ted with 2 most p opulated classes for whic hthe misclassi cati on rate is the greatest: Spruce-Fir and Lo dge-p ole Pine. The parameters of all datasets are summarized in T able 3.
 Note that one of the distinct adv an tages of the metho ds that use sampled or squashed data is that they allo w to do cross-v alidation to optimize the parameters of the SVM classi er. Since m -fold cross-v alidation will increase the computational cost of training b y a factor of m , using it for the full-SMO is out of question. In our exp erimen ts w e used 5-fold cross-v alidation to adjust the v alue of the regularization constan t for the squash-SMO, srs-SMO and b o ost-SMO. F or the full-SMO C w as set to 1 in all runs. Figure 1: Misclassi cation Rate Vs Lik eliho o d Pro-le Length on the Adult dataset.
 W e analyzed the dep endence of the SVM misclassi cati on rate on the length of the lik elih o o d pro le and the n um-ber N c of squashed p oin ts. In Figure 1 the parameter P refers to the squashing ratio, i.e., the n um ber of poin ts in the squashed data relativ e to the mother-data. As P in-creases from 0.5% to 7.5% w e see a gradual decrease in the misclassi cati on rate so that it approac hes the misclassi ca -tion rate of SVMs on the full data. It is also apparen t that for this data (Adult dataset) lik eliho o d pro les of size 50-100 will yield the b est rate. Similar results w ere obtained on the W eb data. In all exp erimen ts describ ed b elo ww ew ere using a xed length of the lik eliho o d pro le set to 100. T able 4: % Decrease from Baseline Error Rate.
 T able 4 sho ws the misclassi ca tion rate on the test set for v arious datasets and algorithms. The rst ro w, baseline er-ror, sho ws the misclassi cati on rate of the trivial prediction that classi es all examples in to the most p opulated class. The rest of the ro ws sho w what p ercen tage of decrease in er-ror relativ e to baseline a particular mo del is able to pro vide. Notice that srs-SMO algorithm is the w orst and full-SMO algorithm is the b est. The di erence in misclassi cati on rate b et w een the t w o extremes ranges from 8% to 18%. It is remark able that p erformance of b oth squash-SMO and b o ost-SMO is m uc h closer to full-SMO than p erformance of srs-SMO.
 T able 5: CPU Time to Learn P arameters of V arious Mo dels for V arious Datasets. F or squash-SMO (srs-SMO) the rst n um b er is the time to do squashing (sampling) and the second is the time to do SMO on the squashed (sampled) data.
 1% squash-SMO 361.7 + 0.2 132.4 + 0.2 682.9 + 2 1% b o ost-SMO 10 7.5 85.3 T able 5 rep orts timing results for v arious algorithms. Srs-SMO turns out to b e the fastest with times to carry out SMO b elo w 2 seconds. The runner up is b o ost-SMO with times ranging from 7.5 for the Adult data to 85 seconds for the forest data. F or b oth srs-SMO and squash-SMO the time to p erform sampling or squashing dominates o v er the time to do SMO and it tak es m uc h longer to do squash-ing than sampling. The full-SMO algorithm is the slo w est, being on a v erage 10-100 times slo w er than the rest of the al-gorithms. While the optimalit y is a signi can tadv an tage of the full-SMO, its time and memory requiremen ts lea v em uc h to b e desired ev en for the mo destly sized problems that w e considered here. As an illustration, in our exp erimen ts w e only allo w ed cac hing of up to 9 million dot pro ducts. When this upp er limit w as reac hed the cac he w as ushed. F or all non-syn thetic datasets this upp er limit w as reac hed and the program o v erall to ok from 80 to 120Mb of memory during training (the training and test data resided in main mem-ory as w ell). Note that in SMO w e could trade memory for sp eed if w e did not allo w cac hing dot pro ducts, but in this case the training will b e ev en slo w er.
 F or the really large datasets squash-SMO migh t outp erform b o ost-SMO since the latter needs a) to store the curren t b o osted sample and the distributio n o v er all training ex-amples and b) m ultiple passes through the dataset to up-date the distribution on examples and sample from it. The Squash-SMO on the other hand will only need 2 full passes o v er the data as w e discussed b efore during the squashing phase and the mother-data will not b e used at all during SVM training.
 Y et another adv an tage of squash-SMO o v er b o ost-SMO is a b etter mo del in terpretabil it y that ma y b e useful for ex-ploratory data analysis and visualiza tion . W e describ e ho w the use of squashing mak es the training of SVMs applicabl e to large datasets. Comparison with the op-timal full-SMO algorithm sho ws that b oth squash-SMO and b o ost-SMO are able to reac h near-optimal p erformance with m uc hlo w er time and memory requiremen ts. On the other hand, the simplest and fastest standard random sampling SMO on a v erage has a considerably higher misclassi cati on rate. W e conclude that statistical tec hniques based on the ideas of sampling (b o ost-SMO) and lik elihood preserv ation (squash-SMO) promise high computational adv an tages for large datasets o v er the standard SMO algorithm.
 Both squash-and b o ost-SMO allo w one to use cross-v alidation to tune parameters of the SVM, suc h as regularization con-stan t C or the width of the k ernel in non-linear SVMs. Suc h tuning is usually imp ossible on the full data due to the high computational cost.
 Although the p erformance of b o ost-SMO and squash-SMO is similar on the b enc hmark problems, w e outlined when and wh y one algorithm should b e preferred o v er another. In particular, squash-SMO o ers a b etter in terpretabil it y of the mo del and can b e exp ected to run faster than SMO on the datasets that do not reside in the main memory .F or the datasets that t in the main memory w esa w that b o ost-SMO is faster than squash-SMO.
 W e see sev eral p ossibili ties to extend this w ork. The gains o ered b y squash-SMO and b o ost-SMO migh tbeev en more signi can t for non-linear SVMs that tak em uc h longer to train. As w e outlined ab o v e squashing itself tak es consid-erable time. F or the high-dimensi ona l data one migh tneed to increase the pro le length to get acceptable estimates for parameters of the mo del. This will result in a time increase and p erformance degradation. Th us, it w ould b e useful to study the conditions under whic h squashing will w ork w ell on the high dimensional data. An in teresting theoretical question is whether it is p ossible to giv eaboundonthe distance b et w een the parameter estimates obtained b ythe full-SMO and squash-SMO. The researc h describ ed in this pap er w as supp orted in part b y NSF CAREER a w ard IRI-9703120, NSF a w ard I IS-9813584, the NIST Adv anced T ec hnology Program and KLA-T encor, the US Departmen t of Energy and La wrence Liv ermore Lab-oratory , HNC Soft w are Inc., Microsoft Researc h, and Smith-Kline Beec ham Researc h. W ew ould also lik e to thank Jian-c hang Mao for making his SMO co de a v ailable to us. [1] S. Ba y . UCI KDD arc hiv e, h ttp://kdd.ics.uci.edu. [2] C. Blak e and C. Merz. UCI rep ository of mac hine [3] G. Bo x, W. Hun ter, and J. Hun ter. Statistics for [4] C. Cortes and V. V apnik. Supp ort-v ector net w orks. [5] W. DuMouc hel, C. V olinsky , T. Johnson, C. Cortes, [6] T. S. Jaak ola and D. Haussler. Probabilisti c k ernel [7] D. Madigan, N. Ragha v an, W. DuMouc hel, M. Nason, [8] E. Osuna, R. F reund, and F. Girosi. T raining supp ort [9] E. Osuna, R. F reund, and F. Girosi. An impro v ed [10] D. P a vlo v, J. Mao, and B. Dom. Scaling-up supp ort [11] J. Platt. F ast training of supp ort v ector mac hines [12] R. E. Sc hapire. An in tro duction to b o osting [13] B. Sc holk opf, C. Burges, and A. Smola (eds). [14] P . Sollic h. Probabilis tic in terpretation and ba y esian [15] V. N. V apnik. Statistic al L e arning The ory . John Wiley [16] G. W ah ba. Supp ort v ector mac hines, repro ducing
