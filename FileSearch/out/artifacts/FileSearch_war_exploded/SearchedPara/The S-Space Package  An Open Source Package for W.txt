 Word similarity is an essential part of understand-ing natural language. Similarity enables meaning-ful comparisons, entailments, and is a bridge to building and extending rich ontologies for evaluat-ing word semantics. Word space algorithms have been proposed as an automated approach for de-veloping meaningfully comparable semantic rep-resentations based on word distributions in text. Many of the well known algorithms, such as Latent Semantic Analysis (Landauer and Dumais, 1997) and Hyperspace Analogue to Language (Burgess and Lund, 1997), have been shown to approximate human judgements of word similar-ity in addition to providing computational mod-els for other psychological and linguistic phenom-ena. More recent approaches have extended this approach to model phenomena such as child lan-guage acquisition (Baroni et al., 2007) or seman-tic priming (Jones et al., 2006). In addition, these models have provided insight in fields outside of linguistics, such as information retrieval, natu-ral language processing and cognitive psychology. For a recent survey of word space approaches and applications, see (Turney and Pantel, 2010).
The parallel development of word space models in different fields has often resulted in duplicated work. The pace of development presents a need for a reliable method for accurate comparisons be-tween new and existing approaches. Furthermore, given the frequent similarity of approaches, we argue that the research community would greatly benefit from a common library and evaluation util-ities for word spaces. Therefore, we introduce the S-Space Package , an open source framework with four main contributions: 1. reference implementations of frequently 2. a comprehensive, highly concurrent library of 3. an evaluation framework for testing mod-4. a standardized interface for interacting with
The package is written in Java and defines a standardized Java interface for word space algo-rithms. While other word space frameworks ex-ist, e.g. (Widdows and Ferraro, 2008), the focus of this framework is to ease the development of new algorithms and the comparison against exist-ing models. Compared to existing frameworks, the S-Space Package supports a much wider vari-ety of algorithms and provides significantly more reusable developer utilities for word spaces, such as tokenizing and filtering, sparse vectors and matrices, specialized data structures, and seam-less integration with external programs for di-mensionality reduction and clustering. We hope that the release of this framework will greatly fa-cilitate other researchers in their efforts to de-velop and validate new word space models. The p/airhead-research/ , which includes a wiki containing detailed information on the algorithms, code documentation and mailing list archives. Word space models are based on the contextual distribution in which a word occurs. This ap-proach has a long history in linguistics, starting with Firth (1957) and Harris (1968), the latter of whom defined this approach as the Distribu-tional Hypothesis: for two words, their similarity in meaning is predicted by the similarity of the distributions of their co-occurring words. Later models have expanded the notion of co-occurrence but retain the premise that distributional similarity can be used to extract meaningful relationships be-tween words.

Word space algorithms consist of the same core algorithmic steps: word features are extracted from a corpus and the distribution of these features is used as a basis for semantic similarity. Figure 1 illustrates the shared algorithmic structure of all the approaches, which is divided into four compo-nents: corpus processing, context selection, fea-ture extraction and global vector space operations.
Corpus processing normalizes the input to cre-ate a more uniform set of features on which the al-gorithm can work. Corpus processing techniques frequently include stemming and filtering of stop words or low-frequency words. For web-gathered corpora, these steps also include removal of non linguistic tokens, such as html markup, or restrict-ing documents to a single language.

Context selection determines which tokens in a document may be considered for features. Com-mon approaches use a lexical distance, syntac-tic relation, or document co-occurrence to define the context. The various decisions for selecting the context accounts for many differences between otherwise similar approaches.

Feature extraction determines the dimensions of the vector space by selecting which tokens in the context will count as features. Features are com-monly word co-occurrences, but more advanced models may perform a statistical analysis to se-lect only those features that best distinguish word meanings. Other models approximate the full set of features to enable better scalability.

Global vector space operations are applied to the entire space once the initial word features have been computed. Common operations include al-tering feature weights and dimensionality reduc-tion. These operations are designed to improve word similarity by changing the feature space it-self. The S-Space framework is designed to be extensi-ble, simple to use, and scalable. We achieve these goals through the use of Java interfaces, reusable word space related data structures, and support for multi-threading. Each word space algorithm is de-signed to run as a stand alone program and also to be used as a library class. 3.1 Reference Algorithms The package provides reference implementations for twelve word space algorithms, which are listed in Table 1. Each algorithm is implemented in its own Java package, and all commonalities have been factored out into reusable library classes. The algorithms implement the same Java interface, which provides a consistent abstraction of the four processing stages.

We divide the algorithms into four categories based on their structural similarity: document-based, co-occurrence, approximation, and Word Sense Induction (WSI) models. Document-based models divide a corpus into discrete documents and construct the vector space from word fre-quencies in the documents. The documents are defined independently of the words that appear in them. Co-occurrence models build the vector space using the distribution of co-occurring words in a context, which is typically defined as a re-gion around a word or paths rooted in a parse tree. The third category of models approximate co-occurrence data rather than model it explic-itly in order to achieve better scalability for larger data sets. WSI models also use co-occurrence but also attempt to discover distinct word senses while building the vector space. For example, these al-gorithms might represent  X  X arth X  with two vectors based on its meanings  X  X lanet X  and  X  X irt. X  3.2 Data Structures and Utilities The S-Space Package provides efficient imple-mentations for matrices, vectors, and specialized data structures such as multi-maps and tries. Im-plementations are modeled after the java.util li-brary and offer concurrent implementations when multi-threading is required. In addition, the li-braries provide support for converting between multiple matrix formats, enabling interaction with external matrix-based programs. The package also provides support for parsing different corpora for-mats, such as XML or email threads. 3.3 Global Operation Utilities Many algorithms incorporate dimensionality re-duction to smooth their feature data, e.g. (Lan-dauer and Dumais, 1997; Rohde et al., 2009), or to improve efficiency, e.g. (Sahlgren et al., 2008; Jones et al., 2006). The S-Space Pack-age supports two common techniques: the Sin-gular Value Decomposition (SVD) and random-ized projections. All matrix data structures are de-signed to seamlessly integrate with six SVD im-plementations for maximum portability, including sparse SVD library. The package also provides a comprehensive library for randomized projec-tions, which project high-dimensional feature data into a lower dimensional space. The library sup-ports both integer-based projections (Kanerva et al., 2000) and Gaussian-based (Jones et al., 2006).
The package supports common matrix trans-formations that have been applied to word spaces: point wise mutual information (Dekang, 1998), term frequency-inverse document fre-quency (Salton and Buckley, 1988), and log en-tropy (Landauer and Dumais, 1997). 3.4 Measurements The choice of similarity function for the vector space is the least standardized across approaches. Typically the function is empirically chosen based on a performance benchmark and different func-tions have been shown to provide application spe-cific benefits (Weeds et al., 2004). To facili-tate exploration of the similarity function param-eter space, the S-Space Package provides sup-port for multiple similarity functions: cosine sim-ilarity, Euclidean distance, KL divergence, Jac-card Index, Pearson product-moment correlation, Spearman X  X  rank correlation, and Lin Similarity (Dekang, 1998) 3.5 Clustering Clustering serves as a tool for building and refin-ing word spaces. WSI algorithms, e.g. (Puran-dare and Pedersen, 2004), use clustering to dis-cover the different meanings of a word in a cor-pus. The S-Space Package provides bindings for tion, the package provides Java implementations of Hierarchical Agglomerative Clustering, Spec-tral Clustering (Kannan et al., 2004), and the Gap Statistic (Tibshirani et al., 2000). Word space benchmarks assess the semantic con-tent of the space through analyzing the geomet-ric properties of the space itself. Currently used benchmarks assess the semantics by inspecting the representational similarity of word pairs. Two types of benchmarks are commonly used: word choice tests and association tests. The S-Space Package supports six tests, and has an easily ex-tensible model for adding new tests. 4.1 Word Choice Word choice tests provide a target word and a list of options, one of which has the desired relation to the target. Word space models solve these tests by selecting the option whose representation is most similar. Three word choice benchmarks that mea-sure synonymy are supported.

The first test is the widely-reported Test of En-glish as a Foreign Language (TOEFL) synonym test from (Landauer et al., 1998), which consists of 80 multiple-choice questions with four options. The second test comes from the English as a Sec-ond Language (ESL) exam and consists of 50 question with four choices (Turney, 2001). The third consists of 200 questions from the Canadian Reader X  X  Digest Word Power (RDWP) (Jarmasz and Szpakowicz, 2003), which unlike the previ-ous two tests, allows the target and options to be multi-word phrases. 4.2 Word Association Word association tests measure the semantic re-latedness of two words by comparing word space similarity with human judgements. Frequently, these tests measure synonymy; however, other types of word relations such as antonymy ( X  X ot X  and  X  X old X ) or functional relatedness ( X  X octor X  and  X  X ospital X ) are also possible. The S-Space Package supports three association tests.

The first test uses data gathered by Rubenstein and Goodneough (1965). To measure word simi-larity, word similarity scores of 51 human review-ers were gathered a set of 65 noun pairs, scored on a scale of 0 to 4. The ratings are then correlated with word space similarity scores.

Finkelstein et al. (2002) test for relatedness. 353 word pairs were rated by either 13 or 16 subjects on a 0 to 10 scale for how related the words are. This test is notably more challenging for word space models because human ratings are not tied to a specific semantic relation.

The third benchmark considers the antonym as-sociation. Deese (1964) introduced 39 antonym pairs that Greffenstette (1992) used to assess whether a word space modeled the antonymy rela-tionship. We quantify this relationship by measur-ing the similarity rank of each word in an antonym pair, w to w ranges from [0 , 1] , where 1 indicates that the most similar neighbors in the space are antonyms. We report the mean score for all 39 antonyms. The content of a word space is fundamentally dependent upon the corpus used to construct it. Moreover, algorithms which use operations such as the SVD have a limit to the corpora sizes they Figure 2: Processing time across different corpus sizes for a word space with the 100,000 most fre-quent words Figure 3: Run time improvement as a factor of in-creasing the number of threads can process. We therefore highlight the differ-ences in performance using two corpora. TASA is a collection of 44,486 topical essays introduced in (Landauer and Dumais, 1997). The second cor-pus is built from a Nov. 11, 2009 Wikipedia snap-shot, and filtered to contain only articles with more than 1000 words. The resulting corpus consists of 387,082 documents and 917 million tokens.

Table 2 reports the scores of reference algo-rithms on the six benchmarks using cosine simi-larity. The variation in scoring illustrates that dif-ferent algorithms are more effective at capturing certain semantic relations. We note that scores are likely to change for different parameter configura-tions of the same algorithm, e.g. token filtering or changing the number of dimensions.

As a second analysis, we report the efficiency of reference implementations by varying the cor-pus size and number of threads. Figure 2 reports the total amount of time each algorithm needs for processing increasingly larger segments of a web-gathered corpus when using 8 threads. In all cases, only the top 100,000 words were counted as fea-tures. Figure 3 reports run time improvements due to multi-threading on the TASA corpus.

Algorithm efficiency is determined by three fac-tors: contention on global statistics, contention on disk I/O, and memory limitations. Multi-threading benefits increase proportionally to the amount of work done per context. Memory limitations ac-count for the largest efficiency constraint, espe-cially as the corpus size and number of features grow. Several algorithms lack data points for larger corpora and show a sharp increase in run-ning time in Figure 2, reflecting the point at which the models no longer fit into 8GB of memory. We have described a framework for developing and evaluating word space algorithms. Many well known algorithms are already provided as part of the framework as reference implementations for researches in distributional semantics. We have shown that the provided algorithms and libraries scale appropriately. Last, we motivate further re-search by illustrating the significant performance differences of the algorithms on six benchmarks.
Future work will be focused on providing sup-port for syntactic features, including dependency parsing as described by (Pad  X  o and Lapata, 2007), reference implementations of algorithms that use this information, non-linear dimensionality reduc-tion techniques, and more advanced clustering al-gorithms.

