 Aapo Hyv  X arinen aapo.hyvarinen@helsinki.fi Dept of Computer Science and HIIT, University of Helsinki, Finland Shohei Shimizu sshimizu@ar.sanken.osaka-u.ac.jp Patrik O. Hoyer patrik.hoyer@helsinki.fi Dept of Computer Science and HIIT, University of Helsinki, Finland Analysis of causal influences or effects has become an important topic in machine learning (Pearl, 2000; Spirtes et al., 1993), and has numerous applications in, for example, neuroinformatics (Roebroeck et al., 2005; Kim et al., 2007) and bioinformatics (Opgen-Rhein &amp; Strimmer, 2007). For continuous-valued vari-ables, such an analysis can basically be performed in two different ways. First, if the time-resolution of the measurements is higher than the time-scale of causal influences, one can estimate a classic autoregressive model with time-lagged variables and interpret the au-toregressive coefficients as causal effects. Second, if the measurements have a lower time resolution than the causal influences, or if the data has no temporal struc-ture at all, one can use a model in which the causal influences are instantaneous, leading to Bayesian net-works or structural equation models (Bollen, 1989). While estimation of autoregressive methods can be solved by classic regression methods, the case of in-stantaneous effects is much more difficult. Most meth-ance information alone is not sufficient to uniquely characterize the model parameters. Prior knowledge of the structure (fixing some of the connections to zero) of the Bayesian network is then necessary for most prac-tical applications. However, a method was recently proposed which uses the non-Gaussian structure of the data to overcome the identifiability problem (Shimizu et al., 2006): If the disturbance variables (external in-fluences) are non-Gaussian, no prior knowledge on the network structure (other than the ubiquitous assump-tion of a directed acyclic graph (DAG)) is needed to estimate the model.
 Here, we consider the general case where causal influ-ences can occur either instantaneously or with consid-erable time lags. Such a model is called the structural vector autoregressive (SVAR) model in econometric theory, in which numerous attempts have been made for its estimation, see e.g. (Swanson &amp; Granger, 1997; Demiralp &amp; Hoover, 2003; Moneta &amp; Spirtes, 2006). We propose to use non-Gaussianity to estimate the model. We show that this variant of the model is iden-tifiable without any other restrictions than acyclicity. To our knowledge, no model proposed for this problem has been shown to be fully identifiable without prior knowledge of network structure. We further propose a computational method for estimating the model based on the theory of independent component analysis or ICA (Hyv  X arinen et al., 2001).
 The proposed non-Gaussian model not only allows es-timation of both instantaneous and lagged effects; it also shows that taking instantaneous influences into account can change the values of the time-lagged coef-ficients quite drastically. Thus, we see that neglecting instantaneous influences can lead to misleading inter-pretations of causal effects. The framework further leads to a generalization of the well-known Granger causality measure.
 The paper is structured as follows. We first define the model and discuss its relation to other models in Sec-tion 2. In Section 3 we propose an estimation method, show its consistency, and discuss an intuitive interpre-tation of the method. Section 4 contains some theoret-ical examples and a theorem on how including instan-taneous effects in the model changes the resulting in-terpretations. The resulting generalization of Granger causality is discussed in Section 5. The validity of the estimation method is demonstrated by simulations on artificial data in Section 6, and experiments on finan-cial and neuroscientific data in Section 7. Section 8 concludes the paper. 2.1. Definition and Assumptions Let us denote the observed time series by x i ( t ) , i = 1 , . . . , n, t = 1 , . . . , T where i is the index of the vari-ables (time series) and t is the time index. All the vari-ables are collected into a single vector x ( t ). Denote by k the number of time-delays used, i.e. the order of the autoregressive model. Denote by B  X  the n  X  n matrix of the causal effects between the variables x i with time lag  X ,  X  = 0 . . . k .
 The causal dynamics in our model are a combination of autoregressive and structural-equation models. The model is defined as where the e i ( t ) are random processes modelling the external influences or  X  X isturbances X . We make the following assumptions on the external influences e i ( t ). First, they are mutually independent , and temporally uncorrelated , which are typical assumptions in autore-gressive models. Second, they are assumed to be non-Gaussian , which is an important assumption which distinguishes our model from classic models, whether autoregressive models, structural-equation models, or Bayesian networks.
 Further, we assume that the matrix modelling instan-taneous effects, B 0 , corresponds to an acyclic graph, as is typical in causal analysis, but this may not be strictly necessary as will be discussed below. The acyclicity is equivalent to the existence of a permu-tation matrix P , which corresponds to an ordering of the variables x i , such that the matrix PB 0 P T is lower-triangular (i.e. entries above the diagonal are zero). Acyclicity also implies that the entries on the diago-nal are zero, even before such a permutation. 2.2. Relation to Other Models This model is a generalization of the linear non-Gaussian acyclic model (LiNGAM) proposed in (Shimizu et al., 2006). If the order of the autore-gressive part is zero, i.e. k = 0, the model is noth-ing else than the LiNGAM model, modelling instanta-neous effects only. As shown in (Shimizu et al., 2006), the assumption of non-Gaussianity of the e i enables estimation of the model. This is because the non-Gaussian structure of the data provides information not contained in the covariance matrix which is the only source of information in most methods. In this sense the model is similar to independent component analysis, which solves the unidentifiability of factor an-alytic models using the assumption of non-Gaussianity of the factors (Comon, 1994; Hyv  X arinen et al., 2001). In fact, the estimation method in (Shimizu et al., 2006) uses an ICA algorithm as an essential part.
 On the other hand, if the matrix B 0 has all zero en-tries, the model in Equation (1) is a classic vector autoregressive model in which future observations are linearly predicted from preceding ones. If we knew in advance that B 0 is zero, the model could thus be es-timated by classic regression techniques since we do not have the same variables on the left and right-hand sides of Equation (1).
 We emphasize that our model is different from classic autoregressive models two important ways: First, the external influences e i ( t ) are non-Gaussian. Second, the lag variable  X  takes the value 0 as well, which brings instantaneous effects into the model in the form of the matrix B 0 . A coefficient B 0 ( i, j ) models the instanta-neous effect of x j ( t ) on x i ( t ) as in a linear Bayesian network, or a structural equation model.
 2.3. Causality vs. Prediction An autoregressive model can serve two different goals: prediction and analysis of causality. Our goal here is the latter: We estimate the parameter matrices B  X  in order to interpret them as causal effects between the variables. This goal is distinct from simply predicting future outcomes when passively observing the time se-ries, as has been extensively discussed in the literature on causality (Pearl, 2000; Spirtes et al., 1993). Thus, we emphasize that our model is not intended to reduce prediction errors if we want to predict x i ( t ) using (pas-for such prediction, an ordinary autoregressive model is likely to be just as good.
 Our model is intended to be superior in causal mod-elling. Causality has an obvious intuitive interpreta-tion, which is typically formalized as the ability to pre-dict the effect of possible new interventions on the sys-tem (Pearl, 2000). Thus, our model should be better in predicting effects of interventions, which is different from conventional time series prediction. 3.1. Combining Least-Squares Estimation and We propose the following method for estimating our model defined in Section 2.1. The method combines classic least-squares estimation of an autoregressive (AR) model with LiNGAM estimation: 1. Estimate a classic autoregressive model for the 2. Compute the residuals, i.e. estimates of innova-3. Perform the LiNGAM analysis (Shimizu et al., 4. Finally, compute the estimates of the causal effect shown in Section 3.3. First, however, we show the derivation of Equation (5) and discuss its deep mean-ing. 3.2. Why Autoregressive Matrices Change due Equation (5) shows a remarkable fact already men-tioned in the Introduction: Consistent estimates of the B  X  are not obtained by a simple AR model fit even for  X  &gt; 0. Taking instantaneous effects into account changes the estimation procedure for all the autore-gressive matrices, if we want consistent estimators as we usually do. Of course, this is only the case if there are instantaneous effects, i.e. B 0 6 = 0; otherwise, the estimates are not changed.
 Why do we have (5)? This is because from (1) we have and thus x ( t ) = Comparing this with (2), we can equate the autore-gressive matrices, which gives ( I  X  B 0 )  X  1 B  X  = M  X  for  X   X  1, and thus (5) is justified.
 While this phenomenon is, in principle, well-known in econometric literature (Swanson &amp; Granger, 1997; Demiralp &amp; Hoover, 2003; Moneta &amp; Spirtes, 2006), Equation (5) is seldom applied because estimation methods for B 0 have not been well developed. To our knowledge, no estimation method for B 0 has been proposed which is consistent without strong prior as-sumptions on B 0 . 3.3. Consistency and Identifiability The consistency of our method relies on two facts. First, in the estimation of an AR model as in (2), it is not necessary that the innovation vector n ( t ) has independent or even uncorrelated elements (for fixed t ); least-squares estimation will still be consistent, as is well known. Thus, least-squares estimation of (2), combined with (5), gives consistent estimators of B  X  for  X   X  1, provided we have a consistent estimator of B 0 . Second, comparison of (7) with (2) shows that the residuals  X  n ( t ) are, asymptotically, of the form ( I  X  B 0 )  X  1 e ( t ). This means which is the LiNGAM model for  X  n ( t ). This shows that B 0 is obtained as the LiNGAM analysis of the residuals, and the consistency of our estimator of B 0 follows from the consistency of LiNGAM estimation (Shimizu et al., 2006). Thus, our method is consistent for all the B  X  . This obviously proves, by construction, the identifiability of the model as well.
 We have here assumed that B 0 is acyclic, as is typical in causal analysis. However, this assumption is only made because we do not know very well how to esti-mate a linear non-Gaussian Bayesian network in the cyclic case. Future work may produce methods which estimate cyclic models, and then we do not need the assumption of acyclicity in our combined model either. We could just use such a new method in Step 3 of the method instead of LiNGAM, and nothing else would be changed. Recent work in that direction is in (Lac-erda et al., 2008); see also (Richardson &amp; Spirtes, 1999) for older methods on Gaussian data. 3.4. Interpretation as ICA of Residuals Another viewpoint on our model is analysis of the cor-relations of the innovations after estimating a classic AR model. Suppose we just estimate an AR model as in (2), and interpret the coefficients as causal effects. Such an interpretation more or less presupposes that the innovations n i are independent of each other, be-cause otherwise there is some structure in the model which has not been modelled by the AR model. If the innovations are not independent, the causal interpre-tation may not be justified. Thus, it seems necessary to further analyze the dependencies in the innovations in cases where they are strongly dependent.
 Analysis of the dependency structure in the residu-als (which are, by definition, estimates of innovations) is precisely what leads to the present model. As a first approach, one could consider application of some-thing like principal component analysis or independent component analysis on the residuals. The problem with such an approach is that the interpretation of the obtained results in the framework of causal anal-ysis would be quite difficult. Our solution is to fit a causal model like LiNGAM to the residuals, which leads to a straightforward causal interpretation of the analysis of residuals which is logically consistent with the AR model. Here we present some theoretical examples of how the instantaneous and lagged effects interact based on the formula in (5).
 An instantaneous effect may seem to be lagged Consider first the case where the instantaneous and lagged matrices are as follows: That is, there is an instantaneous effect x 2  X  x 1 , and no lagged effects (other than the purely autoregres-sive x i ( t  X  1)  X  x i ( t )). Now, if an AR(1) model is estimated for data coming from this model, without taking the instantaneous effects into account, we get the autoregressive matrix Thus, the effect x 2  X  x 1 seems to be lagged although it is, actually, instantaneous.
 Spurious effects appear Consider three variables with the instantaneous effects x 1  X  x 2 and x 2  X  x 3 , and no lagged effects other than x i ( t  X  1)  X  x i ( t ), as given by If we estimate an AR(1) model for the data coming from this model, we obtain This means that the estimation of the simple autore-gressive model leads to the inference of a direct lagged effect x 1  X  x 3 , although no such direct effect exists in the model generating the data, for any time lag. Causal ordering is not changed A more reassur-ing result is the following: if the data follows the same causal ordering for all time lags, that ordering is not contradicted by the neglect of instantaneous effect. A rigorous definition of this property is the following. Theorem 1 Assume that there is an ordering i ( j ) , j = 1 . . . n of the variables such that no effect goes Then, the same property applies to the M  X  ,  X   X  1 as well. Conversely, if there is an ordering such that (13) applies to M  X  ,  X   X  1 and B 0 , then it applies to B  X  ,  X   X  1 as well.
 The proof of the theorem is based on the fact that when the variables are ordered in this way (as-suming such an order exists), all the matrices B  X  are lower-triangular. The same applies to I  X  B 0 . Now, the product of two lower-triangular matrices is lower-triangular; in particular the M  X  are also lower-triangular according to (5), which proves the first part of the theorem. The converse part follows from solv-ing for B  X  in (5) and the fact that the inverse of a lower-triangular matrix is lower-triangular.
 What this theorem means is that if the variables really follow a single  X  X ausal ordering X  for all time lags, that ordering is preserved even if instantaneous effects are neglected and a classic AR model is estimated for the data. Thus, there is some limit to how (5) can change the causal interpretation of the results. The classic interpretation of causality in instantaneous Bayesian network models would be that x i causes x j if the ( j, i )-th coefficient in B 0 is non-zero. In the time series context, this is related to Granger causal-ity (Granger, 1969), which formalizes causality as the ability to reduce prediction error. A simple opera-tional definition of Granger causality can be based on the autoregressive coefficients M  X  : If at least one of the coefficients from x i ( t  X   X  ) ,  X   X  1 to x j ( t ) is (sig-nificantly) non-zero, then x i Granger-causes x j . This is because then the variable x i reduces the prediction error in x j in the mean-square sense if it is included in the set of predictors, which is the very definition of Granger causality.
 In light of the results in this paper, we propose a definition which combines the two aspects: A vari-able x i causes x j if at least one of the coefficients B  X  ( j, i ), giving the effect from x i ( t  X   X  ) to x j ( t ), is (significantly) non-zero for  X   X  0. The condition for  X  is different from Granger causality since the value  X  = 0, corresponding to instantaneous effects, is in-cluded. Moreover, since estimation of the instanta-neous effects changes the estimates of the lagged ones, the lagged effects used in our definition are different from those usually used with Granger causality. A more general formulation of this definition, which is in line with the general formulation of Granger causal-ity, is that the error in the  X  X rediction X  of x j ( t ) is cluded in the set of predictors. Here, we use a rather unconventional definition of the word  X  X rediction X  be-cause we include instantaneous effects. To verify the validity of our method, we first performed experiments with artificial data. In the experiments, we created data in the following manner using the 1. We randomly constructed a strictly lower-2. Next, we generated data with various lengths of 3. We randomly permuted the order of the innova-4. We randomly generated a first-order autoregres-5. The values of the observed signals x i ( t ) were gen-6. Finally, we fed the data to our estimation method. Figure 1 gives the scatterplots of the elements of the estimated parameters versus the generating ones. The left column is for the scatterplots of the estimated causal effects in B 0 versus the generating values. The center column is for the scatterplots of the estimated causal effects in B 1 versus the generating values. The right column is for the scatterplots of the estimated autoregressive coefficients in M 1 versus the generat-ing values of the causal effects in B 1 (here, the esti-mation was invalid because instantaneous effects were ignored).
 For the scatterplots in the left and center columns, the estimation worked well when the sample size grew, as evidenced by the grouping of the data points onto the main diagonal, although for the small sample size 300 the estimation was often inaccurate. On the other hand, the scatterplots in the right column confirmed that the causal effects were not correctly estimated by the ordinary autoregressive coefficients when instanta-neous influences existed since the data points were not very close to the main diagonal. 7.1. Financial Data As a first illustration of the applicability of the method on real data, we analyzed a dataset from a time se-of two observed signals, x 1 : weekly closing price of Toyota stock and x 2 : weekly closing rate of exchange of Japanese Yen to U.S. Dollar in 2007. The number of time points was 50. The maximum, minimum and mean of x 1 were 8,230, 5,870 and 7,102 (JPY). Those of x 2 were 123.86, 108.51 and 117.72 (JPY).
 We analyzed the data using our method with autore-gressive order of 1. The estimated first-order autore-gressive matrix M 1 and residual correlation matrix were as follows: The relatively strong correlation between the residu-als implied that there would be some dependency that had not been modeled by the AR model. Thus, we fit-ted the instantaneous causal model to the residuals, as proposed above. The estimated instantaneous causal effect matrix B 0 and resulting lagged causal effect ma-trix B 1 were as follows: The matrix B 0 is very close to be upper-triangular, which implied that the model was really acyclic (be-cause switching the order of the variables would make B 0 lower-triangular). Further, the instantaneous ef-fect x 2  X  x 1 in B 0 was one order of magnitude larger than the lagged effect in M 1 and thus the lagged co-efficients in M 1 are quite different from those in B 1 , due to the formula in (5).
 Figure 2 shows a graphical representation of the es-timated model for financial data. First, it implies that a higher value of the yen ( x 2 ) had a negative lagged effect (-48.01) on the price of Toyota stock ( x 1 This would be reasonable since Toyota sells many cars abroad, and a higher value of the yen would increase the cost price and decrease the earning. Interestingly, it was also implied that a higher value of the yen had a positive instantaneous effect (56 . 04) on the price of Toyota stock. In other words, for weeks where values of the yen one week before were the (approximately) same, if the yen got more expensive (due to some rea-son other than the value of the yen one week before, perhaps a U.S. recession, for example) then the price of Toyota stock would get more expensive. It would be interesting to further study the economic mechanism with more extensive data.
 7.2. Magnetoencephalographic Data As a second illustration of the applicability of the method on real data, we applied it on magnetoen-cephalography (MEG), i.e. measurements of the elec-tric activity in the brain. The raw data consisted of the 306 MEG channels measured by the Vectorview helmet-shaped neuromagnetometer (Neuromag Ltd., Helsinki, Finland) in a magnetically shielded room at the Brain Research Unit, Low Temperature Labora-tory, Helsinki University of Technology. The sampling frequency was 600 Hz. The measurements consisted of 300 seconds of resting state brain activity from the experiment of (Ramkumar et al., 2007). The subject was sitting with eyes closed, and did not perform any specific task nor was there any specific sensory stim-ulation. The channels were first linearly projected to the signal space to reduce noise (Uusitalo &amp; Ilmoniemi, 1997). In this illustrative experiment, we only consider a single (gradiometer) channel in the right occipital cortex near the midline.
 We considered the interaction of about 10 Hz (alpha) and about 20 Hz (beta) oscillations commonly ob-served in electromagnetic recordings of spontaneous brain activity. We first computed the amplitudes of the oscillations by dividing the data into windows of length of 0.25 seconds, performing fast Fourier trans-form inside each of them, and computing the total Fourier amplitudes (unweighted Euclidean norm of the Fourier coefficients) in the frequency ranges of 8 . . . 12Hz (alpha range, denoted by x 1 ) and 15 . . . 25Hz (beta range, denoted by x 2 ). Thus we obtained two time series of 1,200 points.
 We fitted our model, with autoregressive order of 1 to the data. The obtained matrices are What we see is that the instantaneous model is far from trivial: the effects in B 0 are relatively strong. This is also reflected in B 1 which is now rather differ-ent from M 1 . Thus, the interpretation of the autore-gressive matrices using just the autoregressive model (i.e. M 1 ) or the combined model (i.e. B 1 ) are quite different. In the classic autoregressive case (based on M 1 ), the lagged effect x 1  X  x 2 is relatively strongly positive whereas in the combined model it is quite weak. In fact, that effect is now modelled as an in-stantaneous effect in B 0 . Even more interesting is that the instantaneous model has a strong negative effect x 2  X  x 1 which is not visible at all in the purely au-toregressive matrix M 1 . Thus, the results illustrate how the interpretation of causal effects (and even of the lagged ones) can change drastically when includ-ing the instantaneous effects.
 Using an autoregressive order of 2 did not change the results. We also ran the method many times to exclude the problem of the ICA estimation algorithm (used in LiNGAM estimation) getting stuck in local minima (Himberg et al., 2004), and the result was found to be robust with respect to that manipulation.
 One problem with this experiment is that the causal model estimated by LiNGAM is far from acyclic. Here, we can justify the procedure by using the the-ory of cyclic model estimation proposed by (Lacerda et al., 2008); the estimation here gives the only X  X table X  model according to that theory. Performance of LiNGAM estimation methods in the case of cyclic models, and the possible need for new methods for esti-mating cyclic models are future research topics of great practical importance. However, as discussed above, they are separate from the main contribution of our pa-per in the sense that we can use any such new method to estimate the instantaneous model in our framework. We showed how non-Gaussianity enables estimation of a causal discovery model in which the linear effects can be either instantaneous or time-lagged. Like in the purely instantaneous case (Shimizu et al., 2006), non-Gaussianity makes the model identifiable with-out explicit prior assumptions on existence or non-existence of given causal effects. The classic assump-tion of acyclicity is sufficient although probably not necessary. From the practical viewpoint, an impor-tant implication is that considering instantaneous ef-fects changes the coefficient of the time-lagged effects as well.
 Bollen, K. A. (1989). Structural equations with latent variables . John Wiley &amp; Sons.
 Comon, P. (1994). Independent component analysis X  a new concept? Signal Processing , 36 , 287 X 314. Demiralp, S., &amp; Hoover, K. D. (2003). Searching for the causal structure of a vector autoregression. Ox-ford Bulletin of Economics and Statistics , 65 (sup-plement) , 745 X 767.
 Granger, C. W. J. (1969). Investigating causal re-lations by econometric models and cross-spectral methods. Econometrica , 37 , 424 X 438.
 Himberg, J., Hyv  X arinen, A., &amp; Esposito, F. (2004).
Validating the independent components of neu-roimaging time-series via clustering and visualiza-tion. NeuroImage , 22 , 1214 X 1222.
 Hyv  X arinen, A., Karhunen, J., &amp; Oja, E. (2001). Inde-pendent component analysis . Wiley Interscience. Kim, J., Zhu, W., Chang, L., Bentler, P. M., &amp; Ernst,
T. (2007). Unified structural equation modeling ap-proach for the analysis of multisubject, multivariate functional MRI data. Human Brain Mapping , 28 , 85 X 93.
 Lacerda, G., Spirtes, P., Ramsey, J., &amp; Hoyer, P. O. (2008). Discovering cyclic causal models by inde-pendent components analysis. Proc. 24th Conf. on Uncertainty in Artificial Intelligence (UAI2008) . Helsinki, Finland.
 Moneta, A., &amp; Spirtes, P. (2006). Graphical models for the identication of causal structures in multivariate time series models. Proc. Joint Conference on In-formation Sciences . Kaohsiung, Taiwan.
 Opgen-Rhein, R., &amp; Strimmer, K. (2007). From cor-relation to causation networks: a simple approxi-mate learning algorithm and its application to high-dimensional plant gene expression data. BMC Sys-tems Biology , 1 .
 Pearl, J. (2000). Causality: Models, reasoning, and inference . Cambridge University Press.
 Ramkumar, P., Parkkonen, L. T., He, B. J., Raichle,
M. E., H  X am  X al  X ainen, M. S., &amp; Hari, R. (2007). Iden-tification of stimulus-related and intrinsic networks by spatial independent component analysis of MEG signals. Abstract presented at the Society for Neu-roscience Meeting, San Diego, California.
 Richardson, T. S., &amp; Spirtes, P. (1999). Automated discovery of linear feedback models. In C. Glymour and G. Cooper (Eds.), Computation, causation and discovery , 253 X 302. The MIT Press.
 Roebroeck, A., Formisano, E., &amp; Goebel, R. (2005).
Mapping directed influence over the brain using granger causality and fMRI. NeuroImage , 25 , 230 X  242.
 Shimizu, S., Hoyer, P. O., Hyv  X arinen, A., &amp; Kerminen,
A. (2006). A linear non-Gaussian acyclic model for causal discovery. J. of Machine Learning Research , 7 , 2003 X 2030.
 Spirtes, P., Glymour, C., &amp; Scheines, R. (1993). Cau-sation, prediction, and search . Springer-Verlag. Swanson, N. R., &amp; Granger, C. W. J. (1997). Impulse response functions based on a causal approach to residual orthogonalization in vector autoregression.
J. of the Americal Statistical Association , 92 , 357 X  367.
 Uusitalo, M. A., &amp; Ilmoniemi, R. J. (1997). Signal-space projection method. Med. Biol. Eng. , 32 , 35 X 
