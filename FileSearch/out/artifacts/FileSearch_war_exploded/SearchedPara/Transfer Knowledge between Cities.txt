 The rapid urbanization has motivated extensive research on urban computing. It is critical for urban computing tasks to unlock the power of the diversity of data modalities generated by di sources in urban spaces, such as vehicles and humans. However, we are more likely to encounter the label scarcity problem and the data insu ffi ciency problem when solving an urban computing task in a city where services and infrastructures are not ready or just built. In this paper, we propose a FLexible multimOdal tRAnsfer Learning (FLORAL) method to transfer knowledge from a city where there exist su ffi cient multimodal data and labels to similar kind of cities to fully alleviate the problems of label scarcity and data insu cy. FLORAL learns semantically related dictionaries for multiple modalities from a source domain and simultaneously transfers the dictionaries and labelled instances from the source into a target do-main. We evaluate the proposed method with a real-world study of air quality prediction.
  X 
Information systems  X  Data mining; Geographic information systems;  X  Computing methodologies  X  Transfer Learning ; Urban Computing; Multi-modality; Transfer Learning.
The rapid progress of urbanization has modernized people X  X  lives, but also engendered many challenges in cites, such as tra gestion and air pollution. Recently, the proliferation of big data in cities has fostered unprecedented opportunities to tackle these ur-ban challenges by data science and computing technology, a.k.a., urban computing [35]. Given the complex setting of a city, we usu-ally need to harness the diversity of data (i.e., multi-modality) to solve an urban computing problem. For example, to predict and tackle air pollution, we need to check air quality data from moni-toring stations, pollution emission from factories and vehicles, land
The paper was done when the first author was an intern in Mi-crosoft Research under the supervision of the second author. uses and meteorological data of di ff erent locations [36, 38]. To di-agnose a city X  X  noise situation, we need to consider human mobility, tra ffi c conditions and layout of a neighborhood [37]. Thus, to un-lock the power of knowledge from multiple disparate datasets (i.e., multi-modalities) is a key research problem in urban computing.
The problem becomes more challenging when we conduct ur-ban computing in a  X  X ew X  city where infrastructures and services are not ready or just built, thus the data required by a task are in-su ffi cient. For example, when we conduct air quality prediction in Baoding, we face the following two challenges as shown in Fig-ure 1. 1) The label scarcity problem : the ground truth labels, i.e., air quality data, are very scarce because there exist only a few air quality monitoring stations in Baoding. 2) The data insu problem : there are two types of insu ffi ciency. One refers to struc-tured modality missing . The taxi trajectory data (D4), character-izing the pollution emission from vehicles, are existing in Beijing but missing in Baoding. The other is within-modality insu The meteorology data (D3) in Baoding are not that su ffi cient as in Beijing due to limited weather stations. Figure 1: An example of transferring knowledge from Beijing to
An interesting question arises: can we transfer knowledge from a city where data are su ffi cient, to a city which faces either the label scarcity or the data insu ffi ciency problem? As demonstrated in Fig-ure 1, based on Beijing X  X  data, we can learn the knowledge about underlying connections between di ff erent modalities; e.g., air pol-lution might be related to tra ffi c congestion which would be caused by a dense road network structure. With such knowledge trans-ferred from Beijing, we may be able to infer Baoding X  X  air pollution based on road network structures even if there exists no tra like taxi trajectories. In this example, Beijing is a source domain where knowledge comes from, and Baoding is a target domain that we transfer knowledge to.

To transfer knowledge between di ff erent cities (referred to as do-mains in the rest of this paper) is a challenging task, as data from di ff erent cities may have di ff erent distributions in feature and label spaces. Using the air quality inference as an example, as shown in Figure 2(a), the distributions of humidity (i.e., a kind of feature) in four cities are very di ff erent. The distributions of the four cities X  air quality (i.e., labels) are also di ff erent. Though transfer learning [15] has been proposed to tackle this challenge, none of existing work can solve our problem given the following three unique challenges.
First, we transfer knowledge between source and target domains with multi-modality data rather than single-modality data. Multi-modality data have incommensurable representations. For exam-ple, the Point-Of-Interests (D2) in Figure 1 is characterized as Bool ean values indicating categories of a venue, while the meteorology (D3) is featured as real values. Simply concatenating features ex-tracted from datasets of di ff erent modalities into a single modality compromises the performance of a transfer learning model [18, 36]. Thus, most transfer learning models [15, 30] designed for a single-modality dataset are not applicable to our problem.

Second, though a few multi-view transfer learning algorithms [5, 19, 28, 27, 32] support multi-modality data, none of them can tack-le the data insu ffi ciency problem mentioned in Figure 1. Because of within-modality insu ffi ciency, di ff erent instances may have dif-ferent modalities in a target domain. Thus, the instances cannot be treated equally. When facing the structured modality missing, we need to complement a missing modality with its knowledge repre-sentation from a source domain.

Third, data of di ff erent modalities should have di ff erent weights when transferring between di ff erent source and target domains. For example, when transferring knowledge from Beijing to Shanghai for air quality prediction, road networks may play a more impor-tant role than other modalities (like weather) as the two cities have a very similar structure of road networks (but di ff erent weather con-ditions). When transferring between Beijing and Tianjin (which are geographically close), however, weather conditions of the two c-ities are more similar than other modalities, thereby playing a more important role in the transfer. Existing transfer learning methods cannot well learn the weights for data of di ff erent modalities.
To tackle the three challenges, we propose a FLexible multi-mOdal tRAnsfer Learning (FLORAL) method with the following three contributions:  X 
It enforces multi-modalities to share knowledge and representa-tion structures by learning semantically related dictionaries -each modality has a dictionary which consists of atoms encoding latent semantic meanings; di ff erent modalities have di ff erent dictionaries but all modalities X  dictionaries share the size and latent semantic space; e.g., the third atoms of all modalities X  dictionaries semanti-cally mean  X  X ood air quality X .  X 
It settles the data insu ffi ciency problem, by transferring seman-tically related dictionaries learnt from a source to enrich feature representations of a target domain. Moreover, an algorithm called Multimodal Transfer AdaBoost (MTAB), capable of learning and di ff erentiating di ff erent modalities X  weights, is proposed to lever-age labelled source instances to alleviate the label scarcity problem.  X 
We evaluate our method on air quality prediction in three cities, with performances outperforming six baselines.
In this section, we briefly review the related work in two cat-egories: some representative research on multimodal data fusion, and state-of-the-art transfer learning methods.
There have been many attempts made towards fusing multimodal data. Some of them perform model-level fusion, i.e., generating a model for each data modality and unifying these models X  outputs as the final result. Co-training [36] and multi-kernel learning [34, 38] belong to this category. The other line of research fuses di data modalities in feature level. The most naive way is to directly concatenate features from di ff erent modalities [24]. However, the performance of this method is usually inferior because it introduces overfitting and ignores non-linear interactions between modalities according to [18]. The majority of feature level fusion devote to extract a semantic latent subspace or build a translator to align d-i ff erent modalities. The techniques capable of aligning embrace translation [22], canonical correlation analysis [6], matrix factor-ization [16], manifold alignment [37], coupled dictionary learn-ing [31], and multimodal deep learning [18]. Either model-level or feature-level multimodal data fusion methods require su data in each modality, as well as abundant correspondence between instances across modalities. To solve urban computing tasks in a city facing the data insu ffi ciency problem, which our work focuses on, these methods become powerless and even infeasible (imagin-ing that a modality is missing).
Transfer learning [15] leverages knowledge from a source do-main to facilitate learning in a target domain. Almost all work in this field have been motivated by the scarcity of labelled data in a target domain. Until recently, Yang et. al [30] initiated the setting called heterogeneous transfer learning which enriches the modality in a target domain with the other modality from a source by pro-viding complementary views. This work and its follow-up [16], however, can only handle the case where both source and target domains contain single modality only.

Two strands of research, i.e., multi-task multi-view learning and multi-view transfer learning, enable knowledge transfer between domains with multimodal data. Nevertheless, we first emphasize the di ff erence between multi-task learning and transfer learning: multi-task learning assumes su ffi cient annotated data in each task and treats all tasks equally; while transfer learning cares only the target domain with scarce labelled data. Besides, most multi-task multi-view learning algorithms transfer model parameters, thus ig-nore the di ff erences between tasks [33] or rely on enough labelled data in all tasks to learn the di ff erences [12, 17, 25]. Though some work [7, 9, 29] transfer knowledge in feature-level, IteM 2 [7] can only tackle non-negative feature values, and MAMUDA [9] and HiMLS [29] cannot fully handle the data insu ffi ciency problem, e-specially the within-modality insu ffi ciency.

To the best of our knowledge, there are only a few attempts on multi-view transfer learning. Zhang et. al [32] first proposed the MVTL-LM algorithm that transfers both model parameters and in-stances between domains with multi-views. The Multi-transfer [19] and DISMUTE [5] extend it to multiple source domains and multi-class classification, respectively. Xu et. al [23] proposed an algo-rithm to transfer multi-view instances based on boosting, while it cannot handle structured modality missing. Blitzer et. al [2] point-ed out the limitations of parameter and instance transfer in dealing with a target domain whose distribution distinctly di source X  X . The IMAM [28] and MDT [27] alleviate the limitation-s by performing feature-level knowledge transfer. Unfortunately, none of these work tackles the within-modality insu ffi ciency, and di ff erentiates di ff erent modalities X  weights when transferring.
In this section, we present our method in detail. We first intro-duce the general framework in Figure 3, which involves two ma-jor pipelines, i.e., learning semantically related dictionaries from a source domain (represented by broken blue arrows), and trans-ferring dictionaries and instances from a source to a target domain (shown in red solid arrows). After we introduce the notations and problem definitions, we detail how to learn semantically related dictionaries, and transfer the dictionaries and instances. The com-plexity analysis is given at the end of this section.

Learn semantically related dictionaries : To learn commensu-rable representations for multi-modalities, we first learn semanti-cally related dictionaries from a source domain through a dictionary learning approach. In this approach, we build a graph that connect-s instances across di ff erent modalities and those in each modality. We then cluster the graph into K clusters, while ensuring that each cluster encodes a latent semantic meaning and contains instances from all modalities. Subsequently, for each modality, we build a dictionary by taking the K cluster centres of the modality as atoms. Obviously, di ff erent modalities X  dictionaries have the same size K , and share the K -dimensional latent semantic space.

Transfer dictionaries and instances : To address the data insu ciency problem in a target domain, we transfer the semantically re-lated dictionaries learnt from a source. For each modality in a target domain, we extract original features, and learn enriched represen-tations over this modality X  X  dictionary by sparse coding. Enriched representations make an instance more informative, thus alleviate within-modality insu ffi ciency. As the M dictionaries may influence each other by sharing semantic meanings, the knowledge of those missing modalities (e.g., the second modality illustrated here) are preserved in the dictionaries and enriched representations of exist-ing modalities. Therefore structured modality missing is addressed.
Transferring the dictionaries is not enough to address the label scarcity problem in a target domain. We also transfer labelled in-stances from a source. Before transferring, we meet the follow-ing two prerequisites: 1) learn enriched representations of labelled source instances by sparse coding, in order to make representations of source and target instances consistent; 2) perform max pooling for each target instance to aggregate enriched representations of all existing modalities, so that target instances can be treated equal-ly regardless of within-modality insu ffi ciency. Once these prereq-uisites are satisfied, we apply the Multimodal Transfer AdaBoost algorithm to transfer labelled source instances. The output of the algorithm is a classifier that can predict any target instances.
We follow the basic notations in [8, 31]: suppose that in the tar-get domain we are provided a very few labelled instances T { ,  X  X  X  , t m instances T u = { t 1 ui ,  X  X  X  , t m ui ,  X  X  X  , t M ui } N note the feature vector of the m th modality of the i th labelled and unlabelled instance, respectively. N t l and N t u indicate the num-ber of labelled and unlabelled instances, respectively. Meanwhile, there exists a source domain in which su ffi cient labelled instances S = { s 1 instances S u = { s 1 uj ,  X  X  X  , s m uj ,  X  X  X  , s M uj } N Note that M is the total number of modalities in the source do-some 1  X  m  X  M of some 1  X  i  X  N t l + N t u as a result of dictionaries D 1 ,  X  X  X  , D m ,  X  X  X  , D M for all M modalities from S S , where D m  X  R p m  X  K . Subsequently, we transfer these dictio-naries to the target domain, and obtain enriched representation-s  X   X   X  R K . We obtain  X  T u and  X  S l in the same fashion.  X  T MP is the max pooling result of  X  T l , by aggregating all existing modal-ities {  X  t m li } M m = 1 (for some 1  X  m  X  M ,  X  t m li for any i th instance. The same applies to  X  T MP u . Finally, we learn a classifier h f (  X  T MP u ) by an algorithm known as Multimodal Transfer AdaBoost to transfer labelled source instances, i.e.,  X  S summarize our notations in Table 1.

Sparse coding [13], a technique widely used in machine learning, represents data vectors as sparse linear combinations of basis ele-ments. The set of basis elements is called dictionary. Sparse coding provides an e ff ective way to homogenize representation structures of multi-modalities, by enforcing all modalities X  dictionaries se-mantically related and learning linear combination coe ffi the corresponding dictionary for each modality as new representa-tions. In [20], the authors summarized three main categories of techniques to learn dictionaries: probabilistic learning, reconstruc-tion error minimization, and clustering. Here we prefer clustering because of its advantage in extracting semantically related dictio-naries. However, directly clustering the data in multi-modalities in incommensurable representation structures is impossible. Inspired by [8, 10, 31], we propose a graph clustering algorithm as shown in Figure 4, in which we build a weighted graph to model pairwise similarities between vertices across di ff erent modalities and within each modality. Though the works [8, 31] also learn dictionaries by graph clustering, they either learn a dictionary for single modality, or ignore unlabeled vertices and modality diversity in each clus-ter for multi-modalities. Next, we detail the graph construction, the graph clustering with highly e ffi cient submodular optimization, and the dictionaries inference. Figure 4: The procedures of dictionary learning. Di ff erent shapes rep-We first build an undirected graph G = ( V , E ). The vertex set V consists of all modalities of all instances in the source domain, i.e., V = S l  X  S u . We denote | V | , | V m | , | V l | of all vertices, vertices in the m th modality, labelled vertices and unlabelled vertices, respectively. Following [31], the edge set E models pairwise relations between vertices within each modality, i.e., intra-edges, and across di ff erent modalities, i.e., inter-edges.
For a pair of vertices s m i and s m j in the m th modality, we measure their similarity with the Euclidean distance between their feature vectors according to spectral clustering [21]. The i th and j th ver-tices are connected with an intra-edge if each of them is among the top k similar vertices of the other vertex. This way of constructing intra-edges, i.e., mutual k -NN, has been proved to outperform tra-ditional k -NN in semi-supervised clustering [14]. To weight each intra-edge, spectral clustering [21] applies Gaussian kernels to the similarity between two end vertices of the edge: The more similar s m i and s m j are, the larger the weight of the intra-edge connecting them is.

As for a pair of vertices s m i and s n j in the m th and n th modality, respectively, according to the works in [22, 31], we connect them with an inter-edge whose weight equals to 1, i.e., w m , n and j th instances are correlated. In air quality prediction, a region (denoted by an eclipse in Figure 4) is an instance. Therefore the i th and j th instances are correlated if the two corresponding regions are geographical neighbours.
A natural idea of graph clustering is to partition sparsely con-nected dense subgraphs from each other based on the notion of intra-cluster density versus inter-cluster sparsity. Given a graph G ( V , E ), we select A  X  E , so that the resulting graph G ( V tains exactly K connected components. Clearly, this is a discrete optimization problem. Submodularity [11] , oftentimes viewed as a discrete analog of convexity, is the key to e ff ciently solve discrete optimization problems in machine learning. Thus, we design the objective function to satisfy the "submodulari-ty" condition. Before proceeding to the objective function, we first introduce the definitions of submodularity and monotonicity that are originally defined in [11].

Definition 1. (Submodularity [11]) Let E be a finite set. A set function F :2 E  X  R is submodular if F ( A  X  X  a 1 } )  X  F ( A ) { a , a 2 } )  X  F ( A  X  X  a 2 } ), for all A  X  E and a 1 , a 2 property, also named diminishing marginal gains, states that the impact of adding an element to a larger set is less.

Definition 2. (Monotonically Increasing [11]) A set function F is monotonically increasing if F ( I 1 )  X  F ( I 2 ) for any I
In order to introduce the criteria met by our objective function, we compare a pair of graph clustering results ( C 1 , C 2) for each cri-terion in Figure 5. C 2 more closely complies with each criterion by enforcing O ( C 1) &lt; O ( C 2). We have determined the follow-ing four criteria. 1) The compactness originally proposed in [10] guards the basic idea of graph clustering, i.e., intra-cluster density. Maximizing the objective ensures that densely rather than sparsely connected vertices constitute a cluster. 2) The homogeneity origi-nally proposed in [8] requires each cluster to be homogeneous for labelled vertices, i.e., a cluster should not mix vertices belonging to di ff erent categories. 3) The label balance originally proposed in [10] states that the number of labelled vertices in each cluster stays  X  X alanced X . This constraint avoids to produce clusters with-out category labels, and thereby supports the homogeneity. 4) The modality diversity ensures that each cluster contains vertices from all modalities. The compactness equips the dictionaries with rep-resentation e ff ectiveness. The homogeneity and label balance en-force each dictionary atom, i.e., each cluster center, to encode a latent semantic meaning and be discriminative. We first consider the modality diversity, which is crucial to couple all modalities X  dictionaries to be semantically related. Figure 5: Illustrations of the four criteria met by our objective func-
Compactness: A random walk, starting at a vertex and then ran-domly travelling to a connected vertex, is more likely to stay with-in a cluster than travelling between. Therefore conducting random walks on the graph can discover clusters where the flow tends to gather. The transition probability from a vertex v i to a vertex v defined as a set function P ij ( A ):2 E  X  R for the graph G ( V defined in [10]: which encourages random walks within clusters ( e ij  X  A ) and e-liminates those between clusters ( e ij A ). w i = j : e ij total weights incident to v i . Self loop transition ( i maintain the total transition probability out of v i to be 1.
We follow [10] to define the compactness objective as the en-tropy rate of a random walk [3] measuring uncertainty of a walk: where  X  i is the i th element of the stationary distribution dom walks on dense subgraphs are more uncertain than on sparse subgraphs. Hence maximizing the entropy rate ensures the com-pactness, and enforces that the edges selected into A from E can make each cluster as dense as possible. C ( A ) has been proved to be monotonically increasing and submodular in [10].

Homogeneity: Suppose that for the graph G ( V , A ) given by cur-rent A ,wehave N A connected components, i.e., G 1 ,  X  X  X  , the k th connected component G k , we denote the number of la-belled vertices as | V lk | , and the number of labelled vertices carry-ing the label y as | V l ( y ) k | . G k  X  X  homogeneity is defined as H ( G | max y | V l ( y ) k | in [8], which computes the percentage of those ver-tices carrying the mostly assigned label in G k . The homogeneity for the whole graph G ( V , A ) w . r . t . A is straightforward by averag-ing over all N A connected components as defined in [8]: H ( A ) = Maximizing Equation (4) encourages homogeneity (the first term), but avoids a trivial solution where each cluster contains a single vertex by restricting N A to be as small as possible (the second term). The monotonicity and submodularity of H ( A ) are proved in [8].
Label balance: In [10], the authors defined the balancing func-tion as the entropy of the cluster size distribution, i.e., enforces clusters to have similar sizes. Motivated by the balancing function in [10], we define the objective for label balance as the entropy of the labelled vertices X  distribution:
L ( A ) =  X  so that maximizing L ( A ) enforces labelled vertices to scatter uni-formly across N A clusters. In [10], the authors prove that log p A ( k )  X  N A satisfies monotonicity and submodularity.
Modality diversity: As shown in Figure 4, each cluster is ex-pected to contain vertices from di ff erent modalities. To enable all modalities X  dictionaries semantically related, the diversity of modalities in each cluster should be maximized, i.e., containing vertices from all M modalities. We achieve this by distributing vertices from each modality to all clusters uniformly. Therefore, the objective for each modality is the entropy of the distribution of vertices from that modality across clusters. The overall objective averages all modalities X  entropies: Maximizing M ( A ) encourages each cluster to be diverse, i.e, in-cluding vertices from all M modalities. Previous work [31] ignores the diversity of modalities in each cluster. As mentioned above, the monotonicity and submodularity of  X  k p A ( k ) log p have been proved in [10]. Meanwhile, we are provided with the fact that a linear combination with nonnegative coe serves monotonicity and submodularity [11]. M ( A ), therefore, is also guaranteed to be monotonically increasing and submodular.
Combining the four objective functions introduced, the overall optimization problem for learning can be written as: where  X  ,  X  , and  X  are three trade-o ff parameters to balance the im-portance of the four terms. O ( A ), a linear combination of H ( A ), L ( A ), and M ( A ), is monotonically increasing and submod-ular [11]. Fisher et al. [11] proved that although solving the op-timization problem in Equation (7) is NP-hard, the submodulari-ty of O ( A ) contributes a greedy approximation algorithm with ef-fectiveness and e ffi ciency guarantee. It initiates A =  X  atively selects the edge e  X  E \ A to maximize the marginal gain O ( A  X  e )  X  X  ( A ). In [11], the authors also showed that the algorithm givesa1 / 2-approximation bound on the optimality of the solution. Besides, the algorithm is highly e ffi cient thanks to the diminishing marginal gains property of submodular functions according to [10]. In each iteration it computes the marginal gain for only the edge who holds the second largest gain in the previous iteration, instead of all edges in the set E \ A . The implementation details and time complexity will be discussed in Section 3.5.
In the k th cluster, we calculate the center of vertices from the m th modality as the dictionary atom d m k according to [8, 31]. The final dictionary of the m th modality D m combines K atoms inferred the Algorithm 1 in [31] to Algorithm 1 here, by incorporating un-labelled vertices in constructing the graph (Line 1) and optimizing adi ff erent objective (Line 4).
 Algorithm 1 Learn Semantically Related Dictionaries (LSRD) 1: Construct the graph G = ( V , E ); 3: while N A &gt; K do 5: A  X  A  X   X  e ; 6: end while 7: for m = 1 ,  X  X  X  , M do 8: for k = 1 ,  X  X  X  , K do 10: end for 11: end for
We learn M semantically related dictionaries from the source do-main to unlock the power of sparse coding in homogenizing di ent modalities as stated in Section 3.3. More importantly, we trans-fer the M semantically related dictionaries to the target domain, and apply them to learn enriched representations of target instances, in order to address the data insu ffi ciency problem. Mathematically, for the m th modality of the i th labelled instance in the target do-main (if available), i.e., t m li , we transfer the m th dictionary learnt from the source domain, i.e., D m , and apply sparse coding [13] to learn the enriched representation  X  t m li by where  X  controls the sparsity of enriched representations. We ob-tain the enriched representation  X  t m ui for the m th modality of the i th unlabelled instance, i.e., t m ui , in a similar fashion.
After transferring the dictionaries, the label scarcity problem ne-cessitates a much more powerful solution -transferring abundant labelled instances from the source into the target domain. To en-able instance transfer, the following two prerequisites have to be met first. 1) Learn enriched representations for labelled source in-stances. Mathematically, for the m th modality of the j th labelled source instance, i.e., s m lj , we learn the enriched representation  X  s performing sparse coding [13] over the m th dictionary D m Only in this way can the representation structures of labelled source  X  T and  X  T u . 2) Aggregate enriched representations of all existing modalities for each target instance. We adopt max pooling [26], widely applied in image processing, to aggregate. For the i th la-Algorithm 2 Multimodal Transfer AdaBoost (MTAB) Input:  X  T MP l  X  enriched representations of labelled target in-Output: h f  X  the final hypothesis for  X  T MP u 1: Initialize the weight of the i th (1  X  i  X  N t l ) instance: v 2: Initialize the weight of the m th (1  X  m  X  M ) modality of the 3: for r = 1 ,  X  X  X  , R do 4: for m = 1 ,  X  X  X  , M do 5: Set p m i ( r ) = 6: Train WeakLearner h m ( r ,  X  )on[  X  T MP l ;  X  S m l ] weighted by 7: end for 8: Define the error on  X  T MP l :  X  ( r ) = N t l i = 1 v 9: Define the consistency of M weak learners on  X  T MP l : 10: Set ( r ) =  X  ( r )  X  consistency ( r )( ( r ) &lt; 0 11: Set  X  ( r ) = ( r ) 1  X  ( r ) and  X  = 1 / (1 + 2ln N s 12: Update the weights: 13: end for belled target instance, max pooling maximizes each feature of the enriched representation over all existing modalities, i.e., where  X  t m li could be missing for some 1  X  m  X  M . We obtain the aggregated representation for the i th unlabelled target instance, i.e.,  X  ui , similarly. In this case, we obtain a uniform representation for all target instances regardless of within-modality insu Besides, the representation is robust to unreliable modalities, since max pooling chooses the most responsive modality for each feature.
Afterwards, we propose the Multimodal Transfer AdaBoost al-gorithm to leverage labelled source instances. The algorithm is based on TrAdaBoost [4] in terms of the basic idea, i.e., reduce the distribution di ff erences between domains by adjusting the weights of instances for training in an adaptively boosting fashion. Specifi-cally, the weights of mis-classified target instances increase to make sure that these instances draw enough attention to be classified right in the next iteration, while the mis-classified source instances are down weighted because they are likely the most di ff erent in distri-bution from target instances. However, our algorithm di ff TrAdaBoost [4] in the following two aspects: 1) inspired by two learners for two views in [23], for each iteration it learns M weak learners to handle M modalities, and skilfully combines M learner-s X  results to boost the prediction accuracy; 2) it tackles multi-class classification problems; 3) more importantly, it learns and di tiates weights for di ff erent modalities besides instances. Algorith-m 2 details the whole algorithm.
The computational cost of the FLORAL method comprises two parts. 1) Learn semantically related dictionaries in O( M + terms together are the cost of constructing the mutual k-NN graph within each modality implemented by KD-tree [1], a space par-tition based approach. The third term is the cost to build inter-edges across di ff erent modalities. The last term corresponds to submodular graph clustering implemented by a max heap which stores marginal gains of all edges. Taking the full advantage of the diminishing marginal gains property, submodular clustering is highly computationally e ffi cient by retrieving the top of the heap, re-maximizing the heap, and updating the marginal gain of the top only. 2) Transfer dictionaries and instances in O( M ( K enriched representation. The first term is the cost to solve sparse in O( RM ( N s l + N t l )) by training each weak learner with LIBLIN-instances as well as the number of modalities.
In this section, we evaluate the FLORAL method with the case study of air quality prediction. In the case study, FLORAL transfers knowledge from a source city, i.e., Beijing, to improve accuracies of air quality prediction in three target cities, namely Shanghai, Tianjin and Baoding, which face either the label scarcity or the data insu ffi ciency problem.
We collected the following four data modalities in Beijing: 1) road networks from Bing Maps contain road segments each of which 1 http: // spams-devel.gforge.inria.fr / index.html 2 https: // www.csie.ntu.edu.tw / ~cjlin / liblinear / is described with its end points, length and level of capacity; 2) Point-Of-Interests (POI) from Bing Maps indicate the name, ad-dress, coordinates, category of a venue; 3) Meteorological data crawled from a public website every hour include weather, tem-perature, humidity, barometer pressure, wind strength, and etc; 4) Taxi trajectories generated by over 32,000 taxicabs in Beijing from er, only the first three modalities are available. Table 2 details the statistics of the first three modalities for all cities.

As air quality in a city varies with time and location simulta-neously, we characterize a grid region in an hour of a day as an instance by partitioning each city into grid regions in the size of 1.5km  X  1.5km. For each instance, we extract its features in all modalities. The feature construction for each modality follows [36] in which road network features F r , POI features F p , meteorologi-cal features F m , and taxi tra ffi c features F t are extracted. Specif-ically, F r and F p are spatio features, and F m and F t are temporal features. Note that some modalities of some instances are not avail-able, and the modality of taxi trajectories is missing for all instances of the three target cities. We label an instance with Air Quality In-dex (AQI) values which are collected from ground-based air quality monitor stations in the four cities every hour. The AQI values range from one to six, corresponding to six air quality states, i.e.,  X  X ood X ,  X  X oderate X ,  X  X nhealthy for sensitive groups X ,  X  X nhealthy X ,  X  X ery unhealthy X , and  X  X azardous X , respectively.

We measure the distributional di ff erence in each modality be-tween a source and a target domain with KL-divergence. The larger the KL-divergence is, the more di ff erent the feature distributions of a source and a target domain in a modality are. Table 3 and Figure 6 present the distributional di ff erences between Beijing and the three target cities in the three shared modalities.
 Table 3: KL-divergence in the
We compare our proposed method FLORAL with the following six baselines, evaluated by prediction accuracy: Original. This method trains a classifier for each modality in a tar-get domain. Among all classifiers, this method selects the one with the best prediction accuracy.
 U-Air. This model [36] combines di ff erent modalities by co-training spatio and temporal features.
 LSRD. We learn semantically related dictionaries from a source domain by applying Algorithm 1, transfer the dictionaries to en-rich feature representations in a target domain according to Equa-tion (8)(10), and train classifiers on  X  T MP l .
 Orig + TAB. This method performs TrAdaBoost [4], a state-of-the-art algorithm that transfers instances, on each modality with origi-nal features, and outputs the best result among all modalities. LSRD + TAB. We perform TrAdaBoost on each modality with en-riched features, and output the best result among all modalities. O-riginal features of both source and target domains are enriched by the semantically related dictionaries according to Equation (8)(9). MDT. Multi-view Discriminant Transfer learning (MDT) [27] trans-fers knowledge between domains with multiple views. We adapt MDT to solve our problem which faces the within-modality insuf-ficiency, by discarding those instances with modalities missing. In summary, Original and U-Air do not transfer. LSRD and Orig + TAB perform feature and instance transfer, respectively. L-SRD + TAB directly combines feature and instance transfer. To make Orig + TAB and MDT applicable to our problem, we discard the modalities which are existing in a source but missing in a target domain. We use linear SVM as the base classifier. Given di feature representations for di ff erent models, the trade-o C of linear SVM is set according to 10-fold cross validation.
Performance comparison: We d i ff erentiate the performance comparison by hours for the following two reasons: 1) distribu-tions of temporal features for di ff erent hours, e.g., tra 0am and 8am, could be distinct; 2) di ff erent numbers of instances are available in di ff erent hours. For each hour in a target domain, we first select an hour from a source so that transferring labelled instances in the hour maximizes the performance. Second, we ran-domly select 10% of labelled instances as training data, and the rest as test. In Figure 7, we report the average accuracy over ten such random partitions for each hour.

From Figure 7, we have the following observations. First, com-bining di ff erent modalities outperforms using single modality on-ly. Compared to Original, U-Air unlocks the power of spatio and temporal features collectively in a co-training fashion, and there-by partially addresses the label scarcity problem. Especially, our proposed LSRD algorithm is highly e ff ective, since it addresses the data insu ffi ciency problem in a target domain by enriching feature representations. Second, transferring source labelled instances is also critical to improve the performance. Even though we apply TradaBoost on each modality X  X  original features individually, i.e., Orig + TAB, we see the performance improvement. Third, perfor-mances of the multi-view transfer learning algorithm MDT are not that satisfactory, probably because it fails to tackle the structured modality missing and within-modality insu ffi ciency. Fourth, direct-ly combining feature and instance transfer i.e., LSRD + TAB, still falls behind our method FLORAL. LSRD + TAB cannot learn and di ff erentiate di ff erent modalities X  weights as FLORAL does. Gen-erally speaking, FLORAL outperforms all the baselines in almost all hours of all target cities up to 50%.

The improvement of FLORAL over other baselines achieves the most significant when transferring from Beijing to Tianjin accord-ing to Figure 7(b); transferring to Baoding takes second while trans-ferring to Shanghai ranks third. Table 3, Figure 2(b), and Figure 6 provide the explanations. The KL-divergence values between Tian-jin and Beijing are averagely small for all the three modalities, i.e., road, POI, and meteorology. The distribution of labels, i.e., air quality, in Tianjin is also similar to that in Beijing. However, the feature distributions of Baoding in road and POI largely di those of Beijing, considering that Baoding is a small city. In this case, the meteorology which is similar for the two geographical-ly close cities primarily accounts for the transfer. Figure 9 further confirms the fact: the smaller the KL-divergence between Baoding and Beijing in meteorology, the better FLORAL performs. Figure 9: Hourly air quality pre-
Figure 10 shows the correspondence between each hour in Baod-ing and the hour in Beijing selected by FLORAL. To maximize the prediction accuracy, it is expected that the hour selected from a source is the most similar to each hour in a target domain in dis-tributions. Consequently, we conclude that during 5am-9am and 13pm-17pm Beijing is the most synchronously similar to Baoding.
E ff ectiveness of semantically related dictionaries: The suc-cess of FLORAL highly depends on the quality of semantically related dictionaries learnt by LSRD. In Figure 11, we examine and visualize the dictionary learnt from Beijing during 11am-12pm for the modality of meteorology with the size K = 10. Each dictionary atom is labelled as the mostly assigned label in the cluster which we infer the atom from. The label of an atom is regarded as the la-tent semantic meaning it encodes. The figure tells that the semantic meanings do make a lot of sense, and thereby the learnt dictionary is e ff ective. For example, as the level of humidity increases and the wind speed reduces, the labels of dictionary atoms tend to increase, meaning that the air quality gets worse. It is noted that the rainfall stays unchanged across all atoms, because there is a lack of rain in Beijing and exists seldom raining days in our training data. Figure 11: The meteorology dictionary learnt from Beijing during
Dealing with the label scarcity and data insu ffi ciency prob-lems: In Figure 8, we verify that FLORAL is capable of dealing with the label scarcity and data insu ffi ciency problems. We fo-cus on the performance of air quality prediction in Tianjin during 17pm-18pm. First, we vary the percentage of labelled instances for training in the target domain, i.e., Tianjin. The smaller the per-centage is, the scarcer the labelled data are. Figure 8(a) shows that when the percentage of training data increases, all algorithm-s perform better. Especially, when the labelled data are very s-carce, say the percentage equals to 0 . 1, FLORAL even improves the most over the baselines. Thus we conclude that FLORAL can successfully handle the label scarcity problem, and that is why we select 10% of labelled instances as training data for performance comparison. Second, we compare di ff erent algorithms X  capabili-ties to tackle the structured modality missing in Figure 8(b). We vary available modalities in Tianjin, ranging from single modali-(a) Varying the percentage of labelled ty to three modalities together. Figure 8(b) shows that the perfor-mance gap between FLORAL and the baselines based on LSRD, i.e., LSRD and LSRD + TAB, stays consistent, while the gap be-tween FLORAL and the other baselines increases as more modali-ties are missing. Therefore we prove that learning semantically re-lated dictionaries fully takes the advantage of the modalities which are missing in a target domain but existing in a source, and there-by e ff ectively addresses the structured modality missing. Note that U-Air cannot handle the cases where only spatio or temporal fea-tures are available, and MDT is not applicable in the cases where only single modality is provided. Third, we investigate the capabil-ities of all algorithms to deal with the within-modality insu cy in Figure 8(c), by randomly dropping a percentage of data for each modality. Reasonably, as the dropping percentage increases, the performances of all algorithms decrease. However, the perfor-mances of FLORAL and the baselines based on LSRD decrease much slower than those of the other baselines. The semantically related dictionaries complement the within-modality insu ffi by enriching feature representations.
 Learning and di ff erentiating di ff erent modalities X  weights: The major reason why FLORAL wins over LSRD + TAB is that FLORAL has the ability to learn and di ff erentiate di ff ities X  weights when transferring, which is further validated in Fig-ure 12. No matter which target city FLORAL transfers to, the dis-tribution of labelled source instances X  weights in meteorology sig-nificantly di ff ers from that in tra ffi c. Besides, for each modality, Figure 12: Comparison of the distributions of labelled source instances X  weights in two modalities when transferring to dif-ferent target cities to predict air quality during 17pm-18pm. the distributions of labelled source instances X  weights di ferent target cities. Specifically, the modality of meteorology plays the most important role when transferring from Beijing to Tianjing because the weights are the most likely to lie in 0 . 9  X  ographical closeness of the two cities explains this. However, the modality of tra ffi c is weighted the highest while transferring from Beijing to Shanghai, the two of which are top two cities in Chi-na. We would also clarify why the modality of meteorology seems more important than the modality of tra ffi c for all target cities. It is because the modality of tra ffi c is missing in all target cities so that the meteorology is more likely to dominate.

Varying the percentage of labelled source instances: The per-formances of FLORAL also rely on the amount of labelled in-stances we transfer from a source domain. Figure 13 presents the performances of FLORAL in predicting air quality in Tianjin dur-ing 17pm-18pm, while we vary the percentage of labelled instances in the target domain, i.e., r t , and that in the source, i.e., r taneously. Reasonably, larger r t and r s lead to better performances. Besides, when r s = 0 . 6, the performances of FLORAL start to sat-urate, meaning that 60% of the labelled source instances have been su ffi cient to improve the target domain.
 Figure 13: Varying the percentage of labelled instances in the target and source domain simultaneously.

Parameter sensitivity: We also study the influence of di ff parameter settings on the performances of FLORAL when trans-ferring from Beijing to Tianjin during 17pm-18pm. We investigate three parameters: K , the size of semantically related dictionaries,  X  and  X  , the trade-o ff parameters X  initialization in Equation (7). For space limitation, we do not include the result for  X  , the other trade-o ff parameter X  X  initialization. We perform grid search on and  X  in the range of { 10  X  3 , 10  X  2 , 10  X  1 , 10 0 , the dictionary size K . FLORAL gains the best accuracy at and  X  = 10 as Figure 14(a) shows. In Figure 14(b), by fixing  X  = 100 and  X  = 10, we obtain the best dictionary size K = Figure 14: Study of parameter sensitivity on air quality prediction.
Scalability: We evaluate the scalability of our LSRD algorithm, which is the major computational bottleneck of FLORAL. By using KD-tree for graph construction and submodular optimization for graph clustering, LSRD is highly e ffi cient and capable of handling extremely large graphs with massive vertices as Figure 15 shows.
In this paper, we propose a novel method called FLORAL to transfer knowledge between domains with multimodal data. Par-ticularly, FLORAL enriches feature representations in a target do-main with semantically related dictionaries learnt from a source, and transfers labelled instances from the source. Extensive exper-imental results in the case study of air quality prediction demon-strate the superiority of FLORAL over other state-of-the-art meth-ods. Besides air quality prediction, FLORAL could be applied whenever the target domain encounters the label scarcity and da-ta insu ffi ciency problems. In the future, we would like to extend FLORAL to transfer from multiple source domains. Although find-ing a source domain which contains all modalities in a target is not that di ffi cult, FLORAL can be more flexible by transferring from multiple source domains.
We thank the reviewers for their valuable comments. We also thank the support of China National 973 project 2014CB340304, and Hong Kong CERG projects 16211214 and 16209715.
