 Classic news summarization plays an important role with the exponential document growth on the Web. Many ap-proaches are proposed to generate summaries but seldom simultaneously consider evolutionary characteristics of news plus to traditional summary elements. Therefore, we present a novel framework for the web mining problem named Evolu-tionary Timeline Summarization (ETS). Given the massive collection of time-stamped web documents related to a gen-eral news query, ETS aims to return the evolution trajectory along the timeline, consisting of individual but correlated summaries of each date, emphasizing relevance , coverage , coherence and cross-date diversity . ETS greatly facilitates fast news browsing and knowledge comprehension and hence is a necessity. We formally formulate the task as an op-timization problem via iterative substitution from a set of sentences to a subset of sentences that satisfies the above re-quirements, balancing coherence/diversity measurement and local/global summary quality. The optimized substitution is iteratively conducted by incorporating several constraints until convergence. We develop experimental systems to eval-uate on 6 instinctively different datasets which amount to 10251 documents. Performance comparisons between differ-ent system-generated timelines and manually created ones by human editors demonstrate the effectiveness of our pro-posed framework in terms of ROUGE metrics.  X  Co rresponding author.
 H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing X  Abstracting methods ; I.2.7 [ Artificial Intelligence ]: Natural Language Processing X  Text analysis Algorithms, Experimentation, Performance Evolutionary summarization, timeline, optimization In the beginning, we will answer three possible questions.
Why timelines? The rapid growth of World Wide Web means that document floods spread throughout the Inter-net. Readers get drown in the sea of documents, wondering where to access. General search engines simply return web-pages ranked by query relevance, but they are not quite ca-pable of handling ambiguous intentioned queries, such as a query about evolving news  X  Influenza A  X . People may have the myriad of general interests about the beginning, the evo-lution or the most up to date situation, while IR techniques rank the returned webpages according to their understand-ing of relevance, which is insufficient.

In many cases, even if the ranked documents could be in a satisfying order, readers are tired of navigating every docu-ment in the overwhelming collection: they want to monitor the evolution trajectory of hot topics by simply browsing. Summarization is an ideal solution to provide a condensed, informative document reorganization for faster and better representation of news evolution. Timeline temporally sum-marizes evolutionary news as a series of individual but cor-related component summaries and hence offers an option to understand the big picture of a developing situation. Why not retrieve timelines created by editors? Manually generated timelines are concise, accurate and in-formative but require tremendous human labor of reading and the work is really energy consuming. Hence, it is im-possible to generate timelines by hands for all queries from T able 1: Part of human generated timeline about In-uenza A outbreak in 2009 from Fox News website  X  . users. Limited handcrafted timelines can be retrieved by search engines. Therefore, it is beneficial to automatically generate high-quality timelines from a collection of vari-ous news sources. The application can be embedded into search engines to improve users X  retrieval experience. Google News Timeline 1 tries to provide such technique but it is coarse-grained, which merely clusters news articles into topic groups and sorts them chronologically [7]. Retrieved docu-ments are neither summarized nor distinguished by their relevance, importance or informativeness.

Why not traditional summarization? A timeline is defined as a historical account of events ranged in chrono-logical sequences. Unlike narratives, which select events in an interpretive context, timelines are more event-aware. Summarizing timelines is obviously different from traditional multi-document summarization (MDS). We first study a man-ual timeline of Influenza A epidemic spread trajectory in Ta-ble 1 from Fox News. We discover four prominent attributes from the timeline created by professional editors:
Relevance. As users issue queries for the news, they definitely need information relevant to what they query.
Coverage. To minimize the loss of main information, timelines should cover as many as possible important as-pects during the news evolution, e.g. the first infected case, vaccine development, etc. illustrated in Table 1.
Coherence. Due to the dynamic nature of news over time, events on separate dates may share dependencies such as  X  X ollow-ups X ,  X  X auses X  or  X  X onsequences X . The component summaries within the timeline capture such coherence to www. newstimeline.googlelabs.com reflect the evolution trajectory, e.g. the escalated emergency, vaccine development from research to practical use, etc.
Diversity. Diversity has a double meaning: selected sen-tences tend to avoid information redundancy within a single date and a period of dates, namely cross-date diversity.
Unfortunately, traditional MDS neglects the significant temporal dimension for evolutionary summarization, nor does it incorporate news event characteristics into summariza-tion, such as coherence, cross-date diversity and their bal-ance by considering temporal proximity. Additionally, time-line consists several component summaries and the quality of these summaries should be optimized both locally, i.e., based on adjacent neighbors, and globally, i.e., based on the whole collection. Compared with traditional MDS without such concepts, timeline generation faces with new challenges.
We introduce a novel framework for the web mining ser-vice Evolutionary Timeline Summarization (ETS). Taking a general query issued by users and the returned collection as input, the system automatically outputs a timeline with items of component summaries which represent evolution-ary trajectories on specific dates. According to the scores of timeline attributes, summaries are generated by ranking sentences in a balanced optimization framework through it-erative substitution from a set of sentences to a subset of sen-tences under constraints. We build an experimental system on 6 real datasets to verify the effectiveness of our methods compared with 4 rivals. ETS addresses following challenges:  X  The 1st challenge for ETS is to model the the four at-tributes for component summaries: items are not assumed to be completely isolated because neighboring summaries are generated interdependently due to news characteristics over time. Diversity within an individual summary has been studied by others, but cross-date diversity has not previously been addressed. More importantly, coherence between com-ponent summaries has never been considered.  X  We have global/local criteria to evaluate the qualities of component summaries. The 2nd challenge is to formu-late the task into a balanced optimization problem to gen-erate summaries which satisfy double standards and above attributes. We propose an efficient framework via iterative substitution and enforce it through constraints construction.  X  Due to ETS application scenario, the large-scale Web collection brings certain difficulties. Traditional MDS faces with a limited collection size (tens of documents) while ETS faces much larger corpora. A 3rd challenge is to deal with significant corpus compression by filtering or pre-processing. Our contributions are manifold by solving these challenges.
In Section 2 we start by reviewing previous works. In Sec-tion 3 we formulate ETS task as an optimization problem and define calculation functions based on four attributes. We explain the balanced optimization solution in Section 4 and describe the experiments in Section 5, including exper-imental system, performance comparisons, result discussion and case studies. Finally we draw conclusions in Section 6.  X  Multi-document Summarization (MDS)
MDS has drawn much attention all these years and gained emphasis in workshops and conferences (SIGIR, ACL, DUC, etc.). General MDS can either be extractive or abstractive. The former assigns salient scores to semantic units (e.g. sen-tences, paragraphs) of the documents indicating their impor-tance and then extracts top ranked ones, while the latter de-ma nds information fusion, such as sentence compression and reformulation. In this study we focus on extractive summa-rization to chronicle important news for timeline generation.
Centroid-based method is one of the most popular extrac-tive summarization method. MEAD [14] and NeATS [11] are such implementations, using position, term frequency and theme, etc. MMR [6] algorithm is used to remove re-dundancy. Most recently, the graph-based ranking methods have been proposed to rank sentences or passages based on the  X  X otes X  or  X  X ecommendations X  between each other. Tex-tRank [13] and LexPageRank [3] use algorithms similar to PageRank and HITS to compute sentence importance. Wan et al. improve the graph-ranking algorithm by differentiat-ing intra-document and inter-document links between sen-tences [17]. Cluster information has been incorporated in the graph model to better evaluate sentences [16]. Li et al. use a structural SVM to learn for sentence selection in MDS [9]. However, all these approaches are for traditional MDS and they miss the temporal dimension.

Swan and Allan construct timelines by extracting clusters of noun phrases and named entities [15]. Later they build a system to provide timelines which consist of one sentence per date, considering usefulness and novelty [1]. Chieu et al. build a similar system in units of sentences with interest and burstiness [2]. None of these methods enriches timeline measurement nor involves the evolutionary characteristics of news mentioned above, so we fill in the gaps by generating component summaries which are not completely indepen-dent: they have influence on  X  X eighbors X . ETS is based on a balanced optimization framework via iterative substitution.  X  Understanding News
Topic detection and tracking (TDT) in news streams is extensively studied in the literature and identifies nature of news. Lexical similarity, temporal proximity and query rel-evance are introduced for topic detection as a part of task initiative, which is later combined with improved clustering techniques to establish event linkage. Novelty detection [10] is an important research branch to decide candidate event sentences. News correlation is determined by causal, tem-poral and rich semantic relationships in [4] or hierarchical dependencies in [5]. Features such as named entities, date or place information, and domain knowledge are deeply an-alyzed [21]. In this study, we do not seek to cluster  X  X opics X  like in TDT or in topic models but to utilize evolutionary correlations of news coherence/diversity for summarization. We give a formal definition of ETS as follows:
Input : Given a general query Q = { q 1 , q 2 , . . . , q users where q i is a query word, we obtain a sentence col-lection C from query related documents. We cluster the
Output : A evolutionary timeline which consists of a se-ries of individual but correlated summary items, i.e. I = . . . , I | T | } , where I i on date t i is a subset of C
According to our investigation, we observe that an effec-tive summary should properly consider the following four key requirements: (1) Relevance. Users are more interested in sentences which are related to the given query, which is similarly defined as  X  interest  X  in [2]; (2) Coverage. The summary should keep alignment with the source collection, which is proved to be significant as proposed in [9]; (3) Coher-ence. News changes gradually as time elapses and evolution indicates consistency among component summaries. It is a novel insight never considered before; (4) Diversity. Accord-ing to MMR principle [6] and its applications [17, 16], a good summary should be concise and contain as few redundant sentences as possible, i.e., two sentences providing similar information should not be both present. Under our scenario, we extend to measure cross-date diversity as penalization to balance  X  X oherence X  and  X  X iversity X . All requirements in-volve a measurement of similarity between two word distri-butions  X  1 and  X  2 , which are measured by Kullback-Leibler divergence here. We introduce decreasing/increasing logis-tic functions, L 1 ( x ) = 1 / (1 + e x ) and L 2 ( x ) = e to map the distance into interval [0,1]. V is the vocabulary frequency for word w .

Re levance. Given query Q , users are more interested in query-relevant information, namely relevance measured by F ( I i ). However,  X  Q is too sparse to reflect essential word distribution. Query expansion is introduced by pseudo-relevance feedback to enlarge Q . We retrieve top- X  snippets (semantic units of our system described in Section 5.2) relevant to Q to  X  Q . Larger distance between  X  I i and  X  Q  X  is not desired:
Coverage. Summary I i focuses on minimizing the loss of main information from sub-collection C i . As ETS requires double criteria of summary quality, a good component sum-mary should represent global source C as well to avoid prob-able local bias caused by discordant distributions of C i C . Coverage F cv ( I i ) is merged by an aggregation function for global coverage F G ( I i ) = L 1 ( D KL ( X  I i ||  X  C Mercer (JM) interpolation controlled by parameter  X  :
Coherence. As mentioned above, a timeline consists of a series of individual but correlated summaries. News evolves over time and a good component summary is coherent with neighboring summaries so that a timeline tracks the grad-ual evolution trajectory for multiple correlative news rather than the development by leaps and bounds. Therefore, we use F ch ( I i ) to evaluate coherence to measure the distance between  X  I i and the word distribution  X  N i from I i  X  X  neigh-boring summary sets N i .

Due to the scrutinized study of temporal proximity in news streams, terms on different dates are not equally weighted. Exponential decay is usually utilized to measure temporal distance [20]. Given dtf ( w, I j | t i ) = e  X  | t j  X  t the decayed word distribution is calculated by:
Diversity. Traditional MDS shows a uniform tolerance towards redundancy to all candidate sentences, while in ETS diversity is dynamic: the tolerance arises as temporal gap en larges. Diversity measures the novelty degree of any of the sentence s compared with all other sentences not only within I i , but also within other component summaries with decayed weights. Such cross-date diversity of an average novelty score F d ( I i ) is calculated by leaving out all sentences in I i , one at a time. For diversity, larger distance is desired.
Utility. Given the source collection, the utility of an indi-vidual summary item I i is evaluated based on the weighted combination of these requirements. All function values are between 0 and 1 and for simplicity, we let
U ( I i ) = w 1 F r ( I i ) + w 2 F cv ( I i ) + w 3 F ch ( I
Given the sentence set C i and the compression rate  X  i on t , there are  X  i | C i | out of | C i | possibilities to generate I The ETS task is to predict the optimized sentence subset of I  X  i from the space of all combinations for all dates. The objective function is as follows:
As U ( I i ) is measured by the neighboring summaries in the generated timeline in our framework, we generate I i itera-tively to approximate I  X  i , i.e, maximize U ( I i ) based on the timeline generated in the last iteration.
Based on the proposed metric of U ( . ), during each itera-tion the algorithm tends to highly score sentences which are more relevant to user interests, more aligned with source texts, more coherent with neighboring components gener-ated in the last iteration and more diversified in the time-line. Hence top ranked sentences s according to U ( s ) are strong candidates for the target summaries.

Consider I ( n  X  1) i which consists of  X  i | C i | sentences gen-erated in the ( n-1 )-th iteration and the top  X  i | C i | sentences in the n -th iteration (denoted by S ( n ) i ), they have constraints, we substitute x ( n ) i sentences with y ( n ) x goal is to find a substitutive pair &lt; x i , y i &gt; for I
To measure the performance of such substitution, a dis-criminant utility gain function is employed to quantify the penalty. Therefore, we can pre-dict the substitutive pair by maximizing the gain function  X 
U tion (6) changes into maximization of utility gain by substi-tute  X  x i with  X  y i during each iteration. Formally,
A well generated summary I i should be evaluated highly within the global collection C , and also be optimized given the local set of C i . However, the objectives of two optimiza-tions are not always the same because the word distributions in these source sets C and C i are different. The substitu-tive pair &lt; x , y &gt; may perform well based on the timeline globally while not on the neighboring set locally and vice versa. There is a tradeoff between the global optimization and local optimization and hence we need to balance both.
Recall the aggregation function of linear combination be-tween local and global coverage in Equation (2). The utility for I i can also be rewritten as an interpolation function from ity U ( I i ) | C from the global collection C : The objective Equation (8) is actually to maximize  X  U ( I from all possible substitutive pairs between two iterations to generate I i . The algorithm is shown in Algorithm 1. Al gorithm 1 Interpolative Optimization 1 : Input: C 1 , C 2 , . . . , C | T | ,  X  ,  X  i 2: I i  X  X } for i=1, 2, . . . , | T | 3: repeat 4: for i = 1 to | T | do 5: U  X  ( I i ) = U ( I i ) 6: for all s  X  C i do 7: calculate U ( s ) 8: end for 9: rank s with U ( s ) 10: S i  X  top  X  i | C i | ranked sentences 11: Z i  X  I i  X  X  i 12: X i  X  I i  X  X  i 13: Y i  X  X  i  X  X  i 14: for all &lt; x i , y i &gt; pair where x i  X  X  i , y i 16: end for 17: &lt;  X  x i ,  X  y i &gt; = argmax  X  U x i ; y i 18: I i  X  ( I i  X   X  x i )  X   X  y i 19:  X  U i = U ( I i )  X  X   X  ( I i ) 20: end for 21: until  X   X  U i &lt;  X  Th e threshold  X  is set at 0.0001 in this study. However, Algorithm 1 cannot avoid extreme situations. Significant rise in local utility which offsets much global utility loss still makes an available selection and vice versa. We seek to con-trol such fluctuations to accelerate the convergence of the objective function. We choose the utility-maximized substi-tutive pairs under some constraints which are to ensure the overall utility is non-decreasing while finally both local and global optimizations are reached as iterations accumulate.
Local Optimization. The maximized substitutive pair should have utility improvement within sub-collection C i The local utility loss is not acceptable during the iterations.
Cons traint 1.
Global Optimization. For each date, there can be a global utility loss for improvement in local utility, but with a borderline for such compromise. Global utility loss is ac-ceptable for parts of the timeline but the loss from these dates should be offset by gains from the remaining dates. In other words, the sum of global utility is non-decreasing. Constraint 2.  X 
Constraint 3.  X  is set as the maximum absolute value of | ( X  U x i ; y i from the last iteration and is initialized as positive infinity. Although allow utility compromises for parts of the timeline, we do not desire the situation of significant global utility gain for few dates along with slight utility loss for most others. Hence we set another constraint: for every iteration, the number of dates with global utility loss should not exceed m . In this study we let m =  X  0 . 8  X | I | X  , which allows at most 80% dates with global utility loss.

Constraint 4.
Optimization Problem. Considering the four types of constraints, we propose the balanced maximization frame-work enforcing both local and global optimization. Equation (8) can be rewritten as: subjected to:
Given results from Algorithm 1, suppose the size of the largest status space for a single date is | H | within each itera-pairs, where each element M j;i stands for a possible substi-tution. We calculate the global utility change  X  U ( M j;i local utility change  X  U ( M j;i ) | C i and their linear combination  X 
U find a maximized overall utility  X  U ( M j;i ) at the j -th sta-tus space on date t i , while at the same time global utility  X 
U straints. We select one element at each column by Dynamic Programming . After applying the substitution of M j;i , a summary is hence generated within this iteration and the timeline is created by choosing a path in matrix M | H | X |
We briefly describe the idea of dynamic programming and the details are left to Algorithm 2. Given matrix M , we sort all possible substitutions for I i according to the overall util-matrix M L where elements M L j;i =  X  U ( M j;i ) | C i and matrix M
G where elements M G j;i =  X  U ( M processing column, b is to record how many M G j;i &lt; 0 before column a on the path and c is to record the sum of M G j;i before column a on the path. max { M j;a } denotes the max-imum utility when specify a , b and c . Similar we set another array P [ a ][ b ][ c ] to record the path information. Details are illustrated in Algorithm 2. The worst time complexity is O ( | T | X | H | X  m )  X  O ( | T | 3 ) for Algorithm 2. Al gorithm 2 Dynamic programming with constraints 1 : Input: Matrix M , M L , M G , m ,  X  2: for a = 0 to | T | X  1 do 3: for b = 0 to m do 4: c max = max ( 5: for c =  X   X m to c max do 6: dynamic programming given A [ a ][ b ][ c ] , P [ a ][ b ][ c ] 7: for l = 0 to | H | do 8: if M L l;a +1 &gt; 0 &amp;&amp; 0 &gt; M G l;a +1 &gt; 9: sn=1 10: else if M L l;a +1 &gt; 0 &amp;&amp; M G l;a +1 &gt; 0 then 11: sn=0 12: end if 13: if A [ a +1][ b +sn][ c + M G l;a +1 ] &lt; A [ a ][ b ][ c ]+ M 14: A [ a +1][ b +sn][ c + M G l;a +1 ]= A [ a ][ b ][ c ]+ M 15: store path P [ a +1][ b +sn][ c + M G l;a +1 ]= l a 16: end if 17: end for 18: end for 19: end for 20: end for 21: for b = 0 to m do 22: c max = max ( 23: for c = 1 to c max do 24: find maximum A [ | T | ][ b ][ c ] 25: end for 26: end for 27: trace way back by path P [ | T | ][ b ][ c ] 28: return l | T | , . . . , l 2 , l 1
Since there is no existing standard test set for ETS meth-ods, we construct 6 test sets which consist of news datasets and golden standards to evaluate our proposed framework empirically. We downloaded 10251 news articles from 10 se-lected sources. As shown in Table 2, one of the sources is in China, three of them are in UK and the rest are in the US. We choose them because many of these websites pro-vide timelines edited by professional editors, which serve as golden standards. 6 topics belong to different categories of Rule of Interpretation (ROI) [8]. Statistics are in Table 3.
We present 2 practical systems for ETS, off-line and on-line . Given a topic related corpus, the systems return trajec-tory timelines automatically. Off-line system handles stabi-lized topics with no new occurring while on-line system can support incremental documents from topics still evolving. T able 3: Detailed basic information of 6 datasets.
T opics (Query Words) #Do cs #GT A L S ince 5. Michael Jackson Death 92 5 3 6 4 2 010  X  Preprocessing. As ETS faces with much larger cor-pus compared with traditional MDS, we apply further data compression besides stemming and stop-word removal. We extract text snippets representing atomic  X  X vents X  from all these documents with a toolkit provided by Yan et al. [19]. After the snippet extraction procedure, we compress the cor-pora by discarding non-event texts and filtering those events non-relevant to any of the query words.  X  Compression Rate. After preprocessing, we obtain numerous snippets, temporally tagged according to the pub-lish time of their source documents, and then decompose them into temporally tagged sentences as the global collec-tion C . We partition C according to timestamps of sen-tences, i.e., C = C 1  X  C 2  X  X  X  X  X  C | T | . I i is generated from sub-collection C i . The sizes of component summaries are not necessarily equal. Users specify the overall compression rate  X  , and we extract more sentences for important dates while fewer sentences for others. The importance of dates is measured by the burstiness with probable significant occur-rences [2]. The compression rate on t i is set as  X  i = |  X  Off-line System vs. On-line System. The difference between two systems is whether corpora are temporally up-dating or not. For stabilized corpus, component summaries are optimized based on neighboring summaries on dates be-fore and after them. For evolving corpus, we cannot forecast futures sentence sets, so the on-line system is to consider neighboring summaries previously generated.
We implement the following widely used multi-document summarization algorithms as the baseline systems. Some of the systems are designed for traditional summarization without temporal dimension. The first intuitive generation for such methods is a global summarization on collection C at a uniform compression rate  X  and then distribute the selected sentences to their source dates. The other intuitive one is a local summarization on sub-collection C i with a compression rate  X  i . To these methods we take the average score as their performance. For fairness we conduct the same preprocessing for all algorithms by compression or filtering.
Random: The method selects sentences randomly for each document collection.

Centroid: The method applies MEAD algorithm [14] to extract sentences according to the following parameters: centroid value, positional value, and first-sentence overlap.
GMDS: The Graph-based MDS proposed by Wan et al . [16] first constructs a sentence connectivity graph based on cosine similarity and then selects important sentences based on the concept of eigenvector centrality.

Chieu: Chieu et al. present a similar timeline system [2] with different goals and frameworks, utilizing interest and burstiness ranking but neglecting news evolution.
ETS: Our proposed algorithms with iterative substitution under constraints are tested as ETS 1 for the off-line system and ETS 2 for the on-line system.
To compare with the human timelines, we use ROUGE toolkit (version 1.5.5), which is officially applied by Docu-ment Understanding Conference (DUC) for document sum-marization performance evaluation [12]. The summarization quality is measured by counting the number of overlapping units, such as N-gram, word sequences, and word pairs be-tween the candidate timeline set CT and the ground-truth timelines GT . Several automatic evaluation methods are im-plemented in ROUGE, such as ROUGE-N, ROUGE-L and ROUGE-W, each of which can generate three scores (recall, precision and F-measure). Take ROUGE-N as an example: 1. ROUGE-N-R is an N-gram recall metric as follows: 2. ROUGE-N-P is an N-gram precision metric as follows: 3. ROUGE-N-F is an N-gram F 1 metric as follows: N in these metrics stands for the length of the N-gram and N-gram  X  GT denotes the N-grams in the ground truth time-line while N-gram  X  CT denotes the N-grams in the candidate timeline. Count match (N-gram) is the maximum number of N-gram in the candidate summary and in the set of ground-truth summaries. Count (N-gram) is the number of N-grams in the ground truth summaries or candidate summary.
Furthermore, as the timeline consists of a series of individ-ual summaries which are not equally significant, we evaluate ROUGE F-score for the timeline by the weighted average ROUGE F-score of all summaries, weighted by  X  i : ROUGE-N-F(I) = 1 |
As we have similar conclusions in terms of any of the three scores, in this paper, we only report the average F-measure scores generated by unigram-based ROUGE-1, bigram-based ROUGE-2, and the weighted longest common subsequence based ROUGE-W to compare our proposed method with other implemented systems. These evaluation metrics have b een shown to much agree with human judgments. The weight W is set to be 1.2 in our experiments. Intuitively, the higher the ROUGE scores, the similar the two summaries.
We use a cross validation manner among 6 datasets, i.e., we train parameters on one topic set and examine the perfor-mance on the others. After 6 training-testing processes, we take the average F-score performance in terms of ROUGE-1, ROUGE-2, and ROUGE-W on all sets. The overall results are shown in Figure 1 and details are listed in Tables 4  X 
From the results, we have the following observations:  X  Random has the worst performance as expected.  X  The results of Centroid are better than those of Ran-dom. This is mainly because the Centroid based algorithm takes into account positional value and first-sentence over-lap, which facilitates main aspects summarization.  X  The GMDS system outperforms centroid-based summa-rization methods. This is due to the fact that PageRank-based framework ranks the sentence using eigenvector cen-trality which implicitly accounts for information subsump-tion among all sentences.

Traditional MDS only considers sentence selection from either the global or the local scope, and hence bias occurs. Many sentences are missed, which makes a low recall. Gen-erally the performance of local priority summarization is bet-ter than global priority summarization. Probable bias is to some extent mitigated by searching for worthy sentence in every single date. However, precision drops due to excessive choice of local timeline-worthy sentences.  X  In general, the result of Chieu X  X  method is better than Centroid but unexpectedly, worse than GMDS. The reason in this case may be that Chieu X  X  method does not capture sufficient timeline attributes. The X  X nterest X  X odeled in their algorithms actually performs flat clustering-based summa-rization which is proved to be less useful [18]. GMDS utilizes sentence graph linkage, and partly captures  X  X oherence X .  X  Both ETS 1 and ETS 2 under our proposed framework outperform baselines, indicating that the properties we use for timeline generation are beneficial. ETS 1 in off-line sys-tem performs better than ETS 2 in on-line system, indicat-ing new coming documents do have influence on component summary generation within the timeline. ETS 2 is acceptable if on-line is required due to its advantage over baselines. Table 4: Overall performance comparison on long lasting news. ROI  X  category: Science, Finance. Sys tems R-1 R-2 R-W R-1 R-2 R-W Ra ndom 0 .257 0. 039 0 .081 0 .230 0. 030 0 .071 Cen troid 0 .331 0. 050 0 .114 0 .305 0. 041 0 .108 GM DS 0 .364 0. 062 0 .130 0 .327 0. 054 0 .110 Ch ieu 0 .350 0. 059 0 .128 0 .325 0. 052 0 .109 ET S 1 0 .396 0. 085 0 .139 0 .351 0. 061 0 .121 T able 5: Overall performance comparison on short breaking news. ROI category: Accidents, Disasters. Sys tems R-1 R-2 R-W R-1 R-2 R-W Ra ndom 0 .262 0. 041 0 .096 0 .266 0. 043 0 .093 Cen troid 0 .369 0. 062 0 .128 0 .362 0. 060 0 .129 GM DS 0 .389 0. 084 0 .139 0 .380 0. 106 0 .137 Ch ieu 0 .384 0. 083 0 .139 0 .383 0. 110 0 .138 ET S 1 0 .483 0. 119 0 .163 0 .481 0. 123 0 .160 T able 6: Overall performance comparison on celebri-ties. ROI category: Legal Cases, Politics. Sys tems R-1 R-2 R-W R-1 R-2 R-W Ra ndom 0 .232 0. 033 0 .080 0 .254 0. 039 0 .084 Cen troid 0 .320 0. 051 0 .109 0 .325 0. 053 0 .111 GM DS 0 .341 0. 059 0 .127 0 .359 0. 061 0 .129 Ch ieu 0 .344 0. 059 0 .128 0 .346 0. 060 0 .125 ET S 1 0 .371 0. 081 0 .132 0 .388 0. 083 0 .134  X  The performance on intensive focused news within short time range (Topic 3, 4) is better than on long lasting news.
Having proved the effectiveness of our proposed methods, we carry the next move to identity how relevance , cover-age , coherence , diversity and the 4 constraints take effects to enhance the quality of a summary in strategy selection.
Recall that utility U is the linear combination of local util-ity and global utility, both of which are the weighted sum of relevance , coverage , coherence and diversity under 4 con-straints during the maximization process of U ( I i ). Generally speaking, strategies can be sorted into two categories: pa-rameter tuning and constraint selection. Each time, we tune one strategy while the other one is fixed.
Keeping other parameters fixed, we vary one parameter at a time to examine the changes of its performance from all 6 datasets. The first group of key parameters in our frame-work is w 1 , w 2 and w 3 where w 4 =1-w 1 -w 2 -w 3 . Experimen-tal results indicate coherence and diversity facilitate ETS while relevance demonstrates a relatively weaker influence. Excessive use of these 3 attributes impairs performance, ex-cept coverage, showing its domination in text summariza-tion. We set w 1 =0.1, w 2 =0.4, w 3 =0.3 and hence w 4 =0.2 in our experiments.

Another key parameter in our framework is  X  in Equation (2) to measure the tradeoff between local and global collec-tion. We gradually change  X  from 0 to 1 at the step of 0.1 to examine the effect in Figure 3 (e). The combination of local and global utility outperforms the performance in isolation (  X  =1 or 0). Furthermore, a larger  X  (from 0.5 to 0.7) per-forms relatively better, but when  X  exceeds 0.8, the extreme emphasis on global utility results in performance loss. We take  X  =0.7 as the balance factor.  X  controls the shape of the exponential decay and hence the size of influential neighboring window. We then exam-ine the effect of neighboring summaries in ETS in Figure 3 (f). We vary  X  from 0 (all texts on timeline) to 500 (an approximation of +  X  , no neighbors considered). According to Figure 3 (f), the lines share a similar peak when  X   X  [0.8, 4]. A moderate window size contributes to word distri-bution smoothing and reflects the trend for news evolution but too large a window introduces noise distribution as well. Therefore we choose  X  =0.8.

We then examine the results of different  X  for pseudo-relevance feedback. According to Figure 3 (g), without any query expansion but simply compared with query Q, the performance is far from optimistic. Excessive document ex-pansion impairs performance as well.  X  =100 is shown large enough to smooth the word distribution in our experiments.
Finally we check the effect of overall compression rate  X  which is usually designated by users. If the user would like to read more, he/she might favor a larger  X  . We vary  X  from 0.1 to 1 at the step of 0.1. Generally the lines are down-sloping as our ground-truth timelines are rather small compared with the huge global source collection. Recall is acceptable even when  X  is small while precision drops accu-mulatively as  X  increases.
To understand the effect of each proposed constraint, a series of experiments are conducted, illustrated in Figure 4, consisting all 2 4 combination tests of constraints C1  X  C4.
From Figure 4, we notice Constraint 1 and Constraint 2 are useful. Recall the description of these two constraints. They are to maximize local utility gain and global utility gain and therefore they benefit timeline generation. The effectiveness of Constraint 3 and Constraint 4 seems not ob-vious in Figure 4 (a). Constraint 3 is to restrict the global utility loss for a particular summary. However, these two constraints do help reduce iteration counts to convergence, shown in Figure 4 (b). As iteration accumulates, the change of utility  X  U varies significantly from time to time. It is difficult to set a general borderline of global utility loss arbi-trarily to balance the convergence rate and timeline quality: inappropriate choice of Constraint 3 may cause potential harm to timeline generation. Both Constraint 3 and Con-straint 4 are beneficial in iteration count performance be-cause they reduce the available search space and facilitate early pruning for state paths in Algorithm 2. Figure 4: ROUGE-F and convergence performance comparison among all 16 constraint combinations.
In this paper we present a novel framework for the im-portant web mining problem named Evolutionary Timeline Summarization (ETS), which generates trajectory timelines fro m massive data on the Internet. Given a query related news collection, ETS summarizes an evolution trajectory. We formally formulate ETS task as a balanced optimization problem via iterative substitution, measured on local sub-collections and the global collection. The objective function Utility is measured by four properties: relevance , coverage , coherence and diversity . We implement an off-/on-line sys-tem under such framework as experimental environment.
Abundant experiments are done on real web datasets. We compare numerous approaches, including two ways of im-plementation of 4 baselines and our proposed ETS 1 , ETS 2 Through our experiments, we notice that among these prop-erties, coherence plays an important role in timeline genera-tion, indicating neighboring information is essential in evo-lutionary timeline trajectory: news evolves gradually. We also investigate the balance between local utility and global utility, and obtain the best combination coefficient at  X  =0.7, meaning local utility weights slightly higher. We introduced four constraints, two of which ensure the local and global maximization, while the others ensure fast convergency. In case studies, our automatic timeline presents an informative document reorganization. However, as summaries generated by humans have potential biases, we will provide alternative evaluation metrics to measure ETS performance.
We thank the anonymous reviewers for their valuable and constructive comments. This work is partially supported by NSFC Grant No.60933004, 61050009 and 61073081, and Xiaojun Wan is supported by NSFC Grant No.60873155. [1] J. Allan, R. Gupta, and V. Khandelwal. Temporal [2] H. L. Chieu and Y. K. Lee. Query based event [3] G. Erkan and D. Radev. Lexpagerank: Prestige in [4] A. Feng and J. Allan. Finding and linking incidents in [5] G. P. C. Fung, J. X. Yu, H. Liu, and P. S. Yu. [6] J. Goldstein, M. Kantrowitz, V. Mittal, and [7] X. Jin, S. Spangler, R. Ma, and J. Han. Topic initiator [8] G. Kumaran and J. Allan. Text classification and [9] L. Li, K. Zhou, G.-R. Xue, H. Zha, and Y. Yu. [10] X. Li and W. B. Croft. Improving novelty detection [11] C.-Y. Lin and E. Hovy. From single to multi-document [12] C.-Y. Lin and E. Hovy. Automatic evaluation of [13] R. Mihalcea and P. Tarau. A language independent [14] D. Radev, H. Jing, M. Sty, and D. Tam.
 [15] R. Swan and J. Allan. Automatic generation of [16] X. Wan and J. Yang. Multi-document summarization [17] X. Wan, J. Yang, and J. Xiao. Single document [18] D. Wang and T. Li. Document update summarization [19] R. Yan, Y. Li, Y. Zhang, and X. Li. Event recognition [20] C. C. Yang and X. Shi. Discovering event evolution [21] K. Zhang, J. Zi, and L. G. Wu. New event detection
