 School of Computer Science and Engineering, Jiangsu University of Science and Technology, Zhenjiang, Jiangsu, China
Department of Radiology, Carver College of Medicine, The University of Iowa, Iowa, IA, USA School of Information Engineering, Yancheng Institute of Technology, Yancheng, Jiangsu, China 1. Introduction
In many real world classification tasks, the data sets are typically imbalanced, i.e., some classes have more instances than others. Constructing classification models by these skewed data can be technically a challenging task [13,23]. Its harmfulness mainly reflects in underestimating greatly the classification per-formance of minority class. Especially if the used data set has some other features simultaneously, such as high-dimension, small sample and high noise, this damage will be intensified [2,18,29]. Therefore, class imbalance problem has drawn a significant amount of interest since 2000 from artificial intelli-gence, machine learning and data mining, which can be reflected by the installment of several major workshops, conferences and special issues, including AAAI X 00 [16], ICML X 03 [5] and ACM SIGKDD Explorations Newsletter X 04 [6]. In ICDM X 05, this problem has also been marked as one of the 10 chal-lenging problems in data mining research [33].

In the past decade, researchers developed many effective methods to resolve class imbalance problem, including sampling [4,10,12,19,20], cost-sensitive learning [26,35], one-class Support Vector Machine (SVM) [25], kernel-based method [15,32], active learning [8,9] and ensemble learning [14,22,27] etc. These solutions can alleviate the damage of imbalanced sample distributions more or less. However, most of previous work ignored a key problem: some skewed data sets are unharmful and conducting a classifier, which is specially designed for class imbalance problem, on these data sets can hardly improve and even degenerate classification performance, meanwhile it is more time-consuming, too [21]. Liu et al. [21] observed this problem but didn X  X  give the corresponding solution, thus it is essential to design an efficient strategy to pre-estimate the harmfulness of class imbalance when we encounter skewed classification tasks.

In this study, we systematically analyze the reasons of harmfulness produced by class imbalance and present a novel and simple strategy by scatter matrix based class separability measure to pre-estimate the harmfulness of class imbalance. The estimation is quantitative and automatic with merely using training data sets. We tested the strategy on 12 skewed data sets (6 low-dimensional data sets and 6 high-dimensional data sets) with promising results. The results show that the proposed strategy can approximate accurately divide the imbalanced classification tasks into two groups: harmful and unharm-ful. It guides us to select suitable classification methods for different class imbalance learning tasks. In addition, for unharmful class imbalance tasks, the proposed strategy can help to save running time of classification algorithms and reduce the possibility of performance degeneration.

The remainder of this paper is organized as follows: Section 2 analyzes the reasons of harmfulness produced by class imbalance. Section 3 describes the theory about scatter matrix based class separability measure and the proposed strategy in detail. In Section 4, we evaluate the effectiveness of the proposed strategy by a mass of experiments. At last, we conclude this paper in Section 5. 2. The reasons of harmfulness produced by class imbalance Firstly, let us to consider why some imbalanced classification tasks are harmful while the others not. Without loss of generality, suppose the classification task is binary. Let  X = { ( x i ,y i ) | x i  X  R n , 1 i s, y i  X  X  C 1 ,C 2 }} be training dataset, where C 1 denotes the majority class and C 2 represents the s ,y i = C 2 } be respectively majority and minority class sample sets extracted from the original training n = 1 and in this feature, both classes satisfy Gaussian distribution (see Fig. 1). Then priori probability P to Bayesian formula, posterior probability of two classes could be calculated as: as separating point for two classes would guarantee the error rate is minimal. Error rate P error can be approximately estimated by:
As Fig. 1 shows, when both classes hold the equal number of samples, i.e. P ( C 1 )= P ( C 2 ) , x 1 is more sacrifice for the performance of minority class (i.e., the class C 2 ). The greater the class imbalance ratio ( P ( C 1 ) /P ( C 2 ) ), the more performance degradation the minority class would bear. Therefore, we find that the harmfulness of class imbalance is bound up with two key factors: the degree of overlapping between two-class samples and the class imbalanced ratio, which are similar with the research results of Japkowicz and Stephen [17].

Then we investigate the relationship between sample distribution and the harmfulness of class imbal-ance in more general situation (see Fig. 2). Figures 2(a) and 2(c) show that when there is no overlap (i.e., large margin) between two classes, the classification performance can be fully guaranteed, regardless of the classification task is either balanced or not. But for overlapping classification tasks, as we know, misclassification is inevitable. Figure 2(b) indicates that errors are averagely shared by both classes when the classification task is overlapping and balanced. While in overlapping imbalanced classification task, the performance of minority class is greatly ignored due to there existing more majority class sam-ples than the minority ones in the overlapping region and by traditional classifiers, classification rule is to minimize the total error rate. For example, in Fig. 2(d), all three minority class samples located in the overlapping region are wrongly recognized, causing the vast gap between the performances of both classes. Comparing two subgraphs 2(c) and 2(d), we observe that though both tasks are same in class imbalance ratio (4.0), 2(c) holds less overlapping region (i.e. better separability) than 2(d). That means for the classification task of 2(c), it suffers less harm from class imbalance than the other.
According to the analysis above, we find the fundamental reasons for the harmfulness arouse by class imbalance, i.e., degree of overlapping and class imbalance ratio. Therefore, it is possible to develop one effective evaluation criterion to pre-estimate whether one skewed classification task suffers harmful class imbalance or not with using both indexes mentioned above. 3. Scatter matrix based class separability measure and the proposed strategy 3.1. Computation of Q evaluation criterion
Considering the factors related closely with the harmfulness of class imbalance, class imbalance ratio could be easily acquired by priori sample distribution, then how to quantificationally assess the degree of overlapping? To address this problem, we profit from the idea of scatter matrix based class separability measure [7] to estimate the degree of overlapping between different classes.

Scatter matrix based class separability measure is commonly used due to its simplicity [7]. The scatter matrices include Within-class scatter matrix ( S W ), Between-class scatter matrix ( S B ) and Total scatter matrix S T , they are defined as follows.
 where C is the number of classes, and C is 2 in this study. s i ( i = 1, ... , C ) denotes the number of samples in the i -th class, and x probability and mean vector of the i -th class, respectively. While m denotes the mean vector of all the samples. Among the three scatter matrixes, S W tests the average tightness of the samples belonging to the same class, S B evaluates the degree of separation among the centroids of different categories and S classification task, a large separability means that both classes have small within-class scatter and large between-class scatter, and the class separability measure, J , can be defined as one of the follows. where | A | denotes the value of determinant A, and tr ( A ) represents the trace of the matrix A. In this study, J 3 , which has been used widely, is employed as the class separability measure, consequently to approximatively estimate the degree of overlapping between two classes.
 Suppose the class imbalance ratio R is defined as: where N  X  and N + are number of negative samples (majority class samples) and positive examples (minority class examples), respectively. Then we integrate J 3 with the reciprocal of class imbalance ratio R by simple multiplication to generate an evaluation criterion Q for estimating the harmfulness of class imbalance as: then the formula can be extended as: It is not difficult to observe from Eq. (12) that the measure for harmfulness of class imbalance has been correlated with both key factors indicated in Section 2. A large Q means that large class separability and small class imbalance ratio, meanwhile demonstrates the corresponding classification task suffers less harm from class imbalance, and vice versa. Q measure reflects the degree of damage caused by class imbalance. In practical applications, a threshold T Q should be provided previously to divide them into one of two groups: harmful class imbalance and unharmful class imbalance. In this research, we empirically assign T Q as 0.001. Then the detailed flow of harm test is summarized in Fig. 3.
Figure 3 shows that the harm test is conducted with merely training data, and the testing results would be helpful to guide us to select appropriate data processing approaches and to construct efficient classi-fiers, thus to acquire better classification performance for testing examples. 3.2. Divergence precision
Then how to verify the effectiveness of the proposed Q measure? We intend to utilize the final classi-evaluation of harmfulness arouse by class imbalance by testing examples, we define a novel assessment criterion named as divergence precision . Before discuss this measure, let us introduce several evaluation metrics for class imbalance problems: accuracy ( Acc ), F-measure and G-mean , Acc denotes the overall classification accuracy, while F-measure and G-mean are specifically designed for skewed classification tasks. They may be regarded as functions of the confusion matrix as shown in Table 1 and are calculated as follows: where Precision, Recall, TPR and TNR are further defined as follows:
Among the evaluation metrics above, we combined Acc and G-mean to develop the assessment cri-terion divergence precision for estimating whether classification task really suffers from harmful class optimized so that f (  X  ) can discriminate between the two classes of five particular imbalanced problems have been obtained from the corresponding training set ( S 1 , S 2 , ..., S 5 ) , and then the corresponding classifiers f (  X  i ) have been run over the testing sets. Table 2 reports the results of several measures, including TPR , TNR , Acc and G-mean .

Note the tasks S 1  X  S 3 , they obviously suffer from harmful class imbalance due to TNR is much larger than TPR . The similar observation emerges between Acc and G-mean , thus they may be regarded as an excellent evaluation criterion to provide real estimation for the harmfulness of class imbalance. Then we define divergence precision as: where DP denotes divergence precision and it is the difference of Acc and G-mean . Let us go back to Table 2, for three harmful class imbalance tasks, i.e., S 1  X  S 3 , DP is much larger than 0. The more the harm, the larger DP is. While for two unharmful skewed classification tasks S 4 and S 5 , DP is less than or equal to 0, which may provide reasonable estimation. Just as T Q for Q , a threshold T DP should be assigned previously for DP to recognize real harmful class imbalance tasks and unharmful ones, and we empirically set it as 0.1. Therefore, DP could be regarded as one effective criterion to assess the validity of the proposed Q measure. 4. Experimental results and analysis
In this section, we applied 12 real-world imbalanced data sets to test the proposed strategy. 6 of them are low-dimensional ones, extracting from UCI Repository of Machine Learning Databases [3]. The rest 6 are high-dimensional imbalanced data sets, four of them are DNA microarray data sets correlated with human tumor [1,11,24,31] and the rest 2 are protein mass-spectrometry ones [30], namely Ovarian I and Ovarian II, respectively. They have 39-768 samples, 3 X 15154 attributes, 1.60 X 49.62 class imbalance ratio, and their basic information are summarized in Table 3.

Then we used 10 times X  3-fold cross validation (3-fold CV) to test classification performance, includ-ing three metrics Acc , F-measure and G-mean . In 3-fold CV, using training set to compute Q measure and testing set to calculate divergence precision DP , and then the effectiveness of the proposed strategy could be verified. Without loss of generality, Gaussian-kernel function based support vector machine (SVM) [28] was used as baseline classifier in this research, it X  X  convenient to be replaced by other clas-sification algorithms. Moreover, to provide real classification results, we used signal-noise ratio ( SNR ) index [11] to implement feature selection on 6 high-dimensional data sets, and SNR index is described as: where  X  0 and  X  1 are mean values of the i -th feature f i for two different classes calculated in sample space, whereas  X  0 and  X  1 are their standard deviations, respectively. In this research, we empirically extract 100 features that have close relation with classification [34]. Additionally, to maintain justice of evaluated results, we also conducted a data preprocessing procedure to acquire the normalized value of each feature to be mean 0 and variance 1, the computational formula is listed as: where f ij and f ij represent original and normalized value of the jth sample on the ith feature, respec-tively. While  X  i and  X  i are mean and variance for the ith feature in original dataset, respectively.
Table 4 listed the estimated and real results for harmfulness of class imbalance. Estimated result is validated to be correct when both Q&lt;T Q and DP &gt;T DP are true or false. From Table 4, we observe that the proposed evaluation criterion have correctly estimated 11 data sets among all 12 classification tasks, but only the estimated result on haberman data set doesn X  X  agree with the fact, indicating the effectiveness and feasibility of the proposed evaluation criterion. In addition, we found an interesting phenomenon from the results in Table 4, that is the more harmful the classification task suffers from class imbalance and the larger the class imbalance ratio is, the smaller Q is, such as balance and cmc data sets. While for iris data set, which has acquired the largest Q value (0.158533), it almost suffered no damage form class imbalance, meeting the intention of our original design.

Then we investigated the relationship between classification performance and classification algorithms for harmful and unharmful class imbalance tasks, respectively. Without loss of generality, we ran some traditional and popular class imbalance learning algorithms [4,10,12,20] on these data sets to compare the performance of merely using original data set (ORI). The algorithms include: 1) Random Undersampling (RUS): It arbitrarily takes away some examples of majority class to bal-2) Random Oversampling (ROS): It randomly duplicates some samples belonging to minority class. 3) Synthetic Minority Oversampling Technique (SMOTE, in this study, we abbreviated it as SMO): 4) Borderline-Smote (BSO): BSO [12] is one improved version of SMO. Its idea comes from the 5) One Side Selection (OSS): It has very similar idea with BSO. By cleaning noisy samples, redundant
Based on the algorithms described above, we tested their classification performance by three eval-uation metrics: Acc , F-measure and G-mean . The average results of 10 independent runs are given in Fig. 4.

Figure 4 shows that for those classification tasks which are severely destroyed by class imbalance, e.g., balance and cmc , using specifically designed class imbalance learning algorithms can improve classification performance to a large extent. While those unharmful tasks benefit less and even cause performance degeneration from these classification algorithms, for example, for CNS data set, none class imbalance learning algorithm surpasses the original classifier, which indicates the necessity of this study.

Moreover, we are much more concerned the time-complexity of the proposed strategy. Our testing results show the running time on all 12 data sets were lower than 0.05 seconds, which can even be ignored in practical applications. Therefore, the proposed strategy is effective and efficient in that it doesn X  X  increase excess computational burden to classification tasks.

At last, we compared the running time of various class imbalance learning algorithms on 12 data sets (see Fig. 5). From Fig. 5, we observed that for large sample data sets, using original data to construct classifier is more time-saving than those complex sampling methods, with the exception of RUS and OSS. With the increase of sample size, the gap of running time of different learning algorithms will further increase. As for those small sample sets, the difference can almost be ignored. Therefore, we can save running time when encountering unharmful class imbalance tasks, especially when the data set is large and the used learning method is complex, whereas when the estimated result is harmful, the increased running time is slight and acceptable.

All in all, the proposed strategy is helpful to guide us to select appropriate classification methods when we encounter imbalanced classification problems, meanwhile, to save running time and reduce the possibility of performan ce degeneration for unharmful skewed classification tasks. 5. Concluding remarks
This paper researched a problem which was ignored in nearly all previous work, i.e., how to estimate whether a skewed classification task suffering from harmful class imbalance or not. We examined the reasons of harmfulness produced by class imbalance and found it is bound up with two key factors: degree of overlapping and class imbalance ratio. Then a simple and ingenious strategy named as Q evaluation criterion using scatter matrix based class separability measure was proposed to estimate the harmfulness of class imbalance. To detect the effectiveness of the proposed Q evaluation criterion, we also presented a novel assessment criterion named as divergence precision to assess the real damage arouse by class imbalance. The experimental results on 12 skewed data sets demonstrated the proposed strategy has low time complexity and can help to select applicable learning algorithm when we encounter imbalanced classification tasks.

We expect that our Q evaluation criterion can be applied to real world applications in future. In addi-tion, we noticed there is still unsuccessful estimated case, e.g., haberman data set, thus future work will further try to develop more effective strategy for evaluating the harmfulness of class imbalance. More-over, considering ubiquitous multiclass imbalanced classification tasks in practical applications, we will investigate the possibility of extending current strategy to multiclass tasks in the future work, too. Acknowledgements This work was partially supported by National Natural Science Foundation of China under grant No. 61105057 and Ph.D Foundation of Jiangsu University of Science and Technology under Grant No.35301002.
 References
