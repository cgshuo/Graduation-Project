 A main characteristics of natural language is that significant portions of content conveyed in a mes-sage may not be overtly realized. This is the case for presuppositions: e.g, the utterance Columbus didn X  X  manage to reach India. presupposes that Columbus had tried to reach India . This presup-position does not need to be stated, but is im-plicitly understood. Determining the presupposi-tions of events reported in texts can be exploited to improve the quality of many natural language processing applications, such as information ex-traction, text understanding, text summarization, question-answering or machine translation.

The phenomenon of presupposition has been throughly investigated by philosophers and lin-guists (i.a. Stalnaker, 1974; van der Sandt, 1992). There are only few attempts for practical imple-mentations of presupposition in computational lin-guistics (e.g. Bos, 2003). Especially, presupposi-tion is understudied in the field of corpus-based learning of semantic relations. Machine learning methods have been previously applied to deter-mine semantic relations such as is-a and part-of , also succession , reaction and production (Pantel and Pennacchiotti, 2006). Chklovski and Pantel (2004) explored classification of fine-grained verb semantic relations, such as similarity , strength , antonymy , enablement and happens-before . For the task of entailment recognition, learning of en-tailment relations was attempted (Pekar, 2008). None of the previous work investigated subclassi-fying semantic relations including presupposition and entailment, two relations that are closely re-lated, but behave differently in context.

In particular, the inferential behaviour of pre-suppositions and entailments crucially differs in special semantic contexts. E.g., while presup-positions are preserved under negation (as in Columbus managed/didn X  X  manage to reach In-dia the presupposition tried to ), entailments do not survive under negation ( John F. Kennedy has been/has not been killed ). Here the entail-ment died only survives in the positive sentence. Such differences are crucial for both analysis and generation-oriented NLP tasks.

This paper presents a weakly supervised al-gorithm for learning presupposition relations be-tween verbs cast as a discriminative classification problem. The structure of the paper is as follows: Section 2 reviews state of the art. Section 3 intro-duces our task and the learning algorithm. Section 4 reports on experiment organization; the results are presented in Section 5. Finally, we summarise and present objectives for future work. One of the existing semantic resources related to our paper is WordNet (Fellbaum, 1998). It com-prises lexical semantic information about English nouns, verbs, adjectives and adverbs. Among the semantic relations defined specifically for verbs are entailment, hyponymy, troponymy, antonymy and cause. However, not all of them are well cov-ered, for example, there are only few entries for presupposition and entailment in WordNet.
One attempt to acquire fine-grained semantic relations from corpora is VerbOcean (Chklovski and Pantel, 2004). Chklovski and Pantel used a semi-automatic approach for extracting semantic relations between verbs using a list of patterns. The selection of the semantic relations was in-spired by WordNet. VerbOcean showed good ac-curacy values for the antonymy (50%), similar-ity (63%) and strength (75%) relations. How-ever, VerbOcean doesn X  X  distinguish between en-tailment and presupposition ; they are conflated in the classes enablement and happens-before .
A distributional method for extracting highly associated verbs was proposed by Lin and Pantel (2001). This method extracts semantically related words with good precision, but it does not deter-mine the type and symmetry of the relation. How-ever, the method is able to recognize the existence of semantic relations holding between verbs and hence can be used as a basis for finding and further discriminating more detailed semantic relations. We describe a weakly supervised approach for learning semantic relations between verbs includ-ing implicit relations such as presupposition. Our aim is to perform a type-based classification of verb pairs. I.e., we determine the class of a verb-pair relation by observing co-occurrences of these verbs in contexts that are indicative for their in-trinsic meaning relation. This task differs from a token-based classification, which aims at classify-ing each verb pair instance as it occurs in context. Classified relations. We distinguish between the five classes of semantic relations presented in Table 1. We chose entailment , temporal inclu-sion and antonymy , because these relations may be confounded with the presupposition relation. A special class other/no comprises semantic rela-tions not discussed in this paper (e.g. synonymy) and verb pairs that are not related by a semantic re-lation. The relations can be subdivided into sym-metric and asymmetric relations, and relations that involve temporal sequence, or those that do not in-volve a temporal order, as displayed in Table 1. A Weakly Supervised Learning Approach. Our algorithm starts with a small number of seed verb pairs selected manually for each relation and iteratively classifies a large set of unseen and un-labeled verb pairs. Each iteration has two phases: 1. Training the Classifiers We independently 2. Ensemble Learning and Ranking Each of Features. Both shallow lexical-syntactic and deep syntactic features are used for the classifica-tion of semantic relations. They include: 1. the distance between two analyzed verbs and 2. verb form (tense, aspect, modality, voice), 3. coordinating/subordinating conjunctions 4. adverbial adjuncts 5. PoS-tag-contexts (two words preceding and 6. the length of the path of grammatical func-7. co-reference relation holding between the
In order to extract these features the training corpus is parsed using a deep parser. Initial Subset of Verb Pair Candidates. Unlike other semi-supervised approaches, we don X  X  use patterns for acquiring new candidates for classi-fication. Candidate verb pairs are obtained from a previously compiled list of highly associated verbs. We use the DIRT Collection (Lin and Pan-tel, 2001) from which we further extract pairs of highly associated verbs as candidates for classifi-cation. The advantage of this resource is that it consists of pairs of verbs which stand in a semantic relation (cf. Section 2). This considerably reduces the number of verb pairs that need to be processed as candidates in our classification task.

DIRT contains 5,604 verb types and 808,764 verb pair types. This still represents a huge num-ber of verb pairs to be processed. We therefore filtered the extracted set by checking verb pair fre-quency in the first three parts of the ukWAC cor-pus (Baroni et al., 2009) (UKWAC 1. . . 3) and by applying the PMI test with threshold 2.0. This re-duces the number of verb pairs to 199,393.

For each semantic relation we select three verb pairs as seeds. The only exception is temporal in-clusion for which we selected six verb pairs, due to the low frequency of such verb pairs within a single sentence. These verb pairs were used for building an initial training corpus of verb pairs in context. The remaining verb pairs are used to build the corpus of unlabeled verb pairs in context in the iterative classification process.
 Preprocessing. Given these verb pairs, we ex-tracted sentences for training and for unlabeled data set from the first three parts of the UKWAC corpus (Baroni et al., 2009). We compiled a set of CQP queries (Evert, 2005) to find sentences that contain both verbs of a verb pair and applied them on UKWAC 1. . . 3 to build the training and un-labeled subcorpora. We filter out sentences with more than 60 words and sentences with a dis-tance between verbs exceeding 20 words. To avoid growing complexity, only sentences with exactly one occurrence of each verb pair are retained. We also remove sentences that trigger wrong candi-dates, in which the auxiliaries have or do appear in a candidate verb pair.

The corpus is parsed using the XLE parser (Crouch et al., 2008). Its output contains both the structural and functional information we need to extract the shallow and deep features used in the classification, and to generate patterns.
 Training Corpus. From this preprocessed cor-pus, we created a training corpus that contains three different components: 1. Manually annotated training set. All sen-2. Automatically annotated training set. We 3. Synonymous verb pairs. To further enrich
The overall size of the training set for the first classification step is 15,717 sentences from which 5,032 are manually labeled, 9,918 sentences are automatically labeled and 757 sentences contain synonymous verb pairs. The distribution is unbal-anced: temporal inclusion e.g. covers only 2%, while entailment covers 39% of sentences. We balanced the training set by undersampling entail-ment and other/no by 20% and correspondingly oversampling the temporal inclusion class.
 Patterns. Similar to other pattern-based ap-proaches we use a set of seed verb pairs to induce indicative patterns for each semantic relation. We use the induced patterns to restrict the number of the verb pair candidates and to rank the labelled instances in the iterative classification step.
The patterns use information about the verb forms of analyzed verb pairs, modal verbs and the polarity verbs (only if they are related to the ana-lyzed verbs) and coordinating/subordinating con-junctions connecting two verbs. The analyzed verbs in the sentence are substituted with V1 and V2 placeholders in the pattern. For example, for the sentence: Here we should be careful for there are those who seek and do not find. and the verb pair (find,seek) we induce the following pattern: V2 and do [ not | n X  X  ] V1 . The patterns are extracted automatically from deep parses of the training cor-pus. Examples of the best patterns we determined for semantic relations are presented in Table 2. Table 2: Patterns for Selected Semantic Relations
Pattern ranks are used to compute the reliabil-ity score for instances, as proposed by Pantel and Pennacchiotti (2006). The pattern reliability is cal-culated as follows: where: pmi ( i, p ) -pointwise mutual information (PMI) between the instance i and the pattern p ; max pmi -maximum PMI between all patterns and all instances; r ( i ) -reliability of an instance i . For seeds r ( i ) = 1 (they are selected manually), for the next iterations the instance reliability is:
We also consider using the patterns as a feature for classification, in case they turn out to be suffi-ciently discriminative.
 Training Binary Classifiers. We independently train 5 binary classifiers, one for each semantic re-lation, using the J48 decision tree algorithm (Wit-ten and Frank, 2005).
 Data Sets. As the primary goal of this paper is to classify semantic relations on the type level, we elaborated a first gold standard dataset for type-based classification. We used a small sample of 100 verb pairs randomly selected from the auto-matically labeled corpus. This sample was man-ually annotated by two judges after we had elim-inated the system annotations in order not to in-fluence the judges X  decisions. The judges had the possibility to select more than one annotation, if necessary. We measured inter-annotator agree-ment was 61% ( k  X  0 . 21 ). The low agreement shows the difficulty of decision in the annotation
While the first gold standard dataset of verb pairs was annotated out of context , we constructed a second gold standard of verb pairs annotated at the token level, i.e. in context. This second data set can be used to evaluate a token-based classi-fier (a task not attempted in the present paper). It also offers a ground truth for type-based classifi-cation, in that it controls for contextual ambiguity effects. I.e., we can extract a type-based gold stan-proposed to one judge to annotate the same 100 verb pair types as in the previous annotation task, this time in context. For this purpose we randomly selected 10 instances for each verb pair type (for rare verb pair types only 5). We compared the gold standards elaborated by the same judge for type-based and token-based classification:  X  62% of verb pair types were annotated with  X  10% of verb pair types were assigned con- X  28% of verb pair types were assigned labels
The figures show that for the most part the type-based annotation conforms with the ground truth obtained from token-based annotation. Only 10% of verb pair types were established as conflicting with the ground truth. The remaining 28% can be considered as potentially correct: either the anno-tated data does not contain the appropriate con-text for a given type label or the type-level anno-tation, performed without context, does not fore-see an existing relation. This points to a general difficulty, namely to acquire representative data sets for token-level annotation, and also to per-form type-level annotations without context for the present task.
 Combining Classifiers in Ensemble Learning. Both token-based and type-based classification starts with determining of the most confident clas-sification for instances. Each instance of the cor-pus of unlabeled verb pairs is classified by the in-dividual binary classifiers. In order to select the most confident classification we compare the votes of the individual classifiers as follows: 1. If an instance is classified by one of the clas-2. If an instance is classified as true by more
In contrast to token-based classification that ac-cepts only one semantic relation, for type-based classification we allow the existence of more than one semantic relation for a verb pair. To avoid the unreliable classifications, we apply several filters: 1. If less than 10% of the instances for a verb 2. If a verb pair is classified as positive for more 3. If a verb pair is classified with up to three se-Iteration and Stopping Criterion. After deter-mining the most confident classification we rank the instances, following the ranking procedure of Pantel and Pennacchiotti (2006). Instances that exceed a reliability threshold (0.3 for our exper-iment) are selected for the extended training set. The remainining instances are returned to the un-labeled set. The algorithm stops if the average re-liability score is smaller than a threshold value. In our paper we concentrate on the first iteration. Ex-tension of the training set and re-ranking of pat-terns will be reported in future work.
Table 3: Accuracy for type-based classification Results for type-based classification. We eval-uate the accuracy of classification based on two alternative measures: 1. Majority -the semantic relation with which 2. Without NONE -as in 1., but after removing
We computed accuracy as the number of verb pairs which were correctly labeled by the system divided by the total number of system labels. We compare our results against a baseline of random assignment, taking the distribution found in the manually labeled gold standard as the underlying verb relation distribution. Table 3 shows the accu-Results for token-based classification. We also evaluate the accuracy of classification for token-based classification as the number of instances which were correctly labeled by the system di-vided by the total number of system labels. As the baseline we took the relation distribution on the token level. Table 4 shows the accuracy results for each semantic relation.
 Discussion. The results obtained for type-based classification are well above the baseline with one exception. The best performance is achieved by antonymy (72% and 42% respectively for both
Table 4: Accuracy for token-based classification measures), followed by temporal inclusion , pre-supposition and entailment . Accuracy scores for token-based classification (excluding NONE) are lower at 29% to 13%. Error analysis of randomly selected false positives shows that the main reason for lower accuracy on the token level is that the context is not always significant enough to deter-mine the correct relation.
 Comparison to Related Work. Other projects such as VerbOcean (Chklovski and Pantel, 2004) report higher accuracy: the average accuracy is 65.5% if at least one tag is correct and 53% for the correct preferred tag. However, we cannot ob-jectively compare the results of VerbOcean to our system because of the difference in the set of re-lation classes and evaluation procedures. Simi-lar to us, Chklovski and Pantel (2004) evaluated VerbOcean using a small sample of data which was presented to two judges for manual evalua-tion. In contrast to our setup, they didn X  X  remove the system annotations from the evaluation data set. Given the difficulty of the classification we suspect that correction of system output relations for establishing a gold standard bears a strong risk in favouring system classifications. The results achieved in our experiment show that weakly supervised methods can be applied for learning presupposition relations between verbs. Our work also shows that they are more difficult to classify than other typical lexical semantic rela-tions, such as antonymy. Error analysis suggests that many errors can be avoided if verbs are dis-ambiguated in context. It would be interesting to test our algorithm with different amounts of man-ually annotated training sets and different combi-nations of manually and automatically annotated training sets to determine the minimal amount of data needed to assure good accuracy.

In future work we will integrate word sense disambiguation as well as information about predicate-argument structure. Also, we are go-ing to analyze the influence of single features on the classification and determining optimal feature sets, as well as the question of including patterns in the feature set. In this paper we used the same combination of features for all classifiers. I would like to thank Anette Frank for supervision of this work, Dekang Lin and Patrick Pantel for sharing the DIRT resource and Carina Silberer and Christine Neupert creation of the gold standard.
