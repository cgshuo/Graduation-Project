 question posed by users [28, 26, 11, 32]. Typically, given a question, an ideal QA system is expected to find answer from certain corpuses (such as the Wide World Web) using information retrieval and natural language processing tech-niques. Despite great progress and encouraging results have been reported, traditional automated QA still faces chal-lenges that are not easy to tackle, such as the deep under-standing of complex questions and the sophisticated syntac-tic, semantic and contextual processing to generate answers. It is found that, in most cases, automated approach cannot obtain results that are as good as those generated by manual processing [2, 12].

Along with the proliferation and improvement of under-lying communication technologies, community question an-swering (cQA) has emerged as an extremely popular al-ternative to finding information online, owning to the fol-lowing facts. First, information seekers are able to post their specific questions on any topic and obtain answers provided by other participants. By leveraging community efforts, they are able to get better answers than simply us-ing search engine to find them. Second, in comparison with automated QA systems, cQA usually receives answers with better quality as they are generated based on human intelli-gence. Third, over times, a tremendous number of QA pairs have been accumulated in their repositories, and it facili-tates the preservation and retrieval of answered questions. The most well-known Internet cQA system is Yahoo! An-swers (Y!A), which contains more than 1 billion QA pairs as at Oct 2009, contributed by the general public.
Despite great success has been achieved, existing cQA fo-rums mostly provide only textual answers, as shown in Fig-ure 1. However, a picture is worth a thousand words. In many cases, the questions cannot be well explained using only texts, and it will be much better to visualize the answers with images and videos. Figure 1 (a) illustrates such an ex-ample: for the question  X  How do you cook beef in gravy  X , the answer is described by several long sentences. However, users still can hardly grasp the process. Clearly, it will be much better if there is an accompanying video describing the process. Therefore, the textual answers in cQA can be significantly enhanced by adding multimedia contents, and it will provide answer seekers with better experience.
Actually in cQA corpuses, there are already many an-swers that directly embed hyperlinks to images or videos from which the users can get supplementary information in media form. For example, for the question  X  What are the best steps to take in order to repent  X , the best answer on Y!A is a URL that leads information seekers to YouTube and duplicate removal to obtain a set of accurate and repre-sentative samples for presentation together with the textual answers.

It is worth mentioning that there already exist several ef-forts dedicated to research on automatically answering ques-tions with multimedia data, i.e., the so-called Multimedia Question Answering (MMQA). For example, Yang et al. [36] proposed to extend text-based QA technology to support factoid QA in news video. A photo-based QA system for finding information about physical objects was presented in [37]. Li et al. [20] explored how to leverage YouTube video collections as a source to automatically find videos to describe cooking techniques. But these approaches usually work on certain narrow domains and can hardly be gener-alized to handle general questions in broad domains. This is due to the fact that, in order to accomplish automatic MMQA, we first need to understand questions, which is not an easy task. Our proposed approach in this work does not aim to directly answer the questions, and instead we enrich the community-contributed answers with multimedia con-tent. Our strategy splits the large gap between question and multimedia answer into two smaller gaps, i.e., the gap between question and textual answer and the gap between textual answer and multimedia answer. In our scheme, the first gap is bridged by the crowd sourcing intelligence of community members, and thus we can focus on solving the second gap. Therefore, our scheme can also be viewed as an approach that accomplishes the MMQA problem by jointly exploring human and computer. Figure 3 demonstrates the difference between the conventional MMQA approaches and an MMQA framework based on our scheme. It is worth noting that, although the proposed approach is automated, we can also further involve human interactions. For exam-ple, our approach can provide a set of candidate images and videos based on textual answers, and answerers then can manually choose several candidates for final presentation.
The contributions of this work can be summarized as fol-lows: (1) To the best of our knowledge, this is the first work on automatically enriching textual answers with image and video data for better QA experience. Different from the con-ventional MMQA research that aims to automatically gener-ate multimedia answers with given questions, our approach is built based on the community-contributed answers, and it can thus deal with more general questions and achieve better performance. (2) We investigate the prediction of appropriate answer medium. Here we want to predict whether a textual answer should or should not be enriched with multimedia informa-tion, and which kind of media data should be added. one can seek advice and opinions [2, 12]. However, nearly all existing cQA systems, such as Yahoo! Answer, Answerbag and Ask Metafilter, only support pure text-based answers, which may not provide intuitive and sufficient information.
Some research works have been conducted on multime-dia QA, which aims to answer questions using multime-dia data. An early system named VideoQA was presented in [36]. This system extends the text based QA technology to support factoid QA in news video by leveraging visual contents of news video as well as the text transcripts. Fol-lowing this work, several video QA systems were proposed and most of them rely on the use of text transcript derived from video OCR (Optical Character Recognition) and ASR (Automatic Speech Recognition) outputs [8, 33, 18, 34]. Li et al. [21] presented a solution on  X  X ow-to X  QA by lever-aging community-contributed texts and videos. Kacmar-cik et al. [17] explored a method of creating a non-text in-put mode for QA that relies on specially annotated virtual photographs. An image-based QA approach was introduced in [37], which mainly focuses on finding information about physical objects. Chua et al. [9] in 2009 proposed an ap-proach to extend text based QA research to multimedia QA to tackle a range of factoid, definition and  X  X ow-to X  QA in a common framework. Their system was designed to find multimedia answers from web-scale media resources such as Flicker and YouTube. However, literature regarding multi-media QA is still relatively sparse. As mentioned in Section 1, automatic multimedia QA only works in specific domains and can hardly handle complex questions. Different from the above mentioned works, our approach is built based on cQA. Instead of directly collecting multimedia data for answering questions, our method only finds images and videos to en-rich the textual answers provided by humans. This makes our approach able to deal with more general questions and achieve better performance.
As introduced in Section 1, the first component of our scheme is answer medium selection, as we first need to de-termine whether we need to and if so which type of medium we should add to enrich the textual answers. For some ques-tions, such as  X  what day is President Obama's birthday  X , us-ing pure textual answers is sufficient. But we need to add image or video information for some other questions. For example, for the question,  X  who is Obama  X , it is better to add images to complement the textual answer, whereas we should add videos for answering the question  X  how to cook beef  X . We regard the answer medium selection as a QA pair classification task, i.e., given a question and textual an-swer, we classify it into the following four classes of answer medium combinations: (1) only text; (2) text + image; (3) text + video; and (4) text + image + video. For the  X  X nly text X  class, we do not need to perform more operations, and for the other cases we will need to collect appropriate data by the other two components.

There are some existing research efforts on question clas-sification. Li and Roth [22] developed a machine learning approach that uses the SNoW learning architecture to clas-sify questions into five coarse classes and 50 finer classes. They used lexical and syntactic features such as part-of-speech tags, chunks and head chunks together with two se-mantic features: named entities and semantically related words to represent the questions. Zhang and Lee [39] used Cat egories Class-Specific Related Word List T ext+Image T ext+Video
T ext+Both T able 2: Representative class-specific related words.  X  X ext+Both X  stands for  X  X ext+Image+Video X .

Here head word is referred to as the word specifying the object that the question seeks.The semantics of head words play an important role in deciding answer medium. For instance, for the question  X  what year did the cold war end  X , the head word is  X  X ear X , based on which we can judge that the sought-after answer is a simple date. Therefore, it is reasonable to use textual answer medium. We adopt the method in [16], but the key difference is that we do not use post fix as it better fit our answer medium classification task.
We also extract a list of class-specific related words in a semi-automatic way. We first estimate the appearing fre-quency of each phrase in the positive samples of each class, and collect all the phrases that have the frequencies above a threshold (we empirically set the threshold to 3 in this work). We then manually refine the list based on human X  X  understanding. Examples of class-specific related words for each class is given in Table 2.
Besides question, the answer with rich text can also be an important information clue. For example, for the ques-tion  X  how do you cook beef in gravy  X , we may find a textual answer as  X  cut it up, put in oven proof dish ...  X . Then we can judge that the question can be better answered with a video clip as the textual answer contains many verbs and describes a dynamic process.

For answer classification, we extract bigram text features and verbs 4 . The verbs in the answer will be useful for judg-ing whether the question can be answered with video con-tent. Intuitively, if a textual answer contains many complex verbs, it is more likely to describe a dynamic process and thus it has high probability to be well answered by videos. Therefore, verb can be an important clue.

Based on the bigram text features and verbs, we again build a Naive Bayes classifier with a set of training data, and then perform a four-class classification with the model. The classification results are linearly combined with those of question-based classification.
A ctually we have investigated other features such as un-igram and visually descriptive nouns. Empirical study demonstrates that the combination of bigram and verbs shows promising performance and good generalization abil-ity.
We perform search using the generated queries to collect image and video data with Google image and video search engines respectively. However, commercial search engines, such as Google, Yahoo and Bing, usually index web images and videos using textual information, such as titles, ALT text and surrounding texts on web pages. But frequently the text information does not fully describe content of the images and videos, and this fact can severely degrade the search relevance of web images and videos. Reranking is an approach to improving search relevance by mining the visual information of images and videos. Existing reranking algo-rithms can mainly be categorized into two approaches, one is pseudo relevance feedback [27, 35, 23] and the other one is graph-based reranking [25, 15, 31]. The pseudo relevance feedback approach regards top results as relevant ones and then collects some samples that are assumed to be irrele-vant. A classification or ranking model is learned based on the pseudo relevant and irrelevant samples and the model is then used to rerank the images. The graph-based reranking approach assumes that the relevant images lie on a manifold in visual feature space. Generally, the approach constructs a graph where the vertices are images or videos and the edges reflect their pairwise similarities. Here we adopt the graph-based reranking method in [15]. We re-state the equation from [15] as, where r j ( k ) stands for the state probability of node j in the k -th round of iterations, is a parameter that satisfies 0 &lt; 1, and W ij is the similarity between the i -th and j -th samples. Here r i (0) is the initial relevance score of the sample at the i -th position, which is heuristically estimated as
For images, we directly estimate their similarity based on their Euclidean distance The parameter is simply set to the median of the Euclidean distance of all image pairs.

For videos, we first perform shot boundary detection and then extract a key-frame from each shot using the method which contain m and n key-frames respectively, we employ average distance [24] of all cross-video key-frame pairs for similarity estimation, i.e., Similarly, the parameter is simply set to the median of the Euclidean distance of all video pairs.

However, a problem with existing reranking methods is that they usually use features extracted from the whole im-ages or video frames and they overlooked that many queries are actually person-related. Clearly, for person-related queries, it is more reasonable to use facial features instead of global visual features for reranking. For question-answering, our T able 5: The performance of answer-based classifi-cation with different features.
 T able 6: Results of linear fusion for answer medium selection.

After duplicate removal, we keep the top 10 images and top 2 videos (keeping which kind of media data depend on the classification results of answer medium selection). When presenting videos, we not only provide videos but also illus-trate the key-frames to help users quickly understand the video content as well as easily browse the videos.
We randomly select 5,000 questions and their correspond-ing answers from the dataset used in [29], which contains 4,483,032 questions and their answers from Y!A. Here we use the best answer that is determined by the asker or the community voting 6 . Inspired by [19, 13], we first classify all the questions in cQA into two categories: conversational and informational. Since conversational questions usually only seek personal opinions or judgments, such as  X  Anybody watch the Bears game last night  X , in this work we only con-sider informational questions. In our work, there are 3333 in-formational questions among the 5000 questions. The ques-tions and answers in our dataset cover a wide range of topics, including travel, life, education, etc.

Our answer medium selection and query selection need training data, and thus we split the 3333 QA pairs into two parts, a training set that contains 2666 QA pairs and a test-ing set of the remaining 667 QA pairs. The classifiers for an-swer medium selection and query selection are trained with the training set and evaluated on the testing set. We first evaluate our answer medium selection approach. The ground truths are established by humans. Five human labelers were involved in the process. For every question, each labeler categorized it into one of the following four classes (text, text+image, text+video, text+image+video), and then voting was performed to obtain the final ground truth. For the cases that there are two classes having the same number of ballots, a discussion is carried out among
Th ere are also many research efforts on ranking community-contributed answers or selecting the best answer by machine learning and NLP technologies [29, 14, 3]. These methods can also been integrated with our work and we only need to change the best answer for each question. the labelers to decide the final ground truths. Table 3 il-lustrates the distribution of the four classes. We can see that, more than 50% of the questions can best be answered by adding multimedia contents instead of using purely text. This also demonstrates that our multimedia answering ap-proach is highly desired.

For the component of medium selection, we first investi-gate different feature combinations for the question and an-swer analysis. The results are illustrated in Tables 4 and 5, respectively. It is worth noting that the stop-words are not removed for question classification since some stop-words also play an important part in question classification. But for answering classification, stop words are removed. Stem-ming is performed for both question and answer. From the results we can see that, for both of the two classifiers, in-tegrating all of the introduced features is better than only part of them.

We linearly fuse the two classifiers with a grid search with optimal weighting. Table 6 illustrates the classifica-tion accuracies when combining question-based classifica-tion and answer-based classification. It can be observed that question-based classification achieves better results than answer-based classification. But fusing the classifiers is able to achieve significantly better performance with a final classifi-cation accuracy of 78 : 29%. Table 7 presents representative examples of classified questions for each category.
Now we evaluate the retrieval effectiveness of our query generation and selection approach. The ground truth was also obtained by the five human labelers. For every QA pair, each labeler selected one from the three queries, which are generated from the question, answer and the combination of question and answer, which will provide best informa-tion to retrieve relevant multimedia answers. These labelers are allowed to perform search on the web to compare the query effectiveness. Voting was performed to obtain the fi-nal ground truth. The distribution of the three classes is illustrated in Table 8.

We adopt SVM with RBF kernel, and the parameters, in-cluding radius parameter and the weighting parameter that modulates the regularizer term and loss term, are estab-lished with 5-fold cross-validation. Table 9 illustrates the classification results. Our approach that integrates POS his-togram and retrieval effectiveness features achieves an accu-racy of 71 : 27%.

In order to evaluate our query-dependent strategy, we first randomly selected 25 queries from the person-related ones. For each query, the top 150 images or videos are collected for reranking. We adopt NDCG@10 as our performance evaluation metric, which is estimated by where rel i is the relevance score of i -th image or video in the ranking list, IDCG is the normalizing factor that equals to the DCG of an ideal ordering. Each image or video is labeled to be very relevant (score 2), relevant (score 1) or irrelevant (score 0) to a query by the voting of the five human labelers.
Figures 4 and 5 illustrate the average performance com-parison of our approach and the conventional method that Fi gure 8: The non-person related image search reranking results for the query  X  X -Box360 X . (a)The top images before reranking; (b) the top images af-ter reranking X X X X Fi gure 9: The person-related image search rerank-ing results for the query  X  X acebook CEO X . (a) The top images before reranking; (b) the top images af-ter reranking uses only global features for the 25 person-related queries. Here we have illustrated the performance with different val-ues of the parameter . Smaller means more original ranking information is kept in reranking. When equals 0, the reranked list will be identical to the original ranking list obtained by the text-based search. We can see that, our ap-proach consistently outperforms the method that uses global features. This demonstrates that it is more reasonable to use facial features for person-related queries.

We then randomly selected 100 queries from image and video class, respectively. Figures 6 and 7 illustrate the average performance comparison. For our query-dependent method, we use global features and facial features for non-person-related and person-related queries respectively. While the conventional method only employ global features re-gardless the query oriented. Our approach significantly out-performs the conventional methods. We can see that, our approach can effectively improve search relevance when varies in a wide range. Throughout our rest experiments, we set as 0.65 and 0.8 for image and video reranking, respectively.

We illustrate the top results before and after reranking for two example queries shown in Figures 8 and 9, one about object and one about person. Figure 8(a) shows the top 14 results before reranking about  X  X -Box360 X , while (b) shows the top 14 results after reranking based on global features. Similarly, Figures 9(a) and (b) presents the top results before and after reranking with respect to person-related query  X  X acebook CEO X . We can see that several less relevant results, marked with  X  X  X , are removed from top positions after reranking.

After reranking we perform duplicate removal and present the images or/and videos together with the textual answers, depending on the results of answer medium selection. Fig-ure 10 shows the multimedia answer for 3 example queries.
Finally, we conduct a user study with 20 volunteers that T able 11: Statistics of the comparison of our multi-media answer and the original textual answer with exclusion of questions where only text-based an-swers are sufficient. frequently use Y!A to evaluate the usability of our appli-cation. The users are asked to freely compare the conven-tional textual answers and our multimedia answers for dif-ferent questions, and provide their ratings of the 2 systems. We adopt the following quantization approach: score 1 is assigned to the worse scheme and the other scheme is as-signed with score 1, 2 and 3 if it is comparable, better and much better than this one, respectively. The average rating scores are illustrated in Figure 11. We can clearly see the preference of users towards the multimedia answering. We also perform an analysis of variance (ANOVA) test. The F-statistic of scheme factor is 18 : 11 and it can be derived that p = 1 : 3154 e 004. This indicates that the difference of the two schemes is significant. The F-statistic of user factor is 1.0 and it can be derived that p &gt; 0 : 5, and this indicates that the difference among users is statistically insignificant.
We then conducted a more detailed study. For each ques-tion in the testing set, we simultaneously demonstrate the conventional best answer and the multimedia answer gen-erated by our approach. Each user is asked to choose the preferred one. Table 10 presents the statistical results. We can see that, in about 45 : 3% of the cases users prefer our answer and only in 3 : 2% of the cases they prefer the orig-inal answers. But there are 51 : 5% neutral cases. This is because there are many questions that are classified to be answered by only texts, and for these questions our answer and the original textual answer are the same. If we exclude such questions, i.e., we only consider questions of which the original answer and our answer are different, then the statis-tics will turn to Table 11. We can see that for more than 82 : 4% of the questions, users will prefer the multimedia an-swers, i.e., the added image or video data are helpful. For cases that users prefer original textual answers, it is mainly due to the irrelevant image or video content. For several questions, the added image and video content are not only irrelevant to the question and answer but also distractive, and this thus degrades users X  experience. In this work, we have proposed a scheme to enrich text QA with media Information. For a given QA pair from cQA, our scheme first predicts which medium is appropriate to enrich the original textual answer. Next, it automati-cally generates a query based on the QA knowledge, and retrieves relevant image and video from search engines. Fi-nally, query-dependent reranking and duplicate removal are performed to obtain a set of images and videos for presen-tation along with the original textual answer.

To our knowledge, this is the first work on enriching tex-tual answer with multimedia information, and there is a lot of future work along this research direction. We will further improve the answer medium selection and query selection performance. We will also investigate methods to boost the relevance of the finally selected images and videos, as irrel-
