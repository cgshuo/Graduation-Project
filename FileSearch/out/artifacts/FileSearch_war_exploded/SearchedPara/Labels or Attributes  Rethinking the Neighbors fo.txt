 Many classification tasks involve linked nodes, such as peo-ple connected by friendship links. For such networks, accu-racy might be increased by including, for each node, the (a) labels or (b) attributes of neighboring nodes as model fea-tures. Recent work has focused on option (a), because early work showed it was more accurate and because option (b) fit poorly with discriminative classifiers. We show, however, that when the network is sparsely labeled,  X  X elational classi-fication X  based on neighbor attributes often has higher accu-racy than  X  X ollective classification X  based on neighbor labels. Moreover, we introduce an efficient method that enables dis-criminative classifiers to be used with neighbor attributes, yielding further accuracy gains. We show that these effects are consistent across a range of datasets, learning choices, and inference algorithms, and that using both neighbor at-tributes and labels often produces the best accuracy. H.2.8 [ Database Management ]: Database Applications X  Data Mining Link-based classification; statistical relational learning; semi-supervised learning; collective inference; social networks
Many problems in communications, social networks, bi-ology, business, etc. involve classifying nodes in a graph. For instance, consider predicting a class label for each page (node) in a set of linked webpages, where some node labels are provided for learning. A traditional method would use the attributes of each page (e.g., words in the page) to pre-dict its label. In contrast, link-based classification [2, 13] also uses, for each node, the attributes or labels of neighboring pages as model features. If  X  X eighbor labels X  are used, then an iterative algorithm for collective inference is needed, since many labels are initially unknown [3]. If, on the other hand,  X  X eighbor attributes X  are used, then a single step of relational inference suffices, since all attribute values are known.
Despite the additional complexity of inference, recent work has used collective inference (CI) much more frequently than relational inference (RI) for two reasons. First, multiple al-gorithms for CI (e.g., belief propagation, Gibbs sampling, ICA) can substantially increase classification accuracy [8, 10]. In contrast, comparisons found RI to be inferior to CI [3] and to sometimes even decrease accuracy compared to methods that ignore links [2]. Second, although RI does not require multiple inference steps, using neighbor attributes as model features is more complex than with neighbor labels, due to the interplay between the larger number of attributes (vs. one label) and a varying number of neighbors for each node. In particular, RI does not naturally mesh with popu-lar, discriminative classifiers such as logistic regression.
Most work on link-based classification assumes a fully-labeled training graph. However, often (e.g., for social and webpage networks) collecting the node attributes and link structure for this graph may be easy, but acquiring the de-sired labels can be much more expensive [11, 6]. In response, recent studies have examined CI methods with partially-labeled training graphs, using some semi-supervised learning (SSL) to leverage the unlabeled portion of the graph [15, 1, 6]. However, because using neighbor attributes seemed dif-ficult and unnecessary, none evaluated RI.

Our contributions are as follows. First, we provide the first evaluation of link-based classification that compares models based on neighbor labels (CI) vs. models based on neighbor attributes (RI), for sparsely-labeled networks. Un-like prior studies with fully-labeled training networks, we find that RI is often significantly more accurate than CI. Second, we introduce an efficient technique, Multi-Neighbor Attribute Classification ( MNAC ), that enables discrimina-tive classifiers like logistic regression to be used with neigh-bor attributes, further increasing accuracy. Finally, we show that the advantages of RI with MNAC remain with a variety of datasets, learning algorithms, and inference algorithms. We find that, surprisingly, RI X  X  gains remain even for data conditions where CI was thought to be clearly preferable [3]. Assume we are given a graph G = ( V, E, X, Y, C ) where V is a set of nodes, E is a set of edges (links), each ~x an attribute vector for a node v i  X  V , each Y i  X  Y is a label variable for v i , and C is the set of possible labels. We are also given a set of  X  X nown X  values Y K for nodes V K  X  V , so that Table 1: Types of models, based on the kinds of features Y
K = { y i | v i  X  V K } . Then the within-network classification task is to infer Y U , the values of Y i for the remaining nodes V U with  X  X nknown X  values ( V U = V \ V K ).

For example, given a (partially-labeled) set of interlinked university webpages, consider the task of predicting whether each page belongs to a professor or a student. There are three kinds of features typically used for this task:
Table 1 characterizes classification models based on the kinds of features they use. The simplest, baseline models use only one kind. First, SelfAttrs uses only self attributes. Second, NeighAttrs classifies a node v using only v  X  X  neigh-bors X  attributes. This model has not been previously been studied, but we use it to help measure the value of neighbor attributes on their own. Finally, NeighLabels uses only neighbor labels. For instance, it might repeatedly average the predicted label distributions of a node X  X  neighbors; this performs surprisingly well for some datasets [5].
 Other models combine self attributes with other features. If a model also uses neighbor attributes, then it is perform-ing  X  X elational inference X  and we call it RI . A CI model uses neighbor labels instead, via features like the  X  X ount Stu-dent s X  described above. However, this is challenging, be-cause some labels are unknown and must be estimated, typ-ically with an iterative process of collective inference (i.e., CI ) [3]. CI methods include Gibbs sampling, belief propa-gation, and ICA (Iterative Classification Algorithm) [10].
We focus on ICA, a simple, popular, and effective algo-rithm [10, 1, 6]. ICA first predicts a label for every node in V
U using only self attributes. It then constructs additional relational features X R using the known and predicted node labels ( Y K and Y U ), and re-predicts labels for V both self attributes and X R . This process of feature com-putation and prediction is repeated, e.g., until convergence.
Finally, RCI uses all three kinds of features. Because it uses neighbor labels, it also must use some kind of collective inference such as ICA.
Some early work on link-based classification evaluated mod-els that included neighbor attributes [2, 13]. However, re-cent work has used such models (including RI and RCI very rarely for two primary reasons. First, prior work found that, while using neighbor labels can increase accuracy, us-ing neighbor attributes can actually decrease accuracy [2]. Figure 1: Accuracy for  X  X ene X  using Naive Bayes and Later, Jensen et al. [3] compared CI vs. RI and RCI . They found that CI somewhat outperformed RCI and performed much better than RI . They describe how neighbor attributes greatly increased the parameter space, leading to lower ac-curacy, especially for small training graphs.

Second, it is unclear how to include neighbor attributes in popular classifiers. In particular, nodes usually have a vary-ing number of neighbors. Thus, with neighbor attributes as features, there is no direct way to represent a node in a fixed-sized feature vector as expected by classifiers such as logistic regression or SVMs. With neighbor labels, CI algo-rithms address this issue with aggregation functions (such as  X  X ount X ) that summarize all neighboring labels into a few feature values. This works well for labels, which are dis-crete and highly informative, but is more challenging for at-tributes, which are more numerous, may be continuous, and are individually less informative than a node X  X  label (thus, this approach fared very poorly in early work [2]). These two factors have produced a prevailing wisdom that CI based on neighbor labels is better than RI based on neigh-bor attributes (cf., [10, 9]). This conclusion rested on studies with fully-labeled training graphs, but has been carried into the important domain [11, 6] of sparsely-labeled graphs. In particular, Table 2 summarizes the models used by the most relevant prior work with such graphs. Only one study [15] used models with neighbor attributes (e.g., with RI or RCI and it did not evaluate whether they were helpful. 1
We next show that this prevailing wisdom was partly cor-rect, but that, for sparsely-labeled networks, neighbor at-tributes are often more useful than previously thought.
To aid our discussion, we first preview our results. Fig-ure 1 plots the average accuracy of RCI , RI , and CI for the Gene dataset (also used by Jensen et al. [3]). The x-axis varies the label density (e.g., fraction of nodes with known labels). When label density is high (  X  40%), CI significantly outperforms RI , and RCI as well when density is very high. High density is similar to learning with a fully-labeled graph, and these results are consistent with those of Jensen et al.
However, when density is low ( &lt; 20%), CI is significantly worse than RI or RCI . To our knowledge, no prior work has reported this effect. Why does it occur?
Prior work with neighbor attributes, including that of Xi-ang &amp; Neville [15], used methods like decision trees or Naive Bayes that do not require a fixed-length feature vector. F irst, during inference, many neighbor labels are unknown. Thus, a potential disadvantage of CI vs. RI is that some pre-dicted labels used by CI will be incorrect, while the neighbor attributes used by RI are all known. However, prior work shows that CI  X  X hen learning uses a fully-labeled graph X  can be effective even when all labels in a separate test graph are initially unknown [8, 7]. Thus, having a large number of unknown labels during CI  X  X  inference , while a drawback, is not enough to explain the substantial differences vs. RI .
The key problem is that, at low label density, CI strug-gles to learn the parameters related to label-based features. Labels can be used for learning such features only where both nodes of a link have known labels. In contrast, with neighbor attributes a single node X  X  label makes a link useful for learning, since the node X  X  neighbors X  attributes are all known. Thus, with RI a single labeled node can provide multiple examples for learning (one for each of its links).
For example, for the Gene dataset of Figure 1, when the density is 10% (110 known nodes), CI can learn from an average of only 20 links, while RI can use an average of 340. Thus, for sparsely-labeled networks, the effective training size for features based on neighbor attributes is much larger than for those based on neighbor labels. This compensates for the greater number of parameters required by neighbor attributes, leading to higher accuracy.

Recent work with partially-labeled networks has also ob-served these problems with neighbor labels, but did not con-sider neighbor attributes. Instead, they avoid the problem by discarding methods based on label features and use la-tent features and/or links [12, 11]. Others have proposed non-learning methods that use label propagation or random walks [5]. Yet others have proposed SSL variants to first predict (noisy) labels for the entire network [15, 1, 6].
We later compare against some of these methods (e.g., with [5]). However, Figure 1 X  X  results already include some effective SSL methods [1, 6] (see Section 5.3), and yet sub-stantial problems remain for CI at low density. Section 6.2 compares RCI , RI , and CI more completely and shows that neighbor attributes are helpful with or without SSL.
This section explains existing methods for predicting with neighbor attributes, then introduces a new technique that enables the use of discriminative classifiers.
Let N i be the set of nodes that are adjacent to node v i i.e., in the  X  X eighborhood X  of v i (for simplicity, we assume undirected links here). Furthermore, let X N i be the set of attribute vectors for all nodes in N i ( X N i = { ~x j | v
Suppose we wish to predict the label y i for v i based on its attributes and the attributes of N i . As described above, the variable size of N i presents a challenge. To address this gen-eral issue, prior studies (with neighbor labels) often assume that the labels of nodes in N i are conditionally independent given y i . This assumption is not necessarily true, but often works well in practice [3, 8, 7]. In our context (with neigh-bor attributes), we can make the analogous assumption that the attribute vectors of the nodes in N i (and the attribute vector ~x i of v i itself) are conditionally independent given y Using Bayes rule and this assumption yields where the last step drops all values independent of y i .
To use Equation 1, we must compute p ( ~x i | y i ) and p ( ~x The same technique works for both; we now explain for the latter. We further assume that all attribute values for v (e.g., the values inside ~x j ) are independent given y i have M attributes, then
Plugging this equation (and the equivalent one for p ( ~x into Equation 1 yields a (relational) Naive Bayes classi-fier [8]. In particular, the features used to predict the label for v i are v i  X  X  attributes and v i  X  X  neighbors X  attributes, and these values are assumed to be conditionally independent. Jensen et al. [3] used this simple generative classifier for RI , and a simple extension to add the labels of neighboring nodes as features yields the equations needed for RCI .
The method described above can predict with neighbor at-tributes, and can increase classification accuracy, as we show later. However, it has two potential shortcomings. First, it ignores dependencies among the attributes within a single node. Second, it requires using probabilities like p ( ~x whereas a discriminative classifier (e.g., logistic regression) would compute p ( y i | ~x j ). Thus, the set of classifiers that can be used is constrained, and overall accuracy may suffer.
A new idea is to take Equation 1 and apply Bayes rule to each conditional probability separately. This yields p ( y i | ~x i , X N i )  X  p ( y i ) p ( y i where the last step drops all values independent of y i . We call classification based on Equation 3 Multi-Neighbor Attribute Classification ( MNAC ). This approach requires two conditional models, p ( y i | ~x i ) and p ( y i | ~x a standard (self)attribute-only classifier, while the second is more unusual: a classifier that predicts the label of a node based on the attributes of one of its neighbors. Because the prediction for this latter model is based on just a single attribute vector, any probabilistic classifier can be used, in-cluding discriminative classifiers such as logistic regression.
MNAC  X  X  derivation is simple, but it has not been previ-ously used for link-based classification. McDowell &amp; Aha [6] used a somewhat similar technique to produce  X  X ybrid mod-els X  that combine two classifiers, but the derivation is differ-ent and they did not consider neighbor attributes.
We use all of the real datasets used in prior studies with semi-supervised ICA, and some synthetic data (see Tables 2 &amp; 3). We removed all nodes with no links.
 Cora (cf., [10]) is a collection of machine learning papers. Citeseer (cf., [10]) is a collection of research papers. At-tributes represent the presence of certain words, and links indicate citations. We mimic Bilgic et al. [1] by ignoring link direction, and also by using the 100 top attribute features after applying PCA to all nodes X  attributes.

Gene (cf., [3]) describes the yeast genome at the protein level; links represent protein interactions. We mimic Xiang &amp; Neville [15] and predict protein localization using four attributes: Phenotype, Class, Essential, and Chromosome. When using LR, we binarized these, yielding 54 attributes.
We create synthetic data using Sen et al. X  X  graph generator [10]. Degree of homophily is how likely a node is to link to another node with the same label; we use their defaults of 0.7 with a link density of 0 . 2. Each node has ten binary attributes with an attribute predictiveness of 0 . 6 (see [7]).
All models shown in Table 1 (except NeighLabels ) re-quire learning a classifier to predict the label based on self attributes and/or a classifier to predict based on neighbor attributes. We evaluate Naive Bayes (NB), because of its past use with neighbor attributes [3], and logistic regres-sion (LR), because it usually outperformed NB [10, 1]. For neighbor attributes, LR uses the new MNAC method.

RCI and CI also require a classifier to predict based on neighbor labels. McDowell &amp; Aha [6] found that NB with  X  X ultiset X  features was superior to LR with  X  X roportion X  fea-tures as used by Bilgic et al. [1]. Thus, we use NB for neigh-bor labels, and combine these results with the NB or LR classifiers used for attributes (described above), using the  X  X ybrid model X  method mentioned in Section 4.2 [6].
For sparsely-labeled data, regularization can have a large impact on accuracy. We used five-fold cross-validation on the labeled data, selecting the value of the regularization hyperparameter that maximized accuracy on the held-out labeled data. We used a Gaussian prior with all LR X  X  fea-tures, a Dirichlet prior with NB X  X  discrete features, and a Normal-Gamma prior with NB X  X  continuous features.
We chose default learning and inference algorithms that performed well in prior studies; Section 6.2 considers others.
For learning, we use the SSL-Once strategy: first, learn the classifiers using the attributes and the known labels. Next, run inference to predict labels for every  X  X nknown X  node. Finally, learn new classifiers, using the attributes, known labels, and newly predicted labels. With LR, we also use  X  X abel regularization X  [6] which biases the learning to-wards models that yield sensible label distributions (it does not apply to NB). McDowell &amp; Aha [6] found that these choices performed well overall and had consistent accuracy gains compared to not using SSL. The three baseline models (see Table 1) do not use SSL or label regularization. For inference, CI and RCI use 10 iterations of ICA. For NeighLabels , we use a common baseline based on relax-ation labeling (wvRN+RL [5], see Section 2).
We report accuracy averaged over 20 trials. For each, we randomly select some fraction of V (the  X  X abel density X  d ) to be  X  X nown X  nodes V K ; those remaining form the  X  X nknown label X  test set V U . We focus on the important sparsely-labeled case [11, 6], e.g., d  X  10%. To assess results, we use paired t-tests with a 5% significance level, with appropriate corrections because the 20 test sets are not disjoint [14].
Figure 2 shows average classification accuracy for the real datasets. The solid lines show results with RCI , RI , or using NB as the attribute classifier. The results for Gene were already discussed in Section 3, and match closely the trends for Cora and Citeseer. In particular, for high label density d , CI (and sometimes RCI ) performs best; but, when labels are sparse, using neighbor attributes (with RCI or is essential. When d &lt; 10%, the gains of RCI and RI vs. are, with one exception, all significant (see caption). Results without SSL-Once (not shown) showed very similar trends.
With LR (see Table 4 for partial details), the trends are very similar: CI is significantly better than RI for high d , but the opposite holds for low d . Fortunately, RCI (using neighbor labels and attributes) yields good relative perfor-mance regardless of d , as was true with NB.

For comparison, the dashed lines in Figure 2 show results for RCI with LR (using MNAC ). For Cora and Citeseer, this method beats RCI with NB in all cases. The gains are sub-stantial (usually 8-10%) for very sparse graphs, and smaller but still significant for higher d . Here, LR outperforms NB because the attributes are continuous (a challenging scenario for NB) and because LR does not assume that the (many) attributes are independent. For Gene, however, NB is gen-erally better, likely because NB can use the 4 discrete at-tributes, whereas LR must use the 54 binarized attributes.
Overall, for sparsely-labeled graphs, neighbor attributes appear to be much more useful than previously recognized, and our new LR with MNAC sometimes significantly in-creases accuracy. For simplicity, below we only consider LR.
The results above all used SSL-Once for learning and, where needed, ICA for collective inference. Table 4 examines results where we vary these choices. First, there are different learning variants: No-SSL , where SSL is not used (though label regularization still is), and SSL-EM , where SSL-Once is repeated 10 times, as with McDowell &amp; Aha [6]. Second, for RCI and CI we consider Gibbs sampling instead of ICA. Gibbs has been frequently used, including in the RI vs. CI comparisons of Jensen et al. [3], but sometimes has erratic behavior [7].
 These choices can impact accuracy. For instance, SSL-Once+Gibbs sometimes beats SSL-Once with ICA, but always by less than 1%, and decreases accuracy substantially in several cases with CI . Using SSL-EM instead of SSL-Once leads to more consistent gains that are sometimes substantial, especially for CI .

Thus, for different datasets and classifiers, the best SSL and inference methods will vary. However, our default use of ICA with SSL-Once was rarely far from maximal when RCI , the best model, was used. Moreover, the values in bold show that, regardless of the learning and inference choices, RCI or RI yielded the best overall accuracy (at least for the data of Table 4, which focuses on sparse networks). Also, us-ing RCI or RI with SSL-Once almost always outperformed CI with any learning/inference combination shown.

For comparison, the bottom of Table 4 also shows results with three baseline algorithms. NeighAttrs performs best on average, indicating the predictive value of neighbor at-tributes, even when used alone. We compared RCI , RI , and CI on the synthetic data. First, varying the label density produced results (not shown) very similar to Gene in Figure 2. Second, in Figure 3(a), as homophily increases, both RI and CI improve by exploiting greater link-based correlations. RCI does even better by using both kinds of link features, except at low homophily where using the (uninformative) links decreases accuracy.
Figure 3(b) shows the impact of adding some random at-tributes for each node. We expected CI  X  X  relative perfor-mance to improve as these were added, based on the results and argument of Jensen et al. [3]: CI has fewer parame-ters than RI , and thus should suffer less from high variance due to the random attributes. Instead, we found that RI  X  X  (and RCI  X  X ) gains over CI only increase, for two reasons. Figure 3: Synthetic data results, using LR and 5% label First, Jensen et al. had fully-labeled training data; for our sparse setting, RI has a larger effective training size (see Sec-tion 3), reducing variance. Second, unlike Jensen et al. we use cross-validation to select regularization parameters for the features. The regularization reduces variance for RI CI , and is especially helpful for RI as random attributes are added. If we remove regularization and increase label den-sity, the differences between CI and RI decrease markedly.
Due to lack of space, we can only summarize two addi-tional experiments. First, we evaluated  X  X oft ICA X , where continuous label estimates are retained and used at each step of ICA instead of choosing the most likely label for each node [15]. This slightly increased accuracy in some cases, but did not change the relative performance of RCI and RI vs. CI . Second, we compared our results (using a single partially-labeled graph) vs.  X  X deal X  results obtained using a fully-labeled learning graph or a fully-labeled infer-ence graph. We found that ideal learning influenced results much more than ideal inference for all methods, but that CI was (negatively) affected by realistic (non-ideal) learn-ing much more than RI or RCI were. Thus, as argued in Section 3, RI  X  X  and RCI  X  X  gains vs. CI for sparsely-labeled networks seem to arise more because of CI  X  X  difficulties with learning rather than with inference.
Link-based classification is an important task, for which the most common methods involve computing relational fea-tures. Almost no recent work has considered neighbor at-tributes for such features, because prior work showed they performed poorly and they were not compatible with many classifiers. We showed, however, that for sparsely-labeled graphs using neighbor attributes (with RI ) often signifi-cantly outperformed neighbor labels (with CI ), and that us-ing both (with RCI ) yielded high accuracy regardless of label density. We also introduced a new method, MNAC , which enables classifiers like logistic regression to use neighbor at-tributes, further increasing accuracy for some datasets. Nat-urally, the best classifier depends upon data characteristics; MNAC greatly expands the set of possibilities.

Our findings should encourage future researchers to con-sider neighbor attributes in models for link-based classifica-tion, to include RI and RCI as baselines in comparisons, and to re-evaluate some earlier work that did not consider such models (see Table 2). For instance, Bilgic et al. [1] studied active learning with sparse graphs; their optimal strategies could be quite different if RI or RCI were con-sidered, since they could tolerate learning with fewer and more widely-dispersed labels. Likewise, Shi et al. [11] used semi-supervised ICA with CI as a baseline, but these results could change substantially with RCI instead.

Our results need to be confirmed with additional datasets and learning algorithms. Also, the best methods should be compared against others discussed in Section 2. Finally, we intend to explore these effects in networks that include nodes with different types and some missing attribute values.
Thanks to Gavin Taylor and the anonymous referees for comments that aided this work. This work was supported in part by NSF award number 1116439 and a grant from ONR. [1] M. Bilgic, L. Mihalkova, and L. Getoor. Active [2] S. Chakrabarti, B. Dom, and P. Indyk. Enhanced [3] D. Jensen, J. Neville, and B. Gallagher. Why [4] Q. Lu and L. Getoor. Link-based classification using [5] S. Macskassy and F. Provost. Classification in [6] L. McDowell and D. Aha. Semi-supervised collective [7] L. McDowell, K. Gupta, and D. Aha. Cautious [8] J. Neville and D. Jensen. Relational dependency [9] R. Rossi, L. McDowell, D. Aha, and J. Neville. [10] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Gallagher, [11] X. Shi, Y. Li, and P. Yu. Collective prediction with [12] L. Tang and H. Liu. Relational learning via latent [13] B. Taskar, E. Segal, and D. Koller. Probabilistic [14] T. Wang, J. Neville, B. Gallagher, and T. Eliassi-Rad. [15] R. Xiang and J. Neville. Pseudolikelihood EM for
