 Manik Varma manik@microsoft.com Microsoft Research India, Bangalore, India 560 001 Real world applications often require efficient predic-tion. Non-linear SVMs have prediction costs that are proportional to the number of support vectors and these can grow linearly with the size of the training set. Thus, while non-linear SVMs have defined the state-of-the-art in terms of prediction accuracy on mul-tiple benchmark tasks, their use in computationally intensive real world applications remains limited. The problem is getting exacerbated as large quantities of training data is becoming readily available in many domains.
 Our objective is to ameliorate the situation by re-ducing the cost of non-linear SVM prediction while maintaining classification accuracy above an accept-able threshold. Speeding up SVM prediction is an im-portant research problem which has been approached from multiple perspectives including approximating the kernel function or matrix (B  X az  X avan et al., 2012; Kar &amp; Karnick, 2012; Maji et al., 2013; Rahimi &amp; Recht, 2007; 2008; Vedaldi &amp; Zisserman, 2011; 2012; Williams &amp; Seeger, 2001; Yang et al., 2012), reduc-ing the number of support vectors (Cossalter et al., 2011; Joachims &amp; Yu, 2009; Keerthi et al., 2006) and directly formulating SVM variants with low prediction costs (Ladicky &amp; Torr, 2011).
 We take a kernel learning based approach in this pa-per. The objective in kernel learning is to jointly learn both kernel and SVM parameters. In particular, Lo-calized Multiple Kernel Learning (LMKL) (Gonen &amp; Alpaydin, 2008) aims to learn a different kernel, and hence classifier, for each point in feature space (Ong et al., 2005; Tsang &amp; Kwok, 2006). It has never been considered from the perspective of speeding up SVM prediction since it achieves only a modest reduction in the number of support vectors on average. In this pa-per, we generalize LMKL to learn arbitrary local fea-ture embeddings that go beyond LMKL X  X  non-negative gating functions. We learn local embeddings that are high dimensional, sparse and computationally deep 1 . This enables efficient prediction using primal variables and thereby decouples prediction costs from the num-ber of support vectors and the size of the training set. In our proposed Local Deep Kernel Learning (LDKL) formulation, a composite non-linear kernel product of a local kernel K L and a global kernel K
G (note that the product of two Mercer kernels is also a Mercer kernel). This induces a high dimen-sional feature space embedding of the form  X  ( x ) =  X 
L ( x )  X   X  G ( x ) where  X  L and  X  G are the local and global feature embeddings induced by K L and K G re-spectively and  X  is the Kronecker product. We choose  X 
G to be low dimensional and  X  L to be high dimen-sional and sparse so that classification can be carried out using primal variables as y ( x ) = sign ( w t  X  ( x )) = sign (  X  t L ( x ) W t  X  G ( x )) where the vector w has been re-shaped to the matrix W and t denotes the transpose operator. To make prediction efficient,  X  L is chosen to be tree-structured. Each dimension of  X  L corresponds to a node in a tree and only those dimensions of  X  L ( x ) are non-zero which correspond to the path traversed by x from the root to one of the leaves. Thus, for any x ,  X 
L ( x ) has only log M non-zero dimensions, which can be identified and computed in O (log M ) time, where M is the dimensionality of  X  L . This results in an exponential speed up in prediction time with only a marginal reduction in classification accuracy in most cases. The learnt LDKL kernel also has a simple in-tuitive interpretation. It modulates an overall global similarity between pairs of points by a local similarity which is proportional to the length of the path shared by the two points in the learnt LDKL tree and which is maximal when the two points are in the same local region of space as defined by their common leaf node. Most kernel learning formulations, including Local-ized Multiple Kernel Learning, are optimized using SVM dual parameters with the notable exception of (Orabona et al., 2010; Orabona &amp; Jie, 2011). How-ever, it is more attractive to optimize our LDKL for-mulation using primal stochastic sub-gradient descent since primal predictions are efficient in our case and we do not have to worry about maintaining dual variables or dual sparsity.
 Note that optimizing over the space of trees is a hard non-convex problem. The loss incurred by a train-ing point depends on its embedding which, in turn, depends on the path traversed by the point in  X  L . For instance, parameterizing node k by  X  k ,  X  L which evaluates to unity only if node k lies on the path traversed by x i and is zero otherwise. Modifying  X  k changes the path traversed by each training point and this makes the objective function sharply discon-tinuous. In order to make tree learning amenable to sub-gradient descent,  X  L is relaxed by replacing the signum function with the continuous tanh. However, this has the potential drawback that many entries in  X  L might become non-zero and sparsity might be lost. We tackled this problem by introducing, and adap-tively tuning, a scale parameter within the tanh func-tions during the optimization so as to make them tend to signum functions as convergence was approached. This ensured that, by and large, only a single path was dominant within the tree.
 Experiments on benchmark data sets revealed that LDKL could significantly bring down prediction costs as compared to RBF-SVMs while maintaining classi-fication accuracy above an acceptable threshold. The experiments also revealed that LDKL could yield bet-ter classification accuracies as compared to state-of-the-art methods for speeding up SVM prediction. For instance, on the challenging CoverType data set, an RBF-SVM took days to train and yielded a classifi-cation accuracy of 91 . 21% with a prediction cost that was 1 . 37  X  10 5 times that of a linear SVM. If prediction costs were restricted to 33 times that of a linear SVM, popular methods such as the Random Fourier Features (RFF) (Rahimi &amp; Recht, 2007) yielded a classification accuracy of 58%. LDKL X  X  accuracy was 88 . 21% for the same prediction cost. Thus LDKL could improve clas-sification accuracy by 30% over RFF and could speed up RBF-SVMs by more than 4000 times with a toler-able loss in accuracy. Furthermore, LDKL took only hours to train and required less than 1 Mb of RAM to store its model parameters. This opens up the pos-sibility of learning accurate non-linear SVMs on large data sets beyond the scope of RBF kernels.
 Our main contribution in this paper is to formulate the problem from a local kernel learning perspective where we learn tree-structured features. Competing approaches to speeding up SVM prediction mainly fol-low the kernel approximation paradigm. These ap-proaches suffer from the fact that the kernel is not ap-proximated keeping the task and training set in mind. Thus, modelling power is wasted in learning good ker-nel approximations even far away from the decision boundary. On the other hand, learning a kernel which varies in feature space helps LDKL efficiently encode non-linearities into the classification model which is ex-plicitly trained for the given task and training set. The focus is very much on learning the decision boundary as points that do not violate the margin play no role in the optimization. Furthermore, as the complexity of the learning task grows, LDKL can keep increasing the size of the embedding feature space while incurring only logarithmic prediction costs. This enables LDKL to achieve an exponential speed up as compared to other leading methods. Our optimization ensures that training is efficient and that LDKL can scale to large data sets where the most benefits are to be gained. The LDKL source code can be downloaded from (Jose et al., 2013). Low rank kernel approximation techniques have been specially developed for translation invariant ker-nels (Rahimi &amp; Recht, 2007; 2008), dot product ker-nels (Kar &amp; Karnick, 2012) and homogenous and ad-ditive kernels (Maji et al., 2013; Vedaldi &amp; Zisserman, 2011). These techniques lead to a compact linear rep-resentation of the kernel taking O ( DM ) time to em-bed D dimensional input features into an M dimen-sional space. The main limitation of these techniques is that they do not leverage knowledge about the task or training data set. Thus, modelling power is wasted in designing good kernel approximations even far away from the decision boundary. This is somewhat ame-liorated by Nystr  X om approximations (Vedaldi &amp; Zis-serman, 2012; Williams &amp; Seeger, 2001; Yang et al., 2012) which are cognizant of the feature distribution and come at a higher prediction cost. However, since Nystr  X om methods do not factor training labels into their approximation, they are still oblivious to the de-cision boundary. Finally, (B  X az  X avan et al., 2012) have proposed learning some of the parameters in transla-tion invariant kernel approximations such as the RBF bandwidths in Random Fourier Features. While the type of embedding is fixed a priori , the parameters are learnt from training data and therefore improve performance over the base Random Fourier Features. In contrast, our proposed LDKL approach computes the embedding in O ( D log M ) time thereby gaining an exponential speed up over the kernel approxima-tion techniques. Furthermore, the embedding is learnt from training data by directly minimizing the chosen loss and this can lead to a significant improvement in classification accuracy as shown in our experiments. Methods designed to decrease the number of support vectors can potentially focus on the decision boundary. Post-processing based reduced set methods (Burges &amp; Sch  X olkopf, 1997; Cossalter et al., 2011) suffer from the limitation that the original set of support vec-tors needs to be determined before it can be reduced. This can become costly, and even infeasible for modern data sets, as non-linear SVM training typically scales quadratically in the number of training instances. This problem was addressed in (Keerthi et al., 2006; Tsang et al., 2005; 2007) where the number of support vectors was decreased during the training stage itself. This ap-proach was extended in the Cutting-Plane Subspace Pursuit (CPSP) algorithm (Joachims &amp; Yu, 2009) where support vectors were generated from outside the training set. Finally, LLSVM (Ladicky &amp; Torr, 2011) directly formulates an SVM variant with low predic-tion cost. It can be interpreted as a special case of LDKL with a hand crafted, rather than learnt, local feature embedding without the tree structure. We em-pirically compare LDKL to LMKL, LLSVM and CPSP and demonstrate that LDKL can learn a significantly more accurate classifier for a given prediction cost. Multiple kernel learning aims to learn the kernel from training data and many formulations, kernel parame-terizations and regularizers have been proposed (Aflalo et al., 2011; Bach, 2008; Chapelle et al., 2002; Chen et al., 2008; Cortes et al., 2009a;b; Jain et al., 2012; Kloft et al., 2009; Lanckriet et al., 2004; Rakotoma-monjy et al., 2008; Sindhwani &amp; Lozano, 2011; Sonnen-burg et al., 2006; Vishwanathan et al., 2010; Ye et al., 2008). Recent techniques, such as (B  X az  X avan et al., 2012), have applied standard convex MKL formula-tions to learn the RBF bandwidths in approximate Random Fourier Features (Rahimi &amp; Recht, 2007). In contrast, LDKL does not aim to approximate any given kernel but focusses on learning the best decision boundary in a sparse, high dimensional representation. The literature on learning trees is vast and includes methods for learning trees in feature space (decision trees and random forests), trees in label space (hier-archies, ontologies and structured output prediction) and trees for efficient retrieval (KD Trees, Ball Trees, etc. ). Our approach should not be confused with these methods, nor with tree kernels which determine the similarity of two given trees rather than learn a tree structured feature embedding. Our problem setting and approach should also not be confused with kernel methods for deep learning as these do not focus on efficient SVM prediction. For instance, (Cho &amp; Saul, 2009) design kernels which mimic the computation in multi-layer networks while (Vinyals et al., 2012) learn a deep classifier with layers of linear SVMs. The LMKL formulation (Gonen &amp; Alpaydin, 2008) learns a prediction function of the form where p ( w k | x ) is a non-negative gating function which can be interpreted as the probability of picking clas-sifier w k for a given point x . Thus, a different com-bination of classifiers w k and features  X  k is selected for each x . The gating functions are parameterized by and SVM parameters are jointly optimized using the following non-convex formulation where K  X  ( x i , x j ) = P k p ( w k | x i ) K k ( x i , x The optimization is carried out as a nested two stage procedure (Chapelle et al., 2002; Rakotomamonjy et al., 2008; Jain et al., 2012). In the outer loop, the kernel parameters are optimized using gradient descent while, in the inner loop, the kernel parameters are held fixed and the SVM parameters are learnt using a sin-gle kernel SVM training algorithm. The experiments in (Gonen &amp; Alpaydin, 2008) demonstrated that for different K k including linear, polynomial and RBF ker-nels, LMKL could learn a classifier that was compet-itive with a single kernel RBF-SVM but with slightly fewer support vectors resulting in a minor speed up over the RBF-SVM. It was also demonstrated that combining multiple linear kernels could improve clas-sification accuracy over that of a linear SVM. The Local Deep Kernel Learning formulation learns a non-linear kernel K ( x i , x j ) = K L ( x i , x j ) K G the product of a local kernel K L =  X  t L  X  L and a global kernel K G =  X  t G  X  G leading to the prediction function y ( x ) = sign ( X where w k = P i  X  i y i  X  L mension k of  X  L  X  R M , W = [ w 1 . . . w M ], W ( x ) = W  X  L ( x ) and  X  is the Kronecker product. Thus, LDKL can be thought of as either learning a single fixed linear classifier in  X  G  X   X  L space or a differ-ent classifier for each point in the given global feature space  X  G . Our prediction function in (6) is similar to LMKL X  X  prediction function in (1) and generalizes it since our local features  X  L can be arbitrary and are not restricted to being non-negative. Note that, if multiple heterogeneous global features needed to be combined as in LMKL, LDKL could easily be extended to learn K = P m K L added power of learning a forest of local embeddings. However, in this paper, we focus on the problem of speeding up SVMs given a single global feature. Primal based classification using W decouples predic-tion costs from the number of support vectors and the size of the training set. The global feature embedding should be chosen to be low dimensional, such as lin-ear  X  G ( x ) = x  X  R D or quadratic  X  G ( x ) = x  X  x , in order to make prediction efficient. While the LDKL formulation is general, we report experimental results only for  X  G ( x ) = x in this paper.
 Having fixed the global features to be linear, non-linearities in LDKL are introduced through the lo-cal features. LDKL would reduce to LMKL if we set  X  L LLSVM (Ladicky &amp; Torr, 2011) if the embedding was fixed and not learnt. Instead,  X  L is chosen to be high dimensional, sparse and tree-structured. Each dimen-sion k of  X  L can be thought of as a node in a tree. For a given x , only those dimensions of  X  L ( x ) are non-zero for which the corresponding node lies on the path traversed by x from the root to the leaf. Thus,  X  which is unity only if node k lies on the path traversed by x and zero otherwise
I k ( x ) = Y with C ( l ) being zero if node l is its parent X  X  left child and 1 if it is its parent X  X  right child. Thus, if  X  L has dimensionality M , then only log M dimensions will be non-zero and these can be identified in log M time. Deep representations are typically obtained by func-tion composition. A possible choice of  X  L where k r indexes the r th ancestor of k and f is any smooth non-linear function. LDKL would generate a continuous, piecewise smooth decision boundary if f k all its descendants would also tend to zero. Thus as the decision function transitions from one leaf node to another by switching paths at node k and crossing the leaf node cell boundary defined by  X  t k x = 0, the contributions of all the descendants of node k would smoothly diminish and the decision function would be determined by node k and its ancestors. LDKL could be further generalized if f was no longer restricted to functions of  X  t x but could be any arbitrary function of x . While such a representation yielded much bet-ter classification accuracies than the state of the art, the best results were obtained by a non-compositional formulation with where the parameter set has been augmented with  X  0 = [  X  0 1 , . . . ,  X  0 M ] and  X  is a scaling parameter that could be set via validation.
 LDKL X  X  overall prediction time using (6) is there-fore O ( D log M ) where D is the dimensionality of x . This compares favourably, and leads to an exponen-tial speed up, over all competing methods whose costs are at least O ( DM ). The LDKL kernel K = K L K G also has an intuitive interpretation. Two points are most similar if they belong to the same local region in space as defined by a given leaf node of  X  L . As one of the points moves away from the region to another leaf node, the similarity decreases depending on the num-ber of common ancestors in  X  L . This local similarity is modulated by an overall global linear similarity. The LDKL primal for jointly learning  X  and W from training data { ( x i , y i ) N i =1 } can be formulated as min + Our focus in this paper is on the hinge loss for binary classification where L = max (0 , 1  X  y i  X  t L ( x i ) W t x though other appropriate loss functions for multi-class and multi-label classification, regression and novelty detection could also be plugged in. Note that LDKL does not include an explicit bias term b but instead adds a constant dimension to x whose value is deter-mined through validation. LDKL is optimized via primal stochastic sub-gradient descent. At iteration j , a training point x i is picked at random and W and  X  are updated as where  X  j is the step size at iteration j and where the gradient expressions have been derived for the hinge loss for binary classification, assuming  X  L is non-compositional as in (10) and  X  i is unity if x i is a margin violator and zero otherwise.
 Optimization over the space of trees is a hard non-convex problem. In this paper, we choose the tree structure to be fully balanced (though the tree struc-ture could also have been learnt by introducing l 1 reg-ularized node selection weights which ensure that all child node weights go to zero if a parent X  X  weight goes to zero). The task then boils down to learning the in-dicator function I k for each node k in the tree. Note that I k is sharply discontinuous with respect to the tree parameters  X . Thus, to make optimization via sub-gradient descent tractable, I k is relaxed to  X   X   X  However, this also implies that, for a given x , I k ( x ) might be non-zero for many k . This destroys the sparsity in  X  L and the cost of prediction no longer remains logarithmic. Furthermore, optimization re-mains hard, since the gradient needs to be propagated back from the root to multiple leaf nodes. We tackled this problem by introducing, and adaptively tuning, a scale parameter s I within the tanh functions. Ini-tially, s I was set to a small value so as to ensure that most of the tanh relaxations were not saturated. As optimization progressed, s I was adaptively increased so that tanh( s i  X  t k x ) had tended to sign (  X  t k x ) by the time convergence was approached. This was found to significantly speed up convergence and allowed us to efficiently scale to training sets with more than half a million data points.
 Note that most MKL formulations, including LMKL, are optimized using SVM dual parameters. However, it is advantageous to optimize LDKL in the primal using stochastic sub-gradient descent for three rea-sons. First, primal based prediction using W is signif-icantly more efficient in LDKL than dual based pre-diction using  X  . One could potentially consider dual co-ordinate ascent techniques which also maintain W such as (Hsieh et al., 2008), but then one runs into Banana 1,000 2 55.49 90.21 322.12 88.67  X  0.43 4.52  X  0.05 71x the second problem. A co-ordinate or gradient step in  X  would change K  X  and dual based techniques would no longer be able to take co-ordinate steps but would need to update all dual variables  X  by solving the en-tire single kernel SVM for the new K  X  . Even with warm restarts, this was found to be significantly more expensive than the primal update strategy of taking a sub-gradient step in W with respect to a single training point. Third, for multi-class problems, it is desirable to learn a single feature embedding for all classes using the multi-class hinge loss rather than a 1-vs-All formu-lation. Training time would rise to be quadratic in the number of categories for dual MKL methods. All in all, LDKL X  X  primal based optimizer was found to be orders of magnitude faster than the LMKL optimizer and even state-of-the-art dual based optimizers em-ploying spectral projected gradient descent (Jain et al., 2012). Finally, it should be noted that the vanilla con-vex MKL problem subject to l p&gt; 1 regularization has been optimized in the primal (Orabona et al., 2010; Orabona &amp; Jie, 2011) using stochastic sub-gradient descent. However, since LDKL learns an explicit fea-ture map, it does not have to worry about maintaining dual variables or dual sparsity, and can therefore easily scale to large problems. The performance of LDKL was assessed on multiple benchmark data sets ranging from the small (Banana), where not much improvement is expected, to the large (CoverType) where the most gains are to be had. Ta-ble 1 lists the statistics of all the data sets used in the evaluation. Each data set comes with a predefined training and test set. The training set was further randomly partitioned into 80% for training and 20% for validation. Parameters for all algorithms were cho-sen so as to maximize classification accuracy on the validation set.
 Two types of experiments were carried out. First, LDKL X  X  performance was compared to that of an RBF-SVM in terms of classification accuracy and prediction cost. Second, LDKL X  X  performance was compared to LMKL (Gonen &amp; Alpaydin, 2008) and state-of-the-art methods such as CPSP (Joachims &amp; Yu, 2009), LLSVM (Ladicky &amp; Torr, 2011), Random Fourier Fea-tures (Rahimi &amp; Recht, 2007) and the Nystr  X om kernel approximation (Williams &amp; Seeger, 2001; Yang et al., 2012). These methods are briefly reviewed in Sec-tion 2. For a fair comparison, the implementation as provided by the authors on their websites was used for all algorithms except for Nystr  X om where the SciKit-Learn library was used (Pedregosa et al., 2011). Both the compositional (9) and non-compositional (10) decision functions were tried out in LDKL and the latter was found to lead to better results. We therefore present results only for  X  L Figure 1 plots the decision boundaries for piecewise smooth LDKL for M = 2 2  X  1 , 2 3  X  1 and 2 4  X  1 on the Banana data set. The LDKL parameters  X  W ,  X   X  ,  X 
 X  0 and  X  were set by validation. LDKL X  X  prediction time was controlled by the dimensionality of the lo-cal embedding space M and Figure 2 presents results for varying values of M for each data set. Note that the LDKL formulation is non-convex and mean and standard deviation values are reported for five random initializations.
 Table 1 compares the performance of LDKL with lin-ear and RBF-SVMs. Prediction costs are reported in terms of normalized time which is an algorithm X  X  prediction time divided by a linear SVM X  X  prediction time. LDKL X  X  classification accuracy was found to be better than that of the linear SVM and tended to the RBF-SVM X  X  accuracy in most cases. However, LDKL X  X  prediction time could be orders of magnitude lower than that of the RBF-SVM. For instance, for 6 out of the 8 data sets, LDKL reduced the RBF-SVM X  X  prediction cost by 163 times on average while suffering only a 1 . 3% loss in classification accuracy. On Cover-Type, the speed up was more than 4000 times with a 3% loss in accuracy. In contrast, popular methods such as Random Fourier Features have demonstrated only a thirty times speed up over the RBF-SVM us-ing the given CoverType feature set (Rahimi &amp; Recht, 2007) while achieving the same classification accuracy as LDKL. The CIFAR data set was a challenge for all methods. LDKL achieved a more than 2000 times speed up over the RBF-SVM but also resulted in a 5% loss in accuracy (though the loss incurred by other methods was even greater).
 Table 2 compares LDKL X  X  performance to LMKL and state-of-the-art methods for speeding up non-linear SVM prediction. Having fixed LDKL X  X  prediction time as in Table 1, it tabulates the classification accuracies of all methods for the same prediction time. LDKL X  X  accuracy was found to be as much as 30% higher than kernel approximation methods such as Random Fourier Features (RFF) and Nystr  X om which do not learn from the given training labels. LDKL X  X  accuracy could also be as much as 15% and 10% higher than CPSP X  X  and LLSVM X  X  accuracies respectively which do learn from the given training labels. Finally, LDKL was better than LMKL by 2 . 5% on even the small Ba-nana data set. Unfortunately, the SVM dual based LMKL implementation provided by the authors was unable to scale to any other data set and hence no further comparisons are presented to LMKL. Figure 2 plots the classification accuracies of all methods for a range of prediction times. The scaling on the y -axis starts from the accuracy obtained by a linear SVM and hence if a method performed worse than a linear SVM then its curve would not be visible. As can be seen, LDKL X  X  curve was consistently higher than all the other curves indicating that LDKL X  X  classification accuracy was significantly better than the accuracies of other methods for a fixed prediction time. We developed the Local Deep Kernel Learning formu-lation in this paper to speed up non-linear SVM pre-diction. We generalized the Localized Multiple Kernel Learning formulation (Gonen &amp; Alpaydin, 2008) so as to learn arbitrary local feature embeddings. In par-ticular, we learnt high dimensional, sparse and com-putationally deep local features. These introduced non-linearities into the LDKL model while ensuring that predictions could be made using primal variables in time that is logarithmic in the dimensionality of the local embedding space. We developed efficient primal based routines to optimize over the space of tree-structured local feature embeddings that scale to large training sets with more than half a million train-ing points. Experimental results demonstrated that LDKL could achieve an exponential speed up over the RBF-SVM (more than four thousand times on CoverType) while maintaining classification accuracy over an acceptable threshold. Finally, we also demon-strated that LDKL could achieve significantly better classification accuracies as compared to LMKL and state-of-the-art methods. For instance, for a given pre-diction cost on CoverType, LDKL X  X  classification ac-curacy was 30% higher than that of Random Fourier Features (Rahimi &amp; Recht, 2007) and the Nystr  X om approximation (Williams &amp; Seeger, 2001; Yang et al., 2012), 15% higher than that of CPSP (Joachims &amp; Yu, 2009) and almost 10% higher than that of LLSVM (Ladicky &amp; Torr, 2011). LDKL took only hours to train on CoverType whereas an RBF-SVM took days. Furthermore, LDKL X  X  parameters could be stored in less than 1 Mb of RAM. This opens up the possibility of training non-linear SVMs on large data sets beyond the scope of RBF and other traditional kernels and making accurate predictions efficiently us-ing these learnt models.
 We are grateful to Samy Bengio, Prateek Jain, Pu-rushottam Kar, Yann Lecun, Vinod Nair, Yashoteja Prabhu and Nishal Shah for helpful discussions and feedback. Financial support was provided by the DST and the IRD,IIT-Delhi.

