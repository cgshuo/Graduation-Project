 Verbose web queries are often descriptive in nature where a term based search engine is unable to distinguish between the essential and noisy words, which can result in a drift from the user intent. We present a randomized query reduction technique that builds on an earlier learning to rank based approach. The proposed technique randomly picks only a small set of samples, instead of the exponentially many sub-queries, thus being fast enough to be useful for web search engines, while still covering wide sub-query space. H.3.3 [ Information Search and Retrieval ]: Query for-mulation Algorithms, Experimentation, Performance Verbose Queries, Randomized Algorithm
Recent studies indicate a marked growth in the share of long queries[1] -containing more than 5 terms. Though, a user X  X  intent might be well represented in a long query,  X  X ag of words X  based search engines tend to fail on them, as they are unable to capture the complex structure of queries. This may result in query drift and consequent poor performance.
Existing approaches to address this problem can be clas-sified into two categories. Query term weighing based tech-niques attempt to determine the importance of a term and associate a weight with each term in proportion to its per-ceived importance in the query. Query reduction techniques, on the other hand, focus on identifying the best performing sub-query of the original query by pruning away unimpor-tant terms. The current work belongs to the latter category.
Kumaran et al. [5] (henceforth referred to as  X  X xpRed X ) propose an approach that views identification of best rep-resentative sub-query as a ranking task and apply learning to rank approach using query quality predictors as features. Since an exponential number 2 n  X  1 of sub-queries can be generated from an n -word query, computationally it is very expensive to train a model and evaluate it on all sub-queries. Figure 1: Sub-query scores for a typical query.
 They propose a number of heuristics to reduce this set of sub-queries. Balasubramanian et al. [2] (henceforth referred to as  X  X ingleRed X ) extend this work by adopting regression based formulations and only consider sub-queries reduced by a single term from the original query, as candidates sub-queries, thus making their system web-scalable.

Our technique for query reduction uses a randomization mechanism to prune terms from original query and builds on Kumaran et al. [5] while considering only a small, con-figurable number of candidate sub-queries instead of the ex-ponential number in baseline system. Our technique is web-scalable as it achieves substantial improvements over original query and the gains increase with larger samplings.
Performance improvement on verbose queries can be at-tained by removal of noisy terms such that the resultant shorter query performs better. Moreover, to be useful for web-search, the technique should minimise overhead on the retrieval process. ExpRed achieves the first objective by evaluation of all sub-queries, but is slow because of the ex-ponential number 2 n  X  1 of sub-queries involved in an n word query. SingleRed trades off time for performance by confin-ing the search to sub-queries with one fewer term than the original query. Figure 1, shows the performance of all sub-queries for a representative long query, where y-axis mea-sures their Mean Average Precision (MAP) scores while x-axis indicates sub-query length. Thus, the original query, with largest number of terms is the sub-query with highest x co-ordinate ( x max ) in this plot. It can be observed that maximum achievable performance (indicated by highest y score for an x value) increases till optimal sub-query and de-creases on further addition of terms. As evident from the fig-ure, employing SingleRed X  X  choice of evaluating sub-queries with one fewer term than the original query, only sub-queries with x = x max  X  1 are considered, while the rest are ignored, leaving substantial scope for improvement. We propose an efficient method to explore this space. Another property of the curve is that it is steeper before it peaks while reduc-ing slowly as the original query approaches. This indicates that removal of terms from optimal query, results in greater performance degradation than addition of extra terms. We employ this observation in our sampling technique.
Since, evaluation of all sub-queries is infeasible in a web scenario, we propose a randomized reduction technique that samples only a small number of candidate sub-queries and yet efficiently explores the sub-query space. This method re-moves each query term of the original query, by an  X  X ptimal query length X  dependent probability. We argue that in small number of iterations of this sub-query generation method, with high probability, it is likely to pick at least one  X  X ood X  sub-query. Formally, to generate a sub-query from an n term query, a set of n random numbers R [1 ..n ] between 0.0 and 1.0 are generated. The k th term of this query is discarded if R [ k ] &gt; p , where p = l opt /l q . Here, l opt is the optimal length of queries and l q is the length of query q in words.
Theoretically, this problem can be modeled as a geomet-ric distribution where each new sample can be treated as a Bernoulli trial, where success is defined by selection of one  X  X ood X  query. Thus, if probability of a good sample being picked in a trial is m , then expected number of samples for an efficient sub-query to be picked is 1 /m with a deviation of p (1  X  m ) / ( m 2 ). As an illustrative example, among the 9 word queries in our corpus, 5.84% of their sub-queries scored within 10% of their best performing sub-query. Considering these sub-queries as  X  X ood X , 18 samples would be required to pick one efficient sample, with a standard deviation of 17.
Effectively, a learning to rank formulation is used, where each sub-query is represented by a set of query quality pre-dictors and its MAP acts as the label. The proposed tech-nique is used for generation of sub-queries on train and test data. The model is learned on a large number of candi-date sub-queries. Testing requires generation of only a small number of sub-queries which are ranked according to their scores generated by the model and the top positioned ver-sion is used as representative of the original query. Unfortu-nately, the overall performance of our system is dependent not just on the ability to pick effective sub-queries as can-didates, but also on the discriminative capabilities of the learning formulation. Despite this constraint, as show in Section 4, significant gains in performance were attained.
We use an experimental setup similar to [5]. Robust 2004 dataset consisting of around half million documents is used for experiments, with the  X  X escription X  field of topics acting as long queries. We use the definition of long query described in [3], 5  X  l q  X  12, where l q is the length of query q . Thus, 184 of the 250 judged queries associated with this data, are used. A set of 30 query quality predictors such as mutual information, query clarity, query scope, are used to describe each sub-query. RankSVM implementation of SV M rank [4] is used for ranking sub-queries.

The set of queries are partitioned into 4:1 split of train and test queries. At each iteration a set of twenty mod-els, with different seeds, are learned from a large number of samples generated from the training set of queries. These models are used to predict rankings of an equal number of sets prepared from the test queries and the performance is averaged over the 20 sets. A linear  X  X ultiplication factor X  x determines the number of sub-queries generated for each query and is in proportion to its length. Since, according to [5] the optimal query length lies between 3 and 6, experi-ments were performed by varying l opt between these values, for both train and test data.

Due to space constraints, only the run with best results is depicted in Fig.2. For this run l opt  X  train = 6 is used and plots for various l opt  X  test values with different  X  X ulti-plication factors X  are shown. We believe that performance training requires a large number of sub-queries with decent scores. Thus, as argued in Section 2, a higher average per-formance among candidate queries can be achieved with bias for longer queries. Testing, on the other hand, relies on a single  X  X ood X  sub-query and thus peaks closer to the true optimal length of 3.54. Overall, it is seen that irrespective of the choice of l opt  X  test , performance gains over the origi-nal query and ExpRed are achieved with a sampling of just three times the query length. Moreover, a steady co-relation is observed between the number of samples drawn and the improved performance.
In this paper, we presented a randomization based sam-pling technique for reduction of long queries. We present results which confirm that this technique is more efficient than earlier techniques of selecting candidate queries during prediction. As future work, we plan to apply this sampling technique on more semantically rich units of information such as phrases and clauses. [1] http://weblogs.hitwise.com/alan-long/2009/11/ [2] N. Balasubramanian, G. Kumaran, and V. R. Carvalho. [3] M. Bendersky and W. B. Croft. Analysis of long queries [4] T. Joachims. Optimizing search engines using [5] G. Kumaran and V. R. Carvalho. Reducing long
