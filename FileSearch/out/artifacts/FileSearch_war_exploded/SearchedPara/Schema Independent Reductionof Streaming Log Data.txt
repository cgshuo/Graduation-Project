 Large software systems consist of many i nterconnected components. The opera-tion of such systems is usually monitored by specialized applications that emit a wealth of information in the form of event logs. In this context, a challenging task is to understand in a tractable manner what operations are performed by the system at any given time, in order not only to understand how the system oper-ates, but also to identify situations where the system is performing unscheduled or unexpected tasks. To date, most dynamic analysis approaches are applied off-line, but for most practical applications a real-time on-line analysis is preferred. However, the sheer volume of the emitted logged data makes such an on-line analysis non tractable. The objective is thus to devise log filtering techniques that allow for the selective reduction of logged data according to specific filter-ing criteria. The filtering criteria can be set by the administrators and relate to specific hypotheses or analyses that need to be tested or performed.

In this paper we propose an on-line, schema independent approach that is based on information theory principles to calculate log event similarity with the purpose of analyzing and appropriately grouping streaming events that are emitted by a system X  X  monitoring infrastructure. The technique consists of three main steps. In the first step, which can be performed off-line, a collection of significant log features or  X  X eacons X  ar e selected by the user or by an automated process (e.g. a data mining process), a s being significant to a use case or to a possible type of system incident. Such beacon features may be unsuccessful login attempts to a particular server, a warning for a failed transaction, transaction requests arriving at a high frequency from a group of servers, or a performance degradation alert. Beacon features are then used to generate user defined beacon events . In the second step of the process, ev ent similarity values are computed between the beacon events and the incoming events in the log data set. The similarity scores are computed by comparing attribute values that are weighted by an information content coefficient. Fo r example, if an attribute value is con-stant across all logged events, then this attribute value should not be considered as important for the computation of an overall similarity between two events. The result of the second step is an overal l similarity value for each event with the beacon event set as a whole. In the third step of the process, a threshold is selected and events that exhibit an aggregated similarity value with the beacon set that is above the threshold value, ar e considered as a cohesive collection of events that correspond to a use case or an incident.

The proposed approach has three notable advantages over existing dynamic analysis approaches. First, it is schema independent and it is, therefore, readily available for collections of logged data that are emitted by different monitoring components and not conforming to the same schema or format. This eliminates the need for event schema merging or mapping, a process that is often too com-putationally expensive to be performed for on-line analysis. Second, the proposed approach does not require a training data set, a requirement for most approaches that are based on machine-learning techniques. Rather, the proposed approach is based on the on-line adaptive re-calculation of an information content coefficient for each attribute value, allowing thus the similarity score to be automatically adapted as the system evolves or its operational profile changes. Finally, it can be applied to streaming log data, permitting real time analysis, in contrast to most log analysis techniques that analyze stored logged data in an off-line manner.
This paper is organized as follows: Sect ion 2 presents rela ted work. Section 3 presents the event model. Section 4 discusses the event similarity measure and the selection of the filtered output. Sectio n 5 presents implemen tation consider-ations, Section 6 presents experimental results, while Sections 7 and 8 provide related discussion and conclude the paper. Dynamic program analysis has been extensively used to understand the behavior of software systems. Bruegge et al. [1] proposed a framework to support dynamic analysis by source code instrumentation of systems written in C/C++. M  X  annist  X  o et al. [2] proposed the tool SCED, for modeling dynamic properties of object oriented systems. Both tools require a ccess to the source code, which might not always be the case.

In the area of data set filtering, in [3] two different data filtering and noise reduction techniques are discussed. The first is based on multiple-partitioning filtering while the second is based on iterative partitioning filtering. In [4] a technique that allows for the discovery of processes by analyzing event logs is proposed, while in [5] a log analysis technique has been used to evaluate the evolution of business models by comparing known model templates to actual models. In [6] a domain independent approach is proposed for log reduction. The main difference between the proposed approach and the one presented in [6] is that the approach discussed here uses information theory for event similarity calculation and that it can be applied in streaming log data.

In the field of log analysis by event feature reduction, [7] discusses a technique using Latent Semantic Indexing for log reduction and filtering so that root cause analysis can be tractable for systems emitting large volumes of log data. In [8] a technique to identify event features important for dynamic analysis and, in particular, intrusion detection is presented. The reduced feature sets allow for more tractable analysis to be performed. Compared to our approach, the work in [8] is fine tuned for intrusion detection and aims to reduce features as opposed to events. In [9] the Enhanced Support Vector Decision Function technique is used for selecting important features in lo g entries to support intrusion detection analysis. In [10] a technique based on the maximization of conditional informa-tion and weak dependence is proposed for the selection of important features in data collections. In [11] an information theoretic approach that selects an opti-mal set of attributes by removing irrelevant and redundant features is presented. Similarly, in [12] a hybrid approach for feature selection based on information theory as well as filter and wrapper models is presented. In [13] a two phase approach for network intrusion detection that is based on feature reduction and reasoning using fuzzy clustering and the Dempster-Shafer theory is proposed.
Overall, the main differences of our work with the related work presented in this section are that our approach is domain and schema independent and it does not require training data sets. Furthermore, it can be applied on real-time streaming data, thus allowing for on-line analysis. 3.1 Static Event Model Each event e i of the input stream E = { e 1 ,e 2 ,...e N E } is defined as a set of pairs that consist of an attribute a j  X  X  and its associated value v j,i . More specifically, each event e i in the input stream is defined as: where A = { a 1 ,a 2 ,...a N A } is the set of all attributes a j appearing in the events of the input stream. If an event e i does not have an attribute a j we simply consider the value for this particular attribute to be NULL. This notation can be used to represent any acyclic tree-like structure, such as those commonly generated by system utilities (e.g. XML, JSON), while it can be easily stored in a database, such as MongoDB.

It should be noted that this approach is particularly effective when used with structured data, with its precision increasing as the detail of the representation increases. While free-form attribute string values can still be used as input to the low-level primitive value distance metric calculation, several approaches have been proposed (e.g [14], [15]) to extract schema information from log files. 3.2 Stream Model The implicit insertion of NULL values allows each attribute a j to be defined as stream can be viewed as a collection of discr ete time series, one for each attribute, that are evolving in parallel.

The proposed approach is most effective when each attribute is an independent variable with no correlation to other attributes. Redundant information in the input stream may skew the results of the process by incorrectly emphasizing certain values, while reducing the percei ved importance of others. The problem of normalizing redundant information streams has been studied extensively. For example, several feature selection techni ques have been proposed (e.g. [10], [8]) and can be used as a pre-processing stage to remove redundant features. 4.1 Beacon Event Set Compilation The first phase of the proposed process involves the compilation of a cohesive beacon event set B = { b 1 ,b 2 ,...b N B } that serves as the filtering criterion. These events can be selected directly from the input stream or can be drafted by the operator, as a collection of pseudo-events , by combining features and values that the operator considers important or of interest. The system requires no information on the actual selection process, which allows the use of opaque third-party utilities. 4.2 Similarity Computation In order to compute a similarity measure of each event in the input stream with the beacon set, we propose a three stage process. The results of each stage are fed to the next one, creating a hierarc hical process that is described bellow. Stage 1. The first stage involves the dynamic determination of the importance that an attribute or a value carries in the evaluation of a final overall similarity measure. For example, an attribute for which its value is constant throughout the input stream does not carry any significant information content for the com-putation, while attribute values that vary may carry higher information content . This allows a low-level similarity metric for primitive values (e.g. strings), typi-cally described as dist : E X E X A X  X  , to be used as the basis of an event-level similarity measure.

More specifically, the proposed approach attempts to determine which at-tributes and attribute values offer the best event selectivity with regard to a specific beacon set. For this purpose, a statistical similarity measure based on information theory is introduced. Each attribute a j is considered an independent discrete random variable with an alphabet V j of N V j possible symbols (values).
According to information theory, the entropy H of an independent variable X is a measure of the average information content of each sample of X . Likewise, the information content I is a metric of the importance of a variable as a whole. If X is a discrete time series of N samples with an alphabet of n symbols and a probability mass function of p ( X ), we have:
The probability p ( x i ) of a symbol is equal to its relative frequency within the time series. Specializing formula 3 using the individual attribute value fre-quencies, it is therefore possible to compute the information content of a specific attribute value v j,i , as well as that of a discrete attribute a j as a whole:
The selectivity offered by each event attribute is in direct relation to the in-formation content of the equivalent time series. Conceptually, highly repetitive attributes, such as domain-specific constants, have a limited use as distinguish-ing features between events, but they also exhibit a relatively low entropy that can be used to reduce their participation in the event comparison process. Fur-thermore, attributes with ex treme diversity, e.g. uniqu e identifiers, tend to skew the similarity metric, since they have a high information content despite being of limited value for determining similarity. To offset this issue, the information content I ( v j,i ) contributed by each specific attribute value is taken into account in relation to the information content I ( a j ) of that particular attribute as a whole. This leads to the following definition of the information content fractions IF ( a j )and IF ( v j,i ) for an attribute and an attribute value respectively: The information content fraction is a dimensionless quantity in the [0 , 1] range. Using it as a coefficient lessens the impact of attribute values with low overall contribution. This is especially effective f or attributes with high diversity: a large number of discrete values means that each specific value has a minuscule contri-bution to the overall information content on its own. The resulting IF fraction will be relatively low, reducing the skew normally caused by high-diversity at-tributes. Using this principle, a similarity metric SV i,b,j canbedefinedforthe attribute a j values of input event e i and beacon event b as follows: Stage 2. At the second stage we compute a weight W a j for each attribute a j in the input stream E . This attribute-specific weight is affected by the information content of the attribute and the particular contribution of its specific values in the beacon event set. This enhances the e ffect of less frequent values that are common within the beacon event set and may, therefore, be a distinguishing feature for the selection process.
 Stage 3. The final stage contains the evaluation of a similarity measure between two events and leverages that measure to compare each input stream event with the beacon set as a whole.

Lin [16] offers a formal definition of the concept of similarity, based on three basic intuitive tenets: (a) the more commonality two objects share, the more similar they are; (b) the more differences two objects have, the less similar they are and; (c) two identical objects should always reach the maximum similarity.
Using a weighted mean to leverage the per -attribute similarity values from the previous stages to an event-level metric SE i,b satisfies all three basic conditions. Additionally, it allows the attribute-level weights which are not bounded to form a bounded metric with the same range as the primitive correlation metrics: The final computation involves the determination of an overall similarity SB i of an input event with the beacon set as a whole. To procure a similarity metric between a single event and an event set using a metric defined between single events, it becomes necessary to reexamine t he three basic principles mentioned above. More specifically, the requirement for a maximum similarity result on identical inputs is no longer satisfiable since sets and single items are not directly comparable. Our prototype implementation uses an arithmetic mean, averaging the similarities calculated for the input event with each beacon event: 4.3 Filtered Event Group Selection Conceptually, for the reduced event set we aim to select those events with the highest similarity values. Providing a threshold for that selection, however, is not trivial. Real-time streams are unbounded with regard to space and impose certain latency constraints, while the amount of information that is known a priori is limited. Therefore an adaptive threshold selection method should be used for the last phase of the process.

In our approach, the resulting event group is computed using a dynamic thresh-old, which is determined by detecting significant gaps in the distribution of simi-larity values. More specifically, a sketch of the cumulative distribution function (CDF) of the similarity values is formed in constant space and time, and is up-dated and examined periodically as the input stream is processed. Using a rough derivative computation it is possible to detect plateaus in the CDF, which typi-cally separate similarity value clusters. A heuristic algorithm is then used to select an appropriate threshold, by taking into account the distances between the high-est valued clusters, their size and their position within the similarity value range. Similar techniques based on sub-linear data sketches have been used for approxi-mate percentile determination on real time streams ([17], [18]). 5.1 Adaptivity of the Proposed Approach Adaptivity to Schema Changes. The prototype implementation is com-pletely domain agnostic, with no apriory knowledge of the domain or the event schema. As such, it operates on the inherent assumption that only part of the actual schema has been considered at any given time. An incoming event may contain a number of previously unseen attributes that expand the existing view of the schema of the monitored domain. Each new attribute is essentially treated as a time series that only contained null values until its time of emergence.
However, for most event domains and monitoring systems it is reasonable to assume a fixed number of attributes . Therefore, the set of attributes that are known to the system at runtime will gradually converge towards a constant set, with few or no new additions after a sufficient amount of time.
 Information Content Aging. Typical statistical algorithms are generally tar-geted at data sets and not well suited for streaming input: 1. Common set-based metrics exhibit a form of inertia as the number of recorded 2. Cumulative metrics, e.g. the mean and standard deviation, suffer from nu-To avoid these issues, the proposed system makes use of algorithms that devalue older data points with the passage of time. Daneshgaran et al. [19] provide an information theoretic definition of aging, while Cormode et al. ([20]) describe a generic exponential decay model for aggregate metrics.

Our prototype implementation synchronizes using input events as a time ref-erence and ignores any other perception of time. As a result, the decay model used for the aging process, is a simple step-wise decay model. For each cycle ,the information content currently recorded for all attributes and attribute values is multiplied by a positive decay coefficient no greater than 1: The decay coefficient should be selected with regard to the monitored system, most notably its event rate and other tem poral characteristics. Typical values for the decay coefficient during experimentation were in the [0 . 95 , 1) range. 5.2 Space Complexity and Object Replacement Algorithms Since real-time systems have no known limit for the length of the input stream, their space complexity should be consta nt, or at most sub-linear, with respect to the size of the input. For this reason, it is necessary to approximate the operation of the similarity determination algorithm in a space-efficient manner. From the mathematical formulas at its foundation it is clear that the values with the least impact are those with the lowest contribut ion of information co ntent. Therefore, a potential approximation involves limiting the volume of retained metadata by eliminating the entries that correspond to low-impact values.

Taking the effects of the aging process into account, one can intuitively identify the low-impact values as those that (a) are infrequent or (b) have not appeared recently in the input stream. Selecting items based on frequency and recency is essentially the purpose of object replacement algorithms , more commonly known as caching algorithms . The main intent of a caching subsystem is to use a pre-determined amount of space to store results of past requests, replacing old or infrequent entries when necessary. Therefore, we can use such replacement algo-rithms to provide a hard limit for the space usage of the proposed system. For our prototype implementation, we selected the Adaptive Replacement Cache [21] algorithm, due to its simplicity and overall performance. It provides a good balance between recency and frequ ency, while also being resistant to pathological behaviors and able to adapt to its input. Moreover, it has excel-lent runtime performance and does not n eed external tuning that would require domain-specific knowledge. 5.3 Runtime Performance In order to ensure the scalability of the system for monitoring large infrastruc-tures, several approaches to parallelization are available:  X  Attribute-level parallelization , where each processor handles a subset of the  X  Beacon-level parallelization , where the comparison of an input event with  X  Event level parallelization , where the input events are distributed to a large
These methods are generally orthogonal and can therefore be employed si-multaneously if necessary. The prototype implementation, supports the first two methods and is able to achieve a sustainable throughput of several thousand events per second, when making full use o f a 4-processor personal computer. For the evaluation of the proposed algorithm we used a data set generated within the scope of a competition that was held in conjunction with KDD99, the Fifth International Conference on Knowledge Discovery and Data Mining. The KDD99 data set [22] is a derivative of the DARPA Intrusion Detection Evaluation 1999 data set. As such, KDD99 events represent traffic within a computer network, while specific security breaches are atte mpted. The data set features about 4 . 8 million events and 41 distinct attributes, of both continuous and discrete types. In addition, the KDD99 data set contains embedded classification data for each event, which can serve as a reference for the evaluation of the quality metrics of any selection method. This alleviat es the need for domain-specific tools or manual intervention for the creation of the golden standard against which our system will be evaluated.

The prototype implementation was written in the Java programming language and tested extensively in streaming mod e. For each experiment, the used beacon events were selected randomly from a sp ecific attack type, with the rest of the events that belong to the same attack type being the expected result. 6.1 Similarity Metric Evaluation To evaluate the quality of the similarity metric, it is necessary to examine its selectivity between matching and non-mat ching events. Intuitively, an acceptable metric would provide results with clear separation between potential matches and the rest of the input, allowing the selection of the matching results by means of a simple similarity threshold. Ideally, the separating space would also not contain any noise in the form of infrequent errant values, although their effect on the overall accuracy of the system would be negligible and they could be filtered-out relatively easily using a variety of methods.

Fig.1 illustrates the aforementioned similarity gap for the events from a small range of the input stream. The plotted values were produced using a beacon set of 20 events that match the neptune attack type from the KDD99 data set. The input set contains two event clusters of the same type in this particular range, in the subranges [4810961  X  4817099] and [4827760  X  4827959]. The corresponding similarity values rise from a baseline of about 5  X  10% to values in the 40  X  60% range, with virtually no noise in the range from approximately 20% to 40%, thus being in complete accordance with the golden standard.
Fig.2 depicts a plot of the cumulative distribution function (CDF) of the similarity for the same scenario, calculated over a range of 1 , 000 , 000 events. The CDF of the similarity essentially provides the ratio of rejected events for each possible selection thres hold. Sharp rises in the CDF indicate the existence of a tight cluster around the same value, while a plateau is caused by the lack of any data points in the corresponding range. In Fig.2 a gap in the similarity values, indicated by a plateau in the CDF plot for similarities in the approximate range of 31  X  63%, separates the matching events from the rest of the input stream. A rather conservative threshold selection of e.g. 35% would preserve roughly 20% of the input stream, which is congruent with the ratio of neptune -type events in the same range.

Both figures illustrate the potential usability of the proposed similarity metric as a distinguishing criterion for event selection purposes. In addition, the sig-nificant similarity value difference between matching and non-matching events allows the use of simpler algorithms in the final selection stage, by limiting the need for complex noise-reduction techniques. 6.2 Quality of Results An accepted technique of assessing an info rmation retrieval process is the mea-surement of precision and recall values. For our system, recall is more important than precision, since recall is critical for ensuring that the technique does not discard matching events, especially in a streaming environment where the re-covery of such events might not be possibl e. However, precision still remains an important quality, since it relates to th e reduction effected upon the size of the input, which is the final purpose of the proposed system.

For the evaluation process, the proto type implementation was subjected to a series of experiments, a subset of which is presented in table 1. Table 1 con-tains results from experiments performed over a range of 1 , 000 , 000 events for three different event types and for a variable beacon event set size. The selected scenarios cover a significant event frequency range, with neptune -type events comprising about 20% of the input, while back and teardrop events correspond to 0 . 2% and 0 . 02% respectively. Despite this variation, the system reliably man-ages to retrieve over 95% of the requested events, with a precision that typically lies in the range of 90  X  95%. The system is able to produce acceptable results with as low as 5 beacon events, which makes it usable by human operators with-out the aid of additional tools. Varying the size of the beacon event set, does not generally appear to have a consistently significant effect, although in most cases we observed that larger sets resulted in a slightly higher amount of noise. The teardrop event type, however, is a notable exception, due to the relatively lim-ited cohesion and higher diversity that characterizes its members. As a result, a larger beacon event set is required to allow the system to reliably establish which features characterize the teardrop events, as evidenced by the low pre-cision calculated for the 5-event beaco n set. In addition the small size of the teardrop event set aggravates the effects of any factor that negatively affects the precision of the system. 6.3 Stability Assessment To assess the stability of the system we considered its behavior with respect to the presence of random noise in the beacon event set. The evaluation is particu-larly important, since in real-life cases it is not always possible to define a beacon event set with absolute precision. Table 2 illustrates the experimental results of randomly selected noisy events in the beacon set. Low amounts (e.g. 10%) of noise do not generally have a significant impact, while higher amounts may have a negative effect on precision with the recall being mostly unaffected. On the other hand, event types with limited cohesion, such as the teardrop type, are far more susceptible to the negative effects o f noise, since the beacon events feature a certain amount of noise themselves. It should be noted that these comments are only valid in the case of random noise. The effects of cohesive erroneous events in the beacon set ar e more severe, with the sys tem generally selecting events that match either the requested or the erroneous event set. 6.4 Runtime Performance The runtime performance of the prototype implementation was evaluated from several aspects, in order to assess the feasibility of its deployment in production environments. On a mid-range personal computer with four processor cores, the throughput of the system was macroscopically stable and typically well in excess of 5 , 000 events/second, with 10 , 000 events/second being attainable for smaller beacon event sets. The use of multiple wo rker threads provided a notable perfor-mance increase, as seen in Fig.3, although the prototype had not been adequately optimized for multiple processors. Last, the system had quite reasonable mem-ory requirements, being able to process the KDD99 data set with less than 96 MB of heap memory available to the Java VM. The proposed approach is mainly intended as an efficient schema-independent initial-approach analysis tool. As such, several assumptions were made which can potentially create situations where the system will misbehave:
Each attribute is examined in isolation: The potential usefulness of combi-nations of attribute values is not examined. Addressing this issue in a domain-agnostic manner requires the use of adaptive methods to avoid the introduction of operations with exponential complexity in relation to the combination size.
The system examines each event independently: No temporal relationships are established between events or attribute values. In general the detection of systematic transitions is considered to be beyond the scope of this system, for complexity and performance reasons.

The information content determination algorithm does not take value proxim-ity into account: In some cases it makes sense to consider as equivalent attribute values which happen to be in close numerical or lexicographical proximity.
However, the event selection approach presented in this paper is more flexible than rule-based filters, as it does not require any domain knowledge and can adapt to the input stream. In order for on-line tractable analysis of log data streams to be performed, we should first devise techniques that allow for the selective reduction of the volume of the data that needs to be considered for each analysis. In this paper, we have presented an approach that allows for the on-line selection of logged events that are highly cohesive with respect to a par ticular feature set. The feature set is used to compile a set of user defined events we refer to as beacon event set. The proposed approach is based on information theory to compute a similarity measure between incoming events and the beacon set, and is schema indepen-dent. The result is a highly reduced collection of incoming events that are highly related to specific system behavior. Results obtained from the KDD99 bench-mark data set indicate that the approach can tractably reduce the size of events that need be considered with respect to some important system activity such as intrusion attempts, while maintaining high recall and precision levels. Possi-ble future work includes the extension of the approach to handle combinations of attributes for computing event similarity, the incorporation of an attribute reduction pre-processing phase so that further performance enhancements can be possible, and the investigation of techniques for adaptive threshold selection. This work has been supported by CA Labs and CA Technologies UK.

