 How can one build a distributed framework that allows ef-ficient deployment of a wide spectrum of modern advanced machine learning (ML) programs for industrial-scale prob-lems using Big Models (100s of billions of parameters) on Big Data (terabytes or petabytes)? Contemporary paralleliza-tion strategies employ fine-grained operations and schedul-ing beyond the classic bulk-synchronous processing paradigm popularized by MapReduce, or even specialized operators relying on graphical representations of ML programs. The variety of approaches tends to pull systems and algorithms design in different directions, and it remains difficult to find a universal platform applicable to a wide range of different ML programs at scale. We propose a general-purpose frame-work that systematically addresses data-and model-parallel challenges in large-scale ML, by leveraging several funda-mental properties underlying ML programs that make them different from conventional operation-centric programs: er-ror tolerance, dynamic structure, and nonuniform conver-gence; all stem from the optimization-centric nature shared in ML programs X  mathematical definitions, and the iterative-convergent behavior of their algorithmic solutions. These properties present unique opportunities for an integrative system design, built on bounded-latency network synchro-nization and dynamic load-balancing scheduling, which is ef-ficient, programmable, and enjoys provable correctness guar-antees. We demonstrate how such a design in light of ML-first principles leads to significant performance improvements versus well-known implementations of several ML programs, allowing them to run in much less time and at considerably larger model sizes, on modestly-sized computer clusters. H.3.4 [ Information Storage and Retrieval ]: Systems and Software X  Distributed Systems ; G.3 [ Probability and Statistics ]: Probabilistic algorithms; G.4 [ Mathematical Software ]: Parallel and vector implementations c  X  Machine Learning, Big Data, Big Model, Distributed Sys-tems, Theory, Data-Parallelism, Model-Parallelism
Machine learning (ML) is becoming a primary mechanism for extracting information from data. However, the surging volume of Big Data from Internet activities and sensory ad-vancements, and the increasing needs for Big Models for ultra high-dimensional problems have put tremendous pres-sure on ML methods to scale beyond a single machine, due to space and time bottlenecks. For example, the Clueweb 2012 web crawl 1 contains &gt; 700m web pages as 27TB of text, while photo-sharing sites such as Flickr, Instagram and Facebook are anecdotally known to possess 10s of billions of images, again taking up TBs of storage. It is highly ineffi-cient, if possible, to use such big data sequentially in a batch or scholastic fashion in a typical iterative ML algorithm. On the other hand, state-of-the-art image recognition systems have now embraced large-scale deep learning models with billions of parameters [17]; topic models with up to 10 6 ics can cover long-tail semantic word sets for substantially improved online advertising [26, 31]; and very-high-rank ma-trix factorization yields improved prediction on collaborative filtering problems [35]. Training such big models with a sin-gle machine can be prohibitively slow, if possible.
Despite the recent rapid development of many new ML models and algorithms aiming at scalable application [9, 28, 15, 36, 1, 5], adoption of these technologies remains gen-erally unseen in the wider data mining, NLP, vision, and other application communities for big problems, especially those built on advanced probabilistic or optimization pro-grams. We believe that, from the scalable execution point of view, a main reason that prevents many state-of-the-art ML models and algorithms from being more widely applied at Big-Learning scales is the difficult migration from an aca-demic implementation, often specialized for a small, well-controlled computer platform such as desktop PCs and small lab-clusters, to a big, less predictable platform such as a cor-porate cluster or the cloud, where correct execution of the original programs require careful control and mastery of low-level details of the distributed environment and resources through highly nontrivial distributed programming.

Many platforms have provided partial solutions to bridge this research-to-production gap: while Hadoop [27] is a pop-ular and easy to program platform, the simplicity of its MapReduce abstraction makes it difficult to exploit ML prop-http://www.lemurproject.org/clueweb12.php Figure 1: The scale of Big ML efforts in recent liter-erties such as error tolerance (at least, not without consid-erable engineering effort to bypass MapReduce limitations), and its performance on many ML programs has been sur-passed by alternatives [32, 20]. One such alternative is Spark [32], which generalizes MapReduce and scales well on data while offering an accessible programming interface; yet, Spark does not offer fine-grained scheduling of com-putation and communication, which has been shown to be hugely advantageous, if not outright necessary, for fast and correct execution of advanced ML algorithms [7]. Graph-centric platforms such as GraphLab [20] and Pregel [21] effi-ciently partition graph-based models with built-in schedul-ing and consistency mechanisms; but ML programs such as topic modeling and regression either do not admit ob-vious graph representations, or a graph representation may not be the most efficient choice; moreover, due to limited theoretical work, it is unclear whether asynchronous graph-based consistency models and scheduling will always yield correct execution of such ML programs. Other systems pro-vide low-level programming interfaces [23, 19], that, while powerful and versatile, do not yet offer higher-level general-purpose building blocks such as scheduling, model partition-ing strategies, and managed communication that are key to simplifying the adoption of a wide range of ML methods. In summary, existing systems supporting distributed ML each manifest a unique tradeoff on efficiency, correctness, pro-grammability, and generality.

In this paper, we explore the problem of building a dis-tributed machine learning framework with a new angle to-ward the efficiency, correctness, programmability, and gen-erality tradeoff. We observe that, a hallmark of most (if not all) ML programs is that they are defined by an ex-plicit objective function over data (e.g., likelihood, error-loss, graph cut), and the goal is to attain optimality of this function, in the space defined by the model parame-ters and other intermediate variables. Moreover, these al-gorithms all bear a common style, in that they resort to an iterative-convergent procedure (see Eq. 1). It is noteworthy that iterative-convergent computing tasks are vastly differ-ent from conventional programmatic computing tasks (such as database queries and keyword extraction), which reach correct solutions only if every deterministic operation is cor-rectly executed, and strong consistency is guaranteed on the intermediate program state  X  thus, operational objectives such as fault tolerance and strong consistency are absolutely necessary. However, an ML program X  X  true goal is fast, effi-cient convergence to an optimal solution, and we argue that fine-grained fault tolerance and strong consistency are but one vehicle to achieve this goal, and might not even be the most efficient one.

We present a new distributed ML framework, Petuum , built on an ML-centric optimization-theoretic principle, as opposed to various operational objectives explored earlier. We begin by formalizing ML algorithms as iterative-convergent programs, which encompass a large space of modern ML such as stochastic gradient descent and coordinate descent for determining optimality or fixed-point in optimization programs [3, 12], MCMC and variational methods for graph-ical models [13, 15], proximal optimization and ADMM for structured sparsity problems [6, 4], among others. To our knowledge, no existing ML platform has considered such a wide spectrum of ML algorithms, which exhibit diverse rep-resentation abstractions, model and data access patterns, and synchronization and scheduling requirements. So what are the shared properties across such a  X  X oo of ML algo-rithms X ? We believe that the key lies in recognizing a clear dichotomy between data (which is conditionally independent and persistent throughout the algorithm) and model (which is internally coupled, and is transient before converging to an optimum). This inspires a simple yet statistically-rooted bimodal approach to parallelism: data parallel and model parallel distribution and execution of a big ML program over a cluster of machines. This dichotomous parallel approach keenly exploits the unique statistical nature of ML algo-rithms, particularly three properties: (1) Error tolerance  X  iterative-convergent algorithms are often robust against lim-ited errors in intermediate calculations; (2) Dynamic struc-tural dependency  X  during execution, the changing corre-lation strengths between model parameters are critical to efficient parallelization; (3) Non-uniform convergence  X  the number of steps required for a parameter to converge can be highly skewed across parameters. The core goal of Petuum is to execute these iterative updates in a manner that quickly converges to an optimum of the ML program X  X  objective function, by exploiting these three statistical properties of ML, which we argue are fundamental to efficient large-scale ML in cluster environments.

This design principle contrasts that of several existing frameworks discussed earlier. For example, central to Spark [32] is the principle of perfect fault tolerance and recovery, sup-ported by a persistent memory architecture (Resilient Dis-tributed Datasets); whereas central to GraphLab is the prin-ciple of local and global consistency, supported by a ver-tex programming model (the Gather-Apply-Scatter abstrac-tion). While these design principles reflect important as-pects of correct ML algorithm execution  X  e.g., atomic re-coverability of each computing step (Spark), or consistency satisfaction for all subsets of model variables (GraphLab)  X  some other important aspects, such as the three statistical properties discussed above, or perhaps ones that could be more fundamental and general, and which could open more room for efficient system designs, remain unexplored.
To exploit these properties, Petuum introduces three novel system objectives grounded in the aforementioned key prop-erties of ML programs, in order to accelerate their con-vergence at scale: (1) Petuum synchronizes the parameter states with a bounded staleness guarantee, which achieves provably correct outcomes due to the error-tolerant nature of ML, but at a much cheaper communication cost than con-ventional per-iteration bulk synchronization; (2) Petuum of-fers dynamic scheduling policies that take into account the changing structural dependencies between model parame-ters, so as to minimize parallelization error and synchro-nization costs; and (3) Since parameters in ML programs exhibit non-uniform convergence costs (i.e. different num-bers of updates required), Petuum prioritizes computation towards non-converged model parameters, so as to achieve faster convergence.

To demonstrate this approach, we show how a data-parallel and a model-parallel algorithm can be implemented on Petuum, allowing them to scale to large data and model sizes with improved algorithm convergence times. Figure 1 offers a glimpse of the model scalability achievable on Petuum, where we show a range of Petuum ML programs at large model scales (up to a trillion parameters), on relatively modest clusters (10-100 machines) that are within reach of most ML practitioners. The experiments section provides more detailed benchmarks on a range of ML programs: topic mod-eling, matrix factorization, deep learning, Lasso regression, and distance metric learning. These algorithms are only a subset of the full open-source Petuum ML library 2 , which in-cludes more algorithms not explored in this paper: random forests, K-means, sparse coding, MedLDA, SVM, multi-class logistic regression, with many others being actively devel-oped for future releases. We begin with a principled formulation of iterative-convergent ML programs, which exposes a dichotomy of data and model, that inspires the parallel system architecture (  X  3), algorithm design (  X  4), and theoretical analysis (  X  5) of Petuum. Con-sider the following programmatic view of ML as iterative-convergent programs, driven by an objective function: Iterative-Convergent ML Algorithm: Given data D and loss L (i.e., a fitness function such as RMS loss, like-lihood, margin), a typical ML problem can be grounded as executing the following update equation iteratively, until the model state (i.e., parameters and/or latent variables) A reaches some stopping criteria: where superscript ( t ) denotes iteration. The update function  X 
L () (which improves the loss L ) performs computation on data D and model state A , and outputs intermediate results to be aggregated by F (). For simplicity, in the rest of the paper we omit L in the subscript with the understanding that all ML programs of our interest here bear an explicit loss function that can be used to monitor the quality of con-vergence and solution, as oppose to heuristics or procedures not associated such a loss function.

In large-scale ML, both data D and model A can be very large. Data-parallelism , in which data is divided across ma-chines, is a common strategy for solving Big Data problems, whereas model-parallelism , which divides the ML model, is common for Big Models. Below, we discuss the (different) mathematical implications of each parallelism (see Fig. 2). Petuum is available as open source at http://petuum.org . Figure 2: The difference between data and model par-
In data-parallel ML, the data D is partitioned and as-signed to computational workers (indexed by p = 1 ..P ); we denote the p -th data partition by D p . We assume that the function  X () can be applied to each of these data subsets independently, yielding a data-parallel update equation: In this definition, we assume that the  X () outputs are aggre-gated via summation, which is commonly seen in stochastic gradient descent or sampling-based algorithms. For exam-ple, in distance metric learning problem which is optimized with stochastic gradient descent (SGD), the data pairs are partitioned over different workers, and the intermediate re-sults (subgradients) are computed on each partition and are summed before applied to update the model parameters. Other algorithms can also be expressed in this form, such as portantly, this additive updates property allows the updates  X () to be aggregated at each local worker before transmis-sion over the network, which is crucial because CPUs can produce updates  X () much faster than they can be (indi-vidually) transmitted over the network. Additive updates are the foundation for a host of techniques to speed up data-parallel execution, such as minibatch, asynchronous and bounded-asynchronous execution, and parameter servers. Key to the validity of additivity of updates from different workers is the notion of independent and identically dis-tributed (iid) data, which is assumed for many ML programs, and implies that each parallel worker contributes  X  X qually X  (in a statistical sense) to the ML algorithm X  X  progress via  X (), no matter which data subset D p it uses.
In model-parallel ML, the model A is partitioned and as-signed to workers p = 1 ..P and updated therein in paral-lel, running update functions  X (). Unlike data-parallelism, each update function  X () also takes a scheduling function S p (), which restricts  X () to operate on a subset of the model parameters A : where we have omitted the data D for brevity and clarity. S p () outputs a set of indices { j 1 ,j 2 ,..., } , so that  X () only performs updates on A j 1 ,A j 2 ,...  X  we refer to such selection of model parameters as scheduling .

Unlike data-parallelism which enjoys iid data properties, the model parameters A j are not, in general, independent of each other (Figure 2), and it has been established that model-parallel algorithms can only be effective if the parallel updates are restricted to independent (or weakly-correlated) parameters [18, 5, 25, 20]. Hence, our definition of model-parallelism includes a global scheduling mechanism that can select carefully-chosen parameters for parallel updating.
The scheduling function S () opens up a large design space, such as fixed, randomized, or even dynamically-changing scheduling on the whole space, or a subset of, the model parameters. S () not only can provide safety and correctness (e.g., by selecting independent parameters and thus mini-mize parallelization error), but can offer substantial speed-up (e.g., by prioritizing computation onto non-converged pa-rameters). In the Lasso example, Petuum uses S () to select coefficients that are weakly correlated (thus preventing di-vergence), while at the same time prioritizing coefficients far from zero (which are more likely to be non-converged).
Data-and model-parallel programs are stateful , in that they continually update shared model parameters A . Thus, an ML platform needs to synchronize A across all running threads and processes, and this should be done in a high-performance non-blocking manner that still guarantees con-vergence. Ideally, the platform should also offer easy, global-variable-like access to A (as opposed to cumbersome message-passing, or non-stateful MapReduce-like functional inter-faces). If the program is model-parallel, it may require fine control over parameter scheduling to avoid non-convergence; such capability is not available in Hadoop, Spark nor GraphLab without code modification. Hence, there is an opportunity to address these considerations via a platform tailored to data-and model-parallel ML.
A core goal of Petuum is to allow easy implementation of data-and model-parallel ML algorithms. Petuum provides APIs to key systems that make this task easier: (1) a pa-rameter server system, which allows programmers to access global model state A from any machine via a convenient distributed shared-memory interface that resembles single-machine programming, and adopts a bounded-asychronous consistency model that preserves data-parallel convergence guarantees, thus freeing users from explicit network synchro-nization; (2) a scheduler , which allows fine-grained control over the parallel ordering of model-parallel updates  X ()  X  in essence, the scheduler allows users to define their own ML application consistency rules.
ML algorithms exhibit several principles that can be ex-ploited to speed up distributed ML programs: dependency structures between parameters, non-uniform convergence of parameters, and a limited degree of error tolerance [14, 7, 18, 33, 19, 20]. Petuum allows practitioners to write data-parallel and model-parallel ML programs that exploit these principles, and can be scaled to Big Data and Big Model applications. The Petuum system comprises three compo-nents (Fig. 3): scheduler, workers, and parameter server, and Petuum ML programs are written in C++ (with Java support coming in the near future).
 Scheduler: The scheduler system enables model-parallelism, by allowing users to control which model parameters are updated by worker machines. This is performed through a user-defined scheduling function schedule() (correspond-ing to S ( t  X  1) p ()), which outputs a set of parameters for each worker  X  for example, a simple schedule might pick a ran-dom parameter for every worker, while a more complex scheduler (as we will show) may pick parameters according to multiple criteria, such as pair-wise independence or dis-tance from convergence. The scheduler sends the identities of these parameters to workers via the scheduling control channel (Fig. 3), while the actual parameter values are de-livered through a parameter server system that we will soon explain; the scheduler is responsible only for deciding which parameters to update. In Section 5, we will discuss theoret-ical guarantees enjoyed by model-parallel schedules.
Several common patterns for schedule design are worth highlighting: the simplest is a fixed-schedule ( schedule_fix() ), which dispatches parameters A in a pre-determined order (as is common in existing ML implementations). Static, round-robin schedules (e.g. repeatedly loop over all parameters) fit the schedule_fix() model. Another type of schedule is dependency-aware ( schedule_dep() ) scheduling, which allows re-ordering of variable/parameter updates to accel-erate model-parallel ML algorithms such as Lasso regres-sion. This type of schedule analyzes the dependency struc-ture over model parameters A , in order to determine their best parallel execution order. Finally, prioritized schedul-ing ( schedule_pri() ) exploits uneven convergence in ML, by prioritizing subsets of variables U sub  X  A according to algorithm-specific criteria, such as the magnitude of each parameter, or boundary conditions such as KKT.

Because scheduling functions schedule() may be compute-intensive, Petuum uses pipelining to overlap scheduling com-putations schedule() with worker execution, so workers are always doing useful computation. The scheduler is also re-sponsible for central aggregation via the pull() function (corresponding to F ()), if it is needed.
 Workers: Each worker p receives parameters to be up-dated from schedule() , and then runs parallel update func-tions push() (corresponding to  X ()) on data D . Petuum in-tentionally does not specify a data abstraction, so that any data storage system may be used  X  workers may read from data loaded into memory, or from disk, or over a distributed file system or database such as HDFS. Furthermore, workers may touch the data in any order desired by the program-mer: in data-parallel stochastic algorithms, workers might sample one data point at a time, while in batch algorithms, workers might instead pass through all data points in one iteration. While push() is being executed, the model state A is automatically synchronized with the parameter server via the parameter exchange channel, using a distributed shared memory programming interface that conveniently re-sembles single-machine programming. After the workers fin-ish push() , the scheduler may use the new model state to generate future scheduling decisions.
 Parameter Server: The parameter servers (PS) pro-vide global access to model parameters A (distributed over many machines), via a convenient distributed shared mem-ory API that is similar to table-based or key-value stores. To take advantage of ML-algorithmic principles, the PS imple-ments Stale Synchronous Parallel (SSP) consistency [14, 7], which reduces network synchronization costs, while main-taining bounded-staleness convergence guarantees implied by SSP. We will discuss these guarantees in Section 5. Un-like PS-only systems that only support data-parallelism [19], Petuum X  X  combined scheduler-and-PS design allows for both data-and model-parallel algorithms, which run asynchronously and enjoy provable speedup guarantees with more machines.
Fault tolerance is handled by checkpoint-and-restart, which is suitable for up to 100s of machines; a more sophisticated strategy for 1000s of machines is part of future work. To fur-ther improve network performance, Petuum can be config-ured to obey bandwidth limits and a logical network topol-ogy (e.g. ring, grid or fat-tree).
Figure 4 shows a basic Petuum program, consisting of a central scheduler function schedule() , a parallel update function push() , and a central aggregation function pull() . The model variables A are held in the parameter server, which can be accessed at any time from any function via the PS object. The PS object can be accessed from any func-tion, and has 3 functions: PS.get() to read a parameter, PS.inc() to add to a parameter, and PS.put() to overwrite a parameter. With just these operations, the SSP consis-tency model automatically ensures parameter consistency between all Petuum components; no additional user pro-gramming is necessary. Finally, we use DATA to represent the data D ; as noted earlier, this can be any 3rd-party data structure, database, or distributed file system.
Now we turn to development of parallel algorithms for large-scale distributed ML problems, in light of the data and model parallel principles underlying Petuum. We focus on a new data-parallel Distance Metric Learning algorithm, and a new model-parallel Lasso algorithm, but our strategies apply to a broad spectrum of other ML problems as briefly discussed at the end of this section. We show that with the Petuum system framework, we can easily realize these algorithms on distributed clusters without dwelling on low level system programming, or non-trivial recasting of our ML problems into representations such as RDDs or vertex programs. Instead our ML problems can be coded at a high level, more akin to Matlab or R.
Let us first consider a large-scale Distance Metric Learning (DML) problem. DML improves the performance of other ML programs such as clustering, by allowing domain experts to incorporate prior knowledge of the form  X  X ata points x , y are similar (or dissimilar) X  [29]  X  for example, we could enforce that  X  X ooks about science are different from books about art X . The output is a distance function d ( x,y ) that captures the aforementioned prior knowledge. Learning a proper distance metric [8, 29] is essential for many distance based data mining and machine learning algorithms, such as retrieval, k-means clustering and k-nearest neighbor (k-NN) classification. DML has not received much attention in the Big Data setting, and we are not aware of any distributed implementations of DML.

DML tries to learn a Mahalanobis distance matrix M (symmetric and positive-semidefinite), which can then be used to measure the distance between two samples D ( x,y ) = ( x  X  y ) T M ( x  X  y ). Given a set of  X  X imilar X  sample pairs S = { ( x i ,y i ) } |S| i =1 , and a set of  X  X issimilar X  pairs D = { ( x DML learns the Mahalanobis distance by optimizing min M P s.t. ( x  X  y ) T M ( x  X  y )  X  1 ,  X  ( x,y )  X  X  , and M 0 where M 0 denotes that M is required to be positive semidefinite. This optimization problem tries to minimize the Mahalanobis distances between all pairs labeled as sim-ilar while separating dissimilar pairs with a margin of 1.
This optimization problem is difficult to parallelize due to the constraint set. To create a data-parallel optimization algorithm and implement it on Petuum, we shall relax the constraints via slack variables (similar to SVMs). First, we replace M with L T L , and introduce slack variables  X  to relax the hard constraint in Eq.(4), yielding min L P s.t. k L ( x  X  y ) k 2  X  1  X   X  x,y , X  x,y  X  0 ,  X  ( x,y )  X  X  Using hinge loss, the constraint in Eq.(5) can be eliminated, yielding an unconstrained optimization problem: min L P Unlike the original constrained DML problem, this relax-ation is fully data-parallel, because it now treats the dissim-ilar pairs as iid data to the loss function (just like the similar pairs); hence, it can be solved via data-parallel Stochastic Gradient Descent (SGD). SGD can be naturally parallelized over data, and we partition the data pairs onto P machines. Every iteration, each machine p randomly samples a mini-batch of similar pairs S p and dissimilar pairs D p from its
Figure 5: Petuum DML data-parallel pseudocode. data shard, and computes the following update to L : where I (  X  ) is the indicator function.

Figure 5 shows pseudocode for Petuum DML, which is simple to implement because the parameter server system PS abstracts away complex networking code under a simple get()/read() API. Moreover, the PS automatically ensures high-throughput execution, via a bounded-asynchronous con-sistency model (Stale Synchronous Parallel) that can provide workers with stale local copies of the parameters L , instead of forcing workers to wait for network communication. In Section 5, we will review the strong consistency and conver-gence guarantees provided by the SSP model.

Since DML is a data-parallel algorithm, only the par-allel update push() needs to be implemented (Figure 5). The scheduling function schedule() is empty (because ev-ery worker touches every model parameter L ), and we do not need aggregation push() for this SGD algorithm. In our next example, we will show how schedule() and push() can be used to implement model-parallel execution.
Lasso is a widely used model to select features in high-dimensional problems, such as gene-disease association stud-ies, or in online advertising via ` 1 -penalized regression [11]. Lasso takes the form of an optimization problem: where  X  denotes a regularization parameter that determines the sparsity of  X  , and ` (  X  ) is a non-negative convex loss func-tion such as squared-loss or logistic-loss; we assume that X and y are standardized and consider (8) without an inter-cept. For simplicity but without loss of generality, we let tic) are straightforward and can be solved using the same ap-proach [5]. We shall solve this via a coordinate descent (CD) model-parallel approach, similar but not identical to [5, 25].
The simplest parallel CD Lasso , shotgun [5], selects a ran-dom subset of parameters to be updated in parallel. We now present a scheduled model-parallel Lasso that improves upon shotgun: the Petuum scheduler chooses parameters that are  X  X early independent X  (to be elaborated shortly), thus guar-anteeing convergence of the Lasso objective. In addition, it prioritizes these parameters based on their distance to con-vergence, thus speeding up optimization.

Why is it important to choose independent parameters via scheduling? Parameter dependencies affect the CD update equation in the following manner: by taking the gradient of
Figure 6: Petuum Lasso model-parallel pseudocode. (8), we obtain the CD update for  X  j : where S (  X  , X  ) is a soft-thresholding operator, defined by S (  X  sign(  X  ) ( |  X  | X   X  ). In (9), if x T j x k 6 = 0 (nonzero correlation) ated between features  X  j and  X  k . Hence, they are no longer conditionally independent given the data:  X  j 6 X   X  k | X , y . If the j -th and the k -th coefficients are updated concurrently, parallelization error may occur, causing the Lasso problem to converge slowly (or even diverge outright).

Petuum X  X  schedule() , push() and pull() interface is read-ily suited to implementing scheduled model-parallel Lasso. We use schedule() to choose parameters with low depen-dency, and to prioritize non-converged parameters. Petuum pipelines schedule() and push() ; thus schedule() does not slow down workers running push() . Furthermore, by separating the scheduling code schedule() from the core optimization code push() and pull() , Petuum makes it easy to experiment with complex scheduling policies that involve prioritization and dependency checking, thus facili-tating the implementation of new model-parallel algorithms  X  for example, one could use schedule() to prioritize ac-cording to KKT conditions in a constrained optimization problem, or to perform graph-based dependency checking like in Graphlab [20]. In Section 5, we show that the above Lasso schedule schedule() is guaranteed to converge, and gives us near optimal solutions by controlling errors from parallel execution. The pseudocode for scheduled model par-allel Lasso under Petuum is shown in Figure 6.
We have implemented other data/model-parallel algorithms on Petuum  X  we briefly mention a few, while noting that many others are included in the Petuum open-source library. Topic Model (LDA): For LDA, the key parameter is the  X  X ord-topic X  table, that needs to be updated by all worker machines. We adopt a simultaneous data-and-model-parallel approach to LDA, and use a fixed schedule function sched-ule_fix() to cycle disjoint subsets of the word-topic ta-ble and data across machines for updating (via push() and pull() ), without violating structural dependencies in LDA. Matrix Factorization (MF): High-rank decompositions of large matrices for improved accuracy [35] can be solved by a model-parallel approach, and we implement it via a Figure 7: Key properties of ML algorithms: (a) Non-fixed schedule function schedule_fix() , where each worker machine only performs the model update push() on a dis-joint, unchanging subset of factor matrix rows.
 Deep Learning (DL): We implemented two types on Petuum: a fully-connected Deep Neural Network (DNN) using cross-entropy loss, and a Convolutional NN (CNN) for image clas-sification based off the open-source Caffe project. We adopt a data-parallel strategy schedule_fix() , where each worker uses its data subset to perform updates push() to the full model A . This data-parallel strategy may be amenable to MapReduce, Spark and GraphLab, though we are not aware of DL implementations on them.
Our iterative-convergent formulation of ML programs, and the explicit notions of data and model parallelism, make it convenient to explore three key ML program properties  X  error-tolerant convergence, non-uniform convergence, de-pendency structures (Fig. 7)  X  and to analyze how Petuum exploits these properties in a theoretically-sound manner to speed up ML program completion at Big Learning scales.
Some of these properties have been successfully used in bespoke, large-scale implementations of popular ML algo-rithms: topic models [31, 19], matrix factorization [30, 16], and deep learning [17]. It is notable that MapReduce-style systems (such as Hadoop [27] and Spark [32]) often do not fare competitively against these custom-built ML implemen-tations, and one of the reasons is that the key ML properties are difficult to harness under a MapReduce-like abstraction. Other abstractions may offer a limited degree of opportu-nity  X  for example, vertex programming [20] permits graph dependencies to influence model-parallel execution.
Data-parallel ML algorithms are often robust against mi-nor errors in intermediate calculations; as a consequence, they still execute correctly even when their model parame-ters A experience synchronization delays (i.e. the P work-ers only see old or stale parameters), provided those de-lays are strictly bounded [22, 14, 7, 36, 1, 16]. Petuum exploits this error-tolerance to reduce network communi-cation/synchronization overheads substantially, by imple-menting the Stale Synchronous Parallel (SSP) consistency model [14, 7] on top of the parameter server system, which provides all machines with access to parameters A .
The SSP consistency model guarantees that if a worker reads from parameter server at iteration c , it is guaranteed to receive all updates from all workers computed at and before iteration c  X  s  X  1, where s is the staleness thresh-old. If this is impossible because some straggling worker is more than s iterations behind, the reader will stop until the straggler catches up and sends its updates. For stochastic gradient descent algorithms (such as in the DML program), SSP has very attractive theoretical properties [7], which we partially re-state here:
Theorem 1 (adapted from [7]). SGD under SSP, convergence in probability: Let f ( x ) = P T t =1 f t ( x ) be a convex function, where the f t are also convex. We search for a minimizer x  X  via stochastic gradient descent on each com-ponent  X  f t under SSP, with staleness parameter s and P workers. Let u t :=  X   X  t  X  t f t (  X  x t ) with  X  t =  X  able conditions ( f t are L -Lipschitz and bounded divergence D ( x || x 0 )  X  F 2 ), we have where R [ X ] := P T t =1 f t (  X  x t )  X  f ( x  X  ) , and  X   X  o (1) as T  X  X  X  .
 This means that R [ X ] T converges to O ( T  X  1 / 2 ) in probabil-ity with an exponential tail-bound; convergence is faster when the observed staleness average  X   X  and variance  X   X  are smaller (SSP ensures  X   X  , X   X  are as small as possible). Dai et al. also showed that the variance of x can be bounded, ensuring reliability and stability near an optimum [7].
Naive parallelization of model-parallel algorithms (e.g. co-ordinate descent) may lead to uncontrolled parallelization error and non-convergence, caused by inter-parameter de-pendencies in the model. Such dependencies have been thor-oughly analyzed under fixed execution schedules (where each worker updates the same set of parameters every iteration) [25, 5, 24], but there has been little research on dynamic schedules that can react to changing model dependencies or model state A . Petuum X  X  scheduler allows users to write dy-namic scheduling functions S ( t ) p ( A ( t ) )  X  whose output is a set of model indices { j 1 ,j 2 ,... } , telling worker p to update A 1 ,A j 2 ,...  X  as per their application X  X  needs. This enables ML programs to analyze dependencies at run time (imple-mented via schedule() ), and select subsets of independent (or nearly-independent) parameters for parallel updates.
To motivate this, we consider a generic optimization prob-lem, which many regularized regression problems  X  includ-ing the Petuum Lasso example  X  fit into: Definition: Regularized Regression Problem (RRP) where r ( w ) = P i r ( w i ) is separable and f has  X  -Lipschitz continuous gradient in the following sense: where X = [ x 1 ,..., x d ] are d feature vectors. W.l.o.g., we assume that each feature vector x i is normalized, i.e., k x i k 2 = 1 ,i = 1 ,...,d . Therefore | x &gt; i x j | X  1 for all i,j .
In the regression setting, f ( w ) represents a least-squares loss, r ( w ) represents a separable regularizer (e.g. ` 1 and x i represents the i -th feature column of the design (data) matrix, each element in x i is a separate data sample. In par-ticular, | x &gt; i x j | is the correlation between the i -th and j -th feature columns. The parameters w are simply the regres-sion coefficients.

In the context of the model-parallel equation (3), we can map the model A = w , the data D = X , and the update equation  X ( A,S p ( A )) to where S ( t ) p ( A ) has selected a single coordinate j p dated by worker p  X  thus, P coordinates are updated in every iteration. The aggregation function F () simply allows each update w j p to pass through without change.

The effectiveness of parallel coordinate descent depends on how the schedule S ( t ) p () selects the coordinates j particular, naive random selection can lead to poor conver-gence rate or even divergence, with error proportional to the correlation | x &gt; j a x j b | between the randomly-selected co-ordinates j a ,j b [25, 5]. An effective and cheaply-computable schedule S ( t ) RRP,p () involves randomly proposing a small set of Q &gt; P features { j 1 ,...,j Q } , and then finding P features in this set such that | x &gt; j a x j b | X   X  for some threshold  X  , where j ,j b are any two features in the set of P . This requires at most O ( B 2 ) evaluations of | x &gt; j a x j b | X   X  (if we cannot find P features that meet the criteria, we simply reduce the degree of parallelism). We have the following convergence theorem: where F ( w ) := f ( w ) + r ( w ) and w ? is a minimizer of F . E
P is the average degree of parallelization over all itera-tions  X  we say  X  X verage X  to account for situations where the scheduler cannot select P nearly-independent parameters (e.g. consider a problem where all dimensions are correlated with each other). For most real-world data sets, this is not a problem, and E P is equal to the number of workers. For reference, the Petuum Lasso scheduler uses S RRP (), aug-mented with a prioritizer we will describe soon.

In addition to asymptotic convergence, we show that S RRP trajectory is close to ideal parallel execution: Theorem 3. S RRP () is close to ideal execution: Let S ideal () be an oracle schedule that always proposes P ran-dom features with zero correlation. Let w ( t ) ideal be its param-eter trajectory, and let w ( t ) RRP be the parameter trajectory of S RRP () . Then, for constants C,m,L,  X  P , Proofs for both theorems are in an online supplement 3 .
S RRP () is different from Scherrer et al. [25], who pre-cluster all M features before starting coordinate descent, in order to find  X  X locks X  of nearly-independent parameters. In the Big Data and especially Big Model setting, feature clustering can be prohibitive  X  fundamentally, it requires O ( M 2 ) evaluations of | x &gt; i x j | for all M 2 feature combina-tions ( i,j ), and although greedy clustering algorithms can mitigate this to some extent, feature clustering is still im-practical when M is very large, as seen in some regression problems [11]. The proposed S RRP () only needs to evaluate a small number of | x &gt; i x j | every iteration, and we explain next, the random selection can be replaced with prioritiza-tion to exploit non-uniform convergence in ML problems. http://petuum.github.io/papers/kdd15_supp.pdf
In model-parallel ML programs, it has been empirically observed that some parameters A j can converge in much fewer/more updates than other parameters [18]. For in-stance, this happens in Lasso because the model enforces sparsity, so most parameters remain at zero throughout the algorithm, with low probability of becoming non-zero again. Prioritizing Lasso parameters according to their magnitude greatly improves convergence per iteration, by avoiding fre-quent (and wasteful) updates to zero parameters [18].
We call this non-uniform ML convergence , which can be exploited via a dynamic scheduling function S ( t ) p ( A output changes according to the iteration t  X  for instance, we can write a scheduler S mag () that proposes parameters with probability proportional to their current magnitude ( A pendency structure checking, leading to a dependency-aware, prioritizing scheduler . Unlike the dependency structure is-sue, prioritization has not received as much attention in the ML literature, though it has been used to speed up the PageRank algorithm, which is iterative-convergent [34].
The prioritizing schedule S mag () can be analyzed in the context of the Lasso problem. First, we rewrite it by dupli-cating the original J features with opposite sign, as in [5]: F (  X  ) := min  X  1 2 k y  X  X  X  k 2 2 +  X  P 2 J j =1  X  j . Here, X contains 2 J features and  X  j  X  0, for all j = 1 ,..., 2 J . Theorem 4 (Adapted from [18]). Optimality of Lasso priority scheduler: Suppose B is the set of in-dices of coefficients updated in parallel at the t -th iteration, and  X  X  ( t ) j is the change in  X  j from iteration t  X  1 to t . Let  X  be a sufficiently small constant such that  X  X  X  ( t ) 0 , for all j 6 = k  X  B . Then, the sampling distribution p ( j )  X  (  X  X  ( t ) j ) 2 approximately maximizes a lower bound on This theorem shows that a prioritizing scheduler speeds up Lasso convergence by decreasing the objective as much as possible every iteration. For efficiency, the pipelined Petuum scheduler system approximates p ( j )  X  (  X  X  ( t ) j ) 2 mation from iteration t  X  1; we use p 0 ( j )  X  (  X  ( t  X  1) This is necessary because  X  X  ( t ) j is available only after itera-tion t . Finally, the constant  X  serves as a prior, to ensure all  X   X  X  have a non-zero probability of being updated.
Petuum X  X  ML-centric system design supports a variety of ML programs, and improves their performance on Big Data in the following senses: (1) Petuum implementations of DML and Lasso achieve significantly faster convergence rate than baselines (i.e., DML implemented on single machine, and Shotgun [5]); (2) Petuum ML implementations can run faster than other platforms (e.g. Spark, GraphLab 4 cause Petuum can exploit model dependencies, uneven con-vergence and error tolerance; (3) Petuum ML implementa-tions can reach larger model sizes than other platforms, be-cause Petuum stores ML program variables in a lightweight fashion (on the parameter server and scheduler); (4) for ML programs without distributed implementations, we can im-plement them on Petuum and show good scaling with an in-creasing number of machines. We emphasize that Petuum is, We omit Hadoop, as it is well-established that Spark and GraphLab significantly outperform it [32, 20]. Figure 8: Left: Petuum DML convergence curve with for the moment, primarily about allowing ML practitioners to implement and experiment with new data/model-parallel ML algorithms on small-to-medium clusters; Petuum cur-rently lacks features that are necessary for clusters with  X  1000 machines, such as automatic recovery from machine failure. Our experiments are therefore focused on clusters with 10-100 machines, in accordance with our target users. Experimental settings We used 3 clusters with varying specifications, to show Petuum X  X  adaptability to different hardware:  X  X luster-1 X  machines have 2 AMD cores, 8GB RAM, 1Gbps Ethernet;  X  X luster-2 X  machines have 64 AMD cores, 128GB RAM, 40Gbps Infiniband;  X  X luster-3 X  ma-chines have 16 Intel cores, 128GB RAM, 10Gbps Ethernet. LDA was run on 128 Cluster-1 nodes, using 3.9m English Wikipedia abstracts with unigram ( V = 2 . 5m) and bigram ( V = 21 . 8m) vocabularies. MF and Lasso were run on 10 Cluster-2 nodes, respectively using the Netflix data and a synthetic Lasso dataset with N = 50k samples and 100m fea-tures/parameters. CNN was run on 4 Cluster-3 nodes, using a 250k subset of Imagenet with 200 classes, and 1.3m model parameters. The DML experiment was run on 4 Cluster-2 nodes, using the 1-million-sample Imagenet [10] dataset with 1000 classes (220m model parameters), and 200m sim-ilar/dissimilar statements. In all experiments, we sharded the data over local hard disks.
 Performance of Distance Metric Learning and Lasso We investigate Petuum X  X  DML and Lasso performance: Fig-ure 8 shows the convergence of Petuum and baselines, using a fixed model size (21504  X  1000 distance matrix for DML; 100M features for Lasso). For DML, increasing the machine count consistently increases the convergence speed. Petuum DML achieves 3.8 times speedup with 4 machines and 1.9 times speedup with 2 machines, with potential to continue scaling well with more machines. For Lasso, given the same number of machines, Petuum achieved a significantly faster convergence rate than Shotgun (which randomly selects a subset of parameters to be updated). In the initial stage, Petuum lasso and Shotgun show similar convergence rates because Petuum updates every parameter in the first iter-ation to  X  X ootstrap X  the scheduler (at least one iteration is required to initialize all parameters). After this initial stage, Petuum dramatically decreases the Lasso objective compared to Shotgun, by taking advantage of dependency structures and non-uniform convergence via the scheduler. Platform Comparison Figure 9 (left) compares Petuum to popular ML platforms (Spark and GraphLab) and well-known cluster implementations (YahooLDA [2]). For two common ML programs of LDA and MF, we show the rel-ative speedup of Petuum over the other platforms X  imple-mentations. In general, Petuum is between 2-6 times faster than other platforms; the differences help to illustrate the various data/model-parallel features in Petuum. For MF, Petuum uses the same model-parallel approach as Spark F igure 9: Left: Petuum performance: relative speedup F igure 10: Left: LDA convergence time: Petuum vs and GraphLab, but it performs twice as fast as Spark, while GraphLab ran out of memory. On the other hand, Petuum LDA is nearly 6 times faster than YahooLDA; the speedup mostly comes from scheduling S (), which enables correct, dependency-aware model-parallel execution.
 Scaling to Larger Models We show that Petuum sup-ports larger ML models for the same amount of cluster memory. Figure 10 shows running time versus model size, given a fixed number of machines  X  the left panel compares Petuum LDA and YahooLDA; PetuumLDA converges faster and supports LDA models that are &gt; 10 times larger 5 , allow-ing long-tail topics to be captured. The right panels compare Petuum MF versus Spark and GraphLab; again Petuum is faster and supports much larger MF models (higher rank) than either baseline. Petuum X  X  model scalability is the re-sult of two factors: (1) model-parallelism, which divides the model across machines; (2) a lightweight parameter server system with minimal storage overhead (from using simple arrays and hashmaps). Compared to Petuum, we observed that GraphLab X  X  vertex representation and Spark X  X  RDD representation consumed  X  10x and  X  2-3x memory (re-spectively) to store model variables (Figure 10).
 Fast Cluster Implementations of New ML Programs We show that Petuum facilitates the development of new ML programs without existing cluster implementations. In Figure 9 (right), we present two instances: first, a clus-ter version of the open-source Caffe CNN toolkit, created by adding  X  600 lines of Petuum code. The basic data-parallel strategy was left unchanged, so the Petuum port
LD A model size equals vocab size times number of topics. directly tests Petuum X  X  efficiency. Compared to the orig-inal single-machine Caffe with no network communication , Petuum achieves approaching-linear speedup (3 . 1-times speedup on 4 machines) due to the parameter server X  X  SSP consis-tency for managing network communication. Second, we compare the Petuum DML program against the original DML algorithm proposed in [29] (denoted by Xing2002), which is optimized with SGD on a single-machine (with par-allelization over matrix operations). The intent is to show that a fairly simple data-parallel SGD implementation of DML (the Petuum program) can greatly speed up execu-tion over a cluster. The Petuum implementation converges 3.8 times faster than Xing2002 on 4 machines  X  this pro-vides evidence that Petuum enables data/model-parallel al-gorithms to be efficiently implemented over clusters.
Petuum is a system for writing data-and model-parallel iterative-convergent ML programs, which takes advantage of their properties to speed up convergence: limited error toler-ance, dependency structures between parameters, and non-uniform parameter convergence. We have shown promising results on 100+ machines, though several important prac-tical issues remain to be addressed: fault tolerance strate-gies for 1000s of machines, a standard interface for access-ing input data, integration with the Hadoop/YARN ecosys-tem, and out-of-core model storage for memory-limited sit-uations. We are investigating these as future work.
