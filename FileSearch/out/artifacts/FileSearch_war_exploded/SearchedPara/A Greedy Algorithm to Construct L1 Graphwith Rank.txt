 For graph-oriented learning tasks, a quality graph representation [ 4 ] of input data samples is the key to success. In the past few decades, researchers in machine learning area propose many different methods to solve such tasks, for exam-ple, k -nearest neighbor (kNN) graph and -ball graphs. These methods are very straightforward and proved to be efficient for general data. The reason of these methods X  success is that their construction algorithm acts as a local smooth  X  X ilter X  which sets the weight between faraway data points and source point to zero. The built graph is constructed by many such local star-shape patches (or subgraphs). However, both of them need a user-specified parameter such as k or which is chosen empirically. Considering the versatility and uncertainty of the real world data, a bad selection of parameter k and will lead to an inac-curate conclusion for subsequent machine learning tasks. Recently, a nonpara-metric graph called L 1 graph is proposed by Cheng et al. [ 2 ]. Based on existing sparse representation frameworks [ 10 , 12 ], the construction algorithm of can be described as follows: Given an input data samples X =[ x where each x i ,i  X  [1 ,  X  X  X  ,n ] is a vector that represents one single data sample. The
L dictionary constructed from all data samples except x i itself. The coefficient of sparse coding is used as the edge weight of resulted L 1 -graph. The mathematical definition of sparse coding is: where dictionary  X  i = [ x 1 ,  X  X  X  ,x i  X  1 ,x i +1 ,  X  X  X  ,x sparse code of x i . The coefficients of  X  i could be negative, depending on the choices of L 1 minimization solvers. To make them have the physical meaning of  X  X imilarity X , the absolute value or nonnegative constraints are employed. As we could see from the above description, the L algorithm is nonparametric and the user is not required to input any para-meters except for the solver. The construction algorithm is a pure numerical process based on convex optimization. Cheng et al. [ 2 ] show that three advantages comparing to traditional graph construction methods. They are: (1) robustness to data noise; (2) sparsity; (3) datum-adaptive neighborhood. Their experimental results also prove that L 1 graph has significant performance improvement in many machine learning applications such as spectral cluster-ing, subspace learning, semi-supervised learning, etc. [ 2 ]. Nevertheless, just like each sword has double edges, L 1 graph also bears some disadvantages such as: (1) sensitive to duplications. For example, if every data sample has a duplica-tion, the resulted L 1 graph will only have edge connections between the data sample and its duplication; (2) randomness, the edge and edge weight are highly dependent on the solver; (3) high computational cost [ 2 ]; (4) lost of the local-ity [ 6 , 7 , 15 ]. To overcome these disadvantages, many improved algorithms have such as geometry structure, group effects, etc. However, those algorithms still have high computational cost. This is unpleasant for the large-scale dataset in this  X  X ig-data X  era. To improve, in this paper we propose a greedy algorithm to generate L 1 graph. The generated graphs are called Greedy-algorithm employs greedy L 1 minimization solvers and is based on non-negative orthogonal matching pursuit (NNOMP). Furthermore, we use ranked dictio-naries with reduced size K which is a user-specified parameter. We provide the freedom to the user to determine the ranking strategy such as nearest neighbors, or diffusion ranking [ 3 ]. Our algorithm has significant time-reduction about gen-erating L 1 graphs. Comparing to the original L 1 graph construction method, our algorithm loses the nonparametric characteristics and is only offering a sub-optimal solution. However, our experimental results show that the graph gener-ated by our algorithm has equal (or even better) performance as the original graph by setting K equals to the length of data sample. Our work is a natural extension of existing L 1 graph research. A concise summary of the connection between our proposed Greedy-L 1 graph and other graphs is illustrated in Fig. 1 . The main contributions of our paper can be summarized by 1. We propose a greedy algorithm to reduce the computational time of generat-ing L 1 graph. 2. We introduce the Ranked Dictionary for L 1 minimization solver. This new dictionary not only reduces the time of minimization process but also pre-serves the locality and geometry structure information of input data. 3. Our algorithm removes the randomness of edges in final serves the uniqueness except for the edge weights. Moreover, our algorithm can generate L 1 graphs with lower sparsity. 4. We present experiment and analysis results by applying our algorithm to spec-tral clustering application with different datasets. Our experimental results show that the graphs generated by our proposed greedy algorithm have equal clustering performance even though it is only providing a sub-optimal solution. The organization of our paper is as follows. First, an overview of the disadvan-tages of original L 1 graph construction algorithm will be presented in Sect. 2 . Sec-ond, we will introduce our proposed greedy algorithm in Sect. 3 . After that, we will give a review of existing works on how to improve the quality of we will present our experimental results in Sect. 5 and draw conclusion in Sect. 6 . In this section, we make our attempts to address two problems of original graph construction algorithm. They are: (1) curse of dictionary normalization, and (2) non-local edges. 2.1 Curse of Dictionary Normalization While solving L 1 minimization, the atoms of dictionary are normalized to have unit length. The goal of this step is to satisfy the theoretic requirement of Com-pressive Sensing. The less-ideal part about this normalization is that it is not preserving neighborhood information of input data. This can be illustrated in Fig. 3 . To illustrate this phenomenon, we manually create a toy dataset in 2D and it has two clusters visually. After normalization, we can see that the neigh-bors of a node are changed. This normalization step projects all data samples onto a unit hypersphere and the original geometry structure information is lost. 2.2 Non-local Edges During the construction of L 1 graph, an over-complete dictionary is required for each data sample. The original method simply selects all other data samples as the dictionary. This strategy affords the nonparametric property of However, it also introduces non-local edges. In other words, it doesn X  X  preserve the locality of input data [ 7 ]. This phenomenon can be illustrated in Fig. 4 , In this section, we introduce the concept of ranked dictionary with two different strategies: Euclidean metric and diffusion metric. Furthermore, we present our pro-posed greedy algorithm and describe how to generate Greedy-3.1 Ranked Dictionary The use of k -nearest neighbors as dictionary is proved to have better quality than original L 1 graph [ 7 ]. However, it can not solve the dilemma that there might exist data samples with the same direction but different length in input data. The dictionary normalization process will project them onto to the same location at hypersphere. Since they have the same values, the solver will choose one of them randomly. To avoid this randomness, we need to rank those atoms (or data samples) of dictionary (Fig. 5 ).
 Euclidean Metric. Using Euclidean metric to rank atoms of dictionary is quite straightforward. We rank them by distance. The shorter distance will have a higher rank score. The Euclidean distance is defined as: Diffusion Metric. As pointed out by Yang et al. [ 14 ], many real-world datasets are similar to an intrinsic low dimensional manifold embedded in high dimen-sional ambient space, and the geometry structure of manifold can be used to improve the performance of learning algorithms. We now present a strategy to search dictionaries following the geometry structure of input data. Based on the diffusion theory [ 3 , 5 ], we rank the atoms of dictionary through diffusion matrix. tion matrix; (3) definition of the diffusion process. In our setting, the first stage is to build an affinity matrix A from the input dataset X kernel to define the pairwise distance: where A ( i, j ) is the distance between data sample x i and data sample a normalization parameter. In our configuration, we use the median of K nearest neighbors to tune  X  . The second stage is to define the transition matrix where D is a n  X  n degree matrix defined as Now the diffusion process can be defined as: where W 0 = A and t is the number of steps for diffusion steps. Each row of is the diffusion ranking scores. In this paper, we let t equal to K for the sake of simplicity. Once W t is calculated, the first K data samples with top scores of each row is selected as dictionary. The algorithmic details can be documented as follows: Algorithm 1. Diffusion Dictionary 3.2 Greedy-L 1 Graph We now propose a greedy algorithm to build L 1 graph. Our proposed algorithm is based on non-negative orthogonal matching pursuit (NNOMP) [ 1 , 9 ]. By using this solver, we switch the L 1 minimization problem ( P1 ) back to the original optimization with non-negative constraints ( P2 )as: The main difference between our algorithm and the original NNOMP [ 1 ]is that the atoms of dictionary are ranked. We let the solver choose and assign higher coefficient values to atoms that are closer to source data sample. The detailed processes are described in Algorithm 2 .
 Algorithm 2. Greedy Solver Original L 1 graph [ 2 ] is a pure numerical result and doesn X  X  exploit the physi-cal and geometric information of input data. To improve the quality of several research works are proposed to use the intrinsic structure information of data by adding one or several regularization terms to the Graph Laplacian regularization [ 14 ].
 cost. Zhou et al. [ 16 ]proposea k NN-fused Lasso graph by using the idea of k -nearest neighbors in kernel feature space. With a similar goal, Fang et al. [ 6 ] propose an algorithm which transfers the data into reproducing kernel Hilbert space and then projects them into a lower dimensional subspace. By these oper-ations, the dimension of the dataset is reduced and the computational time is reduced. We now present our experimental results. We first document our configuration of parameters and datasets. Second, we evaluate the effectiveness of our proposed graph construction methods through spectral clustering application. To satisfy the input of spectral clustering algorithm, we transform the adjacency matrix of L 1 graph and experiments are carried out by using Matlab on a PC with Intel 4-core 3.4 GHz CPU and 16 GB RAM. 5.1 Experimental Setup Datasets. To demonstrate the performance of our proposed algorithm, we eval-uate it on seven UCI benchmark datasets including three biological data sets (BreastTissue, Iris, Soybean), two vision image data sets (Vehicle, Image), one chemistry data set (Wine), and one physical data set (Glass), whose statistics are summarized in Table 1 . All of these data sets have been popularly used in spectral clustering analysis research. The diverse combinations of data sets are necessary for our comprehensive studies.
 Parameters Setting. In our experiments, we use the l1 ls solver [ 8 ] for original L graph construction algorithms. We set the solver X  X  parameter  X  to 0 . 1. The threshold  X  threshold of Greedy solver Algorithm 2 is set to 1 e graph and Greedy-L 1 graph, we select three different K values and document their clustering performance results respectively. The K is set to be the multiple of data attribute size. 5.2 Spectral Clustering Performance Baseline. To evaluate the quality of our algorithms, we compare the spectral clustering performance with Gaussian similarity graph, and original The results are documented in Tables 2 and 3 .
 Evaluation Metrics. We evaluate the spectral clustering performance with Normalized Mutual Information (NMI) and Accuracy (AC). NMI value ranges from 0 to 1, with higher values meaning better clustering performance. AC is another metric to evaluate the clustering performance by measuring the fraction of its clustering result that are correct. It X  X  value also ranges from 0 to 1, and the higher the better.
 Greedy-L 1 Graph vs. Gaussian Graph. Overall, the Greedy-using Euclidean metric has better average spectral clustering performance than Gaussian graphs. However, since the Gaussian graph we used are not tuned, the best clustering performance of Gaussian graphs may not occur in our experiments. Greedy-L 1 Graph vs. L 1 Graph. Greedy-L 1 graph has better clustering per-formance than L 1 graph on average. However, for iris and soybean datasets, the L 1 graph shows the best clustering result: Iris (NMI = 0.5943, AC = 0.74); Soy-bean (NMI = 0.7373, AC = 0.6156). The best result of Greedy-Iris (NMI = 0.5106, AC = 0.72); Soybean (NMI = 0.6919, AC = 0.5244). Euclidean Metric vs. Diffusion Metric. The Euclidean metric appears to have better clustering performance than that of diffusion metric in general. This is rather a surprising result to us. Only for Iris dataset, the result of diffusion metric is better than that of Euclidean metric. 5.3 Discussions Running Time. We report the running time of generating L 1 ferent construction algorithms. As we can see from Fig. 6 , the Greedy-have consumed significantly less construction time than that in original Graph Sparsity. We check the sparsity of graphs by calculating the edge density: The results are reported in Table 4 . We can see that Greedy-diffusion metric are more sparse than that with Euclidean metric. In this paper, we have devised a greedy algorithm to construct over, we introduced the concept of ranked dictionary for our greedy solver. Except for the Euclidean metric and diffusion metric that have been discussed in this paper, the user can choose other ranking methods such as manifold rank-ing that could be more appropriate for specific dataset in real applications. Our greedy algorithm can generate sparse L 1 graph faster than the original construction algorithm, and the resulting graphs have better clustering perfor-mance on average than original L 1 graph. Nevertheless, our algorithm could be generalized in a straightforward way by introducing regularization terms such as elastic net into the current solver, which would indicate the quality of generated L graphs could be further improved.

