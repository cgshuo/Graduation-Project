 Department of Statistics, University of Oxford, Oxford OX1 3TG, UK Consider a dataset X = { x 1 ,...,x n } and denote by p ( x 1 ,...,x n |  X  ) the associated likelihood for parameter  X   X   X  . Henceforth we assume that the data are conditionally in-dependent, so that p ( x 1 ,...,x n |  X  ) = Q n i =1 p ( x a prior p (  X  ) , Bayesian inference relies on the posterior terior is intractable and we need to rely on Bayesian com-putational tools to approximate it.
 A standard approach to sample approximately from  X  (  X  ) is the Metropolis-Hastings algorithm (MH; Robert &amp; Casella (2004, Chapter 7.3)). MH consists in building an ergodic Markov chain of invariant distribution  X  (  X  ) . Given a pro-posal q (  X  0 |  X  ) , the MH algorithm starts its chain at a user-defined  X  0 , then at iteration k + 1 it proposes a candidate state  X  0  X  q (  X |  X  k ) and sets  X  k +1 to  X  0 with probability while  X  k +1 is otherwise set to  X  k . When the dataset is large ( n 1 ), evaluating the likelihood ratio appearing in the MH acceptance ratio (1) is too costly an operation and rules out the applicability of such a method.
 The aim of this paper is to propose an approximate imple-mentation of this  X  X deal X  MH sampler, the maximal approx-imation error being pre-specified by the user. To achieve this, we first present the  X  X deal X  MH sampler in a slightly non-standard way.
 In practice, the accept/reject step of the MH step is imple-mented by sampling a uniform random variable u  X  X  (0 , 1) and accepting the candidate if and only if In our specific context, it follows from (2) and the indepen-dence assumption that there is acceptance of the candidate if and only if where for  X , X  0  X   X  we define the average log likelihood ratio  X  n (  X , X  0 ) by and where The pseudocode of MH is given in Figure 1, unusually for-mulated using the expression (3). The advantage of this presentation is that it clearly outlines that the accept/reject step of MH requires checking whether or not (3) holds. So as to save computational efforts, we would like to be able to decide whether (3) holds using only a Monte Carlo approximation of  X  n (  X , X  0 ) based on a subset of the data. There is obviously no hope to be able to guarantee that we will take the correct decision with probability 1 but we would like to control the probability of taking an er-roneous decision. Korattikara et al. (2014) propose in a similar large datasets context to control this error using an approximate confidence interval for the Monte Carlo esti-mate. Similar ideas have actually appeared earlier in the operations research literature. Bulgak &amp; Sanders (1988), Alkhamis et al. (1999) and Wang &amp; Zhang (2006) consider maximizing a target distribution whose logarithm is given by an intractable expectation; in the large dataset scenario this expectation is w.r.t the empirical measure of the data. They propose to perform this maximization using simu-lated annealing, a non-homogeneous version of MH. This is implemented practically by approximating the MH ratio log  X  (  X  0 ) / X  (  X  ) through Monte Carlo and by determining an approximate confidence interval for the resulting esti-mate; see also (Singh et al., 2012) for a similar idea devel-oped in the context of inference in large-scale factor graphs. All these approaches rely on approximate confidence inter-vals so they do not allow to control rigorously the approxi-mation error. Moreover, the use of approximate confidence intervals can yield seriously erroneous inference results as demonstrated in Section 4.1.
 The method presented in this paper is a more robust al-ternative to these earlier proposals, which can be analyzed theoretically and whose properties can be better quantified. As shown in Section 2, it is possible to devise an adaptive sampling strategy which guarantees that we take the cor-rect decision, i.e. whether (3) holds or not, with at worst a user-specified maximum probability of error. This sam-pling strategy allows us to establish in Section 3 various quantitative convergence results for the associated Markov kernel. In Section 4, we compare our approach to the one by Korattikara et al. (2014) on a toy example and demon-strate the performance of our methodology on a large-scale Bayesian logistic regression problem. All proofs are in the supplemental paper. In this section, we use concentration bounds so as to obtain exact confidence intervals for Monte Carlo approximations of the log likelihood ratio (4). We then show how such bounds can be exploited so as to build an adaptive sampling strategy with desired guarantees. 2.1. MC approximation of the log likelihood ratio Let  X , X  0  X   X  . For any integer t  X  1, a Monte Carlo approx-imation  X   X  t (  X , X  0 ) of  X  n (  X , X  0 ) is given by where x  X  1 ,...,x  X  t are drawn uniformly over { x 1 ,...,x without replacement .
 We can quantify the precision of our estimate  X   X  t (  X , X   X  n (  X , X  0 ) through concentration inequalities, i.e., a state-ment that for  X  t &gt; 0 , for a given c t . Hoeffding X  X  inequality without replacement Serfling (1974), for instance, uses where and f  X  t = t  X  1 n is approximately the fraction of used sam-ples. The term (1  X  f  X  t ) in (7) decreases to 1 n as the num-ber t of samples used approaches n , which is a feature of bounds corresponding to sampling without replacement. Let us add that C  X , X  0 typically grows slowly with n . For instance, if the likelihood is Gaussian, then C  X , X  0 is pro-portional to max n i =1 | x i | , so that if the data actually were sampled from a Gaussian, C  X , X  0 would grow in p log( n ) (Cesa-Bianchi &amp; Lugosi, 2009, Lemma A.12).
 If the empirical standard deviation  X   X  t of { log p ( x log p ( x i |  X  ) } is small, a tighter bound known as the empiri-cal Bernstein bound (Audibert et al., 2009) applies. While the bound of Audibert et al. (2009) origi-nally covers the case where the x  X  i are drawn with replace-ment , it was early remarked (Hoeffding, 1963) that Cher-noff bounds, such as the empirical Bernstein bound, still hold when considering sampling without replacement. Fi-nally, we will also consider the recent Bernstein bound of Bardenet &amp; Maillard (2013, Theorem 3), designed specifi-cally for the case of sampling without replacement. 2.2. Stopping rule construction The concentration bounds given above are helpful as they can allow us to decide whether (3) holds or not. Indeed, on the event {|  X   X  t (  X , X  0 )  X   X  n (  X , X  0 ) |  X  c t } , we can de-cide whether or not  X  n (  X , X  0 ) &gt;  X  ( u, X , X  0 ) if |  X   X  ( u, X , X  0 ) | &gt; c t additionally holds. This is illustrated in Figure 2. Combined to the concentration inequality (6), we thus take the correct decision with probability at least 1  X   X  t if |  X   X  t (  X , X  0 )  X   X  ( u, X , X  0 ) | &gt; c |  X  t (  X , X  til the condition |  X   X  t (  X , X  0 )  X   X  ( u, X , X  0 ) | &gt; c Let  X   X  (0 , 1) be a user-specified parameter. We provide a construction which ensures that at the first random time T sion is taken with probability at least 1  X   X  . This adaptive stopping rule adapted from (Mnih et al., 2008) is inspired by bandit algorithms, Hoeffding races (Maron &amp; Moore, 1993) and procedures developed to scale up boosting al-gorithms to large datasets (Domingo &amp; Watanabe, 2000). Formally, we set the stopping time
T = n  X  inf { t  X  1 : |  X   X  t (  X , X  0 )  X   X  ( u, X , X  0 ) | &gt; c where a  X  b denotes the minimum of a and b . In other words, if the infimum in (10) is larger than n , then we stop as our sampling without replacement procedure en-sures  X   X  n (  X , X  0 ) =  X  n (  X , X  0 ) . Letting p &gt; 1 and selecting  X  that (6) holds, the event has probability larger than 1  X   X  under sampling without re-placement by a union bound argument. Now by definition of T , if E holds then  X   X  T (  X , X  0 ) yields the correct decision, as pictured in Figure 2.
 A slight variation of this procedure is actually implemented in practice; see Figure 3. The sequence (  X  t ) is decreas-ing, and each time we check in Step 19 whether or not we should break out of the while condition, we have to use a smaller  X  t , yielding a smaller c t . Every check of Step 19 thus makes the next check less likely to succeed. Thus, it appears natural not to perform Step 19 systematically after each new x  X  i has been drawn, but rather draw several new subsamples x  X  i between each check of Step 19. This is why we introduce the variable t look is Steps 6, 16, and 17 of Fig-ure 3. This variable simply counts the number of times the check in Step 19 was performed. Finally, as recommended in a related setting in (Mnih et al., 2008; Mnih, 2008), we augment the size of the subsample geometrically by a user-input factor  X  &gt; 1 in Step 18. Obviously this modification does not impact the fact that the correct decision is taken with probability at least 1  X   X  .
 In Section 3.1 we provide an upper bound on the total variation norm between the iterated kernel of the approx-imate MH kernel and the target distribution  X  of MH ap-plied to the full dataset, while Section 3.2 focuses on the number T of subsamples required by a given iteration of MHS UB L HD . We establish a probabilistic bound on T and give a heuristic to determine whether a user can expect a substantial gain in terms of number of samples needed for the problem at hand. 3.1. Properties of the approximate transition kernel For  X , X  0  X   X  , we denote by the  X  X deal X  MH kernel targeting  X  with proposal q , where the acceptance probability  X  (  X , X  0 ) is defined in (1). Denote the acceptance probability of MHS UB L HD in Figure 3 by where the expectation in (13) is with respect to u  X  U (0 , 1) and the variables T and x  X  1 ,...,x  X  T described in Section 2. Finally, denote by  X  P the MHS UB L HD kernel, obtained by substituting  X   X  to  X  in (12). The following Lemma states that the absolute difference between  X  and  X   X  is bounded by the user-defined parameter  X  &gt; 0 .
 Lemma 3.1. For any  X , X  0  X   X  , we have |  X  (  X , X  0 )  X   X   X  (  X , X  0 ) | X   X  .
 Lemma 3.1 can be used to establish Proposition 3.2, which states that the chain output by the algorithm MHS UB L HD in Figure 3 is a controlled approximation to the original target  X  . For any signed measure  X  on ( X  , B ( X )) , let norm, where  X  ( f ) = R  X  f (  X  )  X  ( d X  ) . For any Markov kernel Q on ( X  , B ( X )) , we denote by Q k be the k -th iterate kernel defined by induction for k  X  2 through Q k (  X ,d X  0 ) = R Proposition 3.2. Let P be uniformly geometrically er-godic, i.e., there exists an integer m and a probability mea-sure  X  on ( X  , B ( X )) such that for all  X   X   X  , P m (  X ,  X  )  X  (1  X   X  )  X  (  X  ) . Hence there exists A &lt;  X  such that Then there exists B &lt;  X  and a probability distribution  X   X  on ( X  , B ( X )) such that for all  X   X   X  and k &gt; 0 , k  X 
P k (  X ,  X  )  X   X   X  k TV  X  B [1  X  (1  X   X  ) m (1  X   X  )] b k/m c Furthermore,  X   X  satisfies One may obtain tighter bounds and ergodicity results by weakening the uniform geometric ergodicity assumption and using recent results on perturbed Markov kernels (Ferr  X  e et al., 2013), but this is out of the scope of this paper. 3.2. On the stopping time T The following Proposition gives a probabilistic upper bound for the stopping time T , conditionally on  X , X  0  X   X  and u  X  [0 , 1] and when c t is defined by (7). A similar bound holds for the empirical Bernstein bound in (9). Proposition 3.3. Let  X  &gt; 0 and  X  t = p  X  1 pt p  X  . Let  X , X  such that C  X , X  0 6 = 0 and u  X  [0 , 1] . Let and assume  X  6 = 0 . Then if p &gt; 1 , with probability at least 1  X   X  ,
T  X  The relative distance  X  in (18) characterizes the diffi-culty of the step. Intuitively, at equilibrium, i.e., when log p ( x |  X  ) is smooth in  X  , the proposal could be chosen so that  X  n (  X , X  0 ) has positive expectation and a small variance, thus leading to high values of  X  and small values of T . Integrating (18) with respect to  X , X  0 to obtain an informa-tive quantitative bound on the average number of samples required by MHS UB L HD at equilibrium would be desir-able but proved difficult. However the following heuris-tic can help the user figure out whether our algorithm will yield important gains for a given problem. For large n , standard asymptotics (van der Vaart, 2000) yield that the log likelihood is approximately a quadratic form with H n of order n . Assume the proposal q (  X |  X  ) is a Gaus-sian random walk N (  X |  X ,  X ) of covariance  X  , then the ex-mately Trace ( H n  X ) . According to (Roberts &amp; Rosenthal, 2001), an efficient random walk Metropolis requires  X  to be of the same order as H  X  1 n , that is, of order 1 /n . Finally, the expected  X  n (  X , X  0 ) at equilibrium is of order 1 /n , and can thus be compared to  X  ( u, X , X  0 ) = log( u ) /n in Line 19 of MHS UB L HD in Figure 3. The RHS of the first inequal-ity in Step 19 is the concentration bound c t , which has a leading term in  X   X  t / tions we consider in Section 4,  X   X  t is typically proportional to k  X   X   X  0 k , which is of order to break out of the while loop in Line 19, we need t  X  n . At equilibrium, we thus should not expect gains of several orders of magnitude: gains are fixed by the constants in the proportionality relations above, which usually depend on the empirical distribution of the data. We provide a de-tailed analysis for a simple example in Section 4.3. All experiments were conducted using the empirical Bernstein-Serfling bound of Bardenet &amp; Maillard (2013), which revealed equivalent to the empirical Bernstein bound in (9), and much tighter in our experience with MHS UB L HD than Hoeffding X  X  bound in (7). All MCMC runs are adaptive Metropolis (Haario et al., 2001; Andrieu &amp; Thoms, 2008) with target acceptance 25% when the di-mension is larger than 2 and 50% else (Roberts &amp; Rosen-thal, 2001). Hyperparameters of MHS UB L HD were set to p = 2 ,  X  = 2 , and  X  = 0 . 01 . The first two were found to work well with all experiments. We found empirically that the algorithm is very robust to the choice of  X  . 4.1. On the use of asymptotic confidence intervals MCMC algorithms based on subsampling and asymptotic confidence intervals experimentally lead to efficient opti-mization procedures (Bulgak &amp; Sanders, 1988; Alkhamis et al., 1999; Wang &amp; Zhang, 2006), and perform well in terms of classification error when used, e.g., in logistic re-gression (Korattikara et al., 2014). However, in terms of approximating the original posterior, they come with no guarantee and can provide unreliable results.
 To illustrate this, we consider the following setting. X is a synthetic sample of size 10 5 drawn from a Gaus-sian N (0 , 0 . 1 2 ) , and we estimate the parameters  X , X  of a N (  X , X  2 ) model, with flat priors. Analytically, we know that the posterior has its maximum at the empirical mean and variance of X . Running the approximate MH algo-rithm of Korattikara et al. (2014), using a level for the test = 0 . 05 , and starting each iteration by looking at t = 2 points so as to be able to compute the variance of the log likelihood ratios, leads to the marginal of  X  shown in Fig-ure 4(a).
 The empirical variance of X is denoted by a red trian-gle, and the maximum of the marginal is off by 7% from this value. Relaunching the algorithm, but starting each iteration with a minimum t = 500 points, leads to bet-ter agreement and a smaller support for the marginal, as depicted in Figure 4(b). Still, t = 500 works better for this example, but fails dramatically if X are samples from a lognormal log N (0 , 2) , as depicted in Figure 4(c). The asymptotic regime, in which the studentized statistic used in (Korattikara et al., 2014) actually follows a Student dis-tribution, depends on the problem at hand and is left to the user to specify. In each of the three examples of Figure 4, our algorithm produces significantly better estimates of the marginal, though at the price of a significantly larger aver-age number of samples used per MCMC iteration. In par-ticular, the case X  X  log N (0 , 2) in Figure 4(c) essentially requires to use the whole dataset. 4.2. Large-scale Bayesian logistic regression In logistic regression, an accurate approximation of the posterior is often needed rather than minimizing the clas-sification error, for instance, when performing Bayesian variable selection. This makes logistic regression for large datasets a natural application for our algorithm, since the constant C  X , X  0 in concentration inequalities such as (9) can be computed as follows. The log likelihood log p ( y | x, X  ) =  X  log(1 + e  X   X  T x )  X  (1  X  y )  X  T x (19) is L -Lipschitz in  X  with L = k x k , so that we can set We expect the Lipschitz inequality to be tight as (19) is almost linear in  X  . 4.2.1. T HE covtype DATASET We consider the dataset covtype.binary 1 described in (Col-lobert et al., 2002). The dataset consists of 581,012 points, of which we pick n = 400 , 000 as a training set, following the maximum training size in (Collobert et al., 2002). The original dimension of the problem is 54, with the first 10 attributes being quantitative. To illustrate our point with-out requiring a more complex sampler than MH, we con-sider a simple variant of the classification problem using the first q = 2 attributes only. We use the preprocessing and Cauchy prior recommended by Gelman et al. (2008). We draw four random starting points and launch inde-pendent runs of both the traditional MH in Figure 1 and our MHS UB L HD in Figure 3 at each of these four start-ing points. Figure 5 shows the results: plain lines in-dicate traditional MH runs, while dashed lines indicate runs of MHS UB L HD . Figures 5(c) and 5(d) confirm that MHS UB L HD accurately approximates the target poste-rior. In all Figures 5(a) to 5(d), MHS UB L HD reproduces the behaviour of MH, but converges up to 3 times faster. However, the most significant gains in number of sam-ples used happen in the initial transient phase. This al-lows fast progress of the chains towards the mode but, once in the mode, the average number of samples required by MHS UB L HD is close to n . We observed the same be-haviour when considering all q = 10 quantitative attributes of the dataset, as depicted by the train error in Figure 7(a). To investigate the r  X  ole of n in the gain, we generate a 2D bi-nary classification dataset of size n = 10 7 . Given the label, both classes are sampled from unit Gaussians centered on the x-axis, and a subsample of X is shown in Figure 4(d). The results are shown in Figure 6. The setting appears more favorable than in Section 4.2.1, and MHS UB L HD chains converge up to 5 times faster. The average number of sam-ples used is smaller, but it is still around 70% after the tran-sient phase for all approximate chains. 4.3. A Gaussian example To further investigate when gains are made at equilibrium, we now consider inferring the mean  X  of a N (  X , 1) model, using a sample X  X  N ( 1 2 , X  2 X ) of size n . Although sim-ple, this setting allows us analytic considerations. The log likelihood ratio is so that we can set We also remark that Under the equilibrium assumptions of Section 3.2.2, |  X   X   X  | is of order n  X  1 / 2 , so that the leading term t  X  1 / 2 the concentration inequality (9) is of order  X  X n  X  1 / 2 Thus, to break out of the while loop in Line 19 in Figure 3, we need t  X   X  2 X n . In a nutshell, larger gains are thus to be expected when data are clustered in terms of log likelihood. To illustrate this phenomenon, we set  X  X = 0 . 1 . To in-vestigate the behavior at equilibrium, all runs were started at the mode of a subsampled likelihood, using a proposal covariance matrix proportional to the covariance of the tar-get. In Figure 7(b), we show the running average number of samples needed for 6 runs of MHS UB L HD , with n ranging from 10 5 to 10 15 . With increasing n , the number of sam-ples needed progressively drops to 25% of the total n . This is satisfying, as the number of samples required at equilib-rium should be less than 50% to actually improve on usual MH, since a careful implementation of MH in Figure 1 only requires to evaluate one single full likelihood per iteration, while methods based on subsampling require two. We have presented an approximate MH algorithm to per-form Bayesian inference for large datasets. This is a robust alternative to the technique in (Korattikara et al., 2014), and this robustness comes at an increased computational price. We have obtained theoretical guarantees on the re-sulting chain, including a user-controlled error in total vari-ation, and we have demonstrated the methodology on sev-eral applications. Experimentally, the resulting approxi-mate chains achieve fast burn-in, requiring on average only a fraction of the full dataset. At equilibrium, the per-formance of the method is strongly problem-dependent. Loosely speaking, if the expectation w.r.t.  X  (  X  ) q (  X  distribution of the observations is low, then one can expect significant gains. If this expectation is high, then the al-gorithm is of limited interest as the Monte Carlo estimate  X  (  X , X  0 ) requires many samples t to reach a reasonable variance. It would be desirable to use variance reduction techniques but this is highly challenging in this context. Finally, the algorithm and analysis provided here can be straightforwardly extended to scenarios where  X  is such that log  X  (  X  0 ) / X  (  X  ) is intractable, as long as a concentra-tion inequality for a Monte Carlo estimator of this log ratio is available. For models with an intractable likelihood, it is often possible to obtain such an estimator with low vari-ance, so the methodology discussed here may prove useful. A R  X  emi Bardenet acknowledges his research fellowship through the 2020 Science programme, funded by EPSRC grant number EP/I017909/1. Chris Holmes is supported by an EPSRC programme grant and a Medical Research Council Programme Leader X  X  award.
 Alkhamis, T. M., Ahmed, M. A., and Tuan, V. K. Simulated annealing for discrete optimization with estimation. Eu-ropean Journal of Operational Research , 116:530 X 544, 1999.
 Andrieu, C. and Thoms, J. A tutorial on adaptive MCMC. Statistics and Computing , 18:343 X 373, 2008.
 Audibert, J.-Y., Munos, R., and Szepesv  X  ari, Cs.
Exploration-exploitation trade-off using variance esti-mates in multi-armed bandits. Theoretical Computer Science , 2009.
 Bardenet, R. and Maillard, O.-A. Concentrations inequal-ities for sampling without replacement. preprint, 2013. URL arxiv.org/abs/1309.4029 .
 Bulgak, A. A. and Sanders, J. L. Integrating a modified simulated annealing algorithm with the simulation of a manufacturing system to optimize buffer sizes in auto-matic assembly systems. In Proceedings of the 20th Win-ter Simulation Conference , 1988.
 Cesa-Bianchi, N. and Lugosi, G. Combinatorial bandits. In Proceedings of the 22nd Annual Conference on Learning Theory , 2009.
 Collobert, R., Bengio, S., and Bengio, Y. A parallel mixture of SVMs for very large scale problems. Neural Compu-tation , 14(5):1105 X 1114, 2002.
 Domingo, C. and Watanabe, O. MadaBoost: a modification of AdaBoost. In Proceedings of the Thirteenth Annual
Conference on Computational Learning Theory (COLT) , 2000.
 Ferr  X  e, D., L., Herv  X  e, and Ledoux, J. Regular perturbation of V-geometrically ergodic Markov chains. Journal of Applied Probability , 50(1):184 X 194, 2013.
 Gelman, A., Jakulin, A, Pittau, M. G., and Su, Y-S. A weakly informative default prior distribution for logistic and other regression models. Annals of applied Statis-tics , 2008.
 Haario, H., Saksman, E., and Tamminen, J. An adaptive Metropolis algorithm. Bernoulli , 7:223 X 242, 2001. Hoeffding, W. Probability inequalities for sums of bounded random variables. Journal of the American Statistical Association , 58:13 X 30, 1963.
 Korattikara, A., Chen, Y., and Welling, M. Austerity in MCMC land: Cutting the Metropolis-Hastings budget.
In Proceedings of the International Conference on Ma-chine Learning (ICML) , 2014.
 Maron, O. and Moore, A. Hoeffding races: Accelerating model selection search for classification and function ap-proximation. In Advances in Neural Information Pro-cessing Systems . The MIT Press, 1993.
 Mnih, V. Efficient stopping rules. Master X  X  thesis, Univer-sity of Alberta, 2008.
 Mnih, V., Szepesv  X  ari, Cs., and Audibert, J.-Y. Empirical
Bernstein stopping. In Proceedings of the 25th Interna-tional Conference on Machine Learning (ICML) , 2008. Robert, C. P. and Casella, G. Monte Carlo Statistical Meth-ods . Springer-Verlag, New York, 2004.
 Roberts, G. O. and Rosenthal, J. S. Optimal scaling for various Metropolis-Hastings algorithms. Statistical Sci-ence , 16:351 X 367, 2001.
 Serfling, R. J. Probability inequalities for the sum in sam-pling without replacement. The Annals of Statistics , 2 (1):39 X 48, 1974.
 Singh, S., Wick, M., and McCallum, A. Monte carlo mcmc: Efficient inference by approximate sampling. In
Proceedings of the Joint Conference on Empirical Meth-ods in Natural Language Processing and Computational
Natural Language Learning , 2012. van der Vaart, A. W. Asymptotic Statistics . Cambridge University Press, 2000.
 Wang, L. and Zhang, L. Stochastic optimization using sim-ulated annealing with hypothesis test. Applied Mathe-
