 The number of triangles in a graph is a fundamental metric widely used in social network analysis, link classification and recommen-dation, and more. In these applications, modern graphs of interest tend to both large and dynamic. This paper presents the design and implementation of a fast parallel algorithm for estimating the num-ber of triangles in a massive undirected graph whose edges arrive as a stream. Our algorithm is designed for shared-memory multicore machines and can make e ffi cient use of parallelism and the mem-ory hierarchy. We provide theoretical guarantees on performance and accuracy, and our experiments on real-world datasets show ac-curate results and substantial speedups compared to an optimized sequential implementation.
 H.2.8 [ Information Systems ]: Database Management X  data min-ing ; G.2.2 [ Discrete Mathematics ]: Graph Theory X  graph algo-rithms ; F.1.2 [ Theory of Computation ]: Modes of Computation X  parallelism and concurrency Algorithms, Performance triangle counting; streaming algorithm; parallel algorithm; parallel cache oblivious (PCO); massive graphs
The number of triangles in a graph is an important metric in so-cial network analysis [ 30 , 21 ], identifying thematic structures of networks [ 10 ], spam and fraud detection [ 2 ], link classification and recommendation [ 29 ], among others. Driven by these applications and further fueled by the growing volume of graph data, the re-search community has developed e ffi cient algorithms for counting  X  A full version of this paper is available on the e-Print arXiv [27] and approximating the number of triangles in massive graphs. In the past decade, several streaming algorithms have been proposed handle graph evolution and graphs that are too large to fit in memory, but they cannot e ff ectively utilize parallelism beyond the trivial  X  X m-barrassingly parallel X  implementation, leaving much to be desired in terms of performance. In a similar timeframe, a number of parallel algorithms have been proposed (e.g., [ 26 , 9 ]). These algorithms were designed to quickly process large volume of static data, but they cannot e ffi ciently handle constantly changing graphs. As such, despite indications that modern graph datasets are both massive and dynamic, none of the existing algorithms can e ffi ciently handle graph evolution and fully utilize parallelism at the same time.
In this paper, we describe a fast shared-memory parallel algorithm that combines the benefits of streaming algorithms and parallel algorithms. As is standard, the algorithm provides a randomized relative-error approximation: given 0 &lt;  X , X   X  1, a random variable  X  X is an (  X , X  )-approximation of the true quantity X if | with probability at least 1  X   X  .
We present the design and implementation of the coordinated bulk parallel algorithm X  X  fast shared-memory parallel algorithm for approximating the number of triangles in a large graph whose edges arrive as a stream. The algorithm can process the edges of an evolving graph at a high throughput (e.g., several millions of edges / second on a recent 12-core machine), using memory on the order of a few hundreds of MB. It is also useful for processing large static graphs by reaping the benefits of its small memory footprint and e ff ective use of parallelism.
 Features of the Algorithm: The coordinated bulk parallel algo-rithm is designed for multicore machines and has the following features. First, it works in the limited-space streaming model, where only a fraction of the graph can be retained in memory; this al-lows it to process, in a single pass, graphs much larger than the available memory. Second, it has low overhead with respect to an optimized sequential implementation and scales almost linearly with the number of cores.
 Ingredients of the Algorithm: The algorithm owes its performance to two ideas: coordinated bulk processing and cache e ffi processes edges in batches, rather than processing each edge in-dividually; this allows multiple edges to be processed in parallel, leading to a higher throughput. Further, in processing a batch of edges, di ff erent parallel tasks coordinate with each other through a shared data structure; this results in less redundant computation than an approach where the parallel elements process data independently of each other. Moreover, it takes full advantage of the memory hierarchy. The memory system of a modern machine has unfortu-nately become highly sophisticated, consisting of multiple levels of caches and layers of indirection. Navigating this complex system, however, is necessary for a parallel implementation to be e Our algorithm is cache-oblivious [ 11 , 4 ], which allows it to make e ffi cient use of the memory hierarchy (i.e. minimize cache misses) without knowing the specific memory / cache parameters such as the cache size and the line size.

We show how to process a batch of edges in asymptotically the same cost as sorting the batch using a cache-optimal parallel sort (see Theorem 3.1 for a precise statement). Our algorithm is expressed in terms of simple parallel operations such as sorting, merging, parallel prefix, etc. Importantly, in our scheme, the work is asymptotically the same as that of its best-known sequential counterpart.
We also experimentally evaluate the algorithm. Our implemen-tation yields substantial speedups on massive real-world networks. On a machine with 12 cores, we obtain up to 11 . 24x speedup when compared with a sequential version, with the speedup ranging from 7 . 5x to 11 . 24x. In separate a stress test, a large (synthetic) power-law graph of size 167GB was processed in about 1,000 seconds. In terms of throughput, this translates to millions of edges per second.
Approximate triangle counting is well-studied in both streaming and non-streaming settings. In the streaming context, there has been a long line of research [ 14 , 6 , 20 , 15 ], beginning with the work of Bar-Yossef et al. [ 1 ]. Let n be the number vertices, m be the number of edges,  X  be the maximum degree, and  X  ( G ) be the number of triangles in the graph. An algorithm of [ 14 ] uses  X  O ( m  X  space whereas the algorithm of [ 6 ] uses  X  O ( mn / X  ( G )) space. With higher space complexity, [ 20 ] and [ 15 ] give algorithms for the more general problem of counting cliques and cycles, supporting insertion and deletion of edges. A recent work [ 23 ] presents an algorithm with space complexity  X  O ( m  X  / X  ( G )). Jha et al. [ 13 ] give a O ( space approximation algorithm for triangle counting as well as the closely related problem of the clustering coe ffi cient. Their algorithm has an additive error guarantee as opposed to the previously men-tioned algorithms, which had relative error guarantees. The related problem of approximating the triangle count associated with each vertex has also been studied in the streaming context [ 2 , 17 ], and there are also multi-pass streaming algorithms for approximate trian-gle counting [ 16 ]. However, no non-trivial parallel algorithms with the benefits of small-memory streaming algorithms were known so far, other than the na X ve parallel algorithms, which are ine In the non-streaming (batch) context, there are many algorithms for counting and enumerating triangles X  X oth exact and approxi-MapReduce model includes [26, 9, 22].
We will be working with a simple, undirected graph whose edges arrive as a stream, where we assume that every edge arrives exactly once. Let S = ( V , E ,  X  S ) denote the graph on G = ( V , E ), together with a total order  X  S on E . An edge e  X  E is a size-2 set consisting of its endpoints. We write m = | E | for the number of edges and  X  for the maximum degree of a vertex in G . Given S = ( V , E ,  X  neighborhood of an edge e  X  E , denoted by  X  S ( e ), is the set of all edges in E incident on e that  X  X ppear after X  e in the  X  S is,  X  S ( e ) : = { f  X  E : f  X  e ,  X  X  X  f &gt; S e } .

Let T ( G ) (or T ( S )) denote the set of all triangles in G  X  X .e., the set of all closed triplets, and  X  ( G ) be the number of triangles in G .
The notation  X  O suppresses factors polynomial in log m , log (1 / X  ), and 1 / X  .
 For a triangle t  X   X  T ( G ), define C ( t  X  ) to be |  X  S smallest edge of t  X  w.r.t.  X  S . Finally, we write x  X  R that x is a random sample from S taken uniformly at random. Neighborhood Sampling: Our parallel algorithm builds on neigh-borhood sampling [ 23 ], a technique for selecting a random triangle from a streaming graph, which we now state as a set of invariants: Invariant 2.1 Let S = ( V , E ,  X  S ) be a simple, undirected graph G ( V , E ) , together with a total order  X  S on E . The tuple ( f where f i  X  E  X  X  X  X  and  X   X  Z + , satisfies the neighborhood sampling invariant (NBSI) if (1) Level-1 Edge: f 1  X  R E is chosen uniformly from E; (2)  X  = |  X  S ( f 1 ) | is the number of edges in S incident on f (3) Level-2 Edge: f 2  X  R  X  S ( f 1 ) is chosen uniformly from the neigh-(4) Closing Edge: f 3 &gt; S f 2 is an edge that completes the triangle
The invariant provides a way to maintain a random X  X lthough non-uniform X  X riangle in a graph. This directly translates to an unbiased estimator for  X  : Lemma 2.2 ([23]) Let S = ( V , E ,  X  S ) denote a simple, undirected graph G = ( V , E ) , together with a total order  X  S on E . Further, let ( f , f 2 , f 3 , X  ) be a tuple satisfying NBSI. Define random variable X as X = 0 if f 3 =  X  and X =  X   X | E | otherwise. Then, E [ X ]
To obtain a sharper estimate, we run multiple copies of the esti-mate (making independent random choices) and aggregate them, for example, using median-of-means aggregate: Theorem 2.3 ([23]) There is an (  X , X  ) -approximation to the triangle counting problem that requires at most r independent estimators on input a graph G with m edges, provided that r  X  96  X  2  X  m  X  ( G ) Parallel Cost Model: We focus on parallel algorithms that e ciently utilize the memory hierarchy, striving to minimize cache misses across the hierarchy. Towards this goal, we analyze the (theoretical) e ffi ciency of our algorithms in terms of the memory cost X  X .e., the number of cache misses X  X n addition to the standard cost analysis. Recall that work measures the total number of opera-tions an algorithm performs, and depth is the length of the longest chain of dependent tasks in the algorithm. To measure the memory cost, we adopt the parallel cache-oblivious (PCO) model [ 5 ], a well-accepted parallel variant of the cache-oblivious model. In this model, the memory cost of an algorithm A , denoted by Q  X  is given as a function of cache size M and line size B assuming the optimal o ffl ine replacement policy 3 . In the context of a parallel ma-chine, it represents the number of cache misses across all processors for a particular level.
 Parallel Primitives: We build on primitive operations that have existing parallel algorithms with optimal cache complexity and polylogarithmic depth. This helps simplify the exposition and allows us to use existing implementations.

Our algorithm relies on the following primitives: sort , merge , concat , map , scan , extract , and combine . The primitive sort
The term cache is used as a generic reference to a level in the memory hierarchy; it could be an actual cache level (L1, L2, L3), the TLB, or page faults)
In reality, practical heuristics such least-recently used (LRU) are used and are known to have competitive performance with the in-hindsight optimal policy. takes a sequence and a comparison function, and outputs a sorted sequence. The primitive merge combines two sorted sequences into a new sorted sequence. The primitive concat concatenates the input sequences. The primitive map takes a sequence A and a function f , and it applies f on each entry of A . The primitive scan (aka. prefix sum or parallel prefix) takes a sequence A ( A i  X  D ), an associative binary operator  X  (  X  : D  X  D  X  D ), a left-identity id for  X  ( id  X  D ), and it produces the sequence  X  id , id  X  A 1 , id  X  A 1  X  A  X  X  X  X  A | A | X  1  X  . The primitive extract takes two sequences A and B , where B i  X  X  1 ,..., | A |} X  X  null } , and returns a sequence C of length combine takes two sequences A , B of equal length and a function f , and outputs a sequence C of length | A | , where C [ i ] = On input of length N , the cache complexity of sorting in the PCO model 4 is Q  X  ( sort ( N ); M , B ) = O ( N B log M / B concat , map , scan , and combine all have the same cost as scan : Q ( scan ( N ); M , B ) = O ( N / B ). For merge and concat , N is the length of the two sequences combined. We also write sort ( N ) and scan ( N ) to denote the corresponding cache costs when the context is clear. In this notation, the primitive extract has O ( sort ( | B | ) scan ( | A | + | B | )). All these primitives have at most O ( log
In addition, we will rely on a primitive for looking up mul-tiple keys from a sequence of key-value pairs. Specifically, let S =  X  ( k k belongs to a total order domain of keys. Also, let T =  X  k be a sequence of m keys from the same domain. The exact multi-search problem ( exactMultiSearch ) is to find for each k matching ( k i , v i )  X  S . We will also use the predecessor multisearch ( predEQMultiSearch ) variant, which asks for the pair with the largest key no larger than the given key. Existing cache-optimal sort and merge routines directly imply the following cost bounds: Lemma 2.4 ([5, 4]) The algorithms exactMultiSearch ( S , T ) and predEQMultiSearch ( S , T ) each runs in O ( log 2 ( n + m )) depth and O ( sort ( n ) + sort ( m )) cache complexity, where n = | S | and m Furthermore, if S and T have been presorted in the key order, these algorithms take O (log( n + m )) depth and O ( scan ( n + This section presents the coordinated bulk-processing algorithm. For i = 1 ,..., r , let est i be a tuple ( f 1 , f 2 , f 3 on the graph G = ( V , E ,  X  S ), where  X  S gives a total order on E . The sequence of arriving edges is modeled as W =  X  w 1 ,..., w the sequence order defines a total order on W . Denote by G ( V 0 , E 0 ,  X  S 0 ) the graph on E  X  W , where the edges of W all come after the edges of E in the new total order S 0 .

The goal of the algorithm is to take as input estimators est 1 ,..., r ) that satisfy NBSI on G and the arriving edges W , and produce as output estimators est 0 i ( i = 1 ,..., r ) that satisfy NBSI on G . We show the following: Theorem 3.1 Let r be the number of estimators maintained for triangle counting. There is a parallel algorithm bulkUpdateAll that processes a batch of s edges with O ( sort ( r ) + sort ( s )) cache complexity (memory-access cost) and O (log 2 ( r + s )) depth.
To meet these bounds, we cannot a ff ord to explicitly track each estimator individually. Neither can we a ff ord a large number of random accesses. Despite these challenges, our algorithm proceeds in 3 simple steps, which correspond to the main parts of NBSI: Step 1: Update level-1 edges;
Step 2: Update level-2 edges and neighborhood sizes  X   X  X ;
As is standard, we make a technical assumption known as the tall-cache assumption (i.e., M  X   X  ( B 2 )).
 Step 3: Check for closing edges.
 After these steps, the NBSI invariant is upheld; if the application so chooses, it can aggregate the  X  X oarse X  estimates costing no more than the update process itself. In the descriptions that follow, we will write  X  A ( f ) to mean A  X   X  S 0 ( f ), where A  X  S f  X  X  0 is an edge.
 Step 1: Manage Level-1 Edges: The goal of Step 1 is to make sure that for each estimator, its level-1 edge f 1 is a uniform sample of the edges that have arrived so far ( E  X  W ). The conceptual algorithm for one estimator is straightforward: with probability | W | f with a random edge from W ; otherwise, retain the current edge. Implementing this step in parallel is easy using map and randInt . For the estimators receiving a new level-1 edge, we also set its  X  to 0 (this helps simplify the next step).
 Step 2: Update Level-2 Edges and Degrees: The goal of Step 2 is to ensure that for every estimator est i , the level-2 edge est sen uniformly at random from  X  S 0 ( est i . f 1 ) and est Remember that  X  S 0 ( f 1 ) is the set of edges incident on f after it in S 0 , so  X  is the size of  X  S 0 ( f 1 ), and f from an appropriate  X  X ubstream. X 
To describe the conceptual algorithm for one estimator, consider an estimator est i = ( f 1 , f 2 , f 3 , X  ) that has completed Step 1. We define In words,  X   X  is the number edges in E incident on f 1 that arrived after f 1  X  X nd  X  + is the number edges in W incident on f after f 1 . Thus,  X   X  =  X  (inheriting it from the current state, so if f was just replaced in Step 1,  X  was reset to 0 in Step 1). We also note that |  X  S 0 ( f 1 ) |  X  X he total number of edges incident on f after it in the whole stream X  X s  X   X  +  X  + .
 In this notation, the update rule is simply: An efficient implementation: To derive the implementation, we need to answer the question: How to e ffi ciently compute, for ev-ery estimator, the number of candidate edges  X  + and how to sam-ple uniformly from these candidates? This is challenging because all r estimators need to navigate their substreams X  X otentially all di ff erent X  X o figure out their sizes and sample from them. Because of our performance requirements, we cannot a ff ord to explicitly track these substreams. Neither can we a ff ord a large number of random accesses, though this seems necessary at first.

We address this challenge by first introducing the notion of rank and showing how it can be utilized e ff ectively: Definition 3.2 (Rank) Let W =  X  w 1 ,..., w s  X  be a sequence of unique edges. Let H = ( V W , W ) be the graph on the edges W , where V W is the set of relevant vertices on E  X  W . For x , y  X  V x , y, the rank of x  X  y is
In words, if { x , y } is an edge in W , rank ( x  X  y ) is the number of edges in W that are incident on x and appear after xy in W . Otherwise, rank ( x  X  y ) is simply the degree of x in the graph G We provide an example in Figure 1. The rank of all arriving edges can be computed as follows: Figure 1: A 6-node streaming graph where a batch of 5 new edges (solid; arrival order labeled next to them) is being added to a stream of 3 edges (dashed) that have arrived earlier X  X nd examples of the corresponding rank values as this batch is being incoporated. real edge: DF BD Figure 2: Translating rank values to substreams: (Left) f using u = D and v = C , so  X  + = 2. (Right) f 1 = CE using u v = E , so  X  + = 3.
 Lemma 3.3 There is a parallel algorithm rankAll ( W ) that takes a sequence of edges W =  X  w 1 ,..., w s  X  and produces a sequence of length 2 | W | , where each entry is a record { src , dst , rank , pos } such that 1. { src , dst } = w i for some i = 1 ,..., | W | ; 2. pos = i; and 3. rank = rank( src  X  dst ) .
 Each input edge w i gives rise to exactly 2 entries, one per orientation. The algorithm runs in O ( sort ( s )) cache cost and O (log We omit the algorithm X  X  description and proof due to space con-straints; readers are referred to [27].
 Mapping rank to substreams: The following easy-to-verify obser-vation implicitly defines a substream with respect to a level-1 edge f in terms of rank values: Observation 3.4 Let f 1 = { u , v }  X  E  X  W be a level-1 edge. Let F 0 = rankAll ( W ) . The set of edges in W incident on f appears after it X  X .e., the set  X  W ( f 1 )  X  X s precisely the undirected edges corresponding to L  X  R, where and
Hence, there are rank ( u  X  v ) edges in  X  W ( f 1 ) incident on u (those in L ) and rank ( v  X  u ) edges, on v (those in R ) X  X nd thus,  X  + = rank( u  X  v ) + rank( v  X  u ).

The other piece of the puzzle is, how to sample from the sub-streams? For this, we give a  X  X aming system X  for identifying which level-2 edge to pick: Associate each number  X   X  X  0 , 1 ,..., X  it with the edge src = u and rank =  X  ; otherwise, associate it with the edge src = v and rank =  X   X  rank ( u  X  v ). Therefore, the problem of picking a random level-2 replacement edge boils down to picking a random number between 0 and  X  +  X  1 and locating the corresponding edge in the batch of arriving edges. We give two examples in Figure 2 to illustrate the naming system. Please refer to [27] for details.
 Step 3: Locate Closing Edges: The goal of Step 3 is to detect, for each estimator, if the wedge formed by level-1 and level-2 edges closes using a edge from W . This step can be easily implemented by using map to compute the closing edges and using a multisearch to locate them; please refer to [27] for details.
 Cost Analysis: Let s = | W | and r be the number of estimators we maintain. Step 1 is implemented using map , extract , and combine . Therefore, the cache cost for Step 1 is at most O ( sort ( r ) scan ( r + s )), and the depth is at most O ( log 2 ( r + of both Steps 2 and 3 is dominated by a sort, which is at most O ( sort ( r ) + sort ( s )) for memory cost and O ( log 2 Hence, the total cost of bulkUpdateAll is O ( sort ( r ) + memory cost and O (log 2 ( r + s )) depth, as promised.
We have implemented the algorithm from Section 3 and investi-gated its performance on real-world datasets in terms of accuracy, parallel speedup, and parallelization overhead. More detailed analy-sis appears in [27].
 Implementation: We followed the description in Section 3 rather closely. The algorithm combines the  X  X oarse X  estimators into a sharp estimate using a median-of-means aggregate. The main optimization we made was in avoiding malloc-ing small chunks of memory often. The sort primitive uses a PCO sample sort algorithm [ 4 , 25 ], which o ff ers good speedups. We implemented the multisearch routines by modifying Blelloch et al. X  X  merge algorithm [ 4 ] to stop recursing early when the number of  X  X ueries X  is small. Other primitives are standard. The main triangle counting logic has about 600 lines of Cilk code, a dialect of C / C ++ with keywords that allow users to specify what should be run in parallel [ 12 ] (fork / join-type code). Our benchmark programs were compiled with the public version of Cilk shipped with GNU g++ Compiler version 4.8.0 (20130109). Testbed and Datasets: We performed experiments on a 12-core (with hyperthreading) Intel machine, running Linux 2.6.32-279 (CentOS 6.3). The machine has two 2 . 67 Ghz 6-core Xeon X5650 processors with 96GB of memory although the experiments never need more a few gigabytes of memory.

Our study uses a collection of graphs, obtained from the SNAP project at Stanford [ 19 ].We present a summary of these datasets in Table 1. While most of these datasets are social-media graphs, our algorithm does not assume any special property about the datasets. We simulate a streaming graph by feeding the algorithm with edges as they are read from disk. We note now that disk I / O is not a bottleneck in the experiment.

For most datasets, the exact triangle count is provided by the source (which we have verified); in other cases, we compute the exact count using an algorithm developed as part of the Problem-Based Benchmark Suite [ 25 ]. We also report the size on disk of these datasets in the format we obtain them (a list of edges in plain text). In addition, we include one synthetic power-law graph, which we cannot obtain the true count, but it is added for speed testing. Baseline: For accuracy study, we directly compare our results with the true count. For performance study, our baseline is a fairly optimized version of the nearly-linear time algorithm based on neighborhood sampling, using bulk-update, as described in [ 23 ]. We use this baseline to establish the overhead of the parallel algorithm. We do not compare the accuracy between the two algorithms because by design, they produce the exact same answer given the same sequence of random bits. The baseline code was also compiled with the same compiler but without linking with the Cilk runtime. Performance and Accuracy: We perform experiments on graphs with varying sizes and densities. Our algorithm is randomized and may behave di ff erently on di ff erent runs. For robustness, we perform five trials X  X xcept when running the biggest datasets on a single core, where only two trials are used. Table 2 shows for di numbers of estimators r = 200 K , 2 M , 20 M , the accuracy, reported as the mean deviation value, and processing times (excluding I using 1 and all 12 cores (24 threads with hyperthreading), as well as the speedup ratio 5 . Mean deviation is a well-accepted measure of error, which, we believe, accurately depicts how well the algo-rithm performs. In addition, it reports the median I / another experiment, presented in Table 3, we compare our parallel implementation with the baseline sequential implementation [ 23 ]. Evidently, the I / O cost is not a bottleneck in any of the experiments, justifying the need for parallelism to improve throughput.
Several trends are clear from these experiments. First, the al-gorithm is accurate with only a modest number of estimators. In all datasets, including the one with more than a billion edges, the algorithm achieves less than 4% mean deviation using 20 million es-timators, and for smaller datasets, it can obtain better than 5% mean deviation using fewer estimators. As a general trend X  X hough not a strict pattern X  X ccuracy improves with the number of estimators. Second, the algorithm shows substantial speedups on all datasets. On all datasets, the experiments show that the algorithm achieves up to 11 . 24x speedup on 12 cores, with the speedup numbers ranging between 7 . 5x and 11 . 24x. On the biggest datasets using r estimators, the speedups are consistently above 10x. This shows that the algorithm is able to e ff ectively utilize available cores except when the dataset or the number of estimators is too small to fully utilize parallelism. Additionally, we experimented with a big syn-thetic graph (167GB power-law graph). Althogh we were unable to calculate the true count and had to cut short the sequential ex-periment after a few hours, it is worth pointing out that this dataset has 5x more edges than Friendster, and our algorithm running on 12 cores finishes in 1050 seconds (excluding I / O) X  X bout 5x longer than on Friendster, showing empirically that it scales well.
Third, the overhead is well-controlled. Both the sequential and parallel algorithms, at the core, maintain the same neighborhood sampling invariant but di ff er significantly in how the edges are processed. As is apparent from Table 3, for large datasets requiring more estimators, the overhead is less than 1 . 5x with r = smaller datasets, the overhead is less than 1 . 6x with r cases, the amount of speedup gained outweighs the overhead.
We have presented a cache-e ffi cient parallel algorithm for approx-imating the number of triangles in a massive streaming graph. The proposed algorithm is cache-oblivious and has good theoretical per-
A batch size of 16M was used
Like in the (streaming) model, the update routine to our algorithm takes in a batch of edges, represented as an array of a pair of int  X  X . We note that the I / O reported is based on an optimized I in place of the fstream  X  X  cin -like implementation or scanf . formance and accuracy guarantees. We also experimentally showed that the algorithm is fast and accurate, and has low cache complexity. It will be interesting to explore other problems at the intersection of streaming algorithms and parallel processing.
 [1] Z. Bar-Yossef, R. Kumar, and D. Sivakumar. Reductions in [2] L. Becchetti, P. Boldi, C. Castillo, and A. Gionis. E [3] J. W. Berry, L. Fosvedt, D. Nordman, C. A. Phillips, and A. G. [4] G. E. Blelloch, P. B. Gibbons, and H. V. Simhadri. Low [5] G. E. Blelloch, J. T. Fineman, P. B. Gibbons, and H. V. [6] L. S. Buriol, G. Frahling, S. Leonardi, and C. Sohler. Esti-[7] N. Chiba and T. Nishizeki. Arboricity and subgraph listing [8] S. Chu and J. Cheng. Triangle listing in massive networks and [9] J. Cohen. Graph twiddling in a mapreduce world. Computing [10] J.-P. Eckmann and E. Moses. Curvature of co-links uncovers on 12 cores T 12 H with hyperthreading ( in seconds ), and I estimators r is varied.
 Table 3: The median processing time of the sequential algorithm T ( in seconds ), the median processing time of the parallel algorithm running on 1 core T 1 ( in seconds ), and the overhead factor (i.e., T ). [11] M. Frigo, C. E. Leiserson, H. Prokop, and S. Ramachandran. [12] Intel Corp. Intel Cilk Plus, 2013. http://cilkplus.org/ . [13] M. Jha, C. Seshadhri, and A. Pinar. From the birthday paradox [14] H. Jowhari and M. Ghodsi. New streaming algorithms for [15] D. M. Kane, K. Mehlhorn, T. Sauerwald, and H. Sun. Counting [16] M. N. Kolountzakis, G. L. Miller, R. Peng, and C. E. [17] K. Kutzkov and R. Pagh. On the streaming complexity of [18] M. Latapy. Main-memory triangle computations for very large [19] J. Leskovec. Stanford large network dataset collection. http: [20] M. Manjunath, K. Mehlhorn, K. Panagiotou, and H. Sun. Ap-[21] M. E. J. Newman. The structure and function of complex [22] R. Pagh and C. E. Tsourakakis. Colorful triangle counting [23] A. Pavan, K. Tangwongsan, S. Tirthapura, and K.-L. Wu. [24] T. Schank and D. Wagner. Finding, counting and listing all [25] J. Shun, G. E. Blelloch, J. T. Fineman, P. B. Gibbons, A. Ky-[26] S. Suri and S. Vassilvitskii. Counting triangles and the curse [27] K. Tangwongsan, A. Pavan, and S. Tirthapura. Parallel triangle [28] C. E. Tsourakakis, U. Kang, G. L. Miller, and C. Faloutsos. [29] C. E. Tsourakakis, P. Drineas, E. Michelakis, I. Koutis, and [30] S. Wasserman and K. Faust. Social Network Analysis . Cam-
