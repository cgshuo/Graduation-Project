 Naive Bayes (NB) and Support Vector Machine (SVM) models are often used as baselines for other methods in text categorization and sentiment analy-sis research. However, their performance varies sig-nificantly depending on which variant, features and datasets are used. We show that researchers have not paid sufficient attention to these model selec-tion issues. Indeed, we show that the better variants often outperform recently published state-of-the-art methods on many datasets. We attempt to catego-rize which method, which variants and which fea-tures perform better under which circumstances.
First, we make an important distinction between sentiment classification and topical text classifica-tion. We show that the usefulness of bigram features in bag of features sentiment classification has been underappreciated, perhaps because their usefulness is more of a mixed bag for topical text classifica-tion tasks. We then distinguish between short snip-pet sentiment tasks and longer reviews, showing that for the former, NB outperforms SVMs. Contrary to claims in the literature, we show that bag of features models are still strong performers on snippet senti-ment classification tasks, with NB models generally outperforming the sophisticated, structure-sensitive models explored in recent work. Furthermore, by combining generative and discriminative classifiers, we present a simple model variant where an SVM is built over NB log-count ratios as feature values, and show that it is a strong and robust performer over all the presented tasks. Finally, we confirm the well-known result that MNB is normally better and more stable than multivariate Bernoulli NB, and the in-creasingly known result that binarized MNB is bet-ter than standard MNB. The code and datasets to reproduce the results in this paper are publicly avail-able. 1 We formulate our main model variants as linear clas-sifiers, where the prediction for test case k is Details of the equivalent probabilistic formulations are presented in (McCallum and Nigam, 1998).
Let f ( i )  X  R | V | be the feature count vector for training case i with label y ( i )  X  { X  1 , 1 } . V is the set of features, and f ( i ) j represents the number of oc-currences of feature V j in training case i . Define  X  . The log-count ratio is: 2.1 Multinomial Naive Bayes (MNB) In MNB, x ( k ) = f ( k ) , w = r and b = log( N + /N  X  ) . N + ,N  X  are the number of positive and negative training cases. However, as in (Metsis et al., 2006), we find that binarizing f ( k ) is better. We take x ( k )  X  f ( k ) = 1 { f ( k ) &gt; 0 } , where 1 is the indicator func-tion.  X p ,  X q ,  X r are calculated using  X  f ( i ) instead of f in (2). 2.2 Support Vector Machine (SVM) For the SVM, x ( k ) =  X  f ( k ) , and w ,b are obtained by minimizing w T w + C X We find this L2-regularized L2-loss SVM to work the best and L1-loss SVM to be less stable. The LI-BLINEAR library (Fan et al., 2008) is used here. 2.3 SVM with NB features (NBSVM) Otherwise identical to the SVM, except we use x twise product. While this does very well for long documents, we find that an interpolation between MNB and SVM performs excellently for all docu-ments and we report results using this model: where  X  w = || w || 1 / | V | is the mean magnitude of w , and  X   X  [0 , 1] is the interpolation parameter. This interpolation can be seen as a form of regularization: trust NB unless the SVM is very confident. We compare with published results on the following datasets. Detailed statistics are shown in table 1.
RT-s : Short movie reviews dataset containing one Dataset ( N + ,N  X  ) l CV | V |  X  RT-s (5331,5331) 21 10 21K 0.8 CR (2406,1366) 20 10 5713 1.3 MPQA (3316,7308) 3 10 6299 0.8 Subj. (5000,5000) 24 10 24K 0.8 RT-2k (1000,1000) 787 10 51K 1.5 IMDB (25k,25k) 231 N 392K 0.4 AthR (799,628) 345 N 22K 2.9 XGraph (980,973) 261 N 32K 1.8 BbCrypt (992,995) 269 N 25K 0.5 Table 1: Dataset statistics. ( N + ,N  X  ) : number of positive and negative examples. l : average num-ber of words per example. CV: number of cross-validation splits, or N for train/test split. | V | : the vocabulary size.  X  : upper-bounds of the differences required to be statistically significant at the p &lt; 0 . 05 level.

CR : Customer review dataset (Hu and Liu, 2004)
MPQA : Opinion polarity subtask of the MPQA
Subj : The subjectivity dataset with subjective re-
RT-2k : The standard 2000 full-length movie re-
IMDB : A large movie review dataset with 50k full-
AthR, XGraph, BbCrypt : Classify pairs of 4.1 Experimental setup We use the provided tokenizations when they exist. If not, we split at spaces for unigrams, and we filter out anything that is not [ A-Za-z ] for bigrams. We do not use stopwords, lexicons or other resources. All results reported use  X  = 1 , C = 1 , X  = 0 . 25 for NBSVM, and C = 0 . 1 for SVM.

For comparison with other published results, we use either 10-fold cross-validation or train/test split depending on what is standard for the dataset. The CV column of table 1 specifies what is used. The standard splits are used when they are available. The approximate upper-bounds on the difference re-quired to be statistically significant at the p &lt; 0 . 05 level are listed in table 1, column  X  . 4.2 MNB is better at snippets (Moilanen and Pulman, 2007) suggests that while  X  X tatistical methods X  work well for datasets with hundreds of words in each example, they cannot handle snippets datasets and some rule-based sys-tem is necessary. Supporting this claim are examples such as not an inhumane monster 6 , or killing cancer that express an overall positive sentiment with nega-tive words.

Some previous work on classifying snippets in-clude using pre-defined polarity reversing rules (Moilanen and Pulman, 2007), and learning com-plex models on parse trees such as in (Nakagawa et al., 2010) and (Socher et al., 2011). These works seem promising as they perform better than many sophisticated, rule-based methods used as baselines in (Nakagawa et al., 2010). However, we find that several NB/SVM variants in fact do better than these state-of-the-art methods, even compared to meth-ods that use lexicons, reversal rules, or unsupervised pretraining. The results are in table 2.

Our SVM-uni results are consistent with BoF-noDic and BoF-w/Rev used in (Nakagawa et al., 2010) and BoWSVM in (Pang and Lee, 2004). (Nakagawa et al., 2010) used a SVM with second-order polynomial kernel and additional features. With the only exception being MPQA, MNB per-formed better than SVM in all cases. 7
Table 2 show that a linear SVM is a weak baseline for snippets. MNB (and NBSVM) are much better on sentiment snippet tasks, and usually better than other published results. Thus, we find the hypothe-Method RT-s MPQA CR Subj.
 MNB-uni 77.9 85.3 79.8 92.6 MNB-bi 79.0 86.3 80.0 93.6 SVM-uni 76.2 86.1 79.0 90.8 SVM-bi 77.7 86.7 80.8 91.7 NBSVM-uni 78.1 85.3 80.5 92.4 NBSVM-bi 79.4 86.3 81.8 93.2 RAE 76.8 85.7  X   X  RAE-pretrain 77.7 86.4  X   X  Voting-w/Rev. 63.1 81.7 74.2  X  Rule 62.9 81.8 74.3  X  BoF-noDic. 75.7 81.8 79.3  X  BoF-w/Rev. 76.4 84.1 81.4  X  Tree-CRF 77.3 86.1 81.4  X  BoWSVM  X   X   X  90.0 Table 2: Results for snippets datasets. Tree-CRF: (Nakagawa et al., 2010) RAE: Recursive Autoen-coders (Socher et al., 2011). RAE-pretrain: train on Wikipedia (Collobert and Weston, 2008).  X  X oting X  and  X  X ule X : use a sentiment lexicon and hard-coded reversal rules.  X  X /Rev X :  X  X he polarities of phrases which have odd numbers of reversal phrases in their ancestors X . The top 3 methods are in bold and the best is also underlined . sis that rule-based systems have an edge for snippet datasets to be false. MNB is stronger for snippets than for longer documents. While (Ng and Jordan, 2002) showed that NB is better than SVM/logistic regression (LR) with few training cases, we show that MNB is also better with short documents. In contrast to their result that an SVM usually beats NB when it has more than 30 X 50 training cases, we show that MNB is still better on snippets even with relatively large training sets (9k cases). 4.3 SVM is better at full-length reviews As seen in table 1, the RT-2k and IMDB datasets contain much longer reviews. Compared to the ex-cellent performance of MNB on snippet datasets, the many poor assumptions of MNB pointed out in (Rennie et al., 2003) become more crippling for these longer documents. SVM is much stronger than MNB for the 2 full-length sentiment analy-sis tasks, but still worse than some other published results. However, NBSVM either exceeds or ap-proaches previous state-of-the art methods, even the Our results RT-2k IMDB Subj.
 MNB-uni 83.45 83.55 92.58 MNB-bi 85.85 86.59 93.56 SVM-uni 86.25 86.95 90.84 SVM-bi 87.40 89.16 91.74 NBSVM-uni 87.80 88.29 92.40 NBSVM-bi 89.45 91.22 93.18 BoW (bnc) 85.45 87.8 87.77 BoW (b  X  t 0 c) 85.8 88.23 85.65 LDA 66.7 67.42 66.65 Full+BoW 87.85 88.33 88.45 Full+Unlab X  X +BoW 88.9 88.89 88.13 BoWSVM 87.15  X  90.00
Valence Shifter 86.2  X   X  tf.  X  idf 88.1  X   X  Appr. Taxonomy 90.20  X   X  WRRBM  X  87.42  X  WRRBM + BoW(bnc)  X  89.23  X  Table 3: Results for long reviews (RT-2k and IMDB). The snippet dataset Subj. is also included for comparison. Results in rows 7-11 are from (Maas et al., 2011). BoW : linear SVM on bag of words features. bnc : b inary, n o idf, c osine nor-malization.  X  t 0 : smoothed delta idf. Full : the full model. Unlab X  X  : additional unlabeled data. BoWSVM : bag of words SVM used in (Pang and Lee, 2004). Valence Shifter : (Kennedy and Inkpen, 2006). tf.  X  idf : (Martineau and Finin, 2009). Ap-praisal Taxonomy : (Whitelaw et al., 2005). WR-RBM : Word Representation Restricted Boltzmann Machine (Dahl et al., 2012). ones that use additional data. These sentiment anal-ysis results are shown in table 3. 4.4 Benefits of bigrams depends on the task Word bigram features are not that commonly used in text classification tasks (hence, the usual term,  X  X ag of words X ), probably due to their having mixed and overall limited utility in topical text classifica-tion tasks, as seen in table 4. This likely reflects that certain topic keywords are indicative alone. How-ever, in both tables 2 and 3, adding bigrams always improved the performance, and often gives better results than previously published. 8 This presum-ably reflects that in sentiment classification there are Method AthR XGraph BbCrypt MNB-uni 85.0 90.0 99.3 MNB-bi 85.1 +0.1 91.2 +1.2 99.4 +0.1 SVM-uni 82.6 85.1 98.3 SVM-bi 83.7 +1.1 86.2 +0.9 97.7  X  0.5 NBSVM-uni 87.9 91.2 99.7 NBSVM-bi 87.7  X  0.2 90.7  X  0.5 99.5  X  0.2 ActiveSVM  X  90 99 DiscLDA 83  X   X  Table 4: On 3 20-newsgroup subtasks, we compare to DiscLDA (Lacoste-Julien et al., 2008) and Ac-tiveSVM (Schohn and Cohn, 2000). much bigger gains from bigrams, because they can capture modified verbs and nouns. 4.5 NBSVM is a robust performer NBSVM performs well on snippets and longer doc-uments, for sentiment, topic and subjectivity clas-sification, and is often better than previously pub-lished results. Therefore, NBSVM seems to be an appropriate and very strong baseline for sophisti-cated methods aiming to beat a bag of features.
One disadvantage of NBSVM is having the inter-polation parameter  X  . The performance on longer documents is virtually identical (within 0.1%) for  X   X  [  X  , 1] , while  X  =  X  is on average 0.5% better for snippets than  X  = 1 . Using  X   X  [  X  ,  X  ] makes the NBSVM more robust than more extreme values. 4.6 Other results Multivariate Bernoulli NB (BNB) usually performs worse than MNB. The only place where BNB is comparable to MNB is for snippet tasks using only unigrams. In general, BNB is less stable than MNB and performs up to 10% worse. Therefore, bench-marking against BNB is untrustworthy, cf. (McCal-lum and Nigam, 1998).

For MNB and NBSVM, using the binarized MNB  X  f is slightly better (by 1%) than using the raw count feature f . The difference is negligible for snippets.
Using logistic regression in place of SVM gives similar results, and some of our results can be viewed more generally in terms of generative vs. discriminative learning.
