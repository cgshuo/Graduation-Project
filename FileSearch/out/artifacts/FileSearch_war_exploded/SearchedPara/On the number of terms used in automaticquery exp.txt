 Paul Ogilvie  X  Ellen Voorhees  X  Jamie Callan Abstract This paper investigates the number of expansion terms to use in automatic query expansion by examining the behavior of eight retrieval systems participating in the NRRC Reliable Information Access Workshop. The results demonstrate that current sys-tems are able to obtain nearly all of the benefit of using a fixed number of expansion terms per topic, but significant additional improvement is possible if systems were able to accurately select the best number of expansion terms on a per topic basis. When optimizing average effectiveness as measured by mean average precision, using a fixed number of terms increases the score a large amount for a small number of topics but has little effect for most topics. The analysis further suggests that when a topic is helped by automatic feedback, the increase is from a set of terms that reinforce each other rather than from the system finding a single excellent term.
 Keywords Automatic query expansion Blind feedback Pseudo relevance feedback 1 Introduction There is ample evidence that automatic query expansion can improve the average effec-tiveness of information retrieval systems. However, the effectiveness of automatic query expansion from query to query can vary widely. In a review, Efthimiadis ( 1996 ) describes the state of research on query expansion:
Query expansion is an essential element in the retrieval process. The research reviewed not only attests to that but also signals that more research is needed. There are many approaches to query expansion, and research work is needed to improve each approach and to find effective ways to combine different approaches in order to maximize the benefits.

This work addresses these concerns by providing a focused analysis of automatic query expansion, also referred to as blind feedback and pseudo relevance feedback. Within blind feedback there are many different questions to answer, including which documents or passages to examine for candidate terms, which candidate terms to add to the query, how to incorporate the selected terms into the query, and how many terms to select. This work focuses on the number of expansion terms selected.

Two extremes on the number of expansion terms added to the queries have been explored. On one end of the spectrum is the addition of only a small number of carefully selected terms, as in the CLARIT system (Milic-Frayling et al. 1998 ), or the use of lexical affinities (Carmel et al. 2002 ). At the other end of the spectrum is the addition of a large number of carefully weighted expansion terms, as in SMART (Buckley et al. 1994 ) and Language Model approaches (Lavrenko 2001 ; Zhai 2002 ).

While these systems have been individually optimized, there has been little work comparing the behavior across several retrieval systems. The experiments produced by the ARDA NRRC Reliable Information Access workshop provide a novel opportunity to analyze the effect of the number of expansion terms selected on eight retrieval systems. This paper analyzes a set of retrieval experiments where the number of terms added varies from zero to one hundred. These experiments give a unique opportunity to investigate how systems and topics behave under automatic expansion.

One part of this work is an examination of the behavior of systems using statistical tech-niques. The effectiveness and variance of the approaches is analyzed when using a static number of expansion terms optimized for a training set, comparing this performance to that of a system using an oracle providing the best static number of expansion terms and to the per-formance of a system using an oracle that provides the best number of terms on a per-topic basis.
An additional component of this paper investigates the set of topics where most of the participating systems found gains in average precision when using query expansion. In order to gauge whether all systems consistently found a few very helpful query terms, this set of topics was compared to the set of topics where only four or fewer of the additional terms improved retrieval performance.
 The systems and testbed on which the analysis was performed are described in Sect. 3 . Section 5 provides analysis on the number of expansion terms used. Section 6 examines characteristics of the number of terms that help improve performance when query expansion is successful for many systems. Conclusions are summarized in Sect. 7 and related work is reviewed in the following section. 2 Related work There is a large body of work on query expansion, and it would be impractical to review it all here. Much of this work is reviewed by Efthimiadis 1996 ). This section instead focuses on work where trends in data and characteristics of systems are examined.

Buckley et al. ( 1994 ) found that for SMART using Rocchio feedback on the routing task, adding more terms continued to increase performance. This work shows similar results for the SMART system on automatic query expansion for ad-hoc retrieval, but this does not hold for all systems.

Alemayehu ( 2003 ) performed a statistical analysis of the effects of query expansion from blind feedback as shown in the TREC Query Track data. Alemayehu X  X  main con-clusions were that topic, system, and interaction effects play a significant role on the effectiveness of blind feedback, with the topic effect being the greatest. Another conclu-sion was that for SMART and InQuery runs, query expansion was almost always neutral or better than not expanding. The analysis in this paper confirms Alemayehu X  X  analysis for additional systems and also conducts additional analysis which suggests future directions for research.

Billerbeck and Zobel ( 2004 ) explored varying the parameters of automatic query expansion in the Okapi system. They found that the optimal parameters for the Okapi system vary widely from topic to topic. They also showed there is room for significant improvement by optimizing parameters for average precision to topic. This paper con-firms the work of Billerbeck and Zobel for many systems with respect to the number expansion terms to add. This paper also shows that current systems achieve performance near the upper bounds when a static number of expansion terms is chosen across all topics.

Sakai and Robertson ( 2001 ) demonstrated that optimization tables for the Okapi system can be used customize query expansion parameters to topic characteristics. This paper further confirms that parameters need to be topic specific for a variety of systems. 3 Experimental context The process of automatic query expansion is also commonly called blind feedback as a result of an important assumption made in the process. Typically, some number of the top documents or passages of a ranking produced by evaluating the original query against the document collection are assumed to be relevant. These top texts are then examined to find terms to add to the original query. Sometimes external knowledge is used, such as col-lection statistics or linguistic information.

Parameters in the process of automatic query expansion include how the documents or passages are ranked, how terms are ranked and weighted, how many documents or pas-sages are selected, and how many terms are selected. In general, the query reformulation includes both adding expansion terms to the new query and reweighting original query terms.

The experiments described in this paper left the document or passage ranking and term ranking and weighting up to the individual systems. All systems used the text from the top twenty documents. The decision of using twenty documents across all systems was motivated by the desire to increase the comparability of the runs across systems. Fixing the number of documents to twenty isolates one parameter and may make data analysis easier. Passage based expansion techniques were permitted to use multiple passages from the any of the top documents.

It is important to note that the choice of the number of query terms is not independent of any other major variables (term weighting algorithm, number of top documents, etc.). However, the experiments described in this paper attempt to shed some light on how both the number of expansion terms selected impacts effectiveness and also how the other variables indirectly affect performance through the number of terms selected.

The main parameter of interest, the number of expansion terms added, was varied for each system from 0 to 100 terms added. For the range from 0 to 20 terms, the experiments contain runs for each single term added. For the range from 25 to 100 terms, the experi-ments contains runs in increments of five terms (25 ; 30 ; 35 ; ... ; 100). 3.1 Experimental corpus The document collection used for the retrieval experiments presented in this paper used TREC CDs 4 and 5 minus the Congressional Records documents. The Congressional Record documents were omitted since relevance judgments on them were available only for some of the topics. The topics used were TREC ad-hoc topics 301 X 450. In order to facilitate more comparable results across systems a standard set of processed topics were used by all systems. This standard topic processing used the description field of the topics and a shared stopword list. Stemming of terms was left to the individual systems and systems were allowed to use their own stopword lists for document indexing. 3.2 Systems The retrieval runs used in this study were part of the common set of runs created for the Reliable Information Access (RIA) workshop (Harman and Buckley 2009 ). More infor-mation about the systems and their approach to automatic query expansion can be found in the overview elsewhere in the volume (Harman and Buckley 2009 ). The systems included:  X  HITIQA X  X UNY Albany (Small et al. 2004 ; Strzalkowski and Harabagiu 2006 )  X  OKAPI X  X ity University London (Roberston 1990 ; Robertson and Sparck Jones 1976 ;  X  CLARIT and CLJ X  X ust System Evans Research Lab Pittsburgh (Evans and Lefferts  X  Lemur 1  X  X arnegie Mellon University used the divergence minimization query  X  SMART X  X abir Research (Buckley 1985 ; Williamson et al. 1971 )  X  UMASS X  X MASS also used the Lemur Toolkit, but used the Relevance Models  X  MultiText X  X niversity of Waterloo (Clarke et al. 2001 ; Yeung et al. 2003 ). 4 Statistical methods This section describes the statistical methods of analysis used in this paper. Besides the commonly known histogram, the paper also presents kernel density estimates and confi-dence intervals estimated using the boostrap. While it is informative to present p values as is common in analysis of information retrieval experiments, reporting confidence intervals instead has a distinct advantage. Confidence intervals give a sense of the range values can take in addition to whether the results are significant. 4.1 The bootstrap The bootstrap is a non-parametric estimation method. It is particularly good for robust estimates of statistics and confidence intervals. The advantages of using the bootstrap for this analysis include estimates of mean values and a sense for the variance. The confidence providing a sense of what could happen under different topic sets.
 The basics of estimation using the bootstrap in these experiments is straightforward. Pseudocode for the bootstrap algorithm is in Fig. 1 . A large number of sample sets were drawn with replacement from the initial topic set. These sets are of the same size as the topic set, and may contain duplicated topics. The experiments in this paper used n = 5000 sampled sets of 150 topics. The statistic of interest was then estimated on each sampled set. The estimate of the statistic was taken as the median of the sample 5,000 statistics ( b ). A 95% confidence interval was similarly estimated by taking the middle 95% of the values in the 5,000 statistics. This interval is called the bootstrap percentile interval. All experiments in this paper using the bootstrap used the same bootstrap samples. The bootstrap is described in more detail by Efron and Tibshirani ( 1993 ).

Some experiments in this paper use the bootstrap to estimate how well a system can optimize fixed parameters. In these experiments, we reserve 75 of the topics in each sample for training and the other 75 topics for testing. Mean average precision and optimal estimate the best number of expansion terms. Pseudocode for this estimation procedure is in Fig. 2 . 4.2 Kernel density estimates Kernel density estimates provide a way to non-parametrically estimate probability distri-butions from a sample of data. They are similar to histogram density estimates, but provide smoother and more accurate estimation of the distribution. Additional advantages of a density estimate over a histogram are that the density estimates give a better sense of variance and mean values, and they allow easier recognition of trends across systems. Details of the computation of the kernel density estimates can be found in Chapter 20 of (Wasserman 2004 ). 5 Best number of query terms This section of the paper seeks to answer questions regarding the best number of query terms to use for expansion. It asks what the best static number of expansion terms for the systems are and examines whether there are topic and system dependencies. The following analysis also seeks to understand the relationship between the individual topics and the best static number of terms by looking at the best number of expansion terms on a per topic basis and also by seeking to understand how static number of term expansion affects the average precision of individual topics. Finally, this section investigates whether perfor-mance could be further improved by moving away from the static number of expansion term paradigm to one where number of expansion terms added is topic dependent. 5.1 Optimal static number of terms Most systems currently use a static number of query expansion terms across all topics. This paper first strives to understand how the parameter may vary across systems and topic sets.
Table 1 shows the static number of expansion terms for each system that maximizes mean average precision over the 75 test topics in each bootstrap sample. The wide range of mean values for the systems indicate that there is a strong system dependence. The con-fidence intervals give some feel for the effect of topic set on the parameter. The impact of topic set appears to be very strong for some systems (Lemur, CLARIT, UMASS, etc.) but may be small for other systems (HITIQA, CLJ, OKAPI).

A wide variance in the best static number of expansion terms suggests that a system X  X  expansion method is not very stable. Alternatively, it could be the case that adding more terms in that confidence interval will not greatly change the rankings; the system could be important, knowledge of the confidence interval could drive decisions to choose a lower number of expansion terms while still having assurances about the quality of this choice. 5.2 Optimal number of terms per topic One way to investigate why the static number of expansion terms that optimizes mean average precision can vary so much by topic set and system is to look at what the best number of expansion terms is on a per-topic basis. Figure 3 shows histograms of the number of terms that maximize average precision for the individual topics.

For all systems, there is a large number of topics that achieve their heighest average precision with ten or fewer expansion terms. This suggests that even techniques designed to add many terms (HITIQA, Lemur, SMART, UMASS) could do a better job of weighting expansion terms or selecting the number of terms to add on a per query basis.

Additionally, even the systems not designed for adding larger numbers of query terms (particularly OKAPI, CLJ, and CLARIT) still have numerous topics where the optimum number of terms are over 50. This implies that either systems are finding more terms that more terms are added. 5.3 Effects of optimizing the static number of terms on topic performance Since it is common practice to add a static number of expansion terms, it is important to understand how this can affect the average precision of individual topics.

One way to visualize this is with a density estimate of the percent change of average precison given by using the static number terms that optimizes mean average precision over no expansion. Figure 4 shows the kernel density estimates for the systems over the 150 topics. This allows us to examine the effects of static number query expansion relative to the performance of the topics before expansion.

As curves tend to be centered near zero and are somewhat peaked, it can be read from the density estimates in Fig. 4 that most topics are only mildly affected by static term expansion. Automatic query expansion seems to hurt a non-negligible amount of the time, but it does help more often than it hurts. This is demonstrated by the area under positive density estimates. In addition, sometimes query expansion gives large relative improve-percent or more.

It is also striking that the curves across systems are very similar. Lemur X  X  narrow peak around no change in performance can be attributed to the very conservative weighting of expansion terms. The odd shape for CLJ is a result of erratic behavior; a larger topic set may give smoother curves. HITIQA is the only system where the peak of percent change is obviously greater than zero; it may be a result of the relatively low initial performance of HITIQA X  X  system without expansion. 5.4 Effects of estimating the static number of terms on MAP While there can be a wide variance of topic effect on the optimal static number of query terms for a topics set, it is not clear whether systems weighting strategies can cope with this variance. As with estimating the number of terms that optimize mean average precision, the mean average precision of systems using a trained static number of expansion terms can be estimated using the bootstrap (Fig. 1 ). To estimate this effect, some of the topics in each sample T * must be reserved for estimating the number of terms to use and the other topics in T * must be used as a test set to estimate the effect on performance. We use the approach outlined in Fig. 2 . With that in mind, the first 75 training topics in each bootstrap sample were used to estimate the best static number of terms to add using leave one out the remaining 75 test topics were used to measure the MAP of the system when using the optimized static number of terms for the sample T *. The MAP scores and differences in MAP for the 5,000 samples were then used to construct the estimates and confidence intervals presented in Table 2 .

Table 2 shows the results of this experiment. The improvement gained by using a static number of terms estimated using cross-validation is usually positive but may be small. With the exception of HITIQA and UMASS, it is not uncommon for the improvement gained by using expansion to be 5% or less. However, the expected improvement found using expansion is at least 5% for all systems.

Since UMASS has a strong baseline and consistently good improvements in perfor-mance using their expansion method, it may be worth investigating how their expansion method is different than the other systems X  expansion techniques. 5.5 Oracle for best static number of terms The previous section demonstrated that using an estimated static number of expansion terms typically helps performance, but does not address how robust the systems are to errors in the estimation. This section examines the use of an oracle that provides the best static number of expansion terms. We use the same bootstrap samples as in the previous section, but rather than using the best static number of terms chosen on the training sample topics, the oracle approach instead used the static number of expansion terms that opti-mized mean average precision on the test topics. This number was then used to estimate the performance of the oracle system with respect to no expansion and the expansion of a static number of terms estimated using cross-validation.

The results of this experiment are shown in Table 3 . The static number of terms oracle tends to give only small improvements over the performance found when using the cross-validation set parameter. This suggests that systems are quite robust to errors in selecting the best static number of expansion terms to add. This demonstrates that these systems are good at optimizing performance when using a static number of expansion terms using their current term weighting strategies. More importantly, this suggests that there is little room for improvement with the current term weighting strategies and a static number of expansion terms. In order to gain further significant improvements to mean average pre-cision in the future, researchers need to either develop new term weighting and selection algorithms or move away from the static number of expansion term paradigm. 5.6 Oracle for each topic While it is not practical to measure the effects of all possible term selection and weighting schemes, it is practical to give an estimate of the upper bound on the performance could be achieved for a given system by allowing the systems to select the number of terms to add on a per topic basis. For the 75 test topics in each of the bootstrap samples, the number of terms that maximized average precision for each topic was selected.

The behavior of the systems using an per-topic oracle for the number of expansion terms is shown in Table 4 . Using the best number of expansion terms for each topic gives additional large boosts to mean average precsion to all systems over no query expansion and also gives sizeable boosts over expansion using a static number of terms across topics. These boosts range from 15 to 55% over no expansion and from 4 to 30% over expansion using a static number of terms. This suggests that there may be much to gain by researching ways to adaptively select the best number of terms to add to a query. 6 Crucial terms The histograms in Fig. 3 show that for all systems but HITIQA the optimal number of expansion terms is 10 or fewer terms for more than half the topics. This raises the question whether the main effect of automatic query expansion is finding a topic X  X   X  X  X rucial term X  X . Put another way, do most topics that are improved by automatic query expansion have a single term (or very small set of terms) that is common across systems and whose addition accounts for the increase in effectiveness?
The approach used to search for crucial terms was based on the topic categorization methodology developed during the Reliable Information Access workshop. See the  X  X  X opic Categorization X  X  section in Harman and Buckley ( 2009 ) for a full description and moti-vation of this methodology. The main idea of the methodology is to discover common patterns in topic behavior using correlations among different topic clusterings. This improved by automatic query expansion on the one hand, and the cluster containing topics with T significant expansion terms for T [ {1, 2, 3, 4} on the other.

A topic was classified as a topic that is improved by automatic query expansion in the following way. First, the difference in average precision scores between the expanded query and the unexpanded query were calculated for all topics and all systems. This entire set of scores was sorted from largest difference to least difference. The value of the score at voted whether expansion improved the topic. A system voted that expansion did improve the topic if its difference between the expanded and unexpanded scores for the topic was larger than the first quartile score computed earlier. Finally, the topic was accepted as a topic that is improved by automatic query expansion if a majority of the systems voted that it did.

The number of significant expansion terms for a system X  X opic pair was defined as the number of times the average precision score increased by at least 0.005 relative to the previous average precision score as the number of terms added to the query ranged from 0 ... 20 : A topic was classified as having T significant expansion terms if a majority of systems had at least one but no more than T significant expansion terms for the topic.
Of the 150 topics used in the analysis, 21 topics were classified as being improved by automatic query expansion, and 42 topics were classified as having 1 X 4 significant expansion terms. The intersection between these two sets was empty. When the limit was raised to 5 significant terms, 64 topics were classified as having 1 X 5 significant terms, and three topics were in the intersection. These results contradict the hypothesis that topics are amenable to automatic query expansion (only) when the original expression of the topic lacks a single crucial term.

Note that the definition of the topic clusters required agreement among a majority of the systems before classifying a topic. This was done to minimize topic X  X ystem interaction effects and thus derive behavior patterns that can be attributed to the topics themselves. These results do not deny the possibility that an individual system may greatly improve its effectiveness on an individual topic by adding a single term. Rather, they suggest that if a topic will be helped by query expansion at all, it is generally because there is a set of terms that together provide this benefit. A possible explanation for this behavior is that the effect of successful query expansion is to define the appropriate context of the topic. Single words are rarely adequate to represent context, whereas the theme common among several words can be quite effective.

A final caveat to these results is that while we controlled for individual system effects, the topic clusterings we used are still dependent on the set of systems that participated in the workshop. All of the automatic query expansion algorithms used by these systems have been tuned over the years to give good results on average. Systems have been tuned not to give too much weight to any individual change in the query because retrieval effectiveness becomes too erratic and average effectiveness is hurt. The result of such tuning has biased current systems toward finding sets of reinforcing terms and against finding the single exploit them. 7 Conclusions Query expansion has been used with much success over the years, but its advantage has always been its average case performance. Even when it works well, anecdotal evidence and single-system analyses suggest that it hurts nearly as many queries as it helps. Anecdotal evidence suggests that some queries are inherently difficult to improve with query expansion, and that the success of query expansion is often due to finding a small number of excellent terms.

The research reported here with eight high-quality IR systems is one of the first large, multi-system investigations of query expansion. This paper focused primarily on the number of expansion terms to include in a query. However, even this narrow focus, when applied to eight state-of-the-art IR systems, allows a variety of conclusions to be drawn.
The research reported here confirms that query expansion hurts almost as many topics as it helps. It also demonstrates that much of the average case effectiveness of query expansion is due to a small number of topics that are dramatically improved. When these topics are factored out, the average case effectiveness is slightly positive.

Our research also demonstrates that when there is one term that will improve average precision, there are usually several. The effect of the single  X  X  X rucial X  X  expansion term seems to be overstated in conventional wisdom. A particular system may only find one expansion term that improves results significantly, but research with multiple systems reveals that there are usually additional good terms to be found.

For many topics, ten or fewer expansion terms provided the best average precision. Yet there are also many topics where a much larger number of expansion terms provided the best average precision. This bimodal behavior suggests that, for some topics, systems are either finding more good expansion terms or getting better at weighting the terms as more terms are added. There were additional experiments done during the RIA workshop, not reported here, that investigated swapping expansion terms among systems. Analysis of these experiments may provide more insight into this bimodal behavior.

We show that there is no single number of expansion terms that is optimal or near-optimal for all queries, even for a single system. The static number of expansion terms that optimizes mean average precision varies widely across systems and topic sets. However, all systems tend to be fairly robust to errors the setting this parameter. That is, using cross-validation to estimate the best number of expansion terms on a training set will yield mean average precision close to the best possible mean average precision achievable on a test set.
To do better in the future, these systems will need new weighting schemes or will need to move away from the static number of expansion terms paradigm. Our experiments demonstrate that if systems could predict, even relatively inaccurately, how many terms to add for a given topic, they would receive additional large gains to average precision. This finding calls for research in approaches that can more effectively adjust the number of expansion terms on a per-topic basis.
 References
