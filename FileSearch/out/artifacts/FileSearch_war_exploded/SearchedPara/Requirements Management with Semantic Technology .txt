 A major goal of requirements engineering is to achieve a common understanding on the set of requirements between all project stakeholders. Modern IT projects are complex due to the high number and complexity of requirements, and geographically distributed project stakeholders with different backgrounds and terminologies. There-fore, adequate requirements management (ReqM) tools are a major contribution to address these challenges. Current ReqM tools typically work with a common requirements database, which can be accessed by all stakeholders to retrieve informa-tion on requirements content, state, and interdependencies. 
ReqM tools help project managers and requirements engineers to keep the overview on large amounts of requirements by supporting: (a) Requirements categorization by clustering requirements into user-defined subsets to help users find relevant require-ments more quickly, e.g., by sorting and filtering attribute values; (b) Requirements conflict analysis (or consistency checking) by analyzing requirements from different stakeholders for symptoms of inconsistency, e.g., contradicting requirements; and (c) Requirements tracing by identifying dependencies between requirements and artifacts to support analyses for change impact and requirements coverage. Unfortunately, ReqM suffers from the following challenges and limitations:  X  Incompleteness [7] of requirements categorization and conflict identification, in  X  High human effort for requirements categorization, conflict analysis and tracing,  X  Insufficient completeness [6] for conflict analysis and tracing with automated  X  Tracing on syntactic rather than on con cept level: requirements are often traced The use of semantic technologies seems promising to address these challenges: Ontologies provide the means for describing the concepts of a domain and the rela-tionships between these concepts in a way that allows automated reasoning [18]. Automated reasoning can support tasks for requirements categorization, requirements conflict analysis, and requirements tracing. 
In this paper, we propose OntRep, an automated ontology-based reporting approach for requirements categorization, conflict analysis and tracing based on on-tologies and semantic reasoning mechanisms. The main criteria for the evaluation are: correctness and completeness of identified requirements conflicts, effort to develop a project or domain ontology. OntRep aims at lowering the effort for requirements management, while keeping high requirements consistency. 
The OntRep approach automatically categor izes requirements into a given set of categories using ontology classes modeled in Prot X g X  and mapping the terms used in the requirements to these classes. Further, OntRep analyzes the content of the re-quirements and identifies conflicts between requirements. Therefore, conflict analysis is not only based on traditional keyword-matching-approaches, but can also work when different terminologies are used for requirements formulation. 
We empirically evaluate OntRep with a real-life project at Siemens Austria, where six project managers in two teams (a) categ orized the requirements of the case study identify conflicts between requirements. A requirements engineering expert provided control data for all tasks. Then, we performed the same tasks with OntRep to compare the effort necessary and the quality of results. 
The remainder of the paper is organized as follows: Section 2 summarizes related work on requirements categorization, conflict analysis, tracing, and natural language processing technologies; Section 3 introdu ces the OntRep approach and motivates research issues. Section 4 outlines the case study and Section 5 presents results. Section discusses the results, concludes and suggests further work. This section presents related work on natural language processing technologies as foundation for automating the ReqM tasks requirements categorization, conflict anal-ysis, and requirements tracing approaches. 2.1 Requirement Conflicts Detection and Requirements Tracing Requirements conflict with each other if they make contradicting statements about common software attributes [7]. Requirements authors may use different terminol-ogies for specifying requirements, although the terms used can be derived from the same common concepts.  X  In principle there are the following main strategies to identify and eliminate re- X  Automation approaches for conflict analysis ([4][6][12]) that use tools to analyze potential conflicts, could be enormous, burden ing the engineer with the time-intensive and error-prone task of identifying the true conflicts X  [7]. Several approaches address the issue of automated requirements conflict identification: 
The Trace Analyzer by Egyed and Gr X nbacher [7] analyzes the footprint of test cases to generate trace dependencies. If two requirements affect the same part of a system, then their test runs execute overlapping lines of code. Trace dependencies and potential conflicts can be identified among requirements, if their test scenarios exe-cute the same lines of code. However, the Trace Analyzer needs executable code to identify requirements conflicts, which is often not available in early project phases, when conflict analysis is a major goal. 
Heitmeyer et al. [11] describe a formal analysis technique, called consistency checking , for the automated detection of errors, such as type errors, non-determinism, missing cases, and circular definitions, in requirements specifications. The approach only considers syntactical consistency and does not address semantic conflicts. 
Automated requirements tracing approaches are also relevant for requirements con-flict analysis: requirements tracing deals with identifying interdependencies between requirements [10] and conflicts between two requirements can be seen as a particular type of interdependency, i.e., tracing is a precondition for conflict analysis. There are reports on several trace automation approaches, such as Egyed X  X  scenario-driven ap-proach to traceability [6], Jackson X  X  key-phrase-based traceability scheme [12]. Fur-ther, there are the heterogeneous traceability approach Cleland-Huang et al. [4], and approaches by Pinheiro et al. [20], Leuser [14], and McMillan et al. [16]. These ap-proaches use different techniques to identify requirements interdependencies. Some of them require executable code, so they canno t be used for the identification of interde-pendencies in early project phases when there is no sufficient code base. 
Within these trace automation approaches, information retrieval approaches, such as the RETH approach [13], seem of particular interest as they use keyword-matching techniques to identify requirements interd ependencies. However, these techniques do not allow identifying conflicts or other interdependencies between requirements, if they use different terms for similar concepts. In practice these approaches are less effective, because they cannot identify the full set of interdependencies between re-quirements. 
The extended Bakkus-Naur-Form (EBNF) [21] (see Fig. 2 ) is a general formal lan-guage description approach, which is used in the field of requirements analysis to improve the understandability of requirements for humans and machines. EBNF re-quirements templates contain mandatory and optional elements, e.g. conditions, obli-gations, actors, process verbs, which are the basis for clear requirements statements. 2.2 Natural Language Processing Natural language processing (NLP) techniques are useful to parse and extract struc-ture and content of requirements given in natural language for transformation into the structure of an ontology. NLP generally refers to a range of theoretically motivated and computational techniques for analyzing and representing naturally occurring texts [3]. The core purpose of NLP techniques is to achieve human-like language process-ing for a range of tasks or applications [15]. 
The core NLP models used in this research are part-of-speech (POS) tagging and sentence parsers [3]. POS tagging involves marking up the words in a text as corre-parse tree), which provides insight into the grammatical structure and implied hierar-chy of the input text [3]. Standford parser/tagger 1 and OpenNLP 2 are the core set of NLP tools used in this research. Another tool that can be used is WordNet , a large lexical database of English [17]. Nouns, verbs, adjectives and adverbs are grouped into sets of cognitive synonyms (synsets), each expressing a distinct concept. Synsets are interlinked by means of conceptual-semantic and lexical relations. WordNet is a useful building block for requirements analysis (see below). 
These NLP technologies can be used for our purpose, namely to improve the effec-tiveness of requirements management activities like categorization, conflict analysis, and tracing. Due to the limitations of requirements analysis approaches that address only links between requirements based on syntactic equality, we explore in this work an ap-proach based on semantic equality, i.e., OntRep links similar concepts, if they share the same meaning even if their syntactic representations are different. As ontologies are versatile for representing knowledge on requirements and for deriving new links between requirements, we introduce an ontology-based approach for reporting analy-sis results on a set of requirements, so-called ontology-based reporting. 
The goal of the ontology-based reporting approach OntRep is making ReqM tasks like requirements categorization, conflict analysis and requirements tracing more efficient based on the automation of selected steps in these tasks. The following sub-sections provide an overview on the approach and motivate research issues. 
We developed a prototype tool for the OntRep approach a plug-in to Trac 3 , an open source collaboration platform consisting of a Wiki, ticket management system, and subversion integration, which can be extended by Python plug-ins. 
Fig. 1 illustrates the OntRep tool (together with Prot X g X  ) consisting of two main components: 1) Instance fetcher takes input data, e.g., requirement tickets from Trac , analyzes their contents and assigns them requirements categories (classes) defined in the ontology; 2) Reporting component reasons on the input data and generates a re-quirements conflict report based on the analyzed requirements. 3.1 Semantic Requirements Categorization In a first phase natural language texts have to be linked to semantic categories as preparation for further analysis and reporting. The following steps automate require-ments categorization with OntRep (see numbered circles in Fig. 1 ): 1) Define the requirement categories in Prot X g X , e.g., categories X, Y, Z. Each cat-egory is defined as an ontology class in Prot X g X . It is important to define project-relevant  X  X emantic X  categories and not formal ones in order to enable the automated categorization, e.g.,  X  X ecurity X . Typically, these categories can be defined based on a project glossary that contains important project-specific terms. 2) Provide input data to be categorized: Requirements are represented as tickets in Trac . For our research prototype we export these requirements (the small grey circles in Fig. 1 ) via CSV from Trac and import them into the instance fetcher. 3) Remove irrelevant stop-words , like  X  X nd X ,  X  X ny X ,  X  X ut X , which cannot be used for categorization. This step is performed automatically using a standard stop-word list 4 . 4) Bring all remaining words into their root form : this process is called  X  X tem-ming X  based on a well-known algorithm, like the  X  X orter Stemmer X  algorithm [19]. An example is to stem  X  X umping X  to  X  X ump X . 5) Get all synonyms and hyponyms of the analyzed words in the requirements by using the natural language processing library  X  X ordNet X  [17]. For example,  X  X ouse X  is a synonym for  X  X uilding X ,  X  X og X  is a hyponym of  X  X nimal X . Further check all rele-vant substrings of a word like  X  X et X  as a substring of  X  X etwork X . 6) Heuristic-based assignment of each requirement to the defined categories de-pending on the number of hits for 1) synonyms, 2) hyponyms and 3) substring matches. The heuristic checks if the hits for synonym, hyponym and substring matches meet the given threshold values. So the number of met thresholds is between 0 and 3. If this number is equal of higher than the number of thresholds that must be met, the word will be related to that category, otherwise not. If several categories reach these thresholds, the requirement will be categorized into all of these categories (multi-dimensional categorization is allowed) 7) Save the element as an individual of the ontology class , if it is not already in the declared as primary keys (uniquely identi fying the element). If the element has al-ready been saved in another class as well (which could be the case), declare that the new element is the same as the already existing one with the  X  X wl:sameAs X  property. 8) Semantic requirements conflict analysis: If the requirements are formally de-scribed using a specified grammar (e.g., EBNF), the information contained in the textual requirement descriptions can be semantically analyzed in order to identify possible inconsistencies and/or conflicts, see subsection 3.2. 3.2 Semantic Conflict Analysis In the second phase, analysis and reporting approaches build on the mapping of re-quirements to semantic categories. For formally specified requirement semantics, in our case following an EBNF template (see Fig. 2), semantic analysis can identify available facts. These assertions are based on the available requirements, while the available facts are based on the environment and properties of the target system. 
Fig. 3 depicts examples for conflicts between requirements (the CRR conflict type explained in section 4), e.g. the third conflict contains an inconsistency between requirements nr. 16 and nr.13: The  X  X hing to be processed X  part of requirement nr. 16 contains a value of 30 for  X  X umber of index updates X , whereas requirement nr. 13 contains the value 20 for  X  X umber of index updates, which finally is a requirements conflict. 3.3 Research Issues The underlying idea of this research is to use advanced semantic technologies, like ontologies and reasoning mechanisms, to increase the effectiveness and efficiency of ReqM activities. In a large software project, tasks like requirements categorization, conflict analysis, and tracing would need human effort and duration that often prohibits requirements and (b) conflicts that get discovered late and expensively. 
In this context, the main research question of this paper is: To what extent can a se-mantic-technology-based approach, like OntRep, increase the effectiveness and effi-ciency of requirements categorization and conflict analysis compared to a traditional manual approach? In order to address the research question we derived the following variables (according to [8]) to consider for evaluation:  X  The number of requirements determines the effort necessary for categorization  X  Number of requirement categories used to categorize the requirements. Further,  X  Approach for categorization/conflict analysis : e.g., for automation approaches 
Dependent variables that we want to study by the evaluation are:  X  Number of conflicts identified . This number consists of two measures: recall  X  True conflicts that have not been identified (false negatives): subtracting the  X  Plausibility of requirements classification : regarding categorization two kinds of Besides these parameters we also record th e effort for requirements categorization and used for categorization and conflict analysis), categorization effort , and conflict analysis effort . The case study is described in detail in the following section. The following subsections describe the characteristics of the pilot study design. Study Subject. The case study project  X  X echnoweb 2.0 X  is an IT development project with the goal to design and implement a web application that serves as a platform for communication and networking between technology experts within Siemens. This the web application. The project is performed in an agile way using the software de-velopment process  X  X CRUM X  and the configuration &amp; project management platform Trac . In Trac all requirements, tasks and bugs are stored as tickets.
 Study Design, Material, Participants, and Process. We applied the standard prac-tices of empirical software engineering re search according to Freimut et al. [8] and Wohlin et al. [22]. Fig. 4 illustrates the steps of the empirical study. 
A1, B1) Study preparation: This step dealt with the creation/preparation of all arti-facts necessary for the evaluation: 23 requirements in EBNF syntax, 8 categories as input for the requirements categorization step, deployment of 22 seeded conflicts (based on typical requirements conflicts found in practice at Siemens Austria). Fur-ther, we used questionnaires to capture th e individual background experience of the participants and a feedback questionnaire to capture, whether the participants found the approach useful and usable. 
A2, A3) Participant selection and team building : There were 6 participants who performed the manual categorization and conflict analysis tasks. They had similar experience on project management (3 to 5 years) and on requirements management but advanced general software engineering know-how. In addition, we had one expert with deeper know-how and experience, especially in ReqM. Finally, there was one OntRep tool user. He was well familiar with OntRep and had similar experience as the other participants. As described below, the evaluation consisted of individual work and team work. For the latter, the teams were assigned randomly. 
A4, B2) Introduction (Guidelines and Data Collection): Before execution of tasks the study organizer introduced the participants to the project and the manual require-ments categorization and conflict detection tasks. Further, the participants were guided step-by-step through the requirements classification and conflict detection process. The participants were sitting in one room without talking to each other. In the team phase, the two teams (3 participants each) worked in separated rooms. The ex-pert, as well as the OntRep tool user, also worked separately. 
A5) Background questionnaire: before they started with the actual ReqM tasks, the participants filled in the questionnaire. 
A6) Individual requirements categorization: Then the participants read through the 23 given requirements. The participants individually categorized the requirements into one or more of the given 8 categories. Each participant conducts the requirements categorization individually. In addition, one Requirements Engineering expert also does the categorization. The time need ed by each participant is captured. 
A7) Individual requirement s conflict analysis: In addition to the 23 requirements that had been categorized before, further elements were displayed (as rows below the other requirements), namely: 11 constraints (technical and business), and 4 formal documentation rules (documentation guidelines). 
The participants again read through the task description and then had to identify conflicts and enter them into the sheet. A conflict can have one of the following types: conflict between requirements (CRR), conflict of a requirement with a constraint (CRC), conflict of a requirement with a formal guideline, i.e., ill-formed requirement (CRG). In total, the case study data contained: 5 conflicts of type CRR, 7 of type CRC, and 10 of type CRG. After the evaluation of the manual approach, we again have the 6 individual results. Again, one Requirements Engineering expert also con-ducted the conflict analysis. The effort needed by each participant was captured. 
A8) Team requirements categorization &amp; conflict analysis: Afterwards, the par-ticipants harmonized their individual results within 2 randomly assigned groups. Ef-fort was captured for this task. The results are 2 team sheets. A9) Feedback forms: filled in at the end by the participants. 
A10, B6) Evaluation of study results : The manually created results of the expert and the teams were then compared with the result generated by OntRep. The process for the automated approach is: 
B3) Ontology preparation: A tool expert created one ontology class in OntRep (Prot X g X ) for each category and then imported the given requirements from Trac as CSV into OntRep. 
B4) OntRep requirements categorization: The tool then executed the categoriza-tion and generated a final result. We captured the effort to create the ontology classes and to generate the final report. 
B5) OntRep requirements conflict analysis: Then, we again provided the require-ments as CSV-input to OntRep. Further, the tool expert had to model the constraints as facts and the formal guidelines as rules in the ontology. We captured the effort for this. Then, the tool executed the conflict identification and generated a final report. Data Capturing Analysis and Statistical Evaluation. Finally, we analyzed and evaluated the following results: (a) 6 spreadsheets for requirements categorization and categorization spreadsheet and 1 conflict an alysis spreadsheet from a requirement engineering expert, and finally (c) 1 categorization spreadsheet and 1 conflict analysis spreadsheet created with the OntRep appro ach. The results were evaluated with de-scriptive statistics in Excel and R and are described in the following section. The following subsections describe the results of the pilot study regarding require-ments categorization and conflict analysis. 5.1 Requirements Categorization In order to evaluate the requirements categ orization task, we took the categorization result of the requirements engineering expert as reference solution for comparing the results of the manual and automated approaches. 
Table 1 summarizes the results of the manual and automated requirements categori-zation approaches: the rows in the table contain the quality levels of the categorization:  X  X verfulfilled X  means that a requirement was categorized into all correct categories but also into one or more additional ones,  X  X orrect X  means that a requirement was catego-rized in the right categories.  X  X artially correct X  means that a requirement was catego-rized in some but not all of the correct categories,  X  X alse X  means that a requirement was than the individual results: the number of false categorizations is reduced, and the number of correct and overfulfilled categorization is increased. Overfulfillment is not a problem, because all requirements are categorized into the right categories, and into some more categories, but this is just additional information which is allowed. 
Categorization with OntRep was more accurate, i.e., 8 requirements (more than with the manual approach) have been categorized into the right categories without categorizing them in additional categories. On the other hand, comparing the sum of correctly categorized requirements (overfulfilled + correct) shows the lowest value for automation. Further, the number of false categorizations is also the highest. This is due to the fact, 4 requirements were not categor ized at all. The reason therefore is that the terms used in these requirements could not be mapped to the categories, neither through substrings, synonyms or hyponyms. 
The average effort for manual categorization was around 15 min per person. The person minutes. With OntRep the following preparations were necessary to enable the automated categorization: conversion of requirements into EBNF form (30 min.), preparation of ontology classes and user-defined synonyms (14 min.). After this, the run time for categorization was ca. 2 minutes. If the requirements exist in EBNF form, which is the case for some larger projects at Siemens Austria, the effort is similar to the manual average effort of manual cat egorization, but much more scalable. 5.2 Requirements Conflict Analysis We analyzed conflicts of the three types described above, because conflicts of this type can be modeled in OntRep by means of facts and rules. The OntRep results for given data were identified, because OntRep works reliably, when the following pre-requisite are met: requirements exist in EBNF as input via CSV, modeling of glos-sary terms in ontology (10 min.), modeling facts, constraints and rules in the ontology (46 min.). Therefore, the total OntRep prepar ation time is 100 min. for the given case. The overall report generation took 4 minutes. 
In comparison, the manual conflict analysis approach resulted in a lower complete-ness (see Table 2): the individual participants identified only 31.8% of existing conflicts on average. The harmonization of results within the groups brought an im-provement to 47.7%, which means that approximately 3 additional conflicts have been identified by merging of the individual results into one group result. Also the number of false positives was slightly reduced by 1. The correctness of the manual approach was also lower than with the OntR ep approach: 58.8% of identified conflicts were false positives. This percentage could only slightly be reduced by the group harmonization. i.e., ca. 1-2 false positives were been eliminated during team work. 
In addition to comparing the individual and group results with OntRep results, but also had one expert performing the conflict analysis. Compared to the other participants identified traces, and the lowest percentage of false positives. Regarding effort, the expert was also the best with the manual approach: He needed 45 min. for conflict analysis, whereas the other participants needed 97 min. on average. In addition the group phase took 37 min., resulting in an additional group effort 111 person minutes. 5.3 Threats to Validity We addressed threats internal validity [10] of the study by two measures: a) intensive reviews of the study concept and materials, and b) a test run of the study conducted by a test person in order to make sure that the guidelines, explanations, and task descrip-tions are understandable for the participants and to estimate the required effort/time frame. Regarding external validity [30], we performed this initial case study in a pro-fessional context at a software development company. The participants had medium requirements management know-how and advanced software engineering know-how. In addition, we had a requirements engineering expert as experimental  X  X ontrol group X . Nevertheless, the small number of participants might limit the generalization of results. Therefore, we suggest replicating the study in a larger context. 
Further, the requirements in this case study were formulated using the EBNF syn-tax, which is a major condition for OntRep to analyze the requirements. We did not yet analyze the quality of results with a set of requirements, which is not or only partially formulated in EBNF. Further studies are needed to evaluate this. Software and systems engineering projects are complex due to the increasing number and complexity of requirements, and the project participants with different domain backgrounds and terminologies. To keep the overview on requirements, project man-agers conduct requirements categorization, conflict analysis, and tracing. However, the manual conduct of these tasks takes significant effort and is error-prone. 
In this paper we proposed semantic technology as foundation for automating the requirements management tasks and introduced the automated ontology-based report-ing approach OntRep based on a project ontology and a reasoning mechanism. We used requirements formulated in EBNF as input to the proposed OntRep approach, which supports automated requirements categorization and requirements consistency checking. We evaluated the effectiveness and effort the OntRep approach based on a real-world industrial case study with 6 project managers in 2 teams. The study fo-cused on requirements categorization and requirements conflict analysis. During the evaluation the study participants a) categor ized the requirements of the case study project into a set of categories and b) inspected the given project requirements to identify conflicts between requirements. In addition a requirements expert and an OntRep user performed the same tasks to enable comparing the quality of results and the effort for all activities. 
The case study results suggest that OntRep can be an attractive alternative for requirements categorization in typical soft ware development projects, because it pro-vides slightly lower effectiveness with sim ilar effort compared to manual approaches, but much more scalable. OntRep X  X  performance can be increased by adding addi-tional, synonyms or hyponyms to the ontology (which has to be done manually at the moment), so that all used terms in requirem ents can be mapped to categories. Regard-ing conflict analysis, OntRep found all conflicts in the requirements during the em-pirical study, while manual conflict analysis identified only 50 to 60% of the conflicts and produced more false positives with similar effort. OntRep analyzes three types of conflicts at the moment: conflicts between requirements, conflicts between require-ments and some constraints, or conflicts of requirements with some formal guidelines. The OntRep automation approach seems beneficial for project managers who want to manage their requirements with less effort, but in the same turn keep the requirements consistency high. Using the OntRep approach, organizations in software development projects could benefit from reduced manual effort for categorization and conflict analysis, and reduced communication and clarification effort through semi-automated semantic conflict analysis support. 
Further work will focus on the replication of this pilot study in a larger context, i.e., with more participants to improve the external validity of results. In addition, we want to increase the number of requirements to be categorized and analyzed for con-larger sets of requirements. We assume that especially the efficiency of OntRep will improve with the number of requirements when compared to a manual approach. Another aspect is to adapt OntRep for application to a set of requirements. Acknowledgments. We want to thank Alexander Wagner for the prototype imple-mentation of the OntRep concepts and his support during the pilot study. This work has been supported by the Christian Doppler Forschungsgesellschaft and the BMWFJ, Austria. 
