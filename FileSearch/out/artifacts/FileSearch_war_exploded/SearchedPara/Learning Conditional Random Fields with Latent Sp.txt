 The ever increasing usage of acronyms in many kinds of doc-uments, including web pages, is becoming an obstacle for av-erage readers. This paper studies the task of finding expan-sions in documents for a given set of acronyms. We cast the expansion finding problem as a sequence labeling task and adapt Conditional Random Fields (CRF) to solve it. While adapting CRFs, we enhance the performance from two as-pects. First, we introduce nonlinear hidden layers to learn better representations of the input data. Second, we design simple and effective features. We create a hand labeled eval-uation data based on Wikipedia.org and web crawling. We evaluate the effectiveness of several algorithms in solving the expansion finding problem. The experimental results demonstrate that the new method achieves performs bet-ter than Support Vector Machine and standard Conditional Random Fields.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval-Information filtering.
 Algorithms Text mining, sequence labeling, expansion finding
People often use acronyms to avoid using long phrases repeatedly. For example,  X  X OM X  is often used to refer to  X  X ead Only Memory X ; X  X IV X  is often used instead of  X  X uman Immunodeficiency Virus X . Using abbreviations or acronyms can simplify our writing and reading, since the same amount of information is conveyed with less words to process. How-ever, acronyms in documents also introduce challenges for the readers who do not have the necessary domain-specific knowledge to understand the acronyms. The need to estab-lish a database of acronyms is becoming an important task. However, manually collecting open domain acronyms and the corresponding explanations is difficult and costly.
The existing methods for automatically building dictio-naries of acronyms fall into two categories: pattern-matching methods and supervised learning methods. The pattern-matching methods [1] [2] often fail, because the ways of con-structing acronyms varies a lot from case to case, which makes it very difficult to design rules or patterns to achieve high precision and recall. On the other hand, the supervised learning methods [3] [4] that can learn the patterns automat-ically from a large corpus have shown the advantages over pattern-matching methods. However, the supervised learn-ing models used so far can not automatically capture the sequential information well.

In this paper we focus on the acronym search problem: finding the corresponding expansions given acronyms as queries. This solves the major challenge when building an acronym dictionary. It can also be used as an acronym expansion search engine, which could be useful in practice (education, training, etc.) and is not well studied.

Specifically, the main contribution of this paper is three-fold. First, we formulate the expansion finding problem as a sequence labeling task and use Conditional Random Fields (CRF) [5] to solve it. Compared with the previous super-vised learning approach [4], the CRF takes the context and structure information into account, which is very important for the expansion finding problem. This problem is a struc-tured prediction task to recognize a sequence of tokens which construct a corresponding expansion from a sentence. Sec-ond, in order to deal with the complex nonlinear data, we propose to use a neural network and CRF hybrid model that we call SNCRF (Sparse Neural Conditional Random Fields), since the proposed CRF model learns sparse latent features for the expansion finding task. Third, we design simple and effective features and label schemes for this novel sequence labeling task. We test our model on a real dataset collect-ed from Wikipedia. Experimental results indicate that the proposed approach performs better than the state-of-the-art solutions.
Existing methods for automatic acronym search can be classified into two categories: pattern-matching techniques and machine learning-based methods.

The acronym finding systems based on pattern-matching methods, such as AFP (Acronym Finding Program) [1] and TLA (Three Letter Acronym) [2], rely heavily on expert a priori knowledge and require much human effort on de-signing and tuning the rules and patterns. Another recent work [6] uses similar methods to design complex patterns and rules to extract acronyms and expansions in texts.
Machine learning methods require less expert design effort and learn from labeled training data automatically. Chang et al. [3] proposed a supervised learning method to extract acronyms and expansions in biomedical literature. Nadeau and Turney [7] present a machine learning approach that us-es weak constraints to reduce the search space of the acronym and the expansion candidates. Xu and Huang use SVM to search an acronym and its expansions [4].

CRFs can model the interactions between labels and learn the structures and dynamics of sequence data. The CRF [5] model and related models have achieved state-of-the-art performance in many structured prediction tasks, such as Natural Language Processing , bioinformatics, and comput-er vision, etc. To handle the complexity and nonlinearity of the data, Lafferty et al. [8] present a kernelized version of a linear CRF to enable nonlinear mapping. Liu et al. [9] proposed a deep CRF which can learn high level latent fea-tures. A similar model that combines neural network with CRF was proposed by Peng etc.[10] .
Considering the problem of expansion finding in text given acronym queries, this section first analyzes the characteristic of the problem. Then the CRF modeling approach, includ-ing the details of nonlinear CRFs with latent sparse feature learning, is described.
The task of expansion identification, which is also called expansion finding, is to recognize tokens that constitute the long original form of a given acronym. For example, when the system is given an acronym query  X  X BA X , it tries to find a sequence of words  X  X ational Basketball Association X  in the sentences. This task is much related to the more general sequence labeling tasks.

We propose to model the structure of a sequence rather than modeling merely the label for each token. Thus, the task can be formalized as a sequence labeling problem: giv-en an acronym query q and a sentence consisting of token sequence x = ( x 1 , ..., x n ), determine the optimal sequence of token label names y = ( y 1 , ..., y n ) of all possible sequences. With the predicted label sequence y , we can identify whether there is an expansion for the acronym term q ; and which subsequence of the sentence is the corresponding expansion.
Furthermore, we adapt the label scheme which is widely used for sequence labeling tasks in natural language process-ing. For example, the NP chunking tasks usually represent a chunk (e.g., NP) with two labels, the beginning (e.g., B-NP) and inside (e.g., I-NP) of a chunk [11]. We use  X  X  X ,  X  X  X  and  X  X  X  to represent  X  X eginning of expansion X ,  X  X nside of expansion X , and  X  X thers X , respectively. An example of the BIO labeling method is shown in Figure 1.
The CRF models have been very successful in many se-quence labeling tasks. By modeling conditional probabil-ity P ( y | x ) rather than joint probability P ( y , x ), the dis-criminative models are believed to be better than genera-tive models for supervised labeling tasks [12]. In addition, the CRF models allow arbitrary, non-independent features on the observation sequence x , because the probability of a transition between labels may depend on the past and future observations. For the CRF model in the expansion finding problem, we want to learn a mapping from an ob-servation sequence x = { x 1 , x 2 , ..., x T } to a label sequence y = { y 1 , y 2 , ..., y T } . Each observation x t is represented by a feature vector  X  ( x t )  X  R d that is computed from the pair of the t th token and the given acronym.

A linear chain CRF model for the expansion finding prob-lem defines a conditional probability of y as where F ( y , x ) denotes a global feature vector for input ob-servation sequence x and output sequence y , Q centered at t of x . Z ( x ;  X  ) is a normalization factor over all the possible states of y  X  o f length | x | defined as the function f ( y , x , t ) computes a set of features for input sequence x and y at position t , and  X  is a vector of linear weights.

Typically, given a set of training example D = { x ( n ) , y , where x ( n ) t  X  X and y ( n ) t  X  Y , the linear weights can be estimated by maximizing the penalized log-likelihood with respect to the conditional probability There are efficient exact inference algorithms for linear chain-s CRFs such as the Viterbi algorithm [13] and belief propa-gation algorithms [14].
Motivated by [9, 10], we want to make the CRF models have the ability to learn features. To do so, we introduce a nonlinear transformation layer to compute the hidden rep-resentation of input observation vectors. Here the hidden representation can be seen as powerful features learned from input data.

Consider a sequence of observations x = { x ( n ) } N i =1 c = [ I n = y t ] N n =1 to encode y t , e.g., if y t = 2 and C = 4, then c = [0 , 1 , 0 , 0] T , we define our nonlinear-chain CRF as where the global feature function is defined as  X  ( x i ;  X  ) is a nonlinear feature extractor x  X  R M with pa-rameter  X  , and parameter vector  X  = (  X ,  X  ). In its sim-plest form,  X  ( x t ;  X  ) is a Q  X  M vector by concatenating {  X  ( x i ;  X  ) | x i  X  ( x ) t } . There are many other ways to enrich the model, for example, by defining higher-order features c
The proposed model (5) is like a multi-layer neural net-work (NN), which generally optimizes the classifier and hidden-layer features simultaneously. Our model puts a CRF upon  X  ( x i ;  X  ) that amounts to a hidden layer. In fact, our imple-mentation  X  ( x i ;  X  ) itself is a neural network where i = 1 , . . . , M ,  X  is a non-linear (tanh) transfer func-tion, and the parameters  X  include all the weights  X  and bias term b . Therefore, the overall SNCRF hybrid contains two nonlinear hidden layers and one structured-output layer.
The model (5) not only learns the linear parameters, i.e.,  X  , but also optimizes the features by tuning the transforma-tion parameter  X  in transformation function  X  . This capaci-ty of learning features is important, especially when the task is complex.

We use the following objective function to learn the pa-rameters  X   X  : L ( X ) =
The first term in Eq. 8 is the conditional log-likelihood of the training data. The second term constrains the latent feature to be sparse using L 1 norm, where is used to con-trol the weight for its contribution of the learning. In our implementation is fixed to 1. The third term is the log of a Gaussian prior with variance  X  2 , i.e., P ( X )  X  exp( 1 2  X  2
With the sparsity regularization, only a tiny fraction of the hidden units are active in relation to a given stimu-lus. Plenty of research shows that sparse features could improve the model performance [15] [16]. We adapt the methods in [9] to compute the gradients. Then gradient ascent is used to search for the optimal parameter values,  X   X  = arg max  X  L ( X ). Although the proposed model is non-convex, the experimental results show that it can achieve good performance in practice. In our experiments, we per-formed gradient ascent with the BFGS optimization tech-nique.
In this study, we investigate multiple alternative feature functions and explore different combinations.

In the traditional CRF models, there are usually two kinds of feature functions which are state functions and transition feature functions, respectively. A state function computes the features on individual vertex, and a transition function compute the features on an edge on the graph. In our study, we design one type of state feature function and two types of transition functions.

The state functions depend on the dependency of a vertex given a single label. The state function has the form where  X  ( x t ) is the feature vector for x t . For a standard CRF,  X  ( x t ) is the raw input vector, and for SNCRF,  X  ( x is the output of multiple hidden layers. Assuming  X  t has dimension d , we have |Y|  X  d state functions.

The transition functions depend on pairs of labels of ad-jacent vertices. The first transition function is defined as: where There are |Y| X |Y| transition functions and each corresponds to a hidden state variable pair ( y, y  X  ).

In addition to the above two widely used feature functions, we also introduce another function that considers both the vertex and edge information simultaneously
The number of this type is | Y|  X  |Y|  X  d . In the experi-ments, we will show that the third feature function can boost the model performance by a large margin.
Given the model parameters  X  = {  X ,  X  } , prediction of a new test sequence x is to estimate the most probable label sequence y  X  that maximizes our conditional model:
In this study, belief propagation is used to compute the marginal probability P ( y i = a | x ,  X ) for each possible state a  X  Y . Then the predicted label y  X  i is the one associated with the maximum sum of marginal probabilities.
In order to capture various information that might be use-ful for acronym expansion recognition, we design three kinds of features to be used by CRFs.

Orthographical features : Orthographical features de-scribe the structure of the target token (i.e. word to be labeled) without considering the query (i.e. acronym). For example, whether it contains both upper and lower case let-ters, whether it contains digits and whether it contains spe-cial characters. These features are important, because peo-ple often use some orthographical information to emphasize the tokens of expansions. We list the orthographical features in Table 1.

Token-query features : Token-query features describe the relationships between the target token and the given acronym query. For example, whether the first character of the token occurs in the given acronym, whether the token contains an upper case letter that occurs in the acronym.
Context feature : The neighbor tokens are important indicators of a target token X  X  category. We identify a con-text window of size= 3, which includes the target token, the token right before the target token and the token right af-ter the token. We extract the same orthographical features and token-query features for each token in the context win-dow. Specifically, we also extract the context token-query features: (1) whether the initial letter Init t  X  1 of the token at position t  X  1 appears in the acronym and the initial letter Init t of the token at position t appears in the acronym after the letter Init t  X  1 , (2) whether the initial letter Init token at position t appears in the acronym and the initial letter Init t +1 of the next token appears in the acronym after the letter Init t , (3) whether the capitalized letter Cap the token at position t  X  1 appears in the acronym and the capitalized letter Cap t of the current token appears in the acronym after the letter Cap t  X  1 , (4) the capitalized letter Cap t of the token at position t appears in the acronym and the capitalized letter Cap t +1 of the next token appears in the acronym after the letter Cap t .
An evaluation data set is created based on Wikipedia.org in which there are a large amount of acronyms and corre-sponding definitions, a.k.a., expansions. It is common for an expansion to appear without the corresponding acronym nearby.

A random set of acronyms are selected as queries, and possible relevant pages are crawled using them as seeds. Af-ter some pre-processing, such as removing the html tags and segmenting the text into sentences, we manually la-beled the corpus. This results a data set containing 255 unique acronyms, 1,372 distinct expansions, 6,185 expan-sion sentences, and 108,830 words. The average length of an acronym is 4.02 letters. It is worth to note that one acronym may have multiple expansions. On average, an acronym has 5.2 distinct expansions, and each expansion contains 3.11 words. The average sentence length is 17.6 words.
Various approaches for acronym expansion identification are evaluated from two aspects. One is the performance measurements per token, which tell how well the model clas-sifies each token in the sequence. The other is the measure-ment per expansion.

Precision, recall and F1 scores are used to evaluate the performance per token. However, the data is very unbal-anced, as a dominating majority of tokens have label  X  X  X . In-stead of the average performance measurements over all cat-egories, the precision P recP erT oken i , recall P recP erT oken and F1 P recP erT oken i on each category y i  X  { B, I, O } are reported, so that the reported measures are not all over-whelmed by tokens with label  X  X  X .

We define A i as number of correctly labeled tokens of class y , B i as the number of false negative tokens of class y i as the number of false positive tokens of class y i , D i number of true negative tokens of class y i , and A i + B C + D i is the total number of tokens. The definitions are:
F 1 P erT oken i = 2 P recP erT oken i
We realize the dataset created is not a random sample of the whole web, and thus the the recall on the evaluation data set is biased. However, we expect that it is highly correlated with the true recall and can be used to compare different methods.
 The accuracy per token is also used in this paper:
To estimate the performance per expansion, we first find each predicted expansion, which corresponds to a label se-quence that begins with  X  X  X  and ends right before the next  X  X  X . If the expansion is exactly right, a correct expansion i s found. The precision, recall and F1 per expansion are defined as follows:
P recP erExpan =
ReclP erExpan = F 1 P erExpan = 2 P recP erExpan  X  ReclP erExpan
To verify the effectiveness of our proposed model for the expansion finding problem, we compare it with several base-line methods, using five-fold cross-validation. The baseline methods can be categorized into two classes, i.e. sequence models such as CRF, SNCRF, and non-sequence models, lin-ear SVM and kernel SVM which are trained on token-based samples. Moreover, the methods can be orthogonally classi-fied as linear models, namely CRF and SVM, and nonlinear models for the others. Each method is tuned with window size Q = 1 , 3 , 5 ( Q is the size of windows described after equation (1)).

For the non-sequence models, linear SVM and kernel SVM, the training set is decomposed into token-based samples. The penalty parameter C is tuned with values 10 k , k =  X  3 .. 3. There are two parameters to be tuned: the penal-ty parameter C and the RBF kernel parameter  X  . Both the parameters of kernel SVM are also tuned with values 10 k , k =  X  3 .. 3.

For the sequence models, we train the standard linear-chain CRFs and nonlinear-chain SNCRFs. A regularizer is added to avoid overfitting during training. The regulariza-tion parameter is tuned with values 10 k , k =  X  3 .. 3. More-over, we also specify the architecture of the hidden layers for the SNCRF model. In our experiment, we use one hidden layer, and the number of hidden nodes is enumerated in the set { 5,10,15,20,25 } .

The sequence models are evaluated under two configura-tions according to the different combinations of alternative feature functions. For the first configuration, only feature types F v and F e are used. The corresponding models are denoted as CRF v,e /SNCRF v,e . For the second configura-tion, the other transition feature functions F ve were added. The corresponding models are denoted as CRF v,e,ve and SNCRF v,e,ve , respectively.
Figure 2 shows that label  X  X  X  is predicted more accurately than label  X  X  X  and  X  X  X  by each method in all evaluation mea-sures, which is because the data is unbalanced. The number of the tokens with label  X  X  X  is approximately fifteen times of that of tokens with label  X  X  X  and eight times of that of tokens with label  X  X  X .

For the first configuration for sequence model, SNCRF v,e performs better than the linear sequence model CRF v,e . The linear SVM model and CRF v,e model produces similar per-formance. The reason may be that the SVM and CRF have orthogonal ability in classification. The SVM has better generalization by maximizing the margin, while the CRF can take advantage of the sequence structure information of K ernel SVM 0.6971 0.7311 0.7121
SNCRF v,e,ve 0.8429 0.9170 0.8784 the data. Compared to the linear models, both the non-l inear models, kernel SVM and SNCRF v,e , performs bet-ter than the linear counterparts, namely linear SVM and CRF v,e . Moreover, SNCRF which is a nonlinear sequence model achieves the best performance including kernel SVM in all evaluation measures.

The second configuration for sequence model, CRF v,e,ve and SNCRF v,e,ve outperform those of the first configuration. We can see from Figure 2 that they achieve higher precision and recall. As a result, the F1 score of the sequence models with second configuration is much better than those of the first configuration, which implies that introducing the fea-ture function of F ve provide positive effect for the sequence models. It is more important to note that the proposed SNCRF v,e,ve achieves the best performance on all the three measurements.

Figure 3 gives the accuracy per token, which demonstrates consistent comparison results with Figure 2.

Table 2 shows the performance per expansion, which more directly and precisely reflect the actual effectiveness of each model. From Table 2 we have following observations: (1) the linear and nonlinear sequence models, CRF and SNCR-F, outperform the counterpart non-sequence models SVM and kernel SVM respectively, (2) the nonlinear sequence model SNCRF further improves the performance of CRF , (3) the sequence with second configuration achieves bet-ter performance than those of the first configuration, and SNCRF v,e,ve achieves the best performance on the result of per expansion. We also conduct statistical significance tests ( t -test) on the improvements achieved by SNCRF v,e,ve over the other comparison models. The results indicate that all the improvements are statistically significant ( p -value &lt; 0 . 05).

The experimental results demonstrate that the nonlinear sequence model SNCRF v,e,ve fits for the acronym expansion recognition task better than the other state-of-the-art com-pared models.
This paper studies the problem of expansion finding given an acronym. We formulate this task as a sequence labeling task based on the analysis of the problem characteristic. We propose to use a nonlinear sequence model, SNCRF model, for the problem of acronym expansion finding. Furthermore, effective feature functions and features for expansion finding are described in this paper. The experimental results on real data collected from wikipedia.org demonstrate that the pro-posed approach outperforms the state-of-the-art algorithms with a large margin.
The authors would like to thank Aaron Michelony for valuable feedback about the manuscript. This research is supported by the Fundamental Research Foundation for the Central Universities under Grant No.65010571 and the Ph.D. Programs Foundation of Ministry of High Education of Chi-na under Grant No.20100031110096. [1] Kazem Taghva and Jeff Gilbreth. Recognizing [2] Stuart Yeates. Automatic extraction of acronyms from [3] Jeffrey T. Chang, Hinrich Sch  X  A  X  Aijtze, and Russ B. [4] Jun Xu and Yalou Huang. Using svm to extract [5] John Lafferty, Andrew Mccallum, and Fernando [6] Mathieu Roche and Violaine Prince. Managing the [7] David Nadeau and Peter D. Turney. A supervised [8] John Lafferty, Xiaojin Zhu, and Yan Liu. Kernel [9] Jie Liu, Kai Yu, Yi Zhang, and Yalou Huang. Training [10] Jian Peng, Liefeng Bo, and Jinbo Xu. Conditional [11] Lance Ramshaw and Mitch Marcus. Text chunking [12] Vladimir N. Vapnik. The Nature of Statistical [13] L. R. Rabiner. A tutorial on hidden markov models [14] Ben Tasker, Abbeel Pieter, and Daphne Koller. [15] Bruno A. Olshausen and David J. Field. Emergence of [16] Chaitanya Ekanadham. Sparse deep belief net model
