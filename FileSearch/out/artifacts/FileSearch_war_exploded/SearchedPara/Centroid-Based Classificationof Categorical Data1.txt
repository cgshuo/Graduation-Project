 In recent years, centroid-based (alternat ive known as prototype-based) classifi-cation [1], as one of the popular instance-based methods, has gained increasing attention owing to their inherent simplicity and linear-time complexity. In such a classifier, a centroid vector is computed to represent the training samples of each class; then, test sample is assigned to the class that corresponds to its most similar centroid, according to the dist ances between test sample and the centroid vectors. Though simple, the experimental results reported in the literature have validated that the performance of a centroid-based classifier could outperform other algorithms for a wide range of applications [2].

However, the traditional methods cannot be applied to categorical data clas-sification. The difficulties are two-folds. First, when the data are categorical the set mean is an undefined concept, since the attributes can only take discrete val-ues. Consequently, the class centroid, generally defined on the mean of numeric attributes, also becomes undefined for categorical data. Second, the commonly used distance measures, such as the Euclidean distance designed for numeric data, cannot be used for categorical objects. A few alternative measures have been suggested, including the common simple matching coefficient distance [3]. However, they are ineffective in practic e because, essentially, they assume that all the attributes are equally important. The assumption hardly holds in many real-world applications: for example, with high-dimensional data where there are typically a considerable number of noisy attributes that do not contribute to class prediction [4].

In this paper, two new approaches are proposed to make the centroid-based method effective for categorical data classification. We reformulate the defini-tion of centroids for categorical classes, allowing the centroids to be learned from categorical attributes. We propose two attribute-weighting approaches for the computation of weighted distance between categorical object and the cen-troid. The attribute weights are learned in terms of their contributions to class prediction, in a supervised manner. Suppose that we are given a training data set tr consisting of N objects each with one of the K pre-defined classes. The k th class, where k =1 , 2 ,...,K , is denoted by c k containing | c k | objects. A data object is denoted by x =( x 1 ,x 2 ,...,x D ) where D is the number of categorical attributes. The set of categories taken by attribute d =1 , 2 ,...,D is denoted by S d , and an arbitrary category in the set is denoted by s  X  S d .The d th attribute of x can be represented in the vector representation involving only numeric values, given by v ( x d )= &lt;I ( x d = s d 1 ) ,...,I ( x d = s dl ) ,...,I ( x d = s d | S d | ) &gt; ,where I ( This suggests a new formulation for the class centroid in categorical data. Definition 1 . The centroid of c k on the d th attribute is the vector m ( k ) d = &lt; m subject to | S d | l =1 m ( k ) dl =1,where || X || 2 stands for the Euclidean norm.
Using the Lagrangian multiplier technique, the optimization problem defined in Definition 1 can be transformed into an unconstrained problem. Thus, the centroid can be solved by taking derivatives to the objective function with respect to the variables and the Lagrangian multiplier, yielding which is precisely the relative frequency of s dl appearing in the d th attribute of c . Then, we measure the distance between x and the class c k by where M k = { m ( k ) d } D d =1 is the class centroids of c k and w = &lt;w 1 ,..., w ,...,w D &gt; the weighting vector. Her e, each attri bute-weight w d &gt; 0isde-fined to measure the contribution of attribute d to class prediction. The greater the contribution, the larger the weight.

We estimate the weights in a supervised way and present two weighting ap-proaches. The first approach is based on the mutual information ,oneofthe widely used measures to define dependency of variables [5], computed by This will be called mutual-information-based (MI-based for short) weighting approach. Another measure that has been popularly utilized to rank categorical attributes is the Gini index [3]. For s  X  S d , the Gini index is computed by GI ( s )=1  X  K k =1 [ p ( c k | s )] 2 with p ( c k | s )= x  X  c Then, we define the Gini-index-based (GI-based for short) weight as
The methods presented above are applie d to derive new centroid-based clas-sifiers, called FCC (F requency-C entroid-based C lassification). In the training phase of the new classifiers, the centroi ds are learned in terms of Eq. (1) and the attribute weights are learned according to Eq. (3) or Eq. (4), depending on the the MI-base or GI-based weighting approach used. The testing algorithm is similar to the traditional centroid-based classifier [1] but using Eq. (2) as the distance measure. That is, for each test sample x , its class label is predicted as the most similar class in terms of the K distances, using the weights and the centroids learned by the training algorithm. The classifiers are named FCC/MI and FCC/GI, respectively, accord ing to the weighting approach used. We evaluate the performance of FCC/MI and FCC/GI on six widely used real-world data sets, all of which were obtained from the UCI Machine Learning Repository (available at ftp.ics.uci.edu: pub/machine-learning-databases), as Ta-ble 1 shows. Our new classifiers will be co mpared with the traditional centroid-based classification method, called CBC, which is an extension to the the tra-ditional method [1] that makes use of Definition 1 to define the class centroids. Note that the distance measure used in CBC can be regarded as a special case of Eq. (2) where the weights are equally set to 1. We also carried out the classi-fications using C4.5 from the WEKA system [6] to provide a reference point. The classification performance of different classifiers were measured using Macro-F1 measure [2]. Each data set was classified by each classifier for 20 execu-tions using ten-fold cross validation, and the average performances are reported in the format average  X  1 standard deviation . Tables 1 shows the results. In the tables, the highest accuracy is marked in bold typeface, for the algorithm on each data set comparing with others, using the paired t -test with significance level 0.05.

The tables show that the centroid-based method can be made much more ef-fective for categorical data classification by our new approaches used in FCC/MI and FCC/GI. We observe the surprising improvements of the two new classifiers comparing with the traditional method CBC. In fact, CBC fails in classifying all eight data sets, indicating that simple extensions to the traditional method is in-effective. However, when combined with the weighted distance measures, as used in FCC/MI and FCC/GI, the new classifier becomes much more accurate. This can be explained by the observation of [4] that a linear transformation of input features can lead to significant improvement in an instance-based method. The tables also show that our new centroid-ba sed classifiers do significantly better than C4.5 on the data sets. The results largely dues to the attribute-weighting methods of FCC , which are able to recognize the different importance of at-tributes in discriminating the classes. In this paper, we proposed two new classifiers in order to make the traditional centroid-based method effective for categor ical data classification while retain-ing their numerous strengths in terms of classification efficiency. We proposed a new formulation for the centroid of categorical classes, such that the traditional method can be extended for categorical da ta. We also proposed two effective dis-tance measures for the dissimilarity computation of test samples and the training classes. We defined the new distance measures using mutual-information-based and Gini-index-based weighting approaches, which in effect perform soft feature-selection for the categorical attributes during the training process.
