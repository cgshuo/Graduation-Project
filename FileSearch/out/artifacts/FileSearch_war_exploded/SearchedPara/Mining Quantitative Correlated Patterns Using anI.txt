 Existing research on mining quantitative databases mainly focuses on mining associations. However, mining associa-tions is too expensive to be practical in many cases. In this paper, we study mining correlations from quantitative databases and show that it is a more effective approach than mining associations. We propose a new notion of Quantita-tive Correlated Patterns (QCPs), which is founded on two formal concepts, mutual information and all-confidence. We first devise a normalization on mutual information and ap-ply it to QCP mining to capture the dependency between the attributes. We further adopt all-confidence as a qual-ity measure to control, at a finer granularity, the depen-dency between the attributes with specific quantitative in-tervals. We also propose a supervised method to combine the consecutive intervals of the quantitative attributes based on mutual information, such that the interval combining is guided by the dependency between the attributes. We de-velop an algorithm, QCoMine , to efficiently mine QCPs by utilizing normalized mutual information and all-confidence to perform a two-level pruning. Our experiments verify the efficiency of QCoMine and the quality of the QCPs. Categories and Subject Descriptors: H.2.8 [Database Management]: Database Applications -Data Mining General Terms: Algorithms Keywords: Quantitative Databases, Correlated Patterns, Information-Theoretic Approach, Mutual Information
Mining correlations [3, 5, 11, 10, 18, 9] is recognized as an important data mining task for its many advantages over mining association rules [1]. Instead of discovering co-occurrence patterns in data, mining correlations identifies  X 
This work is partially supported by RGC CERG under grant number HKUST6185/03E.
 Copyright 2006 ACM 1-59593-339-5/06/0008 ... $ 5.00. the underlying dependency between the attributes in a pat-tern. More importantly, mining correlations does not rely on the support measure to perform pruning; thus, corre-lated patterns are not restricted to frequently co-occurring attributes, and those infrequent but significant patterns that are too expensive to be obtained by association rule mining can also be discovered. This property of correlation is very useful for the discovery of rarely occurring but important incidents, such as diseases, network intrusions, earthquakes and so on, and their possible causes.

Existing research on mining correlations is primarily con-ducted on boolean databases. However, most attributes in real-life databases can be quantitative , which are numeric values (e.g. salary), and categorical , which are enumerations (e.g. education level). We refer to these databases as quan-titative databases . A boolean database is in fact a special quantitative database that only has categorical attributes with boolean values. Thus, mining quantitative databases is a more general problem in its own right but a harder problem from the technical perspective than mining boolean databases.

In this paper, we propose to mine correlations from quan-titative databases using an information-theoretic approach. We study the properties of Mutual Information ( MI ) [6] on quantitative databases and define Normalized Mutual Infor-mation ( NMI ) that is to be applied in the context of correla-tion mining. Then, we propose a new notion of Quantitative Correlated Patterns ( QCPs ) based on NMI and the well-established correlation measure, all-confidence [12, 11]. This new definition of QCPs achieves two levels of quality control on the mining result. First, we employ NMI to specify a re-quired minimum degree of dependency among all attributes in a pattern. Then, we use all-confidence to enforce corre-lation at a finer granularity on the specific intervals of the quantitative attributes.

The first step in mining quantitative databases is to dis-cretize the large domain of a quantitative attribute into a number of small intervals. During the mining process, con-secutive intervals of an attribute may need to be combined to gain sufficient support value as well as to produce mean-ingful intervals [15, 16]. We develop a supervised interval combining method specifically for correlation mining so that the combined intervals also capture the dependency between the attributes, thereby ensuring the quality of the mined correlations. Our interval combining method utilizes MI to guide the interval combining of one attribute with respect to another attribute. We model the interval combining prob-lem as an optimization problem and devise a fast greedy algorithm as a solution.
 We develop an efficient algorithm, QCoMine , for mining QCPs. The algorithm is built on two effective pruning tech-niques: the attribute-level pruning by NMI and the interval-level pruning by all-confidence. First, at the attribute level, we define an NMI graph on all attributes such that an edge exists between two attributes only if their NMI exceeds a pre-defined threshold. We incorporate the NMI graph into our mining process, which effectively prunes an overwhelm-ing number of uncorrelated patterns that are generated from those attributes with low mutual dependency. Then, at the interval level, all-confidence is applied to further prune the uncorrelated intervals of the highly dependent attributes. With its downward closure property, all-confidence is able to quickly converge a large search space to a small and promis-ing one.

Our experiments show that the supervised interval com-bining method and the pruning by NMI and all-confidence are the keys to efficient correlation mining from quantitative databases. Without any one of them, QCoMine either uses substantially more resources (orders of magnitude greater running time and memory usage) or is unable to complete the mining (exhausting the memory). The patterns mined by QCoMine not only reveal the effectiveness of our inter-val combining method in obtaining meaningful intervals for correlated attributes, but also verify the efficiency of NMI and all-confidence in pruning uncorrelated patterns. We fur-ther examine the feasibility of mining frequent patterns from quantitative databases [15], compared with our approach of mining correlated patterns. We find that frequent patterns are mostly patterns with either very low all-confidence (i.e., uncorrelated) or trivial intervals (i.e., common-knowledge), while the majority of the patterns obtained by QCoMine are rare but highly correlated. When quantitative frequent pattern mining becomes infeasible even under very restric-tive settings such as very large minimum support thresholds, QCoMine still achieves an impressive performance.
 Organization. We give preliminaries in Section 2. We de-fine NMI in Section 3, based on which we propose a new notion of QCPs in Section 4. We present our supervised interval combining method in Section 5 and our mining al-gorithm, QCoMine , in Section 6. Then, we analyze the per-formance study in Section 7. Finally, we discuss related work in Section 8 and conclude our paper in Section 9.
Let I = { x 1 ,x 2 ,...,x m } be a set of distinct attributes or random variables 1 . These attributes can either be cat-egorical or quantitative .Let dom ( x j ) be the domain of an attribute x j , for 1  X  j  X  m .An item , denoted as x [ l is an attribute x associated with an interval [ l x ,u x ], where x  X  X  and l x ,u x  X  dom ( x ). We have l x = u x if x is cat-egorical and l x  X  u x if x is quantitative. A quantitative pattern (or simply called pattern ) is a nonempty set of items with distinct attributes. Given a pattern X , we define its attribute set as attr ( X )= { x | x [ l x ,u x ]  X  X } and its inter-val set as interval ( X )= { [ l x ,u x ] | x [ l x ,u x ] X is called a k -pattern if | attr ( X ) | = k . Similarly, we define
We use the terms attribute and random variable inter-changeably in subsequent discussions. k -attribute set and k -interval set, where k is the cardinality of the respective set. Given two patterns X and Y ,wesay X is a sub-pattern of Y (or Y is a super-pattern of X )if  X  x [ u x ,l x ]  X  X ,wehave x [ u x ,l x ]  X  Y . For brevity, we write a pattern X = { x [ l x ,u x ] ,y [ l y ,u y ] } as x [ l
For simplicity of discussion, we assume a lexicographic order in the set of attributes I . Thus, the items in a pattern are ordered according to the order of their attributes in
A transaction T is a vector v 1 ,v 2 ,...,v m ,where v j  X  dom ( x j ), for 1  X  j  X  m .Wesay T supports a pattern X if  X  x k [ l k ,u k ]  X  X , l k  X  v k  X  u k ,where k  X  X  1 ,...,m A quantitative database D is a set of transactions. The fre-quency of a pattern X in D ,denotedby freq ( X ), is the num-ber of transactions in D that support X .The support of X , denoted by supp ( X ), is the probability that a transaction T in
D supports X , and is defined as supp ( X )= freq ( X )/ |D| Running Example Table 1 shows an employee database as a running example throughout the paper for illustra-tion purpose. The database consists of six attributes: age , education , gender , married , salary and service years . The quantitative attributes include age , salary and service years . All the attributes are labelled with a set of consecu-tive integers. The last column of the table records the sup-port value of each transaction value. An example pattern is X = age [4 , 5] gender [1 , 1] and supp ( X )=0 . 25 + 0 . 19 = 0 . 44.
In this section, we first review the concepts of entropy and mutual information. Then, we propose a normalization of mutual information to make it applicable in mining correla-tions from quantitative databases.

Entropy and Mutual Information ( MI )aretwocentralcon-cepts in information theory [6]. Entropy measures the un-certainty of a random variable, while MI describes how much information one random variable tells about another one. Table 2 lists some notations used throughout this paper. In the context of mining quantitative databases, we have p ( v x )= supp ( x [ v x ,v x ]) and p ( v x ,v y )= supp ( x [ v Definition 1 (Entropy) The entropy of a random variable x , denoted as H ( x ), is defined as
The conditional entropy of a random variable y given an-other variable x , denoted as H ( y | x ), is defined as
H ( y | x )=  X 
The joint entropy of two random variables x and y ,de-noted as H ( x, y ), is defined as
H ( x, y )=  X  Definition 2 (Mutual Information) The Mutual Infor-mation ( MI ) of two random variables x and y , denoted as I ( x ; y ), is defined as
I ( x ; y )=
We now present some properties of MI that are used to develop a normalization on MI. Detailed proof can be found in [6].
 Property 1 I ( x ; y )= H ( x )  X  H ( x | y )= H ( y )  X  H ( y
Property 1 gives an important interpretation of MI. The information that y tells us about x is the reduction in the uncertainty of x given the knowledge of y , and similarly for the information that x tells about y . The greater the value of I ( x ; y ), the more information x and y tell about each other.
 Property 2 I ( x ; y )= I ( y ; x ).

Property 2 suggests that MI is symmetric ,whichmeans the amount of information x tells about y isthesameas that y tells about x .
 Property 3 I ( x ; x )= H ( x ).

Property 3 states that the MI of x with itself is the entropy of x . Thus, entropy is also called self-information . Property 4 I ( x ; y )  X  0.

Property 4 gives the lower bound for MI. When I ( x ; y )= 0, we have p ( v x ,v y )= p ( v x ) p ( v y ) for every possible values of v x and v y , which means that x and y are independent, that is, x and y tell us nothing about each other. Property 5 I ( x ; y )  X  H ( x )and I ( x ; y )  X  H ( y ). Property 5 gives the upper bound for MI.
 Property 6 I ( x ; y )= H ( x )+ H ( y )  X  H ( x, y ).
Property 6 shows that the MI of x and y is the uncertainty of x plus the uncertainty of y minus the uncertainty of both x and y .

Although MI serves as a good measure to quantify how closely two attributes are related to each other, the scale of the MI values does not fall in the unit range as shown by Properties 4 and 5. Property 5 indicates that the MI of two attributes is bounded by the minimum of their entropy. Since the entropy of different attributes varies greatly, the value of MI also varies for different pairs of attributes. To apply MI to our mining problem, we require a unified scale for measuring MI among a global set of attributes. For this purpose, we propose normalized MI as follows.
 Definition 3 (Normalized Mutual Information) The Normalized Mutual Information ( NMI ) of two random vari-ables x and y , denoted as I ( x ; y ), is defined as
Our idea is to normalize the MI of x and y by the maxi-mum MI of x (or y ) and any other attribute in I ,whichis either I ( x ; x )= H ( x )or I ( y ; y )= H ( y )asshownbyProp-erty 5. As a result, we eliminate the localness and make NMI a global measure. We now present some useful properties of NMI as follows.
 Property 7 I ( x ; y )= I ( y ; x ).
 Proof. It follows directly from Property 2.

Property 7 shows that, the same as MI, NMI is also sym-metric.
 Property 8 0  X  I ( x ; y )  X  1.
 Proof. Since I ( x ; x )  X  0, I ( y ; y )  X  0and I ( x ; y ) we have I ( x ; y )  X  0. By Properties 3 and 5, I ( x ; y ) MIN { H ( x ) ,H ( y ) } X  MAX { H ( x ) ,H ( y ) } = MAX { I ( y ; y ) } ,thus I ( x ; y )  X  1.

This property ensures that the value of NMI falls within the unit range [0 , 1].
 Proof. By Properties 1 and 3, we have I ( x ; y )= MIN { I
Property 9 gives the semantics of NMI, that is the min-imum percentage of reduction in the uncertainty of one at-tribute given the knowledge of another attribute . Example 1 Consider the employee database in Table 1, by Definition 2, we can compute I ( age ; married )= v shows that the knowledge of age causes a reduction of 0 . 47 in the uncertainty of married . However, we have little idea how much a reduction of 0 . 47 is. Using the normalization, least 21% of the uncertainty of age and married .

Similarly, we can compute I ( gender ; education )=0 . 40 Note that I ( age ; married ) &gt;I ( gender ; education ), but I ( age ; married ) &lt; I ( gender ; education ). This means that the minimum percentage of reduction in the uncertainty of gender and education is higher than that of age and married , although the MI of the former is lower than that of the latter. The higher MI of age and married is mainly because the entropy of age is much higher than that of education (i.e., H ( age )=2 . 19 &gt;H ( education )=1 . 34), which means a much larger absolute value of uncertainty to be reduced rather than the relative amount. This shows the advantage of NMI over MI.
In this section, we first generalize the concept of all-confi-dence for a quantitative pattern and then propose the notion of quantitative correlated patterns.

There have been a number of measures [3, 12, 11] proposed for correlations. In recent years, all-confidence [12, 11] has emerged as a commonly adopted correlation measure and has been shown in many studies [12, 11, 18, 10, 9] that it reflects the true correlative relationship among attributes more accurately than do other measures. The all-confidence of a boolean pattern is defined as the minimum confidence of all the association rules that can be derived from the pattern . We generalize all-confidence for a quantitative pattern as follows.
 Definition 4 (All-Confidence of a Quantitative Pat-tern) The all-confidence of a quantitative pattern X ,de-noted as allconf ( X ), is defined as
A pattern is said to be interesting if its all-confidence is no less than a given minimum all-confidence threshold  X  .Ac-cording to this definition, any association rule derived from the pattern has confidence no less than  X  , which also indi-cates a high correlation among all the items in the pattern (note that a high-confidence association rule only indicates an implication from the set of items at the left side of the rule to that at the other side).

All-confidence has the downward closure property [12], which means that if a pattern has all-confidence no less than  X  , so do all its sub-patterns. This property also holds for the all-confidence of quantitative patterns since the sub-pattern in quantitative databases is defined in the same way as that in boolean databases.
 Example 2 Given the employee database in Table 1, we consider the pattern X = gender [1 , 1] education [1 , 1] and =0 . 53. Similarly, we can compute the all-confidence of the pattern Y = gender [1 , 1] married [1 , 1] to be allconf ( Y )= 0 . 9, which indicates a higher correlation among its items than that among the items of X .

Although all-confidence is a good measure of correlation among boolean attributes, it is inadequate for reflecting the correlation among quantitative attributes. This is because all-confidence is a measure applied at a fine granularity to the intervals of attributes. However, quantitative attributes often consist of a large number of intervals, we may obtain patterns that have high all-confidence simply as a result of co-occurrence (see an example in Example 3). In this case, all-confidence cannot serve as a true measure of the correla-tion among the attributes in the pattern.

Realizing that the definition of a correlated pattern [3] is a set of attributes that are dependent on each other and that MI is a well-established concept in information theory [6] to capture the dependency among attributes ,weincorporate the concept of MI into the definition of a QCP. In this way, we first ensure that every attribute in a QCP is strongly dependent on each other in the sense that every attribute carries a great amount of information about every other at-tribute in the pattern. Then, we further use all-confidence to guarantee that the intervals of the attributes are also highly correlated.
 Definition 5 (Quantitative Correlated Pattern) Given a minimum information threshold  X  (0  X   X   X  1) and a minimum all-confidence threshold  X  (0  X   X   X  1), a pattern X is called a Quantitative Correlated Pattern ( QCP )ifand only if the following two conditions are satisfied: 1.  X  x, y  X  attr ( X ), I ( x ; y )  X   X  ; 2. allconf ( X )  X   X  .

NMI has several properties that make it a natural mea-sure of correlation. First, NMI is a formal concept for mea-suring dependency between attributes. Second, NMI gives an intuitive meaning for quantifying the degree of depen-dency: NMI has a value of 0 to indicate independence and its value increases, within the unit range, with the increase in dependency. Third, we can define a threshold  X  for NMI to indicate the required minimum percentage of reduction in the uncertainty of an attribute given the knowledge of another attribute.
 Example 3 Given the employee database in Table 1, let  X  = 0 . 2and  X  =0 . 5. The pattern X = gender [1 , 1] education [1 , 1] is a QCP, since I ( gender , education )=0 . 30  X   X  as shown in Example 1, and allconf ( X )=0 . 53  X   X  as shown in Exam-ple 2. However, the pattern Y = gender [1 , 1] married [1 , 1] is not a QCP because I ( gender , married )=0 &lt; X  , although allconf ( Y )=0 . 9  X   X  . The truth is that the attributes gender and married are independent of each other, which can be easily verified by p ( v gender ,v married )=( p ( v son for the high all-confidence of Y is simply because both p ( gender [1 , 1]) and p ( married [1 , 1]) are very high (both of them are 0 . 9), which results in a high co-occurrence of the two items gender [1 , 1] and married [1 , 1]. Obviously, pat-terns such as Y are of little significance because they do not reveal the true correlations between the items in the pat-terns. This explains the necessity of the concept of NMI in the definition of QCPs.
 Problem Description Given a quantitative database D , a minimum information threshold  X  and a minimum all-confidence threshold  X  , the mining problem we are going to solve in this paper is to find all QCPs from D .
Before we mine the quantitative databases, we first dis-cretize the databases, using a discretization method such as equi-depth and equi-width, in order to deal with the contin-uous values and the large domain sizes. We discretize each quantitative attribute into a set of base intervals , each of which is assigned a label. The base intervals are considered as indivisible units during the mining process. Consecutive base intervals may be combined into larger intervals to gain sufficient support value, while a combined interval itself can have a more significant meaning than its composite base intervals. However, it is critical to control the interval com-bining process to avoid a combined interval becoming too trivial. For example, age [0 , 2] refers to infants and is more representative than age [0 , 0], age [1 , 1] or age [2 , 2]; however, age [0 , 100] is simply trivial.

The traditional method of controlling the size of a com-bined interval using a maximum support threshold [15] is inapplicable in our problem of mining correlations. This is because QCPs can be both rare patterns (of low support) and popular patterns (of high support) and thus have a wide range of support value. Other more sophisticated interval combining methods such as [16] have also been proposed but are primarily concerned with mining quantitative asso-ciation rules.

In mining QCPs, it would be advantageous to consider the dependency between the attributes when combining their intervals, because the intervals of an attribute can be com-bined in very different ways with respect to different at-tributes as to reflect specific meanings. For example, com-bining the intervals of the attribute age with respect to married can obtain totally different combined intervals com-pared to that with respect to gender , which is further elab-orated in Example 4.

We find that MI can be used to take into account the dependency between attributes and thus to guide the in-terval combining process to produce meaningful combined intervals. Since the interval combining is performed locally between a pair of attributes, we use MI, instead of NMI which is a global measure for all attributes.

We model the interval combining problem as a supervised optimization problem with MI as the objective function, which is described as follows.

Given two attributes x and y ,where x is quantitative and y can either be categorical or quantitative, we want to obtain the optimal combined intervals of x with respect to y .The objective function,  X  , of the optimization problem is defined as follows:  X  ( x, y )= I ( x ; y )  X  I ( x ; y ) where I ( x ; y ), H ( x )and H ( x, y )aretherespectivevalues of MI and entropy after combining the intervals of x .Note that H ( y ) remains unchanged, because the intervals of y are not combined.

Since both H ( x )and H ( x, y ) always decrease when the intervals of x are combined,  X  can be either positive or neg-ative depending on the rate of decrease of H ( x )and H ( x, y ). Thus, the optimization problem is to maximize the function  X  , that is, to either maximize the gain in MI (if  X &gt; 0) or minimize the loss in MI (if  X &lt; 0).

We now design an algorithm to solve this optimization problem. If x has n base intervals, to find an optimal so-lution will require O (2 n ) computations of MI values, where each MI value is computed from a possible set of combined intervals. Obviously, an exhaustive algorithm is unrealistic. We propose an efficient algorithm which greedily combines two consecutive intervals of x at each time. The idea of the greedy algorithm is described as follows.

At each time, we consider combining two consecutive in-tervals, i x 1 and i x 2 ,of x ,where i x 1 and i x 2 can be either a base interval or a combined interval. Let  X  [ i x 1 ,i x denote the value of  X  ( x, y )when i x 1 and i x 2 are combined with respect to y .
 Our algorithm, GreedyCombine , is shown in Procedure 1. Theidea(Steps13-19)istopickupateachtimethemax-imum  X  [ i x j ,i x j +1 ] ( x, y ) among all pairs of consecutive inter-vals, i x j and i x j +1 , and combine corresponding i x j Algorithm 1 CombineInterval () Procedure 1 GreedyCombine ( x, y,  X  x min , X  y min , flag )
We can efficiently retrieve the maximum  X  [ i x j ,i x j +1 by implementing a priority queue using a heap Q (Step cessed by putting two pointers in  X  [ i x j ,i x j +1 ] ( x, y ). The up-date of their positions in the heap takes only O (log n ) heapify operations. In the worst case when all intervals of x are to be combined into a single interval, the entire combining process takes O ( n log n ) heapify operations ( O (1) each) and O l are the number of base intervals of x and y , respectively.
To avoid a combined interval becoming too trivial, we set a terminating condition,  X  x min , as follows. We first set  X  bethemeanofall  X  [ i x j ,i x j +1 ] ( x, y ) in the heap (Step 11) with extremely small values removed. This initial value of  X  x chosen in order to allow most pairs of consecutive intervals, that have relatively high  X  , to have a chance to be combined before we start combining. Thus,  X  x min serves as the mini-mumgaininMI(if  X  x min &gt; 0) or the maximum loss in MI (if  X  x min &lt; 0) that we require. When intervals are combined, the heap is updated (Step 18) and some  X  [ i x j ,i x j +1 may become less than  X  x min . As a result, the corresponding i x j and i x j +1 will not be combined (Step 20).

If both attributes x and y are quantitative, we combine the intervals of x and y in turn recursively (Steps 21-22). The GreedyCombine terminates when the intervals of both at-tributes cannot be combined any more with respect to their respective  X  min (Steps 6 and 24). We keep a boolean flag as one of the input parameters of GreedyCombine to indicate whether the intervals of the other attribute are combined or not in the last iteration.

The main algorithm, CombineInterval , to combine the in-tervals for pairs of attributes, is shown in Algorithm 1. For each pair of attributes, which contains at least one quantita-tive attribute, CombineInterval invokes GreedyCombine by initializing  X  x min and  X  y min to be  X  X  X  and flag to be 0. Example 4 Consider the employee database in Table 1, where each label of quantitative attributes corresponds to one base interval. Using GreedyCombine , the combined in-tervals of age with respect to married are [1 , 1] and [2 , 5]. This is reasonable because for the transactions with married = 2, they all have a value of 1 for age . While for other trans-actions with married = 1, their values of age fall within the interval [2 , 5]. This reflects the case in real life that most of the people over a certain age, say 35, are married.
However, if we compute the combined intervals of age with respect to gender , the results are [1 , 2], [3 , 4] and [5 , 5], which are totally different from those of age with respect to married . Fewer base intervals of age are combined with respect to gender , because for the transactions with the same value of gender , their values of age scatter over all its possible values. This shows the case that there are young, middle-aged and old employees of both men and women. In this section, we present our algorithm of mining QCPs. Our algorithm utilizes NMI and all-confidence to perform a two-level pruning, which significantly reduces the search space of the mining problem. We first describe the pruning at each level and then present the overall algorithm.
The first condition of Definition 5 requires that, to gener-ate a pattern in the mining process, the NMI of every pair of attributes in the pattern must be at least  X  . This condition enables us to perform pruning at the attribute level of the mining problem. We show how the pruning is performed by introducing the NMI graph as follows.
 Definition 6 (Normalized Mutual Information Graph) A Normalized Mutual Information graph ( NMI graph )isan undirected graph, G =( V, E ), where V = I is the set of nodes and E = { ( x i ,x j ) | x i = x j a nd I ( x i ,x j set of edges.
 Lemma 1 (Necessary Condition) If X isaQCP,then attr ( X ) forms a clique in G .
 Proof. It follows directly from Definitions 5 and 6.
The necessity that the attribute set of a QCP must form a clique in the NMI graph reveals the strong inter-dependence between all attributes in a QCP.

Lemma 1 implies that we can generate the attribute sets of all QCPs by enumerating the cliques in the NMI graph. Since the mining approach without pruning at the attribute level can be modelled as a complete graph, that is, an NMI graph with  X  = 0, the search space is greatly reduced from enumerating all cliques in the complete graph to enumerat-ing all cliques in a much sparser NMI graph. The signifi-cance of this pruning at the attribute level is fully disclosed if we realize that an edge in the NMI graph can generate an enormous number of patterns, which is equal to the size of the cartesian product of the set of intervals of two inci-dent nodes (i.e., attributes) of the edge. We illustrate this concept in further detail in Section 6.2.

The complexity of enumerating all cliques in a graph is exponential. However, we show that the clique enumeration can be seamlessly incorporated into the mining process. Our mining algorithm adopts a prefix tree structure, called the attribute prefix tree , denoted as T attr , which is constructed as follows.
 First, a root node is created at Level 0 of T attr .Thenat Level 1, we create a node for each attribute in I as a child of the root, where each child node is assigned a label as the label of the attribute and the order of the children follows that of the attributes in I . T attr is then constructed in a depth-first manner as follows. For each node u at Level k ( k  X  1) and for each right sibling v of u ,if( u, v )isanedge in G ,wecreateachildnodefor u with the same attribute label as that of v . Then, we continue the construction in the same way with u  X  X  children at Level ( k +1).
 Lemma 2 Let u 1 ,...,u k be a path from a node u 1 at Level 1 to a node u k at Level k of T attr . Then, { u 1 ,...,u forms a clique in G .
 Proof Sketch. By induction on the length of the path, k .
The prefix tree is shown to be a very efficient data struc-ture for mining both frequent and correlated patterns, while Lemma 2 shows that the clique enumeration comes almost free with the construction of T attr . The only extra process-ing incurred is a trivial test of whether ( u, v )isanedgein G . Moreover, the clique enumeration can be terminated ear-lier by the all-confidence pruning described in the following subsection.

Lastly, we provide an easy and objective way of setting the minimum information threshold  X  as in Equation (2), which is the sum of the mean and the standard deviation of all distinct NMI values, so that G retains edges that reveal high mutual dependency between the two incident nodes. We also remark that, similar to the choice of thresholds for other measures such as the minimum support threshold in the frequent pattern mining problem, the choice of  X  can also be determined by domain experts to indicate how correlated they require the attributes in a pattern to be.  X  = MEAN { I ( x ; y ) | x = y } + STD { I ( x ; y ) | x = y Example 5 Given the employee database in Table 1, based on the combined intervals of quantitative attributes, we com-pute the NMI graph G as shown in Figure 1, where  X  =0 . 26 as given by Equation (2). There are only four edges in G , each of which is identified as a strong dependency between two attributes. Other edges do not exist in G because they cannot constitute any QCP. This ensures that uncorrelated patterns, such as gender [1 , 1] married [1 , 1] in Example 3, will not be generated, because there is no edge between gender and married in G .

To find the cliques in G , we construct an attribute prefix tree T attr as shown in Figure 2. It can be easily verified that each k -path in T attr represents a k -clique in G .
Although NMI can effectively eliminate the generation of patterns from uncorrelated attributes, patterns with low all-confidence may still be generated from correlated attributes. This is because a node in the attribute prefix tree T attr ally represents a set of patterns that have the same attribute set but different interval set. Thus, we also need pruning at the interval level. For this purpose, we employ the downward closure property [12] of all-confidence to prune a pattern X and all its super-patterns if allconf ( X ) &lt; X  .
Since the intervals of an attribute are combined in a super-vised way, the same attribute may have different set of com-bined intervals with respect to different attributes. When we join two k -patterns to produce a ( k + 1)-pattern, the intervals of the prefixing ( k  X  1) attributes in the two k -patterns may overlap. In this case, an easy way is to com-pute the intersection of the prefixing ( k  X  1) intervals of the two k -patterns to give the intervals for the ( k +1)-pattern. For example, given age [30 , 40] married [1 , 1] and age [25 , 35] salary [2000 , 3000], we intersect the intervals of age to obtain the new pattern age [30 , 35] married [1 , 1] salary [2000 , 3000].

However, the power of pruning by all-confidence comes from its downward closure property. Producing a ( k +1)-pattern by intersecting the intervals of k -patterns violates the downward closure property of all-confidence. This is be-cause shrinking the intervals in the ( k +1)-pattern may cause a great decrease in the support value of a single item so that the all-confidence of the ( k + 1)-pattern may become larger than that of its composite k -patterns. However, this prob-lem can be addressed if we enumerate all sub-intervals of a (combined) interval before we start to generate a pattern.
We first define the sub-interval of an interval. Given an interval [ l, u ], a sub-interval of [ l, u ]isaninterval[ l ,u ], where l  X  l  X  u  X  u .Weuse[ l ,u ] [ l, u ] to denote [ l ,u ] is a sub-interval of [ l, u ].

Recall that a node at Level k of T attr represents a k -attribute set. We start from Level 2 of T attr to generate 2-patterns. Let { x, y } be the attribute set represented by a node at Level 2, and S x and S y be the set of combined intervals of x and y . Similar to mining quantitative frequent patterns [15], we need to consider all pairs of sub-intervals of x and y as each of them represents a pattern. For each inter-val set { i x ,i y } ,where i x i x , i y i y , i x  X  S x we generate a QCP X = x [ i x ] y [ i y ]if allconf ( X )
The above computation is performed on the cartesian product of two sets of sub-intervals of x and y .Thesize of the cartesian product can be very big since an interval i =[ l, l + n ]has n ( n +1) 2 sub-intervals. Fortunately, our su-pervised interval combining method effectively clusters the base intervals of an attribute into small groups, which dras-tically reduces the size of the cartesian product.
Since the intersection of two overlapping intervals is just a common sub-interval of the two intervals, we ensure that all QCPs will be generated by enumerating all pairs of sub-intervals. Moreover, since all the possible sub-interval com-binations are considered in 2-patterns, which are the basis for generating k -patterns ( k&gt; 2), the downward closure property of all-confidence holds as usual and can be applied to perform the pruning. The sub-intervals are then consid-ered as indivisible intervals during the mining process and not intersected.

For a set of k -patterns generated at a node at Level k ( k 2) of T attr , they often share a large number of common sub-intervals in their prefixing ( k  X  1)-interval sets. Thus, we also use a prefix tree T u interval , called the interval prefix tree ,to keep the interval sets of all the patterns generated by a node u in T attr . The interval prefix tree not only saves memory for storing the duplicate sub-intervals, but also significantly speeds up the join of two k -patterns to produce a ( k +1)-pattern.
We now present our main algorithm, QCoMine ,asshown in Algorithm 2. We first combine the base intervals of each quantitative attribute with respect to another attribute. Then we construct the NMI graph G and use G to guide the con-struction of the attribute prefix tree T attr to perform pruning at the attribute level. Steps 5-13 of Algorithm 2 construct Level 2 of T attr and produce all 2-QCPs. Steps 14-15 in-voke RecurMine , as shown in Procedure 2, to generate all k -QCPs ( k&gt; 2) recursively in a depth-first manner. Note that at Step 6 of RecurMine when two k -patterns are joined, all the prefixing ( k  X  1) intervals should be the same in the two patterns, which means that no interval intersection is performed; in addition, the last intervals of two k -patterns should form the interval set of a corresponding 2-pattern, so as to ensure that the last interval is a sub-interval of one attribute with respect to the other.

To compute the all-confidence of a pattern, we adopt diff-set [19] to obtain the support value of the pattern, while we use an extra field to keep the maximum support value of the items in the pattern. The use of diffset , together with the depth-first strategy, effectively controls memory consumed in the mining process as evidenced by our experiments. Algorithm 2 QCoMine ( D ,  X  ,  X  ) Procedure 2 RecurMine ( u, T attr ,G,k ) Example 6 (Example 5 continued) Let  X  =0 . 26 and  X  = 0 . 6. The T attr constructed by QCoMine is shown in Fig-ure 2. The node gender at Level 2 of T attr represents the 2-attribute set { education , gender } .Both education and gender are categorical, thus all the sub-interval pairs of this attribute set are the six combinations of three values of education and two values of gender . Among the six cor-responding 2-patterns, only education [3 , 3] gender [2 , 2] has all-confidence of 0 . 9, which is greater than  X  .
The node salary at Level 2 of T attr , which is the child of the node education , represents the 2-attribute set { education , salary } . The combined intervals of salary with respect to education are [1 , 1] , [2 , 3] , [4 , 4], which have five sub-intervals: pairs formed for education and salary , among which only one corresponding pattern education [3 , 3] salary [4 , 4] satis-fies the all-confidence.

The node salary at Level 3 is generated by the RecurMine procedure, which joins the two 2-patterns education [3 , 3] gender [2 , 2] and education [3 , 3] salary [4 , 4] to produce a 3-pattern education [3 , 3] gender [2 , 2] salary [4 , 4], which has all-confidence of 0 . 9.
We evaluate the performance of our approach of mining correlations from quantitative databases on real datasets. All experiments were run on a linux machine with an AMD Opteron 844 (1.8GHz) CPU and 8 GB RAM.

We use two real datasets from the commonly used UCI machine learning repository [8]. Table 3 lists the name, the number of transactions, the number of attributes, and the maximum number of base intervals after the discretization, of each dataset. The number of quantitative attributes of each dataset is given in the brackets. The detailed informa-tion of these datasets can be found in [8].

The efficiency of our algorithm QCoMine and the quality of our QCPs are based on three major components that con-stitute QCoMine : the supervised interval combining method, the attribute-level pruning by NMI and the interval-level pruning by all-confidence. Since there is no existing work on mining correlations from quantitative databases, we mainly assess the effect of these three components on the perfor-mance of our approach.

We make three variants of our algorithm: (a) QCoMine, which applies the interval combining method and sets  X  as described by Equation (2); (b) QCoMine-0, which applies the interval combining method and sets  X  =0;and(c) QCoMine-1, which does not apply the interval combining method and sets  X  as described by Equation (2). We test all-confidence from  X  = 60% to  X  =100%. Effect of Supervised Interval Combining. When the interval combining method is not applied, we are only able to obtain the result on the dataset image at  X  = 100% as shown in Figures 3(a-b), while QCoMine-1 runs out of mem-ory on all other cases. QCoMine-1 is inefficient because when we allow the interval of an item to become too triv-ial, patterns will easily gain all-confidence greater than  X  by co-occurrence in the database. The number of patterns ob-tained by QCoMine-1 from image at  X  = 100% is 13.4 times more than that obtained by QCoMine and the difference in-creases rapidly for smaller  X  (700M patterns are returned at  X  = 90% before QCoMine-1 runs out of memory).

We define the span of an interval, [ l, u ], of an attribute as n ,where n is the number of base intervals of the attribute. For example, if age has 100 base intervals, the interval span of [20 , 80] is 60%. We find that, for the patterns returned by QCoMine-1 but not by QCoMine, 35% of them consist of intervals that have a span of 90% (i.e., almost the entire do-main), while most of the rest consist of at least one interval with a span over 30%.

The results show that our supervised interval combining method is effective in defining more meaningful intervals and thus avoids an overwhelming number of trivial patterns be-ing mined, which is essential in the control of memory and CPU usage.
 Effect of Normalized Mutual Information. The per-formance improvement by utilizing NMI as a pruning tool is clearly revealed by the performance difference between QCoMine and QCoMine-0. Figures 3(a-d) show that, com-pared with QCoMine-0, the running time of QCoMine is re-duced by over one order of magnitude for image and almost three orders of magnitude for spambase , while QCoMine con-sumes memory up to 44 times less for image and 10 times less for spambase than does QCoMine-0.

The number of patterns obtained by QCoMine is on av-erage 65 times less for image and 88 times less for spambase than that obtained by QCoMine-0. The extra patterns re-turned by QCoMine-0 are shown to consist of attributes with large interval spans. Recall that QCoMine-0 also applies our interval combining method. However, the result does not mean that the interval combining method is not effective. We investigate the attributes in the datasets and find that, if an attribute x has no or little correlation with another at-tribute y , our interval combining method may return some rather trivial combined intervals for x with respect to y . Such uncorrelated patterns are successfully pruned by the use of NMI in QCoMine and thus not returned.

Therefore, the results demonstrate the effectiveness of NMI both as a measure for correlation and as a tool for pruning unpromising search space.
 Effect of All-Confidence. Figures 3(a) and 3(c) show that the running time of both QCoMine and QCoMine-0 increases only slightly for smaller  X  . This is because the majority of the time is spent on computing the 2-patterns. No matter what the value of  X  is, we need to test every 2-pattern to determine whether it is a QCP, before we can employ the downward property of all-confidence to prune all super-patterns of an uncorrelated pattern. Therefore, the slight difference in running time for different values of  X  in fact reflects the pruning power of all-confidence, since our algorithm only spends a small portion of the time to generate the larger patterns when the pruning starts to work.
The number of patterns returned by QCoMine grows steadily by about 2 times for each decrease in  X  from 100% to 60%. A similar trend is also observed for QCoMine-0, except that when  X  decreases from 100% to 90%, there is a rapid in-crease in the number of patterns that consist of attributes with large interval spans.
In this section, we demonstrate the high complexity of mining quantitative frequent patterns as to further justify the effectiveness of our approach of mining correlations. We implement the algorithm proposed by Srikant and Agrawal [15] using the same prefix tree structure and the diffset [19] as used in QCoMine. We denote this algorithm as MFP in this experiment.

We test five settings of minimum support threshold  X  = 0 . 1% , 1% , 10% , 20% , 30%, for MFP. Since MFP uses a max-imum support threshold ,  X  m , to control the span of a com-bined interval, we set  X  m =1 . 3  X  , which means the support of a combined interval is at most 1 . 3  X  . Figure 4: Quantitative Correlated Patterns v.s. Quantitative Frequent Patterns for image
Figure 4(a) presents the cumulative probability distribu-tion of all-confidence over the patterns obtained by MFP for image .When  X  is small (  X  1%), over 80% of the pat-terns have very low all-confidence of less than 10%. When  X  = 10%, there are still half of the patterns having all-confidence of only 10%. Although most of the patterns have all-confidence greater than 80% when  X  = 20%, these pat-terns are mostly composed of attributes with trivial inter-vals, which are unlikely to be considered as useful knowledge. On the contrary, the support distribution of the patterns obtained by QCoMine, as presented in Figure 4(b), shows that most of the QCPs are rare (as over 70% of them have support less than 2%) and significant (as the items in these patterns are highly correlated). Mining such patterns using MFP requires a small  X  , while MFP with a small  X  may return a large number of uncorrelated patterns.

We also show the running time and memory consumption of MFP at each  X  (as indicated by the upper x -axis) and QCoMine at each  X  (as indicated by the lower x -axis) in Figures 4(c-d). Although they are incomparable, the figures do reflect that mining QCPs is much more stable in the use of resources than mining quantitative frequent patterns.
We do not present the results of MFP for spambase be-cause MFP runs out of memory for all values of  X  ,evenwhen we set  X  m almost the same as  X  . At the point that the mem-ory is exhausted, MFP already returns millions of patterns, which occupies over 20GB of disk space (for each  X  ). The massive number of patterns generated not only results in high memory consumption, but also reveals the difficulty in the use of the patterns for further analysis. On the contrary, QCoMine obtains impressive results for spambase as shown in Figures 3(c-d), which further reveals the effectiveness of mining QCPs over mining quantitative frequent patterns.
We also note that the high memory consumption of MFP is not due to our implementation, since MFP and QCoMine adopt the same depth-first strategy using the same data structure. In fact, the efficiency of QCoMine is primarily due to the supervised interval combining method and the pruning by NMI and all-confidence, as implied by the poor performance of QCoMine-0 and QCoMine-1 in Section 7.1.
Existing research on mining quantitative databases have mainly focused on mining quantitative association rules. This is first studied by Piatetshy-Shapiro [13] with both sides of the rule restricted to a single attribute. Srikant and Agrawal [15] generalize the work by allowing multiple attributes on both sides of the rule. Then, some variants of mining as-sociation rules have also been proposed, such as mining op-timized association rules [7, 4, 14] by finding the optimal values of certain given attributes, and mining association rules with its consequent as a statistical measure [2, 17, 20] (e.g., mean, min, max) of a quantitative attribute.
Wang et al. [16] propose an interestingness-based crite-rion to merge intervals. Their merging criterion is based on association rules, which means that the candidate rules should be generated beforehand and the interval combining is then performed on the rules instead of the attributes. Our objective function, in contrast, is based on the attribute sets, which further guide the generation of QCPs.

In mining correlations from boolean databases, Brin et al. [3] introduce the correlation measures,  X  2 and interest .Co-hen et al. [5] mine highly correlated 2-patterns measured by a symmetric similarity between two boolean attributes. Ma and Hellerstein [11] propose an m -pattern, of which any two subsets are mutually dependent measured by the condi-tional probability. Omiecinski [12] proposes two interesting measures, all-confidence and bond , both of which have the downward closure property. Xiong et al. [18] develop a measure called h-confidence , which is mathematically equiv-alent to all-confidence but defined from a different perspec-tive to capture the degree of affinity in a pattern and to eliminate the cross-support patterns. Later in [10] and [9], all-confidence is shown to be a better measure for correla-tions than  X  2 and interest .
To our knowledge, our paper is the first study on mining correlations from quantitative databases. We propose a new notion of QCPs, which achieves two levels of quality control on correlation based on NMI and all-confidence. We develop a supervised interval combining method to combine the in-tervals according to the dependency between the attributes. We devise an efficient algorithm, QCoMine ,tomineQCPs by utilizing NMI and all-confidence to perform a two-level pruning. Experimental results reveal that our interval com-bining method derives meaningful intervals and effectively eliminates the generation of trivial intervals, the number of which is always too large for the mining to be efficient. Our experiments also demonstrate that NMI is both an effective measure of correlation and a powerful tool for pruning un-promising search space arising from uncorrelated patterns, while all-confidence further ensures a stable performance for QCoMine as well as the quality of QCPs. We also show that QCoMine attains impressive speed and small memory con-sumption even when mining quantitative frequent patterns becomes too expensive, while the QCPs obtained are shown to be more useful than quantitative frequent patterns.
