 Vertical aggregation is the task of incorporating results from specialized search engines or verticals (e.g., images, video, news) into Web search results. Vertical selection is the sub-task of deciding, given a query, which verticals, if any, are relevant. State of the art approaches use machine learned models to predict which verticals are relevant to a query. When trained using a large set of labeled data, a machine learned vertical selection model outperforms baselines which require no training data. Unfortunately, whenever a new vertical is introduced, a costly new set of editorial data must be gathered. In this paper, we propose methods for reusing training data from a set of existing (source) verticals to learn a predictive model for a new (target) vertical. We study methods for learning robust, portable, and adaptive cross-vertical models. Experiments show the need to focus on different types of features when maximizing portability (the ability for a single model to make accurate predictions across multiple verticals ) than when maximizing adaptability (the ability for a single model to make accurate predictions for a specific vertical ). We demonstrate the efficacy of our meth-ods through extensive experimentation for 11 verticals. H.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithms vertical search, distributed information retrieval, query in-tent, domain adaptation  X  work done while at Yahoo! Labs Montreal
Many web search engines provide their users with access to specialized search services, or verticals , that focus on a specific type of media (e.g., blogs, images, video) or domain (e.g., health, music, travel). In some cases, if a user is aware of a relevant vertical, the query can be issued directly to a vertical-specific search engine. However, to improve user-experience, portal search engines have started embedding relevant vertical content in Web search results. This has been referred to as aggregated search . A necessary part of an aggregated search system is vertical selection , the task of deciding, given a query, which vertical back-ends, if any, should be presented alongside Web search results.

Vertical selection can be viewed as a type of resource selec-tion, a well-studied task in distributed information retrieval. Recent work shows that a supervised machine learning ap-proach to vertical selection outperforms traditional resource selection methods [2, 1]. Most of these traditional methods derive evidence exclusively from vertical content and do not require extensive training data [4, 15, 18, 17, 16, 19].
While a supervised approach outperforms state-of-the-art methods, one of its drawbacks is that it requires training data. As is shown in Arguello et al. [2], suitable training data (e.g., vertical relevance judgments for a set of queries) can originate from human annotations. Human annotation is resource intensive in terms of time and money. An an-notation effort may be sensible as a one-time investment. However, in practice, the aggregated search environment is often dynamic. Verticals can be added to or removed from the set of candidate verticals. Documents can be added to or removed from a vertical. The interests of the user population may drift, in effect, changing the set of vertical documents most likely to be requested by users. It may not be feasible to annotate a new set of queries every time the environment undergoes a significant change.

In this paper, our goal is to improve a system X  X  return on editorial investment. We are interested in the following sce-nario. Suppose we have a set of verticals for which we have collected training data in the form of human vertical rele-vance judgements. We refer to these verticals as the source verticals. Then, suppose we have a new vertical associated with no training data. We refer to this vertical as the target vertical. Our objective is to learn a predictive model for the target vertical using only source-vertical training data.
Our solution to this problem focuses on two model prop-erties: portability and adaptability . A portable model is one that can be effectively applied to any target vertical with no additional training data. If a model is portable, it can be used as a  X  X lack box X  for any new vertical. An adaptable model is one that can be tailored to a specific new verti-cal with no additional training data. If a model is adapt-able, its parameters can be automatically adjusted to suit the new vertical at no editorial cost . We will present mod-els which exhibit these properties and evaluate their perfor-mance across a set of 11 target verticals.
In distributed IR, most traditional approaches to resource selection cast the problem as resource ranking and score collections using functions tuned manually on a few train-ing queries. Most of these approaches focus exclusively on the similarity between the query and the collection content, possibly using sampled documents [4, 15, 18, 17, 16, 19]. Of these, so-called large document models treat each collec-tion (or its document samples) as a single monolithic doc-ument and adapt document ranking functions to rank col-lections [4, 18]. In contrast, small document models focus on those (sampled) collection documents most relevant to the query [15, 17, 16, 19]. ReDDE [17], for example, scores collections by their expected number of relevant documents. This expectation is based on the number of collection sam-ples predicted relevant, scaled by a factor proportional to the collection size.

Other approaches cast resource selection as a supervised machine learned classification [2, 1] or rank-learning task [20]. In contrast to the single-evidence resource-scoring methods described above, these approaches have the advantage of easily incorporating multiple types of evidence as input fea-tures, for example, resource-specific query-logs [2] or click-through data [1]. This type of evidence integration is critical when dealing with text-impoverished verticals (e.g., images, video), which are problematic for methods that focus exclu-sively on content-based evidence.

In machine learning, domain adaptation is the task of us-ing training data from one or more source domains to learn a predictive model for a target domain, typically associated with little or no training data. While domain adaptation has not been studied in the context of supervised vertical selection, it has been widely studied in other applications. The domain adaptation problem arises from the fact that the source and target data originate from different distribu-tions.

One line of work performs instance weighting to down-weight the influence of  X  X isleading X  source-domain training instances. Jiang and Zhai [11] do this by discarding source-domain training instances that are misclassified by a model trained on whatever target-domain training data is available. This is slightly different than our problem setting since we assume that we have no training data in the target domain.
A different approach is to perform feature weighing to learn a model that generalizes better to the target domain. Saptal and Sarawagi [14] perform feature subset selection by minimizing a distance function between a single source domain and target domain data distribution. The target data distribution is estimated using predicted labels from a source-trained model. This is different from our work since we want to learn from multiple source domains. Jiang [10] proposes an approach that uses training data from multi-ple domains. First, a generalizable subset of features is identified based on their predictiveness across source do-mains. Then, a model that focuses heavily on these features is used to produce predictions on the target domain. Fi-nally, a target-domain classifier with access to all available features is trained on these predictions. This approach as-sumes binary features and requires using linear classifiers. The framework we propose can handle real-valued features and models that can explore complex features interactions.
Feature subset selection can be viewed as a type of repre-sentation change aimed to make a model more generalizable to the target domain. A similar technique is feature augmen-tation. Given access to some target-domain training data, Daum  X e III [6] augments the feature space by making three versions of each feature: a source-domain, target-domain, and general version. A model is then trained on the union of the source and target training data. This allows the model to effectively weight a source of evidence differently when it is predictive of the target class in only the source domain, only the target domain, or both. Blitzer et al. [3] propose an approach that does this without requiring target-domain training data. The goal is to identify the correspondence of non-pivot features, which may have a different correlation with the target class across domains, based on their predic-tiveness of pivot features, manually identified as similarly correlated with the target class across domains. The as-sumption is that the cross-domain correspondence between pairs of non-pivot features is encoded in the weights assigned to each when training linear classifiers to predict the pres-ence of each pivot feature using non-pivot features.
A different approach is to train a model on the source do-main and then to only fine-tune it using a few target-domain training examples. In the context of web search, Chen et al. [5] propose several ways to fine-tune a model trained us-ing Gradient Boosting Decision Trees, which combines weak models into a more complex model. The approach of ap-pending trees (based on target-domain training data) to a source-trained model has the advantage of accommodating features only present in the target domain.
Let y v ( q ) denote the true relevance label of vertical v with respect to query q . In the general vertical selection setting, the goal is to learn a function f that approximates y . In this work, we focus on the following scenario. Assume we have a set, S , of source verticals each with labeled queries. Then, suppose we are given a new (target) vertical t with no labeled data. Our objective is to learn a function f that ap-proximates y t using only source-vertical training data. The quality of an approximation will be measured by some met-ric that compares the predicted and true query labels. We use notation, to refer to the evaluation of function f on query set Q . This metric could be classification based (e.g. accuracy) or rank-based (e.g. average precision).
In our work, the domain of f is not the universe of possi-ble query strings. Instead, we generate features of the query string which we believe correlate with vertical relevance and are generalizable across queries. In this section, we review the basic feature-based vertical selection model. A more detailed description of our features can be found in our ref-erences [2, 1].
Given query q and vertical v , let  X  v ( q ) be a vector of fea-tures. Whatever the vertical, the semantics of a particular feature are the same. For example, if  X  v ( q ) i refers to the number of times the query was issued by users directly to v , then  X  v 0 ( q ) i refers to the number of times the query was issued to v 0 . As a result, all feature vectors are of the same length.

We generate two types of features, 1. query-vertical features : specific to the query-vertical 2. query features : specific to the query and independent Our approach is to use signals shown to be useful for super-vised vertical selection in previous work. We describe them briefly below.
Query-vertical features are generated from the vertical. In our case, they are derived from the similarity between the query and sampled vertical content and from the similar-ity between the query and queries issued previously to the vertical by users. 2 We use five query-vertical features.
The first four query-vertical features are generated using a retrieval from a centralized sample index, an index that com-bines documents sampled from every vertical. ReDDE.top [2, 1] is a variation of ReDDE [17]. GAVG measures the geo-metric average document score of the vertical X  X  top sampled documents [15]. Soft.ReDDE [2] is a variation of ReDDE.top and uses a soft document-to-vertical membership. We gener-ated two Soft.ReDDE features. One version uses a document-to-vertical membership based on the similarity between the document and a language model constructed from vertical-sampled documents. A second version uses the similarity between the document and a language model constructed from the vertical query-log. Finally, we use the query like-lihood given the vertical X  X  query-log language model. Each query-vertical feature was mass normalized across verticals.
Query features are generated from the query, independent of the vertical. These features are used in previous work and are described more completely in Arguello et al. [2]. Boolean features include regular expressions and dictionary look-ups likely to correlate with vertical intent (e.g., does query con-tain the keyword  X  X ews X ?). Geographic features correspond to the output of a geographic named-entity tagger (e.g., does the query contain a city name?). Categorical features corre-spond to the output of a query-domain categorization algo-rithm (e.g., is the query related to the travel domain?). In total, we use about 120 query features.
We can imagine additionally having query-independent vertical features , for example, whether the vertical has ob-served a sudden increase in query traffic. We do not make use of vertical features.
Vertical-specific query-logs are available to the system un-der the assumption of a cooperative environment.
We adopt a machine learning approach to vertical selec-tion. In all experiments, we used the Gradient Boosted De-cision Trees (GBDT) algorithm [9]. The main component of a GBDT model is a regression tree. A regression tree is a simple binary tree. Each internal node corresponds to a feature and a splitting condition which partitions the data. Each terminal node corresponds to a response value, the predicted output value. GBDT combines regression trees in a boosting framework to form a more complex model. Dur-ing training each additional regression tree is trained on the residuals of the current prediction. In our case, we chose to minimize logistic loss, which has demonstrated effective performance for vertical selection in prior work [1, 2, 8, 7]. That is, the model maximizes, where ` log is the logistic loss function, We adopt GBDT because it is able to model complex feature interactions and has been effective for other tasks such as text categorization [13] and rank-learning [22].
A portable vertical selection model is defined as one that can make vertical relevance predictions with respect to any arbitrary vertical. In other words, a portable model is not specific to a particular vertical, but rather agnostic of the candidate vertical being questioned for relevance.

Let us examine the distinction between a portable and non-portable vertical selection model with an example. Con-sider a single-evidence model that predicts a vertical relevant based on the number of times the query was issued previ-ously to the vertical by users. This type of evidence is likely to be positively correlated with the relevance of the vertical in question. In fact, it is likely to be positively correlated with vertical relevance irrespective of the particular candi-date vertical. On the other hand, consider a model that predicts a vertical relevant if the query is classified as re-lated to the travel domain. This model may be effective at predicting the relevance of a vertical that serves travel-related content. However, it is probably not effective on a vertical that focuses on a different domain. This model is less portable.

Most existing single-evidence resource selection models can be considered portable [18, 17, 16, 15, 19]. For example, ReDDE [17] prioritizes resources for selection based on the estimated number of relevant documents in the collection. This expectation is a function of the number of documents sampled from the collection that are predicted relevant and the estimated size of the original collection. The greater the expectation the greater the relevance irrespective of the particular resource.
The objective of a portable vertical selection model, f ? is to maximize the average performance across source ver-ticals. Our assumption is that if f ? performs consistently well across S , then f ? will perform well on a new (target) vertical t . In general, the portability of a model is defined by a metric that quantifies performance for a vertical s  X  X  and a function that aggregates performance across verticals in S .

For example, the portability,  X  , which uses the arithmetic mean of the logistic loss metric is defined by, where Q s is the set of training queries for source s and Q the set of those sets; similarly, y s provides labels for vertical s and y S is the set of these functions. We refer to the model which optimizes  X  avg log as the basic model. Notice that, As a result, the solution which maximizes  X  avg log lent to the solution which minimizes the logistic loss across all feature-vector/relevance-label pairs from all source ver-ticals. That is, we can perform standard GBDT training on a pooling of each source vertical X  X  training set.
In the basic model X  X  training set, positive instances corre-spond to relevant query-vertical pairs from all source verti-cals. For this reason, we expect the basic model to focus on evidence that is consistently predictive of relevance across source verticals, and hence predictive of the target vertical. In other words, vertical-specific evidence that is conflicting with respect to the positive class should be ignored. The challenge, however, is that the positive instances in the ba-sic model X  X  training pool may be skewed towards the more popular source verticals. This is problematic if these ver-ticals are reliably predicted relevant using vertical-specific evidence, not likely to be predictive of the new target verti-cal. To compensate for this, we consider a weighted average of metrics across verticals. Specifically, where Z = P s  X  X  w s . We use the simple heuristic of weight-ing a vertical with the inverse of its prior, where p s is the prior probability of observing a query with relevant vertical s . This value is approximated with the training data, The goal is to make training instances from minority verti-cals more influential and those from majority verticals less. It is easy to see that Equation 4 is a generalization of Equation 3. Because we use logistic loss, this technique reduces to training with an instance weighted logistic loss where the instances are weighted by w s , the weight of the vertical,  X  As with the basic model, we can use standard GBDT train-ing to optimize for this metric.
An alternative to optimizing for a portable model is to find portable features and to train a model using only those. A portable feature is defined as a feature which is highly correlated with relevance across all verticals. Recall that, across verticals, all features are identically indexed. Let  X  be a predictor based only on the value of feature i . In pre-vious work, the effectiveness of features across verticals was shown to be very dependent on the vertical being consid-ered. In order to address the expected instability of feature predictiveness across verticals, we adopt a harmonic average for our aggregation method.
Additionally, features, on their own, are not scaled to the label range, making the use of logistic loss difficult. Instead of constructing a mapping from a feature value to the ap-propriate range, we adopt a rank-based metric. Let  X  f ( Q ) be the ranking of Q by f . We use average precision as our rank-based metric, where  X  f ( Q ) k denotes the query at rank k and P k precision at rank k ,
In other words, for each feature, we rank queries by fea-ture value and compute the harmonic mean average preci-sion across verticals. 3 Having computed the portability of each feature, we build a portable model by restricting our training to the most portable features.

The most portable features were selected by inspecting the distribution of portability values. Because portability values are in the unit range, we model our data with the Beta distribution. We fit the Beta distribution using the method of moments and then select features whose portability is in the top quartile of this distribution.
Above, we focus on ways of improving the portability of a model by influencing the model to ignore evidence that is vertical-specific. The argument is that a model that focuses heavily on vertical-specific evidence will not generalize well to a new target vertical.

Given access to target-vertical training data, previous work reveals two meaningful trends [2]. First, given a wide-range of input features, most features contribute significantly to performance. In Arguello et al. [2], no small subset of fea-tures was solely responsible for effective vertical prediction. Second, the features that contributed the most to perfor-mance, which characterize the domain of the query, seem to be vertical-specific (assuming that verticals focus on dif-ferent domains). Based on these observations, while ignor-ing vertical-specific evidence seems necessary to improve a
Because we do not know whether the feature value has a positive or negative relationship with the label, we compute  X 
AP ( f,y S , Q S ) using  X  induced in both directions and use the max. model X  X  portability, a model customized to a particular ver-tical is likely to benefit from it.

In the context of adaptation for web search, Chen et al. [5] propose several ways to adapt an already-tuned GBDT model given data in a new domain. Their approach, Tree-based Domain Adaptation (TRADA), essentially consists of con-tinuing the GBDT training process on labeled data from the target domain. More specifically, a set of new regression trees are appended to the existing model while minimizing a loss function (logistic loss, in our case) on the target data.
In our case, the challenge of using TRADA to adapt a model to a specific target is that we lack target-vertical training data. In the context of semi-supervised learning, self-training or bootstrapping [21] is the process of re-training a model using previous model predictions on unlabeled data. We combine self-training and model adaptation in the fol-lowing way. First, we use a portable model to label a set of queries with respect to the target vertical. Then, we use TRADA to adapt the portable model to its own target-vertical predictions.

Tree adaptation provides a method for adjusting the mod-eling of all features. Just as we can select portable features for a portable model, we can select vertical-specific, non-portable features for adapting a model to a specific target vertical. That is, the base model may focus on portable features while the additional trees X  X dded in the context of pseudo-training data X  X ay focus on non-portable features. In this case, we use the same feature portability measure (Equation 5) but select the least portable features for tree augmentation.

Pseudo-labels for the target vertical were produced using the portable model X  X  prediction confidence value with re-spect to the target vertical. We used the simple heuristic of considering the top N % most confident predictions as pos-itive examples and the botton (100  X  N )% predictions as negative examples.
The objective is to predict the relevance of a target vertical given a query. For this reason, we evaluate on a per-vertical basis. Given a set of verticals V with query-vertical relevance labels, each vertical was artificially treated as the new target vertical and all remaining verticals as the source verticals. That is, for each t  X  X  , we set S = V  X  t .

Given a set of queries with vertical-relevance judgements, evaluation numbers were averaged across 10 cross-validation test folds, using the remaining 90% of data for training. GBDTs require tuning several parameters: number of trees, maximum number of nodes per tree, and shrinkage factor (see Friedman [9] for details). These were tuned using a grid search and 10-fold cross-validation on each training fold. In all cases, cross-validation folds were split randomly. Signifi-cance is tested using a 2-tailed unpaired t-test.

TRADA was self-trained using predictions made on the test set. More specifically, at each cross-validation step, a basic model was tuned on the training fold (90% of all queries) and applied to the test fold (10% of all queries). Then, a TRADA model was pseudo-trained using predic-tions on the test fold. In other words, for a given vertical, we trained 10 basic and 10 TRADA models. Rather than tune pseudo-training parameter N , we present results for N = 2 . 5 , 5 , 10%.

We focus on 11 verticals, described in Table 1. These ver-ticals differ in terms of number of documents, domain (e.g., health, finance, travel), document type (e.g., news stories, embedded video clips, images), and level of query traffic. Our evaluation data consists of 25 , 195 unique queries, ran-domly sampled from Web traffic. Given a query, human editors were instructed to assign verticals to one of four rel-evance grades ( X  X ost relevant X ,  X  X ighly relevant X ,  X  X elevant X ,  X  X ot relevant X ) based on their best guess of the user X  X  verti-cal intent. It is possible for a query to have multiple verti-cals tied for a particular relevance grade. For about 25% of queries all verticals were labeled  X  X ot relevant X . These are queries for which a user would prefer to see only Web search results.
We are interested in the accuracy of a model X  X  target-vertical predictions. Given a set of predictions for a target vertical, precision and recall can be computed by setting a threshold on the prediction confidence value. Rather than report a single precision-recall operating point, we evaluate using ranking metrics. These metrics are computed from a ranking of test queries by descending order of prediction confidence value, the probability that the target vertical is relevant to the query.

We adopt two rank-based metrics: average precision (AP) and normalized discounted cumulative gain (NDCG). In com-puting AP, the labels  X  X ost relevant X ,  X  X ighly relevant X , and  X  X elevant X  were collapsed into a single grade: relevant. We then compute average precision according to Equation 6.
NDCG differs from AP in two respects. First, it differenti-ates between relevance grades. Second, given a target verti-cal, t , it favors a model that is more confident on queries for which t is more relevant. Put differently, compared to AP, it punishes high-confidence errors more severely than low-confidence errors. Following J  X  arvelin and Kek  X  al  X  ainen [12], NDCG for a target vertical t , evaluated over queries Q t computed as, where y maps the relevance grade to a scalar ( X  X ost rele-vant X : 3,  X  X ighly relevant X : 2,  X  X elevant X : 1,  X  X ot relevant X : 0). The normalizer Z is the DCG of an optimal ranking of queries with respect to the relevance.
Each vertical is associated with its own search interface. Table 2: Target-trained ( X  X heating X ) results: AP and NDCG.
Recall that our objective is to achieve the best perfor-mance possible without target-vertical training data. A ma-jor motivation is to alleviate the need for a model trained on target vertical data. Therefore, it is useful to measure our performance as a fraction of the performance achievable by a model with access to target training data. We show AP and NDCG results given human-annotated target-vertical train-ing data in Table 2. A target-specific model (using all avail-able features) was trained and tested for each vertical using 10-fold cross-validation. As in all results, we present perfor-mance averaged across test folds. Given our objective, this can be considered a  X  X heating X  experiment. However, these numbers present a kind of upper bound for our methods. Our goal is to approximate these numbers using no target-vertical training data. For this reason, all results beyond this section are normalized by these numbers (i.e., results are reported as percentage of target-trained performance). Also, this normalization facilitates an understanding for the cost-benefit of labeling the new vertical given source-vertical labels and our proposed methods.
Section 4.1.1 describes several query-vertical features that are used as input signals to our models. Prior work shows that each of these can be used as an unsupervised single-evidence vertical predictor [2, 1]. In other words, these methods can make target vertical predictions without train-ing data. To justify the added complexity of our models, we compare against these single-evidence approaches. To con-serve space, we present results for that which performed best in this evaluation: Soft.ReDDE [2]. Soft.ReDDE (the ver-sion for which the vertical language model was derived from vertical samples) performed equal to or better than the next best single-evidence method for all but 3 / 11 verticals based on both AP and NDCG.
We present portability results in Table 3. Across metrics, both vertical balancing (VB) and feature weighting (FW), that is, using only the most portable features, improves the performance of the basic model. Performance across verticals was either statistically indistinguishable or bet-ter. Compared to each other, feature weighting significantly improves the basic model across more verticals (8/11 for both metrics). Compared to Soft.ReDDE, the only method that does noticeably better is the basic model with feature weighting. Performance was significantly better for 4 verti-cals based on AP and 5 based on NDCG. Performance was significantly worse for only one vertical based on AP and no vertical based on NDCG.
 Table 3: Portability results: normalized AP and NDCG.
We present adaptability results in Table 4. TRADA adapts a basic model using its target-vertical predictions as pseudo-training data. Given its superior performance (Table 3), we used the unbalanced basic model with only portable features (basic+FW). We refer to this as the base model. TRADA was tested under two conditions. In the first condition, TRADA is given access to all features for adaptation. In the second condition, it is restricted access to only the non-portable features (those purposely ignored to improve the portability of the base model).

Table 4 presents several meaningful results. First, TRADA performs poorly when the adapted model is given access to all features (columns 4-6). In contrast, when restricted ac-cess to only the non-portable features (TRADA+FW), re-sults improve (columns 7-9).

TRADA+FW performs either equal to or better than the base model in all but one case for AP and in all cases for NDCG. Similarly, across metrics, TRADA+FW signif-icantly outperforms Soft.ReDDE across most verticals for all values of N . It performs significantly worse than Soft.ReDDE in three cases in terms of AP and none in terms of NDCG. Overall, we interpret this as a positive result in favor of adaptability. TRADA succeeds at adapting a portable model to a specific target vertical at no additional editorial cost.
In the previous section, we demonstrated that the perfor-mance improvement for both portable and adaptive models requires measuring individual feature portability. In order to investigate precisely which features were being selected, we plotted the value of  X  havg AP in Figure 1. As it turns out, the same five features were consistently chosen as the most portable for all target verticals (i.e., for all sets of source verticals). Interestingly, these correspond to our five query-vertical features (Section 4.1.1). Conversely, those features which were the least portable were consistently our remain-ing query features (Section 4.1.2). In hindsight, this observa-tion makes sense. Many existing resource selection methods score resources using a single metric and focus exclusively on query-vertical evidence [4, 15, 18, 17, 16, 19]. Despite this observation, we recommend future experiments continue to measure feature portability since this behavior may not gen-eralize to different sets of verticals and different tasks.
Vertical balancing significantly improved the basic model only across 4 / 11 verticals based on AP and 3 / 11 based on NDCG. Recall that we introduced balancing in order to discourage examples from source verticals with high priors dominating the training set. However, this does not address cases where several sources have the same non-portable fea-tures correlated with relevance. For example, video and im-ages tend to be relevant to queries that mention a celebrity name; travel and local tend to be relevant to queries that mention a geographic entity; and finance and jobs tend to be relevant to queries that contain a company name. Even though vertical balancing addresses a single vertical X  X  dominance in the training set, it does not address a small coalition of related verticals causing the model to use non-portable features. We believe that a more robust averaging technique X  X or example, the harmonic average used for fea-ture portability X  X ay result in superior performance in the presence of similar source verticals.

In the previous section, TRADA improves considerably when restricted access to only the non-portable features for adaptation. We believe this is due to the following. TRADA was pseudo-trained using predictions from the base model. This model (our best portable model) used only the most portable features. The set of all features is a superset of these. When TRADA is given access to the same features used by the base model, the adapted model tends to focus on these features in order to better fit the original base-model predictions. Therefore, the non-portable features (purposely inaccessible to the base model) were ignored. As it turns out, our set of non-portable features are highly effective given target-vertical training data. We compared a set of target-trained models using only the portable features (excluding the non-portable ones) with our target-trained models using all features (Table 2). When given access to non-portable features performance improved significantly across all ver-ticals for AP and all but one vertical for NDCG (results suppressed due to space limitations). When restricted ac-cess to only the non-portable features for adaption, TRADA is forced to focus on these highly effective features. This mechanism can be seen as a sort of regularization ensuring that both portable and non-portable features are used for prediction.

With respect to pseudo-training data parameter N , we observe that the optimal value of N seems to be vertical-dependent. There may be two reasons for this. First, the base model performance (column 5 in Table 3) is also vertical-dependent. Given a fixed value of N across verticals, pseudo-labels from some verticals may be noisier than others. Sec-ond, the optimal value of N for a given vertical may correlate with the vertical X  X  prior.
Maximizing the return on editorial investment is an im-portant aspect of any system requiring training data. We presented an ensemble of approaches which significantly im-prove prediction of a new target vertical using only source-vertical training data. Our results demonstrate that model portability , the ability of a model to generalize across dif-ferent target verticals, requires careful attention to feature portability , the ability of a feature to correlate with vertical relevance across different target verticals. We found that those features which seemed to be the most portable X  X nd hence most important for a portable model X  X ere query-vertical features as opposed those that are independent of the candidate vertical. Conversely, when we tried to adapt a model for a specific target, the least portable features ap-peared to be those most important for the adapted model to consider.

Furthermore, we showed that a portable solution can be used to build a target-specific one at no additional editorial cost. Our best approach adapted a portable model using its own target-vertical predictions. This approach consis-tently outperformed both the base model and a competitive alternative which does not require adaptation. Results also showed that, given available resources, human-annotation on the new target vertical remains the best alternative.
This work could be extended in several directions. In terms of portability, vertical balancing may be improved by modeling the similarity (in terms of predictive evidence) between source verticals. In terms of adaptability, further improvements may be achieved by modeling the similarity between each source vertical and the target vertical. We would like to thank Jing Bai, Daniel Boies, Hugues Bouchard, Jean-Fran  X cois Crespo, and Alexandre Rochette for helpful discussions and feedback. This work was sup-ported in part by the NSF grants IIS-0916553 and IIS-0841275 and a generous gift from Yahoo! through its Key Scientific Challenges program. Any opinions, findings, conclusions, and recommendations expressed in this paper are the au-thors X  and do not necessarily reflect those of the sponsors.
