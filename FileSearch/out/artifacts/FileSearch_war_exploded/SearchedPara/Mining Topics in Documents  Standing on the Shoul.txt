 Topic modeling has been widely used to mine topics from documents. However, a key weakness of topic modeling is that it needs a large amount of data (e.g., thousands of doc-uments) to provide reliable statistics to generate coherent topics. However, in practice, many document collections do not have so many documents. Given a small number of documents, the classic topic model LDA generates very poor topics. Even with a large volume of data, unsupervised learning of topic models can still produce unsatisfactory re-sults. In recently years, knowledge-based topic models have been proposed, which ask human users to provide some prior domain knowledge to guide the model to produce better top-ics. Our research takes a radically different approach. We propose to learn as humans do , i.e., retaining the results learned in the past and using them to help future learning. When faced with a new task, we first mine some reliable (prior) knowledge from the past learning/modeling results and then use it to guide the model inference to generate more coherent topics. This approach is possible because of the big data readily available on the Web. The proposed al-gorithm mines two forms of knowledge: must-link (meaning that two words should be in the same topic) and cannot-link (meaning that two words should not be in the same topic). It also deals with two problems of the automatically mined knowledge, i.e., wrong knowledge and knowledge transitiv-ity. Experimental results using review documents from 100 product domains show that the proposed approach makes dramatic improvements over state-of-the-art baselines. H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing Topic Model; Lifelong Learning; Opinion Aspect Extraction.
Topic models, such as LDA [ 4], pLSA [ 12] and their ex-tensions, have been popularly used for topic extraction from text documents. However, these models typically need a large amount of data, e.g., thousands of documents, to pro-vide reliable statistics for generating coherent topics. This is a major shortcoming because in practice few document col-lections have so many documents. For example, in the task of finding product features or aspects from online reviews for opinion mining [13 , 19], most products do not even have more than 100 reviews (documents) in a review website. As we will see in the experiment section, given 100 reviews, the classic topic model LDA produces very poor results.
To deal with this problem, there are three main approaches: 1. Inventing better topic models : This approach may be ef-fective if a large number of documents are available. How-ever, since topic models perform unsupervised learning, if the data is small, there is simply not enough information to provide reliable statistics to generate coherent topics.
Some form of supervision or external information beyond the given documents is necessary. 2. Asking users to provide prior domain knowledge : An ob-vious form of external information is the prior knowledge of the domain from the user. For example, the user can input the knowledge in the form of must-link and cannot-link. A must-link states that two terms (or words) should belong to the same topic, e.g., price and cost . A cannot-link indicates that two terms should not be in the same topic, e.g., price and picture . Some existing knowledge-based topic models (e.g., [1 , 2, 9, 10, 14 , 15 , 26, 28 ]) can exploit such prior domain knowledge to produce better topics. However, asking the user to provide prior do-main knowledge can be problematic in practice because the user may not know what knowledge to provide and wants the system to discover for him/her. It also makes the approach non-automatic. 3. Learning like humans ( lifelong learning ): We still use the knowledge-based approach but mine the prior knowledge automatically from the results of past learning. This ap-proach works like human learning. We humans always retain the results learned in the past and use them to help future learning. That is why whenever we see a new situation, few things are really new because we have seen many aspects of it in the past in some other contexts. In machine learning, this paradigm is called lifelong learn-ing [30, 31 ]. The proposed technique takes this approach.
It represents a major step forward as it closes the learning or modeling loop in the sense that the whole process is now fully automatic and can learn or model continuously.
However, our approach is very different from existing life-long learning methods (see Section 2).
 Existing research has focused on the first two approaches. We believe it is high time to create algorithms and build systems that learn as humans do. Lifelong learning is pos-sible in our context due to two key observations: 1. Although every domain is different, there is a fair amount of topic overlapping across domains. For example, ev-ery product review domain has the topic of price , most electronic products share the topic of battery and some also have the topic of screen . From the topics learned from these domains, we can mine frequently shared terms among the topics. For example, we may find price and cost frequently appear together in some topics, which in-dicates that they are likely to belong to the same topic and thus form a must-link. Note that we have the fre-quency requirement because we want reliable knowledge. 2. From the previously generated topics from many domains, it is also possible to find that picture and price should not be in the same topic (a cannot-link). This can be done by finding a set of topics that have picture as a top topical term, but the term price almost never appear at the top of this set of topics, i.e., they are negatively correlated. Such knowledge can clearly help modeling in a related new domain. The observations also indicate that we need doc-ument collections from a large number of domains, which we call the big data , to mine enough relevant and reliable must-links and cannot-links to help topic modeling in new domains.
 The proposed lifelong learning approach works as follows:
Phrase 1 ( Initialization ): Given n prior document collec-tions D = { D 1 ,...,D n } , a topic model (e.g., LDA) is run on each collection D i  X  D to produce a set of topics S i S =  X  i S i , which we call the prior topics (or p-topics for short). It then mines must-links M from S using a multiple minimum supports frequent itemset mining algorithm [ 20 ].
Phase 2 ( Lifelong learning ): Given a new document collec-tion D t , a knowledge-based topic model (KBTM) with the must-links M is run to generate a set of topics A t . Based on A , the algorithm finds a set of cannot-links C . The KBTM then continues, which is now guided by both must-links M and cannot-links C , to produce the final topic set A t . We will explain why we mine cannot-links based on A t in Sec-tion 4.2 . To enable lifelong learning, A t is incorporated into S , which is used to generate a new set of must-links M .
About knowledge-based topic models, there are two exist-ing ones, DF-LDA [ 1] and MC-LDA [10 ], that can use both must-links and cannot-links to help generate better topics. However, both of them assume that the user-provided must-links and cannot-links are correct and there is no conflict among them. However, these assumptions are violated in our case because of the following issues: (1) The automatically generated must-links and cannot-links can have errors. Blindly trusting them as in DF-LDA and MC-LDA generates poor results (see Section 6). (2) A term may have multiple senses or meanings. This can cause the transitivity problem . That is, if A and B form a must-link, and B and C form a must-link, a topic model, such as DF-LDA, will put all three terms in one topic, which is clearly not always correct. For example, the term light can have two distinct meanings and the system may find two must-links, { light, weight } and { light, bright } . It is clearly unreasonable to put these three terms together under the same topic. MC-LDA has difficulty with this problem too because it only chooses one must-link for each term in each document and ignores the rest, which is undesirable because it can miss a lot of good must-link knowledge.

In this paper, we propose a new topic model, called AMC (topic modeling with Automatically generated Must-links and Cannot-links), whose inference can exploit the automat-ically mined knowledge and deal with the issues of wrong knowledge and transitivity to produce superior topics. Our experiments, using review collections from 100 domains, show that the proposed AMC model outperforms state-of-the-art baseline models significantly.
Knowledge-based topic models have been proposed to in-corporate prior domain knowledge from the user to improve model performance. Existing works such as [1 , 9, 26 ] consid-ered only the must-link type of knowledge (e.g., price and cost ) while [1 , 10 ] also used the cannot-link type of knowl-edge (e.g., price and picture ). Most of the above models also assume the input knowledge to be correct. [ 9] is the first work to address the issue of wrong knowledge in topic models by using the ratio of probabilities of two words under each topic. However, [ 9] only assigns one piece of knowledge (in the form of link or set) to each term, which ignores many pieces of useful knowledge. As shown in Section 6, AMC out-performs it significantly. Other types of knowledge, such as document labels have also been used in [3 , 29 ].
Our work is closely related to transfer learning and life-long learning. Topic models have been used to help transfer learning [ 27 , 34 ]. However, transfer learning in these pa-pers is for traditional supervised classification, which is very different from our work of topic extraction. [17 ] transferred labeled documents from the source domain to the target do-main to produce topic models with better fitting. However, we do not use any labeled data. [35 ] modeled the language gap between topics using a user provided parameter indi-cating the degree of technicality of the domain. In contrast, our proposed AMC model is fully automatic with no human intervention. Another key difference is that transfer learn-ing typically uses the data from one source domain to help the target domain classification, while we use the knowledge obtained from a large number of past (source) domains to help the new (target) domain learning or modeling. In terms of lifelong learning [ 31 , 30 ], LTM [7 ] is the first topic model that performs lifelong learning or modeling. It also improves the model in [ 8], which was not proposed as a lifelong learn-ing model. However, LTM only considers must-links. AMC considers both must-links and cannot-links. AMC also has a more effective must-link mining method and deals with the transitivity or multiple sense problem, which was not tackled in [ 7]. As we will see in Section 6, AMC achieves dramatic improvements.

Since our experiments are carried out using product re-views, aspect extraction in opinion mining [ 19 ] is related. A topic is basically an aspect. Topic models have been used for the task by many researchers [ 5, 10 , 16, 21, 23 , 25 , 26 , 32 , 33 , 37]. However, none of these models mines must-links or cannot-links automatically to help modeling.
This section introduces the proposed overall algorithm, which follows the lifelong learning idea described in the in-troduction section. The algorithm consists of two phases:
Phase 1 -Initialization : Given a set of prior document collections D = { D 1 ,...,D n } from n domains, this step first runs the standard LDA on each domain collection D i  X  D to generate a set of topics S i . The resulting topics from all n domains are unionized to produce the set of all topics S , i.e., Algorithm 1 AMC( D t , S , M ) 1: A t  X  GibbsSampling( D t , N , M ,  X  ); //  X  : no cannot-2: for r = 1 to R do 3: C  X  C  X  MineCannotLinks( S , A t ); 4: A t  X  GibbsSampling( D t , N , M , C ); 5: end for 6: S  X  Incorporate( A t , S ); 7: M  X  MiningMustLinks( S ); S =  X  i S i . We call S the prior topic (or p-topic ) set. A set of must-links are then mined from S , which will be detailed in Section 4.1 . Note that this initialization phase is only applied at the beginning. It will not be used for modeling of each new document collection.

Phase 2 -Lifelong learning with AMC : Given a new/test document collection D t , this phase employs the proposed AMC model to generate topics from D t . To dis-tinguish these topics from p-topics, we call them the current topics (or c-topics for short). AMC is given in Algorithm 1. Line 1 runs the proposed Gibbs sampler (introduced in Sec-tion 5.3 ) using only the must-links M generated from the p-topic set ( S ) so far to produce a set of topics A t , where N is the number of Gibbs sampling iterations. Line 3 mines cannot-links based on the current topics A t and the p-topics S (see Section 4.2 ). Then line 4 uses both must-links and cannot-links to improve the resulting topics. Note that this process can run iteratively. We call these iterations the learning iterations , which are different from the Gibbs it-erations. In each learning iteration, we hope to obtain bet-ter topic results. We will experiment with the number of learning iterations in Section 6. Currently, the function In-corporate ( A t ,S ) (line 6 in Algorithm 1) is very simple. If the domain of A t exists in S , replace those topics of the do-main in S with A t ; otherwise, A t is added to S . With the updated S , a new set of must-links is mined (line 7), which will be used in the next new modeling task by calling AMC.
In this section, we present the algorithms for mining must-links and cannot-links, which form our prior knowledge to be used to guide future modeling.
A must-link means that two terms w 1 and w 2 in it should belong to the same topic. That is, there should be some semantic correlation between them. We thus expect w 1 and w 2 to appear together in a number of p-topics in several domains due to the correlation. For example, for a must-link price , cost , we should expect to see price and cost as topical terms in the same topic across many domains. Note that they may not appear together in every topic about price due to the special context of the domain or past topic modeling errors. Thus, it is natural to use a frequency-based approach to mine frequent sets of terms (words) as reliable must-links.
Before going further, let us first discuss the representa-tion of a topic to be used in mining. Recall that each topic generated from a topic model, such as LDA, is a distribu-tion over terms (or words), i.e., terms with their associated probabilities. Terms are commonly ranked based on their probabilities in a descending order. In practice, top terms under a topic are expected to represent some similar seman-tic meaning. The lower ranked terms usually have very low probabilities due to the smoothing effect of the Dirichlet hyper-parameters rather than true correlations within the topic, leading to their unreliability. Thus, in this work, only top 15 terms are employed to represent a topic. For mining the must-link and cannot-link knowledge, we use this topic representation.

Given a set of prior topics (p-topics) S , we find sets of terms that appear together in multiple topics using the data mining technique frequent itemset mining (FIM). Each item-set is simply a set of terms. The resulting frequent itemsets serve as must-links. However, this technique is insufficient due to the problem with the single minimum support thresh-old used in classic FIM algorithms.

A single minimum support is not appropriate because generic topics, such as price with topic terms like price and cost , are shared by many (even all) product review domains, but specific topics such as screen , occur only in product do-mains having such features. This means that different topics may have very different frequencies in the data. Thus, us-ing a single minimum support threshold is unable to extract both generic and specific topics because if we set this thresh-old too low, the generic topics will result in numerous spu-rious frequent itemsets (which results in wrong must-links) and if we set it too high we will not find any must-link from less frequent topics. This is called the rare item problem in data mining and has been well documented in [ 18 ].
Due to this problem, we cannot use a traditional frequent item mining algorithm. We actually experimented with one such algorithm, but it produced very poor must-links. We thus use the multiple minimum supports frequent itemset mining (MS-FIM) algorithm in [ 20 ]. MS-FIM is stated as follows: Given a set of transactions T , where each transac-tion t i  X  T is a set of items from a global item set I , i.e., t  X  I . In our context, t i is the topic vector comprising the top terms of a topic (no probability attached). An item is a term (or word). T is thus the collection of all p-topics in S and I is the set of all terms in S . In MS-FIM, each item/term is given a minimum itemset support (MIS). The minimum support that an itemset (a set of items) must sat-isfy is not fixed. It depends on the MIS values of all the items in the itemset. MS-FIM also has another constraint, called the support difference constraint (SDC), expressing the requirement that the supports of the items in an itemset must not be too different. MIS and SDC together can solve the above rare item problem. For details about MS-FIM, please refer to [ 20 ].

The goal of MS-FIM is to find all itemsets that satisfy the user-specified MIS thresholds. Such itemsets are called frequent itemsets . In our context, a frequent itemset is a set of terms which have appeared multiple times in the p-topics. The frequent itemsets of length two are used as our learned must-link knowledge, e.g., { battery, life } , { battery, power } , { battery, charge } , { price, expensive } , { price, pricy } , { cheap, expensive }
Note that we use must-links with only two terms in each as they are sufficient to cover the semantic relationship of terms belonging to the same topic. Larger sets tend to contain more errors, i.e., the terms in a set may not belong to the same topic. Such errors are also harder to deal with than those in pairs. The same rationale applies to cannot-links.
Following the same intuition as must-link knowledge min-ing, we also utilize a frequency based approach to mine the cannot-link knowledge. However, there is a major difference. It is prohibitive to find all cannot-links based on the prior document collections D . For a term w , there are usually only a few terms w m that share must-links with w while there are a huge number of terms w c that can form cannot-links with w . For example, only the terms related with price or money share must-links with expensive , but the rest of the terms in the vocabulary of D can form potential cannot-links. Thus, in general, if there are V terms in the vocabulary, there are O ( V 2 ) potential cannot-links. However, for a new or test do-main D t , most of these cannot-links are not useful because the vocabulary size of D t is much smaller than V . Thus, we focus only on those terms that are relevant to D t .
Formally, given p-topics S from all domain collections D and the current c-topics A t from the test domain D extract cannot-links from each pair of top terms w 1 and w in each c-topic A t j  X  A t . Based on this formulation, to mine cannot-links, we enumerate every pair of top terms w 1 and w 2 and check whether they form a cannot-link or not. Thus, our cannot-link mining is targeted to each c-topic with the aim to improve the c-topic using the discovered cannot-links.
To determine whether two terms form a cannot-link, if the terms seldom appear together in p-topics, they are likely to have distinct semantic meanings. Let the number of prior domains that w 1 and w 2 appear in different p-topics be N diff and the number of prior domains that w 1 and w share the same topic be N share . N diff should be much larger than N share . We need to use two conditions or thresholds to control the formation of a cannot-link: 1. The ratio N diff / ( N share + N diff ) (called the support ra-tio ) is equal to or larger than a threshold  X  c . This condi-tion is intuitive because p-topics may contain noise due to errors of topic models. 2. N diff is greater than a support threshold  X  diff condition is needed because the above ratio can be 0, but N diff can be very small, which may not give reliable cannot-links.

Some extracted cannot-link examples are listed below: { battery, money } , { life, movie } , { battery, line } { price, digital } , { money, slow } , { expensive, simple }
We now present the proposed AMC model. As noted ear-lier, due to errors in the results of topic models, some of the automatically mined must-links and cannot-links may be wrong. AMC is capable of handling such incorrect knowl-edge. The idea is that the semantic relationships reflected by correct must-links and cannot-links should also be reason-ably induced by the statistical information underlying the domain collection. If a piece of knowledge (a must-link or a cannot-link) is inconsistent with a domain collection, this piece of knowledge is likely to be either incorrect in general or incorrect in this particular test domain. In either case, the model should not trust or utilize such knowledge.
AMC still uses the graphical model of LDA and its gen-erative process. Thus, we do not give the graphical model. However, the inference mechanism of AMC is entirely dif-ferent from that of LDA. The inference mechanism cannot be reflected in the graphical model using the plate notation.
Below we first discuss how to handle issues with must-links and cannot-links and then put everything together to present the proposed Gibbs sampler extending the P  X olya urn model, which we call the multi-generalized P  X olya urn (M-GPU) model.
There are two major challenges in incorporating the must-link knowledge: 1. A term can have multiple meanings or senses. For exam-ple, light may mean  X  X omething that makes things vis-ible X  or  X  X f little weight. X  Different senses may lead to distinct must-links. For example, with the first sense of light , the must-links can be { light, bright } , { light, lumi-nance } . In contrast, { light, weight } , { light, heavy } indi-cate the second sense of light. The existing knowledge-based topic model DF-LDA [1 ] cannot distinguish multi-ple senses because its definition of must-link is transitive.
That is, if terms w 1 and w 2 form a must-link, and terms w 2 and w 3 form a must-link, it implies a must-link be-tween w 1 and w 3 , i.e., w 1 , w 2 , and w 3 should be in the same topic. We call it the transitivity problem. DF-LDA would incorrectly assume that light , bright , and weight are in the same topic. MC-LDA [10 ] assumes each must-link represents a distinct sense, and thus assigns each term only one relevant must-link and ignores the rest. This misses a lot of good must-links. We propose a method in
Section 5.1.1 to distinguish multiple senses embedded in must-links and deal with the transitivity problem. 2. Not every must-link is suitable for a domain. First, a must-link may not be correct in general due to errors in topic modeling and knowledge mining, e.g., { battery, beautiful } is not a correct must-link generally. Second, a must-link may be correct in some domains but wrong in others. For example, { card, bill } is a correct must-link in the domain of restaurant (the card here refers to credit cards), but unsuitable in the domain of camera. We will introduce a method to deal with such inappropriate knowledge in Section 5.1.2 .
 To deal with the first issue, we construct a must-link graph to distinguish multiple senses in must-links to deal with the transitivity problem. To tackle the second problem, we uti-lize Pointwise Mutual Information (PMI) to estimate the word correlations of must-link terms in the domain collec-tion. These techniques will be introduced in the next two sub-sections and incorporated in the proposed Gibbs sam-pler in Section 5.3 .
In order to handle the transitivity problem, we need to distinguish multiple senses of terms in must-links. As our must-links are automatically mined from a set of p-topics, the p-topics may also give us some guidance on whether the mined must-links share the same word sense or not. Given two must-links m 1 and m 2 , if they share the same word sense, the p-topics that cover m 1 should have some overlapping with the p-topics that cover m 2 . For example, must-links { light, bright } and { light, luminance } should be mostly coming from the same set of p-topics related to the semantic meaning  X  X omething that makes things visible X  of light . On the other hand, little topic overlapping indicates likely different word senses. For example, must-links { light, bright } and { light, weight } may come from two different sets of p-topics as they usually refer to different topics.
Following this idea, we construct a must-link graph G where a must-link is a vertex. An edge is formed between two vertices if the two must-links m 1 and m 2 have a shared term. For each edge, we check how much their original p-topics overlap to decide whether the two must-links share the same sense or not. Given two must-links m 1 and m 2 , we denote the p-topics in S covering each of them as T 1 and T respectively. m 1 and m 2 share the same sense if where  X  overlap is the overlap threshold for distinguishing senses. This threshold is necessary due to errors of topic models. The edges that do not satisfy the above inequality (Equation 1) are deleted.

The final must-link graph G gives us some guidance in selecting the right must-links sharing the same word sense in the Gibbs sampler in Section 5.3 for dealing with the transitivity problem.
To measure the correctness of a must-link in a particu-lar domain, we apply Pointwise Mutual Information (PMI), which is a popular measure of word associations in text. In our case, it measures the extent to which two terms tend to co-occur, which corresponds to  X  X he higher-order co-occurrence X  on which topic models are based [11 ]. PMI of two words (or terms) is defined as follows: where P ( w ) denotes the probability of seeing term w in a random document, and P ( w 1 ,w 2 ) denotes the probability of seeing both terms co-occurring in a random document. These probabilities are empirically estimated from the cur-rent document collection D t : where # D t ( w ) is the number of documents in D t contain the term w and # D t ( w 1 ,w 2 ) is the number of docu-ments that contain both terms w 1 and w 2 . # D t is the total number of documents in D t . A positive PMI value implies a semantic correlation of terms, while a non-positive PMI value indicates little or no semantic correlation. Thus, we only consider the positive PMI values, which will be used in the proposed Gibbs sampler in Section 5.3 .
The main issue here is incorrect cannot-links. Similar to must-links, there are also two cases: a) A cannot-link con-tains terms that have semantic correlations. For example, { battery, charger } is not a correct cannot-link. b) A cannot-link does not fit for a particular domain. For example, { card, bill } is a correct cannot-link in the camera domain, but not appropriate for restaurants.

Wrong cannot-links can also cause conflicts with must-links. For example, the system may find two must-links { price, cost } and { price, pricy } and a cannot-link { pricy, cost } . Existing knowledge-based models, such as DF-LDA [ 1] and MC-LDA [10 ], cannot solve these problems. A further challenge for these systems is that the number of automati-cally mined cannot-links is large (more than 400 cannot-links on average). Both DF-LDA and MC-LDA are incapable of using so many cannot-links. As we will see in Section 6, DF-LDA crashed and MC-LDA generated a large number of additional (wrong) topics with very poor results.
Wrong cannot-links are usually harder to detect and to verify than wrong must-links. Due to the power-law dis-tribution of natural language words [38 ], most words are rare and will not co-occur with most other words. The low co-occurrences of two words do not necessarily mean a nega-tive correlation (cannot-link). Thus, we detect and balance cannot-links inside the sampling process. More specifically, we extend P  X olya urn model to incorporate the cannot-link knowledge, and also to deal with the issues above.
This section introduces the Gibbs sampler for the pro-posed AMC model, which differs from LDA as AMC needs the additional mechanism to leverage the prior knowledge and to also deal with the problems with the prior knowledge during sampling. We propose the multi-generalized P  X olya urn (M-GPU) model for the task. Below, we first introduce the P  X olya urn model which serves as the basic framework to incorporate knowledge, and then enhance it to address the challenges mentioned in the above sub-sections.
Traditionally, the P  X olya urn model works on colored balls and urns. In the topic model context, a term can be seen as a ball of a certain color and a topic as an urn. The distribution of a topic is reflected by the color proportions of balls in the urn. LDA follows the simple P  X olya urn (SPU) model in the sense that when a ball of a particular color is drawn from an urn, the ball is put back to the urn along with a new ball of the same color. The content of the urn changes over time, which gives a self-reinforcing property known as  X  X he rich get richer X . This process corresponds to assigning a topic to a term in Gibbs sampling.

The generalized P  X olya urn (GPU) model [ 22, 24 ] differs from SPU in that, when a ball of a certain color is drawn, two balls of that color are put back along with a certain number of balls of some other colors. These additional balls of some other colors added to the urn increase their proportions in the urn. This is the key technique for incorporating must-links as we will see below.

Instead of involving only one urn at a time as in the SPU and GPU model, the proposed multi-generalized P  X olya urn (M-GPU) model considers a set of urns in the sampling pro-cess simultaneously. M-GPU allows a ball to be transferred from one urn to another, enabling multi-urn interactions. Thus, during sampling, the populations of several urns will evolve even if only one ball is drawn from one urn. This ca-pability makes the M-GPU model more powerful and suit-able for solving our complex problems.
In M-GPU, when a ball is randomly drawn, certain num-bers of additional balls of each color are returned to the urn, rather than just two balls of the same color as in SPU. This is inherited from GPU. As a result, the proportions of these colored balls are increased, making them more likely to be drawn in this urn in the future. We call this the promo-tion of these colored balls. Applying the idea to our case, when a term w is assigned to a topic k , each term w 0 that shares a must-link with w is also assigned to topic k by a certain amount, which is decided by the matrix  X  w 0 ,w (see Equation 5). w 0 is thus promoted by w . As a result, the probability of w 0 under topic k is also increased.
To deal with multiple senses problem in M-GPU, we ex-ploit the fact that each term usually has only one correct sense or meaning under one topic. Since the semantic con-cept of a topic is usually represented by some top terms under it, we refer the word sense that is the most related to the concept as the correct sense. If a term w does not have must-links, then we do not have the multiple sense problem caused by must-links. If w has must-links, the rationale here is to sample a must-link (say m ) that contains w to be used to represent the likely word sense from the must-link graph G (built in Section 5.1.1 ). The sampling distribution will be given in Section 5.3.3 . Then, the must-links that share the same word sense with m , including m , are used to promote the related terms of w .
 To deal with possible wrong must-links, we leverage the PMI measure (in Section 5.1.2 ) to estimate knowledge cor-rectness in the M-GPU model. More specifically, we add a parameter factor  X  to control how much the M-GPU model should trust the word relationship indicated by PMI. For-mally, the amount of promotion for term w 0 when seen w is defined as follows:
To deal with cannot-links, M-GPU defines two sets of urns which will be used in sampling in the AMC model. The first set is the set of topic urns U K d  X  X  1 ...D t } , where each urn is for one document and contains balls of K colors (topics) and each ball inside has a color k  X  X  1 ...K } . This corresponds to the document-topic distribution in AMC. The second set of urns is the set of term urns U W k  X  X  1 ...K } corresponding to the topic-term distributions, with balls of colors (terms) w  X  { 1 ...V } in each term urn.

Based on the definition of cannot-link, two terms in a cannot-link cannot both have large probabilities under the same topic. As M-GPU allows multi-urn interactions, when sampling a ball representing term w from a term urn U W k we want to transfer the balls representing the cannot-terms of w , say w c (sharing cannot-links with w ) to other urns (see Step 5 below), i.e., decreasing the probabilities of those cannot-terms under this topic while increasing their corre-sponding probabilities under some other topic. In order to correctly transfer a ball that represents term w c , it should be transferred to an urn which has a higher proportion of w . That is, we randomly sample an urn that has a higher proportion of w c to transfer w c to (Step 5b below). How-ever, there is a situation when there is no other urn that has a higher proportion of w c . [ 10 ] proposed to create a new urn to move w c to under the assumption that the cannot-link knowledge is correct. As discussed in Section 5.2 , the cannot-link knowledge may not be correct. For example, consider that the model puts battery and life in the same topic k where both battery and life have the highest proba-bility (or proportion), a cannot-link { battery, life } wants to separate them after seeing them in the same topic. In such a case, we should not trust the cannot-link as it may split the correlated terms into different topics.

Based on all the above ideas, we now present the M-GPU sampling scheme as follows: 1. Sample a topic k from U K d and a term w from U W k se-quentially, where d is the d th document in D t . 2. Record k and w , put back two balls of color k into urn
U K d , and two balls of color w into urn U W k . 3. Sample a must-link m that contains w from the prior knowledge base. Get a set of must-links { m 0 } where m 0 is either m or a neighbor of m in the must-link graph G . 4. For each must-link { w,w 0 } in { m 0 } , we put back  X  number of balls of color w 0 into urn U W k based on matrix  X  w 0 ,w (in Equation 5). 5. For each term w c that shares a cannot-link with w :
Based on the above sampling scheme of M-GPU, this sub-section gives the final Gibbs sampler with the conditional distributions and algorithms for the AMC model. Inference of topics can be computationally expensive due to the non-exchangeability of words under the M-GPU models. We thus take the same approach as that for GPU in [24 ] which ap-proximates the true Gibbs sampling distribution by treating each word as if it were the last.

For each term w i in each document d , there are two phases corresponding to the M-GPU sampling process (Section 5.3.2 ):
Phase 1 (Steps 1-4 in M-GPU): calculate the conditional probability of sampling a topic for term w i . We enumer-ate each topic k and calculate its corresponding probability, which is decided by three sub-steps: a) Sample a must-link m i that contains w i , which is likely b) After getting the sampled must-link m i , we create a set c) The conditional probability of assigning topic k to term
Phase 2 (Step 5 in M-GPU): this sampling phase deals with cannot-links. There are two sub-steps: a) For every cannot-term (say w c ) of w i , we sample one b) For each drawn instance q c from Phase 2 a), resample a
This section evaluates the proposed AMC model and com-pares it with five state-of-the-art baseline models: LDA [4 ]: The classic unsupervised topic model.

DF-LDA [1]: A knowledge-based topic model that can use both must-links and cannot-links, but it assumes all the knowledge is correct.

MC-LDA [10 ]: A knowledge-based topic model that also use both the must-link and the cannot-link knowledge. It assumes that all knowledge is correct as well.

GK-LDA [9]: A knowledge-based topic model that uses the ratio of word probabilities under each topic to reduce the effect of wrong knowledge. However, it can only use the must-link type of knowledge.

LTM [7 ]: A lifelong learning topic model that learns only the must-link type of knowledge automatically. It outper-formed [8 ].

Note that although DF-LDA, MC-LDA and GK-LDA can take prior knowledge from the user, they cannot mine any prior knowledge, which make them not directly comparable with the proposed AMC model. We have to feed them the knowledge produced using the proposed knowledge mining algorithm. This enables us to assess the knowledge handling capability of each model. LTM uses its own way to mine and incorporate must-links.
Datasets . We have created two large datasets for our ex-periments. The first dataset contains reviews from 50 types of electronic products or domains (given in the first row of Table 1). The second dataset contains reviews from 50 mixed types of non-electronic products or domains (given in the second row of Table 1). Each domain has 1000 reviews. Using the first dataset, we want to show the performance of AMC when there is a reasonably large topic overlapping. Us-ing the second dataset, we want to show AMC X  X  performance when there is not much topic overlapping. We followed [9 ] to pre-process the dataset. The datasets are publicly available at the authors X  websites.

Parameter Setting . All models were trained using 2000 iterations with an initial burn-in of 200 iterations. The pa-rameters of all topic models are set to  X  = 1,  X  = 0 . 1, K = 15 (#Topics). The other parameters for the baselines were set as suggested in their original papers. For parame-ters of AMC, we estimated its parameters using a develop-ment set from the domain, Calculator, which was not used in the evaluation. The minimum item support count (MIS) for each term is set to Max (4, 35% of its actual support count in the data) and the support difference is 8% [ 18 ]. The sup-port ratio threshold (  X  c ) and support threshold (  X  diff cannot-link mining is 80% and 10 respectively. The overlap ratio threshold  X  overlap for forming a must-link graph edge is 17%. The parameter  X  in Equation 5 is set to 0 . 5, which determines the extent of promotion of words in must-links using the M-GPU model.
This sub-section evaluates the topics generated by each model based on the Topic Coherence measure in [24 ]. Tra-ditionally, topic models are evaluated using perplexity. How-ever, as shown in [ 6], perplexity does not reflect the semantic coherence of individual topics. It can sometimes be contrary to human judgments. The Topic Coherence measure [24 ] was proposed as a better alternative for assessing topic quality. It was shown in [24 ] that Topic Coherence correlates well with human expert labeling. A higher Topic Coherence in-dicates a higher quality of topics. Figure 1: Average Topic Coherence of each model.

In this and the next two sub-sections, we experiment with the 50 Electronics domains, which have a large amount of topic overlapping. We treat each domain as a test set ( D while the knowledge is mined from the rest 49 domains. Since our main aim is to improve topic modeling with small datasets, each test set consists of 100 reviews randomly sam-pled from the 1000 reviews of the domain. We extract knowl-edge from topics generated from the full data (1000 reviews) of all other 49 domains. Since we have 50 domains, we have 50 small test sets. Figure 1 shows the average Topic Co-herence value of each model over the 50 test sets. From Figure 1, we can observe the following: 1. AMC performs the best with the highest Topic Coher-ence value. In the Figure,  X  X MC X  refers to the AMC model with both must-links and cannot-links and  X  X MC-
M X  refers to the AMC model with must-links only. We can see that AMC-M is already better than all baseline models, showing the effectiveness of must-links. AMC is much better than AMC-M which demonstrates that cannot-links are very helpful. These results show that AMC finds higher quality topics than the baselines. Note that in our experiments, we found DF-LDA and
MC-LDA cannot deal with a large number of cannot-links. We have more than 400 automatically mined cannot-links on average for each test set. For DF-LDA, the num-ber of maximum cliques grows exponentially with the number of cannot-links. The program thus crashed on our data. This issue was also noted in [ 36]. For MC-
LDA, it increases the number of topics whenever there is not a good topic to put a cannot-link term in. This results in a large number of topics (more than 50), which are unreasonable and give very poor results. Thus, for both DF-LDA and MC-LDA, we can only show their re-sults with must-links, 2. LTM is better than LDA while clearly worse than AMC.
The additional information from the cannot-links is shown to help produce much more coherent topics. GK-LDA is slightly better than LDA. The wrong knowledge handling method in GK-LDA can cope with some wrong knowl-edge, but not as effective as AMC. 3. We also notice that both DF-LDA and MC-LDA are worse than LDA. This is because they assume the knowl-edge to be correct and lack the necessary mechanism to deal with wrong knowledge. Also, for MC-LDA, it as-sumes each must-link (or must-set in [10 ]) represents a distinct sense or meaning. Thus, it assigns only one must-link to each word and ignores the rest. Then most must-links are not used. This explains also why MC-LDA is worse than DF-LDA.

Iterative improvement (lines 2-5 in Algorithm 1): We found that accumulating cannot-links iteratively is beneficial to AMC. The Topic Coherence value increases slightly from r = 1 to 3 and stabilizes at r = 3 (Algorithm 1). Figure 1 shows the AMC X  X  result for r = 3.

Comparing with LTM using 1000 reviews : To fur-ther compare with LTM, we also conducted experiments in the same setting as [ 7], i.e., each test document collection contains also 1,000 reviews (not 100 as in Figure 1). AMC still improves LTM by 47 points in Topic Coherence, show-ing that AMC can also produce more coherent topics with a large number of test documents.

In summary, we can say that the proposed AMC model generates more coherent topics than all baseline models. Even though DF-LDA, GK-LDA and MC-LDA used our method for knowledge mining, without an effective wrong knowledge handling method, they gave poorer results. The improvements of AMC over all baselines are significant ( p &lt; 0 . 0001) based on paired t-tests.
Here we want to evaluate the topics based on human judg-ment. Two human judges who are familiar with Amazon products and reviews were asked to label the generated top-ics. Since we have a large number of domains (50), we se-lected 10 domains for labeling. The selection was based on the knowledge of the products of the two human judges. Without enough knowledge, labeling will not be reliable. We labeled the topics generated by AMC, LTM and LDA. LDA is the basic knowledge-free topic model and LTM is our ear-lier lifelong learning model that achieves the highest Topic Coherence among the baselines in Figure 1. For labeling, we followed the instructions in [24 ].

Topic Labeling . We first asked the judges to label each topic as coherent or incoherent . The models that generated the topics for labeling were obscure to the judges. In general, a topic was labeled as coherent if its topical words/terms are semantically coherent and together represent a semantic concept; otherwise incoherent .

Word Labeling . The topics that were labeled as coher-ent by both judges were used for word labeling. Each topical word was labeled as correct if it was coherently related to the concept represented by the topic (identified in the topic labeling step); otherwise incorrect .

The Cohen X  X  Kappa agreement scores for topic labeling and word labeling are 0 . 873 and 0 . 860 respectively.
Evaluation Measures . Since topics are rankings of words based on their probabilities, without knowing the exact num-Figure 2: Top &amp; Middle: Topical words Precision @ 5 &amp; Precision @ 10 of coherent topics of each model re-spectively; Bottom: number of coherent (#Coher-ent) topics found by each model. The bars from left to right in each group are for AMC, LTM, and LDA.
 Table 2: Example topics of AMC, LTM and LDA from the Camera domain. Errors are italicized and marked in red. ber of correct topical words/terms, a natural way to evaluate these rankings is to use Precision @ n (or p @ n ) which was also used by other researchers, e.g., [ 9, 37 ], where n is a rank position. Apart from p @ n , we also report the number of coherent topics found by each model.

Results . Figure 2 gives the average Precision @5 (top chart) and Precision @10 (middle chart) of topical words of only coherent topics (incoherent topics are not considered) for each model in each domain. It is clear that AMC achieves the highest p @5 and p @10 values for all 10 domains. LTM is also better than LDA in general but clearly inferior to AMC. This is consistent with the Topic Coherence results in Section 6.2 . LDA X  X  results are very poor without a large amount of data. On average, for p @5 and p @10, AMC im-proves LTM by 8% and 14%, and LDA by 33% and 25% respectively. Significance testing using paired t-tests shows that the improvements of AMC are significant over LTM ( p &lt; 0 . 0002) and LDA ( p &lt; 0 . 0001) on p @5 and p @10.
The bottom chart of Figure 2 shows that AMC also dis-covers many more coherent topics than LTM and LDA. On average, AMC discovers 2 . 4 more coherent topics than LTM and 4 . 7 more coherent topics than LDA over the 10 domains. These results are remarkable. In many domains, LDA only finds 2-4 coherent topics and never more than 5 (out of 15), which again shows that with a small number of documents (reviews), LDA X  X  results are very poor. This section shows some example topics produced by AMC, LTM, and LDA in the Camera domain to give a flavor of the kind of improvements made by AMC. Each topic is shown Figure 3: Average Topic Coherence of AMC com-pared to LDA in different settings (see Section 6.5 ). ALL means Electronics (E) + Non-Electronics (NE) and LDA is equivalent to no knowledge. with its top 10 terms. Errors are italicized and marked in red. From Table 2, we can see that AMC discovers many more correct and meaningful topical terms at the top than the baselines. Note that for AMC X  X  topics that were not discovered by the baseline models, we tried to find the best possible matches from the topics of the baseline models. The topic we show for LDA under  X  X rice X  is the only one that contains a  X  X rice X  related word. Here, the term price is mixed with other terms related to the topic  X  X icture Qual-ity X . From the table, we can clearly see that AMC discovers more coherent topics than LTM and LDA. In fact, the co-herent topics of AMC are all better than their corresponding topics of LTM and LDA.
The above experiments focused on 50 Electronics domains, which have a great deal of topic overlapping. Now we also want to see how AMC performs when the test domain does not have a lot of topic overlapping with the past/prior do-mains. We use two test data settings: the test set is from (1) an Electronics domain or (2) an non-Electronics domain. For each test set setting, we mine knowledge from topics of (a) 50 Electronics domains (E), (b) 50 non-Electronics domains (NE), and (c) all 100 domains (ALL). For each test set, we use both 100 and 1000 reviews. Figure 3 shows the perfor-mance of AMC in each of these settings compared to LDA in terms of Topic Coherence. We can clearly see that AMC performs the best with the knowledge mined from topics of all 100 domains. 50 non-Electronics domains are helpful too because they also share some topics such as price and size . The improvement of AMC in each setting is significant over LDA using paired t-test ( p &lt; 0 . 0001). This clearly shows that AMC is able to leverage the useful knowledge from dif-ferent domains even if the domains are not so related.
This paper proposed an advanced topic model AMC that is able to perform lifelong learning. For such learning, it mines prior knowledge from the results of past modeling and uses the knowledge to help future modeling. Our sys-tem mines two forms of prior knowledge, i.e., must-links and cannot-links, automatically from topics generated from a large number of prior document collections (the big data). The system also identifies some issues with the automatically mined knowledge. The proposed model AMC not only can exploit the learned knowledge but also can deal with the issues of the mined knowledge to generate more accurate topics. Experimental results using review collections from 100 domains showed that the proposed AMC model outper-forms existing state-of-the-art models significantly. In our future work, we plan to study other aspects of lifelong learn-ing in the topic modeling context, e.g., how to maintain the prior topics and how to incrementally update the must-links knowledge when new topics are added to the prior topic set. This work was supported in part by a grant from National Science Foundation (NSF) under grant no. IIS-1111092. [1] D. Andrzejewski, X. Zhu, and M. Craven. Incorporat-[2] D. Andrzejewski, X. Zhu, M. Craven, and B. Recht. A [3] D. M. Blei and J. D. McAuliffe. Supervised Topic Mod-[4] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent Dirich-[5] S. R. K. Branavan, H. Chen, J. Eisenstein, and [6] J. Chang, J. Boyd-Graber, W. Chong, S. Gerrish, and [7] Z. Chen and B. Liu. Topic Modeling using Topics from [8] Z. Chen, A. Mukherjee, and B. Liu. Aspect Extraction [9] Z. Chen, A. Mukherjee, B. Liu, M. Hsu, M. Castellanos, [10] Z. Chen, A. Mukherjee, B. Liu, M. Hsu, M. Castellanos, [11] G. Heinrich. A Generic Approach to Topic Models. In [12] T. Hofmann. Probabilistic Latent Semantic Analysis. [13] M. Hu and B. Liu. Mining and Summarizing Customer [14] Y. Hu, J. Boyd-Graber, and B. Satinoff. Interactive [15] J. Jagarlamudi, H. D. III, and R. Udupa. Incorporating [16] Y. Jo and A. H. Oh. Aspect and sentiment unification [17] J.-h. Kang, J. Ma, and Y. Liu. Transfer Topic Model-[18] B. Liu. Web data mining . Springer, 2007. [19] B. Liu. Sentiment Analysis and Opinion Mining . Mor-[20] B. Liu, W. Hsu, and Y. Ma. Mining association rules [21] Y. Lu and C. Zhai. Opinion integration through semi-[22] H. Mahmoud. Polya Urn Models . Chapman &amp; [23] Q. Mei, X. Ling, M. Wondra, H. Su, and C. Zhai. Topic [24] D. Mimno, H. M. Wallach, E. Talley, M. Leenders, and [25] S. Moghaddam and M. Ester. The FLDA Model for [26] A. Mukherjee and B. Liu. Aspect Extraction through [27] S. J. Pan and Q. Yang. A Survey on Transfer Learn-[28] J. Petterson, A. Smola, T. Caetano, W. Buntine, and [29] D. Ramage, D. Hall, R. Nallapati, and C. D. Manning. [30] D. L. Silver, Q. Yang, and L. Li. Lifelong Machine [31] S. Thrun. Lifelong Learning Algorithms. In S. Thrun [32] I. Titov and R. McDonald. Modeling online reviews [33] H. Wang, Y. Lu, and C. Zhai. Latent aspect rating anal-[34] G. Xue, W. Dai, Q. Yang, and Y. Yu. Topic-bridged [35] S. H. Yang, S. P. Crain, and H. Zha. Bridging the [36] Z. Zhai, B. Liu, H. Xu, and P. Jia. Constrained LDA [37] W. X. Zhao, J. Jiang, H. Yan, and X. Li. Jointly Model-[38] G. K. Zipf. Selective Studies and the Principle of Rela-
