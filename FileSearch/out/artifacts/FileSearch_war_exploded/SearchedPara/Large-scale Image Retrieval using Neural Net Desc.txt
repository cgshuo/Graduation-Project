 H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing; H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval metric indexing; deep convolutional neural network; content-based image retrieval; k-NN search
One of current big challenges in computer science is devel-opment of data management and retrieval techniques that would keep pace with the evolution of contemporary data and with the growing expectations on data processing. Vari-ous digital images became a common part of both public and enterprise data collections and there is a natural requirement that the retrieval should consider more the actual visual con-tent of the image data. In our demonstration, we aim at the task of retrieving images that are visually and semantically similar to a given example image; the system should be able to online evaluate k nearest neighbor queries within a collec-tion containing tens of millions of images. The applicability of such a system would be, for instance, on stock photog-raphy sites, in e-shops searching in product photos, or in collections from a constrained Web image search.

A successful content-based image retrieval system must stand on two pillars: effective image-processing technique to achieve high-quality retrieval, and efficient search tech-niques to make the system work in real time and on a large scale. Our demonstration uses the cutting edge approach of deep convolutional neural networks [4] to obtain power-ful visual features that carry a certain semantic footprint of the image content. These descriptors compared by a dis-tance function seem to very well correspond to the human perception of general visual similarity. Our main contribu-tion is the search engine that can organize large volumes of these complex descriptors so that the similarity queries can be evaluated efficiently. The engine exploits our recent
Specifically, we use the DeCAF 7 feature produced by the last hidden layer of the neural network model provided by Caffe 2 [3], which has been trained by the procedure de-scribed in the original paper [4]. We have extracted these features from a collection of 20 million images (see Sec-tion 2.3 for details). The extraction process consists of image normalization to 256  X  256 pixels, generation of ten overlap-ping patches of 224  X  224, and a forward feed of these patches through the network [4]. The resulting DeCAF 7 feature of an image is the average of the last-but-one layer outputs for the ten image patches. Such extraction takes about 100 ms on our GPU (30 days for the 20M collection). One DeCAF 7 feature is a 4096-dimensional vector taking 16 KB on the disk; the whole 20M dataset has thus some 320 GB of un-compressed data. Given a query image, the same extraction procedure generates feature q  X  DeCAF 7 , where DeCAF 7 denotes the 4096-dimensional feature space; this is schemat-ically depicted in the top part of Figure 2.
In our system, we adopt a broad similarity model based on mutual object distances, specifically, D is a domain of data and  X  is a total distance function  X  : D  X  D  X  X  X  R + 0 ; we assume that this space ( D , X  ) satisfies metric postulates of identity , symmetry and triangle inequality [6]. In our specific case, domain D is the 4096-dimensional DeCAF 7 space and  X  is Euclidean distance, as proposed in the image recognition papers [4, 2]. The actual set of 20M features is denoted as X  X  D ; the search is modeled by the nearest neighbors query k -NN( q ) returning the k objects from X with the smallest distances to given q  X  D :  X  ( q,x ), x  X  X .

Our PPP-Codes index uses multiple recursive Voronoi partitioning of the metric space; in this way, each object x  X  X is mapped onto its PPP ( x ) code, which carries infor-mation about location of x in these Voronoi diagrams [5]. A memory index is then created on PPP ( x ) codes,  X  x  X  X ; each object x is identified by its ID. Given a query k -NN( q ), q  X  D , this index determines a set C ( q )  X  X of candidate objects, or rather object IDs. Objects from C ( q ) are read from an ID-object disk store and are refined by evaluation of  X  ( q,x ),  X  x  X  C ( q ) in order to obtain final answer for k -NN( q ); this process is sketched in the lower part of Figure 2.
The result of this search process is an approximation of the precise k -NN( q ) answer. The precision of the answer depends on the size of the candidate set and so does the response time, because the search costs mainly consist of (1) CPU costs of C ( q ) generation, (2) I/O costs of read-ing C ( q ) objects from the disk and (3) CPU costs of C ( q ) refinement. Because we work with complex and bulky fea-tures, the I/O costs are important, even though we compress the features down to 1/3 of their original size. The PPP-Codes index has about 1 GB in memory and it is designed to provide a very accurate candidate set; for instance, refin-ing 5000 candidate objects results in 80% average recall for 10-NN queries with average response time around 400 ms.
Our demonstration uses a unique collection Profiset 3 con-sisting of 20 million high quality images provided for research purposes by a stock photography company [1]. The search engine is running on a commodity machine with 8 GB of http://caffe.berkeleyvision.org http://disa.fi.muni.cz/profiset/
