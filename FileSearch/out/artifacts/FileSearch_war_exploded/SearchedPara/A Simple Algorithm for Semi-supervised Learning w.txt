 Ming Ji  X  X  mingji1@illinois.edu Tianbao Yang  X  X  yangtia1@msu.edu Binbin Lin  X  binbinlin@zju.edu.cn Rong Jin  X  rongjin@cse.msu.edu Jiawei Han  X  hanj@illinois.edu Equal contribution Although numerous algorithms have been develope-d for semi-supervised learning ( Zhu ( 2008 ) and ref-erences therein), most of them do not have theoreti-cal guarantee on improving the generalization perfor-mance of supervised learning. A number of theories have been proposed for semi-supervised learning, and most of them are based on one of the two assumption-s: (1) the cluster assumption ( Seeger , 2001 ; Rigollet , 2007 ; Lafferty &amp; Wasserman , 2007 ; Singh et al. , 2008 ; Sinha &amp; Belkin , 2009 ) which assumes that two da-ta points should have the same class label or sim-ilar values if they are connected by a path passing through a high density region; (2) the manifold as-sumption ( Lafferty &amp; Wasserman , 2007 ; Niyogi , 2008 ) which states that the prediction function lives in a low dimensional manifold of the marginal distribution P X . It has been pointed out by several stud-ies ( Lafferty &amp; Wasserman , 2007 ; Nadler et al. , 2009 ) that the manifold assumption by itself is insufficient to reduce the generalization error bound of supervised learning. However, on the other hand, it was found in ( Niyogi , 2008 ) that for certain learning problems, no supervised learner can learn effectively, while a manifold based learner (that knows the man-ifold or learns it from unlabeled examples) can learn well with relatively few labeled examples. Compared to the manifold assumption, theoretical results based on cluster assumption appear to be more encouraging. In the early studies ( Castelli &amp; Cover , 1995 ; 1996 ), the authors show that under the assumption that the marginal distribution P X is a mixture of class conditional distributions, the generalization error will be reduced exponentially in the number of labeled examples if the mixture is identifiable. Rigollet ( 2007 ) defines the cluster assumption in terms of density level sets, and shows a similar exponential convergence rate given a sufficiently large number of unlabeled examples. Furthermore, Singh et al. ( 2008 ) show that the mixture components can be identified if P X is a mixture of a finite number of smooth density functions and the separation/overlap between different mixture components is significantly large. Despite the encouraging results, one major problem of the cluster assumption is that it is difficult to be verified given a limited number of labeled exam-ples. In addition, the learning algorithms suggested in ( Rigollet , 2007 ; Singh et al. , 2008 ; Zhang &amp; Ando , 2005 ) are difficult to implement efficiently even if the cluster assumption holds, making them unpractical for real-world problems.
 In this work, we aim to develop a simple algorithm for semi-supervised learning that on one hand is easy to implement, and on the other hand is guaranteed to improve the generalization performance of super-vised learning under appropriate assumptions. The main idea of the proposed algorithm is to estimate the top eigenfunctions of the integral operator from the both labeled and unlabeled examples, and learn from the labeled examples the best prediction function in the subspace spanned by the estimated eigenfunction-s. Unlike the previous studies of exploring eigenfunc-tions for semi-supervised learning ( Fergus et al. , 2009 ; Sinha &amp; Belkin , 2009 ), we show that under appro-priate assumptions, the proposed algorithm achieves a better generalization error bound than supervised learning algorithms.
 To derive the generalization error bound, we make a different set of assumptions from previous stud-ies. First, we assume a skewed eigenvalue distribu-tion and bounded eigenfunctions of the integral oper-ator. The assumption of skewed eigenvalue distribu-tions has been verified and used in multiple studies of kernel learning ( Koltchinskii , 2011 ; Steinwart et al. , 2006 ; Minh , 2010 ; Zhang &amp; Ando , 2005 ), while the as-sumption of bounded eigenvectors was mostly found in the study of compressive sensing ( Cand`es &amp; Tao , 2006 ). Second, we assume that a sufficient num-ber of labeled examples are available, which is also used by the other analysis of semi-supervised learn-ing ( Rigollet , 2007 ). It is the combination of these assumptions that allow us to derive better generaliza-tion error bound for semi-supervised learning. The rest of the paper is arranged as follows. Section 2 presents the proposed algorithm and verifies its effec-tiveness by an empirical study. Section 3 shows the improved generalization error bound for the proposed semi-supervised learning, and Section 4 outlines the proofs. Section 5 concludes with future work. Let X be a compact domain or a manifold in the Eu-clidean space R d . Let D = { x i , i = 1 , . . . , N | x be a collection of training examples. We randomly s-elect n examples from D for labeling. Without loss of generality, we assume that the first n examples are labeled by y l = ( y 1 , . . . , y n )  X   X  R n . We denote by y = ( y 1 , . . . , y N )  X   X  R N the true labels for all the examples in D . In this study, we assume y = f ( x ) is decided by an unknown deterministic function f ( x ). Our goal is to learn an accurate prediction function by Algorithm 1 A Simple Algorithm for Semi-supervised Learning 1: Input 2: Compute ( 3: Compute the prediction b g ( x ) in ( 5 ), where  X   X  = 4: Output prediction function b g (  X  ) exploiting both labeled and unlabeled examples. Be-low we first present our algorithm and then verify its empirical performance by comparing to the state-of-the-art algorithms for supervised and semi-supervised learning. 2.1. A Simple algorithm for Semi-Supervised Let  X  (  X  ,  X  ) : X  X  X  X  R be a Mercer kernel, and let H  X  be a Reproducing Kernel Hilbert space (RKHS) of functions X  X  R endowed with kernel  X  (  X  ,  X  ). We assume that  X  is a bounded function, i.e., |  X  ( x , x ) |  X  1 ,  X  x  X  X . Similar to most semi-supervised learning algorithms, in order to effectively exploit the unlabeled data, we need to relate the prediction function f ( x ) to the unlabeled examples (or the marginal distribution P
X ). To this end, we assume there exists an accurate prediction function g ( x )  X  H  X  with  X  g  X  H  X  R . More specifically, we define Our basic assumption ( A0 ) is that the regression error  X  2  X  R 2 is small, and the maximum regression error of g ( x ) for any x  X  X is also small, i.e., To present our algorithm, we define an integral oper-ator over the examples in D : eigenfunctions and eigenvalues of b L N ranked in the de-scending order of eigenvalues, where  X  b  X  i (  X  ) , b  X  j  X  ( i, j ) for any 1  X  i, j  X  N . According to ( Guo &amp; Zhou , 2011 ), the prediction function g ( x ) can be well approximated by a function in the subspace spanned by the top eigenfunctions of b L N . Hence, we propose to learn a target prediction function b g ( x ) as a linear combination of the first s eigenfunctions, i.e., where s is a parameter that needs to be determined through a simple regression by minimizing the squared error of the labeled examples as shown in ( 1 ). Algo-rithm 1 shows the basic steps of the proposed algorith-m.
 Implementation In step 2 of Algorithm 1 , we need to compute the eigenvalues and eigenfunctions of b L N , which is given as follows ( Smale &amp; Zhou , 2009 ). Let K = [  X  ( x i , x j )] N  X  N be the kernel matrix for the ex-vectors and eigenvalues of K . Then, the eigenvalues and eigenfunctions of b L N are given by where v i j is the j -th element of vector v i . Finally, in step 3 of Algorithm 1 , we need to compute the optimal coefficient  X   X  , which, according to ( Bishop , 2006 ), is given by where D = diag(  X  1 , . . . ,  X  s ), K B = [  X  ( x i , x j cludes the kernel similarity between all the examples in
D and labeled examples, and V = ( v 1 , . . . , v s ). 2.2. Empirical study Three real-world data sets, i.e., insurance, wine, and temperature 1 , are used in our empirical study. The s-tatistics of these datasets are given in Table 1 . The first two datasets are from the UC Irvine Machine Learning Repository ( Frank &amp; Asuncion , 2010 ), while the task of the last dataset is to predict the tempera-ture based on the coordinates (latitude, longitude) on the earth surface. All three datasets are designed for regression tasks with real-valued outputs. We choose these three datasets because they fit in with our as-sumptions that will be elaborated in section 3.2 . We randomly choose 90% of the data for training, and use the rest 10% for testing. We randomly se-lect 2% , 3% , . . . , 9% of the entire dataset as labeled examples. We evaluate the performance by measur-ing the regression error of the testing data. Each ex-periment is repeated ten times and the regression er-rors averaged over the ten trials are reported. Two supervised regression algorithms, i.e., Kernel Ridge Regression ( KRR ) ( Saunders et al. , 1998 ) and Sup-port Vector Regression ( SVR ) ( Drucker et al. , 1996 ), and a state-of-the-art algorithm for semi-supervised regression, i.e., Laplacian Regularized Least Squares ( LapRLS ) ( Belkin et al. , 2006 ), are used as the base-lines. We did not include other baseline algorithm-s for semi-supervised learning because Laplacian reg-ularization yields the state-of-the-art performance of semi-supervised learning. More importantly, our goal is to verify that the proposed algorithm can effectively improve the generalization performance of supervised learning. We refer to the proposed algorithm as Sim-ple Semi-Supervised Learning, or SSSL for short. A RBF kernel function is used for all algorithms, and all the parameters are chosen by cross validation. Tables 2 -4 show the regression errors for the three datasets, respectively. First, as we expected, the per-formance of all learning algorithms improves as the number of labeled examples increases. It is also not surprising to see that the two semi-supervised learn-ing algorithms perform better than the two supervised learning algorithms. Second, the proposed algorith-m (SSSL) outperforms the baseline semi-supervised learning algorithm for almost all the cases, indicating that it is effective for semi-supervised learning. Note that SVR does not perform well on the temperature dataset since this dataset has a perfect manifold struc-ture (the earth surface is a sphere), and SVR fails to capture the manifold structure when the percentage of labeled data is very small.
 To analyze the generalization performance of the pro-posed algorithm, we first consider the simple scenario where we have access to an infinite number of unla-beled examples (i.e., the marginal distribution P X ). We then present the generalization error bound for a finite number of unlabeled examples. Detailed analysis can be found in Section 4 . 3.1. Generalization error for an in nite Given the marginal distribution P X , we define an in-s and eigenvalues of L ranked in the descending order of the eigenvalues, where the eigenfunction-s are normalized according to the distribution, i.e.,  X   X  X   X  i ( x )  X  j ( x ) dP X =  X  ij . We note that in ( 4 ), is the empirical version of L , and  X  L  X  b L N approaches to zero as the number of examples goes to infinity, where  X   X   X  HS is Hilbert Schmidt norm of a linear operator ( Smale &amp; Zhou , 2009 ).
 In order to achieve a better generalization error bound for the proposed semi-supervised learning algorithm, we make the following assumptions about eigenvalues and eigenfunctions:  X  A1 Skewed eigenvalue distribution . Similar  X  A2 Bounded eigenfunctions . There exists a small  X  A3 Sufficient number of labeled examples . We re-Remark 1 Assumption (A1) ensures that the tar-get function can be approximated, with a small er-ror, by a function in the subspace spanned by the top eigenfunctions of L . This is the foundation behind Al-gorithm 1 .
 Remark 2 Assumptions (A2) and (A3) are intro-duced to ensure that all the coefficients {  X   X  i } s i =1 ( 5 ) can be estimated accurately. More specifically, as-sumption (A3) makes it possible to obtain an accurate estimation of the coefficients {  X   X  i } s i =1 . Assumption A2 ensures that labeled examples are associated with all the top eigenfunctions, and therefore a reliable esti-mation can be obtained for all the coefficients through the regression analysis. Intuitively, assumption (A2) are not zeros, which is due to E[  X  i ( x )] is fixed and max x |  X  i ( x ) | is small, otherwise we cannot obtain an accurate estimation of  X   X  . Actually, it is notable that we only need to bound the first s eigenfunctions in M ( s ) = max x sition 2 . From another point of view, if we bound max x |  X  i ( x ) |  X   X   X  i  X  H = 1 / 2009 , pg. 9), then if the first s eigenvalues are large, we can expect the maximum value of the first s eigen-funcitons is small. An example satisfying this property is the Sobolev space of functions defined on the domain [0 , 1] d with uniform distribution (see ( Koltchinskii , 2011 , pg. 16)).
 The following theorem shows the generalization error of Algorithm 1 for an infinite number of unlabeled ex-amples provided that assumptions (A0  X  A3) hold. Theorem 1. Assume ( A0  X  A3 ) hold. Set s = ( aR/ X  ) 2 / ( p  X  1) . Then, with a probability 1  X  2 N  X  have where b g (  X  ) is the function learned by Algorithm 1 . Remark 3 According to ( 2 ),  X  2 is the optimal re-gression error that can be achieved by a prediction function in H  X  . Hence, Theorem 1 shows that given an infinite number of unlabeled examples, the predic-tion function learned by Algorithm 1 achieves almost the optimal performance (up to a constant).
 Remark 4 It is also useful to compare the bound in Theorem 1 to the generalization error bound of su-pervised learning. According to ( Tsybakov , 2008 ), the minimax optimal error if supervised regression (i.e., the best possible regression error of the worst possible distribution) is bounded by  X ( n  X  p/ ( p +1) ) 2 . So if we take the value in assumption ( A3 ) for n  X   X   X  4 / ( p  X  then the generalization error for supervised regression is  X (  X  4 p/ ( p 2  X  1) ). Compared to our bound (i.e., O (  X  when p &gt; 1 + that the generalization error bound of Algorithm 1 is better than that for supervised regression. 3.2. Generalization error for a nite number of We now consider the scenario where only a finite num-ber (i.e., N ) of unlabeled examples are available. The key challenge arising from the finite sample analysis is that we do not have access to the eigenfunctions and eigenvalues of L . Instead, we have to approximate the eigenfunctions and eigenvalues of L by its empiri-cal counterpart b L N . These approximation errors make the analysis more involved. To ensure that the approx-imation does not significantly increase the regression error, we make the following assumptions:  X  B1 Skewed eigenvalue distribution of b L  X  B2 Bounded eigenfunctions . There exists a small  X  B3 Sufficient number of labeled examples . We re- X  B4 Sufficiently large eigengap . Let r Remark 5 Assumptions ( B1  X  B3 ) are the  X  X mpiri-cal X  versions of assumptions ( A1  X  A3 ). Note that un-like assumption ( A2 ) where |  X  i ( x ) | is assumed to be bounded, in assumption ( B2 ), we assume | b  X  i ( x ) / to be bounded. This is because  X  i ( x ) is normalized malized with respect to the functional norm since the marginal distribution P X is unknown. The most im-portant feature of the finite sample analysis is that we introduce a new assumption ( B4 ), where the number of unlabeled examples N plays an important role to bound the eigengap. This additional assumption is designed to address the approximation error in replac-ing the eigenfunctions of L with the eigenfunctions of b L Theorem 2. Assume ( A0 ) and ( B1  X  B3 ) hold. Set
N  X  max Then, with a probability 1  X  4 N  X  3 , we have As indicated by Theorem 2 , the prediction function learned by Algorithm 1 achieves almost the optimal regression error (up to a constant) provided that al-l the assumptions hold and the number of unlabeled examples is sufficiently large.
 Finally, to partially verify the assumptions, we exam-ine the eigenvalue distributions for the chosen datasets (described in Section 2.2 ), as shown in Figure 1 . Due to space limitation, we put the figure for the temper-ature dataset in the supplementary material. We also show in Figure 1 the curves of a 2 k  X  p with p = 2 . 1. It is very clear that the eigenvalues follow a skewed distribution with the power index p &gt; 2. We present the full analysis for the case of infinite number of unlabeled examples, and only sketch the analysis for finite number of unlabeled examples due to lack of space. More detailed analysis can be found in the supplementary materials. 4.1. Analysis for an in nite number of When we have an infinite number of unlabeled ex-amples, the learned prediction function is given by b g ( x ) = obtained by solving the following optimization prob-lem:  X   X  = arg min Using the eigenfunctions of L , we write g ( x ), the op-timal prediction function defined in ( 3 ), as g ( x ) =  X  j  X  j  X  j ( x ). We define g s ( x ), the projection of g ( x ) into the subspace spanned by the top s eigenfunctions, as Using g s ( x ), we decompose the generalization error of b g ( x ) into two parts, i.e.,
E x [( b g ( x )  X  f ( x )) 2 ] The following lemmas bound the two terms on the R.H.S. of the above inequality, separately.
 Lemma 1. Under assumption (A1) , for any s  X  1 , we have Lemma 2. Under assumptions (A2  X  A3) and s = ( aR/ X  ) 2 / ( p  X  1) , with a probability at least 1  X  2 N have where  X  2 = 2 As indicated by Lemma 1 , assumption ( A1 ) guaran-tees an additional small regression error when con-straining the solution to the subspace spanned by the top eigenfunctions of L . As indicated by Lemma 2 , assumptions ( A2  X  A3 ) ensure that g s ( x ), the projec-tion of g ( x ) into the subspace spanned by the top eigenfunctions, can be accurately estimated from the labeled examples. It is easy to see that Theorem 1 immediately follows Lemma 1 and Lemma 2 by not-ing that  X  2 s = O (  X  2 ) and  X  2 = O (  X  2 ) when we set s = ( Ra/ X  ) 2 / ( p  X  1) . Below, we show how to prove both lemmas.
 Proof of Lemma 1 We first show that is bounded. Since  X  g  X  H  X  R , we have and therefore Then we bound the regression error of g s ( x ) as follows: Proof of Lemma 2 The proof of Lemma 2 is signif-icantly more involved. We first introduce some nota-representation of x i derived from the first s eigen-functions. Let Z = ( z 1 , . . . , z n ) include the rep-resentations of all labeled examples, and let y l = ( f ( x 1 ) , . . . , f ( x n ))  X  . Using Z , we rewrite as The following proposition bounds E x [( b g ( x )  X  g s ( x )) using the minimum eigenvalue of ZZ  X  .
 Proposition 1. Assume ZZ  X  is nonsingular. With a probability at least 1  X  N  X  3 , we have The following proposition bounds the minimum eigen-value of ZZ  X  .
 Proposition 2. With a probability at least 1  X  N  X  3 , where N &gt; 0 is a large number, we have where M ( s ) = max x  X  X  The proof for Proposition 1 and 2 can be found in the supplementary materials. Now we are ready to prove Lemma 2 .
 According to assumptions A2  X  A3 and Proposition 2 , we have, with a probability at least 1  X  N  X  3 Combining the above inequality with Proposition 1 , we have, with a probability at least 1  X  2 N  X  3 , 4.2. Analysis for a nite number of unlabeled Define  X   X  the optimal solution that minimizes the re-gression error using the eigenfunctions of b L N , i.e., We further define b  X   X  i =  X   X  i b g ( x ) learned in the presence of a finite number of un-labeled examples as b g ( x ) = troduce h s ( x ) as follows ilar to the previous analysis, we bound the generaliza-tion error of b g ( x ) by
E x [( b g ( x )  X  f ( x )) 2 ] We follow the same path as in the infinite case and present two lemmas to bound the two terms on R.H.S. of the above inequality.
 Lemma 3. Under assumptions B1, B3 and N  X  144 s 2 p  X  2 [ln N ] 2 a  X  2 , with a probability at least 1 2 N  X  3 , we have Lemma 4. Under assumptions B1  X  B3 , with a prob-ability at least 1  X  4 N  X  3 , we have where b  X  2 = 2 The proof for Lemma 4 and Lemma 3 can be found in the supplementary materials.
 Proof of Theorem 2 . Using the condition N  X  144 R 2 [ln N ] 2 / [ r 2 s  X  2 ], we have 36 R 2  X  2 N /r When we set s = ( Ra/ X  ) 2 / ( p  X  1) , we have  X  2 s = O (  X  b  X  s = O (  X  ma 4 , we have, with a probability 1  X  4 N  X  3 , In this work, we present a very simple algorithm for semi-supervised learning. Our analysis shows that un-der appropriate assumptions about the integral oper-ator, the proposed algorithm achieves a better gener-alization error than a supervised learning algorithm. In the future, we plan to further improve the scala-bility of the proposed algorithm by exploring different approaches (e.g., the Nystr  X om method) for efficient-ly estimating eigenfunctions from a large number of unlabeled examples.
 The work was supported in part by the U.S. Army Re-search Laboratory under Cooperative Agreement No. W911NF-09-2-0053 (NS-CTA), NSF IIS-0905215, NS-F IIS-0643494, U.S. Air Force Office of Scientific Re-search MURI award FA9550-08-1-0265, and Office of Navy Research (ONR Award N00014-09-1-0663 and N00014-12-1-0431). The views and conclusions con-tained in this paper are those of the authors and should not be interpreted as representing any funding agen-cies.

