 AT&amp;T Labs  X  Research, 180 Park Avenue, Florham Park, NJ 07932 tribution of a given animal or plant species. This is a crit-ical problem in conservation biology: to save a threatened species, one first needs to know where the species prefers to live, and what its requirements are for survival, i.e., its ecological niche (Hutchinson, 1957).
 of a list of georeferenced occurrence localities, i.e., a set of geographic coordinates where the species has been ob-served. In addition, there is data on a number of envi-ronmental variables, such as average temperature, aver-age rainfall, elevation, etc., which have been measured or estimated across a geographic region of interest. The goal is to predict which areas within the region satisfy the requirements of the species X  ecological niche, and thus form part of the species X  potential distribution (Anderson &amp; Mart  X   X nez-Meyer, 2004). The potential distribution de-scribes where conditions are suitable for survival of the species, and is thus of great importance for conservation. It can also be used to estimate the species X  realized distri-bution , for example by removing areas where the species is known to be absent because of deforestation or other habi-tat destruction. Although a species X  realized distribution may exhibit some spatial correlation, the potential distri-bution does not, so considering spatial correlation is not necessarily desirable during species distribution modeling. indicating the occurrence of the species. Natural history museum and herbarium collections constitute the richest source of occurrence localities (Ponder et al., 2001; Stock-well &amp; Peterson, 2002). Their collections typically have no information about the failure to observe the species at any given location; in addition, many locations have not been surveyed. In the lingo of machine learning, this means that we have only positive examples and no negative examples from which to learn. Moreover, the number of sightings (training examples) will often be very small by machine-learning standards, say a hundred or less. Thus, the first contribution of this paper is the introduction of a scientifi-cally important problem as a challenging domain for study by the machine learning community.
 maximum-entropy (maxent) techniques which have been so effective in other domains, such as natural language pro-cessing (Berger et al., 1996). Briefly, in maxent, one is given a set of samples from a distribution over some space, as well as a set of features (real-valued functions) on this space. The idea of maxent is to estimate the target distribu-tion by finding the distribution of maximum entropy (i.e., that is closest to uniform) subject to the constraint that the expected value of each feature under this estimated distri-bution matches its empirical average. This turns out to be equivalent, under convex duality, to finding the maximum likelihood Gibbs distribution (i.e., distribution that is expo-nential in a linear combination of the features). For species distribution modeling, the occurrence localities of the spe-cies serve as the sample points, the geographical region of interest is the space on which this distribution is defined, and the features are the environmental variables (or func-tions thereof). See Figure 1 for an example.
 detail. Iterative scaling and its variants (Darroch &amp; Ratcliff, 1972; Della Pietra et al., 1997) are standard algorithms for computing the maximum entropy distribution. We use our own variant which iteratively updates the weights on features sequentially (one by one) rather than in paral-lel (all at once), along the lines of Collins, Schapire and Singer (2002). This sequential approach is analogous to AdaBoost which modifies the weight of a single  X  X eature X  (usually called a base or weak classifier in that context) on each round. As in boosting, this approach allows us to use very large feature spaces.
 to be a problem for generalization since it increases the pos-sibility of overfitting, leading others to use feature selection for maxent (Berger et al., 1996). We instead use a regular-ization approach, introduced in a companion theoretical pa-per (Dud  X   X k et al., 2004), which allows one to prove bounds on the performance of maxent using finite data, even when the number of features is very large or even uncountably infinite. Here we investigate in detail the practical efficacy of the technique for species distribution modeling. We also describe a numerical acceleration method that speeds up learning.
 ments we conducted comparing maxent to a widely used existing distribution modeling algorithm; results of the ex-periments are described in Section 4. Quite a number of ap-proaches have been suggested for species distribution mod-eling including neural nets, genetic algorithms, generalized linear models, generalized additive models, bioclimatic en-velopes and more; see Elith (2002) for a comparison. From these, we selected the Genetic Algorithm for Ruleset Pre-diction (GARP) (Stockwell &amp; Noble, 1992; Stockwell &amp; Peters, 1999), because it has seen widespread recent use to study diverse topics such as global warming (Thomas et al., 2004), infectious diseases (Peterson &amp; Shaw, 2003) and invasive species (Peterson &amp; Robins, 2003); many fur-ther applications are cited in these references. GARP was also selected because it is one of the few methods available that does not require absence data (negative examples). rived from the North American Breeding Bird Survey (BBS) (Sauer et al., 2001), an extensive dataset consisting of thousands of occurrence localities for North American birds and used previously for species distribution model-ing, in particular for evaluating GARP (Peterson, 2001). The comparison suggests that maxent methods hold great promise for species distribution modeling, often achiev-ing substantially superior performance in controlled ex-periments relative to GARP. In addition to comparisons with GARP, we performed experiments testing: (1) the per-formance of maxent as a function of the number of sam-ple points available, so as to determine the all important question of how much data is enough; (2) the effective-ness of regularization to avoid overfitting on small sample sizes; and (3) the effectiveness of our numerical accelera-tion methods.
 allow interpretation to deduce the most important limiting factors for the species. A noted limitation of GARP is the difficulty of interpreting its models (Elith, 2002). We show how the models generated by maxent can be put into a form that is easily understandable and interpretable by humans. species distributions. As explained above, we are given a space X representing some geographic region of interest. Typically, X is a set of discrete grid cells; here we only assume that X is finite. We also are given a set of points x ,... ,x m in X , each representing a locality where the species has been observed and recorded. Finally, we are provided with a set of environmental variables defined on X , such as precipitation, elevation, etc.
 range of the given species. In this paper, we formalize this rather vague goal within a probabilistic framework. Although this will inevitably involve simplifying assump-tions, what we gain will be a language for defining the problem with mathematical precision as well as a sensible approach for applying machine learning.
 the view that the localities x 1 ,... ,x m were selected inde-pendently from X according to some unknown probability distribution  X  , and that our goal is to estimate  X  . At the foundation of our approach is the premise that the distribu-tion  X  (or a thresholded version of it) coincides with the biologists X  concept of the species X  potential distribution. Superficially, this is not unreasonable, although it does ig-nore the fact that some localities are more likely to have been visited than others. The distribution  X  may therefore exhibit sampling bias, and will be weighted towards areas and environmental conditions that have been better sam-pled, for example because they are more accessible. estimation :given x 1 ,... ,x m chosen independently from some unknown distribution  X  , we must construct a distri-bution  X   X  that approximates  X  .
 features f 1 ,... ,f n where f j : X  X  R . These features might consist of the raw environmental variables, or they might be higher level features derived from them (see Sec-tion 3.3). Let f denote the vector of all n features. tation under  X  . Let  X   X  denote the empirical distribution , i.e.,  X   X  ( x )= |{ 1  X  i  X  m : x i = x }| /m . In general,  X   X  may be quite distant, under any reasonable measure, from  X  .On the other hand, for a given function f , we do expect  X   X  [ f ] , the empirical average of f , to be rather close to its true expectation  X  [ f ] . It is natural, therefore, to seek an approx-imation  X   X  under which f j  X  X  expectation is equal (or at least very close) to  X   X  [ f j ] for every f j . There will typically be many distributions satisfying these constraints. The maxi-mum entropy principle suggests that, from among all distri-butions satisfying these constraints, we choose the one of maximum entropy, i.e., the one that is closest to uniform. Here, as usual, the entropy of a distribution p on X is de-fined to be H( p )=  X  x  X  X p ( x )ln p ( x ) .
 tion  X   X  of maximum entropy subject to the condition that  X   X  [ f j ]=  X   X  [ f j ] for all features f j . Alternatively, we can consider all Gibbs distributions of the form a normalizing constant, and  X   X  R n . Then, follow-ing Della Pietra, Della Pietra and Lafferty (1997), it can be proved that the maxent distribution described above is the same as the maximum likelihood Gibbs distribution, i.e., the distribution q that minimizes RE(  X   X  q ) where RE( p q )= x  X  X p ( x )ln( p ( x ) /q ( x )) denotes relative entropy or Kullback-Leibler divergence . Note that the neg-ative log likelihood  X   X  [  X  ln( q )] (also called log loss) only differs from RE(  X   X  q ) by the constant H(  X   X  ) ; we there-fore use the two interchangeably as objective functions. 2.1. A sequential-update algorithm ent distribution, especially iterative scaling and its vari-ants (Darroch &amp; Ratcliff, 1972; Della Pietra et al., 1997) as well as the gradient and second-order descent meth-ods (Malouf, 2002; Salakhutdinov et al., 2003). In this paper, we used a sequential-update algorithm that modifies one weight  X  j at a time, as explored by Collins, Schapire and Singer (2002) in a similar setting. We chose this coordinate-wise descent procedure since it is easily appli-cable when the number of features is very large (or infinite). lows. Assume without loss of generality that each fea-ture f j is bounded in [0 , 1] . On each of a sequence of rounds, we choose the feature f j to update for which RE(  X   X  [ f j ] q [ f j ]) is maximized, where  X  is the current weight vector (and where RE( p q ) , for p, q  X  R , is bi-nary relative entropy). We next update  X  j  X   X  j +  X  where The output distribution  X   X  is the one defined by the com-puted weights, i.e., q . Essentially, this algorithm works by altering one weight  X  j at a time so as to greedily max-imize the likelihood (or an approximation thereof). This procedure is guaranteed to converge to the optimal maxi-mum entropy distribution. The derivation of this algorithm, along with its proof of convergence are given in a compan-ion paper (Dud  X   X k et al., 2004) and are based on techniques explained by Della Pietra, Della Pietra and Lafferty (1997) as well as Collins, Schapire and Singer (2002).
 iteration: evaluate the log loss when  X  j is incremented by 2  X  for i =0 , 1 ,... in turn, and choose the last i before the log loss decreases. This is similar to line search methods described in (Minka, 2001). 2.2. Regularization imum entropy distribution  X   X  for which  X   X  [ f j ]=  X   X  [ f However, we do not expect  X   X  [ f j ] to be equal to  X  [ f only close to it. Therefore, in keeping with our motivation, we can soften these constraints to have the form where  X  j is an estimate of how close  X   X  [ f j ] , being an em-pirical average, must be to its true expectation  X  [ f j ] . Maxi-mizing entropy subject to Eq. (2) turns out to be equivalent to finding the Gibbs distribution  X   X  = q which minimizes In other words, this approach is equivalent to maximizing the likelihood of the sought after Gibbs distribution with (weighted) 1 -regularization. This form of regularization also makes sense because the number of training exam-ples needed to approximate the  X  X est X  Gibbs distribution can be bounded when the 1 -norm of the weight vector  X  is bounded. (See (Dud  X   X k et al., 2004) for details.) In a Bayesian framework, Eq. (3) corresponds to a negative log posterior given a Laplace prior. Other priors studied for maxent are Gaussian (Chen &amp; Rosenfeld, 2000) and expo-nential (Goodman, 2003). Laplace priors have been studied in the context of neural networks by Williams (1995). ple modification of the above algorithm. On each round, a feature f j and value  X  are chosen so as to maximize the change in (an approximation of) the regularized objective function in Eq. (3). This works out to be  X   X   X   X  [ f j ]+ln(1+( e  X   X  1) q [ f j ]) +  X  j ( |  X  j +  X  The maximizing  X  , must be either  X   X  j or Eq. (1) with  X   X  [ f replaced by  X   X  [ f j ]  X   X  j (provided  X  j +  X   X  0 )or  X   X  [ f (provided  X  j +  X   X  0 ). Thus, the best  X  (for a given f j be computed by trying all three possibilities. Once f j and  X  have been selected, we only need update  X  j  X   X  j +  X  . As before, this algorithm can be proved to converge to a solution to the problem described above.
 gle regularization parameter  X  as follows. We expect |  X  [ f j ]  X   X   X  [ f j ] | X   X  [ f j ] / deviation of f j under  X  . We therefore approximated  X  [ f by the sample deviation  X   X  [ f j ] and used  X  j =  X   X   X  [ f 3.1. The Breeding Bird Survey et al., 2001) is a data set with a large amount of high-quality location data. It is good for a first evaluation of maxent for species distribution modeling, as the generous quantities of data allow for detailed experiments and sta-tistical analyses. It has also been used to demonstrate the utility of GARP (Peterson, 2001). Roadside surveys are conducted on standard routes during the peak of the nest-ing season. Each route consists of fifty stops located at 0.5 mile intervals. A three-minute count is conducted at each stop, during which the observer records all birds heard or seen within 0.25 mile of the stop. Data from all fifty stops are combined to obtain the set of species observed on the route. There are 4161 routes within the region covered by the environmental coverages described below. 3.2. Environmental Variables American grid with 0.2 degree square cells, and are all included with the GARP distribution, available at http://www.lifemapper.org/desktopgarp. Some coverages are derived from weather station readings during the period 1961 to 1990 (New et al., 1999). Out of these we use annual precipitation, number of wet days, average daily tempera-ture and temperature range. The remaining coverages are derived from a digital elevation model for North America, and consist of elevation, aspect and slope. Each coverage is defined over a 386  X  286 grid, of which 58,065 points have data for all coverages. 3.3. Experimental Design Breeding Bird Survey to model, and considered a route to be an occurrence locality for a species if it had a presence record for any year of the survey. The chosen species and the number of routes where each has occurrence localities are shown in Table 1. The occurrence data was divided into ten random partitions: in each partition, 50% of the occur-rence localities were randomly selected for the training set, while the remaining 50% were set aside for testing. environmental variables ( linear features); squares of envi-ronmental variables ( quadratic features); products of pairs of environmental variables ( product features); and binary features derived by thresholding environmental variables ( threshold features). The latter features are equal to one if an environmental variable is above some threshold, and zero otherwise. Use of linear features constrains the mean of the environmental variables in the maxent distribution, linear plus quadratic features constrain the variance, while linear plus quadratic plus product features constrain the co-variance of pairs of environmental variables.
 subsets of the feature types: linear (L); linear and quadratic (LQ); linear, quadratic and product (LQP); and threshold (T). We also ran GARP on each training set.
 interpretations. Nevertheless, each can be used to (par-tially) rank all locations according to their habitability. To compare these rankings, we used receiver operating charac-teristic (ROC) curves. For each of the runs, we calculated the AUC (area under the ROC curve), and determined the average AUC over the ten occurrence data partitions. See Section 3.5 for further discussion of this metric. favor, as a continuous prediction will typically have a higher AUC than a discrete prediction. We therefore do a second comparison, where we select operating thresh-olds for GARP that have been widely used in practice, and compare the algorithms only at those operating points. We call this an  X  X qualized area test X , and the details are as fol-lows. We applied two thresholds to each GARP predic-tion, namely 1 and 10, corresponding to at least one, or all, best-subset models predicting presence (see Section 3.4 for GARP details). These are the most-often used GARP thresholds (Anderson &amp; Mart  X   X nez-Meyer, 2004). For each of the two resulting predictions, we set thresholds for the maxent models that result in prediction of the same area (geographic extent) as GARP. The predictions, now binary and with the same predicted area, are then simply compared using omission rates (fraction of test localities not predicted present). Again, averages were taken over the 10 random partitions of the occurrence data.
 much less data available than for North American birds. Indeed, species of conservation importance may have ex-tremely few georeferenced locality records, often fewer than 10. To investigate the use of maxent in such limited data settings, we perform experiments using limited subsets of the Breeding Bird data. We selected increasing subsets of training data in each partition, ran all four versions of maxent, and took an average AUC over ten partitions. of  X  and its interaction with sample size, we varied  X  and the number of training examples and took an average AUC over ten partitions for all four versions of maxent. Lastly, to measure the effect of our acceleration method, we per-formed runs using the first random partition for the Logger-head Shrike and the Yellow-throated Vireo, both with and without line search for  X  (as described in Section 2), and measured the log loss on both training and test data as a function of running time. 3.4. Algorithm implementations scribed in Section 2 for 500 rounds, or until the change in the objective function on a single round fell below 10  X  5 For the regularization parameter  X  , to avoid overfitting the test data, we used the same setting of 0 . 1 for all feature types, except threshold features for which we used 1 . 0 .In Section 4.4, we describe experiments showing how sensi-tive our results are to the choice of  X  .
 search procedure, we made composite GARP predictions using the  X  X est-subsets X  procedure (Anderson et al., 2003), as was done in recent applications (Peterson et al., 2003; Raxworthy et al., 2004). We generated 100 binary models, using GARP version 1.1.3 with default parameter values, then eliminated models with more than 5% intrinsic omis-sion (negative prediction of training localities). If at most 10 models remained, they then constituted the best subset; otherwise, we selected the 10 models whose predicted area was closest to the median of the remaining models. The composite prediction gives the number of best-subset mod-els in which each point is predicted suitable (0-10). For Cassin X  X  Vireo, the best subset was empty for most random partitions of occurrence localities, so we increased the in-trinsic omission threshold to 10% for that species. 3.5. ROC curves whose output depends on a threshold parameter. It plots true positive rate against false positive rate for each thresh-old. A point ( x , y ) indicates that for some threshold, the classifier classifies a fraction x of negative examples as pos-itive, and a fraction y of positive examples as positive. The curve is obtained by  X  X oining the dots X .
 tistical interpretation. Pick a random positive example and a random negative example. The area under the curve is the probability that the classifier correctly orders the two points (with random ordering in the case of ties). A perfect classifier therefore has an AUC of 1. However, to use ROC curves with presence-only data, we must interpret as  X  X eg-ative examples X  all grid cells with no occurrence localities, even if they support good environmental conditions for the species. The maximum AUC is therefore less than one (Wi-ley et al., 2003), and is smaller for wider-ranging species. 4.1. Equalized Area Test With a threshold of 1, GARP predicts large areas as having suitable conditions for the species, and all algorithms have area under ROC curve (AUC) very low average omission (with the exception of GARP on Cassin X  X  Vireo). A threshold of 10 causes less over-prediction, and reveals more differences between the algo-rithms. The best results are obtained by maxent with two of the feature sets (LQP and T). These two are superior to GARP on all species, often very substantially; LQ is supe-rior to GARP for all species but BhV. 4.2. ROC analysis the 10 random partitions of the occurrence localities. Ex-ample ROC curves used in computing the averages can be seen in Figure 2, which shows the performance of the al-gorithms on the first random partition for the Loggerhead Shrike and Yellow-throated Vireo.
 linear (L) to linear plus quadratic (LQ) features, with a small further improvement when product features are added (LQP). The AUC for threshold features (T) is similar to LQP. For all species, the AUC for GARP is lower than for all maxent feature sets except sometimes L. Note that GARP is disadvantaged in AUC comparisons by not dis-tinguishing between points in its highest rank (those points predicted present in all best-subset models), as can be seen in Figure 2, where GARP loses area at the left end of the ROC curve. Nevertheless, wherever GARP has data points, maxent with the better feature sets is quite consistently as good as or better than GARP. 4.3. Learning Curve Experiments for an increasing number of training examples on eight of the species. We also include GARP results for full train-ing sets as a base line. As expected, models with a larger number of features tend to overfit small training sets, but they give more accurate predictions for large training sets. Linear models do not capture species distribution very well and are included only for completeness. With the exception of the Plumbeous Vireo, three remaining versions of max-ent outperform L models already for the smallest training sets. LQP models become better than LQ for 30-40 train-ing examples; their performance, however, matches that of LQ already for smaller training sets. T models perform worse than both LQ and LQP for small training sets, but they slightly outperform LQP once training sets reach 400 examples. Learning curves for species with large numbers of examples indicate that for both LQ and LQP about 50-100 examples suffice for a prediction that is close to opti-mal for those models. 4.4. Sensitivity to Regularization larization value  X  for LQP and T versions of maxent. Due to the lack of space we do not present results for L and LQ versions, and give sensitivity curves for only four spe-cies. Curves for the remaining species look qualitatively similar. Note the remarkably consistent peak at  X   X  1 . 0 for threshold feature curves; theoretical reasons for this phenomenon require further investigation. For LQP runs, peaks are much less pronounced and do not appear at the same value of  X  across different species. Benefits of regu-larization in LQP runs diminish as the number of training examples increases (this is even more so for LQ and L runs, not presented here). This is because the relatively small number of features (compared with threshold features) nat-urally prevents overfitting large training sets. 4.5. Feature Profiles characterizes the Gibbs distribution q ( x )= e  X  f ( x ) minimizing the (regularized) log loss. When each feature is derived from one environmental variable then the linear area under ROC curve (AUC) additive contribution combination in the exponent of q can be decomposed into a sum of terms each of which depends on a single environ-mental variable. Plotting the value of each term as a func-tion of the corresponding environmental variable we obtain feature profiles for the respective variables. This decompo-sition can be carried out for L, LQ and T models, but not for LQP models. Note that adding a constant to a profile has no impact on the resulting distribution as constants in the exponent cancel out with Z . For L models profiles are linear functions, for LQ models profiles are quadratic functions, and for T models profiles can be arbitrary step functions. These profiles provide an easier to understand characterization of the distribution than the vector  X  . first partition of the Yellow-throated Vireo and two T runs with different values of  X  . The value of  X  =0 . 01 only prevents components of  X  from becoming extremely large, but it does little to prevent heavy overfitting with numer-ous peaks capturing single training examples. Raising  X  to 1 . 0 completely eliminates these peaks. This is especially prominent for the aspect variable where the regularized T as well as the LQ model show no dependence while the insufficiently regularized T model overfits heavily. Note the rough agreement between LQ profiles and regularized T profiles. Peaks in these profiles can be interpreted as in-tervals of environmental conditions favored by a species. However, from a flat profile we may not conclude that the species distribution does not depend on the corresponding variable since variables may be correlated and maxent will sometimes pick only one of the correlated variables. 4.6. Acceleration stantially accelerated convergence when meaured in terms of log loss both on training and on test data. Log loss on test data in the first partition decreased with running time (measured on a 1GHz Pentium) as follows: The observed acceleration is similar to that obtained by Goodman (2002). Line search made no discernible dif-ference for threshold features. Indeed, while there is an approximation made in the derivation of  X  in Sections 2 and 2.2, the derivation is exact for binary features, hence line search is not needed. Maxent was much faster with threshold features: log loss was within .001 of convergence in at most 50 seconds for both species. important area that deserves the attention of the machine learning community while presenting it with some interest-ing challenges.
 predict species distributions. Maxent only requires posi-tive examples, and in our study, is substantially superior to the standard method, performing well with fairly few ex-amples, particularly when regularization is employed. The models generated by maxent have a natural probabilistic in-terpretation, giving a smooth gradation from most to least suitable conditions. We have also shown that the models can be easily interpreted by human experts, a property of great practical importance.
 modeling cleanly and effectively, there are many other techniques that could be used such as Markov random fields or mixture models. Alternatively, some of our as-sumptions could be relaxed, mainly that of the indepen-dence of sampling. In our future work, we plan to address sampling bias and include it in the maxent framework in a principled manner. We leave the question of alternative techniques to attack this problem open for future research.
