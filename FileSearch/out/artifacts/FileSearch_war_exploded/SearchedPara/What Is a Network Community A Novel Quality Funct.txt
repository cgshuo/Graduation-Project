 In this study, we introduce a novel quality function for a network community, which we refer to as the communi-tude . The communitude has a strong statistical background. Specifically, it measures the Z-score of a subset of vertices S with respect to the fraction of the number of edges within the subgraph induced by S . Due to the null model of a random graph used in the definition, our quality function focuses not only on the inside of the subgraph but also on the cut edges, unlike some quality functions for extracting dense subgraphs. To evaluate the detection ability of our quality function, we address the communitude maximiza-tion problem and its variants for realistic scenarios. For the problems, we propose a two-phase heuristic algorithm together with some modified versions. In the first phase, it repeatedly removes the vertex with the smallest degree, and then obtains the subgraph with maximum communi-tude over the iterations. In the second phase, the algorithm improves the obtained solution using a simple local search heuristic. This algorithm runs in linear time when the num-ber of iterations is fixed to a constant; thus, it is applicable to massive graphs. Computational experiments using both synthetic graphs and real-world networks demonstrate the validity and reliability of the proposed quality function and algorithms.
 H.2.8 [ Database Management ]: Database Applications X  Data Mining ; I.5.3 [ Pattern Recognition ]: Clustering X  Algorithms ; G.2.2 [ Discrete Mathematics ]: Graph The-ory X  Graph Algorithms Algorithms, Experimentation, Theory Networks; Community Structure; Quality Function c  X 
Identifying community structure is a fundamental primi-tive in network analysis [14]. Roughly speaking, a commu-nity (also referred to as a module or cluster ) in a network is a subset of vertices more densely connected with each other than with nodes in the rest of the network. Commu-nities often appear in networked systems from a wide variety of fields such as physics, chemistry, biology, and sociology. Detecting communities is a powerful way to discover com-ponents that have some special roles or possess important functions. For example, let us consider the network rep-resenting the World Wide Web, where vertices correspond to web pages and edges represent the hyperlinks between pages. As mentioned in Gibson et al. [16], communities in this network are likely to be the sets of web pages dealing with the same or similar topics, or sometimes link spam. As another example, consider the protein X  X rotein interac-tion networks, where vertices correspond to proteins within a cell and edges represent the interactions between proteins. Communities in this network are likely to be the grouped proteins that have the same or similar functions within the cell [39].

A large body of work has been devoted to developing com-munity detection algorithms, which can be roughly divided into two categories. First, there are methods that divide a network, most of which maximize a globally-defined quality function. The best known and widely used quality function is modularity , which was introduced by Newman and Gir-van [34]. Here let G = ( V,E ) be an undirected graph con-sisting of n = | V | vertices and m = | E | edges. For a subset of vertices S  X  V , let D [ S ] denote the sum of degrees of the ver-tices in S . Also, let G [ S ] be the subgraph induced by S  X  V , i.e., G [ S ] = ( S,E ( S )), where E ( S ) = {{ i,j } X  E | i,j  X  S } . For simplicity, we denote e [ S ] = | E ( S ) | . The modularity, a quality function for a division C = { C 1 ,...,C k } of V (i.e., S i =1 C i = V and C i  X  C j =  X  for i 6 = j ), can be written as This quantity represents the sum, over all communities, of the fraction of the number of edges within the communities minus the expected fraction of such edges assuming that they are placed at random with the same degree distribu-tion. There are a wide variety of modularity maximization algorithms based on greedy techniques [8, 34], spectral op-timization [33], mathematical programming [2, 4, 31], and other techniques.
Second, there are methods that extract one community (a subset of vertices) from a network. We focus on this category in the present study. Most of these methods also maximize a quality function for a subset of vertices S  X  V that well captures the intuition of a network community. Here define the edge density of S  X  V as  X  [ S ] = e [ S ] / On the other hand, define the cut edges of S  X  V as the subset of edges whose one endpoint is in S and the other is in V \ S , and let cut( S ) denote the number of cut edges of S . Intuitively, quality functions for a network community should prefer a subset of vertices that maximizes the edge density but minimizes the number of cut edges at a time . To date, a large body of work has examined preferable concepts of a network community. Below we briefly review the tra-ditional and state-of-the-art ones (and their corresponding optimization problems) from various research fields such as discrete mathematics, computer science, and physics. Cliques. The most fundamental and well-known concept is a clique. A clique is a subset of vertices in which every pair of vertices is connected by an edge. As even a singleton or an edge is a clique, we are usually interested in finding a maximum clique or a maximal clique , i.e., cliques with the maximum size in a graph and cliques not contained in any other clique in a graph, respectively. Finding a maximum clique is one of the most classical optimization problems in the field of theoretical computer science, which is known to be NP-hard and also hard to approximate. In fact, it is known that under some reasonable computational complex-ity assumption, there exists no polynomial-time algorithm that achieves an approximation ratio of O ( n 1  X   X  ) for any  X  &gt; 0 [19]. As for maximal cliques, Makino and Uno [28] proposed efficient enumerative algorithms.
 Quasi-Cliques. Although the definition of a clique is quite intuitive, it is too strong and restrictive to use practically. In fact, if an input graph is incorrect so that some edges are missing, it might happen that the subset of vertices that should be a clique is not a clique. If it is the case, we could no longer recover the subset of vertices by detecting a clique. On the other hand, the concept of quasi-cliques is more ro-bust. A subset of vertices S  X  V is called an  X  -quasi-clique if e [ S ]  X   X  | S | 2 . There are many algorithms for comput-ing quasi-cliques. For example, Abello et al. [1] proposed an algorithm for finding a maximal quasi-clique in massive sparse graphs. Moreover, Uno [42] designed a fast algorithm for enumerating all  X  -quasi-cliques.
 Densest Subgraphs. Let us define the average degree of S  X  V as 2 e [ S ] / | S | . The densest subgraph is a subset of vertices with the maximum average degree in a graph. The densest subgraph has attracted significant attention because it can be computed in time polynomial in n and m using flow-based algorithm [18] or a linear-programming-based al-gorithm [10]. Furthermore, Charikar [10] proved that the linear-time greedy algorithm proposed by Asahiro et al. [6] finds a 1/2-approximate solution.

The densest subgraph problem does not have any con-straint on the number of vertices of the output subgraph. The densest k -subgraph problem (D k S) can be viewed as a size-constrained variant [13]. In this problem, given an additional input k being a positive integer, we are asked to find S  X  V that maximizes average degree 2 e [ S ] / | S | under the restriction | S | = k . Unfortunately, it is known that such size restriction makes the problem much harder to solve. In Figure 1: Two subgraphs in a network. Which one is more community-like? fact, Khot [20] demonstrated that no PTAS exists for D k S under some reasonable computational complexity assump-tion. The current best approximation ratio is O ( n 1 / 4+  X  any  X  &gt; 0 [7]. Recently, Papailiopoulos et al. [35] proposed an almost linear-time approximation algorithm whose ap-proximation ratio depends on the spectrum of the adjacency matrix of the input graph. They experimentally confirmed that the ratio is usually tight for real-world networks.
Furthermore, Andersen and Chellapilla [5] introduced two relaxed versions of D k S: the densest at-least-k subgraph problem (Dal k S) and the densest at-most-k subgraph prob-lem (Dam k S). They proposed a greedy 1 / 3-approximation algorithm for Dal k S. Note that the NP-hardness of Dam k S is immediate because finding a maximum clique can be re-duced to it. Later, Khuller and Saha [21] demonstrated that Dal k S is also NP-hard and improved the above ap-proximation ratio to 1 / 2 using a linear-programming-based algorithm.
 Optimal Quasi-Cliques. As mentioned above, the densest subgraph problem is attractive because it can be solved in polynomial time and can be approximated within a factor of 1 / 2 in linear time. In fact, it has been one of the most popu-lar optimization problems for finding communities (or dense subgraphs) in practical applications. However, as reported in Tsourakakis et al. [41], the densest subgraph problem of-ten leads to a large subgraph without high edge density. This should not be surprising since the densest subgraph problem just maximizes the average degree rather than the edge density.

Motivated by this issue, Tsourakakis et al. [41] proposed a general framework for finding subgraphs with very high edge density. As a special case of the framework, they in-troduced an optimization problem called the OQC-Problem. In this problem, given an undirected graph G = ( V,E ), we are asked to find S  X  V that maximizes where  X  is a parameter in the interval (0 , 1). The optimal solution to this problem is referred to as the optimal quasi-clique . Note that the OQC-Problem turns the  X  -quasi-clique condition into its objective function. Their computational experiments show that the OQC-Problem with  X  = 1 / 3 leads to subgraphs with very high edge density and small diameter, unlike the densest subgraph problem.
 Ratio Cut. All the above definitions and quality functions focus only on the inside of the subgraph. Thus, they could not distinguish two subgraphs in Figure 1. From now on, we review the definitions and quality functions that take into account the cut edges. The first example is the ratio cut , which is defined as for S  X  V . The ratio cut was introduced for circuit par-titioning [43], and its minimization is known to be NP-hard [29].
 Normalized Cut. The normalized cut of S  X  V is defined as The normalized cut was proposed for image segmentation [37], and its minimization is also NP-hard [37]. Note that the nor-malized cut takes into account the number of edges within the subgraph G [ S ], whereas the ratio cut does not. Conductance. The conductance of S  X  V is defined as Note that S is usually assumed to contain no more than half of the vertices in V . In the context of community detection, the conductance has been used much more widely than the ratio cut and the normalized cut. Indeed, Leskovec et al. [26] employed the conductance for introducing the network com-munity profile plot (NCP plot), which describes the quality of network communities at different size scales. Specifically, the NCP plot is defined as the conductance value of the best subset of vertices with cardinality k in the entire network, as a function of k . That is, they plot the following function value for each k : From the resulting plots, they deduced that in many real-world networks, there are significant communities at very small size scales, and at larger size scales, the best possible communities blend in gradually with the rest of the network. Since the above quantity (2) is intractable to compute, they employed well-studied heuristics to approximate it. Community in a Strong Sense and a Weak Sense. Radicchi et al. [36] introduced some definitions of a commu-nity called a community in a strong sense and a community in a weak sense. A subset of vertices S  X  V is called a community in a strong sense if for every vertex in S , the number of neighbors in S is strictly greater than the num-ber of neighbors outside S . On the other hand, S  X  V is called a community in a weak sense if the sum, over all ver-tices in S , of the number of neighbors in S is strictly greater than the number of cut edges of S . Thus, if S  X  V is a community in a strong sense, then it is also a community in a weak sense.
In this study, we introduce a novel quality function for a network community, which we refer to as the communitude . The advantage of our quality function can be summarized as follows.  X  The communitude has a strong statistical background. In fact, it measures the Z-score of a subset of vertices S  X  V with respect to the fraction of the number of edges within the subgraph G [ S ].  X  The communitude is parameter-free. Several state-of-the-art quality functions for extracting communities (or dense subgraphs) are parameter-dependent. Those parameters are sometimes useful because we could find a community with some condition (e.g., desired edge density) by tuning the parameters. However, it is rarely the case that an appropriate parameter setting is known in advance.  X  Due to the null model of a random graph used in the defi-nition, the communitude focuses not only on the inside of the subgraph but also on the cut edges, unlike the quality functions for extracting dense subgraphs.  X  The communitude maximization accurately detects com-munities in practice. The results of computational ex-periments for synthetic graphs show that the communi-tude maximization algorithms more accurately identify the ground-truth communities than the algorithms based on the other quality functions. Furthermore, the results for real-world networks demonstrate that the communi-tude detects natural and reasonable communities in prac-tical use.  X  The definition of the communitude can be naturally ex-tended to edge-weighted graphs. Many real-world net-works have edge weights; thus, the weighted version of the communitude is useful in practical applications.
To evaluate the detection ability of our quality function, we address the communitude maximization problem. For realistic scenarios, we also introduce three variants of the problem, and investigate the computational complexity of them. For the problems, we propose a two-phase heuristic algorithm together with some modified versions. In the first phase, it repeatedly removes the vertex with the smallest degree, and then obtains the subgraph with maximum com-munitude over the iterations. In the second phase, the al-gorithm improves the obtained solution using a simple local search heuristic. This algorithm runs in linear time when the number of iterations is fixed to a constant; thus, it is appli-cable to massive graphs. Computational experiments using both synthetic graphs and real-world networks demonstrate the validity and reliability of the proposed quality function and algorithms.
Leskovec et al. [27] have thoroughly examined several com-munity detection algorithms in order to compare them and to understand their relative performance and systematic bi-ases of extracted communities. In their study, as a possible quality function of a community S  X  V , the following one was considered: This is based on the modularity function (1) by Newman and Girvan [34], that is, it computes the fraction of the number of edges within the subgraph G [ S ] minus the expected fraction of such edges assuming that they are placed at random with the same degree distribution.

Although Leskovec et al. [27] categorized this quality func-tion as a single criterion score (i.e., functions that consider only one criteria such as the inside of the subgraph and the cut edges), we wish to point out here that the function takes into account both the inside of the subgraph and the cut edges. To see this, consider two subsets of vertices S 1 S 2 such that G [ S 1 ] and G [ S 2 ] are isomorphic. Then, we see that e [ S 1 ] = e [ S 2 ]. If cut( S 1 ) &gt; cut( S 2 ), then D [ S follows; thus, we have Q ( S 1 ) &lt; Q ( S 2 ). From this point of view, the function seems to have some potential as a qual-ity function of a network community. However, Leskovec et al. [27] experimentally demonstrate that the function value tends to increase roughly monotonically towards the bisec-tion of the network. They mentioned that this trend should not be surprising since the function measures the volume of a community in terms of the number of edges, and the volume clearly increases with community size. In fact, as shown in Section 5, maximizing the function (3) produces poor solutions in benchmark graphs. Our quality function can be viewed as the appropriately normalized version of this function. For details on the normalization, see Section 2.
There is another way to view our quality function. Re-cently, Miyauchi and Kawase [30] introduced a quality func-tion called Z-modularity for community detection in net-works. It measures the Z-score of a given division of V with respect to the fraction of the number of edges within commu-nities. Their computational experiments demonstrate that Z-modularity maximization leads to reasonable community structure in practical use. Our proposed quality function can be viewed as the counterpart of their quality function, which is specialized to the case of evaluating a single network community.
This paper is structured as follows. In Section 2, our quality function communitude is introduced. In Section 3, the communitude maximization problem and its variants are presented. In Section 4, our algorithms for the prob-lems are described. The experimental evaluation of our pro-posed quality function and algorithms is conducted in Sec-tion 5. Finally, conclusions and suggestions for future work are given in Section 6.
In this section, we first describe the concept that leads to our quality function, followed by its formal definition.
As mentioned above, the modularity-based quality func-tion (3) seems to have some potential as a quality function of a network community. However, the function value tends to increase nearly monotonically towards the bisection of the network, which means that the optimal solution in terms of the quality function might be too large. Roughly speaking, our quality function is the appropriately normalized version of the function, which resolves the above problem.
Given an undirected graph G = ( V,E ) consisting of n = | V | vertices and m = | E | edges, and a subset of vertices S  X  V , we aim to quantify the community degree of S  X  V in terms of the fraction of the number of edges within the subgraph induced by S . To this end, we consider some ran-dom edge generation process over V , and estimate the prob-ability distribution of the fraction of the number of edges within the subgraph G [ S ]. Specifically, we consider the fol-lowing edge generation process. Place N edges over V at random with the same degree distribution of the graph G . Then, when we place an edge, the probability that the edge is placed within S is given by Note that this edge generation process is the same as the null-model (known as the configuration model [32]) used in the definition of the modularity function (1), with the ex-ception of the sample size. We simply wish to estimate the probability distribution of the fraction of the number of edges within the subgraph G [ S ]. Thus, unlike the null-model, the sample size N is not necessarily equal to the number of edges m .

Let X be a random variable denoting the number of edges generated by the process within S . Then, X follows the binomial distribution B ( N,p ). By the central limit theorem, when the sample size N is sufficiently large, the distribution of X/N can be approximated by the normal distribution N ( p,p (1  X  p ) /N ). Thus, we can quantify the community degree of S in terms of the fraction of the number of edges within the subgraph G [ S ] using the Z-score as follows: We omitted the sample size N in the denominator because it never depends on a given subset of vertices. If S =  X  or V , we define com( S ) = 0. We refer to this quality function as the communitude . Note that the numerator is none other than the modularity-based quality function (3), which was presented in Leskovec et al. [27]. Our quality function can be viewed as a modified version of the function (3), which is normalized by the standard deviation of the fraction of the number of edges within the subgraph.

It should be noted that the communitude value has a trivial upper bound. Specifically, for any undirected graph G = ( V,E ) and any subset of vertices S  X  V , we see that com( S ) is at most 1. Here we put t = e [ S ]  X  D [ S ] / 2, we have
Note also that the communitude can be defined on edge-weighted graphs in a similar manner. In fact, the above edge generation process can be naturally extended by con-sidering the weighted degree distribution. Many real-world networks have edge weights; thus, the weighted version of the communitude is useful in practical applications. It is easy to see that the above upper bound remains valid for this edge-weighted version.
In the previous section, we introduced a new quality func-tion communitude. We are now ready to formally define the communitude maximization problem , which is the cen-tral problem in the present study.
 Problem 1 (Communitude maximization) Given an undirected graph G = ( V,E ) , we are asked to find S  X  V that maximizes com( S ) .
In this subsection, we present three variants of the com-munitude maximization problem for realistic scenarios. Size-constrained variant. In practical applications, it is often the case that we wish to obtain a community that satisfies specified size constraints. In fact, as for the dens-est subgraph problem, size-constrained variants (i.e., D k S, Dal k S, and Dam k S) have been extensively studied [5,13,21]. We define the size-constrained variant of the communitude maximization problem as follows.
 Problem 2 Given an undirected graph G = ( V,E ) and pos-itive integers k min and k max ( k min  X  k max ) , we are asked to find S  X  V that maximizes com( S ) under the restriction k Query-dependent variant. In many real-world applica-tions, we are interested in detecting a community that con-tains a given subset of vertices (called query nodes ). In fact, Sozio and Gionis [38] extensively studied an optimization problem called the community-search problem that models this situation. The query-dependent variant of the commu-nitude maximization problem can be defined as follows. Problem 3 Given an undirected graph G = ( V,E ) and a set of query nodes F  X  V , we are asked to find S  X  V that maximizes com( S ) under the restriction F  X  S .
 Top-k variant. In practical applications, it is often the case that we wish to detect k &gt; 1 communities rather than a single one. The top-k variant models this situation, which was studied in Tsourakakis et al. [41] in the context of dense subgraph extraction. We define the top-k variant of the communitude maximization problem as follows, although it may be possible to use other forms of the objective function. Problem 4 Given an undirected graph G = ( V,E ) and a positive integer k , we are asked to find disjoint subsets of vertices S 1 ,...,S k  X  V that maximize P k i =1 com( S i
Here we investigate the computational complexity of the problems. As for Problem 1, although we believe that it is NP-hard, a formal proof of the hardness is still lacking, which we leave as an open problem for future research. On the other hand, we show the complexity of Problem 2 as the following theorem.
 Theorem 1 Problem 2 is NP-hard.

Proof. We reduce the problem of finding a clique of size k in an undirected graph wherein every vertex has the same degree. This problem is known to be NP-hard [3, 15]. Sup-pose that we are given an instance of this problem: an undi-rected graph G = ( V,E ) consisting of n = | V | vertices and m = | E | edges in which each vertex has degree d , and a positive integer k &lt; n .

We claim that the optimal value of Problem 2 for G = ( V,E ) and k min = k max = k attains Algorithm 1 GreedyPeeling Input: Graph G = ( V,E ) Output: Subset of vertices S  X  V 1: U  X  V , S  X  X  X  2: while U is not empty 3: S  X  X  X  X  U } 4: Pick v  X  U with the smallest degree in G [ U ] 5: U  X  U \{ v } 6: return arg max S  X  X  f ( S ) if and only if G contains a clique of size k . The communitude value for S  X  V with | S | = k is since m = ( n  X  d ) / 2 and D [ S ] = k  X  d . Here e [ S ] is at most 2 = k ( k  X  1) / 2 and achieves this upper bound if and only if G [ S ] is a clique. Therefore, Problem 2 is NP-hard.
Note that from the strict monotonic increase of c  X  with respect to k , it follows that Problem 2 is NP-hard even with-out the lower bound k min on the output community.
In this section, we present algorithms to solve the prob-lems described in Section 3.
First, we describe an algorithm for Problem 1. We use a greedy algorithm to obtain an initial solution, and then apply a simple local search heuristic to improve it. Let f denote an arbitrary quality function of a network commu-nity.
 Greedy algorithm. Our algorithm employs the greedy peeling strategy , which iteratively removes the vertex with the smallest degree in the currently remaining graph, and then returns S  X  V with maximum f ( S ) over the iterations, where f represents the objective function of the problem at hand. This strategy is shown to be a 1 / 2-approximation al-gorithm for the densest subgraph problem [10] and to be an additive approximation algorithm for the OQC-Problem [41]. For convenience, the procedure is described in Algorithm 1. As mentioned in previous works [21,41], the algorithm can be implemented to run in O ( m + n ) time for both the densest subgraph problem and the OQC-Problem using a doubly-linked list of vertices for each degree. Since we can evaluate the communitude value for S  X  V in constant time by keep-ing the values of e [ S ] and D [ S ], the algorithm also runs in O ( m + n ) time for the communitude maximization problem. Local search heuristic. We design a simple local search heuristic to improve a given initial solution S init  X  V . This algorithm visits all the vertices in a random order. For each v  X  V , it adds v to S init or removes v from S init appropri-ately if the objective value is strictly improved. The process continues until a locally optimal solution is obtained or the number of iterations exceeds the specified threshold T Furthermore, we repeat the whole process R max times, and then return the best solution. The detailed procedure is shown in Algorithm 2, where the operator  X  represents the Algorithm 2 LocalSearch Input: Graph G = ( V,E ); Initial solution S init  X  V Output: Subset of vertices S  X  V 1: S  X  X  X  2: for r  X  1 ,...,R max 4: for t  X  1 ,...,T max 5: for v  X  V in a random order 6: if f ( S 0  X  { v } ) &gt; f ( S 0 ) then S 0  X  S 0  X  { v } 7: if there exists no update of S 0 in the last for-loop 8: then break 9: if f ( S 0 ) &gt; f ( S ) then S  X  S 0 10: return S symmetric difference. The time complexity of Algorithm 2 is O (( m + n )  X  R max  X  T max ). In the later experiments, we set R max = 10 and T max =  X  since the number of iterations until convergence is small in practice, as mentioned in Sec-tion 5.2. Thus, we only take locally optimal solutions in the experiments.
 Algorithm for Problem 1. We are now ready to describe our algorithm for Problem 1: it obtains an initial solution S init by Algorithm 1, and then runs Algorithm 2 for S init improve it.
Here we present algorithms to solve the variants of the communitude maximization problem by modifying the above algorithm.
 Algorithm for Problem 2. Let k min and k max ( k min  X  k max ) be positive integers denoting the lower bound and the upper bound, respectively, on the output community. We use the algorithm for Problem 1 with some modifications. In the greedy peeling phase, the modified algorithm obtains the best community S  X  V with k min  X | S | X  k max over the iterations. In the local search phase, the algorithm skips the changes that violate the size constraint.
 Algorithm for Problem 3. Let F  X  V be a set of query nodes. We use Algorithm 2, with some modification, by set-ting F to the initial solution. In the modified algorithm, we skip the updates corresponding to the vertices in F because they must be contained in the output community.
 Algorithm for Problem 4. Let k be a positive integer denoting the number of output communities required. We find a single community at a time by the algorithm for Prob-lem 1, and then remove the vertices in the community from the graph. We continue this process until k communities are found or we are left with an empty graph. It should be noted that the values in the algorithm, such as the degree of a vertex and the communitude value for a subset of vertices, are always computed on the input graph. In particular, this operation is critical for the communitude because we wish to obtain a community with maximum communitude on the original graph rather than the remaining graph.
The purpose of our computational experiments is to eval-uate the validity and reliability of the proposed quality func-tion and algorithms. Our experiments are conducted on var-ious synthetic graphs and well-known real-world networks.
First, we report the results of computational experiments with synthetic graphs. We deal with three types of syn-thetic graphs: the single community model, the planted l -partition model, and the Lancichinetti X  X ortunato X  X adicchi (LFR) benchmark. In these models, once their parameters are set (and some query nodes are chosen in the latter two models), the ground-truth community is fixed. Thus, we can evaluate the quality of the obtained community by compar-ison with the ground-truth using some similarity measure. To this end, we adopt the Jaccard index. The Jaccard index for two sets A and B is defined as which ranges from 0 to 1. For two sets A and B , the higher the Jaccard index is, the more similar they are (and vice versa). In fact, J ( A,B ) = 1 if A and B are identical, and J ( A,B ) = 0 if they are disjoint.

For each of the above models, we use one of the algorithms presented in Section 4. We compare the following five quality functions as an objective function f in the algorithms: the communitude, the modularity-based function (3), the aver-age degree (i.e., the quality function of the densest subgraph problem), the quality function of the OQC-Problem (with  X  = 1 / 3), and the conductance. When the algorithm used is clear from the context, we simply denote the algorithms by COM , Q , DS , OQC , and COND , respectively.
 Single community model. First, we consider the single community model, where exactly one community is planted in a sparse graph. More specifically, the Erd  X os X  X   X enyi ran-dom graph [12] with c vertices and edge probability p in is planted in the Erd  X os X  X   X enyi random graph with n ( &gt; c ) vertices and edge probability p out ( &lt; p in ). Here we set n = 1000. We construct four networks corresponding to combinations of two different community sizes ( c = 50 or 100) and two different edge probabilities inside the commu-nity ( p in = 0 . 2 or 0.4). The parameter p out starts with 0.01 and then increases in stages. We use the algorithm for Prob-lem 1 to maximize the quality functions.

The results are shown in Figure 2. As can be seen, COM most accurately detects the planted community for many cases. Consistent with Tsourakakis et al. [41], DS returns the entire set of vertices for almost all cases. OQC seems not to be robust because its performance depends heavily on the edge probability p in of the planted community (which is not known in advance). In fact, although it accurately detects the community when  X  &lt; p in , whereas it detects too small subgraph when  X  &gt; p in . The algorithms Q and COND pro-duce poor solutions for all settings. Note finally that DS is a 1 / 2-approximation algorithm for the densest subgraph problem [10], and OQC is an additive approximation algo-rithm for the OQC-Problem [41], because the local search heuristic just improves the greedy solution.
 Planted l -partition model. The planted l -partition model was introduced by Condon and Karp [11]. In this model, n vertices are divided into l equally sized groups (consisting of c = n/l vertices each). Two vertices in the same group are connected by probability p in , whereas two vertices in different groups are connected by probability p out ( &lt; p Throughout the experiments, we set n = 1000. We con-struct four networks corresponding to combinations of two Figure 2: Results for the single community model.
 Each point corresponds to an average over 100 graph realizations. Figure 3: Results for the planted l -partition model. Each point corresponds to an average over 100 graph realizations. different edge probabilities inside the groups ( p in = 0 . 2 or 0.4) and two different community sizes ( c = 50 or 100). The parameter p out starts with 0.01 and then increases in stages. We randomly choose a group and select five vertices in it as query nodes. We maximize the quality functions by Algo-rithm 2 for the query nodes.

The results are illustrated in Figure 3. The trend of the results is similar to that of the single community model. It should be noted that this is not a trivial outcome because there are a large number of neighboring communities in a graph generated by this model, unlike the single community model.
 LFR benchmark. In the planted l -partition model, each group in a generated graph forms the Erd  X os X  X   X enyi random graph. Thus, all vertices have approximately the same de-gree. Moreover, all groups have exactly the same size. These phenomena are rarely observed in networks in real-world systems. As a more realistic model, the LFR benchmark was proposed by Lancichinetti et al. [25] for the case of un-weighted and undirected graphs. The LFR benchmark was Figure 4: Results for the LFR benchmark. Each point corresponds to an average over 100 graph re-alizations. then extended to the case of directed and weighted graphs with overlapping communities [23]. The LFR benchmark has often been used to evaluate community detection algo-rithms in the data mining community [9, 40]. We now use the original unweighted and undirected case without over-lapping communities.

In the model, degree distribution and community size dis-tribution follow the power law with exponents  X  and  X  , re-spectively. Furthermore, we can specify the number of ver-tices n , average degree d , maximum degree d max , minimum community size c min , maximum community size c max , and mixing parameter  X  . In particular, mixing parameter  X  in-dicates the mixing ratio of communities, that is, the higher  X  is, the more densely connected the communities are. The model constructs a network consistent with the specified pa-rameters. For more details, see Lancichinetti et al. [25]. In our experiments, we set the parameters the same as used in Lancichinetti and Fortunato [24] as follows:  X  =  X  2,  X  =  X  1, d = 20, and d max = 50. We construct four net-works corresponding to combinations of two different net-work sizes ( n = 1000 or 5000) and two different ranges of community size (( c min ,c max ) = (10 , 50) or (20,100)). We randomly choose a community and select five vertices in it as query nodes. We maximize the quality functions by Al-gorithm 2 for the query nodes.

The results are shown in Figure 4. For the smaller commu-nity size settings (( c min ,c max ) = (10 , 50)), COM and OQC returns nearly the same (and the best) quality solutions. On the other hand, when the network consists of relatively large communities (( c min ,c max ) = (20 , 100)), COM outper-forms the algorithms based on the other quality functions. From the above, we see that COM is the most robust algo-rithm. The algorithm Q again produces poor solutions for all cases, whereas COND is of relatively good quality for this model.
Here we report the results of computational experiments with real-world networks. First, we describe the results of well-known networks: the Zachary X  X  karate club network, the Les Mis  X erables network, and the American college foot-Table 1: Real-world graphs used in the experiments. N etwork n m Description Ka rate 34 78 Social network Mexican 35 117 Social network Dolphins 62 159 Animal social Lesmis 77 254 Co-appearance Polbooks 105 441 Co-purchased Football 115 613 Games network Jazz 198 2,742 Social network USAir97 332 2,126 Airport network Email 1,133 5,451 Email network Polblogs 1,490 16,715 Blog network Wiki-Vote 7,155 100,762 Wikipedia
AS-22july06 22,963 48,436 Internet topology soc-Epinions1 75,879 405,740 Online social web-Stanford 281,903 1,992,636 Web graph com-DBLP 317,080 1,049,866 Co-authorship web-Google 875,713 4,322,051 Web graph
AS-Skitter 1,696,415 11,095,298 Internet topology com-LiveJournal 3,997,962 34,681,189 Online social ball network. Specifically, we use the algorithm for Prob-lem 4 with k = 2 for these networks. Then, we present the numerical results for the networks listed in Table 1, in-cluding large-scale networks such as AS-Skitter (1,696,415 vertices) and com-LiveJournal (3,997,962 vertices). Zachary X  X  karate club network. The first example is the famous karate club network analyzed by Zachary [44], which is often used as a benchmark to evaluate community detec-tion algorithms. It consists of 34 vertices representing the members in a karate club in an American university, in addi-tion to 78 edges representing friendship relations among in-dividuals. Due to a conflict between the club administrator and the instructor, the club members split into two groups, one supporting the administrator and the other supporting the instructor.

The communities extracted by the algorithm are shown in Figure 5. The label of each vertex represents an identi-fication number of the member. For example, the numbers 1 and 34 represent the administrator and the instructor, re-spectively. The dashed line gives the division of the network into the above two groups. As can be seen, the extracted communities never straddle two groups. In fact, S 1 is con-tained in the group supporting the administrator, whereas S 2 is contained in the group supporting the instructor. Fur-thermore, it should be noted that S 1 and S 2 are communities in a strong sense.
 Les Mis  X erables network. The second example is the net-work of the characters in the novel Les Mis  X erables by Victor Hugo, compiled by Knuth [22]. It consists of 77 vertices representing the characters and 254 edges indicating the co-appearance of characters.

The communities extracted by the algorithm are presented in Figure 6. The label of each vertex represents the name of the character. Interestingly, the extracted communities are likely to correspond to specific groups within the story. In fact, S 1 contains major characters belonging to the rev-olutionary student club Friends of the ABC . Moreover, S identifies the characters who appear in Chapter 1 of the book. Both S 1 and S 2 satisfy the condition of a community in a strong sense.
 Figure 5: Communities obtained in Zachary X  X  karate club network: S 1 ( | S 1 | = 10 and com( S 1 ) = 0 . 4254 ) and S 2 ( | S 2 | = 15 and com( S 2 ) = 0 . 4489 ).
 Figure 6: Communities obtained in Les Mis  X erables network: S 1 ( | S 1 | = 15 and com( S 1 ) = 0 . 5775 ) and S ( | S 2 | = 10 and com( S 2 ) = 0 . 7425 ).
 American college football network. The third example is a network of college football teams in the United States, which was derived by Girvan and Newman [17]. There are 115 vertices representing the football teams, in addition to 654 edges connecting teams that played each other in a reg-ular season. The teams are divided into 12 groups called conferences containing approximately 10 teams each. More games are played between teams in the same conference than between teams in different conferences.
 The communities extracted by the algorithm are shown in Figure 7. Each group of circularly-arranged vertices repre-sents a conference. The community S 1 completely recovers a conference consisting of 9 teams. On the other hand, S extracts a conference consisting of 13 teams together with one team from another conference. As can be seen, the ad-ditional team has a few edges to the detected conference, whereas it has no edges to the teams in the original con-ference. Thus, the community S 2 seems to suggest a more reasonable conference than the original one. Both S 1 and S 2 are again communities in a strong sense. Figure 7: Communities obtained in American col-lege football network: S 1 ( | S 1 | = 9 and com( S 1 ) = 0 . 6654 ) and S 2 ( | S 2 | = 14 and com( S 2 ) = 0 . 6493 ). Table 2: Numerical results for real-world graphs.
 N etwork com( S ) | S | e [ S ] cut( S ) Ka rate 0.4254 10 18 14 Mexican 0.3634 13 36 25 Dolphins 0.5613 12 24 16 Lesmis 0.5775 15 64 23 Polbooks 0.5792 41 176 20 Football 0.6654 9 36 25 Jazz 0.4520 46 510 435 USAir97 0.3478 78 456 479 Email 0.5969 25 113 133 Polblogs 0.5149 500 6,904 1,333 Wiki-Vote 0.4046 1,378 12,436 16,496
AS-22july06 0.5691 7,277 12,572 4,510 soc-Epinions1 0.4595 13,474 71,553 62,096 web-Stanford 0.8651 5,749 227,449 11,142 com-DBLP 0.9017 494 15,389 2,765 web-Google 0.9388 184 3,172 408
AS-Skitter 0.7293 157,216 2,570,493 265,258 com-LiveJournal 0.9574 9,323 1,003,448 27,817 Numerical results. For each real-world network listed in Table 1, we extracted a community by the algorithm for Problem 1. The numerical results are shown in Table 2. As can be seen, each community has a small number of cut edges in comparison with the number of edges within the community. In particular, this trend is significant for the communities with relatively large communitude values. It should be noted that all the obtained communities satisfy the condition of a community in a weak sense.

We conclude this section by reporting the running time of the algorithms. Consistent with the time complexity anal-ysis in Section 4, all the implemented algorithms are suf-ficiently fast. In fact, even for the largest network com-LiveJournal, the algorithm for Problem 1 runs in a few tens of minutes on a MacBook Pro (with 2.8 GHz CPU and 16 GB RAM). Although the number of iterations cor-responding to T max (in Algorithm 2) depends on the input graph and the randomness of the vertex order, it is usually small enough to accept. For example, we observed that it is always at most 100 for com-LiveJournal network.
In this study, we have introduced a novel quality function for a network community, which we named the communi-tude . The communitude has a strong statistical background, that is, it measures the Z-score of a subset of vertices S  X  V with respect to the fraction of the number of edges within the subgraph G [ S ]. Due to the null model of a random graph used in the definition, our quality function focuses not only on the inside of the subgraph but also on the cut edges, unlike the quality functions for extracting dense subgraphs. To evaluate the detection ability of our quality function, we have addressed the communitude maximization problem and its variants for realistic scenarios. For the problems, we have proposed a two-phase heuristic algorithm together with some modified versions. The algorithm runs in linear time when the number of iterations is fixed to a constant; thus, it is applicable to massive graphs. Computational experi-ments using both synthetic graphs and real-world networks demonstrate the validity and reliability of the proposed qual-ity function and algorithms. Therefore, we conclude that the communitude could be a potential option for the quality function of a network community.

There are several directions for future research. As men-tioned in Section 3, the computational complexity of Prob-lem 1 should be investigated, although we believe that it is NP-hard. As another direction, it may be interesting to analyze statistical properties of community structure in real-world networks using our quality function. For instance, the NCP plot can be described using the communitude rather than the conductance. That is, for each positive integer k  X  n , we plot the communitude value of the best subset of vertices S  X  V with | S | = k . To this end, effective algo-rithms for the size-constrained variant (Problem 2) should be examined more deeply.
The authors would like to thank the anonymous reviewers for their valuable suggestions. The first author is supported by a Grant-in-Aid for JSPS Fellows (No. 26-11908). The second author is supported by a Grant-in-Aid for Research Activity Start-up (No. 26887014). This work was partially supported by JST, ERATO, Kawarabayashi Large Graph Project. [1] J. Abello, M. G. C. Resende, and S. Sudarsky. Massive [2] G. Agarwal and D. Kempe. Modularity-maximizing [3] C. C. Aggarwal, R. K. Ahuja, J. Hao, and J. B. Orlin. [4] D. Aloise, S. Cafieri, G. Caporossi, P. Hansen, [5] R. Andersen and K. Chellapilla. Finding dense [6] Y. Asahiro, K. Iwama, H. Tamaki, and T. Tokuyama. [7] A. Bhaskara, M. Charikar, E. Chlamtac, U. Feige, and [8] V. D. Blondel, J.-L. Guillaume, R. Lambiotte, and [9] T. Chakraborty, S. Srinivasan, N. Ganguly, [10] M. Charikar. Greedy approximation algorithms for [11] A. Condon and R. M. Karp. Algorithms for graph [12] P. Erd  X os and A. R  X enyi. On random graphs I. Publ. [13] U. Feige, D. Peleg, and G. Kortsarz. The dense [14] S. Fortunato. Community detection in graphs. Phys. [15] M. R. Garey and D. S. Johnson. Computers and [16] D. Gibson, R. Kumar, and A. Tomkins. Discovering [17] M. Girvan and M. E. J. Newman. Community [18] A. V. Goldberg. Finding a maximum density [19] J. H  X astad. Clique is hard to approximate within n [20] S. Khot. Ruling out PTAS for graph min-bisection, [21] S. Khuller and B. Saha. On finding dense subgraphs. [22] D. E. Knuth. The Stanford GraphBase: A Platform for [23] A. Lancichinetti and S. Fortunato. Benchmarks for [24] A. Lancichinetti and S. Fortunato. Community [25] A. Lancichinetti, S. Fortunato, and F. Radicchi. [26] J. Leskovec, K. J. Lang, A. Dasgupta, and M. W. [27] J. Leskovec, K. J. Lang, and M. Mahoney. Empirical [28] K. Makino and T. Uno. New algorithms for [29] D. W. Matula and F. Shahrokhi. Sparsest cuts and [30] A. Miyauchi and Y. Kawase. Z-score-based modularity [31] A. Miyauchi and Y. Miyamoto. Computing an upper [32] M. Molloy and B. Reed. A critical point for random [33] M. E. J. Newman. Modularity and community [34] M. E. J. Newman and M. Girvan. Finding and [35] D. S. Papailiopoulos, I. Mitliagkas, A. G. Dimakis, [36] F. Radicchi, C. Castellano, F. Cecconi, V. Loreto, and [37] J. Shi and J. Malik. Normalized cuts and image [38] M. Sozio and A. Gionis. The community-search [39] V. Spirin and L. A. Mirny. Protein complexes and [40] H. Sun, J. Huang, J. Han, H. Deng, P. Zhao, and [41] C. E. Tsourakakis, F. Bonchi, A. Gionis, F. Gullo, and [42] T. Uno. An efficient algorithm for solving pseudo [43] Y.-C. Wei and C.-K. Cheng. Towards efficient [44] W. W. Zachary. An information flow model for
