 Pseudo-relevance feedback finds useful expansion terms from a set of top-ranked documents. It is often crucial to identify those good feedback documents from which useful expansion terms can be added to the query. In this paper, we propose to detect good feedback documents by classifying all feed-back documents using a variety of features such as the distri-bution of query terms in the feedback document, the similar-ity between a single feedback document and all top-ranked documents, or the proximity between the expansion terms and the original query terms in the feedback document. By doing this, query expansion is only performed using a se-lected set of feedback documents, which are predicted to be good among all top-ranked documents. Experimental results on standard TREC test data show that query expansion on the selected feedback documents achieves statistically signif-icant improvements over a strong pseudo-relevance feedback mechanism, which expands the query using all top-ranked documents.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Storage &amp; Retrieval]: Information Search &amp; Retrieval General Terms: Performance, Experimentation Keywords: Relevance feedback, Feedback document clas-sification
Relevance feedback is a technique that improves query representation using feedback information. A classical rele-vance feedback algorithm was proposed by Rocchio in 1971 [9] for the SMART retrieval system [10]. Pseudo-relevance feed-back (PRF) automatically uses the top-ranked documents in the first-pass retrieval for relevance feedback. A strong assumption behind PRF is that the top-ranked documents are mostly relevant and informative, from which important terms that are closely related to the topic can be extracted. Despite the marked improvement in the retrieval perfor-mance over the first-pass retrieval (e.g. [1, 8]), PRF can also fail, leading to a decreased retrieval performance. As sug-gested by many previous works, the quality of the feedback document set is a key factor that affects query expansion effectiveness (e.g. [1]). A poor feedback document set can be very noisy, so that off-topic expansion terms are added to the query, leading to a degraded retrieval performance.
In the literature of information retrieval (IR), there have been many studies on PRF X  X  effectiveness. For example, a wide range of predictors were proposed to indicate the query performance, which is usually highly correlated with PRF X  X  effectiveness (e.g. [2]). Recently, Cao et al. proposed to refine PRF at the term level [4]. They apply Support Vec-tor Machine (SVM) to select good expansion terms using a list of term features, such as the proximity of the expansion term and the original query terms, or the co-occurrences of the expansion term and the original query terms in the col-lection. While the expansion term selection approach in [4] has been shown to be effective, we suggest that PRF can also be improved by choosing the right documents for relevance feedback, from which expansion terms are extracted.
In this paper, we argue that the quality of feedback doc-uments is a crucial factor that affects PRF X  X  retrieval per-formance. We aim to refine PRF at the document level by differentiating between  X  X ood X  and  X  X ad X  feedback docu-ments. We apply standard classification methods to pick up the high-quality feedback documents, or in other words, to remove the low-quality ones. A list of novel feedback doc-ument features, including the Entropy of query terms in a feedback document, the similarity between a single feedback document and the whole feedback document set, are applied in our study. In addition, we adapt some of the expansion term features used in [4] to the document level, which are also used in our study.

The main contributions of this paper are as follows. We propose a feedback document filtering mechanism based on standard classification algorithms with various document features. Using our proposed feedback document filtering mechanism, only documents predicted to be of good quality by the classifiers are used for relevance feedback. By ex-tensive experiments on standard TREC test collections, we show that the proposed feedback document filtering mech-anism provides statistically significant improvement in the retrieval performance over a PRF baseline, which uses all top-ranked documents for pseudo-relevance feedback.
In this section, we present our proposed feedback docu-ment filtering mechanism. We define the selection of feed-back documents from the pseudo-relevant set as a binary classification problem, where each candidate feedback doc-ument in the pseudo-relevant set is predicted to be either good or bad.
 Generally, standard classification methods, such as Naive Bayes classification or Logistic Regression, can be used to yield a prediction confidence value k , from 0 to 1, for each classification instance. In our case, the classification in-stance is each candidate feedback document in the pseudo-relevant set. Such a k value indicates to which degree the classifier is confident in the prediction outcome.
In this paper, we denote (+)k the confidence value of a feedback document predicted to be good, and (-)k=1-(+)k that of a feedback document predicted to be bad. The higher k is, the more confidence the classifier has on the prediction outcome. For example, a k value of (+)0.90 shows that the feedback document is highly likely to be good, and a k value of (-)0.60 is equivalent to (+)0.40 , which shows that the feedback document is likely to be bad, but with a less confidence value than (+)0.90 has.

In our study, we propose to use the confidence value as a threshold to develop a feedback document filtering mech-anism. Using this mechanism, only a feedback document with a confidence value above the given threshold is used for relevance feedback.

For example, setting the threshold to (+)0.90 implies a hard classifier, which only uses documents that are highly likely to be good for relevance feedback. On the other hand, setting the threshold to (-)0.70 implies a relatively soft clas-sifier, which includes not only the documents in the  X  X ood X  class, but also the documents in the  X  X ad X  class with an absolute confidence value smaller than 0.70.
In this paper, we apply Naive Bayes classification (NB) and Logistic Regression (LR) to classify feedback documents. Although any classification method can be applied for this task, we use these two methods for their excellent trade-off between effectiveness and efficiency. In our experiments, we use Weka X  X  implementation of the above two classifiers with default parameter settings [11]. For the NB classifier, the kernel density estimator instead of the normal distribution is empirically applied for a better effectiveness.
We apply a list of features to assist the classification of feedback documents. The applied features take into account the statistics of the expansion terms and feedback docu-ments in different ways, in an attempt to capture the salient characteristics of the good feedback documents. The applied features are described as follows: We also apply some of the features that were used in [4]. As the features in [4] were defined at the term level, we transform them to the document level by considering all most weighted expansion terms in a feedback document.
In this paper, we train our classifiers in a supervised man-ner. In the next section, we introduce our methods for cre-ating the training data for the supervised learning.
An initial step of our experiments is to create a ground truth, where each candidate feedback document is labelled as either  X  X ood X  or  X  X ad X . Our classifiers for the feedback documents are then trained based on this ground truth through supervised learning. With respect to this issue, an inter-esting research question arises: What is a good feedback document?
An intuitive solution is to consider a feedback document to be  X  X ood X  when it provides an improvement in average precision (AP), compared to the first-pass retrieval. In other words, let AP be the first-pass retrieval performance, and PRFAP(d) be the AP obtained by PRF using document d for feedback, if  X  = PRFAP ( d )  X  AP is larger than zero, we consider d to be a good feedback document, and a bad one otherwise.

A potential problem of the above naive definition of a good feedback document is that it assumes a linear relation between AP and  X , which may not be the case in practise. We suggest that the improvement in AP that we expect from relevance feedback is not linearly related to the first-pass AP. If the first-pass AP is too low, the query expansion mechanism will not have a good enough pseudo-relevant set to extract useful expansion terms [2]. On the other hand, if the first-pass AP is too high, there might be only little room for potential improvement. Therefore, the relation between the first-pass AP (AP) and the improvement in AP brought by query expansion ( X ) can be non-linear.

In this paper, we assumes a quadratic function for the ex-pected decrease in AP brought by a bad feedback document: where  X  ,  X  and  X  are again the parameters of the quadratic function. A feedback document is considered to be good when it does not cause a decrease in the retrieval perfor-mance that exceeds the expectation.
In the following, we perform experiments with our clas-sifiers for feeback documents. We use Terrier 1 for both indexing and retrieval. We apply the DPH model [6], de-rived from the Divergence From Randomness (DFR) frame-work [1], for the first-pass retrieval. Note that DPH is a parameter-free model. All variables in its formula can be directly obtained from the collection statistics. No parame-ter tuning is required to optimise DPH, and we can rather focus on studying PRF. We mainly report the experimental results obtained by using the 50 top-ranked documents for pseudo-relevant feedback for brevity. We have also experi-mented with different numbers of candidate feedback docu-ments used for each query, for which the related results are summarised in Section 6.
 We experiment on the disk4&amp;5 (minus the Congressional Record on disk4) of the TREC collections, and the large-scale DOTGOV2 TREC Web collection. The disk4&amp;5 col-lection contains approximately half a million newswire ar-ticles from various sources, e.g. the Financial Times, the Los Angeles Times, etc. The 249 ad-hoc queries from the TREC 2004 Robust track are used. Out of the 249 top-ics, we use the 125 odd-numbered ones for training, and the 124 even-numbered ones for testing. DOTGOV2 is a very large crawl of the .gov domain, which has more than 25 million documents with an uncompressed size of 423 Giga-bytes. There are 150 ad-hoc topics, from TREC 2004 -2006 Terabyte tracks, associated to DOTGOV2. We use the 75 odd-numbered topics for training, and use 50 out of the 75 even-numbered topics for testing, which is the official setting in the TREC 2008 Relevance Feedback track [3]. All doc-uments and queries are stemmed using Porter X  X  stemmer. Standard stopword removal is also applied. We experiment with title-only queries because it is a realistic setting that reflects the concise nature of real users X  queries.
We firstly optimise the threshold confidence value k . On the train topics, we test each k value from 0 to 1 with an interval of 0.1, and set the threshold to the k value with the best mean average precision (MAP) on the train topics. The obtained k values are then applied on the test topics to determine which documents are used for relevance feedback.
As the aim of this study is to improve PRF by classifying feedback documents, our baseline is Rocchio X  X  PRF, which performs query expansion over all top-ranked documents. Table 1 provides the related evaluation results. In this ta-ble, each cell in the last row contains the obtained MAP, the threshold value obtained on the train topics, and the improvement over PRF in percentage. A star indicates a sta-tistically significant improvement according to the Wilcoxon matched-pairs signed-ranks test at the 0.05 level. For exam-ple,  X 0.2824, (+)0.80, 4.94*  X  (see the result obtained by the Naive Bayes classifier learnt on disk4&amp;5) indicates a MAP of 0.2824 obtained by relevance feedback. Feedback documents that are predicted to be poor with an absolute confidence value higher than 0.80 are filtered out from the pseudo-relevant set. Such a feedback document filtering mechanism provides a 4.94% statistically significant improvement over the PRF baseline. Moreover, a threshold value of 0.50 indi-cates that the feedback document filtering mechanism keeps http://terrier.org Table 2: IR evaluation results on disk4&amp;5 and its test topics with different pseudo-relevant set sizes. The threshold setting is the same as those in Ta-ble 1, namely (+)0.80 for Naive Bayes classifier and (+)0.70 for Logistic Regression. Table 3: IR evaluation results on DOTGOV2 and its test topics with different pseudo-relevant set sizes. The threshold setting is the same as those in Table 1, namely 0.50 for both classification methods. all feedback documents in the  X  X ood X  class and removes all feedback documents in the  X  X ad X  class.

Table 1 shows encouraging results. With appropriate thresh-old setting, our feedback document filtering mechanism sig-nificantly outperforms PRF, which uses all top-ranked doc-uments for relevance feedback. Experiments in this section are conducted with 50 candidate feedback documents per query. In the next section, we vary the number of candidate feedback documents considered for each query to examine the impact the pseudo-relevant set size on the effectiveness of our proposed approach.
In this section, we conduct experiments with different numbers of documents in the pseudo-relevant set. Tables 2 and 3 provide the experimental results on disk4&amp;5 and DOTGOV2, respectively. In the tables, each cell in the last two columns contains the obtained MAP value, and the im-provement in percentage. A star indicates a statistically significant improvement over the baseline according to the Wilcoxon matched-pairs signed-ranks test at the 0.05 level.
According to the results in Tables 2 and 3, PRF shows a high sensitivity to | D | , the size of the pseudo-relevant set. PRF X  X  retrieval performance varies strongly with the change of | D | . On the other hand, the retrieval performance of our feedback document filerting mechanism remains stable, particularly on disk4&amp;5. This indicates that our proposed feedback document filtering mechanism is indeed able to pick up the good feedback documents for different sizes of the pseudo-relevant set.

Overall, our feedback document filtering mechanism has been shown to be robust and effective with a varying size of the pseudo feedback set. It provides a retrieval performance that is at least as good as PRF, even if an optimal pseudo-relevant set size is used. This is a very encouraging finding in that the size of the pseudo-relevant set is an important parameter of PRF, which has a direct impact on PRF X  X  re-trieval performance [5]. On the other hand, our proposed mechanism is able to achieve an effective retrieval perfor-mance without knowing what the actual optimal pseudo-relevant set size is.
In this paper, we have proposed a mechanism for filter-ing feedback documents that refines Pseudo-relevance feed-back (PRF) at the document level. A variety of document features, including the distribution of query terms in the feedback document, the similarity between a single feedback document and all top-ranked documents, or the proximity between the expansion terms and the original query terms in the feedback document, are applied for facilitating the classification of the feedback documents. According to the extensive experimental results, our feedback document fil-tering mechanism provides effective retrieval performance compared to a strong PRF baseline that uses all top-ranked documents for relevance feedback.
 This work is funded by SIMAP: Simulation modelling of the MAP kinase pathway. EC project 2006-2009.
