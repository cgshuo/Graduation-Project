 The vast majority of real world classification problems are imbalanced, meaning there are far fewer data from the class of interest (the positive class) than from other classes. We propose two machine learning algorithms to handle highly imbalanced classification problems. The classifiers are dis-junctions of conjunctions, and are created as unions of par-allel axis rectangles around the positive examples, and thus have the benefit of being interpretable. The first algorithm uses mixed integer programming to optimize a weighted bal-ance between positive and negative class accuracies. Reg-ularization is introduced to improve generalization perfor-mance. The second method uses an approximation in order to assist with scalability. Specifically, it follows a charac-terize then discriminate approach, where the positive class is characterized first by boxes, and then each box boundary becomes a separate discriminative classifier. This method has the computational advantages that it can be easily par-allelized, and considers only the relevant regions of feature space.
 I.2.6 [ Artificial Intelligence ]: Learning Classification; Imbalanced Data; Decision Trees
Our interest is in deriving interpretable predictive classi-fication models for use with imbalanced data. Data clas-sification problems having imbalanced (also called  X  X nbal-anced X ) class distributions appear in many domains, rang-ing from mechanical failure detection or fault detection, to fraud detection, to text and image classification, to medi-cal disease prediction or diagnosis. Imbalanced data cause typical machine learning methods to produce trivial results, that is, classifiers that only predict the majority class. One can not optimize vanilla classification accuracy and use stan-dard classification methods when working with imbalanced data. This is explained nicely by Chawla, Japkowicz, and Kolcz [7] who write:  X  The class imbalance problem is per-vasive and ubiquitous, causing trouble to a large segment of the data mining community.  X 
In order for the models we derive to be interpretable to human experts, our classifiers are formed as a union of axis parallel rectangles around the positive (minority class) ex-amples, and we call such classifiers box drawing classifiers . These are X  X isjunctions of conjunctions X  X here each conjunc-tion is a box. An example of a box drawing classifier we created is in Figure 1, exemplifying our goal to classify the positive examples correctly even if they are scattered within a sea of negative examples. Our classifiers are regularized in several ways, to prefer fewer boxes and larger boxes. We take two polar approaches to creating box drawing classi-fiers, where the first is an exact method, based on mixed in-teger programming (MIP). This method, called Exact Boxes can be used for small to medium sized datasets, and provides a gold standard to compare with. If we are able to make sub-stantial approximations and still obtain performance close to that of the gold standard, our approximations would be justified. Our second method, Fast Boxes makes such an approximation.

Fast boxes takes the approach of characterize then dis-criminate , where we first characterize the positive (minority) class alone, and then bring in the negative examples to form decision boundaries around each clusters of positives. This app roach has significant computational advantages, in that using just the minority class in the first step requires a small fraction of the data, assuming a high imbalance ratio. Also by creating decision boundaries locally in the second step, the number of examples involved in each classifier is smaller; further, creating each classifier separately allows computa-tions to be made parallel, though since the computation for each decision boundary is analytical, that may not be nec-essary for many datasets. The computation is analytical because there is a closed form solution for the placement of the decision boundary. Thus, the discriminate step becomes many parallel local analytical calculations. This is much simpler and scalable than, for instance, a decision tree that chooses splits greedily and fails to scale with dimension and large number of observations.

We make several experimental observations, namely that: box drawing classifiers become more useful as data imbal-ance increases; the approximate method performs at the top level of its competitors, despite the fact that it is restricted to producing interpretable results; and performance can be improved on the same datasets by using the mixed integer programming method.

After related work just below, we describe the advantages of our approach in Section 2. In Section 3, we introduce our two algorithms. Experimental results will be presented in Section 4. Section 4 provides a vignette to show how box drawing models can be interpretable. In Section 5, theoreti-cal generalization bounds will be presented for box drawing classifiers. Section 6 discusses possible approaches to make the MIP formulation more scalable.
Overviews of work on handling class imbalance problems include those of He and Garcia [11], Chawla, Japkowiz and Kolcz [7] and Qi [19]. Many works discuss problems caused by class imbalance [22, 16]. There are many avenues of research that are not directly related to the goal of inter-pretable imbalanced classification, specifically kernel and ac-tive learning methods [20, 23], and work on sampling [1, 6] that includes undersampling, oversampling, and data gener-ation, which can be used in conjunction with methods like the ones introduced here. We use a cost-sensitive learn-ing approach in our methods, similar to Liu and Zhou [14] and McCarthy et al . [15]. We note that many papers on imbalanced data do not experimentally compare their work with the cost-sensitive versions of decision tree methods. We choose to compare with other cost-sensitive versions of de-cision trees as our method is a cost-sensitive method.
There is some evidence that more complex approaches that layer different learning methods seem to be helpful for learning [20, 23], though the results would not be in-terpretable in that case. This, however, is in contrast with other views (e.g., [12]) that for most common datasets, sim-ple rules exists and we should explore them.
 The works most similar to ours are that of the Patient Rule Induction Method (PRIM) [9] and decision tree meth-ods for imbalanced classification (e.g., [13]), as they parti-tion the input space like our work. Approaches that par-tition space tend to recover simple decision rules that are easier for people to understand. Decision tree methods are composed using greedy splitting criteria, unlike our meth-ods. PRIM is also a greedy method that iteratively peels off parts of the input space, though unfortunately we found it to be extremely slow -as described by Sniadecki [21],  X  X RIM is eloquently coined as a patient method due to the slow, step-wise mechanism by which it processes the data. X  X either our Exact Boxes nor Fast Boxes methods are greedy methods, though Fast Boxes makes a different type of approximation, which is to characterize before discriminating. As discussed by Raskutti [20], one-class learning can be useful for highly imbalanced datasets -our characterization step is a one-class learning approach.
We start with the mixed-integer programming formula-tion, which acts as our gold standard for creating box draw-ing classifiers when solved to optimality.
For box drawing classifiers, a minority class (positive) ex-ample is correctly classified only if it resides within at least one box. A majority class (negative) example is correctly classified if it does not reside in any box. We are given training examples { ( x i , y i ) } m i =1 , x i  X  R n , y introduce some notation in Table 1 that we will use through-o ut this subsection. We use this notation from here on. Table 1: Notation for Box Drawings with Mixed Inte ger Programming
The Exact Boxes method solves the following, where the hypothesis space F is the set of box drawings (unions of axis parallel rectangles), where f  X  F has f : R n  X  { X  1 , 1 } . max The objective is a weighted accuracy of positives and neg-atives, regularized by the number of boxes. This way, the number of boxes is not fixed, and a smaller number of clus-ters is preferred (analogous to nonparametric Bayesian mod-els where the number of clusters is not fixed). Our gold standard will be the minimizer of this objective. We now derive the MIP that computes this minimizer.

If i  X  S + , the definitions of e l ijk , e u ijk , w ik , and z to the following constraints: which say that x ij need to be at least margin v away from the lower (resp. upper) boundary of the box in order for e l ijk = 1 (resp. e u ijk = 1). Further, our definitions give rise also to which says that for example i to be in box k , all of the e u and e l ijk are 1 for box k . We also have, still for i  X  S that the example must be in one of the boxes in order to be classified correctly, that is: Continuing this same type of reasoning for i  X  S  X  , the def-constraints: By setting M to be a large positive constant and setting  X  to be a small positive number (to act as a strict inequality), we now have the following formulation:
Here, (5) and (6) are derived from (1), (7) and (8) are derived from (2), (9) and (10) are derived from (3), (11) and (12) are derived from (4), equations (13)-(20) are derived analogously for S  X  . The last constraint (21) is to make sure that the solution that we obtain is not degenerate, where the lower boundary is above the upper boundary. In practice, M should be chosen as a fixed large number and  X  should be chosen as a fixed small number based on the representation of numbers in the computing environment.

In total, there are O ( mnK ) equations and O ( mnK ) vari-ables, though the full matrix of variables corresponding to the mixed integer programming formulation is sparse since most boxes operate only on a small subset of the data. This formulation can be solved efficiently for small to medium sized datasets using MIP software, producing a gold stan-dard box drawing classifier for any specific number of boxes (determined by the regularization constant). The fact that Exact Boxes produces the best possible function in the class of box drawing classifiers permits us to evaluate the quality of Fast Boxes, which operates in an approximate way on a much larger scale.
Fast Boxes uses the approach of characterize then discrim-inate . In particular, we hypothesize that the data distribu-tion is such that the positive examples cluster together rel-ative to the negative examples. This implies that a reason-able classifier might first cluster the positive examples and then afterwards discriminate positives from negatives. The discrimination is done by drawing a high dimensional axis-parallel box around each cluster and then adjusting each boundary locally for maximum discriminatory power. If the cluster assumption about the class distributions is not cor-rect, then Fast Boxes could have problems, though it does not seem to for most real imbalanced datasets we have found, as we show in the experiments. Fast Boxes has three main stages as follows. 1. Clustering stage: Cluster the minority class data into 2. Dividing space stage: The input space of the data is 3. Boundary expansion stage: Each boundary is expanded
Details of each stage are provided below.
In the clustering stage, the minority class data are clus-tered into K clusters. Since this step involves only the mi-nority class data, it can be performed efficiently, particularly if the data are highly imbalanced. Cross-validation or other techniques can be used to determine K . In our experiments, we used the basic k -means algorithm with Euclidean dis-tance. Other clustering techniques or other distance metrics can be used.

After the minority class data are separated into small clus-ters, we construct the smallest enclosing parallel axes rect-angle for each cluster. The smallest enclosing parallel axes rectangle can be computed by taking the minimum and max-imum of the minority class data in each cluster and for each feature. Let l s,j,k and u s,j,k denote the lower boundary and upper boundary for the j -th dimension, for the k -th cluster. Here the subscript s is for  X  X tarting X  boundary, and in the next part we will created a  X  X evised X  boundary which will be given subscript r . The  X  X inal X  boundary will be given subscript f . Define the set X l,j,k as follows: X These are the data points that will be used to adjust the lower boundary of the j -th dimension of the k -th rectangle. Similarly, we let X , These are the training examples that will be used to deter-mine the upper boundary of the j -th dimension of the k -th rectangle.

Figure 2 illustrates the domain for X u,j,k to the right of the blue dashed line. Figure 2: The examples used to determine the right vertic al decision boundary are on the right side of the blue dotted line.

Note that this method is very parallelizable after the clus-tering stage. The dividing space stage computations can be done in parallel for each cluster, and for the boundary ex-pansion stage discussed below, each boundary of each box can be determined in parallel.
In this stage we discriminate between positives and nega-tives by creating a 1-dimensional classifier for each boundary of each box. We use a regularized exponential loss. Specif-ically, for lower boundary j of box k , We minimize the fol-lowing with respect to l r,j,k where l r,j,k refers to the lower boundary of the j -th dimension of k -th revised box being determined by the loss function: where c is the weight for the majority class, c &lt; 1, S set of positive examples in the k -th cluster, S k  X  is the set of examples not in the k -th cluster,  X  is a regularization param-eter that tends to expand the box, and  X  .  X  denotes max( ., 0). For simplicity, we use the same parameter to control the ex-pansion for all the clusters and all the features. Note that the term to give less weight to the points that are not directly oppo-site the box edges (the points that are diagonally away from the corners of the box). To explain these terms, recall that the exponential loss in classification usually operates on the term y i f ( x i ), where the value of f ( x i ) can be thought of as a distance to the decision boundary. In our case, for the lower decision boundary we use the perpendicular distance to the decision boundary | x j  X  l r,j,k | , and include the addi-tional distance in every other dimension p for the diagonal points. For the upper diagonal points we include the dis-tance to the upper boundary u s,p,k , namely x p  X  u s,p,k analogously for the points on the lower diagonal we include distance l s,p,k  X  x p . We perform an analogous calculation for the upper boundary.

Note that we perform a standard normalization of all fea-tures to be between -1 and 1 before any computation begins, which also mitigates numerical issues when dealing with the (steep) exponential loss. Another mechanism we use for avoiding numerical problems is by multiplying each term in the objective by exp(1) and dividing each term by the same factor. We will construct the derivation of the lower boundary as follows. We rewrite the objective to minimize: where Because of the factors of 1 added and subtracted in the ex-ponent, we ensure R l,j,k + is at least exp(  X  1) &gt; 0 . 3, avoiding numerical problems. From there, we can solve for l r,j,k by taking the derivative of the objective and equating it to zero. Then we multiply both sides of the resulting equation by exp ( l s,j,k  X  1  X  l r,j,k ) and solve a quadratic equation. The the result is below. The details have been omitted due to space constraints.
Proposition 1. If R l,j ,k  X  &gt; 0 , the solution to (24) is l .

If R l,j,k  X  = 0 or close to zero, which can happen when there are no points outside the smallest enclosing box in direction j , we set l r,j,k = l j where l j is the smallest value of feature j . I n that case, the boundary effectively disappears from the description of the classifier, making it more interpretable.
The interpretation of the proposition is that the boundary has moved from its starting position l s,j,k by amount 1  X 
Similarly, we let u r,j,k be the revised upper boundary of the j th dimension for the k -th revised box and it can be computed as follows.

Proposition 2. If R l,j,k  X  &gt; 0 , u where The proof and interpretation are similar to Proposition 1.
If R u,j,k  X  = 0 or close to zero, we set v = u j where u the largest possible value for feature j .

After we learn each of the decision boundaries, we per-form a final adjustment that accomplishes two tasks: (i) it ensures that the box always expands rather than contracts, (ii) further expands the box to  X  away from the nearest neg-ative example. This gives us final values l f,j,k and u f,j,k where subscript  X  f  X  is for final. Written out, this is: where  X  is a small number. The boxes always expand for this algorithm, which implies that this algorithm is meant for applications where correct classification of the minority class data is crucial in practice. This expansion step can be omitted if desired, for instance if misclassifying the negative examples is too costly.

The algorithm is summarized as follows: Input: number of boxes K , tradeoffs c and  X  , Data { x i Output: Boundaries of boxes. 1. Normalize the features to be between -1 and 1. 2. Cluster the minority class data into K clusters. 3. Construct the minimal enclosing box for each cluster, 4. Construct data for local classifiers X l,j,k and X u,j,k 6. Compute l r,j,k based on equation (27) and u r,j,k based 7. Perform expansion based on equations (31) and (32). 8. Un-normalize by rescaling the features back to get Note that after the clustering step on the minority class data, all the other steps are easily parallellizable.
Now that we have two very different algorithms for cre-ating box drawing classifiers, we will compare their perfor-mances experimentally.
 We chose to use the area under the convex hull of the ROC curve (AUH) [18] as our evaluation metric; it is frequently used for imbalanced classification problems and considers the full ROC curve (Receiver Operator Characteristic) curve to evaluate performance. To compute the AUH, we compute classifiers for various settings of the tradeoff parameter c , which controls the relative importance of positive and nega-tive classes. Each setting of c corresponds to a single point on the ROC curve, with a count of true and false positives. We compute the AUH formed by the points on the ROC curve, and normalize as usual by dividing it by the number of positive examples times the number of negative examples. The best possible result is an AUH of 1.
 For comparison, we consider logistic regression, SVM with radial basis kernel, CART, C4.5, Random Forests, AdaBoost (with decision trees), C5.0, and Hellinger Distance Decision Tree (HDDT) [8]. Most of these algorithms are listed among the top 10 algorithms in data mining [24]. Among these algo-rithms, only CART, C4.5, C5.0, and HDDT yield potentially interpretable models. HDDT uses Hellinger distance as the splitting criterion, which is robust and skew-insensitive. In addition to the baselines above, we implemented the Patient Rule Induction Method (PRIM) for  X  X ump hunting X  [9]. This method also partitions the input variable space into box shaped regions, but in a different way than our method. PRIM searches iteratively for sub-regions where the target variable has a maxima, and peels them off one at a time, whereas our clustering step finds maxima simultaneously.
The data sets we considered are listed in Table 2. Some data sets (castle, corner, diamond, square, flooded, castle3D, corner3D, diamond3D, flooded3D, flooded3D) are simulated data that are able to be visualized (made publicly available at [10]). The breast and pima data sets were obtained from the UCI Machine Learning Repository [3]. The data set fou rclass was obtained from LIBSVM [4]. The remaining imbalanced data sets were obtained from the KEEL (Knowl-edge Extraction based on Evolutionary Learning) imbal-anced data repository [2]. The Iris0 data set is an imbal-anced version of the standard iris data set, where two of the classes (iris-versicolor and iris-virginica) have been combined to form the majority class.
 Table 2: Summary of datasets used for experiments H ere we compare the performance of Fast Boxes with the baseline algorithms. For each algorithm (except C4.5) we set . . . , 1]. The other parameters were set in data-dependent way; for instance, for SVM with RBF kernel, the kernel width was chosen using the sigest function in the R pro-gramming language. The data were separated into 10 folds, where each fold was used in turn as the test set. We do not prune the decision trees beyond their built-in pruning as previous research shows that unpruned decision trees are more effective in their predictions on the minority class [17, 5], and because it would introduce more complexity that would be difficult to control for. Within the training set, for the Fast Boxes algorithm we used 3-fold cross-validation to select the cluster number and expansion parameter.
Table 5 shows the performances in terms of AUH means and standard deviations. The values that are bolded rep-resent the algorithms whose results are not statistically sig-nificantly different from the best algorithm using a matched pairs sign test with significance level  X  = 0 . 05. When there was more than one best-performing classifier, the one with the smaller standard deviation was chosen as the best per-former for that data set. Fast Boxes was often (but not always) one of the best performers for each dataset. This brings up several questions, such as: Under what conditions does Fast Boxes perform well? How do its parameters effect the result? Does it tend to produce trivial results? Can Ex-act Boxes improve upon Fast Boxes X  results in cases where it does not perform well? Are the results interpretable? These are questions we will address in the remainder of this section. We start with a partial answer to the question of when Fast Boxes performs well -it is when the classes are more imbalanced. Figure 3 shows a scatter plot of the quality of Fast Boxes X  performance versus the imbalance ratio of the dataset. The vertical axis represents our rank in perfor-mance among all of the algorithms we tested. The horizon-tal axis is the number of negatives divided by the number of positives. The performance of Fast Boxes changes from being among the worst performers when the data are not imbalanced (and the cluster assumption is false), to being among the best performers when the data are imbalanced. Figure 3: Ranking of Fast Boxes versus imbalance ratio of data
Below we provide some intuition about Fast Boxes X  clus-ters and the expansion parameter before answering the ques-tions posed just above.
 We expect that if our main modeling assumption holds, which is that the positive examples naturally cluster, there should be a single best number of clusters. If we choose the number of clusters too small, we might underfit, and if we allow too many clusters, we could overfit. Figure 4 il-lustrates the cluster assumption on the diamond3D dataset, where this effect of overfitting and underfitting can be seen.
The expansion parameter is also designed to assist with generalization. We would like our boxes to be able to capture more of the positive cluster than is provided by the tightest box around the training examples, particularly since true positives are worth more than true negatives in our objec-tive function. The exponential loss creates a discriminative classifier, but with a push outwards. Naturally, as we in-crease the expansion parameter, the training AUH will drop as more negative training examples are included within the box. On the other hand, the test AUH tends to increase Figure 4: The effect of the number of clusters on AUH for the data set diamond3D. Fast Boxes was run once for each number of clusters. Training AUH is reported as circles, and testing AUH as stars. before decreasing, as more positive examples are within the expanded box. This effect is illustrated in Figure 5. Figure 5: The effect of the expansion parameter on AUH for the diamond3D data set.

Considering the final expansion stage, Figure 6 illustrates why this stage is necessary. We visualize the iris0 dataset with dimension 1 and dimension 4, where if we had not expanded out to the nearest negative example, we would have missed a part of the positive distribution within the test set.
 When the data are highly imbalanced, we have found that some of the baseline algorithms for producing interpretable models often produce trivial models, that is, models that always predict a single class. This is true even when the weighting factor on the positive class is varied throughout its full range at a reasonably fine granularity. This means that either it is not possible to obtain a meaningful model for the dataset with that method, or it means one would need to work fairly hard in order to find a weighting factor that did not produce a trivial model; that is, the range for which nontrivial models are possible is very small. Figure 3 considers three interpretable methods we compare with, namely CART, C4.5, and C5.0. It shows the fraction of time these algorithms produce trivial models. For CART, C5.0, and Fast Boxes, the percentage was computed over 100 models computed over 10 splits and 10 options for the imbalance parameter. C4.5 does not have a built in imbal-ance parameter, so the percentage was computed over 10 splits. Figure 6: The red and yellow points are negative training p oints and testing point respectively, the blue and green points are positive training points and testing points respectively. If we had used the tightest decision boundary around the positive training examples, we would have missed part of the positive distribution.
 Table 3: Fraction of the time we get a trivial model. Bold indic ates values over 0.5.
 Since we know that Fast Boxes is competitive with other baselines for handling imbalanced data, we would like to know whether Exact Boxes has the potential to yield signifi-can t performance gains over Fast Boxes and other methods. We implemented the MIP using GUROBI on a quad core Intel i7 860 2.8 GHz, 8GB cache, processor with 4 cores with hyperthreading and 16GM of RAM. We first ran the Exact Boxes algorithm for 30 minutes, and if the AUH per-formance was not competitive and the optimality gap was above 1%, we ran it up to 90 minutes for each instance. We did not generally allow the MIP to solve to provable opti-mality. This has the potential to hinder performance, but as we were performing repeated experiments we needed to be able to solve the method repeatedly.

Table 4 shows results from Exact Boxes for several of the smaller data sets, along with the results from Fast Boxes for comparison. Bold font in this table summarizes results from the other baseline algorithms as well: if the entry is in bold, it means that the result is not statistically signifi-cantly different than the best out of all of the algorithms. Thus, for 5 out of 8 datasets we tried, the MIP was among the top performers. Further, the AUH value was substan-tially improved for some of the data sets. Thus, restricting the algorithm to produce a box drawing classifier does not generally seem to hinder performance.
 Table 4: Comparison of test data AUH of inter-pretable methods with Exact Boxes. Bold font in-cludes results from non-interpretable methods.

Note that it is time-consuming to perform cross-validation on the MIP, so the cluster number that we found using cross-validation for Fast Boxes was used for Exact Boxes. We provide a classifier we learned from the glass2 data set that predicts whether a particular glass is a building window that is non-float processed. The other types of glasses are building windows that are float processed, vehicle windows, containers, tableware, and headlamps. The attributes in-clude the refraction index as well as various indices for met-als. These metals include Sodium, Magnesium, Aluminum, Silicon, Potassium, Calcium, Barium, and Iron.

One of the predictive models from Fast Boxes is as fol-lows. To be a particular glass of a building window that is non-float processed: 1) The refractive index should be above 1.5161. 2) Magnesium index must be above 3.3301. 3) Aluminum should be below 1.7897. 4) Silicon should be below 73.0199. 5) Potassium should be below 0.6199. 6) Calcium should be between 8.3101 and 2.3741. 7) Barium should be below 2.6646. 8) Sodium and iron are not important factors.

We believe that this simple form of model would appeal to practitioners because of the natural threshold structure of the box drawing classifiers.
Statistical learning theory will allow us to provide a proba-bilistic guarantee on the performance of our algorithms. We will construct a uniform generalization bound, which holds over all box drawing classifiers with K boxes anchored at M different fixed values for each dimension, where K is fixed. We might choose M j as the count of numbers with at most a certain number of decimal places (say 2 decimal places) in between the largest and smallest possible values for a par-ticular feature. (Often in practice only 2 decimal places are used.) The main step in our proof is to count the number of possible box drawing classifiers. The set of all box drawing classifiers with up to K boxes, with l j and u j attaining the M j values, will be called F .
 Define the empirical risk to be the objective of Exact Boxes with no regularization, and let the true risk R true ( f ) be the expectation of this taken over the distribution that the data are drawn iid from.
Proposition 3. For all  X  &gt; 0 with probability at least 1  X   X ,  X  f  X  F , R To o utline the proof, there are construct a single box, since for each dimension, we select 2 values, namely the lower boundary l j and upper bound-ary u j . To construct multiple boxes, there are at most Q boxes matter. Since the order does not matter, we need to divide the term by K !. Note that this is an upper bound which is not tight since some boxes can be a proper subset or equal to another box. Although we are considering the set of all box drawing classifiers up to K boxes, it suffices to consider box drawing classifiers with exactly K boxes. This can be seen by supposing we constructed a classifier with l &lt; K boxes, and noting the same classifier can be constructed using K boxes by duplicating some boxes. We apply Hoeffding X  X  inequality and the union bound to com-plete the proof. From the experimental outcome, it is clear that Exact Boxes is indeed a competitive solution. The main challenge lies in its computational complexity. There are several ways one might make the MIP more practical: first, one could limit c omputation to focus only a neighborhood of the posi-tive data, and use the solution to this problem to warm start the MIP on the full problem. In that case we would consider only negative points that are close to the positive points in at least one dimension, which can be identified in a single pass through the negative examples. Alternatively, one can perform clustering first as in the Fast Boxes approach, and solve the MIP on each cluster. For each cluster, we would scan through each feature of the data in a single pass and keep only the data that are close to the mean of the cluster center to use in the MIP.
We have presented two new approaches to designing in-terpretable predictive models for imbalanced data settings. Exact Boxes is formulated as a mixed integer program, and acts as a gold standard interpretable modeling technique to compare with. It can be used for small to moderately sized problems. Fast Boxes uses a characterize-then-discriminate approach, and tends to work well when the minority class is naturally clustered (for instance when the clusters represent different failure modes of mechanical equipment). We illu-minated the benefits and limitations of our approaches, and hope that these types of models will be able to provide alter-native explanations and insights into imbalanced problems. In comparing Fast Boxes with gold standard interpretable techniques like Exact Boxes, and with many other methods, we can now judge the power of the class of interpretable models: it is interesting that such simple approaches can achieve comparable performance with even the best state-of-the-art techniques.
 Acknowledgements Funding for this work provided by Siemens. [1] N. Abe. Sampling approaches to learning from [2] J. Alcala-Fdez, A. Fernandez, J. Luengo, J. Derrac, [3] K. Bache and M. Lichman. UCI machine learning [4] C.-C. Chang and C.-J. Lin. LIBSVM: A library for [5] N. V. Chawla. C4.5 and imbalanced data sets: [6] N. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P. [7] N. V. Chawla, N. Japkowicz, and A. Kotcz. Editorial: [8] D. Cieslak, T. Hoens, N. Chawla, and W. Kegelmeyer. [9] J. Friedman and N. Fisher. Bump hunting in [10] S. T. Goh and C. Rudin. 2014. http: [11] H. He and E. Garcia. Learning from imbalanced data. [12] R. C. Holte. Very simple classification rules perform [13] N. Japkowicz. Class imbalances: Are we focusing on [14] X.-Y. Liu and Z.-H. Zhou. The influence of class [15] K. McCarthy, B. Zabar, and G. Weiss. Does [16] R. Prati, G. Batista, and M. Monard. Class [17] F. Provost and P. Domingos. Tree induction for [18] F. Provost and T. Fawcett. Robust classification for [19] Y. Qi. A brief literature review of class imbalanced [20] B. Raskutti and A. Kowalczyk. Extreme re-balancing [21] J. Sniadecki. Bump hunting with sas: A macro [22] G. M. Weiss. Mining with rarity: A unifying [23] G. Wu and E. Y. Chang. Class-boundary alignment [24] X. Wu, V. Kumar, Ross, J. Ghosh, and et al. Top 10
