 Many machine learning problems, such as image processing and text related min-ing, involve high dimensional data. Such data are often inefficiently processed in their original input spaces. The data can be projected to lower dimensional spaces so that they can be processed mor e efficiently in terms of computational time and space. Furthermore, hidden structures or features embedded in data can also be revealed. For example, laten t semantic analysis (LSA)[5] in docu-ment analysis can capture synonymy information which is useful in document retrieving.

Conventionally, dimensionality reduction is done with LSA, PCA [9], proba-bilistic LSA [7], or probabilistic PCA [14]. These algorithms have two common features: they are unsupervised and the derived low dimensional space is usually corresponding to the directions of the original input space where the attributes of data have large variance. There are some other unsupervised approaches such as non-negative matrix factorization (NMF) [11]. These are classical unsupervised approaches in dimensionality reduction.

When label information of observations is available, dimensionality reduction should be guided by label information so that a better low dimensional space can be constructed to reflect this information. Linear discriminant analysis (LDA, Fisher X  X  LDA) [13] can be used to produce a low dimensional space where the intra-class compactness and inter-class separation are accounted. However, it cannot incorporate unlabeled data in cases that labeled data are rare and ex-pensive. Even semi-supervised LDA [3] is proposed to take the advantage of unlabeled data by incorporating the unlabeled data to regularization of LDA, this approach is still not suitable for applications such as domain adaptation [2], [1] where intra-covariance of data of different domains are highly concerned rather than inter-class separation or intra-class compactness considered in LDA. Some other related work including [10] also consider supervised dimensionality reduction from other perspectives.

Recently, supervised probabilistic PCA (SPPCA) [14] and its extension semi-supervised probabilistic PCA (S 2 PPCA) [14] were proposed. In these approaches, the observed data, including its target values, and the latent variables are usu-ally assumed to be linear dependant (with isotropic Gaussian noise) and hence the target values are treated as real values to be regressed. This assumption does not match the classification problems whe re target values are discrete and the loss should be the hinge loss. In this paper, we propose a classification proba-bilistic PCA (CPPCA) in which the class labels of training data are transformed to class conditional probabilities by using a sigmoidal function and thus hinge loss is implicitly employed. The introduction of this nonlinear function causes the model a little bit complex because the posterior probability distributions of latent variables are no longer Gaussian and their exact derivation and fur-ther processing are very difficult. We propose using Laplace approximation with expectation maximization (EM) to approximate these posterior distributions. One of the applications of CPPCA is to tackle domain adaptation problems. Details will be given in Section 3. The rest of this paper is organized as following. In Section 2, the original probabilistic PCA is briefly reviewed and then followed by the formulation of CPPCA. Section 3 gives experimental results of CPPCA when applied to a domain adaptation problem. Finally, conclusions are presented in Section 4. We follow the notation and style of [7] and [14], if possible, so that readers can follow all the work easily. Consider N L labeled observations and N U unlabeled observations denoted by M dimensional vectors { x n } N L + N U n =1 , x n  X   X   X  R M . of latent variables) are denoted by z  X  X  X  R K ,a K dimensional space with K&lt;M .
 2.1 Probabilistic PCA (PPCA) Revisited In PPCA, dimensionality reduction is unsupervised and each observation x is generated as where W x is a M  X  K matrix and  X  x  X  X  M . The vector z is Gaussian distributed with zero mean and unit variance, z  X  X  (0 , I ). In (1), z plays as the vector of latent variables and noise x is assumed isotopic and Gaussian, i.e. x  X  N (0 , X  x 2 I ) in which the noise level depends on  X  x . The parameters of the model thus are  X  = { W x , X  x , X  2 x } .Thevaluesof  X  are optimized corresponding to the maximum log likelihood of all x which can be found by EM or computed explicitly. We quote the EM approach here because this approach will be used subsequently in later subsections. In the E-step, the posterior distribution of z n is given by where M = W T x W x +  X  2 x I . Hence, the expectation of p osterior distribution of z n given x n is and the expectation &lt; z n z T n | x n &gt; can be obtained by where &lt;  X  &gt; denotes the expectation operator over the posterior distribution of z .
 is assumed that the observations x n have zero empirical mean, subtracting the empirical mean if not. In t he M-step, parameters in  X  are updated to and  X  denotes the L 2 norm, and tr(  X  ) denotes the matrix t race. Iteration on EM will eventually gives converged parameters. 2.2 Classification Probabilistic PCA (CPPCA) In this subsection, we consid er the scenario that there are N L + N U observations with the first N L are labeled with y i  X  X  0 , 1 } where i =1 ,...,N L .Inorderto incorporate the label information, the vector of latent variables z i not only be responsible to the generation of the observations x i but also effect the discrete values y i . For unlabeled observations, x j ,where j = N L +1 ,...,N L + N U ,is generated the same as that in the previously described PPCA. Since y i is discrete while z i is real valued, we propose using the sigmoidal function to transform the 1 | z i ). As a result, the CPPCA generates x and y as follows. where x  X  X  (0 , X  2 x I ), v T  X  R K is the transpose of the parameter vector v of the sigmoidal function Thus the parameters of the mode l described by (8) and (9) are  X  = { also N (0 , I ). The posterior of z depends on x ,and y if y is given. By applying the Bayes X  theorem to the labeled observations, From the model equations, x and y are independent given z , hence, For unlabeled observations, By referring to (2), p ( z n | x n )= N (  X  z | x , X  z | x ), where and and not Gaussian and hence p ( z n | x n ,y n ) is not Gaussian. We propose using the Laplace approximation to approximate p ( z n | x n ,y n ) as Gaussian. With this ap-proach, the approximated Gaussian mean is given by the maximum a posteriori (MAP) of the product of terms in the right hand side of (12) and the covariance matrix is given by the negative inverse of the Hessian matrix at MAP.
Let L denotes the logarithm of the right hand side terms of (12). After substituting (14) and (15) into L and dropping terms independent of z n ,we get L Since the quadratic term (including the negative sign) in the above equation is concave and logarithm of a sigmoidal function is concave as well, L is concave and has a maximum. We use the Newton-Raphson method to find the maximum. Gradient of L and the Hessian matrix are given by where The Newton-Raphson iteration equation is then Having found the maximum posterior z  X  n , the Laplace approximation to p ( z n | x n ,y n ) as Gaussian is given by 2.3 EM Learning for CPPCA The parameters in  X  of CPPCA given by (8) and (9) can be learnt by EM. The E-step involves in finding the posterior distribution of z n for each observation, observations with and without labels. Since all posterior distributions are now Gaussian, they can be identified by their own sufficient statistics, &lt; z n &gt; and unlabeled observations, and (21) for labeled observations. &lt; z n z T n &gt; = In the M-step, parameters in  X  can be updated by maximizing the expectation by the following equation, By setting partial derivative with respect to parameters W x , X  x , X  2 x in  X  to zero, the following update equations are obtained. and unlabeled observations as indicated in the subscript. For updating v T in the M-step, setting the derivative of &lt; X  &gt; with respect to v T to zero results in a nonlinear equation in v T .
 Again, this equation can be solved easily with Newton-Raphson method. The Jacobian matrix J of left hand side of (28) is and hence the iteration equation for v T is given by This EM iteration continues until the change of  X  between two consecutive EM iterations below a user specified thres hold. After the EM learning processes, arrival of unseen unlabeled observation  X  x will be mapped to latent variable  X  z having Gaussian distribution identified by (14) and (15). For an unseen labeled observation (  X  x ,  X  y ),  X  z is given by (21). In this section, we describe the application of the CPPCA to domain adaptation of classification problems. Traditionally, many statistical learning algorithms as-sume that training data and test data come from an identical and independent distribution (iid). Unfortunately, this assumption is often inappropriate. Owing to many factors, such as sampling techniques, time changes, etc., training data and test data may have different statistical properties. In domain adaptation problems, the difference is due to training data and test data coming from dif-ferent but related domains. This problem is very common in natural language processing. For example, we may have a large corpus from one domain and large effort and resources has been spent on annotating it. We would like to use the results to analyze corpora coming from other domains.

With training data and test data coming from different domains, learning a classifier such as support vector machine (SVM) [4] with the training data and classifying the test data may have degarded results. Sample re-weighting [15] is one of the popular approaches for domain adaptation. For example, Huang [8] found the weights for instances of samples by minimizing the maximum mean discrepancy (MMD) [6]. This approach, however, cannot be directly applied to high dimensional data with sparse attributes. Some attributes may appear only in training data while some others appear only in test data and hence minimiz-ing MMD is not effective. Another approach for this problem is to project both training data and test data to a common feature structure [1] or a low dimen-sional latent space followed by minimizing the MMD or embed this procedure in finding the low dimensional space as proposed by Pan [12]. These two ap-proaches actually assume that there is a common space where the discrepancy of distributions of training and test data is minimized.

An intuitive approach for domain adaptation without minimizing MMD ex-plicitly is to stack the training and test data together and then perform a prob-abilistic PCA to find the common latent random variables that generate both training and test data. Since now all the data are assumed to be generated from one set of latent random variables, the MMD is automatically minimized in the latent space formed by these variables. The success of this approach depends on whether the stacked data can be represented by these random variables with low error. For domain adaptation of classification task, class labels are usually available. This information can be incorporated by the proposed CPPCA rather than probabilistic PCA so that the low dimensional space found is prevalent to the task. We tested this approach with some experiments. Owing to the limited space, we report the one having larger scale. 3.1 Product Review Adaptation Experiment This experiment deals with an Amazon product review dataset 1 [1] of four dif-ferent types of products: books, DVDs, electronic appliances, and kitchen ap-pliances. Each review has a rating, 0 -5 stars, given by a reviewer. In the experiment, reviews with more than three stars were labeled as positive reviews and those with less than three stars were regarded as negative reviews. Three stars reviews were discarded because their sentiments were ambiguous. Unigram and bigram features were used to form the bag-of-words representation. Since, there were over a million of features, we simply removed less frequent words and then removed reviews with zero words so that 5834 features were left. Table 1 gives a brief summary for this dataset.

In order to compare our results with that of Blitzer et al. [1], we followed their experimental setup. For each domain, 1600 labeled patterns were selected as the training set, and the rest, about 400 labeled patterns, formed the test set. The label information of the test se t was solely used for accuracy evalua-tion and would not be used by any algorithms. The test set and the unlabeled patterns of the domain would be mixed to form the unlabeled set. The goal of the experiment was to try to use the training set of one domain to predict the labels of the test sets of other domains a s accurately as possible. For example, in a domain adaptation of Books domain to DVDs domain, 1600 labeled patterns of book reviews formed the labeled training set. All the unlabeled patterns of book reviews (i.e. 4445 book patterns) were mixed with 392 labeled patterns of DVDs to form the unlabeled set and the task was try to predict the labels of these 392 DVDs patterns accurately. The first part of this experiment is to study the performance of in-domain classification, both the training set and test set coming from the same domain. The data was firstly projected to a 100 dimen-sional space with different projection algorithms and followed by classification done with a logistic classifier. The baseline was the accuracy obtained with a logistic classifier working on the original input space. Results are illustrated in Table 2. Classification accuracies shown i n the table are the means obtained with five-fold cross-validation.

The results in Table 2 show that classification of the dataset in a projected lower dimensional space has some accuracy loss, compared to the baseline logis-tic classifier. PPCA and S 2 PPCA have nearly the same results indicating that the label information of the training set is not properly exploited by S 2 PPCA. On the other hand, CPPCA has better performance because the hinge loss is used. CPPCA has accuracy similar to the baseline logistic classifier. This implies almost all the required classification information is preserved during projections. For the Books domain and DVDs domain, CPPCA even has better accuracy. This is possible because there are additional unlabeled patterns involved in the projections with CPPCA.

The second part of the experiment is concerned with domain adaptation, training set and test set coming from different domains. The results are shown in Figure 1 and 2. In these figures, the thick horizonal line indicates the accuracy of in-domain classification for reference. The name of domains are abbreviated as  X  X  X  for Books,  X  X  X  for DVDs,  X  X  X  for Electronic appliances, and  X  X  X  for Kitchen appliances. Moreover,  X  X   X  D X  means domain adaptation from the Books domain to the DVDs domain, and so on.

From the figures, projection with PPCA and S 2 PPCA followed by logistic clas-sification has only little advantage over classification in the original input space. CPPCA works better. Comparing with the baseline classification, domain adap-tation with CPPCA has significant improvements, about 0.03-0.06 increase in accuracy, for D  X  B, E  X  B, E  X  DandB  X  E. Overall average increase in accu-racy is 0.022. Although the average improvement seems small, the performance of CPPCA is not bad. Firstly, the Kitchen appliances domain and the Electronic appliances domain are very close to each other. It is because many appliances in a kitchen are electronic appliances. As a result, classification accuracy of K  X  Eand E  X  K are quite close to the in-domain accuracy even no domain adaptation. The Books domain and the DVDs domain are also believed to be close to each other.
Finally, we compare our CPPCA results with the quoted results of Blitzer X  X  work [1], which employed the structural correspondence learning (SCL) [2] do-main adaptation algorithm and its variant SCL-MI. Since SCL and SCL-MI used both the attributes of projected spaces and attributes of the original space, we augmented our CPPCA projected patterns with the original input attributes for fairness. The results are illustrated in Figure 3 and 4.

Comparing to baseline accuracy, the average improvements are 0.023, 0.035, and 0.029 for SCL, SCL-MI, and augment ed CPPCA, respectively. The accuracy of SCL-MI for K  X  E exceeds in-domain accuracy because unlabeled patterns were used in SCL-MI while they were ignored in in-domain classification. By comparing the results of CPPCA in Figure 1 and 2 and augmented CPPCA in Figure 3 and 4, it can be seen that augmented CPPCA has average accuracy gain 0.007 only. This shows that CPPCA preserves almost all useful information in projection, and augmenting the original input attributes to it does not benefit much. As a result, CPPCA is very suitable for dimensionality reduction in cases that maintaining classification accuracy is important. In above experiments, we only projected the patterns to 100 dimensional spaces. It is because the perfor-mance of CPPCA for this dataset is quite stable when the projected dimension is above 100. Many data mining problems are high dimensional. Projecting the data to a much lower dimensional space enables the saving of temporal and spatial cost. Classical unsupervised dimensionality reduction algorithms such as PPCA and NMF cannot utilize label information dimensionality reduction. In this paper, we propose CPPCA to incorporate discrete label information of a classification task in deriving a low dimensional projected space. By stacking the training data and test data followed by using CPPCA, domain adaptation can be achieved at low dimensional latent spaces. Experimental results show that this approach has performance advantage over PPCA and S 2 PPCA in terms of accuracy. In-domain experiments also show that nearly all useful classification information can be preserved during projection with CPPCA. Finally, we would like to mention that domain adaptation is not the only application for CPPCA. It can be applied to other problems where PPCA can be applied.
 This work is partially supported by HKBU research grant FRG2/08-09/103.
