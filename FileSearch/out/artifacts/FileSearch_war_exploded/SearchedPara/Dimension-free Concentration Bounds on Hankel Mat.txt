 Many applications in natural language processing, text analysis or computational biology require learning prob-abilistic models over finite variable-size strings such as probabilistic automata, Hidden Markov Models (HMM), or more generally, weighted automata. Weighted automata exactly model the class of rational series, and their al-gebraic properties have been widely studied in that con-text (Droste et al., 2009). In particular, they admit algebraic representations that can be characterized by a set of finite-dimensional linear operators whose rank corresponds to the minimum number of states needed to define the automaton. From a machine learning perspective, the objective is then to infer good estimates of these linear operators from finite samples. In this paper, we consider the problem of learning the linear representation of a weighted automaton, from a finite sample, composed of variable-size strings i.i.d. from an unknown target distribution.
 Recently, the seminal papers of Hsu et al. (2009) for learning HMM and Bailly et al. (2009) for weighted au-tomata, have defined a new category of approaches -the so-called spectral methods -for learning distributions over strings represented by finite state models (Siddiqi et al., 2010; Song et al., 2010; Balle et al., 2012; Balle &amp; Mohri, 2012). Extensions to probabilistic models for tree-structured data (Bailly et al., 2010; Parikh et al., 2011; Co-hen et al., 2012), transductions (Balle et al., 2011) or other graphical models (Anandkumar et al., 2012c;b;a; Luque et al., 2012) have also attracted a lot of interest. Spectral methods suppose that the main parameters of a model can be expressed as the spectrum of a linear operator and estimated from the spectral decomposition of a matrix that sums up the observations. Given a rational series r , the values taken by r can be arranged in a matrix H r whose rows and columns are indexed by strings, such that the lin-ear operators defining r can be recovered directly from the right singular vectors of H r . This matrix is called the Han-kel matrix of r .
 In a learning context, given a learning sample S drawn from a target distribution p , an empirical estimate H S of H p built and then, a rational series  X  p is inferred from the right singular vectors of H S . However, the size of H S increases drastically with the size of S and state of the art approaches consider smaller matrices H U,V S indexed by limited subset of strings U and V . It can be shown that the above learn-ing scheme, or slight variants of it, are consistent as soon as the matrix H U,V r has full rank (Hsu et al., 2009; Bailly, 2011; Balle et al., 2012) and that the accuracy of the in-ferred series is directly connected to the concentration dis-tance || H U,V S  X  H U,V p || 2 between the empirical Hankel ma-trix and its mean (Hsu et al., 2009; Bailly, 2011). On the one hand, limiting the size of the Hankel matrix avoids prohibitive calculations. Moreover, most existing concentration bounds on sum of random matrices depend on their size and suggest that || H U,V S  X  H U,V p || 2 may be-come significantly looser with the size of U and V , com-promising the accuracy of the inferred model.
 On the other hand, limiting the size of the Hankel ma-trix implies a drastic loss of information: only the strings of S compatible with U and V will be considered. In order to limit the loss of information when dealing with restricted sets U and V , a general trend is to work with other functions than the target p , such as the prefix func-tion p ( u ) = P v  X   X   X  p ( uv ) or the factor function P v,w  X   X   X  p ( vuw ) (Balle et al., 2013; Luque et al., 2012). These functions are rational, they have the same rank as p , a representation of p can easily be derived from representa-tions of p or contained in the learning sample.
 A first contribution is to provide a dimension free concen-sults on tail inequalities for sum of random matrices show-ing that restricting the dimension of H is not mandatory. However, these results cannot be directly applied to the prefix and factor series, since the norm of the correspond-ing random matrices are unbounded. A second contribu-tion of the paper is to define two classes of parametrized functions, p  X  and ates between p and p (resp. dimension-free concentration bounds for these classes. These bounds are evaluated on a benchmark made of 11 problems extracted from the PAutomaC challenge (Verwer et al., 2012). These experiments show that the bounds de-rived from our theoretical results are quite tight -compared to the exact values-and that they significantly improve ex-isting bounds, even on matrices of fixed dimensions. These results have two practical consequences for spec-tral learning: (i) the concentration of the empirical Hankel matrix around its mean does not highly depend on its di-mension and the only reason not to use all the information contained in the sample should only rely on computing re-sources limitations. In that perspective, using random tech-niques to perform singular values decomposition on huge Hankel matrices should be considered (Halko et al., 2011); (ii) by constrast, the concentration is weaker for the pre-fix and factor functions, and smoothed variants should be used, with an appropriate parameter.
 The paper is organized as follows. Section 2 introduces the main notations, definitions and concepts. Section 3 presents a first dimension free-concentration inequality for the standard Hankel matrices. Then, we introduce the pre-fix and the factor variants and provide analogous concentra-tion results. Section 4 describes some experiments before the conclusion presented in Section 5. 2.1. Singular Values, Eigenvalues and Matrix Norms Let M  X  R m  X  n be a m  X  n real matrix. The singular values of M are the square roots of the eigenvalues of the matrix M T M , where M T denotes the transpose of M :  X  max ( M ) and  X  min ( M ) denote the largest and smallest singular value of M , respectively.
 In this paper, we mainly use the spectral norms || X || k in-duced by the corresponding vector norms on R n and de- X  || M || 2 =  X  max ( M ) .
 We have: || M || 2  X  p || M || 1 || M ||  X  .
 These norms can be extended, under certain conditions, to infinite matrices and the previous inequalities remain true when the corresponding norms are defined. 2.2. Rational stochastic languages and Hankel matrices Let  X  be a finite alphabet. The set of all finite strings over  X  is denoted by  X   X  , the empty string is denoted by , the length of string w is denoted by | w | and  X  n (resp.  X   X  n denotes the set of all strings of length n (resp.  X  n ). For any string w , let Pref( w ) = { u  X   X   X  | X  v  X   X   X  w = uv } . A series is a mapping r :  X   X  7 X  R . A series r is convergent if the sequence r ( X   X  n ) = P w  X   X   X  n r ( w ) is convergent; its limit is denoted by r ( X   X  ) . A stochastic language p is a probability distribution over  X   X  , i.e. a series taking non negative values and converging to 1.
 Let n  X  1 and M be a morphism defined from  X   X  to M ( n ) , the set of n  X  n matrices with real coefficients. For all u  X   X   X  , let us denote M ( u ) by M u and  X  x  X   X  M x M
 X  . A series r over  X  is rational if there exists an inte-ger n  X  1 , two vectors I,T  X  R n and a morphism M :  X   X  7 X  M ( n ) such that for all u  X   X   X  , r ( u ) = I T M u The triplet  X  I,M,T  X  is called an n -dimensional linear rep-resentation of r . The vector I can be interpreted as a vector of initial weights, T as a vector of terminal weights and the morphism M as a set of matrix parameters associated with the letters of  X  . A rational stochastic language is thus a stochastic language admitting a linear representation. Let U,V  X   X   X  , the Hankel matrix H U,V r , associated with a series r , is the matrix indexed by U  X  V and defined by H U,V r [ u,v ] = r ( uv ) , for any ( u,v )  X  U  X  V . If U = V =  X   X  , H U,V r , simply denoted by H r , is a bi-infinite matrix. In the following, we always assume that  X  U and that U and V are ordered in quasi-lexicographic order: strings are first ordered by increasing length and then, ac-cording to the lexicographic order. It can be shown that a series r is rational if and only if the rank of the matrix H is finite. The rank of H r is equal to the minimal dimension of a linear representation of r .
 Let r be a non negative convergent rational series and let  X  I,M,T  X  be a minimal d -dimensional linear representation of r . Then, the sum I d + M  X  + ... + M n  X  + ... is convergent and r ( X   X  ) = I T ( I d  X  M  X  )  X  1 T where I d is the identity matrix of size d .
 Several convergent rational series can be naturally associ-ated with a stochastic language p :  X  p , defined by p ( u ) = P v  X   X   X  p ( uv ) , the series associ- X  string begins with u , but that in general, the probability that a string contains u as a substring. If  X  I,M,T  X  is a minimal d -dimensional linear representa-tion of p , then  X  I,M, ( I d  X  M  X  )  X  1 T  X  (resp.  X  [ I M
 X  )  X  1 ] T ,M, ( I d  X  M  X  )  X  1 T  X  ) is a minimal linear repre-sentation of p (resp. of these variants of p can be reconstructed from the others. For any integer k  X  1 , let
S Clearly, p ( X   X  ) = S (1) p = 1 , p ( X   X  ) = S (2) p and average length of a string drawn according to p . Let U,V  X   X   X  . For any string w  X   X   X  , let us define the for any ( u,v )  X  U  X  V . For any sample of strings S , let H b H For example, let S = { a,ab } , U = V = { ,a,b } . We have 2.3. Spectral Algorithm for Learning Rational Rational series admit a canonical linear representation de-termined by their Hankel matrix. Let r be a rational series of rank d and U  X   X   X  such that the matrix H U  X   X   X  r (de-noted by H in the following) has rank d .  X  For any string s , let T s be the constant matrix whose  X  Let E be a vector indexed by  X   X  whose coordinates  X  Let H = LDR T be a reduced singular value decom-Then,  X  R T E, ( R T T x R ) x  X   X  ,R T P  X  is a linear representa-tion of r (Bailly et al., 2009; Bailly, 2011; Balle et al., 2012). A quick proof can be found in (Denis et al., 2013). The basic spectral algorithm for learning rational stochastic languages aims at identifying the canonical linear represen-tation of the target p determined by its Hankel matrix H p Let S be a sample independently drawn according to p :  X  Choose sets U,V  X   X   X  and build the Hankel matrix  X  choose a rank d and compute a reduced SVD of  X  build the canonical linear representation Alternative learning strategies consist in learning p or ing the same algorithm, and then to compute an estimate of p . In all cases, the accuracy of the learned representation mainly depends on the estimation of R . The Stewart for-mula (Stewart, 1990) bounds the principle angle  X  between the spaces spanned by the right singular vectors of R and R According to this formula, the concentration of the Han-kel matrix around its mean is critical and the question of limiting the sizes of U and V naturally arises. Note that the Stewart inequality does not give any clear indication on the impact or on the interest of limiting these sets. In-deed, Weyl X  X  inequalities can be used to show that both the numerator and the denominator of the right part of the in-equality increase with U and V . Let p be a rational stochastic language over  X   X  , let  X  be a random variable distributed according to p , let U,V  X   X   X  and let Z (  X  )  X  R | U | X | V | be a random matrix. For instance, Z (  X  ) may be equal to H U,V  X  , H U,V  X  or b H U,V  X  . Concentration bounds for sum of random matrices can be used to estimate the spectral distance between the empir-ical matrix Z S computed on the sample S and its mean (see (Hsu et al., 2011) for references). However, most of classical inequalities depend on the dimensions of the ma-trices. For example, it can be proved that with probability at least 1  X   X  (Kakade, 2010): where N is the size of S , d is the minimal dimension of the matrix Z and || Z || 2  X  M almost surely. If Z = H U,V  X  then M = 1 ; if Z = H U,V  X  , M =  X ( D 1 / 2 ) in the worst case; if Z = b H U,V  X  , || Z || 2 is generally unbounded. These concentration bounds get worse with both sizes of the matrices. Coming back to the discussion at the end of Section 2, they suggest to limit the size of the sets U and V , and therefore, to design strategies to choose optimal sets. We then use recent results (Tropp, 2012; Hsu et al., 2011) to obtain dimension-free concentration bounds for Hankel matrices. More precisely, we extend a Bernstein bound for unbounded random matrices from (Hsu et al., 2011) to non symmetric random matrices by using the dilation principle (Tropp, 2012).
 Let Z be a random matrix, the dilation of Z is the symmet-ric random matrix X defined by and || X || 2 = || Z || 2 , tr ( X 2 ) = tr ( ZZ T ) + tr ( Z || X 2 || 2  X  Max ( || ZZ T || 2 , || Z T Z || 2 ) . We can then reformulate the result that we use as fol-lows (Denis et al., 2013).
 Theorem 1. Let  X  1 ,..., X  N be i.i.d. random variables, and for i = 1 ,...,N , let Z i = Z (  X  i ) be i.i.d. matrices and X i the dilation of Z i . If there exists b &gt; 0 , X  &gt; 0 , and k &gt; 0 such that E [ X 1 ] = 0 , || X 1 || 2  X  b, || E  X  2 and tr ( E ( X 2 Pr We will then make use of this theorem to derive our new concentration bounds. Section 3.1 deals with the standard case, Section 3.2 with the prefix case and Section 3.3 with the factor case. 3.1. Concentration Bound for the Hankel Matrix H U,V p Let p be a rational stochastic language over  X   X  , let S be a sample independently drawn according to p , and let U,V  X   X   X  . In this section, we compute a bound on || H U,V S  X  H U,V p || 2 which is independent from the sizes of U and V and holds in particular when U = V =  X   X  . Let  X  be a random variable distributed according to p , let Z (  X  ) = H U,V  X   X  H U,V p be the random matrix defined by Z u,v = 1  X  = uv  X  p ( uv ) and let X be the dilation of Z . Clearly, E ( X ) = 0 . In order to apply Theorem 1, it is nec-essary to compute the parameters b, X  and k . We first prove a technical lemma that will provide a bound on E ( X 2 ) . Lemma 1. For any u,u 0  X  U , v,v 0  X  V , Proof.

E and The second inequality is proved in a similar way. Next lemma provides parameters b, X  and k needed to apply Theorem 1.
 Lemma 2. || X || 2  X  2 , E ( Tr ( X 2 ))  X  2 S (2) p and || E ( X 2 ) || 2  X  S (2) p .
 p ( uv ) |  X  1 + p ( u  X   X  )  X  2 . Therefore, || Z ||  X   X  2 . In a similar way, it can be shown that || Z || 1  X  2 . Hence, 2. For all ( u,u 0 )  X  U 2 : ZZ T [ u,u 0 ] = P v  X  V Z u,v Therefore, In a similar way, it can be proved that E ( Tr ( Z T Z ))  X  S and therefore, E ( Tr ( X 2 ))  X  2 S (2) p . 3. For any u  X  U , Hence, || ZZ T ||  X   X  S (2) p . It can be proved, in a sim-ilar way, that || Z T Z ||  X   X  S (2) p , || ZZ T || 1  X  S || Z T Z || 1  X  S (2) p . Therefore, || X 2 || 2  X  S (2) p We can now prove the main theorem of this section: Theorem 2. Let p be a rational stochastic language and let S be a sample of N strings drawn i.i.d. from p . For all t &gt; 0 , Proof. Let  X  1 ,..., X  N be N independent copies of  X  , let Z i = Z (  X  i ) and let X i be the dilation of Z i for i = 1 ,...,N . Lemma 2 shows that the 4 conditions of The-orem 1 are fulfilled with b = 2 , X  2 = S (2) p and k = 2 . This bound is independent from U and V . It can be noticed that the proof also provides a dimension dependent bound by replacing S (2) p with P ( u,v )  X  U  X  V p ( uv ) , which may re-sult in a significative improvement if U or V are small. 3.2. Bound for the prefix Hankel Matrix H U,V p The random matrix Z (  X  ) = H U,V  X   X  H U,V p is defined that || Z || 2 may be unbounded if U or V are unbounded: || Z || 2 =  X ( |  X  | 1 / 2 ) . Hence, Theorem 1 cannot be directly applied, which suggests that the concentration of Z around its mean could be far weaker than the concentration of Z . For any  X   X  [0 , 1] , we define a smoothed variant of p by Note that p 1 = p , p 0 = p and that p ( u )  X  p  X  ( u )  X  p ( u ) for any string u . Therefore, the functions p  X  are natural in-termediates between p and p . Moreover, when p is rational, each p  X  is also rational.
 Proposition 1. Let p be a rational stochastic language and let  X  I, ( M x ) x  X   X  ,T  X  be a minimal linear representation of p . Let T  X  = ( I d  X   X M  X  )  X  1 T . Then, p  X  is rational and  X  I, ( M x ) x  X   X  , T  X   X  is a linear representation of p Proof. For any string u , p  X  ( u ) = P n  X  0 I T M u  X  n I Note that T can be computed from T  X  when  X  and M  X  are known and therefore, it is a consistent learning strategy to learn p  X  from the data, for some  X  , and next, to derive p . For any 0  X   X   X  1 , let Z  X  (  X  ) be the random matrix defined by for any ( u,v )  X  U  X  V . It is clear that E ( Z  X  ) = 0 and we show below that || Z  X  || 2 is bounded if  X  &lt; 1 . any 0  X   X   X  1 and any k  X  1 , let Lemma 3. Proof. Indeed, let u  X  U .

X S When U and V are bounded, let l be the maximal length of a string in U  X  V . It can easily be shown that || Z  X  which holds even if  X  = 1 .
 Lemma 4. | E ( Z  X  [ u,v ] Z  X  [ u 0 ,v ]) |  X  p  X  ( u 0 u,u 0 ,v  X   X   X  .
 Lemma 5. Proof. Indeed, In the same way, Similar computations provide all the inequalities. Therefore, we can apply the Theorem 1 with b = 1 1  X   X  + S Theorem 3. Let p be a rational stochastic language, let S be a sample of N strings drawn i.i.d. from p and let 0  X   X  &lt; 1 . For all t &gt; 0 , Pr Remark that when  X  = 0 we find back the concentration bound of Theorem 2, and that Inequality 2 provides a bound when  X  = 1 . 3.3. Bound for the factor Hankel Matrix H The random matrix b Z (  X  ) = b H U,V  X   X  H || b
Z || 2 is generally unbounded. Moreover, unlike the prefix case, || b Z || 2 can be unbounded even if U and V are finite. Hence, the Theorem 1 cannot be directly applied either. We can also define smoothed variants of b p  X  ( u ) = which have properties similar to functions p  X  :  X  p  X   X  if  X  I, ( M x ) x  X   X  ,T  X  be a minimal linear representa-However, proofs of the previous Section cannot be directly extended to is often used in the proofs, while provides a tool which allows to bypass this difficulty. Lemma 6. Let 0 &lt;  X   X  1 . For any integer n , ( n + 1)  X  K  X  where Proof. Let f ( x ) = ( x + 1)  X  x . We have f 0 ( x ) =  X  ( x + 1) ln  X  ) and f takes its maximum for x M =  X  1  X  1 / ln  X  , which is positive if and only if  X  &gt; 1 /e . We have f ( x M ) = (  X  e X  ln  X  )  X  1 .
 Lemma 7. Let w,u  X   X   X  . Then, Proof. Indeed, if w = xuy , then | xy | = | w | X  X  u | and u appears at most | w | X  X  u | + 1 times as a factor of w . For  X   X  [0 , 1] , let b Z  X  (  X  ) be the random matrix defined by and, for any k  X  0 , let It can easily be shown that E ( b Z  X  ) = 0 , S ( k ) S It can be shown that || b Z  X  || 2 is bounded if  X  &lt; 1 . Lemma 8. (Denis et al., 2013) Eventually, we can apply the Theorem 1 with b = (1  X   X  ) Theorem 4. Let p be a rational stochastic language, let S be a sample of N strings drawn i.i.d. from p and let 0  X   X  &lt; 1 . For all t &gt; 0 , Remark that when  X  = 0 we find back the concentration bound of Theorem 2. We provide experimental evaluation of the proposed bounds in the next Section. The proposed bounds are evaluated on the benchmark of PAutomaC (Verwer et al., 2012) which provides sam-ples of strings generated from several probabilistic au-tomata, designed to evaluate probabilistic automata learn-ing. Eleven problems have been selected from that bench-mark for which sparsity of the Hankel matrices makes the use of standard SVD algorithms available from NumPy or SciPy possible. The size N of the samples is 20 , 000 ex-cept for the problem 4 where N = 100 , 000 . Table 1 shows some target properties of the selected problems: the size of the alphabets and the exact values of S ( k ) p computed for the different targets p . Figure 1 shows the typical behavior of S For each problem, the exact value of || H U,V S  X  H U,V is computed for sets U and V of the form  X   X  l , trying to maximize l according to our computing resources. It is compared to the bounds provided by Theorem 2 and Equa-tion (1), with  X  = 0 . 05 (Table 2). The optimized bound ( X  X pt. X ), refers to the case where  X  2 has been calculated over U  X  V rather than  X   X   X   X   X  (see the remark at the end of Section 3.1). Tables 3 and 4 show analog comparisons for the prefix and the factor cases with different values of  X  . Similar results have been obtained for all the problems of PautomaC. We can remark that our dimension-free bounds are significantly more accurate than the one provided by Equation (1). Notice that in the prefix case, the dimension-free bound has a better behavior in the limit case  X  = 1 than the bound from Eq. (1). This is due to the fact that in our bound, the term that bounds || Z || 2 appears in the term while it appears in the 1  X  Additional experiments confirm the implications of these results for spectral learning (see (Denis et al., 2013)). We have provided dimension-free concentration inequali-ties for Hankel matrices in the context of spectral learn-ing of rational stochastic languages. These bounds cover 3 cases, each one corresponding to a specific way to exploit the strings under observation, paying attention to the strings themselves, to their prefixes or to their factors. For the last two cases, we introduced parametrized variants which al-low a trade-off between the rate of the concentration and the exploitation of the information contained in data. A consequence of these results is that there is no a priori good reason, aside from computing resources limitations, to restrict the size of the Hankel matrices. This suggests an immediate future work consisting in investigating recent random techniques (Halko et al., 2011) to compute singu-lar values decomposition on Hankel matrices in order to be able to deal with huge matrices. Then, a second aspect is to evaluate the impact of these methods on the quality of the models, including an empirical evaluation of the behavior of the standard approach and its prefix and factor exten-sions, along with the influence of the parameter  X  . Another research direction would be to link up the prefix and factor cases to concentration bounds for sum of random tensors and to generalize the results to the case where a fixed number  X  1 of factors is considered for each string. This work was supported by the French National Agency for Research (Lampada -ANR-09-EMER-007).
 Anandkumar, A., Foster, D.P., Hsu, D., Kakade, S., and
Liu, Y.-K. A spectral algorithm for latent dirichlet allo-cation. In Proceedings of NIPS , pp. 926 X 934, 2012a. Anandkumar, A., Hsu, D., Huang, F., and Kakade, S.
Learning mixtures of tree graphical models. In Proceed-ings of NIPS , pp. 1061 X 1069, 2012b.
 Anandkumar, A., Hsu, D., and Kakade, S.M. A method of moments for mixture models and hidden markov mod-els. Proceedings of COLT -Journal of Machine Learning Research -Proceedings Track , 23:33.1 X 33.34, 2012c. Bailly, R. M  X  ethodes spectrales pour l X  X nf  X  erence grammat-icale probabiliste de langages stochastiques rationnels . PhD thesis, Aix-Marseille Universit  X  e, 2011.
 Bailly, R., Denis, F., and Ralaivola, L. Grammatical in-ference as a principal component analysis problem. In Proceedings of ICML , pp. 5, 2009.
 Bailly, R., Habrard, A., and Denis, F. A spectral approach for probabilistic grammatical inference on trees. In Pro-ceedings of ALT , pp. 74 X 88, 2010.
 Balle, B. and Mohri, M. Spectral learning of general weighted automata via constrained matrix completion. In Proceedings of NIPS , pp. 2168 X 2176, 2012.
 Balle, B., Quattoni, A., and Carreras, X. A spectral learning algorithm for finite state transducers. In Proceedings of ECML/PKDD (1) , pp. 156 X 171, 2011.
 Balle, B., Quattoni, A., and Carreras, X. Local loss opti-mization in operator models: A new insight into spectral learning. In Proceedings of ICML , 2012.
 Balle, B., Carreras, X., Luque, F. M., and Quattoni, A.
Spectral learning of weighted automata: A forward-backward perspective. To appear in Machine Learning, 2013.
 Cohen, Shay B., Stratos, Karl, Collins, Michael, Foster, Dean P., and Ungar, Lyle H. Spectral learning of Latent-
Variable PCFGs. In ACL (1) , pp. 223 X 231. The Asso-ciation for Computer Linguistics, 2012. ISBN 978-1-937284-24-4.
 Denis, F., Gybels, M., and Habrard, A. Dimension-free Concentration Bounds on Hankel Matrices for Spectral Learning (Long Version). arXiv:1312.6282, 2013.
 Droste, M., Kuich, W., and Vogler, H. (eds.). Handbook of Weighted Automata . Springer, 2009.
 Halko, N., Martinsson, P.G., and Tropp, J.A. Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions. SIAM Rev. , 53(2):217 X 288, 2011.
 Hsu, D., Kakade, S.M., and Zhang, T. A spectral algorithm for learning hidden markov models. In Proceedings of COLT , 2009.
 Hsu, D., Kakade, S. M., and Zhang, T. Dimension-free tail inequalities for sums of random matrices. ArXiv e-prints , 2011.
 Kakade, S. Multivariate analysis, dimensionality reduction, and spectral methods. Lecture Notes (Matrix Concentra-tion Derivations), 2010.
 Luque, F.M., Quattoni, A., Balle, B., and Carreras, X.
Spectral learning for non-deterministic dependency pars-ing. In Proceedings of EACL , pp. 409 X 419, 2012.
 Parikh, A.P., Song, L., and Xing, E.P. A spectral algo-rithm for latent tree graphical models. In Proceedings of ICML , pp. 1065 X 1072, 2011.
 Siddiqi, S., Boots, B., and Gordon, G.J. Reduced-rank hid-den Markov models. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS-2010) , 2010.
 Song, L., Boots, B., Siddiqi, S.M., Gordon, G.J., and
Smola, A.J. Hilbert space embeddings of hidden markov models. In Proceedings of ICML , pp. 991 X 998, 2010. Stewart, G. W. Perturbation theory for the singular value decomposition. In SVD and Signal Processing II: Algo-rithms, Analysis and Applications , pp. 99 X 109. Elsevier, 1990.
 Tropp, J.A. User-friendly tail bounds for sums of random matrices. Foundations of Computational Mathematics , 12(4):389 X 434, 2012.
 Verwer, S., Eyraud, R., and de la Higuera, C. Results of the PAutomaC probabilistic automaton learning compe-tition. Journal of Machine Learning Research -Proceed-
