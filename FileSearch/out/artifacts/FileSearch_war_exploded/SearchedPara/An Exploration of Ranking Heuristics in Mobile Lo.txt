 Users increasingly rely on their mobile devices to search local entities, typically businesses, while on the go. Even though recent work has recognized that the ranking signals in mo-bile local search (e.g., distance and customer rating score of a business) are quite different from general Web search, they have mostly treated these signals as a black-box to ex-tract very basic features (e.g., raw distance values and rating scores) without going inside the signals to understand how exactly they affect the relevance of a business. However, as it has been demonstrated in the development of general information retrieval models, it is critical to explore the un-derlying behaviors/heuristics of a ranking signal to design more effective ranking features.

In this paper, we follow a data-driven methodology to study the behavior of these ranking signals in mobile local search using a large-scale query log. Our analysis reveals interesting heuristics that can be used to guide the exploita-tion of different signals. For example, users often take the mean value of a signal (e.g., rating) from the business result list as a  X  X ivot X  score, and tend to demonstrate different click behaviors on businesses with lower and higher signal values than the pivot; the clickrate of a business generally is sublinearly decreasing with its distance to the user, etc. Inspired by the understanding of these heuristics, we further propose different transformation methods to generate more effective ranking features. We quantify the improvement of the proposed new features using real mobile local search logs over a period of 14 months and show that the mean average precision can be improved by over 7%.
 H.3.3 [ Information Search and Retrieval ]: Miscella-neous Algorithms, Human Factors, Measurement Mobile local search, search log analysis, ranking heuristics
The wide availability of internet access on mobile devices, such as phones and personal media players, has allowed users to search and access Web information while on the go. Ac-cording to a recent report from comScore 1 , as of July 2011, there were over 234 million mobile users in U.S., with nearly 50 percent searching on their mobile devices. The availabil-ity of continuous fine-grained location information on these devices has enabled mobile local search, which employs user location as a key factor to search for local entities, to over-take a significant part of the query volume. Sohn et al. X  X  study [34] found that 38% of mobile information needs are local. This is also evident by recent reports by BIA/Kelsey which show that 30% of all search volume will be local in na-ture by 2015, as well as by the rising popularity of location-based search applications such as Google Local, Bing Local, and Yelp.

Even though mobile local search is similar to general Web search in that they both boil down to a similar problem of relevance/click prediction and result ranking, there are two fundamental differences and also challenges in developing effective ranking functions for mobile local search.
First, the ranking signals in mobile local search are quite different from general Web search. On the one hand, Web search handles a wide range of Web objects, particularly webpages, while mobile local search focuses mostly on rank-ing local businesses (e.g., restaurants). Therefore, special domain knowledge about the ranking objects in mobile local search could be exploited to improve ranking accuracy. For instance, businesses may receive ratings and reviews from their customers thanks to the Web 2.0 services, which have been shown to be useful signals for ranking businesses [3, 43]. On the other hand, local search users generally prefer businesses that are physically close to them; this is partic-ularly critical for mobile users who are on the go and their range of reach might be limited. For example, a user would be more likely to visit a restaurant within 1 kilometer than another one within 2 kilometers to get breakfast, if the two restaurants are similarly good on other aspects. The dis-tance between the result business and the user X  X  location has been recognized as an important ranking signal in mo-bile local search [17, 3, 26]. In fact, the customer rating score, the number of reviews, and the distance are all shown explicitly to users in the search result user interface of mo-bile local search, as shown in Figure 1, and therefore play an important role in influencing the user X  X  click decision. http://www.comscore.com/ http://www.biakelsey.com/ Properly studying and modeling how this information af-fects user click behavior is arguably the key to improving ranking accuracy.

In spite of the recognition of these new ranking signals, previous work has mostly treated them as a black-box to ex-tract very basic features (e.g., raw rating scores and distance values) without going inside the signals to study how exactly they affect the relevance or clickrate of a business. For ex-ample, it is unclear how the clickrate of a business changes with its rating score: does a lower rating score necessarily lead to a lower clickrate? In the aforementioned restau-rant example, a 1 kilometer difference in distances may lead to significantly different clickrates of two restaurants, but would the same 1 kilometer distance difference also cause similar clickrate difference of another two restaurants that are further away from user X  X  location, e.g., 10 and 11 kilo-meters instead of 1 and 2 kilometers away? It is critical to understand the underlying behaviors/heuristics of a ranking signal, which would guide us to design more effective rank-ing features; this has been demonstrated extensively in the development of retrieval functions for general Web search, e.g., [30, 33, 11, 25].

Second, similarly to personalization in Web search [32, 36, 35, 13, 37, 28], personal preference affects user click behav-ior and can therefore constitute an important ranking signal for mobile local search [38]. For instance, knowing that a mobile user searching for restaurants prefers Chinese food, we can rank more Chinese restaurants on the top to avoid bothering the user with other types of restaurants. How-ever, conversely to Web search, it is non-trivial to build user profiles for capturing personal preference of different busi-nesses in mobile local search. On the one hand, the text associated with each business is often very sparse so that it would be hard to build content-based user profiles proposed previously [32, 36]. On the other hand, due to the local nature, a user tends to only click nearby businesses, so it is hard to find users who live far away from each other but share similar business click patterns, making it difficult to apply the collaborative filtering approaches (e.g., [35, 13]) or statistical topic modeling approaches (e.g., [20, 4, 27]), for user profiling.

Inspired by the understanding of these challenges, in this paper, we explore the ranking heuristics behind these new signals in mobile local search to develop more effective rank-ing features. Specifically, our contributions are threefold.
First, we follow a data-driven methodology to study the behavior of the new ranking signals in mobile local search us-ing a large-scale query log. Our analysis reveals interesting heuristics that can be used to guide the exploitation of differ-ent signals. For example, we reveal a common phenomenon for all signals involved in our study, i.e., users often take the mean value of a signal from the business result list as a  X  X ivot X  score, and tend to demonstrate different click behav-iors on businesses with lower and higher signal values than the pivot; the clickrate of a business generally is sublinearly decreasing with its distance to the user, etc. Motivated by these heuristics, we further propose different normalization methods to generate more effective ranking features.
Second, we exploit domain knowledge of businesses, i.e., business category information used in a commercial search engine, and employ a back-off strategy to estimate the user preference of business categories instead of specific busi-nesses: two users from different locations, though hardly co-clicking any specific business, may still be interested in similar business categories. We study how such a user pref-erence signal affects the clickrate of a business and design effective strategies to generate personalization features.
Third, we develop a clickrate prediction function to lever-age the complementary relative strengths of various signals, by employing a state-of-the-art predictive modeling method, MART [15, 16, 40]. In doing this, we hope to exploit the strength of machine learning to quantify the improvement of the proposed features. We evaluate the performance of the proposed new features using real mobile local search logs over a period of 14 months, with an emphasis on those diffi-cult queries, and show that the mean average precision can be improved by over 7%.
There have been several large scale studies in the past on mobile query log analysis for deciphering mobile search query patterns [22, 23, 9, 41, 8, 38]. The goal of these studies is to provide quantitative statistics on various aspects of mobile search that can help gain better insight on the mobile users X  information needs. However, few of these efforts have provided insight about the ranking issue.

In early studies of local search or geographic search, some work attempted to identify queries that may not contain an explicit geographic reference (e.g., a city name) but have a  X  X eo-intent X  nevertheless [39, 2, 42], while some others fo-cused on improving the query processing and ranking ef-ficiency [7, 12, 10]. They are all orthogonal to our work in that we study how to improve the ranking accuracy for  X  X eo-intent X  queries.

Recently, the task of improving the ranking accuracy of mobile local search has also begun to attract efforts [1, 24, 3, 26] which have already recognized that the ranking signals in mobile local search (e.g., distance and rating score of a busi-ness) are quite different from general Web search. However, these studies have mostly treated such ranking signals as a black-box to extract very basic features (e.g., raw distance values and rating scores). Although some statistics of these signals are also often used as complementary features, such as the average distance and the standard deviation of dis-tance in the current location [26], existing work relies purely on machine learning techniques to combine all features with-out going inside the signals to understand how exactly they affect user click behaviors. In contrast to existing studies, our work is a first attempt at understanding the behaviors and heuristics of these ranking signals for click prediction in mobile local search.

It has been previously demonstrated that understanding the behaviors/heuristics of a ranking signal is critical in the development of retrieval functions for Web search [30, 33, 14, 11, 25]. For example, the term frequency signal, which assigns a higher score to a document if it contains more occurrences of the query term, should be normalized to pre-vent the contribution of repeated occurrences from growing too large due to the burstiness phenomenon [30, 14], and the term frequency signal should also be normalized by doc-ument length since long documents tend to use the same terms repeatedly [30, 33, 14]. All effective retrieval models in Web search have implemented these heuristics [14], and previous work has also shown that a retrieval function tends to work poorly if any desirable retrieval heuristic is violated [14, 25]. Inspired by the successes and lessons from the de-velopment of retrieval models, we thus explore the ranking heuristics in mobile local search and try to understand how exactly a ranking signal in mobile local search is related to the clickrate/relevance of a business.

Additionally, opinionated content has also been exploited in some existing search tasks. For example, in opinion re-trieval [29], the goal of the task is to locate documents (pri-marily blog posts) that have opinionated content; Ganesan and Zhai [18] studied the use of the content of customer re-views to represent an entity (e.g., business) in entity ranking; Zhang et al. [43] proposed different methods to aggregate the counts of thumb-ups and thumb-downs for rating pre-diction, etc. Different from previous work, our work aims at understanding the relationship between the clickrate of a business and its rating score and number of reviews in mobile local search.

Personalized search has attracted much attention in Web search [32, 36, 13, 28, 38]. To the best of our knowledge, no previous work has exploited personalization for mobile local search. In this paper, we design appropriate approaches to exploiting personalization in mobile local search and quan-tify their impact on ranking accuracy using real mobile local query logs.
To the process of acquiring relevance judgments and eval-uating mobile local search is a challenging problem. First, the Cranfield style evaluation that has been used in the eval-uation of many traditional information retrieval tasks [31] would not work here, since the relevance judgments in mo-bile local search are particularly dependent on the search context, e.g., location of the user [24, 26]. Second, ask-ing users to make explicit relevance judgments can be very costly because it is necessary to cover a diverse set of queries in different contexts. Third, although Joachims et al. have developed methods for extracting relative relevance judg-ments from user clickthrough data in general Web search [21], it is unclear if these methods also work for mobile lo-cal search where the position bias of clickthrough and the interpretation of clickthrough may be different from general Web search. Due to these problems in applying traditional evaluation methods, in our work, we choose to follow the previous work on mobile local search [3, 26] and simply use clicks to approximate the relevance judgments. Although each individual user click may not be very reliable, the ag-gregation of a great number of user clicks from a large-scale query log could still provide powerful indicator of relevance. Thus, it is very likely that features that help improve click prediction will be useful in ranking as well.

Therefore our task is, given a query, to predict if a candi-date business would be clicked, and then rank the candidate businesses based on the click prediction. In our experiments, the query is sampled from the search log, while the candi-date businesses are all those businesses that were shown to the user for that query. To learn and evaluate a click predic-tion model, we split the query log into four parts. The first 9 months of data is kept out as the  X  X istory X  data, and is used purely for estimating the popularity of a business and the user X  X  personal preference. We sample queries from the next 3, 1, and 1 months of data respectively for training, validating, and testing the click prediction models.
We apply three preprocessing steps to make these four datasets more practical and representative: (1) We exclude queries that did not receive any click or received clicks for every candidate business, since these queries will not influ-ence the average ranking performance in the experiments. (2) We identify and filter out queries that match a busi-ness name exactly, e.g.,  X  X tarbucks X ; solving such kind of queries is a relatively easy task, since users usually have clearer information needs (e.g., visiting a nearby Starbucks) as compared to other more general queries, e.g.,  X  X offee X . In doing this, we place an emphasis on difficult queries in our study. (3) We empirically remove queries (from all the four datasets) by  X  X sers X  who issued more than 1000 queries in total in the training, validation, and test datasets, because such  X  X sers X  are more likely to be robots. Finally, we obtain 60475, 18491, and 23152 queries as the training, validation, and test queries; the average number of clicks and the aver-age number of candidate businesses for each query are 1 . 34 and 17 respectively.
Since we need to leverage multiple signals for click predic-tion, we seek help from machine learning. We adopt MART [40], a learning tool based on Multiple Additive Regression Trees, to provide a common learning framework on top of which we can compare the performance of different rank-ing heuristics and ranking features. MART is based on the stochastic gradient boosting approach described in [15, 16] which performs gradient descent optimization in the func-tional space. In our experiments on click prediction, we used the log-likelihood as the loss function, used steepest-decent (gradient descent) as the optimization technique, and used binary decision trees as the fitting function.

We construct a training instance for each query-business pair, which consists of a set of features (e.g., distance, rat-ing, etc.) and a click label which indicates if the user clicks the business (1 for click and 0 otherwise). The training and validation data are fed into MART to build a binary classi-fication model, which we use to estimate the probability of clicks in the test data.

Note that the choice of machine learning algorithms is not critical in our paper: we only take machine learning as a black-box tool to evaluate the proposed heuristics and fea-tures. We chose MART mainly because it can potentially handle non-linear combination of features, and it is widely adopted in current commercially available search and adver-tisement ranking engines.

The main goal of our experiments is to explore and evalu-ate effective ranking heuristics for boosting the special rank-ing signals in mobile local search. Our baseline feature set contains 6 representative features that are selected based on previous research studies [17, 24, 26]. These features are: (1) the distance between the query and the business loca-tions, (2) the popularity measure of the business as defined by the number of clicks in the history search logs, (3) the clickrate of the business in the history data as defined by the
Table 1: Relative feature importance in baseline number of clicks divided by the number of impressions and defined as 0 if it did not occur in the history data, (4) the customer rating score of the business in a range of [0 , 10], (5) the number of customer reviews of the business, and (6) a time code that represents both the time frame within a day (one out of five time frames) that the query was submit-ted and the day of the week that the query was submitted (weekend or weekday). Since we only re-rank top-ranked businesses from a commercial mobile local search engine, and those businesses that do not match the query keywords at all have already been eliminated, we thus do not involve any textual matching feature and focus only on the special signals of mobile local search.

We evaluate the retrieval performance in terms of MAP (Mean Average Precision) and the precision at different re-call levels. Because we are only re-ranking a set of top-ranked businesses in response to a query, the recall score will be the same for any ranking model. In other words, MAP will be only influenced by the positions of the relevant results. So we believe that, in our study, MAP is a good measure to capture the ranking accuracy of top-ranked re-sults. Besides, we also compare the importance of each fea-ture in constructing the MART model, following the relative importance measure proposed in [15].
In mobile local search, the rating score, the number of re-views, and the distance are not only the key back-end rank-ing signals, but also important information displayed explic-itly in the search result UI as shown in Figure 1. Although the  X  X ersonal preference X  signal is not explicitly shown to users, users certainly know their own preference. That be-ing said, these four ranking signals are directly observable to users, and users X  click behaviors presumably would be heav-ily dependent on these signals. Therefore, understanding how exactly a user X  X  decision relates to these signals would potentially lead to better ways of modeling these signals and thus to improving mobile local search. In this section, we study in detail these four ranking signals.
Intuitively, the customer rating score of a business would significantly affect users X  click behaviors in mobile local search, since users are used to consulting other people X  X  ratings and opinions about an entity to help make their own decision [29]. To verify this intuition, we trained a click prediction model using the baseline features described in the previous section and examined the relative importance of the differ-ent features. The results shown in Table 1 indicate that con-versely to our intuition, the importance of the rating score as a feature is relatively low in our baseline system.
To examine this observation, we analyze the likelihood of relevance (i.e., clickrate) of businesses of all rating scores, and plot these likelihoods against the rating score to obtain a  X  X lick pattern X . Intuitively, the likelihoods should increase monotonically with the rating score.

To estimate these likelihoods, we sort all the businesses in the training data in order of increasing rating score, and di-vide them into several equal sized (i.e., 1000)  X  X ins X , yielding 1032 different bins. We select the mean rating score in each bin to represent the bin on the graphs used in later analysis. We can then compute the probability of a randomly selected business from the i th bin getting clicked, which is the ratio of the number of clicked businesses from the i th bin, and the bin size. In terms of conditional probability, given a business b , this ratio of the i th bin can be represented by p ( b is clicked | b  X  Bin i ).

Figure 2(a) shows how the probabilities obtained from the above analysis relate to the mean rating score in a bin. Sur-prisingly, there seems to be no clear relationship between the likelihood of click and the rating score.

This anti-intuitive observation could be possibly caused by the potentially incomparable rating scores across differ-ent queries: result businesses retrieved by some queries may have higher rating scores than that retrieved by some other queries. To verify this, we adopt the popular zero-one score normalization method, which linearly normalizes the rating scores of every query to a range of [0 , 1]. Such a normaliza-tion strategy has been widely used for feature normalization in many learning to rank tasks, e.g., [6]. After that, we do a similar analysis of the probabilities of click, but against the normalized rating score. The results are shown in Fig-ure 2(b). Unfortunately, there is still no clear relationship, suggesting that the ineffectiveness of the rating score as a feature is not purely caused by the incomparable score range.
To diagnose the problem, we further look into the distri-bution of rating scores of businesses that get clicked. Since the distribution of rating scores is intuitively related to the Figure 3: Rating of clicked businesses, plotted against the mean rating score of the corresponding business category. type/category of businesses, we do this analysis in a category-aware way. Specifically, we compare the rating score of a clicked business with the mean rating score of all businesses from the same category. In doing this, we hope to under-stand if the clicked businesses are among the highly rated businesses in a category. The comparison results are illus-trated in Figure 3, in which we show 10% of randomly se-lected clicked businesses from the training data.
Interestingly, we can see that the rating scores of most of the clicked businesses are above their corresponding cat-egory mean rating score. For example, when the category mean rating score is 4, few businesses with a rating score lower than 4 get clicked. This shows that the rating score is indeed useful and is directly related to users X  click behaviors. However, how does a user know the mean rating score of a category so as to click businesses above this score?
In reality, we expect that users do not really know the mean score of a category. However, users may be able to have an approximate estimation of this mean score through looking over the retrieved business list: the retrieved busi-nesses for a query often belong to the same category, and thus could be used as a sample set of businesses from that category, the mean rating score of which can be viewed as approximately the mean category rating. If the above as-sumption is true, it may suggest that, users often take the mean rating score from the business result list as a  X  X ivot X  score, and tend to click businesses with higher scores than this pivot. This intuitively makes sense: a user X  X  click deci-sion, although influenced by the rating score, is not entirely based on it, but if the rating of a business is above his/her expectation (i.e., the pivot) which is learned from the result list in an ad hoc way, the business would be more likely to be clicked.

Inspired by this analysis, we propose to normalize rating scores using the mean rating score of the retrieved busi-nesses. A straightforward approach is to divide the original rating score using this mean value, which not only makes the rating scores more comparable across queries but also aligns them at the corresponding mean rating score of each query. We plot the probabilities of click against the normal-ized rating score in Figure 2(c). It is clear that the proba-bility of click increases monotonically with the normalized rating when the normalized rating is larger than 1 (i.e., the mean point), while the probability tends to be random when the normalized rating is lower than 1. This is an empirical Table 2: Comparison of methods for modeling rat-ing scores.  X  X orm+Pred X  combines methods tagged using  X  . b / z / m / c / q / r indicates the significance over Baseline, ZeroOneNorm, MeanNorm, AutoNorm-C, AutoNorm-Q, and RatingPred respectively, at the 0 . 001 level using the Wilcoxon non-directional test. verification of the fact that users tend to take the mean rat-ing score from the observed results as a  X  X ivot X  score, and a clear demonstration of different click behaviors on businesses with lower and higher scores than the pivot score.
To examine if the proposed simple mean normalization scheme can really improve ranking accuracy, we use the nor-malized rating score to replace the original rating score and learn a new model, labeled as  X  X eanNorm X , and compare its performance with the baseline model. In addition, we also take the widely used zero-one strategy for rating nor-malization as another baseline run, which is labeled as  X  X e-roOneNorm X . The comparison results are reported in Table 2, and show that the mean normalization scheme works the best, achieving significant improvement over both baseline runs. At the same time, the zero-one normalization does not improve the accuracy of the baseline model. This suggests that we can improve ranking performance by pointing out the mean rating value.

Another approach to explicitly normalizing the rating of a business with the mean rating of all result businesses would be to encode the mean rating of result businesses as an ad-ditional feature and let the training algorithm itself decide how to do the normalization. To evaluate this approach, we train a new model, labeled as  X  X utoNorm-Q X , by adding the mean rating score of businesses from the same single query into the baseline feature set. We also train another model, labeled as  X  X utoNorm-C X , in which we add the mean rating score of businesses from the same category into the baseline. We present the performance of these two runs in Table 2. The results demonstrate that AutoNorm-Q works much better than AutoNorm-C, confirming our analysis that users select the  X  X ivot X  from the businesses shown to them. AutoNorm-Q improves over the baseline by approximately 3%. Moreover, AutoNorm-Q improves over MeanNorm, sug-gesting that the mean normalization scheme can be boosted by optimizing the way of exploiting the  X  X ean X  in a super-vised manner.
It is often the case that a business does not receive any customer rating. In the presence of missing rating scores, a default value of 0 is often used, which, however, may be inaccurate: (1) a business that does not receive any customer rating does not necessarily mean that it should be rated low; (2) it could be unfair to use the same default rating score for all businesses. Even if a business receives a rating score, it may still be inaccurate if the rating is only contributed by a very small number of customers. Therefore, more accurate prediction/smoothing of ratings could potentially improve ranking accuracy, and we propose a cluster-based method to predict/smooth rating values.

The basic idea is based on the cluster hypothesis [19], and averages rating scores r ( x ) from all businesses x in the same cluster C to smooth the rating r ( b ) of business b so as to obtain an updated rating r ( b ). The intuition is that busi-nesses in the same cluster should receive similar ratings, and the rating stability of a cluster would benefit an individual business. Formally, where f is a function to control rating update. The key component is thus the business cluster. We use two types of clusters: business category and business chain. The for-mer allows us to use the rating of all businesses in a given category to estimate the rating of an unrated business in that category (e.g. use all businesses in the  X  X offee &amp; Tea X  category to estimate the rating score of a Starbucks busi-ness). The latter approach, allows us to estimate the rating score of a business by exploiting the rating score of other businesses belonging to the same chain (e.g. use different Strabucks coffeehouses rating scores to estimate the rating score of an unrated Starbucks coffeehouse).

There are two challenges with this approach: how to choose function f and how to leverage the evidences from two types of clusters. Inspired by the effective performance of auto-matic feature normalization in the previous section, we also let the learning algorithm optimize these two factors in a supervised way. Specifically, we provide both a category mean rating and a business-chain mean rating as two sep-arate features to the learning algorithm. In addition, we also introduce two description variables for these two new features, i.e., the size of the category and the size of the business chain, to the learning algorithm.

This method is labeled as  X  X atingPred X , and we present the experiment results in Table 2. We can see that Rating-Pred improves over the baseline significantly, suggesting the proposed cluster-based smoothing can indeed improve rating values. Furthermore, we combine RatingPred with the pro-posed feature normalization methods, leading to a new run labeled as  X  X orm+Pred X . It is observed from Table 2 that Norm+Pred outperforms either single method alone, sug-gesting that smoothing ratings and normalizing ratings are complementary to each other. Norm+Pred improves over the baseline by more than 4 . 5%.
The count of reviews represents another signal from the opinionated content, which can intuitively reflect the popu-larity of a business. However, we find that the importance of this signal is also low in the baseline model, as shown in Table 1. Similarly to the rating score analysis, we reveal that this is because users often take the mean review count from their observed businesses as a X  X ivot X , and demonstrate different click patterns on businesses with lower and higher review count than the pivot. However, the learning algo-rithm fails to capture this important information. To better exploit the strengths of this signal, we need to feed this Table 3: Comparison of methods for modeling review counts.  X  X orm+Pred X  combines meth-ods tagged using  X  . The description of notations b / z / m / c / q / r are the same as Table 2. pivot number (i.e., mean review count) to the learning al-gorithm. That is, we should either manually normalize the review count using the pivot, or introduce the pivot as an additional feature for automatic normalization.

The experimental results presented in Table 3 show that the ranking performance can be significantly improved by the X  X ean X  normalization scheme. Different notations in the table are defined similarly to their counterparts in Table 2 but applied to normalize review count. Specifically, the sim-ple mean normalization method (i.e., MeanNorm) performs significantly better than the widely used score normalization method (i.e., ZeroOneNorm) which does not leverage the mean value; also, automatic normalization (i.e., AutoNorm-Q) works more effectively than manual normalization (i.e., MeanNorm).

In addition, similar to the rating score, we can also im-prove ranking performance by smoothing review count based on the cluster hypothesis to make it more accurate (this run is labeled as  X  X eviewsPred X ). The mean normalization scheme and the cluster-based smoothing can be leveraged together (i.e., Norm+Pred) to further improve performance, achieving 4 . 5% improvement in MAP.
Local search differs from other search tasks mainly be-cause its ranking signals feature geographical distance. In fact, distance has also been shown to be one of the most important features in both previous work, e.g., [26], and our work, as shown in Table 1.

To understand how distance affects the click patterns of users, we first plot in Figure 4 (left) the likelihood of click for a business against its distance from the user. Interest-ingly, there is indeed a monotonically decreasing trend of the likelihood with respect to distance; this may explain why distance appears to be the most important feature in our experiments. Furthermore, we observe that the likelihood is decreasing sub-linearly with distance: the likelihood de-creases with distance, but the decreasing speed drops as dis-tance becomes large. This intuitively makes sense: a restau-rant within 1 mile would have clear advantages over another similar restaurant within 2 miles, but two restaurants within 9 miles and 10 miles may not have much difference. That is, the relevance of a business is more sensitive to its distance when the distance value is smaller.

With the Box-Cox transformation analysis [5], we find there is approximately a logarithm transformation. To il-lustrate it, we plot the likelihood of click with respect to the logarithm transformation of distance in Figure 4 (right). Figure 4: Probability of click for businesses from a bin, plotted against the mean distance (left) and Log(distance) (right) scores of this bin. Figure 5: Distance of clic ked businesses, plotted against the mean (traveling) distance of the corre-sponding business category.
 Linear scaling in distance would overly penalize businesses that are relatively far away from the user, while the loga-rithm transformation generally improves modeling distance.
Similar to ratings and review counts, distance is also ob-servable to users. One interesting question is if there is also a  X  X ivot X  click phenomenon. To answer this question, we plot the distance of clicked businesses against the mean travel-ing distance to businesses in the same category, as shown in Figure 5. Indeed, the plot shows that users tend to click businesses closer than the category mean distance, suggest-ing that the  X  X ean X  normalization scheme could also be ap-plicable in the case of the distance feature. Yet, the  X  X ivot X  phenomenon of distance is not as clear as that of ratings and review counts. One possible reason is that users can generally understand distance better than ratings and re-view counts, because distance is a concrete concept, while ratings and review counts appears to be more abstract and subjective; as a result, users tend to rely more on the statis-tics of the observed business list to make sense of ratings and review counts, but absolute distance itself may have already made much sense. This is also consistent with our previ-ous observation that there is a clear relationship between raw distance values and the probability of click, as shown in Figure 4 (left).

We now verify our analysis using empirical experiments, the results of which are reported in Table 4. We first apply the simple mean normalization method to divide distance by the mean distance value of all observed businesses for the same query. This run is labeled as  X  X eanNorm X . We can see it improves over the baseline system significantly, which suggests that feature normalization still helps even though absolute distance values are already largely compa-Methods MAP P@0.3 P@0.5 P@0.8 Baseline .419 .441 .434 .403 MeanNorm .429 b .451 b .443 b .414 b ZeroOneNorm+ .428 b .451 b .443 b .413 b MeanNorm+ .435 mz .457 mz .449 mz .420 mz AutoNorm .434 m .456 m .449 m .419 m AutoNorm+ .435 ma .457 ma .450 ma .420 ma Table 4: Comparison of methods for modeling dis-tance. Methods with an indicator  X + X  applies loga-rithm transformation. b / z / m / a indicates the signif-icance over Baseline, ZeroOneNorm+, MeanNorm, and AutoNorm respectively, at the 0 . 05 level using the Wilcoxon non-directional test. rable. We next compare  X  X eanNorm X  with  X  X eanNorm+ X  in which the simple mean normalization method is applied to log (distance). Apparently,  X  X eanNorm+ X  works more ef-fectively, confirming our analysis that the logarithm trans-formation is useful for better modeling distance. In addi-tion, we create another run  X  X eroOneNorm+ X  which differs from  X  X eanNorm+ X  in that the zero-one normalization is used. We observe that  X  X eroOneNorm+ X  works significantly worse than  X  X eanNorm+ X , confirming our analysis that the  X  X ean X  normalization scheme works well for distance, and suggesting that users would also like to click businesses with a distance smaller than the pivot (i.e., mean distance in the result list).

Finally, we also evaluate two automatic mean normaliza-tion runs, namely  X  X utoNorm X  and  X  X utoNorm+ X , where we introduce the mean value of distance and log (distance) in the search results as additional features into the training process to let the learning algorithm automatically normal-ize the distance and the log (distance) features respectively. First,  X  X utoNorm+ X  outperforms  X  X utoNorm X , though the improvement is small; this suggests that sub-linear trans-formation of distance is beneficial, yet its advantage tends to be weakened as we use automatic feature normalization, because MART can potentially handle non-linear combina-tion automatically. Second, by comparing  X  X utoNorm(+) X  with  X  X eanNorm(+) X , we can see that automatic normaliza-tion can work better than or comparable to the simple mean normalization. Overall, both automatic normalization and manual normalization can improve over baseline by approx-imate 4% with the proposed new features.
Our goal is to build user profiles so that we can compute the user preference of a business so as to rank businesses in a user adaptive way. However, it is non-trivial to build content-based user profiles [32, 36] in mobile local search, since the text associated with each business is often very sparse. Thus, we choose to use the collaborative filtering approach [20, 27, 4], based on the history click data, to estimate the likelihood that a user u likes business b ,for-mally the conditional probability P ( b | u ). Yet there is an-other challenging problem: due to the local nature of the task, a user tends to only click nearby businesses, so the co-occurrences are also of local nature and thus very sparse. For example, it is very hard to find users who live far away from each other but share similar business click patterns. To solve this problem, we exploit the domain knowledge of businesses and instead estimate the likelihood that a user u likes business category c , i.e., P ( c | u ): as a business category can cover businesses from different locations, co-occurrences of categories and users can happen across locations. As a by-product, the number of categories, which is about 3000, is only about 1 / 5000 of the number of businesses, signifi-cantly reducing the dimension. Our experiments show that we can build profiles for 1 million users in several hours on a single machine.

Although the category information of a business that the user has clicked can be obtained directly from the history data, due to the data sparseness problem of many users, we follow the idea of statistical topic modeling to estimate P ( c | u ) in a more smoothing way. First, we introduce hid-den variables Z with states z for every user-category pair. The possible set of states z is assumed to be finite and of size k . We empirically set k = 100 in our work. We can map our problem to the standard topic modeling problem: the original document-term matrix is replaced by a user-category matrix, and the original co-occurrence relationship is replaced by a click. In our work, we adopt PLSA [20] and LDA [4]. Since these two models perform similarly ef-fectively in our experiments, we only show the results based on PLSA.

Considering observations in the form of clicks ( c, u )ofcat-egories and users, PLSA models the probability of each click (i.e., a user u clicks a business of category c )asamixture of conditionally independent multinomial distributions: Since our problem is to estimate user preference, we will work with the conditional model The model can be estimated using the Expectation Maxi-mization (EM) algorithm to obtain parameters P ( z | u )and P ( c | z ). Now, we have constructed a user profile P ( c Then given any business b ,since b maybelongtomultiple categories, we average the conditional probabilities of these corresponding categories as the user preference of a business, i.e., P ( b | u ).

Some users may have more history clicks, and the profiles of these users would intuitively be more reliable than that of some other users who make fewer clicks. In order to make the model more intelligent so as to be able to automatically learn how much personalization we should apply for each user, we encode both the user preference, i.e., P ( b | u ), and the number of history clicks of the user into the learning algorithm. We do not do any normalization, and this run is Personalization MAP P@0.3 P@0.5 P@0.8 Baseline .419 .441 .434 .403 NoNorm .420 b .442 b .434 b .404 b MeanNorm .420 b .442 b .434 b .404 b MeanNorm+ .428 nm .450 nm .442 nm .411 nm AutoNorm .427 nm .450 nm .442 bm .411 bm AutoNorm+ .428 nm .450 nm .443 nm .412 nm Table 5: Comparison of methods for modeling user preference. Methods with an indicator  X + X  applies logarithm transformation. b / n / m indicates the sig-nificance over Baseline, NoNorm, and MeanNorm respectively, at the 0 . 01 level using the Wilcoxon non-directional test. labeled as  X  X oNorm X . We compare it with the baseline and observe that, though  X  X oNorm X  outperforms the baseline significantly, the improvement is indeed minor.

To examine the reason, we follow Section 4.1.1, and plot the probability of click for businesses against the user pref-erence, as shown in Figure 6 (a). We can see that the prob-ability of click generally does not vary a lot when the user preference changes; this may be one possible reason why  X  X oNorm X  does not work very well. We then apply the zero-one normalization and generate another plot in Figure 6 (b). It shows that the zero-one normalization essentially stretches the plot along the x-axis. There also appears to be an increasing trend only when the user preference is very small. Next, we try the mean normalization method in Fig-ure 6 (c). It shows clearly that when the user preference is below the mean value (i.e., x = 1) of the current search results, the probability of click increases monotonically with user preference and the increasing speed decreases as the user preference approaches its mean value. However, after the user preference reaches the mean value, the probabil-ity of click even has a tendency to decrease slightly. Again, this observation shows that users choose the mean value as a pivot score and have different click behaviors on the two sides of the pivot. Furthermore, it is interesting to see that the probability of click is maximized when the user prefer-ence is around the mean value: too low preference may mean that the business is not interesting to the user (i.e., irrelevant business), while too high preference may indicate that the business could be too similar to what the user clicked before (i.e., redundant business). The pivot observation seems to demonstrate that a user X  X  decision may like an exploration-exploitation tradeoff: exploit what he/she knows but mean-while explore what he/she does not know. Methods MAP P@0.3 P@0.5 P@0.8 Baseline .419 .441 .434 .403 All .449 .472 .464 .433 All-Rating .448 .471 .463 .432 All-Reviews .449 .472 .464 .433 All-Distance .441 .464 .456 .425 All-Personalization .448 .471 .463 .431 All-Rating-Reviews .442 .464 .456 .426 All-Rating-Reviews .436 .458 .450 .420 -Personalization Table 6: Sensitivity analysis. It shows that com-bining the proposed new features (i.e.,  X  X ll X ) can improve the Baseline over 7% .

Inspired by the analysis above, we develop a manual mean normalization run ( X  X eanNorm X ) and an automatic mean normalization run ( X  X utoNorm X ) for feature normalization. According to the results shown in Table 5,  X  X utoNorm X  im-proves over both the baseline and  X  X eanNorm X  significantly, while  X  X eanNorm X  does not perform very well. We hypoth-esize that this could be because of the sublinear increasing curve of the mean normalization method, as shown in Figure 6 (c): similar to our observations of distance normalization, automatic normalization using MART can potentially han-dle non-linear combination well, while manual normalization cannot. To verify our intuition, we first apply a logarithm transformation based on the Box-Cox transformation anal-ysis [5] onto user preference and then add the two normal-ization methods on top of the transformed feature, leading to two new runs  X  X eanNorm+ X  and  X  X utoNorm+ X . Table 5 shows that these two runs perform similarly well and the best among all methods, verifying our hypothesis and also showing the necessity of sublinear transformation. Overall, by normalizing the personalization features, we can obtain over 2% MAP improvements.
We combine the most effective modeling methods for all the four signals into our final model, including X  X orm+Pred X  from Table 2 for modeling the customer rating, X  X orm+Pred X  from Table 3 for modeling the review count,  X  X utoNorm+ X  and  X  X eanNorm+ X  from Table 4 for modeling distance, and  X  X utoNorm+ X  and  X  X eanNorm+ X  from Table 5 for model-ing user preference. The final model is labeled as  X  X ll X  and shown in Table 6. Table 7 lists the 23 features in the  X  X ll X  model as well as where each feature comes from. We can see that the  X  X ll X  model outperforms the baseline by more than 7%, suggesting that understanding the behaviors and heuristics behind ranking signals can indeed lead to better modeling methods and thus improving ranking accuracy.
We have shown that the selected modeling methods (now as components in the final  X  X ll X  model) perform very well in modeling each individual signal. Now we turn to examine how sensitive of these methods when we combine them to-gether. We remove some new modeling method(s) at a time, while keeping all other modeling methods and the baseline features. For example, X  X ll-Rating X  X n Table 6 is constructed by excluding from the  X  X ll X  model the proposed novel fea-tures in the X  X orm+Pred X  X ethod for rating modeling, while the features occurring in other models, including all baseline features, are kept. From Table 6, we can see that when we remove  X  X istance X , the performance drops clearly, suggest-Table 7: Relative feature importance in the final model ing that  X  X istance X  is very sensitive in the final model and its effect cannot be replaced. However, when we exclude  X  X ating X ,  X  X eviews X , or  X  X ersonalization X , the performance only decreases slightly or even does not change. It suggests that the effect of these signals may have a large overlap; as a result, although they perform well as individual signals, their performance could not add together.

To go a step further, we remove X  X ating X  X nd X  X eviews X  X t the same time, and find that its performance degrades much more than that of  X  X ll-Rating X  and  X  X ll-Reviews X . This ob-servation confirms that ratings and review counts are highly redundant to each other, which intuitively makes sense. Af-ter removing  X  X ating X  and  X  X eviews X , we also remove  X  X er-sonalization X  in the last row of Table 6. We can see the per-formance degradation is much larger than when we remove  X  X ersonalization X  from the  X  X ll X  model, suggesting that the personalization features also tend to be redundant to  X  X at-ing X  and  X  X eviews X . This is interesting, which may suggest that personal preference tends to have significant overlap with global preference of a business.

Finally, we go in depth to the feature level to analyze what are the relative importance of each feature, as shown in Ta-ble 7. Apparently, distance, with the proposed normaliza-tion method, appears to be the most important feature. The two popularity measures from the Baseline are the second and the third most important features. These observations are consistent with the findings from previous work (e.g., [26]) that distance and popularity dominate the ranking sig-nals of mobile local search. However, other signals have also contributed many useful features. For example, the top-6 features covers all feature contributors.

Two particularly interesting observations are that (1) three mean values are ranked very high, and (2) 8 out of the top-12 features are directly related to the mean normalization scheme, suggesting that the proposed  X  X ean X  normalization scheme indeed helps model signals in mobile local search. Due to the effectiveness of the proposed new features, many features from the baseline have been pushed to the bottom of Table 7.
In this paper, we follow a data-driven methodology to study the ranking heuristics/behaviors of multiple ranking signals, including the customer rating score, the number of reviews, the geographic distance, and the user preference, in mobile local search using a large-scale query log.
Our analysis reveals interesting heuristics that can be used to guide the exploitation of different signals. First, we re-veal a common phenomenon for all these four signals: users often take the mean value of a signal from the business re-sult list as a  X  X ivot X  score, and tend to demonstrate differ-ent click behaviors on businesses with lower and higher sig-nal values than the pivot. Inspired by this understanding, we proposed a  X  X ean X  normalization scheme to encode the pivot information into feature normalization or into model training, which has been shown to improve modeling these signals significantly. Second, we find that the clickrate of a business is increasing/decreasing sublinearly with user-preference/distance; as a result, linear scaling in these sig-nals would bias to/against some particular businesses. In or-der to overcome this problem, we propose sublinear transfor-mation methods for modeling these signals based on power transformation analysis, which work very well. We quantify the improvement of the proposed methods using real mobile local search logs over a period of 14 months and show that the mean average precision can be improved significantly.
