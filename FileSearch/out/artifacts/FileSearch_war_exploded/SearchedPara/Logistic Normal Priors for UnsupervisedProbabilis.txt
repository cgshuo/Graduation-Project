 Unsupervised learning of structured variables in data is a difficult problem that has received considerable recent attention. In this paper, we consider learning probabilistic grammars , a class of structure models that includes Markov models, hidden Markov models (HMMs) and probabilistic context-free grammars (PCFGs). Central to natural language process-ing (NLP), probabilistic grammars are recursive generative models over discrete graphical structures, built out of conditional multinomial distributions, that make independence as-sumptions to permit efficient exact probabilistic inference.
 There has been an increased interest in the use of Bayesian methods as applied to probabilis-tic grammars for NLP, including part-of-speech tagging [10, 20], phrase-structure parsing [7, 11, 16], and combinations of models [8]. In Bayesian-minded work with probabilistic grammars, a common thread is the use of a Dirichlet prior for the underlying multinomials, because as the conjugate prior for the multinomial, it bestows computational feasibility. The Dirichlet prior can also be used to encourage the desired property of sparsity in the learned grammar [11].
 A related widely known example is the latent Dirichlet allocation (LDA) model for topic modeling in document collections [5], in which each document X  X  topic distribution is treated as a hidden variable, as is the topic distribution from which each word is drawn. 1 Blei and Lafferty [4] showed empirical improvements over LDA using a logistic normal distribution that permits different topics to correlate with each other, resulting in a correlated topic model (CTM). Here we aim to learn analogous correlations such as: a word that is likely to take one kind of argument (e.g., singular nouns) may be likely to take others as well (e.g., plural or proper nouns). By permitting such correlations via the distribution over the Figure 1: A graphical model for the logistic normal probabilistic grammar. y is the deriva-tion, x is the observed string. parameters, we hope to break independence assumptions typically made about the behavior of different part-of-speech tags.
 In this paper, we present a model, in the Bayesian setting, which extends CTM for proba-bilistic grammars. We also derive an inference algorithm for that model, which is ultimately used to provide a point estimate for the grammar, permitting us to perform fast and exact inference. This is required if the learned grammar is to be used as a component in an application.
 The rest of the paper is organized as follows.  X  2 gives a general form for probabilistic grammars built out of multinomial distributions.  X  3 describes our model and an efficient variational inference algorithm.  X  4 presents a probabilistic context-free dependency gram-mar often used in unsupervised natural language learning. Experimental results showing the competitiveness of our method for estimating that grammar are presented in  X  5. A probabilistic grammar defines a probability distribution over a certain kind of structured object (a derivation of the underlying symbolic grammar) explained through step-by-step stochastic process. HMMs, for example, can be understood as a random walk through a probabilistic finite-state network, with an output symbol sampled at each state. PCFGs generate phrase-structure trees by recursively rewriting nonterminal symbols as sequences of  X  X hild X  symbols (each itself either a nonterminal symbol or a terminal symbol analogous to the emissions of an HMM). Each step or emission of an HMM and each rewriting operation of a PCFG is conditionally independent of the others given a single structural element (one HMM or PCFG state); this Markov property permits efficient inference.
 In general, a probabilistic grammar defines the joint probability of a string x and a gram-matical derivation y : where f k,i is a function that  X  X ounts X  the number of times the k th distribution X  X  i th event occurs in the derivation. The parameters  X  are a collection of K multinomials  X   X  1 ,...,  X  K  X  , the k th of which includes N k events. Note that there may be many derivations y for a given string x  X  X erhaps even infinitely many in some kinds of grammars. HMMs and vanilla PCFGs are the best known probabilistic grammars, but there are others. For example, in  X  5 we experiment with the  X  X ependency model with valence, X  a probabilistic grammar for dependency parsing first proposed in [14]. A natural choice for a prior over the parameters of a probabilistic grammar is a Dirichlet prior. The Dirichlet family is conjugate to the multinomial family, which makes the inference more elegant and less computationally intensive. In addition, a Dirichlet prior can encourage sparse solutions, a property which is important with probabilistic grammars [11]. However, in [4], Blei and Lafferty noticed that the Dirichlet distribution is limited in its expressive power when modeling a corpus of documents, since it is less flexible about cap-turing relationships between possible topics. To solve this modeling issue, they extended the LDA model to use a logistic normal distribution [2] yielding correlated topic models. The logistic normal distribution maps a d -dimensional multivariate Gaussian to a distribution exponentiating the normally-distributed variables and normalizing.
 Here we take a step analogous to Blei and Lafferty, aiming to capture correlations between the grammar X  X  parameters. Our hierarchical generative model, which we call a logistic-normal probabilistic grammar , generates a sentence and derivation tree  X  x , y  X  as follows (see also Fig. 1): We now turn to derive a variational inference algorithm for the model. 2 Variational Bayesian inference seeks an approximate posterior function q (  X  , y ) which maximizes a lower bound (the negated variational free energy) on the log-likelihood [12], a bound which is achieved using Jensen X  X  inequality: We make a mean-field assumption, and assume that the posterior has the following form: where q (  X  k,i |  X   X  k,i ,  X   X  2 k,i ) is a Gaussian N (  X   X  k,i ,  X   X  2 k,i ). Unfolding the expectation with respect to q ( y ) in the second term in Eq. 2, while recalling that  X  is a deterministic function of  X  , we have that: where  X  f k,i is the expected number of occurrences of the i th event in distribution k , under q ( y ). 3 The logarithm term in Eq. 4 is problematic, so we follow [4] in approximating it with a first-order Taylor expansion, introducing K more variational parameters  X   X  1 ,...,  X   X  K : We now have
E q [log p ( x , y |  X  )]  X  E q (  X  ) Note the shorthand  X   X  k,i to denote an expression involving  X   X  ,  X   X  , and  X   X  . The final form of our bound is: 4 Since, we are interested in EM-style algorithm, we will alternate between finding the max-imizing q (  X  ) and the maximizing q ( y ). Maximization with respect to q (  X  ) is not hard, because q (  X  ) is parametrized (see Appendix A). The following lemma shows that fortu-nately, finding the maximizing q ( y ), which we did not parametrize originally, is not hard either: Lemma 1. Let r ( y | x ,e  X   X  ) denote the conditional distribution over y given x defined as: Eq. 8.
 interested in maximizing from Eq. 8 are the following, after writing down  X  f k,i explicitly: However, note that: where D KL denotes the KL divergence. To see that, combine the definition of KL divergence does not depend on q ( y ). Eq. 11 is minimized when q = r .
 Interestingly, from the above lemma, the minimizing q ( y ) has the same form as the prob-abilistic grammar in discussion, only without having sum-to-one constraints on  X  (leading to the required normalization constant). As in classic EM with probabilistic grammars, we never need to represent q ( y ) explicitly; we need only  X  f , which can be calculated as expected feature values under r ( y | x ,e  X   X  ) using dynamic programming.
 As noted, we are interested in a point estimate of  X  . To achieve this, we will use the above variational method within an EM algorithm that estimates  X  and  X  in empirical Bayes fashion, then estimates  X  as  X  , the mean of the learned prior. In the E-step, we maximize ascent. We optimize each of these separately in turn, cycling through, using appropriate optimization algorithms for each (conjugate gradient for  X   X  , Newton X  X  method for  X   X  , a closed form for  X   X  , and dynamic programming to solve for  X  f ). In the M-step, we apply maximum likelihood estimation with respect to  X  and  X  given sufficient statistics gathered from the variational parameters in the E-step. The full algorithm is given in Appendix A. Dependency grammar [19] refers to linguistic theories that posit graphical representations of sentences in which words are vertices and the syntax is a tree. Such grammars can be context-free or context-sensitive in power, and they can be made probabilistic [9]. De-pendency syntax is widely used in information extraction, machine translation, question Figure 2: An example of a dependency tree (derivation y ). NNP denotes a proper noun, VBD a past-tense verb, and JJ an adjective, following the Penn Treebank conventions. answering, and other natural language processing applications. Here, we are interested in unsupervised dependency parsing using the  X  X ependency model with valence X  [14]. The model is a probabilistic head automaton grammar [3] with a  X  X plit X  form that renders in-ference cubic in the length of the sentence [6].
 Let x =  X  x 1 ,x 2 ,...,x n  X  be a sentence (here, as in prior work, represented as a sequence of part-of-speech tags). x 0 is a special  X  X all X  symbol, $, on the left of every sentence. A that map each word to its sets of left and right dependents, respectively. Here, the graph is constrained to be a projective tree rooted at x 0 = $: each word except $ has a single parent, and there are no cycles or crossing dependencies. y left (0) is taken to be empty, and y right (0) contains the sentence X  X  single head. Let y ( i ) denote the subtree rooted at position recursively: where first y ( j ) is a predicate defined to be true iff x j is the closest child (on either side) the general setting of Eq. 1, we index these distributions as  X  1 ,...,  X  K . Figure 2 shows a dependency tree and its probability under this model. Data Following the setting in [13], we experimented using part-of-speech sequences from the Wall Street Journal Penn Treebank [17], stripped of words and punctuation. We follow standard parsing conventions and train on sections 2 X 21, 5 tune on section 22, and report final results on section 23.
 Evaluation After learning a point estimate  X  , we predict y for unseen test data (by parsing with the probabilistic grammar) and report the fraction of words whose predicted parent matches the gold standard corpus, known as attachment accuracy. Two parsing methods were considered: the most probable  X  X iterbi X  parse (argmax y p ( y | x ,  X  )) and the minimum as the loss function.
 Settings Our experiment compares four methods for estimating the probabilistic gram-mar X  X  parameters: EM Maximum likelihood estimate of  X  using the EM algorithm to optimize p ( x |  X  ) [14]. EM-MAP Maximum a posteriori estimate of  X  using the EM algorithm and a fixed sym-VB-Dirichlet Use variational Bayes inference to estimate the posterior distribution p (  X  | VB-EM-Dirichlet Use variational Bayes EM to optimize p ( x |  X  ) with respect to  X  . Use VB-EM-Log-Normal Use variational Bayes EM to optimize p ( x |  X  ,  X  ) with respect to Initialization is known to be important for EM as well as for the other algorithms we experiment with, since it involves non-convex optimization. We used the successful initializer from [14], which estimates  X  using soft counts on the training data where, in an n -length sentence, (a) each word is counted as the sentence X  X  head 1 n times, and (b) each word x i attaches to x j proportional to | i  X  j | , normalized to a single attachment per word. This initializer is used with EM, EM-MAP, VB-Dirichlet, and VB-EM-Dirichlet. In the case of VB-EM-Log-Normal, it is used as an initializer both for  X  and inside the E-step. In all experiments reported here, we run the iterative estimation algorithm until the likelihood of a held-out, unannotated dataset stops increasing.
 For learning with the logistic normal prior, we consider two initializations of the covariance matrices  X  k . The first is the N k  X  N k identity matrix. We then tried to bias the solution by injecting prior knowledge about the part-of-speech tags. Injecting a bias to parameter estimation of the DMV model has proved to be useful [18]. To do that, we mapped the tag set (34 tags) to twelve disjoint tag families. 6 The covariance matrices for all dependency distributions were initialized with 1 on the diagonal, 0 . 5 between tags which belong to the same family, and 0 otherwise. These results are given in Table 1 with the annotation  X  X amilies. X  Results Table 1 shows experimental results. We report attachment accuracy on three subsets of the corpus: sentences of length  X  10 (typically reported in prior work and most similar to the training dataset), length  X  20, and the full corpus. The Bayesian methods all outperform the common baseline (in which we attach each word to the word on its right), but the logistic normal prior performs considerably better than the other two methods as well.
 The learned covariance matrices were very sparse when using the identity matrix to ini-tialize. The diagonal values showed considerable variation, suggesting the importance of variance alone. When using the  X  X ag families X  initialization for the covariance, there were 151 elements across the covariance matrices which were not identically 0 (out of more than 1,000), pointing to a learned relationship between parameters. In this case, most covariance matrices for  X  c dependencies were diagonal, while many of the covariance matrices for the stopping probabilities (  X  s ) had significant correlations. We have considered a Bayesian model for probabilistic grammars, which is based on the logistic normal prior. Experimentally, several different approaches for grammar induction were compared based on different priors. We found that a logistic normal prior outperforms earlier approaches, presumably because it can capitalize on similarity between part-of-speech tags, as different tags tend to appear as arguments in similar syntactic contexts. We achieved state-of-the-art unsupervised dependency parsing results. Attach-Right 38.4 33.4 31.7 38.4 33.4 31.7 EM 45.8 39.1 34.2 46.1 39.9 35.9 EM-MAP,  X  = 1 . 1 45.9 39.5 34.9 46.2 40.6 36.7 VB-Dirichlet,  X  = 0 . 25 46.9 40.0 35.7 47.1 41.1 37.6 VB-EM-Dirichlet 45.9 39.4 34.9 46.1 40.6 36.9 VB-EM-Log-Normal,  X  (0) k = I 56.6 43.3 37.4 59.1 45.9 39.9 VB-EM-Log-Normal, families 59.3 45.1 39.0 59.4 45.9 40.5 Table 1: Attachment accuracy of different learning methods on unseen test data from the Penn Treebank of varying levels of difficulty imposed through a length filter. Attach-Right attaches each word to the word on its right and the last word to $. EM and EM-MAP with a Dirichlet prior (  X  &gt; 1) are reproductions of earlier results [14, 18].
 The authors would like to thank the anonymous reviewers, John Lafferty, and Matthew Harrison for their useful feedback and comments. This work was made possible by an IBM faculty award, NSF grants IIS-0713265 and IIS-0836431 to the third author and computa-tional resources provided by Yahoo.
 [1] A. Ahmed and E. Xing. On tight approximate inference of the logistic normal topic [2] J. Aitchison and S. M. Shen. Logistic-normal distributions: some properties and uses. [3] H. Alshawi and A. L. Buchsbaum. Head automata and bilingual tiling: Translation [4] D. Blei and J. D. Lafferty. Correlated topic models. In Proc. of NIPS , 2006. [5] D. Blei, A. Ng, and M. Jordan. Latent Dirichlet allocation. Journal of Machine Learning [6] J. Eisner. Bilexical grammars and a cubic-time probabilistic parser. In Proc. of IWPT , [7] J. Eisner. Transformational priors over grammars. In Proc. of EMNLP , 2002. [8] J. R. Finkel, C. D. Manning, and A. Y. Ng. Solving the problem of cascading errors: [9] H. Gaifman. Dependency systems and phrase-structure systems. Information and [10] S. Goldwater and T. L. Griffiths. A fully Bayesian approach to unsupervised part-of-[11] M. Johnson, T. L. Griffiths, and S. Goldwater. Bayesian inference for PCFGs via [12] M. I. Jordan, Z. Ghahramani, T. S. Jaakola, and L. K. Saul. An introduction to [13] D. Klein and C. D. Manning. A generative constituent-context model for improved [14] D. Klein and C. D. Manning. Corpus-based induction of syntactic structure: Models [15] K. Kurihara and T. Sato. Variational Bayesian grammar induction for natural language. [16] P. Liang, S. Petrov, M. Jordan, and D. Klein. The infinite PCFG using hierarchical [17] M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. Building a large annotated [18] N. A. Smith and J. Eisner. Annealing structural bias in multilingual weighted grammar [19] L. Tesni`ere.  X  El  X ement de Syntaxe Structurale . Klincksieck, 1959. [20] K. Toutanova and M. Johnson. A Bayesian LDA-based model for semi-supervised The algorithm for variational inference with probabilistic grammars using logistic normal routine in the E-step (suppressed for clarity). There are variational parameters for each training example, indexed by ` . We denote by B the variational bound in Eq. 8. Our stopping criterion relies on the likelihood of a held-out set (  X  5) using a point estimate of the model.

