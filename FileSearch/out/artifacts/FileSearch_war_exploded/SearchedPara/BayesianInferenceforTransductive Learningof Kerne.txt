 Zhih ua Zhang zhzhang@cs.ust.hk Dit-Y an Yeung dyyeung@cs.ust.hk James T. Kw ok jamesk@cs.ust.hk Hong Kong In recen t years, kernel metho ds have rapidly gained much popularit y due to their  X  X xibilit y and theoreti-cal elegance. For most kernel-based learning metho ds in existence, the practitioner has to presp ecify a kernel function in adv ance before learning pro ceeds. Cho os-ing a good kernel for the problem at hand is more of an art than a science. Since a kernel induces a feature space, it is easy to understand that an appropriate ker-nel choice should tak e into accoun t the empirical data available for a learning problem. Since in practice we often deal with  X nite-sized data sets, almost all infor-mation in the kernel function can be enco ded by the kernel matrix. As a result, one could bypass the learn-ing of the kernel function by just learning the kernel matrix instead. For simplicit y, from now on, we do not mak e any distinction between learning the kernel function and learning the kernel matrix.
 Some recen t studies pursue to learn the kernel matrix from empirical data automatically . One ma jor issue to consider is what constitutes a good kernel matrix. More speci X cally , we need a criterion for optimizing the kernel matrix. The alignment was prop osed as suc h a criterion de X ned in the form of a similarit y measure between kernel matrices (Cristianini et al., 2002). Based on this criterion, sev eral metho ds have been prop osed for optimizing the kernel matrix, in-cluding a spectral metho d (Cristianini et al., 2002), semi-de X nite programming (SDP) (Lanc kriet et al., 2002), the Gram-Sc hmidt metho d (Kandola et al., 2002), and a gradien t-based metho d (Bousquet &amp; Her-rmann, 2003). Crammer et al. (2003) cast the kernel matrix learning problem under the boosting paradigm for constructing an accurate kernel from simple base kernels. In the case that there are missing data in the kernel matrix, Tsuda et al. (2003) dev elop ed a para-metric approac h to kernel matrix completion using the em algorithm based on information geometry (Amari, 1995). In their approac h, the Kullbac k-Leibler (KL) divergence is used for measuring the similarit y between kernel matrices.
 Assuming that the kernel matrix is a random posi-tive de X nite matrix follo wing the Wishart distribution (Gupta &amp; Nagar, 2000), Zhang et al. (2003b)  X rst pro-posed a generativ e mo del of the kernel matrix. In the transductiv e setting, Zhang et al. (2003a) presen ted a Bayesian hierarc hical mo del and sho wed that the exp ectation-maximization (EM) algorithm (Dempster et al., 1977) can be used to learn the kernel matrix through maximum a posteriori (MAP) estimation or, more generally , maxim um penalized likeliho od estima-tion. In particular, given the kernel matrix on the training data, the EM algorithm is used to alternately infer the kernel matrix on the test data and the kernel matrix relating the training data to the test data, as well as the parameter matrix of the Wishart distribu-tion for the kernel matrix.
 In this pap er, based on the Bayesian hierarc hical mo del prop osed by Zhang et al. (2003a), we use the Tanner-W ong data augmen tation algorithm (Tanner &amp; Wong, 1987) for Bayesian inference to solv e the kernel matrix learning problem under the transductiv e set-ting. The Tanner-W ong algorithm is closely related to Gibbs sampling whic h is a type of Mark ov chain Mon te Carlo (MCMC) metho d. Lik e the EM algorithm, the Tanner-W ong algorithm has been widely used in statis-tical inference for incomplete data problems (Sc hafer, 1997). Moreo ver, it also strongly resem bles the EM al-gorithm. The Tanner-W ong algorithm consists of the Imputation step (I-step) and the Posterior step (P-step). The I-step sim ulates a random dra w of some complete-data su X cien t statistics, whereas the E-step of EM computes the exp ectation of the complete-data su X cien t statistics. Typically , the implemen tation of the I-step is very similar to that of the E-step. On the other hand, the P-step of the Tanner-W ong al-gorithm is a random dra w from some complete-data posterior, while the M-step of EM performs maximiza-tion of the complete-data likeliho od. In other words, both the I-step and the E-step are used to estimate the values of the missing data, while the P-step and the M-step are both used to estimate the unkno wn values of the mo del parameters. Our results in Section 3 fur-ther demonstrate this strong resem blance between the Tanner-W ong algorithm and the EM algorithm.
 For our kernel matrix learning problem, the computa-tional requiremen ts would be very high if the Tanner-Wong algorithm works on matrix variate distributions. To mak e our metho d feasible in practice, we presen t in this pap er a simpli X ed Bayesian hierarc hical mo del so that the Tanner-W ong algorithm can work on distribu-tions over random variables or random vectors (instead of random matrices). This is motiv ated by some exist-ing kernel matrix learning metho ds (Crammer et al., 2003; Cristianini et al., 2002; Lanc kriet et al., 2002; Tsuda et al., 2003), whic h constrain the target ker-nel matrix to a weigh ted com bination of some  X xed base kernel matrices so that the kernel matrix learn-ing problem can be simpli X ed to the estimation of the weigh ting coe X cien ts. Apparen tly, the performance of these metho ds dep ends critically on the choice of the base kernel matrices. However, very little has been ad-dressed on how to presp ecify the base kernel matrices. Usually , they are obtained from the eigen vectors of an empirical kernel matrix on the input space. In this pap er, by a symmetric-de X nite generalized eigenprob-lem we asso ciate the kernel matrix on the input space and the kernel matrix on the output space. Based on this eigenproblem, we presen t an e X cien t approac h to choosing the base kernel matrices by exploiting infor-mation not just from the input kernel matrix but also from the partial output kernel on the training set. The rest of this pap er is organized as follo ws. Sec-tions 2 presen ts a basic Bayesian hierarc hical mo del for the kernel matrix learning problem. In Section 3, we devise a general Tanner-W ong data augmen tation al-gorithm for making inference on the basic mo del. Sec-tion 4 gives a simpli X ed Bayesian hierarc hical mo del and the corresp onding Tanner-W ong algorithm. We also presen t an e X cien t metho d for choosing the base kernel matrices. Section 5 presen ts the exp erimen tal results and the last section gives some concluding re-marks. We consider the kernel matrix learning problem for classi X cation in a transductiv e setting. Let the train-ing set and test set be T = f ( x 1 ; y 1 ) ; : : : ; ( x and ~ T = f ( x n spectiv ely, where x i 2 R q for i = 1 ; : : : ; n 1 + n y i 2 f 1 ; 2 ; : : : ; c g for i = 1 ; : : : ; n 1 and y i kno wn for i = n 1 +1 ; : : : ; n 1 + n 2 . Letting n = n we refer to X = f x 1 ; : : : ; x n f y put set, resp ectiv ely. We de X ne a kernel matrix K on ( T [ ~ T )  X  ( T [ ~ T ) and partition it as where n 1  X  n 1 and n 2  X  n 2 matrices K 11 and K 22 are de X ned on the training and test sets, resp ectiv ely, and n  X  n 1 matrix K 21 (= K 0 12 ) characterizes the similar-ity between the training and test data.
 In general, de X nition of the kernel matrix K dep ends on the problem considered and the prior kno wledge available. Let k I : X  X X ! R and k O : Y  X Y ! R de-note the input kernel and output kernel, resp ectiv ely. We de X ne A as the input kernel matrix for X using input kernel k I and B as the output kernel matrix for Y using output kernel k O . Augmen ting any in-put vector x with the corresp onding output y to form a vector z = ( x ; y ), we de X ne a kernel matrix K on ( X  X  Y )  X  ( X  X  Y ) in this pap er. Speci X cally , in our exp erimen ts to be presen ted later, we de X ne the ker-nel matrix K = ( A + B ) = 2 as the discriminan t kernel (Zhang, 2003), where A =  X  exp(  X k x i  X  x j k 2 = X  )  X  is the standard Gaussian kernel and B is the ideal ker-nel (Cristianini et al., 2002) based on the training set, i.e., Follo wing the generativ e mo del form ulation of the ker-nel matrix in (Zhang et al., 2003a), we now assume that the kernel matrix K is distributed according to a Wishart distribution W n ( r;  X  ) (Gupta &amp; Nagar, 2000), as p ( K j  X  ; r ) = Here  X   X  0 is an n  X  n positiv e de X nite parameter matrix, 1 r  X  n is the degree of freedom, and C ( n; r ) = 2 where  X (  X  ) is the Gamma function.
 The parameter matrix  X  is left completely unsp eci X ed in the mo del. However, its uncertain ty is in X  X enced by a higher-lev el prior distribution. In particular, we use a conjugate prior on  X  , i.e.,  X  is distributed accord-ing to the inverted Wishart distribution I W n (  X ;  X  ) (Gupta &amp; Nagar, 2000), as p (  X  j  X  ;  X  ) = where  X   X  0 is an n  X  n hyperparameter matrix. It also follo ws that D =  X   X  1 is distributed according to W n (  X ;  X   X  1 ). Moreo ver, the conditional distribution of D on K is W n ( r +  X ; ( K +  X  )  X  1 ) (Gupta &amp; Nagar, 2000).
 As for  X  , we could again de X ne  X  as a random ma-trix and then incorp orate another higher-lev el prior. However, for simplicit y,  X  is held  X xed in this pap er. In particular, we use the Gaussian kernel on the in-put set X to de X ne  X  . In general, other kernels, suc h as the commonly used polynomial kernel, Laplacian kernel and linear kernel (de X ned on X ), may also be used. Another possibilit y is to de X ne  X  as a weigh ted com bination of di X eren t kernel matrices. Moreo ver, for simplicit y, r and  X  are also held  X xed in this pap er. Our mo del is thus a hierarc hical mo del with three lev-els. The  X rst level corresp onds to a random Wishart matrix, the second level corresp onds to the parame-ter matrix of the Wishart matrix, and the third level corresp onds to the hyperparameter matrix of the pa-rameter matrix.
 The observ ed data set pro vides a particular realiza-tion of K . With an abuse of notation, we will denote this realization again by K . Note that K represen ts the partially observ ed kernel matrix, since only the K 11 part of K in (1) is available from the observ ed data, while both K 21 (and hence K 12 ) and K 22 are missing. In other words, if we consider this as a miss-ing data problem, then the incomplete (observ able) data is K 11 , the complete data is ( K 11 ; K 21 ; K 22 ), and the goal is to infer the missing data ( K 21 and K 22 ) and the unkno wn mo del parameters (  X  , or equiv a-lently D =  X   X  1 ). Consider that K  X  0 if and only if K 11  X  0 and K 22  X  1  X  0 (Horn &amp; Johnson, 1985), plemen t of K 11 . We tak e f K 11 ; K 21 ; K 22  X  1 g instead as the complete data to ensure that K is alw ays positiv e de X nite. Thus, the likeliho od function of the complete data is While Zhang et al. (2003a) prop osed an EM algo-rithm to solv e this missing data problem, we prop ose using the Tanner-W ong data augmen tation algorithm (Tanner &amp; Wong, 1987) as an alternativ e metho d for solving the same problem. In general, for the complete data Y = ( Y obs ; Y mis ) with unkno wn parameter  X  , the Tanner-W ong data aug-men tation starts from an initial parameter estimate  X  (0) and then rep eats the follo wing two steps in an alternating manner: It can be sho wn that this iterativ e pro cedure yields a stationary distribution p (  X ; Y mis j Y obs ). Further-more, the stationary distributions of the subsequences data augmen tation is a special case of Gibbs sampling with ( Y mis ;  X  ) partitioned into Y mis and  X  . It is also closely related to the EM algorithm, where the I-step corresp onds to the E-step and the P-step corresp onds to the M-step.
 For our hierarc hical mo del de X ned in the previous sec-tion, we have Y = K with Y obs = K 11 and Y mis = ( K 21 ; K 22  X  1 ), and  X  = D . Thus, Bayesian inference is based on the join t densit y of all variables, i.e., p ( K 11 ; K 21 ; K 22  X  1 ; D ) = (4) As for K in (1),  X  , D and  X  are similarly partitioned as From (Zhang et al., 2003a) or (Gupta &amp; Nagar, 2000), we have It is easy to see that D 11 =  X   X  1 11 rameter estimate D ( t ) , the I-step sim ulates K 21 and K 22  X  1 by dra wing from the follo wing distributions: and the P-step dra ws the parameter D from the fol-lowing distribution: Here Q ( t ) = ( K ( t ) +  X  )  X  1 and it is partitioned into Q = ditioning on all other variables. The P-step in (6) can also be expressed as the follo wing equiv alen t form: From the I-step in (5), we can see that only the values of D  X  1 22 D 21 and D  X  1 22 are required. Since our ultimate goal is to estimate the missing data K 21 and K 22  X  1 but not D , this motiv ates us to directly dra w D  X  1 22 and D  X  1 22 , instead of D 11  X  2 , D 21 and D 22 , in the P-step. Note that D  X  1 22 D 21 is a regression matrix. From results on the distribution of the regression matrix (see (Gupta &amp; Nagar, 2000)), we can obtain an alternativ e P-step as where s = r +  X   X  n 2 + 1 and T s T  X  m; M ; X ; Y  X  is a p  X  q random matrix with matrix variate t -distribution (Gupta &amp; Nagar, 2000) as p ( T j m; M ; X ; Y ) = K  X  has a strong resem blance to the EM algorithm pre-sen ted in (Zhang et al., 2003a). A ma jor practical problem with the metho d presen ted in the previous section is its high computational re-quiremen ts, since the Tanner-W ong algorithm has to work on matrix variate distributions. Moreo ver, to the best of our kno wledge, it is intractable to dra w a matrix from a matrix-v ariate normal distribution or a matrix-v ariate t -distribution, although it is tractable to dra w a matrix from a Wishart distribution. There-fore, it is necessary to seek an e X ectiv e and tractable implemen tation of the data augmen tation algorithm. In this section, we prop ose a simpli X ed Bayesian mo del that mak es it possible to dev elop an e X cien t imple-men tation.
 In the kernel matrix learning literature (Crammer et al., 2003; Cristianini et al., 2002; Lanc kriet et al., 2002; Tsuda et al., 2003), it is common to constrain the target kernel to a weigh ted com bination of some available base kernels so that the learning problem is simpli X ed to the estimation of the weigh ting coe X -cien ts. In particular, the target kernel matrix G is expressed as G = P n i =1  X  i  X  i  X  0 i with  X  i &gt; 0. Let U 0 = [  X  1 ;  X  2 ; : : : ;  X  n ] and L = diag (  X  1 ;  X  2 Then G = U 0 LU . We refer to U and  X  i  X  0 i 's as the base matrix and the base kernel matrices, resp ectiv ely. Now, given U , we want to estimate  X  i 's and hence G to appro ximate some desired kernel H , suc h as the ideal kernel (Cristianini et al., 2002), based on some crite-rion like the kernel alignmen t or the KL divergence between G and H . This motiv ates us to devise a sim-pli X ed Bayesian hierarc hical mo del and then an e X -cien t implemen tation of the Tanner-W ong algorithm. 4.1. A Simpli X ed Bayesian Hierarc hical Mo del We kno w that if X s W n ( r; Y ) and there exists a non-singular matrix C suc h that C 0 YC =  X  where  X  is diagonal, then C 0 XC s W n ( r;  X  ) (Gupta &amp; Nagar, 2000). We apply this prop erty to our basic mo del pre-sen ted in Section 2. Assume that we are given a non-singular matrix C suc h that C 0  X C is diagonal and we de X ne W = C 0 KC , then W is distributed according to the Wishart distribution with a diagonal parameter matrix  X  = diag (  X  1 ; : : : ;  X  n )  X  0. In particular, we give a mo del as Note that a Wishart matrix with a diagonal parameter matrix is not diagonal. So our simpli X ed mo del is fea-sible and the kernel matrix learning problem becomes estimating the matrices W 21 and W 22 .
 In a Bayesian hierarc hical framew ork,  X  can have its own prior distribution. Here, we assume that the  X  's are a priori conditionally indep enden t given some hyperparameters. In particular, we use the inverted Gamma distribution as a conjugate prior for the  X  j 's. Thus,  X   X  1 j s G (  X ;  X  ), i.e., it follo ws a Gamma distribution. In this pap er, we  X x the hyperparameter  X  and use as the prior for  X  .
 Our goal is to estimate both the missing data W 21 and W 22 and the unkno wn parameter  X  . In order to en-sure that W  X  0, we treat ( W 11 ; W 21 ; W 22  X  1 ) instead of ( W 11 ; W 21 ; W 22 ) as the complete data. Our prob-lem then becomes estimating the missing data W 21 and W 22  X  1 and the parameter  X  from the observ ed data W 11 . Bayesian inference is based on the join t densit y of all variables, i.e., The follo wing theorem (Gupta &amp; Nagar, 2000) will be very useful in the sequel.
 Theorem 1 Supp ose W s W n ( r;  X  ) with W ;  X   X  0 partitione d as wher e W 11 and  X  1 are both of size n 1  X  n 1 . Then (i) W 11 s W n (ii) W 21 j W 11 s N ( 0 ;  X  2  X  W 11 ) ; and Using Theorem 1(iii), (10) can thus be rewritten as leading to a simpli X ed hierarc hical mo del. 4.2. Tanner-W ong Algorithm for the In this subsection, we apply the Tanner-W ong algo-rithm to carry out Bayesian inference on the simpli- X ed hierarc hical mo del in (11). That is, we sim ulate the missing data W 21 and W 22  X  1 and the parameter  X  iterativ ely by (2) and (3). Giv en the curren t pa-rameter estimate  X  ( t ) = diag (  X  ( t ) 1 ; : : : ;  X  ( t ) sim ulates W 21 and W 22  X  1 by dra wing from the follo w-ing distributions: and the P-step dra ws the parameters  X  j 's and hyper-parameter  X  from the follo wing distributions: Here w ( t ) j is the ( j; j )th elemen t of W ( t ) . The I-step can be easily obtained from Theorem 1(ii) and (iii), and the deriv ation of the P-step is based on the full condition. Let W 0 21 = [ b 1 ; b 2 ; : : : ; b n is an n 1 -dimensional vector. Then the I-step (12) is equiv alen t to one that separately sim ulates b j for j = 1 ; : : : ; n 2 , instead of W 21 , by Our revised Tanner-W ong data augmen tation algo-rithm is tractable because it involves only random dra ws of the Gamma variable, the Gaussian vector, and the Wishart matrix.
 We can see that the computational cost of the cur-ren t algorithm is dominated by the I-step (13) for W 22  X  1 . Here we give a further predigestion. In par-ticular, we appro ximate W 22  X  1 with a diagonal matrix diag (  X  1 ;  X  2 ; : : : ;  X  n for j = 1 ; : : : ; n 2 . 4.3. Generalized Eigenproblem for Cho osing Having form ulated a simpli X ed hierarc hical mo del and devised a tractable Tanner-W ong algorithm, we now come to the question of how to choose the nonsingular matrix C . Note that C plays the same role as the base matrix U men tioned earlier in this section. In most ex-isting kernel matrix learning metho ds, a widely used approac h is to set U to be the eigen vector matrix of an empirical input kernel matrix. For our mo del, since the hyperparameter matrix  X  is a priori speci X ed, we can use the matrix consisting of the eigen vectors of  X  as a base matrix. In this subsection, by form u-lating a symmetric-de X nite generalized eigenproblem, we presen t an e X cien t approac h to specifying the base kernel matrices.
 Lik e K , we partition the input kernel matrix A and output kernel matrix B as Obviously , A is kno wn. For B , however, B 11 is kno wn but B 12 and B 22 are both unkno wn. Therefore, our problem is to estimate B 12 and B 22 from A and B 11 . We tackle this problem by form ulating a symmetric-de X nite gener alize d eigenpr oblem (Golub &amp; Loan, 1996). For the kernel matrices A 2 R n  X  n and B 2 R n  X  n , we consider a generalized eigen value system of the form Here and later, we assume that A  X  0 and B  X  0. With the generalized eigenproblem (Golub &amp; Loan, 1996), we kno w that A and B can be sim ultaneously diagonalized. More speci X cally , there exists a nonsin-gular matrix Q suc h that where  X  = diag (  X  1 ; : : : ;  X  n ) is a diagonal matrix and  X  1  X   X   X   X   X   X  n  X  0 are the roots of (16). However, it is intractable to determine suc h Q since some entries of B are unkno wn. On the other hand, it is easy to sim ultaneously diagonalize A 11 and B 11 . Our point of departure is to obtain an appro ximation of Q through co-diagonalizing A 11 and B 11 . The follo wing theorem pro vides a metho d for our purp ose.
 Theorem 2 Given A  X  0 and B  X  0 partitione d as in (1), ther e exists a nonsingular matrix such that wher e  X  1 is diagonal.
 The pro of of this theorem is given in App endix A. This theorem gives us a base matrix C and its pro of pro ce-dure pro vides a metho d to compute C . Denote the i th column vector of C by c i , then c i c i 's constitute a set of base kernel matrices. Recall that, unlik e the usual base matrix U , C is not orthonormal. Although or-thonormalit y is unnecessary for our problem, one may choose to orthonormalize C by using suc h metho ds as the Gram-Sc hmidt metho d since C is nonsingular. It is worth y to note that C is de X ned by using not only information from the input kernel matrix A but also information from the partial output kernel matrix B 11 . Information from both input and output is exp ected to be useful for classi X cation and regression problems. Let B = C 0 SC and K = C 0 WC . We partition S and W into tively. By simple arithmetic calculations, it is obvious that S 11 =  X  1 is diagonal. However, it is not alw ays true that S 12 = 0 and S 22 is diagonal. As a result, this implies that S is not necessarily diagonal. We now work with W instead of K so that the original transductiv e learning problem can be simpli X ed. In other words, we treat C as a base matrix and W as a matrix to be estimated. To demonstrate the e X cacy of our metho d, we apply it to the classi X cation problem on the ionosphere and wine data sets from the UCI Mac hine Learning Rep os-itory and also the USPS handwritten digit data set with 16  X  16 digit images. For simplicit y, we only use digits 1, 2, 3 and 4 corresp onding to four classes with 1269, 929, 824 and 852 examples, resp ectiv ely. As dis-cussed in Section 2, we set the complete kernel matrix as K = ( A + B ) = 2. We then follo w the pro cedure describ ed above to estimate the missing entries in K . We  X rst apply the metho d given in Section 4.3 to ob-tain the base matrix C as de X ned in (17). Next, we let W 11 = ( I +  X  1 ) = 2 where  X  1 is de X ned in (18) and then use the Tanner-W ong algorithm given in Section 4.2 to obtain W 21 and W 22 . We run the algorithm for 2,000 iterations. The  X rst 1,000 iterations are treated as burn-in; inference is based only on the remaining 1,000 iterations. We use the follo wing settings for the hyperparameters: r = n + 1 in (7),  X  = 3 : 0 in (8),  X  = 0 : 5 and  X  =  X  n 1 = tr( W 11 ) in (9) where  X  is a constan t in [0 : 9 ; 3 : 0] (here we set  X  = 1 : 2). In the last step, we set K = C 0 WC to obtain the complete kernel matrix K .
 After obtaining K , we implemen t the kernel nearest mean (KNM) classi X er using K . Details of its im-plemen tation can be found in (Zhang et al., 2003a). To compare it with some existing kernel classi X cation metho ds based on the input kernel A , we also im-plemen t kernel Fisher discriminan t analysis (KFD A), supp ort vector mac hine (SVM), and KNM. Moreo ver, we also implemen t the KNM classi X er describ ed in (Zhang et al., 2003a), whic h uses the EM algorithm to complete K . For con venience, we denote it as KNM-EM. Exp erimen ts on these classi X ers are performed with the follo wing parameter settings. We set  X  = 2 : 5 for the ionosphere and wine data sets and set  X  to be the average Euclidean distance for the training examples in the USPS data set. For SVM, we set the regular-ization parameter C = 300. The results are averaged over 30 random splits of the data. For ionosphere and wine , we use 60% of the data for training and the re-maining 40% for testing, while for USPS we use 99% for training and 1% for testing.
 Table 1 sho ws the classi X cation results. The standard deviations over 30 random splits are also sho wn in-side brac kets. In the table, KNM-TW1 refers to our metho d with the I-step for W 22  X  1 based on (13), while KNM-TW2 refers to the metho d when (15) is used instead for the I-step. We can see that the two meth-ods give very similar results. This sho ws that even though the more e X cien t KNM-TW2 metho d only mak es use of the appro ximate I-step in (15), it is ef-fectiv e enough in deliv ering comparable performance. Thus, for the much larger USPS data set, we use only KNM-TW2 whic h is more e X cien t. Note that KNM-EM also gives comparable classi X cation accuracies as KNM-TW's, although KNM-EM works under a fully Bayesian setting on the kernel matrix K . The classi- X cation accuracies of KFD A and SVM are also close, though generally lower. In this pap er, we have prop osed two Bayesian hier-archical mo dels for kernel matrix learning using the Tanner-W ong data augmen tation algorithm, whic h is a varian t of MCMC metho ds for missing data problems. Moreo ver, by form ulating a symmetric-de X nite gener-alized eigenproblem, we presen t a metho d for choosing the base kernel matrices. We have demonstrated the  X  X xibilit y and e X cacy of our metho d under the clas-si X cation setting. Although the form ulation in this pap er is based on the classi X cation problem, it can be extended to the regression problem when the output space is con tinuous rather than discrete. In our pre-vious work (Zhang et al., 2003b), we sho wed that the r parameter in the Wishart distribution is equal to the dimensionalit y of the feature space induced by K and it can be estimated using EM (Zhang et al., 2003a; Zhang et al., 2003b). In principle, we may also de X ne a prior distribution for r , suc h as a Gamma distribution, and then use the approac h presen ted in this pap er to estimate it.
 This researc h has been partially supp orted by the Researc h Gran ts Council of the Hong Kong Special Administrativ e Region under gran ts HKUST6195/02E and DAG03/04.EG36.
 Since A  X  0 and B  X  0, it then follo ws from (Golub &amp; Loan, 1996) that A 11  X  0, A 22  X  1  X  0, and B 11  X  0. Furthermore, from (Golub &amp; Loan, 1996), there exist nonsingular matrices C 1 and C 2 suc h that where  X  1 is diagonal and C 0 2 C 2 = A 22  X  1 . Note that
So we have Putting C = A = C 0 C .
 Amari, S. (1995). Information geometry of the EM and em algorithms for neural net works. Neur al Net-works , 8 , 1379{1408.
 Bousquet, O., &amp; Herrmann, D. J. L. (2003). On the complexit y of learning the kernel matrix. Advanc es in Neur al Information Processing Systems 15 . Cam-bridge, MA: MIT Press.
 Crammer, K., Keshet, J., &amp; Singer, Y. (2003). Kernel design using boosting. Advanc es in Neur al Informa-tion Processing Systems 15 . Cam bridge, MA: MIT Press.
 Cristianini, N., Kandola, J., Elissee X , A., &amp; Sha we-
Taylor, J. (2002). On kernel target alignmen t. Ad-vanc es in Neur al Information Processing Systems 14 . Cam bridge, MA: MIT Press.
 Dempster, A. P., Laird, N. M., &amp; Rubin, D. B. (1977). Maxim um likeliho od from incomplete data via the
EM algorithm. Journal of the Royal Statistic al So-ciety Series B , 39 , 1{38.
 Golub, G. H., &amp; Loan, C. F. V. (1996). Matrix compu-tations . Baltimore: The Johns Hopkins Univ ersit y Press. Third edition.
 Gupta, A., &amp; Nagar, D. (2000). Matrix variate distri-butions . Boca Raton, FL: Chapman &amp; Hall/CR C. Horn, R. A., &amp; Johnson, C. R. (1985). Matrix analysis . Cam bridge, UK: Cam bridge Univ ersit y Press. Kandola, J., Sha we-T aylor, J., &amp; Cristianini, N. (2002). Optimizing kernel alignment over combina-tions of kernels (Technical Rep ort 2002-121). Neu-roCOL T.
 Lanc kriet, G. R. G., Cristianini, N., Ghaoui, L. E.,
Bartlett, P., &amp; Jordan, M. I. (2002). Learning the kernel matrix with semi-de X nite programming. Pro-ceedings of the 19th International Confer ence on Machine Learning (pp. 323{330). Sydney , Australia. Schafer, J. L. (1997). Analysis of incomplete multi-variate data . Chapman &amp; Hall.
 Tanner, M. A., &amp; Wong, W. H. (1987). The calcula-tion of posterior distributions by data augmen tation (with discussion). Journal of the Americ an Statisti-cal Asso ciation , 82 , 528{550.
 Tsuda, K., Akaho, S., &amp; Asai, K. (2003). The em algorithm for kernel matrix completion with auxil-iary data. Journal of Machine Learning Research , 4 , 67{81.
 Zhang, Z. (2003). Learning metrics via discriminan t kernels and multidimensional scaling: Toward ex-pected Euclidean represen tation. Proceedings of the 20th International Confer ence on Machine Learning (pp. 872{879). Washington, D.C., USA.
 Zhang, Z., Kw ok, J. T., Yeung, D. Y., &amp;
Xiong, Y. (2003a). Bayesian transductive learn-ing of the kernel matrix using Wishart processes (Technical Rep ort HKUST-CS03-09). Depart-men t of Computer Science, Hong Kong Univ er-sity of Science and Technology . Available from ftp://ftp.cs.ust.hk/pub/techrep ort/03/tr03-09.ps.gz . Zhang, Z., Yeung, D. Y., &amp; Kw ok, J. T. (2003b).
Gaussian-Wishart processes: A statistic al view of kernels and its applic ation to kernel learn-ing (Technical Rep ort HKUST-CS03-15). Depart-men t of Computer Science, Hong Kong Univ er-sity of Science and Technology . Available from
