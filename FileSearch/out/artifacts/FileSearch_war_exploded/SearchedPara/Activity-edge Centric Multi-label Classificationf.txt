 Multi-label classification of heterogeneous information network-s has received renewed attention in social network analysis. In this paper, we present an activity-edge centric multi-label classi-fication framework for analyzing heterogeneous information net-works with three unique features. First, we model a heterogeneous information network in terms of a collaboration graph and multi-ple associated activity graphs. We introduce a novel concept of vertex-edge homophily in terms of both vertex labels and edge la-bels and transform a general collaboration graph into an activity-based collaboration multigraph by augmenting its edges with class labels from each activity graph through activity-based edge classi-fication. Second, we utilize the label vicinity to capture the pair-wise vertex closeness based on the labeling on the activity-based collaboration multigraph. We incorporate both the structure a ffi n-ity and the label vicinity into a unified classifier to speed up the classification convergence. Third, we design an iterative learning algorithm, AEC lass , to dynamically refine the classification result by continuously adjusting the weights on di ff erent activity-based edge classification schemes from multiple activity graphs, while constantly learning the contribution of the structure a ffi nity and the label vicinity in the unified classifier. Extensive evaluation on real datasets demonstrates that AEC lass outperforms existing represen-tative methods in terms of both e ff ectiveness and e ffi ciency. H.2.8 [ Database Applications ]: Data Mining Multi-label Classification; Heterogeneous Network; Activity-based Edge Classification; Collaboration Multigraph; Label Vicinity
Multi-label classification has received increasing attention in both data mining and machine learning over the last decade [10 X 22]. In contrast to single-label classification, multi-label classification analysis adopts a more realistic view that entities in the real world are often associated with multiple class labels simultaneously. For example, most people in a social network belong to multiple social groups and participate in multiple types of activities with di ff erent degrees of engagement. Most of web pages in the web graph may cover multiple topics at di ff erent intensities.

Existing multi-label classification e ff orts for networked data fo-cus on designing e ff ective and yet scalable algorithms [17 X 22]. Al-though previous studies di ff er from one another in the concrete ap-proaches to mining the linkage structure, to the best of our knowl-edge, they all su ff er from two weaknesses: (1) None of previous studies separate di ff erent types of activity graphs from the hetero-geneous information networks and exploit the correlations among the set of class labels within each activity graph and across multiple activity graphs; and (2) None of previous works combine both the vertex-centric multi-label classification and the edge-centric multi-label classification to boost the e ff ectiveness and e ffi ciency.
In this paper we show that by utilizing activity-edge centric ap-proach, we can incorporate the two missing dimensions to improve both the accuracy and the complexity of multi-label classification analysis. First, we argue that entities in the real world may involve themselves in multiple activity networks. These activity network-s may provide abundant information about heterogeneous entities and links, and how entities are linked in the context of each of ac-tivity networks. We aim to utilize these activity networks to find a natural and cheap way to identify the inter-dependencies among labels. Second, based on di ff erent activity networks, an entity can be tagged by a subset of K labels with di ff erent class-membership distributions. We model the class-membership distribution for each of activity networks as multi-labeled edges. Third, we consider not only the labels of related vertices but also the possible labels of as-sociated edges to further enhance the accuracy of multi-label clas-sification. We integrate the vertex-centric labeling and edge-centric labeling into a unified classifier with di ff erent weights. An iterative method is proposed to learn the weights towards the classification objective.

This paper makes the following original contributions to multi-label classification for networked data.  X  We model a heterogeneous information network in terms of a collaboration graph and multiple associated activity graphs, and cluster activity vertices in each activity graph into K categories with the given K class labels. Clustering each activity graph pro-vides a natural way to capture the dependencies among activity categories within activity graphs.  X  We introduce a novel concept of vertex-edge homophily in terms of both vertex labels and edge labels, and transform a general collaboration graph into an activity-based collaboration multi-graph by augmenting its edges with class labels from each activ-ity graph through activity-based edge classification.  X  We utilize the structure a ffi nity to capture the pairwise topolog-ical similarity of vertices and the label vicinity to capture the pairwise vertex closeness based on the labeling on the activity-based collaboration multigraph. We incorporate both the struc-(a) Coauthor Graph ture a ffi nity and the label vicinity into a unified classifier to speed up the classification convergence.  X  We design an iterative learning algorithm, AEClass , to dynami-cally refine the classification result by continuously adjusting the weights on di ff erent activity-based edge classification schemes from multiple activity graphs, while constantly learning the con-tribution of the structure a ffi nity and the label vicinity in the uni-fied classifier. To make the classification process converge fast, a sophisticated nonlinear fractional programming problem with multiple weights is transformed to a straightforward parametric programming problem of a single variable.  X  Empirical evaluation over real multi-label datasets demonstrates the competitiveness of AEC lass against state-of-the-art methods, in terms of both inference performance and time complexity.
We address the problem of multi-label classification for networked data by employing our activity-edge centric multi-label classifica-tion algorithm. First, we model a heterogeneous information net-work in terms of two types of information networks: (1) a collab-oration graph at the instance level, which is the target of multi-label classification, and (2) a collection of its associated activity graphs at the category level. For example, the DBLP bibliography dataset may consist of three types of vertices: authors, publication venues (e.g., conferences, journals), and title terms in the publi-cations. An author can publish in multiple venues and his papers may contain multiple terms. If the target of multi-label classifica-tion is to infer author X  X  labels, then we transform the DBLP dataset into a primary collaboration network for authors at the instance lev-el and two associated activity networks (conference-similarity net-work and term-similarity network) at the category level. The col-laboration network is defined based on both labeled and unlabeled instances with the given K class labels, where each vertex repre-sents one instance and each edge reflects the collaborative relation-ships between pairwise instances, e.g., the number of co-authored publications. Each of associated activity networks is constructed with all the associated activities as vertices. Similar activities are linked together with each edge value indicating the similarity be-tween pairwise activities, such as product purchasing activity net-work, sport activity network or conference activity network. Giv-en that each entity in the collaboration network may participate in multiple activities in each of activity networks, we cluster all activ-ities in each activity networks into K categories. Then we construct a collaboration multigraph by augmenting the original collabora-tion graph based on N activity networks as follow: For each pair of vertices with an edge in the collaboration graph, if both have par-ticipated in at least one of N activity networks, then we will add up to K edges between this pair of vertices.

Figures 1 gives an illustrative example extracted from the DBLP dataset, consisting of three graphs: a collaboration graph of au-thors, a conference activity graph and a term activity graph. For ease of presentation, we only choose the co-authored papers pub-lished in three top DB conferences of SIGMOD , VLDB and ICDE , and three top DM conferences of KDD , ICDM and SDM . In Figure 1 (a), ochre labels and green labels represent that authors are giv-en predefined class labels of DB and DM respectively. In addition, ochre block or green block in each vertex rectangle represents the proportion of an author belonging to class DB or DM . We want to use the label information of four labeled authors to learn the class-membership probabilities of Philip S. Yu over classes DB and DM . In Figure 1 (b), blue numbers measure the similarity scores be-tween pairwise conferences. We utilize a multi-typed soft cluster-ing framework, NetClus [25], to cluster conferences and terms into 24 CS research areas [31] simultaneously. According to confer-ence X  X  clustering distribution over 24 categories and ranking score in each category, we calculate the similarities between conferences in the conference activity graph. Similarly, red numbers in Figure 1 (c) measure the similarity scores between terms. We then choose a category with the highest probability for each conference or each term as its primary category and put them into the corresponding primary categories. This operation actually produces a hard clus-tering result for each activity network. As shown in Figure 2, the conferences and terms in Figures 1 (b) and (c) are put into their individual primary categories respectively.
 We formally define the above concepts as follows.

A collaboration graph is denoted as CG = ( V , F ), where V is the set of vertices representing the entities in CG , such as customers or authors, and F is the set of edges denoting the collaborative rela-tionships between members. We use N CG to represent the size of V , i.e., N CG = | V | .

An activity graph is defined by AG i = ( U i , F i ), where u  X  U notes an activity vertex in the i th associated activity network AG and f  X  F i is a weighted edge representing the similarity between two activity vertices, such as functional or manufacture similarity. We denote the size of each activity vertex set as N AG i = | U vertex set U i is partitioned into K disjoint primary categories, de-noted by U ip (1  X  p  X  K ), such that U i = S K p = 1 U ip  X  for  X  1  X  p , q  X  K , p , q and each activity category U with one of the K class labels, c p .

Given a collaboration graph CG = ( V , F ) and its N associat-ed activity graphs AG i = ( U i , F i ) (1  X  i  X  N ), a collaboration multigraph denoted as MG = ( V , E ), is an activity-edge augment-ed multigraph, where V has the same definition in CG and E is the set of edges satisfying the following condition: for each edge ( v , v j )  X  F in CG , we create a set of parallel edges between the pair of vertices in E . Each set of edges has up to K labeled edges and each edge corresponds to one activity category labeled by c ( p  X  { 1 ,  X  X  X  , K } ) in each of the N activity graphs.
The problem of multi-label classification of multigraph is de-fined as follows: let C = { c 1 , c 2 ,  X  X  X  , c K } be a finite set of K pos-sible class labels. Given a collaboration multigraph MG = ( V , E ) with a set of multi-label training instances V l  X  V initially labeled using the given K class labels, and a set of multi-label testing in-stances V u = V  X  V l unlabeled. For presentation brevity, we as-sume that the vertices in V are ordered and the first l vertices are labeled and the remaining vertices are unlabeled. Thus we have V = { v 1 ,  X  X  X  , v l , v l + 1 ,  X  X  X  , v N CG } . Let an instance v ciated with a subset of labels in C ,i.e., we use a binary vector y = ( y 1 i , y 2 i ,  X   X  X  , y K i )  X  { 0 , 1 } K , in which y the label set of v i . We use Y = { y 1 ,  X  X  X  , y l , y l + 1 a possible labeling for the instance set V . Y l = { y 1 ,  X  X  X  , y cates the observed multi-label set assigned to V l and Y u represents the multi-label set to be determined. The task of our activity-edge centric multi-label classification of multigraph is to use the label information of the training instances in V the label set Y u for the testing instances in V u .
Compared to existing multi-label relational classifiers outlined in Section 1, AEC lass improves both the accuracy and the e ffi -ciency of multi-label classification by incorporating four mining strategies: (1) activity-based edge classification; (2) edge label de-pendency; (3) vertex label vicinity; and (4) weight learning. We first introduce the overall design of AEC lass . We then describe each part of AEC lass in detail in the next subsections.  X  Activity-based edge classification, which consists of five tasks. (1) given a collaboration graph CG , choose N suitable activity graphs AG i based on the specific context defined by the classifi-cation objective; (2) cluster all AG i s into K activity categories; (3) construct an label dependency graph based on the cluster-ing of each AG i to identify inter-dependencies among K class labels; (4) based on K categories of each AG i , split and classify each unlabeled edge in CG into at most K labeled edges; and (5) transform CG and all AG i s into a unified multigraph MG by integrating N edge classification schemes of CG based on each
AG i weighted by  X  (1) 1 =  X  X  X  =  X  (1) N = 1 N .  X  A ctivity-edge centric vertex classification, which includes four tasks. (1) initialize a transition probability T (1) j of MG ; (2) ini-tialize a classification kernel K (1) j ; (3) infer the class-membership vector X (1) j on each class c j ; and (4) produce the class-membership vector Y (1) j by refining X (1) j with label dependency graphs.  X  Iterative learning, which has four steps. (1) solve the paramet-ric programming problem for classification objective to update structure a ffi nity T ( t + 1) j of CG with  X  ( t ) 1 ,..., X  by combining the structure a ffi nity T ( t + 1) j and the label vicini-round.
Existing classification models assume the existence of vertex ho-mophily, namely, similar vertices in nature are connected to each other with social links. For example, Philip S. Yu and Wei Fan have many co-authored works published on DM conferences, as shown in Figure 4 (a). However, the truth is not always like this. enti-ties that are connected together may be similar in di ff erent ways with respect to a given set of K class labels. As is known to all, Philip S. Yu and Ming-Syan Chen are experts on data mining, i.e., they both have more research publications in the area of data min-ing than in any other academic area such as database. However, as seen in Figure 4 (a), they have more co-authored papers published on DB conferences. Thus the vertex homophily is insu ffi cient to accurately infer the possible labels of an author. This motivates us to propose the concept of vertex-edge homophily, the principle that both links and their associated vertices should be similar and like-ly belong to the same classes, to further improve the accuracy of multi-label classification.

In order to capture the vertex-edge homophily in the multi-label classification, we first perform activity-based edge classification. (a) Conference-based Splitting For each activity graph and the original collaboration graph CG , we first construct an activity-edge augmented collaboration graph CG i by examining each edge and the pair of connected entities in CG and splitting each edge into a set of parallel edges based on each activity in AG i that this pair of entities have in common. The size of each set of parallel edges is at most N AG i , i.e., the number of activity vertices in AG i . Figures 3 (a) and (b) present the activity-edge aug-mented collaboration graphs of Figure 1 (a) based on conference activities in Figure 1 (b) and term activities in Figure (c) respec-tively. Each edge in Figure 1 (a) is divided into multiple edges in terms of the common conference venues or the common title terms in co-authored publications between the pair of co-authors.
However, such activity-based edge augmentation may lead to substantial increase in size of activity-edge augmented collabora-tion graphs. We address this issue by introducing activity-based edge augmentation with edge classification to e ffi ciently improve the scalability of classification. Concretely, we utilize the cluster-ing result by NetClus, i.e., the probability distribution of each ac-tivity over K categories, to infer the class labels of parallel edges in each CG i over the K categories.

Given the probability of the m th activity in AG i belonging to clus-ter (class) c j produced by NetClus, denoted by P ( L m = c can compute the class-membership probability of edge ( v p belonging to class c j based on AG i , denoted by P ( L pq where W ( p , q ) represents the value on edge ( v p , v q m ( p , q ) denotes the value on the m th edge between v p CG i , which is based on the m th activity in AG i . If there does not exist such an edge between v p and v q , then W i m ( p , q ) is equal to 0.
After generating the class-membership distribution of each edge in CG i , we reduce CG i to an activity-edge augmented collaboration graph CG i w ith classified edges by grouping at most N AG edges between any pair of vertices in CG i into at most K parallel edges in CG i .
 where W i j ( p , q ) represents the value on the edge with label c tween v p and v q in CG i . For ease of presentation, assuming that SIGMOD , VLDB , ICDE , database , query and relational only be-long to class DB with the probability of 1, and KDD , ICDM , SDM , mining , clustering and frequent just belong to class DM with the probability of 1, two CG i s in Figure 4 present the edge-classification results of two CG i s in Figures 3 respectively.
As N edge classification schemes of CG , i.e., CG i s (1  X  i  X  N ), may have di ff erent degree of contributions to vertex classification, we propose to integrate N edge classification schemes into a unified collaboration multigraph with di ff erent weighting factors  X   X 
N through dynamic weight tuning mechanism. Thus the unified weight value on the edge with label c j between v p and v (a) Conference-based Classifi-c ation
Note that W ( t ) j ( p , q ) keeps changing with  X  ( t ) dynamic weight learning. We set the initial W (1) j ( p , q ) with equal weighting factors of  X  (1) 1 ,  X  X  X  , X  (1) N = 1 N .

F igure 5 (a) shows the unified multigraph for our running ex-ample in Figure 1 by combining the links with the same labels between the same vertex pair from two activity-based edge clas-sification schemes in Figure 4 with equal weighting factor of 0.5.
With the unified multigraph MG , we below describe the activity-edge centric vertex classification, which integrates the activity-edge labels with the vertex labels among structurally relevant instances through transition probability on collaboration multigraph. Definition 1. [Transition Probability on Collaboration Multigraph] Let MG = ( V , E ) be a collaboration multigraph where V is the set of entity vertices and E is the set of parallel edges denoting the collaborative relationships on di ff erent classes between entities of MG . The transition probability on MG at the t th iteration can be defined by normalizing the edge values as follows.
 where T ( t ) j ( p , q ) represents the transition probability on the edge with label c j between v p and v q in MG . Here, we assume that Y i.e., the labels of the vertices in V l , are fixed during the classification process. Figure 5 (b) presents the transition probabilities of parallel edges from Philip S. Yu to other authors based on the collaboration multigraph in Figure 5 (a).
 We express the above transition probability in a matrix form. 1 ,  X  X  X  , 1 specifies l ones, and d p = P N CG r = 1 P K m = 1 N
CG ). T ( t ) j determines the transition probability on those edges with the class label of c j in MG .

Instead of decomposing the multi-label classification problem in-to a set of binary classification problems, we construct a unified normalize parallel edges with di ff erent class labels. The original transition operation is actually divided into two steps: (1) choose those edges with the objective class label in terms of classification objective; and (2) select an edge with the largest value from the above edges to jump.

Now we define the initial unified classification kernel K only utilizes the structure information of MG , i.e., those edges with label c j , due to the lack of label vicinity at initialization.
Since we have ordered the vertices in V such that the labeled nodes V l are indexed before the unlabeled nodes V u , we rewrite the unified classification kernel K (1) j as a block matrix. where K (1) jll is an l  X  l identity matrix representing the transition probability among labeled vertices, we set the l  X  ( N CG matrix K (1) jlu to be zero matrix since the labels on the vertices in V are fixed, the ( N CG  X  l )  X  l matrix K (1) jul specifies the transition probability from unlabeled vertices to labeled vertices, and K an ( N CG  X  l )  X  ( N CG  X  l ) matrix denoting the transition probability among unlabeled vertices.

Suppose that the class-membership matrix is denoted by X = [ X j  X  K ) based on class c j , we use its individual classification kernel K j to iteratively infer the probabilities of vertices on class c
Let X j = [ X jl ; X ju ] be the class-membership vector, where X indicates the probabilities of the labeled vertices in V l to class c j , and X ju represents the probabilities of the unlabeled vertices in V u belonging to class c j . Due to the labels on the vertices in V l are fixed, Eq. (8) is equivalent to the following formula.
After the t th iteration, the class-membership matrix is updated as follow.

Compared to existing multi-label relational classifiers, we ar-gue that AEClass based on the activity-edge augmented collab-oration multigraph can significantly improve the performance of multi-label classification: (1) accuracy improvement. Based on the vertex-edge homophily, we classify each edge in CG into at most K parallel edges in MG . During the classification process, AEClass only picks up those vertices and links with the same label as the current objective class c j , i.e., K ( t ) j and X ( t  X  1) ference. For example, given the class-membership probabilities of Ming-Syan Chen on classes DB and DM in Figure 5 (a), we want to infer the class-membership probabilities of Kun-Lung Wu on DB and DM . AEClass will produce a positive probability on DB and a zero probability on DM since there exists no edge with label DM between these two authors. In contrast, existing classifiers will out-put a higher probability on DM than on DB for any positive edge value between two authors in the original CG in Figure 1 (a). In fact, Kun-Lung Wu is known as a database researcher without any data mining publications. (2) e ffi ciency improvement. Based on the vertex homophily, no matter which class the current objective is, existing classifiers need to check each neighbor of a vertex and summarize the labels of all neighbors. In comparison, AEClass per-forms the similar summary at lower cost. When we classify an edge in CG into m parallel edges in MG , m is often much smaller than the number of K class labels. Suppose that the current objective
F igure 6: Edge Label Dependency by Category Similarity class is c j , for a neighbor of a vertex, there may not exist an edge with label c j between the vertex and this neighbor. Thus, the num-ber of neighbors of a vertex with edge label c j can be much smaller than the number of its neighbors, thus reducing the amount of un-necessary computations. Concretely, by utilizing the vertex-edge homophily, AEClass only needs to consider those links with label c and associated neighbors and further stops label propagation to the circle of those irrelevant neighbors (without link with label c in the next iterations. For the above example, we can safely ignore the operation of inferring the probability of Kun-Lung Wu on D-M since there exists no edge with label DM between two authors. More importantly, AEClass prevents the probability of Ming-Syan Chen on DM from being di ff used to both Kun-Lung Wu and the neighbor-based circle of Kun-Lung Wu .
We argue that the underlying correlations among di ff erent ac-tivity categories can have significant impact on the performance of multi-label classification. Based on activity graph partition, we first define the edge label similarity to capture the inter-dependencies a-mong K activity categories within each of N activity graphs.
Definition 2. [Edge Label Similarity] Let AG i = ( U i , F i th activity graph (1  X  i  X  N ), S i ( u m , u n ) be the similarity score between two activities u m , u n  X  U i in AG i , and U ip categories of U i with class labels of c p , c q  X  C respectively. The activity category similarity between c p and c q with respect to AG is also referred to as the edge label dependency between two edge labels c p and c q , and is defined as follow.

Figure 6 shows two edge label dependency graph by activity cat-egory similarity based on two class labels DB and DM with respect to the conference graph and the term graph in Figure 2 respectively.
We thus incorporate them into our AEClass framework to adjust vector on class c j , denoted by Y ( t ) ju , can be defined by integrating class-membership vectors on other classes in terms of the similarity scores between class c j and other classes.
 where the weight  X  ( t ) i for AG i is the same as in Eq. (3). The adjusted class-membership matrix is thus defined as follow.

One disadvantage of conventional iterative classifiers is that they often need lots of iterations to converge to a stationary distribution and the repeated label propagation causes a non-trivial computa-tional cost. Wang et al. [22] proposed a dynamic label propaga-tion (DLP) model by fusing both data features and data labels to improve the e ff ectiveness on multi-class / multi-label classification. However, the DLP model failed to quantify the weighted contribu-tions from data features and data labels such that it often can not work well on real classification tasks. We model the label vicinity to capture the pairwise vertex closeness based on the labeling on the activity-based collaboration multigraph by following the simi-lar idea. To improve both e ff ectiveness and e ffi ciency of classifica-tion, we design an iterative learning method to dynamically refine the classification results by continuously quantifying and adjust-ing the weights on the structure a ffi nity and on the label vicinity towards the classification objective.

Based on the transition probability T ( t ) j on CG , we define a di ff u-sion process to map the multigraph space into an N CG -dimensional sition probabilities on the edges with label c j from vertex v other vertices and P (  X  ( t ) j ( i )) = N (  X  ( t ) j ( i ) | T based on a heuristics rule: two instance vertices with highly simi-lar class-membership distributions are likely to be highly similar to as the similarity between vertices based on the class-membership distribution. Similarly, we map this label-based similarity space in-specifies the label-based similarity between vertex v i and the oth-linear projection operation based on T ( t + 1) j .  X  With the linear projection, we generate the following formula. The corresponding marginal distribution is given below.
 kernel K ( t + 1) j may lead to a degeneration at the beginning of clas-sification if the learned label information of vertices in V enough to infer the label-based similarity scores, we adjust K by integrating the label-based similarity through T ( t ) similarity scores. The label vicinity ( T ( t + 1) j Y ( t ) )( tatively measures the extent of similarity between vertices and their neighbors based on the current labeling.
Classification analysis often utilizes the F1 score, i.e., the har-monic mean of precision and recall, to evaluate the accuracy of test-ing instances. The objective of multi-label classification of multi-graph is to maximize the Macro-F1 score [32], i.e., the unweighted mean of F1 score on classes. To define the Macro-F1 score, we first introduce an indicator function.
 where I ( X  y j i = 1) indicates whether the label c j is assigned to an instance vertex v i .

Definition 3. [Macro-F1] Let MG = ( V , E ) be a collaboration multigraph, y i be the true label vector of the i th instance vertices in V and  X  y i be the predicted label vector, and the Macro-F1 score is defined below.

Assuming  X  = max define an s-shape function to approximate the indicator function. The decision rule determining  X  y j i = 1 if Y ( i , j ) &gt;  X / 2, i.e., S ( Y ( i , j )) &gt; 0 . 5 is represented as follow.
 The Macro-F1 score is thus approximated as follow.

According to Eqs.(3)-(17), the Macro-F1 score is a fractional function of multi variables  X , X , X  1 ,  X  X  X  , X  N with non-negative real coe ffi cients. On the other hand, the numerator and the denominator of Macro-F1 are both polynomial functions of the above variables. Without loss of generality, we rewrite Eq.(22) as follow. where there are m polynomial terms in the numerator and n poly-nomial terms in the denominator, a i and o i are the coe ffi cients of the i th terms respectively, and b i , c i , d ij , p i , q of corresponding variables in the i th terms respectively.
Definition 4. [Multigraph Classification Objective] Let MG = ( V , E ) be a collaboration multigraph,  X , X , X  1 ,  X  X  X  , X  ing factors defined in Eqs.(3) and (17), respectively. The goal of multi-label classification of multigraph is to maximize the Macro-F1 score. subject to  X  +  X  = 1,  X , X  &gt; 0, P N j = 1  X  j = 1,  X  j &gt;
For ease of presentation, we revise the original objective as the following nonlinear fractional programming problem (NFPP).
Definition 5. [Nonlinear Fractional Programming Problem] Let , X  vised below. subject to  X  +  X  = 1,  X , X  &gt; 0, P N i = 1  X  i = 1,  X  i &gt;
Our classification objective is equivalent to maximize a quotient of two polynomial functions of multiple variables. It is very hard to perform function trend identification and estimation to determine the existence and uniqueness of solutions. Therefore, we want to transform this sophisticated NFPP into a easily solvable problem.
T heorem 1. The NFPP in Definition 5 is equivalent to a polyno-mial programming problem with polynomial constraints (PPPPC). subject to  X  +  X  = 1 ,  X , X  &gt; 0 , P N i = 1  X  i = 1 ,  X  0 6  X  6 1 / g (  X , X , X  1 ,..., X  N ) .
 Proof. If (  X ,  X ,  X  1 , ...,  X  N ,  X  ) is an optimal solution of PPP-PC, then  X  = 1 / g (  X ,  X ,  X  1 , ...,  X  N ) . Thus  X  f (  X ,  X ,  X  (  X , X , X  1 ,..., X  N ) of NFPP, the constraints of PPPPC are satis-fied by setting  X  = 1 / g (  X , X , X  1 ,..., X  N ) , so  X  f (  X , X , X   X  f (  X ,  X ,  X  1 , ...,  X  N ) , i.e. f (  X , X , X  1 ,..., X  f (  X ,  X ,  X  1 , ...,  X  N ) / g (  X ,  X ,  X  1 , ...,  X  N ) . ble solution (  X , X , X  1 ,..., X  N , X  ) of PPPPC we have  X  f (  X , X , X   X  ) 6 f (  X , X , X  1 ,..., X  N ) / g (  X , X , X  1 ,..., X  N ) 6 f (  X ,  X ,  X  / g (  X ,  X ,  X  1 , ...,  X  N ) =  X  f (  X ,  X ,  X  1 , ...,  X  ,  X  N ) .

A lthough PPPPC is a polynomial programming problem, the polynomial constraints make it very hard to solve. We further sim-plify it as an nonlinear parametric programming problem (NPPP).
Definition 6. [Nonlinear Parametric Programming Problem] Let  X  subject to  X  +  X  = 1,  X , X  &gt; 0, P N i = 1  X  i = 1,  X  i &gt; T heorem 2. The NFPP in Definition 5 is equivalent to the NPP-P in Definition 6, i.e.,  X  = max  X  g (  X , X , X  1 ,..., X  N ) = 0 .
 f (  X ,  X ,  X  1 , ...,  X  N )  X   X  g (  X ,  X ,  X  1 , ...,  X  N , X 
N )  X   X  g (  X , X  1 ,..., X  N ) 6 f (  X ,  X ,  X  1 , ...,  X  N )  X   X  g (  X ,  X ,  X  = 0 . We have  X  = f (  X ,  X ,  X  1 , ...,  X  N ) / g (  X ,  X ,  X  ..., X  N ) / g (  X , X  1 ,..., X  N ) . Therefore,  X  is a maximum value of NF-PP and (  X ,  X ,  X  1 , ...,  X  N ) is an optimal solution of NFPP.
Conversely, if (  X ,  X ,  X  1 , ...,  X  N ) solves NFPP, then we have  X  = f (  X ,  X ,  X  1 , ...,  X  N ) / g (  X ,  X ,  X  1 , ...,  X  N ) ..., X  N ) . Thus f (  X , X  1 ,..., X  N )  X   X  g (  X , X  1 ,..., X  ,  X 
N )  X   X  g (  X ,  X ,  X  1 , ...,  X  N ) = 0 . We have  X  (  X  ) = 0 and the maxi-mum is taken at (  X ,  X ,  X  1 , ...,  X  N ) .

Now the original NFPP has been successfully transformed into the straightforward NPPP. This transformation can e ffi ciently speed up the classification convergence due to the following properties. T heorem 3.  X  (  X  ) is convex.

Proof: Suppose that (  X ,  X ,  X  1 , ...,  X  N ) is an optimal solution of  X  ((1  X   X  )  X  1 +  X  X  2 ) with  X  1 ,  X  2 and 0 6  X  6 1 .  X  ((1  X   X  )  X   X  X  2 ) = f (  X ,  X ,  X  1 , ...,  X  N )  X  ( (1  X   X  )  X  1 +  X  X  2  X  ( f (  X ,  X ,  X  1 , ...,  X  N )  X   X  2 g (  X ,  X ,  X  1 , ...,  X   X  )  X   X  1 g (  X ,  X ,  X  1 , ...,  X  N ) ) 6  X  max  X , X  1 ,..., X  N ) + (1  X   X  ) max  X  ) =  X   X  (  X  2 ) + (1  X   X  )  X  (  X  1 ) . Thus,  X  (  X  ) is convex. T heorem 4.  X  (  X  ) is monotonically decreasing.
 Proof: Suppose that  X  1 &gt; X  2 and (  X ,  X ,  X  1 , ...,  X  solution of  X  (  X  1 ) . Thus,  X  (  X  1 ) = f (  X ,  X ,  X  1 , ...,  X  ,  X  N ) &lt; f (  X ,  X ,  X  1 , ...,  X  N )  X   X  2 g (  X ,  X ,  X  1 , ...,  X   X , X  1 ,..., X  N )  X   X  2 g (  X , X , X  1 ,..., X  N ) =  X  (  X  2 ) . T heorem 5.  X  (  X  ) = 0 has a unique solution.

Proof: Based on the above-mentioned theorems, we know  X  (  X  ) is continuous as well as decreasing. In addition, lim  X   X  +  X  and lim  X   X  X  X  X   X  (  X  ) =+  X  .

The procedure of solving this NPPP includes two parts: (1) find such a reasonable parameter  X  (  X  (  X  ) = 0), making NPPP equivalent to NFPP; (2) given the parameter  X  , solve a polynomial program-ming problem about the original variables  X , X , X  1 ,..., X  N weight adjustment mechanism is an iterative procedure to find the solution of  X  (  X  ) = 0 and the corresponding weights after each iteration of the classification process. We first generate an ini-tial unified classification kernel K (1) j with equal weights of produce an initial classification result on the collaboration multi-graph. According to the initial classification result, we then calcu-late an initial  X  (  X  ). Since  X  (  X  ) is a monotonic decreasing function and  X  (0) = max we start with an initial  X  = 0 and solve the subproblem  X  (0) by using existing fast polynomial programming model to update the weights  X , X , X  1 ,..., X  N . The parameter  X  is gradually increased by  X  = f (  X , X , X  1 ,..., X  N ) / g (  X , X , X  1 ,..., X  N ) to help the algorith-m enter the next round. The algorithm repeats the above-mentioned iterative procedure until  X  (  X  ) converges to 0.

By assembling di ff erent pieces together, we provide the pseudo code of our AEClass classifier in Algorithm 1. Algorithm 1 A ctivity-E dge Centric Multi-label Class ification 1: Invoke NetClus to partition each of N kinds of activities into K clusters 2: Calculate the category similarity on each AG i in Eq.(11); 3: Execute the edge classification on CG based on each AG 4: Construct the collaboration multigraph MG ; 6: Calculate the transition probability T (1) j for each c 10: Compute the Macro-F1 score in Eq.(22);
W e have performed extensive experiments to evaluate the perfor-mance of our AEClass classifier on real graph datasets.
The first real-world dataset is extracted from the DBLP Bibli-100 , 000 authors from all research areas and 712 , 834 associated links where vertices represent authors and edges represent their coauthor relationships, and two associated activity graphs: con-ference graph and term graph. According to [31], we categorize research areas into 24 fields: AI, AIGO, ARC, BIO, CV, DB, DIS-T, DM, EDU, GRP, HCI, IR, ML, MUL, NLP, NW, OS, PL, RT, SC, SE, SEC, SIM, WWW. We utilize a multi-typed soft cluster-ing framework, NetClus [25], to cluster conferences and terms into 24 categories simultaneously. According to conference X  X  or ter-m X  X  clustering distribution over 24 categories and ranking score in each category, we calculate the similarities between conferences or terms. The classification goal is infer research areas of each author. API call user.getfriends to collect the list of friends and construct a friendship graph with 50 , 000 users and 496 , 611 associated links where vertices represent users and edges denote their friendships. The two activity networks: artist graph and track graph are generat-ed by invoking the API calls artist.getSimilar and track.getSimilar respectively. By calling the API calls user.getTopArtists and us-er.getTopTracks , we classify each friendship edge in terms of the same artists or the same tracks shared by two users. The classifi-cation task is to assign each user to a subset of 21 music genres in the database: acoustic, ambient, blues, classical, country, electron-ic, emo, folk, hardcore, hip hop, indie, jazz, latin, metal, pop, pop punk, punk, reggae, rnb, rock, soul. 1 h ttp: // dblp.uni-trier.de / xml / 2 http: // www.last.fm / api (a) Macro-F1
The third real dataset is extracted from the Internet Movie Database (IMDb) 3 . We construct a collaboration graph with 10 , 000 high-ly prolific actors and 270 , 227 links where vertices represent ac-tors and edges specify their costar relationships in terms of co-appearance of actors in the same movies. We build a movie activity graph where edges denote co-direct relationship between movies, i.e., movies are directed by the same directors. The objective is to associate each actor with a subset of 22 movie genres: Action, Adventure, Animation, Biography, Comedy, Crime, Documentary, Drama, Family, Fantasy, Film-Noir, History, Horror, Music, Musi-cal, Mystery, Romance, Sci-Fi, Sport, Thriller, War, Western.
We compare AEClass with two representative link-based classi-fication algorithms, LBC [1], wvRN [2], and two recently devel-oped multi-label classifiers, EdgeCluster [18], SCRN [20]. All four methods perform multi-label classification on a single weight-ed graph based on the assumption of vertex homophily. The de-tailed introductions for four methods are presented in Section 5. Note that LBC is originally a multi-class classifier. In order to compare all algorithms, we modify the last step in LBC and use the posterior probability distribution over K classes as the multi-label classification result. AEClass integrates multiple information networks into a unified multigraph with combining both the vertex-centric multi-label classification and the edge-centric multi-label classification based on vertex-edge homophily. It also integrates both the structure a ffi nity and the label vicinity into a unified clas-sifier through dynamic weight tuning mechanism.

Evaluation Measures We use three measures to evaluate the quality of classification results generated by di ff erent methods. The first measure is Macro-F1 defined in Definition 3. Given the same definitions in Eq.(18), other two metrics are defined as follows. Micro-F1 [32] represents the harmonic mean of micro average of precision and recall. The larger the value, the better the quality. where  X  represents the XOR operation, and || X  || 1 specifies the l 1-norm. Hamming Loss [33] measures the loss between true labels and predicted labels. The smaller the value, the better the quality.
Figures 7-9 exhibit the classification quality on DBLP, Last.fm and IMDb by varying the proportion of labeled vertices respective-ly. For each proportion of labeled vertices, we average the perfor-mance scores over 10 cross-validation folds. The average perfor-mance scores with standard deviations of five multi-label classifi-cation methods are reported with respect to three evaluation mea-sures of Macro-F1, Micro-F1 and Hamming Loss. We make the following observations on the performances by di ff erent methods. 3 h ttp: // www.imdb.com / interfaces (a) Macro-F1 (a) Macro-F1
First, AEClass, SCRN and wvRN significantly outperform LBC and EdgeCluster on all three evaluation measures. We first cate-gorize five multi-label classification methods into non-transductive learning methods and transductive learning methods, based on how they utilize topological structure information. As non-transductive learning methods, both LBC and EdgeCluster only utilize the di-rect links between vertices in the graph, i.e., one-hop structure in-formation, to produce vertex X  X  features. As transductive learning approaches, AEClass, SCRN and wvRN make full use of both di-rect links and indirect edges (the circle of friends) between vertices through iterative graph propagation, i.e., multiple-hop structure in-formation, to further improve the classification quality. These re-sults demonstrate the importance of exploiting both direct links and indirect edges for multi-label classification in networked data.
Second, SCRN always outperforms wvRN on three graph dataset-s. Although SCRN and wvRN exploit the very similar relational inference framework, SCRN improves wvRN by integrating both the network topology and the social context features extracted by EdgeCluster into the classifier. A careful examination reveals that these two approaches are very close in terms of prediction perfor-mance in many situations, in spite of the optimization adopted by SCRN. A reasonable explanation is that both of them are only based on the assumption of vertex homophily, i.e., the principle that sim-ilar vertices in nature are connected to each other with social links.
Finally, among all five classification methods, AEClass achieves the best classification performance on all three real datasets for al-l three evaluation measures. Compared to other algorithms, AE-Class averagely achieves 14.6% Macro-F1 increase, 12.1% Micro-F1 boost and 5.2% loss reduction on DBLP, 10.2% Macro-F1 growth, 9.9% Micro-F1 increase and 4.1% loss decrease on Last.fm, and 16.7% Macro-F1 increase, 16.2% Micro-F1 boost and 7.5% loss re-duction on IMDb, respectively. Note that even if the proportion of labeled vertices is very small, such as 2% and 4%, AEClass still can achieve comparable accuracy on all datasets. Concretely, there are four critical reasons for high accuracy of AEClass: (1) the structure information from associated activity networks boosts the e ff ective-ness of classification. Activity network partition provides us with additional activity labels; (2) the multigraph organization integrates both the vertex-centric multi-label classification based on vertex homophily and the edge-centric multi-label classification based on vertex-edge homophily to leverage the classification performance; (3) Activity network partition captures the inter-dependencies a-mong multiple class labels; and (4) the iterative learning algorithm help the classifier achieve a good balance among di ff erent activity-based edge classification schemes and an e ff ective integration of the structure a ffi nity and the label vicinity. Figures 10 (a), (b) and (c) present the classification time on D-BLP, Last.fm and IMDb with di ff erent proportions of labeled ver-tices respectively. First, LBC has lowest runtime in seconds com-pared to all other algorithms in all experiments, since it is a logistic regression classifier by aggregating the labels of neighbors as ver-tex X  X  feature vector. Second, EdgeCluster, a linear SVM classifier with an edge clustering scheme to extract sparse social dimension-s, is slightly slower than LBC since the linear SVM approaches generally fall behind the LR methods in speed. Both LBC and EdgeCluster are faster than other three methods because both only utilize the direct links between vertices, i.e., one-hop structure in-formation. In comparison, AEClass, SCRN and wvRN use both di-rect links and indirect edges (the circle of friends) between vertices, i.e., multiple-hop structure information. Thus, the last three classi-fiers have higher time complexity than the first two models but they achieve better classification quality. Third, wvRN is consistently faster than SCRN on all three datasets. SCRN improves wvRN by integrating the social dimensions extracted by EdgeCluster into the classifier. This improvement results in an additional computational cost for calculating the class propagation probability of each vertex on each class. Finally, AEClass significantly outperforms the other two transductive learning based multi-label classifiers: SCRN and wvRN. Although SCRN and wvRN execute the classification on a general graph, AEClass does classification on an activity-based collaboration multigraph by augmenting its edges with class labels from each activity graph. There are three main reasons for high ef-ficiency of AEClass: (1) the multigraph organization increases the size of dataset but reduces the computational cost of classification. As we discussed in Subsection 3.2, based on the vertex homophi-ly, no matter which class the current objective is, both SCRN and wvRN need to check each neighbor of a vertex and summarize the labels of all neighbors. In contrast, AEClass only picks up those vertices and links with the same label as the current objective class to execute the inference. Most importantly, AEClass stops the label propagation through irrelevant neighbors (without a link with the same label as the current objective class) in the future iterations; (2) the label vicinity between vertices based on the class-membership distribution over K classes is integrated into the classifier; and (3) we transform the original nonlinear fractional programming prob-lem of multiple weights into a nonlinear parametric programming problem of single variable. According to Theorems 2-5, solving  X  (  X  ) for a given  X  is a polynomial programming problem which can be sped up by existing fast polynomial programming model.
Figure 11 (a) and (b) exhibit the trend of classification conver-gence in terms of Macro-F1, Micro-F1, and Hamming Loss on D-BLP with 4% label nodes and Last.fm with 5% label vertices. Both the Macro-F1 values and the Micro-F1 scores in two figures keep increasing and have concave curves when we iteratively perform the tasks of vertex labeling, weight update and kernel adjustment during the classification process. On the other hand, the Hamming Loss values decrease with the classification iterations and have a convex curve. The classification process converges very quickly, usually in eight iterations for Last.fm and nine iterations for DBLP. Figure 12 (a) and (b) show the tendency of weight update on DBLP and Last.fm respectively.  X  and  X  in two figures represen-t the weights of structure a ffi nity and label vicinity in the unified classifier K ( t + 1) j in Eq.(17) respectively.  X  1 and  X  (a) denote the weights of the conference graph and the term graph respectively.  X  1 and  X  2 in Figure 12 (b) represent the weights of the artist graph and the track graph respectively. We keep the con-straints  X  +  X  = 1 and  X  1 +  X  2 = 1 during the classification process. We observe that all the weights converge as the clustering process con-verges. An interesting phenomenon is that  X  first increases and then decreases with the iterations and the  X  curve has a converse trend. A reasonable explanation is that there is lack of enough labeling information at the beginning of classification such that the unified classifier has to rely mostly on the structure a ffi nity to achieve a good classification performance. After a few iterations, we have e-nough labeling information to utilize both the structure a ffi nity and the label vicinity to classify vertices. An interesting finding is that the term weight is increasing but the conference weight is decreas-ing with more iterations. A reasonable explanation is that people who have many publications on the same conferences may have di ff erent research topics but people who have many papers with the same terms usually have the same research topics. For exam-ple, both database papers and data mining papers are published on VLDB . Similarly, the track weight increases but the artist weight decreases with more iterations. This is because users who favor the same artists may belong to di ff erent music genres since the artist-s are often related to multiple genres but users who like the same tracks usually belong to the same music genres.
We examine some details of the experiment results on DBLP 100 , 000 Authors when the proportion of labeled vertices is equal to 32% based on the coauthor graph, the conference graph and the term graph. Table 1 shows the set of authors and their class-membership probabilities after seven iterations based on 24 con-ference categories and 24 term categories. We only present most prolific DBLP experts in the area of database (DB), data mining (DM), machine learning (ML) and information retrieval (IR). The class-membership scores in Table 1 are normalized by di ff eren-t (conference or term) categories for each author. We observe that the predicted class memberships of authors are consistent with their Table 1: Class-membership Probabilities of Authors Based on C onference and Keyword Partitions from DBLP actual research areas. For those experts with unique research areas, such as Michael J. Carey and Michael Stonebraker , the primary re-search areas for them in the predicted result are obviously consis-tent with their actual research areas; For those researchers known to work in multiple research areas, the predicted class-membership distributions also correspond to their current research activities. For example, both Jiawei Han and Philip S. Yu are experts on data min-ing and database, though their DM probabilities are slightly higher since each of them and their circle of co-authors have more DM pa-pers. This table also shows that each author has a class-membership score in each category. This demonstrates that our AEClass model can make each author quickly reach each class label.
Node classification in networked data has attracted active re-search in the last decade [1 X 9]. LBC [1] is a network-only deriva-tive of the link-based classifier which creates a feature vector for a node by aggregating the labels of neighboring nodes, and then uses logistic regression to build a discriminative model based on these feature vectors. wvRN [2] presented a weighted-vote rela-tional neighbor classifier to solve link-based classification problem-s based solely on the class labels of linked neighbors. DYCOS [6] exhibited a node classification model in dynamic information net-works with both text content and links. RankClass [7] integrates classification and ranking in a mutually enhancing process to pro-vide class summaries for heterogeneous information networks.
Multi-label classification is gaining attention in recent years [10 X  16]. Read et al. [12] reduces the complexity and potential for er-ror with a pruning procedure to focus on core relationships within multi-label sets. IBLR [13] proposed a multi-label classification approach to combine model-based and similarity-based inference with the estimation of optimal regression coe ffi cients. LEAD [15] decomposes a multi-label learning task into a set of single-label classification problems with a Bayesian network to encode the con-ditional dependencies of labels as well as the feature set. Guo and Gu [16] proposed a generalized conditional dependency network for model training using binary classifiers and label predictions us-ing Gibbs sampling inference.

Multi-label classification in networked data has been extensively studied in recent years [17 X 22]. Sun et al. [17] presented a hyper-graph spectral learning formulation for multi-label classification, where a hypergraph is constructed to exploit the correlation infor-mation among di ff erent labels. EdgeCluster [18] presented a social-dimension based approach for collective behavior prediction with an edge clustering scheme to extract sparse social dimension s and a linear SVM classifier for discriminative learning. SCRN [20] is a multi-label iterative relational neighbor classifier by considering both network topology and social context features. PIPL [21] facil-itates the multi-label learning process by mining label correlations and instance correlations from the heterogeneous networks.
Recent works on heterogeneous social network analysis [7,9,23 X  30] combine links and content into heterogeneous information net-works to improve the quality of querying, ranking and clustering. Cai et al. [23] proposed to learn an optimal linear combination of d-i ff erent relations on heterogeneous social networks in terms of their importance on a certain query. GenClus [28] proposed a model-based method for clustering heterogeneous networks with di ff erent link types and di ff erent attribute types. Yu et al. [29] presented a query-driven discovery system for finding semantically similar substructures in heterogeneous networks.

To our knowledge, this work is the first one to address the prob-lem of activity-edge centric multi-label classification of heteroge-neous multigraph with the prior knowledge of multiple activity graphs by dynamically adjusting their individual contributions.
We have presented an edge-centric multi-label classification ap-proach for mining heterogeneous information networks. First, we integrate the primary social network and multiple associated activi-ty networks into a unified multigraph with edge classification. Sec-ond, we combine both the structure a ffi nity and the label vicinity based on multiple activity networks into a unified classifier. Third, an iterative learning algorithm is proposed dynamically refine the classification result by continuously adjusting the weights on di ff er-ent activity-based edge classification schemes from multiple activ-ity graphs, while constantly learning the contributions of the struc-ture a ffi nity and the label vicinity in the unified classifier. Acknowledgement. This material is based upon work partially supported by the NSF under Grants IIS-0905493, CNS-1115375, IIP-1230740, and a grant from Intel ISTC on Cloud Computing. [1] Q. Lu and L. Getoor. Link-based classification. In ICML X 03 . [2] S. A. Macskassy and F. Provost. A simple relational [3] D. Zhou, O. Bousquet, T. N. Lal, J. Weston, and [4] J. Neville and D. Jensen. Relational dependency networks. [5] S. A. Macskassy and F. Provost. Classification in networked [6] C. C. Aggarwal and N. Li. On Node Classification in [7] M. Ji, J. Han, and M. Danilevsky. Ranking-based [8] S. Bhagat, G. Cormode, and S. Muthukrishnan. Node [9] X. Kong, P. S. Yu, Y. Ding, and D. J. Wild. Meta path-based [10] S. Godbole and S. Sarawagi. Discriminative methods for [11] G. Chen, Y. Song, F. Wang, and C. Zhang. Semi-supervised [12] J. Read, B. Pfahringer, and G. Holmes. Multi-label [13] W. Cheng and E. Hullermeier. Combining instance-based [14] K. Dembczynski, W. Cheng, and E. Hullermeier. Bayes [15] M.-L. Zhang and K. Zhang. Multi-label learning by [16] Y. Guo and S. Gu. Multi-label classification using [17] L. Sun, S. Ji, and J. Ye. Hypergraph spectral learning for [18] L. Tang and H. Liu. Scalable learning of collective behavior [19] S. Peters, Y. Jacob, L. Denoyer, and P. Gallinari. Iterative [20] X. Wang and G. Sukthankar. Multi-label relational neighbor [21] X. Kong, B. Cao, and P. S. Yu. Multi-label classification by [22] B. Wang, Z. Tu, and J. K. Tsotsos. Dynamic label [23] D. Cai, Z. Shao, X. He, X. Yan, and J. Han. Community [24] Y. Zhou, H. Cheng, and J. X. Yu. Graph clustering based on [25] Y. Sun, Y. Yu, and J. Han. Ranking-based clustering of [26] T. Yang, R. Jin, Y. Chi, and S. Zhu. Combining link and [27] Y. Zhou, H. Cheng, and J. X. Yu. Clustering large attributed [28] Y. Sun, C. C. Aggarwal, and J. Han. Relation strength-aware [29] X. Yu, Y. Sun, P. Zhao, and J. Han. Query-driven discovery [30] Y. Zhou and L. Liu. Social influence based clustering of [31] T. Chakraborty, S. Sikdar, V. Tammana, N. Ganguly, and [32] R.-E. Fan and C.-J. Lin. A study on threshold selection for [33] X. Zhang, Q. Yuan, S. Zhao, W. Fan, W. Zheng, and
