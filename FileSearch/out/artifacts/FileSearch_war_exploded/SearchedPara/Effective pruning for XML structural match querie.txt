 1. Introduction
Extensible Markup Language (XML) has become the de facto standard for data exchange over the Internet because of its inborn features, such as machine-independence, vendor-independence, flexibility, scalability. The structure of XML data can be defined by DTD (Document Type Definition) or XML Schema. Due to the proliferation of XML documents, many query lan-guages, such as XPath and XQuery are designed to retrieve XML fragments.

One of the challenges is efficiently processing queries over large XML document collections. A large amount of work has been reported on XML query optimization. It can be partitioned into two main areas: efficient structural query processing in relational databases [1,6,10,19,20,22 X 24,41,47,49] ; and XML structure indexing [7,11,25,26,29,30,34] . Effective pruning of candidate documents for structural query processing is still a very active research topic. Structural query processing refers to finding all occurrences of a given set of structural relationships (such as parent X  X hild and ancestor X  X escendant relation-ships) in an XML database. A core component of XPath, XQuery and tree pattern queries is the structural relationships spec-ified in them. The following is an example query that retrieves all sections of papers that contain at least one figure and at least one table:
Most existing structural query processing algorithms suffer from the following drawbacks: 1. Inability to complement each other without modifying the query processing engine itself. That is, one algorithm cannot be laid on top of another to further speed-up query processing without modifying the existing query processing engine. 2. Incapability of being customized to take advantage of different structural and usage characteristics. For example, if the height of the documents in the collection varies significantly, then the system can be customized to use the height of the document as a property for pruning candidate documents. However, if all documents in the collection have the same height, then height should not be used as a pruning property and other properties may be more appropriate.
In this paper, we propose a new approach, the Property-Driven Pruning Algorithm (PDPA), which overcomes the two drawbacks mentioned above. PDPA overcomes the first drawback by introducing the feature of structural query processing independence , which allows PDPA to be used on top of any existing structural query processing engine without modifying the query engine at all, as long as it processes queries in a top-down manner. Most existing engines process queries in this way. Our solution involves adding extra information to the XML documents and the input queries. The query processing automatically speeds up when the underlying query execution engine processes the modified query on the modified docu-ments. To demonstrate this, we have built PDPA on top of the Oracle Berkley DB XML [12] with no modification of the data-base engine itself. PDPA overcomes the second drawback by introducing the plug-and-play properties feature. This allows users to add customized properties into PDPA, which incorporates both structural and usage characteristics. Once a set of properties is plugged into PDPA, then PDPA uses them to prune candidate documents. The embedded properties are stripped from the query results before they are returned to the user.

Fig. 1 shows an example query searching over a collection of three XML documents. The query looks for houses that have a study room and a garage with a remote control. Note only property1.xml matches the query. The question is how do we find this fact quickly? We do this by adding a height property to the query and to the three documents. Using the height property we can prune out the two documents property2.xml and property3.xml when processing the XPath query. The rea-son is property2.xml and property3.xml both have height lower than that of the query and therefore cannot match the query.
The pruning takes advantage of the top-down processing approach to reduce the number of candidate documents in the search space. Using properties for pruning is challenging, because there is a need to estimate the impact of both including and excluding the property. The cost estimation is difficult, since the query can be complex and the number of documents that match a set of properties is difficult to estimate.

Apart from the height properties mentioned above we propose a number of other properties as well. One such property is the element summary property, which just counts the number of occurrences of a certain tag in the query tree versus the document tree. If the document does not contain enough occurrences of the sought tag then it can be pruned away. This property can be effective in a number of practical situations. For example a query on an XML database for restaurants may contain a wifi tag to indicate the user wishes to find restaurants that offer a certain type of wifi. If the document col-lection contains only 5% of restaurants that offer any type of wifi, then the fact that the query asks for wifi can be used to prune away 95% of the documents. Other data sets such as the DBLP [40] and swissprot [40] contain varying number of tags.
These data sets can be queried in ways that require different number of occurrences of the tags, which in turn can be pruned by element summary properties.

PDPA has two phases, the offline and the online phase. The offline phase adds pruning properties into the original XML documents based on the pruning power of the added properties. Properties that have higher pruning power are placed on top of properties that have less pruning power. The pruning power of a property refers to how many documents can be filtered by using the property during query processing. Pruning power is determined by query workload information which includes all queries that have been executed in the past in addition to how often they are executed. The online phase selects a subset of the properties to be incorporated in the query. Only a subset of properties is selected because some properties have low pruning power for the particular query being executed. We have proposed two algorithms for selecting properties, an exhaustive and a greedy heuristic based algorithm. We have conducted comprehensive experiments to compare the perfor-mance of PDPA against the original query engine without running PDPA.

The rest of this paper is organized as follows: Section 2 presents the related work in the area of structural query process-ing for XML documents. The problem is defined in Section 3, while Section 4 discusses the proposed PDPA in great detail.
Sections 5 and 6 present a detailed experimental setup and evaluation of the experimental results, respectively, and Section 7 concludes this paper. 2. Related work
We discuss related work under the following three categories: structural indexes, relational database-based structural query processing and mixed mode processing.

The structural index-based approaches is the most similar to our work. In this approach, various types of indexed sum-maries of XML document structures are built. These summaries are used to efficiently answer structural queries. Three early
XML documents by indexing all paths from the root. This leads to the following drawbacks: a large storage space is needed; it sider query workload characteristics. The A( k )-index [26] improves over the 1-index by reducing the large storage size requirement. It achieves smaller storage size by storing only approximate information for paths longer than k . However, it still does not isolate highly selective sub-structures to take full advantage of them nor does it consider query workload characteristics. The APEX [11] and the D(K)-index [7] index structures improve on the previously mentioned index structures by adapting themselves to changing query workload characteristics. However, they do not distinguish between highly and lowly selective sub-structures and therefore do not take advantage of this important feature of XML document structures to build the most effective data structure. Elghandour et al. [15] propose a system that helps users decide which indexes to build for an XML document collection. The system is tightly coupled to the query optimizer using database statistics to per-form cost estimation. Chen et al. [9] propose an XML indexing approach which allow every fully specified XML query to be the XML document but rather allows users to plug in different types of pruning properties. These properties are then only used if they are found to be highly selective for a particular query and document collection.

Among the index-based approaches, the work that is most similar to ours is the Minimal Infrequent Structures (MIS) in-dex [29]. This approach finds MIS structures in the XML document collection and then indexes the document collection using these structures. The MIS structures are those structures that have high selectivity and are therefore the most effective ones for pruning. Their work does not take query workload information into consideration when finding MIS structures which means they may index structures that are very selective but never used since no query has that structure. They have a rigid definition of MIS which precludes simple yet powerful pruning properties like height and maximum number of children across all nodes of a document which can be used in PDPA.

Kim [27] proposed an approach that uses element trees and distribution encoded bitmaps to efficiently skip elements when processing structural joins. This technique exploits the distribution of elements as well as the context information to efficiently process structural join queries. In contrast to this work, PDPA does not provide a faster method of processing documents and queries in order to prune the number of documents that need to be considered during query processing. Therefore, PDPA can be used in conjunction with the method proposed by Kim.

There has been much investigation into how best to use relational database systems to process XML structural queries, mostly based on some variant of storing the following information per element: document ID; presort order; postsort order; and level number. This information is stored in a table in the database. Structural query processing involves performing var-form the multi-predicate join. The results showed MPMGJN can outperform standard RDBMS join algorithms by an order of magnitude. Al-Khalifa et al. [1] improved over MPMGJN by creating an I/O and CPU optimal join algorithm that uses stacks for matching binary structural relationships against an XML database. Chien et al. [10] and Jiang et al. [23] proposed various types of indexes to speed-up join processing. Bruno et al. [6] developed a novel holistic twig join algorithm which matched intermediate results. Jiang et al. [24] improved on the twig processing approach of Bruno et al. [6] by extending it to take advantage of all or partly indexed XML documents. Jiang et al. [22] then extended twig processing to efficiently handle
OR predicates. Grust et al. [19] proposed a database index structure that supports all XPath axes which can live completely inside a relational database system. The implementation can benefit from R-trees which are now part of most relational dat-abases. Grust et al. [20] next proposed a novel algorithm called the staircase join which uses tree properties such as subtree size, intersection of paths, inclusion and disjointness of subtrees to improve XPath performance. The above algorithms, un-like our algorithm, do not take advantage of query workload information to speed-up structural query processing. Chen et al. [8] proposed an XML structure indexing technique which is effective at reducing IO and avoids generating redundant inter-mediate results.

Wang et al. [44] proposed a novel index structure called ViST which represented both XML documents and queries by structure-encoded sequences. Then query processing is equivalent to finding subsequence matches. ViST supports dynamic index updates and relies solely on B+-trees. Rao and Moon [37,38] proposed PRIX which is a way of indexing XML documents and processing twig patterns in an XML database using Pr X fer sequences. PRIX allows holistic processing of a twig pattern without breaking the twig into root-to-leaf paths and processing these paths individually. Prasad and Kumar [36] improved on PRIX by proposing a variation of the Pr X fer sequences. They establish interesting properties to the sequencing method which leads to a new efficient algorithm. Results show their method outperforms PRIX in a variety of situations. Wang and Meng [43] address the problem of query equivalence with respect to subsequence matching of XML documents and also introduce a performance-oriented principle for sequencing tree structures. The results showed their approach outperform earlier methods. Tatikonda et al. [42] have proposed a method for processing structural joins by representing the XML tree as a sequence and then processing the query by searching for the longest common subsequences. Their method uses a se-quence of inter-linked early pruning steps coupled with a simple index structure that enabled a reduction in the search space. This method was found to be up to 2 or 3-fold faster than current state-of-the-art techniques. Again PDPA can be used in conjunction with these techniques by placing the pruning properties at the top of the documents which corresponds to changing the beginning of the sequences.
 Koch et al. [28] propose a string matching approach for efficient searching and navigation of XML documents and streams.
Their technique can be used for pre-filtering XML documents. It differs from existing techniques by usually requiring only a fraction of the input at a time for processing and thereby uses both main memory and the CPU very efficiently. Experiments show their algorithms when used on an in-memory query engine can experience speed-ups of two orders of magnitude.
In the mixed mode processing approach proposed by Halverson et al. [21], the XML query processing engine uses a mix-ture of native and relational database technologies. The system uses a cost model to find the optimal combination of tech-niques to use for a given query. The system is very complex and does not take advantage of query workload information to enhance query performance. Gou and Chirkova [18] survey high performance techniques for querying large XML data repos-itories. In particular they are interested in techniques for matching twig patterns. They study two major classes of XML query processing techniques: the relational approach and the native approach.

Boncz et al. [4] developed the MonetDB Relational XQuery system which uses relational data management infrastructure to create a fast and scalable XML database. The system implements all essential XML database functionalities. The system extended state-of-the-art techniques by developing techniques such as loop-lifted staircase join and efficient relational query evaluation strategies for XQuery theta-joins with existential semantics.

The minimization of tree pattern queries has been studied by Amer-Yahia et al. [2]. They propose algorithms that identify and eliminate redundant nodes in the pattern early. This produces a tree pattern that can be matched against the document collection more efficiently. They consider tree pattern minimization in the presence and absence of integrity constraints.
There has also been work on the containment and equivalence of fragments of XPath expressions. Deutesh and Tannen [13,14] investigates containment of XPath expressions under various integrity constraints and provides decidability, unde-cidability and hardness results. Miklau and Suciu [33] study the containment and equivalence problems for XPath queries involving wildcards, branching and descendant edges. They prove for this type of queries the problem is co-NP complete and develop an efficient algorithm for special cases of these queries. Michiels et al. [32] identified one of the shortcomings systems do not support tree patterns in their XML algebra. Accordingly they propose an XML algebra that does support tree patterns.

Query optimization for XML queries have been studied in the existing literature [3,31,48] . McHugh and Widom [31] pro-pose a cost-based query optimizer for the XML database Lore . They define logical and physical query plans, database statis-tics, a cost model and describe plan enumeration for reducing the large search space. Their optimizer is fully implemented in tree pattern matching. They performed an extensive performance study of the relative merits of the different algorithms and reported their relative trade-offs. Balmin et al. [3] propose heuristic-based rewrite transformations to group XPath expres-sions to optimize concurrent evaluation of multiple XPath expressions. They also propose cost-based optimization to order the groups within the execution plan.

An important component of query optimization is selectivity estimation. There has been much existing work on selectiv-ity estimation for XML documents [16,45,46] . One of the most recent work is by Fisher and Maneth [16]. They propose a selectivity estimation technique for XML documents which estimates selectivity for all XPath axes, gives a guaranteed range which the actual selectivity lies in and allows incremental updates to the XML database.

XML documents can be modeled as graphs. Gou and Chirkova [17] propose an algorithm which finds the top-ranked twig-pattern matches from large graphs. They propose the DP-B algorithm which retrieves the exact top-ranked match from potentially exponentially many matches in linear space and time. A second proposed algorithm called DP-P can run in far less than linear time and space in practice.

Bressan et al. [5] propose strategies to prune XML documents to smaller sizes with respect to a given query. The parts of the documents pruned are guaranteed to not affect the result of the query. Since the documents are smaller the queries can be processed faster. Our work differs from theirs in that we prune away entire documents instead of parts of documents. We therefore do not need to store the pruned documents whereas they need to store the smaller pruned documents in order to feed it to the query execution engine. Therefore our work has lower storage overhead compared to theirs. In addition our work can be in conjunction with theirs by first using our algorithms to prune away the documents that cannot be in the re-sult set before using their algorithm to reduce the size of the documents.

Moro et al. [35] propose algorithms to efficiently determine which subscribers to push incoming XML documents to in a publish and subscribe scenario. They propose the BoXFilter, which is based on a new treelike indexing structure that orga-nizes the queries based on their similarity, which is then used to prune away queries that are not related to the incoming documents. They use string matching techniques to determine the similarity between queries. Their work differs from ours in that we prune documents instead of queries.

Sanz et al. [39] propose a method of finding XML documents that are similar in terms of both content and structure to a given query. They argue this is useful for internet applications since there XML document collections often have heteroge-nous structure. In contrast to this work we find exact matches to queries instead of approximate matches. 3. Problem definition
This section defines the pruning problem in terms of two phases: the offline and online phase. The goal of the offline phase is to find an optimal ordering of properties to be added into documents, which leads to minimum overall query pro-cessing cost for a given query workload. The goal of the online phase is to find the optimal ordering of properties to be in-cluded and rewrite the query with these properties, which leads to minimum processing cost of executing a specific query. 3.1. Query definition
Our system is designed to handle the tree structure component of XPath, XQueries and tree pattern queries. We handle both parent X  X hild predicates and ancestor X  X escendent predicates and a mixture of them. Currently we do not consider wild-For example: //paper//section[figure AND table].

When the properties of an XML document matches the properties of a query that does not mean the XML document itself matches the query. It just means we are not able to prune out the XML document from the result set using the properties. 3.2. Offline phase definition
The offline phase is executed when the query engine is offline. This phase generates and adds pruning properties into the documents, which are then used in the online phase.

To make the pruning more effective, a set of properties that have high pruning power for the particular set of documents and historically collected queries is selected. Next, the order by which the properties are added to the document needs to be determined. Properties with high pruning power should be added toward the top of the document since they more dramat-ically reduce the work for lower placed properties. However, a property which is very effective for one query may not be effective for a different query, hence query workload information is used to determine the overall pruning power of each property. Therefore, the main problem of the offline phase is to determine the pruning power of the properties for a given workload and use that to order the inclusion of the properties into documents.

The problem is formally defined as follows: 1. Given a set of XML documents D , and a set of n XML queries with associated workload statistics, let Qs be the historical query workload statistics, which consists of a set of query frequency pairs that can be extracted from a query log Q . For-mally, Qs is defined as follows: 2. Given a set of m function pairs ument d 2 D , a property p , an XML query q and outputs whether d matches q in terms of p . C is minimized, where C is defined as follows: where cost  X  q i ; s  X  is the time needed to process query q 3.3. Online phase definition
The online phase is the period when the query engine is ready to answer queries. When a query is given, only the prop-given query. Then the problem becomes which subset of the remaining properties should be used for processing the query. The properties used should be the ones that are most effective in pruning away candidate documents for the query.
The offline phase added the set of properties according to descending overall pruning power for the given workload. How-ever, during the online phase we are optimizing for the particular query currently being run, hence we should adjust the use of the properties based on their pruning power for the particular query. We achieve this by excluding properties that have weak pruning power for the particular incoming query. Using inappropriate properties that have weak pruning power re-duces query processing performance by increasing the amount of matching without pruning away many documents. There-fore, the online phase is focused on finding the particular subset of properties to use for the current query. The problem is formally defined as follows:
Given an XML query q , a document collection D with embedded properties D minimized. 4. Property-driven pruning algorithm
The previous section defined the problem that needs to be solved for the offline and online phases. The proposed solution to the problem is called the Property-Driven Pruning Algorithm (PDPA). PDPA has the twin features of structural query pro-cessing independence and plug-and-play properties . This section provides a detailed description of PDPA. 4.1. Overview of the approach
The Property-Driven Pruning Algorithm (PDPA) adds extra pre-pruning properties to the documents and queries in order to accelerate query processing, which will work in any query processing engine that processes queries in a top-down man-ner. The feature is termed structural query processing independence . PDPA also describes a general algorithm, which provides the flexibility of taking any set of properties as input and modifying the XML documents and queries for the most effective pruning. The feature is termed plug-and-play properties .

PDPA is separated into two phases, the offline and online phases. During the offline phase, documents and historical que-ries are analyzed and pruning-related information is combined to determine the order of the properties added.
Table 1 gives some example properties that can be used in PDPA. It is important to note that these are example properties only and that other properties can also be used.

For example, consider the XML document tree shown in Fig. 2 a. As the height of the tree is 3, then the property named  X  X eightProp X  with attribute value 3 is added into the document (shown in Fig. 2 b). Later when a query comes with a height greater than 3, the document can be pruned using the height property since the document is shorter than the query and therefore cannot match it.

The elementary summary property is the summarized information of the element extracted from historical queries, which has a higher pruning power according to Eq. (4) in Section 4.2. The number of times that the selected element appears in a query or a document is then considered as the value of the property. For example, if A is the selected element and it appears three times in an XML document, then the property named  X  X Prop X  with attribute value 3 will be added into the document.

When using an XML DBMS which processes queries in a top-down manner, the order of the properties is extremely important, because more selective properties will prune more candidate documents for a query, thus leading to fewer scans selected properties.

During the online phase, a subset of the properties is selected, based on the estimated cost. The selected properties are added to queries (the order remains the same). Statistics encapsulating the pruning power are used to decide which subset of properties should be included. The query is then rewritten by adding selected properties.

The high-level overview of the system is shown in Fig. 3 . First of all, the XML documents are stored in an XML DB. Sec-ondly, the original documents and historical query log are passed into the offline system. The offline system analyzes the input data and outputs a list of properties in descending order of pruning power. Where pruning power incorporates the selectivity of the pruning property for the queries in the workload and the frequency of the queries using the property.
Thirdly, the XML documents are modified with properties extracted from the previous step. In addition, a histogram is gen-erated, which will be used for the online phase. The histogram contains information regarding the number of documents that match a particular range of property values. Fourthly, queries and histograms generated in the offline phase are passed into the online system. The online system analyzes queries in terms of the histograms and then chooses a subset of the proper-queries are executed in the XML DB.
 1 the original document and the historical query workload are analyzed together to arrive at a list of pruning properties and the histogram needed in the online phase. Next in line 2 the properties are ranked according to their pruning power and line 1 we extract the properties from the query. Next in line 2 we use the histograms created by the offline algorithm to se-lect a subset of the properties with the highest pruning power for the current query to embed into the query. Finally in lines 3 and 4 we embed the selected properties in the query and execute the query, respectively. The embedded properties are stripped from the query results before they are returned to the user. 4.2. Offline system
The offline phase firstly analyzes the document set and the query workload to find the properties using Eq. (4), and then computes the optimal ordering of these properties using Eq. (5). Finally, the ordered properties are embedded into the ori-ginal documents. We store the properties as elements inside the documents themselves because this allows us to directly use existing XML query processing engines without modification. The properties are stored at the top of the document be-cause most processing engines process documents top-down and thereby automatically use the tags to prune the search space. If we stored the index in a separate place then we will need to modify existing XML query processing engines to incor-porate PDPA. In contrast, most existing indexing techniques are designed to be placed inside the query processing engine itself and hence the index is stored separate from the documents.

Adding the properties to the documents and query is simply done by adding a sequence of elements at the top of the XML documents and queries. We can assign each element a name such as e1 and have an associated attribute called value (as shown in Fig. 2 b). For example for the height property the value attribute states the minimum height of a document that can potentially match the query. We also have a small mapping from each element name to the actual definition of the prop-erty. For example we can say element e1 correspond to  X  X  X arent child summary (d, x, y) X .

The overall offline system is shown in Fig. 5 , which consists of three sub-phases, the analyzing phase, ranking phase and modifying phase. During the analyzing phase, historical queries are analyzed to find properties with high pruning power.
There are many types of properties that can be used including those described in Table 1 . The developer can customize PDPA to use any set of properties. In this section, we demonstrate how element summary properties (first mentioned in Table 1 ) can be used to achieve high pruning power.

An element summary property e  X  X  e t ; e x  X  consists of a tag name, e ument. By using this information, we can prune a document if e element summary properties are extracted on a basis of a score which represents the pruning power of the property. The score uses statistics of the query workload to assign a score indicative of the pruning power of the property for the given workload. The following equation defines score  X  e  X  for an element summary property e : where n is the number of queries in the query log Q and freq  X  q average height of element e t in query q i , where height is defined as the number of node traversals away from the root node.
Note in the case of ancestor X  X escendent (AD) relationships (e.g. A//B//C) the height is still computed as the number of tra-versals (in this case down ancestor X  X escendent links) from the root node. We could put some ad hoc scaling factor to ele-if it contains a mixture of AD and parent child(PC) relationships, e.g. A/B//C/D). Therefore to keep the algorithm simple we treat both PC and AD relationships the same. Experimental results show our algorithm works very well for queries with AD relationships.
 According to Eq. (4), elements occurring in queries with higher frequency have higher scores. Furthermore, because the
XML DBMS mostly processes queries in a top-down manner, the element further down from the root should be pruned more aggressively (hence is given a higher score), since pruning them late may cause more processing before finding the document does not match. For example a node that is at the bottom of the query tree will not normally be matched against the doc-uments until other nodes closer to the top of the tree have been matched. Using such a node as a pruning node effectively moves it up the query tree so it can be used to prune away documents quicker. On the other hand there is little benefit in ument collection.

It is important to note that Eq. (4) does not take the document collection into consideration when determining the score assigned to a property. This is because it is too computationally expensive to assess the pruning power of every possible ele-ment summary property against the entire document collection. Hence we focus on assessing the pruning power of the prop-are not used in many queries or are high up in the query tree. We use Eq. (5) to further refine our choice of properties and rank them in terms of pruning power. It is there that we incorporate the document collection.

Eq. (4) is related to Eq. (3) of the problem definition since Eq. (4) uses the height of the element within the query as an indicator for the execution cost savings from using that element. In addition Eq. (5) also incorporates the execution fre-quency of the query in the cost.

We show an example of how Eq. (4) can be used to compute the score of the element summary properties for documents d1 and d2 in Fig. 6 , when given a query workload consisting of the queries q1 and q2 in Fig. 7 . In the example the element summary properties are denoted by their corresponding tag name. For example the element summary property that prunes using tag A is denoted as A. The scores are computed as follows:
Too many properties increase the size and complexity of documents and can slow down query processing. Hence, Eq. (4) can be used to find a threshold number of properties with the highest pruning power to include in the documents. However, this is only one possible way of selecting properties.

During the ranking phase, a list of selected properties is ranked using a ranking equation that actually measures how many documents are pruned away using the property for the given query workload. This is not too time consuming, because the analysis step described above has already narrowed down the number of properties to be considered. The equation to calculate the ranking score for each selected property p 0 is as follows: where n is the number of queries in the query log Q ; prop  X  q property p 0 and freq  X  q i  X  is the frequency of query q
Eq. (5) gives a higher score to properties that have higher pruning power throughout the whole document collection by incorporating historical query frequencies. To compute prop  X  q the pruning property p 0 and record how many documents were pruned by the pruning property. Eq. (5) is related to Eq. (3) of the problem definition since the prop  X  q i ; p 0 ; D  X  term is the number of documents pruned by property p to the execution cost of q i term of Eq. (3), since the more documents are pruned by the property the lower the cost of exe-cuting the query. Eq. (5) also incorporates the execution frequency of the query.

The following shows how Eq. (5) can be used to compute the ranking scores of the element summary properties of the document collection shown in Fig. 6 and the queries in Fig. 7 . In this example the document collection is denoted using k .
The scores are computed as follows:
During the modifying phase, the ranked properties are added into the original documents in descending order according to the ranking score. In addition, an index for each property is created so that during the online phase, some of the inserted properties can be bypassed if they are deemed to be not very selective for the current query. 4.3. Online system
As mentioned in the problem statement, the online phase first removes the properties which do not apply to a given query. Next, it finds the best combination of properties from the remaining properties to embed into the query. There is a can be complex and the number of documents that match each property is unknown.

Fig. 8 shows an example document collection with three embedded properties and a query using only two out of the three properties. The documents and query have element summary properties (ESP) embedded in them. For example the top prop-erty for document 1 is an ESP property that has an attribute value equal to 1. This means there is only one tag B in document 1. If a query has more than 1 tag B then it cannot match the query. Among the three properties embedded in the documents therefore is unselected. The reason is the first property allows us to prune document 1 and the second property allows us to prune document 2 but the third property does not help us prune any properties. In the online system we use various statis-tics to compute whether each properties should be selected or not.

The overall online system is described in Fig. 9 . Firstly, the input query is scanned to find the properties that satisfy the built-in properties. Secondly, if we are using the exhaustive algorithm (described in Section 4.3.1), we must then estimate different combinations of properties. Fourthly, the properties with minimum estimated cost are selected. Note: the proper-ties which are selected depend on whether the exhaustive or greedy (described in Section 4.3.2) algorithm is used. Lastly, the query is rewritten to include the selected properties and executed. 4.3.1. Exhaustive algorithm
We first present an exhaustive algorithm to find the optimal subset of applicable properties (properties that apply to the query) to be included into a query. This algorithm calculates the estimated query processing cost of embedding all possible combinations of properties into the query. Then, the combination of properties with the lowest estimated cost is embedded into the query. The equation to calculate the estimated cost of processing one document for query q when using a set of se-lected properties  X  p 1 ; p 2 ; ... ; p m ; D  X  is shown as follows: where p i is the i th selected property, F is the estimated cost per document to process added properties, sel  X  p for processing the original query, and D is the document collection. B is estimated online by taking a small partition of the document set and executing the query on it and then dividing the time by the number of documents.

The selectivity of a property p i for query q is defined as the fraction of documents remaining after property p prune the document set for query q . This is determined by using a histogram for p uments for each value of e x of p i .

The left hand side of Eq. (6) is the additional processing cost of adding the set of properties p side is the reduced cost of executing the query as a result of adding the properties. The costs of adding a property p the product of the selectivities of all properties before p son for this is lower selectivity (higher pruning power) before p uments will already be pruned out before getting to p i . This idea is also used to compute the reduced cost of executing the query as a result of adding the properties.

We now show an example of cost of a set of properties for a document collection. Fig. 10 shows an example document with three properties embedded in the top of it and a query q 1. For example, we are interested in computing the cost of including the first two of the embedded properties ( p 1 and p 2). According to Eq. (6), the cost will be as follows: more instances of tag B.

The run-time complexity of the exhaustive algorithm is O  X  2 given query, since it needs to compute Eq. (6) for every combination of m properties for 1 4.3.2. Greedy heuristic algorithm
The exponential run-time complexity of the exhaustive algorithm may mean the overhead of running the algorithm out-weighs any potential pruning benefits derived. Hence, we have developed a faster greedy heuristic algorithm that incurs less processing overhead and may find the approximately optimal combination. The heuristic is based on the intuition that prop-erties that have lower selectivity (stronger pruning power) should be used before properties with higher selectivity. Using properties with lower selectivity earlier means a lot of documents are pruned quickly, resulting in less documents remaining for pruning by properties with weaker pruning power. This results in less overall query processing time.
Fig. 12 presents the greedy heuristic algorithm. The algorithm first narrows the properties down to only those that apply to the query and then it selects the property with the lowest selectivity to be the first property included. It then considers which property to include next from among those properties ranked lower than the selected property. Again, it is the property with lowest selectivity among these properties that is next included. This is continued until there are no more low-er ranked properties to be selected.

The run-time complexity of the greedy algorithm is O  X  a 2  X  where a is the total number of properties that apply to a given 5. Experimental setup
This section describes the detailed setup of the experimental environment used to test the performance of PDPA. 5.1. Software and hardware
All experiments for this paper are carried out using Oracle Berkeley DB XML [12] version 2.4 on an Intel Core Duo 1.83GHz laptop with 1 G RAM running Windows XP professional. Oracle Berkeley DB XML version 2.4 offers the following features to optimize performance: a unique dynamic indexing system that enables optimized retrieval of XML content; targeted indexes and a statistical, cost-based query planning engine; flexible indexing of XML nodes, elements, attributes and meta-data; and node level indexes which improve query performance, especially for large XML documents. 5.2. Synthetic data and query set We generated synthetic data to conform to three different distribution functions: uniform, normal, and zipf distribution. The distribution functions are used to determine the height of the document and the number of children for each node.
While generating each XML document set, a list of parameters is used, which are the maximum height of a document, the maximum number of children per node, the number of documents and the distribution function. The default values of the parameters used are shown in Table 2 .

All experimental query sets are generated using the same approach as used to generate data sets (see Section 6.3). How-lowing three categories:
PC(/) Parent X  X hild relationship only.
AD(//) Ancestor X  X escendant relationship only.
MIX(/ j //) A mix of both parent X  X hild and ancestor X  X escendant relationships.
To generate the queries, we again use the three different distribution functions: uniform, normal, and zipf distribution to determine the height of the query and the number of children for each query node. When generating each XML query set, a list of parameters are used, which includes all parameters needed to generate data sets and additional query type. The de-fault values of the parameters used are shown in Table 3 .
 5.3. Real data and query set
The real data set we used was the collection of all XML documents describing computer science journal and conference
October, 2002. The data collection contains 3,332,130 elements, 404,276 attributes, a maximum depth of 6 and an average depth of 2.90228.

We generated queries which find all articles that contained the following elements: cite[0 X 10] between 0 and 10 citations. author[1 X 10] between 1 and 10 authors. cdrom[0 X 1] denotes whether we search for articles with the cdrom tag or not. ee[0 X 1] denotes whether we search for articles which have an associated electronic proceeding or not. series[0 X 1] denotes whether we search for articles which are part of a series or not. where the number between [0 X  X ] above is determined using a uniform random distribution for each query. In our experi-ments, we generated 10 real queries conforming to the specification above. 5.4. Algorithm setup The experimental results are based on comparing the following four algorithms:
Original This algorithm simply runs the original queries on the original data without any extra processing. It gives the bottom-line processing time of input queries.
 PDPA-E This is PDPA using the exhaustive algorithm to select properties during the online phase.
 PDPA-G This is PDPA using the greedy heuristic algorithm to select properties during the online phase.

PDPA-A This is PDPA with property selection disabled during the online phase. This excludes the overhead of calculating the estimated processing cost of using different combinations of properties. Comparing this algorithm with PDPA-E and
PDPA-G will illustrate whether the benefits gained from cost estimation during the online phase outweighs the overhead incurred to perform the cost estimation.

When estimating the cost per document for processing the original query (term B of Eq. (6)) we used 10% of the doc-ument collection. For the synthetic data set, the PDPA algorithms add ten properties into each document. However, in one experiment we varied the number of properties. Two of the properties are structure attribute properties: the maximum height of a document and the maximum children elements. The remaining are element summary properties, which are scored and ranked using Eqs. (4) and (5) . The eight that we use are the ones that give the highest score according to Eq. (4).

For the real data set, we used five element summary properties: cite, author, cdrom, ee and series. We did not use max-imum height and the maximum number of children elements since for this particular data set, these properties are not effec-tive for pruning. 6. Experimental results
This section presents the results of five different experiments. In the first experiment, the query distribution is varied. In the second experiment, the data distribution is varied. In the third experiment, the number of documents is varied for the synthetic data. In the fourth experiment, the number of documents were varied for the real data. In the fifth experiment, the number of candidate pruning properties were varied for the synthetic data. In order to gain more confidence in the correct-ness of our implementation we have compared the output of the original XQuery engine against the various variants of PDPA. We found the outputs were the same.
 6.1. Experiment 1: Varying query distribution
Fig. 13 shows the processing time for uniformly, normally, and zipf distributed queries with different query types on 10,000 uniformly, normally, and zipf distributed documents, respectively. As shown in Fig. 13 , PDPA-G outperforms Original in all cases by up to twofold, except PC query type based on uniformly distributed data and query. The reason for this is the pruning power of PDPA properties on normally or zipf distributed data and query is much stronger than that on uniformly distributed data and query. PDPA-G also outperforms PDPA-E because of its smaller processing overhead, and outperforms
PDPA-A because of its better selection of pruning properties, which has stronger pruning power for the given query. PDPA-G and PDPA-A outperform the original query processing for all data and query distributions for the AD and MIX query types. The reason for this is that executing AD or MIX queries requires further look-ups to determine the finishing point. PDPA uses the embedded properties to pre-determine whether the candidate document requires further examination at the early stage, and significantly reduces the number of unnecessary look-ups when executing an AD or MIX query in a top-down manner.
PDPA-E sometimes performs worse than the original processing due to the additional overhead of exhaustively searching through all possible combinations of properties. It seems that sometimes this overhead outweighs any benefits gained from using properties.

PDPA-G and PDPA-A perform similarly to or sometimes worse than the original query processing for the PC query type child relationship does not exist. PDPA-G and PDPA-A perform worse than the original query processing when both the query and data are uniformly distributed. This means PDPA-G and PDPA-A are not suitable for use in situations where data is uni-formly distributed. 6.2. Experiment 2: Varying data distribution
Fig. 14 shows the processing time for normal distributed queries on 10,000 uniformly, normally, and zipf distributed documents.

As shown in Fig. 14 , the processing time when using PDPA-G on normally and zipf distributed data is approximately 50% less than when using Original. However, the improvement when using PDPA-G on uniformly distributed data is relatively small. The reason for this is that many candidate elements are spread over the whole uniform distributed document collec-tion and the selectivity of most elements is relatively low. Therefore, the pruning power of PDPA would be relatively weak.
However, as mentioned in the previous experiment, uniformly distributed data are not common. 6.3. Experiment 3: Varying number of documents for synthetic data
Fig. 15 shows the processing time when varying the number of documents for uniformly, normally, and zipf distributed queries on uniformly, normally, and zipf distributed documents, respectively. This experiment is aimed at exploring the sca-lability of the various algorithms. When the number of documents is 5000, which indicates the document collection is rel-over the small number of documents. However, when the number of the documents increases, PDPA shows much better improvement on performance for all distributions in contrast to Original. The processing time difference between PDPA-G tage as the number of documents that may be pruned increases.

Furthermore, another observation from Fig. 15 b and c is that PDPA-E outperforms PDPA-A when the number of docu-ments is relatively large. The overhead of estimating the execution cost done by PDPA-E becomes a smaller proportion of processing time and PDPA-E X  X  ability to select better pruning properties becomes a larger advantage. PDPA-G outperforms
PDPA-E in all conditions since it makes the best trade-off between computation overhead of cost estimation and perfor-mance gain from adding the properties that are effective at pruning. 6.4. Experiment 4: Varying the number of documents for the real data Fig. 16 shows the processing time when varying the number of documents for the real DBLP data which was described in Section 5.3. In this experiment, we varied the number of documents from 10,000 to 500,000.

The results for this experiment show similar trends to Fig. 15 b. When we analyzed the data, we found that the data was normally distributed hence the similar results compared to the normally distributed synthetic data. These results are very encouraging since it shows that PDPA-G outperforms Original by up to twofold for the real data set.
 6.5. Experiment 5: Varying the number of candidate pruning properties
Fig. 17 shows the processing time when the number of candidate pruning properties used by the PDPA algorithms were varied from 2 to 16.
 The results for this experiment show PDAP-G performed the best for all values tested. This shows the effectiveness of
PDAP-G at efficiently picking the best pruning properties from among the candidate properties. PDAP-E spends too much time searching for the optimal set of pruning properties. The benefits of such an exhaustive search is outweighed by the high cost of the search process itself. PDAP-A shows poor performance when the number of candidate properties is high since it picks all the candidates irrespective of pruning effectiveness. Some of the properties are not effective at pruning and hence add to the processing cost of the query. 7. Conclusion
In this paper, we have proposed an effective pruning approach called the Property-Driven Pruning Algorithm (PDPA), feature means that PDPA can be laid on top of any existing XML query engine, which processes queries in a top-down man-ner. This is accomplished by pre-modifying the documents and queries before they are inserted into the query engine instead of modifying the query engine itself. The second feature means that PDPA can be customized to take advantage of different structural and usage characteristics. This is accomplished by designing the offline and online phases of PDPA so that it can work with any pruning property.

Experimental results show that PDPA consistently improves query execution performance for all the situations we tested for both real and synthetic data sets. From these experimental results, it can be concluded that PDPA X  X  ability to pre-prune candidate documents leads to a significant speed-up of structural query processing.

For future work, we would like to develop new pruning properties to insert into PDPA and assess the utility of the other properties described in Table 1 in detail. This will give users of PDPA a larger range of already designed properties to plug into PDPA. In addition, new PDPA selection algorithms would also be developed to suit different types of properties. Cur-rently PDPA is designed to work for elements in XML documents. However extending it to work for attributes would not be hard. It just means parsing the XML documents for both attributes and elements when creating the properties and also embedding attribute properties inside the queries. An in depth study into how to take the most representative sample and the size of the sample for estimating cost B of Eq. (6) is an important area of future work. Finally, plugging PDPA into other XML query processing engines and comparing its performance is an important direction for future research. Finally, plugging
PDPA into other XML query processing engines which use other state-of-the-art query processing algorithms and comparing its performance is an important direction for future research.

References
