 Dep ending on a web searc her's familiarit y with a query's tar-get topic, it may be more appropriate to sho w her intr oduc-tory or advanc ed documen ts. The TREC HARD [1] trac k de-ned topic familiarit y as meta-data asso ciated with a user's query . We instead de ne a user-indep enden t and query-indep enden t mo del of topic-familiarit y required to read a documen t, so it can be matc hed to a given user in resp onse to a query . An introductory web page is de ned as A web page that doesn't presupp ose any backgr ound know l-edge of the topic it is on, and to an extent intr oduc es or de nes the key terms in the topic. while an advanced web page is de ned as A web page that assumes sucient backgr ound know ledge of the topic it is on, and familiarity with the key technic al/ im-portant terms in the topic, and potential ly builds on them. We dev elop a metho d for biasing the initial mix of docu-men ts returned by a searc h engine to increase the num ber of documen ts of desired familiarit y level up to position 5, and up to position 10. Our metho d involves building a sup er-vised text classi er, incorp orating features based on reading level, the distribution of stop-w ords in the text, and non-text features suc h as average line-length. Using this familiarit y classi er, we achiev e statistically signi can t impro vemen ts at reranking the result set to sho w introductory documen ts higher up the rank ed list. Our classi er can be seamlessly integrated into curren t searc h engine technology without in-volving any ma jor mo di cations to existing architectures. H.3.1 [ Information Storage and Retriev al ]: Con ten t Analysis and Indexing General Terms: Algorithms Keyw ords: Familiarit y, Web Searc h, Personalization
We decouple our evaluation of familiarit y re-ranking from the evaluation of topic-relev ance. We assume that the top 20 documen ts returned by a searc h engine are all relev ant to the query , and evaluate our familiarit y reranking on this set. Data was obtained by querying the Inktomi searc h engine with 40 queries dra wn uniformly at random from 2004 Ya-hoo! Searc h query logs. Minor pro cessing of the queries like remo val of adult queries and queries not in English, man-ual correction of spelling mistak es, and man ual insertion of spaces between words if they were absen t was done. The top 20 documen ts returned for eac h query were randomly perm uted, and presen ted to three annotators to lab el as in-troductory or advanc ed . The num ber of queries for user 1, 2 and 3, were resp ectiv ely 16, 24, and 13, and the total num-ber of documen ts lab eled (training instances) were 508, 766, and 463. Tw o queries were in common to enable measure-men t of inter-annotator agreemen t, for whic h we used Co-hen's Kappa statistic [3], and obtained co ecien ts of 0.32, 0.59, 0.48 between annotators 1 and 2, 1 and 3, and 2 and 3 resp ectiv ely (who co-annotated 36, 38, and 37 documen ts resp ectiv ely). These co ecien ts indicate fair to mo derate levels of agreemen t.
We examined three kinds of features whic h could be pre-dictiv e of familiarit y, and built a classi er FAMCLASS to com bine them. The three di eren t feature types were: 1. Stop-w ord features, whic h are predictiv e in sev eral text 2. Elev en features we designed based on various charac-3. Features used to determine reading level [4] [7]
We sough t an algorithm that could handle non-linearit y as well as a mix of di eren t feature types with di eren t value ranges. Random forests [2] pro ved to be the most con venien t choice. We used a forest of thousand trees in our exp erimen ts.
People exp ect to nd the information they are looking for in the rst page of searc h engine results i.e. within Table 1: Baseline (default searc h engine ranking) per-Table 2: Baseline performance versus classi er per-the rst ten documen ts. This holds true for a familiarit y-avored searc h too. To compare our approac h against the baseline (default searc h engine ordering), we measured the prop ortion of introductory documen ts at ranks one (p@1), ve (p@5), and ten (p@10) in top 20. The classi er results we rep ort are all based on leave-one-query-out validation: we partition the lab eled documen ts based on query , and for every fold we hold out all the documen ts asso ciated with a certain query , train on the remainder of the documen ts and rank the held out documen ts. Table 1 consolidates the results for classi ers trained on the di eren t subsets of fea-tures and documen ts and rep orts the performance of the baseline ranking as well. For the classi ers trained on the pooled data, we remo ve the two queries in common to the annotators, since they may disagree on some of those docu-men ts, whic h yields 46 queries. The performance of per-user (per-annotator) trained classi ers (Micro Avg. in Table 1) when trained on all features or stop words is close to the performance of the pooled classi ers, even though there are signi can tly more training documen ts available for the latter (Section 1). We observ e that in man y cases the performance of the classi ers is signi can tly higher than the baseline for p@5 and p@10, in particular for the per-user trained clas-si ers. Table 2 rep orts performances for per-user classi ers and the baseline for eac h annotator. introductory so, enough, just, in, adv anced if, cause, while, way, Table 3: The stop words with highest coecien ts across
Stop-w ords app eared to be the most imp ortan t features in our classi er, when we brok e down the feature sets into the three sub-t ypes. To study the con tribution of eac h fea-ture to the classi er's decision, we trained a linear classi-er and examined the coecien ts of the features 1 . In Ta-ble 4 we sho w the top-ranking stop-w ords (in the top 20, when features are sorted by decreasing magnitude of coe-cien t). Note that \help" is indicativ e of introductory con-ten t. The other stop-w ords are suggestiv e of di erences in genre or writing style, with the adv anced words perhaps suggestiv e of more formal or scien ti c writing, while the in-troductory words are suggestiv e of informal or collo quial writing. Note that app earance of a single highly weigh ted feature suc h as \help" in a documen t does not imply that the classi er will necessarily output \introductory" for the documen t. The presence of other features in the documen t, as well as the frequency of the feature in the documen t (in case of stop words), also a ects the classi er's output. Fea-tures other than stop words that were often rank ed high by the linear classi er included sev eral reading level features (syllables-p er-w ord, percen t-complex-w ords) whic h had pos-itiv e coecien ts, indicativ e of introductory documen ts, and one non-textual feature: average-w ord-length, whic h had a negativ ely coecien t, indicativ e of adv anced documen ts.
FAMCLASS can re-rank in the order desired i.e. based on adv anced or introductory preferences. It can be extended to handle even greater gran ularit y (classes) of familiarit y, sub ject to availabilit y of suitable training data. Our exp eri-men ts indicate that we can perform searc h result biasing for arbitrary users on arbitrary queries. 1 The performance of the linear classi er was comp etitiv e with the performance of random forests
