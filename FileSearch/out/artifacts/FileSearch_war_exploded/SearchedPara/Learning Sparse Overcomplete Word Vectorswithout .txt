 A word representation is often a vector associated with each word. And each dimension of the vector corresponds to a feature. Based on the distributional hypothesis [12], a variety of methods have been proposed in the NLP commu-nity, such as clustering-based methods like Brown clustering [2] and matrix-based methods like LSA [14] and HAL [16]. Recently, distributed word repre-sentations, which could date back several decades have dominated word feature learning because of their effectiveness in a variety of natural language processing (NLP) tasks, such as part-of-speech tagging, semantic role labeling [4], parsing [28], sentiment analysis [29], language modeling [30], paraphrase detection [6] and dialogue analysis [13]. Distributed word representations (also called word embeddings ) are often low-dimensional, dense, real-valued vectors induced by training a neural model [18]. These vectors X  X hich capture rich semantic and syntactic relationships of words X  X ould serve as input features for higher-level algorithms in NLP pipelines and help to overcome data sparseness and the curse of dimensionality. and storing aspects. Even though there is no evidence that interpretability is a must for word representations, it is believed that this property is helpful for a number of NLP tasks [7, 31]. Additionally, interpretability could help to un-derstand computational models that involve word vectors as features. Sparse representation is considered as a potential choice for interpretable word repre-sentations. It is believed that human brain represents the information in a sparse way [22, 1]. Experiments also showed that the gathered descriptions for a given word are typically limited to approximately 20 X 30 features in norming studies [33]. This is a strong clue that sparsity could help the learning to find the under-lying factors that generate a word. These sparse underlying factors constitute a word X  X  ideal representation. In short, it is reasonable to impose sparsity property to word representations.
 pled sparse representation method that learns sparse overcomplete word repre-sentations directly from the raw unlabeled text. The resulting sparse vectors are comparable to or outperform dense vectors on different evaluation tasks. Sec-ond, we design an efficient and easy-to-parallelize algorithm, which is based on noise-contrastive estimation (NCE) to train our proposed model. Third, we pro-pose a clustering-based adaptive updating scheme for noise distributions used by NCE for effective learning. This updating scheme makes the noise distributions approximate the data distribution, and thus pushes the learning improves with fewer noise samples.
 trusion tasks. The first two tasks are used to examine the expressive power of the learned representations and the last task is for interpretability. On the word analogy task, the results show that our proposed sparse model can achieve com-petitive performance with the state-of-the-art models under the same settings. On the word similarity task, the proposed model outperforms the competitors. For the interpretability, experimental results demonstrate that the sparse word vectors are much more interpretable.
 work, which focuses on sparse representations. Section 3 describes the main ap-proach of learning sparse word representations, including the structure of the model and parameter estimation methods specially designed for this model. Sec-tion 4 presents the experiments to evaluate the learned representations and then we conclude the paper in Section 5. In practice, sparsity constraints are useful in various NLP tasks, such as POS-tagging [32], dependency parsing [17] and document classification [34]. It has been shown that imposing sparse Dirichlet priors in Latent Dirichlet Allocation (LDA) is useful for downstream NLP tasks like POS-tagging [32], and improves interpretability [25]. tions. For example, Murphy et al. (2012) improved the interpretability of word vectors by introducing sparse and non-negative constrains into matrix factor-ization [21]. Levy and Goldberg (2014) showed that the sparse word vectors of word-context co-occurrence PPMI statistics also possess linguistic regularities that present in dense neural embeddings [15]. Faruqui et al. (2015) transformed dense word vectors into sparse representations using dictionary learning method and showed the resulting sparse vectors are more similar to the interpretable features typically used in NLP [7]. Sun et al. (2016) imposed ` 1 regularization to a dense neural embedding model (namely, CBoW [18]) and obtained sparse word vectors, which are comparable to dense vectors on expressive power with better interpretability. Chen et al. (2016) proposed to use sparse linear combi-nation of common words to represent uncommon ones, which results in sparse representation of words that is effective of compressing neural language models. a way to improve separability and interpretability in image, speech, and signal processing [23, 27, 5] and to increase stability in the presence of noise [5]. The existing sparse overcomplete word representation methods generally require two steps of learning procedures [7]. The first step is to train an embedding model, such as CBoW , SkipGra [18] and GloVe [26] to obtain dense feature vectors of the words in the vocabulary. And the second step is to learn the sparse repre-sentation of each word by fixing the dense word embeddings. Because the prior that each word has a sparse structure is not imposed to learn dense embeddings, the first step could potentially lose the information to learn sparse codes for the second step.
 sparse vectors using sparse coding techniques, our method does not learn sparse representation in a pipeline way, which reduces the risk of losing sparse struc-ture on the stage of inducing dense word embeddings. Different from Sun et al. X  X  method [31], we learn  X  X vercomplete X  representations, each dimension of which corresponds to an atom and the embedding of atoms are learned simultaneously. In this section, we will talk about a model that try to discover the fundamental elements that constitute each word. We call these fundamental elements word atoms . Word atoms and words are analogs to atoms and molecules in Chem-istry, respectively. The types of atoms are very small, but they can make up a huge number of different molecules. Likewise, we expect the limited word atoms could represent a large number of words in the vocabulary. A word atom can be regarded as an indivisible semantic or syntactic object.
  X  X ood representations result in good performances to predict context words X . In addition, we assume that each word is composed of a few word atoms . In detail, each word is assumed to be represented by a sparse, linear combination of word atoms X  vectors. This assumption is similar to but different from Chen et al. X  X  [3], which assume that each uncommon word is represented by a sparse, linear combination of the common ones. A word should not have too many semantic or syntactic components, so the sparseness assumption is reasonable.
 scribe briefly the denotations. Let the vocabulary V = { w 1 ,w 2 ,...,w n } , con-texts C = { c 1 ,c 2 ,...,c n 0 } . Each column of B  X  R d  X  n b , and C  X  R d  X  n c are word and context atoms, respectively. 4 n b and n c are the number of word and context atoms, respectively; d is the dimension of atom vectors. For any given word w  X  V , its vector representation is w = B X  , where  X  , called the sparse representation of w , is the coefficients that are used to combine word atoms to make up the word. Similarly, for a context c  X  C , its vector representation is c = C X  , where  X  is the sparse representation of c .
 rounding context words. The softmax model is used to model the distribution of a word X  X  surrounding contexts. The word-context pair ( w,c ) is drawing from plain text. Concretely, for in-put plain text that consists of N words w 1 ,w 2 ,...,w N , the word-context pair ( w i ,w j ) is drawn such that | j  X  i | &lt; `/ 2, where ` is the window size. because of the difficulty of computing the normalization constant (a.k.a. par-tition function) for each word. In the literature, there are several methods to confront the partition function of a single distribution , such as MCMC-based al-gorithms, pseudo-likelihood, (denoising) score matching, Noise-Contrastive Esti-mation (NCE) [11]. But not all of these methods can be applied to discrete-input models like (1). 3.1 Parameter Estimation We will adopt NCE to train model (1). The basic idea of NCE is to train a logistic regression classifier to discriminate between samples from the data distribution and samples from some  X  X oise X  distributions. It is a parameter estimation tech-nique that is asymptotically unbiased and is suitable to estimate the parameters of a model with few number of random variables [9]. And it is also applicable to discrete-input models. One issue to apply NCE to train model (1) is that our model is a series of distributions that share the same parameters, which does not accommodate to NCE X  X  setting. Following the work using NCE to train neural language models [20, 3] and word embeddings [19], we define the training objective as the expectation of all distributions X  NCE objective functions. fairly small, we could learn a parameter corresponds to the partition function of each conditional distribution following the standard procedures NCE suggested to handle unnormalized probabilities [11]. Denote these parameters as a vector z = ( z 1 ,z 2 ,...,z V ). Suppose to draw k negative instances per positive instance. Taking the sparseness requirement on the parameter  X  and  X  into consideration, the resulting parameter estimation model for (1) is and where  X  k ( x ) = 1 / (1 + k  X  exp(  X  x )) is the logistic function parameterized by k ; z s i = +1 and  X  1 is a variable indicates whether the corresponding instance is extracted from the corpus (namely, positive instances) or drawn from a noise distribution (namely, negative instances); D is the training dataset, including positive and negative instances;  X  is a hyperparameter used to control the degree of sparseness of S  X  and S  X  .
 applying NCE to model (1). The second term X  X hich is a ` 1 regularization X  encourages sparse solutions for  X  and  X  .
 the vector representations of the word atoms and the sparse codes, following Mikolov et al. X  X  suggestion in [18], we adopt a simplified version of NCE, which called  X  X egative sampling (NS) X  to learn word representations. This is done by redefine the first term of model (2) as where  X  ( x ) = 1 / (1 + exp(  X  x )) is the sigmoid function.
 use a suffix to indicate which training algorithm is applied: when NCE (NS) is used, we call the model SpVec-nce ( SpVec-ns ). Note that the SpVec model has two sets of word representations: one for target words and one for context words, which is the same as SkipGram . For SkipGram , the word analogy test experiments show that target word embeddings and context word embeddings have similar structures: both embeddings encode the relationship between words by the difference of corresponding words. Additionally, the sparse representation of words is a description of the structure of a word: it determines how a word is composed from word atoms. Therefore, a natural question is that: is it possible to enforce the words and contexts to have the same sparse representations? This can be done by setting S  X  and S  X  to share an identical parameter set, which introduces another variant of SpVec . We denote it with a prefix: s-SpVec , which means the sparse vectors are s hared.
 stances. When NS is used, an identical distribution p n ( w )  X  #( w ) 0 . 75 is used. When NCE is used, dynamic distributions are used to draw negative instances, which is inspired by self-contrastive estimation (SCE) [10]. Concretely, we use M distributions that are updated after every 5% of progress using the following three steps. 1. Compute the dense words and contexts embeddings: U = BS  X  , V = CS  X  . 2. Apply k -means to cluster word embeddings into M classes. 3. Specify a distribution for every cluster X using the following formula. The cluster dependent noise distributions have the property that for any w  X  X  , p
X ( c )  X  p ( c | w ). It is intractable to apply SCE to our model for large vocabulary sizes because of the memory consumption. Using clustering-based method is a good approximation while keeps the memory consumption relatively low.
 learning [22]. Our model does not require any initial representation in advance but learns from the plain text directly. This makes it convenient to learn sparse codes for objects without obvious initial representations like words. More im-portantly, it potentially captures more sparse structures of words to learn sparse representations directly from data instead of intermediate dense representations. This is because it could lose information during the learning of intermediate em-beddings. If the information about the sparse structure of a loses, it could be difficult or impossible to recover these structures in the following sparse coding step. 3.2 Optimization Algorithm In this subsection, we introduce an easy-to-parallelize algorithm to train SpVec . This algorithm is based on Stochastic Proximal Gradient Descent (SPGD) [24]. Take SpVec-ns as an example. 5 Define the per-instance loss function as f =  X  log  X  ( s  X  &gt; B &gt; C X  ). Suppose the gradients of per-instance loss w.r.t all param-eters are obtained, the parameters are updated by the following formulas. arg min u g ( u ) + 1 Therefore, we design an algorithm that updates parameters on mini-batches. The core idea is to update the parameters based on the loss function defined on mini-batches, which is carefully arranged so that the gradients can be expressed by simple matrix-matrix products.
 where m is the size of mini-batches. The index vector w and the first row of c positive instance. Similarly, w and c i, : , 1 &lt; i  X  k + 1 form negative instances. We can prove culate the gradients of parameters and using (3)  X  (4) to update the parameters. Note that there could be duplicated word or context index in w or c and du-plicated gradients w.r.t  X  and  X  should be combined before updating when the program is running in parallel. All gradients are calculated by matrix products, which means it is easy to leverage existing high performance algebra libraries to parallelize the computations. In this section, we will evaluate the resulting sparse representations on two similarity-based tasks and investigate the interpretability of our SpVec model. 4.1 Experimental Settings We use the English Wikipedia dump (July, 2014) as the corpus to train all the models. After some preprocessing such as document extraction, markup re-moving, sentence splitting, tokenization, lowercasing and text normalization, the plain text corpus contains about 1.6 billion running words.
 or context atoms is set to be 1024 and the dimension of these atom embeddings is set to be 200. The ` 1 regularization penalty  X  was set empirically such that the overall sparsity of words exceeds 95% for all variants of SpVec . The size of mini-batch is set to be 1024. The learning rate is dynamically updated using learning rate  X  0 and the minimum learning rate  X  end are set to be 5  X  10  X  5 and 1  X  10  X  6 , respectively. Following Mikolov et al. X  X  work [18], we use windows with random sizes to draw positive instances and the largest distance between a target word and a context word is 8. During training, we draw 8 negative instances for each positive instance.
 of the sparse representations. This is done by setting the values that is less than a small fraction of the largest element of the vector (in absolute sense) be zero, 0.05.
 on the same corpus with the same settings as our models if possible for fair comparison. The first two models, which are implemented in the word2vec tool 9 are both trained with negative sampling since NCE is not implemented in the tool. The PPMI matrix is built based on the word-context co-occurrence counts with window size as 8. 4.2 Word Analogy The word analogy task can be used to evaluate models X  ability to encode lin-guistic regularities between words, which is introduced by Mikolov et al. [18]. We use two word analogy test sets, namely, Google and MSR , both containing test case like  X  run is to running as walk is to walking  X . The Google dataset 10 contains 19,544 analogy questions, which can be categorized into semantic and morpho-syntactic related subsets [18]. The MSR dataset 11 contains 8,000 anal-ogy questions, categorized according to part-of-speech; all of them are morpho-syntactic.
 missing. Following Mikolov et al. X  X  work [18], for question  X  a is to b as c is to  X , we apply d = arg max d  X  X \{ a,b,c } cos( c  X  a + b , d ) to fill the blank. Table 1 shows the result on word analogy tasks. It shows that word analogy is more challenging for sparse models. None of sparse models outperforms SkipGram or CBoW . Nevertheless, SpVec models can achieve similar performance comparing with SkipGram or CBoW . We also find that all variants of SpVec have similar performance on this task. 4.3 Word Similarity One important indicator to assess the quality of word representations is the clus-tering property X  X imilar words should have similar vectors. We use WordSim353 dataset [8] to investigate the similarity aspect of the resulting word vectors. This dataset contains 353 word pairs along with their similarity/relatedness scores. We use the sparse word vectors to retrieve and rank the most similar words. For every word w in WordSim353 , we rank its similar words by cosine similarities. The ground truth of w  X  X  similar words is a set U ( w ), which is a collection of all the words in WordSim353 that the similarity score with w is higher than 0.6. be comparable to SC and higher than SkipGram  X  X , which in turn is expected to be higher than PPMI . This means that the similarities induced by SpVec models are more consistent with human cognitions. 4.4 Interpretability In this subsection, we talk about the interpretability of the learned sparse vec-tors. We visualize 8 selected words X  sparse vectors from s-SpVec-nce in Fig. 2. We find that similar words have similar sparse patterns and dissimilar words possess different sparse codes. We also observe some interpretable patterns from this figure. For example, the dimensions marked by arrows clearly relate to the plural and singular aspects of words.
 sparse word vectors quantitatively by word intrusion task. The details of con-struction test data for this task are described in [7] and [31]. Roughly, it sorts words dimensionally and chooses the top 5 and an intruder word to form an instance. An intruder word is a word from the bottom half of the sorted list that is in top 10% of a sorted list corresponds to another dimension.
 where a i is the average distance between the intruder word and top words for the i -th instance; and b i is the average distance between the top words for the i -th instance. This measure is first introduced by Sun et al. [31]. The higher the ratio is, the stronger interpretability the representation possesses.
 shows that the interpretability of dense models is weak while sparse representa-tions illustrate much stronger interpretability. This confirms that the sparse rep-resentations are more interpretable than the dense ones. Moreover, the SpVec variants outperform other sparse representation models significantly on the in-terpretability aspect. Compared to SC , the reason might be that our method directly learns the sparse word vectors from the data instead of transforming pre-trained dense vectors to sparse codes that SC does, and thus can avoid the information loss caused by the pipeline of learning sparse representations. In this paper, we propose a method to learn sparse word vectors directly from the plain text, which is based on two assumptions: (1) each word is composed of a few fundamental elements and (2) good representations result in good performances to predict context words. We also give an efficient and easy-to-parallelize algo-rithm that based on NCE to train the proposed model. Additionally, a clustering-based adaptive updating scheme for noise distributions is proposed for effective learning when NCE is applied.
 form very well. The performance loss due to imposing sparse structure on word representations is limited. On the word similarity task, our models outperform dense representations like SkipGram , which is considered to be a strong com-petitor. On the interpretability aspect, the sparse representations significantly more interpretable than dense ones. The experiments demonstrate the effective-ness of our learned sparse vectors in interpretability.

