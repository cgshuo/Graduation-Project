 mode al g orithm eval u ation req u ires that we know the correct answers, i.e., which doc u ments in a collection are relevant to a q u ery. A standard approach to obtain relevance j u d g ments when m u ltiple al g orithms are involved is by a process called P oolin g combined with TREC assessor j u d g ments has g enerated many val u able the TREC Crowdso u rcin g track (since 2011) spearheaded research on alternative mechanisms for collectin g relevance j u d g ments [12]. Contin u in g in the same spirit we propose two hybrid crowd/machine approaches for collectin g j u d g ments. In each an al g orithm (cl u stered or non-cl u stered) selects doc u ments for j u d g ment (in contrast to poolin g ) and crowd workers provide j u d g ments. addressin g domain differences (News vers u s Biomedicine) and controllin g for worker crowd worker to j u d g e relevance for a q u ery related to news and g eneral web information than for a q u ery related to more technical chemical patents or in level of commitment to the task etc. We know that m u ltiple j u d g ments are needed pair. We address this g oal with an ANOVA desi g n experiment. The ANOVA power al g orithm X  X ollection combination. Moreover, each worker m u st j u d g e all topics in a g iven combination. For practical reasons (e. g ., to avoid worker fati gu e), we limit this comparable in topics (prevalence of relevant information) and doc u ments (e. g ., word complexity, n u mber of sentences). We st u dy both main and interaction effects. 
O u r second g oal is to cond u ct another experiment (with News) u sin g typical settin g s seen in crowdso u rcin g papers. Also we u se f u ll doc u ments and more topics (20). We u se a majority vote from three j u d g ments for each decision. We compare o u r approaches with each other and with poolin g in efficiency, effectiveness and cost. 
As hi g hli g hts of o u r res u lts: in o u r first experiment we find, for example, that the cl u stered approach achieves si g nificantly better recall while the non-cl u stered one achieves better precision, F-score and LAM. In the second experiment, the non-cl u stered approach achieves F-score of 0.73 while the cl u stered approach is j u st short of 0.8. LAM (Lo g istic Avera g e Misclassification rate) scores are aro u nd 0.05. Tho ug h not strictly comparable these res u lts appear competitive with the best TREC 2012 res u lts. When compared to poolin g , o u r methods are more efficient and cost far less. Th u s despite wide concerns abo u t q u ality of work done by crowdso u rcin g (e. g ., [11], [28]), o u r methods provide res u lts of hi g h q u ality at m u ch lower cost. Moreover, o u r methods are scalable and easy to extend to other relevance assessment campai g ns. In IR experiments research g ro u ps have obtained relevance j u d g ments from many payment u s u ally in the form of money. Besides cost there is the impracticality of g ettin g j u d g ments for every doc u ment retrieved. Th u s we see wide u sa g e of samplin g for relevance j u d g ments (and other kinds of decisions). Utilizin g non-experts thro ug h crowdso u rcin g platforms has proven to be cost-effective (e. g .,[2],[11],[14]). However, a gg re g ation of noisy labels into a sin g le consens u s label. Even within the TREC/NIST framework, considerable variability across trained assessors still exists (e. g ., [4], [26]); this is shown to increase when non-experts are u sed [21]. To address noise, typically a majority vote is obtained across several j u d g ments [18]. 
Several others have reco g nized the challen g es of relevance assessment that are not met by poolin g , and have introd u ced some new approaches. Soboroff et. al. examined some methods that co u ld approximate poolin g , incl u din g u sin g a shallower poolin g depth and identification of the d u plicate doc u ments normally removed by poolin g , improvin g u pon standard poolin g methods [20]. We b u ild on their methods, u sin g the and Joho sidestep poolin g by u sin g existin g TREC j u d g ments as inp u ts to a relevance feedback al g orithm [23], which provides val u able information to identify which doc u ments to investi g ate f u rther. Carterette et. al. al g orithmically determined the smallest test collection size to j u d g e u sin g a variation of MA P [3]. Their method u ses ranked lists to choose doc u ments for j u d g ment, which is one aspect we u se in o u r own methods. Yilmaz et. al. u sed stratified samplin g with poolin g at different k-depths in [30], providin g g ood res u lts, b u t the foc u s in their st u dy was primarily on improvin g existin g IR eval u ation meas u res. 
In 2011, the TREC Crowdso u rcin g track was be gu n to examine the crowd's ability to provide q u ality relevance assessments while addressin g the above challen g es [12]; this contin u ed with the TRAT s u b-track in 2012 [22] and 2013. A n u mber of approaches u sin g the crowd were examined. The top-performin g BU P T-Wildcat [29] u sed an elaborate m u lti-sta g e prescreenin g process for crowd workers and an E-M Ga u ssian process to determine a consens u s label amon g these workers. The complexity in their method, req u irin g the development, eval u ation, and testin g of prescreenin g tools s ugg ests that it mi g ht have diffic u lty in scalin g . Likewise, in 2012, Skierarchy, a top-performer req u ires an impressive b u t complex hierarchy of experts and crowd workers [16]; this approach too mi g ht have problems with increasin g scale d u e to the req u irement of more s u bject-matter experts, which are often in limited s u pply. In contrast we u se a hybrid machine-h u man approach involvin g f u sion of ranks across s u bmitted lists of retrieved doc u ments, optional text-feat u re based cl u sterin g , doc u ment batchin g and selection, and criteria to stop the relevance j u d g ment process. The power of o u r approach is in its simplicity and in its effectiveness. The approach is an extension of o u r earlier work in TREC [8]; refined doc u ment selection strate g ies. In addition, we present more extensive experiments in m u ltiple domains compared to the TREC TRAT effort. We propose two al g orithms (cl u stered and non-cl u stered) to select doc u ments for relevance j u d g ments. Consider a typical TREC ad hoc retrieval task with a set of M doc u ments ranked by system estimated relevance to T . Both al g orithms start with the takes a limited n u mber (e. g ., 100) of the top ranked doc u ments. O u r advanta g e is that we need not u se an artificial c u toff. We then calc u late a score CW for each doc u ment in U with respect to topic T . Doc u ments in U are ranked by their CW scores. C S T (d) is the Bord a co un t which takes into acco u nt doc u ment rank [6]. maxim u m n u mber of doc u ments that may be s u bmitted in a r u n. For r u ns not retrievin g d , rank is eq u al to N. The TREC campai g ns g enerally allow a maxim u m of 1000 s u bmitted doc u ments per r u n per topic. In trainin g r u ns u sin g 10 separate topics we tested  X  from 0 to 1 in increments of 0.05. For each  X  we assessed the res u ltin g rankin g of U u sin g S X   X  . trainin g topics for each dataset the best res u lts were at  X  = 0.8. We therefore u se this val u e in o u r experiments. At this point o u r two al g orithms deviate as described next. 3.1 Algorithm 1  X  Non-clustered Approach Doc u ments in U ranked by their CW T score are partitioned into batches of eq u al size. Startin g with the top rankin g batch, crowd workers j u d g e doc u ments till a batch with no relevant doc u ments is reached. J u d g ment then stops with all remainin g doc u ments batch size is 20. For this trainin g step and for later trainin g steps we r u n the approach u sin g the g old standard data to sim u late crowd relevance assessment. This best-case scenario places a ceilin g in the effectiveness of o u r al g orithms. 3.2 Algorithm 2  X  Clustered Approach The motive is to involve text feat u res in the doc u ment selection process. The well-known cl u ster hypothesis [13] indicates text u ally similar doc u ments are more likely to be co-relevant (or co-non-relevant) to a g iven topic than dissimilar doc u ments. We first cl u ster doc u ments in U u sin g k-means representin g each doc u ment with a word-a u tomatically selected for j u d g ment followed by the next ranked batch. If a batch accommodate doc u ments retrieved by possibly distinct retrieval al g orithms. 
We establish k for k-means u sin g standard approaches (e. g ., [4], [16]). We eval u ate k=5 thro ug h 20 in increments of 3 and calc u late the variance in the n u mber of relevant doc u ments appearin g in each cl u ster. Greater variance implies an increasin g tendency for relevant doc u ments to concentrate in fewer cl u sters, which is desirable. We then explore val u es of k one u nit away on either side of the best val u e. As a res u lt, we set k=11 for both collections in o u r experiments. Batch size remains 20 doc u ments as in al g orithm 1. 4.1 Datasets and Documents G e n er a l D o ma i n : News Dataset. This is the TREC-8 ad hoc task (TREC disks 4 and TREC-8 ad hoc task [27] and topics 401-443. S peci a lized D o ma i n : OHSUMED Dataset. This is the TREC-9 filterin g track dataset, topics 1  X  43 [19]. 
OHSUMED has only titles, abstracts and metadata whereas the News doc u ments first 7 sentences for News; for OHSUMED, we u se the title and the abstract. These red u ced News doc u ments are shown to crowd workers and they are u sed when cl u sterin g (section 3.2). Collection feat u res (after this step is completed) are provided in Table 1. Rows 3 to 6 of Table 1 ill u strate differences intrinsic to the domains. 4.2 Topics, Runs, Gold Standard Data To prevent topic differences from biasin g res u lts, we identified three comparable topics (in prevalence of relevant doc u ments) from each collection. Int u itively prevalence, the percenta g e of s u bmitted doc u ments that is relevant, may indicate topic diffic u lty. P revalence for News topics ran g es from (0.03, 2.46) and for OHSUMED (0.04, 4.62). We ranked the OHSUMED topics that fall in the overlappin g re g ion (0.04, 2.46) and divided them into 3 g ro u ps of ro ug hly eq u al size. Randomly selectin g one OHSUMED topic from each g ro u p we then identified 3 News topics that most closely matched in prevalence (see Table 2). For each selected topic all doc u ments in s u bmitted r u ns of past TREC participants are collected. 
For News the TREC-8 ad hoc task obtained binary relevance assessments u sin g poolin g and TREC experts. For OHSUMED the TREC-9 filterin g task provides assessments in one of three relevance states ( X  X ot relevant X ,  X  X artially relevant X , or  X  X efinitely relevant X ). Followin g [2] we g ro u p  X  X artially relevant X  and  X  X efinitely relevant X  doc u ments as  X  X elevant X . It sho u ld be noted that OHSUMED relevance j u d g ments were obtained in earlier st u dies [9, 10] and not via TREC expert assessors and poolin g . We sim u late poolin g (selectin g top 100 doc u ments) with OHSUMED. 4.3 Participants Crowd participants were from Amazon Mechanical T u rk (MT u rk). P articipants were permitted to participate once (as tracked by I P address and MT u rk ID). They were compensated $0.20/batch of 20 doc u ments assessed. For each al g orithm X  X ollection collection. P articipants not completin g the f u ll assessment of 3 topics had their assessments removed and the task g iven to other crowd participants. J u d g ments were collected via a web interface. Table 3 provides the means and standard deviations across o u r fo u r metrics: recall, precision, F-score and LAM. Statistically si g nificant differences are marked by * (p &lt; 0.05) and ** (p &lt; 0.002). Examinin g main effects we find that the non-cl u stered al g orithm g ives si g nificantly s u perior precision, F-score and LAM compared to the recall. In main effects we also find that the biomedical domain provides si g nificantly res u lts later. P ost-hoc analyses were cond u cted on all statistically si g nificant pairin g s between al g orithm and domain for each meas u re. All pairs tested were also fo u nd to between domains on these meas u res. The two-way, al g orithm  X  domain interaction res u lts are similar in precision and F-score; combinations involvin g the non-cl u stered al g orithm and either domain are si g nificantly s u perior to combinations involvin g the cl u stered approach. For LAM this pattern is seen only for OHSUMED. For recall, We reject all b u t one of the two-way interaction n u ll hypotheses. The relative performance of the two al g orithms is consistent across collections. The non-cl u stered al g orithm provides better precision, F-score, and LAM while the improvement in recall is consistent with the cl u ster hypothesis. However, this is at the expense of precision; non-relevant doc u ments are also attracted towards relevant doc u ments thro ug h similarity. In the combined F-score, the simpler non-cl u stered al g orithm wins over cl u sterin g . 
Another aspect that mi g ht have ca u sed lower precision for the cl u stered approach 3.2). Given 11 cl u sters/topic and 20 doc u ments/batch we have a minim u m of 220 j u d g ments. Retrospectively we feel precision mi g ht improve if we are more selective distinctive relevant doc u ments. We address this in Section 7.1. 
S u rprisin g ly the performance for OHSUMED is better than for News on all meas u res. We expected familiarity with the domain to favo u r g eneral news stories and not biomedicine. A possible explanation is that with News we had to limit o u r doc u ment to the first 7 sentences of text in order to possible len g th-based bias across domains. It may be that the text necessary for relevance j u d g ments appears o u tside of these initial 7 sentences for News. OHSUMED have foc u sed abstracts which may have enabled more acc u rate j u d g ments. We address this in the next experiment. 
Finally tho ug h not strictly comparable, o u r best LAM score for News (0.049) is better than the best scores obtained in TREC (also for News). Importantly, this is achieved while maintainin g reasonable scores for the other three meas u res. 6.1 Comparing the Algorithms and Poolin g: Efficiency and Effectiveness We compare the al g orithms with each other and with poolin g in efficiency balanced u nion of all retrieved doc u ments s u bmitted by all r u ns (this is typically set to 1000 in TREC r u ns). Th u s we r u n the risk of j u d g in g a lar g e n u mber of doc u ments; most are likely to be non-relevant. 
P oolin g for the 3 OHSUMED topics with N=100 wo u ld have j u d g ed 50% to 62% of the s u bmitted doc u ments; for News 6.7 to 16%. In contrast the cl u stered approach j u d g ed 8 to 14% for OHSUMED and 5 to 9.4% for News. The non-cl u stered approach j u d g ed 1% to 5% for OHSUMED and 0.6% to 3% for News. The savin g s start with the f u ll set of s u bmitted doc u ments. The cl u stered approach is less efficient cl u sters (220 doc u ments). The non-cl u stered approach has no s u ch minim u m. doc u ments. Altho ug h the means presented earlier indicate effectiveness, we can look at the res u lts in more detail. On avera g e across the topics, the non-cl u stered al g orithm eval u ated 3.1% of the s u bmitted OHSUMED doc u ments, to find 68.0% of the relevant doc u ments; the cl u stered al g orithm eval u ated 11.0% findin g 77.5%. For News the non-cl u stered al g orithm only eval u ated 1.8% of the collection findin g 46.8% of the relevant doc u ments. The percenta g es were low for topics 421 and 436. With cl u sterin g 7.6% of the News s u bmissions were eval u ated, with 73.2% of the effectiveness of o u r strate g ies in f u t u re work. 
P artic u larly noteworthy is o u r al g orithm  X  s s u ccess even when there are only a few relevant doc u ments  X  for example, the non-cl u stered al g orithm only eval u ated 0.6% of the 15,636 doc u ments retrieved for topic 403, b u t was able to find nearly all of the 21 relevant doc u ments. 6.2 Comparing Methods on Cost Relevance j u d g ments costs are important g iven b u d g etary constraints. The 2007 TREC Le g al track overview doc u ment is one in which TREC relevance assessment costs are indicated. Also they note that h u man assessors eval u ate on avera g e 20 doc u ments per ho u r. Their relevance assessment cost was estimated at $150 an ho u r, or $7.50 per doc u ment [25]. A total of 9442 j u d g ments wo u ld have been made with poolin g settin g N = 100 for OHSUMED and 4758 j u d g ments were made for News. $70,815 and $35,685 for OHSUMED and News respectively. Admittedly assessment for TREC Le g al wo u ld have been amon g st the costliest. However, even if we were to red u ce the TREC cost drastically to $1/j u d g ment, the TREC poolin g process for the 3 topics in OHSUMED and News wo u ld be $9,442 and $4,758 respectively. In comparison, o u r cost was $0.22/batch of 20 doc u ments, incl u din g Amazon Mechanical T u rk overhead fees; this is sli g htly more than $0.01/doc u ment. The cost for all 24 participants to eval u ate the same 3 News topics was $792.00 for the cl u stered al g orithm and $179.52 for the non-cl u stered al g orithm. For OHSUMED these costs are $496 and $143 respectively. Usin g only 3 crowd workers and the majority decision, as disc u ssed in [1, 2], we can red u ce these costs f u rther by 87.5%. 6.3 Detecting Potentially Relevant Documents Another aspect of o u r approach is that since we start with all s u bmitted doc u ments there is the possibility of discoverin g relevant doc u ments missed by the poolin g doc u ments that were not incl u ded in the TREC pool and so were not j u d g ed and ass u med non relevant. There were also fo u r doc u ments that o u r participants tho ug ht were relevant that were declared non relevant by TREC assessors and 10 doc u ments in the reverse direction. These n u mbers may appear to be minor and yet they co u ld in a different context make an appreciable difference. We present res u lts from a second experiment with the News dataset. We u se only 3 on a majority vote. We selected 17 additional topics randomly from the News dataset and added these to o u r 3 topics from experiment 1 (topics 403, 421, and 436). Also we u se the f u ll text of News items rather than j u st the first 7 sentences pl u s headline. doc u ment. Similar to the earlier experiment, each participant is expected to eval u ate 3 s u bstit u te participant was solicited. 
The mean precision, recall, F-score, and LAM scores are stron g : 0.8826, 0.6 282, 0.7270, and 0.0499 respectively for the non-cl u stered al g orithm, and 0.8174, 0.7645, 0.7861 and 0.0524 respec t tests fo u nd that the non-cl precision and in LAM (bot h non-cl u stered one in recall score, these res u lts are con had u sed a lar g er n u mber The F-score res u lts indicat more than adeq u ately m a experiment. In g eneral, sc o experiment 1 (3 topics) w precision). Foc u sin g only o 19 of 24 meas u rements (2 g ives better res u lts and o difference is that experime n the first 7 sentences. It ap p 7.1 Reducing the Num b In o u r cl u stered al g orithm, may be why this al g orit h al g orithm. If we can b e performance overall. We e x rank the 11 cl u sters by th e lowest to hi g hest mean eva l When all 11 cl u sters ar e News the best F-score is o b it is with the top 3 cl u s Experiment 2 yields simil a 3. These emphasize that research we will also explo r locate a majority of the relevant doc u ments in two types of collections at a fraction of the cost of poolin g . In both experiments we obtain LAM scores for News that are competitive with the best 2012 TREC Crowdso u rcin g res u lts [22] (tho ug h the experiments are not strictly comparable). We find that contrary to some predictions [15] and o u r own expectations, res u lts in OHSUMED (e. g ., LAM is 0.037), a more challen g in g domain, are also stron g . Overall we have demonstrated that o u r hybrid backs u p the earlier findin g s by Soboroff et. al. [23], Carterette et. al. [3], and Sanderson and Joho [20]; it b u ilds on aspects of their methods with a cl u sterin g rob u st desi g n considerin g caref u lly potential variations across crowd workers. 
There are a n u mber of ways in which we can improve o u r approach and extend this methods. This will parallel efforts s u ch as [23]. Third we wo u ld like to cond u ct topic level analysis of o u r data. Some topics are likely more challen g in g for crowd workers likely to be challen g in g . A fo u rth direction is to analyze the crowd j u d g ments to see the extent to which there is consens u s. O u r dataset is rich in that we have each q u ery X  workers/al g orithm). This will offer insi g hts into variations across workers. Last, we and how system rankin g s coordinate with the f u ll pool. 
