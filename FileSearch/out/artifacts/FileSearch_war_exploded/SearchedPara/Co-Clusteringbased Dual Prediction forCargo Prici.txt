 This paper targets the problem of cargo pricing optimization in the air cargo business. Given the features associated with a pair of origination and destination, how can we simulta-neously predict both the optimal price for the bid stage and the outcome of the transaction (win rate) in the decision stage? In addition, it is often the case that the matrix rep-resenting pairs of originations and destinations has a block structure, i.e., the originations and destinations can be co-clustered such that the predictive models are similar within the same co-cluster, and exhibit signi cant variation among different co-clusters. How can we uncover the co-clusters of originations and destinations while constructing the dual predictive models for the two stages?
We take the rst step at addressing these problems. In particular, we propose a probabilistic framework to simul-taneously construct dual predictive models and uncover the co-clusters of originations and destinations. It maximizes the conditional probability of observing the responses from both the quotation stage and the decision stage, given the features and the co-clusters. By introducing an auxiliary dis-tribution based on the co-clustering assumption, such con-ditional probability can be converted into an objective func-tion. To minimize the objective function, we propose the COCOA algorithm, which will generate both the suite of predictive models for all the pairs of originations and desti-nations, as well as the co-clusters consisting of similar pairs. Experimental results on both synthetic data and real data from cargo price bidding demonstrate the effectiveness and efficiency of the proposed algorithm.
 I.5.2 [ Pattern Recognition ]: Design Methodology| clas-si er design and evaluation ; I.5.3 [ Pattern Recognition ]: clustering| algorithm ; I.5.4 [ Pattern Recognition ]: Ap-plications c  X  Algorithms; Performance Co-clustering; dual predictive models
Revenue management in the air cargo business is a fast growing eld. It usually consists of two stages: the bidding stage where the vendor provides a bidding price with respect to a pair of origination and destination, or OD pair, and the decision stage where the customer makes a decision whether to accept this price or not. Compared to other industries such as passenger airlines or hotels, this eld is more chal-lenging in multiple respects due to the speci c character-istics of cargo inventory, cargo business, and cargo booking behavior. This renders traditional yield management models ineffective or inefficient, thus necessitates the development of new models.
 Here we focus on the following three major challenges. First, as illustrated in Figure 1, the number of transactions varies signi cantly among different OD pairs. For those pairs whose transaction volume is small, the resulting predictive model tends to be inaccurate due to the lack of training data. Second, most existing techniques construct predic-tive models for the two stages separately, thus prevent key information to be shared by these models. Third, the origi-nations and destinations can be naturally co-clustered such that the underlying predictive models are similar within each co-cluster. This property has not been exploited for improv-in g model performance and gaining insights into different OD pairs.

To address these challenges, in this paper, we propose a novel probabilistic framework to simultaneously construct dual predictive models for each OD pair, while uncovering the co-clusters of originations and destinations. It is based on the conditional probability of observing the two types of responses from the two stages, given the features with re-spect to the OD pair, and the mappings for co-clustering. We approximate this probability using an auxiliary distribu-tion that satis es the co-clustering assumption, and develop a special case of the framework based on generalized linear models. Furthermore, we propose an effective and efficient algorithm named COCOA for solving the resulting optimiza-tion problem, whose computational complexity is linear with respect to the total number of OD pairs. Finally, we evaluate the performance of COCOA from various aspects using both synthetic data and real data on cargo price optimization.
To the best of our knowledge, we are the rst to tackle the problem of cargo price optimization from a holistic per-spective. In other words, we encapsulate multiple correlated models into a single probabilistic framework. The main ad-vantage is that it allows key information to be shared among the different models. For example, the co-clusters provide regularization for the dual predictive models, and accurate dual predictive models in turn will improve the performance of co-clustering. Notice that for the OD pairs with a small transaction volume, such regularization helps alleviate the problem of small training set size.

The main contributions of this paper can be summarized as follows. 1. A novel probabilistic framework for cargo price opti-2. An effective and efficient algorithm named COCOA for 3. Experimental results on both synthetic and real data
The rest of the paper is organized as follows. In Section 2, we brie y review the related work. The probabilistic frame-work for simultaneous dual prediction and co-clustering is proposed in Section 3, where we also present the COCOA algorithm. Section 4 shows promising results on both the synthetic data as well as a real data set for air cargo price optimization. Finally, we conclude in Section 5.
In this section, we review the related work on multi-label learning, multi-way clustering, and cargo pricing optimiza-tion.
Multi-label learning studies the problem where each ex-ample is associated with a set of labels [34]. One key is-sue is to exploit correlations or dependencies among mul-tiple labels. According to [39], existing strategies for la-bel correlation exploitation can be grouped into three cate-gories: rst-order, second-order, and high-order approaches. First-order methods assume that labels are independent, and multi-label learning problem can be transformed into a number of independent binary classi cation problems, e.g., ML-kNN [40]. Second-order approaches consider the pair-wise relations between labels. Then the multi-label learning problem is transformed into the label ranking problem which aims at properly ranking every relevant-irrelevant label pair for each training instance, e.g., Rank-SVM [10]. Various methods have been proposed for high-order label correlation learning. For example, LEAD [39] employed Bayesian net-work to encode the conditional dependencies of the labels as well as the feature set, with the feature set as the common parent of all labels. The LS-ML algorithm was proposed for multi-label learning to extract common subspace shared among multiple labels [18]. A hypergraph spectral learn-ing formulation was proposed for multi-label classi cation to exploit the correlation information among different labels using hypergraph [31]. TRAM [20] studied the problem of transductive multi-label learning by utilizing the informa-tion from both labeled and unlabeled data. LIFT [38] con-structed features speci c to each label by conducting cluster-ing analysis on its positive and negative instances, and then performed training and testing by querying the clustering results. MAHR [15] aimed to discover the label relationship via a boosting approach with a hypothesis reuse mechanism. A generic empirical risk minimization (ERM) framework was proposed for large-scale multi-label learning [36]. A theoret-ical analysis on multi-label consistency was proposed in [12]. The authors proved a necessary and sufficient condition for the consistency of multi-label learning based on surrogate loss functions. Another related work is MLLOC [16], which assumed that the label correlation may be shared by only a subset of instances rather than all the instances.
Our problem setting for cargo pricing optimization is dif-ferent from multi-label learning. This can be seen from the fact that the two types of responses are obtained based on different sets of inputs: for the bidding stage, the inputs include the features for each OD pair; and for the decision stage, in addition to these features, the inputs also include the estimated price from the predictive model of the bidding stage. Furthermore, in our proposed framework, we leverage the intrinsic co-clusters of originations and destinations to facilitate information sharing, which is particularly bene -cial for those OD pairs with a small transaction volume.
Different from traditional clustering techniques [17], which are designed to group objects so as to maximize within clus-ter similarity and between cluster dissimilarity, for sparse re-lational data, co-clustering or bi-clustering methods [24] aim at simultaneously cluster objects of each type. These meth-ods typically produce groupings of better quality by leverag-ing clusters of other types in the similarity measure [14]. The information-theoretic co-clustering method [8] is among the rst to address this problem, which monotonically increases the preserved mutual information by intertwining the row and column clustering. Follow up work includes the gener-ative model for evolutionary heterogeneous clusters in dy-namic networks [32], the evolutionary co-clustering method proposed in [19], the MMRC model proposed in [23], the minimum Bregman information principle proposed in [1], the general binary clustering model and its variations pro-posed in [21], minimum sum-squared residue co-clustering p roposed in [6], etc. The proposed technique is also re-lated to our previous work in [41], where we studied the co-clustering of multiple time series that t into a matrix.
Co-clustering has been generalized to handle more than two object types, i.e., multi-way clustering. Examples in-clude Consistent Bipartite Graph Co-partitioning (CBGC) [11], which aims at collectively clustering star-shaped relation-ships among different types of objects; spectral relational clustering [22], which iteratively embeds each type of objects into low dimensional spaces and bene ts from the interac-tions among the hidden structures of different types; collec-tive matrix factorization [29], which assumes shared param-eters among factors when an entity participates in multiple relations; etc. Furthermore, researchers have proposed var-ious techniques to automatically determine the number of clusters for each object type, such as cross-associations [3], AutoPart [2], PaCK [14], etc.

The major difference between existing methods for multi-way clustering and our proposed work is as follows. Here the inputs of co-clustering are the dual predictive models for both the bidding stage and the decision stage, and our goal is to jointly infer the mappings for co-clustering as well as the dual predictive models.
The air cargo industry has substantially grown over the past decades, driving the need of a structured environment with the explicit goal of maximizing revenues by offering optimized bidding prices. Air cargo companies use bidding price to accept/reject incoming bookings: if the rate of the booking is lower than the bidding price value then the book-ing is rejected, otherwise it is accepted. Bidding price con-trols are revenue based and have the advantages of being simple, having a natural interpretation as the marginal value of a given resource, and have a very good revenue perfor-mance [33, 5].

Optimal bidding price methods were introduced [30], and extended by [28], and [35]. [4] addressed the issue of origin-destination-speci c demand, which is common for low fare passengers cargo, versus the itinerary-speci c demand, widely used in the passenger models. They extend three popular models to incorporate origin-destination demand and intro-duce a routing algorithm tailored towards the special struc-ture of the ight networks and their objectives. The simula-tion results report the superiority of an extended probabilis-tic model over a rst come rst serve policy applied to cargo revenue management. [26] proposed a dual ascent scheme to solve the Lagrangian of the probabilistic model used in [4].
Our proposed framework is signi cantly different from ex-isting techniques due to the fact that it addresses the prob-lem of pricing optimization from a holistic perspective. In particular, it bridges the bidding stage and the decision stage by jointly learning the dual predictive models, and it leverages the intrinsic co-clusters of originations and des-tinations to enable information sharing among different OD pairs. Therefore, it is able to improve the performance of both predictive models, which eventually lead to increased revenue.
In this section, we propose a probabilistic framework to si-multaneously construct dual predictive models, and uncover the co-clusters of originations and destinations, followed by the introduction of the COCOA algorithm for solving the optimization problem. Suppose that there are O originations and D destinations. Let x i;j;k 2 R d denote the features extracted with respect to the i th origination and the j th destination for the k th action, i = 1 ; : : : ; O , j = 1 ; : : : ; D , k = 1 ; : : : ; K K i;j denotes the number of transactions with respect to the i th origination and the j th destination. For x i;j;k , we aim to predict two types of responses: y (1) i;j;k 2 R + , which denotes the quoted price in the bidding stage, and y (2) i;j;k 2 f which denotes the outcome of the transaction (successful or not) in the decision stage. Notice that our problem setting is different from traditional multi-label learning [37] in the sense that the estimated ^ y (1) i;j;k is a function of x the estimated ^ y (2) i;j;k is a function of both x i;j;k whereas multi-label learning assumes that the prediction of different labels is based on the same set of inputs. Fur-thermore, in our proposed probabilistic framework, we ex-ploit the 2-dimensional structure formed by combinations of O originations and D destinations in order to gain deeper understanding of the groupings of originations and destina-tions, which in turn help improve the predictive models.
We propose to use generalized linear models to predict the two models respectively. For example, we could use the gression model; and the logit link function g 1 ([ x  X  i;j;k which leads to the logistic regression model.
 Notice that for each stage s = 1 ; 2, we can construct an O D array B ( s ) such that B ( s ) i;j , the element in the i and the j th column, is set to ( s ) i;j . We assume that the rows and columns of B ( s ) can be re-arranged such that B ( s ) a block structure where the elements within the same block are similar to each other, and the elements across different blocks are dissimilar. This is equivalent to co-clustering the O originations to R row clusters, and the D destinations to C column clusters. Furthermore, in pricing optimiza-tion, it is usually the case that the block structure is shared across the two stages. Let u 1 ; : : : ; u O and v 1 ; : : : ; v the O originations and D destinations respectively; Let R denote the mapping from each origination ( u 1 ; : : : ; u one of the R row clusters (^ u 1 ; : : : ; ^ u R ), and C mapping from each destination ( v 1 ; : : : ; v D ) to one of the and C such that R ( u i ) = ^ u r and C ( v j ) = ^ v c that the probability p ( s ) ( ( s ) ; u i ; v j j R ; C ) of having for the combination of the i th origination and the j th desti-nation can be approximated with an auxiliary distribution q p : where ( s ) i;j is a normalization parameter such that q ( s ) valid probability distribution. According to Equation 1, the auxiliary distribution q ( s ) ( ( s ) ; u i ; v j j R ; C clustering assumption, and it can be decomposed into ve non-constant terms, all of which are derived from p ( s ) ): p (^ u r ; ^ v c ), which is the joint probability of the r and the c th column cluster; p ( s ) ( u i j ^ u r ) ( p ( s ) the probability of having the i th origination u i (the j tination v j ) in the r th row cluster (the c th column cluster); having ( s ) given the i th origination (the j th destination). Notice that p (1) (^ u r ; ^ v c ) = p (2) (^ u r ; ^ v c tion that the block structure is shared by B (1) and B (2) Therefore, we omit the superscript of this term for brevity.
Lemma 3.1 in [41] shows that certain probabilities derived from p ( s ) ( ) are preserved in the auxiliary distribution q including: q
Based on the above discussion, the conditional probability of observing the data (the two types of responses y ( s ) the vectors of parameters ( s ) i;j given the features x i;j;k the mappings R , C ) can be expressed as follows. = +  X  +  X  +  X  +  X  In Equation 3, the overall probability is approximated by the sum of ve terms: the rst two terms come from the gen-eralized linear models to predict y (1) i;j;k and y (2) i;j;k term is xed given R and C ; the fourth term measures the probability of having ( s ) i;j for the i th origination of the r row cluster; and the last term measures the probability of Finally, both the vectors of parameters ( s ) i;j and the map-pings R , C can be obtained by maximizing Equation 3 with speci c choices of the probabilities. In this subsection, we specify the probabilities used in Equation 3, which originate from the application of pricing optimization.

For predicting y (1) i;j;k , we use linear regression model, and j x j th destination respectively, ( s ) constants. Furthermore, it is straight-forward to see that the MLE estimate of both expectations can be expressed as origination-wise and destination-wise average, i.e., E p ( s )
B efore specifying probabilities p ( s ) ( u i j ^ u r ) and p we rst compute the expectations of the vector ( s ) with respect to each row/column cluster based on q ( s ) ( ). First of all, the conditional distribution of the vector ( s ) given the r th row cluster can be expressed as follows. q = p where we repeatedly applied Equation 2 to replace the prob-abilities derived from q ( s ) ( ) with those derived from p As discussed before, both p ( s ) ( ( s ) j u i ) and p ( s ) low Gaussian distributions. Therefore, the expectation of ( s ) given ^ u r can be derived as follows. Similarly, the expectation of ( s ) given ^ v c can be derived as follows. Us ing these expectations, p ( s ) ( u i j ^ u r ) and p ( s ) speci ed as follows.
Based on the above speci ed probabilities, we have the fol-lowing objective function, which is the negative log probabil-ity of observing the data. Minimizing the objective function with respect to ( s ) i;j ; R ; C will lead to the dual predictive models with respect to the bidding price and the outcome of the transaction, as well as the co-clustering of origina-tion/destination pairs. ) g +  X  where 1 ; : : : ; 5 are constants that depend on 0 , ( s )
To minimize the objective function in Equation 7, we pro-pose the following algorithm named COCOA based on block coordinate descent, which is described in Algorithm 1.
The COCOA algorithm works as follows. It takes as input the features x i;j;k , two types of responses y (1) i;j;k the total number of row clusters R , the total number of column clusters C , parameters 1 ; : : : ; 5 , and numbers of iteration steps n 1 , n 2 . Then it proceeds by alternating the optimization with respect to ( s ) i;j and R , C . Finally, it outputs both the vectors ( s ) i;j , as well as the two mappings
R and C that generate the co-clustering of O originations and D destinations.

To be speci c, in Steps 1 and 2, we initialize both ( s ) : ;j to be 0 vector, and initialize both R and C by ran-domly assigning each origination/destination to one of the row/column clusters. Then we initialize both p ( s ) ( u i p ( s ) ( v j j ^ v c ) assuming uniform distribution among the origi-nations/destinations within the same row/column cluster. Next we repeat the following steps n 1 times until conver-gence. In Steps 5 and 6, we solve for ( s ) i;j via regularized risk minimization. Notice that here we have two regularization wise average. Steps 8 to 20 form an inner loop for updat-ing the mappings R and C . In the inner loop, we rst compute the expectations of ( s ) within each row/column cluster in Steps 9 and 10. Then we update mapping R by assigning each origination to its closest row cluster (Steps 11 to 14), and mapping C by assigning each destination to its closest column cluster (Steps 15 to 18). Finally, we update both p ( s ) ( u i j ^ u r ) and p ( s ) ( v j j ^ v Al gorithm 1 COCOA Algorithm 2: Randomly initialize R and C ; 3: Initialize p ( s ) ( u i j ^ u r ) = 1 j 4: for t = 1 to n 1 do 5: Solve for (1) i;j by minimizing 6: Solve for (2) i;j by minimizing  X  8: for t  X  = 1 to n 2 do 9: Compute ^ ( s ) r; : using Equation 4, r = 1 ; : : : ; R ; 10: Compute ^ ( s ) : ;c using Equation 5, c = 1 ; : : : ; C ; 11: for i = 1 to O do 12: Let ^ r arg min r 13: Update R ( u i ) ^ u ^ r ; 14: end for 15: for j = 1 to D do 16: Let ^ c arg min c 17: Update C ( v j ) ^ v ^ c ; 18: end for 19: Update p ( s ) ( u i j ^ u r ) and p ( s ) ( v j j ^ v 20: end for 21: end for
In this subsection, we discuss the proposed COCOA algo-rithm from various aspects.

First of all, in COCOA , ( s ) i;j is obtained via regularized risk minimization. In the rst iteration of the outer loop, Steps 5 and 6 are reduced to ridge regression and L 2 reg-ularized logistic regression respectively. In the following it-erations, both are regularized by the origination-wise and destination-wise average. Notice that in the training stage, for the prediction of y (2) i;j;k , an alternative choice is to use [ x we could use the estimated price x  X  i;j;k (1) i;j to replace the real price y (1) i;j;k . In this way, the ridge regression model in Step 5 of COCOA for predicting y (1) i;j;k will have an additional regularizer mated price instead of the real price is that it provides a mo re accurate model for the test stage, since during the test stage, the real price will be unknown. However, it comes with the cost of coupling the estimation of (1) i;j and (2) which might affect the convergence of the iterative algo-rithm. Investigating effective and efficient algorithms for coupled parameters is one of our future research directions.
Second, the computational complexity of COCOA is shown in the following lemma.

Lemma 1. The time complexity of COCOA is O [ n 1 ( d 2 : 376 d  X  plexity is O [ d 3 + ODd ] .
 Proof. The time complexity can be proven based on the fact that the computational cost of COCOA is dominated by regularized risk minimization in Steps 5 and 6, as well as the inner loop between Steps 8 and 20. The computa-tional complexity of regularized risk minimization is d 2 : 376 d  X  Winograd algorithm [7] for matrix inversion, and logistic re-gression using coordinate ascent, conjugate gradient ascent, quasi-Newton method, or iterative scaling [25]. Within the inner loop, the time complexity of Steps 9 and 10 is O ( OD ), Steps 11 to 14 is O ( ORd ), Steps 15 to 18 is O ( DCd ), and Step 19 is O ( Od + Dd ).

On the other hand, the space complexity includes both the space requirement for d d matrix inversion, the stor-age of all the vectors of parameters ( s ) i;j , the probabili-j = 1 ; : : : ; D , r = 1 ; : : : ; R , c = 1 ; : : : ; C .
Lemma 1 implies that the time complexity of COCOA scales linearly with respect to the number of origination-destination pairs, and the total number of transactions across all origination-destination pairs. It also depends on both n and n 2 . As we will show in the next section, empirically the number of iterations required for the inner loop n 2 and the outer loop n 1 to converge typically does not exceed 8.
In this section, we demonstrate the effectiveness of the proposed COCOA algorithm both on synthetic and real data. To our best knowledge, COCOA is the rst algorithm for co-clustering based dual prediction framework and we com-pare its performance with an advanced hierarchical cluster-ing and prediction methodology, hglm , which is currently adopted by a worldwide cargo company. Methodology de-tails are described in 4.2.1. Different from COCOA , hglm performs co-clustering and prediction separately. In addi-tion, this two-step method tends to introduce extra vari-ances by using outputs from the rst step model as inputs for the second model. Also, there is no adaptive feedback process to improve the performance for both models. CO-COA can potentially tackle these challenges by integrating a regularized linear submodel for the bid price prediction and a generalized linear submodel for the win-rate prediction in a consistent framework. In this subsection, we rst test the performance of CO-COA on a synthetic data set which mimics the real world problem. The synthetic data consists of 3 row clusters and 3 column clusters. Each row and column cluster consists of different number of\originations"and\destinations"(ranges from 3 to 5). The speci c vectors for i th row, j th column y i;j; : ( s = 1 for the linear model and s = 2 for the generalized linear model) are generated as following. 1. For each row and column cluster, generate cluster spe-2. Given row cluster mean ^ ( s ) r; : and standard deviation 3. Given column cluster mean ^ ( s ) : ;c and standard devia-4. ( s ) i;j is nalized through the weighted average of ( s ) 5. For each OD pair, generate input features x i;j; : and
We set R; C = 3, ( s ) i; : and ( s ) : ;j to be 0 vectors and ran-domly generate cluster members as starting points. We use 5-fold cross validation to choose . After the algorithm con-verges, the estimated winning probability for each OD pair is shown in Figure 3(b). The red color stands for higher win-ning probability given all other conditions the same and the blue color denotes relatively lower wining probability. We also use the solid lines to illustrate the clustering boundaries. As we can see, OD pairs in similar colors are grouped to-gether, which implies COCOA recovers all the clusters. The winning probability for the OD pairs before co-clustering is also shown in Figure 3(a) for comparison.
In this subsection, we test the performance of COCOA on a real cargo pricing optimization problem. We select 20 originations and 20 destinations with relatively high volume of transactions. Among the resulting 400 OD pairs, about 25% of them have less than 20 transactions. Such OD pairs are excluded from training , which is estimated based on its cluster membership's average ^ ( s ) r; : and ^ ( s ) action is accompanied with historical bidding prices and bid-ding stages (win or loss) and several other features, including number of cargo pieces, cargo weight, cargo volume, lead time and customer size, etc. Based on domain knowledge and the initial study, we set R; C = 3. Similarly to the generate cluster members, and use the 5-fold cross valida-tion to choose . However, we tested with other settings and found that our algorithm is insensitive to these starting values. To make a fair comparison, hglm is also given the same number of row/column clusters R=C .

In the following sub sections, we describe in detail the evaluation of COCOA in terms of the co-clustering results, predictive likelihood, improvement of revenue, and conver-gence rate.
We rst introduce an advanced Hierarchical clustering and prediction methodology framework currently adopted by a worldwide cargo company. Notice that this problem has a natural hierarchical structure, e.g., different transactions are grouped to different OD pairs. The rst step of the frame-work is to cluster OD pairs based on the win rate effects directly coming from the OD pairs and use the following Hi-erarchical Logistic Regression Model [27] to estimate such effect. where y ijk is the k th cargo-price bidding stage (win or loss) for the i th origination and the j th destination; X ijk be the corresponding xed effects which include bidding spe-ci c variables and customer market information. Let Z ij stand for the random effects coming from the OD (origina-tion and destination) pair ( i; j ). ij is the coefficient for the xed effects and u ij is OD pair ( i; j ) effect estimation. In a hierarchical model, observations are grouped into clus-ters (e.g., origination-destination in this cargo-price bidding problem), and the distribution of an observation is deter-mined not only by the common structure among all clusters but also by the speci c structure of the cluster where this observation belongs to. So the random effect component, different for different clusters, is introduced into the model.
In the second step, the cargo company would co-cluster the OD pairs based on the homogeneous effects for the win rates that are estimated through Model (8) or u ij . Each cell of the matrix is the random effect estimation of a speci c OD pair. The basic idea of co-clustering consists in making permutations of objects and variables in order to draw a correspondence structure (e.g., pattern recognition) for the most similar effects. The density for each block is given by [13]: where u ij is OD pair effect for ORIG= i and DEST= j and = ( kl ; 2 kl ) is the cluster-speci c mean and variance.
However, this two step method may introduce extra vari-ances from estimating the rst framework and using the out-puts as inputs for the second modeling. Also, there is no adaptive feedback process to improve the performance for both models.

This two step method has been very successful in help-ing the cargo company to develop an automatic optimized pricing machinery to increase revenue. However, this two step method does not bridge the information sharing and connection among the two modeling.
Figure 4 shows the win probability prediction before and after co-clustering by COCOA algorithm. Red color stands for higher win probability given all other conditions the same, and the solid lines demonstrate the clustering bound-aries. After co-clustering, the OD pairs are rearranged and OD pairs in similar colors are grouped together. The three row clusters generated from the 20 originations show clear geographical pattern. To be speci c, airports, such as AMS, BUD, CGN and DUS from European cities are in the same row cluster; airports, such as ATL, IAH, JFK and ORD from US cities, and airports, such as BOM, NRT and PVG from Asian and paci c area belong to the other two row clusters, respectively. In addition, the three row clusters present strong difference in average bidding price. For ex-ample, the average bidding price corresponding to airports ATL, AMS and BOM are about 4 e 4, 2 e 4 and 5 : 5 e 4 unit, respectively. These three airports are examples from each of the three row clusters, respectively. The price distinction can be explained by the difference in three critical features, i.e., lead time, customer size, and cargo volume. As shown in Figure 2, the cargo volume of AMS is about half of that of ATL and BOM. The customer size of BOM is relatively large compared to ATL and AMS. This matches our intu-ition that large customer size and cargo volume lead to high bidding price and vice versa. The three column clusters present similar patterns.
Figure 5 presents the comparison between COCOA and hglm based on the real data set in terms of total log likeli-hood of price and win probability. In Figure 5, the x -axis is the fraction of transactions per OD pair used for train-ing, and y -axis is the total log likelihood of price and win probability prediction normalized by test sample size. For each training sample fraction, we repeat the experiment for 20 times and report the mean and standard deviation of the normalized log likelihood as an error bar plot. Figure 5 shows that COCOA obtains not only larger mean log likeli-hood value but also smaller variation of log likelihood value than hglm over all training sample fractions. COCOA out-performs hglm because it leverages the block structure in the prediction stages of both price and win probability.
In reality, the objective function can be extended to in-clude a weight ( ) on the price prediction and win proba-bility prediction to re ect one's preference, i.e. l ( ) (1 ) l ( ) (2) . The impact of on the price optimization is presented in Figure 6, where the x -axis is a range of values and the y -axis is the total log likelihood of price and win probability on the test data. The log likelihood value obtains the maximum at = 0 : 2 which corresponds to a relatively high weight on the win probability. On the other hand, the variation of the log likelihood increases signi -cantly at very small or large values, such as 0.1 and 0.9. This is because the prediction of undominated objective re-sults in large variation. These ndings can help practition-ers select appropriate values. The total expected revenue which is given by the summation of predictive price multiple by predictive win probability of all the transaction on the test data is compared with actual revenue. As shown in Fig-ure 7, COCOA can improve revenue signi cantly, although the variation of revenue prediction increases given relatively large values. This again demonstrates the superer perfor-mance of COCOA .
We evaluate the time complexity of COCOA based on the inner loop. We set n 2 as a sufficiently large number and assume the inner loop converges if the co-clustering mem-bership does not change. At the rst iteration of the outer loop, we report the co-clustering objective function value of the inner loop at each iteration. As shown in Figure 8, the inner loop converges quicky, which is typically less than 8 iterations.
In this paper, we studied the problem of cargo pricing op-timization, and proposed a probabilistic framework to max-imize the conditional probability of observing two types of responses from the two stages (the bidding stage and the decision stage) given the features for each OD pair and the mappings for co-clustering. Compared with existing work, the main advantage of the proposed framework is three-fold. First of all, it allows information sharing among all the OD pairs, which signi cantly boosts the performance on the OD pairs with a small transaction volume. Second, it bridges the two stages by jointly learning the dual predictive mod-els. Finally, it leverages the intrinsic co-clusters of origina-tions and destinations to improve the model performance. Furthermore, we instantiated the framework with both an auxiliary distribution designed based on the co-clustering as-sumption, and generalized linear models for the two types of responses. We also proposed an iterative algorithm named COCOA for solving the resulting optimization problem in an effective and efficient manner. The performance of COCOA is demonstrated on both synthetic and real data sets. [1] A. Banerjee, I. S. Dhillon, J. Ghosh, S. Merugu, and [2] D. Chakrabarti. Autopart: Parameter-free graph [3] D. Chakrabarti, S. Papadimitriou, D. S. Modha, and [4] V. CHen, D. Guenther, and E. Johnson. Routing [5] W. Chiang, C. Chen, and X. Xu. An overview of [6 ] H. Cho, I. S. Dhillon, Y. Guan, and S. Sra. Minimum [7] D. Coppersmith and S. Winograd. Matrix [8] I. S. Dhillon, S. Mallela, and D. S. Modha.
 [9] I. S. Dhillon, S. Mallela, and D. S. Modha.
 [10] A. Elisseeff and J. Weston. A kernel method for [11] B. Gao, T.-Y. Liu, X. Zheng, Q. Cheng, and W.-Y. [12] W. Gao and Z.-H. Zhou. On the consistency of [13] G. Govaert and M. Nadif. Clustering with block [14] J. He, H. Tong, S. Papadimitriou, T. Eliassi-Rad, [15] S.-J. Huang, Y. Yu, and Z.-H. Zhou. Multi-label [16] S.-J. Huang and Z.-H. Zhou. Multi-label learning by [17] A. K. Jain and R. C. Dubes. Algorithms for Clustering [18] S. Ji, L. Tang, S. Yu, and J. Ye. Extracting shared [19] S. Ji, W. Zhang, and J. Liu. A sparsity-inducing [20] X. Kong, M. K. Ng, and Z.-H. Zhou. Transductive [21] T. Li. A general model for clustering binary data. In [22] B. Long, Z. M. Zhang, X. Wu, and P. S. Yu. Spectral [23] B. Long, Z. M. Zhang, and P. S. Yu. A probabilistic [24] S. C. Madeira and A. L. Oliveira. Biclustering [25] T. P. Minka. A comparison of numerical optimizers for [26] B. Rao. A convex programming model for cargo [27] L. Ronnegard, X. Shen, and M. Alam. hglm: A [28] R. Simpson. Using network ow techniques to nd [29] A. P. Singh and G. J. Gordon. Relational learning via [30] B. Smith and C. Penn. Analysis of alternative [31] L. Sun, S. Ji, and J. Ye. Hypergraph spectral learning [32] Y. Sun, J. Tang, J. Han, C. Chen, and M. Gupta. [33] K. Talluri and G. Van Ryzin. The Theory and [34] G. Tsoumakas and I. Katakis. Multi-label [35] E. Williamson. Airline network seat control . PhD [36] H.-F. Yu, P. Jain, P. Kar, and I. S. Dhillon. [37] M. Zhang and Z. Zhou. A review on multi-label [38] M.-L. Zhang. Lift: Multi-label learning with [39] M.-L. Zhang and K. Zhang. Multi-label learning by [40] M.-L. Zhang and Z.-H. Zhou. Ml-knn: A lazy learning [41] Y. Zhu and J. He. Co-clustering structural temporal manufacturing. In ICDM , 2014.
