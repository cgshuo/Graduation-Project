 The human visual system provides an arena in which objects co mpete for our visual attention, and a given object may win the competition with support from a num ber of influences. For an example, an moving object in our visual periphery may capture our atte ntion because of its salience , or the degree to which it is unusual or surprising given the overall visual scene [1]. On the other hand, a piece of fruit in a tree may capture our attention because of its relevance to our current foraging task, in which we expect rewarding items to be found in certai n locations relative to tree trunks, and to have particular visual features such as a reddish color [2 , 3]. Computational models of each of these influences have been developed and have individually b een extensively characterized in terms of their ability to predict an overt measure of attention, na mely gaze position [4, 5, 6, 3, 7, 8, 9]. Yet how do the real biological factors modeled by such system s interact in real-world settings [10]? Often salience and relevance are competing factors, and som etimes one factor is so strong that it Figure 1: Our computational framework for generating detec tor templates which can be used to detect key events in video sequences. A human game player int eracts with a video game, generating a sequence of video frames from the game, and a sequence of eye position samples from the game player. The video frames feed into computational models for predicting bottom-up (BU) salience and top-down (TD) relevance influences on attention. These p redictions are then compared with the observed eye position using a  X  X ormalized scanpath salienc y X  (NSS) metric. Finally, the video game sequence is annotated with key event times, and these are use d to generate event-locked templates from each of the game-related signals. These templates are u sed to try to detect the events in the original game sequences, and the results are quantified with metrics from signal detection theory. overrides our best e ff orts to ignore it, as in the case of oculomotor capture [11]. H ow does the visual system decide which factor dominates, and how does this vary as a function of the current task? We propose that one element of learning sophisticated visua l or visuomotor tasks may be learning which attentional influences are important for each phase of the task. A key question is how to build models that can capture the e ff ects of rapidly changing task demands on behavior.
 Here we address that question in the context of playing chall enging video games, by comparing eye movements recorded during game play with the predictions of a combined salience / relevance com-putational model. Figure 1 illustrates the overall framewo rk. The important factor in our approach is that we identify key game events (such as destroying an ene my plane, or crashing the car during driving race) which can be used as proxy indicators of likely transitions in the observer X  X  task set. Then we align subsequent analysis on these event times, such that we can detect repeatable changes in model predictive strength within temporal windows aroun d the key events. Indeed, we find signif-icant changes in the predictive strength of both salience an d relevance models within these windows, including more than 8-fold increases in predictive strengt h as well as complete shifts from predictive to anti-predictive behavior. Finally we show that the predi ctive strength signatures formed in these windows can be used to detect the occurrence of the events the mselves. Five subjects (four male, one female) participated under a p rotocol approved by the Institutional Review Board of the University of Southern California. Subj ects played two challenging games on a Nintendo GameCube: Need For Speed Underground (a car racin g game) and Top Gun (a flight combat game). All of the subjects had at least some prior expe rience with playing video games in general, but none of the subjects had prior experience wit h the particular games involved in our experiment. For each game, subjects first practiced the g ame for several one-hour sessions on di ff erent days until reaching a success criterion (definition fo llows), and then returned for a one-hour eye tracking session with that game. Within each game, subje cts learned to play three game levels, and during eye tracking, each subject played each game level twice. Thus, in total, our recorded data set consists of video frames and eye tracking data from 60 cli ps (5 subjects  X  2 games per subject  X  3 levels per game  X  2 clips per level) covering 4.7 hours.
 Need For Speed: Underground (NFSU). In this game, players control a car in a race against three other computer-controlled racers in a three-lap race , with a di ff erent race course for each game level. The game display consists of a first-person view, as if the player were looking out the windshield from the driver X  X  seat of the vehicle, with sever al  X  X eads-up display X  elements showing current elapsed time, race position, and vehicle speed, as w ell as a race course map (see Figure 2 for sample game frames). The game controller joystick is use d simply to steer the vehicle, and a pair of controller buttons are used to apply acceleration or brakes. Our  X  X uccess X  criterion for NFSU was finishing the race in third place or better out of the four r acers. The main challenge for players was learning to be able to control the vehicle at a high rate of simulated speed (100 + miles per hour) while avoiding crashes with slow-moving non-race tra ffi c and also avoiding the attempts of competing racers to knock the player X  X  vehicle o ff course. During eye tracking, the average length of an NFSU level was 4.11 minutes, with a range of 3.14 X 4.89 minu tes across the 30 NFSU recordings. Top Gun (TG). In this game, players control a simulated fighter plane with a success criterion of destroying 12 specific enemy targets in 10 minutes or less. Th e game controller provides a simple set of flight controls: the joystick controls pitch (forward X  X a ckward axis) and combined yaw / roll (left X  right axis), a pair of buttons controls thrust level up and do wn, and another button triggers missile firings toward enemy targets. Two onscreen displays aid the p layers in finding enemy targets: one is a radar map with enemy locations indicated by red triangles, and another is a direction finder running along the bottom screen showing the player X  X  current compas s heading along with the headings to each enemy target. Players X  challenges during training inv olved first becoming familiar with the flight controls, and then learning a workable strategy for us ing the radar and direction finder to e ffi ciently navigate the combat arena. During eye tracking, the average length of a TG level was 5.29 minutes, with a range of 2.96 X 8.78 minutes across the 30 TG recordings.
 Eye tracking . Stimuli were presented on a 22 X  computer monitor at a resolu tion of 640  X  480 pixels and refresh rate of 75 Hz. Subjects were seated at a viewing di stance of 80 cm and used a chin-rest to stabilize their head position during eye tracking. Video ga me frames were captured at 29.97Hz from the GameCube using a Linux computer under SCHED_FIFO scheduling, which then displayed the captured frames onscreen for the player X  X  viewing and while simultaneously streaming the frames to disk for subsequent processing. Finally, subjects X  eye p osition was recorded at 240Hz with a hardware-based eye-tracking system (ISCAN, Inc.). In tota l, we obtained roughly 500,000 video game frames and 4,000,000 eye position samples during 4.7 ho urs of recording. We developed a computational model which uses existing buil ding blocks for bottom-up and top-down components of attention to generate new eye position pr ediction maps for each of the recorded video game frames. Then, for each frame, we quantified the deg ree of correspondence between those maps and the actual eye position recorded from the game player. Although the individual models form the underlying computational foundation of our current study, our focus is not on testing their individual validity for predicting eye movem ents (which has already been established by prior studies), but rather on using them as components of a ne w model for investigating relationships between task structure and the relative strength of competi ng influences on visual attention; therefore we provide only a coarse summary of the workings of the models here and refer the reader to original sources for full details.
 Salience . Bottom-up salience maps were generated using a model based on detecting outliers in space and spatial frequency according to low-level feature s intensity, color, orientation, flicker and motion [4]. This model has been previously reported to be sig nificantly predictive of eye positions across a range of stimuli and tasks [5, 6, 7, 8].
 Relevance . Top-down task-relevance maps were generated using a model [9] which is trained to associate low-level  X  X ist X  signatures with relevant eye po sitions (see also [3]). We trained the task-relevance model with a leave-one-out approach: for each of t he 60 game clips, the task-relevance model used for testing against that clip was trained on the vi deo frames and eye position samples from the remaining 59 clips.
 Model / human agreement . For each video game frame, we used the normalized scanpath s aliency (NSS) metric [6] to quantify the agreement between the corre sponding human eye position and the Figure 3: Signal detection results of using event-locked si gnatures to detect visual events in video game frame sequences. See Section 5 for details. model maps derived from that frame. Computing the NSS simply involves normalizing the model prediction map to have a mean of zero and a variance of one, and then finding the value in that normalized map at the location of the human eye position. An N SS value of 0 would represent a model at chance in predicting human eye position, while an NS S value of 1 would represent a model for which human eye positions fell at locations with salienc e (or relevance) one standard deviation above average. Previous studies have typically used the NSS as a summary statistic to describe the predictive strength of a model across an entire sequence of fi xations [6, 12]; here, we use it instead as a continuous measure of the instantaneous predictive str ength of the models. We annotated the video game clips with several pieces of addi tional information that we could use to identify interesting events (see Figure 2) which would serv e as the basis for event-locked analyses. These events were selected on the basis of representing tran sitions between di ff erent task phases. We hypothesized that such events should correlate with chan ges in the relative strengths of di ff erent influences on visual attention, and that we should be able to d etect such changes using the previously described models as diagnostic tools. Therefore, after ann otating the video clips with the times of each event of interest, we subsequently aligned further ana lyses on a temporal window of -5s /+ 5s around each event (shaded background regions in Figure 2, ro ws b X  X ). From those windows we extract the time courses of NSS scores from the salience and r elevance models and then compute the average time course across all of the windows, giving an even t-locked template showing the NSS signature of that event type (Figure 2e).
 TG  X  X issile fired X  events. In the TG game we looked for times when the player fired missile s (Figure 2, column 1). We selected these events because they r epresent transitions into a unique task phase, namely the phase of direct engagement with an enemy pl ane. During most of the TG game playing time, the player X  X  primary task involves actively u sing the radar and direction finder to locate enemy targets; however, during the time when a missile is in fl ight the player X  X  only task is to await visual confirmation of the missile destroying its target. Fi gure 2, column 1, row a illustrates one of the  X  X issile fired X  events with captured video frames at -150 0ms, 0ms, and + 1500ms relative to the event time. Row b uses one of the 30 TG clips to show how the even t times represent transitions in a continuous signal (number of missiles fired); a -5s /+ 5s window around each event is highlighted by the shaded background regions. These windows then propagat e through our model-based analysis, where we compare the eye position traces (row c) with the maps predicted by the BU salience (row f) and TD relevance (row g) to generate a continuous sequence of NSS values for each model (row d). Finally, all of the 658 event windows are pooled and we comput e the average NSS value along with a 98% confidence interval at each time point in the window , giving event-locked template NSS signatures for the  X  X issile fired X  event type (row e). Those s ignatures show a strong divergence in the predictiveness of the BU and TD models: outside the event window, both models are significantly but weakly predictive of observers X  eye positions, with NSS values around 0.3, while inside the event window the BU NSS score increases to an NSS value around 1.0, w hile the TD NSS score drops below zero for several seconds. We believe this reflects the t ask phase transition. In general, the TD model has learned that the radar screen and direction finde r (toward the bottom left of the game screens) are usually the relevant locations, as illustrate d by the elevated activity at those locations in the sample TD maps in row g. Most of the time, that is indeed a good prediction of eye position, reflected by the fact that the TD NSS scores are typically high er than the BU NSS scores outside the event window. However, within the event window, players shi ft their attention away from the target search task to instead follow the salient objects on the disp lay (enemy target, the missile in flight), which is reflected in the transient upswing in BU NSS scores.
 TG  X  X arget destroyed X  events. In the TG game we also considered times when enemy targets wer e destroyed (Figure 2, column 2). Like the  X  X issile fired X  even ts, these represent transitions between task phases, but whereas the  X  X issile fired X  represented tra nsitions from the enemy target search phase into a direct engagement phase, the  X  X arget destroyed  X  events represent the reverse transition; once the player sees that the enemy target has been destroyed , he or she can quickly begin searching the radar and direction finder for the next enemy target to eng age. This is reflected in the sample frames shown in Figure 2, column 2, row a, where leading up to t he event (at -600ms and 0ms) the player is watching the enemy target, but by + 600ms after the event the player has switched back to looking at the direction finder to find a new target. The analys is proceeds as before, using -5s /+ 5s windows around each of the 328 events to generate average eve nt-locked NSS signatures for the two models (row e). These signatures represent the end of the dir ect engagement phase whose beginning was represented by the  X  X issile fired X  events; here, the BU NS S score reaches an even higher peak of around 1.75 within 50ms after the target being destroyed, and then quickly drops to almost zero by 600ms after the event. Conversely, the TD NSS score is belo w zero leading up to the event, but then quickly rebounds after the event and transiently goes a bove its baseline level. Again, we believe these characteristic NSS traces reflect the observer X  X  task transitions.
 NFSU  X  X tart speed-up X  events . In the NFSU game, we considered times at which the player jus t begins recovering from a crash (Figure 2, column 3); players  X  task is generally to drive as fast as possible while avoiding obstacles, but when players inevit ably crash they must transiently shift to a task of trying to recover from the crash. The general drivin g task typically involves inspecting the horizon line and and focus of expansion for oncoming obst acles, while the crash-recovery task typically involves examining the foreground scene to deter mine how to get back on course. To automatically identify crash recovery phases, we extracte d the speedometer value from each video game frame to form a continuous speedometer history (Figure 2, column 3, row b); we identified  X  X tart speed-up X  events as upward-turning zero crossings i n the acceleration, represented again by shaded background bars in the figure. Again we computed avera ge event-locked NSS signatures for the BU and TD models from -5s /+ 5s windows around each of the 522 events, giving the traces in row e. These traces reveal a significant drop in TD NSS scores d uring the event window, but no significant change in BU NSS scores. The drop in TD NSS scores l ikely reflects the players X  shift of attention away from the usual relevant locations (horizon l ine, focus of expansion) and toward other regions relevant to the crash-recovery task. However, the l ack of change in BU NSS scores indicates that the locations attended during crash recovery where nei ther more nor less salient than locations attended in general; together, these results suggest that d uring crash recovery players X  attention is more strongly driven by some influence that is not captured we ll by either of the current BU and TD models. Having seen that critical game events are linked with highly significant signatures in the time course of BU and TD model predictiveness, we next asked whether thes e signatures could be used in turn to predict the events themselves. To test this question, we b uilt event-locked  X  X etector X  templates from three sources (see Figure 1): (1) the raw BU and TD predic tion maps (which carry explicit information only from the visual input image); (2) the raw ey e position traces (which carry explicit information only from the player X  X  behavior); and (3) the BU and TD NSS scores, which represent a fusion of information from the image (BU and TD maps) and fro m the observer (eye position). For each of these detector types and for each event type, we co mpute event-locked signatures just as described in the previous section. For the BU and TD NSS score s, this is exactly what is represented in Figure 2, row e, and for the other two detector types the ana lysis is analogous. For the BU and TD prediction maps, we compute the event-locked average BU a nd TD prediction map at each time point within the event window, and for the eye position trace s we compute the event-locked average x and y eye position coordinate at each time point. Thus we hav e signatures for how each of these detector signals is expected to look during the critical eve nt intervals.
 Next, we go back to the original detector traces (that is, the raw eye position traces as in Figure 2 row c, or the raw BU and TD maps as in rows f and g, or the raw BU and TD NSS scores as in row d). At each point in those original traces, we compute the correlation coe ffi cient between a temporal window in the trace and the corresponding event-lo cked detector signature. To combine each pair of correlation coe ffi cients (from BU and TD maps, or from BU and TD NSS, or from x and y eye position) into a single match strength, we scale the individual correlation coe ffi cients to a range of [0...1] and then multiply, to produce a soft logical  X  X nd X  operation, where both components must have high values in order to produce a high output: where hi event represents the event-locked template for that signal, and r ( , ) represents the correlation coe ffi cient between the two sequences of values, rescaled from the natural [-1...1] range to a [0...1] range. This yields continuous traces of match strength betw een the event detector templates and the current signal values, for each video game frame in the data s et.
 Finally, we adopt a signal detection approach. For each even t type, we label every video frame as  X  X uring event X  if it falls within a -500ms /+ 500ms window around the event instant, and label it as  X  X uring non-event X  otherwise. Then we ask how well the match strengths can predict the label, for each of the three detector types (BU and TD maps alone, eye position alone, or BU and TD NSS). Figure 3 shows the results using several signal detect ion metrics. Each row represents one of the three event types ( X  X issile fired, X   X  X arget destroyed , X  and  X  X tart speed-up X ). The first column (panels a, d, and g) shows the histograms of the match strengt h values during events and during non-events, for each of the three detector types; this gives a qua litative sense for how well each detector can distinguish events from non-events. The strongest sepa ration between events and non-events is clearly obtained by the BU&amp;TD NSS and eye position detectors for the  X  X issile fired X  and  X  X arget destroyed X  events. Panels b, e, and h show ROC curves for each detector type and event type, along with values for area-under-the-curve (AUC) and d-prime (d X  ); panels c, f, and i show precision / recall curves with values with for the maximum F 1 measure along the curve ( F 1 = (2 p r ) / ( p + r ), where p and r represent precision and recall). Each metric reflects the sa me qualitative trends. The highest scores overall occur for  X  X arget destroyed X  events, follow ed by  X  X issile fired X  and  X  X tart speed-up X  events. Within each event type, the highest scores are ob tained by the BU&amp;TD NSS detector (representing fused image / behavioral information), followed by the eye position dete ctor (behavioral information only) and then the BU&amp;TD maps detector (image in formation only). Our contributions here are twofold: First, we reported seve ral instances in which the degree of corre-spondence between computational models of attention and hu man eye position varies systematically as a function of the current task phase. This finding suggests a direct means for integrating low-level computational models of visual attention with higher-leve l models of general cognition and task performance: the current task state could be linked through a weight matrix to determine the degree to which competing low-level signals may influence overall s ystem behavior.
 Second, we reported that variations in the predictive stren gth of the salience and relevance models are systematic enough that the signals can be used to form tem plate-based detectors of the key game events. Here, the detection is based on signals that represe nt a fusion of image-derived information (salience / relevance maps) with observer-derived behavior (eye posit ion), and we found that such a combined signal is more powerful than a signal based on imag e-derived or observer-derived in-formation alone. For event-detection or object-detection applications, this approach may have the advantage of being more generally applicable than a pure com puter vision approach (which might require development of algorithms specifically tailored to the object or event of interest), by virtue of its reliance on human / model information fusion. Conversely, the approach of deri ving human behavioral information only from eye movements has the adva ntage of being less invasive and cum-bersome than other neurally-based event-detection approa ches using EEG or fMRI [13]. Further, although an eye tracker X  X  x / y traces amounts to less raw information than EEG X  X  dozens of leads or fMRI X  X  10,000s of voxels, the eye-tracking signals also c ontain a denser and less redundant rep-resentation of cognitive information, as they are a manifes tation of whole-brain output. Together, these advantages could make our proposed method a useful app roach in a number of applications. References
