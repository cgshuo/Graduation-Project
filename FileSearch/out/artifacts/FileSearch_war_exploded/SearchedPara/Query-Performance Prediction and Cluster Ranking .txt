 We show that two tasks which were independently addressed in the information retrieval literature actually amount to the exact same task. The first is query-performance prediction; i.e., estimating the effectiveness of a search performed in re-sponse to a query in the absence of relevance judgments. The second task is cluster ranking, that is, ranking clus-ters of similar documents by their presumed effectiveness (i.e., relevance) with respect to the query. Furthermore, we show that several state-of-the-art methods that were inde-pendently devised for each of the two tasks are based on the same principles. Finally, we empirically demonstrate that using insights gained in work on query-performance predic-tion can help, in many cases, to improve the performance of a previously proposed cluster ranking method.

The observation that the effectiveness of retrieval meth-ods can substantially vary from one query to another gave rise to a large body of work on query-performance predic-tion ( QPP ) [3]. The task is predicting the effectiveness of a retrieval performed in response to a query when relevance judgments are not available. Post-retrieval prediction meth-ods, which are our focus here, analyze the result list of the documents most highly ranked.

Another task that also attracted much research attention is cluster ranking ( CR ) [20, 21, 16, 13, 14, 23, 9]; specifi-cally, ranking clusters of similar documents based on their presumed relevance to the information need expressed by the query. Following the cluster hypothesis [32], several re-searchers showed that if the result list of top-retrieved docu-ments is clustered, then among these clusters there are some that contain a very high percentage of relevant documents [12, 30, 14]. Furthermore, positioning the constituent docu-ments of these clusters at the top ranks of the final result list was shown to yield retrieval performance that substantially transcends that of document-based retrieval [12, 30, 14].
The QPP and CR tasks mentioned above might seem at first glance to be quite different. Indeed, all previous reports of methods addressing these tasks have made no connections between the two. However, we show that the two tasks ac-tually amount to the exact same task. That is, estimating the relevance of a document set to the information need ex-presses by a query.

Our second contribution is showing that some (state-of-the-art) methods that were independently devised for QPP and CR rely on the exact same principles; these principles directly touch on the underlying common grounds of the two tasks. Our third contribution is an operational one that emerges following the realization that the QPP and CR tasks are the same. We empirically show that a previously proposed cluster ranking method [21] can be improved in quite a few cases if the induced ranking is, in fact, reversed. The insight for this ranking reve rsal rises from considering a query-performance prediction method [27] that relies on the same core principle but which uses it in an opposite manner.
Post-retrieval query performance predictors can be roughly categorized into those that [3] (i) measure the clarity of the result list with respect to the corpus [5, 1, 4, 11], (ii) measure different notions of the robustness of the result list [34, 33, 36, 2, 37], and (iii) analyze retrieval scores in the result list [31, 8, 37, 27, 6, 7]. We show that several state-of-the-art predictors, specifically, some that are based on estimating result list robustness [37] and on analysis of retrieval scores [31, 37, 27, 24, 7] use the exact same principles utilized in some cluster ranking methods [21, 13, 14, 23].

The cluster hypothesis was used in some work to devise query-performance predictors [33, 8]. For example, a result list was assumed to be effective to the extent that similar documents are assigned with similar retrieval scores [8]. The coherence of the result list, as manifested in its clustering tendency, was also used for prediction [33]. Yet, connections to the cluster ranking task and methods devised for this task were not established in contrast to the work we present here.
Query performance predictors were employed upon clus-ters of similar documents for estimating the quality of rele-vance models [18] constructed from these clusters [28]. How-ever, the connections between query performance prediction (QPP) and cluster ranking (CR) at the task level and at the method level, which we address here, were not discussed.
Let D [ q ; k ] res denote the result list of the k documents that are the highest ranked by a retrieval method employed in response to query q upon corpus D . The query performance prediction (QPP) task [3] is estimating the effectiveness of D res in lack of relevance judgments. We use P QP P ( q ; D to denote the effectiveness estimate assigned by a prediction
Let Cl ( D [ q ; k ] res ) denote the set of clusters of similar docu-ments that are created from the result list D [ q ; k ] res clustering algorithm. The cluster ranking (CR) task [20, 21, 16, 13, 14, 23, 9] is estimating the effectiveness (i.e., relevance) of each cluster c (  X  Cl ( D [ q ; k ] res )) with respect to the information need expressed by q ; P CR ( q ; c ) denotes the estimate. The estimate is used for ranking the clusters in Cl ( D [ q ; k ] res ). The cluster ranking can then be transformed into document ranking using various approaches [15, 20].
Thus, we get that while P QP P ( q ; D [ q ; k ] res )isanestimate for the effectiveness of the entire result list, P CR ( q ; c )isan estimate for the effectiveness of a subset of documents, c , in D [ q ; k ] res . Hence, both the QPP and the CR tasks actu-ally amount to the same task. Formally, let S be a set of documents. The task is estimating, in the absence of rele-vance judgments, the probability p ( S | q )that S contains in-formation pertaining to the information need expressed by q . Indeed, P QP P and P CR are two such estimation methods. In Section 3.1 we show that several state-of-the-art query-performance prediction methods are based on the exact same principles that underlie some cluster ranking methods. The estimates {P CR ( q ; c i ) } assigned to the set of clusters c (  X  Cl ( D [ q ; k ] res )), which were created for the same query q ,are used for ranking the clusters relatively to each other. On the other hand, P QP P is used to assign a prediction value to a single result list retrieved for a query. This difference means, for example, that the approach of using inter-cluster similarities for devising P CR [13] is not applicable in the QPP case, since using inter-list similarities cannot convey much information for lists retrieved for different queries.
The prediction quality of P QP P is measured by the cor-relation between the estimates it assigns to queries and the ground truth effectiveness for the queries [3]. Thus, the esti-mates are sometimes normalized for inter-query compatibil-ity [37, 27], unless they are transformed to direct measures of retrieval effectiveness [10]. This normalization is not needed when using P CR to rank clusters for the same query.
In what follows we discuss the analogies between princi-ples used for devising several QPP and CR methods. These shared principles rise, not surprisingly, due to the equiva-lence of the tasks that was discussed above.
The state-of-the-art query feedback (QF) QPP method [37] is based on the following principle. A query model is induced from the result list D [ q ; k ] res andusedforretrievalover the corpus. The number of highest ranked documents that are also among the highest ranked in D [ q ; k ] res is the prediction value. The hypothesis is that for a result list D [ q ; k not manifest much  X  X uery drift X  and hence is effective, the ranking induced over the corpus using a model of D [ q ; k not drift much from the ranking used to create D [ q ; k ]
As it turns out, the same approach employed by QF is also implemented by an effective cluster ranking method ( X  X elf faithfulness X ) [14]. Specifically, a query model is induced from cluster c and used for retrieval over the corpus. The higher c  X  X  constituent documents are ranked by this retrieval, the less query drift c is presumed to manifest; accordingly, the higher is the estimate for the effectiveness of c .
We next discuss three QPP methods that are based on analysis of retrieval scores. These methods, as it turns out, were also employed by several CR approaches.
 Maximal retrieval score. The maximal retrieval score in D res was shown to be an effective QPP estimate [31]. Clus-ters were also ranked, in some work [19, 26, 23], based on the highest retrieval score of their constituent documents. method [37] measures the difference between the average re-trieval score in D [ q ; k ] res and the retrieval score assigned to the corpus. The basic premise is that high retrieval scores in D res attest to its effectiveness. The use of the corpus re-trieval score is to ensure inter-query compatibility of predic-tion values as the retrieval scores themselves are not compat-ible across queries for various retrieval methods [37]. Indeed, some recent work [29] shows that WIG is highly effective if retrieval scores are normaliz ed to begin with and the corpus retrieval score is not used; in this case, WIG amounts to using only the average retrieval score as the prediction value [29]. This prediction principle was also employed for the CR task. Clusters were ranked based on the mean retrieval score of their constituent documents [23, 13, 25].
Using the average retrieval score of a set of documents either for QPP or for CR, as discussed above, results in the following interesting interpret ation when retrieval scores are induced using the language modeling framework. Specifi-cally, the retrieval score assigned to document d in response ity that q can be generated from a language model induced from d . Then, the average retrieval score in D [ q ; k ] is equivalent to the retrieval score assigned to a geometric mean representation of D [ q ; k ] res as was recently shown [27]. Interestingly, work on CR in the language modeling frame-work [23, 25] demonstrated the empirical merits of ranking a cluster using the exact same approach just mentioned for estimating QPP for D [ q ; k ] res ; that is, by using the retrieval score assigned to the cluster X  X  geometric-mean-based repre-sentation. Arguments based on information geometry were used to advocate a geometric-mean-based representation for sets of documents (e.g., in the context of CR) [25]. QPP method [27], as well as other QPP approaches [24, 7], measure the standard deviation of retrieval scores in D [ The hypothesis is that high deviation corresponds to reduced query drift, and thereby, indicates improved retrieval effec-tiveness [27]. The (formal) support for the hypothesis was based on the fact that the mean retrieval score in D [ q ; several retrieval methods, the retrieval score of a centroid-based representation of D [ q ; k ] res that manifests query drift.
As it turns out, there is work on CR that ranks a clus-ter by the deviation of the retrieval scores of its constituent documents from the retrieval s core of the cluster [21]. The cluster X  X  retrieval score is based on the similarity between the query and a specific form of a centroid-based represen-tation of the cluster, namely, the big document that results from concatenating its constituent documents 1 . However, in contrast to the case for QPP, clusters were ranked in as-cending order of deviation [21], based on the premise that effective clusters are those for which the deviation is small.
Thus, while the exact same principle of using the vari-ance of retrieval scores of documents in a set was employed for both the QPP and the CR tasks, it was used in a re-versed manner. In Section 4 we show that ranking clusters in descending order of variance, a principle adapted from the work on QPP mentioned above [27, 24, 7], can outper-form in many cases the use of an ascending order, which was originally proposed for ranking clusters [21].
Following the discussion in Section 3.1.2, the goal of the exploration to follow is contrasting two opposite hypothe-ses with regard to the connection between the variance of retrieval scores in a cluster and the cluster X  X  (presumed) rel-evance to the query; that is, whether decreased variance as originally proposed [21], or increased variance as implied by work on QPP [27, 24, 7], attests to increased relevance.
The TREC corpora and queries used for experiments are specified in Table 1. Titles of TREC topics served for queries. We applied Krovetz stemming to queries and documents; stopwords (on the INQUERY list) were removed only from queries. The Lemur/Indri toolkit was used for experiments.
Let x and y be a query, a document, or a cluster of doc-uments. A cluster is represented by the concatenation of its constituent documents as is standard in work on cluster-based retrieval [20, 16, 22, 21, 13, 14]; the order of concate-nation has no effect since we use unigram language mod-els that assume term independence. Specifically, p [  X  ] x notes the Dirichlet-smoothed unigram language model in-duced from x with the smoothing parameter  X  .Weuse Sim ( x, y ) def =exp
Using these clusters X  retrieval scores, which reflect surface-level query similarities, for ranking clusters is known to yield relatively poor retrieval performance [16, 13, 14]. This find-ing potentially implies that this cluster representation also manifests query drift. language-model-based similarity between x and y [15];  X  = 1000 following previous recommendations [35].

We rank all documents d in the corpus in response to query q by Sim ( q, d ). This initial ranking, henceforth referred to as init. rank. , amounts to the standard KL retrieval ap-proach [17]. The k = 50 highest ranked documents, which form the result list D [ q ; k ] res , are then clustered using a simple nearest-neighbors-based clustering method [16, 13, 14, 23]. Specifically, for each d (  X  X  [ q ; k ] res ) we create a cluster that contains d and the 4 documents d (  X  X  [ q ; k ] res ; d = d )that yield the highest Sim ( d, d ). Using such small overlapping nearest-neighbors-based clusters was shown to be highly ef-fective for cluster-based retrieval [15, 16, 21, 13, 14, 23]. Let deviations of retrieval scores of documents in c from c  X  X  re-trieval score. (All clusters contain 5 documents as noted above.) In what follows we use DevAsc to refer to a rank-ing of the clusters in ascending order of the deviation, which corresponds to previous work on cluster ranking [23]; De-vDesc refers to a ranking in descending order of the devi-ation, which corresponds to the principle employed in QPP methods [27, 24, 7]. As reference comparisons we use Arith-Mean [13, 23] and GeomMean [23, 25] which refer to a ranking of the clusters based on the arithmetic mean, and geometric mean, respectively, of the retrieval scores ( Sim ( q, d )) of their constituent documents. To transform a ranking of clusters to a ranking of documents in D [ q ; i.e., to re-rank D [ q ; k ] res  X  we follow previous work [20, 21, 23] and order the clusters top to bottom and then replace them by their constituent documents omitting repeats; documents within a cluster are ranked by their retrieval scores. We report the MAP@50 (as the number of documents in D res is 50) and precision at 5 (p@5) performance numbers. Note that p@5 corresponds to the percentage of relevant doc-uments in the highest ranked cluster, as all clusters contain 5 documents. Statistically significant differences of perfor-mance are determined using the two-tailed paired t-test at the 95% confidence level. Table 2 presents the experimental results. Except for GOV2 2 ,itisalwaysthecasethatDevDescoutperforms DevAsc; in several cases, the performance differences are quite substantial and statistically significant. Hence, we see that the hypothesis postulated in work on QPP [27], about increased variance of retrieval scores within a set of docu-ments attesting to improved effectiveness, can translate to a more effective cluster ranking method than that originally proposed (i.e., that based on the opposite hypothesis) [21]. Yet, both DevDesc and DevAsc are substantially outper-formed by ArithMean and GeomMean and the initial rank-ing. Integrating methods based on the average and variance of retrieval scores is a future venue to explore.
We showed that the query-performance prediction (QPP) task and the cluster ranking (CR) task essentially amount to the exact same task. It is not surprising, therefore, that
Initial experiments with several settings for the ClueWeb collection resulted in findings similar to those observed for GOV2. although the two tasks were independently addressed in pre-vious work, several of the methods used to tackle them use the same principles. We discussed these shared principles for a variety of QPP and CR methods.

In addition, we empirically showed that using insights gained in some work on query-performance prediction can help to improve the retrieval performance of a cluster rank-ing method. For future work we intend to explore whether additional cluster ranking methods can be devised based on principles employed by query-performance prediction meth-ods and vice versa.
 Acknowledgments We thank the reviewers for their com-ments. This paper is based upon work supported in part by the Israel Science Founda tion under grant no. 557/09, by IBM X  X  Ph.D. fellowship and SUR award, by Google X  X  and Yahoo! X  X  faculty research awards, and by Miriam and Aaron Gutwirth Memorial Fellowship. Any opinions, findings and conclusions or recommendations expressed here are the au-thors X  and do not necessarily reflect those of the sponsors.
