 On-Line Transaction Processing (OLTP) Systems are widely used in a variety of web applications such as social networking and online shopping. As the num-ber of users grows exponentially in r ecent years, OLTP systems are required to deal with a large number of concurrent requests. Therefore, the concurrent transaction management has become a critical issue for these applications. Conventional OLTP systems, however, are suboptimal for this demand[1, 2]. In these systems, data is global and shared among all threads, as shown in Figure 1(a). User queries are directly dispatched to threads using simple rules like round robin, regardless of potential contention among executing queries. Locks and latches are adopted to ensure consistency. However, Pandis et. al. [3] figured out that the shared-everything transactio n execution models used in conventional OLTP systems limit the system X  X  concurre ncy. Since the transactions executed by work threads are arbitrary and not coordinated, these worker threads contend with each other for shared resources. When the number of work threads increases, the degree of contention between thre ads increases at the same time, which downgrades the system X  X  performance seriously.

In order to overcome the drawbacks of shared-everything models, Stonebraker et. al. proposed shared-nothing transaction execution models[4]. Different from shared-everything models, the data resources in the database are divided into partitions and each partition is allocated to one worker thread as shown in Figure 1(b). In this way, a partition can only be accessed by a single thread, and as a result, contentions among worker threads are significantly alleviated. Costly lock acquiring and releasing can be eliminated in shared-nothing models and overall performance is improved. Systems can also exploit extra parallelism by dividing a larger query into constituent smaller queries according to data partition and attached them to separate threads[5].

Although partitioning has many advantages, it also brings some problems. The performance of shared-nothing models is sensitive to data skew[6, 7]. A thread strictly corresponds to one partition in shared-nothing models, so in skewed workload where accesses concentrate on only a part of partitions, some threads are assigned excessive work while others are idle. This unbalanced assignment significantly deteriorates the overall performance and wastes the precious pro-cessor resource. Although so me repartitioning strategies are proposed, these ap-proaches incur heavy extra cost. Besides, many workloads can not be perfectly partitioned. It X  X  quite har d for each transaction to acces s only a single partition since the partition is relatively small. In this case, executing transactions need inter-partitions access, which requires c ostly distributed co nsensus protocols to ensure ACID property [8, 9].
 In this paper, we propose bCATE (Balanced Contention-Aware Transaction Execution), a novel concurrent transactio n execution model for highly concurrent OLTP systems. Our transaction execution model takes the advantages of both shared-everything and shared-nothing models by combining their merits. The architecture of bCATE is illustrated in Figure 1(c). bCATE divides the database into different conflict partitions and each conflict partition is assigned a master thread and a set of slave threads. Master threads are used to detect the potential contention among queries and slave thre ads are used to execute queries. Queries are firstly submitted to master threads to see if they conflict with any existing queries. If a query does not conflict with other queries, it then is delivered to a slave thread and executed by the slave thread.

Since all queries executed by slave threads don X  X  conflict with any other ex-ecuting queries, slave threads could ex ecute without any blocks and waits. As a result, the processing capability wasted due to the contention among trans-actions is reduced in bCATE. Besides, as each conflict partition in bCATE is shared among a set of slave threads, bCATE is more resistant to skewed load compared with shared-nothing models . When the load is skewed across conflict partitions, bCATE can bypass costly repartitioning and just need to re-distribute slave threads among conflict partitions to balance the load. The contributions of this paper are: 1. We present a novel transaction execution model to reduce the total amount of 2. We take efficient approaches to get rid of the problems brought by data 3. We implement our prototype in Shore-MT and evaluate the transaction exe-The rest of this paper is structured as follows. In Section 2, we survey related work. We give an overview our transaction execution model in Section 3 and evaluate our model in Section 4. In Section 5, we conclude the paper. Many works have been done on transactio n execution to improve the perfor-mance of OLTP systems. Shared-everything multi-threaded models are widely used to fully utilize CPU and I/O resources. As contention is the major perfor-mance bottleneck in shared-everything models, there is a significant amount of effort toward reducing the contention in OLTP systems. Several previous stud-ies focus on the contention in the data and instruction cache [10 X 12] while some other studies focus on the contention in the log manager. Johnson et al. proposed a novel log buffer [13] to improve log manager X  X  scalability. The contention in the lock manager is also studied. Johnson et al. [14] addressed that the lock manager is a main source of contention in OLTP systems and proposed a technique called SLI to bypass the bottleneck.

Partitioning is a primary way in scaling OLTP systems. A number of auto-matic partitioning schemes have been investigated in the past [15 X 17] and these schemes are orthogonal to our approach. Pandis et al. proposed a logical shared-nothing model called DORA [3] to eliminate locking in OLTP systems. Both DORA and bCATE use conflict detection to eliminate locking during execu-tion, but DORA can only support simple partitioning schemes and suffers more from data skew than bCATE. In their later work, a physiological partitioning approach is proposed to eliminate page latching [18] which can also be applied in bCATE. Despite of the benefits, partitioning also brings a lot of problems including data skew and distributed transactions. Some researchers proposed complex concurrency control schemes [19, 20] to reduce the overhead caused by distributed transactions while other researchers attempted to minimize the num-ber of distributed transactions by creating initial partitioning schemes [21, 9]. Repartitioning is a common way to alleviate data skew[22], but it incurs heavy extra cost due to data migration. By sharing a partition among a set of threads, bCATE is more resistant to skewness and could employ lightweight thread mi-gration to balance the workload. In the section, we will describe our transaction execution model in detail. To begin with, we give a brief view of our transaction execution model in Section 3.1. Detailed implementation in bCATE X  X  design is described in later sections and at last, we introduce the benefits of bCATE X  X  design in Section 3.4. 3.1 Transaction Execution The execution of a transaction in bCATE is illustrated in Figure 2, in which the numbers indicate the processing steps in our model.

The database is first partitioned into different conflict partitions according to the partitioning schemes defined by users. bCATE provides a convenient in-terface for user to define their partitioning schemes. User queries, which are expressed in relational calculus, are first rewritten in the form of database op-erators and translated into a sequence of small and local queries on conflict partitions according to the partitioning schemes (step 1 in Figure 2). Then these local queries are routed to corresponding conflict partitions (step 2). Techniques for query decomposition and localization which are well studied in distributed databases can be applied here[23].
Both master threads and slave threads act in an event-driven manner. When enqueued in a conflict partition, local q ueries are first delivered to the conflict partition X  X  master threa d. On receiving a local query, the master thread first detects the potential contention betw een this query and other executed queries to find if this query is executable (step 3). The detailed implementation of con-tention detection is introduced in sectio n 3.2. A query isn X  X  executable when it conflicts with any previous ly executed queries or currently executing queries. If a query isn X  X  executable, it will be pushed into a waiting list (step 4); otherwise, it will be delivered to a slave thread (step 5).

Executable local queries are dispatched to slave threads as evenly as possible so that the load in the conflict partition can be balanced. Since there is no con-tention among executable local queries, slave threads can execute local queries in parallel without any waits or blocks (step 6).

When a transaction is aborted or committed, all the transaction X  X  queries become committed and should be removed from all the corresponding partitions. The transaction sends messages to all the conflict partitions involving in its execution (step 7) when it terminate s. When the master thread receives the message, it will commit all the transaction X  X  queries in the conflict partition (step 8). After committing a executed query, some other queries conflicting with the query might become executable. The mas ter thread probes the waiting list to find all those pending queries that can be executable now and dispatches these executable queries to slave threads(step 9). 3.2 Contention Detection Contention is detected during query executi on in conventional shared-everything models. When a worker thread in shared-everything models executes a query, it first picks up all the tuples the query will access at first and uses the identifiers of these tuples to acquire locks. The locks will be held by this work thread if there are no queries co nflicting with the query.
This method doesn X  X  work in bCATE. Different from shared-everything mod-els, bCATE detects contention in master threads before queries are executed. Master threads have no knowledge of th e tuples a query will access. A simple approach is to lock the whole partition, but it will significantly reduce the con-currency since a query will block all the queries with incompatible modes in the same partition even they are accessing different tuples.

We solve the problem by using predicate locking protocol[24]. Different from locking in conventional databases, predicates, but not tuples, are used as the units of locking. We illustrate how conflict detection works using a simple ex-ample. Supposing there are two queries: Q1. SELECT * FROM table WHERE id = 3 Q2. DELETE * FROM table WHERE id &gt; 2 Q1 X  X  predicate is id = 3 and Q2 X  X  predicate is id &gt; 2. As the tuples satisfying Q1 X  X  predicate also satisfy Q2 X  X  predicate, i t X  X  implied that Q1 would access the tuples which may be accessed by Q2 simultaneously. Since Q1 X  X  lock mode (Share) and Q2 X  X  lock mode (Exclusive) are not compatible, it will lead to potential contention if Q1 and Q2 are executed con currently. Therefore, we can say that Q1 conflicts with Q2 and if Q2 is executed but not uncommitted in the system, Q1 would be put in the wait list and can X  X  be executed until Q2 commits. bCATE keeps a list of all the executed but uncommitted queries in each partition. Whenever a new query arrives, the master thread checks the query X  X  compatibility with all the executed queries in the list using predicate locking protocol. If the query doesn X  X  conflict with all the queries in the list, then the query is executable and will be dispatched to a slave thread. Since the number of uncommitted queries is limited in the partition, the overhead of conflict detection is relatively low compared to the overhead of query execution. 3.3 Load Balancing Though bCATE is more resistant to skew than shared-nothing models, bCATE still needs to balance the load in case that the performance is hurt by data skew. Shared-nothing systems usually balance the load by costly repartitioning the database according to the load distribution. bCATE X  X  design allows us to have a lightweight but efficient load balancing mechanism. bCATE monitors the partition X  X  load and calculates the ideal number of each partition X  X  slave threads. Let P be the total number of partitions, L i be the load of the i th partition and S i be the number of slave threads in i th partition. In an ideal situation, the number of each partition X  X  slave threads should be adaptive to the partition X  X  load. So the ideal number of slave threads for each partition by: When the number of the partition X  X  slave threads S i is less than I i  X , for a given upper bound , , we increase the number of this partition X  X  slave threads to the ideal number I i . When the number of the partition X  X  slave threads S i is greater than I i + -for a given lower bound -, we reduce the number of this partition X  X  slave threads to the ideal number I i .

As each partition is assigned with a reasonable number of slave threads, the load in the partition can be distributed among these threads, which alleviates the performance degradation due to data skew. 3.4 Benefits of bCATE Since data may be accessed by different t hreads simultaneously in shared-everything models, the worker threads must first probe the lock table and acquire locks before executing to ensure correct ness. The lock can X  X  be acquired at once if the query conflicts with any executing qu eries and the worker thread will have to wait until the lock is acquired (shown in Figure 3(a)). As a result, the processing capacity is wasted due to unnecessary waiting.

Different from shared-everything OLTP models, data can only be accessed by a single thread in shared-nothing models, so worker threads don X  X  need to acquire locks for executing queries. However, the parallel capacity inside a partition is scarified since the queries in the sa me partition have to been executed by a thread serially (shown in Figure 3(b)). It gets worse when the load is skewed in the system. Threads in heavy-loaded partitions are busy executing while threads in light-loaded partitions are idle.
 bCATE outperforms both of them in te rms of the efficiency of query pro-cessing. By separating query execution into two phases, threads in bCATE can reduce the performance degradation due to unnecessary waits. When a query is not executable, master threads in bCATE will not wait and just move forward to process the next queries in the queue. As the queries don X  X  conflict with each other, slave threads can execute these queries in parallel perfectly, fully utilizing the computing capacity. Furthermore, by allowing a set of threads to execute queries in parallel, bCATE can exploit more parallelism in query processing. Though contention detection can not b e avoided in bCATE and contention is detected in master threads serially, b CATE still benefits more from executing queries in parallel because the overhead of contention detection is relatively low compared to query execution.

Though bCATE still suffers the problems brought by partitioning, it X  X  less painful for bCATE than for shared-nothing models. The increased load in the affected partitions can be distributed am ong slave threads. Besides, as there is no resources bound to slave threads in bCATE, it X  X  easier to move slave threads than data resources from a partition to another partition. By redistributing slave threads among partitions, it makes bCATE more efficient to balance the load than shared-nothing systems. In order to evaluate our transaction execution model, we implement a proto-type bCATE system on top of Shore-MT[1]. Since Shore-MT does not have a SQL front end, all the transactions are hard-coded. We also implement a shared-everything (SE) model and a state-of-the-art shared-nothing (SN) model [3] on Shore-MT as the competitors in our experiments. To reduce the influ-ence brought by partitioning schemes, we use horizontal partitioning schemes for both the shared-nothing model and bCATE. The database is partitioned into 24 partitions in the shared-nothing model to fully utilize the CPU cores while the database is partitioned into 4 partitions and each partition is assigned 8 slave threads in bCATE. 4.1 Experiment Setup We perform all our experiments on a Lenovo R680 server running CentOS 5.4 with 48GB of RAM and two processors of Intel E7-4807 processors, each of which contains six cores clocked at 1.86GHz. We configure Shore-MT with 4GB buffer to ensure that the database size fits in the buffer pool size. As such, the flushing of log is the only I/O in the system.
 Two benchmarks are adopted in our evaluation: Nokia X  X  Network Database Benchmark [25] (TATP, also known as  X  X M1 X ) and Sysbench [26].

TATP is an open source workload designed specifically for high-throughput applications. TATP contains eight kinds of extremely short transactions. 80 per-cent of the transactions are r ead-only transactions an d the remaining 20 percent are update transactions. There X  X  almost no range query in TATP X  X  transactions. We use a 1,000,000 subscriber TATP data set occupying approximate 1GB in our experiments.

Sysbench is a multi-threaded benchmark tool for evaluating database server performance. We use a Sysbench data set with 1,000,000 rows. There are five types of transaction in our experiments including Point Queries, Range Queries, Update Queries, Delete Queries and Insert Queries. The portion of Point Queries is 80 percent and the others are 5 percent, respectively. 4.2 Concurrency and Scalability We first quantify how effectively bCATE improves the concurrency and scala-bility for highly-concurrent OLTP systems by comparing the shared-everything model (SE), the shared-nothing model (SN) and our bCATE model. The number of transactions executed is employed as the criterion. Then scalability on each model is calculated by dividing its throughput by the performance in the system when there is a single thread. The performance of three models as the number of concurrent threads in system increases is shown in Figure 4 and Figure 5 shows the scalability of these models.

In general, bCATE exhibits good performance in the presence of highly con-current workloads as bCATE can eliminate the potential contention and exploit extra parallelism when executing transa ctions. It can be seen that the shared-everything model experiences serious concurrency and scalability problems with the increment of concurrent threads in Sysbench. The shared-nothing model exhibits a better scalability in Sysbench while bCATE outperforms the shared-nothing model by exploiting extra parallelism in partitions.

When evaluating these models in TATP, bCATE stands out in both per-formance and scalability. Transactions in TATP are extremely short and they merely conflicts with each other. As a result, the shared-nothing model suffers more from the extra cost brought by partitioning than the benefits from con-tention elimination. Both shared-everything model and bCATE scale well when the number of concurrent threads is small. But when the number of concurrent threads continues to increase, the shared-everything model X  X  throughput reaches its peak while bCATE X  X  throughput remains increasing. 4.3 Tolerance to Skew Data skew is common in real workloads. To evaluate the three models X  perfor-mance in skewed workload, we use Zipfian distribution with different skew factor to generate imbalanced accesses.

We evaluate bCATE X  X  performance in sk ewed workload with various config-urations. First, we partition the database into different numbers of partitions with the same number of slave threads. The performance of these configura-tions as skew factor increases is illustrated in Figure 6(a). As the skew factor increases, these configurations exhibit s imilar performance. Therefore, bCATE X  X  performance in skewed load is not as sensitive as shared-nothing models to par-titioning schemes.

Next, we partition the database into the same number of partitions and assign different number of slave threads to these partitions. Since each partition in bCATE is shared among a set of slave threads, the skewed load inside a partition can be naturally distributed across these threads. Therefore, the configurations with more slave threads are expected to tolerate more data skew because there are more slave threads in each partition. Figure 6(b) confirms our expectations. We can observe that the configuration with more threads in a partition tolerates more skewness. Thus, increasing the number of slave threads in heavy-loaded partitions can efficiently eliminate the performance degradation due to data skew.

Finally, we examine the impact of data skew on all the three models as illus-trated in Figure 7. With the increment of skewness, the performance of shared-nothing model downgrades sharply, wher eas the shared-everything model and bCATE provide stable performance. Wh en the skew factor exceeds 0.7, most queries are routed to a single partition. In this case, the throughput of bCATE drops as the slave threads in the partition cannot complete the queries in time. In this paper, we proposed a novel tran saction execution model called bCATE to improve the concurrency and scalability for highly concurrent OLTP systems. Based on the observation that conventional shared-everything models suffer a significant performance degradation due to contention between threads, we elim-inated the overhead by partitioning and contention detection. Sharing in the low level makes bCATE naturally resistant to data skew. The evaluation shows that a significant amount of throughput improvement is obtained for high-concurrent workloads and the performance is less influenced by data skew in bCATE. Acknowledgements. This research was supported by the National Natural Science Foundation of Chin a under Grant No . 61073019 and 61272155.
