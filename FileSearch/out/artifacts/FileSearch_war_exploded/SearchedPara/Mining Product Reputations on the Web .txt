 k-tateishi@bq.jp.nec.com, t-fukushima@cj.jp, nec.com 
Knowing the reputations of your own and/or competitors' products is important for marketing and customer relation-ship management. It is, however, very costly to collect and analyze survey data manually. This paper presents a new framework for mining product reputations on the Inter-net. It automatically collects people's opinions about target products from Web pages, and it uses text mining techniques to obtain the reputations of those products. vance syntactic and linguistic rules to determine whether any given statement is an opinion or not, as well as whether such any opinion is positive or negative in nature. We first collect statements regarding target products using a general search engine, and then, using the rules, extract opinions from among them and attach three labels to each opinion, labels indicating the positive/negative determination, the product name itself, and an numerical value expressing the degree of system confidence that the statement is, in fact, an opinion. The labeled opinions are then input into an opinion database. meaningful information included in the database, is then conducted. We specify target categories using label val-ues (such as positive opinions of product A) and perform four types of text mining: extraction of 1) characteristic words, 2) co-occurrence words, 3) typical sentences, for in-dividual target categories, and 4) correspondence analysis among multiple target categories. and effectiveness of the framework, which offers a drastic re-duction in the overall cost of reputation analysis over that of conventional survey approaches and supports the discovery of knowledge from the pool of opinions on the web. 
Permission to make digital or hard copies of all permission and/or a fee. SIGKDD 02 Edmonton, Alberta, Canada Copyright 2002 ACM 1-58113-567-X/02/0007 ...$5.00. 
Knowing the reputations of your own and/or competitors' products is important for marketing and customer relation-ship management. Questionnaire surveys are conducted for this purpose, and open questions are generMly used in the hope of gaining valuable information about reputations. 
One problem in dealing with survey data is that the man-ual handling of it is both cumbersome and very costly, es-peciaUy when it exists in large volume, and computerized mining of open answers (i.e., the answers to open questions) is crucial. For this purpose, we have previously proposed a text-classification-type survey analysis tool[15] that is par-ticularly well-suited to brand image analysis. Throughout this paper, we refer to this tool as SA 1. 
Another problem is that it is very costly to gather the large volume of high quality survey data, which is necessary for meaningful analysis of reputations. One approach which promises to reduce costs in this regard is the recently pro-posed opinion extraction [24], which is able to automatically extract opinions about specific products as expressed on the web. This can be thought of as a kind of information extrac-answering (see e.g., [13],[6],[17],[22],[23],[12],[18],[19],[25]), both of which have extensively been explored in the field of web text retrieval. 
The purpose of this paper is to provide a general new framework for automatically collecting and analyzing opin-ions on the Internet. With it, it is possible to drastically reduce total costs for marketing research and analysis and to support the knowledge discovery on the Internet. This framework has been created by combining the opinion ex-traction technique developed in [24] with text mining method-ologies, two of which were previously employed in SA [15]. 
The key to the combination of opinion extraction with text mining is opinion labeling. In the opinion extraction process, labels are attached to each of the opinions, and then, in the text mining process, supervised learning from labeled opin-ions is conducted to extract statistically meaningful knowl-edge, which we refer to here as reputation. 
Let us roughly illustrate how the proposed framework works. A user inputs, for example, three PDA(personal dig-ital assistants) product names (products A, B, and C). The system then collects people's opinions about them from the 
Internet and attaches three labels to each: 1) the name of the product referred to, 2) the positive/negative nature of the opinion, and 3) opinion-likeliness, i.e., a numerical value 1This tool is available by the name of SurveyAnalyzer in 
Japan. SurveyAnalyzer TM is a trademark of NEC Corpo-ration in Japan. marketing. our framework, was developed by Tateishi, lshiguro, and 
Fukushima [24]. [t is closely related to information extrac-a wrapper or a specific extraction procedure is built auto-matically or marmally and used to extract specific pieces of information requested by a user. For example, Shopbot [9], [7] uses HTML tags to automatically extract product prices or specifications. Our proposed framework, by way of contrast, attaches labels to extracted information, which makes it possible to apply a supervised learning approach to text mining, and this distinguishes it from conventional information extraction techniques. 
QA (question-answering) (see e.g., [13],[6],[17],[22],[23],[12]), which has extensively been explored in the field of text re-trieval. In fact, it can be conducted by asking an ordinary QA system what; opinions exist for target products. The 
QA system will output opinions, giving each a fikelihood of its being included in an answer to that question. Among 
QA systems, Tateishi et.al.'s system [24] is particularly well suited to opinion extraction since it prepares specialized dic-tionaries for major product fields and unique syntactic rules for calculating opinion-fikeliness. These improve opinion-search results significantly, and it reportedly offers a partic-ularly high rate of accuracy with respect to searching out opinions about a target product: A precision rate of 86.6% for the top 17.1% of total search results, while the portion of total search results actually containing opinions, obtained by a general-purpose search engine (Google) was 15.9% in total[24]. alyzing open answers in questionnaire data, including those which employ text-clustering techniques (e.g., [14]). The idea here is to view each answer as a vector of words, and to cluster vectors on the basis of similarity measures. Such methods are effective for summarizing (grouping) answers, but they are not effective for extracting analysis target char-acteristics. Methods for analyzing open answers on the basis of associations between words have also been proposed (e.g., [11]). More specifically, associations between word pairs are calculated on the basis of their co-occurrences in open an-swers, and they are visually presented on a two-dimensional positioning map. In most of the previous work on position-ing maps, redundant words tend to appear too frequently, making the maps hard to understand. This suggests the ne-cessity of preprocessing step in which characteristic words are first extracted. proposed by Li and Yamanishi [15]. Texts consist of open answers contained in questionnaire results, and categories are specified on the basis of closed answers (i.e., answers to closed questions, for which possible responses have bnen fimited, as in check fists). In this approach, a classifica-tion rule that assigns a text into one of some number of categories is learned from training examples, and the key-words that appear in the rule can be thought of as those that are characteristic of the category. The key to this ap-proach is supervised learning from labeled examples, which is well-suited to the analysis of the labeled opinions that the opinion extraction system produces. technique (here, opinion extraction[24]) with a text classifi-cation technique (here, that of Li and Yamanishi[15]). To the best of our knowledge, such a combination has never been reported before. Section 2 below gives a brief sketch of our reputation anal-ysis framework. Section 3 describes the opinion extraction technique proposed in [24]. Sections 4-7 describe how we analyze extracted opinion data with, respectively, rule anal-ysis, co-occurrence analysis, typical sentence analysis, and correspondence analysis. In Section 8 we evaluate the va-lidity and effectiveness of our framework. Section 9 gives concluding remarks. Figure 1 gives a flow overview for our reputation mining 
Product names ~ -~-x~ v Mining results ~ Positive/negative determining 
Opinion-likeliness calculation framework. The system performs two functions: opinion extraction and reputation analysis. A user can first input product names (e.g., Products A, B, and C, all of which are cellular phones) into the system, and the opinion extrac-tion function will use a search engine to collect web pages that include those names. It then extracts sentences that express opinions regarding these products and inputs them into an opinion database. The text mining function, which is the major subject of this paper, takes as input an analysis condition specifying the target category, and it outputs its mining results. The opinion extraction function consists of the following modules [24]: Web page collection module, positive/negative determining module, and opinion-likeliness calculation mod-ule 1. Web page collection module: 
This module uses a crawler to collect web pages relevant to input product names. 2. Positive/negative determining module: 
For each of the collected pages, this module first extracts sets of sentences that include evaluation-expressions about the products. These are checked with a previously prepared evaluation-expression dictionary. It then selects from among them sentences in which evaluation expressions are located within a certain distance of the relevant product name, and it designates these as opinions. Here each evaluation-expression is registered in the dictionary as being either positive or negative. For example, in the field of computer equipment, "fast," "good," "light," "satisfied," and "recommendable" are positive evaluation expressions, while "heavy, .... easily broken," "noisy," and "unstable" are negative. On the basis of evaluation expression dictionary entries, the "positive" or "negative" nature of each opinion is determined, taking into account, naturally, linguistic negation. If, for example, an inherently "positive" expression like "low-cost" appears within a certain distance of a negating expression, such as "insufficient," the opinion as a whole will be deemed "nega-tive." 3. Opinion-likeliness calculation module: 
For each opinion obtained by the previous module, this mod-ule calculates its opinion-likeliness score, a real value rang-ing from 1 to 5, indicating the relative likelihood that the statement represents an opinion: the higher the score, the higher the likelihood. This score is calculated using syn-tactic property rules, which can either be learned manually from training examples (see [24] for details) or by a standard machine learning technique, such as decision-tree-induction. The labeled opinions are input into the opinion database. 
Table 1 shows an example of 6 such opinions. (In this paper, all opinions have been translated from Japanese to English.) 
The first step in mining opinions here is to extract key-words that are indicative of a specified category. In order to do this, we learn text classification rules and association rules from examples (see [15],[16]). The learned rules are basically lists of words that must be present for a new text to be classified into a specific category. These are the char-acteristic words of the category. Extracting characteristic words for each category helps us to discover differences in opinions between the target category and other categories. 
The task of rule-based text classification can be described as follows: We have a number of categories, each already containing a number of texts as training examples; we are to automatically acquire a set of rules from them and then clas-sify new texts on the basis of those acquired rules. Here we employ text classification based on a stochastic decision list [26],[15],[16] consisting of an ordered sequence of IF-THEN-
ELSE rules for assignment of new opinions to a given cate-gory. The condition part (IF part) may require the simulta-neous presence of several words or simply the presence of a single word, and the consequence part (THEN part) specifies a category. Each rule also attaches a probability (relative frequency) value to its assignment. 
The words in the condition part of the obtained classifi-cation rules for a specific category represent the character-istics of opinions there. We can extract characteristic words for a specific category in the opinion database by learning text classification rules, using the opinion database itself for training examples. 
For example, Figure 2 shows a classification rule for the category specified by [product name = cellular phone A] 
Product name Nature [ Opinion-likeliness cellular phone A Positive 4.05 cellular phone A Negative 2.74 cellular phone C Negative cellular phone E Positive cellular phone B Positive Figure 4: Word Histogram for Cellular Phone A 
Alternatively, we may calculate l(x) using extended stochas-tic complexity [27] as follows: 
Here, m denotes the length of x, ml the number of "r's in x, H(z) = -zlog~ z-(1-z)logs(1-z), and C is a cer-tain positive constant. Note that the stochastic complexity of x is interpreted as the shortest code length required to encode x using a given probabilistic model (in this case, a Bernoulli model) under the prefix coding condition [21]. 
Here, stochastic complexity is also considered from the view-point of a statistical decision theory a loss for predicting x in the case where the logarithmic loss function is used as a distortion measure. Extended stochastic complexity can be considered to be a general extension of stochastic complex-ity, in the sense that a general loss function is employed as a distortion measure (in (2), we use a "discrete loss"). We calculate the score of w as follows: 
Score(w) = ! (I(O) -(I(E(w)) + I(D -E(w)))). (3) 
Score(w) represents information gain achieved by the se-lection of w, which intuitively shows how much the stochas-tic complexity of the original data sequence can be reduced by separating it into two parts: that which contains w and that which does not. A larger Score(w) indicates that w is either characteristic of the set of all "1" texts or of the set of all "0" texts. If we were to eliminate the second term from stochastic complexity formula (1), leaving only the en-tropy (the first term), Score(w) would then become equiv-alent to the mutual information often used in decision-tree splitting[20]. Notice here that stochastic-complexity-based 
Score(w) is a more precise measure of information included in a data sequence of finite length than entropy, and the for-mer will converge to the latter as the length of the sequence increases to infinity. 
In the learning of classification rules, we calculate, on the rules. Here, the condition of a rule may include not only the presence of one word, but also the simultaneous presence of several words. We select as a first rule that for which the Score(w) value is the largest. We then remove from the data those that satisfy the condition of the first rule. For the remaining data, we again calculate the Score(w) value for each of the remaining possible rules, and select as a second rule that for which the Score(w) value is the largest. We repeat this process until we cannot find any rule which is significant in terms of Score(w). 
In the learning of association rules, we calculate, on the basis of all the data, a Score(w) value for each of the pos-sible rules, and sort the rules in descending order of their 
Score(w) values. Note that this algorithm is different from conventional association rule mining algorithms (see e.g., [2]), which perform based on support ("Total Freq." in this paper) and confidence ("Freq./Total Freq." in this paper) only. 
For each characteristic word or phrase extracted from open answers belonging to a specific category, we extract a list of words or phrases that co-occur with that word or phrase. Through this list we are better able to understand the contexts in which the characteristic keywords appear. Table 2 shows a co-occurring word list for the "No. 1" Characteristic Word I Co-occurring Word Freq. Score no problem(s) i boot up 3 0.0070 and "no problem" shown in Figure 2. With no contextual information, characteristic words such as "No. 1" or "no problem" have little meaning for us, but co-occurrence anal-ysis can, for example, help us here to see that cellular phone 
A is recognized as the No. 1 candidate for something, and that there is no problem in booting up cellular phone A. These contexts help form reputations for cellular phone A. 
Below we describe how to calculate a co-occurrence score for any given pair of keywords. For a word or phrase w, let 
D~ denote the sequence of texts including w. For another word or phrase w', let D~o(w') denote the subsequence of D I(D~-D~o (w')) be the stochastic complexities of D~, D~(w'), 
D~ -D~o(w') (as calculated in (1)), respectively, we define the co-occurrence score of w ~ with respect to w as where m is the number of texts included in D~o. The larger 
Score(w I : w) is, the larger the degree of co-occurrence of w' relative to w is. 
Maximizing Score(w': w) w.r.t, w I is equivalent to mini-mizing I(Dw(w'))+I(O~o-D~(w')) w.r.t, w' since I(D~) is independent of w ~. Notice also that Score(w ~ : w) is asym-metric with respect to w and w'; that is, Score(w ~ : w) 
Score(w : w I) in general. 
For a set of opinions belonging to a specific category, we give a score to each of them, with a high score indicating a high possibility of its being a typical opinion for the cate-345 gory. This gives the user a simple overview of tendencies in original opinion data. 
Table 3 shows a list of typical sentences in the category specified by [product name = cellular phone A]. The charac-teristic words extracted above (such as "benchmark result", "no problem(s)', etc.) appear with high frequency in typical high-scoring sentences. 
Below we describe how to calculate a score for any given opinion sentence s. Let W be a set of all words appearing in opinion database and C be a set of all categories. Let Nc be the number of opinions belonging to the category c E C. Let N = ~cec N:. We calculate the occurrence probability of c using its MAP estimate: where j3 is a positive constant usually set to 1/2. 
For a category c E C, let Dc denote the set of opinion sentences belonging to c and let m~ be the number of oc-currences of w in D~. We can then calculate the occurrence probability of w in c by using its MAP estimate: where fl is a positive constant usually set to 1/2. 
Suppose that an opinion s is represented as a sequence of words wl,... ,wr, where wi E W (i = 1,... ,T). We calculate the score of s using the Bayes posterior probability of the category c for given s as follows: where we used the naive Bayes assumption that each wl is independent. 
We conduct correspondence analysis in order to get what we call a two-dimensional positioning map over the set of analysis objects and keywords extracted from the opinion database. The map visually shows the relationships between the categories and characteristic words, with distance on the map being a representation of correspondence (closeness). 
Before performing correspondence analysis, characteris-tic words are extracted for the categories designated by the user. Specifically, words considered indicative of individual target categories are extracted on the basis of rule analyses. These extracted keywords are in fact equivalent to those con-tained at the upper reaches of the association rules for the targets. We then construct a table that contains frequency data for extracted words for each of the target categories. 
Correspondence analysis can be viewed as an extension of principal component analysis (PCA) (which is similar to Singular Value Decomposition). It is conventionally per-formed, as in [5], on the basis of the frequency data table. Examples of the result of correspondence analysis are shown in the next section. 
Note that if correspondence analysis is conducted from original data without extracting characteristic words, an un-readable positioning map, in which many unnecessary words appear, may be generated as a result. That is why using rule analysis in preprocessing is crucial to effective corre-spondence analysis. 
We conducted experiments in three different product fields: cellular phones, PDAs, and Internet service providers. We input the names of five cellular phones, four PDAs, and five Internet service providers into the system's opinion ex-traction section. The web page collection module ran on a SunEnterprise250 with a 400MHz UltraSPARK-II and a 512MB memory. It collected 1200 pages for each product name (total: 16800) within 3 hours. The positive/negative determining module and the opinion-likeliness calculation module ran on a DELL Precision420 with an 866MHz Pen-tiumlII and a 768MB memory. They processed the pages within 1 hour. Labeled opinions with opinion likeliness not less than 2.5 were recorded in three separate databases in accord with their respective product fields (cellular phones, 
PDAs, and ISPs). The number of data records (extracted opinions) was, respectively, 519, 1195, and 605. We describe below the results of text mining for each of the fields. 
We extracted characteristic words for each of the cellular phones on the basis of learned association rules and per-formed a correspondence analysis of them. The text mining was processed on an NEC MA86T with an 866MHz Pentium 
III and a 256MB memory. It took about 10 seconds to learn rules for a single product, as opposed to roughly 2 hours for manual rule analysis (conducted for time comparison). The correspondence analysis took about 5 seconds. 
The rule analysis results gave a good indication of the reputation of each cellular phone. For example, we discov-ered that the red body of cellular phone C had captured the public's fancy, and that cellular phone D attracted peo-ple who wanted to replace older models. Figure 5 shows the obtained positioning map. The words plotted around each cellular phone are the top four characteristic words for each. We can see that cellular phone A has a good reputa-tion for its basic performance ("benchmark result", "fast", "no problem(s)'), and cellular phone B has a bad reputa-tion ("doesn't work well", "slow"). We can also see that for cellular phone D its "display" is more of an issue than its basic performance since the position of its product name is located closely to "display" but far from words representing performance. 
In reputation mining for PDAs (personal digital assis-tants), we used both of the product name label and posi-tive/negative nature label. We extracted characteristic words for each of the PDAs on the basis of learned association rules for connecting positive opinions to product names. The tar-get category was defined as [product name = X, nature = positive]. 
We discovered that PDA A has a good reputation with respect to the use of email when away from home/work. We also discovered that its monochrome screen version is popu-lar. Since the monochromatic quality is generally regarded as negative, this reputation is very interesting. A search of the original opinion data for the word "monochrome" in-dicated that the long running time, the reasonable price, and the large keyboard of the monochrome version are rea-sons for its good reputation. Figure 6 shows the obtained positioning map for four PDAs. Although all of the prod-ucts have good reputations, they show almost no sharing of "1 "2 
Figure 6: Positioning Map for PDAs and Charac-teristic Words 
Figure 7: Positioning Map for ISPs and Character-istic Words attribute-words, which implies that, in terms of extracted words, these products do not compete with each other. 5.3 Internet Service Providers they form three well-separated clusters. Providers within a single cluster have similar reputations. For example, two closely located ISPs, D and E, share reputations for both "economy" and "sense of security", and it is easy to imagine that they might compete each other. 6. CONCLUDING REMARKS product reputations on the web. It consists of an opinion ex-traction portion and a text mining portion; the former works as an application-specific question-answering system, and the latter conducts four fundamental tasks: characteristic word extraction, co-occurring word extraction, typical sen-tence extraction, and correspondence analysis. The key to combining these two parts is opinion labeling, which makes it possible to conduct supervised learning in the text min-ing portion. We used real data to empirically demonstrate that the proposed framework is able to help users discover significantly important knowledge regarding the reputations of products of interest, and to drastically reduce the cost of collecting and analyzing opinions. Our framework could, of course, also be well applied to mining reputations far be-yond the area of industrial products, for example, individu-als, events, services, companies, governments, etc. 7. REFERENCES [1] B. Adelberg, Nodose -a tool for semi-automatically [2] R. Agrawal and R. Srikant, Fast algorithms for mining [3] M.R. Anderberg, Cluster Analysis for Applications, [4] N. Ashish ~md C. Knoblock, Wrapper generation for [5] J.P. Benzecri, Correspondence Analysis Handbook, [6] V. Chandhri and R. Fikes, Answering Systems, the [7] D. Clark, Shopbots Become Agents for Business [8] M. Craven, D. DiPasquo, D. Freitag, A. McCallum, T. [9] R. Doorenbos, O. Etzioni, and D. Weld, A scalable [10] D. Florescu, A. Levy, and A. Mendelzon, Database [11] Fujitsu, Symfoware World [12] S. Harabagiu, M. Pasca, and S. Maiorano, 348 
