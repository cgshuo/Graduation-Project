
Covariance and correlation estimates have important appli-cations in data mining. In the presence of outliers, classical estimates of covariance and correlation matrices are not re-liable. A small fraction of outliers, in some cases even a single outlier, can distort the classical covariance and cor-relation estimates making them virtually useless. That is, correlations for the vast majority of the data can be very er-roneously reported; principal components transformations can be misleading; and multidimensional outlier detection via Mahalanobis distances can fail to detect outliers. There is plenty of statistical literature on robust covariance and correlation matrix estimates with an emphasis on affine-equivariant estimators that possess high breakdown points and small worst case biases. All such estimators have unac-ceptable exponential complexity in the number of variables and quadratic complexity in the number of observations. In this paper we focus on several variants of robust covariance and correlation matrix estimates with quadratic complex-ity in the number of variables and linear complexity in the number of observations. These estimators are based on sev-eral forms of pairwise robust covariance and correlation es-timates. The estimators studied include two fast estimators based on coordinate-wise robust transformations embedded in an overall procedure recently proposed by [14]. We show that the estimators have attractive robustness properties, and give an example that uses one of the estimators in the new Insightful Miner data mining product. 
Data Mining, Outliers, Robust Statistics, Robust Estima-tors, Scalable Algorithm Covariance and correlation matrices estimated from pos-sibly very large data tables are used for a variety of purposes permission and/or a fee. SIGKDD '02 Edmonton, Alberta, Canada 
Copyright 2002 ACM 1-58113-567-X/0210007 ...$5.00. in data mining. For example, pairwise sample correlation co-efficients are often examined in an exploratory data analysis (EDA) stage of data mining to determine which variables are highly correlated with one another. Estimated covari-ance matrices are used as the basis for computing principal components for both general principal components analy-sis (PCA), and for manual or automatic dimensionality re-duction and variable selection. Estimated covariance matri-ces are also the basis for detecting multidimensional outliers through computation of the so-called Mahalanobis distances of the rows of a data table. lation matrix estimates, motivated by either Gaussian maxi-mum likelihood or simple method of moments principles, are very sensitive to the presence of multidimensional outliers. 
Even a small fraction of outliers can distort these classical es-timates to the extent that the estimates are very misleading, and virtually useless in any of the above data mining appli-cations. To cope with the problem of outliers, statisticians have invented robust methods that are not much influenced by outliers for a wide range of problems, including estima-tion of covariance and correlation matrices. We illustrate the extent to wlhich outliers can distort classical correlation matrix estimates and the value of having a robust correla-tion matrix estimate with the small five-dimensional data set example illustrated in Figures 1-3. data set called "Woodmod". This data clearly has at least Figure 2: Classical and Robust Correlations for 
Woodmod Data. not univariate outliers, i.e., they do not show up as well detached outliers in any of the variables. Figure 2 shows the result of computing all pairwise classical correlations by both the classical method (sample correlation coefficients) and a particular robust method known as the Fast MCD. 
The lower left triangle of values shows both the classical and robust' correlation coefficient estimates, while the upper right triangle of ellipses visually represent the contours of a bivariate Gaussian density with zero means, unity vari-ances, and correlation coefficients given by the classical and robust correlation coeffient estimates. A nearly circular el-lipse indicates an estimated correlation coefficient of nearly zero. A narrow ellipse with its major axis oriented along the +45 degree (-45 degree) direction indicates a large positive (negative) estimated correlation coefficient. From the visual representation you immediately see differences between the classical and robust correlations, sometimes very substan-tial differences, including changes of sign. For example the classical correlation between V4 and V5 is -.24 whereas the robust correlation is +.65. The latter is quite consistent with what you might expect if you deleted the small cluster of outliers occurring in the scatterplot of V4 versus V5 in Figure 1. to use the classical squared Mahalanobis distance: 
In the above expression xi is the i-th data vector of dimen-sion p (the transpose of the i-th row of your data table), is the vector of sample means of the columns of your data table, and C is the usual sample covariance matrix estimate. 
Under the assumption that the data is multivariate normal and that you use known values p and C in place of the above estimates, the d2(x~) would have chi-squared distri-bution with p degrees of freedom. With reasonably large sample sizes the sample mean vector and sample covariance matrix will be close to their true values, and it is common practice to use the square root of chi-square (with p degrees of freedom) percent point such as .95 or .99 as a threshold to compare d(xl) with, and declare xi to be an outlier if it exceeds this threshold. If you follow this classical approach for the Woodmod data of Figure 1, you get the results in the right-hand panel of Figure 3. The horizontal dashed line is the square-root of the 95% point of a chi-squared distribution with 5 degrees of freedom. Clearly no points are declared outliers by the classical Mahalanobis distance approach. This is because the outliers have distorted the Use of the quadrant correlation and Huberized esti-mates of approach (ii) above, which are very trans-parent in the way they work, and enable fast scalable computation for data mining applications, with com-plexity O(n)  X  O(p 2) for the resulting p x p covariance or correlation matrix. Computation of attractive maximum bias and break-down points for the component robust pairwise corre-lation and covariance estimates. Monte Carlo comparison of FMCD and QC using a new class of contamination models appropriate for data mining applications.  X  Use of the [14] method of obtaining positive definite-ness. Introduction of a new way of computing reliable thresh-olds for detecting multidimensional outliers with ro-bust Mahalanobis distances, based on robust fitting of gamma distributions to the distances. In Section 2 we describe the quadrant correlation and Hu-berized estimates, introduce the new contamination model and calculate maximum biases and breakdown points of the estimates, and compare their performance. This section also describes how to insure that the resulting covariance or cor-relation matrix is positive definite. Section 3 briefly dis-cusses the setting of thresholds for outlier detection with robust Mahalanobis distances, including the new method based on robust fitting of a gamma distribution to the dis-tances. Section 4 describes the Insightful Miner pipeline implementation of the new robust covariance and outlier detection method, and Section 5 gives two examples. 
Statisticians use contamination or mixture models to study the performance of robust alternatives to classical statistical procedures wheEL these procedures are applied to messy data sets that contain outliers. Most multivariate contamination models for numeric data proposed to date (see for exam-ple [6]) assume that the majority of the observations (rows in your data table) come from a nominal distribution such as a multivariate normal distribution, while the remainder come from another multivariate distribution that generates outliers. Specifically it is assumed that a p-dimensional row vector y of your data table has a multivariate mixture dis-tribution of the form: where for example Fo is a multivariate normal distribution with mean # and scatter matrix E, i.e., Fo = N(p, ~), and H is an arbitrary multivariate distribution. Under this model a fraction (1 -e) of the rows on average are distributed ac-cording to Fo and are therefore the majority or "core" data, while a fraction e of the rows are from H and generate out-liers that deviate from the core behavior of the data. We stress that such outliers could be "bad" data due to record-ing errors of all kinds, or they could be a highly informative subset of the data that leads to new scientific discovery or improved business operation. There is a need for more real-istic contamination models for applications in data mining. We note that the above mixture model may be equivalently represented as a very special case of the following replace-ment model where x represents the core data, ~ is an arbitrary random vector represent!mg outliers, and B = Diag(B1, B2,..., Bp) is a p  X  p diagonal matrix such that B, B2,.  X   X , Bp are Bernoulli random variables with marginal probabilities P(B~ = 1) = 1-P(B~ = O) = ei. Specifically, if x has distribution Fo and i has distribution H, and in addition the diagonal matrix B of Bernoulli random variables has the special completely dependent structure then y generated by the replacement model has the classical multivariate mixture distribution (2) above. I p I 1 2 5 10 15 20 25 50 100 I Percentage 95 90 77 60 46 36 28 8 1 Table 1: Percentage of Perfectly Observed Cases (Rows) as a Function of the Number of Variables in the Data Set. 
Unfortunately model (2) does not adequately model re-ality for many kinds of large multivariate data sets that arise in data mining applications. It may often happen in data mining applications that outliers occur in each of the variables independently of the other variables, or in special dependency patterns other than the complete dependency pattern (4). Different values of ei and different dependence structures among the B~s generate different contamination neighborhoods. We focus in this paper on the former situ-ation, which we call the "independent outliers in variables (IOIV)", with equal probabilities of an outlier occurring in each variable (and reserve treatment of unequal probabilities of outliers and special dependency patterns for a subsequent study). This amounts to assuming that the B~ (i --1,... ,p) are independent with constant ei = e. This leads to a com-pletely different situation than with the classical outlier gen-erating mixture model (2). For example, if observations in each column of the data table are spoiled on average 5~0 of the time, independently from column to column, the proba-bility of a perfectly observed row becomes exceedingly small when the number of columns increases (see Table 1). 
We have learned from our study of the new IOIV model above that robust affine equivariant methods may not be very reliable in the case of large data sets because the op-erations needed to compute affine equivariant robust esti-mates tend to "propagate" the effect of the outliers. We illustrate this fact by comparing the performance of the affine equivariant Fast Minimum Covariance Determinant (FMCD) with that of the simple non-affine equivariant ro-bust covariance estimate based on pairwise quadrant corre-lation coefficient (QC) estimates, defined below, under the new contamination model. The results are shown in Figure 4, which displays the distance between the two robust esti-mates and the true covariance matrix (the ratio of condition numbers as proposed by [13] versus the fraction of contami-nation e). The QC based estimate clearly out-performs the FMCD for p --20 and 30. We can see that as p increases FMCD performs very poorly and practically breaks down at e --0.10 for p ----20 and at e = .05 for p --30. Based on Table 1 and Figure 4 we conjecture that the maximum BP of any affine equivariant scatter estimates under the new contamination model is very small for large p. In the least favorable dependence configuration the maximum BP may be bounded above by 1 -0.51/p (e.g. 0.03 for p = 20). Pair-wise methods like QC are much cheaper to compute and more reliable in these situations. 
The following theorem illustrates the type of results that one obtains when using IOIV models: you do best using the smallest possible number of columns at a time. More pre-cisely it shows that for calculating the multivariate location we do better off processing each column of the data table separately, in that we minimize the maximum bias by using the coordinate-wise median. The family of multivariate dis-tribution functions F generated by (3) is a contamination neighborhood of Fo and will be denoted by .T'. 
THEOREM 1. Suppose that Fo = N(#, O"21). The coordi-Table 2: Maximum Bias Calculations for Different Values of c and p, Under the Classical Contamina-tion Model of Size e = 0.05. By the Cauchy-Shwartz inequality Differenciating the right hand side with respect to a and using the Cauchy-Shwartz inequality again we can verify that this derivative is non-negative for all a &lt; b. Therefore, letting (e/(1 -e))/b =/~ and noticing that r(Ho) = A/B we can write r(H)_&lt; (1 e)B X eb-(1 e~: = 1+/3 The second inequality follows because is increasing in b. An analogous reasoning gives [7] stated inequalities (6) and (7) without providing a proof. The result follows now because the function p = g (r) is non-decreasing. Table 2 shows maxbias calculations for different values of c, and p using the result from Theorem 2. The table exhibits that the QC (c = 0) is a very attractive alternative. 
Consider the robust Mahalanobis squared distances (1) where the p-dimensional vector ~ is a robust estimate of the mean of the observation vectors x~ and C is a robust covariance matrix estimate based on one of the pairwise methods described in the previous section, The example in the introduction indicated that use of such robust distances can lead to reliable detection of multidimensional outliers, whereas use of tile classical Mahalanobis distances based on the Gaussian maximum likelihood estimates of mean and covariance can completely fail to find such outliers. In that 
The timing results confirm the above complexity claim, 
The Choice of Robust Scale Estimate. The IQD 
The Robust Mahalanobis Distance. The above ro-Figure 6: I-Miner Outlier Detection Visual Flow 
Map. of 9 numeric variables and one categorical variable. The 9 numeric variables are the percentages of various chemical constituents of the glass. We computed the QC based robust covariance matrix: and robust Mahalanobis distances for the sub-table consisting of the first five columns of the above table, using the Insightful Miner Outlier Detection node. 
Figure 6 shows an I-Miner visual data and compuation flow diagram that accomplishes the following: the Read Text File node reads a user-specified text file; the Filter Columns node lets the user select particular variables/columns (in this ex-ample the first five variables); the Outlier Detection node implements the algorithm described in Section 4; the Table 
View provides a table of values of the variables with each vector observation in a row, along with a column containing the robust Mahalanobis distances and a column containing "Yes" or "No" indicating whether or not a table row was de-clared an outlier. The Chart node computes the histogram of the robust distances, as shown in Figure 7. 
Miner, the outlier detection algorithm generated the follow-ing output in a report window: sults in declaring that approximately 29% of the data points are outliers, while the remaining 71% of the data represents a central core. The histogram in Figure 7 clearly shows a cluster of large distances around 850 to 900 which are much larger than the automatic 99% chi-squared (with 5 degrees of freedom) threshold of 15.1 used above. plots in Figure 8 reveals an interesting aspect of the mul-tivariate structure that is reasonably consistent with these observations. One sees that the data appears to have a cen-tral core that is roughly elliptical in the pairwise views, along with broadly scattered outliers and the distinctive rod-like structure. The latter is due to the fact that 41 of the ob-servations of the Mg variable have value zero. This was evidently because the data values were not recorded or were tection algorithm with a 99% chi-squared threshold of 15.1 does is identify the diffuse outliers as well as the extreme outlying rod caused by the zero Mg. In other words the outlier detection algorithm is behaving quite as anticipated X  acter is the separation of the pure rod outlier as the most ex-treme set of distances, distances that are well beyond those of the diffuse outliers closer to central bulk of the data. If you use a threshold of 200 to set aside outliers you will set aside the pure rod, and this is not an unreasonable first step. 
In a second step you will find the remainder of the diffuse outliers. covariance matrix based robust distances to iteratively clus-ter multivariate data by itera~ive removal of outlier groups, monitored by histograms or density estimates of the robust distances, and subsequent iteration on the sub-clusters. This possibility bears further investigation. Figure 12: Differences between Classical and Robust Correlations for Modified PVA Data. [1] M. B. Abdullah. On a Robust Correlation Coefficient. [2] P. Davies. Asymptotic Behavior of S-Estimates of [3] S. J. Devlin, R. Gnanadesikan and J. R. Kettenring. [4] D. L. Donvho. Breakdown Properties of Multivariate [5] R. Gnanadesikan and J. R. Kettenring. Robust [6] F. Hampel, P. Ronchetti, P. Rousseeuw and W. [7] P. J. Huber. Robust Statistics. John Wiley &amp; Sons, [8] G. S. Manku, S. Rajagopalan and B. Lindsay. [9] A. Marazzi and C. Ruffieux. Implementing [10] R. Maronna. Personal Communication. In [11] R. Maronna. Robust M-Estimators of Multivariate [12] R. A. Maronna, W. A. Stahel and V. Yohai. [13] R. Maronna and V. Yohai. The Behaviour of the 
