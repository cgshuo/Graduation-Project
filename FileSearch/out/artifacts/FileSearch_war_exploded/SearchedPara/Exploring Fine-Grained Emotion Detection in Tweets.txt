 In sentiment analysis, emotion provides a promi s-ing direction for fine -grained analysis of subjective content (Aman &amp; Szpakowicz, 2008; Chaumartin, 2007) . Sentiment analysis is mainly focused on d e-tecting the subjectivity (objective or subjective) (Wiebe et al. , 2004) or semantic orientation (pos i-tive or negative) (Agarwal et al. , 2011; Ko u-loumpis et al. , 2011; Pak &amp; Paroubek, 2010; Pang et al. , 2002) of a unit of text (i.e., coarse -grained classification schemes) rather than a specific em o-tion. Often times, knowing exactly how one reacts emotionally towards a particular entity, topic or event does matter (Mo hammad et al. , 2014) . For example, while anger and sadness are both neg a-tive emotions, distinguishing between them can be important so businesses can filter out angry cu s-tome rs and respond to them e f fectively.

Automatic emotion detection on Twitter pr e-sents a different set of challenges because tweets exhibit a unique set of characteristics that are not shared by other types of text. Unlike traditional text, tweets consist of short messages expressed within the limit of 140 characters. Due to the length limitation, language used to express em o-tions in tweets differs significantly from that found in longer documents (e.g., blogs, news, and st o-ries). Language use on Twitter is al so typically i n-formal (Eisenstein, 2013 ; Baldw in et al. , 2013 ) . It is common for abbreviations, acronyms, emot i-cons, unusual orthographic elements, slang, and misspellings to occu r in these short messages . On top of that, retweets (i.e., propagating messages of other users), referring to @username when r e-sponding to another user X  X  tweet, and using #hashtags to represent topics are prevalent in tweets. Even though users are restrict ed to post o n-ly 140 characters per tweet, it is not uncommon to find a tweet containing m ore than one emotion .
Emotion cues are not limited to only emotion words such as happy , amused , sad , miserable , scared , etc. P eople use a variety of ways to express a wide range of emotions. For instance, a person expressing happiness may use the emot ion word  X  X appy X  ( Exam ple 1 ), the int erjection  X  X oop X  (E x-ample 2 ), the emoticon  X :)  X  (Example 3 ) or the emoji  X   X   X  ( Examp le 4 ). Example 1 :  X  X  can now finally say I am at a place in my life where I am happy with who am and the stuff I have coming for me in the future #blessed X  [Happiness] Example 2 :  X  X ts midnight and i am eating a lion bar woop X  [Happiness] Example 3 :  X  Enjoying a night of #Dexter with @DomoniqueP07 :)  X  [H appiness] Example 4 :  X  X he wait is almost over LA, will be out in just a little!  X   X   X   X   X  [Happiness]
In addition to explicit expressions of emotion, users on Twitter also express their emotions in fi g-urative forms through the use of idiomatic expre s-sions ( Ex ample 5 ) , similes ( Ex ample 6 ), meta phors (Example 7 ) or other de scriptors ( Ex ample 8 ). In these figurative expressions of emotion, each word if treated individually does not directly convey any emotion. When combined together and, depending on the context of use, they act as implicit indicators of emotion. Automatic emotion detecto rs that rely solely on the recognition of emotion words will likely fail to recognize the emotions conveyed in these examples.
 Example 5 :  X  X ter2459 it was!!! I am still on cloud nine ! I say and watched them for over two hours. I couldn't leave! They are in credible! X  [Happiness] Example 6 :  X  X etting one of these bad boys in your cereal box and feeling like your day simply coul d-n't get any better http://t.co/Fae9EjyN61 X  [Happ i-ness] Example 7 :  X  X oving the #IKEAHomeTour d X cor #ideas! Between the showroom and the catalog I am in heaven  X  [Happiness] Example 8 :  X  X  did an adult thing by buying stylish bed sheets and not fucking it up when setting them up. *cracks beer open*  X  [Happiness]
The occurrence of an emotion word in a tweet does not always indicate the tweete r X  X  emotion. The emotion word  X  X appy X  in Example 9 is not used to describe how the tweeter feels about the tune but is instead used to characterize the affe c-tive quality or affective property of the tune (Ru s-sell, 2003; Zhang, 2013) . The tweeter attributes a happy quality to the tune but is in fact expressing anger towards the  X  X appy X  tu ne. Similarly, #Ha p-piness in Example 10 is part of a book X  X  title so the emotion word hashtag functions as a topic more than an expression or description of an individual X  X  emotion. The common practice of using emotion word hashtags to retrieve self -annota ted examples as ground truth to build emotion classifiers, a method known as  X  X istant supervision X  (Hasan et al. , 2014; Mohammad, 2012; Mohammad &amp; K i-rit chenko, 2014; Wang et al. , 2012) , is susceptible to this wea k ness. Example 9 :  X  X Anjijade I was at this party on the weekend, that happy tune was played endlessly, really not my stuff, it was like the cure's torture ha X  [Anger] Example 10 :  X  X ear Carrie Goodwiler's audition for the audio version of my book #Happiness &amp; Honey on #SoundCloud X  [No Emotio n]
These challenges associated with detecting fine -grained emotion exp ressions in tweets remain a virgin territory that has not been thoroughly e x-plored. To start addressing some off these cha l-lenges, we present a m anually -annotated tweet corpus that cap tu res a diversity of emotion expre s-sions a t a fine -grained level. We describe the grounded theory approach used to develop a co r-pus of 5,553 tweets manually annotated with 28 emotion categories . The corpus captures a variety of explicit and implicit emotion expressions for these 28 emotion categories , including the exa m-ples described above. 
Using this carefully curated gold standard co r-pus, w e report our preli minary efforts to train and evaluate machine learning models for emotion classification . We examin e if common machine learning techniques known to perform well in coarse -grained emotion and sentiment classific a-tion can also be applied successful ly on this set of fine -grained emotion categories . The contributions of this paper are two -fold : a) Identifying machine learning alg o rithms that b) Comparing the machine learning perfo r mance 2.1 Corpus The corpus contains 5,553 tweets and is deve l oped using small -scale content analysis. To ensure that the tweets included in the corpus are repr e sentative of the population on Twitter, we employed four samp ling strategi es: randomly sampling tweets r e-trieved using common stopwords (RA N DOM : 1450 tweets ), sa m pling u s ing to pi cal hashtags (TOPIC : 1310 tweets ), sa m pling u s ing @usernames of US Sen a tors (SEN -USER : 1493 tweets ) and sampling using @usernames of ave r-age users randomly selected from Twitter (AVG -USER : 1300 tweets ) . Tweets were sampled from the Twi t ter API and two pu b licly available d a-tasets: 1) the SemEval 2014 tweet data set (N a kov et al., 2013; Rose nthal et al. , 2014) , and 2) the 2012 US pres i dential elections data set (M o ha m-mad et al., 2014) . The proportion of tweets from each of the four samples is roughly balanced.

The corpus was annotated by g raduate stude nts who were interested in undertaking the task as part of a class project (e.g., Natural Language Pr o-cessing course) or to gain research experience in content an alysis (e.g., i ndependent study) . A total of 18 annotators worked on the annotation task over a period of ten months. Annotators were fir st instructed to annotate the v a lence of a tweet. Em o-tion valence can be pos i tive, negative or neutral. Positive emotions are e voked by events causing one to express pleasure ( e.g., happy, relaxed, fasc i-nation, love) while neg a tive emotions are evoked by events causing one to e x press displeasure (e.g., anger, fear, sad) . Em o tions that were neither pos i-tive nor negative were co n sid ered to be neutral (e.g. surprise). Valence was useful to help annot a-tors distinguish between tweets that contained emotion and those that did not.

To uncover a set of emotion categories from the tweets, we used an adapted grounded theory a p-proach develop ed by Glaser &amp; Strauss ( 1967) for the purpose of building theory that emerges from the data. Using this approach, annotat ors were not given a predefined set of labels for emotion categ o-ry. Instead, the emotion categories were formed inductively based on the emotion tags or labels suggested by annotators. Ann otators were required to identify emotion tag when valence for a twe et was labeled as either  X  X ositive X ,  X  X egative X  or  X  X eutral X . For emotion tag, annotators were i n-structed to assign an emotion label that best d e-scribe d the overall e motion expressed in a tweet . In cases where a tweet contained multiple emotions, annotator s were asked to first identify the primary emotion expressed in the tweet, and then also i n-clude the other emotions ob served .

The annotation task was conducted in an iter a-tive fashion. In the first iteration, also referred to as the training round, all ann otators annotated the same sample of 300 tweets from the SEN -USER sample. Annotators were expected to achieve at least 70% pairwise agreement for valence with the primary researcher in order to move forward. The annotators achieved a mean pairwise agreemen t of 82% with the researcher. Upon passing the training round, annotators were assigned to annotate at least 1,000 tweets from one of the four samples (RA N-DOM, TOPIC, AVG -USER or SEN -USER) in subsequent iterations. Every week, a nnotators worked inde pendent ly on annotating a subset of 150  X  200 tweets but met with the researcher in groups to discuss disagreements , and 100% agre e-ment for valence and emotion tag was achieved a f-ter discussion . In these weekly meetings, the r e-searcher also facilitated the discus sions among a n-notators working on the same sample to merge, remove, and refine suggested emotion tags. 
Annotators suggested a total 246 distinct em o-tion tags. To group the emotion tags into categ o-ries, a nnotators were asked to perform a card sor t-ing exerc ise in different teams to group emotion tags that are variants of the same root word or s e-mantically similar into the same category. Annot a-tors were divided into 5 teams, and each team r e-ceived a pack of 1 X  x 5 X  cards containing only the emo tion tags used by the all members in their r e-spective teams. This task organized the emotion tags into 48 emotion categories. 
To refine the emotion categories, we collected pleasure and arousal ratings for each emotion cat e-gory name from Amazon Mechanical Turk (AMT). Ba sed on 76 usable responses, the emotion categ o-ry names were mapped on a two -dimensional plot. Emotion categories that were closely clustered t o-gether on the plot and semantically related to one another were further merged r e sulting in a final set of 28 emo tion categories. Finally, all emotion ca t-egory labels in the corpus were systematically r e-placed by the app ropriate 28 emotion category l a-bels . Overall, annotators achieved Kri p pendorff X  X   X  = 0.61 for valence and  X  = 0.50 for the set of 28 emotion categori es. Each tweet was assigned gold labels for valence and emotion cat e gory. 2.2 Emotion Distributions This section describes the distribution of gold l a-bels for three emotion class stru ctures: 1) em o-tion/non -emotion, 2) valence , and 3) 28 emotion ca te g o ries . As shown in Table 1 , the o verall distr i-b u tion between tweets containing emo tion and those that do not is roughly balanced . S lig htly over half of the tweets (53 %) c on tain emotion.

The class distribution becomes more unbalanced with the finer -grained emotion classes, valence ( Table 2 ) and 28 emotion categories ( Table 3 ). For valence, 33% of the tweets containing emotion are positive, 13% are negative and only 3% are ne u-tral. Emotion classes become even sparser with the 28 emotion categories. The most frequent cat e gory is ha ppiness (13%) while the least frequent categ o-ry is jealousy (0.09%). 2.3 Machine Learning Experiments We ran a series of experiments to identify a set of machine learning algorithms that generally pe r-form well for this task. Four machine learning a l-gorithms we re found to perform well in this pro b-lem space: support vector machines (SVM) (Alm et al. , 2005; Aman &amp; Szpakowicz, 2007; Brooks et al., 201 3; Cherry et al. , 2012) , Bayesian networks (Sohn et al., 2012; Strapparava &amp; Mihalcea, 2008) , decision trees (Hasan et al. , 2014) , and k -nearest neighbor (KNN) (Ha san et al., 2014; Holzman &amp; Pottenger, 2003) . The features were held constant across different classifiers in the candidate set. As a starting point, a unigram (i.e., bag -of -words) model, which has been shown to work re a sonably well for t ext classification in sentiment analysis (P ang et al., 2002; Salvetti et al. , 2006) , was ch o-sen. Although limited, t he unigram bag -of -words features captures not only emotion words but all words in a tweet , thus incre asing the likelihood of the classifiers to handle the figurative e x pressions of emotion. 
We tokenized the text in the corpus and extrac t-ed all unique terms as features. We created a cu s-tom tokenizer to better handle elements that are common in tweets. In particular, the tokenizer re c-ognizes emoticons, emojis, URLs and HTML e n-coding. The tokenizer also handles common a b-breviations and contractions. Text was encoded in UTF -8 in order to preserve the emojis. We then evaluated the effect of case normalization (i.e, lowercasing), stemming, and a minimum word fr e-quency threshold ( f = 1, 3, 5 and 10) as a means to reduce the number of features. Classifiers were evaluated using 10 -fold cross validation.

To make experiments more manageable, we frame the problem as a multi -class classification task . Each tweet was assigned to only one emotion label. For tweets with multiple labels, only the primary label (i.e., first label) was assigned to the tweet, and the other labels were ignored. We ca r-ried out two sets of exper iments. First, we created one single classifier ( multi -class -single : one versus one ) to distinguish between 29 classes (i.e., 28 emotion categories and no emotion ). Second, we ran experiments using Weka X  X  MultiClassClassif i-er , a meta -classifier that mapped a multi -class d a-taset into multiple two -class classifiers ( multi -class -binary : one versus all ), one for each emotion and one for no emotion , thus resulting in a setup with 29 binary classifiers in total. Unfortunately, the multi -class -binary setup was not designed to handle instances with multiple labels but it offered a straightforward implementation of multiple bin a-ry classifications for preliminary analysis. About 92% of the corpus contained instances with only a single label so overall classification p erformance is expected to be close to that of a multi -label class i-fier. 3.1 Machine Learning Algorithms We fou nd that the use of stemming, case normal i-zation and applying a word frequency threshold of 3 produced consistently good resul ts. 
Based on the micro -averaged F1 shown in Table 4 , the two machine learning algorithms that yielded the best performance were Sequential Minimal O p-timization (SMO), an algorithm for training SVM (Platt, 1998) and Bayesian Networks (BayesNet) (Bouckaert, 1967) . T he per formance ranking di f-fers slightly betwee n the four machine learning a l-gorithms across t he two experimental setups with SVM being the top performing classifier in multi -class -single while Baye sNet in multi -class -binary . A more in -depth analysis of the best performing classifier for each emotion category al so shows that BayesNet and S VM yield the best perfo r-mance for over half of the emotion categ o ries. 3.2 Comparison with Baselines T hree baselines are first established as the basis of comparison for all other classifiers .  X  Majority -class baseline: The majority -class  X  Random baseline: The random baseline cla s- X  OneR: OneR is a simple classifier that uses a 
We compare the S VM and BayesNet class i fiers to the three baselines as shown in Table 5 . In terms of accuracy, S VM and BayesNet outper form the majority -class and random baselines in both multi -class -single and mu lti -class -binary . BayesNet co r-rectly pre dicts roughly 60 % of the i n stances while S VM correctly predicts roughly 50%. In te rms of F1, S VM and BayesNet exc eed the perfo r mance of all the three baselines. 3.3 Levels of Granularity Table 6 shows the performance of classifiers for fine -grained versus coarser -grained class stru c tures across three lev els of granularity : 1) emotion pre s-ence/absence (2 classes), 2) emotion valence (5 classes) and, 3) emotion category (28 classes). SVM and Ba yesNet perform significantly better than the majority -class baseline across all three levels of granularity using a flat classification a p-proach. The majority class for valence and emotion ca t e gory is none .
Comparing across the three levels of granularity, better performance is observed when there are fewer classes . For example, a classifier tra ined to distinguish between 2 classes ( emotion and none ) yields higher performance than a class i fier trained to distinguish between 29 classes (28 emotion ca t-egories and none ) . T he drop in classifier perfo r-mance from coarser to finer levels of granularity is gradual. Note that the performance of a classifier trained to classify 29 classes is not a great deal worse than a classifier dealing with fewer classes (2 or 5). A clo ser anal y sis of the F1 per emotion category shows that the classifiers are able to c o r-re ctly predict some cat e gories better than the ot h-ers. For instance , SVM and BayesNet ach ieve F1 grea t er than 0.7 for grat i tude . The performance measures in Table 6 a re micro averages across all classes . T he perfo r mance results r eported here are intended to show a realistic assessment of machine learning perfo r mance in classifying the 28 emotion categ o ries that emerged from the open coding task. W e included even the poor performing categories in the comp u tation of the micro avera g es. Automatic fine -grained emotion detection is a cha l lenging task but we have demonstrated that it is feasible to train a classifier to perform decently well in classifying as many as 28 emotion categ o-ries . Our 28 emotion catego r ies is an extension to the six to eight emotion categories commonly -used in the state -of -the -art (Alm et al., 2005; Aman &amp; Szpakowicz, 2007; Mohammad, 2012) . S ome of the 28 emotion categories overlap with those found in existing emotion theories such as Plutchik X  X  ( 1962) 24 categories on the wheel of emotion and Shaver et al.  X  X  (2001) tree -structured list of em o-tions. Existing emotion theories in psychology are not developed specifically based on emotions e x-pressed in text. Therefore, our emotion categori es offer a more fitting framework for the study of emotion in text.

Existing classifiers achieve only moder ate pe r-formance in detecting emotion s in tweets even those trained with a significant amount of data co l-lected using distant supervision (Mohammad, 2 012; Ro b erts et al. , 2012; Wang et al., 2012) . Our preliminary classifiers trained with less data show results that are comparable to existing coarse -grained classifi ers. R esults from o ur preliminary machine learning experi ments conclude that S VM and Baye sNet classi fiers produce consistently good performance for fine -grained emotion classific a-tion . Therefore, we plan to continue our machine learning experiment with more sophisticated fe a-ture selection strategies, ensemble methods and more balanced training data using both S VM and BayesNet.

There is no stark difference in classifier perfo r-mance between fine -grained and coarse -grained emotion classes. Classifiers perform poorly for a handful of emotion categories with very low fr e-quenc y. We will need to gener ate more positive examples for these classes to improve classifier performance. We plan to add another 10,000 ann o-tated tweets in the corpus to increase the size of training and evaluation data. We will make the emotion corpus available in the f u ture.

We acknowledge that the multi -class setup may not be the mos t suitable implementation of this classification task given that the corpus contains tweets annotated with multiple emotion categories. We chose the multi -class setup to simplify the classification task and make the machine l earning experiments more manageable in this preliminary stage . We plan to eva lu ate t he e f fe c tiv e ness of these alg o rithms with mu l ti -label class i fiers in our future work . We thank the annotators who volunteered in pe r-forming the a nnotation tas k. We are grateful to Dr. Elizabeth D. L iddy for her insights in the study .
