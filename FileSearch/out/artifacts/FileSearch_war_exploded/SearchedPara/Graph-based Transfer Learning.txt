 Transfer learning is the task of leveraging the information from labeled examples in some domains to predict the labels for examples in another domain. It finds abundant practi-cal applications, such as sentiment prediction, image clas-sification and network intrusion detection. In this paper, we propose a graph-based transfer learning framework. It propagates the label information from the source domain to the target domain via the example-feature-example tripar-tite graph, and puts more emphasis on the labeled exam-ples from the target domain via the example-example bi-partite graph. Our framework is semi-supervised and non-parametric in nature and thus more flexible. We also develop an iterative algorithm so that our framework is scalable to large-scale applications. It enjoys the theoretical property of convergence. Compared with existing transfer learning methods, the proposed framework propagates the label in-formation to both the features irrelevant to the source do-main and the unlabeled examples in the target domain via the common features in a princi pled way. Experimental re-sults on 3 real data sets demonstrate the effectiveness of our algorithm.
 H.2.8 [ Database Management ]: Database Applications  X  Data Mining Algorithm, experimentation Transfer learning, graph-based
Transfer learning refers to the process of leveraging the information from a source domain to train a better classifier for a target domain. Typically there are plenty of labeled examples in the source domain, whereas very few or no la-beled examples in the target domain. Transfer learning is of key importance in many real applications. For example, in sentiment analysis, we may have many labeled movie re-views (labels obtained according to the movie ratings), but we are interested in analyzing the polarity of reviews about an electronic product [4]; in face recognition, we have many training images under certain lightening and occlusion con-ditions based on which a model is trained, but practically the model will be used under totally different conditions [14].
Generally speaking, transfer learning can follow one of the following three scenarios: 1. The source domain and the target domain have the 2. The source domain and the target domain have the 3. The source domain and the target domain have dif-In this paper, we focus on the second scenario, which some-times is formalized as the problem that the training set and the test set have different feature distribution [7].
The main contribution of this paper is to develop a graph-based transfer learning frame work based on separate con-structions of a tripartite graph (labeled examples -features -unlabeled examples) and a bipartite graph (labeled exam-ples -unlabeled examples). By propagating the label in-formation from labeled examples (mostly from the source domain) to unlabeled examples (from the target domain) via the features on the tripartite graph, and by imposing domain related constraints on the bipartite graph, we are able to learn a classification function that takes values on all the unlabeled examples in the target domain. Finally, these examples are labeled according to the sign of the function values. The proposed framework is semi-supervised since it makes use of unlabeled examples to help propagate the label information. Furthermore, in the second transfer learning scenario (which we are interested in), the labeling functions in different domains may be closely related to the feature distribution; thus unlabeled examples are helpful in con-structing the classifiers. How ever, our framework is differ-ent from traditional semi-supervised learning due to the fact that labeled examples from different domains are treated dif-ferently in order to construct an accurate classifier in the tar-get domain, whereas in traditional semi-supervised learning, all the labeled examples are treated in the same way. The framework is also non-parametric in nature, which makes it more flexible compared with parametric models.

The proposed transfer learning framework is fundamen-tally different from existing graph-based methods. For ex-ample, the authors of [9] proposed a locally weighted ensem-ble framework to combine multiple models for transfer learn-ing, where the weights of different models are approximated using a graph-based approach; the authors of [12] proposed a semi-supervised multi-tas k learning framework, where t -step transition probabilities in a Markov random walk are incor-porated into the neighborhood-conditional likelihood func-tion to find the optimal parameters. Generally speaking, none of these methods try to propagate the label informa-tion to the features irrelevant to the source domain and the unlabeled examples in the target domain via the common features. Some non-graph-based methods try to address this problem in an ad-hoc way, such as [4], whereas our paper provides a principled way to do the propagation.

The rest of the paper is organized as follows. Firstly, Sec-tion 2 introduces the tripartit e graph and a simple iterative algorithm for transfer learning based on this graph. Then in Section 3, we present the graph-based transfer learning framework and associate it with the iterative algorithm from Section 2. Experimental results are shown in Section 4, fol-lowed by some discussion. Section 5 introduces related work. Finally, we conclude the paper in Section 6.
In this section, we first introduce the tripartite graph that propagates the label information from the source domain to the target domain via the features. Using this graph, we can obtain a classification function that takes values on all the unlabeled examples from the target domain. Then we present an iterative algorithm to find the classification function efficiently.
Let X S denote the set of examples from the source do-main, i.e. X S = { x S 1 ,...,x S m } X  R d ,where m is the number of examples from the source domain, and d is the dimen-sionality of the feature space. Let Y S denote the labels of these examples, i.e. Y S = { y S 1 ,...,y S m } X  X  X  1 , 1 where y S i is the class label of x S i ,1  X  i  X  m . Similarly, for the target domain, let X T denote the set of examples, i.e. ples from the target domain. Among these examples, only the first n examples are labeled, i.e. Y T = { y T 1 ,...,y { X  1 , 1 } n ,where y T i is the class label of x T i ,1  X  i Here 0  X  1, i.e. only a small fraction of the examples in the target domain are labeled, and = 0 corresponds to no labeled examples in the target domain. Our goal is to find a classification function for all the unlabeled examples in with a small error rate.
Let G (3) = { V (3) ,E (3) } denote the undirected tripartite graph, where V (3) is the set of nodes in the graph, and E is the set of weighted edges. V (3) consists of three types of nodes: the labeled nodes, i.e. the nodes that correspond to the labeled examples (most of them are from the source domain); the feature nodes, i.e. the nodes that correspond to the features; and the unlabeled nodes, i.e. the nodes that correspond to the unlabeled examples from the target domain. Both the labeled nodes and the unlabeled nodes are connected to the feature nodes, but the labeled nodes are not connected to the unlabeled nodes, and the nodes of the same type are not connected either. Furthermore, there is an edge between a labeled (unlabeled) node and a feature node if and only if the corresponding example has that feature, i.e. x S i,j =0( x T i,j =0),where x S i,j ( x the j th feature component of x S i ( x T i ), and the edge weight is set to x S i,j ( x T i,j ). Here we assume that the edge weights are non-negative. This is true in many applications, such as document analysis where each feature corresponds to a unique word and the edge weight is binary or equal to the tfidf value. In a general setting, this may not be the case. However, we could perform a linear transformation to the features and make them non-negative.

Fig. 1a shows an example of the tripartite graph. The diamond-shaped nodes correspond to the feature nodes, the lighter circle nodes correspond to the examples from the source domain, and the darker circle nodes correspond to the examples from the target domain. Notice that the la-beled nodes are on the left hand side, the feature nodes are in the middle, and the unlabeled nodes are on the right hand side. The intuition of the graph can be explained as follows. Consider sentiment classification in different do-mains as an example. Each of the diamond-shaped nodes in Fig. 1a corresponds to a unique word; the lighter circle nodes correspond to labeled movie reviews; and the darker circle nodes correspond to product reviews that we are in-terested in. The labeled reviews on the left hand side of Fig. 1a propagate their label information to the unlabeled product reviews via the feature nodes. Notice that each of the two domains may have some unique words that never occur in the other domain. For example, the word  X  X ctor X  often occurs in a movie review, but may never occur in a product review; similarly, the word  X  X olyethylene X  may oc-cur in a product review, but is never seen in a movie review. Based on this graph structure, the label information can be propagated to the domain-specific words, i.e. the words ir-relevant to the movie reviews, which will help classify the unlabeled product reviews.

Given the tripartite graph, we define affinity matrix A (3) which is ( m + n + d )  X  ( m + n + d ). The first m + n rows (columns) correspond to the labeled nodes, the next n  X  n rows (columns) correspond to the unlabeled nodes, and the remaining d rows (columns) correspond to the feature nodes. Therefore, A (3) has the following block structure where 0 a  X  b is an a  X  b 0matrix, A (3 , 1) and A (3 , 2) sub-matrices of A (3) ,and(  X  ) T is the transpose of a matrix. j th column of A (3 , 1) ( A (3 , 2) ). Based on the discussion above, A A (3) are non-negative. Furthermore, define diagonal matrix D (3) ,whichis( m + n + d )  X  ( m + n + d ). Its diagonal A i,j denote the element in the i A (3) . Similar as A (3) , D (3) has the following block structure where D (3 , 1) , D (3 , 2) and D (3 , 3) are diagonal matrices whose diagonal elements are equal to the row sums of A (3 , 1) , A normalized affinity matrix S (3) =( D (3) )  X  1 / 2 A (3) which also has the following block structure ( D of S (3) are also non-negative.
Given the tripartite graph and the corresponding affin-ity matrix, we can define three functions f L , f F and f which take values on the labeled nodes, the feature nodes, and the unlabeled nodes respectively. Note that the function value of f U will be used to classify the unlabeled examples in the target domain, and the function value of f F can be used to infer the polarity of the features. Similarly, define three vectors y L , y F and y U , whose lengths are equal to the number of labeled nodes m + n , the number of fea-ture nodes d , and the number of unlabeled nodes n  X  n respectively. The elements of y L are set to be the class label of the corresponding labeled example, whereas the ele-ments of y F and y U could reflect our prior knowledge about the polarity of the features and the unlabeled examples, or simply 0 if such information is not available. For the sake of notation simplicity, let f =[( f L ) T , ( f U ) T , ( f y =[( y L ) T , ( y U ) T , ( y F ) T ] T .

To find the classification function with a low error rate, we propose to minimize the following objective function, which is motivated by [25].
 Q ( f )= 1 where  X  is a small positive parameter, I a  X  b is an a  X  b iden-tity matrix, and f i and y i are the i th element of f and y respectively.

This objective function can be interpreted as follows. The the label smoothness of f . In other words, neighboring nodes on the graph should have similar f values. The second term,  X  f  X  y 2 , measures the consistency of f with the label infor-mation and the prior knowledge encoded in y . By minimiz-ing Q 1 , we hope to obtain a smooth classification function f U with a small error rate.

In our implementation, we fix f L = y L .Inthisway,we can make better use of the label information in y L .This modification distinguishes our method from the manifold ranking algorithm proposed in [25], where each element of f needs to be optimized. Minimizing Q 1 with the above constraint, we have the following lemma.

Lemma 1. If f L = y L , Q 1 is minimized at where  X  = 1 1+  X  .
 Proof. Replacing f L with y L , Q 1 becomes Therefore,  X  X  F =2 f
Notice that in Lemma 1, in order to get f U  X  and f F  X  , we need to solve matrix inversions. This is computationally expensive especially when the number of unlabeled examples in
X T or the number of features is very large. To address this problem, we propose the following iteration steps to obtain the optimal solutions. f where f U ( t )and f F ( t )denote f U and f F at the t th tion. The two equations can be interpreted as follows. Based on Equation 3, if an example has many positive (negative) features or it is believed to be positive (negative) a priori, its function value would be large (small), indicating that it is a positive (negative) example. Based on Equation 4, if a feature is contained in many positive (negative) labeled examples, or it is shared by many unlabeled examples with large (small) function values, or it is believed to be positive (negative) a priori, its function value would be large (small). In this way, the label information is gradually propagated to the unlabeled examples in the target domain and the features irrelevant to the source domain via the common features on the tripartite graph.

The following theorem guarantees the convergence of the iteration steps.

Theorem 1. When t goes to infinity, f U ( t ) converges to f  X  and f F ( t ) converges to f F  X  .

Proof. According to Equations 3 and 4 For the sake of simplicity, let V = S (3 , 2) ( S (3 , 2) (1 we assume that t is an even number. Therefore, the above equation can be written as follows. f ( t )=  X  2 Vf U ( t  X  2) + v =(  X  2 V ) t 2 f U (0) + ( where f U (0) is the initial value of f U .Since  X  = 1 1+ 0 &lt; X &lt; 1. Therefore, if the eigenvalues of V are in [-1,1], we have Hence, if t is an even number, With respect to the eigenvalues of V , we have the following lemma.
 Lemma 2. The eigenvalues of V are in [-1,1].

Proof. Notice that V = S (3 , 2) ( S (3 , 2) ) T =( D (3 , ( D A V Furthermore, it is easy to see that d j =1 V (1) i,j =1,  X  n  X  n ,and n  X  n V i,j are the elements of V the j th column. Therefore, for the i th row of V (1) ( V  X  1  X  i  X  n  X  n ,itsrowsumis According to Perron-Frobenius theorem [18], since the el-ements of V (1) ( V (2) ) T are non-negative, the spectral ra-dius of V (1) ( V (2) ) T is 1. Furthermore, since V is similar to V (1) ( V (2) ) T , its spectral radius is also 1. Therefore, the eigenvalues of V are in [-1,1].
 Therefore, using Lemma 2, we have shown that if t is an even number, as t goes to infinity, f U ( t )convergesto f U  X  conclusion also holds when t is an odd number. Finally, applying similar techniques to f F , we can show that as t goes to infinity, f F ( t )convergesto f F  X  .

Comparing the above iterative steps with Equations 1 and 2, we can see that they avoid solving matrix inversions directly. In our experiments, the number of iteration steps until convergence is always less than 30. Therefore, these iterative steps are an efficient alternative to Equations 1 and 2.
 Based on Equations 3 and 4, we design the following TRITER (TRIpartite-graph-based TransfER learning) al-gorithm to minimize Q 1 , which is shown in Algorithm 1. It works as follows. First, we set y L ( f L ), y U and y F according to the label information or our prior knowledge. f U (0) and f (0) are initialized to y U and y F respectively. Next, we update f U and f F according to Equations 3 and 4. Finally, we classify all the unlabeled examples in X T according to the corresponding elements in f U .
 Algorithm 1 TRITER Algorithm for Transfer Learning Input: The set of examples from the source domain X S and Output: The labels of all the unlabeled examples in X T . 1: Set y L ( f L ) according to the label information; set y 2: for i =1: t do 3: Calculate f U ( i )and f F ( i )accordingtoEquations3 4: end for 5: for i =( n +1): n do 6: If the function value of f U ( t )at x T i is positive, y 7: end for
In Section 2, we have introduced a tripartite graph that connects the examples from the source domain and the tar-get domain with the features, and have designed the TRITER algorithm to minimize the objective function Q 1 efficiently. Although simple and straight-forward, Q 1 isnotbestsuited for transfer learning. This is because the label information from the source domain and the target domain is propagated in the same way. If the labeled examples from the source domain dominate the labeled nodes, the label information of the small number of labeled examples from the target do-main would be flooded, and the resulting classification func-tion for the target domain may be largely biased. In other words, since our goal is to construct an accurate classifier in the target domain, the labeled examples from the same do-main should be more important than the labeled examples from different domains.

To address this problem, in this section, we propose the graph-based transfer learning f ramework. In this framework, in addition to the tripartite graph, we also design a bipar-tite graph to make better use of the labeled examples from the target domain. Based on the two graphs, we present objective function Q 2 and the optimal solutions. Further-more, under certain conditions, the solutions to Q 2 can be obtained by minimizing a slightly modified version of Q 1 the TRITER algorithm.
Let G (2) = { V (2) ,E (2) } denote the undirected bipartite graph, where V (2) is the set of nodes in the graph, and E is the set of weighted edges. V (2) consists of two types of nodes: the labeled nodes which correspond to the labeled examples from both the source domain (majority) and the target domain (minority); the unlabeled nodes which cor-respond to the unlabeled examples from the target domain. Each labeled node is connected to each unlabeled node, with the edge weight indicating the domain related similarity be-tween the two examples, whereas the same type of nodes are not connected.

Fig. 1b shows an example of the bipartite graph which has the same labeled and unlabeled nodes as in Fig. 1a. Sim-ilarly, the lighter circle nodes correspond to the examples from the source domain, and the darker circle nodes corre-spond to the examples from the target domain. The labeled nodes on the left hand side are connected to each unlabeled node on the right hand side. Again take sentiment classi-fication in different domains as an example. The labeled nodes correspond to all the labeled reviews, most of which are movie reviews, and the unlabeled nodes correspond to all the unlabeled product reviews. The edge weights are set to reflect the domain related similarity between two reviews. Therefore, if two reviews are both product reviews, one la-beled and one unlabeled, their edge weight would be large; whereas if two reviews are from different domains, the movie review labeled and the product review unlabeled, their edge weight would be small. In this way, we hope to make better use of the labeled product reviews to construct the classifi-cation function for the unlabeled product reviews.
Let A (2) denote the affinity matrix for the bipartite graph, which is ( m + n )  X  ( m + n ). The first m + n rows (columns) correspond to the labeled nodes, and the remaining n  X  n rows (columns) correspond to the unlabeled nodes. Accord-ing to the structure of the bipartite graph, A (2) has the following form.
 where A (2 , 1) is the sub-matrix of A (2) . Note that the ele-ments of A (2) are set to be non-negative. Let D (2) denote the ( m + n )  X  ( m + n ) diagonal matrix, the i th diagonal element of which is defined D (2) i = m + n j =1 A (2) i,j , i =1 ,...,m + n , where A (2) i,j is the element of A (2) in the i th row and the j th column. Similar as A (2) , D (2) has the following block structure.
 where D (2 , 1) and D (2 , 2) are diagonal matrices whose diago-nal elements are equal to the row sums and the column sums of A (2 , 1) respectively. Finally, let S (2) denote the normalized has the following block structure.

In Subsection 2.2, we introduced a tripartite graph which propagates the label information from the labeled nodes to the unlabeled nodes via the feature nodes; and in Subsec-tion 3.1, we introduced a bipartite graph which puts high weights on the edges connecting examples from the same domain and low weights on the edges connecting examples from different domains. In this section, we combine the two graphs to design objective function Q 2 . By minimizing Q we can obtain a smooth classification function for the unla-beled examples in the target domain which relies more on the labeled examples from the target domain than on those from the source domain.
 For the sake of simplicity, define g =[( f L ) T , ( f U ) easy to see that g = Bf ,where B =[ I ( m + n )  X  ( m + n Thus the objective function Q 2 canbewrittenasfollows.
Q 2 ( f )= 1 + 1 where  X  and  X  are two positive parameters. Similar as in Q measures the label smoothness of f on the tripartite graph; the label smoothness of f on the bipartite graph; and the third term,  X  f  X  y 2 , measures the consistency of f with the label information and the prior knowledge. It should be pointed out that the first two terms in Q 2 can be com-bined mathematically; however, the two graphs can not be combined due to the normalization process.

Based on Q 2 , we can claim that our method is differ-ent from semi-supervised learning, which treats the labeled examples from different domains in the same way. In our method, by imposing the label smoothness constraint on the bipartite graph, we can see that the labeled examples from the target domain have more impact on the unlabeled examples from the same domain than the labeled examples from the source domain. In the next section, we will com-pare our method with a semi-supervised learning method experimentally.

Similar as before, we fix f L = y L , and minimize Q 2 with respect to f U and f F . The solutions can be obtained by the following lemma.
Lemma 3. If f L = y L , Q 2 is minimized at  X  f  X  =((  X  +  X  +  X  ) I (  X  Proof. Replacing f L with y L , Q 2 becomes Therefore,
In Equation 5, if we ignore the matrix inversion term in thefront,wecanseethat  X  f U  X  gets the label information from the labeled nodes through the following two terms: the tripartite graph and the bipartite graph respectively. Recall that y L is defined on the labeled nodes from both the source domain and the target domain. In particular, if a labeled node is from the target domain, its corresponding row in S 2 , 1 would have large values, and it will make a big labeled nodes from the source domain, whose corresponding rows in S 2 , 1 have small values, and their contribution to would be small as well.

Similar to objective function Q 1 , we can also design an iterative algorithm to find the solutions of Q 2 . However, in the following, we focus on the relationship between Q 1 and Q , and will introduce an iterative algorithm based on the TRITER algorithm to solve Q 2 .

Comparing Equations 1 with 5, we can see that they are very similar to each other. The following theorem builds a connection between objective functions Q 1 and Q 2 .
Theorem 2. If f L = y L ,then  X  f U  X  can be obtained by minimizing Q 1 with the following parametrization
Proof. Replacing  X  , y L , y U and y F with  X  , y L , y U y F respectively in Equations 1, we get Equations 5.
The most significant difference between the parameter set-tings in Theorem 2 and the original settings is in the defini-tion of y U .Thatis, y U consists of two parts, one from its own prior information, which is in proportion to  X y U ,and the other from the label information of the labeled exam-ples, which is in proportion to  X  ( S (2 , 1) ) T y L . Note that the second part is obtained via the bipartite graph, and it en-codes the domain information. In other words, incorporating the bipartite graph into the transfer learning framework is equivalent to working with the tripartite graph alone, with a domain specific prior for the unlabeled examples in the target domain and slightly modified versions of  X  and y F
Finally, to minimize Q 2 , we can simply apply the TRITER algorithm with the parameter settings specified in Theo-rem 2, which usually converges within 30 iteration steps.
In this section, we present some experimental results, and compare the proposed graph-based transfer learning frame-work with state-of-the-art techniques.
To demonstrate the performance of the proposed graph-based transfer learning frame work, we perform experiments in the following 3 areas. 1. Sentiment classification (SC). In this area, we use the 2. Document classification (DC). In this area, we use the 3. Intrusion detection (ID). In this area, we use the KDD
The details of the transfer learning tasks are summarized in Table 1. Notice that in SC and DC, we tried both binary features and tfidf features. It turns out that binary features lead to better performance. Therefore, we only report the experimental results with the binary features here. Note that the features in ID are not binary.

In our proposed transfer learn ing framework, the bipartite graph is constructed as follows. A (2 , 1) is a linear combina-tion of two matrices. The first matrix is based on domain information, i.e. its element is set to 1 iff the corresponding labeled and unlabeled examples are both from the target domain, and it is set to 0 otherwise. The second matrix features with an unlabeled example, the corresponding ele-ment in this matrix is large. Note that this is only one way of constructing the bipartite graph with domain information. Exploring the optimal bipartite graph for transfer learning is beyond the scope of this paper.

We compare our method with the following methods. 1. Learning from the target domain only, which is de-2. Learning from the source domain only, which is de-3. Learning from both the source domain and the target 4. Semi-supervised learning, denoted semi-supervised . 5. The transfer learning toolkit developed by UC Berke-6. The boosting-based transfer learning method [7], which
For the graph-based transfer learning framework, we set  X  =0 . 01, which is consistent with [25], y F =0,and y U =0 in all the experiments. For  X  and  X  , we test their impact on the performance using SC, which is shown in Fig. 2. From this figure, we can see that the performance of our method is quite stable within a wide range of  X  and  X  . Therefore, in the following experiments, we set  X  =5and  X  =1. Figure 2: Impact of  X  and  X  on the performance of the proposed method.

Fig. 3 to Fig. 9 compare the proposed graph-based trans-fer learning framework with the baseline methods on the 7 transfer learning tasks. In these figures, the x-axis is the number of labeled examples from the target domain, and the y-axis is the average test error in the target domain over 20 runs (labeled examples from the target domain are ran-domly picked in each run). The error bars are also shown in these figures. Based on these results, we have the following observations. First of all, it is easy to see that our graph-based method is the best of the 7 methods in all the tasks in terms of the average error rate. Second, the graph-based method is very stable in terms of the small error bars, especially compared with target only . This is consistent with our intuition since target only totally ignores the source domain, and only uses the label information from the target domain to
Figure 5: Comparison on the second task of DC.
Figure 6: Comparison on the third task of DC. construct the classification function. When the number of labeled examples from the target domain is small, its per-formance varies a lot depending on the specific labeled ex-amples. In contrast, the graph-based method considers the label information from both the source domain and the tar-get domain, therefore, it is not very sensitive to the specific labeled examples from the target domain. Third, the perfor-mance of semi-supervised is always much worse than our method. This is because in all our experiments, the number of labeled examples from the target domain is much smaller than that from the source domain, which is quite common in practise. Therefore, with semi-supervised , the labeled examples from the target domain is flooded by those from the source domain, and the performance is not satisfactory. Fourth, in most of the experiments, the average performance
Figure 8: Comparison on the second task of ID. of the graph-based method and target only is getting closer as we increase the number of labeled examples from the tar-get domain. This is because with the graph-based method, the labeled examples from the target domain have more im-pact on the classification function than those from the source domain. As the number of labeled examples from the target domain increases, their impact tends to dominate. So the performance of the graph-based method and target only will get closer. Finally, in some experiments, such as Fig. 4 and Fig. 6, the gap between the graph-based method and source+target is getter larger. This is reasonable since in source+target , we are combining the source domain and the target domain in a naive way. So the performance gain caused by more labeled examples from the target domain is not as significant as the graph-based method.
There has been significant amount of work on transfer learning in machine learning research. One of the early at-tempts aims to achieve better generalization performance by jointly modeling multiple related learning tasks, and trans-ferring information among them, i.e. multi-task learning [3, 5, 19]. It usually tackles the problem where the feature space and the feature distribution P ( x ) are identical whereas the labeling functions are different. Further developments in the area include combining labeled data from the source do-main with labeled or unlabeled data from the target domain, which leads to transfer learning methods for k -nearest neigh-bor [19], support vector machines [21], and logistic regres-sion [11]. Another line of research focuses on Bayesian lo-gistic regression with a Gaussian prior on the parameters [2, 10]. There are also specialized transfer learning techniques for certain application areas, such as adapting context-free grammar [17], speech recognition [13], and sentiment pre-diction [4].

Transfer learning is closely related to concept drifting in stream mining, in which the statistical properties of the target variable change over time. These changing proper-ties might be the class prior P ( y ), the feature distribution P ( x | y ), the decision function P ( y | x ) or a combination of all. Multiple approaches have been developed, such as en-semble approaches [20], co-clustering [6], and local structure map [9]. Transfer learning is also relevant to sample bias correction, which is mostly concerned with distinct training distribution P ( x |  X  ) and testing distribution P ( x | known parameters  X  and  X  . Several bias correction methods have been developed based on estimating the probability that an example is selected into the sample and using re-jection sampling to obtain unbiased samples of the correct distribution [23, 22, 8].

Our proposed framework is motivated by the graph-based methods for semi-supervised learning [26, 25]. In the frame-work, the tripartite graph propagates the label information from the source domain to the target domain via the fea-tures, and the bipartite graph makes better use of the label information from the target do main. This framework is fun-damentally different from previous work on transfer learning and related areas. It propagates the label information in a principled way, which is in contrast to some ad-hoc methods based on pivot features [4]; it directly associates the polarity of features with the class labels of all the examples, which is in contrast to previous graph-based methods [12, 9] that do not model this relationship with the graph structure.
In this paper, we proposed a new graph-based framework for transfer learning. It is based on both a tripartite graph and a bipartite graph. The tripartite graph consists of three types of nodes, and it propagates the label information via the features. The bipartite graph consists of two types of nodes, and it imposes the domain related smoothness con-straint between the labeled examples and the unlabeled ex-amples. Based on the two graphs, we have designed an ob-jective function Q 2 , which is a weighted combination of the label smoothness on the tripartite graph, the label smooth-ness on the bipartite graph, and the consistency with the label information and the prior knowledge. Closed form so-lutions to Q 2 have been developed. Furthermore, we have built the connection between Q 2 and the objective function Q , which is solely based on the tripartite graph. Finally, based on the above connection, we have designed an itera-tive algorithm to find the solutions to Q 2 . Different from existing transfer learning methods, the proposed framework propagates the label information to both the features irrele-vant to the source domain and the unlabeled examples from the target domain via the common features in a principled way. Experimental results on several transfer learning tasks demonstrate the superiority of the proposed framework over state-of-the-art techniques. For future work, we are inter-ested in investigating the theoretical bounds of the perfor-mance for graph-based transfer learning algorithms and the applications to large-scale data sets. [1] Kdd cup 99. In http://kdd.ics.uci.edu/databases [2] R. K. Ando and T. Zhang. A framework for learning [3] J. Baxter. A bayesian/information theoretic model of [4] J. Blitzer, M. Dredze, and F. Pereira. Biographies, [5] R. Caruana. Multitask learning. In Machine Learning , [6] W. Dai, G.-R. Xue, Q. Yang, and Y. Yu. Co-clustering [7] W. Dai, Q. Yang, G.-R. Xue, and Y. Yu. Boosting for [8] W. Fan, I. Davidson, B. Zadrozny, and P. S. Yu. An [9] J. Gao, W. Fan, J. Jiang, and J. Han. Knowledge [10] S.-I. Lee, V. Chatalbashev, D. Vickrey, and D. Koller. [11] X. Liao, Y. Xue, and L. Carin. Logistic regression [12] Q. Liu, X. Liao, and L. Carin. Semi-supervised [13] J. luc Gauvain and C. hui Lee. Maximum a posteriori [14] A. M. Mart  X   X nez. Recognition of partially occluded [15] B. Pang, L. Lee, and S. Vaithyanathan. Thumbs up? [16] J. Rennie. 20 newsgroups. In [17] B. Roark and M. Bacchiani. Supervised and [18] H. Roger and J. Charles. Matrix Analysis . Cambridge [19] S. Thrun. Is learning the n-th thing any easier than [20] H. Wang, W. Fan, P. S. Yu, and J. Han. Mining [21] P. Wu and T. G. Dietterich. Improving svm accuracy [22] B. Zadrozny. Learning and evaluating classifiers under [23] B. Zadrozny and C. Elkan. Learning and making [24] J. Zhang, Z. Ghahramani, and Y. Yang. Learning [25] D. Zhou, J. Weston, A. Gretton, O. Bousquet, and [26] X. Zhu, Z. Ghahramani, and J. Lafferty.

