 Feature selection is one of the key pro cedures to get a better result from the data mining process. However, it is difficult to determine the relevant feature subset before the mining procedure. At practical data mining situations, data miners often face a problem to choose the best feature subset for a given data set. If it contains irrelevant or/and redundant features, a data miner can X  X  get any satisfactory results from mining/machine learning scheme. Irrelevant features not only lead to lower performance of the results, but also preclude finding potentially existing useful knowledge. Besides, redundant features not affect the performance of classification task, but influence the readability of the mining result. To choose a relevant feature subset, data miners have to take trial-and-error testing, expertise for the given fea ture set, or/and heavy domain knowledge for the given data set.

Feature selection algorit hms (FSAs) have been developed to select a relevant feature subset automatically as a data pre-processing in a data mining process. The performance of FSA is always affected by a given data set. To keep their per-formance higher, a user often tries to ex ecute prepared FSAs to his/her dataset exhaustively. Thus a proper FSA selection is still costly work in a data mining process, and this is one of the bottle necks of data mining processes.
To above problems, we have developed a novel feature selection scheme based on constructive meta-level processing. We have developed a system to construct proper FSAs to each given data set with this scheme, which consists of de-composition of FSAs and re-construction of them. To de-compose current FSAs into functional parts called  X  X ethods X , we have analyzed currently representative FSAs. Then we have constructed the featu re selection method repository, to re-construct a proper FSA to a given data set.

After constructing the feature select ion method repository, we have imple-mented a system to choose a proper FSA to each given data set, searching possible FSAs obtained by the method repository for the best one. Taking this system, we have done a case study to evaluate the performance of FSAs on 32 UCI common data sets. As the result, th e performance of FSAs has achieved the best performance, comparing with rep resentative higher performed FSAs. After constructing a feature set to descri be each instance more correctly, we take a FSA to select an adequate feature subset for a prepared learning algorithm.
To improve classification tasks at data mining, many FSAs have been devel-oped [2, 3, 4]. As shown in the survey done by Hall [5], wrapper methods [6] such as forward selection and backward elimination have high performance with high computational costs. Besides, filter methods such as Relief [7, 8], Informa-tion Gain and FOCUS [9] can be executed more quickly with lower performance than that of wrapper methods. Some advanced wrapper methods such as CFS [10], which executes a substitute evaluator instead of a learned evaluator, have lower computational costs than wrapper methods. However, these performances are still non-practical, comparing with wrapper methods.

We also developed a novel FSA called  X  X eed Method X  [1]. Seed Method has achieved both of practical computational cost and practical performance, be-cause it improves wrapper forward select ion method, determining a proper star-ing feature subset for given feature set. With an adequate starting subset, this method can reduce the search space of 2 n feature subsets obtained by n features. To determine an adequate starting subset, the method extracts a feature subset with Relief.F and C4.5 decision tree [11] from given feature set.
 Although studies done by [6, 12, 13] have shown each way to characterize FSAs, they have never discussed any way to construct a proper FSA to a given data set. So, a data miner still selects FSA with exhaustive executions of prepared FSAs, depending on his/her expertise. Weka [14] and Yale [15] provide many feature selection components and framew orks to users. We can construct several hundred FSAs with these materials. However, they never support to choose a proper one. At the field of meta-learning, there are many studies about selective meta-learning scheme. There are two approaches as selective meta-learning. One in-cludes bagging [16] and boosting [17], combining base-level classifiers from multi-ple training data with different distributions. In these meta-learning schemes, we should select just one learning algorithm to learn base-level classifiers. The other approach includes voting, stacking [18] and cascading [19], which combines base-level classifiers from different learning algorithms. METAL [20] and IDA [21] are also selective meta-learning approach, selecting a proper learning algorithm to the given data set with a heuristic score, which is called meta-knowledge.
Constructive meta-level processing scheme [22] takes meta-learning approach, which controls objective process with meta-knowledge as shown in Fig.1. In this scheme, we construct a meta-knowledge, re presenting with method repositories. The meta-knowledge consists of information of functional parts, restrictions of combinations of each functional part, and the ways to re-construct object algo-rithms with the functional parts.
 3.1 Issues to Implement a Method Repository To build up a method repository, we should consider the following three major issues: how to de-compose prepared algorithms into functional parts, how to restrict the combinations of the functional parts, and how to re-construct a proper algorithm to a given data set.

To implement a feature selection method r epository, we have considered above issues to identify feature selection methods(FSMs) in typical FSAs. Fortunately, FSAs have a nature as a search problem on possible combinations of features, which is pointed out in some papers [6, 12, 13]. With this nature, we have been able to identify generic methods in FSAs. Then we have also identified specific FSMs, which get into each implemented functional parts 1 . At the same time, we have also defined data types which are input/output/referenced for these methods. Thus we have organized these methods into a hierarchy of FSMs and a data type hierarchy. With these hierarchies, the system constructs FSAs to a given data set, searching possible FSAs obtained by the method repository for aproperone. To implement constructive m eta-level feature selection scheme, we have to build a feature selection method repository a nd the system to construct proper FSAs to given data sets with the featur e selection method repository. 4.1 Constructing a Feature Selection Method Repository Firstly, we have identified the following four generic methods: determining initial set, evaluating attribute subset, testing a search termination of attribute subsets and attribute subset search operation. This identification is based on what FSAs can be assumed one kind of search problems. Considering the four generic meth-ods, we have analyzed representative FS As implemented in Weka[14] attribute selection package 2 . Then we have build up a feature selection method repository.
After identifying 26 specific methods from Weka, we have described restric-tions to re-construct FSAs. The restriction has defined with input data type, output data type, reference data type , pre-method and post-method for each method. With this description, we have d efined control structures with these generic four methods as shown in Fig.2. The control structure (I) corresponds ordinary that of filter approach FSAs. Besides, with the control structure (II), we can construct hybrid FSAs, which is combined wrapper and filter FSAs. Of course, we can also construct analyzed filter and wrapper FSAs with these control structure.

At the same time, we have also defined method hierarchy, articulating each method. Fig.3 shows us the method hierarchy of feature selection. Each method has been articulated with the following roles: input data type, output data type, reference data type, pre-method, and post-method. With these roles, we have also defined combinations of FSMs.
To articulate data types for input, output and reference of methods, we have also defined data type hierarchy as shown in Fig.4.
 4.2 The System to Construct a Proper FSA with a Feature To re-construct a proper FSA to given data set, the system have to search possi-ble FSAs obtained by the FSM repository for the most proper one. This process is also one of the search problems. Then we have designed the system with the fol-lowing procedures: construction, instantiation, compilation, test, and refinement. The system chooses a proper FSA with these procedures as shown in Fig.5.
Each function of procedures is described in detail as follows: Construction procedure constructs a specification of the initial feature selection algorithm, selecting each specific method at random. Instantiation procedure transforms constructed or refined specifications to the intermediate codes. Compilation procedure compiles the intermediate codes to executable codes such as com-mands for Weka. Go &amp; Test procedure executes the executable codes to the given data set to estimate the performance of FSAs. If the number of refinement doesn X  X  come to the given limitation number Refinement procedure refines specifications of executed FSAs w ith some search operations. After implementing the feature selectio n method repository and the system to construct proper FSAs to given data sets, we have done a case study to evaluate an availability of our constructive meta-level feature selection scheme.
In this case study, we have taken 32 common data sets from UCI ML reposi-tory [23], which are distributed with We ka. With the implemented feature selec-tion method repository, the system has been able to construct 292 FSAs. The system has searched specification space of possible FSAs for the best FSA to each data set with the following configuration of GA operation at  X  X efinement X  procedure: Population size. Each generation has  X  individuals.
 Selection. We take roulette selection to sel ect 60% individuals for parents. Crossover. Each pair of parents is crossed over single point, which is selected Mutation. Just one gene of selected child is mutated, selecting just one child Elite Preservation. The best individual is preserved on each generation. 5.1 The Process to Select a FSA Firstly, the system selects proper FSAs to each data set, estimating the actual performance with the performance of n -fold cross validation. The selection phase has done at  X  X o &amp; Test X  procedure in Fig.5. This selection phase has been re-peated multiple times in each construction of FSA with our system. Finally, the system output just one FSA, which has the highest  X  X valuation score X  as shown in Fig.6.
We have taken averaged predictive accuracy EstAcc ( D )of n -fold cross vali-dation from predictive accuracies acc ( evd i ) for each validation data set evd i as the following formulations: acc ( evd i ) is a percentage score from the numb er of correctly predicted instances crr ( evd i ) and size of each validation set size ( evd i ).
 According to this evaluation scores, the GA refinement searched for proper FSAs to each given data set. We have set up population size  X  =10andmax-imum generation N = 10 in this case study. So this set of GA operations has repeated maximum 10 times to each data set. Finally, the best FSA included in a final generation has been selected as output of our constructive meta-level feature selection system. 5.2 The Process of the Evaluation We have designed the process of this eva luation for representative FSAs and constructed FSAs to each data set as shown in Fig.7.

In this evaluation, we have applied each FSA to each whole data set. Then n -fold cross validation have been performed on each data set with selected feature subset. The performances of each data set Acc ( D ) have been averaged predictive accuracies acc ( vd i ) from each fold as the following formulations: Where vd i means i -th validation set of the n -fold cross validation.
We have compared the performance of ou r constructive met a-level feature selection system with the following FSAs: Whole feature set , Seed method , and Genetic Search [24]. All of them have been evaluated with the same way as shown in the evaluation phase of Fig.7. We had done wrapper forward selection, Relief.F, Seed method and  X  X enetic Sea rch X  to the data sets previously. Then the two methods were selected beca use of their higher performance. 5.3 Results and Discussions of the Evaluation Table1 shows us the accuracies from whol e feature set, subset selected by seed method, subset selected by  X  X enetic Search X  and subset selected by FSAs which constructed with our constructive meta-l evel feature selection system. Each score is the averaged accuracy calculated from 10-fold cross validation. The significance of the average for all of the data sets has tested with t-test. The comparison between the averages of our system and the other FSAs shows the statistically significant difference, where p&lt; 0 . 05 for the other FSAs.

Table1 also shows us the result of the best performances, comparing among per-formances of the FSAs. To the 17 data sets, FSAs composed by our system have achieved the best performance. To breast-c ancer, colic, hepatitis, ionosphere, iris and kr-vs-kp, whole feature set wins sel ected feature subsets, because all of the evaluated FSAs have not been able to sel ect whole feature sets. They tend to out-put smaller feature subset, because they believe in that there are some irrelevant features in the given feature set. If we had defined the control structure for filter method Fig.2, the system would have selected whole feature subset with  X  X hole set X  method in Fig.3.

To anneal, audiology, breast-w, diabetes, heart-h, letter, primary-tumor, splice and waveform-5000, FSAs composed by our system have not achieved the best performance, comparing with the other FSAs. The evaluation scores to estimate actual performances have not worked co rrectly on these cases. However, these disadvantages are not significant differences statistically.
 Fig.8 shows us the FSA composed by our system to heart-statlog data set. This algorithm consists of initial set determination with  X  X eed method X  &amp; elim-ination unique features using Factor Anal ysis result, feature subset evaluation with CFS method, backward elimination, and stopping with the number of back-tracks 3 . Although this algorithm bases on backward elimination method, the combination of methods has been never seen in any study of FSAs. As this ex-ample, our system has been also able to construct a novel FSA automatically, reconstructing feature selection methods on the repository. We present a novel meta-level feature se lection approach based on constructive meta-level processing with method repo sitories. This scheme chooses a proper FSA to the given data set, re-constructing the FSA with a FSMs repository.
To evaluate the availability of our approach, we have done an empirical ex-periment with 32 UCI common data sets. Our constructive meta-level feature selection system has significantly outper formed than representative FSAs, which have higher performance compared with the other FSAs. The result also shows that our constructive meta-level featu re selection system have been able to con-struct a proper algorithm to given feature set automatically.
As feature work, we will improve criterion to choose a proper FSA, consider-ing search time to select a proper one, e xecution time of selected FSA and its performance.

