 We are entering the age of big data. The challenges are that these data not only present in massive amount but also co-exist in heterogeneous forms including texts, hypertexts, images, graphics, videos, speeches and so forth. For exam-ple, in dealing with social network analysis, data present in network connection accompanying with users X  profiles, their comments, activities. In medical data understanding, the patients X  information usually co-exists with medical infor-mation such as diagnosis codes, demographics, laboratory tests. This deluge of data requires advanced algorithms for analyzing and making sense out of data. Machine learning provides a set of methods that can automatically discover low-dimensional structures in data which can be used for reasoning, making decision and predicting. Bayesian methods are increasingly popular in machine learning due to their resilience to over-fitting. Parametric models assume a finite number of parameters and this number needs to be fixed in advance, hence hinders its practicality. Bayesian nonparametrics, on the other hand, relax the assumption of parameter space to be infinite-dimensional, thus the model complexity, e.g., the number of mixture components, can grow with the data 1 Two fundamental building blocks in Bayesian nonparametric models are the (hierarchical) Dirichlet processes [ 14 ] and Beta processes [ 15 ]. The former is usu-ally used in clustering models, whereas the later is used in matrix factorization problems. Many extensions of them are developed to accommodate richer types of data [ 12 , 16 ]. However, when dealing with multiple covariates, these models often treat them independently, hence fail to explicitly model the correlation among data sources. The presence of rich and naturally correlated covariates calls for the need to model their correlation with nonparametric models. In this paper, we aim to develop a full Bayesian nonparametric approach to the problem of multi-level and contextually related data sources and modelling their correlation. We use a stochastic process, being DP, to conditionally  X  X ndex X  other stochastic processes. The model can be viewed as a generalization of the hierarchical Dirichlet process (HDP) [ 14 ] and the nested Dirichlet process (nDP) [ 12 ]. In fact, it provides an interesting interpretation whereas, under a suitable parameterization, integrating out the topic components results in a nested DP, whereas integrating out the context components results in a hierarchical DP. For simplicity, correlated data channels are referred as two categories: content and context(s ). In each application, which the covariates constitute content or context(s) is determined by the nature of data. For instance, in pervasive com-puting application , we choose the bluetooth co-location of user as content while contexts are time and location.
 Our main contributions in this paper include: (1) a Bayesian nonparametric approach to model multiple naturally correlated data channels in different areas of real-world applications such as pervasive computing, medical data mining, etc.; (2) a derivation of efficient parallel inference with Gibbs sampling for mul-tiple contexts; (3) a novel application on understanding latent activities contex-tually dependent on time and place from mobile data in pervasive applications. A notable strand in both recent machine learning and statistics literature focuses on Bayesian nonparametric models of which Dirichlet process is the crux. Dirich-let process and its existence was established by Ferguson in a seminal paper in 1973 [ 4 ]. A Dirichlet process DP (  X , H ) is a distribution of a random probabil-ity measure G over the measurable space (  X , B ) where H is a base probability measure and  X &gt; 0isthe concentration parameter. It is defined such that, for any finite measurable partition ( A k : k =1 ,...,K )of  X  , the resultant random vector ( G ( A 1 ) ,...,G ( A k )) is distributed according to a Dirichlet distribution with parameters ( H ( A 1 ) ,...,H ( A k )). In 1994, Sethuraman [ 13 ] provided an alternative constructive definition which makes the discreteness property of a Dirichlet process explicitly via a stick breaking construction. This is useful while dealing with infinite parametric space and defined as
G = tion in [ 11 ], we hereafter write  X   X  GEM (  X  ). Due to its discreteness, Dirichlet processes is used as a prior for mixing proportion in Bayesian mixture models. Dirichlet processes mixture models (DPM) [ 1 , 7 ] which are nonparametric coun-terpart of well-known Gaussian mixture models (GMM) 2 with the relaxation of the number of components to be infinite were first introduced by Antoniak [ 1 ] and elaborated efficiently computational aspect by Neal [ 7 ].
 elled together. From statistical perspective, it is interesting to extend the DP to accommodate these collections with dependent models. MacEachern [ 6 ]intro-duced framework that induces dependencies over these collections by using a stochastic process to couple them together. Following this framework, Nested Dirichlet process [ 12 ] induces dependency by using base measure as another Dirichlet process shared by collections which are modeled by Dirichlet process mixtures. Another widely used model driven by idea of MacEachern is hierar-chical Dirichlet process [ 14 ] in which dependency is induced by sharing stick breaking representation of a Dirichlet process. All of these models are supposed to model single variable in data. In topic modeling, for instance, HDP is used as a nonparametric counterpart of Latent Dirichlet Allocation (LDA) to model word distributions over latent topics. In this application, the model ignores other co-existing variables such as time, authors.
 dent factors. With such independent assumption, he can not leverage the correlated nature of data. There are several works dealing with these situations. Recently, the work by Nguyen et. al. [ 8 ] tried to model secondary data channel (called con-text ) attached with primary channel ( content ). In this model, secondary data chan-nel is collected in group-level, e.g time or author for each document (consisting of words) or tags in each image. In the case of other data sets, observations are not at group-level but data point-level. For instance, in pervasive computing, each bluetooth co-location of each user includes several observations such as co-location, time stamp, location, etc. There is a motivation for modelling in these kind of appli-cations. Dubey et. al. [ 2 ] tried to model topics over time where time are treated as context. The models can only handle one context while modelling but can not leverage the multiple correlated data channels. Another work by Wulsin et. al. [ 16 ] proposed the multi-level clustering hierarchical Dirichlet process (MLC-HDP) for clustering human seizures. In this model, authors assumed that data channels are clustered into multi-level which may not suitable for aforementioned data sets. In consequence, there is the need for nonparametric models to handle naturally corre-lated data channels with certain dependent assumptions. In this paper, we propose a model that can model jointly the topic and the context distribution. Our method assumes a conditional dependence between two sets of stochastic process (content-context) which are coupled in a fashion similar to nested DP. The content models the primary observation with HDP and the dependent co-observations are mod-eled as nested DP with group index provided by the stochastic process from the content side. The set of DPs from the context side is further linked hierarchically in the similar fashion to HDP. Since our inference derivations rely on hierarchical Dirichlet processes, we briefly review hierarchical Dirichlet processes and some use-ful properties for inference. The justification for these properties can be found in [ 1 , 14 , Proposition 3].
 Let consider the case when we have a corpus with J documents. With the assumption that each document is related to several topics, we can model each document as a mixture of latent topics using Dirichlet process mixture. Though different documents may be generated from different topics, they usually share some of topics each others. Hierarchical Dirichlet process (HDP) models this topic sharing phenomenon. In HDP, the topics among documents are coupled using another Dirichlet process mixture G 0 . For each document, a Dirichlet process G j ,j =1 ,...,J , is used to model its topic distribution. Formally, generative representation is as below: Similar to DPs, stick breaking representation of HDP is described as follows  X  =  X  1:  X   X  GEM (  X  ) G 0 = Given the HDP model as described in Equation ( 3 )and  X  j samples from G j for all j =1 ,...,J . All of these samples of each group G grouped into M j factors  X  j 1 ,..., X  jM j . These factors from all groups can be grouped into K sharing atoms  X  1 ,..., X  K . Then the posterior distributions stick breaking of G 0 (denoted as  X  =(  X  1 ,..., X  K , X  new )is Another useful property for posterior of number of cluster K of a Dirichlet process is that if G  X  DP (  X , H )and  X  1 ,..., X  N be N i.i.d samples from G . These  X   X  X  values can be grouped into K clusters where 1  X  K  X  N . The conditional probability of K given  X  and N is where s ( N, k ) is the unsigned Stirling number of the first kind. 3.1 Context Sensitive Dirichlet Processes Model Description: Suppose we have J documents in our corpus, and each has N j words of which observed values are x ji  X  X . From topic modeling perspective, there are a (specified or unspecified) number of topics among documents in corpus where each document may relate to several topics. We have an assumption that each of these topics is correlated with a number of realizations of context(s) (e.g. time). To link the context with topic models we view context as distributions over some index spaces, governed by the topics discovered from the primary data source (content), and model both content and contexts jointly. We impose a conditional structure in which contents provide the topics, upon which contexts are conditionally distributed. Loosely speaking, we use a stochastic process to model content, being DP, and to conditionally  X  X ndex X  other stochastic processes which models contexts.
 groups. Each of group is modeled by a random probability distribution G shares a global random G 0 probability distribution. G 0 is draw from a DP with a base distribution H and concentration parameter  X  . The distribution G as a base distribution in a DP with concentration parameter  X  to construct G for groups. The specification for this HDP is similar to Equation ( 2 )inwhich the  X  ji  X  X  are grouped into global atoms  X  k ( k =1 , 2 ,... ).
 is assumed to depend on the topic atom  X  ji of x ji . Furthermore, the context observations of a given topic S k = { s ji |  X  ji =  X  k } a mixture Q k . Given the number of topics K , there are the same number of context groups. Now to link these context groups, we again use the hierarchical structure that have the similar manner with HDP [ 14 ] where Q global random probability distribution Q 0 . Formally, generative specification for conditional independent context is as follows ever, for the context size we have to take into account of the partition as induced by the content atoms. The stick breaking construction for context is ing scheme for inference. We briefly describe inference result of model. We also assume conjugacy between F and H for content distributions as well as Y and R for context distributions since the conjugacy allows us to integrate out the atoms  X  k and  X  m . The sampling state space now consists of more, we endow Gamma distributions as priors for hyperparameters and sample through each Gibbs iteration. During sampling iterations, we main-tain the following counting variables: n jk -the number of content observations in document j belong to content topic k , the marginal counts are denoted as n = k n jk ,and n .k = j n jk ; w km -the number of context observations given the topic k belong to context m . The marginal counts are denoted simi-larly to n jk . Sampling equations for content side are described below. Sampling z : the sampling of z ji have to take into account of influence from the context apart from cluster assignment probability and likelihood. The first term of above equation in the RHS is the predictive likelihood of prior at the content side similar to HDP in [ 14 ] while the second term indicates the predictive likelihood of the observation for content topic k (except x the content topic k . As a result, conditional sampling for z Sampling  X  : we use the posterior stick breaking of HDP in Equation ( 4 ). In order to sample m , we use the result from Equation ( 5 ), i.e. m (  X  X  ) m s ( n number of the first kind and compute m k = J j 1 m jk .
 Next, we present sampling derivations for context variables.
 Sampling l : given the cluster assignment of content observations ( text observations are grouped into K groups of context. Let of context observations indexed by the same content cluster k . i.e. { s : z ji = k,  X  j, i } , while s  X  ji k is the same set as s posterior probability of l ji is computed as follows The first term is the conditional Chinese restaurant process given content cluster k while the second term, denoted as y  X  s ji a form of predictive likelihood in a standard Bayesian setting of which like-lihood function is Y , conjugate prior S and a set of observation s j i : l j i = m, z j i = k, j = j, i = i . The sampling equation for l (a) Generative view for pro-dependent on both z and l . Let isolate context variables l same topic z ji = k into one group l k { l ji : z ji = k, vations are also isolated in the similar way s k { the context side is modeled with the structure similar to HDP in which the observations related Q k are s k . We can sample as follows ( Dir ( h . 1 ,...,h .M , X  ) where h .m ,m =1 ...M are auxiliary variables which rep-resent number of active context factors associated with atom m . Similar to sampling m , the value of each h .m will be computed using samples h (  X  m ) h s ( w km ,h )for h =1 ...w km and summed up as h pling  X  and  X  is identical to HDP and therefore we refer to [ 14 ] for details. Sampling other hyperparameters is also doable, one can refer to [ 10 ] for details. 3.2 Context Sensitive Dirichlet Processes with Multiple Contexts Model Description. When multiple contexts exist for a topic, the model can easily be extended to accommodate this. The generative and stick breaking spec-ifications for content side remain the same as in Equation ( 2 ) and ( 3 ). The spec-ification for multiple contexts will be duplicated from one context in Equation ( 6 ). Figure ( 1 ) depicts the graphical model for context sensitive Dirichlet process with multiple contexts. The generative model is fications of context side in Equation ( 7 )for C contexts which is provided below for all c =1 ,...,C : Algorithm 1. Multiple Context CSDP Gibbs Sampler
Q Inference: using the same routing and assumptions on conjugacy of H and F , R c and Y c , we derive the sampling equations for variables as follows Sampling z : in multiple context setting, the sampling equation of z the influence from multiple context rather than one: It is straightforward to apply the result for one context case. The final sam-pling equation for z ji is Sampling derivation of  X  is unchanged compared with one context.
 Sampling equations of l 1 ...C , 1 ...C are similar to one context case where each set of context variables { l c , c } is dependent given sampled values of perform sampling for each context in parallel thus the computation complexity in this case should remain the same as in the single context case given enough number of core processors to execute in parallel. We summarize sampling pro-cedure for the model in Algorithm 1 . In this section we demonstrate the application of our model to discover latent activities from social signals which is a challenging task in pervasive comput-ing. We implemented model using C# and ran on Intel i7-3.4GHz machine with installed Windows 7. We then used Reality Mining, a well-known data set col-lected at MIT Media Lab [ 3 ] to discover latent group activities. The model not only improves grouping performance but also reveals when and where these activities happened. In the following sections, we briefly describe data set, data preparation, parameter settings for the model and exploratory results as well as clustering performance using our proposed model. 4.1 Reality Mining Data Set Reality Mining [ 3 ] is a well-known mobile data set collected by MIT Media Lab on 100 users over 9 months (approximately 450.000 hours). The collected information includes proximity using Bluetooth devices, cell tower IDs, call logs, application usage, and phone status. To illustrate the capability of proposed model, we extract proximity data recorded by Bluetooth devices and users X  location via cell tower IDs. In order to compare with the results from [ 9 ], we preprocessed to filter users whose affiliations are missing or who do not share affiliation with others and then sampled proximity data for every 10 minutes. In the end we had 69 users. For each user, at every 10 minutes, we obtained a data point of 69-dimension which represents co-location information with other users. Each data point is an indicator binary vector of which i -th element set to 1 if the i -th user is co-located and 0 otherwise (self-presence set to 1). In addition, we also obtain the time stamp and cell ID data vectors. As a consequence, we have 69-user data groups. Each data point in group includes three observations: co-location vector, time stamp, cell tower ID. 4.2 Experimental Settings and Results In proposed model, one data source will be chosen as content, the rest will be considered as contexts. We use two different settings in our experiment. is (69-dimension) Multinomial distribution (corresponding to F distribution in model), time and cell tower IDs are modelled as Gaussian and Multinomial distributions respectively (corresponding to Y 1 and Y 2 distribution in model). We use the conjugate prior H as Dirichlet distribution, while R GaussianGamma and Dirichlet distributions, respectively. We run the data set with 4 different settings for comparison: HDP -standard use of HDP on co-location observations (similar to [ 9 ]); CSDP-50% time -co-location and 50% time stamp data (supposing 50% missing) used for CSDP; CSDP-time -similar to CSDP-50% time, except that whole time stamp data are used; CSDP-celltower -resembling to CSDP-time but additional cell tower ID observations are used. ities of users. It fails to answer more refined questions such as when and where these activities happened? Our proposed model can naturally be used to model the addi-dents) usually happened at specific time on Monday, Tuesday and Thursday while topic 5 ( master frosh students) mainly gathered on Monday and Friday (less often on the other days). Similarly, when we modelled cell tower IDs data, the results revealed a deeper understanding on latent activities. In Figure ( 2b ), we can observe student group activities, apart from Sloan School building ( cell no.1 or 40 ), they sometimes gathered at the restaurants ( cell no. 44 ).
 When using more contextual information, it does not only provide more exploratory information but also help the classification to be more discriminated. When using only time as context in Figure ( 2a ), the user no. 94 is (confusingly) recognized in both topic 2 and 6. But when location data is incorporated into our proposed model, the user no. 94 is now dominantly classified into topic 6. To quan-titatively evaluate proposed model when using more context data, we use the same setting with the work in [ 9 ]. First, we ran the data model to discover the latent activities among users. We then used the Affinity Propagation (AP) algorithm [ 5 ] to perform clustering among users with similar activities. We evaluated cluster-ing performance using popular metrics: F-measure, cluster purity, rand index (RI) and normalized mutual information (NMI). As it can be clearly seen in Table 1 , with more contexts we observed, CSDP achieves better clustering results. Purity and NMI are significantly improved when more contextual data are observed while other metrics slightly improved when modelling with contextual data. cell towers) as contexts. The conjugate pairs are remained the same in previous setting. In Figure ( 3 ), we demonstrate top 4 time topics including Friday, Thurs-day (upper row), Tuesday, and Monday (lower row) which are Gaussian forms. The groups of users who gathered in that time stamp are depicted under each Gaussian. It is easy to notice that the group with user 27, 58 usually gathered on Friday and Monday whereas other groups met on all four time slots. We propose a full Bayesian nonparametric approach to model explicit corre-lation structures in heterogeneous data sources. Our key contribution is the development of a context sensitive Dirichlet processes, its Gibbs inference and its parallelability. We have further demonstrated the proposed model to discover latent activities from mobile data to answer who (co-location), when (time) and where (cell-tower ID)  X  a central problem in context-aware computing appli-cations. With its expressiveness, our model not only discovers latent activities (topics) of users but also reveals time and place information. Qualitatively, it was shown that better clustering performance than without them. Finally, although the building block of our proposed model is the Dirichlet process, based on HDP, it is straightforward to apply other stochastic processes such as nested Dirich-let processes or hierarchical Beta processes to provide alternative representation expressiveness for data modelling tasks.

