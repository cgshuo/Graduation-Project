 We consider the problem of estimating rates of rare events for high dimensional, multivariate categorical data where several dimen-sions are hierarchical. Such problems are routine in several data mining applications including computational advertising, our main focus in this paper. We propose LMMH , a novel log-linear model-ing method that scales to massive data applications with billions of training records and several million potential predictors in a map-reduce framework. Our method exploits correlations in aggregates observed at multiple resolutions when working with multiple hier-archies; stable estimates at coarser resolution provide informative prior information to improve estimates at finer resolutions. Other than prediction accuracy and scalability, our method has an inbuilt variable screening procedure based on a  X  X pike and slab prior X  that provides parsimony by removing non-informative predictors with-out hurting predictive accuracy. We perform large scale experi-ments on data from real computational advertising applications and illustrate our approach on datasets with several billion records and hundreds of millions of predictors. Extensive comparisons with other benchmark methods show significant improvements in pre-diction accuracy.
 H.1.1 [ Information Systems ]: Models and Principles Algorithms, Theory, Experimentation Computational Advertising, Display Advertising, Spike and Slab Prior, Gamma-Poisson, Spars Contingency Tables, Count Data
Jointly estimating occurence rates of rare events for large num-ber of attribute combinations ( cells ) is an important data mining problem that arises in several applications like computational ad-vertising[5], disease mapping[9], ecology[10], adverse drug reac-tion[11], and many others. The main difficulty in such simultane-ous rate estimation is the paucity of data and absence of events at fine resolutions, it is common to observe a large fraction of cells with zero or a few event occurences. Hence rate estimates obtained independently for each cell are often unreliable and noisy. In gen-eral, a  X  X mall sample size correction X  obtained by properly pool-ing information across different data aggregates provide better esti-mates. In fact, aggregating data reduce variance due to larger sam-ple size but introduces bias; disaggregation on the other hand re-duce bias but incurs more variance. When data is hierarchical, bor-rowing strength from aggregates across multiple dimensions and multiple resolutions often lead to estimates with a good bias-variance tradeoff. How to perform such borrowing in an accurate and scal-able fashion when working with high dimensional data is the prob-lem we address in this paper. Specifically, we describe rate estima-tion methods that exploit cell correlations in data that is hierarchi-cal along more than one dimensions. Such data are commonplace in many scenarios including computational advertising, our main motivating application in this paper.

Computational advertising is a new scientific sub-discipline that is at the foundation of building large scale automated systems to select advertisements (ads) in online advertising applications. An important goal in online advertising is to find the best match between a given user in a given context and a suitable ad . Different variations of the problem arise depending on the context consid-ered. For instance, in sponsored search the context is a query issued by the user; in contextual and display advertising the context is a publisher page visited by the user and so on. The definition of what constitutes a  X  X est match X  is a complex one and involves maximiz-ing value for users, publishers and advertisers. However, one key input that is often required to facilitate good matches are estimates of rare events like click rates and conversion rates. In fact, a signif-icant fraction of online advertising is performance based whereby an advertiser pays if an user performing a web search or visiting a publisher page responds positively to the ad. The positive response is typically measured in terms of click-through rate (CTR) on ads. Revenue models based on conversion rates (CVR) where payment is made when users perform some positive action (e.g. buying a product) on the advertiser landing page are also becoming popular. In this paper we provide a novel log-linear model to estimate such rates in high dimensional and large scale computational advertising problems.

The rate estimation problems described above entail several chal-lenges. First, success (click and conversion) rates are typically low, especially in display and contextual advertising. Second, although massive amounts of data is obtained from large scale advertising systems (several billion ads served), the dimensionality o f the at-tribute space is large and typically consists of cells that c an poten-tially run into several millions. Furthermore, data on new c ells are frequently added to the system. In fact, to see this curse of d imen-sionality issue, note that an ad display is an interaction am ong ele-ments in the tryad (user, publisher ,ad) and typically gener ates a re-sponse (no-click, click, conversion). However, the number of pub-lishers, ads and users in the system is typically large. The d ata dis-tribution among cells is also unbalanced since the best ad ma tches typically tend to be more concentrated. Hence, it is usual to see a small number of cells account for a large fraction of data (h ead) with the remaining sparsely distributed among a large numbe r of cells (long tail). Thus, there is often extreme data sparsen ess at the finest resolution cells for which estimates are required to p erform good ad matches.

The main assumption we make is that although each dimen-sion (publisher, ad, user) is a high-dimensional categoric al variable with large number of values, they are hierarchical and hence facil-itate data pooling at multiple resolutions. For instance, p ublishers maybe arranged in a hierarchy based on URL prefix rollups, use rs maybe characterized by several attributes some of which are hi-erarchical (e.g. Geo) and there is a natural hierarchy for ad s that aggregates into campaigns which in turn are aggregated into adver-tisers.

We provide estimation methods that can exploit correlation s when considering cross-product of such multi-dimensional hier archical categorical variables. In fact, our method borrows from est imates at coarser resolutions when there is sparseness at finer reso lutions. For instance, if users from Manhattan have seen a Honda Accor d ad on New York Times 10,000 times in the past and clicked 500 times, we do not use pooling and provide an estimate that is cl ose to 500 / 10000 = 1 / 20 . Consider another scenario where San Jose Mercury News site saw only 100 visits from users in Sunnyvale California and obtained 5 clicks, an estimate of 1 / 20 is perhaps not as reliable in this case. Here we may want to  X  X orrow stren gth X  from aggregate click rate estimate of all California visits to Mer-cury News. But how much should we borrow and from where should we borrow under different sample size scenarios? In t he ex-ample above, is it best to borrow from user visits in Californ ia? Or is it better to borrow from user visits to all news sites in Cal ifornia? Or shall we use some other aggregates? The answer often depen ds on correlations among cells at finer resolutions that have co mmon ancestor cells at coarser resolutions. This is a non-trivia l problem with high-dimensional multivariate categorical data. We p rovide a solution to this problem through a novel statistical method which we shall call LMMH (Log-linear Model for Multiple Hierarchi es) in the rest of the paper.

Other than estimation accuracy of rates, it is also desirabl e to have a parsimonious model (i.e. a model with small number of parameters). Models with large number of parameters have hi gh memory requirements that makes cost-effective online ad se lection difficult. Disk access for large models is an option but it has an ad-verse impact on throughput, often not acceptable in large sc ale sys-tems. Our method achieves both competing objectives of accu racy and size. The idea employed is simple and based on thresholding -cells that borrow too much strength from ancestors are prun ed and completely fallback on ancestors. This eliminates the n eed to store parameters for pruned cells and significantly reduc es the model size. However, the thresholding operation is not appl ied as a post-processing step to model output but is in fact a built-i n feature of the model itself. This provides a principled modeling fra mework to obtain models that are both accurate and parsimonious.
Finally, our model fitting procedure have to scale to massive amounts of data that are routine in computational advertisi ng ap-plications. Massive in this paper would refer to applicatio ns with terabytes of training data and millions of potential predic tors. More explicity, the entire training data cannot fit in memory usin g com-modity hardware, computing paradigms like map-reduce[7] p ro-vide an attractive way to scale computations in such scenari os. We provide a scalable and simple model fitting algorithm for LMM H that scales gracefully to massive data mining applications in a map-reduce framework. In fact, we demonstrate scalability by fit ting models to datasets obtained from a real-world display adver tising system that consists of several billions records with hundr eds of millions of cells.

Our contributions are as follows. We propose LMMH, a novel statistical method to estimate rates of rare events with hig h dimen-sional, multivariate and hierarchical categorical data. L MMH im-proves prediction by exploiting correlations in aggregate s and ex-tends previous work for a single hierarchy. Besides accurac y, we also address the issue of parsimony to reduce memory require ments for online scoring by taking recourse to a novel variable scr eening procedure. Our screening procedure is part of the model and b ased on a  X  X pike and slab X  prior that ensures parsimony without hu rting accuracy. We provide a scalable model fitting procedure thro ugh a sequential  X  X ne-at-a-time-update X  iterated conditiona m odes (ICM) model fitting algorithm in a map-reduce framework. We illust rate our method on large datasets obtained from a real-world comp uta-tional advertising system.

The rest of the paper is organized as follows  X  We begin with a description of data underlying our method in section 2 with m odel description and fitting in section 3 Experiments are describ ed in section 6 and we end with a brief discussion in section 7.
In this section, we describe the data characteristics under lying our LMMH approach. We begin with a description of event level data that is generated when there is a three way interaction b etween a user and ad on a publisher. This is followed by assumptions w e make about our hierarchical attributes.
Each record in our data represents information for a tryadic inter-action which occurs when an ad is shown to a user in a context, w e shall call this an event . For exposition, we consider display adver-tising where context is a publisher. The user interacts with the ad and may respond in several ways -do nothing, click on it and co n-vert subsequently on the advertiser landing page, or click o n it but not convert. Our goal is to predict response for future inter actions accurately to facilitate selection of best matching ads. As described in section 1, this is challenging due to high-dimensionalit y and data sparseness. However, we assume curse of dimensionality cou ld be mitigated to some extent since some of the categorical varia bles are hierarchical. For ease of exposition, we consider two hierarchical attrib utes. Using display advertising as our running example, we assume the attributes are publisher and advertiser hierarchies respe ctively. Each element in the publisher and advertiser hierarchies are dir ected paths of length m and n respectively and are denoted by i = i 1  X  X  X  X  i m (publisher hierarchy) and j = j 1  X  j 2  X  X  X  X  X  X  j m (advertiser hierarchy)respectively. Nodes with increasi ng suffix in a path represent finer resolution of data aggregation. For ex ample, one of the dataset used in this paper estimate conversion rat es with a publisher hierarchy where paths are of length 2 (i.e. m = 2 ), i represents the publisher type and i 2 provides the publisher id associated with publisher type i 1 . For the advertiser hierarchy in this dataset, we have paths of length 4 (i.e. n = 4 ) where j the advertiser, j 2 is a converion-id instrumented by advertiser j to track conversions on traffic routed to the landing page, j campaign-id and j 4 is the ad-id. While for publisher hierarchy i strictly nested within i 1 (each publisher is of exactly of one type), this is not true for advertiser hierarchy. For instance, a si ngle ad can participate in multiple campaigns and a single campaign can be part of multiple conversion-ids for a given advertiser. H owever, we assume that an ad associated with an event belongs to a uniq ue path in the advertiser hierarchy; our methodology works for any hierarchical attributes with such directed acyclic (DAG)s tructure which is more general than strict trees. Also, for notationa l ease we assume paths of fixed lengths m and n for each event but our method easily generalizes to scenarios where events are ass ociated with paths of varying lengths. In this section, we provide technical details of our LMMH app roach. We describe our model for two hierarchies and then provide ge ner-alizations to multiple hierarchies.

The raw event data is aggregated for cross-product of paths z = and Tries respectively. For instance, in our running exampl e the key z aggregates data from all displays of ad j 4 that belongs to adver-tiser j 1 on a publisher i 2 when it participates in campaign j conversion-id j 2 . The definition of Success and Tries depends on the response prediction problem. In estimating conversion rate per click for example, S z represents number of conversions obtained on key z out of E z clicks. Denoting by  X  z the true rate parame-ter for key z , our goal is to estimate  X   X  X  for different keys z . The simple ratio estimator  X   X  z = S z /E z that obtains estimates indepen-dently for different keys z is reliable only for large sample sizes, this is not true for computational advertising application s where an overwhelming fraction of keys z have small sample size. Hence estimating the  X   X  X  by borrowing strength at different resolutions is an attractive way to reduce variance.
The node ids in the hierarchical paths of our attributes ofte n have additional meta-data associated with them. For instance, p ublish-ers and advertisments can be classified into content categor ies like sports, finance; it is possible to extract additional inform ation from page content and so on. Models that simultaneously utilize b oth meta-data and event statistics for nodes have better accura cy for sparse data. For instance, we may have little data for a parti cular sports advertiser when displayed on a small sports publishe r but large amounts of response data for sports ads when displayed on sports publisher. Fusing such information with node statistics may lead to improved performance. Use of such meta-data is also u se-ful in cold-start scenarios where one has to predict respons e for a new ad on a new/old publisher. Such methods that combine meta -data with statistics at node-ids have been shown to work bett er in the context of other applications[2, 16]. Other than meta -data on node-ids, additional information like daypart, behavio ral tar-geting attributes for users may also be available. We denote by x k the covariates obtained through meta-data and additional n on-hierarchical attributes for the k th event; i k and j k denote the as-sociated hierarchy paths. Denoting by p k the true rate for event k with covariate x k and paths z k = ( i k , j k ) , we assume the follow-ing decomposition where b k is a probability estimate(baseline probability) that is co m-puted from a baseline model which is a function of covariates x and  X  z k is a cell-specific correction factor which is only a func-tion of the hierarchical paths and does not depend on covaria tes. Covariate-based baseline models have been reported in the l itera-ture  X  while several of them are based on logistic regression tech-niques[15], there are exceptions that use other methods[8] . Since one has access to large amounts of data, it is possible to obta in reli-able estimates of b k with techniques like logistic regression. Given baseline probabilities at event level, sufficient statisti cs for key z now consist of ( S z , E z ) , where E z is now the expected number of successes under the baseline model instead of total number o f tries. More concretely, if F z denote the set of events corresponding to key z , E z is given by
We now describe our method for estimating corrections  X  z ciated with keys z by exploiting correlations in multi-dimensional hierarchical aggregates. The main idea is to assume  X  z for each z can be written as a log-linear model with m.n terms that are as-sociated with all cross-products of nodes in paths i and j . More specifically, we assume where  X  i s ,j t is the state parameter associated with node pair ( i Denoting by  X  the state parameter vector associated with all node-pairs, we note that the dimension of  X  in our applications is ex-tremely large; in one of our example datasets there are appro xi-mately 100M unknown state parameters.

As discussed in section 1, it is also important to obtain a parsimo-nious solution without sacrificing much accuracy. Here, parsimon y implies a large fraction of estimated state parameters have a value that is exactly 1.0 and hence need not be stored. This is impor tant since non-parsimonious solutions bloat up the model size an d may require prohibitive amounts of memory. Finally, our model fi tting method should scale to high-dimensional data that consists of bil-lions of records and hundreds of millions of node pairs. We no w describe our modeling techniques for estimating  X  that achieves all three objectives  X  accuracy, parsimony and scalability.
Our model assumes S z | E z ,  X   X  Poisson ( E z  X  z ) , where S conditionally independent given  X  and  X  z is a log-linear function of log (  X  ) s as described in equation 2. Since our goal is to esti-mate rates of rare events, Poisson assumption is reasonable and have been widely used in applications involving rare event e stima-tion(see [11],[9] for examples). Thus, the log-likelihood of  X  under the Poisson model is given by
Problems with MLE  X  One can compute maximum likelihood estimates (MLE) of  X  by maximizing l (  X  ) . In fact, state parame-ters of node pairs at coarser resolutions are shared by larger number of keys z and are hence estimated with higher precision. The prob-lem occurs with pairs at finer resolutions where MLE overfit th e data. This happens since our data is high dimensional and rat es are small, it is natural to observe large fraction of keys z with low Tries and zero Success S z . However, the true rates in such cases are not zero but small, MLE nevertheless provide zero probab ility estimates. To see this more clearly, we consider a toy exampl e with a two level hierarchy where level 1 has one node (root) and level 2 has two child nodes. Denote by ( S 1 , E 1 ,  X  1 ) , ( S and ( S 12 , E 12 ,  X  12 ) statistics and states for the root and two child nodes respectively, where S 1 = S 11 + S 12 and E 1 = E 11 We shall use this toy example extensively to explain other co ncepts later on in this section. The log-likelihood of this three no de hier-archy is given by If S 11 = 0 , it is easy to see that the maximizer of log-likelihood should occur at a point where  X  11 = 0 ; and hence  X  11 = 0 . This is so since the only term involving  X  11 when S 11 = 0 is  X   X  which is maximized at  X  11 = 0 . Such an estimator is problematic in our application due to data sparseness and rareness of suc cess; it is more intuitive in such cases to exploit correlations th at are induced due to hierarchical aggregations of data. For insta nce, in this case assume S 11 = 0 , E 11 = 5 but S 12 = 10 , E 12 Since the two leaf nodes are siblings (e.g. two ads from the sa me advertiser), the rates are expected to be correlated and sin ce node 12 has more data, one can intuitively improve the estimate of  X  by borrowing strength from the estimate of  X  12 . Next, we describe how to enhance our model to exploit such correlations and imp rove the MLE.

Incorporating hierarchical correlations  X  The problem of in-corporating correlations for hierarchical data have been s tudied in the literature before where hierarchical correlations are incorpo-rated through a tree-structured autoregressive model[18] . In our toy example, the autoregressive model will assume  X  11 and  X  equals the parent rate  X  1 in expectation but there is statistical vari-ation that is given by some error distribution. The spread of this error distribution depends on the correlation among siblin gs; small spread imply higher correlation and in the extreme when ther e is no spread, sibling rates completely fallback on the parent r ate. By writing  X  11 =  X  11  X  1 and  X  12 =  X  12  X  1 and assuming  X  equals 1 in expectation, we also obtain a model where the corre-lation is now induced by sharing parameter  X  1 with both children. The latter is sometimes referred to as non-centered paramet rization while the former is called centered parametrization[14]. I n fact, existing work assume an additive autoregressive structure on the logarithmic scale, i.e., where D ( ,  X  ) is an error distribution with mean and scale  X  . For most applications this is assumed to be Gaussian[18] but recent work also use double exponential priors[10]. The correspon ding non-centered parametrization for this model would assume In fact, it is easy to show that for an additive tree-structur ed autore-gressive model with Gaussian errors, the centered and non-c entered parametrization are equivalent due to the reproducibility property of the Gaussian distribution; a similar result may not hold f or other distributions like double exponential. However, both repr esenta-tions exploit hierarchical correlations; the centered doe s so by con-straining parameters at finer resolutions to be close to ance stors while non-centered achieves the same effect by sharing ance stor pa-rameters among descendants. In our toy example, the paramet er  X  is shared with the two descendants. The choice of a represent ation (centered or non-centered) is generally dictated by comput ational considerations and the structure of the hierarchical data.
In this paper we adopt the non-centered parametrization pri mar-ily because it is easier to generalize to complex multi-reso lution structures induced by multiple hierarchies. We also model t he state parameters in our non-centered parametrization on the orig inal scale instead of working on the logarithmic scale, i.e.,  X  s are identically and independently (i.i.d) distributed as  X  X  (1 , a ) ; the error distri-bution D in this case is centered at 1 with scale a . For computa-tional scalability and to ensure parsimonious parameter es timates, we assume D (1 , a ) to be  X  (  X  ; a, P ) , a 2-component mixture of a Dirac and a Gamma distribution given by i.e., with probability P the parameter  X  is exactly 1 (i.e., state not important) and with probability (1  X  P ) it is drawn from a Gamma distribution with mean 1 and variance 1 /a . Such priors are known as  X  X pike and slab X  priors in the literature[12] but their us e for mod-eling hierarchical data have not been considered before. Th ey en-courage automatic variable selection in regression proble ms and their use for non-hierarchical count data was explored by [1 1] in a fraud detection application. We experiment with two versi ons in the paper -a) 1-component Gamma which assumes P = 0 ; this leads to dense solutions and b) 2-component Gamma which assumes P = . 5 ; i.e. a-priori before seeing the data there is a 50% chance of a variable not being important (performance was not sensitive to the choice of P ). For a fixed value of a , combining the prior on  X  with the likelihood gives the log-posterior of  X  as The state estimates  X   X  for a fixed a are now obtained by maximizing the log-posterior in equation 4.

Estimating the mode of  X   X  One can optimize Equation 4 to obtain a mode of  X  by using standard sub-gradient descent methods but to ensure scalability, we instead work with a simple  X  X ne -at-a-time X  sequential update procedure that is also known as Iter ated conditional mode (ICM) algorithm in the literature[13]. Th e fit-ting algorithm is simple -we cycle through the state paramet ers for node pairs and update them one at a time, by computing the one dimensional mode of the conditional posterior of node st ate assuming others are fixed at their latest values. More specifi cally, indexing node pair suffixes ij from 1 ,  X  X  X  , M without any loss of generality and denoting by  X  k all nodes except the k th one, we iteratively find the one dimensional modes of the conditiona l pos-terior [  X  k |  X   X  k , Data ] until convergence, i.e., at the t our algorithm we update the state of k th node to  X  t k , the mode of the conditional posterior For our toy example for instance, at iteration t we compute modes respectively.
In this section, we show that the conditional mode of a state given others is given by a closed form expression. To simplif y no-tations, we again appeal to our toy example which is sufficien t to understand the derivation. Consider the conditional distr ibution of [  X  1 |  X  11 ,  X  12 , Data ] which is proportional to Since S 11 , S 12 are conditionally independent given the node states, it can be easily shown that the conditional distribution is p ropor-tional to Poisson ( S 1 , E  X  1  X  1 )  X  (  X  1 ) where E  X  In fact, E  X  1 can be interpreted as expected Success after adjusting for the corrections of all nodes that appear along paths whic h in-cludes the node being updated. Note that this is a simple mode l that involves combining a Poisson likelihood of a single obs erva-tion with a Gamma mixture and admits a closed form posterior t hat we will derive in a moment. We note that this fact generalizes to all states in our model and the analytical conditional poste rior for every node state can be obtained in our problem by combining a Poisson likelihood based on observed Success at that node wi th ad-justed expected Success and the 2-component Gamma prior. Th us, it is enough to derive the mode of  X  for the following model for sequential update algorithm X  We first provide the expression for the posterior mode of the m odel in Equation 5 and then derive the formula in the next section. The tools used in the derivation would also help in understandin g some results that are presented later. Before providing the expe ssion for the mode, we begin with some preliminaries to introduce esse ntial notations.
 Prelim 1 : A random variable  X  is said to follow a Gamma distri-bution with mean and effective sample size a if and only if the density function is given as g (  X  ; a, a ) = a a X   X ( a X  ) note that Var (  X  ) =  X  2 = /a and we shall use the notation  X   X  Gamma ( ,  X  2 = /a ) . If a &gt; 1 , the mode of Gamma ( ,  X  /a ) is given as  X  = ( a  X  1) /a . In our context, a and a can be interpreted as psuedo number of Successes and Tries respect ively. Prelim 2 : Here we point out the construction of a negative bi-nomial distribution as scale mixture of Poisson. Let S |  X , E P oisson ( E  X   X  ) and  X   X  Gamma ( , /a ) . Then, the marginal distribution of S is a negative binomial with probability mass func-tion Some algebra yields We shall denote this by NB ( S ; a, , E  X  ) . Note that E ( S ) = E ( E is the expectation operator) and V ar ( S ) = E  X  (1 + E  X  a Poisson, mean equals variance; thus a negative binomial di stribu-tion is more heavy tailed than Poisson. This is often referre d to as overdispersion in the literature[6].
 Prelim 3 : The marginal distribution when S |  X , E  X   X  P oisson ( E  X   X  ) and  X   X   X  (  X  ; a, P ) = P 1(  X  = 1) + (1  X  P ) Gamma (  X  ; 1 , 1 /a ) is a mixture of Poisson and negative binomial, i.e., We are now ready to provide the expression for the mode of mode l in Equation 5. We state the result in the form of a theorem but fi rst we begin with a simple Lemma Lemma 1 Assuming a &gt; 1 and P = 0 , the posterior mode  X  model in Equation 5 is given by The proof follows from conjugacy of Gamma-Poisson model, it is easy to see from Bayes theorem that the posterior distributi on of  X  when P = 0 is Gamma (  X  ; S + a E  X  + a , ( E  X  + a )) . We now state our main theorem that gives us the mode for arbitrary value of P . Theorem 1 Assuming a &gt; 1 and P  X  [0 , 1] , the posterior mode for model in Equation 5 is given by 8 &lt; : where
Assuming P = . 5 for simplicity of interpretation, Q is the log-likelihood ratio of data ( S, E  X  ) to test if the data is generated from a Poisson distribution with  X  = 1 relative to a heavy tailed negative binomial counterpart. If the data is well supported by the Po isson distribution, the mode equals 1 and the variable  X  is pruned . The proof of the theorem follows by computing the posterior dist ribu-tion of  X  which from Bayes theorem can be shown to be [  X  | S, E  X  ] = q 1(  X  = 1) + (1  X  q ) Gamma (  X  ; S + a where Q = log ( q 1  X  q ) . To compute the mode of this distribution, note that the mode has to be either 1 or  X   X  m , the mode of the 1-component Gamma (  X  ; S + a E  X  + a , E  X  + a ) depending on the value of the density at these points. Thus, the mode is 1 if density at 1 is higher than at  X   X  m , i.e., q + (1  X  q ) g (1; s + a, E  X  + a ) &gt; (1  X  q ) g (  X   X  log( g (  X   X  m ; s + a, E  X  + a )  X  g (1; s + a, E  X  + a )) &lt; Q and hence the theorem follows.

Correlations induced by LMMH  X  LMMH induces correla-tions among nodes sharing same ancestor(s). Although it is n ot easy to obtain expressions for such correlations analytica lly, we provide mathematical intuition on how the correlations get induced through our simple toy example using a 1-component Gamma pri or (i.e. P = 0 ). First, note that the marginal density of Success in each sibling node ( S 11 and S 12 ) conditional on state of parent node  X  1 being known are independently distributed negative-binom ials due to shared parameter  X  1 , the joint marginal density of S S 12 are no longer independent and given by expected value of product of two negative binomial probabil ities with respect to the Gamma distribution on  X  1 . This simple example clearly illustrates the fact that although our multi-hiera rchy model is composed of conditionally independent sub-models, it in duces correlations in the counts of our multi-dimensional data. I n fact, the smoothing induced due to such correlation structure is an im portant aspect that provides good predictive performance.
For K ( &gt; 2) hierarchies, there are several options. A natural appraoch is to model the cells in the entire K dimensional space; we did not find this to work well in practice due to enormous in-crease in sparseness when working with cross-product of mor e than 2 large hierarchies. Instead, we advocate the use of all 2-fa ctor model; i.e., a log-linear model that is the product of node-p air terms from ` K 2  X  hierarchies. There is no conceptual or programming diffi-culty with this extension since we use an iterative fitting al gorithm. However, we assume that K is small in our applications (e.g 3  X  5 ) but each hierarchy is high dimensional. With large K , this issue requires further research.
We describe our scalable model fitting procedure in a map-red uce framework based on the ICM algorithm described in section 3. We describe our algorithm for updating states with a cross-p roduct of two hierarchies that are DAGs with K 1 and K 2 levels respec-tively. We note that the conditional posteriors of states fo r nodes at the ( m, n ) th level ( m = 1 ,  X  X  X  , K 1 , n = 1 ,  X  X  X  , K dependent of each other and can be updated together in parall el. This forms the basis of our scalable map-reduce algorithm. A lso, since the correction for a key z = ( i , j ) is the product of states for all node pairs, we linearize the 2-d cross product space and update the node pairs in the following order by hierarchy lev els: (1 , 1) , (1 , 2) , . . . , (1 , K 2 ) , (2 , 1) , . . . , ( K k = 1 , . . . , M , where M = K 1 K 2 . In the mapper when pro-cessing nodes at k th level, we join the correction from the corre-sponding parent node at level ( k  X  1) in the linearized hierarchy to get corrected expected success. In the reducer, we aggreg ate over these success, and corrected expected success, while d iscount-ing previous iteration X  X  correction for the node pair in que stion, to compute the updated correction for the current node pair. Th e re-ducer task uses the multiple-outputs feature in hadoop 0.20 .1 for speed, scalability and outputs both the data and correction s simul-taneously. Also the whole logic is implemented nicely into a sin-gle map-reduce task which reduces hadoop task setup/intial ization overhead.

Specifically, for each k = ( i s , j t ) node pair in the conjunction of paths z = ( i, j ) , we compute the state variable  X  t the expected success E  X  z . Each map task gets a chunk of joined input records of conjunction of paths ( data ) with success S expected success E  X  z , and parent state variable  X  t k  X  1 outputs the key k and value data , S z and E  X  z  X  t k  X  1 join the input with previous iterations state variable  X  t  X  1 puts the key k , value data with S z and E  X  z  X  t k  X  1 / X  computes the updated  X  t k with aggregated S z , E  X  z  X  t outputs key k , value  X  t k .

For K ( &gt; 2) hierarchies, we compute the state variables of node pairs from any 2 hierarchies at a time to get corrected expect ed success. We then take these updated success, and expected su c-cess to apply onto the next two hierarchies, So overall we wil l have  X  invocation of the algorithm. Order of the hierarchies does n ot matter as we iterate till convergence. We provide psuedo cod e in Algorithm 1 Algorithm 1 Psuedocode for map-reduce implementation Initialize the global constant a , the state variables  X  0 Iterate until convergence, Iterate t over the conjunction of paths z = ( i, j ) in the data, Iterate over all node pairs ( i s , j t ) , indexed by k = 1 , . . . , M . Note that ( k  X  1) is M from ( t  X  1)  X  X h iteration, when k = 1 and t &gt; 1 . For 1 X  X t iteration with k=1, ( k  X  1) would be treated as record id and the corresponding parent node state variable as 1 .
 where,  X  t k is computed for key k using P S z , P E  X  z  X  using mode formula described in Theorem 1.
Other than work already mentioned in other sections, there i s a rich literature in statistics on multi-level hierarchica l model that is directly related to our work[3]. However, these methods h ave been applied to single hierarchies and for small problems an d are generally referred to as nested random-effects model. Ther e is no work in this literature that considers multiple hierarchie s in a high dimensional scenario as we do in this paper. Reliable rate es tima-tion by exploiting hierarchical correlations for large sca le compu-tational advertising applications was considered in our ea rlier [1] but only for single hierarchies. The method proposed only ap -plies to tree data, it does not work with general directed acy clic graphs. Moreover, the computational efficiency discussed i n that paper only works with Gaussian response which is not a satisf ac-tory assumption when the goal is to estimate the absolute rat es. In machine learning,[10] also considered such a problem for a s ingle class problem when predicting species distribution by geog raphic location but also for single hierarchy. They considered mod el par-simony by assuming L 1 prior through a centered parametrization. However, their application was not large scale compared to t he datasets we illustrate in this paper. We provide a scalable g ener-alization to multiple hierarchies that exploits correlati ons and pro-vides parsimonious model (through a spike and slab prior) in large scale applications. Recent work that performs fast and larg e scale regression with embarassingly large number of predictors o n very large applications is also directly related to our work[17] . However, such methods fail to exploit the hierarchical correlations that are of-ten present in data arising in computational advertising. W e show that LMMH outperforms such methods significantly by exploit ing the correlations in section 6.
In this section, we illustrate performance of LMMH through several datasets obtained from a real-world computational adver-tising application. None of the datasets are publicly avail able, we were not able to obtain benchmark dataset that was large and w here the goal was to estimate rates of rare events with hierarchic al and high-dimensional categorical variables. Instead of creat ing con-trived examples from datasets available in existing reposi tories, we provide a thorough analysis on real-world scenarios by comp arison with state-of-the-art and simple baseline methods on our da tasets. We note that comparison with some state-of-the-art methods re-quired additional map-reduce implementations on our part t o en-sure scalability. Every attempt would be made to release som e of the datasets used at a later date.
We provide a brief overview and motivation for the display ad -vertising response prediction problems that are illustrat ed later with real-world datasets.

Background  X  X nline display advertising is a multi-billion dol-lar industry where advertisers display ads on publisher pag es (e.g. Nordstrom, Nike, Coke). Unlike performance based advertis ing in sponsored search and contextual matching, advertisers in d isplay advertising may design ad campaigns with different product goals in mind. One important objective is building brand awarenes s for promoting future sales, possibly targeted at a user segment . This is similar in spirit to advertising on television and popula r maga-zines. Advertisers with this objective in mind generally op t for the Cost-per-Milli (CPM) model, whereby ad opportunities (use r visits on publisher pages) are priced in bundles of 1000 and are paid for by the advertiser irrespective of user actions. On the other extreme, advertisers with products that have rare repeat sales (e.g. auto, re-frigerator, insurance) may care more about immediate than f uture sales, the goal however may still be to target a certain user s egment. For instance, an automobile manufacturer may target users w ho are interested in baseball for selling a new sports car model. Ot her advertisers maybe somewhere in between and care both about f u-ture and immediate sales; a major telecom company may want to build brand awareness but still promote immediate sales of i ts new calling plan.

Guaranteed and Non-guaranteed display advertising  X  X iven the diverse nature of advertiser objectives, it is no surpri se dis-play advertising is sold under different revenue models and have given rise to a complex ecosystem of buyers (advertisers), s ellers (publishers) and intermediaries (ad-networks). We provid e a brief overview to motivate the data mining problem addressed in th is paper. At a high level, there are two broad ways of delivering dis-play advertising -guaranteed and non-guaranteed. In guara nteed delivery, advertisers reserve a fixed number of user visits t argeted at a user group ahead of time on publisher X  X  pages who guarant ee these visits at a certain price. For instance, Johnson &amp; John son may wish to target 100 million visits by females on Yahoo! ast rol-ogy last week of January 2010. The publisher has to guarantee the delivery of visits, advertisers typically pay higher CPM to procure such a guarantee. Non-guaranteed delivery on the other hand does not provide such a guarantee on visit volumes. It follows the  X  X ay as you go X  strategy where each ad opportunity is sold through a real-time auction. The auction is facilitated by commercia l inter-mediaries called ad-networks who connect advertisers to pu blish-ers (e.g. Ad.com, Value Click etc) and share a certain percen tage of revenue that accrue from transactions. Beyond ad-networ ks, the  X  X xchange X  provides a platform for buyers and sellers to tra nsact across network boundaries (e.g. RightMedia exchange). Thi s is sometimes also referred to as  X  X etwork-of-networks X  model . In this section, we analyze a subset of data obtained from the Ri ght-Media exchange. We emphasize that the data analyzed in this p aper is in no way representative of data obtained from our entire s ystem, it is a subset obtained from a certain set of geographic locat ions for the purposes of illustration in this paper.

Response Prediction Problem: normalization across pricin g types  X  Advertisers participate in an auction using multiple pric-ing types like CPM, Cost-per-click(CPC), Cost-per-action (CPA) on the exchange, hence it raises a fundamental question of ho w to select a winner. The solution we currently employ is to nor-malize across pricing types and create a single denomination -ex-pected CPM (eCPM). Thus, for CPC and CPA, eCPM = Pr(click or conversion)  X  Cost, where Cost is a function of advertiser bid dis-counted by revenue shared with intermediaries. Thus, predi cting rates of rare response (clicks and conversions) is a fundame ntal problem to successfully conduct such auctions in the exchan ge us-ing this strategy. In fact, errors in estimating rates may le ad to arbitrage and provide unfair advantage to some pricing type . Un-derestimation on the other hand for a pricing type (e.g. CPA) would make it an unattractive mode of participation and the exchan ge may lose a certain set of advertisers. Thus, accurate estimatio n of click and conversion rates is an important problem in non-guarant eed display advertising on Right Media exchange. We created three large datasets to conduct our experiments  X  (1) Post-View conversions ( PVC ) (2) Post-Click conversions ( PCC ) and (3) Click data ( CLICK ). We provide a brief description of event level data for all three types and then provide details of the models we fitted to these datasets.

PVC  X  Each event in this case consists of a binary response where success happens if a post-view conversion takes place . Ad-vertisers participating as post-view conversion instrume nt their ads with a pixel that gets triggered and stores the ad view information by the user (e.g. in the browser cookie or some user data store ) when a user gets exposed to the ad on a publisher site. If the sa me user then ends up on the advertiser site (through some other p ath) and converts (e.g. buys a product) according to the pixel defi nition, a conversion is registered. The conversion rates for PVC are ex-tremely low (not revealed due to reasons of confidentiality) . The PVC data we consider had approximately 7 B events in training and roughly 240 M in test. Other than age and gender for users, we have information on sizeid for each ad. We also have recency and frequency information on when ads were displayed to each use r that are categorized into several bins. Our data consists of two hi-erarchical attributes, namely, publisher hierarchy and an advertiser hierarchy. The former has two levels (publisher type  X  publisher id) while the latter has four levels (advertiser  X  conversion-id  X  campaign  X  ad-id). We note that the advertiser hierarchy is not a strict tree but a DAG, a single ad can participate in multiple cam-paigns for instance. Conversion-id is the pixel id instrume nted by advertiser to track conversions on their landing page.

CLICK  X  X uccess in this case occurs if a user clicks on an ad, all other variables in this dataset are same as PVC except for the advertiser hierarchy that consists of 2 levels (advertiser  X  adid). The number of events in this data is much larger, training set 90 B while test set 3 B .

PCC  X  Here, an event is generated when a user clicks on an ad; success occurs when the user converts after the click-th rough. This is distinct from PVC since here the conversion has to occur subsequent to click-through on the ad and not by following an y other path in a stipulated time period. Recency and frequenc y are of course absent in this data since they are measurements ass ociated with ad displays and not ad clicks. Other variables are all sa me as PVC . The total number of events in this data was approximately . 5 B in training and 20 M in test. For the purposes of illustration in this paper, we note that PCC and CLICK data are not aligned and collected from different subsets, hence no valid inference could be made by combining PCC and CLICK event information.
We now describe the LMMH variations that we ran on the three datasets. The baseline model for all datasets include covariates based on user age, user gender, publisher type, recency, fre quency and sizeid. Appropriate baseline models were fitted after te sting several variations using generalized linear mixed effects models (GLMM) in R[4]; the models we selected for each dataset are th e ones with minimums AIC. We note that the generalized linear m ixed model routines are computationally expensive, hence we per formed the operations using a map-reduce approximation. In partic ular, we randomly partition the datasets, fit separate GLMM to each and combine the results by using a weighted average with weights being inversely proportional to the estimated variance. In pract ice, one can run other methods like logistic regression with L 2 regulariza-tion, we found the GLMM to provide better results for small mo d-els we fitted to these datasets. We shall refer to these as GLMMB when reporting results.

For the hierarchical correction models, we computed cell es ti-mates for cross-products of following hierarchies  X  a) For PCC we used publisher  X  advertiser hierarchies. b) For CLICK we used (publisher-type  X  (recency, frequency)  X  publisher-id)  X  (advertiser  X  adid). We included (recency,frequency) bins in the hierarchy since click rates are known to vary by degree of pre vi-ous exposure. c) For PVC , we used the same model as CLICK except for a four level advertiser hierarchy. For all models , we ran 1) 1-component Gamma (i.e. we choose P = 0 ) which pro-vide estimates that are not exactly 1 and 2) 2-component Gamma (i.e. we choose P = . 5 ) for which some  X  are estimated as 1 and hence could be removed when storing the model in ad-servers f or online scoring. For all models (including other variations we de-scribe later), tuning parameters (e.g. a ) are selected through cross-validation. For LMMH, a has an intuitive interpretation as psuedo number of successes, we have found that it is enough to consid er values in the range of 2  X  10 for cross-validation. We shall refer to the two variations with P = 0 , . 5 as LMMH-1C and LMMH-2C respectively when reporting results. Other than these, we a lso ran two models that 3)Only considers publisher-id  X  adid correc-tions and 4)Only considers publisher-id  X  advertiser. We do this to show the benefits of using the entire hierarchies for the pu rposes of estimation. These will be referred to as FINE and COARSE respectively. Next, we describe the methods that were used f or comparison with LMMH.

Variations of logistic regression  X  We tested three different variations of logistic regression that differ in the number of fea-tures included in the model. We shall refer to them as Log I , Log II and Log III respectively. All logistic regressions were fitted in a map-reduce framework using conjugate gradient method (CG ) along with L 2 regularization on the coefficients to ensure stable model fitting. The regularization parameter was selected th rough cross-validation and the maximum number of CG iterations wa s set to 50. The three variations only differ in the features, t he fitting procedure was the same.
Since obtaining the absolute estimates are important in our dis-play advertising application, we report on average test-lo glikelihood under Bernoulli model as our prediction accuracy measure. S pecif-ically, for a model the average test log-likelihood avgLL is
P k ( Succ k  X  log ( X  p k ) + ( T ries k  X  Succ k )  X  log (1  X   X  p Instead of reporting the absolute log-likelihood numbers t hat can provide information on absolute value of probabilities, we report percent improvement is log-likelihood of a method relative to our covariate-only based baseline GLMMB , i.e., Further, we split test data into 20 equal parts and report the distri-bution of log-likelihood lifts for each method to provide a m easure of statistical variation in test set metrics.
We first begin by providing statistics on the number of cells i n each of our datasets. The total number of state parameters th at were estimated by our LMMH were 81595746(  X  81 M ), 6039376(  X  6 M ) and 16517629(  X  16 . 5 M ) for CLICK, PVC and PCC respec-tively. Our best 2-component models for these datasets base d on spike and slab prior had 4429106 (  X  4 . 4 M ), 35694 (  X  35 K ) and 148748(  X  150 K ) estimates that were different than 1 and hence needs to be stored. The 1-component gamma did not provide sol u-tions that were exactly 1 , however a large number of estimates were close to 1 but variable removal would now require taking recourse to post-processing procedures which distorts the canonica l model results. Our 2-component model provides extremely parsimo nious models that are lightweight and could be easily stored in mem ory to faciliate cost-effective online ad selection.

We also note that our modeling approach coupled with computa -tion in a map-reduce framework is extremely scalable; we are able to process billions of records with hundreds of millions of c ells in a few hours. For CLICK data set, we took 135 minutes with 50 reducers, for PVC 123 minutes with 25 reducers and PCC 109 min -utes with 20 reducers for learning the models. In contrast to this, the LogI , LogII and LogIII took 4, 6 and 7 hours for CLICK each employing 80 reducers, 3, 4.5, 5 hours for PVC with 40 reducer s and 4.5, 8 and 9 hours for PCC with 80 reducers. Next, we examin e the predictive accuracy of our models as given in Figure 1 for three datasets for different variations of our approach (include s simple baselines) and three different variations of logistic regr ession de-scribed before.

First, we note that all algorithms tested show lift relative to our covariate only baseline GLMMB, which clearly shows that usi ng information at nodes is essential for good performance. Our two baseline variants {COARSE, FINE} that use only partial hier archi-cal information are significantly worse than others. Surpri singly, COARSE is better than FINE in PCC only; this is probably since PCC is sparse both in terms of Success and Tries unlike PVC and CLICK which are sparse in terms of Successes but have a lot mor e Tries. All three variants of logistic regression are worse t han our LMMH variants; increasing the number of features helps logi stic regression except in PVC which is too sparse and starts over-fitting. Both LMMH variants (1-C and 2-C) have similar performance an d are significantly better than all other methods. This clearl y shows that incorporating hierarchical correlations through our approach helps improve accuracy compared to other log-linear models like logistic regression that does not incorporate such informa tion. The fact that both COARSE and FINE baselines are significantly wo rse also shows that smoothing alone is not enough to achieve good per-formance, it is imperative to combine smoothing with hierar chical information at multiple resolutions.

Although not visible on the plots, a two-sample test conduct ed on the 20 partition statistics revealed 2-C is slightly better than 1-C but the difference although statistically significant (we will always find statistical significance with such massive data) is not prac tically significant. However, as seen before LMMH 2-C can get compa-rable accuracy along with model parsimony induced through t he spike and slab 2-component Gamma prior.
We proposed a new log-linear model LMMH for estimating rare rates in high dimensional, multivariate categorical data c onsisting of several hierarchies. Our method provides accurate predi ctions by exploiting hierarchical correlations, parsimony by usi ng a spike and slab prior and is scalable to extremely large applicatio ns with billions of records and hundreds of millions of predictors i n a map-reduce framework.

Several aspects of our problem needs further research. We we re motivated by computational advertising applications and a ssumed a small number of large hierarchies; in applications where t his is not the case the issue of appropriately selecting the releva nt cross-product of hierarchies to consider is an open issue. Another impor-tant issue is incremental learning of our LMMH model in an onl ine fashion. Variance computation for fast explore/exploit th rough our multi-hierarchy model is also an interesting direction.
We thank Ozgur Cetin for kindly providing us with the map-reduce logistic regression code. We thank Krishna Prasad Ch itra-pura and Sachin Garg for helpful discussions. [1] D. Agarwal, A. Z. Broder, D. Chakrabarti, D. Diklic, [2] D. Agarwal and B.-C. Chen. Regression-based latent fact or [3] A.Gelman and J.Hill. Data Analysis using [4] D. Bates and D. Sarkar. lme4: Linear mixed-effects models [5] A. Broder. Computational advertising. In SODA  X 08: [6] A. C. Cameron and P. K. Trivedi. Regression Analysis of [7] J. Dean and S. Ghemawat. Mapreduce: a flexible data [8] K. Dembczynski, W.Kotlowski, and D.Weiss. Predicting a ds X  [9] D.G.Clayton and J.Kaldor. Empirical bayes estimates of [10] M. Dudik, D. M. Blei, and R. E. Schapire. Hierarchical [11] W. DuMouchel and D. Pregibon. Empirical bayes screenin g [12] H.Ishwaran and J. Rao. Spike and slab variable selectio n: [13] J.Besag. On the statistical analysis of dirty pictures . Journal [14] O.Papaspiliopoulos, G. O. Roberts, and M. Skold. A gene ral [15] M. Richardson, E. Dominowska, and R. Ragno. Predicting [16] D. H. Stern, R. Herbrich, and T. Graepel. Matchbox: larg e [17] K. Weinberger, A. Dasgupta, J. Langford, A. Smola, and [18] L. Zhang and D. Agarwal. Fast computation of posterior
