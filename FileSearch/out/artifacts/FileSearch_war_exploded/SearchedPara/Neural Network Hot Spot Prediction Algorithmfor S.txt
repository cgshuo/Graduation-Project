 A lot of objects on the Web present important information for users. Some of the objects are frequently requested by users, but there are huge numbers of objects that are rarely requested. Generally, a request pattern of web documents followed Zipf X  X  law [1-3]. A frequently requested object by users is called a  X  X ot spot X . It forms 90% of the total amounts of request by the 10% of hot spots that hold a high rank for Web server [2] and forms 70% of the total requests by the 25  X  40% of hot spots that hold a high rank in the case of a Proxy server [3]. These hot spots will cause problems. That is, a server that is requested by sudden huge hot spots not only transfers information within a short time to users, but it also will make the functions of the server perfectly stop due to a huge amount of traffic.
 plays an important role in an effective and efficiency transmission of data in the Internet. The cache can quickly supply a cached object to users even though an excessive load will be applied to the original server, and network lies in a very congested state. In the case of requesting a cached object, it will reduce consumption of the source of server and amount of works in server or network so that it give a favor for all the users.
 Locator) in the shared proxies in a distributed web caching uses a hash routing method. This method improves the existing hashing methods [4] that largely affected by the increase or decrease of the proxy and maps URL without any effects from the increase or decrease of the proxy.
 ICP, CARP, and so on. The squid server uses the CARP [5] to make a load balancing. There are many studies [6] proposed to use a DNS (Domain Name Server) to make a load balancing. However, the load balancing is not considered in the previously investigated results of the studies for the shared proxies from the viewpoint of hot spot.
 proxies using an adaptive controlled replication method [7] applied by a hash routing in the results of studies in recent years. At this time, it is divided into two spaces, such as a space that will store hot spots from other proxies, and another space that will store the objects, which are assigned to its own cache. spots replicated from other proxies to its own cache will maximize the efficiency of a proxy. However, there are some problems that the traffic increased by the in-crement in the amount of objects brought by the replication and some replicated objects will be thrown away b ecause of no requests.
 hashing algorithm. This will supply directly the request objects by requesting for the cache that is stored by using a hash routing. The characteristic of the proposed method performs a prefetch by estimating whether a hot spot will be required in the future. A prediction model using a neural network that is useful to build a time series modeling is obtained after analyzing the request patterns of the proxy access log files and estimates hot spots that will occur in the future. For the estimated hot spots, it is possible to improve the system performance by applying a prefetch to the proxy that has a low level load using the load information for each proxy as follows.  X  It makes short the service time for the request of hot spot by prefetching hot  X  The efficiency of the proxy will increase by distributing hot spots to the shared  X  The quality of service will increase due to the increase in the hit rate for the 2.1 Network System A caching system proposed in this paper adopted a Global Hosting System (GHS) [9] and was applied by round-robin DNS method. A global hosting system is presented in Figure 1 and the processes for the response are as follows. First of all, let us assumed that users already know the IP of the Content Provider (CP). If a user requests a web page to the CP using process 1, the CP will provide a text that is included in HTML. In addition, it will supply an embedded object like images or media files that are included in other documents using a proxy that is located near the user. That is, if a user requests an HTML page, the CP will send a page that includes the embedded object URLs instead of a typical page. At this time, a user creates a new embedded object URL by hashing the embedded object URLs that are received by using a script and connects to the local DNS through process 2 in order to find the proxy IP that includes the new embedded object URLs. The upper level DNS decides the locations of users in network through process 3. In addition, it makes a connection to the lower level DNS that is located near the user by using a function of the domain delegation. Here, the lower level DNS provides the proxy IP address where the embedded objects are mapped by using a consistent hashing method through process 4. In addition, the embedded objects will be requested by connecting the proxy IP address that is provided in process 5. If the requested materials exist, the proxy will supply the embedded objects. Otherwise, the proxy server requests the embedded objects again to the server through process 6. Furthermore, the proxy server supplies it to the user and stores it in its own cache. the load balancing using the round-robin method to each proxy for the estimated hot spots. A global hosting system (GHS) has a drawback in which a certain proxy will be swamped because of requests for a certain proxy due to hot spots. However, this study introduces a process for analyzing the dynamic characteristic of hot spots and for estimating in advance through process 6 presented in Figure 1. At this time, the prefetch will be performed according to priority of the low load proxy using the load information for each proxy that were obtained from the proxyhelper of the shared proxy. 2.2 Computer Simulation A simulator was configured to simulate the system proposed in the previous section using the PERL language. The simulator was built by using a modular method to make an easy modification and management. The performance test for a web caching system will be operated by the various settings, such as the numbers of proxies, memory size of the queue, and processing time. A cache policy for the memory that is related to the storing space of the proxy [13] is applied by the LRU (Least Recently Used) method. A service policy for the proxy server will follow the FIFO scheme. The processing speed of the proxy can be controlled by the option of process time in the program, and it means the time used to process a single object. Moreover, the parameter of process time used in the program is also applied to present the time to process single data. That is, the parameter of real process time can be produced by applying the calculations in which the total objects that are requested for a constant period of time are divided by the time required. In the case of process time &gt; real process time , there is no miss because the performance of the proxy can sufficiently handle the requested services. However, in the case of process time &lt; real process time , a miss exists because the request is not processed by the time delay. Moreover, if the data that is stored in a queue of the proxy is requested, it will be processed as a hit. Otherwise, it will be recorded as a miss.
 file of Test. The hashing is performed by the MD5 [10] for the objects that come from the Web module. In addition, the hashed objects will be stored in the selected proxy using a consistent hashing. The load conditions for each proxy will be checked by the proxyhelper every 30 minutes in order to perform the load distribution for the request of the shared proxy.
 and processes all objects. First, the Dns module hashes the objects using the MD5 and assigns it to the 1000 virtual caches that are charged by each proxy. Then, the proxy ID of the hashed object will be decided. If the requested object is a hot spot, the load balancing will be performed. Moreover, in the case of an object that is not a hot spot is requested, the assignment is achieved by the virtual caches that are hashed by the MD5. 2.3 Prefetch Method The load information of the shared proxies will be collected by the proxyhelper presented in Figure 1 in order to prefetch the estimated hot spots to the shared proxies using a neural network model. At this time, the original server decides how many proxies will be used and which proxies will be used to prefetch the hot spots. Although the numbers of proxies are decided by the optimal method, various numbers of the proxies are decided through several additional simulations to analyze the system proposed in this study. In addition, the issue that which proxy is to be used to prefetch is solved by the order of the proxy that has a low load using the load information of each proxy. An experimental data produced by analyzing the access logs file from the NASA server. Let us consider the analyzing method of the access logs file, producing an experimental data and its characteristics that are used in the prediction process of hot spots. 3.1 Analyzing the Access Logs Files from the NASA Server The access logs file records all processes performed by the server. If user contacts to the server, all related information for the work are stored in an access logs file. That is, if a user requests a particular web page, the server will access all objects that are related to the web pages. In addition, all the processes to handle the requested objects will be stored in the access logs file not only the web pages that the users requested, but also the image files, linked data, and various other information. Therefore, it becomes an important information that shows the objective of the site visiting based on the data, which includes the numbers of requested data, contact times, numbers of the visitors, and routes of the visiting through the analyzing of the access logs file. Moreover, it stores the request for the processes of the server and its success or failure. In the case of failure, the information to solve the problem is stored in it.
 the information as follows. sts-68/news/sts-68-mcc-05.txt HTTP/1.0 X  200 1839.
 verification, user approval, time, request HTTP, state code, and amounts of transmission by the order of its record. This study extracts the request time and HTTP among the information of the log files and uses it as a data to estimate hot spots. 3.2 Producing an Experimental Data A pattern for the request of the extracted data is verified by analyzing the access logs file from the NASA server follows Zipf X  X  law. However, the results are not suited to the objective of this study that will improve the performance of the system for the state in which hot spots increase rapidly because the amount of the request for hot spots is very low. Therefore, this study produces a proper experimental data for the characteristics of this study using the Surge program [11] as a load generator that was developed in Boston University. Although the production of data was achieved by the same amount of the requested amount in the period of time request for the access logs file of the NASA server, the ex-perimental data was produced by the execution of the Surge program to request much higher than that of the general request of data for the amount of requests for hot spots. A hot spot prediction can be performed by analyzing some objects that have a high frequency of requests in the access logs file of a particular proxy. A prediction for the future request will be processed by using the existing amount of requests for hot spots from the information obtained from the objects being analyzed. A time series method using a multilayer perceptron neural network is applied as a prediction method.
 method to minimize the sum of the square of error. This learning method has a merit that it converges faster than that of the Backpropagation method, which uses a gradient descent method, due to the use of a second-order method. This paper selects 4 nodes for the input layer, 8 nodes for the hidden layer, and 1 node for the output layer for the neural network model. In addition, the gradi-ent of the activation function was applied by the value of 2. The learning was performed by scaling the values of 0.01  X  0.99 for all pattern inputs. experimental data and lower one shows the error values for each input pattern. Figure 3 presents the results of the requested amounts of hot spots and actual requested amounts of hot spots using the neural network. This process shows the results of the verification stage of a neural network modeling, and the data used in this process was not used in the learning process. The amounts of the requests presented in the figures are the converted values of the amounts of the requests for the output of the neural network that was previously scaled. The solid line shown in the figures presents the amounts of the request, the values marked by + are the amounts of the request for the predicted hot spots by using the neural network. The error rate between the two requests is 8.7%. It can be seen that the neural network modeling is good for considering the characteristics of the system. The prediction of hot spots will be performed by using the neural network predic-tion model as previously produced in Chapter 4. In addition, this paper compares the prefetching method that distributes the expected hot spots to the web caches in advance for the load distribution method proposed in this study with the ex-isting consistent hashing method through a computer simulation. The simulator that was configured in Chapter 2 will be used to the simulation. In addition, the configurations for a load balancing and various web caching are also investigated. 5.1 Performance Evaluation for the Load Balancing For the performance evaluation, this study compares the round-robin method using a DNS, which is based on the consistent hashing method, and the load balancing method using a hot spot prediction proposed in this paper. Figure 4 illustrates the standard deviation for the loads that are requested by the proxies when the loads are distributed to several shared proxies. Moreover, the num-bers of hot spots that are to be distributed and numbers of proxies that are to be prefetched are considered as the parameters in the simulation. For the distribution of hot spots, the tests were performed by various methods of the distributed prefetch in which the largest numbers of requests for hot spots were applied to multiple proxy servers that have the smallest load, and more than one hot spot was also applied to multiple proxy servers. As shown in the figure, the x-axis means the various methods used in the test, that is, it presents numbers of simulations by using a particular distribution method. The y-axis presents the standard deviation of load that is applied to the proxy. That is, the simulation was performed using various distribution methods for the 14 different cases as presented in the figure. As presented in the graph, the consistent hashing (10) means that hot spots are distributed to 10 proxies in the consistent hashing method, and HSPA (10,10) means that the two hot spots that have the highest frequency are distributed to each of the 10 proxies respectively using the HSPA algorithm. That is, a hot spot that has the highest frequency will be distributed to 10 proxies and that has the second highest frequency will also be applied to another 10 proxies. In this paper, a hot spot that has the highest frequency will be expressed by HS(1) and the second one will be expressed by HS(2). From the results of the simulation, it can be seen that the load distribution will be performed more smoothly up to two hot spots than that of one hot spot. In addition, in the case of the distribution for two hot spots, it shows the high-est performance in the load distribution that the HS(1) is applied to 8 proxies and HS(2) applied to 6 proxies. In the case of the distribution of multiple hot spots, therefore, it can be seen that the performance of the whole system will be affected according to the methods of the combination between numbers of the distributed proxies and hot spots.
 5.2 Performance Evaluation for the Hit Rate A comparison for the performances of the load balancing for the consistent hash-ing method and HSPA method was presented in the previous section. In order to examine the performance of the hit rate that is important for the performance of a web caching system, a test was applied to the two hot spots by combining various processing speeds of the server and cache sizes. The tests were applied to the proxies that have 1 second and 12 seconds respectively for the time required to process for one request. Figures 5 and 6 present the results of the distribution of the 10 proxies using the existing consistent hashing method and the suggested method of the hit rates according to the processing speed and load information of each proxy for the distribution of the two hot spots.
 for the time required to process for one request by the distribution of 10 proxies using the consistent hashing method and of 6 or 4 proxies using the proposed HSPA method. As a result, it is verified that the distribution of two hot spots for the shared proxies shows a higher hit rate than that of the load balancing method by the distribution of one hot spot. Moreover, it can be seen that the HSPA method shows the increase in the hit rate on the average compared with the consistent hashing method. From the results of various tests, the highest hit rate of the distributed fetch can be produced by the 6 proxies for the HS(1) and the same proxies for the HS(2).
 seconds for the time required to process for one request by the distribution of 10 proxies using the consistent hashing method and of 8 proxies using the proposed HSPA method. In this test, it presents the highest hit rate of the distributed patch in the case of the 8 proxies for the HS(1) and the same proxies for the HS(2). The HSPA method proposed in this study shows a higher hit rate than that of the consistent hashing method. size of the distribution proxy. The load distribution was applied to two hot spots in which the HS(1) that was distributed by 4 proxy servers and HS(2) that was also distributed by 4 proxy servers. A server that has the fast processing speed in the HSPA method shows the highest hit rate. For the slow processing speed, the highest hit rate can be obtained by the distribution of 8 proxies for HS(1) and HS(2) respectively.
 can be verified by the overall characteristics for the distributed web caching system. That is, numbers of the distributed proxies will decrease in the case of the small amount of work and fast processing speed. On the other hand, in the case of the large amount of work and slow processing speed, hot spots will be distributed to several proxies.
 This paper investigated the problems that are caused by hot spots in a dis-tributed web caching system and proposed the HSPA method that increases the performance of the load balancing and hit rate in a distributed web proxy system. The HSPA method produces a prediction model, which will predict hot spots possibly requested in the future. In addition, it will distribute a congestion of the request that will be caused by the expected hot spots in the future to the present proxy that has a low load by the prefetch using a prediction model. In order to verify the performance of the method proposed in this study, a test that includes a neural network modeling, configuration of the simulator system, con-sisting of the request data, load balancing, and performance of the hit rate was applied. Moreover, the consistent hashing algorithm used in the present method was implemented to compare the performance of the proposed system.
 by the load balancing test was better than that of the consistent hashing al-gorithm. For the test of the hit rate, in addition, it was investigated that the HSPA algorithm proposed by this study was better than the existing method by comparing the consistent hashing method in which the queue size that is the capacity of a processing speed and memory storage in the performance of a proxy server was decided differently. As a result, a hot spot prediction algorithm proposed in this study will increase the hit rate and application of the shared proxies for the distribution system.
 Acknowledgements. The authors thank KEPCO R-2004-B-120 and RRC (KOSEF) R-12-1998-026-02003 for their financial support.

