 Dong Yu dongyu@microsoft.com Microsoft Research, Redmond, USA Frank Seide fseide@microsoft.com Microsoft Research Asia, Beijing, P.R.C.
 Gang Li ganl@microsoft.com Microsoft Research Asia, Beijing, P.R.C.
 Context-dependent deep-neural-network HMMs (CD-DNN-HMMs) are a recently proposed acoustic-model-ing technique for HMM-based speech recognition [1, 2] that combines three techniques: the hybrid approach of modeling HMM state emission densities through scaled likelihoods from an MLP [3]; traditional acous-tic co-articulation modeling of speech through context-dependent phoneme models (crossword triphones with tied states); and deep networks, leveraging Hinton X  X  deep-belief-network (DBN) pre-training procedure. The power of this model was first shown through a 16% relative recognition error reduction over conventional CD-GMM-HMMs on a business search task [1, 2]. This work describes our subsequent e ff orts [4] on scaling it up in terms of training-data size (from 24 hours to 309), model complexity (from 761 output classes to 9304), depth (up to 9 hidden layers), and task (from voice queries to speech-to-text transcription). The model achieves a one-third word-error reduction on the publicly available benchmark of phone-call transcrip-tion (Switchboard 2000 NIST Hub5/RT03S-FSH). In HMM-based large-vocabulary speech recognition, speech is modeled by hidden Markov models (HMMs), where each word X  X  HMM is decomposed into phoneme HMMs. These are commonly three-state left-to-right HMMs, where each state X  X  emission probability is a mixture of Gaussians (GMM). Co-articulation is mod-eled by context-dependent (CD) phonemes, such as triphones . Due to data scarcity, triphone states are commonly tied with similar other states.
 A limitation of GMMs is their di ffi culty to use high-dimensional features, such as multiple consecutive frames of short-term spectral features. To address this, it was proposed in the early 90 X  X  to replace GMMs with artificial neural networks (ANNs). The ANNs are trained to classify observation vectors into HMM state labels [3], and state posteriors are converted to scaled likelihoods for use as HMM state emissions. However, these early attempts were limited to shallow models (1 X 2 hidden layers) and monophone states as ANN outputs (even when CD phones were modeled) [5, 6]. The CD-DNN-HMM extends these hybrid ANN-HMMs two-fold: First, we model tied triphone states directly. It was long assumed that thousands of tri-phone states were too many to be accurately modeled by an MLP, but [1] has shown that it works very well. Secondly, we use a deep MLP with with many hid-den layers. Many layers of simple non-linearities can model complicated non-linearities and are more e ffi -cient in representing structures since lower-layer fea-ture detectors can be reused by the higher-layer feature detectors. Also, each layer is constrained by the adja-cent layers and so it is less likely to cause over-fitting (although it is more likely to cause under-fitting). The key enablers to the training of these were the deep belief network (DBN) pre-training algorithm proposed by Hinton [7], as well as the advent of a ff ordable, mas-sively parallel computing devices (GPGPUs). Algo-rithm 1 summarizes the training procedure [4]. First, a conventional CD-GMM-HMMs is trained. Secondly, the DNN, after initialization as a DBN, is trained as a frame classifier, where the class labels are state labels assigned to each input frame through forced alignment using the CD-GMM-HMM. Midway, the alignment is updated once using the DNN model.
 Algorithm 1 CD-DNN-HMM Training Procedure We evaluate the e ff ectiveness of CD-DNN-HMMs on speech-to-text transcription of telephone conversa-tions, a considerably di ffi cult task. We use the pub-licly available 309-hour  X  X WBD-I X  training set and as-sociated benchmark sets, as well as two in-house sets. Recognition is single-pass without speaker adaptation. Table 1 shows that compared to our discriminatively trained CD-GMM-HMM baseline, the word-error rate (WER) on the  X  X T03S-FSH X  benchmark drops from 27.4% to 18.5% X  X  rather significant one-third reduc-tion. Much of the gain carries over to less well-matched sets (voicemail, teleconferences). The 309h CD-DNN-HMM system also reaches our best multi-pass system (18.6%, last row), which uses 6 times as much acoustic training data and speaker adaptation.
 Further experiments show that the deep network is in-deed critical X  X  shallow 1-hidden-layer network using the same number of parameters as the 7-hidden-layer one leads to five percentage points worse word-error rate. We also find that as an alternative to DBN pre-training, it is possible to discriminatively pre-train the model in a supervised layer-growing fashion. By using CD-DNN-HMMs, a one-third word-error re-duction has been achieved on a di ffi cult benchmark task, compared to a discriminatively trained conven-tional CD-GMM-HMM [4]. Recent improvements on smaller tasks [1, 2] do carry over to larger corpora and speech-to-text transcription. The remarkable accuracy gains are due to three factors: direct modeling of tied triphone states through the DNN; e ff ective exploita-tion of neighbor frames by the DNN; and the e ffi cient and e ff ective modeling ability of deeper networks.
