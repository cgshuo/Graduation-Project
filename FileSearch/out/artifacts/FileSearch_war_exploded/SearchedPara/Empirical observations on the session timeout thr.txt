 1. Introduction
With the explosion in web traffic and numerous companies being highly dependent on the web for their operations, data mining of web related information (web mining) is becoming increasingly important. Web mining ( Cooley, Mobasher, &amp; Sri-vastava, 1997, 1999 ) allows companies to further understand their users X  behavior and demographic information, which in workload information, such as hits per user or session, enabling system administrators to improve usability, availability and Ribeiro, 2002; Squillante, Yao, &amp; Li, 1999 ).

One of the most popular units used to analyze traffic, workload and user behavior is the session. For example, Chen, Fow-ler, and Fu (2003) presented several algorithms that allow web miners to efficiently calculate the number of user sessions with some session timeout threshold (STT). Pallis, Angelis, and Vakali (2005) proposed a technique to discover relationships between user sessions; the user sessions were identified using Chen et al. X  X  (2003) proposed technique. The session measure has also been investigated by many researchers (Go X eva-Popstojanova, Mazimdar, &amp; Singh, 2004; Go X eva-Popstojanova, no model has been proposed to estimate the STT used to generate session length data.

Many other studies have also concentrated on determining various session-related workloads based on a predefined con-stant value for STT. For example, Montgomery and Faloutsos (2001) defined STT to be 2 h long. Tian, Rudraraju, and Li (2004) used 15 min and 2 h as two different STTs; these two STTs were then applied to both websites investigated in that study.
Chen et al. (2003) and Go X eva-Popstojanova et al. (2006) assigned the STT value to be 30 min and use it for all websites in their studies. This 30 min value is also used by other researchers (Berendt, Mobasher, Spiliopoulou, &amp; Wiltshire, 2001;
Mahoui &amp; Cunningham, 2000; Mat-Hassan &amp; Levene, 2005; Spiliopoulou, Mobasher, Berendt, &amp; Nakagawa, 2003 ). Further-more, this 30 min figure is commonly used in many commercial web applications (Huang, Peng, An, &amp; Schuumans, 2004 ).
For example, Google Inc. uses the 30 min timeout value for their Analytics web application. value of 25.5 min (rounded up) determined by Catledge and Pitkow (1995) . While this standard period is often used, it is far from obvious that it provides any meaningful guidance in estimating user session lengths. In fact, a recent study shows that session lengths can be as long as 6 h and 32 min, the average period spent on RuneScape.com. Furthermore, with the advent of AJAX ( Garrett, 2005 ) and other interactive technologies, the session length values will be further impacted as websites X  interactivity features begin to rival that of desktop applications. Hence, using the same STT for all websites may not lead to accurate results.
 This paper has the following contributions:
A model, based on empirical observations, for estimating the STT is presented. Although the model has limitations, it pro-vides an initial step that will allow future studies to expand upon. Furthermore, this model is proven to be applicable at many different resolutions and to two uniquely different websites.

The concept that STT varies for each website is empirically proven. This encourages future research on web server logs to be performed using a customized STT value per website rather than a constant that X  X  applied to all websites. Empirical investigation on data sets with very long collection periods. The benefits are discussed in Section 4. The remaining sections of this paper are organized as follows: Section 2 discusses current approaches used to identify STT.
The new session threshold timeout model is proposed in Section 3. Section 4 provides a brief description of the websites under investigation, and the properties and characteristics of these websites. The results for this model when applied to the websites under investigation are discussed in Section 5. Finally, Section 6 presents our conclusions. 2. Related works
Other approaches have been proposed to calculate STT. For example, Catledge and Pitkow (1995) determined the STT to be 25.5 by claiming that the most statistically significant events occurred within 1.5 standard deviations (25.5 min) from the mean between each user interface event which was 9.3 min. However, no definition of  X  X  X ignificant events X  was supplied; and why 1.5 standard deviations are selected is never discussed. More importantly, only four percent of the accessed web pages were dynamic pages. Hence, the investigation was heavily based on static web content, which is increasingly rare in modern applications.

He and Goker (2000) performed an empirical investigation of the session value by initially setting STT to a very large va-lue, then slowly decreasing it until they achieved a stable point where the number of activities remains stable for both short sessions and long sessions. However, the data used was very limited. Not only that, they provided no formal definition of the stable point of the system and provided a range of values of  X  X  X omewhere between 10 and 15 min X  for STT. They also claimed that this range is suitable for all websites on the World Wide Web. This finding is questionable because of niche specific web-sites that can attract different user demographic groups. For example, users visiting www.youtube.com can spend a long per-iod online watching various video clips; while visitors of www.onlineconversions.com would use the site to perform quick metric conversions and quickly finish their sessions. Huang et al. (2004) proposed a dynamic approach to determining STT.
Basically, the approach tries to detect general behavioral patterns of website usage. The approach assumes that these pat-terns can be approximated by sequences of hypertext interactions. A session  X  X  X nds X  when a user deviates from a learned pattern. However, an approach to determine the parameters used and how session identification results can be measured are not discussed. Not only that, the time for learning or discovering patterns is unknown and the site cannot be updated as the learning needs to be repeated whenever evolution takes place.

Huntington, Nicholas, and Jamali (2008) proposed a set of STT X  X  based on the content retrieved by the user. They demon-strated that the STT for each content type can be retrieved based on the estimated view time (the time between the logged request to download the article and the next request) from the server log files. However, the method used to estimate the view time has several limitations. If the user requests a page, reads it, then closes the browser window without performing any additional action then the estimated view time would be inaccurate. Furthermore, most web pages contain multiple con-tent types. For example, a web page can contain both a Menu content type and an Abstract content type; the authors do not discuss a method for classifying these pages and how the STT can be retrieved from these multiple content pages.
While the previous papers successfully introduced the idea of a session timeout threshold, their treatment of the concept was either exceptionally brief, imprecise or contained many unsolved issues. Given, the relative importance of this metric, it is believed that this situation needs to be urgently resolved. However, no simple unique definition of this concept is likely to exist; and hence a protracted investigation is required. 3. Observations of the STT and the proposed model
A session is defined as a sequence of actions undertaken by a user within a period of time. Sessions offer much finer grained information than the standard number of users metric. However, because the Hyper Text Transfer Protocol (HTTP) is a stateless protocol, session information cannot be easily captured. Hence, web applications often use session-based tech-session ends and the next one begins, a session timeout threshold (STT) is often used. In other words, a STT is a pre-defined period of inactivity that allows web applications to determine when a new session occurs. That is, let t equal the session timeout threshold and s is the set of sessions: " s 2 Sessions For (user) (session_time_start( s i +1 )  X  session_time_end( s For web server logs, the STT is determined by the time between the current request and the previous request.
The user session metric is particularly interesting to web mining researchers because they provide finer grain of informa-tion than the usual user count. (Menasc X , Almeida, Fonseca, &amp; Mendes, 1999 ), Arlitt and Williamson (1997) and Pitkow for the number of sessions to be estimated accurately.

A STT is best viewed as a  X  X  X esign parameter X , a mechanism for website workload evaluation rather than a concept with an absolute definable theory. Hence, this discussion is best considered as an attempt to produce an initial model that will allow web administrators to estimate this design parameter for their websites. From a philosophical viewpoint, the definition of session timeout threshold has many of the characteristics of a  X  X  X icked problem X  (Rittel &amp; Webber, 1973 ). That is, the prob-lem has many complexities such as changing, incomplete or contradictory requirements. Hence, any solution will experience optimal solutions; or satisficing to utilize Simon X  X  term from economics (Simon, 1955 ). Our research recognizes these limitations.

Empirical observations of the number of sessions versus the STT show that the estimation of STT can be viewed as a par-titioning problem. The problem can be approached as a question of defining two regions or surfaces ( S resent the lengths before and after the threshold value ( x monotonically declines as STT increases. Within each region, the data points are  X  X  X elatively stable and vary smoothly. X  S represents a potentially steeply declining curve, where the choice of session length has a significant impact upon the result.
S is in fact two segments ( S 2 and S 3 ); S 2 can be characterized as a linear segment with a  X  X  X entle gradient X ; the choice in session length has limited impact in this region. Whereas, S be modeled by the constant number of sessions that it represents which is equal to the number of users and is the lower bound of the system. In terms of the model and its usage, the S
The behavior across S 1 and S 2 can be modeled as a rate of change statement, or, more specifically, as a requirement to min-imize the rate of change of the curvature across S 1 and the rate of change of the gradient across S
Hence, the problem can simply be recast as a question of finding the threshold value that minimizes the overall rate of change of the system given that the system consists of two separate regions. Although this paper also uses the minimal change as the STT as proposed by He and Goker (2000) , a mathematical model, which is lacking in the previous study, is now presented to describe this minimal change.

Mathematically, let f be a function which maps STTs to the number of sessions; where w tive becomes:
In reality the second term can be approximated because the number of sessions has the number of users as a strict lower bound and hence beyond this point the rate of change of the gradient is always zero.

In addition, the above model can be viewed as combining two different sets of workload information: users who do not plan to utilize the website. They are often directed to the website via a query entered into a search engine; however the site does not meet their requirements, but their decision cannot be made without them initially entering the site. Hence, on average, the sessions of users who briefly visit the site are extremely short and it is debatable whether these  X  X  X isits X  should be considered as genuine sessions. Commonly these users only have a single interaction with the website; these visits can be viewed as having zero duration. ple web pages before finishing their session. Some of these users include users who are purchasing a product through an online store. These users will visit multiple web pages looking for product information before purchasing the item through an interactive shopping cart. Here the websites meet the users X  requirements and provide information or services that actively engage the users. On average, these sessions are more extended than sessions of the first workload information.

It is far from obvious that this model should include data from both information sets; however, any partitioning is guar-anteed to be less than perfect. The model can be trivially extended to accommodate this possibility by replacing the constant lower bound of the first integration term.

Finally a suitable basis for the weighting terms needs to be defined. Unfortunately, no obvious theoretical basis exists. S shouldpossessasignificantlength,andhenceaweightingfunctionasafunctionoftheinverseofthelengthofthisregionseems appropriate. However, it is not clear that this concept yields any suitable formulation for S
Hence, the reformulated model is: where h P 0 ; x h &lt; x i &lt; x j ; and
While the model is now complete, it clearly has limitations in terms of numerical stability given the estimation of higher-order differentials. For elongated data collection periods, this should not present a problem because the aggregated data will, in general, experience an averaging or smoothing effect. However, for short-term data, we can expect that the data will devi-ate from long-terms norms and can be viewed as more  X  X  X oisy X . Hence, there exists a strong possibility that such short-term data may require a smoothing approximation before the data is presented to the model. Based on the model discussed, we propose to describe STT as the upper bound of the range denoted by the boundaries between S value at x i +1 in our model. 4. Description of the websites under investigation
This paper will investigate server logs from two websites. The first website is a website for a company that specializes in online databases (Site A). This website is a commercial website that is very critical to Company A X  X  operation. The website utilizes the PHP (http://www.php.net ) scripting language, MySQL
Daemon. 3 In order to observe potential trends and patterns for this website, the log files chosen cover 27 months of operation from December 2004 to February 2007. This website represents a typical business website. That is, the site is a dynamic website with a mixed amount of static and dynamic pages  X  these are pages generated dynamically depending on the customers X  inputs; its users are customers who are either looking to purchase a product or to register for a training course. The website contains several online databases. Users are charged for the time used to access these databases; these subscriptions are a main source of revenue for the company which is why the website is very critical to the organization. For the 27 months covered, Site A re-ceived approximately 1.9 million hits and 63,500  X  X  X nique X  visitors. The site transferred 33.5 Gbytes of data. The second website is www.ece.ualberta.ca , the website for the Department of Electrical and Computer Engineering at the
University of Alberta. This site, although important to the organization, is non-commercial and not mission critical. This web-site is a dynamic website that utilizes the ColdFusion 4 scripting language, and the Apache HTTP Daemon. To investigate the data, the log files were chosen to cover 11 months of data. For this period, the ECE website received approximately 2.42 million hits, 203,896  X  X  X nique X  visitors and transferred a total amount of 22.6 Gbytes of data. The data from this second website serves as a check to ensure that the trends observed with Site A are not unique to one particular website.

The log files under investigation are stored in the Common Log Format (CLF)
Site A. To provide maximum flexibility with the analysis, a custom log parser was created in Ruby. All necessary information was extracted and imported into a DBMS. The approach used can be seen as a deep log analysis technique ( Nicholas, Hunting-at least two requests: one to mark start time of the session and one to mark the end time of the session. Hence, all users with only one request are removed from the log files. That is, all IPs that only have one record in the data are removed. A total of 10,938 users and 28,336 users are removed for Site A and ECE, respectively.

Table 4.1 provides a summary of the properties of the logs used in previous studies and this study. Websites with an asterisk are commercial websites.

This table shows that the longest period that previous studies have collected data is over a 12 month period, compared to 27 months in this study. Furthermore, many of the previous studies are not performed on a commercial website. For studies that use logs from commercial websites, the periods covered are extremely short (50 min to 2 weeks). This study investigates the log file from a commercial website for a much longer period (27 months). Hence, it is believed that this study presents the first long-term analysis of a commercial website. This long data period provides several benefits over short data periods.
A short data collection period cannot truly capture users X  behaviors because their behavior is by definition variable; and only a single snapshot of their behavior is likely to be captured with the short data collection period. For example, a new user to a Wiki may simply read articles, once familiar with the website the user may choose to participate in other activ-ities such as posting comments, providing feedback or even editing articles.
An organization X  X  behavior also affects its website traffic patterns. Advertising campaigns, various public announcements will often increase the amount of traffic. For example, GoDaddy.com X  X  website experienced a 1500% increase in traffic fol-lowing its Super Bowl ad campaign. Other websites advertised during Super Bowl Sunder also had their traffics increased.
Short-term collection either overstates these actions if it is performed near a major activity or understates them if per-formed far from the activity.

Well known trends and periodic patterns such as the  X  X  X eekend effect X  will distort short-term collection resulting in skewed data. In fact, Arlitt and Jin (2000) have demonstrated that websites have very different workload intensities on weekdays versus weekends. Hence, if the data period is short, the analysis will be skewed by such effects.
Major web events will also affect the data sets gathered within a short time frame. For example, popular YouTube videos are known to result in millions of hits to YouTube X  X  website within a short period of time before the site X  X  traffic returns to normal. A website being mentioned on another popular website such as Slashdot will also cause the website X  X  traffic to increase. This is commonly known as the Slashdot-effect.

Short collection periods can experience distortion due to either higher than normal or lower than normal activities from robots. For example, many ticket scalpers use robots to automatically purchase event tickets from Ticketmaster when they first go on sale. This is especially true for popular events where tickets can be sold within minutes of being available online. Short collection periods would result in skewed data from the activities of these robots.

Users have very low brand loyalty. If quality of service (such as response period) is poor, users leave quicker than normal (the inverse will be at some-level true)  X  this impacts session statistics and again short-collection periods can get skewed because of the quality of service differing from the long-term norm. For example, a user many visit a website during main-tenance which may cause the website to response much slower than usual. The quality of service during this maintenance period cannot be considered as the normal QoS for the website. 5. STT results and discussion
In order to apply the model discussed, the effects that different threshold values have on the total number of sessions are calculated for the two websites under investigation. The explored STT values are from 1 to 120 min in 1 min intervals. To search for repeating effects, four different resolutions are investigated: days, weeks and months.
 5.1. Removing automated requests
While applying the model, it was discovered that robots and other automated systems used to request resources need to be removed from the server logs in order for the model to be used effectively. That is, systems that automatically request a resource from the website after a set period of time will cause the model X  X  description of the regions to be incorrect. For example, Fig. 5.1 shows the number of sessions versus STT before the removal of several site monitoring systems from the log files for Site A.

This figure shows many distinct regions. Close examination of the server log reveals that two monitoring services are used to monitor the website X  X  status. The first service requests a resource from Site A every 30 min while the second service re-quests a resource from Site A every 66 min. Removal of these records from the server logs was simple because the resources these services request are unique and are not publicly available. ECE X  X  log files were also parsed to remove robots that auto-matically request the  X  X  X obots.txt X  resource every 60 min.

Although, it is infeasible to remove all automated requests from the server logs, web administrators need to remove all identifiable requests. Several techniques to identify them can be used by web administrators to remove automated requests.
Most well known robots have a signature line that is included with every request as part of the USER AGENT field of the log file. For example,  X  X  X ooglebot-Image/1.0 X  can be used to identify a robot from Google that is indexing the website X  X  images. For web monitoring services, web administrators can simply dedicate a special resource that only these services can access.
This resource can then be easily identified within the log files. Armed with adequate information, web administrators can eliminate most automated requests from the server logs which will enable the STT to be estimated more accurately. For the two websites under examination, 77,530 automated  X  X  X sers X  are removed from ECE and 34,625 automated  X  X  X sers X  are removed from Site A. 5.2. Day resolution investigation
One hundred weekdays and 50 days on the weekends for each website were randomly chosen for this investigation. Two sample day graphs for the websites can be seen in Figs. 5.2 and 5.3.

Informal observations show distinct surface regions in Figs. 5.2 and 5.3 which suggests that the model is applicable at this resolution. Hence, the model was applied and the x i +1 (STT) results for each website were obtained. Unfortunately, nothing is known about the distributional characteristics of this value, and hence we utilized both parametric and non-parametric mea-sures to explore this concept. The results, presented in Table 5.1 and Figs. 5.4 X 5.7 , show that the two websites have very different workload intensities and behaviors, which suggests that they are unlikely to share the same STT value.
These results show that the values at x i +1 are not stable and vary depending on the day under observation. This is expected because web workloads (similar to other workloads) have day and weekly periodicity; hence different days in the week usu-ally have different workload behaviors and intensities. In order to determine differences between the weekdays versus weekends dataset, an F -test was performed using 50 random samples from the weekday and weekend datasets. Table 5.2 displays the results.
 The results show that the null hypothesis (the weekdays and weekends workloads are the same) can be rejected for Site A. However, for the ECE datasets, the results are less clear.

The mean x i +1 value for Site A is less than ECE; furthermore, Site A, with a standard deviation of 2.13 for weekdays and 1.13 for weekends, has a tighter spread of values compared to ECE. The STT is smaller on weekends for both websites which further confirms web traffic generalization as discussed by Arlitt and Jin (2000) and Pitkow (1999) . The differences in results may be attributed by the different user profiles the two websites experienced. A hypothesis can be proposed based on the fact that users are charged for usage time for Site A, hence, the STTs are generally shorter. Clearly, this hypothesis needs more exploration before it can be confirmed.
The results seem to empirically demonstrate that the threshold does exist for this  X  X  X ine resolution X  data; while some var-iation exists the results by no means look random or unsystematic. Using SPSS, several QQ plots for the following distribu-tions were examined: Chi-square, Exponential, Gamma, Half-normal, Laplace, Lognormal, Normal, Pareto, Student t , Weibull, and Uniform. The estimated STT data shows a  X  X  X easonable X  fit to several of the distributions; Figs. 5.8 and 5.9 show the QQ plots for two  X  X  X ossible X  distributions, Normal and Gamma, respectively.
 This possibility is explored more formally, but only for a normal distribution. We applied the Shapiro X  X ilk test ( Shapiro &amp;
Wilk, 1972 ) to the data sets. This test was selected as it tends to be more powerful than other common normality tests, such as Anderson X  X arling and Kolmogorov X  X mirnov (Stevens &amp; D X  X gostino, 1986 ), and does not require that the mean or variance of the hypothesized normal distribution to be specified in advance. Table 5.3 displays the results using the Shapiro X  X ilk test function in SPSS. The results show that three of the four p -values for both websites are less than 0.05; hence, these three data sets are (probably) not normally distributed. 5.3. Week resolution investigation In order to examine the week resolution, 50 random weeks were chosen for Site A and 25 random weeks were chosen for ECE. Figs. 5.10 and 5.11 show the number of sessions versus STT for two of the weeks.

As was expected, on average, the week resolution curves are smoother than the day resolution curves. However, Figs. 5.10 and 5.11 still clearly display distinct surface regions which, again, suggest the applicability of the model. The x from applying the model to these random weeks are shown in Table 5.4 and Figs. 5.12 and 5.13 .

These results show that the STT for both websites have settled between the STT for weekdays and weekends. This is ex-pected because of the averaging effect. However, Site A and ECE still have different STT values, which mean that the STT should not be a single constant for all websites at this resolution. QQ plots for several distributions at this resolution have broadly the same results as the day resolution.

Table 5.5 shows results from the Shapiro X  X ilk test which again implies that both data sets possess data which is prob-ably non-normal.
 5.4. Month resolution investigation
Figs. 5.14 and 5.15 show two sample graphs for ECE and Site A at one month resolution. In total, all 27 months were inves-tigated for Site A, and all 11 months were investigated for ECE.
Initial observations show that Fig. 5.14 displays a seemingly smooth curve due to the averaging effect. However, Fig. 5.15 displays visible distinct surfaces as described in the model. Determining t by casually observing the figures is now difficult and error-prone. Using the provided model, the x i +1 (STT) values can be calculated for all the graphs. Table 5.6 and Figs. 5.16 and 5.17 display the results from the calculations. QQ plots for various distributions at this resolution again show broadly the same results as obtained in the week and day resolutions.

The results show that both the variation and the range are reduced with the increase in the collection period; and that the possibility exists that any estimation of t at this level of resolution might be considered as a  X  X  X ong-term X  norm. However, this can only safely be performed if the administrator knows that there have been no major modifications to the website, which may affect its users X  behaviors or the usability of the website. That is, if the operational profile of the website remains sta-tionary. For example, an online store may add a  X  X  X sers X  Review X  section which causes users to spend more time at the store to read the reviews. If an administrator uses a 1 month period before this feature is added, the t value calculated from x will be different than the 1 month period after this feature is added. Table 5.7 shows the results from the Shapiro X  X ilk test; and at this resolution, data from both websites needs to be considered as again (probably) non-normal. An investigation of the behavior of STT for the total period of investigation for both websites repeats the previous patterns; the details are omit-ted for the sake of brevity. 6. Conclusions
STT is an important value used to investigate the session workload. As discussed, many researchers use a single static value for the STT. Not only that, the commonly used value (30 min) cannot accurately portray the many different user pro-files that websites possess. Having a more accurate STT will allow the session workload to be estimated more accurately. This is important in web mining as accurate data will allow companies to increase the usability and functionality of their web-sites. Hence, through empirical observations, this paper introduces a model aimed at enabling web administrators to be able to determine an accurate STT value for their website. The paper shows that using a single STT for all websites is not feasible due to the different user profiles for each website. The model is then applied to two websites. The first website is from a commercial website that is critical to the company X  X  operation. The second website is an academic website that serves as a cross reference of the results.

The model, when applied at different resolutions, shows that the results are similar with the STT being shifted depending on the workload and intensity of the resolution. That is, the resulting graphs all have very similar shapes. The results show that the model, while applicable, should only be used at the correct resolution due to different workload behaviors and intensities. Web administrators looking to study the session workload unit should obtain all possible requirements for the study before deciding the correct resolution for applying the model. For example, a study examining user behaviors per month should apply the model at the month resolution. For a long-term overall trend, the month resolution can also be used, however, the STT should be recalculated if there are any modifications that can affect the website X  X  usability.
Although an absolute correct value for the STT cannot be guaranteed; it is believed that the STT can be estimated more accurately with the proposed model. The model, while simple, allows researchers and web administrators a more dynamic method to obtain STTs that specifically target the websites that they are investigating. Having a more accurate STT is ben-eficial because it allows more accurate data to be mined while also improving the security and usability of a website. For example, if an online banking website uses an extremely short STT then the user is constantly required to login which de-creases the site X  X  usability. However, if it uses an extremely long STT then the risk of the user X  X  account being compromised is increased due to attacks such as XRF ( Shiflett, 2004 ), and XSS (OWASP., 2008 ). Furthermore, having an inaccurate STT can lead to session addition (division of a session into two new ones) and subtraction (combining two short sessions into a long one) errors as discussed by Huntington et al. (2008) .

The QQ plot results show that the STT data can be fitted by more than one distribution. However, besides the normal dis-tribution, no further formal tests were performed to determine which distributions can be used. Our future work includes formal exploration to determine whether single parametric distributions can be used to describe the model and whether a single  X  X  X ptimal X  distribution exists from a casual perspective.
 References
