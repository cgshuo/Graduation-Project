
In this paper we investigate the general problem of discov-ering recurrent patterns that are embedded in categorical sequences. An important real-world problem of this na-ture is motif discovery in DNA sequences. We investigate the fundamental aspects of this data mining problem that can make discovery "easy" or "hard." We present a gen-eral framework for characterizing learning in this context by deriving the Bayes error rate for this problem under a 
Markov assumption. The Bayes error framework demon-strates why certain patterns are much harder to discover than others. It also explains the role of different parameters such as pattern length and pattern frequency in sequential discovery. We demonstrate how the Bayes error can be used to calibrate existing discovery algorithms, providing a lower bound on achievable performance. We discuss a number of fundamental issues that characterize sequential pattern dis-covery in this context, present a variety of empirical results to complement and verify the theoretical analysis, and apply our methodology to real-world motif-discovery problems in computational biology. 
Data sets in the form of categorical sequences (defined on a finite alphabet of symbols) frequently occur in a va-riety of real-world applications. Examples of such applica-tions include computational biology (DNA, RNA, and pro-tein sequences), telecommunication networks (alarm mes-sage sequences), and user modeling (sequences of Web-page requests). An important data mining problem in this con-text is the unsupervised discovery of recurrent patterns in such sequences, i.e., the detection and discovery of relatively short, relatively rare, possibly noisy, repeated substrings in the data. What makes the problem difficult is that relatively little is known a priori about what these patterns may look like and there are typically a combinatorially-large number of possible patterns that could be present. permission and/or a fee. SIGKDD '02 Edmonton, Alberta, Canada Copyright 2002 ACM 1-58113-567-X/02/0007 ... $5.00. viewpoint. What is the effect on pattern discovery of alpha-bet size? of sequence length? of pattern frequency? There is a long tradition in statistical pattern recognition and ma-chine learning of providing mathematical bounds on the dif-ficulty of learning problems as a function of fundamental problem characteristics. Well-known examples of this ap-proach for multivariate classification problems include the 
Bayes error rate as a lower bound on the average error rate of any possible classifier for a given set of features (Chow 1957; Duda and Hart, 1973; McLachlan, 1992 (chapter 1 in particular); Ripley, 1996) and the risk minimization frame-work for upper-bounding test error rates (Vapnik, 1998). 
Prior work on the Bayes error rate has led to fundamen-tal and important insights into the nature of classification in multivariate feature spaces. In particular, the Bayes er-ror rate provides a theoretical target (it terms of the lowest achievable average error rate) for the performance of any classifier on a given problem. The Bayes error rate can only be computed exactly if we know the true conditional densi-ties (or distributions) and class probabilities for the classifi-cation problem, e.g., if we assume the classes are multivari-ate Gaussian. For most practical problems of course the true distributions are not known, but nonetheless, these theoret-ical results provide fundamental insight into the nature of multivariate classification problems and quantify the role of problem dimensionality, class separation, and so forth (e.g., see Chapter 3.8 in Duda and Hart (1973) for a quantification of how dimensionality affects the Bayes error rate under a Ganssian model). 
In this paper we will apply the general Bayes error rate framework to sequential pattern discovery problems. As with prior work on the Bayes error rate, we will need to as-sume that the data are being generated by a specific type of model in order to compute the Bayes error. In particular, we will use a first-order Markov framework as our base model. 
If we make an analogy with the role of multivariate Gaussian models for classification, the first-order Markov model can be viewed as much less restrictive in scope (for sequential data) than Gaussians are for the multivariate case. Thus, we will use a Markov model as a useful baseline reference framework for characterizing the pattern discovery problem, focusing on a particular class of Markov patterns that can be modeled by a hidden Markov model with certain con-straints. 
It is important to understand the nature of learning with this baseline model before we can understand detection and discovery more complex pattern structures. For example, if learning is hard even in the ease where the correct functional form of the model is assumed known, it is reasonable to infer that real-world problems (where we may not be able to assume knowledge of the correct form of the pattern) will be even harder. 
Among sequential pattern discovery applications, motif-discovery in computational biology is the single application in this general class of problems that has received the most attention in prior work. It is certainly an important motiva-tor for the work we will describe in this paper. Nonetheless our focus will be on understanding a general Class of pattern discovery problems in sequences, with a view towards under-standing the fundamental nature of this rather challenging unsupervised learning problem. 
The primary novel contributions of this paper are as fol-lows: 
Figure 1: Example of the HMM state transitions for a pattern whose most likely instantiation is ABBD in a uniform background sequence. of motifs (e.g., one per sequence) and their exact or ex-pected lengths, but there is typically little or no knowledge on where the motifs occur in each sequence or what symbols they contain. sequence, we will use hidden Markov models (HMMs), where there is a hidden state for each position in the pattern (L states for patterns of length L), and a background state to model the background. We can think of the HMM as a generative' process (a method for simulating sequences) for generating patterns of length L, with a particular consensus pattern, a noise level 6, and frequency F. Specifically, we assume a Markov process with L + 1 states, consisting of a background state B and L pattern states, P1 to PL. simplest case of fixed-length patterns we assume a strictly "linear" state transition matrix, where the background state 
B can only transition to either itself or to the first pattern state P1, where each pattern state Pi can only transition to state P~+I, 1 _&lt; i _&lt; L -1, and where PL can only transi-tion back to the background state B. This can be viewed as being similar to the product multinomial model for block motifs suggested by Liu et al. (1995). Under this assump-tion, two occurrences of a pattern can only differ due to substitution errors. We will later relax this assumption to allow for insertion and deletion states. sequence on the L + 1 states. The state-sequence gener-ated in this manner is not directly observed, hence, the den nature of the Markov model. We will let hj denote the value of the hidden state variable at position j, where ha E {B, P1,... ,PL}. The generative model produces an observed symbol for each hidden state value h a in the se-quence. The symbols are produced according to a state-specific multinomial distribution. For the background state 
B the multinomial distribution on symbols corresponds to the frequency with which each symbol appears in the back-ground (we use a uniform distribution by default). For the pattern states Pi, 1 &lt; i &lt; L, the multinomial probability is 1 -(L -1)E for the consensus symbol in position i and e for the L -1 non-consensus symbols. Thus each pattern state has an output multinomial distribution that is typi-cally "tuned" to a specific symbol for that position. 4 for a consensus pattern ABBD, using a 5-state HMM. The background state is characterized by a high entropy distri-bution for the background frequencies of symbols in the se-quences. Emissions in each of the L pattern states have low entropy with the probability mass concentrated on the consensus symbol in the corresponding position of the pat-
Figure 2: Analytical and empirical estimates of the normalized probability of error as the symbol sub-stitution probability varies, L = 10, F = 0.01. 
Figure 3: Analytical and empirical estimates of the normalized probability of error as the pattern fre-quency varies, L = 10, n~ = 1. est. Figure 2 shows how both empirical and analytical es-timates of the normalized probability of error change as we vary the expected number of substitution errors. The dots correspond to empirical evaluations of the Bayes error on se-quences of length 10 ~, and the dotted and solid lines plotted next to each other correspond to the analytical approxima-tion under the IID and IID-pure assumptions respectively. 
Figure 3 shows the normalized Bayes error as we vary the pattern frequency while the pattern length and substitution probability are fixed. Note that the empirical estimates be-come very noisy as the probability of patterns approaches zero, and an accurate empirical evaluation of the Bayes error becomes very expensive eomputationally due to the lengths of sequences required to get accurate results. Also, the solid line that corresponds to the analytical approximation cor-rectly captures the non-linearity of this dependence. The "switching" that is seen on these plots occurs whenever sub-strings with one more substitution relative to the consensus pattern become recognized as the patterns. We also see that the IID-pure approximation (dotted line) starts to deviate from the empirical results only when the marginal pattern 
Plugging in the specific values for our hypothetical problem, even the optimal detection algorithm will in-correctly misclassify on the order of 20% of all pattern symbols even if no substitutions are present in the pat-tern model. Naturally, allowing substitutions can only increase this error rate.  X  Given the pattern length and pattern frequency, what is the substitution probability e such that the opti-mal procedure misses all of the patterns and classifies them as background? All patterns will be recognized as background if 
In our hypothetical problem this corresponds to a sub-stitution probability of e = 0.28. Equivalently, if the average number of substitutions is greater than ns = 1.39, the optimal procedure will miss all of the pat-terns, and classify them all as background.  X  Given a particular L and e, what is the value of the pattern frequency such that the optimal procedure misses all of the patterns and classifies them as background? 
All oi:currences of a pattern will be classified as the background if 
In our example, if the pattern frequency is less than 3 in a thousand, the optimal procedure misses all the patterns, and classifies them as background (with e = 0.28).  X  If we fix the pattern frequency, pattern length and probability of substitution, we can find the maximum number of substitutions k  X , such that substrings with up to k* substitutions are recognized as patterns, and all others are classified as background. The number k* is given by the following expression: 
In the hypothetical example above, occurrences of the consensus pattern with even a single substitution error will be classified as background.  X  What is the expected false positive/false negative rate of the optimal procedure? This quantity can be com-puted using k* from the calculations above. In our toy problem, the normalized Bayes error rate is equal to 0.87. Approximately 23% of these errors will be made due to false negatives, and 77% due to false pos-
Rives, i.e., the optimal detection procedure is going to overestimate substantially the number of patterns even when the true model is known. This dominance of false positives in the Bayes error expression is typ-ical for these problems, and provides a theoretical ex-planation for the observed empirical fact that pattern detection algorithms for biological sequences tend to systematically suffer from high false positive rates (see Robison et al., 1998). 
Figure 7: Normalized probability of error for the learned models as a function of training set size with L=10, F=0.01, n,=2. simulated problems. We used the same shifting heuristic for restarting the EM algorithm between runs for both IID EM and HMM to avoid partial solutions (see Liu et ai. (1995), 
Bailey and Elkan (1995) and Chudova and Smyth (2002) for details). 
We performed experiments with data simulated from known true HMM models in the class of challenge problems from 
Pevzner and Sze (2000). The performance of the models was measured by the normalized error rate on relatively long se-quences of unseen data to provide stable estimates of the error rate. Note that this error rate can exceed a value of 1 for estimated models, i.e., the error rate of some models is higher than simply assigning all symbols to the background class, which has a normalized error rate of 1 by definition. 
We studied the effect of increasing the size of the train-ing data for problems with different Bayes error rates. The length of the training sequence was varied from 1K to 16K symbols. For a given training set size, the estimated model can differ from the original true model due to (1) noise in the actual training patterns, and (2) ambiguity in locating the boundaries of the patterns within the training data. The results below allow us to explore the contribution of each of these two factors to the overall algorithm performance. Figures 7 and 8 illustrate the results of these experiments. 
Each point on the plots was obtained by averaging over 20 different training sets of the same size for the same true model. On both plots, the lowest curve shows the nor-malized Bayes error rate of the problem (given by the true model)--the lower bound on the probability of error of any covering the patterns. 
The lower two curves in Figure 7 are worth discussing fur-ther. The lowest one is the Bayes error rate (estimated em-pirically here) and is the best any discovery algorithm could possibly do on this problem (e.g., using the true model, with an infinite amount of training data). The next curve is the performance of the HMM algorithm where it is told the lo-cations of the patterns (also estimated empirically using a standard supervised HMM training algorithm). The differ-ence between these two lower curves is in effect the con-tribution to the error rate simply from parameter estima-tion of the multinomials, i.e., because of small sample sizes, even if one were told where the patterns are, one would still get noisy estimates of the pattern parameters and this con-tributes to additional error. We can call this contribution to the error the "parameter estimation error." 
The three "real" algorithms must of course also discover the parameter locations, and naturally their error curve is systematically higher than for the "known location" curve. 
In fact, the distance between a real algorithm curve and the "known location" curve can be considered a contribution to error that is coming from not knowing where the parameters are, a "pattern location error." 
This allows us to decompose the total error of any algo-rithm into three additive components: (1) the basic Bayes error, (2) additional error due to noise in the parameter es-timates (the difference between the Bayes error and "known location" curves) and, (3) further additional error from not knowing where the patterns are located (the difference be-tween the "known location" curves and the real algorithm curves). This tells us, for example, that we can only ex-pect a pattern discovery algorithm to have high accuracy in terms of pattern detection if all three error terms are close to zero, i.e., (1) the pattern itself is easily detectable even when known (the Bayes error is close to 0), (2) the parameter esti-mation noise is low (typically implying that we have a large amount of data or prior knowledge), and (3) that the algo-rithm being used can efficiently discover the patterns given that the other two constraints are met. 
The curves for the unsupervised algorithms are quite close to each other across different values of the training set size, indicating that the IID model is quite adequate for non-structured patterns, and that the EM and Gibbs learning algorithms axe both equally successful in finding the pattern locations on these simulated problems. From a practical viewpoint it is interesting to note that the algorithms require significantly different computation times for learning. While the IID Gibbs can fit a model in seconds, the HMM can take several minutes due to the more complex forward-backward calculations. 
Finally, we consider an application of the Bayes error rate framework to actual motif-finding problems in DNA se-quences. Evaluation of the Bayes error rate requires knowl-edge of the true data generating model, which is not possi-ble for real-world problems. However, if the locations of the instances of a given pattern are known from experimental data, then one can consider the supervised version of the problem and construct the corresponding model from these instances (which is what we do here). Given a reasonable number of instances of the pattern, we would expect the error rate to approximate the Bayes error of the true model. with different Bayes error rates. demonstrated the use of the Bayes error rate framework for analyzing problems of this nature in a Markov context. In particular the Bayes error provides insight on how different factors influence the learnability of certain types of patterns. 
For the particular problem of motif-discovery, we discovered that for both simulated and real data sets the Bayes error rate can be quite high. Directions for future work include multivariate extensions and the design and application of new algorithms for pattern finding. 
This work was supported by the National Science Foun-dation under grants IIS-9703120 and IIS-0083489, and by grants from NASA and the Jet Propulsion Laboratory, the National Institute of Standards and Technology, Lawrence 
Livermore National Laboratory, the UCI Cancer Center, Mi-crosoft Research, and an IBM Faculty Partnership award. Bailey, T. and Elkan C. (1995) Unsupervised learning of 
Baldi, P., Chauvin, Y., McClure, M. and Hunkapiller, T. Buhler, J., and Tompa, M. (2001), Proceedings off the Fifth 
Chow, C. K. (1962) A recognition method using neighbor 
Chu, J. T. (1974) Error Bounds for a Contextual Recog-
Cliudova, D. and Smyth P. (2002) Pattern discovery in se-
Duda, R. O., and Hart, P. E. (1973) Pattern Recognition Eddy, S.R. (1995) Multiple Alignment Using Hidden Markov Van Helden, J.V., Abdre, B, and Collado-Vides, J. (1998) 
