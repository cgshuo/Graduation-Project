 Pieter Abbeel pabbeel@cs.st anf ord.edu Andrew Y. Ng ang@cs.st anf ord.edu Giv en a sequen tial decision making problem posed in the Mark ov decision pro cess (MDP) formalism, a num-ber of standard algorithms exist for nding an optimal or near-optimal policy . In the MDP setting, we typi-cally assume that a rew ard function is given. Giv en a rew ard function and the MDPs state transition prob-abilities, the value function and optimal policy are ex-actly determined.
 The MDP formalism is useful for man y problems be-cause it is often easier to specify the rew ard function than to directly specify the value function (and/or op-timal policy). However, we believ e that even the re-ward function is frequen tly dicult to specify man u-ally . Consider, for example, the task of high way driv-ing. When driving, we typically trade o man y dif-feren t desiderata, suc h as main taining safe follo wing distance, keeping away from the curb, staying far from any pedestrians, main taining a reasonable speed, per-haps a sligh t preference for driving in the middle lane, not changing lanes too often, and so on . . . . To specify a rew ard function for the driving task, we would have to assign a set of weigh ts stating exactly how we would like to trade o these di eren t factors. Despite being able to driv e comp eten tly, the authors do not believ e they can con den tly specify a speci c rew ard function for the task of \driving well." 1 In practice, this means that the rew ard function is of-ten man ually tweak ed (cf. rew ard shaping, Ng et al., 1999) until the desired beha vior is obtained. From con-versations with engineers in industry and our own ex-perience in applying reinforcemen t learning algorithms to sev eral rob ots, we believ e that, for man y problems, the dicult y of man ually specifying a rew ard function represen ts a signi can t barrier to the broader appli-cabilit y of reinforcemen t learning and optimal con trol algorithms.
 When teac hing a young adult to driv e, rather than telling them what the rew ard function is, it is much easier and more natural to demonstrate driving to them, and have them learn from the demonstration. The task of learning from an exp ert is called appr en-ticeship learning (also learning by watching, imitation learning, or learning from demonstration).
 A num ber of approac hes have been prop osed for ap-pren ticeship learning in various applications. Most of these metho ds try to directly mimic the demonstrator by applying a sup ervised learning algorithm to learn a direct mapping from the states to the actions. This literature is too wide to surv ey here, but some ex-amples include Samm ut et al. (1992); Kuniy oshi et al. (1994); Demiris &amp; Hayes (1994); Amit &amp; Mataric (2002); Pomerleau (1989). One notable exception is given in Atkeson &amp; Schaal (1997). They considered the problem of having a rob ot arm follo w a demonstrated tra jectory , and used a rew ard function that quadrat-ically penalizes deviation from the desired tra jectory . Note however, that this metho d is applicable only to problems where the task is to mimic the exp ert's tra-jectory . For high way driving, blindly follo wing the ex-pert's tra jectory would not work, because the pattern of trac encoun tered is di eren t eac h time. Giv en that the entire eld of reinforcemen t learning is founded on the presupp osition that the rew ard func-tion, rather than the policy or the value function, is the most succinct, robust, and transferable de nition of the task, it seems natural to consider an approac h to appren ticeship learning whereb y the rew ard function is learned. 2 The problem of deriving a rew ard function from ob-serv ed beha vior is referred to as inverse reinforcemen t learning (Ng &amp; Russell, 2000). In this pap er, we assume that the exp ert is trying (without necessar-ily succeeding) to optimize an unkno wn rew ard func-tion that can be expressed as a linear com bination of kno wn \features." Even though we cannot guaran tee that our algorithms will correctly reco ver the exp ert's true rew ard function, we sho w that our algorithm will nonetheless nd a policy that performs as well as the exp ert, where performance is measured with resp ect to the exp ert's unknown rew ard function. A ( nite-state) Mark ov decision pro cess (MDP) is a tu-ple ( S; A; T; ; D; R ), where S is a nite set of states; A is a set of actions; T = f P sa g is a set of state transition probabilities (here, P sa is the state transition distribu-tion upon taking action a in state s ); 2 [0 ; 1) is a discoun t factor; D is the initial-state distribution, from whic h the start state s 0 is dra wn; and R : S 7! A is the rew ard function, whic h we assume to be bounded in absolute value by 1. We let MDP n R denote an MDP without a rew ard function, i.e., a tuple of the form ( S; A; T; ; D ).
 We assume that there is some vector of features : S ! [0 ; 1] k over states, and that there is some \true" rew ard function R ( s ) = w ( s ), where w 2 R k . 3 In order to ensure that the rew ards are bounded by 1, we also assume k w k 1 1. In the driving domain, migh t be a vector of features indicating the di eren t desiderata in driving that we would like to trade o , suc h as whether we have just collided with another car, whether we're driving in the middle lane, and so on. The (unkno wn) vector w speci es the relativ e weigh ting between these desiderata.
 A policy is a mapping from states to probabilit y distributions over actions. The value of a policy is Here, the exp ectation is tak en with resp ect to the ran-dom state sequence s 0 ; s 1 ; : : : dra wn by starting from a state s 0 D , and picking actions according to . We de ne the exp ected discoun ted accum ulated fea-ture value vector ( ), or more succinctly the feature expectations , to be Using this notation, the value of a policy may be writ-ten E s R is expressible as a linear com bination of the fea-tures , the feature exp ectations for a given policy completely determine the exp ected sum of discoun ted rew ards for acting according to that policy . Let denote the set of stationary policies for an MDP . Giv en two policies 1 ; 2 2 , we can construct a new policy 3 by mixing them together. Speci cally , imag-ine that 3 operates by ipping a coin with bias , and with probabilit y picks and alw ays acts according to , and with probabilit y 1 alw ays acts according to 2 . From linearit y of exp ectation, clearly we have that ( 3 ) = ( 1 ) + (1 ) ( 2 ). Note that the randomization step selecting between 1 and 2 occurs only once at the start of a tra jectory , and not on ev-ery step tak en in the MDP . More generally , if we have found some set of policies 1 ; : : : ; d , and want to nd a new policy whose feature exp ectations vector is a con vex com bination P n i =1 i ( i ) ( i 0 ; P i i = 1) of these policies', then we can do so by mixing together the policies 1 ; : : : ; d , where the probabilit y of picking i is given by i .
 We assume access to demonstrations by some exp ert
E . Speci cally , we assume the abilit y to observ e tra jectories (state sequences) generated by the exp ert starting from s 0 D and taking actions according to
E . It may be helpful to think of the E as the optimal policy under the rew ard function R = w T , though we do not require this to hold.
 For our algorithm, we will require an estimate of the exp ert's feature exp ectations E = ( E ). Speci -cally , given a set of m tra jectories f s ( i ) 0 ; s ( i ) generated by the exp ert, we denote the empirical esti-mate for E by 4 In the sequel, we also assume access to a reinforcemen t learning (RL) algorithm that can be used to solv e an MDP n R augmen ted with a rew ard function R = w T . For simplicit y of exp osition, we will assume that the RL algorithm returns the optimal policy . The general-ization to appro ximate RL algorithms o ers no special diculties; see the full pap er. (Abb eel &amp; Ng, 2004) The problem is the follo wing: Giv en an MDP n R, a feature mapping and the exp ert's feature exp ecta-tions E , nd a policy whose performance is close to that of the exp ert's, on the unknown rew ard function R = w T . To accomplish this, we will nd a policy ~ suc h that k (~ ) E k 2 . For suc h a ~ , we would have that for any w 2 R k ( k w k 1 1), The rst inequalit y follo ws from the fact that j x T y j k x k 2 k y k 2 , and the second from k w k 2 k w k 1 1. So the problem is reduced to nding a policy ~ that induces feature exp ectations (~ ) close to E . Our appren ticeship learning algorithm for nding suc h a policy ~ is as follo ws: Upon termination, the algorithm returns f ( i ) : i = 0 : : : n g .
 Let us examine the algorithm in detail. On iteration i , we have already found some policies (0) ; : : : ; ( i 1) The optimization in step 2 can be view ed as an inverse reinforcemen t learning step in whic h we try to guess the rew ard function being optimized by the exp ert. The maximization in that step is equiv alen tly written max t;w t (10) From Eq. (11), we see the algorithm is trying to nd a rew ard function R = w ( i ) suc h that E ward on whic h the exp ert does better, by a \margin" of t , than any of the i policies we had found previously . This step is similar to one used in (Ng &amp; Russell, 2000), but unlik e the algorithms given there, because of the 2-norm constrain t on w it cannot be posed as a linear program (LP), but only as a quadratic program. 5 Readers familiar with supp ort vector mac hines (SVMs) will also recognize this optimization as be-ing equiv alen t to nding the maxim um margin hyper-plane separating two sets of points. (Vapnik, 1998) The equiv alence is obtained by asso ciating a lab el 1 with the exp ert's feature exp ectations E , and a lab el 1 with the feature exp ectations f ( ( j ) ) : j = 0 :: ( i 1) g . The vector w ( i ) we want is the unit vector orthogonal to the maxim um margin separating hyperplane. So, an SVM solv er can also be used to nd w ( i ) . (The SVM problem is a quadratic programming problem (QP), so we can also use any generic QP solv er.) In Figure 1 we sho w an example of what the rst three iterations of the algorithm could look like geo-metrically . Sho wn are sev eral example ( ( i ) ), and the w ( i ) 's given by di eren t iterations of the algorithm. Now, supp ose the algorithm terminates, with t ( n +1) . (Whether the algorithm terminates is discussed in Section 4.) Then directly from Eq. (10-12) we have: 8 w with k w k 2 1 9 i s.t. w T ( i ) w T E : (13) Since jj w jj 2 jj w jj 1 1, this means that there is at least one policy from the set returned by the al-gorithm, whose performance under R is at least as good as the exp ert's performance min us . Thus, at this stage, we can ask the agen t designer to man ually test/examine the policies found by the algorithm, and pick one with acceptable performance. A sligh t exten-sion of this metho d ensures that the agen t designer has to examine at most k + 1, rather than all n + 1, di eren t policies (see footnote 6).
 If we do not wish to ask for human help to select a policy , alternativ ely we can nd the point closest to
E in the con vex closure of (0) ; : : : ; ( n ) by solving the follo wing QP: min jj E jj 2 ; s : t : = P i i ( i ) ; i 0 ; P i i = 1 : Because E is \separated" from the points ( i ) by a margin of at most , we kno w that for the solution we have jj E jj 2 . Further, by \mixing" together the policies ( i ) according to the mixture weigh ts i as discussed previously , we obtain a policy whose feature exp ectations are given by . Follo wing our previous discussion (Eq. 6-9), this policy attains performance near that of the exp ert's on the unkno wn rew ard func-tion. 6 Note that although we called one step of our algorithm an inverse RL step, our algorithm does not necessarily reco ver the underlying rew ard function correctly . The performance guaran tees of our algorithm only dep end on (appro ximately) matc hing the feature exp ectations, not on reco vering the true underlying rew ard function. 3.1. A simpler algorithm The algorithm describ ed above requires access to a QP (or SVM) solv er. It is also possible to change the algorithm so that no QP solv er is needed. We will call the previous, QP-based, algorithm the max-margin metho d, and the new algorithm the projec-tion metho d. Brie y , the pro jection metho d replaces step 2 of the algorithm with the follo wing: In the rst iteration, we also set w (1) = E (0) and (0) = (0) . The full justi cation for this metho d is deferred to the full pap er (Abb eel and Ng, 2004), but in Sections 4 and 5 we will also give con vergence results for it, and empirically compare it to the max-margin algorithm. An example sho wing three iterations of the pro jection metho d is sho wn in Figure 2. Most of the results in the previous section were predi-cated on the assumption that the algorithm terminates with t . If the algorithm sometimes does not ter-minate, or if it sometimes tak es a very (perhaps ex-ponen tially) large num ber of iterations to terminate, then it would not be useful. The follo wing sho ws that this is not the case.
 Theorem 1. Let an MDP n R, featur es : S 7! [0 ; 1] k , and any &gt; 0 be given. Then the appr entic eship learn-ing algorithm (both max-mar gin and projection ver-sions) will terminate with t ( i ) after at most iter ations.
 The previous result (and all of Section 3) had assumed that E was exactly kno wn or calculated. In prac-tice, it has to be estimated from Mon te Carlo samples (Eq. 5). We can thus ask about the sample complex-ity of this algorithm; i.e., how man y tra jectories m we must observ e of the exp ert before we can guaran tee we will approac h its performance.
 Theorem 2. Let an MDP n R, featur es : S 7! [0 ; 1] k , and any &gt; 0 ; &gt; 0 be given. Supp ose the appr en-ticeship learning algorithm (either max-mar gin or pro-jection version) is run using an estimate ^ E for E obtaine d by m Monte Carlo samples. In order to en-sure that with probability at least 1 the algorithm terminates after at most a numb er of iter ations n given by Eq. (14), and outputs a policy ~ so that for any true rewar d R ( s ) = w T ( s ) ( k w k 1 1 ) we have it suc es that The pro ofs of these theorems are in App endix A. In the case where the true rew ard function R does not lie exactly in the span of the basis functions , the algorithm still enjo ys a graceful degradation of perfor-mance. Speci cally , if R ( s ) = w ( s ) + " ( s ) for some residual (error) term " ( s ), then our algorithm will have performance that is worse than the exp ert's by no more than O ( k " k 1 ). 5.1. Gridw orld In our rst set of exp erimen ts, we used 128 by 128 gridw orlds with multiple/sparse rew ards. The rew ard is not kno wn to the algorithm, but we can sample tra-jectories from an exp ert's (optimal) policy . The agen t has four actions to try to move in eac h of the four com-pass directions, but with 30% chance an action fails and results in a random move. The grid is divided into non-o verlapping regions of 16 by 16 cells; we call these 16x16 regions \macro cells." A small num ber of the re-sulting 64 macro cells have positiv e rew ards. For eac h value of i = 1 ; : : : ; 64, there is one feature i ( s ) indi-cating whether that state s is in macro cell i . Thus, the rew ards may be written R = ( w ) T . The weigh ts w are generated randomly so as to give sparse rew ards, whic h leads to fairly interesting/ric h optimal policies. In the basic version, the algorithm is run using the 64-dimensional features . We also tried a version in whic h the algorithm kno ws exactly whic h macro cells have non-zero rew ards (but not their values), so that the dimension of is reduced to con tain only features corresp onding to non-zero rew ards.
 In Figure 3, we compare the max-margin and pro jec-tion versions of the algorithm, when E is kno wn ex-actly . We plot the margin t ( i ) (distance to exp ert's pol-icy) vs. the num ber of iterations, using all 64 macro-cells as features. The exp ert's policy is the optimal policy with resp ect to the given MDP . The two al-gorithms exhibited fairly similar rates of con vergence, with the pro jection version doing sligh tly better. The second set of exp erimen ts illustrates the perfor-mance of the algorithm as we vary the num ber m of sampled exp ert tra jectories used to estimate E . The performance measure is the value of the best policy in the set output by the algorithm. We ran the al-gorithm once using all 64 features, and once using only the features that truly corresp ond to non-zero rew ards. 8 We also rep ort on the performance of three other simple algorithms. The \mimic the exp ert" al-gorithm picks whatev er action the exp ert had tak en if it nds itself in a state in whic h it had previously observ ed the exp ert, and picks an action randomly oth-erwise. The \parameterized policy stochastic" uses a stochastic policy , with the probabilit y of eac h action constan t over eac h macro cell and set to the empiri-cal frequency observ ed for the exp ert in the macro cell. The \parameterized policy ma jorit y vote" algorithm tak es deterministically the most frequen tly observ ed action in the macro cell. Results are sho wn in Fig-ure 4. Using our algorithm, only a few sampled exp ert tra jectories|far few er than for the other metho ds| are needed to attain performance approac hing that of the exp ert. (Note log scale on x -axis.) 9 Thus, by learning a compact represen tation of the rew ard func-tion, our algorithm signi can tly outp erforms the other metho ds. We also observ e that when the algorithm is told in adv ance whic h features have non-zero weigh t in the true rew ard function, it is able to learn using few er exp ert tra jectories. 5.2. Car driving simulation For our second exp erimen t, we implemen ted a car-driving sim ulation, and applied appren ticeship learn-ing to try to learn di eren t \driving styles." A screen-shot of our sim ulator is sho wn in Figure 5. We are driving on a high way at 25m/s (56mph), whic h is faster than all the other cars. The MDP has ve di eren t ac-tions, three of whic h cause the car to steer smo othly to one of the lanes, and two of whic h cause us to driv e o (but parallel to) the road, on either the left or the righ t side. Because our speed is xed, if we want to avoid hitting other cars it is sometimes necessary to driv e o the road.
 The sim ulation runs at 10Hz, and in the exp erimen ts that follo w, the exp ert's features were estimated from a single tra jectory of 1200 samples (corresp onding to 2 min utes of driving time). There were features in-dicating what lane the car is curren tly in (including o road-left and o road-righ t, for a total of ve fea-tures), and the distance of the closest car in the curren t lane. 10 Note that a distance of 0 from the nearest car implies a collision. When running the appren ticeship learning algorithm, the step in whic h reinforcemen t learning was required was implemen ted by solving a discretized version of the problem. In all of our exp er-imen ts, the algorithm was run for 30 iterations, and a policy was selected by insp ection (per the discussion in Section 3).
 We wanted to demonstrate a variet y of di eren t driv-ing styles (some corresp onding to highly unsafe driv-ing) to see if the algorithm can mimic the same \style" in every instance. We considered ve styles: 1. Nice: The highest priorit y is to avoid collisions 2. Nast y: Hit as man y other cars as possible. 3. Righ t lane nice: Driv e in the righ t lane, but go 4. Righ t lane nast y: Driv e o -road on the righ t, but 5. Middle lane: Driv e in the middle lane, ignoring After eac h style was demonstrated to the algorithm (by one of the authors driving in the sim ulator for 2 min utes), appren ticeship learning was used to try to nd a policy that mimics demonstrated style. Videos of the demonstrations and of the resulting learned poli-cies are available at In every instance, the algorithm was qualitativ ely able to mimic the demonstrated driving style. Since no \true" rew ard was ever speci ed or used in the exp eri-men ts, we cannot rep ort on the results of the algorithm according to R . However, Table 1 sho ws, for eac h of the ve driving styles, the feature exp ectations of the exp ert (as estimated from the 2 min ute demonstra-tion), and the feature exp ectations of the learned con-troller for the more interesting features. Also sho wn are the weigh ts w used to generate the policy sho wn. While our theory mak es no guaran tee about any set of weigh ts w found, we note that the values there gener-ally mak e intuitiv e sense. For instance, in the rst driving style, we see negativ e rew ards for collisions and for driving o road, and larger positiv e rew ards for driving in the righ t lane than for the other lanes. We assumed access to demonstrations by an exp ert that is trying to maximize a rew ard function express-ible as a linear com bination of kno wn features, and pre-sen ted an algorithm for appren ticeship learning. Our metho d is based on inverse reinforcemen t learning, ter-minates in a small num ber of iterations, and guaran-tees that the policy found will have performance com-parable to or better than that of the exp ert, on the exp ert's unkno wn rew ard function.
 Our algorithm assumed the rew ard function is express-ible as a linear function of kno wn features. If the set of features is sucien tly rich, this assumption is fairly unrestrictiv e. (In the extreme case where there is a separate feature for eac h state-action pair, fully gen-eral rew ard functions can be learned.) However, it remains an imp ortan t problem to dev elop metho ds for learning rew ard functions that may be non-linear func-tions of the features, and to incorp orate automatic fea-ture construction and feature selection ideas into our algorithms.
 It migh t also be possible to deriv e an alternativ e ap-pren ticeship learning algorithm using the dual to the LP that is used to solv e Bellman's equations. (Manne, 1960) Speci cally , in this LP the variables are the state/action visitation rates, and it is possible to place constrain ts on the learned policy's stationary distribu-tion directly . While there are few algorithms for ap-pro ximating this dual (as opp osed to primal) LP for large MDPs and exact solutions would be feasible only for small MDPs, we consider this an interesting direc-tion for future work.
 Acknowledgemen ts. This work was supp orted by the Departmen t of the Interior/D ARP A under con-tract num ber NBCHD030010.

