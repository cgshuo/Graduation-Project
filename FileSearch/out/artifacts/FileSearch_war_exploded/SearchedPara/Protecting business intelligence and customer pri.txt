 Ling Qiu  X  Yingjiu Li  X  Xintao Wu Abstract Nowadays data mining plays an important role in decision making. Since many data mining tasks to external service providers. However, most organizations hesitate to do so due to the concern of loss of business intelligence and customer privacy. In this paper, we present a Bloom filter based solution to enable organizations to outsource their tasks of mining association rules, at the same time, protect their business intelligence and customer privacy. Our approach can achieve high precision in data mining by trading-off the storage requirement. 1 Introduction 1.1 Background and motivation The pervasive impact of business computing has made information technology (IT) an indis-pensable part of daily operations and the key to success for organizations. Many organizations have accumulated large amount of data from various channels in today X  X  digitalized age. It is important to make these data available for decision making. Data mining, as one of the IT services needed by organizations, provides such a technique for the exploration and analysis of these raw data so as to reveal hidden information and knowledge; it has also been realized as an important way for discovering knowledge from the data and converting  X  X ata rich" to  X  X nowledge rich" so as to assist strategic decision making. The benefits of using data mining for decision making have been demonstrated in various industries and governmental health care [ 32 ]. Among all of the available data mining methods, the discovery of associa-tions between business events or transactions is one of the most commonly used techniques. Association rule mining has been an important application in decision support and marketing strategy [ 25 ] for an organization.

Let us consider a typical application scenario as follows. In an organization (e.g., an enter-prise or a governmental sector), there are several divisions including an IT division which provides IT services for the whole organization. A functional division may have to delegate or outsource its data mining tasks to the IT division due to the lack of IT expertise and powerful computing infrastructure which are usually centrally managed by the IT division.
This scenario can be extended to a more general circumstance in which all divisions are individually independent organizations (or companies). This is because in today X  X  fast-paced business environment, it is impossible for any single organization to understand, develop, and implement every IT needed. By outsourcing, an organization can obtain specific human resources (e.g., skilled programming personnel) and technological resources (e.g., more pow-erful computing infrastructure) for its needs of IT services (e.g., data analysis) with lower ment comprising of a center server and some edge servers.

The practice of outsourcing data mining tasks involves extensive collaboration (e.g., exchange or share of data) across different organizations. Either the raw data or the revealed information after analysis contains the business intelligence (BI) and customer privacy of an organization. There is a security concern of potential risk of exposing private information in outsourcing activities [ 28 ]. Without proper security policy and technology, these priva-cies could be very vulnerable to security breaches. Therefore, to protect BI and customer regulation and technology [ 27 ]. In this paper, we focus on the technology-based solutions. When outsourcing mining tasks, 1 we should protect the following three elements which may expose BI and customer privacy: (1) the source data which contain all transactions and items; (2) the mining requests which are itemsets of interests; and (3) the mining results which are frequent itemsets and association rules.

There are various methods proposed to preserve privacy in data mining; but they cannot protect all three elements simultaneously. This is because with these methods, when a first party 2 outsources its mining tasks to a third party, 3 it has to provide the source database (which might be someway encrypted) together with some additional information (e.g., plain text of mining requests) without which the mining tasks may not be carried out. Given this mation to the third party, or unable to prevent the third party from (either intentionally or unintentionally) deciphering (and possibly spreading) further information from the mining results (which would be sent back to the first party) with the additional information. 1.2 Overview of our paper 1.2.1 Our solutions Given the situation discussed in the previous paragraphs, to protect BI and customer privacy when outsourcing data mining tasks, we should control the direct access to the original data. In other words, the third party should not be allowed to access the original transactional data while performing mining tasks, and should not be able to interpret the mining results.
In this paper, we present a Bloom filter based approach which provides an algorithm for privacy preserving association rule mining with computation efficiency and predictable (controllable) analysis precision. The Bloom filter [ 10 ] is a stream (or a vector) of binary bits. It is a computationally efficient and irreversible coding scheme that can represent a set of objects while preserving privacy of the objects. With our approach, firstly the source data are converted to Bloom filter representation and handed over to a third party together with mining algorithms. Then the first party sends its mining requests to the third party. Mining requests are actually candidates of frequent itemsets which are also represented by Bloom filters. Lastly, the third party runs the mining algorithms with source data and mining requests, and comes out the mining results which are frequent itemsets or association rules represented by Bloom filters. In the above mining process, what the first party exposes to the down private information from Bloom filters. Therefore, all three elements aforementioned are fully protected by Bloom filters.
 The goal of protecting BI and customer privacy during outsoucing can be achieved by tions containing different numbers of items are mapped to Bloom filters with the same length. This prevents an adversary from deciphering the compositions of transactions by analyzing the lengths of transactions. Second, Bloom filters support membership queries. This allows an authorized third party to carry out data mining tasks with only Bloom filters (i.e., Bloom filters of either transactions or candidates of frequent itemsets). Third, without knowing all in the Bloom filter of a transaction by counting the numbers of 1 X  X  and 0 X  X . This is because the probability of a bit in a Bloom filter being 1 or 0 is 0.5 given that the parameters of the Bloom filter are optimally chosen (see Sect. 3 for details). 1.2.2 Main contributions A condensed (5-page) version of our study has been published in [ 34 ]. In this version, we present in details the mathematical analysis model for problem formulation and error rate estimation, the analysis of the mining algorithm, and the results and analysis of experiments.
In brief, our main contributions include the following:  X  We propose an approach allowing to outsource data mining tasks while protecting BI and  X  We present solid theoretical analysis for our approach. Our analysis shows that a tradeoff  X  We test the proposed approach rigorously on both real and synthetic datasets. Our exper-1.2.3 Organization of the paper The remaining sections are organized as follows. We review the related work in Sect. 2 . We revisit the basics of Bloom filters and formulate our data mining problem in Sect. 3 .In Sect. 4 , we present theoretical analysis for the formulated problems and estimate the errors in data mining. We also present a method of using multiple groups of Bloom filters to further reduce data mining errors. Following that in Sect. 5 , we develop a multi-phased algorithm and conduct a set of empirical simulations to verify our approach. Lastly we conclude our paper in Sect. 6 with a discussion of contribution, limitation, and future research directions. 2 Literature review Association rule mining has been an active research area since its introduction [ 2 ]. Many algorithms have been proposed to improve the performance of mining association rules or frequent itemsets. An interesting direction is the development of techniques that incorporate privacy concerns.

One type of these techniques is perturbation based, which perturbs the data to a certain degree before data mining so that the real values of sensitive data are obscured while non-based approach was proposed for decision tree learning. This approach was further studied in considered the problem of limiting disclosure of sensitive rules, aiming at selectively hiding some frequent itemsets from large databases with as little impact on other, non-sensitive frequent itemsets as possible. The idea is to modify a given database so that the support of a given set of sensitive rules decreases below a predetermined threshold. Similarly, in [ 37 ] a method is presented for selectively replacing individual values with unknowns from a database to prevent the discovery of a set of rules, while minimizing the side effects on non-data set by quantifying how much information is preserved after sanitizing the data set. In in which the data has been randomized to preserve the privacy of individual transactions. One problem of perturbation-based approach is that it may introduce some false association rules. Another drawback of this approach is that it cannot always fully preserve data privacy while achieving high mining precision [ 23 , 24 ].
 secure multi-party computation [ 40 ]. Though this approach can preserve privacy, it works only in distributed environment (with several parties to collaborate in mining process) and needs sophisticated protocols (secure multi-party computation based), which makes it infea-sible for our scenario.

Both types of techniques are designed to protect privacy by masquerading the source data, and cannot protect privacy distillable from the mining requests and results accessible to data miners.

Related, but not directly relevant to our work, is the research in outsourced databases explored a new paradigm for data management in which a third party service provides host database as a service. They proposed several encryption techniques to process as many que-ries as possible at the service providers X  site without having to decrypt the data. Agrawal et al. [ 3 ] presented an order-preserving encryption scheme for numeric data that allows com-parison operations to be directly applied on encrypted data. However, encryption is time consuming and it may require auxiliary indices. It is only designed for certain type of queries but may not be suitable for complex tasks such as association rule mining. Most recently Ra  X  s hiding some attributes of the source data. Hiding (or replacing) some attributes may break the integrity of source data and thus the mining results may not be meaningful if it is applied to our application scenario. 3 Problem formulation Our research question is how to outsource the association rule data ming tasks, at the same time, protect BI and customer privacy. We propose a Bloom filter based approach with which an authorized third party (e.g., an edge server or an IT division as mentioned earlier) will perform the association rule mining based on the dataset transformed by Bloom Filters and report the frequent itemsets with controllable error boundary.

In this section, we first briefly review the basics of Bloom filter, and then present the mathematic formalization for our problem. 3.1 Bloom filter revisited A Bloom filter is a simple, space-efficient, randomized data structure for representing a set of objects so as to support membership queries.
 Definition 3.1 Given an n -element set S ={ s 1 ,..., s n } and k hash functions h 1 ,..., h k n -element set) to its Bloom filter.

For membership queries, i.e., whether an item x  X  S ,wehash x to the Bloom filter of S amemberof S .Ifyes,wesay x is in S although this could be wrong with some probability. Definition 3.2 For an element s and a set S ,define s  X  B S if s hashes to all 1 X  X  in the Bloom as the probability of s  X  B S while s /  X  S ,orPr ( s  X  B S | s /  X  S ) .
 Assuming that all hash functions are perfectly random, we have the following Lemma 3.3 Given an n-element set S ={ s 1 ,..., s n } and its Bloom filter B ( S ) of length m constructed from k hash functions, the probability for a specific bit in B ( S ) being 0 is and the probability for a specific bit being 1 is
Then the false positive rate of B ( S ) is
Given n and m, f is minimized when p 0 = p 1 = 0 . 5 and k = m n ln 2 ,inwhichcase f = 1 / 2 k = ( 0 . 6185 ) m / n .
 What X  X  missing from traditional Bloom filter? Traditional Bloom filters cannot solve the privacy issue because anybody knowing the hash function can derive the original itemsets based on Bloom Filters.

To preserve the privacy of Bloom filters, we propose keyed Bloom filter by augmenting the hash functions h i with a secret key K . To represent set S ,anelement s  X  S is inserted set to 1. Without knowing the secret key, one is unable to derive the original set by examining a Bloom filter. Without further mention we always assume that Bloom filters in our paper are constructed with secret keys. 3.2 Our problem Fundamentally we want to provide a solution for privacy preserving frequent itemsets min-ing. The frequent itemset mining has been a common task in many data mining projects for the past decade. From frequent itemsets, one can easily derive all association rules. The mining of frequent itemsets of association rules has a wide range of applications in many areas, from the analysis of customer preferences to DNA patterns.

For market basket data, we define each transaction, such as a list of items purchased, as a subset of all possible items.
 Definition 3.4 Let I ={ I 1 ,..., I d } beasetof d boolean variables called items. Let data-such that T i  X  I . The support of an itemset S over I , denoted support ( S ) ,isdefinedasthe is defined as support ( S )/ N .
 Problem 1 Traditional problem: frequent itemsets mining. Mathematically, given a trans-action database D over I and a threshold  X   X  X  0 , 1 ] , traditional research focuses on finding all frequent itemsets F S  X  2 I such that freq ( FS )  X   X  .

Our idea is to transform transaction database to a collection of Bloom filters to preserve the privacy in frequent itemset mining. Each transaction T i  X  T is transformed to Bloom that the mining process is done on the (keyed) Bloom filters B ( I i ) of the items rather than on the items themselves.
 Problem 2 Our research problem: privacy preserving frequent itemsets mining. Given (i) all Bloom filters B ( FS ) of itemsets F S  X  2 I such that freq ( FS )  X   X  .
Normally without knowing the secret key, the third party, which may not be fully trusted, will be unable to interpret sensitive and private information either from the contents of dat-abases or from mining results. However, we still need to handle some extreme cases for the protection of BI and customer privacy. Case 1, some transactions may contain only one item. The number of 1 X  X  in these Bloom filters are no more than but very close to k . Therefore, the outsourced database may divulge partial individual items. To prevent such divulgence, one item numbers are smaller than a threshold. Case 2, candidates of frequent 1-itemsets are exactly individual items which may divulge some sensitive information. To prevent this, we into all transactions before outsoucing. At the same time, each candidate of frequent 1-item-sets is inserted with a virtual item randomly chosen from k 1 to k i and is sent to the third party together with mining requests (see Sect. 5.1 for detailed mining process). Thus the edge servers cannot easily identify candidates of frequent 1-and 2-itemsets because both types of candidates look alike. This method can be applied to conceal candidates of frequent white noise discussed in case 1. 4Analysis In this section, we analyze the possible error rates introduced by mining the frequent itemsets based on Bloom filters instead of the original dataset. For any given itemset, the frequency learnt from Bloom filters may be larger than its real frequency learnt from original transac-binary vector of length m through k hash functions. 4.1 Preliminaries The false positive of a Bloom filter was defined for checking an element from a Bloom filter. Now we extend the concept of false positive to checking an itemset from a Bloom filter. s  X  B T i , and define S B T i otherwise. The false positive rate for checking S from the Pr ( S  X  B T i | S T i ) .

Due to the false positive of checking an itemset from a Bloom filter, the support or fre-quency learnt from a collection of Bloom filters is different from that learnt from original transactions. Regarding such support and frequency, we have the following as Bsupport ( S )/ N .
 Lemma 4.3 In the setting of Definition 4.2 , the following statements hold : (i) S  X  B T i If S T i ,thenS  X  B T i with probability f i , and S B T i with probability 1  X  f i . (iv) B freq ( S )  X  freq ( S ) .
 Theorem 4.4 Given an itemset S and a transaction T i , the false positive rate of checking S from the Bloom filter of T i is indicates the number of 1  X  X  in a binary vector.
 Proof From Eq. ( 1 ), one can derive that the false positive rate for checking any single item s  X  S  X  T i is p k and k is the number of bits to which the item is hashed. From Lemma 4.3 , we know that S positive rate f i for checking S from B ( T i ) is p || B ( S  X  T i ) || 1 = 1  X  e  X  kn i / m S from the Bloom filter of T i is bounded: Proof According to the definition of false positive, we have S T i and thus 1  X || B ( S  X  T i ) 1 4.2 False positive and false negative B (
S ) , the server cannot compute the frequency freq ( S ) directly. The approaches to solving the traditional frequent itemset mining problem cannot be applied directly to solving our data mining problem.

According to Lemma 4.3 (i), the frequency B freq ( S ) can be derived from those Bloom revised threshold. Note that freq ( S )  X   X  is required in our data mining problem; thus, we not necessarily the same, we need to define the false positive rate and false negative rate for checking an itemset from all Bloom filters.
 Definition 4.6 Given an itemset S and N Bloom filters B ( T i ) , i = 1 ,..., N ,thefalse positive rate for checking S from all Bloom filters using revised threshold  X   X   X  , denoted as f freq ( S )&lt; X  ) .Thefalsenegativerateforchecking S fromallBloomfiltersusing  X  ,denotedas f freq ( S )  X   X  ) .

If the revised threshold  X  is the same as the threshold  X  , then the false negative rate will be zero due to the fact B freq ( S )  X  freq ( S ) (Lemma 4.3 (iv)); however, the false positive rate may be greater than zero in this case. In general, one may use  X   X   X  so as to balance and the lower the false positive rate. If the false negative rate is of major concerns, one may choose  X  =  X  to zero out false negative rate. Note that choosing  X  &lt; X  is meaningless as it To formalize our analysis, we define a random variable for checking an itemset against a Bloom filter. Then we re-write in terms of the defined variables the frequency of an itemset learnt from Bloom filters and the false positive/negative rates for checking an itemset from all Bloom filters.
 Definition 4.7 For an itemset S and a transaction T i such that S T i , define a random 0 X 1 variable e i such that e i = 1if S  X  B T i and e i = 0if S B T i .

The defined variable indicates whether S  X  B T i ;thatis, e i = 1 with probability f i and e = 0 with probability 1  X  f i (see Lemma 4.3 ). In other words, e i represents a Bernoulli assume that S T i for the first N  X  ( 1  X  freq ( S ) ) transactions T i .Thenwehave itemset S. The false positive and false negative rates for checking S from all Bloom filters using a revised threshold  X   X   X  are 4.3 Estimate of false positive and false negative if n i n , the transaction data can be clustered into multiple groups such that in each group, the size of each transaction is equal or close to the average size of the transactions in the group. The data mining task can be easily extended to each group. For simplicity, we assume that n i = n in our analysis; we leave the multi-group case in Sect. 4.4 .

Let k = m n ln 2 be the optimal number of hash functions that are used for generating the Bloom filter of length m for each transaction, which consists of n items. According to Lemma 3.3 and Corollary 4.5 , the false positive rate f i for checking an itemset S from a Bloom filter has a lower bound and an upper bound:
First consider a special case where f i =  X  f for all i = 1 ,..., N . In this case s e is the 1 checking an itemset S from all Bloom filters are f f
Since the cumulative binomial probability C ( X ,  X ,  X  f ) is monotonic increasing with  X  f , from formulae ( 3 )to( 5 ) it is easy to know the lower bounds and upper bounds for the false positive rate and false negative rate in general case: In a special case where  X  =  X  ,wehave f  X   X  = 0and where A = N  X  (  X   X  freq ( S ) ) and B = N  X  ( 1  X  freq ( S ) ) .

Equation ( 6 ) indicates that the greater the number k of hash functions, the smaller the false freq ( S ) in the average case. Let E [ X ] denote the mean of a random variable. From Eq. ( 2 ), we have
Recall the bounds for f i (see Eq. 3 ). In the false positive case (where freq ( S )  X   X  ), we have Similarly, in the false negative case (where  X   X  freq ( S )&lt; X  ), we have but the difference is bounded. Note that || B ( S ) ||  X  k .Wehave The longer the Bloom filters (i.e., the greater the k ), the smaller the difference between of frequencies decreases exponentially. For example, if k  X  20, then the difference of fre-quencies will be less than 10  X  6 . This means that in the average case, the frequency of an itemset detected from Bloom filters will be greater than that detected from original data by at most 10  X  6 . Note that the above analysis is conducted for each itemset. The overall false positive and false negative rates depend on the distribution of itemsets and their frequencies. Empirical study will be conducted in Sect. 5 . 4.4 Multiple groups of Bloom filters of each transaction is equal or close to the average size of all transactions. This might not be true in many real data sets. A simple solution is to cluster the transactions into multiple groups such that in each group, the size of each transaction is equal or close to the average size of the transactions in the group. Across all groups, the same number k of hash functions are used for generating Bloom filters. In each group, k = m n ln 2 is optimized for the length m of Bloom filters and the average size n of the transactions in the group. This means that for different groups, the length of Bloom filters are different depending on the average size of transactions in the group. Roughly speaking, long Bloom filters will be used for long transactions, while short Bloom filters for short transactions.

Using different Bloom filters for different groups of transactions will not only save storage requirement but also increase the precision of data mining. Note that the bounds presented in the previous section are based on k . Since the same k is optimized across multiple groups, the bounds can be used to estimate the false positive and false negative rates in all groups.
In the case of multiple groups, the transactions are represented by the Bloom filters of different lengths. This requires that each candidate itemset be represented by Bloom filters of different lengths too. This is because in our data mining problem, the Bloom filter of each candidate itemset needs to be checked against the Bloom filters of the same length. To avoid transmitting multiple Bloom filters for each candidate itemset between client and server, only the longest Bloom filter of each candidate itemset is sent to the server at one time. In data mining process, the outsourced server needs to transform it into different lengths so as to check it against different groups of Bloom filters. We provide a simple solution called  X  -folding to transform the Bloom filters.
 Definition 4.9 Given an m -bit Bloom filter B ,  X  -folding of B , denoted as B  X  ,isdefinedasa  X  where bi t ( B , i ) denotes the i th bit of B ,  X  the bitwise OR, and 0 &lt; X   X  1.
From the definition of Bloom filter, it is clear that vector B  X  is a Bloom filter generated with the same k hash functions which are used for generating Bloom filter B .
 Given a Bloom filter B ( S ) of longest length m of candidate itemset S , the frequency B freq ( S ) is computed by checking B ( S ) against the Bloom filters of transactions in all groups. For a particular group in which the Bloom filters have length m  X  m (note that transformed Bloom filter has the length m . 5 Experiments To evaluate the performance of our Bloom filter based method for mining frequent itemsets, we conduct experiments based on both synthetic data and real data. A framework of our method is shown in Algorithm 1 . 5.1 Algorithm Algorithm 1 can be divided into three phases: counting phase (lines 3 X 5), pruning phase (lines 6 X 8), and candidates generating phase (lines 9 X 10) in each round ,where indicates the size of each candidate itemset dealt with. In the counting phase, each candidate filter is checked against all transaction filters and the candidate X  X  count is updated. In the pruning than a revised threshold N  X   X  . Finally, in the candidates generating phase, new candidate Bloom filters are generated from the Bloom filters discovered in the current round. The new candidates will be used for data mining in the next round.
 Algorithm 1 Mining frequent itemsets from Bloom filters 5.1.1 Efficient counting To improve the efficiency in the counting phase, we organize the Bloom filters of the trans-actions of each group in a tree hierarchy and use every q bits to partition them at different nodes; the Bloom filters in each node share the same first q bits. A node splits if it contains more than c Bloom filters, where c is another parameter. At the end of partition, each leaf node contains limited number of Bloom filters, while each non-leaf node (except the root) is associated with a q -bit segment with which the node shares.

Because of the randomness of keyed hash functions, the distribution of Bloom filters is toindexupto c  X  2 qL Bloom filters. Given q = 5and c = 20, for example, a 4-level tree can be used to index 20M Bloom filters.
 Heuristic 5.1 Let s be the q -bit segment associated with a non-leaf node and B ( S ) be a 0 , then no Bloom filter in the subtree rooted at the non-leaf node needs to be checked in the counting phase.

In the counting phase, we traverse the tree to compare each candidate filter with the trans-According to the above heuristic, we may skip some subtrees in the counting process.
An alternative way to do this is to organize candidate filters in a tree structure and update their counts appropriately while traversing the tree for each transaction filter. In multiple group case, this solution requires that the tree of candidate filters be built differently for each group, while in the above method a static tree of transaction filters can be used for any candidate filters. 5.1.2 Candidates generating a multiple step interaction can be conducted between client and server in candidates gener-sends it to the server for data mining. This process can be done repetitively in each round. As mentioned in Sect. 3.2 , with C 1 (i.e., Bloom filters of individual items) an edge server may decipher partial sensitive data (e.g., the compositions of transactions). Therefore, for the concern of protecting BI and preserving customer privacy, it is advisory not to outsource frequent 1-itemset mining tasks, or to use alternative choice of concealing C 1 discussed in Sect. 3.2 .

The candidate generation C + 1 = can_gen ( F ) in this case is conducted at client side for privacy reasons. The Bloom filters in F are transformed back to itemsets with the help of secret key. From this collection of itemsets, the client generates a new set of candidate itemsets using the well-known method apriori _ gen as proposed in [ 4 ]. The basic idea of apriori _ gen is that a candidate itemset of length + 1 is generated only if all its subsets of length appear in the collection of itemsets. The client may also edit the set of candi-dates according to application requirements and constraints. Finally, the client transforms the candidate itemsets to Bloom filters and sends them back (in C + 1 ) to the server. All of our experiments presented in the next section are based on this scenario.

Another choice to perform candidate generation is at server side. However, the server has no secret key to perform the hash functions, so it cannot transform back and forth between
B itemsets that are unions of any two frequent itemsets (clearly, no frequent itemset is missed in this process). The disadvantage of this solution is that the server cannot exploit Apriori property using apriori _ gen at itemset level. 5.2 Experiment settings Rigorous experiments are conducted on both real data and synthetic data so as to evaluate the performance of our Bloom filter method in terms of mining precision, storage requirement, and computation time. All the experiments are run on a Compaq desktop computer with Pentium-4 CPU clock rate of 3.00GHz, 3.25GB of RAM, and 150GB harddisk, running Microsoft Windows XP Professional Version 2002 with SP2. 5.2.1 Real data The real data sets we adopt for this experiment are BMS-POS, BMS-WebView-1, and BMS-WebView-2 available at http://www.ecn.purdue.edu/KDDCUP . The dataset BMS-POS con-tains several years worth of point-of-sale data from a large electronics retailer, whereas the datasets BMS-WebView-1 and BMS-WebView-2 contain several months worth of click-stream data from two e-commerce websites. 5.2.2 Synthetic data We generate synthetic data using the transaction generator designed in IBM Quest project [ 4 ]. A synthetic dataset contains 100 X 750K transactions; each transaction is generated from a set of 1,000 frequent itemsets. The size of each frequent itemset is picked from Poisson distribution with mean 4. There are totally 1,000 distinct items. The size of each transaction is picked from Poisson distribution with mean 10. We use four sets of synthetic data, namely Syn1, Syn2, ..., and Syn4. 5.2.3 Distributions of real data and synthetic data The distributions of transaction sizes for both real data and synthetic data are shown in many real data sets used for mining association rules, while the synthetic data generated by IBM generator with Poisson distribution have been most widely used for testing the scalabil-ity of association rule mining algorithms. We use the real data to test the mining precision, storage requirement, computational time, and the tradeoffs among them. We use the synthetic data for scalability test. We show that our method works well for both types of data. 5.2.4 Mining precision For mining precision, we compare the result of our method with the solution to the traditional problem (see Problem 1 ). We use the standard association rule mining algorithm Apriori [ 4 ]  X  in our method, the mining precision is measured in terms of overall false positive rate F +  X  discovered by Apriori, and F be the set of Bloom filters discovered by our algorithm. Then  X  Overall false positive rate: F +  X  = | F  X  F | |  X  Overall false negative rate: F  X   X  = | F  X  F | | and false negative rates discussed in Sect. 4.3 . Given a data set and a data mining threshold, particular to each itemset, while the overall false positive and false negative rates are the between the two measurements. 5.2.5 Storage requirement The storage requirement is measured in terms of the length of Bloom filters and the num-ber of transactions that an outsourced server needs to store. If there are multiple groups of transactions represented by Bloom filters of different lengths, then the storage requirement is the sum of the product of Bloom filter length and number of transactions in each group. Let there be g groups. For each group i , let the length of Bloom filters be m i and the number of transactions N i .Then 5.2.6 Experiment parameters Table 1 gives the parameters used in our experiments as well as their default values. In our experiments, unless otherwise indicated, only one parameter is changed at a time while others are kept at their default values.
 The number k of hash functions is the optimal parameter to determine the length m i of is, k = m i n
Recall that for any itemset, the average difference of the revised frequency and the original frequency is upper-bounded by 2  X  k (see Eq. 7 ). We use the coefficient  X  (where 0  X   X   X  1) to calculate the revised frequency threshold  X  =  X  +  X   X  2  X  k . This coefficient is used to balance between the overall false positive and false negative rates.

When the transactions are divided into multiple groups, the grouping is based on the dis-tribution of transaction size. In our experiments, the grouping is carried out by the following divided into two groups such that Group 1 contains those transactions whose lengths are not greater than n 1 and otherwise for Group 2. Likewise, Group 2 is further divided into two groups. The grouping operation is continued until the original dataset is divided into g groups. 5.3 Experimental results 5.3.1 Data mining time versus data conversion time In the first set of experiments, we study the data mining time versus the data conversion time by changing the number k of hash functions from 25 to 40.

Figure 2 shows that the time of mining frequent itemsets is much more than the time of converting raw data to Bloom filter representation, the more transactions, the more mining fies the worthiness of pre-processing (i.e., data format conversion before outsourcing mining tasks). 5.3.2 Change mining thresholds In the second set of experiments, we study the mining precision and running time by changing g = 4, k = 30, and  X  = 0.

Figure 3 shows that the false positive rate is less than 6% for datasets BMS-WebView-1 and BMS-WebView-2, and less than 1% for datatset BMS-POS. For all datasets, the running time is decreasing with mining threshold. The reason is that a larger mining threshold will cause fewer frequent itemsets to be discovered in a shorter time. In addition, the experiments on dataset BMS-POS require more time than those on datasets BMS-WebView-1 and BMS-WebView-2 because dataset BMS-POS contains approximately 6 X 8 times more transactions than other two datasets.

The false negative rates are 0 for all datasets when  X  = 0. We also run experiments for the revised frequency threshold  X  =  X  +  X   X  2  X  k when  X  varies from 0.2 to 1.0 with an increment of 0.2. Since our experimental results are the same as compared with the cases for which  X  = 0, the mining precision is not sensitive to the change of frequency threshold in our experiments. Therefore, in the following, we use  X  = 1% and  X  = 0 and only consider the false positive in our analysis. 5.3.3 Change number of hash functions In the third set of experiments, we study the mining precision, computation cost, and storage requirement with the change of the number k of hash functions. We change k = from 20 to 40 and keep the other parameters at their default values, i.e., g = 4,  X  = 1%, and  X  = 0.
Figure 4 shows the mining precision with the change of k . There is a globally decreasing is nearly 5% for k = 20, and is less than 1% for k  X  25. For datasets BMS-WebView-1 and BMS-WebView-2, the false positive rates are below 5% for k  X  30. It also shows that the running time changes slightly with k . The running time is around 8min for dataset BMS-POS and within 1min for datasets BMS-WebView-1 and BMS-WebView-2. The storage require-ment is linearly increasing with k for all datasets based on the figure. The reason is that the larger the k , the longer the Bloom filters, and the more storage is required to store the Bloom filters. The experimental results show that high mining precision can be achieved by increasing the number of hash functions. Consequently, the storage requirement increases linearly due to the use of longer Bloom filters. 5.3.4 Change number of groups In the fourth set of experiments, we study the mining precision, computation cost, and storage requirement by changing the number g of groups from g = 2 to 5. The other parameters are kept at their default values, i.e., k = 30,  X  = 1%, and  X  = 0.

Figure 5 shows that the false positive rate decreases with the number of groups. For data-sets BMS-WebView-1 and BMS-WebView-2, the false positive rate can be as low as 5% for g  X  3; whereas for dataset BMS-POS, the false positive rate is less than 2.5% for g  X  2. It also shows that running time and storage requirement change little with parameter g .The number of groups without increasing much the storage requirement and the running time. 5.3.5 Scalability In the last set of experiments, we use synthetic datasets Syn1, Syn2, ..., and Syn4 to test the scalability of our algorithm with respect to the running time, mining precision, and storage requirement. We let k = 20 and keep other parameters at their default values, i.e., g = 4,  X  = 1%, and  X  = 0.

Figure 6 shows that the false positive rate for all synthetic datasets is no more than 1%, and that the running time and the storage requirement increase linearly with the number of transactions which is scaled up from 100K in dataset Syn1 to 750K in dataset Syn4. The scalability is also verified by experiments for k &gt; 20 (e.g., k = 25 and 30) with zero false positive rate.

From the experimental results, we can conclude that the running time and storage require-ment are scalable with the number of transactions while the mining precision is reasonably high. 5.4 Summary of experiments One can draw the following conclusions from out experiments: (1) in the various cases of our experiments, zero false negative rate can be achieved. This result is not sensitive to the change of mining threshold; (2) by increasing the number of hash functions in formulating by increasing the number of groups in data mining, the false positive rate decreases without increasing the running time or the storage requirement; and (4) the running time and the storage requirement are scalable with the number of transactions that are processed in data mining. 6 Conclusions The contribution of this paper is threefold. First, we proposed a new approach to outsourc-ing association rule mining tasks while protecting BI and customer privacy. The proposed approach is different from previous solutions in that it can protect BI and customer privacy while outsourcing mining tasks, at the same time, maintain the precision of mining results. Second, we performed theoretical analysis on the false positive and false negative rates in data mining. We also estimated the upper and lower bounds for the mining errors. Third, we investigated the tradeoffs between mining precision and storage requirement. Rigorous experiments were conducted on typical real and synthetic datasets showing that our approach can save storage significantly without sacrificing much mining precision, security level, and running time.

We are planning to investigate how to protect BI and customer privacy while outsourcing other mining tasks such as decision tree mining. It is also interesting to study privacy pre-serving techniques for mining specific types of frequent itemsets, such as maximum itemsets and closed itemsets.
 References Author X  X  biographies
