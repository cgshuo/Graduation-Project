 fi fi ers, CSBC partitions them by using a clustering algorithm. Then CSBC 1. Introduction
Classi fi cation is the most important task in pattern recognition institute. Therefore, since the beginning of the pattern recognition science, one of the most challenging problems in this fi eld was introducing a general classi fi er that can learn properly every dataset of any given problem. Many classi fi ers have been proposed to learn problems thus far. However, all of them have their own positive and negative aspects. So they are good only for speci problems. But there is no strong solution to recognize which classi fi er is a better or good classi fi er for a speci fi nding the best classi fi er is an impractical problem, we must use another approach. Thus might be using many inaccurate classi-fi ers, where each of them is assigned to a subspace of dataset as an ensemble (i.e. use their gathering vote as the decision of ensem-ble). Ensemble learning is a strong approach to produce a near-to-optimal classi fi er for any problem. This method reinforces the ensemble in error-prone subspaces, and hence can lead to better performance of classi fi cation. In general the following sentence can be true to say that the result of combination of diverse for each ensemble to be successful. The existence of diversity in an ensemble ensures that those classi fi ers are independent of each other. It means that misclassi fi cations do not occur simultaneously. Kuncheva showed that increasing the number of diverse classi can lead to better performance (even perfect accuracy) ( Kuncheva, 2005; Minaei-Bidgoli et al., 2004; Alizadeh et al., 2011 ). Also, ensemble philosophy is applicable to Bayesian Networks ( Pe X a, 2011 ). The main challenge in the creation of classi fi er ensemble is to provide a general approach to ensure diversity, which is an important factor for an ensemble. It means that if an ensemble of classi fi ers has to be a successful ensemble, they should be diverse enough to cover their errors. Creating some suitable diverse classi fi ers that can participate in an ensemble is a challenging problem. There are a number of ways to obtain a desired diversity in an ensemble. Kuncheva proposed some approaches based on the metrics that indicates the amount of similarities or differences among classi fi ers' outputs ( Kuncheva and Whitaker, 2003; Parvin et al., 2013a, 2013b, 2013c, 2011c, 2011d; Rezaei et al., 2011 ).
Clustering is a process of assigning a group of objects into clusters. So objects in the same cluster are more similar to each other than the objects in other clusters. This is used a lot in some applications of data mining, especially for information retrieval, text categorization and text ranking ( Yang, 2006; Dasgupta and Ng, 2010; Amig X  et al., 2011; Kurland and Krikon, 2011 ).
Giacinto and Roli, by producing a large number of arti fi neural network classi fi ers by different initializations of their parameters and then selecting a subset of them based on their distances in output space, proposed an approach to hosting the classi fi ers with a high degree of diversity.

Hamid et al. were inspired from the clustering and selection method and proposed a new clustering and selection method that enables one to reduce the drawbacks of the simple ensemble methods in creating diversity ( Parvin et al., 2001, 2013c, 2013e ).
They consider how the base classi fi ers are created. They also investigated the usage of Boosting and Bagging method as a source of diversity generation on Giacinto and Roli's method. At fi rst, they trained a large number of classi fi ers using the
Boosting and Bagging method, and then partitioned them based on the output over the training set. Finally they chose a classi randomly and then inserted it into the ensemble. The weighted majority voting mechanism was used as the consensus function of the ensemble.
 In this paper a novel method, named the Classi fi er Selection
Based on Clustering (CSBC), has been proposed. This method can provide the necessary diversity between ensemble classi fi using the clustering technique. And it uses the Bagging method as a generator of base classi fi ers. Base classi fi ers are still decision tree classi fi er or the multilayer perceptron classi during the creation of an ensemble. Then the clustering algorithm partitions the classi fi ers. Weighted majority vote mechanism was used as the consensus function of ensemble. We investigated how the number of clusters can affect the performance of the CSBC method. We based our study on a large number of real datasets of the UCI repository to reach a de fi nite result. In this paper we investigate the better selection of the classi fi er from each cluster, how to select a good parameter according to the dataset and the effect of the training set ratio of each base classi fi er on CSBC's performance.

Machine learning is an important paradigm in arti fi cial intelli-gence, and Arti fi cial Neural Network (ANN) is a common approach to learning. Unlike the traditional approach, ANN has some properties such as self-adaptivity, ability to generalize, and so on. But the source of these properties is not explained well. They are almost explained by the comparison between ANNs and real neural networks. From bionic point of view, these formal and biological neurons do not have anything in common.

An ANN is a model that enables one to obtain any input and produce the desired set of outputs. An ANN includes two base elements: neurons and connections. An ANN is a set of neural network with connections between them. From another perspective an ANN includes two distinct views: topology and learning. Topology is related to the existence or nonexistence of a connection. Learning in an ANN indicates the power of topology connections. Multi-Layer Perceptron (MLP) is one of the most representative of ANNs. There are different methods to set the power of connections in MLP. One method is setting the weights using a priori knowledge. Another method is to train the MLP, and then using the teaching patterns and fi nally changing the weights based on some learning rules. In this paper we use MLP as the base classi fi er.
 training algorithms are proposed for any given ANN architecture. However, these algorithms are all over classical and external to
ANNs. Neural networks, which include coded learning algorithm within astrocytic nets, are rather interesting, because different learning rules can be made internal for them. But the source and structure of these rules remain unclear. Therefore, it seems that there is a paradox between the self-learning capabilities of ANNs and their distinctions from the traditional algorithms. Outwardly,
ANNs cannot solve the machine learning problems. Particularly, over-learning is one dif fi cult subject and there is no good explana-tion in the ANN theory for it. Indeed limiting the training time prevents over-learning. Despite all that, the popularity of ANN is not by chance. However, their bene fi ts must be considered, in order to improve them in the future. Decision tree (DT) is one of the versatile classi fi ers in the machine learning fi eld. DT is an unstable classi fi er that can introduce different outputs in succes-sive trainings on the same condition. DT uses a tree-like graph or model for decision. The type of presentation helps experts under-stand the classi fi ers ( Yang, 2006; Parvin et al., 2011a, 2011b ). The natural instability of this method can be a source of diversity for classi fi er ensemble. An ensemble of a number of DTs is similar to a Random Forest (RF) algorithm, which is one of the powerful ensemble algorithms. This algorithm was developed by Breiman (1996 ). In this paper, DT is used as the base classi fi er. related works. In Section 3 , the proposed method is explained.
Section 4 demonstrates the results of our proposed method against traditional methods. Finally, the conclusion is presented in Section 5 . 2. Related work number of classi fi ers that use different training sets: Bagging and
Boosting. Both of them are two sources for diversity generation and they are the best ensemble methods.
 data item in TS is represented as O i and m is the number of data items in TS. Training phase of CSBC is shown in Fig. 1 , which uses the Bagging method as the base classi fi er generation. Bootstrap AGGregatING.
 the ensemble are made by bootstrap copies of the training set.
Using different training sets can ensure the necessary diversity of ensembles. It is noticeable that Bagging cannot ensure the neces-sary diversity.
 proposed by Breiman. RF is a method for ensemble creation that uses a decision tree as the base classi fi er generator. In
Forest  X  , an ensemble of decision trees must be built by creating independent identically distributed random vectors and each vector is used to grow a decision tree. Random Forest also cannot ensure the necessary diversity of ensembles. In this paper  X 
Random Forest  X  algorithm is implemented as a version of Bagging classi fi er and it is compared with the proposed method. It must be noticed that we modi fi ed  X  Random Forest  X  before usage.
The Boosting method is inspired by the Hedge(  X  ) algorithm which is an online learning algorithm. This algorithm assigns weights to strategies that are used to predict the outcome of a certain event. At this point we want to relate the Hedge( classi fi er combination problem. In Freund and Schapire (1997 )
Boosting is de fi ned as  X  general problem of creating a very accurate prediction rule by combining rough and moderately inaccurate rules of thumb  X  . The main idea of Boosting is creating a team of classi fi ers incrementally (adding one classi fi er into the team at a time). The classi fi er that joins the ensemble at step k is trained on a selected dataset from the trained dataset Z . The sampling distribution starts uniformly and progresses toward increasing likelihood of  X  dif fi cult  X  data points. Therefore distribution is updated at each step, and the likelihood of each misclassi object from previous step increases. In this section, correspon-dence with Hedge(  X  ) is transposed. Classi fi ers in D are events and the data points in Z are strategies in which their probability distribution is updated at each step. This algorithm is inspired by
ADAptiveBoosting, and is called AdaBoost. Another version of it is the arc-x4 algorithm, which is the same as the latest version of ADAboost ( Kuncheva, 2005 ).

Giacinto and Roli (2001) proposed clustering and selection method. First they produced a large number of MLP classi fi with different initialization and then partitioned their outputs by a clustering method and chose one classi fi er from each cluster.
Finally, the selected classi fi ers were considered as an ensemble and majority voting was their aggregation function.

Hamid et al. proposed a framework to develop the combined classi fi ers. In this framework, a number of trained data-bags are bootstrapped from the trained dataset at fi rst and then a set of weak base classi fi ers is created; each classi fi er is trained on a distinct data-bag. After that classi fi ers are partitioned using the clustering algorithm in order to release similar classi fi ensemble and choose a diverse subset of classi fi ers. In the partitioning phase, the outputs of classi fi ers on the training dataset are considered as a new feature space. One classi selected from each cluster randomly to create the fi nal ensemble.
Then different votes are gathered to form an ensemble to produce consensus vote. And then the weighted majority voting mechan-ism is applied as their aggregation function. The weights are speci fi ed based on the accuracies of the base classi fi ers on training dataset ( Parvin et al., 2001 ). 3. Classi fi er selection by clustering
The main idea of classi fi er selection using the clustering method is to use the diverse classi fi ers obtained from the modi Bagging and Boosting mechanism. A number of classi fi ers are trained by Bagging or Boosting. The training phase of CSBC is shown in Fig. 2 in which the modi fi ed Bagging method is used as the base classi fi er generator. Then a random classi fi er is selected from each cluster. This method selects one classi fi er from each cluster and considers these classi fi ers as a diverse ensemble. So it performs the traditional Bagging and Boosting (which use all of the nearest classi fi ers to the head of each cluster. This method can produce more diversity ensemble than others that select the classi fi ers randomly. Pseudo-code of training phase of CSBC, which uses the modi fi ed Bagging and modi fi ed Boosting methods as the base classi fi er generator, is shown in Fig. 2
With due attention to Fig. 2 , n subset containing b % of the training dataset are bootstrapped at fi rst. The i th dataset boot-strapped with b % is denoted as DB i which is the i th data-bag. Cardinality of DB i is equal to m b /100. After that a classi trained into each DB i . Suppose that C i is a classi fi er that was trained into DB i . Then C i is tested over the whole training dataset and its accuracy is calculated. The output of the i th classi the i th data item TS is a vector which is marked O ij . O of the i th classi fi er over the whole of the training dataset is equal to O i and its accuracy is presented by P i . The only difference between the proposed algorithms and the Bagging method is related to the b value. In the proposed method b is in [30 in the Bagging method, b  X  100. The training phase of the CSBC method, which uses modi fi ed Boosting as a base classi fi erator, is shown in Fig. 3 . Similar to that in Fig. 2 , a subset containing b % of the training dataset is selected. Then the classi fi er is trained on this subset. After that the fi tested in the whole training dataset, O1 and O2 are the results. Next the subset containing b % of the training dataset is obtained using the O1; this mechanism is continued until the O i obtained by O i 1 . The difference between the proposed method and the Boosting method is related to the b value. In Boosting, b  X  100 and in the proposed approach b is in [30  X  100]. Further informa-tion on Boosting can be found in Kuncheva (2005 ). Pseudo-code of the classi fi er selection-based clustering framework and its illus-tration are depicted in Figs. 4 and 5 respectively. In the CSBC a dataset of classi fi ers named DC is created at fi rst. The ith data item data item in DC is identi fi ed as X ip which is obtained from Eq. (1) : X where j and k are obtained by Eqs. (2) and (3) , respectively. j  X   X  p = c  X   X  2  X  where c is the number of classes. k  X  p j c  X  3  X 
Features of the DC dataset are obtained from different opinions over real data items of the under-learning dataset. A new dataset contains n data item where any of them stands for a classi N feature where N  X  m c. n is a prede fi ned parameter which indicates the number of produced classi fi ers from Bagging and Boosting. After producing the DC dataset it is partitioned by using the clustering algorithm and its result is some clusters of classi-fi ers. The number of clusters is denoted by r . The outputs of classi fi ers in a cluster are the same. It means that these classi have low diversity. So it is better to use one of them in the ensemble instead of using all of them. For escaping from outlier classi fi ers, we ignore the clusters that have a number of classi under the prede fi ned threshold.
 In this paper we assumed the ensemble of n classi fi ers {C1, C2,
C3, ... ,C n } is denoted E , and there are c classes. And applying the ensemble over data sample O i produces the following result:
D  X 
Now the ensemble decides the data sample O j to belong to class q according to Eq. (5) : q  X  argmax c where w j is the effect weight of classi fi er j , which is obtained optimally ( Kuncheva, 2005 ) according to Eq. (6) : w  X  log p j 1 p
P indicates the accuracy of classi fi er j in TS. a tie breaks randomly in Eq. (5) . We consider vector L j for data item O j . L belongs to class q and zero otherwise. Now we can calculate the accuracy of classi fi er CK over TS using Eq. (7) : p 4. Experimental study and discussion
Section 4.1 . The next section describes the used datasets and then the settings of experimentations and fi nally the experimental results are presented. 4.1. Evaluation metric the experiments have been carried out by using a 4-fold cross-validation. The results of this 4-fold cross-validation are repeated in 10 independent runs. It means that to investigate the accuracy of methods on a dataset, e.g. Iris , its accuracy is calculated by 4-fold cross-validation and its result is represented by acc scenario is repeated until reaching acc 10 . So acc i , i
Accuracy of the method over the Iris dataset is equal to the average accuracies acc i in 10 independent runs. 4.2. Datasets datasets and one arti fi cial dataset. We tried to maintain the diversity of the dataset and also their number of classes, features and samples to be true. Usage of large number of diverse datasets can improve the validation of the results and lead to de fi results. The information about used datasets is shown in Table 1 . Half-Ring datasets are described in Minaei-Bidgoli et al. (2014 ). are normalized. In these datasets, all experiments are performed over the normalized features in the starred dataset. It means that each feature is normalized with a mean of 0 and variance of 1, N (0, 1). The arti fi cial Half-Ring dataset is depicted in Fig. 6 . 4.3. Experimental setting The measure of decision for each decision tree is based on the
Gini measure. The threshold of pruning is set to 2. The classi parameters are fi xed during their usages.

All of the used MLPs in our experiments had two hidden layers including 10 and 5 neurons, respectively, in the fi rst and second hidden layers. All of them are trained in 100 epochs.

In experiments the value of parameters n , b and threshold for accepting a cluster are set to 151, 30, and 2. (i.e. only the clusters with one classi fi er is dropped down). We use the 4-fold cross-validation in our experiments. We use the k -means clustering algorithm with different k parameters. 4.4. Experimental results
To see the effect of parameter r on the performance of classi fi cation over CSBS methods (by Bagging, Boosting and Gianito) with two base classi fi ers (MLP, DT) see Figs. 7
These fi gures show the accuracy of different methods by 4-fold cross-validation on some benchmarks. With due attention to these fi gures, increasing the cluster number parameter r does not always lead to performance improvement. R  X  15 is the best choice for all of the datasets. It means that if n  X  151 then r  X  15 is the best choice for cluster number parameter. In other words, using the 10% of base classi fi ers in the fi nal ensemble can be a good option. And then a classi fi er which contains about 10 classi fi ers is selected from each cluster. So it enables the method to select the classi that covers other classi fi ers.

The performance of CSBC by Boosting over some datasets with n  X  151 and different r are shown in Figs. 7 and 8 , respectively, while MLP and DT are used as the base classi fi er. The same results are depicted in Figs. 9 and 10 for CSBC using Gianito's method, respectively, while the same classi fi ers are used as the base classi fi er. Figs. 11 and 12 fi nally represent the performances of CSBC by the Bagging method, respectively, while the same classi fi ers are used as the base classi fi er. Figs. 13 and 14 depict
C the averaged accuracies over all 14 different datasets. Fig. 15 shows the effect of sampling rate over the performance of two different proposed methods.

The results of CSBC by Gianoto's method and the same base classi fi ers are shown in Figs. 9 and 10 .

The performance of the CSBC method by Bagging and the same base classi fi ers is represented in Figs. 11 and 12 .
Figs. 13 and 14 depict the averaged accuracies over all 14 different datasets. Fig. 13 represents the performances of the proposed framework by using MLP as the base classi fi er and
Fig. 14 represents the performances of the proposed framework by using DT as the base classi fi er.

Boosting and Giacinto and Roli's ensemble to generate base classi fi ers. Also it is better to use r  X  33 for all 14 ensembles.
In other words, it is better to use the 22% of base classi fi nal ensemble.

Comparison between Figs. 13 and 14 shows that using a decision tree as a base classi fi er can lead to increasing the gap between three approaches in generating an ensemble of base classi fi ers. Because a decision tree is sensitive to its training set, so the use of a decision tree as the base classi fi er is consistent with the Bagging mechanism.

You can see the effect of sampling rate over the two proposed methods. In Fig. 15 to reach these results, we use the decision tree as the base classi fi er. As is obvious, if b is a very low value, the performance is very weak, and a very high value of b cannot improve the performance. It even causes to decrease the perfor-mance for values after 40%.

Table 2 shows the averaged accuracies obtained from different ensembles by DT as the base classi fi er. Table 3 shows the accuracies obtained from the same ensemble methods presented in Table 1 by MLP as the base classi fi er. The parameter r  X  2 is used in both tables.

If we select only at most 22% of the base classi fi ers, the accuracy of the ensemble can be better than the full ensemble.
Also, it is better than the Boosting method and the proposed method based on Boosting.

Because the selected classi fi ers in this manner have different outputs, they are more suitable than ensemble of all them. It is noticeable that the Boosting method is diverse enough to be a complete ensemble. And in a Boosting ensemble, each member can cover the errors of the previous members. In the CSBC method, a classi fi er is selected from a cluster randomly. It means that a random classi fi er is selected from each part. Table 4 shows the performance of the three methods for selecting a classi fi partition. These methods are RS, TAMS and TNTCCS. RS stands for  X  random selection  X  , TAMS stands for  X  the most accurate selection .

In TAMS, the most accurate classi fi er is selected from a cluster as the index classi fi er of that cluster. In order to guarantee the of the training dataset. TNTCCS stands for  X  the nearest-to-cluster-center selection  X  . In this method the nearest classi fi center is selected. The measure of distance is the same measure used in the partitioning algorithm.
 followed by RS. Although we expected TMAS to be the best method, the experimental results show it is the worst one. 5. Conclusion and future works performance of classi fi cation. In the proposed approach we used the modi fi ed version of Bagging as the base classi fi er selection. We used the k -means method to partition the base classi fi ers and a random classi fi er is selected from each partition. A set contains all of the selected classi fi ers considered as an ensemble. Because each cluster is selected based on the classi fi ers' output, it is likely that we choose one classi fi er from each cluster. And then a diverse ensemble is created from them. This method is better than the traditional Bagging and Boosting methods, which use all of the classi fi ers as an ensemble. Also, the TNTCCS is a better method than RS. It can produce an ensemble with high diversity.
Using the decision tree as a base classi fi er can increase the gap between the tree methods in generating an ensemble of base classi fi ers. Because the decision tree is sensitive to its trained set, we use it as a base classi fi er, which is consistent with the Bagging method.

When we use the 22% of the base classi fi er, the accuracy of their ensemble is better than their full ensemble and also it is better than the Boosting method. As a result, using the 22% of the base classi fi er can be a good option.

The parameters and the aims of this paper are as follows: (1) Clustering of classi fi ers is better than ensembles created by the
Bagging and Boosting methods. (2) Clustering of classi fi well by Bagging; however, it does not work by Boosting as well as Bagging.

Our future work is to further study the variance of the method, since it is said that Bagging can reduce variance and Boosting can simultaneously reduce variance and error rate.
 References
