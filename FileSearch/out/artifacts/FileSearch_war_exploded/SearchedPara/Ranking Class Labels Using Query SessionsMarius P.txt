 Motivation : The offline acquisition of instances ( rio de janeiro , porsche cayman ) and their correspond-ing class labels ( brazilian cities , locations , vehicles , sports cars ) from text has been an active area of re-search. In order to extract fine-grained classes of instances, existing methods often apply manually-created (Banko et al., 2007; Talukdar et al., 2008) or automatically-learned (Snow et al., 2006) extraction patterns to text within large document collections.
In Web search, the relative ranking of documents returned in response to a query directly affects the outcome of the search. Similarly, the quality of the relative ranking among class labels extracted for a given instance influences any applications (e.g., query refinements or structured extraction) using the extracted data. But due to noise in Web data and limitations of extraction techniques, class labels ac-quired for a given instance (e.g., oil shale ) may fail to properly capture the semantic classes to which the instance may belong (Kozareva et al., 2008). In-evitably, some of the extracted class labels will be less useful (e.g., sources , mutual concerns ) or incor-rect (e.g., plants for the instance oil shale ). In pre-vious work, the relative ranking of class labels for an instance is determined mostly based on features derived from the source Web documents from which the data has been extracted, such as variations of the frequency of co-occurrence or diversity of extraction patterns producing a given pair (Etzioni et al., 2005). Contributions : This paper explores the role of Web search queries, rather than Web documents, in inducing superior ranking among class labels ex-tracted automatically from documents for various in-stances. It compares two sources of indirect ranking evidence available within anonymized query logs: a) co-occurrence of an instance and its class label in the same query; and b) co-occurrence of an in-stance and its class label, as separate queries within the same query session. The former source is a noisy attempt to capture queries that narrow the search re-sults to a particular class of the instance (e.g., jaguar car maker ). In comparison, the latter source nois-ily identifies searches that specialize from a class (e.g., car maker ) to an instance (e.g., jaguar ) or, conversely, generalize from an instance to a class. To our knowledge, this is the first study comparing inherently-noisy queries and query sessions for the purpose of ranking of open-domain, labeled class in-stances.
The remainder of the paper is organized as fol-lows. Section 2 introduces intuitions behind an approach using queries for ranking class labels of various instances, and describes associated ranking functions. Sections 3 and 4 describe the experi-mental setting and evaluation results over evaluation sets of instances associated with Web search queries. The results illustrate the higher quality of the query-based, re-ranked lists of class labels, relative to alter-native ranking methods using only document-based counts. Ranking Hypotheses : We take advantage of anonymized query logs, to induce superior ranking among the class labels associated with various class instances within an IsA repository acquired from Web documents. Given a class instance I , the func-tions used for the ranking of its class labels are cho-sen following several observations.  X  Hypothesis H 1 : If C is a prominent class of an instance I , then C and I are likely to occur in text in contexts that are indicative of an IsA relation.  X  Hypothesis H 2 : If C is a prominent class of an instance I , and I is ambiguous, then a fraction of the queries about I may also refer to and contain C .  X  Hypothesis H 3 : If C is a prominent class of an instance I , then a fraction of the queries about I may be followed by queries about C , and vice-versa. Ranking Functions : The ranking functions follow directly from the above hypotheses.  X  Ranking based on H 1 (using documents): The first hypothesis H from previous work (Etzioni et al., 2005). In prac-tice, a class label is deemed more relevant for an in-stance if the pair is extracted more frequently and by multiple patterns, with the scoring formula: where F req ( C , I ) is the frequency of extraction of C for the instance I , and Size ( { P attern ( C ) } ) is the number of unique patterns extracting the class label C for the instance I . The patterns are hand-written, following (Hearst, 1992): h [..] C [such as | including] I [and | , | .] i , where I is a potential instance (e.g., diderot ) and C is a potential class label (e.g., writers ). The bound-aries are approximated from the part-of-speech tags of the sentence words, for potential class labels C ; and identified by checking that I occurs as an entire query in query logs, for instances I (Van Durme and Pas  X ca, 2008).

The application of the scoring formula (1) to can-didates extracted from the Web produces a ranked list of class labels L  X  Ranking based on H 2 (using queries): Intu-itively, Web users searching for information about I sometimes add some or all terms of C to a search query already containing I , either to further spec-ify their query, or in response to being presented with sets of search results spanning several mean-ings of an ambiguous instance. Examples of such queries are happiness emotion and diderot philoso-pher . Moreover, queries like happiness positive psy-chology and diderot enlightenment may be consid-ered to weakly and partially reinforce the relevance of the class labels positive emotions and enlighten-ment writers of the instances happiness and diderot respectively. In practice, a class label is deemed more relevant if its individual terms occur in pop-ular queries containing the instance. More precisely, for each term within any class label from L we compute a score T ermQueryScore . The score is the frequency sum of the term within anonymized queries containing the instance I as a prefix, and the term anywhere else in the queries. Terms are stemmed before the computation.

Each class label C is assigned the geometric mean of the scores of its N terms T words:
The geometric mean is preferred to the arithmetic mean, because the latter is more strongly affected by outlier values. The class labels are ranked according to the means, resulting in a ranked list L case of ties, L L  X  Ranking based on H 3 (using query sessions): Given the third hypothesis H for information about I may subsequently search for more general information about one of its classes C . Conversely, users may specialize their search from a class C to one of its instances I . Examples of such queries are happiness followed later by emo-tions , or diderot followed by philosophers ; or emo-tions followed later by happiness , or philosophers followed by diderot . In practice, a class label is deemed more relevant if its individual terms occur as part of queries that are in the same query session as a query containing only the instance. More precisely, for each term within any class label from L we compute a score T ermSessionScore , equal to the frequency sum of the anonymized queries from the query sessions that contain the term and are: a) ei-ther the initial query of the session, with the instance I being one of the subsequent queries from the same session; or b) one of the subsequent queries of the session, with the instance I being the initial query of the same session. Before computing the frequen-cies, the class label terms are stemmed.

Each class label C is assigned the geometric mean of the scores of its terms, after ignoring stop words:
The class labels are ranked according to the geo-metric means, resulting in a ranked list L case of ties, L from L Unsupervised Ranking : Given an instance I , the ranking hypotheses and corresponding functions L of them) can be used together to generate a merged, ranked list of class labels per instance I . The score of a class label in the merged list is determined by the inverse of the average rank in the lists L and L ing formula: where N is the number of input lists of class labels (in this case, 3), and Rank( C , L in the input list of class labels L L the list L the absolute scores of the class labels within the in-put lists, the outcome of the merging is less sensitive to how class labels of a given instance are numeri-cally scored within the input lists. In case of ties, the scores of the class labels from L secondary ranking criterion. Thus, every instance I from the IsA repository is associated with a ranked list of class labels computed according to this rank-ing formula. Conversely, each class label C from the IsA repository is associated with a ranked list of class instances computed with the earlier scoring formula (1) used to generate lists L
Note that the ranking formula can also consider only a subset of the available input lists. For in-stance, Score L L Textual Data Sources : The acquisition of the IsA repository relies on unstructured text available within Web documents and search queries. The queries are fully-anonymized queries in English sub-mitted to Google by Web users in 2009, and are available in two collections. The first collection is a random sample of 50 million unique queries that are independent from one another. The second col-lection is a random sample of 5 million query ses-sions. Each session has an initial query and a se-ries of subsequent queries. A subsequent query is a query that has been submitted by the same Web user within no longer than a few minutes after the initial query. Each subsequent query is accompanied by its frequency of occurrence in the session, with the corresponding initial query. The document collec-tion consists of a sample of 100 million documents in English.
 Experimental Runs : The experimental runs corre-spond to different methods for extracting and rank-ing pairs of an instance and a class:  X  from the repository extracted here, with class labels of an instance ranked based on the frequency and the number of extraction patterns ( Score from Equation (1) in Section 2), in run R  X  from the repository extracted here, with class labels of an instance ranked via the rank-based merging of: Score R occurrence of an instance and its class label in the same query; Score run R occurrence of an instance and its class label, as sep-arate queries within the same query session; and Score H 1+ H 2+ H 3 from Section 2, in run R u , which corresponds to re-ranking using both types of co-occurrences in queries. Evaluation Procedure : The manual evaluation of open-domain information extraction output is time consuming (Banko et al., 2007). A more practi-cal alternative is an automatic evaluation procedure for ranked lists of class labels, based on existing re-sources and systems.

Assume that there is a gold standard, containing gold class labels that are each associated with a gold set of their instances. The creation of such gold stan-dards is discussed later. Based on the gold standard, the ranked lists of class labels available within an IsA repository can be automatically evaluated as fol-lows. First, for each gold label, the ranked lists of class labels of individual gold instances are retrieved from the IsA repository. Second, the individual re-trieved lists are merged into a ranked list of class labels, associated with the gold label. The merged list can be computed, e.g., using an extension of the Score H 1+ H 2+ H 3 formula (Equation (4)) described earlier in Section 2. Third, the merged list is com-pared against the gold label, to estimate the accu-racy of the merged list. Intuitively, a ranked list of class labels is a better approximation of a gold label, if class labels situated at better ranks in the list are closer in meaning to the gold label.
 Evaluation Metric : Given a gold label and a list of class labels, if any, derived from the IsA repository, the rank of the highest class label that matches the gold label determines the score assigned to the gold label, in the form of the reciprocal rank of the match. Thus, if the gold label matches a class label at rank 1, 2 or 3 in the computed list, the gold label receives a score of 1, 0.5 or 0.33 respectively. The score is 0 if the gold label does not match any of the top 20 class labels. The overall score over the entire set of gold labels is the mean reciprocal rank (MRR) score over all gold labels from the set. Two types of MRR scores are automatically computed:  X  MRR f considers a gold label and a class label to match, if they are identical;  X  MRR p considers a gold label and a class label to match, if one or more of their tokens that are not stop words are identical.

During matching, all string comparisons are case-insensitive, and all tokens are first converted to their singular form (e.g., european countries to european country ) using WordNet (Fellbaum, 1998). Thus, in-surance carriers and insurance companies are con-sidered to not match in MRR MRR give credit to less relevant class labels, such as insur-ance policies for the gold label insurance carriers . Therefore, MRR pessimistic estimate of the actual usefulness of the computed ranked lists of class labels as approxima-tions of the gold labels. IsA Repository : The IsA repository, extracted from the document collection, covers a total of 4.04 mil-lion instances associated with 7.65 million class la-bels. The number of class labels available per in-stance and vice-versa follows a long-tail distribu-tion, indicating that 2.12 million of the instances each have two or more class labels (with an average of 19.72 class labels per instance).
 Evaluation Sets of Queries : Table 1 shows sam-ples of two query sets, introduced in (Pas  X ca, 2010) and used in the evaluation. The first set, denoted Q is obtained from a random sample of anonymized, class-seeking queries submitted by Web users to Google Squared. The set contains 807 queries, each associated with a ranked list of between 10 and 100 gold instances automatically extracted by Google Squared.

Since the gold instances available as input for each query as part of Q they may or may not be true instances of the respec-tive queries. As described in (Pas  X ca, 2010), the sec-ond evaluation set Q Q query in Q inspection. The 40 queries from Q with between 8 and 33 human-validated instances.
As shown in the upper part of Table 2, the queries from Q of 2 tokens per query. Queries from Q paratively shorter, both in maximum (3 tokens) and average (1.4 tokens) length. The lower part of Ta-ble 2 shows the number of gold instances available as input, which average around 70 and 17 per query, for queries from Q another view on the distribution of the queries from evaluation sets, Table 3 lists tokens that are not stop words, which occur in most queries from Q paratively, few query tokens occur in more than one query in Q Evaluation Procedure : Following the general eval-uation procedure, each query from the sets Q Q corresponding set of instances. Given a query and its instances I from the evaluation sets Q a merged, ranked lists of class labels is computed out of the ranked lists of class labels available in the underlying IsA repository for each instance I . The evaluation compares the merged lists of class labels, with the corresponding queries from Q Accuracy of Lists of Class Labels : Table 4 summa-rizes results from comparative experiments, quanti-fying a) horizontally, the impact of alternative pa-rameter settings on the computed lists of class la-bels; and b) vertically, the comparative accuracy of the experimental runs over the query sets. The ex-perimental parameters are the number of input in-stances from the evaluation sets that are used for re-trieving class labels, I-per-Q, set to 3, 5, 10; and the number of class labels retrieved per input instance, C-per-I, set to 5, 10, 20.
 Four conclusions can be derived from the results. First, the scores over Q Q input set of instances available in Q Q the corresponding queries. Second, when I-per-Q is fixed, increasing C-per-I leads to small, if any, score improvements. Third, when C-per-I is fixed, even small values of I-per-Q, such as 3 (that is, very small sets of instances provided as input) produce scores that are competitive with those obtained with a higher value like 10. This suggests that useful class labels can be generated even in extreme scenarios, where the number of instances available as input is as small as 3 or 5. Fourth and most importantly, for most combinations of parameter settings and on both query sets, the runs that take advantage of query logs (R lar, when I-per-Q is set to 10 and C-per-I to 20, run R among the top three to four class labels returned (score 0.278); and as a partial match among the top one to two class labels returned (score 0.636), as an average over the Q score of 0.278 over the Q is 27% higher than with run R
In all experiments, the higher scores of R R labels, relative to R rameter settings described in Table 4, values around 10 for I-per-Q and 20 for C-per-I give the highest scores over both Q
Among the query-based runs R highest scores in Table 4 are obtained mostly for run R . Thus, between the presence of a class label and an instance either in the same query, or as separate queries within the same query session, it is the lat-ter that provides a more useful signal during the re-ranking of class labels of each instance.

Table 5 illustrates the top class labels from the ranked lists generated in run R from both Q computed class labels are relatively resistant to noise and variation within the input set of gold instances. For example, the top elements of the lists of class la-bels generated for computer languages are relevant and also quite similar for Q list of gold instances in Q items (e.g., acm transactions on mathematical soft-ware ). Similarly, the class labels computed for eu-ropean countries are almost the same for Q although the overlap of the respective lists of 10 gold instances used as input is not large. The table shows at least one query ( park slope restaurants ) for which the output is less than optimal, either because the class labels (e.g., businesses ) are quite distant se-mantically from the query (for Q output is produced at all, due to no class labels being found in the IsA repository for any of the 10 input gold instances (for Q ever, the computed class labels arguably capture the meaning of the original query, although not neces-sarily in the exact same lexical form, and sometimes only partially. For example, for the query endan-gered animals , only the fourth class label from Q identifies the query exactly. However, class labels preceding endangered animals already capture the notion of animals or species (first and third labels), or that they are endangered (second label).
Figure 1 provides a detailed view on the distribu-tion of queries from the Q for which the class label that matches the query oc-curs at a particular rank in the computed list of class labels. In the first graph of Figure 1, for Q query matches the automatically-generated class la-bel at ranks 1, 2, 3, 4 and 5 for 18.9%, 10.3%, 5.7%, 3.7% and 1.2% of the queries respectively, with full string matching, i.e., corresponding to MRR for 52.6%, 12.4%, 5.3%, 3.7% and 1.7% respec-tively, with partial string matching, corresponding to MRR scores are obtained for Q lar, the query matches the class label at rank 1 and 2 for 50.0% and 17.5% (or a combined 67.5%) of the queries from Q 52.6% and 12.4% (or a combined 67%), with partial string matching.
 Discussion : The quality of lists of items extracted from documents can benefit from query-driven rank-ing, particularly for the task of ranking class labels of instances within IsA repositories. The use of queries for ranking is generally applicable: it can be seen as a post-processing stage that enhances the ranking of the class labels extracted for various in-stances by any method into any IsA repository.
Open-domain class labels extracted from text and re-ranked as described in this paper are useful in a variety of applications. Search tools such as Google Squared return a set of instances, in response to class-seeking queries (e.g., insurance companies ). The labeling of the returned set of instances, using the re-ranked class labels available per instances, al-lows for the generation of query refinements (e.g., insurers ). In search over semi-structured data (Ca-farella et al., 2008), the labeling of column cells is useful to infer the semantics of a table column, when the subject row of the table in which the column ap-pears is either absent or difficult to detect. The role of anonymized query logs in Web-based information extraction has been explored in tasks such as class attribute extraction (Pas  X ca and Van Durme, 2007), instance set expansion (Pennacchiotti and Pantel, 2009) and extraction of sets of similar entities (Jain and Pennacchiotti, 2010). Our work compares the usefulness of queries and query ses-sions for ranking class labels in extracted IsA repos-itories. It shows that query sessions produce better-ranked class labels than isolated queries do. A task complementary to class label ranking is entity rank-ing (Billerbeck et al., 2010), also referred to as rank-ing for typed search (Demartini et al., 2009).
The choice of search queries and query substitu-tions is often influenced by, and indicative of, vari-ous semantic relations holding among full queries or query terms (Jones et al., 2006). Semantic relations may be loosely defined, e.g., by exploring the ac-quisition of untyped, similarity-based relations from query logs (Baeza-Yates and Tiberi, 2007). In com-parison, queries are used here to re-rank class labels capturing a well-defined type of open-domain rela-tions, namely IsA relations. In an attempt to bridge the gap between informa-tion stated in documents and information requested in search queries, this study shows that inherently-noisy queries are useful in re-ranking class labels ex-tracted from Web documents for various instances, with query sessions leading to higher quality than isolated queries. Current work investigates the im-pact of ambiguous input instances (Vyas and Pantel, 2009) on the quality of the generated class labels.
