 Many large-scale Web applications that require ranked top-k re-trieval are implemented using inverted indices. An inverted in-dex represents a sparse term-document matrix, where non-zero el-ements indicate the strength of term-document associations. In this work, we present an approach for lossless compression of inverted indices. Our approach maps terms in a document corpus to a new term space in order to reduce the number of non-zero elements in the term-document matrix, resulting in a more compact inverted in-dex. We formulate the problem of selecting a new term space as a matrix factorization problem, and prove that finding the optimal so-lution is an NP-hard problem. We develop a greedy algorithm for finding an approximate solution.

A side effect of our approach is increasing the number of terms in the index, which may negatively affect query evaluation perfor-mance. To eliminate such effect, we develop a methodology for modifying query evaluation algorithms by exploiting specific prop-erties of our compression approach.
 H.3.1 [ Information Storage And Retrieval ]: Indexing methods Algorithms, Experimentation, Performance
Web search engines and other large-scale information retrieval systems typically have to process query workloads of thousands of requests per second over large collections of documents. Usually, the result of the retrieval is a ranked list of the top few ( k ) results.
Top-k retrieval is defined as follows. Given a query Q and a doc-ument corpus Docs , find the k documents { D 1 , D 2 , . . . , D Docs that have the highest score, according to some scoring func-tion Score ( D, Q ) . Both the query and the documents are sets of terms from the same high-dimension space. Scoring is usually per-formed based on the overlapping terms between the query and the document. The document corpus Docs can be represented as a two dimensional matrix, denoted as V , with m terms (rows) and n documents (columns). The values of elements in V reflect how strongly terms are associated with documents (e.g., tf-idf).
Inverted indices are the prevailing implementation of scalable top-k retrieval. In an inverted index, each term T appearing in the corpus Docs is associated with a posting list , which enumerates the documents that contain T . Each posting list consists of a se-quence of document identifiers, each of which associated with an optional payload (e.g., term frequency). An inverted index can also be viewed as a sparse representation of the matrix V that stores only non-zero matrix elements.

In several applications, top-k queries are processed while the user is waiting for the results, which imposes very strict bounds on query latency. Due to such requirements, memory-resident in-dices are becoming more popular. To reduce the amount of re-quired memory, and hence the system cost, compression techniques (e.g., [6, 18, 21]) are heavily used to reduce the size of the inverted indices. Compression techniques are divided into two categories: lossy compression, where quality of the results might be affected by compression, and lossless compression, where the quality is not affected. In this work, we focus on lossless compression.
In this paper, we propose a novel lossless compression technique that holistically compresses multiple posting lists by taking advan-tage of similarities between them. This in contrast to the many existing lossless compression approaches that compress each post-ing list individually [16, 21]. The compression approach we pro-pose can be applied before the standard per-posting-list compres-sion in order to combine the benefits of both methods. In spirit, the technique presented in this paper is related to matrix factorization methods such as Non-negative Matrix Factorization [10, 13], La-tent Dirichlet Allocation [4], Singular Value Decomposition [19], and Principal Component Analysis [11]. All of these techniques map the documents from the space of the original terms into a lower dimensional space. However, unlike previous factorization methods, we aim at providing an exact factorization of the input matrix in order to avoid any information loss, while reducing the number of non-zero elements in the resulting factors. Furthermore, we do not restrict the dimensionality of the factor matrices.
For example, the top term-document matrix in Figure 1 is fac-tored into two matrices W and H such that: (1) the product of the factors is equal to the input matrix, and (2) the factor matrices con-tain fewer non-zeros than the input matrix. Note that the rank of the second factor H (five), is higher than the rank of the input ma-trix (two). To answer queries, we use the first factor to map (i.e., rewrite) query terms from the original term space to a new space of meta-terms (e.g., T 1 is mapped to  X  1 ,  X  2 ,  X  3 ,  X  4 in Figure 1). The rewritten query is used for searching the index built from the sec-ond factor using any top-k search algorithms. In this paper, we prove that finding the optimal (i.e., the sparsest) factorization is NP-hard, and we develop a greedy algorithm that efficiently finds an approximate solution.

Although lossless compression helps reducing the amount of memory required for storing a given index, it usually incurs a com-putational overhead due to the need of decompressing the index data [18, 21]. Such overhead should be minimized in order to keep query latency small. The computational overhead in our approach is due to rewriting a query using a number of meta-terms that is greater than the number of the original terms. For example, the number of meta-terms in Figure 1 is five, while the original num-ber of terms is two. We show how to eliminate such negative effect by exploiting some unique characteristics of our compression tech-nique. More specifically, we show that standard query processing approaches such as No-Random-Access (NRA) algorithm [8] can be modified to search the compressed index as fast as the original algorithm searches the uncompressed index.
In this paper, we use the vector-space representation of docu-ments and queries. That is, documents and queries are represented as vectors in a multidimensional space, where each term is a dimen-sion. Let  X  = { T 1 , T 2 , . . . , T m } be a set of terms. A document D is a vector ( d 1 j , d 2 j , . . . , d m j ) . When a term T D , the element d i j is non-zero, and its value is typically related to the number of times T i occurs in D j . Similarly, a query Q is rep-resented by a vector ( w 1 , w 2 , . . . , w m ) , where non-zero elements correspond to terms appearing in the query, and their values are term weights in the query.

Given a corpus of n documents Docs = { D 1 , . . . , D n } , and a query Q , a common task in many information retrieval systems is to retrieve the k documents with the highest score according to some scoring function Score ( D, Q ) . In this work, we assume the scoring function is defined as the inner product of document and query vectors: Score ( D j , Q ) = P m i =1 d i j  X  w i .
Many information retrieval systems use inverted indices as their main data structure for top-k retrieval. An inverted index is a col-List L i is a vector containing weights of term T i in all documents posting lists is used, where zero entries are omitted.

A na X ve top-k algorithm would examine all entries in the posting lists relevant to the query, compute the scores of found documents, and return the top-k documents. However, the total number of doc-uments in the relevant posting lists is typically much larger than k , especially when Q contains frequent terms. Many top-k search al-gorithms (e.g., [5, 8]) aim at retrieving the top-k documents while examining only a fraction of entries in the relevant posting lists.
We represent an index by an m  X  n term-document matrix V (Figure 1). We denote by V [ T, D ] the value of the element in V corresponding to a row T and a column D . We use k V k 0 to denote the number of non-zero elements in V . Top-k retrieval is defined as computing scores of all documents, represented by a size-n vector S , and picking the top-k documents with highest scores. The trans-pose of vector S is equal to Q T V , where the superscript T denotes the transpose operator.

Typical document collections, such as Web corpora, contain re-dundant elements in their term-document matrices due to duplicate or near-duplicate contents. For example, news articles are usually shared across multiple Web sites. In this case, two documents D and D y that refer to the same article would contain several iden-tical sentences consisting of terms T a , T b , . . . , T the term-document matrix would contain two identical sets of val-ues: V [ T a , D x ] , . . . , V [ T p , D x ] , and V [ T Another example that leads to redundancy in term-document ma-trix is co-occurrence of subsets of terms in multiple documents. For example, terms  X  X ritney X  and  X  X pears X  usually co-occur in docu-ments related to music.

A known technique for reducing redundancy in a matrix is matrix factorization. The simplest form of factorization is decomposing V into two matrices: an m  X  r matrix W and an r  X  n matrix H , such that V = W H . Note that since our goal is lossless index compres-sion, we consider the exact formulation and not the approximate one ( V  X  W H ). Our objective is to minimize the total number of non-zero elements in W and H (i.e., k W k 0 + k H k 0 ). Intuitively, factoring V into W H transforms the set of terms  X  into another space, denoted  X  , consisting of r meta-terms {  X  1 ,  X  2 , . . . ,  X  r } . Matrix W linearly maps terms in  X  to meta-terms in  X  (and vice-versa), while matrix H represents an inverted index of Docs in space  X  . Figure 1 is an illustration of such a factor-ization, where terms { T 1 , T 2 } are linearly mapped into meta-terms {  X  1 , . . . ,  X  5 } using matrix W , and documents are represented as combinations of these meta-terms in matrix H . Note that although r &gt; m (i.e., the number of rows in H is greater than the number of rows in V ), the total number of non-zeros in W and H is less than the number of non-zeros in V .

Evaluation of query Q is performed on the inverted index repre-sented by H , after rewriting Q according to W . Specifically, we rewrite the query vector Q into vector Q  X  such that Q  X  T In other words, each term T in Q is replaced by a set of meta-terms {  X  : W [ T,  X  ] 6 = 0 } , and the weight of each term  X  in Q is w  X  W [ T,  X  ] , where w is the weight of T in Q . Any standard search algorithm can be used to retrieve the top-k documents from the compressed index H using query Q  X  . The following theorem proves that searching the original inverted index using Q is equiv-alent to searching the index represented by H using Q  X  . T HEOREM 1. L et W and H be the result of factoring V (i.e., W H = V ). Let A ( V, Q, k ) be the top-k documents for query Q using inverted index V and the scoring function in Section 2 (ties are broken by some predefined criteria). Let the rewritten query be Q  X  such that Q  X  T = Q T W . Then, A ( V, Q, k ) = A ( H, Q
The proof is described in [3]. An immediate consequence is that standard top-k algorithms can still be used for searching the com-pressed indices without any loss in quality.
 R
EMARK It is possible to interpret the intermediate space  X  a s a space of meta-documents , rather than meta-terms . In this case, the matrix W represents an inverted index of meta-documents in the original term space  X  , and H is a mapping from meta-documents to documents in Docs . However, existing top-k retrieval algorithms that employ early termination cannot be used on W , since top-k meta-documents do not necessarily contain the top-k documents.
In this section, we consider the following exact sparse matrix factorization problem. Given a matrix V , obtain W and H that The following theorem states that the problem of obtaining the sparsest exact factorization is NP-hard.

T HEOREM 2. G iven a matrix V , the problem of obtaining two matrices W and H , subject to the constraint W H = V , such that k W k 0 + k H k 0 is minimum is NP-hard.

Proof of Theorem 2 is provided in [3]. Hardness of the problem is proved by a reduction from an NP-complete problem, namely the S
S ince obtaining the optimal factorization is computationally in-feasible, we propose an iterative greedy algorithm for getting an exact factorization that might not have the minimal number of non-zeros. The key idea is to start with a trivial factorization W ( I m is the identity matrix of rank m ) and H 0 = V , and iteratively improve the current solution ( W t , H t ) by a sequence of local trans-formations on W t and H t , obtaining W t +1 and H t +1 . At each step, we ensure that k W t +1 k 0 + k H t +1 k 0 &lt; k W W
The transformation performed at each step is described as fol-lows. At step t , given matrices W t and H t , the algorithm looks for a submatrix H s t of H t defined by a subset R of H t  X  X  rows, and a subset C of H t  X  X  columns, such that the rank of H s t is one (i.e., rows are multiples of each others):
Clearly, keeping only one representative row from H s t , and en-coding other rows in H s t as multiples of the representative row, would reduce the number of non-zero values in H t . Unfortunately, identifying the largest rank-1 submatrix H s t is equivalent to the problem of finding the largest bi-cluster [15], which is an NP-hard problem. Thus, our algorithms considers only submatrices consist-ing of exactly two rows (i.e., | R | = 2 ) at each step.
For efficiency, in each iteration, our algorithm identifies z rank-1 submatrices that are composed of two rows R = {  X  i ,  X  j sets of columns C 1 , . . . , C z (i.e., Equation 1 holds for submatrices ( R, C 1 ) through ( R, C z ) ). Rows  X  i and  X  j can then be rewritten as linear combinations of a set of z new common subvectors, de-noted  X  r +1 , . . . ,  X  r + z , and two new remainder vectors  X   X  r + z +2 that retain values of columns that are not in C 1 Figure 2: Combining term vectors (a) matrices W t and H t matrices W t +1 and H t +1 after combining  X  1 and  X  2 The new vectors  X  r +1 , . . . ,  X  r + z +2 are appended to matrix H , and vectors  X  i and  X  j are removed from H t , resulting in matrix H t +1 . Matrix W t is modified by mapping the orig-inal terms  X  i ,  X  j to  X  r +1 , . . . ,  X  r + z +2 , obtaining W gorithm 1 describes the procedure in more details. Function GetCorrelatedSubmatrices ( H t ) , which we describe in Sec-tion 4.1, is responsible for extracting the sets R and C 1 that maximize space saving.

Figure 2 shows an example of combining two meta-terms  X  1  X  into common subvectors  X  3 ,  X  4 ,  X  5 , and remainder vectors  X  Without loss of generality, we assume hereafter that  X  1 =  X   X   X  =  X  z = 1 .
 Algorithm 1 ComputeFactorization( V ) 2: r  X  m 3: t  X  0 4: repeat 7: if failed to find correlated submatrix then 8: break 12: r  X  r + z + 2 13: t  X  t + 1
The goal of function GetCorrelatedSubmatrices ( H t ) is to return correlated submatrices, defined by R = {  X  i ,  X  j C , . . . , C z . Our algorithm heuristically finds the submatrices that would result in the highest reduction of space. In the following, we first describe how to find the sets C 1 , . . . , C z given R , then for-mulate the potential saving from combining two meta-terms, and finally how to find R .
 Denote by X [ p ] the value of the element at index p in vector X . For two rows  X  i and  X  j in H t , we compute vector  X  of length n such that for q  X  { 1 , . . . , n } ,  X  [ q ] =  X  i [ q ] equals 0 otherwise. Each set C p is a maximal subset of columns (documents) in H t that have the same non-zero value in  X  (which is eventually the value of the corresponding coefficient  X  tion 2). For example, in Figure 2, the first four cells have the same value in  X  , namely 2 / 3 , and thus constitute a common subvector (  X  ). If is clear that C 1 , . . . , C z are pairwise disjoint because each set C p corresponds to a unique non-zero ratio in  X  . We rely on this property to improve query evaluation performance (Section 5).
The space saving resulting from combining  X  i and  X  j is com-puted as follows. Combining  X  i and  X  j in Algorithm 1 reduces the number of non-zero elements in H t by P z p =1 | C p | because each subvector corresponding to C p is stored twice in H t and only once in H t +1 . All the terms that were mapped to either  X  i or  X  now mapped to additional z meta-terms  X  r +1 , . . . ,  X  r + z For example, in Figure 2, T 1 is originally mapped to  X  1 combining  X  1 and  X  2 , T 1 is mapped to extra 3 meta-terms, namely  X  ,  X  4 ,  X  5 , (besides the remainder meta-term  X  6 ), which results in three additional elements in W . Formally, the overall space saving when combining  X  i and  X  j is:
In the following, we describe how to efficiently identify a pair of rows in H t , with the highest potential savings. A straightfor-ward approach is to compute the potential space saving, based on Equation 4, for all pairs of rows and return the pair with the highest savings. However, the complexity of such approach is quadratic in the number of rows in H t , which is prohibitively expensive. To reduce the number of pair-wise comparisons, we use a blocking technique to heuristically prune a large number of pairs that have low potential space saving. Specifically, we partition the rows in H t into several disjoint blocks, based on the number of non-zeros in each row, and we only compute the potential saving for pairs of rows within the same block. The main hypothesis here is that rows that have significantly different number of non-zero elements are uncorrelated, and they are unlikely to have overlapping elements. Limiting the Number of Subvectors. Our compression ap-proach iteratively reduces the index size at the cost of increas-ing the number of meta-terms. That is, there is a trade-off be-tween the space savings and the increase in the number of meta-terms. In particular, it may not be beneficial to introduce new meta-terms whose space savings are below some threshold. The length of a new meta-term  X  r + p represents its maximum potential saving, according to Equation 4. Therefore, we modify algorithm GetCorrelatedSubmatrices ( H t ) such that it generates a new meta-term only if its length is greater than or equal to a threshold  X  . Consider the example depicted in Figure 2. Assuming that  X  = 3 , only the first two meta-terms  X  3 ,  X  4 would be generated, while the third meta-term  X  5 would not be generated (i.e., it becomes part of the remainder meta-terms  X  6 ,  X  7 ).
 MapReduce Implementation. To scale the algorithm to large matrices, we parallelize it according to the MapReduce model [12]. We notice that combining two rows of matrix H t is done indepen-dently of other rows on H t . Therefore, it is possible to combine multiple pairs of rows of H t in parallel, as long as pairs are mutu-ally disjoint. Such observations allow the following parallelization scheme: (1) map each row in H t to a specific block, based on the number of non-zeros in each row, (2) compute the potential space saving for each pair of rows in the same block in parallel, (3) for each block, identify a set of disjoint pairs of rows that maximize the overall saving, (4) for each block, combine all pairs and output the (disjoint) parts of H t +1 and W M , and (5) collect all the parts and construct H t +1 , and W t +1 = W t W M .
 Updating the Compressed Index. Due to the dynamic nature of documents copra that are found in the Web, we must be able to update inverted indices with minimal computational cost, without the need of reconstructing the entire index from scratch. In the following, we show how to append a new document to the index, and how to remove an existing document (updating a document can be decomposed into removing the old version of the document, and inserting the new version). We notice that at least one meta-term is a remainder meta-term, denoted Rem ( T i ) , that is uniquely mapped to the original term T i (i.e, W [ T i , Rem ( T i  X  j 6 = i ( W [ T j , Rem ( T i )] = 0) ). In order to add a new document D that mention term T i , it is sufficient to add a new posting to the posting list of Rem ( T i ) . To remove a document D from the index, we remove all elements in the column in H that corresponds to D . Note that the index should be reconstructed periodically (e.g., at idle times) to compress the newly inserted documents.
A side-effect of our approach is having a number of meta-terms in the rewritten query that is larger than the number of terms in the original query, which can lead to longer query response times. In this section, we show how to mitigate this undesirable effect by exploiting unique characteristics of our compression scheme. As a case study, we show how to modify the Non-Random-Access algorithm (NRA) [8]. Note that there exist a plethora of search algorithm that might be more efficient than NRA, especially for memory-resident indices. However, we chose the NRA algorithm because of its simplicity.

We denote by L 1 , . . . , L h the posting lists corresponding to the terms with non-zero weight in query Q , where h = k Q k 0 let w 1 , . . . , w h be the weights associated with L 1 , . . . , L The score of a document D can be rewritten as Score ( D, Q ) = P i =1 w i  X  L i ( D ) , where L i ( D ) denotes the weight of document D in list L i . The NRA algorithm requires posting lists to be sorted in descending order of term frequencies.

The NRA algorithm retrieves documents from lists L 1 , . . . , L in a round robin order. Having the lists sorted enables comput-ing upper and lower bounds on document scores in each list. Ev-ery time a document is retrieved, the lower and upper bounds of retrieved documents, as well as unseen documents, are updated. Once k documents are found whose lower bounds are greater than or equal to the upper bounds of all other documents (including the unseen ones), the algorithm terminates.

Score bounds are computed as follows. Let x i denote the weight of the last document retrieved from list L i , if L i is not completely read by the algorithm, or 0 otherwise. During execution of the algorithm, the score upper bound of each retrieved document D , denoted Score ( D, Q ) , is computed as follows: where L i ( D ) is the weight of D in list L i if D has appeared in L , and x i otherwise. The upper bound for unseen documents is P i =1 w i  X  x i . Similarly, the lower bound of each retrieved docu-ment D , denoted Score ( D, Q ) , is where L i ( D ) is the weight of D in list L i , if D has appeared in L and 0 otherwise.

In the following, we describe our modifications to the NRA al-gorithm. The score upper bound of each retrieved document D is computed by assuming that each undiscovered weight L i equal to x i (Equation 5). Recall that all meta-term lists correspond-ing to the same original term are disjoint (Section 4.1). Thus, it is possible to compute a tighter score upper bound by setting undis-covered weight L i ( D ) to zero, instead of x i , if D has appeared in some list L j such that L i and L j are disjoint.

Therefore, instead of considering lists of meta-terms indepen-dently, we create a two-level document retrieval scheme as follows. We create a virtual list for each original query term T i list is traversed by probing the disjoint lists of the corresponding meta-terms. We use a priority queue P Q i to implement retrieval from the virtual list of T i . Let M ( T i ) , {  X  : W [ T the set of meta-terms that term T i is rewritten into. Initialization of a priority queue P Q i is performed by inserting a pair (  X , D ) for each meta-term  X  in M ( T i ) , where D is the document with the highest score in  X  . The score of each pair (  X , D ) in the queue is equal to the score of D in list  X  multiplied by the weight W [ T Retrieving next document from the virtual list of T i is equivalent to retrieving the document D in the pair (  X , D ) at the head of the priority queue. After each retrieval from P Q i , we insert a new pair (  X , D  X  ) in P Q i , where D  X  is the next document to be retrieved from the list of  X  .

We modify the NRA algorithm to use virtual lists as follows. The algorithm initializes priority queues P Q 1 , . . . , P Q inal query terms T 1 , . . . , T h . Retrievals from each list L placed by retrievals from the corresponding virtual list. The follow-ing theorem states the correctness of the modifications, and gives an upper bound on the runtime overhead.

T HEOREM 3. Q uery results obtained by the NRA algorithm when processing query Q using the uncompressed index V are equal to the results obtained by the modified NRA algorithm when processing the same query Q using a compressed index H and a term rewriting matrix W such that V = W H . Furthermore, the modified NRA algorithm performs at most P T probes than the unmodified NRA algorithm.

The proof is provided in [3]. Some meta-terms in the rewritten query might be shared across multiple terms in the original query (i.e., M ( T i )  X  M ( T j ) 6 =  X  for T i , T j  X  Q and i 6 = j ). We further reduce the number of probes by keeping each meta-term  X  in exactly one priority queue P Q i of a term T i such that  X   X  M ( T i ) , and removing occurrences of  X  in other priority queues.
In this section, we evaluate the effectiveness of our compres-sion technique and its impact on query execution time. We show that our technique is orthogonal to the compression techniques that compress posting lists individually (e.g., var-byte encoding [16]), and thus can be integrated with such techniques as we demonstrate in this section. We perform our evaluation on memory-resident in-dices since this is the dominant approach in modern search engines due to the growing memory capacities of modern machines, as well as index partitioning techniques. Our index factorization algorithm ran on a Hadoop cluster. Query evaluation latency was measured by a single-threaded Java process running on an Intel Xeon 2.00GHz 8-core machine with 32GB RAM. Both compressed and uncompressed indices were preloaded into RAM prior to query evaluation. We used TREC WT10g document corpus [1], which contains 1.7M documents. We indexed only the textual content of the documents and discarded HTML tags. We removed rare terms that appear in less than three documents, thus reducing the number of unique terms from 5.4M to 1.6M. These rare terms account for less than 1% of the index size. Each posting contains 4-byte integer for docID and 4-byte integer Figure 3: Compression ratio for term frequency. For query workload, we used 50,000 queries that are randomly selected from the AOL query log [17]. Unless specified otherwise, the minimum savings threshold  X  is set to 100.
To measure the compression ratio, we use the relative reduction in the size of the inverted index: (Uncompressed Index Size -Com-pressed Index Size) / (Uncompressed Index Size). We consider both matrices W and H when computing the size of a compressed index. Note that for factorization-based compression only, the com-Compression Performance. We selected two compressed indices obtained after 8 and 35 iterations of our algorithm. Figure 3 shows the compression ratio for the two indices. We observe that after 8 iterations, our algorithm compresses the index by 20%. When we additionally compress each posting list in the compressed index us-ing the gap-based var-byte encoding [16], the overall compression reaches 46%. At iteration 35, the compression ratios further in-crease to 29% and 50%, respectively. The size of matrix W , which maps the original terms to the meta-terms, is less than 1% of the compressed index size in all iterations.

By lowering the saving threshold  X  to 0, our approach archives a compression ratio of 35% after 30 iterations (Figure 7(b)).
When limiting the number of mappers/reducers to 100 per each job, each iteration took 22 minutes in average. The runtime of the first few iterations is slightly above average (e.g., the first iteration took 27 minutes, while the second iteration took 23 minutes). Query Evaluation Latency. Figure 4 shows the average query latency for different numbers of retrieved documents ( k ) using the compressed indices at iterations 8 and 35. We do not show the la-tency of the unmodified NRA algorithm on the compressed indices as it is orders-of-magnitude higher and would distort the plot. In some cases, searching a compressed index outperforms searching the uncompressed index (e.g., for k = 20 , the latency at compres-sion ration of 20% is 6% lower than the latency when using the uncompressed index).
 Size of the Factor Matrices. Figure 5 depicts the relative num-ber of non-zero elements in W and H compared to the number of non-zeros in V at various iterations of the compression algorithm. Observe the monotonicity of the curve due to the property of our algorithm that never increases the number of non-zero elements in the factors. Matrix W , which is used for query rewriting, is much smaller than H (e.g., k W k 0 is less than 1% of k V k 0 ). Integration with Var-byte Encoding. In this experiment, we show the behavior of var-byte encoding [16] when applied to our compressed index. Figure 6 shows the effectiveness of the var-byte encoding at various compression ratios of our factorization algo-rithm. The relative stability of effectiveness of var-byte encoding Figure 5: Relative reduc-tion in non-zeros Figure 7: The effect of  X  on (a) the number of meta-terms, and (b) the compression ratio suggests that it is orthogonal to our compression technique, and thus both techniques can be used together to achieve higher com-pression ratios. For example, combining the two techniques en-ables compression ratio of 50% at iteration 35 (Figure 3), which is higher than using our compression technique alone (29%), or using var-byte encoding alone (34%).
 The Saving Threshold  X  . In this experiment, we analyze the effect of  X  on the total number of meta-terms in the compressed index (Figures 7(a)), and on the compression ratio (Figures 7(b)). Changing  X  from 0 to 100 reduces the total number of meta-terms in the compressed index at iteration 30 from 6.5M to 1.8M. At the same time, the compression ratio is reduced by only 6%.
Lossless compression of inverted indices has been an active topic for the past few years. Most of the developed techniques (e.g., variable-byte encoding, gamma-coding and delta-coding [16, 18]) aim at generating an efficient encoding of the entries in a posting list, and thus can be integrated with our approach (cf. Section 6). We also envision integrating our approach with lossy compression techniques, such as static pruning [6].

Several matrix factorization approaches have been proposed such as Non-negative Matrix Factorization [10, 13, 14], Principal Component Analysis [11], Latent Semantic Analysis [7], and Sin-gular Value Decomposition [19]. Their goal is to factor a given matrix into two (or three) factor matrices that (optionally) exhibit some level of sparseness. Such techniques provide a close approx-imation of the input matrix, while our approach provides an exact factorization of the input matrix. Modifying such algorithms to be lossless is not straightforward. For example, one na X ve approach is to compute the remainder matrix R = V  X  W H so that V can be compactly represented using W, H , and R (i.e., V = W H + R ). Unfortunately, there is no guarantee that sparseness of W and H would lead to sparseness of R . In fact, the size of R can be larger than the size of V because elements in V with values equal to zero may have non-zero values in the product W H . Another related work in the context of signal and image processing considers the problem of representing a signal (i.e., a vector) using a linear com-bination of a small number of basis vectors from a dictionary (e.g., [2, 20]). The problem of selecting the optimal dictionary is simi-lar to the problem we consider, with two main differences: (1) the dimensions of the factor matrices are selected in advance, and (2) the sparseness is required only for the encoding vectors (matrix W ) and not for the basis vectors (matrix H ).
We presented a novel approach for lossless compression of in-verted indices based on exact matrix factorization. We proved that obtaining the optimal factorization is NP-hard, and developed an efficient greedy factorization algorithm. We described how to mod-ify a typical top-k search algorithm to eliminate the query time overhead. Our experiments show that our technique achieves com-pression ratio of 35% while incurring negligible increase in the query evaluation time. Variable-byte encoding can be integrated with our approach to achieve overall compression ratios up to 50%.
