 The effectiveness and scalability of MapReduce-based im-plementations of complex data-intensive tasks depend on an even redistribution of data between map and reduce tasks. In the presence of skewed data, sophisticated redistribution approaches thus become necessary to achieve load balanc-ing among all reduce tasks to be executed in parallel. For the complex problem of entity resolution with blocking, we propose BlockSplit , a load balancing approach that supports blocking techniques to reduce the search space of entity res-olution. The evaluation on a real cloud infrastructure shows the value and effectiveness of the proposed approach. H.2.4 [ Systems ]: Distributed Databases; H.3.4 [ Systems and Software ]: Distributed Systems Algorithms, Performance MapReduce, Entity Resolution, Load Balancing, Data Skew
Cloud computing has become a popular paradigm for effi-ciently processing computational-and data-intensive tasks. Such tasks can be executed on demand on powerful distributed hardware and service infrastructures. The Map-Reduce (MR) model [2] supports the transparent parallel ex-ecution of complex tasks on cloud infrastructures. However, the (cost-) effectiveness and scalability of MR programs de-pend on effective load balancing approaches to evenly utilize available nodes. This is particularly challenging for data-intensive tasks where skewed data redistribution may cause node-specific bottlenecks and load imbalances.

We propose and evaluate an effective load balancing ap-proach to data skew handling for MR-based entity resolu-tion (ER). Note that MR X  X  inherent vulnerability to load imbalances due to data skew is relevant for all kind of pair-wise similarity computation, e.g., document similarity com-putation [ 4] and set-similarity joins [ 9]. Such applications can therefore also benefit from our load balancing approach though we study MR-based load balancing in the context of ER only. ER is the task of identifying entities referring to the same real-world object [ 7]. It is a pervasive problem and of critical importance for data quality and data integration, e.g., to match product offers for price comparison portals. ER techniques usually compare pairs of entities by evaluat-ing multiple similarity measures. Na  X   X ve approaches examine the Cartesian product of n input entities. However, the re-sulting complexity of O ( n 2 ) is inefficient for large datasets. The common approach to improve efficiency is to adopt so-called blocking techniques [ 1]. They utilize a blocking key based on the values of one or several entity attributes to partition the input data into multiple partitions (blocks) and restrict the subsequent matching to entities of the same block. For example, it is sufficient to compare entities of the same manufacturer when matching product offers.

Even with blocking, ER remains a costly process that can take up to days for large datasets [ 8]. The MR model is well suited to execute blocking-based ER in parallel. Sev-eral map tasks can read the input entities in parallel and redistribute them among several reduce tasks based on the blocking key. This guarantees that all entities of the same block are assigned to the same reduce task so that different blocks can be matched in parallel by multiple reduce tasks. However, such a basic MR implementation is susceptible to severe load imbalances due to skewed block sizes since the match work of an entire block is assigned to a single reduce task. As a consequence, large blocks prevent the utilization of more than a few nodes. The absence of skew handling mechanisms can therefore tremendously deteriorate runtime efficiency and scalability of MR programs. Furthermore, idle nodes may produce unnecessary costs because public cloud infrastructures usually charge per utilized machine hours.
We therefore propose BlockSplit , a general load balancing approach that addresses the mentioned skew problems. It takes the size of blocks into account and assigns entire blocks to reduce tasks if this does not violate load balancing con-straints. Larger blocks are split into smaller chunks based on the input partitions to enable their parallel matching within multiple reduce tasks. The evaluation uses real-world data and demonstrates the importance of skew handling for MR-based ER and the efficiency of BlockSplit . Figure 1: Schematic overview of the MR-based matching process with load balancing.
MapReduce (MR) is a programming model designed for parallel data-intensive computing in cluster environments with up to thousands of nodes [ 2]. A computation is ex-pressed with two user defined functions, map and reduce , that are executed in parallel on disjoint partitions of the in-put data. The map function transforms input entities to key-value pairs whereas reduce is called for each key that occurs as map output. Within the reduce function, one can access the list of all corresponding values. A MR cluster consists of a set of nodes that run a fixed number of map and reduce processes. For each MR job execution, the number of map tasks ( m ) and reduce tasks ( r ) is specified. Each process can execute only one task at a time. After a task has fin-ished, another task is automatically assigned to the released process using a framework-specific scheduling mechanism.
Besides map and reduce , a MR dataflow relies on three fur-ther functions. First, the function part partitions the map output and thereby distributes it to the available reduce tasks. All keys are sorted with the help of a comparison function comp . Finally, each reduce task employs a group-ing function group to determine the data chunks for each reduce function call. Each of these functions operates only on (parts of) the key of key-value pairs and does not take the values into account. The use of extended (composite) keys and an appropriate choice of part , comp ,and group supports sophisticated partitioning and grouping behavior and will be utilized in our load balancing approach.

As discussed in the introduction, parallel ER using block-ing can be easily implemented with MR. The map function can be used to determine for every input entity its block-ing key and to output a key-value pair ( blocking key , entity ). The default hash partitioning strategy would use the block-ing key to distribute key-value pairs among reduce tasks so that all entities sharing the same blocking key are as-signed to the same reduce task. Finally, the reduce function is called for each block and computes the matching entity pairs within its block. We call this straightforward approach Basic . However, the Basic strategy is vulnerable to data skew due to blocks of varying size. Therefore, the execution time may be dominated by a single or a few reduce tasks. We describe our load balancing approach BlockSplit for Figure 2: Example dataflow for computation of the block distribution matrix (MR Job1 of Figure 1).
 ER for one data source R . The input is a set of entities and the output are the entity pairs that are considered to be the same. We perform ER processing within two MR jobs as illustrated in Figure 1. Both jobs are based on the same number of map tasks and the same partitioning of the input data. The first job calculates a so-called block distri-bution matrix (BDM) that specifies the number of entities per block separated by input partitions. The matrix is used by the map phase of the second MR job to tailor entity re-distribution for parallel matching of blocks of different size.
BlockSplit generates one or several so-called match tasks per block and distributes match tasks among reduce tasks. Furthermore, it uses the following two ideas:  X 
Small blocks are processed within a single match task sim-ilar to Basic . Large blocks are split according to the m input partitions into m sub-blocks. All entities within the same sub-block are compared with each other. Further-more, pairs of sub-blocks are processed to evaluate their
Cartesian product. This ensures that all comparisons of the original block will be computed.  X 
BlockSplit determines the number of comparisons per match task and assigns match tasks in descending size among reduce tasks. This implements a greedy load bal-ancing heuristic ensuring that the largest match tasks are processed first and makes it unlikely that two large match tasks are processed by the same reduce task.
 In the following, we describe the computation of the BDM and our BlockSplit strategy. Further information including pseudo-code can be found in [ 5].
The block distribution matrix (BDM) is a b  X  m matrix that specifies the number of entities of b blocks across m put partitions. The BDM computation using MR is straight-forward as illustrated by Figure 2 for an example dataset. The 14 entities ( A -O ) are divided into two input partitions  X  0 and  X  1 and each entity has a blocking key ( w -z )that is denoted as index. For example, the map output key of Figure 3: Example dataflow for the load balancing strategy BlockSplit (MR Job2 of Figure 1).
 M is z. 1because M  X  X  blocking key equals z and M appears in the second partition (partition index=1). This key is as-signed to the last reduce task that outputs [ z, 1 , 3] because there are 3 entities in the second partition for blocking key z .Thecombined reduce outputs correspond to the BDM. To assign block keys to rows of the BDM, we use the order of the blocks from the reduce output, i.e., we assign the first block (key w ) to block index position 0, etc.

The block sizes in the example vary between 2 and 5 enti-ties. The match work to compare all entities per block with each other thus ranges from 1 to 10 comparisons; the largest block with key z entails 50% (10 of 20) of all comparisons although it contains only 35% (5 out 14) of all entities.
As illustrated in Figure 1, map produces an additional output  X  i per input partition that contains the original en-tities annotated with their blocking keys. This output is not shown in Figure 2 to save space but used as input in the second MR job (see Figure 3).
The realization of BlockSplit makes use of the BDM as well as of composite map output keys. Map generates a carefully constructed composite key that (together with associated partition and group functions) allows a balanced load dis-tribution. The composite key thereby combines information about the target reduce task(s), the block of the entity, and the entity itself. The map function may generate multiple keys per entity if this entity is supposed to be processed by multiple reduce tasks. The reduce phase performs the ac-tual ER and computes match similarities between entities of the same block. Since the reduce phase consumes the vast majority of the overall runtime (  X  95% in our experiments), our load balancing strategy solely focusses on data redis-tribution for reduce tasks. Other MR-specific performance factors as data locality are therefore not considered.
Map outputs key-value pairs with key=( reduce index block index split )andvalue=( entity ). The reduce task index has a value between 0 and r  X  1 and is used by part to realize the desired assignment to reduce tasks. Grouping is done on the entire key and  X  since the block index is part of the key  X  ensures that each reduce function only receives entities of the same block. The split value indicates what match task has to be performed by the reduce function, i.e., whether a complete block or sub-blocks need to be processed.
During the initialization, each of the m map tasks reads the BDM and computes the number of comparison per block and the total number of comparisons P over all b blocks  X  P checks if the number of comparisons is above the average reduce task workload, i.e., if If the block  X  k is not above the average workload it can be processed within a single match task (denoted as k.  X  in the block index and split components of the map output key). Otherwise, it is split into m sub-blocks based on the m input partitions leading to the following 1 2  X  m  X  ( m  X  1) + m tasks:  X  m match tasks, denoted with key components k.i ,forthe individual processing of the i th sub-block for i  X  [0 ,m  X  k.i  X  j with i, j  X  [0 ,m  X  1] and i&lt;j , for the computation of the Cartesian product of sub-blocks i and j Note that the BDM holds the number of entities per (block, partition) pair and map can therefore determine which input partitions contain no entities of  X  k , resulting in less match tasks. However, in favor of readability we assume that all m input partitions contain at least one entity. To determine the reduce task for each match task, all match tasks are first sorted in descending order of their number of comparisons. Match tasks are then assigned to reduce tasks in this order so that the current match task is assigned to the reduce task with the lowest number of already assigned pairs. In the following, we denote the reduce task index for match task k.x with R(k.x) .

After the described initialization phase, the map function is called for each input entity. If the entity belongs to a block  X  k that has not to be split, map outputs one key-value pair with composite key= R(k.*).k.* . Otherwise, map outputs m key-value pairs for the entity. The key R(k.i).k.i represents the individual sub-block i of block  X  k and the remaining m  X  1keys R ( k.i ) .k.i  X  j (for j  X  [0 ,m  X  1] and j = i represent all combinations with the other m  X  1 sub-blocks. This indicates that entities of split blocks are replicated times to support load balancing. The map function emits the entity as value of the key-value pair; for split blocks we annotate entities with the partition index for use in the reduce phase.

In our running example, only block  X  3 (blocking key z ) is subject to splitting into m =2 sub-blocks. The BDM (see Figure 2) indicates for block  X  3 that  X  0 and  X  1 contain two and three entities, respectively. The resulting sub-blocks  X  3 . 0 and  X  3 . 1 lead to the three match tasks 3.0 , 3.0  X  1 3.1 that account for 1, 6, and 3 comparisons, respectively. The resulting ordering of match tasks by size ( 0.* , 3.0  X  1 2.* , 3.1 , 1.* ,and 3.0 ) leads for three reduce tasks to the distribution shown in Figure 3. The replication of the five entities for the split block leads to 19 key-value pairs for the 14 input entities. Each reduce task has to process between six and seven comparisons indicating a good load balancing for the example. Figure 4: Execution times for different data skews using 114,000 entities.
In the following we evaluate our BlockSplit strategy re-garding the degree of data skew and the number of available nodes ( n ). We ran our experiments with real-world datasets on the Amazon EC2 cloud infrastructure using Hadoop.
We first evaluate the robustness of our load balancing strategies against data skew for a dataset containing about 114,000 product descriptions. To this end, we control the degree of data skew by generating block distributions that follow an exponential distribution. Given a fixed number of blocks b =100, the number of entities in the k t h block is pro-portional to e  X  s  X  k .Theskewfactor s  X  0 thereby describes the degree of data skew. Note that the data skew, i.e., the distribution of entities over all blocks, determines the overall number of entity pairs. We are therefore interested in the average execution time per entity pair when comparing load balancing strategies for different data skews.

Figure 4 shows the average execution time per 10 4 pairs for different data skews ( n =10, m =20, r =100). The Basic strat-egy explained in Section 2 is not robust against data skew because a higher data skew increases the number of pairs of the largest block. For example, for s =1 Basic needs 225ms per 10 4 comparisons which is more than 12 times slower than BlockSplit . However, the Basic strategy is slightly faster for a uniform block distribution ( s =0) because it does not suf-fer from the additional BDM computation and load balanc-ing overhead. The BDM influence becomes insignificant for higher data skews because the data skew does not affect the time for BDM computation but the number of pairs. This is why the execution time per pair is reduced for increasing s . In general, BlockSplit is stable across all data skews.
To analyze the scalability of BlockSplit , we use a larger dataset of approx. 1.4 million publication records. The blocking is formed by the first three letters of the publi-cation title. We vary the number of nodes from 1 up to 100. For n nodes, the number of map tasks is set to m =2  X  n and the number of reduce tasks is set to r =10  X  n , i.e., adding new nodes leads to additional map and reduce tasks.
The resulting execution times and speedup values are shown in Figure 5. In contrast to Basic , BlockSplit shows its abil-ity to evenly distribute the workload across available nodes. It scales almost linearly up to 40 nodes. For larger n the speedup increase slightly alleviates due to the decreasing workload (number of entity comparisons) per reduce task. Atthesametime,therelativefractionoftheMRoverhead for task initialization and task shutdown is increasing. Figure 5: Execution times and speedup of the Block-Split strategy.
Load balancing and skew handling are well-known data management problems and MR has been criticized for hav-ing overlooked the skew issue [ 3]. MR X  X  inherent vulnerabil-ity to data skew is relevant for all kind of pairwise similarity computation, e.g., pairwise document similarity [ 4]orset-similarity joins [9]. MR has already been employed for ER (e.g., [10 ]) but we are only aware of one load balancing mech-anism. [6] studies load balancing for Sorted Neighborhood that is by design less vulnerable to skewed data.
We proposed a load balancing approach, BlockSplit ,for parallelizing blocking-based entity resolution using the widely available MapReduce framework. The approach is capable to deal with skewed blocking key distributions and effectively distributes the workload among all reduce tasks by splitting large blocks. Our evaluation in a real cloud en-vironment demonstrated that it is robust against data skew and scales with the number of available nodes.

In future work we will extend our approach to multi-pass blocking that assigns multiple blocks per entity. We will further investigate how our load balancing approach can be adapted for MapReduce-based implementations of other data-intensive tasks, such as join processing or data mining.
