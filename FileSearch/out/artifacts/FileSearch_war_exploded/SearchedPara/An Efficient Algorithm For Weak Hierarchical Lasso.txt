 Linear regression is a widely used tool in data mining and machine learning. In many applications, fitting a regression model with only linear effects may not be sufficient for pre-dictive or explanatory purposes. One strategy which has recently received increasing attention in statistics is to in-clude feature interactions to capture the nonlinearity in the regression model. Such model has been applied successfully in many biomedical applications. One major challenge in the use of such model is that the data dimensionality is significantly higher than the original data, resulting in the small sample size large dimension problem. Recently, weak hierarchical Lasso, a sparse interaction regression model, is proposed that produces sparse and hierarchical structured estimator by exploiting the Lasso penalty and a set of hi-erarchical constraints. However, the hierarchical constraints make it a non-convex problem and the existing method finds the solution of its convex relaxation, which needs additional conditions to guarantee the hierarchical structure. In this paper, we propose to directly solve the non-convex weak hierarchical Lasso by making use of the GIST (General Iter-ative Shrinkage and Thresholding) optimization framework which has been shown to be efficient for solving non-convex sparse formulations. The key step in GIST is to compute a sequence of proximal operators. One of our key technical contributions is to show that the proximal operator associ-ated with the non-convex weak hierarchical Lasso admits a closed form solution. However, a naive approach for solv-ing each subproblem of the proximal operator leads to a quadratic time complexity, which is not desirable for large-size problems. To this end, we further develop an efficient al-gorithm for computing the subproblems with a linearithmic time complexity. We have conducted extensive experiments on both synthetic and real data sets. Results show that our proposed algorithm is much more efficient and effective than its convex relaxation.
 H.2.8 [ Database Management ]: Database Applications, Data Mining Algorithms Sparse learning; non-convex; weak hierarchical Lasso; prox-imal operator
Consider a linear regression model with the outcome vari-able y and d predictors x 1 , . . . , x d : where w 0 is the bias term, w i , i = 1 . . . , d is the coefficient and  X  N (0 ,  X  2 ) is the noise term. In many applications, a simple linear regression model is not sufficient for predictive or explanatory purposes. One strategy which has recently received increasing attention in statistics is to include inter-action terms into the model to capture the nonlinearity of the data [17, 22]. For example, the linear model including terms of order-2 and lower has the following form: where the cross-product term x i x j , i 6 = j refers to as the in-teraction variable (one may view x 2 i as a special interaction variable), and w 0 i s and Q  X  R d  X  d are the main effect and in-teraction effect coefficients respectively. Applications with interaction regression models are omnipresent. For exam-ple, in psychological study, the effectiveness of using 3-way interactions was demonstrated in testing psychological hy-pothesis [9]; there are strong evidences found in [4] that genetic-environment interactions have significant effects on conduct disorders; the research in [11] found a couple of evidences of gene-environment interactions in predicting de-pression status; in [26], the interaction between continuance commitment and affective commitment was found signifi-cant in predicting job withdraw intentions and absenteeism; [13] discovered that brain-derived neurotrophic factor inter-acts with early life stress in predicting cognitive features of depression and anxiety.
However, the use of higher order terms leads to data of high dimensionality. For instance, for regression model (1), if one wants to add all terms of order-k and lower, then there will be a total of O ( d k ) variables, which is computationally demanding for parameter estimation even when k and d are fairly small. Thus, an efficient approach that is able to deal with huge dimensionality is desired in such cases, and the sparse learning methodology is one promising approach for tackling such problem [27, 18, 7, 5, 32]. In this paper, we focus on the model (2) with pairwise interactions, i.e., two-factor interactions. Note that the analysis can be extended to the model with higher-order interactions.

In general, not all of the main effects and interactions are of interest, thus it is critical to select the variables of great significance. One simple approach for high dimensional in-teraction regression is to directly apply the Lasso [27], also known as the  X  X ll-pairs Lasso X  [2] in the case of two-factor interactions. However, the all-pairs Lasso estimator does not account for any structural information which has been shown to be important for prediction and interpretation of the high dimensional interaction regression model [2, 30, 25, 29, 6]. In statistics, a hierarchical structure between main effects and interaction effects has been shown to be very ef-fective in constraining the search space and identifying im-portant individual features and interactions [2, 30, 25, 29, 6]. Specifically, the hierarchical constraint requires that an interaction term x i x j is selected in the model only if the main effects x i and/or x j are included. Strong theoretical properties have been established for such hierarchical model [29, 30]. The hierarchical structure is supported by the ar-gument that large main effects may result in interaction of more importance, and it is desired in a wide range of appli-cations in engineering and underlying science. Traditional approaches to fit such a model typically follow the following two-step procedures [22]: (i) Fit a linear regression model that only includes the (ii) Fit the reformulated model with the identified indi-Since even a small d may lead to a huge amount of interac-tion variables, the two-step procedure is still time-consuming in many applications. Recently, there have been growing re-search efforts on imposing the hierarchical structure on main effects and interactions in the regression model with novel sparse learning methods. In [2], in order to enable feature se-lection and impose heredity structures, the authors proposed strong hierarchical Lasso which adds a set of constraints to the Lasso formulation to achieve the strong hierarchy where the interaction effects are non-zero only if the corresponding main effects are non-zero. In [25], a Lasso-type penalized least square formulation called VANISH was proposed to achieve the strong hierarchy between the interaction effects and main effects. In [29], a type of non-negative garrote method was proposed to achieve the heredity structures. In [30], the Composite Absolute Penalties were proposed to achieve heredity structures for interaction models. In con-trast to the above works which fulfill the hierarchical struc-ture via solving convex problems, Choi et al. in [6] formu-lated a non-convex problem to achieve the strong hierarchy by assuming that the coefficient of an interaction term is a product of a scalar and main effect coefficients. Different from the strong hierarchy, the weak hierarchy between the main effects and the interaction effects requires that an in-teraction is included in the model only if at least one of the main effects is included in the model. In mathematical form, Q i,j 6 = 0 only if w i 6 = 0 OR w j 6 = 0. The weak hierarchy can be considered as a structure in between the strong hierarchy and no hierarchical structure [2, 29, 30]. Specifically, weak hierarchy allows those interactions with only one significant  X  X arent X  (main effect) to be included in the model. Several existing empirical studies have demonstrated the stronger predictive power of weak hierarchical model [19]. In our study, we mainly focus on the interaction regression model with weak hierarchical structure.

We follow the weak hierarchical Lasso approach recently proposed by [2] to fit the pairwise interaction regression model with the weak hierarchy. By imposing restrictions of the weak hierarchy and taking advantage of the Lasso penalty [27] that leads to sparse coefficients, the weak hier-archical Lasso is able to simultaneously attain a hierarchical solution and identify important main effects and interac-tions. However, the set of constraints restricting hierarchi-cal constraints make the problem non-convex; the algorithm proposed in [2] aims to solve a convex relaxation. The con-vex relaxation, however, requires additional conditions to guarantee the weak hierarchy, which is not desirable.
In this paper, we propose to directly solve the weak hi-erarchical Lasso using the GIST (General Iterative Shrink-age and Thresholding) optimization framework recently pro-posed by [15]. The GIST framework has been shown to be highly efficient for solving large-scale non-convex problems. The most critical step in GIST is to compute a sequence of proximal operators [23]. In this paper, we first show that the proximal operator related to weak hierarchical Lasso admits an analytical form solution by factorizing unknown coefficients into sign matrices and non-negative coefficients. However, a naive method of computing the subproblem of the proximal operator leads to a quadratic time complex-ity, which is not desirable for large-size problems. To this end, we further develop an efficient algorithm for solving the subproblems, which achieves a linearithmic time complexity. We evaluate the efficiency and effectiveness of the proposed algorithm and compare it with the convex relaxation in [2] and other state-of-the-art methods using synthetic and real data sets. Our empirical study demonstrates the high effi-ciency of our algorithm and the superior predictive perfor-mance of weak hierarchical Lasso over the competing meth-ods.

The remaining of the paper is organized as follows: we give a brief review of the weak hierarchical Lasso and its convex relaxation in Section 2. In Section 3, we derive the closed form solution to the proximal operator of the original weak hierarchical Lasso by decomposing the unknown coefficients into signs and the non-negative coefficients. Then, we show how the associated proximal operator can be computed ef-ficiently. We report the experimental results in Section 4. We conclude this paper in Section 5. In this section, we briefly review the weak hierarchical Lasso and its corresponding convex relaxed formulation [2]. Suppose we are given n pairs of data points { ( x i , y i R d  X  R . Let Y  X  R n  X  1 be the vector of outcome and X  X  R n  X  d be the design matrix. Let Z  X  R n  X  ( d  X  d ) be the matrix of interactions where Z wise product). Thus, Z ( i ) captures the pairwise interactions between the i -th feature and all d features. Note that, we include the quadratic terms x 2 i in the interaction model for clearer presentation, however our analysis is still applicable if they are not included in the model. By assuming that Y is centered and X, Z are column-wise normalized to zero mean and unit standard deviation, we can set the bias term w 0 = 0. Thus, in matrix form, the pairwise interaction re-gression model can be expressed as where  X  N ( 0 ,  X  2 I ) and  X  X ec X  is the vectorization operator that transforms a matrix to a column vector by stacking the columns of the matrix. Thus, the least square loss function of (3) is given by: Then, the weak hierarchical Lasso formulation takes the form of [2]: where k Q k 1 = P i,j | Q i,j | and  X  is the Lasso penalty param-eter.

Note that the constraints in (5) guarantee the weak hi-erarchical structure since the coefficient Q i,j of interaction x x j is non-zero only if at least one of its main effects is included in the model, i.e., w i 6 = 0 or w j 6 = 0. However, the imposed hierarchical constraints make problem (5) non-convex. Instead of solving (5), Bien et al. in [2] proposed to solve the following relaxed version: where 1 represents a column vector of all ones. In view of (6), we can see that k w k 1 is relaxed to w + + w  X  . Problem (6) is convex and can be solved by many efficient solvers such as FISTA [1]. However, Bien et al. in [2] showed that problem (6) needs an additional ridge penalty to guarantee the weak hierarchical structure of the estimator. In this pa-per, we propose an efficient algorithm which directly solves the non-convex weak hierarchical Lasso formulation in (5).
In this section, we propose an efficient algorithm named  X  X WHL X , which stands for X  e fficient W eak H ierarchical L asso X , to directly solve the weak hierarchical Lasso. eWHL makes use of the optimization framework of GIST (General Itera-tive Shrinkage and Thresholding) due to its high efficiency and effectiveness for solving non-convex sparse formulations. One of the critical steps in GIST is to compute the proxi-mal operator associated with the penalty functions. As one of our major contributions, we first factorize the unknown coefficients into the product of their signs and magnitudes; and then show that the proximal operator of (5) admits a closed form solution in Section 3.1. Another major contribu-tion is that we present an efficient algorithm for computing the proximal operator associated with the non-convex weak hierarchical Lasso in Section 3.2. The time complexity of solving each subproblem of the proximal operator can be reduced from quadratic to linearithmic. We then summa-rize our algorithm for computing the proximal operator in Section 3.2.
In this section, we show how to derive the closed form so-lution of the proximal operator associated with (5) in detail. Let P = n ( a, B ) , a  X  R d , B  X  R d  X  d k B  X  ,j k 1  X | a j and the indicator function be defined by Given a sequence n w ( k ) , Q ( k ) o , the proximal operator as-sociated with weak hierarchical Lasso is: = arg min + t where t ( k ) &gt; 0.

Simple algebraic manipulation leads to where Thus, problem (5) can be solved by iteratively solving the proximal operator in (9). Because R ( w, Q ) is an indicator function, we can rewrite the proximal operator (9) as arg min We omit the superscripts for notational simplicity.
The vector of main effect coefficients can be written as where  X  w j = | w j | , j = 1 , . . . , d and S 0  X  R d  X  d matrix whose j -th diagonal element is the sign of w j , i.e. , S j,j = sign( w j ). We define and we assume in this paper that the sign operator is applied on vectors or matrices elementwise. Similarly, we factorize each column of the interaction coefficient matrix as Q  X  ,j S e
Q  X  ,j , j = 1 . . . , d , where e Q i,j = | Q i,j | and S the diagonal sign matrix. Then, the proximal operator (10) is equivalent to arg min where e Q ,  X  w and S j , j = 0 , . . . , d are the unknown vari-ables, is defined as the element-wise X  X reater than or equal to X  comparison operator, i.e., for V, U  X  R d  X  1 , V U  X  V i  X  U i , i = 1 . . . , d . Therefore, the solutions of the origi-nal weak hierarchical Lasso can be obtained by iteratively solving (12). Note that the amounts of l 1 penalties on w and Q can be different. Here we use the same penalty pa-rameter  X  for notational simplicity and consistency with the original formulation of weak hierarchical Lasso (5) studied in [2]. Though the factorization introduces more variables and constraints, we show that the resulting proximal oper-ator admits a closed form solution. More importantly, we show that each sub-problem of the proximal operator can be solved by the proposed eWHL algorithm in linearithmic time. Indeed, the factorization of w and Q into their signs and magnitudes is the first key to directly solve the original weak hierarchical Lasso.

It is clear that the proximal operator in (12) can be de-coupled into d subproblems:
Next, we show that (13) has a closed form solution. Since and  X  w j  X  0, S 0 j,j must have the same sign as v j , that is, w j has the same sign as v j . Otherwise, the value of 2  X  w j  X  S one can show that S j i,i , i.e., the sign of Q i,j , must be the same as the sign of U i,j . Thus, the diagonal elements diag( S show how to compute  X  w and e Q .
 (13) is equivalent to arg min
After rearrangement, problem (14) can be expressed as:
We solve (15) by deriving its dual problem. Let  X   X  0 be the Lagrangian multiplier dual variable of the first inequality constraint. Define the Lagrangian function of (15) as: l (  X ,  X  w, e Q ) = 1 where we omit the subscripts for simplicity with a little abuse of notation. Since the constraint 1 T e Q  X   X  w is affine, the strong duality holds for the minimization problem (15). Thus, the dual problem of (15) is: max By rearranging the terms, (16) is equivalent to: min where h (  X  ) =  X   X  v X   X  1 2  X  2 +  X  1 T q U  X  1 2  X  2 1 T
For fixed  X  , in order to obtain the minimum of the objec-tive function in (17), we conclude that due to the constraints  X  w  X  0 and e Q 0 . Therefore, if we obtain a dual optimal solution  X   X  that maximizes the dual problem (17), then we can readily compute the closed form solution to (13) and thus to (12). That is, w  X  = S 0  X  w  X  , Q  X   X  ,j = S j e Q  X   X  ,j where diag( S 0 ) = sign( v the optimal dual solution  X   X  .
Next, we show how to efficiently compute the dual opti-ascending order. Without loss of generality, we assume:
There are four possible cases about the locations of  X  . We discuss how to identify the optimal dual solution  X   X  in each of the four cases.
 Case 1 : in (17) at  X   X  becomes Function (20) is a quadratic function with respect to  X  and the unconstrained maximum is achieved at the axis of sym-h q U to achieve the maximum objective value of (17). It can be further concluded that, in Case 1 , among all the intervals on the left of  X   X  v , the maximum objective value of (17) is achieved at the q U G .
 Case 2: the objective value in (17) at  X  is similar to (20): By a similar argument, we can set  X  =  X   X  v to achieve the maximum. Combining the results of Case 1 and Case 2 , we conclude that, we may only consider  X  in the range [max (  X   X  v, 0) , +  X  ]. Note that when L = d , that is q any value in the interval h q U d ,  X   X  v i .
 Case 3: When . . .  X  q U L  X   X   X  v  X   X   X  q U L +1  X  . . . , the value of the objective function in (17) at  X   X  becomes Again, (22) is a quadratic function of  X  and P d i = L +1 q otherwise the maximum is achieved at Case 4: value in (17) is similar to (22): If If If q
Since we know exactly the value of  X  for all the four cases, one naive way to find the optimal  X   X  is to enumerate all the possible locations and pick the one that maximizes the objective function value in (17). However, evaluating the objectives for all possible locations from max(  X   X  v, 0) to leads to a quadratic time algorithm for solving (17). Inter-estingly, we show below that the time complexity of solving (17) can be reduced to O ( d log( d )).
 Let us first list some useful properties as follows: Given the ordered sequence (19):
Properties 2-6 also apply for adjacent intervals h  X   X  v, and h q U L +1 , q U L +2 i in Case 3.

We omit the proof of Properties 1-6 since they are direct applications of 1-D quadratic optimization. Property 1 indi-cates that it is sufficient for the algorithm to start searching  X   X  from Case 3. Properties 2 &amp; 3 imply that, for some in-terval, if the axis of symmetry is on the right hand side of the interval, then one only needs to consider the intervals to the right. Similarly, Properties 4 &amp; 5 indicate that, for some interval, if the axis of symmetry is on the left hand side of the interval, then one only needs to consider the intervals to the left. Property 6 combined with Properties 1-5 imply that, for certain interval, if it contains the axis of symmetry, then  X   X  is the axis of symmetry point. Thus, we can draw the following conclusion: (1) if max q U d ,  X  v &lt; 0, then (2) if  X   X  v &gt; q U d , then h q U culated by a constant operation based on the value from the last step, and the time complexity of searching  X   X  reduces from quadratic to O ( d log( d )) as the computation is domi-nated by the sorting operation. Once  X   X  is determined, we can compute  X  w and e Q by (18). Note that, the subproblem of the proximal operator associated with the convex relax-ation in [2] is solved by searching for the dual variable in a different way with time complexity O ( d 2 ).

In summary, we reformulate the proximal operator for the original weak hierarchical Lasso by factorizing the unknown coefficients. The reformulated proximal operator is shown to admit a closed form solution, which enables directly solv-ing the weak hierarchical Lasso problem. Moreover, the subproblem of the proximal operator can be computed effi-ciently with a time complexity of O ( d log( d )). The detailed algorithm for solving the proximal operator (12) is described in Algorithm 1. We give the details of eWHL algorithm in Algorithm 1 Computation of the Proximal Operator of Weak Hierarchical Lasso Input: v  X  R d  X  1 , U  X  R d  X  d , t  X  R + ,  X   X  R + 1:  X  v = sign( v ) v  X   X  t 1 ; 2: for j = 1 : d do 3: c =  X   X  v j ; 4: Sort q U  X  ,j to get a sequence S in ascending order where 5: if S d &lt; 0 and c &lt; 0 then 6:  X  w j = 0 ; 7: else 8: if S d &lt; c then 9:  X  = max( c, 0); 10: else 11: k = d ; 12: while 1 do 13: c = c + S k ; 14: k = k  X  1; 15: if c/ ( d + 1  X  k )  X S k then 16:  X  = c ; 17: break; 18: end if 19: end while 20: end if 21:  X  w j = max( X  v j +  X , 0 ); 22: end if 23: end for 24: w = sign( v )  X  w ; Algorithm 2. Following [15], we choose the step size t ( k ) the Barzilai-Borwein (BB) Rule.
In this section, we evaluate the efficiency and effective-ness of the proposed algorithm on both synthetic and real data sets. In our first experiment, we compare the effi-ciency of our proposed algorithm and the convex relaxation of weak hierarchical Lasso [2] on synthetic data sets where the weak hierarchical structure holds between main effects and interaction effects. In our second experiment, we com-pare the classification performance of the weak hierarchical Lasso with other classifiers and sparse learning techniques on the data collected from Alzheimer X  X  Disease Neuroimaging Initiative (ADNI) 1 .
In this experiment, we compare the efficiency of the pro-posed eWHL algorithm with the convex relaxation on syn-thetic data sets. Our algorithm is built upon the GIST framework which is available online [16]. The source code of the convex relaxed weak hierarchical Lasso (cvxWHL) was available in the R package  X  hierNet  X  [3] where the optimiza-http://www.adni-info.org/ Algorithm 2 The Efficient Weak Hierarchical Lasso Algo-rithm (eWHL) 2: repeat 4: repeat 6: until line search criterion is satisfied 8: until stop criterion is satisfied tion procedure was implemented by C. Since the proposed algorithm in this paper directly solves the non-convex weak hierarchical Lasso (5), and the eventual goal of the convex relaxed weak hierarchical Lasso is also to find a good  X  X e-laxed X  solution to the original problem, we compare the two algorithms in terms of the objective function in (5). In the experiment, entries of X  X  R n  X  d are i.i.d generated from the standard normal distribution, i.e., X i,j  X  N (0 , 1). The matrix of interactions, Z , is then generated via the nor-malized X where Z = h Z (1) , Z (2) , . . . , Z ( d ) i , Z Z Q  X  R d  X  d are generated based on the weak hierarchical the ratio of coefficient sparsity, i.e., the portion of zero en-tries in w and Q , from 30% to 85%. Then, the outcome vector Y is constructed as Y = Xw + 1 2 Z  X  vec( Q ) + where X and Z are normalized to zero mean and unit standard deviation and  X  N ( 0 , 0 . 01  X  I ). We use sample size n = 100 and 200 and we choose the number of main effects d from { 100 , 200 , 300 , 400 , 500 , 600 } . The parameter of the l penalty,  X  , is chosen from { 1 , 3 , 5 , 10 , 20 } . All algorithms are executed on a 64-bit machine with Intel(R) Core(TM) quad-core processor (i7-3770 CPU @ 3.40 GHz) and 16.0 GB memory. We terminate the algorithm when the maximum relative difference of the coefficients between two consecu-tive iterations is less than 1 e  X  5 . We run 20 trials for each setting and report the average execution time. The detailed results are shown in Table 1.

From Table 1, we observe that eWHL is significantly faster than cvxWHL. Our algorithm is up to 25 times faster than the competing algorithm. As the dimension increases, the running time of cvxWHL increases much faster than our pro-posed algorithm. Specifically, when the number of individ-ual features increases to 400 (corresponds to 80200 interac-tions), cvxWHL may take more than one thousand seconds, while the proposed eWHL is reasonably fast even when the number of total variables is around two hundred thousands.
To make further comparisons of efficiency, we randomly generate three synthetic data sets where the weak hierarchi-cal structure between main effects and interactions holds. The three data sets are of the same sample size n = 100 and Figure 1: Comparison of the running time and the number of iterations by the two algorithms. Three synthetic data sets are generated where the portions of zeros in the ground truth are 85%, 60%, 30% re-spectively. The plots in the same row correspond to the same data set. The plots in the left column present the running time and those in the right col-umn show the number of iterations. the number of individual features is d = 300. The ratios of zero entries in the ground truth are 85%, 60% and 30% respectively. The regularization parameters are chosen from run cvxWHL, and then the objective value of (5) in the fi-nal step is recorded. Then, we run the proposed eWHL and terminate the algorithm when the objective value of (5) is less than the one obtained by cvxWHL. The running time and the number of iterations needed to achieve the same objective value of both algorithms are reported in Figure 1. We can observe from Figure 1 that the proposed eWHL is much faster than cvxWHL.

Moreover, we also conduct an experiment to compare the recovery performance of eWHL and cvxWHL. We gener-ate synthetic data sets with sample size n = 100 and the number of individual features is d = 50 (1225 cross inter-actions). The number of non-zero main effects varies from { 3 , 4 , 5 , 6 , 7 } and the number of non-zero interaction effects thetic data sets are generated with noise  X  N ( 0 , 0 . 01  X  I ). We run both eWHL and cvxWHL with parameter selected via 5-fold cross-validation. Then we compute the sensitivity and specificity of recovery (where non-zero entries are posi-tive and zero entries are negative). The means of sensitivity and specificity are plotted in Figure 2. We can observe that both algorithms achieve high recovery rate while directly solving the original weak hierarchical Lasso leads to slightly better performance in recovering the non-zero effects.
In this experiment, we compare the weak hierarchical Lasso with its convex relaxation as well as other classifiers on the Alzheimer X  X  Disease Neuroimaging Initiative (ADNI) data set.

In Alzheimer X  X  Disease (AD) research, Mild Cognitive Im-pairment (MCI) is an intermediate state between normal el-derly people and AD patients [24]. The MCI patients are considered to be at high risk of progression to AD. Many recent work focus on how to accurately predict the MCI-AD conversion and identifying significant bio-markers for the prediction [8, 10, 12, 19, 21, 28, 31, 14].

In this experiment, we compare the classification perfor-mance of the proposed eWHL with the convex relaxation and other classifiers on the task of discriminating the MCI Figure 2: Comparison of eWHL and cvxWHL in terms of recovery on synthetic data sets. subjects who convert to dementia ( i.e., MCI converter) within a three-year period from the MCI subjects who remain at MCI ( i.e., MCI non-converter). The features used in the experiment (provided by our clinical collaborators) involve demographic information such as age, gender, years of ed-ucation, clinical information such as scores of mini men-tal state examination (MMSE), Auditory Verbal Learning Test (A.V.L.T.), and the bio-markers including status of Apolipoprotein E, volume of hippocampus, thickness of Mid Temporal Gray Matter. There are 133 samples in total and the number of individual features is 36 (corresponds to 630 two way interactions). The interactions are generated by the normalized individual features and are normalized before entering the model. Since this is a classification task with binary labels, we replace the least square loss with logistic loss in the weak hierarchical Lasso. Besides the non-convex and convex weak hierarchical Lasso, we apply random for-est (RF), Support Vector Machine (SVM) and sparse logistic regression on main effects, and on both main effects and in-teractions, respectively. We report the means and standard deviations of accuracy, sensitivity and specificity obtained from 10-fold cross-validation. The penalty parameters are tuned via 5-fold cross-validation in the training procedure. The sample statistics are shown in Table 3 and the classifi-cation performance is reported in Table 2.
 Table 3: The statistics of the ADNI data set used in our experiment. The MCI converters (MCI-cvt) are characterized as positive samples and the MCI non-converters (MCI non-cvt) are used as negative samples.

From Table 2, we can observe that, if we only use indi-vidual features for classification, then all the classifiers are biased towards the positive class, i.e., MCI converter. When interactions are included, we observe that the performances of random forest and SVM become worse. One possible rea-son is that the large number of variables brought by the interactions weakens their discriminative power. This is not the case for sparse logistic regression, which demonstrates the importance of feature selection. We can observe from the table that the convex relaxed weak hierarchical Lasso and the non-convex weak hierarchical Lasso achieve much better classification performance than the competitors. The improvement of the classification performance demonstrates the effectiveness of imposing hierarchical structures in inter-action models. In addition, the superior classification per-formance (around 77% accuracy, sensitivity and specificity) of the proposed eWHL demonstrates that directly solving the non-convex weak hierarchical Lasso leads to solutions of higher quality than the convex relaxation.
In this paper, we propose an efficient algorithm, eWHL, to directly solve the non-convex weak hierarchical Lasso. One critical step in eWHL is to compute the proximal operator associated with the non-convex penalty functions. As one of our major contributions, we show that the proximal op-erator associated with the regularization function in weak hierarchical Lasso admits a closed form solution. Further-more, we develop an efficient algorithm which computes each subproblem of the proximal operator with a time complexity of O ( d log d ). Extensive experiments on both synthetic and real data sets demonstrate the superior performance of the proposed algorithm in terms of efficiency and accuracy.
In the future, we plan to apply the non-convex weak hi-erarchical Lasso to other important and challenging appli-cations such as depression study [20]. In addition, we plan to extend the proposed techniques to solve the non-convex strong hierarchical Lasso formulation.
This work was supported in part by NIH (R01 LM010730) and NSF (IIS-0953662, MCB-1026710, and CCF-1025177). [1] A. Beck and M. Teboulle. A fast iterative [2] J. Bien, J. Taylor, and R. Tibshirani. A lasso for [3] J. Bien and R. Tibshirani. hierNet: A Lasso for [4] R. J. Cadoret, W. R. Yates, G. Woodworth, M. A. [5] E. J. Candes and J. Romberg. Quantitative robust [6] N. H. Choi, W. Li, and J. Zhu. Variable selection with [7] A. d X  X spremont, L. El Ghaoui, M. I. Jordan, and [8] C. Davatzikos, P. Bhatt, L. M. Shaw, K. N.
 [9] J. F. Dawson and A. W. Richter. Probing three-way [10] D. Devanand, G. Pradhaban, X. Liu, A. Khandji, [11] T. C. Eley, K. Sugden, A. Corsico, A. M. Gregory, [12] C. Fennema-Notestine, D. J. Hagler, L. K. McEvoy, [13] J. Gatt, C. Nemeroff, C. Dobson-Stone, R. Paul, [14] P. Gong, J. Ye, and C. Zhang. Multi-stage multi-task [15] P. Gong, C. Zhang, Z. Lu, J. Huang, and J. Ye. A [16] P. Gong, C. Zhang, Z. Lu, J. Huang, and J. Ye. GIST: [17] T. Hastie, R. Tibshirani, J. Friedman, T. Hastie, [18] K. Koh, S.-J. Kim, and S. Boyd. An interior-point [19] H. Li, Y. Liu, P. Gong, C. Zhang, J. Ye, A. D. N. [20] Y. Liu, Z. Nie, J. Zhou, M. Farnum, V. A. Narayan, [21] D. A. Llano, G. Laforet, and V. Devanarayan. [22] D. C. Montgomery, E. A. Peck, and G. G. Vining. [23] N. Parikh and S. Boyd. Proximal algorithms.
 [24] R. C. Petersen. Mild cognitive impairment clinical [25] P. Radchenko and G. M. James. Variable selection [26] M. J. Somers. Organizational commitment, turnover [27] R. Tibshirani. Regression shrinkage and selection via [28] J. Ye, M. Farnum, E. Yang, R. Verbeeck, V. Lobanov, [29] M. Yuan, V. R. Joseph, and H. Zou. Structured [30] P. Zhao, G. Rocha, and B. Yu. The composite [31] J. Zhou, J. Liu, V. A. Narayan, and J. Ye. Modeling [32] H. Zou, T. Hastie, and R. Tibshirani. Sparse principal
