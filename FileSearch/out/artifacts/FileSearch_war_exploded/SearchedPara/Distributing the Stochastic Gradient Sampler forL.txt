 Learning large-scale Latent Dirichlet Allocation (LDA) models is beneficial for many applications that involve large collections of documents.Recent work has been focusing on developing dis-tributed algorithms in the batch setting, while leaving stochastic methods behind, which can effectively explore statistical redun-dancy in big data and thereby are complementary to distributed computing.The distributed stochastic gradient Langevin dynam-ics (DSGLD) represents one attempt to combine stochastic sam-pling and distributed computing, but it suffers from drawbacks such as excessive communications and sensitivity to partitioning of datasets across nodes. DSGLD is typically limited to learn small models that have about 10 3 topics and 10 3 vocabulary size. In this paper, we present embarrassingly parallel SGLD (EPS-GLD), a novel distributed stochastic gradient sampling method for topic models. Our sampler is built upon a divide-and-conquer ar-chitecture which enables us to produce robust and asymptotically exact samples with less communication overhead than DSGLD. We further propose several techniques to reduce the overhead in I/O and memory usage. Experiments on Wikipedia and ClueWeb12 documents demonstrate that, EPSGLD can scale up to large mod-els with 10 10 parameters (i.e., 10 5 topics, 10 5 vocabulary size), four orders of magnitude larger than DSGLD, and converge faster.  X  Mathematics of computing  X  Probabilistic algorithms;  X  Computing methodologies  X  Distributed algorithms; large-scale LDA, stochastic gradient sampler, embarrassingly par-allel MCMC
Topic models are useful statistical tools for mining latent topic representations in document data, with Latent Dirichlet Allocation (LDA) [8] as one of the most popular examples. LDA has been  X  Corresponding author.
 used in various tasks, including text classification [27], informa-tion retrieval [21], recommendation [10] and social network anal-ysis [3, 5]. Since exact inference is intractable, various approx-imate methods have been developed, with Markov Chain Monte Carlo (MCMC) as a main workhorse. A MCMC method builds a sampler that simulates samples from a proposal distribution and ac-cepts each sample with a rate specified by its posterior probability given observed data. A conventional MCMC method adopts batch update, that is, the whole dataset is processed for a single iteration. Apparently, the overhead for such batch methods increases signifi-cantly as the model size and/or data size get larger.

There are two ways to scale up inference for topic models  X  stochastic subsampling and distributed computing . Stochastic sub-sampling reduces the per-iteration cost by randomly drawing a sub-set (or mini-batch) of documents and constructing an (unbiased) estimate of the data statistics (e.g., gradients). Such methods save computation cost by effectively exploring the statistical redundancy that is commonly observed in large-scale datasets. For topic mod-els, representative work includes stochastic variational inference (SVI) [9] and stochastic gradient MCMC, such as stochastic gra-dient Riemannian Langevin dynamics (SGRLD) [17], an extension of stochastic gradient Langevin dynamics (SGLD) [23] for LDA (See [28] for an overview). Compared to SVI, which often relies on some restricting mean-field assumption and has dense per-token update for LDA, we focus on SGLD methods, which converge to the target posterior distribution under certain scheme of annealing the step sizes and has sparse per-token update, a desirable property for learning a large number of topics.

Distributed methods parallelize the computation on the full dataset over multiple compute nodes. Representative work for topic models includes Yahoo!LDA [1], LightLDA [25] and many oth-ers [4, 13, 12, 16], which follow a common pattern that the glob-ally shared topic-word matrix is updated asynchronously over mul-tiple workers, and workers constantly communicate with a master node to keep the local copy of the topic-word matrix up-to-date through some operations, e.g., a parameter server in Yahoo!LDA or pull/push-ing a matrix block from/to the master node in LightLDA. In either case the write operation is exclusive for any element in the matrix, which means only one worker at a time can modify the ele-ment (See Fig. 1(b) for an illustration of LightLDA). Various efforts have been made on reducing the per-token sampling complexity for better efficiency on learning large-scale LDA models [24, 11], with the recent success on an O (1) sampler [25]. However, these meth-ods rely heavily on the communications among compute nodes and can have substantial latency in waiting to gain the lock of element, which can seriously slow down the convergence.

Though fast per-token samplers have been extensively investi-gated in a sophisticated distributed system, little work has been done on investigating stochastic methods within a distributed paradigm to jointly explore data redundancy and distributed com-puting. One exception is distributed SGLD (DSGLD) [2]. In DS-GLD, the dataset is split and stored locally on multiple compute nodes, and each individual SGLD sampler moves from one node to another after each iteration in order to visit the local corpus there. This behavior forms a unique trajectory (or chain) for the sampler across nodes. The distribution of computation is achieved by running multiple chains at the same time, and exchanging them among nodes after each iteration. Fig. 1(a) illustrates DSGLD, where the topic-word distribution (a core part for LDA) must be carried together with the model. There are two significant draw-backs in DSGLD. First, since the model needs to be carried to the next node after each iteration, the number of communications grows linearly to the number of chains and the number of itera-tions. For large-scale LDA, where the model size can be as large as 10 10 (e.g., K = 10 5 topics and V = 10 5 unique words), such overhead is prohibitive. Hence, DSGLD is often limited to small-scale applications (e.g., V = 10 3 and K = 10 3 ) [2]. Second, DSGLD makes use of a chain coupling technique to average over the samples drawn from individual samplers in order to aggregate into global samples. This strategy can generally reduce the esti-mation variance, however, since samples are drawn from different subset posteriors (or sub-posteriors), such a vanilla averaging oper-ation can lead to bad aggregation if subset posteriors differ signifi-cantly. In other words, DSGLD is sensitive to noise or non-random partitioning of the datasets, and this can eventually lead to a slow convergence.

To address the above challenges, we present embarrassingly par-allel SGLD (EPSGLD), a novel distributed version of SGLD sam-plers for topic models. EPSGLD has less communication over-head and at the same time overcomes all drawbacks in DSGLD to achieve better scalability. We build our algorithm based on the recent progress on divide-and-conquer (or embarrassingly parallel) sampling methods, which allow worker nodes to draw samples only from their sub-posteriors, and then approximate the full-data pos-terior (true posterior) samples in master node using some aggrega-tion methods, such as weighted average [15], kernel density esti-mators [19], and posterior median [14] (See [28] for an overview). One of the central feature for the embarrassingly parallel methods is that individual workers need no/few communications when sam-pling from sub-posteriors, which significantly reduces the network I/O and latency. This feature is excellent for models that contain too many parameters to be carried across compute nodes, and can help to increase the scalability of distributed methods. In our im-plementation, EPSGLD can scale up to V = 10 5 , K = 10 5 (or even larger), which is four orders of magnitude larger than DSGLD, while the communication cost grows at O ( I 1 3 ) rate, where I is the number of iterations. On the other hand, the aggregation methods produce asymptotically exact samples from the true posterior [15], and are, therefore, less sensitive to noise in datasets, leading to a significantly faster convergence than DSGLD.
 Technically, we address two challenging problems to develop EPSGLD for large-scale LDA. First, despite that the embarrass-ingly parallel methods can work without communication, the syn-chronization across local LDA models is still needed due to two reasons: (1) the global variables (i.e., topic-word distribution) in LDA depend on the assignments to all the local variables (i.e., topic assignments for all words in local datasets); and (2) in or-der to draw samples for local variables, samplers need to access the latest global variables that are usually generated using aggre-gation methods and stored in the master node [1]. This synchro-nization is important for distributed LDA models to converge to the optimal solution, and Newman et al. suggested to synchro-nize after each iteration of the local sampler [16]. For EPSGLD, this practice can again lead to O ( I ) communication cost, and is thus not suitable. To avoid frequent synchronizations, we investi-gate the properties of embarrassing parallel methods, and find that they can correct the sub-posteriors towards the true posterior as the training proceeds, and this phenomenon can be characterized as an Expectation-maximization (EM) process. In that case, EPSGLD can converge to the optimal solution with significantly fewer syn-chronizations. In the experiments, we conduct a series of carefully designed tests to demonstrate the power of this property, and show how the performance of EPSGLD can be influenced by the fre-quency of synchronizations.

Second, though various methods have been proposed to increase the scalability of the distributed batch samplers, such as fast sam-pling algorithms [24, 11, 25] and model-parallelism [26, 25], such work is still lacking for stochastic samplers. We fills up this re-search void and discuss several memory and I/O strategies that can improve the model scalability. For example, some embarrassingly parallel methods such as Weierstrass sampler [19] require to store all the global samples in each worker node in order to perform ag-gregation, which lead to excessive usage of disk and memory as the space complexity grows linearly. We propose to take advan-tage of synchronization operation and only hold the latest copy of the global samples in each worker node, resulting in an O (1) space complexity in disk. We also note that the number of unique words contained in each mini-batch is sufficiently smaller than the vocabulary size, and we propose the matrix slicing technique X  X or each iteration, we only load into memory those word-slices that will be used by the SGLD sampler, saving a great amount of memory space. Additionally, we also show how fast sampling algorithms can be integrated into SGLD samplers. All these techniques help EPSGLD to scale up to models with 10 10 parameters.

To summarize, we propose EPSGLD, a novel distributed stochastic gradient MCMC sampler, to learn large-scale LDA mod-els. Built upon an embarrassingly parallel method, EPSGLD can produce asymptotically exact samples with significantly fewer syn-chronizations than DSGLD. We also propose strategies to reduce communication overhead and memory usage to increase scalability. The experiments demonstrate that EPSGLD can scale up to LDA models with 10 10 parameters, where the number of communica-tions can be reduced to O ( I 1 3 ) . We also compare with LightLDA, a state-of-the-art distributed batch method. Our results show that EPSGLD converges faster at the beginning, and reaches a compara-ble optimum with LightLDA at the scale of V = 10 5 and K = 10
Outline : We first introduce the preliminaries about LDA, SGLD and distributed sampling. Then we present EPSGLD and solutions to the above two issues. Section 4 conducts a set of experiments to evaluate our method and compare with state-of-the-art distributed samplers such as DSGLD and LightLDA. Finally, we conclude.
LDA is a probabilistic generative model for topic discovery in text documents. It describes how the words in documents are ex-plained by a set of K topics, each of which is a V -dimensional topic-word distribution  X  k , where V is the vocabulary size. The topics are often assumed to follow a conjugate Dirichlet prior, that is,  X  k  X  Dir (  X  ) , with hyperparameter  X  . For each document w that contains N d tokens, where each token in it is denoted as w a K -dimensional topic mixing distribution  X  d is sampled from a Dirchlet prior Dir (  X  ) . Then, for each token, a topic assignment z dn is sampled from a multinomial distribution z dn  X  Multi (  X  followed by sampling the token itself again from a multinomial dis-tribution w dn  X  Multi (  X  z dn ) . The matrix that contains all  X  denoted as  X  , and the one that contains all  X  d is  X  .

The inference problem for LDA is to determine the posterior dis-tribution P ( z ,  X  ,  X  | w ) , where w and z denote all the tokens and their corresponding topic assignments in the whole dataset respec-tively. However, the exact inference is intractable. The solution to this is to use approximate inference methods such as Gibbs sam-pling which is a special case of MCMC simulation [7]. By ex-ploring the conjugacy, we can do collapsed Gibbs sampling [6] by integrating out  X  and  X  . Namely, we have the collapsed pos-uated as P ( w | z , X  ) = R P ( w | z ,  X ) P ( X  |  X  ) d  X  and P ( z |  X  ) = R
P ( z |  X ) P ( X  |  X  ) d  X  . Then, a collapsed Gibbs sampler iterates over all the tokens and draws the topic assignment for each token w dn from the local conditional distribution P ( z dn | rest, w ) , where rest denotes the rest topic assignments excluding z dn . By some al-gebra, we can show that the local conditional distribution is: where n dk denotes the number of times that topic k is assigned to document d , n kw denotes the number of times that topic k is assigned to word w , superscript  X  z dn denotes the counts matrix without z dn , and we omit the condition w for simplicity. Note that token w dn is different from word w : different tokens can refer to the same word, but the word w is unique and the number of unique words is V . In this process, one updates the counts matrices n and n kw according to the sampling results, and after a sufficiently large number of iterations, we can normalize the counts matrices by row to obtain the  X  and  X  .
In the standard form, Eq. (1) takes O ( K ) time to sample a new topic z dn for token w dn . A variety of methods have been proposed to reduce this per-token sampling complexity. Here we briefly in-troduce three of them. SparseLDA [24] decomposes Eq. (1) into three terms: P ( z dn = k | rest )  X   X  X  When sampling, one first uniformly chooses one of the three prob-ability buckets, and then samples the topic from that bucket. If the second or third term is chosen, it takes O ( K d ) or O ( K sample z dn , where K d is the number of different topics that doc-ument d has, and K w is the number of different topics to which word w is assigned. Since LDA is often sparse, the complexity O ( K d + K w ) is much lower than O ( K ) . AliasLDA [11] presents another way to decompose Eq. (1): where the second term is sparse and can be sampled with O ( K time, and the first term uses a stale copy of n kw , out of which one can build an alias table and sample from it with O (1) time. Since the sampler draws topic from a stale copy, an extra Metropolis pro-cess is used to correct the bias in the samples. Recently, LightLDA reduces the complexity to O (1) . It decomposes Eq. (1) as: Instead of choosing the bucket, it samples topic from either the first term or the second term at one time, and chooses the other term the next time. While the second term is sampled through an alias table, the first term can also be sampled with O (1) time as long as it keeps track of the topic assignments list z d = { z d 1 ,  X  X  X  ,z document d (as it is often the case in practice), reaching the O (1) overall complexity.
As the data size gets extremely large, the time for running one iteration in batch methods can be unacceptable. One approach to scaling up the inference process is to use stochastic subsampling. Due to the statistical redundancy in big data, the idea has proven to be effective in various SGLD samplers [23, 17]. However, the vanilla SGLD cannot be readily applied to LDA due to two rea-sons [17]: (1) the probability simplex that defines LDA has its own boundary, such that a gradient step can get outside of the sim-plex; and (2) the distribution for LDA can be very skewed, and the vanilla SGLD is likely to be insufficient in exploring the probabil-ity space. To address the problems, the stochastic gradient Rie-mannian Langevin dynamics (SGRLD) [17] sampler for LDA was proposed. Here we briefly introduce SGRLD, and for simplicity, we do not distinguish between SGLD and SGRLD in the sequel.
Let w ( s ) denote the mini-batch sampled from w at the s th itera-tion, and the numbers of documents in w ( s ) and w are denoted as D ( s ) and D respectively. The inference in SGRLD is performed on the unnormalized topic-word matrix T , where we compute the gra-dient and update T with it in each iteration. In T , the distribution of topic k and its probability mass of each word w is denoted as t t kw (Correspondingly, we also have  X  k and  X  kw ). The prior of T (topic k and word w ) is given as P ( t kw ) = Gamma (1 , 1) . Thus the normalized distribution  X  k is obtained from  X  k = t k each iteration s , T is updated as follows: where ( s ) denotes the step size at the s th iteration and n notes the number of times topic k is assigned to w in document d .  X  kw is obtained from  X  kw  X  N (0 , ( s ) ) and d : w w ( s ) denotes each document in the mini-batch. The expecta-tion E z dn | rest,  X  is not analytically tractable, but can be esti-mated by performing the collapsed Gibbs sampling on distribution P ( z dn | rest,  X ) over the mini-batch data In this case, two temporary counts matrices n dk and n kw ated to record the updates in topic assignments. Then these two matrices are used in Eq. (2) to compute the expectation. In other words, SGLD still uses collapsed Gibbs sampling, but with a smaller size of documents, which helps to reduce the computa-tion cost. Therefore, SGLD is compatible with the alias table tech-nique used in AliasLDA and LightLDA, which means SGLD can also draw samples with O (1) complexity.
The other approach to increasing the scalability is distributed computing. The distributed systems for batch samplers have been extensively studied in order to learn large-scale LDA models [1, 25, 4, 13, 12, 16]. Despite of different techniques and implementations proposed in each method, the overall process is similar. In Ya-hoo!LDA, the global counts matrix n kw is stored in a master node; each worker node i maintains a local copy of it  X  n kw,i local counts matrix n kw,i . We note that the local copy  X  n different from that in the master node (i.e., n kw ). This is referred to as the delayed update technique, where the worker can delay the synchronization of n kw with the master for a certain number of iterations in order to reduce communications. For each worker, it updates n kw,i with Eq. (1); and the master synchronizes and updates the global matrtix n kw using Another similar method is proposed in LightLDA. As shown in Fig.1(b), the global matrix is split into multiple blocks along the word-axis. For every worker, it pulls a block from master, updates it with its local data and sends it back to the master node. During this process, the block in master is locked until it is returned by the worker, so that these blocks can be updated asynchronously without conflicts. Hence, LightLDA is referred to as a model-parallelism method, where it partitions the model and storing a part of the model on each node; then, partial updates are carried out on each node [28]. In these methods, synchronizations play a central role in counts matrix update, and they expect equality between the global matrix and its per-machine copies after a complete synchronization.
Another parallel design is the embarrassingly parallel method, which assumes that there is a generative process that leads to diver-gent copies of the same random variables in each node [1]. This method follows several criteria: (1) each node only has access to its local data; (2) each node performs MCMC independently, with-out communication; and (3) samples from each worker are aggre-gated using approximate methods which yield asymptotically exact samples from the full-data posterior [15]. Formally, if we develop embarrassingly parallel method based on the SGRLD sampler, the sub-posterior samples for LDA should be T i s (local unnormalized topic-word matrices); then the global unnormalized topic-word ma-trix which is aggregated from those T i s is denoted as  X  and the true (full-data) posterior is therefore denoted as P ( X  | w ) . Suppose there are n workers and we split our data w into { w 1 ,  X  X  X  , w Then, we have where w i is the subset at node i and each term P ( X  | w i posterior [15]. Note that we neglect the local variables z and  X  since they are documents-related and are unnecessary to aggregate globally. Eq. (4) suggests that the posterior of full data can be represented by the product of sub-posteriors. Sampling from this product of posteriors remains the main difficulty in the embarrass-ingly parallel design [19]. To do this, various approximate meth-ods have been proposed [15, 14, 19], such as weighted average and kernel density estimators. These methods are excellent for models with a large amount of parameters, since they allow the update on model to be performed without communication. But just like the case for SGLD and SGRLD, these frameworks are not readily ap-plicable to LDA X  X wo problems need to be addressed, and Section 3 is dedicated to solving these problems.
We now present how to combine the SGRLD sampler with an embarrassingly parallel method to explore data redundancy and dis-tributed computing for better scalability. For simplicity, we develop our distributed method based upon the Weierstrass sampler [19] (Note that the following discussion is also applicable to other em-barrassingly parallel methods, such as [15, 14], since they follow a similar distributed design).

For x  X  R q , the Weierstrass transform [22] of a function f ( x ) is where the multivariate kernel function K h ( x , y ) is defined as: in which H is a q -by-q diagonal matrix, where the elements on its diagonal are the same and defined as the tuning parameter h . We denote  X  k as the distribution of the topic with integer label k in  X  . Under the common independent prior P 0 ( X ) = Q k P 0 we have the factorization form of each sub-posterior P ( X  | w Q k P (  X  k | w i ) in Eq. (4). Therefore, we can deal with each  X  arately. Formally, according to Eq. (4) and the factorization form of each sub-posterior as above, we have P (  X  k | w )  X  Q i P (  X  If we substitute P (  X  k | w i ) with f ( x ) , and y with t i ), we have
P (  X  k | w )  X  Y Note that we use approximation to replace the limit h  X  0 , as long as we set h to a small enough value (A recommended value of h is around that P ( t k,i | w i ) now represents the sub-posterior in node i . The integration in Eq. (6) is in fact a marginalization performed on joint probability distribution It marginalizes all samples from sub-posteriors and gives the pre-dictive distribution of  X  k . Hence this intergation can be estimated by a standard block-wise Gibbs sampler: where H 0 is the diagonal matrix with all its diagonal elements dicates how the samples are drawn from each sub-posterior: The sub-posterior is multiplied with a kernel term which penalizes the samples that are far from the true posterior mode; in such way the sub-posteriors are corrected towards the true posterior. The second part of Eq. (8) suggests that the global samples are obtained by sampling from a Gaussian distribution. This process is usually per-formed by a master node which collects local t k,i s from workers asynchronously. The inference of the corrected local sub-posterior P ( t k,i |  X  k , w i ) is performed in each worker node, and can be im-plemented by the SGRLD sampler. Formally, we take the derivative of P ( t k,i |  X  k , w i ) , and re-arrange it into the SGRLD update equa-tion, where we obtain the corrected version (one parameter case) Note we use E as the abbreviation of the expectation term in Eq. the true posterior mode by an amount which is governed by h . As the training proceeds, h shall decrease and finally vanish [19].
Embarrassing parallel methods can produce samples without communication. However, as we discussed above, synchroniza-tions are important for distributed LDA methods to converge to the optimal state. This property also holds in our method. As we inspect Eq. (8), the sampling of P ( t k,i |  X  k , w global matrix  X  ; and the sampling of P (  X  k | t k, 1 ,  X  X  X  ,t depends on the local T i s. Therefore,  X  has a similar role as the counts matrix  X  n kw,i in Yahoo!LDA, and should be synchronized across all nodes. Formally, EPSGLD can be categorized as a data-parallelism method, where the whole dataset is partitioned across machines and computations are performed on each node given a local copy of the globally share model [28]. Other examples of this type include approximate distributed LDA (AD-LDA) [16] and Ya-hoo!LDA. In [16], Newman et al. did an extensive analysis on the role of synchronization in data-parallelism methods: when sam-pling independently, the topics (with the same integer label) in each worker can drift apart, so that topic k on one worker may diverge from topic k in another worker. Through synchronization, matrices in each node are updated to be consistent, leading to the equivalent predictive power as their non-distributed counterparts. However, if the synchronization interval increases, it is likely for this type of methods to converge to a suboptimal solution, since topics in each node can drift far apart. They concluded that there are two ways to prevent data-parallelism methods from failing: (1) increase the number of workers; and (2) apply frequent synchronizations.
Though the first method is easy to follow for all distributed al-gorithms, the second advice is not suitable in our method, since the number of iterations required for the stochastic sampler to converge to a stationary state can be in the order of thousands and an O ( I ) communication overhead is unacceptable. In order to avoid fre-quent synchronizations without imposing too much accuracy loss, we seek solutions by investigating the properties of Eq. (8). We know that the main function of synchronization is to prevent top-ics in each node to drift apart, so if we are able to constrain local samplers so that they tend to draw samples with similar statisti-cal meaning, we can then safely reduce the frequency of synchro-nizations. Therefore, we can propose the third way to ensure the validity of the data-parallelism methods, that is to correct the sub-posterior towards the true posterior, so that topics are not likely to drift apart even though they are sampled independently.

Fortunately, such mechanism indeed exists in Eq. (8). As we inspect Eq. (8), we find that the sub-posterior is multiplied with a kernel term which makes the distribution biased towards the global matrix  X  . And  X  is sampled from P (  X  k | t k, 1 ,  X  X  X  ,t to estimate the unknown true posterior. As the training proceeds, this approximation becomes accurate and results in the local T being biased towards the true posterior. Hence, we find that the third solution is already contained in our method, and functions as an EM-style process, where in the E-step we aggregate  X  as an approximation of the true posterior; and in the M-step we correct sub-posteriors towards  X  so that samples in T i s tend to have similar statistically meaning. By iteratively applying these two steps, the sub-posteriors are then biased towards the true posterior.
We use a toy example to illustrate this process. Suppose we are to aggregate 3 T i s from workers 1, 2 and 3. As shown in Fig.2, for the second topic, t 2 , 1 and t 2 , 3 have the same statistical property, while topic t 2 , 2 in T 2 diverges form them. In E step, we average over T s and use Eq.(8) to obtain  X  and send it to workers. In M step, the worker 2 updates T 2 . In sampling each topic, Eq. (9) is biased towards  X  so that the SGRLD sampler updates the t 2 , 2  X  . Then for the next EM step, the aggregated second topic  X  should be more similar to those in T 1 and T 3 , which in turn makes t 2 , 2 to be more similar to t 2 , 1 and t 2 , 3 . Given enough number of such steps, the topics will not drift apart even with less frequent synchronizations. We note that this process is essentially differ-ent from what is applied in DSGLD, where it also produces global samples by averaging the T i s using chain coupling technique. As we have discussed, the EM process holds only if the sub-posteriors are biased towards  X  . In DSGLD, such mechanism is missing, thus increasing the synchronization interval (or trajectory length in DS-GLD) can significantly influence the convergence.

To demonstrate the effectiveness of the EM process, we run a modest scale inference task ( K = 10 2 , V = 10 5 ) with 3 dis-tributed settings: EPSGLD with synchronization after each itera-tion, EPSGLD with synchronization after 5 iterations, and EPS-GLD with synchronization after 5 iterations but the kernel term in Eq. (9) is removed (which is a similar setting to that of the DS-GLD). The result is shown in Fig.2(c). The  X  X PSGLD Sync=1 X  curve sets up the lower bound for approximation error. The  X  X PS-GLD Sync=5 X  curve has a similar performance when compared to the lower bound, while the EPSGLD without correction converges to a suboptimal state. This suggests that the EM process can sig-nificantly reduce the accuracy loss caused by the decrease of syn-chronization frequency.
We have shown that the EM process contained in EPSGLD can enable the method to work with less frequent synchronizations. But as we notice, the decrease of synchronizations not only influences the convergence of our method, but also the effectiveness of how the kernel term can correct the sub-posterior in Eq. (8). It sug-gests that if one updates T i and  X  rigorously according to Eq. (8), then worker nodes will need to acquire the latest  X  to compute the kernel term after each iteration. We note that this property is resulted from the design of Weierstrass sampler itself, and differs Figure 2: The graph uses a toy example to illustrate the EM process: (a) 3 T diverges from those in T 1 and T 3 ; (b) worker updates T kernel term in Eq. (9) is removed. (Best viewed in color). from the need of synchronizations for LDA as discussed above. In other words, this property still exists if we apply our method to other models such as logistic regression. In order to eliminate the communications, Wang et al. [19] suggested to generate all  X  s us-ing approximate methods such as Laplace approximation and store them for training, so that the sampler can draw new T any communication. However, since our model can contain 10 parameters, storing all the samples is not viable. Given that com-munications are inevitable, we resort to address this issue through a series of well-scheduled synchronizations, as detailed below.
A useful strategy for this problem is to apply delayed update. For example, Yahoo!LDA allows worker i to save a local stale copy of the global counts matrix  X  n kw,i , and updates global n (3) after a few iterations. Another example is DSGLD  X  Instead of jumping to another worker at each iteration as shown in Fig.1(a), the trajectory sampling is used to delay the jump by a number of steps  X  , so the communications are then O ( I  X  ) ; this technique can be used to control the level of approximation by trading off compu-tation time with asymptotic accuracy [2]. The idea underlying this strategy is that LDA is usually very sparse and changes slowly in each iteration of the update, so the stale copy of the model is not likely to be out-of-date in a short period.

In EPSGLD, h is initially set to be relatively large in order to en-courage samplers to explore the posterior space, so the kernel term penalizes less for the samples being different from the stale copy of  X  ; as the training proceeds, h becomes smaller and matters a lot in penalizing the samples being far from  X  , but at this point since the model is adequately sparse and the step size is small, the new samples usually concentrate on the true posterior mode and thus are similar to  X  . Therefore, in either case, given that the aggregated  X  can correctly approximate the true posterior (as is often the case), we can reuse the stale  X  in local sampling for a certain number of iterations. Additionally, the timing for synchronization in EPSGLD is determined by the master node, so worker nodes will respond to the master node immediately after receiving the notification. This helps to reduce the latency in synchronization.

The only remaining issue is to determine the schedule for com-munications. It is not a trivial task as we take into account its effect on the performance of the EM process, the effectiveness of the ker-nel correction and the overall computational efficiency. Here we propose to determine the schedule by putting different choices into test and see how EPSGLD performs under certain communication patterns. To do this, we first present a model in order to describe different schedules formally: where  X  I = 1 n P i I i , and I i is the number of iterations in worker i , c 1 and c 2 are parameters to be determined. During training, the master keeps track of the iteration of each worker and computes  X  I , once  X  (  X  I ) gives 1, the master will inform workers to synchro-nize. The model is flexible to describe most of the likely schedules: c = 1 ,c 2 = 1 means communicate-per-iteration, c 1 =  X ,c 2 lets the synchronization delay for  X  iterations, c 1 = 1 ,c concentrates more communications at the beginning and less after-wards, and so on. To determine the parameters. Our approach is to run a grid search on c 1 , c 2 and compare the speed and perplexity of each case. By investigating the curves of perplexity against iter-ations under different settings of c 1 , c 2 , we gain some insights into the relationship between communication scheduling and the overall performance. The experiment is shown in Section 4.
In large-scale LDA, manipulating a complete T i (or  X  ) can be difficult as either holding it in memory or sending it to other nodes is a nontrivial task. Hence, an appropriate memory strat-egy for EPSGLD is needed to improve the scalability. Here we draw inspirations from LightLDA, which partitions the counts ma-trix n kw into multiple data blocks (along the word-axis) that are small enough to be held in memory, and the worker only holds and updates one block at each time. For our method, we find that the number of unique words contained in one mini-batch is signifi-cantly smaller than the vocabulary size, so it is sensible to only load those word-slices that occur in the mini-batch.

To better understand our corpus, Fig.3 illustrates the histogram of the number of unique words in a mini-batch and 4-mini-batches set of the Wiki corpus (See Section 4 for detail on this corpus). It suggests that most of the mini-batches contain only 4,000 differ-ent words, and the maximum is approximately bounded by 10,000, while a 4-mini-batches set usually contains 10,000 words and is bounded by 20,000. That means one only needs to load a T with 10 4 unique words in order to update for several iterations. In practice, we can load as many word-slices as possible at once to avoid frequent disk I/O. Given the model size and hardware en-vironment, we load T i slice that covers words in 4-mini-batches set. In our implementation, we group the documents into 4-mini-batches datasets, and each set has a data structure where batch is the actual dataset,  X  1 ,  X  X  X  , X  4 are V-dimensional boolean vectors that indicate which word slices should be loaded for these batches respectively, and  X  =  X  1 |  X  2 |  X  3 |  X  the  X  X r X  operation. Figure 3: (a) The histogram of the number of different words in one mini-batch data of the Wiki corpus; (b) The histogram of the number of different words in 4-mini-batches data of the Wiki corpus.
 Algorithm 1 Master node init:  X  I while not called to stop do end while
This technique is also applicable in synchronization. Worker and master nodes can send the matrix slice together with its mask vec-tor, instead of the complete one. If we denote the mask for worker i at iteration s as  X  ( s ) i , and the matrix slice made up with these slices as  X  ( s ) i ( T i ) . The mask for all modified slices after last synchro-synchronization, the worker only sends  X   X  i ( T i ) and master first computes the global mask  X   X  =  X   X  1 |  X   X  buffer to receive the local samples.
Based on our previous discussions, we present the pseudo code of our EPSGLD in Algorithm 1 and Algorithm 2 . In the method, the master node keeps collecting the number of iterations I chronize their masks and obtain the  X   X  , and then the master receives  X   X  ( T i ) from workers and averages them to get  X   X  ( X ) , finally sends it to workers. For the worker, it continues to update with a SGRLD sampler, until it is informed to synchronize, and the following op-erations are similar to those in master.
We now present more empirical results on large datasets. We fo-cus on two tasks: (1) We determine the parameters in communica-tion scheduling, and examine the effectiveness of the EM process at a larger scale; and (2) We compare the performance with DSGLD and LightLDA, two representative distributed methods. Though our main focus is to develop a better distributed SGLD sampler, LightLDA is also included as a state-of-the-art distributed batch method to demonstrate the scalability of EPSGLD.
 Algorithm 2 Worker node input: dataset w i , tuning parameter h , local copy  X  while not called to stop do end while Table 1: Statistics of the two datasets, where L denotes the number of tokens.
 Table 1 summaries the datasets in our experiments. The Wiki dataset contains the latest web pages from Wikipedia Clueweb12 is a subset of Clueweb12 dataset, a large crawl of web pages 2 . Note that the Clueweb12 data also share the property that we discussed in Section 3.3. For each corpus, we take the top 10 most frequent words (with stop words excluded) as our vocabu-lary and parse the documents into a bags-of-words format. In order to apply large-scale LDA inference, the Gibbs sampling used in SGLD is implemented using alias table. The step size for SGLD is represented using function = ( a + b  X  I )  X  1 3 as recommended in [18]. We determine the a and b by comparing the performance on a grid based on 242 runs, and we pick a = 10 5 . 2 ,b = 10 default values. For each document, 20% words are extracted, and all these words are used as the held-out test set.

All codes are implemented using Python and its supporting li-braries (numpy, h5py, etc.). The code of SGRLD provided in [17] is used as reference. The critical parts of the code are compiled into C using Cython to improve performance. The token throughput of a serial Gibbs sampler (used in both SGLD and LightLDA) with alias table is approximately 300K/sec. It is relatively slower than what is provided in Yuan et al. X  X  implementation, which is around 1M/sec [25]. This is due to the fact that the code is implemented using Python and Cyhton instead of C.
We first determine the parameters c 1 and c 2 in Eq. (10) that con-trol the frequency of synchronizations, and investigate how this fre-quency influences the convergence of EPSGLD. The performance measure is the perplexity on the test set with respect to the num-ber of documents visited. This helps us to compare how effective different methods can explore the data information for learning. In this case, we would expect the serial sampler to have the best performance, since the approximation in distributed samplers may lead to a waste of data. Therefore, comparing with the serial sam-pler can help to investigate the power of distributed methods in approximating the true posterior, which further indicates whether the method design is good or not. We also present the curves of perplexity against time for comparison.

We search for the parameters by running a grid search on c and c 2 , where c 1 ranges from 1 to 10 and c 2 ranges from 1 to 4. EPSGLD is distributed to 12 machines with model size K = 10 https://dumps.wikimedia.org/ http://www.lemurproject.org/clueweb12.php/ DSGLD is distributed to 12 machines.
 V = 10 5 . We report the results on the Wiki dataset in Fig.4(a) and Fig.4(b), where each legend in the graph represents a certain parameter configuration (e.g.,  X  n 2  X  means c 1 = 1 ,c 2 keep illustration uncluttered, we only present those curves that are informative for determining c 1 and c 2 .

Schedule and Convergence : We can see that EPSGLD is gen-erally insensitive to the value of c 1 ,c 2 , except the  X 2 X  case, where EPSGLD synchronizes only twice at the 500th iteration and the 1000th iteration. The most frequent case  X  n  X  converges just as fast as the least frequent case  X  n 4  X , which has 5 synchronizations in 1000 iterations. The only difference comes from the slight increase of ending perplexity, which means as the frequency of synchro-nization decreases, EPSGLD may converge to slightly bad sub-optimums. Additionally, Fig.4(b) demonstrates that as the com-munication frequency decreases the convergence speeds up signif-icantly.

Another insight into the role of synchronizations can be gained by inspecting Fig.4(a): Since the model changes drastically at the beginning of training, it is important to cover this stage with a suffi-ciently large number of synchronizations (e.g., with schedule  X  n or  X  n 3  X ); otherwise, if the initial timing is missed, as it is the case in the curve marked by  X 2 X , the EM process may fail to bring it back to the right track. This fact is sensible since the step size decreases at O ( I  X  1 3 ) speed. Thus, if the  X  fails to approximate the true pos-terior at the beginning due to the lack of synchronizations, it will be difficult to correct it afterwards since the step size has annealed. And this eventually leads to a sub-optimal solution.
 Comparison with serial SGRLD : The curve of the serial SGRLD is also shown in Fig.4(a) in order to set up the upper bound of the performance for distributed samplers. However, Fig.4(a) suggests that the best performance is actually bound by EPSGLD with  X  n  X  schedule; EPSGLD with some other schedules also out-performs the serial sampler in convergence. This phenomenon seems counter-intuitive, but is in fact sensible: it can be explained by the effect of variance reduction [2]. As the master node averages over T i s and generates  X  , the variance of the gradient estimator re-duces by 1 n . This means that the noise injected by the SGRLD sampler reduces by certain amount, and therefore, the gradient is less stochastic, leading to a faster convergence.

Suggestion on the parameters : The experiment demonstrates that, with the help of the EM process, EPSGLD requires signifi-cantly fewer synchronizations to reach satisfactory results. In de-termining an appropriate schedule for training, one must ensure that the initial stage is covered with a sufficiently large number of syn-chronizations. As indicated by Fig. 4(a), it is recommended to take the  X  n 3  X  schedule (i.e., c 1 = 1 ,c 2 = 3 ) as the lower bound for the frequency of synchronizations, in which case the communications grow only at O ( I 1 3 ) rate, and the loss in accuracy is acceptable.
Comparison with DSGLD : We also test the DSGLD under the same distributed setting. The perplexity-documents curves for DS-GLD with different trajectory lengths (iterations delayed before jumping to another node) are shown in Fig.4(c). As the graph sug-gests, DSGLD converges slower than serial SGRLD, this is due to DSGLD is sensitive to the noise of partitioning of the datasets across nodes; as the trajectory length increases, DSGLD converges to significantly worse sub-optimums due to the lack of correction in sub-posterior.
In this section, we test the performance of EPSGLD in handling large-scale models in two settings: (1) we compare the performance of two stochastic distributed methods X  X PSGLD and DSGLD, to-gether with the serial SGRLD sampler; and (2) we compare with LightLDA to demonstrate the scalability of EPSGLD.
We distribute EPSGLD and DSGLD to 24 machines with the model size K = 10 5 ,V = 10 5 . The communication schedule of EPSGLD is  X  2 n 2  X , and the trajectory length of DSGLD is 10. The results are shown in Fig. 5. In Fig. 5(a), with more workers and a larger model size, EPSGLD still maintains an advantage in approx-imating true posterior samples, leading to a faster convergence than the serial SGRLD. In general, EPSGLD reaches the same perplex-ity as DSGLD does using only a quarter of the documents. When comparing the performance against time, as shown in Fig. 5(b), we note that the overhead in communication has seriously bounded the performance of DSGLD, while EPSGLD maintains an obvious ad-vantage against the other two stochastic samplers. Finally, we examine how EPSGLD performs in comparison with LightLDA, a state-of-the-art distributed batch method, on both Wiki and Clueweb12 datasets. The distributed settings are the same as in the previous test. In LightLDA, the n kw is split into 24 slices, and the distributed method is implemented using MPI so that it maintains all the important features. But the parameter server and the hybrid data structure are not implemented, since they are gen-erally applicable to any method. The collapsed Gibbs sampler with (b) Perplexity of three stochastic samplers against time with model size K = 10 Perplexity of EPSGLD and LightLDA samplers against time with model size K = 10 O (1) complexity is shared by both EPSGLD and LightLDA. The results are shown in Fig. 6. Fig. 6(a) suggests that EPSGLD can reach the same perplexity by visiting much fewer documents than LightLDA on both datasets. Fig. 6(b) demonstrates that EPSGLD has a faster speed to reach a good model than LightLDA, especially on the larger Clueweb12 dataset.

However, we should be aware that there are still some poten-tial drawbacks in EPSGLD compared with LightLDA. First, the two figures suggest that LightLDA has a better ending perplexity. This is due to two disadvantages of SGLD, as stated in [23]: (1) SGLD does not require an MH step to correct the samples (since it is performed based on the whole dataset), and this leads to a certain amount of discretization error; and (2) as the step size de-creases, the mixing rate slows down, and thus leading to a strong correlation in samples. Such phenomenon is also observed in the experiments of [17], where the Gibbs sampler has a better ending perplexity than SGRLD. But thanks to the variance reduction, this disadvantage can be alleviated, and EPSGLD even reaches a com-parable optimum with LightLDA. Another drawback is the diffi-culty for EPSGLD to scale up to the same magnitude as LightLDA ( V = 10 6 , K = 10 6 ). The main issue is due to the element of T is float instead of integer, so it can be hard to convert the T sparse matrix in order to hold it in memory.

But as we mentioned before, EPSGLD can be seen as an example of data-parallelism, and LightLDA is the one of model-parallelism. These two paradigms are complementary. For example, [20] pre-sented a two layer LDA system, where layer 1 is model-parallelism and layer 2 consists of multiple local model-parallelism clusters performing updates on a globally distributed model [28]. In that case, we can jointly combine the power of two methods. We leave a systematical investigation in future work.
We present EPSGLD, a novel distributed stochastic gradient sampling method for large-scale LDA inference, which scales up the inference by jointly exploring statistical redundancy and dis-tributed computing. Different from previous methods such as DS-GLD, EPSGLD is built upon an embarrassingly parallel method, so that it can produce asymptotically exact samples from the true posterior with significantly less communication overhead. We also present communication scheduling and useful memory strategies to reduce the I/O and memory usage. Finally, a set of carefully de-signed tests are conducted, which demonstrate the effectiveness of our methods. Our results on large-scale LDA models demonstrate that EPSGLD significantly outperforms DSGLD and has a faster speed to reach a good model compared with LightLDA.
 This work is supported by the National Basic Research Program (973 Program) of China (No. 2013CB329403), National NSF of China (Nos. 61322308, 61332007), the Youngth Top-notch Talent Support Program, and the Special Program for Applied Research on Super Computation of the NSFC-Guangdong Joint Fund (the second phase). [1] A. Ahmed, M. Aly, J. Gonzalez, S. Narayanamurthy, and [2] S. Ahn, B. Shahbaba, and M. Welling. Distributed stochastic [3] J. Chang and D. M. Blei. Relational topic models for [4] J. Chen, K. Li, J. Zhu, and W. Chen. WarpLDA: a Cache [5] N. Chen, J. Zhu, F. Xia, and B. Zhang. Discriminative [6] T. Griffiths. Gibbs sampling in the generative model of latent [7] G. Heinrich. Parameter estimation for text analysis. [8] M. Hoffman, F. R. Bach, and D. M. Blei. Online learning for [9] M. D. Hoffman, D. M. Blei, C. Wang, and J. Paisley. [10] R. Krestel, P. Fankhauser, and W. Nejdl. Latent dirichlet [11] A. Q. Li, A. Ahmed, S. Ravi, and A. J. Smola. Reducing the [12] M. Li, D. G. Andersen, J. W. Park, A. J. Smola, A. Ahmed, [13] Z. Liu, Y. Zhang, E. Y. Chang, and M. Sun. Plda+: Parallel [14] S. Minsker, S. Srivastava, L. Lin, and D. B. Dunson. Robust [15] W. Neiswanger, C. Wang, and E. Xing. Asymptotically [16] D. Newman, A. Asuncion, P. Smyth, and M. Welling.
 [17] S. Patterson and Y. W. Teh. Stochastic gradient riemannian [18] Y. W. Teh, A. Thi X ry, and S. Vollmer. Consistency and [19] X. Wang and D. B. Dunson. Parallel mcmc via weierstrass [20] Y. Wang, X. Zhao, Z. Sun, H. Yan, L. Wang, Z. Jin, L. Wang, [21] X. Wei and W. B. Croft. Lda-based document models for [22] K. Weierstrass.  X ber die analytische darstellbarkeit [23] M. Welling and Y. W. Teh. Bayesian learning via stochastic [24] L. Yao, D. Mimno, and A. McCallum. Efficient methods for [25] J. Yuan, F. Gao, Q. Ho, W. Dai, J. Wei, X. Zheng, E. P. Xing, [26] X. Zheng, J. K. Kim, Q. Ho, and E. P. Xing. Model-parallel [27] J. Zhu, A. Ahmed, and E. P. Xing. Medlda: maximum [28] J. Zhu, J. Chen, and W. Hu. Big learning with bayesian
