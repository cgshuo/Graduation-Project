 The queries submitted by users to search engines often poorly describe their information needs and represent a potential bottleneck in the system. In this paper we investigate to what extent it is possible to aid users in learning how to for-mulate better queries by providing examples of high-quality queries interactively during a number of search sessions. By means of several controlled user studies we collect quantita-tive and qualitative evidence that shows: (1) study partic-ipants are able to identify and abstract qualities of queries that make them highly e  X  ective, (2) after seeing high-quality example queries participants are able to themselves create queries that are highly e  X  ective, and, (3) those queries look similar to expert queries as defined in the literature. We con-clude by discussing what the findings mean in the context of the design of interactive search systems.
 Categories and Subject Descriptors: H.3.3 Information Search and Retrieval General Terms: Measurement, Experimentation, Human Factors Keywords: Search expertise; Reflection; Behavioural Change; User Study
Much of the IR research in the last half century has, with great success, focused on developing improved retrieval mod-els to enhance the utility of retrieval systems for the end user [41]. In this line of research search queries submitted to a retrieval system are considered as a given. The focus is placed on what to do systematically to return relevant documents given this limited representation of the user X  X  information need. A complementary approach with poten-tially more scope for future performance gains is to focus on giving the system more to work with by assisting users in creating better queries for specific search systems [32, 24].  X  All three authors contributed equally to this work.
Considerable evidence exists showing that many users do not know how to generate good queries. Analyses of search transaction logs show that people use short queries, espe-cially on the Web [2] and even in this familiar domain a good proportion of searches fail completely [13]. In many search domains, including Web and Email search (with domain-specific search systems and interfaces), expert users achieve better retrieval e  X  ectiveness than novices and demonstrate di  X  erent querying behaviour [3, 12, 43]. Moreover, despite the fact that most users today have to navigate through a range of search systems in their digital life, it has been re-ported that many users are inflexible in their approach and tend to use the same querying strategies regardless of task and available search system [29].

Typical solutions to assist users in creating e  X  ective search queries are the use of search UI features, such as query suggestions [35], related searches [36] or query autocom-pletion [5]. Alternatively, systems can employ context and personalisation techniques [11], which involve storing (and learning from) personal search histories and preferences to understand what a user knows and likes [18].

A third approach is to educate users about how to become better searchers [28] or to help users reflect on their own be-haviour by comparing it to experts [6]. This method has the advantage that it is complementary to technical solutions. Our research continues along this path by investigating to what extent we can teach users how to pose better search queries to a particular search system. In contrast to ex-isting approaches, we aim to understand if users are able to recognise, compare and contrast the properties of their own queries with good queries (provided by the system) and make changes to the queries they generate based on these insights. This is a new way of thinking about query sug-gestions; instead of providing automated examples for users to simply click on, we present them in a way that leads the user to reflect on his own behaviour, positively influencing his actions as a result.
 The two principle research questions we answer are: RQ1 Are users able to notice di  X  erences between good RQ2 How e  X  ectively can users learn and abstract from good
It is well-recognised that searchers have di culties com-municating their information needs [7, 38, 24]. Taylor writes of a series of stages a user goes through when seeking in-formation. These range from experiencing a visceral need, which is  X  X robably inexpressible in linguistic terms X  to a compromised need -a  X  X epresentation of the inquirer X  X  need within the constraints of the system and its files X  X 38]. There-fore, in order to generate successful queries, the user must overcome several cognitive challenges: 1) to determine him-self what the need is and what kind of document will solve it; 2) to choose terms that describe that document well out of a very large set of possibilities [15] and 3) to communicate using the system X  X  vocabulary and not his own [9].
Many interactive solutions have been designed to help the user overcome these challenges and improve the representa-tions of information needs systems have to work with. The following subsection briefly reviews such work.
IR systems can attain better descriptions of information needs by explicitly asking for certain details. The I3R sys-tem o  X  ered a means for users to provide terms and concepts they felt were important and identify relationships between these terms and other concepts in the domain [10]. Similarly, Kelly and Fu [24] used clarification forms to elicit additional information about the search context from users. The forms queried users on what they knew and what they would like to know about the topic and why. These were shown to be helpful in achieving improved retrieval performance.
A second technique is to assist the user to iteratively improve their own queries by adding additional terms sug-gested by the system, commonly referred to as interactive query expansion (IQE) [17]. This approach gives the user much more control over the search than if the query were to be expanded automatically (i.e. where the system selects expansion terms without user input [31]). Although IQE can o  X  er improved performance [25], it has been shown that users are poor at identifying the terms that will o  X  er the best improvement to their queries [33, 1]. This finding is intriguing with respect to our aims as it begs the question of whether or not users are able to identify qualities of good terms or whether they just assume terms suggested by a system will automatically be of a high quality.

Relevance feedback systems [34] are a further means to ex-pand queries without explicitly choosing terms. Instead, rel-evance judgements are solicited on the returned documents. In addition to expanding queries, other scholars have in-vestigated the performance of systems suggesting similar or related queries e.g. [36].

Improving user queries need not be achieved via technical solutions. One group in the 2007 SIGIR workshop break-out group identified a spectrum of possible solutions from manually-led approaches (based on improved information literacy and teaching) through to automatic, system-based approaches (based on more intelligent systems) [32]. The fol-lowing section reviews literature on changing user behaviour via primarily non-technical means.
Behaviour change support systems are  X  X nformation sys-tems designed to form, alter, or reinforce attitudes or be-haviours or both without using coercion or deception X  [30]. Within the context of search, changes can be made to the underlying retrieval engine or to the interface to  X  X udge X  peo-ple towards submitting longer or better queries or to look deeper in the results list [6]. Altering the size [14] and word-ing [8] of the search box, for example, has been shown to in-fluence the length of queries submitted. Moreover providing a simple  X  X oogle-like X  search interface as opposed to a com-plicated multi-field catalogue search can radically alter user behaviour [27]. Training users on how to construct queries can improve search behaviour [26]. For example, providing guidance on the advanced features that can help with spe-cific search tasks can improve performance for these tasks and users are able to preserve and use the knowledge gained weeks later [28]. Moreover, allowing users to reflect on their own behaviour and, importantly, compare their behaviour to other, expert users, enables individuals to improve their own habits. In [6] users, after reflection, spent longer con-sidering search results and issued longer queries. They also used a wider range of techniques and search engine features.
We extend some of the ideas in [6] here. Rather than inviting users to compare their behaviour with that of ex-perts, however, we investigate if they are able to learn by comparing their own queries to examples generated by the system to be near optimal for the task at hand. In doing so we relate the kinds of approaches shown in Section 2.1 with the approaches in this section. We attempt to  X  X udge X  users to improve their queries via high-quality examples shown via widgets similar to those described above.
The aim of our work is to establish whether showing users of an unfamiliar search system examples of high-quality queries (for a small number of information needs) enables them to create better-performing queries themselves. We investigate to what extent users learn more successful query-ing behaviours from those examples.

Based on our two research questions (Section 1) we de-vised the following research hypotheses: H1 Users are able to adapt their querying behaviour to pose H2 Users are able to identify characteristics of high-H3 A small number of  X  X raining queries X  is su cient to en-H4 A user who receives training with queries he can relate H5 A user who receives training with queries he can relate
We conducted a number of user studies (Figure 1), each re-quiring the automatic generation of high-quality queries for a given information need and search system (described in Section 4.1). To address the issue of predicted performance, we performed an initial user study (Section 4.2) to investi-gate participants X  perceptions of the generated queries. In contrast to the later studies, participants were not given ac-cess to our search system, their judgement was solely based on their own past experience. Figure 1: Overview of our experimental design.

Concurrently with the User Perception Study , we per-formed a Pilot Study (Section 4.3) which gave us qualitative insights into the characteristics of good queries that users were able to identify. The results of these two studies then allowed us to conduct a larger Main Study (Section 4.4) and a follow-up Variable Training Size Study (Section 4.5) with a consistent design, but di  X  erent training parameters. The aim here was to better understand how much training is required to achieve an e  X  ect. In each of these studies par-ticipants were asked to perform a series of ad-hoc retrieval tasks using our search system.

To maintain maximum control over the experiments and have access to complete statistics of the collection the par-ticipants were searching over, we used a standard test col-lection: AQUAINT 1 together with the 50 TREC 2005 Ro-bust track queries [40]. As our indexing and search engine we chose Apache SOLR 2 . To provide our study participants with a familiar user interface for searching the collection, we developed a web-based front-end in PHP (Figure 3).
In this section, we present an overview of each study and its results in turn.
In generating the  X  X igh-performing X  query examples, we make the assumption that a query q a is better than an-other query q b for a given information need if q a returns a higher Average Precision (AP) score. It is also important that the queries are understandable by humans and are not excessively long. Therefore, we are not interested in queries that happen to return good results because of a statistical anomaly or because they are overly verbose and specific.
Candidate queries were obtained via a recursive, greedy search algorithm. For each topic and its corresponding set of relevant documents, a collection was built consisting of only those relevant documents. The query building pro-cess is initiated by first considering only queries of length 1 (i.e. single-term queries) and choosing each of the top 100 terms from the topic-specific document collection (after stop words had been removed). Each initialisation of the recursive method takes in a base query and adds each of the top 100 terms to it. All 100 new potential queries are run against the entire collection using the standard SOLR search system and the AP score of the top 50 returned doc-uments is computed. The list of queries is then ranked by their AP values and the top 10 are added to the candidate query list. Subsequently, the algorithm is initiated again with new base query having the newly-selected term added to the end. This recursive process was continued up to a
We removed duplicate documents in a pre-processing step, to provide a better and more familiar user experience. http://lucene.apache.org/solr/ query length of 4. At the end of the process any duplicate queries were removed and the top 100 queries (according to AP) were selected as the final list of candidates.
Note that this approach di  X  ers significantly from previous methods proposed in the literature for generating queries, e.g. [4], as our goal is fundamentally di  X  erent. Rather than generating queries which appear to be samples from the col-lection (i.e. stochastically drawn from collection statistics), we are specifically interested in queries which yield high performance, are understandable and would, potentially, be posed by real users. Other related approaches used to find optimal queries in Boolean systems (e.g. [37]) were inappro-priate due to di  X  erences in the underlying retrieval system. While our greedy approach does not produce globally op-timal queries, it quickly produces large numbers of queries with an AP score of around 0.4. Concrete examples of gen-erated queries can be found in the last column of Table 4.
Considering the top 100 queries for each topic, the me-dian AP obtained by the generated queries over the first 20 returned results was 0.389. On a per-topic basis, the me-dian was 0.391, the lowest average achieved was 0.054 and the maximum was 0.948 (IQR=0.31). Overall, 28 out of 50 topics had at least one query with an AP score greater than 0.5 and only 11 topics had any queries in the top 100 with an AP score below 0.2.
To gain insights into how users perceive our high-quality queries (and as a precursor to answering hypotheses H4 and H5 ), we conducted a crowd-sourcing experiment on the CrowdFlower 3 platform.
Each crowd-sourced task consisted of one search topic (in natural language form) and one of the queries generated in Section 4.1. Specifically, the workers were instructed as follows: Four questions had to be answered on a 5-point Likert scale: 1. How much do you know about the topic of the infor-2. How surprised are you about the generated query sug-3. Would you use this suggestion in an actual search? (1: 4. What do you think the search result quality will be if http://www.crowdflower.com
Each job consisted of 10 tasks and workers were paid 12 cents (a standard rate). In this and all following Crowd-Flower experiments the participants were restricted to coun-tries where English is a native language.

For each of the Robust track topics, the 15 most e  X  ec-tive queries generated were judged by CrowdFlower work-ers. Each query was judged by 3 workers, and thus, for each topic 45 judgements were collected. Three examples of top-ics, generated suggestions and worker ratings are shown in Table 4.
Our workers found many of the search topics rather chal-lenging with an average topic knowledge rating of 2 . 21. The most familiar topics tended to be of broad interest to many di  X  erent communities; the two with the highest average knowl-edge ratings (3.00 and 2.89 respectively) were What fac-tors contributed to the growth of consumer on-line shopping? (topic 639) and Identify drugs used in the treatment of men-tal illness. (topic 383). In contrast, search topics focus-ing on very specific themes or entities tended to elicit the lowest familiarity ratings; the topic with the lowest average knowledge rating (1.58) was What is the status of The Three Gorges Project? (topic 416).

When considering how unexpected the presented sugges-tions were (i.e. the  X  X urprise X  factor) we found that the vast majority of queries (more than 80%) were at least somewhat expected, receiving a rating between 1 and 3 (top-left of Fig-ure 2). Only a small number of suggestions were considered to be extremely surprising and those were mostly found in topics our study participants knew little about. This in-dicates that our query generation approach is achieving its goal of generating queries understandable to humans. Figure 2: Histograms of  X  X urprise X  (top-left),  X  X earch quality X  (top-right) and  X  X uggestion usage X  (bottom) ratings across the 750 di  X  erent generated query suggestions, each rated by three users.

Of note is that fewer than 7% of judgements estimated the queries to achieve a very high quality of search results (top-right of Figure 2), while in contrast nearly 17% of the judgements were rated as likely to return very low quality search results. This result indicates that users are not able to judge the quality of query suggestions well, corroborat-ing previous findings that users are unable to di  X  erentiate good search terms from bad ones [33, 1]. This result can only be partially explained by their lack of topical domain knowledge as the correlation between knowledge ratings and search quality ratings was moderate (but significant) with r =0 . 35.

Lastly, we consider the question of to what extent users would use the shown suggestions in an actual search (bot-tom of Figure 2). Not unexpectedly, the correlation between the estimated search result quality and the potential usage is high ( r =0 . 77). Based on the ratings we have to conclude that many suggestions are not convincing, only a small num-ber would definitely be used (9% of those rated 5) while 30% would definitely not be used (ratings of 1).

In summary, we find that user perception of our high-quality queries varies; many of them are not recognised as being e  X  ective. We make use of this result in the Main Study : one group of users receives high-quality suggestions recognised as high quality in this study, while another group of users receives high-quality suggestions that were rated as low quality in this study.
The pilot study had three goals: (i) to test the validity of our system and task setup, (ii) to learn more about ex-perimental factors such as participant fatigue, and (iii) most importantly, to collect qualitative data in order to establish whether participants are able to notice qualities of example queries that make them so e  X  ective as hypothesised in H2 .
The participants (n=22) consisted of university students and sta  X  members recruited via email lists and announce-ments in lectures from a major European university. Al-though the participants were not native English speakers, all had advanced English language skills. They were given access to our search system and asked to complete 10 search tasks. As seen in Figure 3 the information need was promi-nently displayed to the participants. Each time they issued a query (1) , its retrieval e  X  ectiveness was displayed (5) in terms of the number of returned relevant documents within the top 20 results and the average precision (which was re-ferred to as  X  X earch performance score X ). Any relevant doc-uments returned by the search were highlighted in blue (4) .
The participants were instructed to submit queries that they believed would return relevant documents (i.e. useful and containing information pertinent to the task). They were told that the documents had already been evaluated for relevance and that each submitted query would be scored in terms of how many relevant documents were returned in the top 20 results and the positions of those documents within the ranked list. This second score is simply average precision as used during the automatic query generation process and users were encouraged to focus on this to determine how well they were doing in the task. Users could move on to the next topic with a click on the New topic, please button (6) . Due to the interactive nature of the study, we selected 10 of the 50 Robust TREC topics by first eliminating those that that were either very di cult or very easy for our search system (measured in average precision achieved when using the title of the search topic as query) and then drawing randomly from the remaining topics 4 .

The study participants were provided with query sugges-tions as shown in Figure 3 (3) similarly to how Web search
The topics used were 303*, 362, 367*, 375*, 378, 383*, 401, 426*, 638* and 689. * indicates those that were later also used in the Main Study . engines often present query suggestions. After participants have posed their first two queries to the system for a par-ticular topic, they are shown a number of our high-quality query suggestions. All displayed suggestions are more e  X  ec-tive (achieve an AP at least 10% higher) than the partici-pant X  X  previous queries. Thus, di  X  erent participants receive di  X  erent suggestions, depending on the quality of their ini-tial queries. The interface conveys to the participant that these are high-quality queries and they are encouraged to use them (Figure 3 (3) ).

To test hypothesis H2 , i.e. to establish whether users are able to learn from high quality query examples, after every use of a suggestion participants were prompted to describe in a text box why they considered it to be e  X  ective:  X  You used the suggested query [query] .Consideringyourprevious queries for this topic (shown below), what do you think is it about the suggested query that makes it so e  X  ective?  X . The pilot findings help fine-tune our setup for the Main Study . Overall, the setup worked well, however we did es-tablish fatigue to be a considerable factor. Figure 4 plots for each topic in sequence (recall that study participants re-ceive the 10 topics in random order) the AP achieved by all queries submitted for the n th topic across all study partic-ipants. It is evident that over time (i.e. queries submitted for later topics) the retrieval e  X  ectiveness degrades. In par-ticular after the 7 th topic, the median AP is close to zero.
To investigate hypothesis H2 we analysed the free-text explanations from participants describing why they believe the example queries performed so well. The responses show that participants were indeed able to identify positive query characteristics. In total 81 descriptions were supplied and out of the 22 participants, 15 gave at least one description of a suggestion. 3 participants gave descriptions for all of the suggested queries they used.

We analysed the responses qualitatively using an a n-ity diagramming technique, a process allowing the discovery and validation of patterns in qualitative data [16]. 12 codes were generated describing qualities participants assigned to high-performing suggestions. These are shown in Table 1. Table 1: Overview of the query categories identified during the pilot study
Not only does the established coding scheme provide ev-idence that users are capable of noticing and abstracting di  X  erences between the suggested queries and their own -a prerequisite to learning -but the responses given are similar to those reported in the literature as being useful query re-formulation strategies [23] or typical for queries submitted Figure 4: Pilot study: Average precision over se-quences of topics showing fatigue. The n th element of the box plot contains the AP achieved over all queries across all users submitted for the n th topic the users worked on (since topics were issued in ran-domised order, the topic sequence di  X  ers per user). by system and domain experts. For example, a common way to improve queries is to either make them more specfic (C1) or general (C2) [23]. Experts submit queries which are more elaborate [21] (C7, C11, C12), use broader and more var-ied vocabulary [39] (C1, C2), exploit synonyms and related concepts [22](C11), and include terms not used in topic de-scriptions [21]. Moreover, domain experts often search with queries containing specialist or domain knowledge [42] (C9, C12).

We take this as evidence to accept hypothesis H2 . It is important to point out, however, that some of the par-ticipants explicitly mentioned in their responses that they would not be able to create some of the examples due to lack of domain knowledge or vocabulary (C10, C13).
We conclude that, despite the fact that participants are not universally able to recognise good queries (Section 4.2.2), our pilot data show that for many queries people can deter-mine a range of properties that explain good performance.
The main study addresses hypotheses H1 , H3 , H4 &amp; H5 and draws from the outcomes of the two previously discussed user studies.
In this study, we use search topics that our workers in the user perception study considered themselves knowledgeable about to reduce the potential influence of domain knowledge on our results. We base our choice of experimental condi-tions on the reported perceptions of queries to reflect H2 and we reduced the number of tasks to six in an e  X  ort to counteract the fatigue e  X  ect observed in the pilot.
We use a between-groups design with participants ran-domly assigned to one of three experimental conditions:
For groups G exp high and G exp low , where suggestions are given, we split tasks into two phases: the first four top-ics are considered the training phase , where suggestions are shown, and the final two tasks are referred to as the test phase , where no suggestions are presented. Suggestions are provided using the same approach and interface as in the Pi-lot Study , i.e. suggestions were only given after two freely-created queries had been submitted and when there were queries available that would increase the AP score by at least 10%. Again, topics were issued in random order.
The participants (n=91, 29 in G exp high ,34in G exp low and 28 in G control ) were also recruited via CrowdFlower and were paid 50 cents for the completion of a job. A job consisted of using our search system on six ad-hoc retrieval tasks; the study participants were not informed about the two phases of the study, they simply performed six search tasks (after four of which the query suggestion UI element was removed).
We first compare the e  X  ectiveness of the issued queries, before looking at properties of the submitted queries and the fatigue factor.
 Figure 5: Main study: Querying performance over groups. Left: training topics. Right: test topics.
The fairest way to compare the performance across groups is to consider only the first 2 queries submitted by each participant for each topic. Doing so means we only con-sider queries submitted before suggestions are provided for a topic. Kruskal-Wallace rank sum tests show no signifi-cant di  X  erence between the groups on the training topics (p-value=0.320) but a significant di  X  erence for the test topics (p-value=0.002), with both experimental groups ( G exp high and G exp low ) performing significantly better than G control
If we consider all queries submitted for the test topics, not just the first 2 (as now no suggestions are shown to any user group), then these results become even clearer as shown in the top half of Table 2; participants of the G exp high group issue on average queries achieving an AP of 0.10, while par-ticipants of the alternative experimental condition G exp low achieve an AP of 0.06. The control group G control at this stage submits queries which are an order of a magnitude worse, with a mean AP of 0.004.

Figure 5 presents an alternative view of the submitted queries X  e  X  ectiveness across groups; the left boxplot shows the retrieval e  X  ectiveness for the training topics whereas on the right the e  X  ectiveness for the testing topics is shown. It is evident that participants who receive high-quality training suggestions perform better on average, but also that they are able to achieve much higher maximum average precision scores.
 Table 2: Average AP values aggregated across the first two queries of the training topics (column II), the first two queries of the test topics (column III) and all queries submitted for the test topics (column IV).  X  indicates a significant improvement over the G control condition (Kruskal-Wallace rank sum test, p-value  X  0.01).

If we look at how retrieval e  X  ectiveness changes as partic-ipants query more on the same topic, we see a strong trend where G exp high and G exp low continue to improve while those in G control do not (Figure 6). At query position 1 there is very little di  X  erence between the groups; G control is only scoring on average 0.005 worse than G exp high . However, this pat-tern changes quickly with the experimental groups able to achieve steadily more e  X  ective queries the more they submit, which is not the case for the control group G control . By the 4 considerably to 0.135.

These findings provide strong evidence of retrieval e  X  ec-tiveness improvements for the experimental groups over the control group. The analyses so far, however, do not evi-dence a significant di  X  erence in performance gain between
Beyond simply considering the retrieval e  X  ectiveness at-tained by a given query, we can also look at other properties of it that relate to its e  X  ectiveness or quality. These prop-erties (shown in Table 3) go some way towards explaining the observed improvements in performance achieved by both experimental groups. We evaluated the submitted queries with metrics reflecting the literature on expert querying be-haviour (see Section 4.3.2). On many of these metrics the ex-perimental groups G exp high and G exp low significantly outper-form the control group G control . The trend is generally that group G exp high scores highest, group G exp low scores slightly Figure 6: Main study: Average precision over se-quences of queries on test topics. Each point in the plot represents the mean AP of all queries submitted as n th query. Truncated at query 10 as later queries have very few data points associated with them. lower, but often not significantly so, and group G control achieves the poorest scores. Participants in G exp high and G exp low , for example, tended to submit longer queries (in both words and characters), which is noteworthy as the example queries they were shown were designed not to be overly long.

Out of all three groups, participants in G exp high submitted the rarest query terms. We measured this both in terms of the IDF statistics for the collection (i.e. their query terms feature significantly less often in the test corpus as a whole) and in terms of the number of overall participants who sub-mitted those terms (we refer to this as median UserCount-Term in Table 3). Comparing the Jaccard-coe cient scores for query and topic description terms across the experimen-tal conditions reveals that participants of G exp high were also the most likely to take terms from the topic descriptions given to them. These results suggest that a good query cre-ation strategy was to use rare terms and seek inspiration from the topic descriptions, echoing findings from the liter-ature [44]. While this could be negatively construed, since topic descriptions do not exist in real-life and users actu-ally have di culties in describing what they want [38], this finding does not explain the whole picture as there is no sig-nificant correlation (r=0.21) between AP and the overlap of queries with the topic descriptions (Jaccard score).
G exp high participants also submitted significantly more queries per topic than G control participants. However, this is less likely to explain the performance gains as there is no significant di  X  erence in the median number of queries sub-and G control . From the median time per topics it is also evident that G exp high and G exp low spend significantly more time working on each topic than G control .

One factor that could potentially a  X  ect the results is that of fatigue; are groups G exp high and G exp low doing better be-cause they are feeling less fatigued by the task, perhaps as a result of getting some assistance in the early topics or by being shown that high performance was possible and thus in-creasing motivation? There are a number of metrics we can consider to try to ascertain if fatigue is present: the amount of time spent per query (query duration) and the amount of time spent per topic (topic duration) and the number of queries submitted. For all 3 groups the median query dura-tion does seem to decrease slightly over the topics -linear models show a significant negative coe cient over the top-: G issued during the test phase. ics of between -0.6 and -0.99. This is not the case for topic duration, however, as there is no significant trend for any of the 3 groups meaning that they all spend roughly the same amount of time on each topic. The same consistency is also present when looking at the number of queries submitted. There is no significant correlation between topic sequence and number of queries for any of the groups although groups G exp high and G exp low do submit more queries overall. These factors do not point strongly to fatigue being a factor, al-though the subtle changes in query duration do suggest that users are spending less time thinking about each query as time goes on, which may explain the consistent reduction in average precision.
An obvious question to ask, given these results, is what impact does the number of training topics given to the test groups have on performance. A final study investigated to what extent the number of training topics (hypothesis H3 ) influences a user X  X  ability to formulate good queries. Figure 7: Training-size study: Average precision over sequences of queries on test topics. Each point in the plot represents the mean AP of all queries submitted as n th query. Truncated at query 10 as later queries have very few data points associated with them. We used the same setup and experimental design as in the Main Study and varied only the ratio between training and test topics: in this study we used two topics for training, and the remaining four topics for testing. As in the Main Study , participants (n=57, 19 participants in each condition) were recruited via CrowdFlower.
The results from this study were analysed in the same fashion as those from the main study as can be seen in the bottom half of Table 2. The major finding of the Main Study holds in this experiment as well: both experimental groups outperform the control group wrt. e  X  ectiveness. Thus, even a very small amount of training (2 topics) is useful and aids users in learning to formulate better queries.

In contrast to the Main Study , and unsurprising given the lower amount of training, we observe a smaller di  X  erence in retrieval e  X  ectiveness across the test topics: 0 . 05 ( G 0 . 054 ( G exp low ) and 0 . 024 ( G control ) respectively.
These results suggest that some form of learning is tak-ing place and that the relative improvements are smaller if less training is given. They also serve to further highlight the unexpected finding that there is little di  X  erence between G
Hypothesis H1 has been shown to hold -users are in-deed able to adapt their search behaviour to an unfamiliar search system. While G control (which received no training) does not adapt, we clearly see significant changes in query-ing behaviour in both experimental groups (i.e. those who received training).

Our pilot study served to confirm hypothesis H2 ; the study participants were indeed able to determine a set of characteristics that well-performing queries contain. Recog-nising such characteristics is a necessary requirement for learning how to create better queries in general and not just for specific topics.

The main study and the follow-up focusing on the train-ing set size provide evidence for hypothesis H3 . The two experimental groups outperform G control significantly, both when being shown two and four training topics respectively. Thus, even a very small set of training topics is su cient to improve users X  ability to pose good queries.
 Our results do not support H4 . In terms of AP, although Figure 5 hints that G exp high may have outperformed G exp low and the estimated result quality (QUAL). the di  X  erence is not significant. There were some features of the queries that were statistically distinguishable between these groups, but we feel that the evidence is not strong enough to claim that H4 holds.

Finally, based on the evidence in Figures 6 and 7, we have to reject H5 -our participants in both experimental groups had a comparable learning rate (though with di  X  erent abso-lute performance scores).

Previous work has presented mixed evidence for people X  X  ability to accurately determine which query terms will have utility. Our findings suggest this is a complex behaviour. Al-though participants were able to identify positive character-istics of queries shown to be e  X  ective (Section 4.3.2), many high-performing queries were not predicted to be such (Sec-tion 4.2.2). Perhaps these potentially contradictory find-ings indicate a potential systems bias, i.e. do users implic-itly trust suggestions presented by the system as good? Is it only when doubt is introduced by explicitly questioning users about the queries that they perceive suggestions to be potentially not of good quality? What does this mean for the learning e  X  ect? This line of thought opens up many fas-cinating questions of how query suggestions are presented.
Our work has added to the small base of literature demon-strating means for users to learn how to provide higher-quality queries. One limitation of our work has to do with the time period of learning. Our findings support the claim that being shown good suggestions can lead to users learning how to produce better queries, however this is only demon-strated over the period of a session i.e. the test groups achieved better performance for later queries and for later topics. Ideally, however, what we want to show is learning over longer periods of time, such as weeks [28] and months [6] as previous studies have done. This requires a di  X  erent mode of evaluation as crowd-sourcing is not suited to such tasks and represents an important next stage in our project.
A further limitation, with respect to how our findings may be used, is that in a real-life scenario a search system would normally not have access to relevance judgements. This means our method of creating queries cannot typically be applied. We argue that there are situations, though, that may be ideally suited to such an approach. For example in web search we have implicit indicators for di cult tasks (i.e. where better queries might be required) [20] and we also have good models for determining search success based on user behaviour [19]. When such instances combine (i.e. when users are successful in tasks they have been struggling with), this might be the perfect time to present a query sug-gestion, perhaps along the line of X  X he following query would get this page further up the ranking X . Another potential use-case might be to present examples when a user switches context or to a new search-engine where new strategies are required. It has been suggested that users tend not to vary their strategy [29] and our approach might help encourage more diverse or tailored behaviour.
The set of user studies described in this paper have demon-strated that it is possible to use high-quality query exam-ples to influence the queries users submit themselves. We have shown that users can recognise and abstract positive qualities of good queries. Users change the properties of the queries they submit and achieve better retrieval per-formance after seeing good examples for other tasks. Our findings open up a range of interesting questions relating to how query examples should be presented and how this af-fects learning and the influence of learning duration, i.e. is user behaviour influenced over the longer term? Finally is domain knowledge an important factor? We hope to address these issues in upcoming work. [1] P. Anick, Using terminological feedback for web search [2] A. Arampatzis and J. Kamps, Astudyofquerylength , [3] A. Aula, N. Jhaveri, and M. K  X  aki, Information search [4] L. Azzopardi, M. De Rijke, and K. Balog, Building [5] H. Bast and I. Weber, Type less, find more: fast [6] S. Bateman, J. Teevan, and R.W. White, The search [7] N.J. Belkin, Helping people find what they don X  X  know , [8] N.J. Belkin, D. Kelly, G Kim, J-Y Kim, H-J Lee, [9] J.L. Bennett, The user interface in interactive [10] W.B. Croft and R.H. Thompson, I3r: A new approach [11] Z. Dou, R. Song, and J. Wen, Alarge-scaleevaluation [12] D. Elsweiler, Supporting human memory in personal [13] B.M. Evans and E.H. Chi, An elaborated model of [14] K. Franzen and J. Karlgren, Verbosity and interface [15] G.W. Furnas, T.K. Landauer, L.M. Gomez, and S.T. [16] J. Hackos and J. Redish, User and task analysis for [17] D. Harman, Towards interactive query expansion , [18] M. Harvey, F. Crestani, and M.J. Carman, Building [19] A. Hassan, R. Jones, and K. L. Klinkner, Beyond dcg: [20] A. Hassan, R. W. White, S. T Dumais, and Y. Wang, [21] H.A. Hembrooke, L.A. Granka, G.K. Gay, and E.D. [22] I. Hsieh-yee, E  X  ects of search experience and subject [23] B.J. Jansen, D.L. Booth, and A. Spink, Patterns of [24] D. Kelly and X. Fu, Eliciting better information need [25] J. Koenemann and N.J. Belkin, Acaseforinteraction: [26] W. Lucas and H. Topi, Training for web search: Will [27] D. McKay and G. Buchanan, Boxing clever: how [28] N. Moraveji, D. Russell, J. Bien, and D. Mease, [29] J. Nielsen, Incompetent research skills curb users X  [30] H. Oinas-Kukkonen and M. Harjumaa, Towards [31] S.E. Robertson, On term selection for query [32] K. Rodden, I. Ruthven, and R.W. White, Workshop [33] I. Ruthven, Re-examining the potential e  X  ectiveness of [34] I. Ruthven and M. Lalmas, Asurveyontheuseof [35] B. Shneiderman, Dynamic queries for visual [36] B. Smyth, E. Balfe, J. Freyne, P. Briggs, M. Coyle, [37] E. Sormunen, Amethodformeasuringwiderange [38] R.S. Taylor, Question-negotiation and information [39] P. Vakkari, Changes in search tactics and relevance [40] E.M. Voorhees, The trec 2005 robust track , SIGIR [41] E.M. Voorhees, D.K. Harman, et al., Trec: Experiment [42] R.W. White, S.T. Dumais, and J. Teevan, [43] R.W. White and D. Morris, Investigating the querying [44] P. Willett and I. Ruthven, Relevance behaviour in trec ,
