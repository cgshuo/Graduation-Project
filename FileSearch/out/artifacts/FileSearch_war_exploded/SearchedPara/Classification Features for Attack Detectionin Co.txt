 Collaborative recommender systems are highly vulnerable to attack. Attackers can use automated means to inject a large number of biased profiles into such a system, resulting in recommendations that favor or disfavor given items. Since collaborative recommender systems must be open to user input, it is difficult to design a system that cannot be so at-tacked. Researchers studying robust recommendation have therefore begun to identify types of attacks and study mech-anisms for recognizing and defeating them. In this paper, we propose and study different attributes derived from user profiles for their utility in attack detection. We show that a machine learning classification approach that includes at-tributes derived from attack models is more successful than more generalized detection algorithms previously studied. H.3.3 [ Information Storage and Retrieval ]: Informa-tion Search and Retrieval; H.3.4 [ Information Storage and Retrieval ]: Systems and Software; I.2.6 [ Artificial Intelligence ]: Learning Experimentation, Algorithms, Security collaborative filtering, recommender systems, robustness, at-tack detection Research has established the vulnerabilities of recommender systems using collaborative filtering techniques, in the face
This research was supported in part by the National Sci-ence Foundation Cyber Trust program under Grant IIS-0430303.
 Copyright 2006 ACM 1-59593-339-5/06/0008 ... $ 5.00. of what has been termed  X  X hilling X  or  X  X rofile injection X  attacks in which a malicious user enters biased profiles in order to influence the system X  X  behavior [4, 1, 6, 9].
Recent work in this area has focused on detecting and pre-venting profile injection attacks. Chirita et al. [5] proposed several metrics for analyzing rating patterns of malicious users and evaluate their potential for detecting such attacks. Su, et al. [13] developed a spreading similarity algorithm in order to detect groups of similar attackers. O X  X ahony et al. [10] developed several techniques to defend against the attacks described in [6] and [9], including new strategies for neighborhood selection and similarity weight transfor-mations. In our prior work we introduced the attack model-specific approach to profile classification that is more fully explored here. [3, 8].

To detect attacks via pattern classification, known attack models are used to build a training set where standard data mining techniques are used to build a classifier. With such an approach, the closer an attacker mimics a known attack model, the greater the chance of detection. Of course, the attacker may build profiles that deviate from these mod-els and thereby evade detection. However, our most effec-tive attack models are derived by reverse engineering the recommendation algorithms to maximize their impact. We hypothesize, therefore, that they are optimal in the sense of providing maximum impact on the recommender system with the least amount of effort from the attack.

Our detection model is based on the construction of a set of attributes that are calculated for each profile in the database. Supervised learning methods are then used to build classifiers based on these attributes, which are trained to discriminate between genuine profiles and those that are part of an attack. In particular, we are investigating a sim-ple nearest-neighbor classification using kNN. We compare our model-based approach with that described in [5] and demonstrate improved performance, especially for smaller, more difficult to detect, attacks. A profile injection attack against a recommender system consists of a set of attack profiles inserted into the system with the aim of altering the system X  X  recommendation be-havior with respect to a single target item i t . An attack that aims to promote i t , making it recommended more of-ten, is called a push attack , and one designed to make i recommended less often is a nuke attack [9]. Figure 1: The general form of an attack profile.

An attack model is an approach to constructing attack profiles based on knowledge of the recommender system, its rating database, its products, and/or its users. The general form of an attack profile is depicted in Figure 1. The attack profile consists of an m -dimensional vector of ratings, were m is the total number of items in the system. The profile is partitioned in four parts. The null partition, I  X  ,arethose items with no ratings in the profile. The single target item i will be given a rating as determined by the function  X  ,gen-erally this will be either the maximum or minimum possible rating, depending on the attack type. As described below, some attacks require identifying a group of items for special treatment during the attack. This special set I S receives ratings as specified by the function  X  . Finally, there is a set of filler items I F whose ratings are added as specified by the function  X  . It is the strategy for selecting items in I I
F and the functions  X  ,  X  ,and  X  that define an attack model and give it its character.

Two basic attack models, introduced originally in [6] are the random and average attacks. Both of these models in-volve the generation of attack profiles using randomly as-signed ratings given to some filler items in the profile. In the random attack, the assigned ratings are based on the overall distribution of user ratings in the database. In our formalism, I S is empty, the contents of I F are selected ran-domly, and the function  X  generates random ratings centered around the overall average rating in the database. The av-erage attack is very similar, but the rating for each filler item is computed based on more specific knowledge of the individual mean for each item.

Of these attacks, the average attack is by far the more effective, but it may be impractical to mount, given the de-gree of system-specific knowledge of the ratings distribution that it requires. Further, as we show in [2], it is ineffec-tual and hence unlikely to be employed against an item-based formulation of collaborative recommendation. Our own experiments yielded three additional attack models: the bandwagon, segment and love/hate attacks described below. See [4, 1, 2] for additional details.

The bandwagon attack is similar to the random attack, but it uses some additional knowledge, namely the identification of a few of the most popular items in a particular domain: blockbuster movies, for example. This information is easy to obtain and not system-dependent. The set I S contains these popular items and they are given high ratings in the attack profiles. In our studies, the bandwagon attack works almost as well as the more knowledge-intensive average attack.
The segment attack is designed specifically as an attack against the item-based algorithm. Item-based collaborative recommendation generates neighborhoods of similar items, rather than neighborhoods of similar users. The goal of the attack therefore is to maximize the similarity between the target item and the segment items in I S .Thesegment items are those well-liked by the market segment to which the target item i t is aimed. The items in I S are given high ratings to make them similar to the target item, also rated highly in a push attack, and the filler items are given low ratings, making them different from the target item. This attack proved to be highly effective against the item-based algorithm as expected, but it also works well against user-based collaborative recommendation.

Our experiments also showed that the segment attack worked poorly as a nuke attack, since the dislikes of a market segment are more dispersed than its preferences. Our final attack model, the love/hate attack is simple but nonethe-less effective attack against both item-based and user-based algorithms. It associates a low rating with the target item and high ratings with the filler items I F . Our aim is to learn to label each profile as either being part of an attack or as coming from a genuine user using attributes derived from each individual profile. These at-tributes come in two varieties: generic and model-specific. The generic attributes are basic descriptive statistics that attempt to capture characteristics that tend to make an attacker X  X  profile look different from a genuine user. The model-specific attributes are implemented to detect charac-teristics of profiles generated by our attack models. We expect the overall statistical signature of attack profiles will differ significantly from that of authentic profiles. This difference comes from two sources: the rating given the tar-get item, and the distribution of ratings among the filler items. As many researchers in the area have theorized [6, 5, 9, 7], it is unlikely if not unrealistic for an attacker to have complete knowledge of the ratings in a real system. As a re-sult, generated profiles will deviate statistically from those of authentic users. This variance may be manifested in many ways, including an abnormal deviation from the system av-erage rating, or an unusual number of ratings in a profile. As a result, an attribute that captures these anomalies is likely to be informative in identifying attack profiles.
Prior work in attack profile classification has focused on detecting the general anomalies in attack profiles. Chirita et al. [5] introduced several attributes for detecting these differ-ences often associated with attack profiles. One of these at-tributes, Rating Deviation from Mean Agreement (RDMA), was intended to identify attackers through examining the profile X  X  average deviation per item, weighted by the inverse of the number of ratings for that item. We propose two variants of the RDMA attribute which we have found to be valuable as well when used in a supervised learning context.
First, we propose a new attribute Weighted Deviation from Mean Agreement (WDMA) that is strongly based on RDMA, but places higher weight on rating deviations for sparse items. We have found this variant to provide higher information gain. Let U be the universe of all users u in the database. Let P u be a profile for user u , consisting of a set of ratings r u,i for some items i in the universe of items to be rated. Let n u be the size of this profile in terms of the numbers of ratings. Let l i be the number of ratings provided for item i byallusers,and r i be the average of these ratings. The WDMA attribute can be computed as follows:
The second variation of the RDMA measure which we call Weighted Degree of Agreement (WDA) uses only the numerator of the RDMA equation, capturing the sum of the differences of the profile X  X  ratings from the item X  X  average rating divided by the item X  X  rating frequency. It is computed as follows: This .

In addition to rating deviations, some researchers have hypothesised that attack profiles are likely to have a higher similarity with their top 25 closest neighbors than real users would, because they are all being generated using the same process whereas genuine users have preferences that are more dispersed [5, 11]. This hypothesis was confirmed in our ear-lier experiment, which found that the most effective attacks are those in which a large number of profiles with very simi-lar characteristics are introduced. This intuition is captured in the Degree of Similarity with Top Neighbors (DegSim) fea-ture, also introduced in [5].

The DegSim attribute is based on the average similarity of the profile X  X  k nearest neighbors and is calculated as follows: where W u,v is the similarity between users u and v calculated via Pearson X  X  correlation, and k is the number of neighbors.
One well-known characteristic of correlation-based mea-sures is their instability when the number of data points is small. Since it is the number of items co-rated by two users that determines their similarity, this factor can be taken into account and similarity decreased when two users have few items that they have co-rated. This feature is computed as follows. Let I u,v be the set of items i such that ratings exist for i in both profiles u and v ,thatis r u,i and r v,i are defined. |
I u,v | is the size of this set. The similarity of profiles u and v is adjusted as follows: The co-rate factor can be taken into account when calculat-ing DegSim , producing the attribute DegSim .

A third generic attribute that we have introduced is based on the number of total ratings in a given profile. Some attacks require profiles that rate many if not all of the items in the system. If there is a large number of possible items, it is unlikely that such profiles could come from a real user, who would have to enter them all manually. We capture this idea with the measure Length Variance (LengthVar) that is a measure of how much the length of a given profile varies from the average length in the database.
 where n u is the average length of a profile in the system.
In our experiments, we found that the generic attributes are insufficient for distinguishing a true attack profiles from eccentric but authentic profiles. This is especially true when the profiles are small, containing fewer filler items. Such attacks can still be successful, so we seek to augment the generic attributes with some that are designed specifically to match the characteristics of our attack models.
As shown in Section 2, attacks can be characterized based on the features of their partitions i t (the target item), I lected items), and I F (filler items). Model-specific attributes are those that aim to recognize the distinctive signature of a particular attack model. These attributes are based on par-titioning each profile in such a way as to maximize the pro-file X  X  similarity to one generated by a known attack model. Statistical features of the ratings that make up the partition can then be used as detection attributes. One useful prop-erty of partition-based features is that their derivation can be sensitive to additional information (such as time-series or critical mass data) that suggests likely attack targets.
Our detection model discovers partitions of each profile that maximizes its similarity to the attack model. To model this partitioning, each profile is split into two sets. The set P u,T contains all items in the profile with the profile X  X  maximum rating (or minimum in the case of a nuke attack); the set P u,F consists of all other ratings. Thus the intention is for P u,T to approximate { i t } X  I S and P u,F to approximate I . (We do not attempt to differentiate I T from I S .) It is these partitions, or more precisely, their statistical features that we use as detection attributes.
 Average Attack Detection Model. The average attack model divides the profile into three partitions: the target item given an extreme rating, the filler items given other ratings (determined based on the attack model), and un-rated items. The model essentially selects an item to be the target and all other rated items become fillers. By the definition of the average attack, the filler ratings will be pop-ulated such that they closely match the rating average for each filler item. We would expect that a profile generated by an average attack would exhibit a high degree of similarity (low variance) between its ratings and the average ratings for each item except for the single item chosen as the target.
The formalization of this intuition is to iterate through all the highly-rated items, selecting each in turn as the possible target, and then computing the mean variance between the non-target (filler) items and the overall average. Where this metric is minimum, the target item is the one most compat-ible with the hypothesis of the profile as being generated by an average attack, and the magnitude of the variance is an indicator of how confident we might be with this hypothesis. More formally, then, we compute MeanV ar for a profile P u as follows. First we define the set of ratings that are poten-tial targets P u,T = { i  X  P u , such that r u,i = r max } for nuke attacks.). P u,F is the rest of the profile: P u
We compute MeanV ar twice, once where P u,T contains items given the maximum rating, and once where P u,T con-tains items given the minimum rating. Whichever of the calculations yields the lowest value, we consider this the op-timal partitioning for P u,T and P u,F , and the value so com-puted we use as the Filler Mean Variance feature for classi-fication purposes. We also compute Filler Mean Difference , which is the average of the absolute value of the difference between the user X  X  rating and the mean rating (rather than the squared value as in the variance.)
Finally, in an average attack, we would expect that attack profiles would have very similar within-profile variance: they would have more or less similar ratings for the filler items and an extreme value for the target item. So, our third model-derived feature is Profile Variance , simply the vari-ance associated with the profile itself.
 Segment Attack Detection Model. For the segment attack model, the partitioning feature that maximizes the attack X  X  effectiveness is the difference in ratings of items in the P u,T set compared to the items in P u,F . Thus we intro-duce the Filler Mean Target Difference (FMTD) attribute. The attribute is calculated as follows: Target Focus Detection Model. All of the attributes thus far have concentrated on inter-profile statistics; target focus, however, concentrates on intra-profile statistics. Here we are seeking to make use of the fact that a single profile cannot really influence the recommender system. Only a substantial attack containing a number of targeted profiles can achieve this result. It is therefore profitable to exam-ine the density of target items across profiles. One of the advantages of the partitioning associated with the model-based attributes described above is that a set of suspected targets is identified for each profile. For our Target Model Focus attribute (TMF), we calculate the degree to which the partitioning of a given profile focuses on items common to other attack partitions, and therefore measures a consensus of suspicion regarding each profile. To calculate TMF for a profile, first we define F i , the degree of focus on a given item, and then select from the profile X  X  target set the item that has the highest focus and use its focus value. Specifically,
Although the TMF attribute focuses on model target den-sity; it is easy to see how a similar approach could be used to incorporate other evidence of suspicious profiles for example from time series data or unsupervised detection algorithms. This type of attribute could significantly reduce the impact a malicious user could make by constraining the number of profiles they could inject before they risk the detection of their entire attack effort. The results in this paper were generated using the publicly-available Movie-Lens 100K dataset 1 . This dataset consists of 100,000 ratings on 1682 movies by 943 users. All ratings are integer values between one and five where one is the http://www.cs.umn.edu/research/GroupLens/data/ lowest (disliked) and five is the highest (most liked). Our data includes all the users who have rated at least 20 movies.
The attack detection and response experiments were con-ducted using a separate training and test set by partitioning the ratings data in half. The first half was used to create training data for the attack detection classifier used in later experiments. For each test the second half of the data was injected with attack profiles and then run through the clas-sifier that had been built on the augmented first half. This approach was used since a typical cross-validation approach would be overly biased as the same movie being attacked would also be the movie being trained for.

Thetrainingdatawascreatedbyinsertingamixofthe attack models described above for both push and nuke at-tacks at various filler sizes that ranged from 3% to 100%. Specifically the training data was created by inserting the first attack at a particular filler size, and generating the detection attributes for the authentic and attack profiles. This process was repeated 18 more times for additional at-tack models and/or filler sizes, and generating the detection attributes separately. For all these subsequent attacks, the detection attributes of only the attack profiles were then added to the original detection attribute dataset. This ap-proach combined with the average attribute normalizing fac-tor described above, allowed a larger attack training set to be created while minimizing over-training for larger attack sizes (10.5% total across the 19 training attacks).
The segment attack is slightly different from the others in that it focuses on a group of items that are similar to each other and likely to be popular among a subset of users. We have developed several user segments defined by preferences for movies of particular types. These experiments use the Harrison Ford segment (movie s starring Harrison Ford) as part of the training data and the Horror segment (popular horror movies) for attack testing.

For measuring classification performance, we use the stan-dard measurements of precision and recall. Since we are pri-marily interested in how well the classification algorithms detect attacks, we look at each of these metrics with respect to attack identification. Thus precision is calculated as: where # true positives is the number of attack profiles cor-rectly identified as attacks, # false positives is the number of authentic profiles that were misclassified, and # false negatives is the number of attack profiles that were misclassified.
Based on the training data described above, k NN with k = 9 was used to make a binary profile classifier with PA u =0 if classified as authentic and PA u = 1 if classified as attack . To classify unseen profiles, the k nearest neighbors in the training set are used to determine the class using one over Pearson correlation distance weighting. All segment attack results reflect the average over the 6 combinations of Horror segment movies. Classification results and k NN classifier were created using Weka [12].

In all experiments, to ensure the generality of the results, 50 movies were selected randomly that represented a wide range of average ratings and number of ratings. Each of these movies was attacked individually and the average is reported for all experiments.
The Chirita et al. algorithm was also implemented for comparison purposes (with  X  = 10), and run on the test set described above. It computes the probability that a profile u is an attack profile ( PA u ) using an ad-hoc calculation tied to how much a profile X  X  RDMA exceeds the system average. In the comparative results shown in the next section, it should be noted that there are a number of methodological differ-ences between the results reported in [5] and those shown here. The attack profiles in [5] used 100% filler size and targeted 3 items simultaneously. We concentrate on a sin-gle item and vary filler size. Also their results were limited to target movies with low average ratings and few ratings, while the 50 movies we have selected represent both a wider range of average ratings and variance in rating density. Table 1: Information gain for detection attributes. Table 1 shows the attributes described in the previous sec-tion and the information gain calculated for each attribute over the training data. The attributes with the highest gain are those using the  X  X eviation from mean agreement X  con-cept from [5]: WDMA, RDMA, and WDA. The different measures are actually useful at different filler sizes, so none really subsumes the others. The LengthVar attribute is very important for distinguishing high filler sizes, since few real users rate anything close to 100% of the available items. In-terestingly, TMF, which uses our crude measure of which items are under attack, also has strong information gain. This suggests that further improvements in detecting likely attack targets could yield even better detection results.
Figures 2 and 3 compare the detection capabilities of our algorithm using model-specific features with the Chirita al-gorithm for the basic attacks: random and average. For both precision and recall, the model-specific algorithm is domi-nant. Precision is particularly a problem for the Chirita al-gorithm: many false positive identifications are made. How-ever, as the authors point out, this is probably not too sig-nificant since discarding a few real users will not generally impact the system X  X  recommendation performance, and our experiments showed that generally this was true. We also see that the model-specific version has better recall espe-cially a low filler sizes: recall that the Chirita algorithm was tuned for 100% filler sizes, so this is not surprising.
Figures 4 and 5 extend these results to examine the band-wagon and segment attacks. Again a similar pattern is seen. Precision is a bit lower, especially for the segment attack, Figure 2: Classifier precision against 1% average and random attacks.
 Figure 3: Classifier recall against 1% average and random attacks. but recall is extremely high for the model-specific algorithm. Chirita again suffers at low filler sizes. There is an inter-esting dip at 3% filler size, around the average number of ratings per user. The LengthV ar attribute is not useful at this point because the attack profiles do not differ in length from a typical user.

Nuke attack results are shown in Figures 6 and 7. Three attacks are shown: average, random and love/hate. Again, precision is low for the Chirita algorithm and recall results are similar to those seen for the push attacks, except that the love/hate attack proves to be difficult for Chirita to detect at high filler sizes.
Profile injection attacks are a serious threat to the ro-bustness and trustworthiness of collaborative recommender systems and other open adaptive systems. An essential com-ponent of a robust recommender system is a mechanism to detect profiles originating from attacks so that they can be quarantined and their impact reduced.

In this paper, we demonstrate a classification approach to attack detection, introducing a number of detection features based on attack models. We show that classifiers built using these features can detect attacks well to help improve the stability of a recommender under m ost attack scenarios. The segment and love/hate attacks prove to be the most wily opponents. They are the most effective at avoiding detection particularly at low filler sizes. We are continuing to study the problem of detection for these attacks. Figure 4: Classifier precision against 1% bandwagon and segment attacks.
 Figure 5: Classifier recall against 1% bandwagon and segment attacks. [1] R. Burke, B. Mobasher, and R. Bhaumik. Limited [2] R. Burke, B. Mobasher, C. Williams, and [3] R. Burke, B. Mobasher, C. Williams, and R. Bhaumik. [4] R. Burke, B. Mobasher, R. Zabicki, and R. Bhaumik. [5] P. Chirita, W. Nejdl, and C. Zamfir. Preventing [6] S. Lam and J. Reidl. Shilling recommender systems Figure 6: Classifier precision against 1% nuke at-tacks.
 Figure 7: Classifier recall against 1% nuke attacks. [7] B. Mobasher, R. Burke, R. Bhaumik, and C. Williams. [8] B. Mobasher, R. Burke, C. Williams, and R. Bhaumik. [9] M. O X  X ahony, N. Hurley, N. Kushmerick, and [10] M.P. OMahony, N.J. Hurley, and G. Silvestre. [11] P. Resnick, N. Iacovou, M. Suchak, P. Bergstrom, and [12] I. H. Witten and E. Frank. Data Mining: Practical [13] H. Zeng X. Su and Z. Chen. Finding group shilling in
