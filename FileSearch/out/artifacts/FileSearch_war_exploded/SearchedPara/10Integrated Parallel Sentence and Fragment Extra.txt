 In statistical machine translation (SMT) [Brown et al. 1993; Och and Ney 2003; Koehn 2010], because translation knowledge is acquired from parallel corpora (sentence-aligned bilingual texts), the quality and quantity of parallel corpora are crucial. How-ever, currently, high-quality parallel corpora of sufficient size are only available for a few language pairs, such as languages paired with English and several European language pairs. Moreover, even for these language pairs, the available domains are limited. For the rest, composed of the majority of language pairs and domains, only few or no parallel corpora are available. This scarceness of parallel corpora has become the main bottleneck for SMT.

Comparable corpora are a set of monolingual corpora that describe roughly the same topic in different languages but are not exact translation equivalents of each other. Exploiting comparable corpora for SMT is the key to addressing the scarceness of parallel corpora. The reason for this is that comparable corpora are far more available than parallel corpora, and there is a large amount of parallel data contained in the comparable texts.

Previous studies proposed extracting either parallel sentences [Munteanu and Marcu 2005; Smith et al. 2010; S  X tef  X  anescu and Ion 2013; Chu et al. 2014] or fragments [Munteanu and Marcu 2006; Quirk et al. 2007; Aker et al. 2012; Chu et al. 2013b] from comparable corpora based on the comparability of the corpora. The assumption in previous studies is that in comparable corpora with high comparability, there are many parallel sentences, and thus previous studies only focus on parallel sentence extraction from this kind of corpora [Munteanu and Marcu 2005; Smith et al. 2010; S  X tef  X  anescu and Ion 2013; Chu et al. 2014]. However, in comparable corpora with low compara-bility, there are few or no parallel sentences, only parallel fragments in comparable sentences, and thus parallel fragment extraction is more appropriate [Munteanu and Marcu 2006; Quirk et al. 2007; Aker et al. 2012; Chu et al. 2013b; Gupta et al. 2013]. 1
One important fact that most previous studies ignore is that there could be both par-allel sentences and fragments in many comparable corpora. 2 Wikipedia is one typical example of such comparable corpora. In Wikipedia, articles in different languages on the same topic are manually aligned via interlanguage links by the authors, making it a valuable multilingual comparable corpus. However, these aligned articles have var-ious degrees of comparability. Some Wikipedia authors translate the article from one language to another, which produces parallel sentences in these article pairs. Other au-thors write the aligned articles by themselves, thus causing the article pairs to contain few or no parallel sentences but many parallel fragments. Moreover, even the trans-lated article pairs may later diverge because of independent edits in either language, and both parallel sentences and fragments can exist in these article pairs. Figure 1 shows an example of Chinese X  X apanese comparable texts describing a French city  X  X  ` ete X  from Wikipedia, in which both parallel sentences and fragments are contained. Because both parallel sentences and fragments are helpful for SMT, we believe that it is better to extract both of them instead of only focusing on one.

In this work, we exploit the Chinese X  X apanese Wikipedia as a case study. We pro-pose an integrated system to extract both parallel sentences and fragments from the Chinese X  X apanese Wikipedia for SMT. A special characteristic of the Chinese X  Japanese languages is that they share common Chinese characters 3 [Chu et al. 2013a], and we exploit them for both parallel sentence and fragment extraction. The integrated system consists of two major components:  X  Parallel sentence extraction : This follows the method of our previous study [Chu et al. 2014] and is used to identify parallel sentences from comparable sentences. In our previous study, we only focused on extracting parallel sentences from Chinese X 
Japanese Wikipedia, whereas in this study, we further extract parallel fragments from comparable sentences. Our parallel sentence extraction method is inspired by
Munteanu and Marcu [2005] and mainly consists of a parallel sentence candidate filter and classifier for parallel sentence identification. We further developed it in the following two aspects in our previous study [Chu et al. 2014]:  X  X sing common Chinese characters for the filter to solve the domain-dependent  X  X mproving the classifier by introducing Chinese character features together with
The identification of parallel sentences from comparable sentences is based on the classification probability given by the classifier, and we empirically determine the classification probability thresholds for parallel and comparable sentences in our experiments.  X  Parallel fragment extraction : This procedure follows that of our previous study [Chu et al. 2013b] and is used to extract parallel fragments from comparable sentences. In our previous study, we proposed an accurate parallel fragment extraction method. We located parallel fragment candidates using an alignment model and used an accurate lexicon-based filter to identify the truly parallel ones. In this study, we further extend it by using common Chinese characters for the lexicon-based filter to improve its coverage. Our previous study only focused on extracting parallel fragments from a very nonparallel scientific comparable corpus, whereas in this study we extract the parallel fragments from comparable sentences in the Chinese X  X apanese Wikipedia. Experimental results on the Chinese X  X apanese Wikipedia show that both of our pro-posed parallel sentence and fragment extraction methods significantly outperform pre-vious studies, and the integrated extraction of parallel sentences and fragments sig-nificantly improves SMT performance. Our system is language independent, except for the use of common Chinese characters; however, a similar idea can be applied to other language pairs that share cognates. Our system also can be applied to comparable corpora other than Wikipedia. As no previous studies extract both parallel sentences and fragments from comparable corpora in an integrated framework, in this section we describe the related work of parallel sentence and fragment extraction separately. As parallel sentences tend to appear in similar article pairs, many studies first conduct article alignment from comparable corpora and then identify the parallel sentences from the aligned article pairs. Cross-lingual information retrieval technology is com-monly used for article alignment [Utiyama and Isahara 2003; Fung and Cheung 2004; Munteanu and Marcu 2005; Gahbiche-Braham et al. 2011]. Large-scale article align-ment from the Web has been studied as well [Nie et al. 1999; Resnik and Smith 2003; Zhang et al. 2006; Fung et al. 2010; Uszkoreit et al. 2010]. This study extracts parallel sentences from Wikipedia. Wikipedia is a special type of comparable corpora because article alignment is established via interlanguage links. Approaches without article alignment have also been proposed [Tillmann 2009; Abdul-Rauf and Schwenk 2011; S  X tef  X  anescu et al. 2012; Ling et al. 2013]. These studies directly retrieve candidate sentence pairs and select the parallel sentences using various filtering methods.
Parallel sentence identification methods can be classified into two different ap-proaches: classification [Munteanu and Marcu 2005; Tillmann 2009; Smith et al. 2010; Bharadwaj and Varma 2011; S  X tef  X  anescu et al. 2012] and translation similarity mea-sures [Utiyama and Isahara 2003; Fung and Cheung 2004; Fung et al. 2010; Abdul-Rauf and Schwenk 2011]. Similar features such as word overlap and sentence length X  X ased features are used in both of these approaches. We believe that a machine learning approach can be more discriminative with respect to the features, and thus we adopt a classification approach with novel features sets.

Most previous studies use supervised or semisupervised methods that require exter-nal resources in addition to the comparable corpora. These studies differ in their use of a manually created seed dictionary [Utiyama and Isahara 2003; Fung and Cheung 2004; Adafre and de Rijke 2006; Lu et al. 2010], a seed parallel corpus [Zhao and Vogel 2002; Munteanu and Marcu 2005; Tillmann 2009; Smith et al. 2010; Gahbiche-Braham et al. 2011; Abdul-Rauf and Schwenk 2011; S  X tef  X  anescu et al. 2012; S  X tef  X  anescu and Ion 2013; Ling et al. 2013], or link structure and metadata in Wikipedia [Bharadwaj and Varma 2011]. This study uses a seed parallel corpus. An unsupervised method has also been proposed by Do et al. [2010]; however, their method suffers from high computational complexity.

Previous studies extract parallel sentences from various types of comparable corpora, such as bilingual news articles [Zhao and Vogel 2002; Utiyama and Isahara 2003; Munteanu and Marcu 2005; Tillmann 2009; Do et al. 2010; Gahbiche-Braham et al. 2011; Abdul-Rauf and Schwenk 2011], patent data [Utiyama and Isahara 2007; Lu et al. 2010], social media [Ling et al. 2013], and the Web [Nie et al. 1999; Resnik and Smith 2003; Zhang et al. 2006; Ishisaka et al. 2009; Jiang et al. 2009; Fung et al. 2010; Hong et al. 2010]. However, few studies have been conducted to extract parallel sentences from Wikipedia [Adafre and de Rijke 2006; Smith et al. 2010; Bharadwaj and Varma 2011; S  X tef  X  anescu and Ion 2013]. Previous studies are interested in language pairs between English and other languages such as German or Spanish. We focus on Chinese X  X apanese, where parallel corpora are scarce. Munteanu and Marcu [2006] were the first to attempt extraction of parallel fragments from comparable sentences. They extracted subsentential parallel fragments using a log likelihood ratio (LLR) lexicon estimated on a seed parallel corpus and a smoothing filter. They showed the effectiveness of fragment extraction for SMT. Their method has a drawback in that they do not locate the source and target fragments simultaneously, which cannot guarantee that the extracted fragments are translations of each other. We solve this problem by using an alignment model to locate the source and target fragments simultaneously.

Quirk et al. [2007] introduced two generative alignment models to extract paral-lel fragments from comparable sentences. However, the extracted fragments slightly decrease SMT performance when they are appended to in-domain training data. We believe that this is because the comparable sentences are quite noisy, and hence the alignment models cannot accurately extract parallel fragments. To solve this prob-lem, we only use alignment models for parallel fragment candidate detection and use an accurate lexicon-based filter to guarantee the accuracy of the extracted parallel fragments.

In addition to the preceding studies, there are some other efforts. Hewavitharana and Vogel [2011] proposed a method that calculates both the inside and outside prob-abilities for fragments in a comparable sentence pair, showing that the context of the sentence helps fragment extraction. Riesa and Marcu [2012] used a syntax-based align-ment model to extract parallel fragments from noisy parallel data. Gupta et al. [2013] translated a source fragment with an existing SMT system and identified the target fragment by calculating the similarity between the translated source and target frag-ments. Fu et al. [2013] proposed a method that is based on hierarchical phrase-based force decoding. Afli et al. [2013] attempted to extract parallel fragments from multi-modal comparable corpora. Supervised methods have also been proposed for parallel fragment extraction [Aker et al. 2012]. Zhang and Zong [2013] went a step further in that they not only extracted parallel fragments but also estimated translation proba-bilities for the extracted fragments to construct a translation model. Our study differs from these in that it focuses on the task of accurately extracting parallel fragments and the best approach for achieving it. In contrast to some other language pairs, Chinese and Japanese share Chinese char-acters. In Chinese, the Chinese characters are called hanzi , whereas in Japanese, they are called kanji . Hanzi can be divided into two groups: simplified Chinese (used in mainland China and Singapore) and traditional Chinese (used in Taiwan, Hong Kong, and Macau). The number of strokes needed to write characters has been largely re-duced in simplified Chinese, and the shapes may be different from those in traditional Chinese. Because kanji characters originated from ancient China, many common Chi-nese characters exist between hanzi and kanji. We previously created a Chinese char-acter mapping table between traditional Chinese, simplified Chinese, and Japanese [Chu et al. 2013a]. 4 Table I gives some examples of common Chinese characters from that mapping table along with their Unicode.
 Because Chinese characters contain significant semantic information and common Chinese characters share the same meaning, they can be valuable linguistic clues for many Chinese X  X apanese natural language processing tasks. Many studies have exploited common Chinese characters. Tan and Nagao [1995] used the occurrence of identical common Chinese characters in Chinese and Japanese (e.g.,  X  X orld X  in Table I) in an automatic sentence alignment task for document-level aligned text. Goh et al. [2005] detected common Chinese characters where kanji are identical to traditional Chinese but different from simplified Chinese (e.g.,  X  X reeze X  in Table I). Using a Chinese encoding converter 5 that can convert traditional Chinese into simplified Chinese, they built a Japanese-simplified Chinese dictionary, partly using the direct conversion of Japanese into Chinese for Japanese kanji words. We previously made use of the Unihan database 6 to detect common Chinese characters that are visual variants of each other (e.g.,  X  X wo X  in Table I) and show the effectiveness of common Chinese characters in Chinese X  X apanese phrase alignment and Chinese word segmentation optimization for Chinese X  X apanese SMT [Chu et al. 2013a].

We previously investigated the coverage of common Chinese characters on a scientific paper abstract parallel corpus, showing that more than 45% of Chinese hanzi and 75% of Japanese kanji are common Chinese characters [Chu et al. 2013a]. This phenomenon also happens in the case of parallel fragments. Therefore, common Chinese characters can be powerful linguistic clues to identify both parallel sentences and parallel frag-ments. In this study, we exploit common Chinese characters in both parallel sentence and fragment extraction. This study extracts parallel sentences and fragments from the Chinese X  X apanese Wikipedia. The overview of our parallel sentence and fragment extraction system is presented in Figure 2. We first align articles on the same topic in the Chinese and Japanese Wikipedia via the interlanguage links ((1) in Figure 2). Next, we generate all possible sentence pairs using the Cartesian product from the aligned articles and discard the pairs that do not pass a filter that reduces the candidate pairs by keep-ing more reliable sentences ((2) in Figure 2). 7 Then, we use a classifier trained on a small number of parallel sentences from a seed parallel corpus to classify the parallel sentence candidates into parallel and comparable sentences based on the classifica-tion probability 8 given by the classifier ((3) in Figure 2). As the noise in comparable sentences will decrease the SMT performance, we further apply parallel fragment extraction. We use two steps to accurately extract parallel fragments. We first detect parallel fragment candidates using alignment models ((4) in Figure 2). We then filter the candidates using probabilistic translation lexicons to produce accurate results ((5) in Figure 2).

Steps (2), (3), (4), and (5) in Figure 2 form the four main components of our system, which are described in detail in Sections 4.1, 4.2, 4.3, and 4.4. A parallel sentence candidate filter is necessary because it can remove most of the noise introduced by the simple Cartesian product sentence generator and reduce the computational cost of parallel sentence and fragment identification. Previous studies use a filter with sentence length ratio and dictionary-based word overlap conditions [Munteanu and Marcu 2005]. Although the sentence length ratio condition is domain independent, the word overlap condition is not. 9 Wikipedia is an open domain database, and thus using a domain-dependent condition for filtering may decrease the perfor-mance of our system. In the scenario where an open domain dictionary is unavailable, we must search for alternatives that are robust against domain diversity and can effectively filter noise.

Because common Chinese characters are domain independent and an effective way to filter the noise introduced by the simple Cartesian product sentence generator, here we propose using them for the filter. We compared four different filtering strategies: dictionary-based word overlap (Word), common Chinese character overlap (CCO), and their logical combinations. We define them as follows:  X  Word filter : Uses a dictionary-based word overlap.  X  CCO filter : Uses a common Chinese character overlap.  X  Word and CCO filter : Uses the logical conjunction of the word and common Chinese character overlaps.  X  Word or CCO filter : Uses the logical disjunction of the word and common Chinese character overlaps.
 The common Chinese character overlap is calculated based on the Chinese character mapping table in Chu et al. [2013a]. In our experiments, we used a 1-gram common Chinese character overlap with a threshold of 0 . 1 for Chinese and 0 . 3 for Japanese. Note that a same sentence length ratio threshold is used as an additional filtering condition for all four filters. In our experiments, we set the sentence length ratio threshold to two. We compare the performance of the different filtering strategies in Section 5.2.2. Because the parallel and comparable sentences are determined by the classifier, it is the core component of the extraction system. In this section, we first describe the training and testing process, and then introduce the features we use for the classifier. 4.2.1. Training and Testing. We use a support vector machine classifier [Chang and Lin 2011]. Training and testing instances for the classifier are created following the method of Munteanu and Marcu [2005]. We use a small number of parallel sentences from a seed parallel corpus as positive instances. Negative instances are generated by the Cartesian product of the positive instances excluding the original positive instances, and they are filtered by the same filtering method used in Section 4.1. Moreover, we randomly discard some negative instances for training when necessary 10 to guarantee that the ratio of negative to positive instances is less than five for the performance of the classifier. Figure 3 illustrates this process. 4.2.2. Features. In this study, we reuse the features proposed in previous studies (we call these the basic features) and propose three novel feature sets, namely Chinese character (CC) features, non-CC word features, and content word features.
Basic features. The basic features were proposed in Munteanu and Marcu [2005]:  X  X entence length, length difference, and length ratio.  X  X ord overlap X  X he percentage of words on each side that have a translation on the other side (according to the dictionary).  X  X lignment features:  X  X ercentage and number of words that have no connection on each side.  X  X op three largest fertilities.  X  X ength of the longest contiguous connected span.  X  X ength of the longest unconnected substring.
 The alignment features 11 are extracted from the alignment results of the parallel and nonparallel sentences used as instances for the classifier. Note that alignment features may be unreliable when the quantity of nonparallel sentences is significantly larger than the parallel sentences.

CC features. We use the example of a Chinese X  X apanese parallel sentence presented in Figure 4 to explain the CC features in detail using the following features:  X  X umber of Chinese characters on each side (Zh: 18, Ja: 14).  X  X ercentage of characters that are Chinese characters on each side (Zh: 18 / 20 = 90%,
Ja: 14 / 32 = 43%).  X  X atio of Chinese characters on both sides (18 / 14 = 128%).  X  X umber of n-gram common Chinese characters (1-gram: 12, 2-gram: 6, 3-gram: 2, 4-gram: 1).  X  X ercentage of n-gram Chinese characters that are n-gram common Chinese char-acters on each side (Zh: 1-gram: 12 / 18 = 66%, 2-gram: 6 / 16 = 37%, 3-gram: 2 / 14 = 14%, 4-gram: 1 / 12 = 8%; Ja: 1-gram: 12 / 14 = 85%, 2-gram: 6 / 9 = 66%, 3-gram = :2 / 5 = 40%, 4-gram: 1 / 3 = 33%).
 The n-gram common Chinese characters are detected using the Chinese character mapping table in Chu et al. [2013a].

Non-CC word features. Chinese X  X apanese parallel sentences often contain alignable words that do not consist of Chinese characters, such as foreign words and num-bers, which we call non-Chinese character (non-CC) words. Note that we do not count Japanese kana as non-CC words. Non-CC words can be helpful clues to identify parallel sentences. We use the following features:  X  X umber of non-CC words on each side.  X  X ercentage of words that are non-CC words on each side.  X  X atio of non-CC words on both sides.  X  X umber of the same non-CC words.  X  X ercentage of the non-CC words that are the same on each side.

Content word features. The word overlap feature proposed in Munteanu and Marcu [2005] has the problem that function words and content words are handled in the same way. Function words often have a translation on the other side, and thus erroneous parallel sentence pairs with a few content word translations are often produced by the classifier. Therefore, we add the following content word features:  X  X ercentage of words that are content words on each side.  X  X ercentage of content words on each side that have a translation on the other side (according to the dictionary).
 We determine a word as a content or function word using predefined part-of-speech (POS) tag sets of function words for Chinese and Japanese accordingly. 12 Figure 5 shows an example of comparable sentences extracted by our system from Chinese X  X apanese comparable corpora. The alignment results are computed by IBM models [Brown et al. 1993] with symmetrization heuristics [Koehn et al. 2007]. We notice that the truly parallel fragments  X  X rinceton advanced research institute X  and  X  X SA New Jersey State Princeton X  are aligned, although there are some incorrectly aligned word pairs. We believe that this kind of alignment information can be helpful for fragment extraction. However, we need to develop a method to separate the truly parallel fragments from the aligned fragments. Therefore, we propose a two-step paral-lel fragment extraction method. In the first step, we detect parallel fragment candidates using alignment models, and in the second step, we apply a lexicon-based filter to pro-duce accurate fragments. In this section, we describe the parallel fragment candidate detection method.

For alignment, we use the parallel sentences together with the comparable sentences, which can help improve the alignment accuracy for the comparable sentences. We treat the longest spans that have monotonic and non-null alignment as parallel fragment candidates. The reason we only consider monotonic ones is that, based on our obser-vation, the ordering of alignment models on comparable sentences is unreliable. Quirk et al. [2007] also produced monotonic alignments in their generative model. Monotonic alignments are not sufficient for many language pairs. In the future, we plan to develop a method to deal with this problem. The non-null constraint can limit us from extract-ing incorrect fragments. Similar to previous studies, we are interested in fragment pairs with size  X  3. Taking the comparable sentences in Figure 5 as an example, we extract the fragments in dashed rectangles as parallel fragment candidates. The parallel fragment candidates cannot be used directly because many of them are still noisy, as shown in Figure 5. To produce accurate results, we use a lexicon-based filter. We filter a candidate parallel fragment pair with probabilistic translation lexicons. The lexicon pair can be extracted from a seed parallel corpus. However, as described in Section 4.1, the lexicons extracted from a domain-specific seed parallel corpus are domain dependent. Fortunately, we already have the parallel sentences extracted by our system from Wikipedia that are domain independent. Therefore, we append the extracted parallel sentences to a seed parallel corpus to generate the lexicons (referred to hereafter as the combined parallel corpus). 13 Different lexicons may have different filtering effects. Here, we compare the following three types of lexicon:  X  IBM Model 1 : The first lexicon we use is the IBM Model 1 lexicon, obtained by running
GIZA++, 14 which implements the sequential word-based statistical alignment model of the IBM models on the combined parallel corpus.  X  LLR : The second lexicon we use is the LLR lexicon. Munteanu and Marcu [2006] showed that the LLR lexicon performs better than the IBM Model 1 lexicon for parallel fragment extraction. One advantage of the LLR lexicon is that it can produce both positive and negative associations. Munteanu and Marcu [2006] developed a smoothing filter that applies this advantage. We extracted the LLR lexicon from the automatically word-aligned combined parallel corpus using the same method as
Munteanu and Marcu [2006].  X  SampLEX : The last lexicon we use is the SampLEX lexicon. Vuli  X  c and Moens [2012] proposed an associative approach for lexicon extraction from parallel corpora that relies on the paradigm of data reduction. They extract translation pairs from many smaller subcorpora that are randomly sampled from the original corpus, based on some frequency-based criteria of similarity. They showed that their method outper-forms IBM Model 1 and other associative methods such as LLR in terms of precision.
We extracted the SampLEX lexicon from the combined parallel corpus using the same method as Vuli  X  c and Moens [2012].

To gain new knowledge that does not exist in the lexicon, we apply a smoothing filter similar to Munteanu and Marcu [2006]. For each aligned word pair in the fragment candidates, we score the words in both directions according to the extracted lexicon. If the aligned word pair exists in the lexicon, we use the corresponding translation probabilities as the scores. For the LLR lexicon, we use both positive and negative association values. If the aligned word pair does not exist in the lexicon, we set the scores in both directions to  X  1. There is the one exception X  X hen the aligned words are the same, which can happen for numbers, punctuation, abbreviations, and so on. In this case, we set the scores to 1 without considering the existence of the word pair in the lexicon. Note that in Chinese X  X apanese, aligned words can consist of the same common Chinese characters. We make use of our Chinese character mapping table [Chu et al. 2013a] to detect these word pairs. For these word pairs, we also set the scores to 1, and we discuss the effect of this in Section 5.3. After this process, we obtain initial scores for the words in the fragment candidates in both directions.
We then apply an averaging filter to the initial scores to obtain filtered scores in both directions. The averaging filter sets the score of one word to the average score of several words around it. We believe that the words with initial positive scores are reliable because they satisfy two strong constraints, namely their alignment according to the alignment models and existence in the lexicon. Therefore, unlike in Munteanu and Marcu [2006], we only apply the averaging filter to the words with negative scores. Moreover, we add the constraint that we only filter a word when both its immediately preceding and following words have positive scores, which further guarantees accuracy. For the number of words used for averaging, we used five (two preceding words and two following words). The heuristics presented here produced good results on a development set.

Finally, we extract parallel fragments according to the filtered scores . We extract word aligned fragment pairs with continuous positive scores in both directions. Fragments with less than three words may be produced in this process, and we discard them as done in previous studies. Parallel sentence and fragment extraction and translation experiments were conducted on Chinese X  X apanese data. In all of our experiments, we preprocessed the data by segmenting and POS tagging Chinese and Japanese sentences using a tool proposed in our previous study [Chu et al. 2013a] and JUMAN [Kurohashi et al. 1994], respectively.
In this section, we first describe the data used in our experiments. Next, we conduct parallel sentence extraction and translation experiments, which is treated as the base-line in our experiments. We then perform parallel fragment extraction experiments. Finally, we conduct translation experiments using both the parallel sentences and fragments to show the effectiveness of our proposed integrated system. The seed parallel corpus we used is the Chinese X  X apanese section of the Asian Sci-entific Paper Excerpt Corpus (ASPEC). 15 This corpus is a scientific domain corpus provided by the Japan Science and Technology Agency (JST) 16 and the National In-stitute of Information and Communications Technology (NICT). 17 It was created by the Japanese project  X  X evelopment and Research of Chinese X  X apanese Natural Lan-guage Processing Technology, X  and contains 680k sentences (18.2M Chinese and 21.8M Japanese tokens, respectively).
 In addition, we downloaded the Chinese 18 (9/21/2012) and Japanese 19 (9/16/2012) Wikipedia database dumps. We used an open-source Python script 20 to extract and clean the text from the dumps. Because the Chinese dump is a mixture of traditional and simplified Chinese, we converted all traditional Chinese to simplified Chinese using a conversion table published by Wikipedia. 21 We aligned the articles on the same topics in Chinese and Japanese via the interlanguage links, obtaining 162k article pairs (2.1M Chinese and 3.5M Japanese sentences, respectively). We evaluated the classification accuracy and conducted extraction and translation experiments to verify the effectiveness of our proposed parallel sentence extraction method. We also investigated the effect on different classification probability thresholds for parallel sentence identification. 5.2.1. Classification Accuracy Evaluation. We evaluated classification accuracy using two distinct sets of 5k parallel sentences from the seed parallel corpus for training and testing, respectively. For the support vector machine classifier, we used the LIBSVM toolkit [Chang and Lin 2011] 22 with fivefold cross validation and a radial basis function kernel. In this section and in Section 5.2.2, we report the results for a classification probability threshold of 0 . 9, namely we treat the sentence pairs with classification probability  X  0 . 9 as parallel sentences. We address the effect of different thresholds in Section 5.2.3. We used the word alignment tool GIZA++ to generate a dictionary from the seed parallel corpus and calculate the alignment features. For the dictionary, we kept the top five translations with translation probabilities higher than 0 . 1for each source word. 23 Word overlap was calculated based on that dictionary. We report the results using word overlap filtering for easier comparison to previous studies. The word overlap threshold was set to 0 . 25. We compared the following feature settings:  X  Munteanu+, 2005 : The basic features proposed in Munteanu and Marcu [2005] only.  X +CC: Adding the CC features.  X  +Non-CC : Adding the non-CC word features.  X  +Content : Adding the content word features.
 We evaluated the performance of classification by computing the precision, recall, and F-measure, defined as follows: where classified well is the number of pairs that the classifier correctly identified as parallel, classified parallel is the number of pairs that the classifier identified as parallel, and true parallel is the number of actual parallel pairs in the test set. Note that we only used the top result identified as parallel by the classifier for evaluation.
Classification results are shown in Table II. We can see that the Chinese character features can significantly improve the accuracy compared to  X  X unteanu+, 2005. X  Our proposed non-CC word and content word overlap features further improve the accuracy. 5.2.2. Extraction and Translation Experiments. We extracted parallel sentences from Wikipedia and evaluated the Chinese-to-Japanese SMT performance using the ex-tracted sentences as training data. For decoding, we used the state-of-the-art phrase-based SMT toolkit Moses [Koehn et al. 2007] with the default options, except for the dis-tortion limit (6  X  20). We trained a 5-gram language model on the Japanese Wikipedia (10.7M sentences) using the SRILM toolkit 24 with interpolated Kneser-Ney discount-ing. For tuning and testing, we used two distinct sets of 198 parallel sentences. These sentences were randomly selected from the sentence pairs extracted from Wikipedia by our system with different methods, and the erroneous parallel sentences were man-ually discarded 25 because the tuning and testing sets for SMT require truly parallel sentences. Note that for training, we kept all of the sentences extracted by different methods except for the sentences duplicated in the tuning and testing sets. Tuning was performed by minimum error rate training [Och 2003], and it was rerun for every experiment. The other settings were the same as the ones used in the classification experiments described in Section 5.2.1.

Parallel sentence extraction and translation results using different methods are shown in Table III. We report the Chinese-to-Japanese translation results on the test set using the BLEU-4 score [Papineni et al. 2002].  X  X unteanu+, 2005, X   X +CC, X   X +Non-CC, X  and  X +Content X  denote the different features described in Section 5.2.1.  X  X ord, X   X  X CO, X   X  X ord and CCO, X  and  X  X ord or CCO X  denote the four different filtering strate-gies described in Section 4.1.  X  X entences (#) X  denotes hereafter the number of sentences extracted by different methods after discarding the sentences duplicated in the tuning and testing sets, which were used as training data for SMT. For comparison, we also conducted translation experiments using the seed parallel corpus as training data, denoted as  X  X eed. X  The significance test was performed using the bootstrap resampling method proposed by Koehn [2004].

We can see that the Seed system does not perform well. The reason for this is that the Seed system is trained on a seed parallel corpus that is a scientific domain corpus. This differs from the tuning and testing sets that are open domain data extracted from Wikipedia, leading to a high out of vocabulary (OOV) word rate. The systems trained on the parallel sentences extracted from Wikipedia perform better than Seed. This is because they consist of the same domain data as the tuning and testing sets, and the OOV word rate is significantly lower than Seed.

Compared to Munteanu+, 2005, our proposed CC, non-CC word, and content word features improve SMT performance significantly. One reason for this is that our pro-posed features can improve the recall of the classifier, which extracts more parallel sentences and hence causes the OOV word rate to be lower than Munteanu+, 2005. The other reason is that our proposed features improve the quality of the extracted sentences.

The CCO filter shows better performance than the Word filter, indicating that for open domain data such as Wikipedia, using common Chinese characters for filtering is more effective than a domain-specific dictionary. The Word and CCO filter decreases the performance because the number of extracted sentences decreases significantly, leading to a higher OOV word rate. The Word or CCO filter also shows poor performance, and we suspect the reason is the increase of erroneous parallel sentence pairs.
For the best-performing method, +Content with CCO filter, we manually estimated 100 sentence pairs that were randomly selected from the extracted sentences. We found that 64% of them are actual translation equivalents, whereas the other erroneous parallel sentences only contain a small amount of noise. Based on our analysis, the majority of errors occur when one sentence in a sentence pair contains a small amount of extra information that does not exist in the other sentence. These sentence pairs are extracted because most parts are parallel and the classifier gives them relatively high scores. Figure 6 shows some examples of the extracted parallel sentences including some noisy sentence pairs. Because SMT models are robust to this kind of noise, the noisy sentence pairs can also be used to improve SMT performance. For these sentence pairs, it is not necessary to further apply parallel fragment extraction. 5.2.3. Effect on the Classification Probability Threshold. The classifier is used to identify the parallel sentences from comparable sentences in our system, and the classification probability threshold is the criterion. In this section, we investigate the effect of using different thresholds for parallel sentence identification.

In our experiments, we compared the effects of different thresholds from 0 . 1to 0 . 9 in intervals of 0 . 1 and treated the sentences pairs with classification probability greater than or equal to the threshold as parallel sentences. Sentence extraction was performed using the best-performing method, +Content with CCO filter, described in Section 5.2.2. We conducted Chinese-to-Japanese translation experiments using the parallel sentences extracted using different thresholds as training data. The other settings were the same as the ones used in the translation experiments described in Section 5.2.2.

Table IV shows the translation results for different thresholds. We can see that threshold 0 . 9 shows the best performance. When the threshold is lowered, although more sentence pairs are extracted, the SMT performance decreases. The reason for this is that the additional sentences extracted by lowering the threshold are comparable sentences that contain noise, negatively affecting the SMT. We aim to extract the parallel fragments from these comparable sentences to further improve SMT.
In the following section, we treated the sentence pairs with  X 0 . 1  X  classification probability &lt; 0 . 9 X  as comparable sentences, 26 obtaining 169k sentences. We performed parallel fragment extraction from these comparable sentences. We also used the par-allel sentences that were extracted with threshold 0 . 9 to assist the parallel fragment extraction, obtaining 126k sentences. 27 The SMT system trained on these parallel sen-tences is treated as the baseline system in Section 5.4. In our experiments, we compared our proposed fragment extraction method with that of Munteanu and Marcu [2006]. For our proposed method, we applied word alignment using GIZA++ on the comparable sentences together with the parallel sentences de-scribed in Section 5.2.3 for parallel fragment candidate detection. For the lexicon-based filter, different lexicons may have different effects. Therefore, we compared the IBM Model 1, LLR, and SampLEX lexicons, which were all generated from the combined parallel corpus that appends the parallel sentences described in Section 5.2.3 to the seed parallel corpus described in Section 5.1.

The fragment extraction results are shown in Table V. We can see that the av-erage size of the fragments (i.e., the number of words in the fragments) extracted by Munteanu and Marcu [2006] is unusually long, which is also reported in Quirk et al. [2007]. Our proposed method extracts shorter fragments. The IBM Model 1 and LLR lexicons extract more fragments than SampLEX, and the average size is slightly larger. The reason for this is that SampLEX generates a smaller lexicon compared to IBM Model 1 and LLR. Common Chinese characters help to extract more fragments, especially when we use a smaller lexicon (i.e., SampLEX).
To evaluate accuracy, we randomly selected 100 fragments extracted using different lexicons. A more reliable way to evaluate the accuracy is creating a much larger test set that contains a representative sample of data points (i.e., fragments) under scrutiny, and evaluating the precision, recall, and F-measure like Hewavitharana and Vogel [2011]; however, we leave this as future work. We manually evaluated the accuracy based on the number of exact matches. As we evaluated the accuracy manually, the statistical significance could not be evaluated. Note that the exact match criterion has a bias against Munteanu and Marcu [2006], because their method extracts subsenten-tial fragments that are quite long. We found that only six of the fragments extracted by  X  X unteanu+, 2006 X  were exact matches, whereas for the remainder, only partial matches are contained in long fragments. Our proposed method can extract signifi-cantly more exactly matched fragments, whereas the remainders are partial matches. As to the effects of different lexicons, LLR and SampLEX outperform the IBM Model 1 lexicon. We think the reason is the same as the one reported in previous studies: that the LLR and SampLEX lexicons are more precise than the IBM Model 1 lexicon.
We also analyzed the noisy fragment pairs extracted by our proposed method. We found that these noisy pairs are extracted because the lexicon-based filter fails to filter the incorrectly aligned word pairs in the parallel fragment candidates. Most filtering failures are caused by the noisy translation lexicon, and score smoothing also can lead to some failures. Moreover, some filtering failures occur because of both reasons. Table VI shows examples of fragment pairs extracted by our proposed method using the LLR lexicon for the lexicon-based filter. In examples 5 and 6, the noisy parts  X  (begin) X  and  X  (a case particle), X   X  (fusion) X  and  X  (nuclear), X  and  X  (Japan) X  and  X  (,) X  are extracted because they are incorrectly aligned by the alignment model and exist in the translation lexicon. In example 7,  X  (academic) X  and  X  (reference) X  are incorrectly aligned, but they do not exist in the translation lexicon; thus, the initial score of this word pair is  X  1. However, after smoothing, the score becomes positive, and thus this noisy pair is extracted. In example 8,  X  (rank) X  and  X  (inauguration) X  is a noisy translation lexicon pair and incorrectly aligned. Furthermore,  X  (rank) X  and  X  (a case particle) X  are also incorrectly aligned, but they do not exist in the translation lexicon. However, after smoothing the score becomes positive, causing this noisy fragment pair.

Based on this analysis, we think that to further improve the accuracy, first, a more efficient alignment model should be used for parallel fragment candidate detection to decrease the number of incorrectly aligned word pairs. Second, the effectiveness of the lexicon-based filter should be further improved. Using a more accurate translation lexicon is the key to improving the lexicon-based filter because the effectiveness of smoothing also highly depends on the accuracy of the translation lexicon. Further cleaning of the noisy translation pairs is a possible way to achieve this [Aker et al. 2014]; however, we leave this as future work. We conducted Chinese-to-Japanese parallel sentence and fragment integrated trans-lation experiments by appending the extracted fragments to a baseline system. The baseline system used the parallel sentences described in Section 5.2.3 as SMT training data. The other settings were the same as the ones used in the translation experiments described in Section 5.2.2.

We report the translation results on the test set using BLEU-4 [Papineni et al. 2002]. The results of the Chinese-to-Japanese translation experiments are shown in Table VII. For comparison, we also show the translation results of the baseline sys-tem (labeled  X  X aseline X ) and the system that appends the extracted comparable sen-tences to the baseline system (labeled  X +Comparable sentences X ). The significance test was performed using the bootstrap resampling method proposed by Koehn [2004]. We can see that appending the extracted comparable sentences and fragments ex-tracted by Munteanu and Marcu [2006] has a negative impact on translation qual-ity. Our proposed method outperforms the Baseline, +Comparable sentences, and Munteanu+, 2006 methods, indicating the effectiveness of our proposed integrated ex-traction system and our proposed method for extracting useful parallel fragments for SMT.

We compared the phrase tables produced by different methods to investigate the reason for the different SMT performances. We found that all methods increased the size of the phrase table, meaning that new phrases are acquired from the extracted data. The sizes are larger for the Comparable sentences and Munteanu+, 2006 methods than they are for our proposed method because these methods extract more data, leading to lower OOV word rates. However, the noise contained in the data extracted by the Comparable sentences and Munteanu+, 2006 methods produces many noisy phrase pairs, which may decrease SMT performance. Our proposed method extracts accurate parallel fragments, leading to correct new phrases. The LLR lexicon shows the best performance because it extracts more accurate fragments than IBM Model 1 and extracts both more and larger parallel fragments than SampLEX.

The parallel sentences described in Section 5.2.3, the parallel fragments extracted by the best method of LLR, and the tuning and testing sets used in the translation ex-periments are available at http://lotus.kuee.kyoto-u.ac.jp/  X  chu/wiki_zh_ja/data.tar.gz. Extracting parallel data from comparable corpora is an effective way to solve the scarceness of parallel corpora that SMT suffers. Previous studies extract either par-allel sentences or fragments from comparable corpora. In this article, we proposed an integrated system to extract both parallel sentences and fragments from compara-ble corpora to improve SMT. We first applied parallel sentence extraction to identify parallel sentences from comparable sentences. We then extracted parallel fragments from the comparable sentences. Moreover, we proposed novel methods to improve the parallel sentence and fragment extraction components in our system. Experiments con-ducted on the Chinese X  X apanese Wikipedia verified the effectiveness of our proposed system and methods.

As future work, because our study showed that common Chinese characters are helpful for both Chinese X  X apanese parallel sentence and fragment extraction, we plan to apply a similar idea to other language pairs by using cognates. Moreover, in this article, we only conducted experiments on Wikipedia. Our proposed system is expected to work well on other comparable corpora where both parallel sentences and fragments trend to appear, such as bilingual news articles, social media, and the Web. We plan to do experiments on these comparable corpora to construct a large parallel corpus for various domains.

