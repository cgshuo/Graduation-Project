 We study the dynamics of information propagation in environments of low-overhead personal publishing, using a large collection of WebLogs over time as our example domain. We characterize and model this collection at two levels. First, we present a macroscopic characterization of topic propagation through our corpus, formal-izing the notion of long-running  X  X hatter X  topics consisting recur-sively of  X  X pike X  topics generated by outside world events, or more rarely, by resonances within the community. Second, we present a microscopic characterization of propagation from individual to individual, drawing on the theory of infectious diseases to model the flow. We propose, validate, and employ an algorithm to induce the underlying propagation network from a sequence of posts, and report on the results. Over the course of history, the structure of societies and the rela-tions between different societies have been shaped to a great ex-tent by the flow of information in them [11]. More recently, over the last 15 X 20 years, there has been interest not just in observing these flows, but also in influencing and creating them. Doing this requires a deep understanding of the macro-and micro-level struc-tures involved, and this in turn has focused attention on modeling and predicting these flows.
 The mainstream adoption of the Internet and Web has changed the physics of information diffusion. Until a few years ago, the major barrier for someone who wanted a piece of information to spread through a community was the cost of the technical infrastructure required to reach a large number of people. Today, with widespread access to the Internet, this bottleneck has largely been removed. In this context, personal publishing modalities such as weblogs have become prevalent.
 Weblogs, unlike earlier mechanisms for spreading information at the grassroots level, offer the opportunity for direct, frequent, and low-cost observation of information flow at the individual level. This in turn enables applications that were not previously possible. Given the huge sums of effort and money spent by corporations and political organizations to spread their message, timely feedback and monitoring are vital to maximizing the impact of a marketing, polit-ical, or other campaign. On the other side, users are inundated with organizations clamoring for their attention. We want to leverage the blogging community to identify newsworthy events, as evidenced by spikes in postings of the relevant communities.
 We are interested in the dynamics of information propagation in environments of low-overhead personal publishing, such as web pages, Weblogs, bulletin boards, and netnews. We focus in this pa-per on Blogspace , the space of all weblogs. Of course, personal publishing doesn X  X  occur in isolation. It is influenced by, and in-fluences, the older more mainstream media sources. Thus, in our analysis, we include both Weblog postings and news articles from sources such as Reuters and the AP Newswire.
 An obvious course of analysis of blogspace would be based on the link structure manifested in blogrolls and such. We posit that blogspace exhibits distinct structures when examined at different temporal granularities. At a coarse granularity, we find the kind of structure described in [22]. Our focus is not so much on this struc-ture of blogspace as in the diffusion of information, as reflected in who influences whom, which is a much more dynamic structure. In doing this, we find that traditional media sources such as Reuters and AP (who do not typically appear in blogrolls) still have an enor-mous influence. Thus, we believe that our study applies more gen-erally to the diffusion of information in environments of personal publishing and not just to blogspace.
 There are many dimensions along which information diffusion can be characterized. In this paper, we explore the following: Topics: We are interested in first identifying the set of postings that Individuals: Though the advent of personal publication gives ev-The propagation of information has been studied extensively in the context of gossiping and broadcasting [18] in networks of a variety of forms, but the focus of that literature is essentially algorithmic in nature. Here, we are interested in models of information dispersion in which nodes in the network may or may not be interested in the information, and thus may or may not pass along the information to their neighbors.
 The problem of understanding diffusion through a population has been studied in a number of communities, ranging from thermo-dynamics to epidemiology to marketing. Maxwell and others were the first to provide a rigorous analysis of this problem, in the con-text of thermodynamics. In that and subsequent work, statistical mechanics has looked at various models for the diffusion of par-ticles of one gas in an other. Though this setting is superficially different, we find much that we can borrow from that field if we look at information as a kind of particle. Much previous research investigating the flow of information through networks has been based upon the observation of a deep analogy between the spread of disease and the spread of information in net-works. The analogy between infection and information allows one to bring results of centuries of study of epidemiology to bear on questions of information diffusion. (See, for example, the book of Bailey [4] for some of the extensive work in this field.) The classical disease-propagation models in epidemiology are based upon the cycle of disease in a host: a person u is first susceptible (S) to the disease, and, if u is then exposed to the disease by an infectious contact, then u herself becomes infected (I) (and infec-tious ) with some probability p . The disease then runs its course in host u , and u is then recovered (R) (or removed , depending on the virulence of the disease). A recovered individual is immune to the disease for some period of time, but the immunity may eventu-ally wear off, leaving u once again susceptible. Thus SIR models diseases in which recovered hosts are never again susceptible to the disease X  X s with a disease conferring lifetime immunity, like chicken pox, or a highly virulent disease from which the host does not recover X  X hile SIRS models the situation in which a recov-ered host eventually becomes susceptible again X  X s with influenza, e.g. An important parameter of a network is its epidemic threshold : what is the minimum transmission probability  X  so that a disease spreads to infect a constant fraction of the network if a single seed node is initially infected? (In the model we consider in Section 5.2, unlike the typical model in epidemiology, the transmission proba-bility p = p ( u, v ) varies from edge to edge in the network.) In blogspace, one might interpret the SIRS model as follows: ini-tially, person u is not interested in topic x , but may become inter-ested (S); u is actively interested in and posting on topic has tired of topic x and is no longer posting on it (R); and forgotten her boredom, and now may potentially become interested in topic x again (S). For example, Girvan et al. [13] study a SIR model with mutation , in which a node u is immune to any strain of the disease which is sufficiently close to a strain with which was previously infected. They observe that (with appropriate set-tings of parameters) it is possible to generate periodic outbreaks, where the disease oscillates between periods of epidemic outbreak and periods of calm while it mutates into a sufficiently new form that it can cause another major outbreak. In blogspace, one could imagine a blogger writing about Arnold qua movie star, growing bored of the topic, and then, after the topic of Arnold has evolved sufficiently, beginning to blog again about Arnold qua governor. (We observe this kind of ebb and flow in the popularity of various  X  X piky chatter X -type memes. See Section 4.2.1 .) The majority of the epidemiology literature, including the work of Girvan et al. [13], focuses on the case of  X  X ully mixed X  or  X  X o-mogeneous X  networks, in which a node X  X  contacts at any time step are chosen randomly from all other nodes in the population X  X .e., there is no underlying network defining the contacts of each node. More recently, as the importance of network structure has become more clear, studies have begun to explore disease and information propagation on models of realistic networks.
 In a model of small-world networks defined by Watts and Strogatz [31], Moore and Newman [24] calculate the epidemic threshold. However, this model does not account for some interesting and seemingly very important properties of real networks. A power-law network is one in which the probability that the degree of a node is k is proportional to k  X   X  , for a constant tween 2 and 3 . Power laws have been observed in many important real-world networks [23], including the social network defined by blog-to-blog links [22]. We now review some previous research on epidemic spreading on networks that follow a power law.
 Pastor-Satorras and Vespignani [28] analyze an SIS model of (com-puter) virus propagation in power-law networks, showing that X  X n stark contrast to random or regular networks X  X he epidemic thresh-old is zero . (In other words, for any probability  X  &gt; 0 transmission across an edge of the network, an epidemic will oc-cur!) The epidemic threshold of power-law networks has also an interpretation in terms of the robustness of the network to random edge failure. Suppose that each edge in the network is deleted in-dependently with probability (1  X   X  ) ; we consider the network  X  X o-bust X  if most of the nodes are still connected. It is easy to see that nodes that remain in the same component as some initiator the edge deletion process are exactly the same nodes that according to the disease transmission model above. This question has been considered from the perspective of error tolerance of net-works like the Internet: what happens to the network if a random (1  X   X  ) -fraction of the links in the Internet fail? Many researchers have observed that power-law networks exhibit extremely high er-ror tolerance [2; 7].
 These results suggest that modeling information dispersion in blogs using this kind of transmission model is insufficient, since this falsely predicts that almost every node in the network will become  X  X nfected X  with a topic if there is a non-zero probability of pick-ing up a topic from a neighbor. One refinement is to consider a more accurate model of power-law networks. Egu  X   X luz and Klemm [12] have demonstrated a non-zero epidemic threshold under the SIS model in power-law networks produced by a certain generative model that takes into account the high clustering coefficient  X  X he proportion of triangles that are  X  X losed, X  i.e., the probability that two people v and w will be friends if they have a common friend u  X  X ound in real social networks [31].
 One can also resolve this discrepancy by a modification to the model of transmission. Wu et al. [33] consider the flow of information through real and synthetic email networks (generated according to a power-law distribution) under a model in which the probability that a node u will forward a meme to a neighbor v of u decays as the graph distance d ( s, u ) from the original source node s increases. (The model is inspired by the observation of homophily in social networks: a person is biased towards having friends with similar interests to her own.) They observe that meme outbreaks under this model are typically limited in scope X  X nlike in the cor-responding model without decay, where the epidemic threshold is zero X  X xactly as one observes in real data. Newman et al. [27] have also empirically examined the simulated spread of email viruses by examining the network defined by the email address books of a user community. The spread of a piece of information through a social network can also be viewed as the propagation of an innovation through the net-work. (For example, the URL of a website that provides an new, valuable service is such a piece of information.) Thus we can speak of bloggers adopting a topic t , analogous to adopting a new tech-nology like, for example, blogs themselves.
 In the field of sociology, there has been extensive study of the dif-fusion of innovation in social networks, examining the role of the process of word of mouth in spreading innovations. At a particular point in time, some nodes in the network have adopted the innova-tion, and others have not. Two fundamental models for the process by which new nodes adopt have been considered in the literature: In the Independent Cascade model of Goldenberg, Eitan, and Muller [14], we are given a set of N nodes, some of which have already adopted. At the initial state, some non-empty set of nodes are  X  X cti-vated. X  At each successive step, some (possibly empty) set of nodes become activated. The episode is considered to be over when no new activations occur. The set of nodes are connected in a directed graph with each edge ( u, v ) labeled with a probability p node u is activated in step t , each node v that has an arc is activated with probability p u,v . This influence is independent of the history of all other node activations. (If v is not activated in that time step, then u will never activate v .) The General Cas-cade model of Kempe, Kleinberg, and Tardos [19] generalizes the Independent Cascade model X  X nd also simultaneously generalizes the threshold models described above X  X y discharging the inde-pendence assumption.
 Kempe et al. are interested in a related problem on social networks with a marketing motivation: assuming that innovations propagate according to such a model, and given a number k , find the nodes S  X  innovation if S  X  a product to S  X  One of the challenges in any study involving tens of thousands of publishers is the tracking of individual publications. Fortunately for us, most of the publishers, including the major media sources, now provide descriptions of their publications using RSS ( rich site summary , or, occasionally, really simple syndication ) [20]. RSS, which was originally developed to support the personalization of the Netcenter portal, has now been adopted by the weblog commu-nity as a simple mechanism for syndication. In the present work, we focus on RSS because of its consistent presentation of dates X  X  key feature for this type of temporal tracking.
 Our corpus was collected by daily crawls of 11,804 RSS blog feeds. We collected 2K X 10K blog postings per day X  X undays were low, Figure 1: Number of blog postings (a) by time of day and (b) by day of week, normalized to the local time of the poster. Wednesdays high X  X cross these blogs, for a total of 401,021 post-ings in our data set. (Each posting corresponds to an  X  X tem X  entry in RSS.) Complementing this, we also crawled 14 RSS channels from rss.news.yahoo.com hourly, to identify when topics were be-ing driven by major media or real-world events, as opposed to aris-ing within blogspace itself. The blog entries were stored as par-ent/child entities in WebFountain [32] and analyzed with a half-dozen special-purpose blog annotators to extract the various date formats popular in RSS, convert to UTF8, detag, etc.
 See Figure 1 for the profile of blog postings within a day and from day-to-day, normalized by the poster X  X  time zone. The most fre-quent posting is at 10AM. There is a pronounced dip at 6 and 7PM (the commute home? dinner? Must-See-TV?), an odd plateau be-tween 2 and 3AM and a global minimum at 5AM. Posting seems to peak midweek, and dips considerably on weekends. In this section, we explore the topics discussed in our data. We differentiate between two families of models: (i) horizon models, which aim to capture the long-term changes (over the course of months, years, or even decades) in the primary focus of discussion even as large chatter topics X  X ike Iraq and Microsoft, as of this writing X  X ax and wane; and (ii) snapshot models, which focus on short-term behavior (weeks or months) while the background  X  X hatter X  topics are assumed to remain fixed. This paper explores snapshot models; we do not address horizon models, but instead raise the issue as an interesting open problem. To support our goal of characterizing topic activity, we must first find and track topics through our corpus. The field of topic detec-tion and tracking has studied this problem in depth for a number of years X  X IST has run a series of workshops and open evalua-tion challenges [30]; see also, for example, [3]. Our requirements are somewhat different from theirs; we require schemes that pro-vide views into a number of important topics at different levels (very focused to very broad), but rather than either high precision or high recall, we instead require that our detected set contain good representatives of all classes of topics. We have thus evaluated a range of simple techniques, chosen the ones that were most effec-tive given our goals, and then manually validated different subsets of this broader set for use in particular experiments.
 Our evaluations of these different techniques revealed some unex-pected gaps in our intuition regarding blogspace; we give a brief walkthrough here. First, we treated references to particular web-sites as topics, in the sense that bloggers would read about these  X  X nteresting X  sites in another blog and then choose to write about them. However, while there are over 100K distinct links in our corpus, under 700 of them appear 10 times or more X  X ot enough to chart statistically significant information flows. Next, we con-sidered recurring sequences of words using sequential pattern min-ing [1]. We discovered under 500 such recurrent sequences, many of which represented automatically generated server text, or com-mon phrases such as  X  X  don X  X  think I will X  and  X  X  don X  X  understand why. X  We then turned to references to entities defined in the TAP ontology [16]. This provided around 50K instances of references to 3700 distinct entities, but fewer than 700 of these entities oc-curred more than 10 times. The next two broader sets provided us with most of the fodder for our experiments. We began with a naive formulation of proper nouns: all repeated sequences of up-percase words surrounded by lowercase text. This provided us with 11K such features, of which more than half occurred at least 10 times. Finally, we considered individual terms under a ranking de-signed to discover  X  X nteresting X  terms. We rank a term t ratio of the number of times that t is mentioned on a particular day i (the term frequency tf ( i ) ) to the average number of times mentioned on previous days (the cumulative inverse document fre-quency). More formally, tfcidf ( i ) = ( i  X  1) tf ( i ) / P Using a threshold of tf ( i ) &gt; 10 and tfcidf ( i ) &gt; 3 roughly 20,000 relevant terms.
 All features extracted using any of these methods are then spot-ted wherever they occur in the corpus, and extracted with metadata indicating the date and blog of occurrence. To understand the structure and composition of topics, we man-ually studied the daily frequency pattern of postings containing a large number of particular phrases. We analyzed the 12K individ-ual words most highly ranked under the tfcidf ranking described above. Most of these graphs do not represent topics in a classi-cal sense, but many do. We hand-identified 340 classical topics, a sample of which is shown in Table 1.
 Next, based on our observations, we attempt to understand the structure and dynamics of topics by decomposing them along two orthogonal axes: chatter , internally driven, sustained discussion; and spikes , externally induced sharp rises in postings. We then re-fine our model by exploring the decomposition of these spikes into subtopics, so that a topic can be seen as the union of chatter and spikes about a variety of subtopics. Figure 2: Three types of topic patterns: the topic  X  X hibi X  (green) is Just Spike ;  X  X icrosoft X  (blue) is Spiky Chatter ; and  X  X lzheimer X  X  X  (red) is Mostly Chatter . There is a community of bloggers interested in any topic that ap-pears in postings. On any given day, some of the bloggers express new thoughts on the topic, or react to topical postings by other blog-gers. This constitutes the chatter on that topic.
 Occasionally, an event occurring in the real world induces a reac-tion from bloggers, and we see a spike in the number of postings on a topic. Spikes do not typically propagate through blogspace, in the sense that bloggers typically learn about spikes not from other blogs, but instead from a broad range of channels including main-stream media. Thus, we can assume all informed authors are aware of the topical event and have an opportunity to write about it. On rare occasions, the chatter reaches resonance , i.e., someone makes a posting to which everyone reacts sharply, thereby caus-ing a spike. The main characteristic of resonance is that a spike arises from either no external input or a very small external input. The formation of order (a spike) out of chaos (chatter) has been observed in a variety of situations [29], though observation of our data reveals that this happens very rarely in blogspace. In fact, the only sustained block re-posting meme that we observed in our data consisted of the  X  X occdrnig to rscheearch at an elingsh uinervtisy it deosn X  X  mttaer in waht oredr the ltteers in a wrod are, the olny iprmoetnt tihng is taht the frist and lsat ltteer is at the rghit pclae X  story which came out of nowhere, spiked and died in about 2 weeks (with most postings over a four-day period).
 Depending on the average chatter level and pertinence of the topic to the real world, topics can be roughly placed into one of the fol-lowing three categories, with examples shown in Figure 2: Just Spike: Topics which at some point during our collection win-Spiky Chatter: Topics which have a significant chatter level and Mostly Chatter: Topics which were continuously discussed at rel-Spiky Chatter topics typically have a fairly high level of chatter, with the community responding to external world events with a spike; their persistent existence is what differentiates Spiky Chat-ter from spikes. They consist of a superposition of multiple spikes, plus a set of background discussion unrelated to any particular cur-rent event. For example, the Microsoft topic contains numerous spikes (for example, a spike towards the end of our window around a major announcement about Longhorn, a forthcoming version of Windows) plus ongoing chatter of people expressing opinions or offering diatribes regarding the company and its products. In this section, we refine our model of Topic = Chatter + Spikes by examining whether the spikes themselves are decomposable. Intuitively, the community associated with a topic can be seen as randomly choosing a subtopic and posting about it. When an ex-ternal world event occurs, it is often particular to something very specific X  X hat is, a subtopic X  X specially for complex topics. In this section, we consider a subtopic-based analysis using the spikes in the complex, highly posted topic  X  X icrosoft X  as a case study. Microsoft was especially appropriate for this analysis, as several Microsoft-related events occurred during the collection of our data set, including the announcement of blog support in Longhorn. We used a multi-step process to identify some key terms for this ex-periment. First, we looked at every proper noun x that co-occurred with the target term  X  X icrosoft X  in the data. For each we com-pute the support s (the number of times that x co-occurred with the target) and the reverse confidence c r := P ( target | x ) Thresholds for s and c r were manipulated to generate rational term sets. As is common with these cases, we do not have a hard-and-fast support and confidence algorithm, but found that s in the range of 10 to 20 and c r in the range of 0 . 10 to 0 . 25 worked well. For the target  X  X icrosoft, X  this generates the terms found in Table 2. Of course, this is not a complete list of relevant subtopics, but serves rather as a test set. For these terms, we looked at their occurrences, and defined a spike as an area where the posts in a given day ex-ceeded  X  + 2  X  . We then extended the area to either side until a local minimum less than the mean was reached. We refer to posts during these intervals as spike posts .
 Now, having identified the top coverage terms, we deleted spike posts related to one of the identified terms from the Microsoft topic. The results are plotted in Figure 3. The de-spiked posts line shows a considerable reduction in the spikes of the Microsoft graph, with minor reduction elsewhere. Note that even in the spiky area we Figure 3: The topic density for posts on Microsoft, both before and after spike removal. Table 3: Top coverage spike terms for Windows. Terms on a grey background are also spike terms for Microsoft (Table 2). are not getting a complete reduction, suggesting we may not have found all the synonymous terms for those spike events, or that subtopic spikes may be correlated with a latent general topic spike as well.
 This analysis in no way implies that the topics in Table 2 are atomic. We also explored the subtopic  X  X indows X  X  X ne of the subtopics with better coverage X  X nd looked at its decomposition. The proper noun selection was performed as before, generating the term set in Table 3. There is some duplication of terms from Table 2, as the topics  X  X icrosoft X  and  X  X indows X  overlap significantly. However, some terms unique to Windows appear, especially the comparison to Apple (Apple, Steve Jobs, Quicktime, Mac, Macs, Macintosh). Applying these terms to the Windows posting frequency, we see the results in Figure 4. Again, we see a similar reduction in spikes, indicating that we have found much of the spiky behavior of this topic. As might be expected with a more focused topic, the top 24 spike terms have better coverage for  X  X indows X  than for  X  X i-crosoft, X  leaving a fairly uniform chatter.
 This case study strongly supports our notion of a spike and chatter model of blog posting. While not presented here, similar behavior was observed in a number of other topics (terrorism, Linux, the California recall election, etc.). Having presented a qualitative decomposition of topics into chatter and spikes, we now present measurements to quantify the nature of these spikes. Each chatter topic can be characterized by two parameters corresponding to the chatter level (distribution of the number of posts per day) and the spike pattern (distribution of the frequency, volume, and shape of spikes).
 To perform these evaluations, we hand-tagged a large number of topics into the categories given in Section 4.2.1 . Of those hand-Figure 4: The topic density for posts on Windows, both before and after spike removal.
 Figure 5: Distribution of spike duration and period within chatter topics. tagged topics, 118 fell into the chatter category; we performed this characterization study on those topics. We used the simple spike definition of Section 4.2.2 to determine where the spikes occurred in each chatter topic; an examination of the spikes found by this algorithm led us to believe that, while simple, it indeed captures our intuition for the spikes in the graph.
 To begin, the average number of posts per day for non-spike re-gions of our collection of chatter topics ranges between 1.6 to 106. The distribution of non-spike daily average is well-approximated by Pr[ average number of posts per day &gt; x ]  X  ce  X  x .
 Next, we focus on characteristics of spike activity. Figure 5 shows the distribution of spike durations and periods. Most spikes in our hand-labeled chatter topics last about 5 X 10 days. The median pe-riod between spike centers is about two weeks.
 Figure 6 shows the distribution of average daily volume for spike periods. The median spike among our chatter topic peaks at 2.7 times the mean, and rises and falls with an average change of 2.14 in daily volume. We have covered the high-level statistical  X  X hermodynamic X  view of the data in terms of aggregates of posts at the topic level; now Figure 6: Average daily volume of spikes within chatter topics. we turn to a view more akin to particle dynamics, in which we attempt to uncover the path of particular topics through the various individuals who make up blogspace. We begin in Section 5.1 by characterizing individuals into a small number of classes, just as we did for topics in the previous section. Next, in Section 5.2 we formulate a model for propagation of topics from person to person through blogspace, and we present and validate an algorithm for inducing the model. Finally, we apply the model to real data, and give some preliminary applications.
 Our model is akin to traditional models of disease propagation, in which individuals become  X  X nfected X  by a topic, and may then pass that topic along to others with whom they have close contact. In our arena, close contact is a directed concept, since a may read the blog of b , but not vice versa. Such a model gives a thorough under-standing of how topics may travel from person to person. Unfortu-nately, we do not have access to direct information about the source that inspired an author to post a message. Instead, we have access only to the surface form of the information: the sequence in which hundreds, thousands, or tens of thousands of topics spread across blogspace. Our algorithm processes these sequences and extracts the most likely communication channels to explain the propaga-tion, based on the underlying model. We begin with a quick sense of the textual output of our users. Figure 7 shows the distribution of the number of posts per user for the duration of our data-collection window. The distribution closely approximates the expected power law [23].
 We now wish to classify these users. We adopt a simple set of predicates on topics that will allow us to associate particular posts with parts of the lifecycle of the topic. Given this information, we will ask whether particular individuals are correlated with each section of the lifecycle. The predicates are defined in the context of a particular time window, so a topic observed during a different time window might trigger different predicates. See Table 6 for the definitions of these predicates.
 Table 4 shows the fraction of topics that evince each of these re-gions. We can then attempt to locate users whose posts tend to appear in RampUp, RampDown, MidHigh, or Spike regions of topics. However, we must exercise caution in tracking this cor-respondence: for example, we wish to avoid capturing users who simply happened to post more frequently during the early part of our data-collection window, and thus are more likely to post during regions identified as RampUp by our predicates. To overcome this difficulty, we consider the probability p i that a post on day into a given category (e.g., RampUp). For any given user, we then consider the pair ( t i , c i ) of total posts on day i and posts in the category on day i , respectively. The total number of posts in the category is C = P contributes the same number of posts each day, but does so without bias for or against the category. The expected number of posts in the category for the random user is then P dom user produces a sum of independent random variables, each of which is simply a series of Bernoulli trials with some bias depend-ing on the day, we can determine the probability that the random user would produce C or more posts in the category, and therefore determine the extent to which we should be surprised by the be-havior of the given user. We set our threshold for surprise when the number of occurrences is more than three standard deviations beyond the mean of the random user.
 Using this technique, we give the number of users who are unusu-ally strong contributors to each region in Table 5. In some cases, as for the Up region, the numbers are relatively low, but the total number of posts in the region is also quite small. The correlation is quite strong, leading us to suggest that evaluating broader defini-tions of a  X  X amp up X  phase in the discussion of a topic may identify a larger set of users correlated with this region. For regions such as Mid or Spike, the number of associated users is quite substan-tial, indicating that there are significant differing roles played by individuals in the lifecycle of a topic. We derive our formal model from the Independent Cascade model of Goldenberg et al. [14] and generalized to the General Cascade Predicate Algorithm Region
RampUp All days in first 20% of
RampDown All days in last 20% of
MidHigh All days during middle
Spike For some day, number of Model by Kempe et al. [19]. We are given a set of N nodes, corre-sponding to the authors. At the initial state of each episode, some (possibly empty) set of nodes have written about the topic. At each successive state, some (possibly empty) set of authors (including possibly some who have already written before) write about the topic. The episode is considered to be over when no new articles appear for some number of time steps, the Timeout Interval . With the Independent Cascade Model, the set of authors are con-nected in a directed graph with each edge labeled with a probability. When author v writes an article at time t , each node w that has an arc from v to it writes an article about the topic with the probabil-ity  X  ( v, w ) , the copy probability . This influence is independent of history whether any other neighbors of w have written an article. The General Cascade Model can be seen as generalizing this by eliminating the assumption of independence.
 We introduce the notion that a user may visit certain blogs fre-quently, and other blogs infrequently; we capture this with an edge property r u,v , denoting the probability that u reads v on any given day. We also introduce the notion of stickiness of a topic, Formally, propagation in our model occurs as follows. If a topic exists at vertex u on a given day, then we compute the probability that it will propagate from u to a neighboring vertex v as follows. Node v reads the topic from node u on any given day with reading probability r u,v , so we choose a delay from an exponential distri-bution with parameter r u,v . With probability S , the stickiness of the topic, the topic will  X  X tick X  with v . And finally, with probabil-ity  X  u,v , the copy probability , the author of v will choose to write about it. If v reads the topic and it does not stick, or is not copied, then v will never choose to copy that topic from u ; there is a single opportunity for the topic to propagate down any given edge. Alternately, one may imagine that once u is infected, v will become infected with probability S X  u,v r u,v on any given day, but once the r u,v coin comes up heads, no further trials are made.
 Thus, given the transmission graph (and, in particular, the reading frequency r and the copy probability  X  for each edge), and given the stickiness S of a particular meme, the distribution of propaga-tion patterns is now fully established. Given a community and a timeout interval, our goal is therefore to learn the arcs and associ-1 Stickiness of a topic is analogous to virulence in the disease prop-agation literature.
 ated probabilities from a set of episodes. Using these probabilities, given a new episode, we would like to estimate the stickiness of the new episode from an initial fragment of the episode. Then, we would like to be able to predict the propagation pattern that will be associated with the episode.
 We now present a few possible extensions to the model: In the following, we make a closed world assumption that all occur-rences of a topic other than the first one are the result of communi-cation via edges in the model. As described above, this assumption can be weakened by introducing an  X  X utside world X  node with ap-propriate parameters into the model.
 A topic in the following is a URL, phrase, name, or any other representation of a meme that can be tracked from page to page. We gather all blog entries that contain a particular topic into a list [( u blog, where u i is the universal identifier for blog i , and time at which blog u i contained a reference to the topic. We refer to this list as the traversal sequence for the topic.
 We shall make critical use of the following observation: we wish to induce the relevant edges among a candidate set of  X ( n 2 and we have only limited data, but the fact that blog a appears in a traversal sequence, and blog b does not appear later in the same sequence gives us evidence about the ( a, b ) edge X  X hat is, if a regular reader of a  X  X  blog with a reasonable copy probability, then sometimes memes discussed by a should appear in b  X  X  blog. Thus, we gain information from both the presence and absence of entries in the traversal sequence.
 We present an iterative algorithm to induce the transmission graph. Assume that we have an initial guess at the value of ( r,  X  ) edge, and we wish to improve our estimate of these values. We adopt a two-stage process: Step 1: Using the current version of the transmission graph, com-Step 2: For fixed u and v , recompute ( r,  X  ) based on the posterior We are given as input the traversal sequence for a particular topic. For each v in the sequence, we consider all previous vertices the sequence, and compute Pr( u  X  v ) , the probability that the topic would have traversed from u to v given the delay between and v in the sequence. We then normalize by the sum of these prob-abilities to compute posteriors over all nodes u of the probability that each was v  X  X  source of inspiration. That is, setting  X  =  X  u,v , and  X  to be the delay in days between u and v In practice, for efficiency reasons, we consider only the 20 values of w closest to v , and require propagation to occur within 30 days. We perform the following operation for each fixed u, v . First, we require a sequence S 1 of triples ( p,  X , s ) , each correspond-ing to some topic appearing in u and then v , where p is the posterior probability that the topic traveled from u to v as computed above,  X  is the delay in days between the appearance of the topic in in v , and s is the stickiness of the topic. We also require a sequence S of pairs ( X  , s ) for topics with stickiness s in which u v did not appear later in the sequence, and  X  days elapsed between the appearance of u and the end of our snapshot.
 We can then estimate an updated version of r,  X  as follows: where Pr[ a  X  b ] = (1  X  a )(1  X  (1  X  a ) b ) is the probability that a geometric distribution with parameter a has value  X  b . We now have an improved guess at the transmission graph, so we can return to step 1 and recompute posteriors, cycling through the process until convergence. During step 1, we use our model of the graph to guess how data traveled. During step 2, we use our guess about how data traveled to improve our model of the graph. For our data sets, the values of r and  X  converge within between 2 and 5 iterations, depending on the data, to a vector of values within 1% of the limiting value under the L 2 norm. In order to validate the algorithm, we created a synthetic series of propagation networks, ran each synthetic network to generate ob-servable sequences of infection by particular topics, and then ran our mining algorithm to extract back the underlying propagation network. The synthetic graphs are modified Erd  X  os-Renyi random graphs: 2 a number of vertices n is fixed, as is a target degree benchmarks based on power law random graphs [6; 21].
 Table 7: Mean and standard deviation for r and  X  in low-traffic synthetic benchmark. Correct values:  X  = 0 . 66 ,  X  = 0 . 1 Each vertex selects d out-neighbors uniformly with replacement from the vertex set; all parallel edges and self-loops are then re-moved. Each edge is then given a ( r,  X  ) value; we used and  X  = 1 / 10 for our tests.
 We began with a series of graphs with n = 1000 and d = 3 . For such graphs, we seeded a number of topics at each vertex, ranging from 20 to 60. Due to the small value of  X  , we saw on average between 2 and 6 topics originating from each vertex. We consid-ered only edges that were traversed by at least three topics with probability at least 0.1. We then compared the resulting edge set against the edge set from the original propagation network. An edge was counted as erroneous if it appeared in only one of those two graphs X  X n other words, in this benchmark we penalize for both missing edges and unnecessary edges. Of 3000 edges, the algorithm requires little data to infer the correct edges: once it saw 6 topics per node on average, it correctly inferred 2663 of the 3000 edges, plus 4 erroneous additional edges. For this benchmark, the algorithm converges in two iterations. The mean and standard de-viation of the inferred values of r and  X  for this experiment are shown in Table 7.
 Next, we turn to a propagation model with higher degrees in which topics tend to take off and propagate throughout the graph, making it more difficult to learn exactly how the information had traveled. The parameters are n = 500 , d = 9 , and we take 20 topics per node. Topic sizes range from 1 to slightly over 200. The estimated r values have mean 0 . 73 and standard deviation 0 . 12 ; the have mean 0 . 08 and standard deviation 0 . 03 . The system identifies almost all relevant edges (to within 1% ), and identifies a further almost 9% spurious edges due to the more complex structure of this task. Thus, both the edges and the estimated parameters of the edges are very close to the underlying model. Now that we have validated the algorithm on synthetic data, we val-idate the model itself against our data. We run the graph induction algorithm as described above on all the ProperName sequences in our dataset. As we have seen, roughly 20% of these sequences con-tain spikes, and fewer than 10% contain RampUp and RampDown areas. So the dataset consists of both signal and noise. Rather than introducing a  X  X eal world X  node to modeling communication through the general media, we restrict our attention to topics for which at least 90% of the occurrences are in blogspace, rather than in our RSS media content. This focuses on about 7K topics. To validate that the model has in fact discovered the correct edges, we performed two experiments. First, we downloaded the top 100 blogs as reported by http://blogstreet.com . Of the 100 blogs, 70 of them were in our RSS-generated dataset. We then used the model to rank individual nodes of the network based on the amount of traffic flowing through those nodes. Of the 70 nodes in our dataset, 49 were in the top 10% of blogs in our analysis; 40 were in the top 5%, and 24 were in the top 1.2%.
 As a second validation, we ranked all edges in the final model by the expected number of topics that flowed down the edge, and pro-Figure 8: Distribution of Inverse Mean Propagation Delay ( Copy Probability (  X  ).
 duced the top 200. We hand-examined a random sample of this set, and in 90% of the cases were able to find a link between the two blogs. Note that we were able to make use of the structure of blogspace in the discovery of these links (i.e., blogrolls, and userids appearing inline), while the algorithm did not have access to these mechanisms, and made its determinations based on topics alone. Figure 8 shows the distributions of r and  X  as learned by the algo-rithm on the approximately 7K topics described above. Most edges have an expected propagation delay ( 1 /r ) of fewer than 5 days; the mean is 0.28 and the standard deviation is 0.22. Copy probabilities are quite low, with mean 0.04 and standard deviation 0.07, indicat-ing that even bloggers who commonly read from another source are selective in the topics they choose to write about.
 Figure 9 shows the distribution of expected traffic along each edge; i.e., over the set of 11K given topics, for a particular edge how many times does b read about something on a and conse-quently write about it? The iteration converges to about 4000 edges with traffic. Popular edges might have 50 expected copies; the me-dian edge has 1 X 2 total expected messages that traverse it. Blogspace, by virtue of its fine grained observability, offers a fertile testbed for developing and testing models of information diffusion, especially through the medium of personal publishing. In this pa-per, we showed how by using macro (topical) and micro (individ-ual) models, various structures and behaviors can be understood, ranging from the strong driving effect of outside world events on what is being discussed to the applicability of traditional socio-logical models of influence to bloggers. Employing such charac-terizations allows applications to take advantage of these rapidly emerging web phenomena. [1] Rakesh Agrawal and Ramakrishnan Srikant. Mining sequen-[3] J. Allan, editor. Topic Detection and Tracking: Event-based [4] Norman Bailey. The Mathematical Theory of Infectious Dis-[5] Venkatesh Bala and Sanjeev Goyal. A strategic analysis of [6] Albert-L  X  aszl  X  o Barab  X  asi and R  X  eka Albert. Emergence of scal-[7] B  X  ela Bollabas and Oliver Riordan. Robustness and vulner-[8] Duncan S. Callaway, M. E. J. Newman, Steven H. Stro-[9] Reuven Cohen, Keren Erez, Daniel ben Avraham, and [10] Paolo Crucitti, Vito Latora, Massimo Marchiori, and Andrea [11] Jared Diamond. Guns, Germs, and Steel . Random House, [12] V  X   X ctor M. Egu  X   X luz and Konstantin Klemm. Epidemic thresh-[13] Michelle Girvan, Duncan S. Callaway, M. E. J. Newman, [14] Jacob Goldenberg, Barak Libai, and Eitan Muller. Talk of the [15] Mark Granovetter. Threshold models of collective behavior. [16] R. V. Guha and Rob McCool. TAP: A system for integrating [17] Hans Haller and Sudipta Sarangi. Nash networks with het-[18] Sandra M. Hedetniemi, Stephen T. Hedetniemi, and Arthur L. [19] David Kempe, Jon Kleinberg, and  X  Eva Tardos. Maximiz-[20] Andrew King. The evolution of RSS. http://www. [21] R. Kumar, P. Raghavan, S. Rajagopalan, D. Sivakumar, [22] Ravi Kumar, Jasmine Novak, Prabhakar Raghavan, and An-[23] M. Mitzenmacher. A brief history of lognormal and power law [24] Cristopher Moore and M. E. J. Newman. Epidemics and [25] Stephen Morris. Contagion. Review of Economic Studies , 67, [26] M. E. J. Newman. The spread of epidemic disease on [27] M. E. J. Newman, Stephanie Forrest, and Justin Balthrop. [28] Romauldo Pasto-Satorras and Alessandro Vespignani. Epi-[29] Steven Strogatz. Sync: The emerging science of spontaneous [30] Topic Detection and Tracking (TDT-2003). http://www. [31] D. Watts and S. Strogatz. Collective dynamics of  X  X mall-[32] WebFountain. http://www.almaden.ibm.com/ [33] Fang Wu, Bernardo A. Huberman, Lada A. Adamic, [34] H. Peyton Young. The diffusion of innovation in social net-
