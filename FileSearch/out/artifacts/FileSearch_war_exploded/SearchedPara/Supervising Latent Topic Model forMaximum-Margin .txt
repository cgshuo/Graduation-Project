 With tremendous text information made available online, there is a growing demand to analyze and manage large corpuses of electronic text. Learning low-dimensional se-mantic representations of text documents is a common and often necessary step for various applications and text analyses. For example, this low-dimensional semantic rep-resentation has been used for structurally browsing a text corpus and categorizing and clustering text documents in information retrieval domain.

A recent trend in learning low-dimensional s emantic representations focuses on gen-erative latent probabilistic models based on so-called topics . The belief behind those latent topic models is that a document consisting of a large number of words might be concisely described as a smaller number of semantic themes. A topic is a probability distribution over words of a vocabulary, and is used to statistically describe a semantic theme. Then, a document is s emantically represented as a mixture of topics.
In the literature, most popular latent topi c models are Latent Dirichlet Allocation (LDA) [2] and Probabilistic Latent Semantic Analysis (pLSA) [5]. Particularly, LDA is a Bayesian version of pLSA. Both models are unsupervised to simultaneously dis-cover topics and low-dimensional topic representations of documents, and have been successfully used in various applications [7,3].

Unfortunately, semantic representations produced in this unsupervised manner may not necessarily be good features for classification and regression tasks. The reason is that topics are learned without considering document labels to be predicted, for exam-ple the categories of news postings. Those unsupervised learned topics describe seman-tic themes that generally happen in all documents and don X  X  describe semantic themes discriminative across document categories. Therefore, semantic representations of doc-uments based on general topics are not well distinctive against document categories and those two-step approaches of building predictors subsequently on them would result in sub-optimal performance. We believe that topics should contain as much discriminative information as possible from document labels such that semantic representations based on them are suitable for prediction.

In this paper, we propose an approach to integrate the latent topic model for learning low-dimensional semantic representations and support vector machine or regression for training max-margin predictors into one single learning objective. By coupling them together, we are able to supervise the latent topic model with benefits of the maximum margin principle, and guide it to discover topics describing discriminative information and generate semantic repres entations more suitable for prediction tasks. Due to the op-timization hardness of the single learning objective, we propose a tight variational upper bound for the single learning objective and develop a novel step-wise convex algorithm for optimizing the upper bound. For both classification and regression tasks, we present experiments showing that our approach can achieve better predictive power and is able to discover much lower dimensional representations than two-step approaches and also three state of art methods. We first introduce notations that will be used throughout the paper; then review latent topic models and max-margin classifier and regressor.
 a fixed vocabulary with totally V words. Following a common bag-of-word assumption, we represent this document as a bag of words. The document could have a response label y , which is either a categorical class, or a continuous real number. Let D be a corpus of M labeled documents. The problem in this work is to learn a good predictor using D as training data for predicting the response label of a new document. 2.1 Latent Topic Model To represent a document by semantic topics, we define a K -topic vocabulary T . Each
This topic representation implies a gene rative process to documents. For each word n in a document d ,we 1. draw a topic assignment z n |  X   X  Mult (  X  ) ,where z n has the 1-of-K representation; Both LDA and pLSA are based on the above generative process. The difference is that LDA introduces a Dirichlet prior to  X  for alleviating the potential overfitting problem of pLSA. For classification and regression tasks, both models show no difference in prediction performance. Ther efore, to keep our approach simple, we recruit pLSA as the topic model in our approach.

Given a corpus D with M documents, pLSA minimizes the following negative log likelihood to learn topics  X  and also estimate topic proportion  X  for each document: 2.2 Max-margin Classification and Regression The empirical topic proportion  X  z m from the latent topic model and the document label m are used to build max-margin predictors, for example support vector machine (SVM) [4] for classification and support vector regressor (SVR) [10] for regression. from the corpus D for classification by minimizing the following loss function: where the first term measures the classification error of &lt; X ,b&gt; and the second is a penalty term on  X  to avoid over-fitting. Similarly, if y m is continous, we can learn a SVR &lt; X , b&gt; with a pre-defined precision from Corpus D for regression by minimizing the following loss function: where the first term measures the prediction error of &lt; X , b&gt; on the precision, and the second is a penalty term on  X  too.
 Because  X  z m is a hidden variable, we need to minimize the expected two loss functions integrable max function in two loss functions. To circumvent this difficulty, two-step form  X  , this alternative makes possible using standard SVM and SVR solver. spectively according to Jensen X  X  inequality . It is obvious that mi nimizing lower bounds As discussed in the introduction, the goal is to learn semantic topics  X  describing dis-criminative themes among documents in the corpus D such that representations of doc-uments by those discriminative topics could be suitable for prediction purpose.
To achieve this goal, two requirements should be satisfied together. On one hand, topics  X  should be common to make possible a to-be-predicted document be well de-scribed by them too. This suggests that the negative log likelihood of the latent topic On the other hand, the empirical topic proportions  X  Z can be used to build good pre-SVM and SVR, which measure how well they are used for prediction, should be mini-mized too.

To satisfy both requirements together, we intuitively couple them linearly by a posi-tive tradeoff  X  to a single learning objective. For classification and regression tasks, we need to minimize the following singl e learning objectives respectively: Therefore, in the single learning objectives , the expected losses of max-margin predic-tors are used to penalize or supervise the latent topic model to generate discriminative topics suitable for prediction. We name this framework maximum margin latent topic model, shortly denoted as MMpLSA .

To solve those two learning problems, we have to remove or integrate out hidden variables z m,n so that convex optimization could be applied to them.
 the multinomial distribution of z m,n . We have the following alternative form of the log likelihood function: respectively.
 non-integrable max functions involved in them. One way to circumvent this closed same problem as two step approaches: minimizing lower bounds of single learning ob-jectives doesn X  X  guarantee they will be small too.
 objectives are approximately minimized too.
 3.1 Variational Upper Bounds for Expected Max-margin Loss Functions where  X  is a variational variable which gives the upper bound one degree of freedom to tightly approximate the max function.

Based on the variational upper bound of max function, we then give the variational Armed with the variational upper bound of the expected classification loss function proposed in the previous section, we can approximately minimize the single learning objective for classification (Eq. 1) by minimizing its following upper bound: parameter  X  in the variational upper bound of the exp ected classification loss. Because must be satisfied in this minimization.
 fore, we could iteratively minimize it with respect to one of variables with the rest of variables fixed. Because every iteration redu ces its overall value, this iterative mini-mization procedure will cause the value of objective upper bound to converge to a local minimum. Next, we describe this iterative procedure below starting from the simplest iterating step: O
PTIMIZE  X  : Because variational variables ar e uncoupled to each other in the objec-tive upper bound, we can divide the optimization for  X  into M subproblems, one per variational variable. There is only one term involving the variational variables in the objective upper bound. Therefore, the objective upper bound is simplified to the below for optimizing each variational variable: O and b and also all constraints don X  X . With them dropped, the optimization for &lt; X ,b&gt; is simplified to the following unconstrained optimization problem: which tries to choose &lt; X ,b&gt; for good prediction. The Hessian matrix of &lt; X ,b&gt; is: Hessian matrix involves  X  , which is supposed to be large enough for well approximating the max function as shown in Sec 3.1. But when  X  is big, the Hessian matrix could be ill-conditioned, which will lead to the instability of our algorithm solving this optimization problem. Our stable solution is that we first solve the minimization problem for a small to update optimal  X  and b , and so on. In implementation, we start  X  from 10 and increase it by 20 until it reaches 200 .
 O
PTIMIZE  X  : Topic proportion  X  m s are uncoupled to each other. Therefore, we can di-vide the optimization for  X  into M subproblems, one per topic proportion. By dropping the third term of objective upper bound without involving  X  and constraints on  X  ,the optimization for  X  is simplified to the following constrained optimization problem: which involves  X  too and has the same ill-conditioned problem when  X  is large as the O
PTIMIZE  X  : Only the first term of objective upper bound involves  X  . By keeping it and also constraints on  X  , we simplify the optimization for  X  to the following constrained optimization problem: cation is given in Algorithm 1. We discuss the implementation detail in the next section. 4.1 Implementation In the beginning of the optimi zation procedure, for each topic t , we initialize its multino-we also initialize each value of  X  and b by sampling a standard normal distribution.
Every time when the optimization procedure optimizes  X  and  X  and b ,wedocross preselected for cross validation is estimated by minimizing the negative log likelihood with  X  fixed: Similar to classification, by utilizing the variational upper bound of the expected re-gression loss function, we can approximately minimize the single learning objective for regression (Eq. 2) by minimizing its following upper bound: Due to space limitation, please refer to [11] for the detail of the optimization procedure, which shares many commons with the optimization procedure for classification. In this section, we conducted experiments to evaluate our apporach MMpLSA with state of art methods and also a baseline from two-step approaches; we report extensive per-formance results on both text classification and regression. Our experiments are able to demonstrate the advantages of applying the max-margin principle to supervise la-tent topic models. MMpLSA can learn from data a compacter latent representation that contains more plentiful information for prediction. 6.1 State of Art Approaches There have been moderate efforts on supervis ing latent topic models for classification and regression in the literature. The most earliest work is sLDA [1], which supervises LDA for regression by assuming a normal distribution of the response y of a document class after a linear transformation should be nearby to each other. Parameters of linear transformations are learned by maximizing the conditional likelihood of the response classes. But DiscLDA can X  X  guarantee that topic proportions of different classes after linear transformations are well separat ed, which is critical for classification.
Applying the max-margin principle to supervise latent topic models could avoid drawbacks of sLDA and DiscLDA. For example, SVR doesn X  X  require document la-bels to be normally distributed and SVM could help forcing topic proportions of dif-ferent classes to be well separated by a good margin. The most recent MedLDA [13] is an approach utilizing the max-margin principle. However, MedLDA uses the lower bounds of expected SVM and SVR loss functions to supervise latent topic models. It ex-tremely simplifies inference algorithms, but it is problematic as we discuss in Section 3. Our approach MMpLSA also recruits the max-margin principle to supervise latent topic models. But different to MedLDA, we propose tight variational upper bounds of ex-pected loss functions. Based on upper bounds, we develop a stepwise convex algorithm for optimization, totally different to EM algorithms used by those existing approaches. 6.2 Text Classification To be able to compare MMpLSA with DiscLDA and MedLDA, we also evaluated MM-pLSA on the 20 Newsgroups dataset containing postings to Usenet newsgroups. As Dis-cLDA and MedLDA, we formulated the same classification problem for distinguishing postings from two newsgroups: alt.atheism and talk.relgion.misc , a hard task due to the content similarity between them. We also used the training/testing split provided in the 20 Newsgroups dataset to make possible a fair comparison among them.

To obtain a baseline from two-step approaches, we first fit all the data to pLSA model, and then used empirical topic proportions as features to train a linear SVM for prediction. This baseline is denoted as pLSA+SVM for the rest of section.

For both pLSA+SVM and MMpLSA , 30% of training postings were randomly chosen for cross validation. For the number of topics from 2 to 10, we ran the experiment five times and report accuracies in the Fig. 1. We can observe that MMpLSA performs much better than unsupervised pLSA+SVM. In other words, supervising the latent topic model can discover discriminative topics for better classification.

We further compared MMpLSA with MedLDA and DiscLDA. Lacoste-Julien et al.[8] reported that DiscLDA achieves best accuracy 83.0% at 60 topics. Zhu et al. [13] didn X  X  report the accuracy of MedLDA, but re ported the relative improvement ratio of MedLDA against a two-step approach. The best relative improvement ratio is around 0.2, achieved at 20 topics. We show results of comparison between MMpLSA and them in the Fig. 2.

Fig. 2 (Left) reports both the accuracy mean of five runs of MMpLSA and the accu-racy of the run with best cross validatio n against the best accuracy of DiscLDA. We can see that when the number of topics is small, MMpLSA is noticeably better than Dis-cLDA. MMpLSA achieved the best accuracy 84.7% with 3 topics, a 2% relative accuracy improvement and a 95% relative number of topics reduction. Therefore, compared with DiscLDA, the max-margin principle used by MMpLSA helps in discovering much fewer topics but with more discriminative information. However, when the number of topics increased, the performance of MMpLSA downgraded. The possible reason is that dis-criminative information is limited and using more than necessary topics to describe it could cause over-fitting.

Fig. 2 (Right) illustrates relative improvement ratio of accuracy mean of MMpLSA against pLSA+SVM v.s. best relative improvement ratio of MedLDA achieved at 20 topics. MMpLSA is better than MedLDA in all cases. It suggests that MMpLSA has ad-vantages of learning more discriminative topics by using the upper bound of expected classification loss in optimization not the lower bound as MedLDA.
 6.3 Text Regression To compare MMpLSA with sLDA and MedLDA on regression, we evaluated MMpLSA on the public available movie review dataset [9], in which each review is paired with a
To obtain a baseline from two-step approaches, we first fit training reviews to pLSA model, and then used empirical topic proportions as features to train a linear SVR . We denote this baseline as pLSA+SVR.

Following sLDA and MedLDA, we also ran an 5-fold experiment on the same dataset to evaluate pLSA+SVR and MMpLSA , and assessed the quality of predictions by Pre-for test, the rest were for training with 25% of reviews randomly chosen for tuning parameters.

Fig. 3 (Left) shows the results. We can see that the supervised MMpLSA can get much better results than the unsupervised two-step approach pLSA+SVR. Moreover, the performance of MMpLSA is consistent for numbers of topics ranging from 5 to 25. It suggests that MMpLSA can discover most discriminative information with few topics and simply increasing number of topics won X  X  improve performance.

We further compared MMpLSA with sLDA and MedLDA. Zhu et al. [13] showed that sLDA and MedLDA have similar perfo rmance and MedLDA is only better than MMpLSA to this best result in the literature. MMpLSA is noticeably better than this best result for all numbers of topics. MMpLSA achieved the best pR 2 mean 0.5285 with 9
The experimental result shows again that applying the max-margin principle to su-pervise latent topic models helps in discovering a much compacter semantic represen-tation with more discriminative information for prediction than state of art approaches. We have proposed MMpLSA that applies the max-margin principle to supervise latent topic models for both classification and regression. MMpLSA integrates learning latent topic representations and training a max-margin predictor into one single learning ob-jective. This integration gen erates topics describing discriminative themes in the corpus so that topic representations of documents are more suitable for prediction. Due to the optimization hardness of single learning objectives, we proposed tight variational upper bounds for them and developed step-wise convex procedures for optimizing those upper bounds. We studied the predictive power of MMpLSA on movie review and 20 News-groups data sets, and found that MMpLSA performed noticeably better in prediction with significantly fewer topics than state of art models. These results illustrate the benefits of the max-margin supervised latent topic model when dimension reduction and predic-tion are the ultimate goals. However, discriminative information in documents is always limited. MMpLSA could possibly over-fit documents if it is asked to discover more dis-criminative topics than real discriminative topics existing in documents. Therefore, one of future work could be introducing priors to topics in the MMpLSA for alleviating pos-sible over-fitting.

