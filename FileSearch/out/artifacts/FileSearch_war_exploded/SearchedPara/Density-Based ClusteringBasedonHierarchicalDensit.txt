 Density-based clustering [1,2] is a popular clustering paradigm. However, the existing methods have a number of limitations: (i) Some methods (e.g., DB-SCAN [3] and DENCLUE [4]) can only provide a  X  X lat X  (i.e. non-hierarchical) labeling of the data objects, based on a global density threshold. Using a single density threshold can often not properly characterize common data sets with clusters of very different densities and/or nested clusters. (ii) Among the meth-ods that provide a clustering hierarchy , some (e.g., gSkeletonClu [5]) are not able to automatically simplify the hierarchy into an easily interpretable repre-sentation involving only the most significant clusters. (iii) Many hierarchical methods, including OPTICS [6] and gSkeletonClu, suggest only how to extract a flat partition by using a global cut/density threshold, which may not result in the most significant clusters if the se clusters are characterized by different den-sity levels. (iv) Some methods are limited to specific classes of problems, such as networks (gSkeletonClu), and point sets in the real coordinate space (e.g., DECODE [7], and Generalized Single-Linkage [8]). (v) Most methods depend on multiple, often critical input parameters (e.g., [3], [4], [7], [8], [9]).
In this paper, we propose a clustering approach that, to the best of our knowl-edge, is unique in that it does not suffer from any of these drawbacks. In detail, we make the following contributions: (i) We introduce a hierarchical clustering method, called HDBSCAN, which generates a complete density-based clustering hierarchy from which a simplified hierarchy composed only of the most signifi-cant clusters can be easily extracted. (ii) We propose a new measure of cluster stability for the purpose of extracting a set of significant clusters from possi-bly different levels of a simplified cluster tree produced by HDBSCAN. (iii) We formulate the task of extracting a set of significant clusters as an optimization problem in which the overall stability of the composing clusters is maximized. (iv) We propose an algorithm that finds the globally optimal solution to this problem. (v) We demonstrate the advancement i n density-based clustering that our approach represents on a variety of real world data sets.

The remainder of this paper is organized as follows. We discuss related work in Section 2. In Section 3, we redefine DBSCAN, and we propose the algorithm HDBSCAN in Section 4. In Section 5, we introduce a new measure of cluster stability, propose the problem of extracting an optimal set of clusters from a cluster tree, and give an algorithm to so lve this problem. Section 6 presents an extensive experimental evaluation, and Section 7 concludes the paper. Apart from methods aimed at getting approximate estimates of level sets and density-contour trees for continuous-va lued p.d.f.  X  e.g., see [8] and references therein  X  not much attention has been give n to hierarchical density-based clus-tering in more general data spaces. The works most related to ours are those in [6,9,5,10]. In [6], a post-processing procedure to extract a simplified cluster tree from the reachability plot produced by the OPTICS algorithm was proposed. This procedure did not become as popular as OPTICS itself, probably because it is very sensitive to the choice of a critical parameter that cannot easily be deter-mined or understood. Moreover, no automatic method to extract a flat clustering solution based on local cuts in the obtained tree was described. In [9], an im-proved method to extract trees of significant clusters from reachability plots was proposed that is less sensitive to the user settings than the original method in [6]. However, this method is based on heuristics with embedded threshold values that can strongly affect the results, and the problem of extracting a flat solution from local cuts in the cluster tree was practically untouched; the only mentioned (ad-hoc) approach was to arbitrarily take all the leaf clusters and discard the others. In [5], the original findings from [6,9,11] were recompiled in the particular context of community discovery in compl ex networks. However, no mechanism to extract a simplified cluster tree from the resu lting (single-linkage-like) clustering dendrogram was adopted, and only a method producing a global cut through the dendrogram was described. The algorithm AUTO-HDS [10] is, like our method, based on a principle used to simplify clustering hierarchies, which in part refers back to the work of [12]. The clustering hierarchy obtained by AUTO-HDS is typically a subset of the one obtained by our method HDBSCAN. Conceptually, it is equivalent to a sampling of the HDBSCAN hierarchical levels, from top to bottom, at a geometric rate contro lled by a user-defined parameter, r shave . Such a sampling can lead to an underestimation of the stability of clusters or even to missed clusters, and these side effects can only be prevented if r shave  X  0. In this case, however, the asymptotic running time of AUTO-HDS is O ( n 3 )[13] (in contrast to O ( n 2 log n ) for  X  X ufficiently large X  values of r shave ). In addition, the stability measure used in AUTO-HDS has the undesirable property that the stability value for a cluster in one branch of the hierarchy can be affected by the density and cardinality of other clusters lying on different branches. AUTO-HDS also attempts to perform local cuts through the hierarchy in order to extract a flat clustering solution, but it uses a greed y heuristic for selecting clusters that may give suboptimal results in terms of an overall stability. Let X = { x 1 ,  X  X  X  , x n } be a data set of n objects, and let D be an n  X  n matrix containing the pairwise distances d ( x p , x q ), x p , x q  X  X , for a metric distance d (  X  ,  X  ). 1 We define density-based clusters based on core objects alone: Definition 1. (Core Object): An object x p is called a core object w.r.t.  X  and m pts if its  X  -neighborhood contains at least m pts many objects, i.e., if |
N cardinality. An object is called noise if it is not a core object.
 Definition 2. (  X  -Reachable): Two core objects x p and x q are  X  -reachable w.r.t.  X  and m pts if x p  X  N  X  ( x q )and x q  X  N  X  ( x p ).
 Definition 3. (Density-Connected): Two core objects x p and x q are density-connected w.r.t.  X  and m pts if they are directly or transitively  X  -reachable. Definition 4. (Cluster): A cluster C w.r.t.  X  and m pts is a non-empty maximal subset of X such that every pair of objects in C is density-connected. Based on these definitions, we can devise an algorithm DBSCAN* (similar to DBSCAN) that conceptually finds clusters as the connected components of a graph in which the objects of X are vertices and every pair of vertices is adja-cent if and only if the corresponding objects are  X  -reachable w.r.t. user-defined parameters  X  and m pts . Non-core objects are labeled as noise.

Note that the original definitions of DBSCAN also include the concept of border objects, i.e., non-core objects that are within the  X  -neighborhood of a core object. Our new definitions are more consistent with a statistical interpretation of clusters as connected components of a level set of a density (as defined, e.g., in [14]), since border objects do not technically belong to the level set (their estimated density is below the threshold). The new definitions also allow a precise relationship between DBSCAN* and its hierarchical version, to be discussed next. This was only approximately possible between DBSCAN and OPTICS. In this section, we introduce a hierarch ical clustering method, HDBSCAN, which can be seen as a conceptual and algorithmic improvement over OPTICS. Our method has as its single input parameter a value for m pts , which is a classic smoothing factor in density estimates whose behavior is well understood (meth-ods with a corresponding parameter, e.g., [6,10,7,8], are quite robust to it). Different density levels in the resulting d ensity-based cluster hierarchy will then correspond to different values of the radius  X  .

For a proper formulation of the density-based hierarchy w.r.t. a value of m pts , we define the notions of core distance and a symmetric reachability distance (following the definition used in [11]), a new notion of  X  -core objects ,aswellas the notion of a conceptual , transformed proximity graph, which will help us to explain a density-based clustering hierarchy.
 Definition 5. (Core Distance): The core distance of an object x p  X  X w.r.t. m Definition 6. (  X  -Core Object): An object x p  X  X is called an  X  -core object for every value of  X  that is greater than or equal to the core distance of x p w.r.t. m Definition 7. (Mutual Reachability Distance): The mutual reachability dis-tance between two objects x p and x q in X w.r.t. m pts is defined as d Definition 8. (Mutual Reachability Graph): It is a complete graph, G m pts ,in which the objects of X are vertices and the weight of each edge is the mutual reachability distance (w.r.t. m pts ) between the respective pair of objects. having weights greater than  X  . From Definitions 4, 6, and 8, it is straightforward to infer that clusters according to DBSCAN* w.r.t. m pts and  X  are the connected components of  X  -core objects in G m pts , X  ; the remaining objects are noise. Con-sequently, all DBSCAN* partitions for  X   X  [0 ,  X  ) can be produced in a nested, hierarchical way by removing edges in decreasing order of weight from G m pts . Proposition 1. Let X be a set of n objects described in a metric space by n  X  n pairwise distances. The partition of this data obtained by DBSCAN* w.r.t m pts and  X  is identical to the one obtained by first running Single-Linkage over the transformed space of mutual reachability distances, then, cutting the result-ing dendrogram at level  X  of its scale, and treating all resulting singletons with d core ( x p ) &gt; X  as a single class representing  X  X oise X .
 Proof. Proof sketch as per discussion above, after Definition 8.
 Proposition 1 states that we could implement a hierarchical version of DBSCAN* by an algorithm that first computes a Single-Linkage hierarchy on the space of
Algorithm 1. HDBSCAN main steps transformed distances (i.e., mutual reachability distances) and, then, processes this hierarchy to identify c onnected components and noi se objects at each level. Here, we propose a more efficient and elegant equivalent solution.

A density-based cluster hierarchy has to represent the fact that an object o is noise below the level l that corresponds to o  X  X  core distance. To represent this in a dendrogram, we propose to include an additional dendrogram node for o at level l representing the cluster containing o at that level and higher. To directly construct such a hierarchy, we propose an extension of a Minimum Spanning Tree (MST) of the Mutual Reachability Graph G m pts , from which we then can construct the extended dendrogram by removing edges in decreasing order of weights. More precisely, we extend the MST with edges connecting each vertex o to itself (self-loops), where the edge weight is set to the core distance of o . These  X  X elf edges X  will then be considered when removing edges.

Algorithm 1 shows the pseudo-code for HDBSCAN, which has as inputs a value for m pts and the data set X . It produces a clustering tree that contains all partitions obtainable by DBSCAN* (w.r.t. m pts ) in a hierarchical, nested way. It also contains nodes that indicate when an isolated object changes from core (i.e., dense) to noise. The result is called the  X  X DBSCAN hierarchy X . Using an implementation of Prim X  X  algorithm based on an ordinary list search (instead of a heap) to construct the MST, the method can be fully implemented in O ( dn 2 ) running time, where d is the number of data attributes. Also, by noticing that only the currently processed hierarchi cal level is needed at any point in time, the algorithm needs to keep in main memory essentially the data set X and the extended MST that can be constructed directly from it, which requires O ( dn ) space. If a data matrix D is provided as input in lieu of X , the algorithm requires O ( n 2 ) space instead, but its time complexity reduces to O ( n 2 ).
Algorithm 2. HDBSCAN step 4.2.2 with (optional) parameter m clSize  X  1 4.1 Hierarchy Simplification The HDBSCAN hierarchy can easily be visualized as a traditional dendrogram or related representations. However, these plots are not easy to interpret or pro-cess for large and  X  X oisy X  data sets, so it is a fundamental problem to extract from a dendrogram a summarized tree of only  X  X ignificant X  clusters. We propose a simplification of the HDBSCAN hierarchy based on a fundamental observation about estimates of the level sets of continuous-valued probability density func-tions (p.d.f.), which refers back to Hartigan X  X  concept of rigid clusters [14], and which has also been employed similarly by Gupta et al. in [10]. For a given p.d.f., there are only three possibilities for the evolution of the connected components of a continuous density level set when in creasing the density level (decreasing  X  in our context) [12]: (i) the component shrinks but remains connected, up to a density threshold at which either (ii) the component is divided into smaller ones, or (iii) it disappears. This observation can be applied to the HDBSCAN hierarchy to select only those hierarchi cal levels in which new clusters arise by a X  X rue X  X plitofacluster,orinwhichcl usters disappear; these are the levels in which the most significant changes i n the clustering structure occur. When decreasing  X  , the ordinary removal of noise objects from a cluster is not a  X  X rue X  split; a cluster only shrinks in this case, so it should keep the same label.
We can generalize this idea by setting a minimum cluster size, a commonly used practice in real cluster analysis (see, e.g., the notion of a particle in AUTO-HDS [10]). With a minimum cluster size, m clSize  X  1, components with fewer than m clSize objects are disregarded, and their disconnection from a cluster does not establish a  X  X rue X  split. We can adapt HDBSCAN accordingly by changing Step 4.2.2 of Algorithm 1 as shown in Algorithm 2: a connected component is deemed spurious if it has fewer than m clSize objects or, for m clSize =1,ifit is an isolated, non-dense object (no edg es). Any spurious component is labeled as noise and its removal from a bigger component is not considered as a cluster split. In practice, this can reduce the size of the hierarchy dramatically.
The optional parameter m clSize represents an independent control for the smoothing of the resulting cluster tree, in addition to m pts .TomakeHDBSCAN more similar to previous density-based approaches and to simplify its use, we can set m clSize = m pts ,whichturns m pts into a single parameter that acts as both a smoothing factor and a threshold for the cluster size. In many applications a user is interested in extracting a  X  X lat X  partition of the data, consisting of the prominent clust ers. Those clusters, however, may have very different local densities and may not b e detectable by a single, global density threshold, i.e., global cut through a hierarchical cluster representation. In this section, we describe an algorithm that provides the optimal global solution to the formal optimization problem of maximizing the overall stability of the set of clusters extracted from the HDBSCAN hierarchy. 5.1 Cluster Stability Without loss of generality, let us initia lly consider that the data objects are de-scribed by a single continuous-valued attribute x . Following Hartigan X  X  model [14], the density-contour clusters of a given density f ( x )on at a given density level  X  are the maximal connected subsets of the level set defined as { x | f ( x )  X   X  } . Most density-based clustering algorithms are to some extent based on this concept. The differences lie in the way the density f ( x ) and the maximal con-nected subsets are estimated, e.g., DBSC AN* estimates the density-contour clus-ters for a density threshold  X  =1 / X  and a (non-normalized) K -NN estimate (for K = m pts ) of the density f ( x ), given by 1 /d core ( x ).

HDBSCAN produces all possible DBSCAN* solutions w.r.t. a given value of m pts and all thresholds  X  =1 / X  in [0 ,  X  ). Intuitively, when increasing  X  (i.e., decreasing  X  ), clusters get smaller and smaller, until they disappear or break into sub-clusters; more prominent clusters will  X  X urvive X  longer after they appear. To formalize this concept, we adapt the notion of excess of mass [15]: Imagine increasing the density level  X  , and assume that a density-contour cluster C i appears at level  X  min ( C i ).Theexcessofmassof C i is defined in Equation (1), and illustrated in Figure 1, where the darker shaded areas represent the excesses of mass of three clusters, C 3 , C 4 ,and C 5 . The excess of mass of C 2 (not highlighted in the figure) encompasses those of its descendants C 4 and C 5 . The excess of mass exhibits a monotonic behavior along any branch of the hier-archical cluster tree. As a consequence, this measure cannot be used to compare the stabilities of nested clusters, such as C 2 against C 4 and C 5 .Tobeableto do so, we introduce here the notion of Relative Excess of Mass of a cluster C i , which appears at level  X  min ( C i ), as: which C i is split or disappears. For example, for cluster C 2 in Figure 1 it follows that  X  max ( C 2 )=  X  min ( C 4 )=  X  min ( C 5 ). The corresponding relative excess of mass is represented by the lighter shaded area in Figure 1.

For a HDBSCAN hierarchy, where we have a finite data set X , cluster labels, and density thresholds associated with each hierarchical level, we can adapt Equation (2) to define the stability of a cluster C i as:
S ( C i )= where  X  min ( C i ) is the minimum density level at which C i exists,  X  max ( x j , C i ) is the density level beyond which object x j no longer belongs to cluster C i ,and  X  max ( C i )and  X  min ( x j , C i ) are the corresponding values for the threshold  X  . 5.2 Optimization Algorithm Let { C 2 ,  X  X  X  , C  X  } be the collection of all clusters in the simplified cluster hier-archy (tree) generated by HDBSCAN, except the root C 1 , and let S ( C i )denote the stability value of each cluster. The goal is to extract the most  X  X rominent X  clusters (plus possibly noise) as a  X  X lat X  , non-overlapping partition. This task can be formulated as an optimization problem with the objective of maximizing the sum of stabilities of the extracted clusters in the following way: where  X  i ( i =2 ,  X  X  X  , X  ) indicates whether cluster C i is included in the flat solu-tion (  X  i =1)ornot(  X  i =0), L = { h | C h is a leaf cluster } is the set of indexes of leaf clusters, and I h = { j | j =1and C j is ascendant of C h ( h included) } is the set of indexes of all clusters on the path from C h to the root (excluded). The constraints prevent nested clust ers on the same path to be selected.
Algorithm 3. Solution to Problem (4)
To solve Problem (4), we process every node except the root, starting from the leaves (bottom-up), deciding at each node C i whether C i or the best-so-far selection of clusters in C i  X  X  subtrees should be selected. To be able to make this decision locally at C i , we propagate and update the total stability  X  S ( C i )of clusters selected in the subtree rooted at C i in the following, recursive way: where C i l and C i r are the left and right children of C i (for the sake of simplicity, we discuss the case of binary trees; the generalization to n-ary trees is trivial).
Algorithm 3 gives the pseudo-code for finding the optimal solution to Problem (4). Figure 2 illustrates the algorithm. Clusters C 10 and C 11 together are better than C 8 , which is then discarded. However, when the set { C 10 , C 11 , C 9 } is compared to C 5 , they are discarded as C 5 is better. Clusters { C 4 } and { C 5 } are better than C 2 ,and C 3 is better than { C 6 , C 7 } , so that in the end, only clusters C 3 , C 4 ,and C 5 remain, which is the optimal solution to (4) with J = 17.
Step 2.2 of Algorithm 3 can be implem ented in a more efficient way by not setting  X  (  X  ) values to 0 for discarded clusters down in the subtrees (which could happen multiple times). Instead, in a simple post-processing procedure, the tree can be traversed top-down in order to find, for each branch, the shallowest cluster that has not been discarded (  X  (  X  ) = 1). Thus, Algorithm 3 can be implemented with two traversals through the tree, one bottom-up and another one top-down. This results in an asymptotic complexity of O (  X  ), both in terms of running time and memory space, where  X  is the number of clusters in the simplified tree (which is typically much smaller than the number of data objects). Data Sets. We report the performance on 9 individual data sets plus the av-erage performance on 2 collections of data sets, representing a large variety of application domains and data characteristics (no. of objects, dimensionality, no. of clusters, and distance function). The first three data sets ( X  X ellCycle-237 X ,  X  X ellCycle-384 X , and  X  X eastGalactose X ) represent gene-expression data. CellCycle-237 and CellCycle-384 were made public by Yeung et al. [16]; they contain 237 resp. 384 objects (genes), 4 resp. 5 known classes, and have both 17 dimensions (conditions). YeastGalactose contains a subset of 205 objects (genes) and 20 dimensions (conditions) used in [17], with 4 known classes. For these data sets we used Euclidean distance on the z-score normalized ob-jects, which is equivalent to using Pearson correlation on the original data. The next three data sets are the Wine, Glass, and Iris from the UCI Repos-itory [18], containing 178, 214, resp. 150 objects in 13, 9, resp. 4 dimensions, with 3, 7, resp. 3 classes. For these da ta sets we used Euclidean distance. The last three individual data sets consist of very high dimensional represen-tations of text documents. In particular,  X  X rticles-1442-5 X  and  X  X rticles-1442-80 X , made available upon request by Naldi et al. [19], are formed by 253 ar-ticles represented by 4636 and 388 dimensions, respectively.  X  X brilpirivson X  [20] is composed of 945 articles represented by 1431 dimensions and is avail-able at http://infoserver.lcad.icmc.usp.br/infovis2/PExDownload. The number of classes in all three document data set s is 5, and we used the Cosine measure as dissimilarity function. In addition to individual data sets we also report average performance on two collections of data sets, which are based on the Amsterdam Library of Object Images (ALOI) [21]. Image sets were created as in [22] by randomly selecting k ALOI image categories as class labels 100 times for each k =2 , 3 , 4 , 5, then sampling (without replacem ent), each time, 25 images from each of the k selected categories, thus resulting in 400 sets, each of which con-tains 2, 3, 4, or 5 clusters and 50, 75, 100, or 125 images (objects). The images were represented using six different descr iptors: color moments (144 attributes), texture statistics from the gray-level co -occurrence matrix (88 attributes), Sobel edge histogram (128 attributes), 1st order statistics from the gray-level histogram (5 attributes), gray-level run-length matrix features (44 attributes), and gray-level histogram (256 attributes). We report average clustering results for the texture statistics, denoted by  X  X LOI-TS88 X , which is typical for the individual descriptors. We also show results for a 6-d imensional representation combining the first principal component extract ed from each of the 6 descriptors using PCA, denoted by  X  X LOI-PCA X . We used Euclidean distance in both cases. Algorithms. Our method, denoted here by  X  X DBSCAN(EOM) X  (EOM refers to cluster extraction based on Ex cess Of Mass), is compared with: (i) AUTO-HDS [10]; and (ii) a method referred to here as  X  X PTICS(AutoCl) X , which consists of first running OPTICS, and then using the method proposed by Sander et al. [9] to extract a flat partitioning. We set m pts ( n  X  in AUTO-HDS, MinPts in OPTICS) equal to 4 in all experiments. The speed-up control value  X  in OPTICS was set to  X  X nfinity X , thereby eliminating its effect. For AUTO-HDS, we set the additional parameters r shave to 0.03 (following the authors X  suggestion to use values between 0.01 and 0.05) and particle size , n part ,to m pts  X  1. The corresponding parameter m clSize in HDBSCAN was set equivalently to m pts . 2 Quality Measures. The measures we report are the Overall F-measure [23] and Adjusted Rand Index [24], denoted by  X  X Score X  resp.  X  X RI X , which are measures commonly used in the literature. In addition, we also report the fraction of objects assigned to clusters (as oppos ed to noise), denoted by  X %covered X . Clustering Results. The results obtained in our experiments are shown in Table 1. The highest obtained values for each data set are highlighted in bold. Note that HDBSCAN(EOM) outperforms the other two methods in a large majority of the data sets (in many cases by a large margin) and, in almost all cases, it covers a larger fraction of obj ects while having also high FScore and ARI values. A high fraction of clustered obj ects is only good when also the clustering quality is high. E.g., for CellCycle-384, OPTICS(AutoCl) covers 100% of the data, but with an ARI of 0, a meaningless clustering. In one of the only two cases where HDBSCAN(EOM) does not per form best, YeastGalactose, its ARI is very close to (and its FScore matches that of) the  X  X inner X , OPTICS(AutoCl).
The collections of image data sets, ALOI-TS88 and ALOI-PCA, allowed us to perform paired t-tests with respect to ARI and FScore, confirming that the observed differences in performance between all pairs of methods is statistically significant at the  X  =0 . 01 significance level. This means that the methods are doing indeed different things, and, in particular, that HDBSCAN(EOM) significantly outperforms the othe rs on these data set collections. A novel density-based clustering approach has been introduced that provides: (i) a complete density-based clustering hier archy representing all possible DBSCAN-like solutions for an infinite range of density thresholds and from which a simpli-fied tree of significant clusters can be extracted; and (ii) a flat partition composed of clusters extracted from optimal loca l cuts through the cluster tree. An exten-sive experimental evaluation on a wide variety of real world data sets has shown that our method performs significantly better and more robust than state-of-the-art methods. Our work lends itself to a number of interesting challenges for fu-ture work, which includes integration of semi-supervision and the consideration of subspaces.

