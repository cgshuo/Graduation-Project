 (1) Profit(l.O)=NTB-NC=N(TB-C) 
Using values for N, T, B, and C as above, the offer has an not) which can be folded into the estimated benefit or cost values for simplicity. or C but only on whether TB -C &gt; 0 or (2) TBIC&gt;I i.e. only on the benefit/cost ratio and the target frequency. profit by selecting a subset of customers, as examined in the next section. The Promised Land offer to P2 is rn~re profitable than making an offer to all. offer A to P2 (3) Projit(P,, TJ = NP, ( Tz B -C) 
The standard measure for comparing the target frequency of a lift, defined as Lift (P,) = T/T. 
We can rewrite (3) as (4) Projit(P2,Tu) = NP, (T* Lift(Pd B -C) formulate it as 
Campaign Profitability Condition: (5) Lijii(PJ &gt; UBT achieve to make the campaigns profitable. select a subset with lift &gt; 5 to achieve profit. 
We should also note that condition (5) is the minimum (4). optimal subset. 
To compute the actual lift data miners go through the lengthy process of examining the data available, selecting the data, preprocessing and cleaning is often very time consuming [2] and may take a couple of months before even the first model appears. 
Can we say something about the expected lift for typical the modeling? and has different underlying patterns. Furthermore, different modeling algorithms generally produced different results [6]. 
However, we examined a number of actual business from the financial and telecommunications arena and found a surprising the lift. 
In this section we present an empirical observation about the non-target population. We are not claiming that every model come up with a poor or random model with worse results. What problems there is a rough estimate for the best lift one can expect. overall accuracy? 
A modeling method, such as neural network or a decision tree 
The validation set is then sorted in descending order by the target probability. The frequencies of actual and predicted 5% increments. Generally speaking, if the modeling method is the model-based targeting is. While accuracy measures correct customer behavior for everybody, but find a good subset of a fraction. So Lift(S%,T) is the same as Lift(O.OS,T) decreasing with increasing P Table 1. Kdd-Cup 97 data for BNB program for problems with smaller T. at T is usually much lower than that. We have collected data 
Table 2 shows a few examples from our recent practice. In has the same proportion of the targets as the full data set. usually not available and was estimated by linear interpolation of lifts at lower and higher percentages, lift at T is estimated as 1% increments, which reduces lift interpolation error. We have experimented with neural net packages (Predict from 
Aspen Technology), decision trees (C4.8, (X.0, and CART), and several version of Bayesian methods. In general, lifts are comparable within the rough guidelines we are looking, but for are usually smaller than for neural nets. We explain this by decision trees being less sensitive to precise patterns with smaller support, which neural nets can pick up and which can influence lift at the top of the list. looked at the relationship between Y = loglO(Lift) and X = loglO(l/T) (or 1og10(100/2%) if 2% is expressed as a percentage) and computed a regression between these terms. 
The regression produced a formula (with R * = 0.86) formula 6 above) values of loglO(Lift(Z X ,T)) for the cases in smaller values of lift. 
A simplification of this formula is or shown in Table 2b. Where is usually within 20% of the actual lift. This led us to GPS Lift Rule of Thumb: sqrt( l/T) formula. and lower lifts. Second, there are classes of problems where either target data is not available. Almost all of our examples deal with predicting customer behavior using previous examples of same or related customer behavior, e.g. predicting attrition using previous examples of attrition. We also had experience with models that try to predict customer behavior using purely typically less than the GPS lift of sqrt (l/T). as the target behavior. For example, suppose we are predicting which records non-payment of insurance would be a leaker, since insurance and mortgage are frequently paid on the same bill. 
Table 2b: Actual and Estimated Lift Statistics or cloning problems, when one tries to model look-alikes of data, and those fields manifest themselves by contributing to very high lifts, 
Finally, there are examples of truly predictable outcomes. A very strong rule like 
For the KDD-CUP-97 data (Table I), log log regression gives a = 0.027, b =0.489, and R2= 0.988. Fig. 3 shows a plot of actual and estimated CumPHits for the 
KDD-CUP-97 data. non-paying customers. Since such rules are usually known, data satisfying them should be excluded from modeling. analysis is simplified by looking at CumPHits(P), an properties. CumPHits(P) is defined as the cumulative percent of the model-sorted list and is related to lift by list expressed as a fraction (between 0 and I). From the definition of CumPHits, we observe that 
From the observation that Lift(P) is decreasing monotonically 
Finally, based on the previous section, we are looking for a formula consistent with which is equivalent to CumPHits(T) = T * Lift(T) 2 T *T-o.5 = sqrt(l X ) 
We examined a number of lift tables used in the previous section and compared regression of P vs CumPHits, P vs log(CumPHits), log(P) vs log(CumPHits). Again, the best results were obtained from the log vs log regression. 
Here are the results from performing regression on 15 of the problems above (for some problems we were unable to do the looking for the regression (8) loglO(CumPHits) = a + 6 loglO 
I 60 
Table 3: Coefficients for regression of loglO(CumPHits) vs log1 O(P%) Averaging the coefficients over 16 cases we get average R* = some correlation between b and T , with R2= 0.66 (10) loglO(CumPHits(P,T)) = 0.041+(1.13 T+O.467)loglO(P) (11) CumPHits(P,T) = 1.1 P o.467+ X . X 3 T (12) CumPHits(P) = sqrt(P) While the estimate Lift(P) -l!sqrt(P) produces a reasonably (13) 0.4 loglO &lt; log10 (CumPHits(P)) &lt; 0.6 loglO (14) P  X  X  &lt;: CumPHits(P) c P  X  X  or, dividing the above equation by P Fig 4 shows the ranges the actual CumPHits curve for KDD-(I 6) Profit(P) = NP (TB*Lift(P) -C) selecting first P percent of the list: (17) Profit(P) z NP((TB/sqrt(P)) -C) = 
Let K = TB/C. This profit is maximized when (18) F(K,P) = K* sqrt(P) -P is maximized, for 0 5 P 5 1. Figure 5 shows the curve of K*sqrt(P) -P for K = 1 
We can find the maximum of F(K,P) by finding when its derivative is zero. Since a!x X  X du = fix X - X , the equation for derivative of F(K,P) equal to zero is or or (19) P = (K/2)2 for K=I is achieved when P = (K/2)2= 0.25. modeling to select a subset of the list will not be useful. 
However, when K = TB/C &lt; 2 , maximum profit is achieved a subset of the population. We can state this condition as: 
We can rewrite the Profit formula (17) as (20) Profit(P) = NC(K* sqrt(P) -P) 
By substituting P = (K/2) X  we get the maximum value (2 1) MaxProfit(N, T,B, C) z NC(K*A% -(Kn, X ) =NCK X  /4 This formula gives an expected value of campaign MaxProfit. a range estimate for lift, based on (11) 
Then the profit from selecting a subset P to contact can be written as (22) Profit(P) = NP (TBP - X  -C) or, substituting K = TBX (23) Projit(P)=NCP(KP-d-l)=NC(K*P( X  X d)-P) By a similar reasoning, is maximized when (24) Pmx(d) = (( 1-W ) X  X  For example, when d=0.4, PhlAX(d) = 0.279K  X  X  ; for d=0.5, P,,(d) = 0.25 K2 and for d=0.6, PhlAX(d) = 0.217KL.67. 
Substituting (24) into formula for MaxProfit, we see that the estimated maximum profit is (25) MuxProfi(N,T,B,C) =NC (K P,,(d) (led) -P,,(d)) = different values of d. d Pt X  X Ax(4 Max Profit 0.4 0.28 K2.5 0.186 NCK=. X  0.5 0.25 K2 0.250 NCK 2 0.6 0.22 K  X  X  X  0.326 NCK  X  X  Table 4. Variation of Maxprofit and P,,(d) with d 
Paper [4] provides an overview of typical issues related to maximizing payoffs using different modeling approaches. [l] investigates how to maximize lift for a specific decile or [9] investigates comparison of classifiers when dealing with skewed class distributions and non-uniform costs, which is the case with most applications of targeted marketing. We present heuristics for deciding when to consider applying itself and how to estimate expected profits. While we find that the heuristic formulae give a reasonable agreement (see section 5.4 for a discussion of exceptions), with the data we looked at from telecom and financial domains we need to expand our set of cases. Even though these approximations may be valid only for specific domains and useful as rule of thumb estimates. In our discussion we have assumed a sorted, ranked list of subscribers where the subsetting may happen by choosing a However with methods such as induced rules from a decision rules in section 2 and 3 can still be applied. Apart from expanding the data from different domains and evaluating the robustness of the proposed formulae for estimating lift, the lift curve and expected profits, another the whole population and in sub-populations (may be ordered) selected by a model. Other interesting avenues include examining the empirical distribution of target density function, from which the cumulative density function could be obtained by integrating and testing these results on additional data. [i] Bhattacharya, S., Direct Marketing Response models using Genetic Algorithms, Proceedings of the KDD-98: Fourth International Conference on Knowledge Discovery and Data Mining, AAAI Press, Menlo Park, CA, pp 144-148. [2] Brachman R. and Anand T., The Process of Knowledge Discovery in Databases: A Human centered approach, In 
R. (Eds.), 1996. Advances in Knowledge discovery and data mining. AAAI Press, Menlo Park. [3] Elkan, C. Boosting and Naive Bayesian Learning. Technical 
Report No. CS97-557, September 1997, UCSD. May 1997. [4] Ling, C.X., and Li, C, Data Mining for Direct Marketing: Problems and solutions, In Proceedings of KDD-98: Fourth International Conference on Knowledge Discovery and Data 
Mining, AAAI Press, Menlo Park, CA, pp 73-79. [5] Masand B. and Piatetsky-Shapiro G., A comparison of different approaches for maximizing business payoff of prediction models, Proceedings of KDD-96: the Second International Conference on Knowledge Discovery and Data 
Mining, AAAVMIT press, pp 19.5-20 1. [6] Michie, D., Spiegelhalter, D.J. and Taylor, C.C., (eds.), Machine Learning, Neural and Statistical Classification ,ElIis 
Horwood, 1994. [71 Parsa, I. KDD-CUP-97 www.epsiion.com/KDDCUP/index.htm . [8] Piatetsky-Shapiro, G., et al, An Overview of Issues in Developing Industrial Data Mining and Knowledge Discovery Applications Proceedings of the Second International 
Conference on Knowledge Discovery and Data Mining, (KDD-96), p. 89, AAAI Press, 1996. [9] Provost, F. and Fawcett, T., Analysis and visualization of Classifier performance: Comparison under Imprecise Class and Cost distributions, In KDD-97: Proceedings of the third International Conference on Knowledge Discovery and Data Mining of KDD-97, Newport Beach, CA, AAAI-Press, Menlo 
Park, Ca, pp 43-48. 
