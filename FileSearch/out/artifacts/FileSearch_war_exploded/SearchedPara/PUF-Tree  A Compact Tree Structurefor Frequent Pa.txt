 Since the introduction of frequent pattern mining [1], there have been numerous studies on mining and visualizing frequent patterns (i.e., frequent itemsets )from precise data such as databases (DBs) of market basket transactions [8,11,12]. Users definitely know whether an item is present in, or is absent from, a transac-tion in these DBs of precise data. However, there are situations in which users are uncertain about the presence or absence of some items or events [3,4,5,10,14,16]. For example, a physician may highly suspect (but cannot guarantee) that a pa-tient suffers from flu. The uncertainty of such suspicion can be expressed in terms of existential probability. For instance, a patient may have a 90% likelihood of having the flu, and a 20% likelihood of having a cold regardless of having the flu or not. With this notion, each item in a transaction t j in DBs containing precise data can be viewed as an item with a 100% likelihood of being present in t j . To deal with uncertain data, the U-Apriori algorithm [6] was proposed in PAKDD 2007. As an Apriori-based algorithm, U-Apriori requires multiple scans of uncertain DBs. To reduce the number of DB scans (down to two), the tree-based UF-growth algorithm [13] was proposed in PAKDD 2008. In order to com-pute the expected support of each pattern exactly , paths in the corresponding UF-tree are shared only if tree nodes on the paths have the same item and same existential probability. Hence, the UF-tree may not be too compact. In an attempt to make the tree compact, the UFP-growth algorithm [2] groups similar nodes (with the same item x and similar existential probability values) into a cluster. However, depending on the clustering paramet er, the correspond-ing UFP-tree may be as large as the UF-tree (i.e., no reduction in tree size). Moreover, UFP-growth returns not only the frequent patterns but also some in-frequent patterns (i.e., false positives). As alternatives to trees, hyper-structures were used by the UH-Mine algorithm [2], which was reported [15] to outperform UFP-growth.

Recently, we studied fast tree-based min ing of frequent patterns from uncer-tain data [14]. In the current paper, we study the following questions: Can we further reduce the tree size (e.g., smaller than the UFP-tree)? Can the resulting tree be as compact as the FP-tree? How to mine frequent patterns from such a compact tree? Would such a mining algorithm be faster than UH-Mine? Our key contributions of this paper are as follows: 1. a p refix-capped u ncertain f requent pattern tree (PUF-tree) ,whichcanbe 2. a mining algorithm (namely, PUF-growth ), which is guaranteed to find The remainder of this paper is organized as follows. The next section presents background and related works. We then propose our PUF-tree structure and PUF-growth algorithm in Sections 3 and 4, respectively. Experimental results are shown in Section 5, and conclusions are given in Section 6. In this section, we first give some background information about frequent pattern mining of uncertain data (e.g., existential probability, expected support), and we then discuss some related works.

Let (i) Item be a set of m domain items and (ii) X = { x 1 ,x 2 ,...,x k } be a k -itemset (i.e., a pattern consisting of k items), where X  X  Item and 1  X  k  X  m . Then, a transactional DB = { t 1 ,t 2 ,...,t n } is the set of n transactions, where each transaction t j  X  Item . The projected DB of X is the set of all transactions containing X .

Unlike precise DBs, each item x i in a transaction t j = { x 1 ,x 2 ,...,x h } in an uncertain DB is associated with an existential probability value P ( x i ,t j ) , which represents the likelihood of the presence of x i in t j [9]. Note that 0 &lt; P ( x i ,t j )  X  1. The existential probability P ( X, t j ) of a pattern X in t j is then the product of the corresponding existential probability values of items within X when these items are independent [9]: P ( X,t j )= x  X  X P ( x, t j ). The expected support expSup ( X ) of X in the DB is the sum of P ( X,t j )overall n transactions in the DB: expSup ( X )= n j =1 P ( X,t j ). A pattern X is frequent in an uncertain DB if expSup ( X )  X  a user-specified minimum support threshold minsup .GivenaDBand minsup , the research problem of frequent pattern mining from uncertain data is to discover from the DB a complete set of frequent patterns having expected support  X  minsup .
 Recall from Section 1 that the tree-based UF-growth algorithm [13] uses UF-trees to mine frequent patterns from uncertain DBs in two DB scans. Each node in a UF-tree captures (i) an item x , (ii) its existential probability, and (iii) its occurrence count. Tree paths are shared if the nodes on these paths share the same item, existential probability -value. In general, when dealing with uncertain data, it is not uncommon that the existential probability values of the same item vary from one transaction to another. As such, the resulting UF-tree may not be as compact as the FP-tree. See Fig. 1, which shows a UF-tree for the DB presented in Table 1 when minsup =0.5. The UF-tree contains four nodes for item a with different probability values as children of the root. Efficiency of the corresponding UF-growth algorithm, which finds all and only those frequent patterns, partially relies on the compactness of the UF-tree.
In an attempt to make the tree more compact, the UFP-growth algo-rithm [2] was proposed. Like UF-growth, the UFP-growth algorithm also scans the DB twice and builds a UFP-tree .Asnodesforitem x having similar existen-tial probability values are clustered into a mega-node, the resulting mega-node in the UFP-tree captures (i) an item x , (ii) the maximum existential probability value (among all nodes within the cluster), and (iii) its occurrence count (i.e., the number of nodes within the cluster). Tree paths are shared if the nodes on these paths share the same item but similar existential probability values. In other words, the path sharing condition is less restrictive than that of the UF-tree. By extracting appropriate tree paths and constructing UFP-trees for subsequent projected DBs, UFP-growth finds all frequent patterns and some false positives at the end of the second DB scan. The third DB scan is then required to remove those false positive.
The UH-Mine algorithm [2] stores all frequent items in each DB transaction in a hyper-structure called UH-struct . As UH-Mine does not take advantage of prefix-sharing, the size of the resulting UH-struct is always as large as that of the DB for the frequent items. However, UH-Mine was reported [2,15] to be faster than UFP-growth. To reduce the size of the UF-tree and UFP-tree, we propose the prefix-capped uncertain frequent pa ttern tree (PUF-tree) structure, in which important information about uncertain data is captured so that frequent patterns can be mined from the tree. The PUF-tree is con structed by considering an upper bound of existential probability value for each item when generating a k -itemset (where k&gt; 1). We call the upper bound of an item x r in a transaction t j the (prefixed) item cap of x r in t j , as defined below.
 Definition 1. The (prefixed) item cap I Cap ( x r ,t j ) of an item x r in a trans-action t j = { x 1 ,...,x r ,...,x h } ,where1  X  r  X  h , is defined as the product of P ( x r ,t j ) and the highest existential probability value M of items from x 1 to x  X  1 in t j (i.e., in the proper prefix of x r in t j ): I Example 1. Consider an uncertain DB with four transactions as presented in the Note that I Cap ( x r ,t j ) provides us with an important property of covering the existential probabilities of all possible patterns containing x r and its prefix in t , as stated in the following theorem.
 Theorem 1. Let X be a k -itemset in transaction t j (where k&gt; 1). The exis-tential probability of any of its non-empty proper subset Y that shares the same suffix x r as X (i.e., Y  X  X  X  t j )isalwayslessthanorequalto I Cap ( x r ,t j ). t , and (iii) Y = { x t ,...,x r } be a k -itemset (where k &lt;k ) and a proper subset of X (i.e., Y  X  X  X  t j ). Then, recall from Section 2 that the existential proba-Since the expected support of X is the sum of all existential probabilities of X over all the transactions containing X ,the cap of expected support of X can then be defined as follows.
 Definition 2. The cap of expected support expSup Cap ( X ) of a pattern X = { x DB) of all item caps of x k in all the transactions that contain X : expSup Cap ( X ) Based on Definition 2, expSup Cap ( X ) for any k -itemset X = { x 1 ,...,x k } can be considered as an upper bound to the expected support of X , i.e., expSup ( X )  X  expSup Cap ( X ). So, if expSup Cap ( X ) &lt; minsup ,then X cannot be frequent. Conversely, if X is a frequent pattern, then expSup Cap ( X )mustbe  X  minsup . Hence, we obtain a safe condition with respect to expSup Cap ( X )and minsup . This condition, which helps us to obtain an upper bound of the expected support for each pattern, could be safely applied for mining all frequent patterns.
As the expected support satisfies the downward closure property [1], all non-empty subsets of a frequent pattern are also frequent. Conversely, if a pattern is infrequent, then none of its supersets can be frequent. In other words, for Y  X  X , (i) expSup ( X )  X  minsup implies expSup ( Y )  X  minsup and (ii) expSup ( Y ) &lt; minsup implies expSup ( X ) &lt; minsup .
 In contrast, the cap of expected support generally does not satisfy the downward closure property because expSup Cap ( Y ) can be less than expSup Cap ( X )forsome proper subset Y of X .SeeExample3.
 However, for some specific cases (e.g., when X &amp; Y share the same suffix item x r ), the cap of expected support satisfies the downward closure property, as proved in Lemma 1. We call such property the partial downward clo-sure property : For any non-empty subset Y of X such that both X &amp; Y ends with x r ,(i) expSup Cap ( X )  X  minsup implies expSup Cap ( Y )  X  minsup and (ii) expSup Cap ( Y ) &lt; minsup implies expSup Cap ( X ) &lt; minsup . Lemma 1. The cap of expected support of a pattern X satisfies the partial downward closure property.
 Proof. Let (i) X &amp; Y be two itemsets such that Y  X  X , and (ii) X &amp; Y share the same suffix item x r . Then, Y must be present in any transaction when-ever X is present, but not vice versa. The number of transactions containing Y is higher than or equal to the number of transactions containing X . For any transaction t j in which X &amp; Y are present, they share the same suffix item x r Y  X  t j } X  n j =1 I Cap ( x r ,t j ) | X  X  t j } = expSup Cap ( X ). As a result, if expSup Cap ( X )  X  minsup ,then expSup Cap ( Y )  X  expSup Cap ( X )  X  minsup . Conversely, if expSup Cap ( Y ) &lt; minsup ,then expSup Cap ( X )  X  expSup Cap ( Y ) &lt; minsup . In other words, the cap of expected support of a pattern satisfies the partial downward closure property.
 To address the limitation of path sharing of UF-tree, we avoid keeping multiple nodes for the same item having different existential probability values. The basic idea is that, instead of storing the exact existential probability value for a node in the transaction, we store in the PUF-tree node the maximum existential prob-ability value of the prefix from that node up to the root (i.e., the item cap of the item). For any node in a PUF-tree, we maintain (i) an item and (ii) its prefixed item cap (i.e., the sum of all item caps for transactions that pass through or end at the node).

How to construct a PUF-tree? With the first scan of the DB, we find distinct frequent items in DB and construct a header table called I-list to store only frequent items in some consistent order (e.g., canonical order) to facilitate tree construction. Then, the actual PUF-tree is constructed with the second DB scan in a fashion similar to that of the FP-tree [7]. A key difference is that, when inserting a transaction item, we fir st compute its item cap and then insert it into the PUF-tree according to the I-list order. If that node already exists in the path, we update its item cap by adding the computed item cap to the existing item cap. Otherwise, we crea te a new node with this item cap value. For better understanding of the PUF-tree construction, see Example 4. Example 4. Consider the DB in Table 1, and let the user-specified support threshold Note that we are not confined to sorting and storing items in descending order of expected support. We could use other orderings such as descending order of item caps or of occurrence counts. An interesting observation is that, if we were to store items in descending order of occurrence counts, then the number of nodes in the resulting PUF-tree would be the same as that of the FP-tree .
Note that the sum of all item cap values (i.e., total item cap ) for all nodes of an item in a PUF-tree (which is computed when inserting each transaction) is maintained in the I-list . In other words, an item x r in the I-list of the PUF-tree maintains expSup Cap ( X )for X = { x 1 ,x 2 ,...,x r } .
 Lemma 2. For a k -itemset X = { x s ,...,x r } (where k&gt; 1) in a DB, if expSup Cap ( X ) &lt; minsup ,thenany k -itemset Z = { x t ,...,x r } (where k &gt;k ) in the DB that contains X (i.e., Z  X  X ) cannot be frequent.
 Proof. Let X &amp; Z be two itemsets such that (i) X  X  Z and (ii) they share the same suffix item x r . Following Lemma 1, if expSup Cap ( X ) &lt; minsup ,then expSup Cap ( Z )  X  expSup Cap ( X ) &lt; minsup .As expSup Cap ( Z )servesasan upper bound to expSup ( Z ), we get expSup ( Z )  X  expSup Cap ( Z ). Hence, if expSup Cap ( X ) &lt; minsup ,then Z cannot be frequent because expSup ( Z )  X  expSup Cap ( Z ) &lt; minsup .
 The above lemma allows us to prune the constructed PUF-tree further by re-moving any item having a total item cap (in the I-list )lessthan minsup .(The horizontal node traversal pointers allow us to visit such nodes in the PUF-tree in an efficient manner.) Hence, we can remove item d from the PUF-tree in Fig. 2(c) because expSup Cap ( d ) &lt; minsup . This results in a more compact PUF-tree, as shown in Fig. 2(d). This tree-pruning technique can save the mining time as it skips those items.

Let F ( t j )bethesetoffrequentitemsintransaction t j . Based on the afore-mentioned tree construction mechanism, the item cap in a node x in a PUF-tree maintains the sum of item caps (i.e., total item cap) of an item x for all trans-actions that pass through or end at x . Because common prefixes are shared, the PUF-tree becomes more compact and avoids having siblings containing the same item but having different existential probability values. However, as the item caps of different items in a transa ction can be different, the item cap of a node in a PUF-tree does not necessarily need to be greater than or equal to that of all of its child nodes.
It is interesting to note that, although item caps of a parent and child(ren) are not related, a PUF-tree can be a highly compact tree structure. The number of tree nodes in a PUF-tree (i) can be the same to that of an FP-tree [7] (when thePUF-treeisconstructedusingthefr equency-descending order of items) and (ii) is bounded above by t results can be generated because a PUF-tree contains F ( t j ) for all transactions and it stores the total item cap for a node. Mining based on this item cap value ensures that no frequent k -itemset ( k&gt; 1) will be missed.

Furthermore, recall that the expected support of X = { x 1 ,...,x k } is com-puted by summing the products of the existential probability value of x k with those of all items in the proper prefix of X over all n transactions in the DB, i.e., computed based on the existential probability value of x k and the highest exis-tential probability value in its prefix provides a tighter upper bound (than that based on highest existential probability value in transactions containing X )be-cause the former involves only the existential probability values of items that are in X whereas the latter may involve existential probability values of items that are not even in X . Here, we propose a pattern-growth mining algorithm called PUF-growth ,which mines frequent patterns from our PUF-tr ee structure. Recall that the construc-tion of a PUF-tree is similar to that of the construction of an FP-tree, except that item caps (instead of occurrence frequencies) are stored. Thus, the basic operation in PUF-growth for mining frequent patterns is to construct a pro-jected DB for each potential frequent pattern and recursively mine its potential frequent extensions.

Once an item x is found to be potentially frequent, the existential probability of x must contribute to the expected supp ort computation for every pattern X constructed from { x } -projected DB (denoted as DB x ). Hence, the cap of ex-pected support of x is guaranteed to be the upper bound of the expected support of the pattern. This implies that the complete set of patterns with suffix x can be mined based on the partial downward closure property stated in Lemma 1. Note that expSup Cap ( X ) is the upper bound of expSup ( X ), and it satisfies the partial downward closure property. So, we can directly proceed to generate all potential frequent patterns from the PUF-tree based on the following corollary. Corollary 1. Let (i) X be a k -itemset (where k&gt; 1) with expSup Cap ( X )  X  minsup in the DB and (ii) Y be an itemset in the X -projected DB (denoted as DB X ). Then, expSup Cap ( Y  X  X )intheDB  X  minsup if and only if expSup Cap ( Y ) in all the transactions in DB X  X  minsup .
 Proof. Let (i) X be a k -itemset (where k&gt; 1) with expSup Cap ( X )  X  minsup in the DB and (ii) Y be an itemset in the X -projected DB (i.e., Y  X  DB X ). Then, due to the mining process (especially, the construction of projected DBs) in the PUF-growth algorithm, itemset ( Y  X  X ) in the DB shares the same suffix (i.e., X )asitemset Y in DB X . Moreover, due to the definition of projected DBs, the transactions that contain ( Y  X  X ) in the DB are identical to those transactions that contain Y in DB X . Hence, expSup Cap ( Y  X  X )intheDB equals to expSup Cap ( Y )in DB X .Consequently,if expSup Cap ( Y  X  X )  X  minsup in the DB, then expSup Cap ( Y )  X  minsup in DB X ,andviceversa.
 Based on Lemma 1 and Corollary 1, we apply the PUF-growth algorithm to our PUF-tree for generating only those k -itemsets (where k&gt; 1) with caps of expected support  X  minsup . Similar to UFP-growth, this mining process may also lead to some false positives in the resulting set of frequent patterns at the end of the second DB scan, and all these false positives will be filtered out with the third DB scan. Hence, our PUF-growth is guaranteed to return the exact set of frequent patterns (i.e., all and only those frequent patterns with neither false positives nor false negatives).
 Example 5. The PUF-growth algorithm starts mining from the bottom of the I-list As shown in Example 5, PUF-growth finds a complete set of patterns from a PUF-tree without any false negatives. We compared the performances of our PUF-growth algorithm with existing al-gorithms (e.g., UF-growth [13], UFP-growth [2] and UH-Mine [2]) on both real and synthetic datasets. The synthetic datasets, which are generally sparse, are generated within a domain of 1000 items by the data generator developed at IBM Almaden Research Center [1]. We also considered several real datasets such as mushroom , retail and connect4 . We assigned a (randomly g enerated) existential probability value from the range (0,1] to each item in every transaction in the dataset. The name of each dataset indicates some characteristics of the dataset. For example, the dataset u100k10L 10 100 contains 100K transactions with av-erage transaction length of 10, and each item in a transaction is associated with an existential probability value that lies within a range of [10%, 100%]. Due to space constraints, we present here the results on some of the above datasets.
All programs were written in C and run with UNIX on a quad-core processor with 1.3GHz. Unless otherwise specified, runtime includes CPU and I/Os for I-list construction, tree construction, mining, and false-positive removal (of PUF-growth). The results shown in this section are based on the average of multiple runs for each case. In all experiments, minsup was expressed in terms of the percentage of DB size, and the PUF-trees were constructed using the descending order of expected support of items. 5.1 Compactness of PUF-Trees The UF-tree, UFP-tree and PUF-tree all a rranged items in the same order (e.g., descending order of expected support). Hence, depending on the clustering pa-rameter, the number of nodes of a UFP-tree (in its best case) could be similar to that of a PUF-tree. Note that the size of UFP-tree was larger than that of PUF-tree because the UFP-tree stored extra cluster information in nodes. The size of UF-tree was larger than that of the other three because the UF-tree may contain multiple nodes for the same item (under the same parent). So, in the first experiment, we compared the compactn ess of our PUF-trees with the UF-tree 1 .
The node counts between PUF-trees and UF-trees, as presented in Table 2, show that PUF-trees were more compact than UF-trees for both sparse and dense represents the gain of PUF-tr ees over UF-trees (e.g., for mushroom 50 60 ,the PUF-tree contained 8108 nodes, which was only 6.68% of the 121205 nodes in the UF-tree). The gain of PUF-trees was much promising in dense datasets (e.g., mushroom 50 60 ) than sparse datasets (e.g., u100k10L 10 100 ) because the PUF-tree is more likely to sha re paths for common prefixes in dense datasets. In contrast, the UF-tree contains a distinct tree path for each distinct item, existential probability pair, and thus not as compact as the PUF-tree. 5.2 Runtime Recall that UH-Mine was shown to outperform the UFP-growth [2,15]. So, we also compared our PUF-growth with UH-Mine. Figs. 4(a) X (c) show that PUF-growth took shorter runtime than UH-Mine for datasets u100k10L 50 60 , u100k10L 10 100 and mushroom 50 60 . The primary reason is that, even though the UH-Mine finds the exact set of frequent patterns when mining an exten-sion of X , it may suffer from the high computation cost of calculating the ex-pected support of X on-the-fly for all transactions containing X . Such computa-tion may become more costly when minin g a large number of patterns (e.g., in mushroom 50 60 ) and long patterns (e.g., in u100k10L 50 60 , u100k10L 10 100 , retail 50 60 ). 5.3 Number of False Positives In practice, although both UFP-tree and PUF-trees are compact, their corresponding algorithms generate some false positives. Hence, their overall performances depend on the number of f alse positives generated. In this ex-periment, we measured the number of false positives generated by UFP-growth and PUF-growth. Due to space constraint s, we present results (in percentage) using one minsup value for each of the two datasets (i.e., u10k5L 80 90 and u100k10L 50 60 ) in Table 3. In general, PUF-growth was observed to remark-ably reduce the number of false positiv es when compared with UFP-growth. The primary reason of this improvement is that upper bounds of expected support of patterns in clusters are not as tight as the upper bounds provided by PUF-growth. In a UFP-tree, if a parent has several children, then each child will use higher cluster values in the parent to generate the total expected support. If the total number of existential probability values of that child is still lower than that of the parent X  X  highest cluster value, then the expected support of the path with this parent and child will be high. This results in more false positives in long run. 5.4 Scalability To test the scalability of PUF-growth, we applied the algorithm to mine frequent patterns from datasets with increasing s ize. The experimental result presented in Fig. 4(d) indicates that our PUF-growth algorithm (i) is scalable with respect to the number of transactions and (ii) can mine large volumes of uncertain data within a reasonable amount of time.

The above experimental results show that our PUF-growth algorithm effec-tively mines frequent patterns from uncertain data irrespective of distribution of existential probability values (whether most of them have low or high values, whether they are distributed into a narrow or wide range of values). In this paper, we proposed the PUF-tree structure for capturing important information of uncertain data. In addition, we presented the PUF-growth al-gorithm for mining frequent patterns. The algorithm uses the PUF-tree to obtain upper bounds to the expected support of frequent patterns (i.e., item caps , which are computed based on the highest existential probability of an item in the prefix); it guarantees to find all frequent patterns (with no false nega-tives). Experimental results show the e ffectiveness of our PUF-growth algorithm in mining frequent patterns from our PUF-tree structure.
 Acknowledgements. This project is partially supported by NSERC (Canada) and University of Manitoba.

