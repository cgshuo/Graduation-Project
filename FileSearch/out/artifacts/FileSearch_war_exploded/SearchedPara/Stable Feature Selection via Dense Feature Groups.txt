 Many feature selection algorithms have been proposed in the past focusing on improving classification accuracy. In this work, we point out the importance of stable feature se-lection for knowledge discovery from high-dimensional data, and identify two causes of instability of feature selection al-gorithms: selection of a minimum subset without redundant features and small sample size. We propose a general frame-work for stable feature selection which emphasizes both good generalization and stability of feature selection results. The framework identifies dense feature groups based on kernel density estimation and treats features in each dense group as a coherent entity for feature selection. An efficient algo-rithm DRAGS (Dense Relevant Attribute Group Selector) is developed under this framework. We also introduce a gen-eral measure for assessing the stability of feature selection algorithms. Our empirical study based on microarray data verifies that dense feature groups remain stable under ran-dom sample hold out, and the DRAGS algorithm is effective in identifying a set of feature groups which exhibit both high classification accuracy and stability.
 H.2.8 [ Database Management ]: Database Applications-data mining; I.2.6 [ Artificial Intelligence ]: Learning Algorithms Feature selection, stability, high-dimensional data, kernel density estimation, classification
Feature selection, the problem of selecting a minimum subset of original features for best predictive accuracy, has attracted strong interest in the past several decades. A great variety of feature selection algorithms have been developed and proven to be effective in improving predictive accuracy for classification in many application domains [21]. The sub-tle issue of feature redundancy is also resolved by algorithms which minimize redundancy and maximize relevance among selected features for classification [2, 11, 22, 30].
However, a relatively neglected issue is the stability of se-lected feature sets, which remains an unresolved problem. This problem is particularly important for knowledge discov-ery from high-dimensional data, where the goal of knowledge discovery is often to identify features best explaining the differences between classes or subsets of samples from thou-sands of features. For example, in biological applications (e.g., microarrays, mass spectrometry), the primary goal of domain experts in conducting high-throughput experiments is often to detect leads for some biologically relevant marker genes or proteins, rather than building models for predicting diseases or phenotypes of novel samples [4, 23].

Although many feature selection algorithms are effective in selecting a subset of predictive features for sample class prediction, they are not necessarily reliable to identify can-didate features for subsequent costly biological validation. One may be tempted to choose the set of features producing the best predictive accuracy as a starting point for valida-tion. However, for the same data, many different subsets of features can result in the same or similarly good accu-racy [12, 25]. The large number of predictive subsets and the disparity among them reveals the instability of feature selection algorithms. As a consequence, domain experts are unlikely to have the confidence to investigate any single sub-set of predictive features.

One cause of such instability is the classic goal of feature selection which aims to select a minimum subset of features necessary for constructing a classifier of best predictive ac-curacy [18, 19]. Many feature selection algorithms thus discard features which are relevant to the target concept but highly correlated to the selected ones. For the purpose of knowledge discovery from features, such minimum sub-set misses important knowledge about redundant features. Moreover, among a set of highly correlated features, differ-ent ones may be selected under different settings of a feature selection algorithm. The problem is usually severe for high-dimensional data with many highly correlated features.
Another cause of the instability of feature selection algo-rithms is the relatively small number of samples in high-dimensional data. Take microarray data for example, the typical number of features (genes) is thousands or tens of thousands, but the number of samples is often less than a hundred. For the same feature selection algorithm, a differ-ent subset of features may be selected each time when there is a slight variation in the training data. Such instability has been confirmed by our observations from experiments as well as recent work studying the stability of feature se-lection algorithms under training data variations [10, 17].
The two causes of instability are independent, and amplify the effect of each other on feature selection from data with many redundant features but limited samples. In order to provide domain experts with stable feature selection results, we have to overcome both causes of instability. In this paper, we propose a general framework for stable feature selection which aims to achieve not only good classification accuracy but also stable feature selection results .

Our framework is motivated by a key observation that in the sample space, the dense core regions (peak regions), measured by probabilistic density estimation, are stable with respect to sampling of the dimensions (samples). For exam-ple, a spherical Gaussian distribution in the 100-dimensional space will likely be a stable spherical Gaussian in any of the subspaces. The features near the core of the spheri-cal Gaussian, viewed as a core group are likely to be stable under sampling, although exactly which feature is closest to the peak could vary. Another observation is that the features near the core region are highly correlated to each other, and thus should have similar relevance scores w.r.t. some class labels, assuming the class labels are locally consistent. Thus these features can be regarded as a single group in feature ranking. And we can pick any one of them in final classifi-cation. In this sense, the feature group is a stable entity.
The rest of the paper is organized as follows. In Section 2, we review previous work on feature selection, in contrast with our work. In Section 3, we introduce preliminaries on kernel density estimation. In Section 4, we describe in de-tail the proposed stable feature selection framework and the DRAGS algorithm under this framework. In Section 5, we propose a general measure of stability for feature selection results. Section 6 evaluates the effectiveness of the DRAGS algorithm in terms of both classification accuracy and sta-bility based on microarray data sets. Section 7 provides a summary of this work and some future directions.
For many years, feature selection has been generally viewed as a problem of searching for an optimal subset of features guided by some evaluation measures. Various feature selec-tion methods can broadly fall into the filter model and the wrapper model depending on their evaluation measures [18]. Filter methods use measures of intrinsic data characteris-tics [9, 21, 29], and wrapper methods rely on the perfor-mance of a predefined learning algorithm to evaluate the goodness of a subset of features [18]. For high-dimensional data, filter methods are often preferred due to their compu-tational efficiency. As to search strategy, a simple way is to evaluate each feature independently and form a subset based on top-ranked features. Such univariate methods have been shown effective in some applications [13, 20]. However, they do not work well when features highly correlate or interact with each other. Various subset search methods evaluate features in a subset together and select a small subset of relevant but non-redundant features [2, 11, 22, 30]. They have shown improved classification accuracy over univariate methods. Another way is to weight all features together ac-cording to maximum margin. The margin can be defined either by the distance between a selected data point and its nearest neighbors in the same and different classes (as in ReliefF-based weighting) [6, ? , 24] or by the distance be-tween support vectors (as in SVM-based weighting) [14, 27]. An advantage of such methods is that optimal weights of features can be estimated by considering features together. A subset of top-ranked features can be selected based on a single pass of weighting features [6, ? , 24] or a recursive feature elimination (RFE) procedure [14, 27].

All work discussed above only focuses on the generaliza-tion ability of feature selection methods, and pays little at-tention to their stability; methods were not deliberately de-signed to achieve stable results and hence not evaluated in terms of stability either. In contrast, our work addresses the two causes of instability of feature selection algorithms identified in Introduction. Another distinction is that our proposed framework identifies coherent feature groups and treats each group as a single entity during feature evaluation and subsequent learning tasks, while previous work treats each feature as an entity for evaluation and classification.
Clustering has been applied to feature selection, by clus-tering features and then selecting one (or a few) representa-tive features from each cluster [3, 5, 15], or simultaneously clustering and selecting features [16], to form a final feature subset. Intuitively, clustering features can illuminate rela-tionships among features and facilitate feature redundancy analysis; a feature is more likely to be redundant to some other features in the same cluster than features in other clusters. However, an optimal clustering result does not in-dicate that features in each cluster are coherent in terms of relevance and can be treated as a single entity. More importantly, existing feature clustering methods for feature selection do not consider the stability of feature groups, and therefore, are essentially different from our framework for stable feature selection based on dense feature groups.
Two recent papers have studied the stability issue of fea-ture selection under small sample size, and compared a few existing feature selection algorithms [10, 17]. For each algo-rithm, they measured the stability of selected features when various random subsets of the same training data were used for feature selection. They both concluded that different al-gorithms which performed equally well for classification had a wide difference in terms of stability, and recommended to empirically choose the best feature selection algorithm according to both accuracy and stability measured by re-peatedly sampling of the training data. Such procedure is computationally very costly, and is subject to ad hoc choice of a predefined pool of feature selection algorithms and clas-sification algorithms used for evaluation. Moreover, the best outcome of such procedure is limited to the stability of ex-isting feature selection algorithms which often suffer from the two causes of instability discussed in Introduction.
Significant effort is needed in order to have a comprehen-sive comparison of the stability of various existing feature selection algorithms which apply different evaluation mea-sures and search strategies. Our work takes a paradigm shift from this direction, and is clearly different from previ-ous work on stability study. To the best of our knowledge, our work is the first to propose a feature selection algorithm which directly provides stable feature selection results by addressing the two causes of instability.
Kernel density estimation (known as Parzen window) is the most popular non-parametric method for estimating prob-abilistic density functions [26]. Given a data set of n data points D = { x i } n i =1 in the d -dimensional space R d known multivariate kernel density estimator is given by where  X  p (x) is an estimate of the unknown pdf , K (x) is a radially symmetric, non-negative kernel function integrating to one, and h is a fixed bandwidth (window size).
In many applications of machine learning and pattern recognition, it is often useful to identify the modes of the underlying density p (x), which are located at the zeros of the gradient  X  p (x) = 0 . The mean shift procedure [7] is an elegant way to estimate the locations of these zeros without estimating the density. Given a data set D and a kernel function K as introduced before, the mean shift vector is defined by i.e, the difference between the weighted sample mean, using the kernel for weights, and x, the center of the kernel.
Let { y j } j =1 , 2 ,... denote the sequence of successive loca-tions of the kernel K , where, is the weighted mean at y j computed based on kernel K and y 1 is the center of the initial position of the kernel. Such iterative movement of the kernel along the direction defined by the mean shift vector can start with any data point x  X  D .

It is proven [7] that if a kernel K has a convex and monoton-ically decreasing profile, both sequences { y j } j =1 , 2 ,... {  X  p (y j ) } j =1 , 2 ,... converge, and {  X  p (y j ) } cally increasing. In addition, the magnitude of each suc-cessive mean shift vector (derived from (2) and (3)) converges to zero, and the gradient of the density estimate (1) computed at the stationary point y c is zero  X   X  p (y c ) = 0 . Two simple kernels which satisfy the condition are the flat kernel and Gaussian kernel.
Our proposed framework for stable feature selection iden-tifies dense feature groups based on kernel density estima-tion, and treats features in each dense group as a coherent entity for feature selection.
Kernel density estimation operates on a set of data vectors x , x 2 , ..., x n , defined by a d -dimensional feature space. In this work, we apply such method to estimate the density of a set of feature vectors f 1 , f 2 , ..., f n in a data set. In order to do so, we need to transpose the data matrix representing the data set; original feature vectors become data vectors in the new feature space defined by the original data vectors. Algorithm 1 DGF (Dense Group Finder) Input: data D = { x i } n i =1 , bandwidth h
Output: a number of dense feature groups G 1 , G 2 , ..., G for i = 1 to n do end for
For every unique peak p r , add x i to G r if || p r  X  x i Optional: Eliminate feature groups of low density We use the multivariate density estimator in (1) to evaluate the density of a feature; a feature with higher value of  X  p (x) is denser than a feature with lower value.

Our proposed framework is motivated by a key observa-tion that the dense core regions (peak regions), measured by probabilistic density estimation, are stable with respect to sampling of the dimensions. For example, in a spheri-cal Gaussian distribution, data in each dimension follow the distribution where x p is the coordinate in p -th dimension of a vector x. Thus, the total distribution in 100 dimensions is just Clearly, taking off any 50 dimensions, the rest will still be a spherical Gaussian. The features near the core of the spher-ical Gaussian, viewed as a core group are likely to be stable under sampling, although exactly which feature is closest to the peak could vary.

In order to identify such dense feature groups, we need to group together dense features which are close to the same density peak. Based on the mean shift procedure, we pro-pose the DGF (Dense Group Finder) algorithm. As shown in Algorithm 1, DGF first finds a number of unique density peaks in the data, and then decides dense feature groups based on density peaks. The main part of DGF is the it-erative mean shift procedure for all n features, which has a time complexity of O (  X n 2 d ), where  X  is the number of iter-ations for each mean shift procedure to converge, and d is the dimensionality in the transposed data (i.e., the number of samples in the original feature space). Normally, it only takes a few steps for each mean shift procedure to converge.
A difficulty in kernel density estimation is the choice of kernel bandwidth h . If h is very large, the whole data will have only one peak. On the other hand, if h is very small, every data point will be a peak. Fortunately, there is a nice way to estimate it from the K-Nearest Neighbors (KNN) point of view. For each data point x i , we can find its KNNs, and compute the average distance from x i to its KNNs. This average distance is a reasonable length which captures the local density near x i . We can further compute the average of the average distance of KNNs for all data points to get a global average length. For a data set with n features, the possible values of K range from 1 to n . The smaller the chosen K value (hence the smaller bandwidth h ), the higher correlation features included in each dense group will have. Therefore, in order to find coherent dense feature groups, a reasonable K value should be sufficiently small but away from 1. Clearly, when K=1, the bandwidth will be zero, and every data point will be a peak.

The major difference of DGF from other mean-shift based clustering algorithms [8] lies in the last two steps after find-ing all the unique peaks. Clustering algorithms based on mode seeking aim to create continuity-based clusters among data points, and, therefore, they group all data points at-tracted to the same peak into one cluster of arbitrary shape. Each resulting cluster may contain data points with low den-sity which are far way from the peak. Our goal is to identify dense feature groups, and therefore, DGF only includes fea-tures into a feature group if they are close to a unique peak. Feature groups of low density can be eliminated in an op-tional step. In our work, we eliminate a feature group G r if the average distance of the associated density peak P r its KNNs is above the kernel bandwidth decided in the way described above.
The maximum pair-wise distance among features in the same dense feature group identified by DGF is limited by the kernel bandwidth. Under a sufficiently small bandwidth h &gt; 0, features in each dense feature group will be highly correlated to each other, and thus should have similar rele-vance scores with respect to some class labels, assuming the class labels are locally consistent. Thus these features can be regarded as a single group in relevance based ranking. And we can pick any one of them in final classification. There-fore, our general framework for stable feature selection is to first identify dense feature groups and then select relevant feature groups among dense feature groups. To decide the relevance of each dense group, the framework treats features in each dense group as a coherent entity.
 We propose the DRAGS (Dense Relevant Attribute Group Selector) algorithm under this general framework. As shown in Algorithm 2, DRAGS first finds a number of dense fea-ture groups based on DGF, and then evaluates the relevance of each feature group based on the average relevance of fea-tures in each group. DRAGS has the same time complexity as DGF if feature relevance is measured based on individual feature groups. DRAGS can be easily extended to consider interactions among feature groups when deciding group rel-evance under its general framework. In this work, since our investigative emphasis is on the effectiveness of dense feature groups for stable feature selection, we use the simple method of individual feature evaluation to determine the group rele-vance in DRAGS. As to relevance measures, various existing feature evaluation measures such as correlation, dependency, and distance [21] can be chosen depending on data charac-teristics. We use F -statistic, a commonly used statistical measure for identifying differentially expressed genes, as the relevance measure for experiments on microarray data sets.
For the sake of a simple model, like most other feature selection algorithms, DRAGS is able to provide a compact feature subset for classification by only selecting one repre-sentative feature from each dense and relevant feature group. Since features in each dense group are highly correlated, DRAGS naturally deals with the redundancy among rele-vant features. DRAGS also overcomes the two causes of in-Algorithm 2 DRAGS (Dense Relevant Attribute Group Selector) Input: data D , bandwidth h , relevance measure  X (  X  )
Output: selected relevant feature groups G 1 , G 2 , ..., G
Find dense feature groups G 1 , G 2 , ..., G m = DGF( D, h ) for i = 1 to m do end for Rank G 1 , G 2 , ..., G m according to  X ( G i )
Select top k most relevant groups (or based on a threshold) stability discussed in Introduction. As to instability caused by eliminating redundant features, DRAGS keeps highly cor-related features in a coherent feature group. Such coherent feature groups provide valuable knowledge about how rele-vant features are correlated. Features in all groups together provide a more comprehensive set of important features than any single subset of features selected by methods eliminat-ing redundant features. As to instability caused by small sample size, DRAGS ensures the stability of feature groups identified from a small number of samples by evaluating the density of features and identifying dense feature groups.
Measuring the stability of feature selection algorithms re-quires some similarity measures for two sets of feature selec-tion results. Let R 1 = { G i } | R 1 | i =1 and R 2 = { G two sets of feature selection results, where each G i and G represents a group of features. In a special case when each G i and G j only contains a single feature, R 1 and R 2 become two subsets of features. In such case, the similarity between R 1 and R 2 can simply be decided by where the subscript ID indicates that the similarity is de-cided by matching feature indices between the two subsets. Measures of similar forms have been used for assessing the stability of selected feature subsets in related papers [10, 17] discussed in Section 2. In this work, we develop a measure which extends existing similarity measures in two aspects. First, it is directly applicable to assess the similarity be-tween two sets of feature groups in a general case. Second, it considers the similarity of feature values in addition to feature indices, which makes it informative when two fea-ture subsets contain a large portion of different but highly correlated features. This general similarity measure for two sets of feature selection results is defined based on maximum weighted bipartite matching.
 Given a bipartite graph G = ( V, E ), with vertex partition V = V 1  X  V 2 , and edge set E = { ( u, v ) | u  X  V 1 , v  X  V called a weighted bipartite graph if every edge ( u, v ) is asso-ciated with a weight w ( u,v ) , and a complete bipartite graph if every u in V 1 is adjacent to every v in V 2 . A matching M in G is a subset of non-adjacent edges in E . The problem of maximum weighted bipartite matching (also known as the assignment problem) is to find an optimal matching where the sum of the weights of all edges in the matching has a maximal value. There exist various efficient algorithms (e.g, the Hungarian algorithm) for finding an optimal solution.
Given two sets of feature selection results, R 1 = { G i } and R 2 = { G j } | R 2 | j =1 , we model R 1 and R 2 together as a weighted complete bipartite graph G = ( V, E ), where V = R 1  X  R 2 , and E = { ( G i , G j ) | G i  X  R 1 , G j  X  R 2 w ( G i ,G j ) is determined by the similarity between a pair of feature groups G i and G j . The overall similarity between R 1 and R 2 is defined as: where M is a maximum matching in G .

Depending on how to decide w ( G forms of Sim M : Sim M ID and Sim M V , where the subscripts and V respectively indicate that each weight is decided based on feature indices or feature values. When G i and G j repre-sent feature groups with more than one feature, for Sim M each weight w ( G Sim ID in (5); For Sim M V , each weight can be decided by the correlation coefficient between the centers or the most representative features of the two feature groups. In the special case when G i and G j represent individual features, for Sim M ID , since w ( G otherwise, Sim M ID becomes Sim ID ; for Sim M V , each weight can be simply decided by the correlation coefficient between the two individual features. Therefore, the proposed simi-larity measure in (6) is a general measure for assessing the similarity between two sets of feature selection results.
Given the general similarity measure, we define stability of a feature selection algorithm as the average similarity of various sets of results produced by the same feature selection algorithm under training data variations. Let Sim M ( R, R denote the similarity between two sets of results R and R from the full set of samples and a subset of samples, respec-tively. Each subset of samples can be obtained by randomly sampling or bootstrapping the full set of samples. The sta-bility of an algorithm over q subsets of samples is given by: It is worth to note that the stability can also be measured based on pair-wise similarity of results from different subsets of samples. We use formula (7) because it is more efficient to compute than pair-wise comparison. Moreover, it directly captures how different the result will be from the result ob-tained based on the full data, when some training samples are randomly removed.
In this section, we empirically study the framework for stable feature selection based on dense feature groups. The study is conducted in two parts. In Section 6.2, we verify that dense feature groups are stable with respect to sample hold out. In Section 6.3, we verify that feature selection from dense feature groups according to group relevance produces feature groups which are both highly predictive and stable. Before we delve into experimental results and discussions, we first present the setup of various experiments.
We experimented with six frequently studied public mi-croarray data sets 1 , characterized in Table 1. Following the original work on Colon data [1], for each data set, we nor-malized each feature vector so that the mean over its com-ponents is zero and the standard deviation is one. Note that because of the normalization, the Euclidean distance between two feature vectors x i and x j is related to r , the Pearson correlation between x i and x j : | x i  X  x j | 2 where d is the number of dimensions of the feature vectors. Due to such relationship, dense feature groups identified based on kernel function using Euclidean distance consist of features which are highly correlated to each other.
In order to evaluate the stability of dense feature groups under sample hold out, each data set was randomly par-titioned into 3 folds, with each fold containing 1/3 of all the samples. Algorithm 1, DGF, was repeatedly applied to 2 out the 3 folds, while a different fold was hold out each time. This process was repeated 10 times for different par-titions of the data set. Overall, a total of 10  X  3 different subsets of samples were used to generate different sets of fea-ture groups by DGF. DGF was also applied to the full set of data in order to produce a reference set of feature groups R for Sim M ( R, R i ), the average Sim M ( R, R i ) over 30 folds.
In order to evaluate the generalization ability and stability of Algorithm 2, DRAGS, each of the 30 subsets of samples in the previous study was used as the training set to select relevant feature groups from dense feature groups produced by DGF, and then train classifiers based on selected fea-ture groups. The corresponding hold-out fold was used as the test set. One representative feature (the one with the highest average similarity to all other features in the group) from each selected group was used for both training and testing. Both sophisticated SVM (linear kernel) and simple KNN (K=1) classification algorithms (Weka X  X  implementa-tion [28]) were used to evaluate the generalization ability of the representative features of feature groups selected by DRAGS. The average predictive accuracy over the 30 folds was used as the measure for generalization ability. To con-firm that the selected relevant dense feature groups remain stable, the stability of DRAGS was measured in the same setting as in the previous study for DGF, except that dense but irrelevant feature groups were excluded from the stabil-ity measurement.

As discussed in Section 2, feature clustering has been used for feature selection. For comparison purpose, we investi-gated whether simple K-means clustering can produce fea-ture groups which are both stable and predictive. With-out prior knowledge about the optimal number of clusters http://www.cs.binghamton.edu/  X  lyu/KDD08/data/ in each data set, the performance of K-means was evalu-ated under a wide range of K values. For each K value, K-means was repeated 50 times with random initial seeds, and the clustering result with minimum WSS (Within clus-ters Sum of Squared errors) was used for performance eval-uation. First, we evaluated the stability of feature clusters from K-means under sample hold out. Under each K value, the stability of K feature clusters was evaluated in the same setting as DGF. Then, we evaluated the generalization abil-ity and stability of the feature clusters selected based on relevance. Like existing work [16], a cluster was selected for classification if its representative feature was among the top k ( k&lt; K) according to relevance score. The rest of the pro-cedure was the same as in evaluating DRAGS. In addition, we also tested the classification performance using represen-tative features from all K clusters like in [3] and found the results (not included in the paper) were consistently inferior than those from the above approach.

Additionally, we evaluated the performance of a well-known feature selection algorithm for small sample classification, SVM-RFE (RFE in short) [14], under the same setting as DRAGS. RFE recursively eliminates features based on SVM. At each iteration, it trains an SVM classifier, ranks features according to some score function, and eliminates one or more features with the lowest scores. Since RFE is computation-ally intensive, as in [14], we chose to first eliminate half of the remaining features at each iteration and then switch to one feature at a time when only a small number of features (50 in these experiments) were left. Figure 1: Various distributions of average Pearson correlation of K-Nearest Neighbors for all features, under K=1,2,...,10,15,20,25, for Colon data.

For DRAGS, we need to specify the number of nearest neighbors considered by each feature in order to determine the kernel bandwidth h for the mean shift procedure. Fig-ure 1 depicts a series of distributions of average Pearson correlation of KNNs for all features under increasing val-ues of K (from right to left) in Colon data set. We can see that the distributions are highly skewed and show little spread before K reaches 4, which indicates that the average Pearson correlation of KNNs does not adequately capture the heterogeneity of the underlying density distribution un-der those small values of K. Such information can be easily obtained based on pair-wise Pearson correlation among all features. We examined several data sets and observed very similar trend as in Figure 1. In order to find coherent dense feature groups, we prefer a K value which is small but able to capture the heterogeneity of the data. In our experiments, we uniformly set K=5 for all the data sets.
In this section, we evaluate the stability of dense feature groups produced by DGF under sample hold out based on the stability measure in (7), which has two forms Sim M V and Sim M ID depending on whether the similarity is measured based on feature values or feature indices. We also compare the stability of DGF with K-means algorithm under the set-ting described in Section 6.1. To compute Sim M V , for both DGF and K-means, Pearson correlation of group centers is used to determine a maximum matching between two sets of feature groups R and R i ( R from the full data set and R i from the i th random subset). To compute Sim DGF, features in each dense group are used to determine a maximum matching. For K-means, a maximum matching is determined in two ways: using all features in each cluster regardless of its size or 5 features closest to each cluster cen-ter (up to 5 if there are less than 5 features in the cluster). The higher stability value between the two is reported.
Figure 2 reports the stability values of DGF and K-means based on Sim M V and Sim M ID for each of the six microarray data sets used in our study. We can clearly observe from every data set that DGF is highly stable in terms of both measures when the top k ( k =4, 6,..., 50) dense groups are evaluated. For all data sets except SRBCT, the stability score based on Sim M V is almost perfect for every k value, indicating Pearson correlation is almost 1 for all pairs of group centers under the best matching between two sets of feature groups. This observation verifies that density peaks in the sample space are highly stable with respect to sample hold out (even when 1/3 of the samples were removed in our experiments). The stability scores based on Sim M ID show the same trend, although they are less perfect than Sim M Overall, more than 70% of the features in one dense feature group match with those in its matching group under the best matching for most k values, in five out of the six data sets. This further verifies that dense feature groups around density peaks are highly stable as well.

In contrast, K-means is much less stable than DRAGS in terms of both measures with only one exception (SRBCT, K=4). As the number of feature clusters increases, the sta-bility of K-means degrades, that is, the resulting clusters become more sensitive to the variations of the dimensions (samples) included in computing the similarity between fea-tures. For all data sets, the Sim M ID scores are close to 0 for large numbers of clusters (e.g., k&gt; 20), which indicates al-most no overlap between any pair of matching clusters, con-sidering either all features in each cluster or several closest features to each cluster center. These observations suggest that grouping features without considering the density of feature groups is not effective for stable feature selection.
We now evaluate the generalization ability and stability of selected feature groups by DRAGS. We also compare DRAGS with K-means based feature selection and RFE fea-ture selection algorithm under the previously described set-ting. Table 2 compares the average predictive accuracies (over 30 folds) for SVM and 1NN based on selection results from these three algorithms under a wide range of k , where k stands for the number of feature groups for DRAGS and K-means, and the number of features for RFE, respectively. The last value in each row is the average of accuracies across all the k values for each algorithm.
 Between DRAGS and RFE, the accuracies resulted from DRAGS are significantly higher than those from RFE under all values of k for two data sets (Colon and SRBCT). For the other four data sets, DRAGS performs similar to RFE un-der large k values, but significantly higher when k  X  10 (ex-cept Lymphoma). Such observations suggest that features in each dense group selected by DRAGS are coherent in terms of class discrimination, and therefore, good accuracy can be achieved by using representative features (one from each group) from only a few most relevant feature groups, rather than using a large subset of dozens of features like RFE. More importantly, DRAGS not only provides k features for classification, but also includes in its result features highly correlated to these k features. This is desirable for appli-cations where the goal of knowledge discovery is to iden-tify features best explaining the differences between classes. Comparing DRAGS with K-means based feature selection, the accuracies resulted from DRAGS are significantly higher than those from K-means under all values of k for SRBCT, and generally similar to those from K-means for the other five data sets.

At last, we evaluate the stability of DRAGS, K-means, and RFE. Figure 3 shows the stability of the three algo-rithms. We can clearly observe from all data sets that DRAGS remains highly stable in terms of both measures based on the top k relevant feature groups selected from dense feature groups. Therefore, we conclude that DRAGS can identify feature groups which together lead to good pre-diction of the class and are stable under sample hold out. K-means remains much less stable than DRAGS when the the top k relevant feature clusters among all K clusters are measured. For RFE, its stability values based on Sim M ID are consistently almost zero under any k value for all data sets, which shows that almost none of the features selected from a training fold matches with the set of features selected from the full data set. Its stability values based on Sim are higher due to the correlation between features selected based on a training fold and those selected based on the full data set, but RFE is overall much less stable than DRAGS. Such observations indicate that RFE is ineffective in provid-ing stable results under training data variations, although it can select large subsets of features of good prediction.
In this paper, we have identified the importance of stable feature selection, and proposed a general feature selection framework for stable feature selection based on dense fea-ture groups. We have also proposed a general measure of stability. Our empirical study based on various microarray data sets has verified that the proposed framework is effec-tive for stable feature selection, and the DRAGS algorithm developed within this framework produces feature groups which together lead to good classification accuracy and are stable under sample hold out.

Because DRAGS limits the selection of relevant feature groups from dense feature groups identified by DGF, DRAGS may not necessarily include some of the most relevant fea-tures determined according to individual feature ranking in any of its selected feature groups, if those features are lo-cated in the sparse region of the data distribution. Some improvements to DRAGS can be studied in the future work. Another interesting future direction is to develop additional feature selection algorithms under the proposed framework, for example, by using other methods to evaluate the rele-vance of dense feature groups. Figure 3: Stability of DRAGS, K-means, and RFE according to Sim
We would like to thank Yue Han for his effort in imple-menting the SVM-RFE algorithm and obtaining compara-tive results for it. [1] U. Alon, N. Barkai, D. A. Notterman, et al. Broad [2] A. Appice, M. Ceci, S. Rawles, and P. Flach.
 [3] W. Au, K. C. C. Chan, A. K. C. Wong, and Y. Wang. [4] M. Berens, H. Liu, L. Parsons, L. Yu, and Z. Zhao. [5] R. Butterworth, G. Piatetsky-Shapiro, and D. A. [6] B. Cao, D. Shen, J. Sun, Q. Yang, and Z. Chen. [7] Y. Cheng. Mean shift, mode seeking, and clustering. [8] D. Comaniciu and P. Meer. Mean shift: a robust [9] M. Dash and H. Liu. Consistency-based search in [10] C. A. Davis, F. Gerick, V. Hintermair, C. C. Friedel, [11] C. Ding and H. Peng. Minimum redundancy feature [12] L. Ein-Dor, I. Kela, G. Getz, D. Givol, and [13] G. Forman. An extensive empirical study of feature [14] I. Guyon, J. Weston, S. Barnhill, and V. Vapnik. Gene [15] T. Hastie, R. Tibshirani, D. Botstein, and P. Brown. [16] R. J  X  ornsten and B. Yu. Simultaneous gene clustering [17] A. Kalousis, J. Prados, and M. Hilario. Stability of [18] R. Kohavi and G. H. John. Wrappers for feature [19] D. Koller and M. Sahami. Toward optimal feature [20] T. Li, C. Zhang, and M. Ogihara. A comparative [21] H. Liu and L. Yu. Toward integrating feature selection [22] H. Peng, F. Long, and C. Ding. Feature selection [23] M. S. Pepe, R. Etzioni, Z. Feng, et al. Phases of [24] M. Robnik-Sikonja and I. Kononenko. Theoretical and [25] R. L. Somorjai, B. Dolenko, and R. Baumgartner. [26] M. P. Wand and M. C. Jones. Kernel Smoothing . [27] L. Wang, J. Zhu, and H. Zou. Hybrid huberized [28] I. H. Witten and E. Frank. Data Mining -Pracitcal [29] E. Xing, M. Jordan, and R. Karp. Feature selection [30] L. Yu and H. Liu. Efficient feature selection via [31] Y. Zhang, C. Ding, and T. Li. A two-stage gene
