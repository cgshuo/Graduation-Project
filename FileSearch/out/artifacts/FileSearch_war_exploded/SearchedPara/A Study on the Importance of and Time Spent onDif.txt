 Applying data mining and machine learning algorithms re-quires many steps to prepare data and to make use of mod-eling results. This study investigates two questions: (1) how time consuming are the pre-and post-processing steps? (2) how much research energy is spent on these steps? To an-swer these questions I surveyed practitioners about their ex-periences in applying modeling techniques and categorized data mining and machine learning research papers from 2009 according to the modeling step(s) they addressed. Survey results show that model building consumes only 14% of the time spent on a typical project; the remaining time is spent on pre-and post-processing steps. Both survey responses and the categorization of research papers show that data mining and machine learning researchers spend the major-ity of their energy on algorithms for constructing models and significantly less energy on other steps. These findings collectively suggest that there are research opportunities to simplify the steps that precede and follow model building. Many steps are involved in applying machine learning and data mining to real problems [1; 2; 3; 6; 12]. The heart of this iterative process is the actual machine learning and/or data mining step, during which the practitioner poses the problem, selects or designs an algorithm, tunes hyperparam-eters, etc. The output of this step is typically a model that can be used to make predictions about future data (e.g., a decision tree) or that helps summarize and visualize the in-put data (e.g., a dendrogram produced by hierarchical clus-tering). Before model learning, however, the data itself must be collected and prepared. Similarly, much work can be re-quired after the model is learned to understand, evaluate, and make use of the modeling results.
 Conventional wisdom, accumulated from individual experi-ences, holds that these steps are both time consuming and crucial to successful applications, but little published data exists to support or disprove this belief. Existing studies of the end-to-end modeling process state that model build-ing comprises only a small portion of project time [1; 3], and occasionally estimate the size of this step (e.g.,  X 15 to 25% of the overall e  X  ort X  [6, p. 90]), but do not provide quantitative data or citations to support the observations. The two exceptions are the 2nd Annual Data Miner Survey (which found that 20% of project time was spent generat-ing models) [8] and a KDnuggets poll (which found that the majority of data miners spend 60% or more of their time on data cleaning and preparation) [4].
 the survey on ml-news@googlegroups.com and KDnuggets. Twenty-four respondents completed the survey during this period. I discarded two of these that contained dummy val-ues for all questions (e.g., zero percent time spent during all modeling stages), leaving 22 survey responses. 3 The initial results from the first run were posted on my web page in early March 2010 and were published in my dissertation [5]. In mid-March I re-opened the survey but only advertised it with a note on the web page of initial results. Two com-pleted surveys were collected over March and April. The second major data collection spanned September 2010 to 10 January 2011. During this second round, I announced the survey on www.metaoptimize.com , ml-news@googlegroups. com , and corpora@uib.no . Thirty-three people completed surveys during the second round, resulting in 57 total re-sponses.
 The survey questions are reproduced in Figure 1. Time percentages were normalized to sum to 100 for each survey response to correct nine responses that did not sum to 100. This preprocessing put all the responses on the same scale and facilitated comparing the relative energy spent in each step. Times for eight responses originally summed to values between 90 and 110, and normalization produced mi-nor adjustments in percentage values. The times in the ninth response originally summed to 27%, and the survey partic-ipant commented that the survey did not include the steps where the remaining time was spent (e.g., project planning, finding a good learning algorithm, publishing results). Nor-malization converted this answer to how much relative time was spent among the steps included in the survey. In parallel with collecting the initial survey results, I man-ually categorized the papers published at ICML 2009 and KDD 2009. This categorization was completed before ana-lyzing the survey results. Each paper was labeled as address-ing one of eight categories: the six modeling stages from the survey (see Figure 1) plus two extra categories. The Domain Knowledge category covered papers reporting new domain knowledge or ways to take advantage of domain knowledge. The final category, Other , captured papers that did not fit easily elsewhere.
 Conclusions from this categorization are limited by the fact that it is based on one person X  X  subjective judgment. Only big picture trends can be considered reliable, and the exact percentages of papers in each category should be viewed with skepticism. A future study with multiple annotators could repeat this categorization if the exact proportions per category is su ciently interesting to the community. Fifty-seven completed surveys were collected. Respondents varied greatly in their experience (Table 1). Areas of ex-pertise included medicine, robot control, natural language processing, customer modeling and retention, advertising, fi-nance, computer vision, bioinformatics, semantic audio pro-cessing, and child language acquisition.
In the comments of one discarded survey, a respondent stated that he/she simply wanted to view all of the survey questions. Table 1: Number of systems completed by survey respon-dents. Figure 2: Allocation of time spent building systems with ma-chine learning or data mining components. Time estimates were collected from practitioners with experience deploying systems. Boxes show the 25th and 75th quantiles of time spent per stage; the line within each box marks the median time spent. Whiskers show the minimum and maximum time spent.
 The relative time spent in each stage also varied greatly by project (Figure 2). Data collection and preparation were the most time consuming stages, based on median values (both 20% of project time). In the typical project, only 14% of the e  X  ort was actually spent learning the model. In comparison, practitioners spent 10% of project time on each of the other steps (median values). In other words, the stages that pre-cede and follow model building are individually roughly as time consuming as learning the model. However, projects with such an even distribution of e  X  ort across all steps were relatively rare. Out of 57 responses, only four reported time allocations in which the longest and shortest step were sep-arated by 15 percentage points or less. Instead, individual projects usually required larger time commitments on one or two stages and smaller time commitments for a single step (usually 5% or less of project time).
 Respondents generally rated most modeling steps as impor-tant to building successful systems; in contrast, they felt that the research community focuses the bulk of its energy on learning algorithms (Figure 3 vs. Figure 4). As with an-swers about how much time was spent per step, respondents individually attributed varying importances to the di  X  erent steps. Twenty-two respondents rated at least one step not important or slightly important . Conversely, 51/57 partici-pants rated four or more steps important (or critically im-portant ), and 30/57 participants rated five or more steps important. Unlike the relative evenness of time spent per energy or enormous energy on how to learn a model. Table 2: Time spent on various data mining tasks. Sam-ple size was 265 data miners. Source: Rexer Analytics [8]. Reproduced with permission.
 More surprising is how consistently (and highly) respondents rated the importance of all the modeling steps. Access to data and dirty data are consistently reported as the biggest challenges to data miners [7, p. xvii] [8; 9; 10; 11], and counted in multiple categories.
 While the majority of respondents felt that all modeling steps are important to success, the research community is strongly focused on the model building step. This can be seen in the topics of recent conference papers and in the an-swers of survey respondents. This supports previous quali-tative observations: The results in Figures 4 and 5 hint that more research is being done today on other modeling steps than is described in the above quotes from the mid-1990 X  X . One plausible hy-pothesis is that interest in the practical issues around data mining and machine learning has been growing as learning algorithms have been applied to new tasks. An interest-ing study would be to track the number of research papers addressing di  X  erent modeling steps from the 1990 X  X  to the present day. It is important to note the potential limitations of the re-sults in Section 3. First and most importantly, the survey is prone to self-selection bias because respondents decided to participate or not based on their personal motivations. As engineering.
 This di  X  erence also represents a research opportunity. It seems fair to say that the practical steps preceding and following model building are the limiting factor for data driven analysis and applications. To maximize the impact and adoption of our data mining and machine learning algo-rithms, we should strive to simplify the other steps as much as possible. While many application obstacles are task spe-cific (making general purpose solutions unrealistic), there re-main modeling issues that span applications (e.g., handling missing values, detecting data outliers, estimating predic-tion reliability). Innovations that remove or mitigate these issues have the potential to change how and where learning algorithms are applied.
 The participation of dozens of data mining and machine learning practitioners and researchers made this study pos-sible: thank you to everyone who completed the survey. I am grateful to Rich Caruana for suggesting the idea of con-ducting a survey and for helpful comments throughout the work. Thank you to Yisong Yue, Daria Sorokina, Philip Kegelmeyer, and Alex Niculescu-Mizil for thoughtful com-ments on an early draft. Finally, thank you to Karl Rexer at Rexer Analytics for sharing the results of their Annual Data Miner Surveys.
 This work was started at Cornell University and completed at Sandia National Laboratories. It was funded by the Na-tional Science Foundation (grants DUE-0734857 and IIS-0612031) and by an Early Career Award from the Labora-tory Directed Research &amp; Development (LDRD) program at Sandia National Laboratories. Sandia National Labo-ratories is a multiprogram laboratory operated by Sandia Corporation, a wholly owned subsidiary of Lockheed Mar-tin Corporation, for the United States Department of En-ergy X  X  National Nuclear Security Administration under con-tract DE-AC04-94AL85000. [1] C. E. Brodley and P. Smyth. Applying classifica-[2] P. Chapman, J. Clinton, R. Kerber, T. Khabaza, [3] U. Fayyad, G. Piatetsky-Shapiro, and P. Smyth. From [4] KDnuggets. What percent of time in your data min-[5] M. A. Munson. Outside the Machine Learning Black-[6] G. Piatetsky-Shapiro, R. Brachman, T. Khabaza, 4. Anonymous:  X . . . when you X  X e not working on the bench-The text of the survey announcement is included here for completeness.

