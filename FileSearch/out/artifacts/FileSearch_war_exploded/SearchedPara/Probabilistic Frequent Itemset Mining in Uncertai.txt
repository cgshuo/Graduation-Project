 Probabilistic frequent itemset mining in uncertain trans-action databases semantically and computationally differs from traditional techniques applied to standard  X  X ertain X  transaction databases. The consideration of existential un-certainty of item(sets), indicating the probability that an item(set) occurs in a transaction, makes traditional tech-niques inapplicable. In this paper, we introduce new prob-abilistic formulations of frequent itemsets based on possible world semantics. In this probabilistic context, an itemset X is called frequent if the probability that X occurs in at least minSup transactions is above a given threshold  X  . To the best of our knowledge, this is the first approach ad-dressing this problem under possible worlds semantics. In consideration of the probabilistic formulations, we present a framework which is able to solve the Probabilistic Frequent Itemset Mining (PFIM) problem efficiently. An extensive experimental evaluation investigates the impact of our pro-posed techniques and shows that our approach is orders of magnitude faster than straight-forward approaches.
 H.2.8 [ Database Applications ]: Data Mining Algorithms, Theory Uncertain Databases, Frequent Itemset Mining, Probabilis-tic Data, Probabilistic Frequent Itemsets
Association rule analysis is one of the most important fields in data mining. It is commonly applied to market-basket databases for analysis of consumer purchasing be-haviour. Such databases consist of a set of transactions, each containing the items a customer purchased. The most important and computationally intensive step in the mining process is the extraction of frequent itemsets  X  sets of items that occur in at least minSup transactions.

It is generally assumed that the items occurring in a trans-action are known for certain. However, this is not always the case. For instance;
In such applications, the information captured in transac-tions is uncertain since the existence of an item is associated with a likelihood measure or existential probability. Given an uncertain transaction database, it is not obvious how to identify whether an item or itemset is frequent because we generally cannot say for certain whether an itemset ap-pears in a transaction. In a traditional (certain) transaction database, we simply perform a database scan and count the transactions that include the itemset. This does not work in an uncertain transaction database.

Dealing with such databases is a difficult but interesting problem. While a naive approach might transform uncer-tain items into certain ones by thresholding the probabili-ties, this loses useful information and leads to inaccuracies. Existing approaches in the literature are based on expected support, first introduced in [6]. Chui et. al. [5, 6] take the uncertainty of items into account by computing the ex-pected support of itemsets. Itemsets are considered frequent if the expected support exceeds minSup . Effectively, this approach returns an estimate of whether an object is fre-quent or not with no indication of how good this estimate is. Since uncertain transaction databases yield uncertainty w.r.t. the support of an itemset, the probability distribution of the support and, thus, information about the confidence of the support of an itemset is very important. This in-formation, while present in the database, is lost using the expected support approach.

Example 1. Consider a department store. To maximize sales, customers can be analysed to find sets of items that are (a) Uncertain Trans-action Database Figure 1: Example application of an uncertain trans-action database. all purchased by a large group of customers. This informa-tion can be used for advertising directed at this group. For example, by providing special offers that include all of these items along with new products, the store can encourage new purchases. Figure 1(a) shows such customer information. Here, customer A purchases games every time he visits the store and music (CDs) 20% of the time. Customer B buys music in 70% of her visits and videos (DVDs) in 40% of them. The supermarket uses a database that represents each customer as a single uncertain transaction , also shown in Figure 1(a).
The uncertain data model applied in this paper is based on the possible worlds semantic with existential uncertain items .

Definition 2. An uncertain item is an item x  X  I whose presence in a transaction t  X  T is defined by an existential probability P ( x  X  t )  X  (0 , 1) . A certain item is an item where P ( x  X  t )  X  X  0 , 1 } . I is the set of all possible items.
Definition 3. An uncertain transaction t is a transac-tion that contains uncertain items. A transaction database T containing uncertain transactions is called an uncertain transaction database .

An uncertain transaction t is represented in an uncertain transaction database by the items x  X  I associated with an existential probability value 1 P ( x  X  t )  X  (0 , 1]. Example uncertain transaction databases are depicted in Figures 1 and 2. To interpret an uncertain transaction database we apply the possible world model. An uncertain transaction database generates possible worlds , where each world is de-fined by a fixed set of (certain) transactions . A possible world is instantiated by generating each transaction t i  X  T according to the occurrence probabilities P ( x  X  t i ). Con-sequently, each probability 0 &lt; P ( x  X  t i ) &lt; 1 derives two possible worlds per transaction : One possible world in which x exists in t i , and one possible world where x does not exist in t i . Thus, the number of possible worlds of a database increases exponentially in both the number of transactions and the number of uncertain items contained in it.

Each possible world w is associated with a probability that that world exists, P ( w ). Figure 1(b) shows all possible
If an item x has an existential probability of zero, it does not appear in the transaction.
 Figure 2: Example of a larger uncertain transaction database. worlds derived from Figure 1(a). For example, in world 6 both customers bought music, customer B decided against a new video and customer A bought a new game.

We assume that uncertain transactions are mutually inde-pendent. Thus, the decision by customer A has no influence on customer B. This assumption is reasonable in real world applications. Additionally, independence between items is often assumed in the literature [5, 6]. This can be justi-fied by the assumption that the items are observed indepen-dently. In this case, the probability of a world w is given by: For example, the probability of world 5 in Figure 1(b) is P ( Game  X  t A )  X  (1  X  P ( Music  X  t A ))  X  P ( Music  X  t (1  X  P ( V ideo  X  t B )) = 1 . 0  X  0 . 8  X  0 . 7  X  0 . 6 = 0 . 336.
In the general case, the occurrence of items may be de-pendent. For example, the decision to purchase a new music video DVD may mean they are unlikely to purchase a mu-sic CD by the same artist. Alternatively, some items must be bought together. If these conditional probabilities are known, they can be used in our methods. For example, the probability that both a video and music are purchased by customer B is P ( { V ideo,Music }  X  t B ) = P ( V ideo  X  t )  X  P ( Music  X  t B | V ideo  X  t B ).
An itemset is a frequent itemset if it occurs in at least minSup transactions, where minSup is a user specified pa-rameter. In uncertain transaction databases however, the support of an itemset is uncertain; it is defined by a dis-crete probability distribution function (p.d.f). Therefore, each itemset has a frequentness probability 2  X  the probabil-ity that it is frequent. In this paper, we focus on the problem of efficiently calculating this p.d.f. and extracting all proba-bilistic frequent itemsets ;
Definition 4. A Probabilistic Frequent Itemset (PFI) is an itemset with a frequentness probability of at least  X  .
The parameter  X  is the user specified minimum confidence in the frequentness of an itemset.

We are now able to specify the Probabilistic Frequent Item-set Mining (PFIM) problem as follows; Given an uncertain transaction database T , a minimum support scalar minSup and a frequentness probability threshold  X  , find all proba-bilistic frequent itemsets.
Frequentness is the rarely used word describing the prop-erty of being frequent.
We make the following contributions: The remainder of this paper is organised as follows; Section 2 surveys related work. Section 3 presents our probabilistic support framework. Section 4 shows how to compute the frequentness probability in O ( | T | ) time. Section 5 presents a probabilistic frequent itemset mining algorithm. Section 6 presents our incremental algorithm. We present our experi-ments in Section 7 and conclude in Section 8.
There is a large body of research on Frequent Itemset Min-ing (FIM) but very little work addresses FIM in uncertain databases [5, 6, 11]. The approach proposed by Chui et. al [6] computes the expected support of itemsets by sum-ming all itemset probabilities in their U-Apriori algorithm. Later, in [5], they additionally proposed a probabilistic fil-ter in order to prune candidates early. In [11], the UF-growth algorithm is proposed. Like U-Apriori, UF-growth computes frequent itemsets by means of the expected sup-port, but it uses the FP-tree [9] approach in order to avoid expensive candidate generation. In contrast to our proba-bilistic approach, itemsets are considered frequent if the ex-pected support exceeds minSup . The main drawback of this estimator is that information about the uncertainty of the expected support is lost; [5, 6, 11] ignore the number of pos-sible worlds in which an itemsets is frequent. [18] proposes exact and sampling-based algorithms to find likely frequent items in streaming probabilistic data. However, they do not consider itemsets with more than one item. Finally, except for [15], existing FIM algorithms assume binary valued items which precludes simple adaptation to uncertain databases. To the best of our knowledge, our approach is the first that is able to find frequent itemsets in an uncertain transaction database in a probabilistic way. Assuming minSup is a constant.

Existing approaches in the field of uncertain data man-agement and mining can be categorized into a number of research directions. Most related to our work are the two categories  X  probabilistic databases  X  [4, 12, 13, 3] and  X  proba-bilistic query processing  X  [7, 10, 17, 14].

The uncertainty model used in our approach is very close to the model used for probabilistic databases. A probabilis-tic database denotes a database composed of relations with uncertain tuples [7], where each tuple is associated with a probability denoting the likelihood that it exists in the rela-tion. This model, called  X  tuple uncertainty  X , adopts the pos-sible worlds semantics [3]. A probabilistic database repre-sents a set of possible  X  X ertain X  database instances (worlds), where a database instance corresponds to a subset of un-certain tuples. Each instance (world) is associated with the probability that the world is  X  X rue X . The probabilities re-flect the probability distribution of all possible database in-stances. In the general model description [13], the possible worlds are constrained by rules that are defined on the tu-ples in order to incorporate object (tuple) correlations. The ULDB model proposed in [4], which is used in Trio [1], sup-ports uncertain tuples with alternative instances which are called x-tuples. Relations in ULDB are called x-relations containing a set of x-tuples. Each x-tuple corresponds to a set of tuple instances which are assumed to be mutually exclusive, i.e. no more than one instance of an x-tuple can appear in a possible world instance at the same time. Prob-abilistic top-k query approaches [14, 17, 12] are usually asso-ciated with uncertain databases using the tuple uncertainty model. The approach proposed in [17] was the first approach able to solve probabilistic queries efficiently under tuple in-dependency by means of dynamic programming techniques. In our paper, we adopt the dynamic programming technique for the efficient computation of frequent itemsets in a prob-abilistic way.
Recall that previous work was based on the expected sup-port [5, 6, 11].
 Definition 5. Given an uncertain transaction database T , the expected support E ( X ) of an itemset X is defined as E ( X )= P t  X  T P ( X  X  t ) .
 Considering an itemset frequent if its expected support is above minSup has a major drawback. Uncertain transaction databases naturally involve uncertainty concerning the sup-port of an itemset. Considering this is important when eval-uating whether an itemset is frequent or not. However, this information is forfeited when using the expected support ap-proach. Let us return to the example shown in Figure 2. The expected support of the itemset { D } is E ( { D } ) = 3 . 0. The fact that { D } occurs for certain in one transaction, namely in t 2 , and that there is at least one possible world where X occurs in five transactions are totally ignored when using the expected support in order to evaluate the frequency of an itemset. Indeed, suppose minSup = 3; do we call { D } frequent? And if so, how certain can we even be that { D } is frequent? By comparison, consider itemset { G } . This also has an expected support of 3, but its presence or absence in transactions is more certain. It turns out that the prob-ability that { D } is frequent is 0 . 7 and the probability that G is frequent is 0 . 91. While both have the same expected support, we can be quite confident that { G } is frequent, in contrast to { D } . An expected support based technique does not differentiate between the two.

The confidence with which an itemset is frequent is very important for interpreting uncertain itemsets. We there-fore require concepts that allow us to evaluate the uncertain data in a probabilistic way. In this section, we formally in-troduce the concept of probabilistic frequent itemsets.
In uncertain transaction databases, the support of an item or itemset cannot be represented by a unique value, but rather, must be represented by a discrete probability distri-bution.
 Definition 6. Given an uncertain (transaction) database T and the set W of possible worlds (instantiations) of T , the support probability P i ( X ) of an itemset X is the probability that X has the support i . Formally, where S ( X,w j ) is the support of X in world w j . Intuitively, P i ( X ) denotes the probability that the support of X is exactly i . The support probabilities associated with an itemset X for different support values form the support probability distribution of the support of X .

Definition 7. The probabilistic support of an itemset X in an uncertain transaction database T is defined by the sup-ues i  X  { 0 ,..., | T |} . This probability distribution is called support probability distribution . The following statement Returning to our example of Figure 2, Figure 4(a) shows the support probability distribution of itemset { D } .

The number of possible worlds | W | that need to be con-sidered for the computation of P i ( X ) is extremely large. In fact, we have O (2 | T | X | I | ) possible worlds, where | I | denotes the total number of items. In the following, we show how to compute P i ( X ) without materializing all possible worlds.
Lemma 8. For an uncertain transaction database T with mutually independent transactions and any 0  X  i  X | T | , the support probability P i ( X ) can be computed as follows:
P i ( X ) = X (a) Support proba-bility distribution of { D } Figure 4: Probabilistic support of itemset X = { D } in the uncertain database of Figure 2.
 Note that the transaction subset S  X  T contains exactly i transactions.

Proof. The transaction subset S  X  T contains i transac-tions. The probability of a world w j where all transactions in S contain X and the remaining | T  X  S | transactions do not contain X is P ( w j ) = Q t  X  S P ( X  X  t )  X  Q t  X  T  X  S t )). The sum of the probabilities according to all possi-ble worlds fulfilling the above conditions corresponds to the equation given in Definition 6.
Recall that we are interested in the probability that an itemset is frequent, i.e. the probability that an itemset oc-curs in at least minSup transactions.

Definition 9. Let T be an uncertain transaction database and X be an itemset. P  X  i ( X ) denotes the probability that the support of X is at least i , i.e. P  X  i ( X ) = P | T | k = i given minimal support minSup  X  { 0 ,..., | T |} , the probabil-ity P  X  minSup ( X ) , which we call the frequentness probability of X , denotes the probability that the support of X is at least minSup .

Figure 4(b) shows the frequentness probabilities of { D } for all possible minSup values in the database of Figure 2. For example, the probability that { D } is frequent when minSup = 3 is approximately 0 . 7, while its frequentness probability when minSup = 4 is approximately 0 . 3.

The intuition behind P  X  minSup ( X ) is to show how confi-dent we are that an itemset is frequent. With this policy, the frequentness of an itemset becomes subjective and the decision about which candidates should be reported to the user depends on the application. Hence, we use the mini-mum frequentness probability  X  as a user defined parameter. Some applications may need a low  X  , while in other applica-tions only highly confident results should be reported (high  X  ).
 In the possible worlds model we know that P  X  i ( X ) = P to Equation 1 by
P  X  i ( X ) = X
Hence, the frequentness probability can be calculated by enumerating all possible worlds satisfying the minSup condi-tion through the direct application of Equation 2. This naive approach is very inefficient however. We can speed this up significantly. First, note that typically minSup &lt;&lt; | T | and the number of worlds with support i is at most | T | Hence, enumeration of all worlds w in which the support of X is greater than minSup is much more expensive than enu-merating those where the support is less than minSup . Using the following easily verified Lemma, we can compute the fre-quentness probability exponentially in minSup &lt;&lt; | T | . Q
Despite this improvement, the complexity of the above ap-proach, called Basic in our experiments, is still exponential w.r.t. the number of transactions. In Section 4 we describe how we can reduce this to linear time.
This section presents our dynamic programming approach, which avoids the enumeration of possible worlds in calculat-ing the frequentness probability and the support distribu-tion. We also present probabilistic filter and pruning strate-gies which further improve the run time of our method.
The key to our approach is to consider it in terms of sub-problems. First, we need appropriate definitions;
Definition 11. The probability that i of j transactions contain itemset X is
P i,j ( X ) = X where T j = { t 1 ,...,t j }  X  T is the set of the first j transac-tions. Similarly, the probability that at least i of j transac-tions contain itemset X is P
Note that P  X  i, | T | ( X ) = P  X  i ( X ), the probability that at least i transactions in the entire database contain X . The key idea in our approach is to split the problem of computing P  X  i, | T | ( X ) into smaller problems P  X  i,j ( X ), j &lt; | T | . This can be achieved as follows. Given a set of j transactions T j = { t 1 ,...,t j }  X  T : If we assume that transaction t tains itemset X , then P  X  i,j ( X ) is equal to the probability that at least i  X  1 transactions of T j \{ t j } contain X . Oth-erwise, P  X  i,j ( X ) is equal to the probability that at least i transactions of T j \{ t j } contain X . By splitting the problem in this way we can use the recursion in Lemma 12, which tells us what these probabilities are, to compute P  X  i,j by means of the paradigm of dynamic programming.
 Lemma 12. P  X  i,j ( X ) =
P where The above dynamic programming scheme is an adaption of a technique previously used in the context of probabilistic top-k queries by Kollios et. al [17].
 (1  X  P ( X  X  t j ))  X  P  X  i,j  X  1 ( X ).
 Using this dynamic programming scheme, we can compute the probability that at least minSup transactions contain itemset X by calculating the cells depicted in Figure 5. In the matrix, each cell relates to a probability P  X  i,j marked on the x -axis, and j marked on the y -axis. Note that according to Lemma 12, in order to compute a P  X  i,j the cell to the left and the cell to the lower left of P Knowing that P  X  0 , 0 = 1 and P  X  1 , 0 = 0 by definition, we can start by computing P  X  1 , 1 . The probability P  X  1 ,j can then be computed by using the previously computed P  X  1 ,j  X  1 all j . P  X  1 ,j can, in turn, be used to compute P iteration continues until i reaches minSup , so that finally we obtain P  X  minSup, | T |  X  the frequentness probability (Def-inition 9).
 Note that in each line (i.e. for each i ) of the matrix in Figure 5, j only runs up to | T | X  minSup + i . Larger values of j are not required for the computation of P minSup, | T |
Lemma 13. The computation of the frequentness proba-bility P  X  minSup requires at most O ( | T | X  minSup) = O ( | T | ) time and at most O ( | T | ) space.

Proof. Using the dynamic computation scheme as shown in Figure 5, the number of computations is bounded by the size of the depicted matrix. The matrix contains | T | X  minSup cells. Each cell requires an iteration of the dynamic com-putation (c.f. Corollary 12) which is performed in O (1) time. Note that a matrix is used here for illustration purpose only. The computation of each probability P i,j ( X ) only re-quires information stored in the current line and the previous line to access the probabilities P i  X  1 ,j  X  1 ( X ) and P Therefore, only these two lines (of length | T | ) need to be preserved requiring O ( | T | ) space. Additionally, the proba-bilities P ( X  X  t j ) have to be stored, resulting in a total of O ( | T | ) space.

Note that we can save computation time if an itemset is certain in some transactions. If a transaction t j  X  T contains itemset X with a probability of zero, i.e. P ( X  X  t j ) = 0, transaction t j can be ignored for the dynamic computation because P  X  i,j ( X ) = P  X  i,j  X  1 ( X ) holds (Lemma 12). If | T is less than minSup , then X can be pruned since, by defini-tion, P  X  minSup ,T 0 = 0 if minSup &gt; T 0 . The dynamic compu-tation scheme can also omit transactions T j where the item has a probability of 1, because P  X  i,j ( X ) = P  X  i  X  1 ,j  X  1 due to P ( X  X  t j ) = 1. Thus, if a transaction t j contains X with a probability of 1, then t j (i.e. the corresponding column) can be omitted if minSup is reduced by one, to com-pensate the missing transaction. The dynamic programming scheme therefore only has to consider uncertain items. We call this trick 0-1-optimization .
To further reduce the computational cost, we introduce probabilistic filter strategies. These reduce the number of probability computations in the dynamic algorithm. Our probabilistic filter strategies exploit the following monotonic-ity criteria;
First, if we increase the minimal support i , then the fre-quentness probability of an itemset decreases.
 P Intuitively, this result is obvious since the predicate  X  X he support is at least i  X  implies  X  X he support is at least i + 1 X . The next criterion says that an extension of the uncertain transaction database leads to an increase of the frequentness probability of an itemset.
 P  X  i,j ( X )  X  (1  X  P ( X  X  t j +1 )) t The intuition behind this lemma is that one more transac-tion can increase the support of an itemset. Putting these results together; P t Next, we describe how these monotonicity criteria can be exploited to prune the dynamic computation.
Lemma 16 can be used to quickly identify non-frequent itemsets. Figure 6 shows the dynamic programming scheme for an itemset X . Keep in mind that the goal is to com-pute P minSup , | T | ( X ). Lemma 16 states that the probabilities P 6), are conservative bounds of P minSup , | T | ( X ). Thus, if any lower than the user specified parameter  X  , then X can be pruned.
Figure 6: Visualization of the Pruning Criterion
We now have the techniques required to efficiently identify whether a given itemset X is a probabilistic frequent itemset (PFI). In this section, we show how to find all probabilistic frequent itemsets in an uncertain transaction database. Tra-ditional frequent itemset mining is based on support prun-ing by exploiting the anti-monotonic property of support: S ( X )  X  S ( Y ) where S ( X ) is the support of X and Y  X  X . In uncertain transaction databases however, recall that support is defined by a probability distribution and that we mine itemsets according to their frequentness probabil-ity. It turns out that the frequentness probability is anti-monotonic:
Lemma 17.  X  Y  X  X : P  X  minSup ( X )  X  P  X  minSup ( Y ) . In other words, all subsets of a probabilistic frequent itemset are also probabilistic frequent itemsets.
 the probability is defined over all possible worlds. Here, I is an indicator variable that is 1 when z = true and 0 oth-erwise. In other words, P  X  i ( X ) is the relative number of worlds in which S ( X )  X  minSup holds, where each occur-rence is weighted by the probability of the world occurring. Since world w i corresponds to a normal transaction database with no uncertainty, S ( X,w i )  X  S ( Y,w i )  X  Y  X  X due to the anti-monotonicity of support. Therefore, I S ( X,w i )  X  minSup I P  X  i ( Y ) ,  X  Y  X  X .

We can use the contra-positive of Lemma 17 to prune the search space for probabilistic frequent itemsets. That is, if an itemset Y is not a probabilistic frequent itemset, i.e. P  X  minSup ( Y ) &lt;  X  , then all itemsets X  X  Y cannot be probabilistic frequent itemsets either.

Our first algorithm is based on a  X  X arriage X  of traditional frequent itemset mining methods and our uncertain item-set identification algorithms. In particular, we propose a probabilistic frequent itemset mining approach based on the Apriori algorithm ([2]). Like Apriori, our method iteratively generates the probabilistic frequent itemsets using a bottom-up strategy. Each iteration is performed in two steps, a join step for generating new candidates and a pruning step for calculating the frequentness probabilities and extracting the probabilistic frequent itemsets from the candidates. The pruned candidates are, in turn, used to generate candidates in the next iteration. Lemma 17 is exploited in the join step to limit the candidates generated and in the pruning step to remove itemsets that need not be expanded.
Our probabilistic frequent itemset mining approach allows the user to control the confidence of the results using  X  . However, since the number of results depends on  X  , it may prove difficult for a user to correctly specify this parameter without additional domain knowledge. Therefore, this Sec-tion shows how to efficiently solve the following problems, which do not require the specification of  X  ; In our incremental algorithm (Algorithm 1), we keep an Active Itemsets Queue (AIQ) that is initialized with all one-item sets. The AIQ is sorted by frequentness probability in descending order. Without loss of generality, itemsets are represented in lexiographical order to avoid generating them more than once. In each iteration of the algorithm, i.e. each call of the getNext() -function, the first itemset X in the queue is removed. X is the next most probable fre-quent itemset because all other itemsets in the AIQ have a lower frequentness probability due to the order on AIQ , and all of X  X  X  supersets (which have not yet been gener-ated) cannot have a higher frequentness probability due to Lemma 17. Before X is returned to the user, it is refined in a candidate generation step. In this step, we create all supersets of X obtained by adding single items x to the end of X , in such a way that the lexiographical order of X  X  x is maintained. These are then added to the AIQ after their respective frequentness probabilities are computed (Section 4). The user can continue calling the getNext() -function un-til he has all required results. Note that during each call of the getNext() -function, the size of the AIQ increases by at most | I | . The maximum size of the AIQ is 2 | I | is no worse than the space required to sort the output of a non-incremental algorithm.
In many applications however, relatively few top prob-abilistic frequent itemsets are required. For instance, the store in Example 1 may want to know the top k = 100. Top-k highest frequentness probability queries can be efficiently computed by using Algorithm 1 and constraining the length Algorithm 1 Incremental Algorithm //initialise AIQ = new PriorityQueue FOR EACH x  X  I
AIQ .add([ x , P  X  minSup ( x )]) //return the next probabilistic frequent itemset getNext() RETURNS X X = AIQ .removeFirst() FOR EACH ( x  X  I \ X : x = lastInLexOrder ( X  X  x )) of the AIQ to k  X  m , where m is the number of highest fre-quentness probability items already returned. Any itemsets that  X  X all off X  the end can safely be ignored. The rational behind this approach is that for an itemset X at position p in the AIQ , p  X  1 itemsets with a higher frequentness than X exist in the AIQ by construction. Additionally, any of the m itemsets that have already been returned must have a higher frequentness probability. Consequently, our top-k algorithm contrains the size of the initial AIQ to k and re-duces its size by one each time a result is reported. The algorithm terminates once the size of the AIQ reaches zero.
In this Section we present efficiency and efficacy experi-ments. First, we give efficiency results obtained utilizing the different methods of computing the probabilistic support (cf. Sections 3 and 4). Then, we discuss the performance and utility of the proposed probabilistic frequent itemset mining algorithms (cf. Sections 5 and 6). In all experiments, the runtime was measured in milliseconds (ms).
We evaluated our frequentness probability calculation meth-ods on several artificial datasets with varying database sizes | T | and densities. The density of an item denotes the ex-pected number of transactions in which an item may be present (i.e. where its existence probability is in (0 , 1]). The probabilities themselves were drawn from a uniform distribution. Note that the density is directly related to the degree of uncertainty. If not stated otherwise, we used a database consisting of 10 , 000 to 10 , 000 , 000 uncertain trans-actions and a density of 0 . 5. The frequentness probability threshold  X  of was set to 0 . 9.

We use the following notations for our frequentness prob-ability algorithms: Basic : basic probability computation (Section 3.2), Dynamic : dynamic probability computation (Section 4.1), Dynamic+P : dynamic probability compu-tation with pruning (Section 4.2), DynamicOpt : dynamic probability computation utilizing 0-1-optimization (Section 4.1) and DynamicOpt+P : 0-1-optimized dynamic proba-bility computation method with pruning.
Figure 7 shows the scalability of the probability calcula-tion approaches when we vary the number of transactions, | T | . The runtime of the Basic approach increases exponen-tially in minSup as explained in Section 3.2, and is therefore not applicable for a | T | &gt; 50 as can be seen in Figure 7(a). Our approaches Dynamic+P and DynamicOpt+P scale linearly as expected when using a constant minSup value. The 0-1-optimization has an impact on the runtime when-ever there is some certainty in the database. The perfor-mance gain of our pruning strategies depends on the used minSup value. In Figures 7(b), 7(c) and 7(d) the scala-bility of Dynamic and Dynamic+P is shown for differ-ent minSup values expressed as percentages of | T | . It is notable that the time complexity of O ( | T | X  minSup ) be-comes O ( | T | 2 ) if minSup is chosen relative to the database size. Also, it can be observed that the higher minSup , the higher the difference between Dynamic and Dynamic+P ; a higher minSup causes the frequentness probability to fall overall, thus allowing earlier pruning.
We now evaluate the effectiveness of our pruning strat-egy w.r.t. the density. minSup is important here too, so we report results for different values in Figure 8. The pruning works well for datasets with low density and has no effect on the runtime for higher densities. The reason is straightfor-ward; the higher the density, the higher the probability that a given itemset is frequent and, thus, cannot be pruned. Re-garding the effect of minSup ; a larger minSup value decreases the probability that itemsets are frequent and therefore in-creases the number of computations that can be pruned. The break-even point between pruning and non-pruning in our experiments is when the density is approximately twice the minSup value, since, due to the method of creating our datasets, this corresponds to the expected support. At this value, all itemsets are expected to be frequent.

Overall, with reasonable parameter settings our pruning strategies achieve a significant speed-up for the identification of probabilistic frequent itemsets.
Figure 9 shows the influence of minSup on the runtime when using different densities. The runtime of Dynamic directly correlates with the size of the dynamic computation matrix (Figure 5). A low minSup value leads to few matrix rows which need to be computed, while a high minSup value leads to a slim row width (see Figure 5). The total number of matrix cells to be computed is minSup  X  ( | T | X  minSup + 1), with a maximum at minSup = | T | +1 2 . As long as the minSup value is below the expected support value, the approach with pruning shows similar characteristics; in this case, almost all item(sets) are expected to be frequent. However, the speed-up due to the pruning rapidly increases for minSup above this break-even point.
Figure 8: Runtime evaluation w.r.t. the density.
Experiments for the probabilistic frequent itemset mining algorithms were run on a subset of the real-world dataset accidents 4 , denoted by ACC . It consists of 340 , 184 transac-tions and 572 items whose occurrences in transactions were randomized; with a probability of 0 . 5, each item appearing for certain in a transaction was assigned a value drawn from a uniform distribution in (0 , 1]. Here we use AP to denote the Apriori-based and IP for the incremental probabilistic itemset mining algorithms.

We performed Top-k queries on the first 10 , 000 transac-tions of ACC using a minSup = 500 and  X  = 0 . 1. Figure 10(a) shows the result of IP . Note that the frequentness probability of the resulting itemsets is monotonically de-
The accidents dataset [8] was derived from the Frequent Itemset Mining Dataset Repository (http://fimi.cs.helsinki.fi/data/) (a) Output: AP vs. IP creasing. In contrast, AP returns probabilistic frequent itemsets in the classic way; in descending order of their size, i.e. all itemsets of size one are returned first, etc. While both approaches return probabilistic frequent itemsets, AP returns an arbitrary frequentness probability order, while IP returns the most relevant itemsets first.

Next we performed ranking queries on the first 100 , 000 itemsets (Figure 10(b)). In this experiment, our aim was to find the m -itemset X with the highest frequency probabil-ity of all m -itemsets, where m  X  X  2 , 3 , 4 } . We measured the number of itemsets returned before X . It can be seen that the speed up factor for ranking (and thus top-k queries) is several orders of magnitude and increases exponentially in the length of requested itemset length. The reason is that AP must return all frequent itemsets of length m  X  1 be-fore processing itemsets of length m , while IP is able to quickly rank itemsets in order of their frequentness proba-bility, therefore leading to better quality results delivered to the user much earlier.
The Probabilistic Frequent Itemset Mining (PFIM) prob-lem is to find itemsets in an uncertain transaction database that are (highly) likely to be frequent. To the best of our knowledge, this is the first paper addressing this problem under possible worlds semantics. We presented a framework for efficient probabilistic frequent itemset mining. We theo-retically and experimentally showed that our proposed dy-namic computation technique is able to compute the exact support probability distribution of an itemset in linear time w.r.t. the number of transactions instead of the exponential runtime of a non-dynamic computation. Furthermore, we demonstrated that our probabilistic pruning strategy allows us to prune non-frequent itemsets early leading to a large performance gain. In addition, we introduced an iterative itemset mining framework which reports the most likely fre-quent itemsets first.
