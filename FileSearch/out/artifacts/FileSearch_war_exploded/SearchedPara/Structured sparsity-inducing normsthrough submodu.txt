 parametric Bayesian process based priors for topic models.
  X  -norm and the cardinality function.
 contributions:  X  We make explicit links between submodularity and sparsity b y showing that the convex enve-lope of the function w 7 X  F (Supp( w )) on the  X  extension of the submodular function (Section 3). (Section 6), that extend classical results for the  X  by the exact same analysis and algorithms. in Section 7, where they outperform related greedy approach es [3]. vector w and a matrix Q , w Finally, for w  X  R p and A  X  V , w ( A ) = P Throughout this paper, we consider a nondecreasing submodular function F defined on the power set 2 V of V = { 1 , . . . , p } , i.e., such that: Classical examples are the cardinality function (which wil l lead to the  X  of V into B 1  X  X  B k = V , the set-function A 7 X  F ( A ) which is equal to the number of groups B follows; given w  X  R p 0 , the value f ( w ) is then defined as: to minimizing f over [0 , 1] p [13].
 Submodular polyhedron and greedy algorithm. We denote by P the submodular poly-hedron [12], defined as the set of s  X  R p (0,1)/F({2}) points), F ( A ) = | A | (leading to the  X  we have a representation of f as a maximum of linear functions [12, 9], i.e., for all w  X  R p following  X  X reedy algorithm X : order the components of w in decreasing order w and then take for all k  X  X  1 , . . . , p } , s of allowed sparsity patterns (see Section 6.2).
 Separable sets. A set A is separable if we can find a partition of A into A = B faces are exactly the sets { s We denote by T the set of such sets. This implies that P = { s  X  R p function, stable and inseparable sets are singletons. of show that we indeed define a norm and that it is the desired conv ex envelope: Proposition 1 (Convex envelope, dual norm) Assume that the set-function F is submodular, non-decreasing, and strictly positive for all singletons. Defin e  X  : w 7 X  f ( | w | ) . Then: (i)
 X  is a norm on R p , (ii)  X  is the convex envelope of the function g : w 7 X  F (Supp( w )) on the unit  X  (iii) the dual norm (see, e.g., [16]) of  X  is equal to  X   X  ( s ) = max We provide examples of submodular set-functions and norms i n Section 4, where we go from set-set of extreme points of the unit ball (see proof in [15] and ex amples in Figure 1): play a role in concentration inequalities derived in Sectio n 6. the number of jumps in the indicator vector sequence. be active, red: variables that should be left out). From left to right:  X  variables come in together), mix of the two norms (correct be havior). for supervised learning. Some existing norms are shown to be examples of our frameworks (Sec-in Section 4.1 (see, e.g., [9]). 4.1 Norms defined with non-overlapping or overlapping group s P should not be considered in the norm). It is a norm as soon as  X  to the nondecreasing submodular function F ( A ) = P are replaced by  X  result of [2] to the new case of  X  examples of norms, with various topologies of groups.
 Hierarchical norms. Hierarchical norms defined on directed acyclic graphs [1, 5, 6] correspond have been applied to bioinformatics [5], computer vision an d topic models [6]. Norms defined on grids. If we assume that the p variables are organized in a 1D, 2D or 3D of making the first gap relatively smaller. This corresponds to adding a constant times the  X  are then allowed, but contiguous ones are encouraged rather than forced . pursue such extensions here. 4.2 Spectral functions of submatrices Given a positive semidefinite matrix Q  X  R p  X  p and a real-valued function h from R define tr[ h ( Q )] as P p can thus define the set-function F ( A ) = tr h ( Q (see, e.g., [12, 9]). Thus, since for q  X  (0 , 1) ,  X  q = q sin q X  our knowledge, provide novel examples of such functions.
 If columns in A ) and we obtain a weighted cardinality function and hence and a weighted  X  In a frequentist setting, the Mallows C form tr X  X  associated to the set A and have good behavior when used within a non-convex framewo rk. This X examples with F ( A ) = tr( X  X  4.3 Functions of cardinality then  X ( w ) = P p However, the algorithms and analysis presented in Section 5 and Section 6 apply to this case. toolboxes may not be used.
 Subgradient. From  X ( w ) = max Proximal operator. Given regularized problems of the form min L is differentiable with Lipschitz-continuous gradient, proximal methods have been shown to be  X  X STA X  and its accelerated variants  X  X ISTA X  [23], which are compared in Figure 4. for submodular function minimization, namely the minimum-norm-point algorithm, which has no complexity bound but is empirically faster than algorithms with such bounds [12]: Proposition 3 (Proximal operator) Let z  X  R p and  X  &gt; 0 , minimizing 1 minimum-norm-point algorithm.
 support of the unique solution of the proximal problem and th at with a sequence of submodular and are thus not directly applicable to our situation of non-increasing submodular functions. (e.g., the  X  heavily on decomposability properties of our norm  X  . 6.1 Decomposability For a subset J of V , we denote by F by F F is (see, e.g., [12]).
 We denote by  X  a stable set). Note that  X  actually equal to  X (  X  w ) where  X  w stances, we can decompose the norm  X  on subsets J and their complements: Proposition 4 (Decomposition) Given J  X  V and  X  (i)  X  w  X  R p ,  X ( w ) &gt;  X  J ( w J ) +  X  J ( w J c ) , (ii)  X  w  X  R p , if min 6.2 Sparsity patterns Proposition 5 (Stable sparsity patterns) Assume y  X  R n has an absolutely continuous density is unique and, with probability one, its support Supp(  X  w ) is a stable set. 6.3 High-dimensional inference the norm  X  is decomposable and we use this property extensively in this section. We denote by zero and one, and, as soon as J is stable it is strictly positive (for the  X  we denote by c ( J ) = sup the  X  The following propositions allow us to get back and extend we ll-known results for the  X  results for the  X  through the decomposition properties from Proposition 4.
 normal vector. Let Q = 1 support Supp( w  X  ) of w  X  . Define  X  = min for  X  &gt; 0 , ( X  J )  X  [( X  and has support equal to J , with probability larger than 1  X  3 P  X   X  ( z ) &gt;  X  X  X  ( J )  X  n multivariate normal with covariance matrix Q .
 normal vector. Let Q = 1 Let T be the set of stable inseparable sets. Then P ( X   X  ( z ) &gt; t ) 6 P norm columns. A set J of cardinality k is chosen at random and the weights w  X  standard multivariate Gaussian distribution and w  X  where  X  is a standard Gaussian vector (this corresponds to a unit sig nal-to-noise ratio). Proximal methods vs. subgradient descent. For the submodular function F ( A ) = | A | 1 / 2 (a much faster than subgradient descent.
 Relaxation of combinatorial optimization problem. We compare three strategies for solving the combinatorial optimization problem min k = 40 ), (right) lower-dimensional case ( p = 120 , n = 120 , k = 40 ). Table 1: Normalized mean-square prediction errors k X  X  w  X  Xw  X  k 2 by  X  Non factorial priors for variable selection. We now focus on the predictive performance and compare our new norm with F ( A ) = tr( X  X  tion by  X  robust than the  X  be similarly associated with symmetric submodular set-fun ctions such as graph cuts [24]. Acknowledgements. This paper was partially supported by the Agence Nationale d e la Recherche (MGA Project) and the European Research Council (SIERRA Pro ject). The author would like to thank Edouard Grave, Rodolphe Jenatton, Armand Joulin, Jul ien Mairal and Guillaume Obozinski for discussions related to this work. [12] S. Fujishige. Submodular Functions and Optimization . Elsevier, 2005. [16] S. P. Boyd and L. Vandenberghe. Convex Optimization . Cambridge University Press, 2004. [19] R. A. Horn and C. R. Johnson. Matrix analysis . Cambridge Univ. Press, 1990. [21] C. L. Mallows. Some comments on C
