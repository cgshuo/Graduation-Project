 ORIGINAL PAPER Herv X  D X jean  X  Jean-Luc Meunier Abstract We present a method for structuring a document according to the information present in its different orga-nizational tables: table of contents, tables of figures, etc. This method is based on a two-step approach that leverages functional and formal (layout-based) kinds of knowledge. The functional definition of organizational table, based on five properties, is used to provide a first solution, which is improved in a second step by automatically learning the form of the table of contents. We also report on the robustness and performance of the method and we illustrate its use in a real conversion case.
 Keywords Document structuring  X  Table of contents recognition  X  Functional approach  X  Machine learning 1 Introduction The last few decades have seen the proliferation of electron-ically created documents and the increase of their usage in human activities. Unfortunately not all of these electronic documents allow for easy use beyond screen viewing or paper printing. Reasons for this restriction include, among others, the unavailability of the document in native format, the dep-recation or disappearance of the original authoring environ-ment, but also the case of scanned paper documents.
There is interest in converting those documents to a struc-tured format. The motivations for converting documents are diverse, typically including the intent to reuse or repurpose parts of the documents, the desire for document uniformity across a database of information, facilitating document searches, and so forth. This is of particular importance for organizations that aim at optimizing their document process-ing, e.g. authoring/repurposing/publishing. Together with the advent of XML technologies, it creates a growing need for so-called legacy document conversion tools and methods so as to transform unstructured print-or view-ready documents into appropriately structured documents that allow for further automatic processing.

The first steps of this conversion, such as geometrical analysis, OCR or logical analysis, have been well studied by the research and commercial communities, but further steps involving the reconstruction of higher level structure or semantic structure deserve more work.

We are interested in this paper in the detection of one fre-quent tool used for helping reader in her reading: the table of contents. The purpose of such tool (with pagination, indexes) is to help the reader navigating through the book. If we have some evidence of its existence in the Early Middle Ages, its consistent usage occurs in Occident from the twelfth century, and soared up in the sixteenth and seventeenth centuries (this navigation relies on pagination, which becomes reliable with the appearance of printing press).

Based on the observation that a table of contents reflects a logical organization of the content of the document and that documents often (at least for books) contain a table of contents, we are concerned in this paper with the detection and reconstruction of such tables of contents for structuring documents.

This approach has already been explored in few previous works including [ 6 , 7 , 9 , 10 ]. What motivates and differenti-ates the present work is the aim to develop a robust method generic enough to be applied on any document, without any specific knowledge about the document or the collection it may belong to.
We therefore propose a generic characterization of a table of contents (hereafter ToC) together with a set of associated methods so as to detect a ToC from any given document, and eventually to recognize its logical hierarchical organization and to structure the document accordingly. The presented method is based on a two-step approach: a first step uses generic properties in order to recognize ToC, and a second step exploiting the specificities of the document layout in order to fine-tune this recognition process.

Furthermore, the same method can be used in order to detect other organizational tables such as tables of figure, tables of tables, etc. under the condition that the referred objects (figures, tables) have been identified.

The next section of this article explains the functional characteristics allowing us to identify a ToC. Then Sect. 3 presents the two-step approach based on functional and for-mal knowledge, and secondly details its implementation in the case of the recognition of table of contents. Section 4 presents the detection of other organizational tables based on the same method. Section 5 reports on the evaluation of this method, and eventually we discuss Sect. 6 the generality and the limitations of the method. 2 Characterizing a table of contents 2.1 The problem Several approaches, reviewed in Sect. 6 , have been explored to determine the ToC of a document in the past. They exploit features specific to certain document classes, like the ToC layout, the page-or section-numbering scheme, or indenta-tion.

We acknowledge that the set of features generally used to appropriately present the ToC to the reader is not extremely large, as it generally consists of a combination of different font sizes or indentations together with particular pieces of information such as heading name, numbering, dot leader and so on.

However, the observation of documents taken at random shows clearly a large number of combinations. Thus, writ-ing a grammar that covers (almost) all the cases is extremely challenging. Everyday documents (books, journals, techni-cal documents) illustrate the variety of ToCs: different num-bering systems, different information present (headings or more), different ways to show to the reader the organization (font size, or indentation or typographical clues, e.g. all cap-italized or not), presence of section numbering at some spe-cific levels. For a specific collection, a descriptive approach can be effective, layout knowledge about this collection can be provided by rules or annotations (for Machine Learning approaches).

We believe that the approaches proposed so far do not permit in the general case to:  X  detect very accurately the ToC in absence of a priori  X  determine the reference from ToC entries to document  X  recognize other organizational tables, when their objects
We present in the remaining article a method address-ing these issues by detecting tables of contents through their functional behavior instead of their form. 2.2 Our two-step method The design of this method has been guided by the interest in developing a generic but nevertheless robust method that uses some intrinsic properties of the object known as a table of contents, in order to solve the layout variability problem stated before. We believe that the ToC function within the document imposes certain functional properties, which we can exploit to this end.

While performing well, this approach can still be com-plemented by exploiting the traditional layout information. This improvement is performed in a second step, where the formal regularities of the object provided by the functional step are extracted and exploited.

As evaluation will show, the two-step approach offers a robust solution for facing the layout variability issue. In the next sections, we develop the notions and use of functional and formal knowledge characterizing these two steps. This two-step method has been applied for other document com-ponent detection such as page number detection, page header and footer detection. It is explained in detail in [ 3 ]. 2.2.1 Functional knowledge The functional knowledge, which we consider, relies on rela-tions that document elements shares with the other elements in the document. As explained in [ 3 ], this kind of knowledge can not be used for recognizing all document elements, but is of first interest for an element such as table of contents. We define a ToC as a series of contiguous references to some parts of the document. This functional definition is very sim-ilar to the one given in [ 9 ]: A TOC is simply a collection of references to individual articles of a document, no matter what layout it follows . A comparison with this work is given in Sect. 6 . More precisely, this functional definition leads to define a ToC as an element following these properties: 1. Contiguity: a ToC consists of a series of contiguous ref-2. Textual similarity: each reference shares some level of 3. Ordering: the references and the referred parts appear in 4. Optional elements: a ToC may include (a few) elements 5. No self-reference: all references refer outside the contig-
This functional definition contrasts with definitions found in previous works, which are based on formal description of the ToC. For instance, [ 11 ] defines a ToC as  X  X othing but text lines with a structured format. X 
Our hypothesis is that it is not required to describe the structured format , and that those five properties are suffi-cient for the entire characterization of a ToC, independently of the document class and language. In order to exploit such properties (esp. Property 2), the present method works at the document level (the whole document is required as input). 2.2.2 Formal knowledge If this functional approach allows a robust identification of table of contents, we acknowledge that the form of tables can also be informative. Usually we are all able to recog-nize such tables at a first and quick glance only using their form and without having to check every reference. In order to improve the robustness of our method, layout information is then used in a second step. This layout information consists in the traditional characterization of textual elements OCR engines can provide such as page position, font name, font size, and information about the case (uppercase, lowercase, bold and italic cases). This kind of information is also easily extracted from digital formats (PDF for instance).

Whereas such information is usually used in combination with a priori knowledge, through rules or annotations, so as to directly recognize elements, we use it without such a priori knowledge.

The underlying idea is that elements belonging to the same document objects (ToC entries for instance) should share some formal regularities. These regularities do not need to be known a priori: a smooth and efficient way to infer such formal regularities is to consider the output of the functional recognition as annotated data that will be used in order to train a classifier model learnt using traditional supervised Machine Learning methods. The classifier is then used in order to improve the document elements recognized by the functional step (see Sect. 3.3 ).

An important distinction with other learning approach is that we apply the learned model only over the current docu-ment: we only try to improve the detection for this document through this formal step (Fig. 1 ). 3 Recognizing a table of contents We now explain in detail our two-step method. We will illus-trate the important aspects with the ToC shown in Fig. 2 , while Fig. 3 illustrates both the character recognition and text segmentation. 3.1 Document pre-processing Our input data usually corresponds to a document segmented into pages. Each page is composed of an ordered sequence of text blocks. Actually the segmentation in page is optional for the method while the notion of order (property 3) is nec-essary: the input block ordering must match the human read-ing flow of the document. In many cases, the proper reading flow must be determined, especially for multi-column doc-uments. For this purpose, we use an algorithm based on the well-known XY-cut algorithm [ 11 ],whichisabletoseg-ment them into blocks. This segmentation provides elements roughly corresponding to the notion of paragraphs. Note that the present method tolerates also a degraded input that only contains lines: the textual similarity (see Sect. 3.2.1 ) is then performed between lines instead of between paragraphs.
Page headers and page footers also deserves to be iden-tified, especially for documents where page headers or page footers correspond to section headings (they can then degrade the link determination step [see Sect. 3.3 ]). They are often automatically detected and ignored during the next steps. The method for identifying headers and footers is detailed in [ 2 ]. 3.2 Identifying the ToC using functional knowledge Four steps allow us to identify the area of the document con-taining the ToC text and the corresponding references. First a textual similarity is computed for all text fragments pair-wise. Then a list of ToC candidates is generated and scored. Their generation is based on the five previously properties (Sect. 2.2.1 ). Finally the best candidate is selected as ToC of the document and its references are determined. 3.2.1 Links computation Firstly, links are defined between each pair of text blocks in the whole document satisfying a textual similarity criterion. Each link includes a source text fragment and a target text fragment. As mentioned earlier, these fragments can corre-spond to line or paragraph depending on the level of pre-processing. Figure 4 shows the different links created from the ToC entries of our example. For most of the ToC entries, the number of links is quite low except for entries having corresponding headers (such as Appendix ).

Only links with a value higher than a given threshold  X  are kept. This threshold is an important parameter of the method which can be adapted depending on the level of OCR noise.
We use two similarity measures depending on the nature of the document. If the document is a digital one (usually a PDF file from which the text has been extracted), the simi-larity measure is the Jaccard coefficient: we use the ratio of words shared by the two fragments, considering spaces and punctuation as word separators. If the document required to be OCRed, the similarity measure has to take into account possible OCR errors: we introduce here a normalized edit-distance based measure between fragments (usually words).
Whenever the distance is above the similarity threshold  X  , a pair of symmetric links is created. In practice, 0.4 is a good threshold value to tolerate textual variation between the ToC and the document body while avoiding too many noisy links. The computation of links is quadratic to the number of text blocks and takes most of the total computation time. However, searching for the ToC in the N first and last pages of the document leads to linear complexity without loss of generality (Fig. 5 ). 3.2.2 Candidates generation Next, all possible ToC candidate areas are built. A brute force approach works fine. It consists in testing each text block as a possible ToC start and extending this ToC candidate further in the document until it is no longer possible to comply with the five properties identified above. A ToC candidate is then a set of contiguous text blocks, from which it is possible to select one link per block so as to provide an ascending order for the target text blocks. In this whole process, we account for the possible presence of optional elements in the ToC by relaxing the contiguity property. We have chosen to introduce an additional parameter that defines how many consecutive optional elements can be tolerated in a ToC. 3.2.3 Final link determination For each ToC candidate, we select the best link for each of its entries by finding a global optimum for the table of contents while respecting the five ToC properties. A weight is associated with each link, which is proportional to the sim-ilarity level that led to the link creation. A Viterbi best-path algorithm [ 5 ] is adequate to effectively determine the global optimum. Figure 6 illustrates an example, in which the table of contents includes text blocks #1, #2, #3 and #4. The text block #1 is the source text block for two possible links: (#1, #5), and (#1, #7). The weight for the (#1, #5) link is 0.3, while the weight for the (#1, #7) link is 0.4, and so on for other text blocks. Any links which would violate the non self refer-encing property are suitably omitted. An arrow indicates the possible choice complying with all properties for # i + 1if# j was chosen as the target for # i . The bold value in parentheses indicates the best achievable score when selecting one pos-sible target for an entry of the ToC given previous selection for previous entries.

Viterbi algorithm consists in finding the best choice for item # i given best choice for the previous ToC item (the score of a path is the sum of its node weights). This is achieved by maintaining the best possible score at each stage together with the corresponding # j as illustrated in Fig. 7 . The dashed arrow shows the global optimum.
To account for optional elements (property 4), we simply allow an arrow to  X  X ump over X  a series of consecutive sources. The trellis (Fig. 8 ) illustrates this when allowing one hole. Computation of the best score at each stage is done identi-cally. The number of maximum allowed holes is a parameter of the method.

Multiple global optimums may exist given a trellis, in par-ticular when multiple targets with identical similarity levels exist for one source text. For the latter case, we systematically choose the first target.

It is possible to determine the links of all candidates before ranking them, as the computational cost is order of magnitude lower than computing the links themselves.

We will discuss the evaluation of this functional approach in Sect. 6 , but as can be seen, it achieves very good perform for both detecting ToC entries and links between ToC entries and the referred blocks in the document. The next section discusses a way to improve this performance using the form of the ToC. 3.3 Using formal knowledge Since the functional approach reaches a high average success rate per document, we propose to learn the form of each ToC, individually, from the output of the functional method and then refining these results by using the learned model on the same document. We aim at learning a binary classifier of the links: Does a link  X  X elong to the ToC X  or not, for each doc-ument. Once the best ToC and its links have been produced by the functional approach, the following steps occur: 1. Select a set of positive and negative examples from the 2. Learn a model for classifying ToC links between the ToC 3. Measure the model quality using an N -fold validation; 4. Compute a prediction for each link outgoing from a ToC 5. Select again the best links for the Toc using the model
It is important to notice that the goal of this step is to improve the results of the functional step, and that the learnt model will only be used over the current document, whereas the traditional usage of Machine Learning is to apply the model over other data.
 We will now detail each of these steps.
 3.3.1 Select a set of positive and negative examples from For each ToC entry of the best functional ToC, if it has a selected link, then this link becomes a positive example. We get then one positive example per ToC entry, not taking into account the optional elements. In order to gather negative examples, we consider the links that were not selected. We choose a number of links from this set of negative candidates. The more examples we have the better but on the other hand it can be critical to have a balanced or near balanced training set, with for instance 2 negatives for one positive example. Other more sophisticated algorithms can be of course imple-mented for negative example selection. If we consider the example shown in Fig. 7 , we can see that our current method for selecting negatives examples does not provide any nega-tive ones for the ToC entry #4, since there is only one pos-sibility (#14). To make sure that enough negative examples are drawn, we include some fake links at random by picking out the ToC entry and a referred part chosen randomly. 3.3.2 Train a machine learning algorithm For our experiment, we have used a logistic regression algo-rithm, which accepts continuous features and unbalanced training set.

In order to model our data (pairs of text blocks), we do use layout and typographic features, together with the textual similarity level which remains also very informative.
Given a text block, we can build a feature vector including:  X  x , y , width, height of the text bounding box in the docu- X  Boolean indicating if all characters are uppercase for both  X  Boolean indicating if all characters are lowercase both  X  Font-size (normalized on [0,1]);  X  Font-color (three RGB values on [0,1]);
And finally for each pair (source block, target) we build a feature vector as follow:  X  Features of the source block;  X  Features of the target block;  X  Textual similarity between the two text blocks;
We did not include other typographic (font-name, style, etc.) at first stage, but they can easily be integrated, if they are available. Let us note that this information may be absent or unreliable in OCR outputs. 3.3.3 Measure the model quality using an N-fold validation In order to improve over the results of the functional method, we validate the model over the training set. If the validation shows a performance below a certain threshold, we do not go further with this method and retain the functional method result.

In practice, given the small training set size, we do an N -fold validation with somehow large value of, e.g. N = 10, and require the N-fold validation to reach a certain success rate, e.g. 60%. The success rate we use is the F1 measure on the positive class. 3.3.4 Select again the best links for the Toc using the ML To each link of each entry we assign a weight which cor-responds to the prediction provided by the learned model. This prediction typically is a probability. Using the exact same Viterbi shortest path algorithm used in the functional method, we select zero or one link per entry so as to maximize the sum of the weight of the selected links while respecting the functional constraints. The difference with the previous selection resides in the weights used: now it is the model pre-diction instead of the textual similarity (which is part of the feature set and may therefore play some important role in the prediction computation). A refined ToC becomes available. 4 Detection of other organizational tables Note that the method we have presented detects not only tables of contents, but other kinds of tables as well: tables of figures, of tables, etc. Indeed, these organizational tables share the same properties as the ones described in Sect. 2.2 used for table of contents identification. The most common organizational tables are tables of figures and of tables, but depending on the type of document, others can be found such as tables of Algorithms, of Equations, of Program files, of Laboratory Exercises. The entries of these tables generally correspond to the caption associated to the referred object. Such tables can be selected as ToC if their score is higher than the score of the true table of contents (typically when the organizational table is much longer than the ToC). In order to avoid such situation, we have to assign to the table candidates a table type . This table type will depend on the objects already recognized in the document: figures, tables, equations, etc. So as to correctly label the candidates, we rely on the following strategy: a score is assigned to an organi-zational table candidate respective to a selected object type based on proximity of the associated linked text fragments of the organizational table to objects of the selected object type. For each table entry, a proximity score is computed between this entry and objects present in the page. We currently use the following score in order to compute this proximity: L where the coordinates h ,w indicate the vertical and horizon-tal distances, respectively, between the linked text fragment T and the nearest object O on the page, H , W indicate the verti-cal and horizontal dimensions, respectively, of the page, and L link is the proximity measure for the linked text fragment T . Note that the proximity measure of Eq. ( 1 ) ranges between L a largest distance away on the page and L link = 1 corre-sponding to a zero distance (e.g., an overlap or contacting adjacency) between the linked text fragment T and the near-est object O. The score for a selected organizational table respective to a selected object type is then given by com-bining the proximity measures of the linked text fragments (given in Eq. ( 1 )), for example using a weighted sum: (
Score ) t = where N is the number of linked text fragments associated with entries of the organizational table (or, correspondingly, N is the number of entries in the organizational table), the index n ={ 1 ,..., N } ranges over all of the linked text frag-ments, t indexes the selected object type, ( L link ) n , the proximity measure L link for the n th linked text fragment respective to the nearest object of selected object type t , and (Score) t denotes the score for the organizational table respec-tive to the selected object type t. Since L link ranges between 0 and 1 and Eq. ( 2 ) is normalized by the ( 1 / N ) factor, it follows that (Score) t given in Eq. ( 2 ) also ranges between 0 and 1, with higher values indicating closer proximity between objects of the selected object type t and the linked text frag-ments associated with the entries of the organizational table.
This method highly depends on the ability to detect the so-called organizational objects. For some document types, the state-of-the-art OCR engines provide precise-enough information. For other document types such as technical doc-uments, the figure or table extraction is more challenging, and impacts the result of this method: no sufficiently strong correlation is found between the recognized objects and the linked text fragments. 5 Evaluation We present in this section the different evaluations we per-formed. We describe the corpora of documents and the eval-uation measure, and then present the results of several runs under different parameters. Referring to the five properties explained in Sect. 2.2 , all the ToCs in the test data correspond to  X  X ompliant X  ToCs. Then this evaluation can be considered as a quality upper-bound limit since all the ToCs respect these properties. The test collections were chosen since a semi-automatic evaluation was possible over a large number of documents (a manual annotation is extremely tedious with such data). 5.1 The corpora We have tested the method with two different corpora, one constituted from Open Office documents and the other from digitized books. The table below gives the corpora size and we give more details on each corpus in the two sections below. 5.1.1 Building semi-automatically the ground truth In order to build the ground truth necessary to our evalu-ation, we rely on a mechanism of the PDF format called bookmark: it consists in a hierarchy of clickable hyperlinks available in a side panel in the document viewer and meant to reflect the document ToC. This is of particular interest to us because using the open-source convertor pdf2xml ( http:// sourceforge.net/projects/pdf2xml/ ), it is then possible to also extract both the textual content and the list of ToC hyperlinks, which can act as our ground truth.

We were able to extract the title of an entry, which we will call the label, and the destination of the hyperlink, in term of page number and vertical position on the page in pixels. So for each ToC entry, we have a triplet: label, page number, vertical position. The page number refers here to the physical page count from first page, which is page 1. The vertical position is an offset from 0 on the page top up to the page height. The documents we considered were single column and the horizontal position of the destination could be ignored.

In this way, we constituted a ground truth dataset for our test from bookmarked PDF documents.

As a matter of fact, in some documents the PDF bookmarks hierarchy did not match the ToC one, which often was less deep than the bookmarks. We were therefore forced to inspect the bookmark on the Toc in some cases in order to indicate in some case what was the ToC maximal depth so as to limit the bookmark reading to that depth. This occurred for less than 5% of the documents. 5.1.2 Open office corpus Open Office is a well-known open-source office software suite, providing among others a word processing software application. The latter is of particular interest to us since it is able to export a document in PDF format together with the ToC available as PDF bookmark.

We did a series of search for Open Office documents over the Internet using a search engine and collected documents in English and French that had an automatically built ToC. About half of the documents we found came from the Open Office project documentation while the other half were random documents, including reports, user manuals, speci-fications, literature, etc.

We collected 167 documents varying in length from 3 to 1,250 pages, with a respective ToC length of 4 and 1,057 entries. The total number of page is 7,799 pages for 9,481 ToC entries in total.

The histograms give various views on the corpus in terms of numbers of pages and length of the ToCs (Figs. 9 , 10 ).
We partially reproduce a typical ToC of this corpus to illustrate that they are of quite classical and contemporary composition (Fig. 11 ). 5.1.3 e-Books corpus We also found on the Internet a source of digitized out-of-copyright French books available in PDF. Many of them had a valid set of bookmarks. We collected in this way an addi-tional 500 documents varying in length from 6 to 1,169 pages, with a respective ToC length of 6 and 111 entries. The ToC lengths range from 1 to 317 entries.

In fact, the presence of very short ToCs showed a prob-lem with our corpus: the PDF bookmarks often include one or two additional hyperlinks to some additional pages added to the book by the editor. When the book has a ToC, these additional links often are integrated in it, but in absence of book ToC we have a mismatch between the PDF bookmark and the (non-existent) book ToC. This is visible on the bad results (0% in precision and recall) achieved on the 11 books with a ToC shorter than 3. We kept these 11 books in our test set anyway.

The total number of page is 131,946 pages for 15,895 ToC entries in total (Figs. 12 , 13 ).

We looked in more detail at the bin for ToC length below 5, as shown in the histogram (Fig. 14 ).

Below is reproduced a ToC of this collection (Fig. 15 ). 5.2 Document pre-processing We pre-processed each document before applying the ToC detection method as follows: 1. Conversion to XML using pdf2xml. The PDF bookmarks 2. Removal of headers and footers as described in [ 2 ]. This 3. Text flow re-ordering and segmentation in line and para-
Step 2 and 3 are subject to a certain error rate. In our experience, most documents are perfectly processed while probably less than 5% of them exhibit problems. Typically, repeating titles may be considered as headers and removed. Or a two-line title may be considered as two paragraphs by the segmenter. Both these exemplified errors may impact the Toc detection. On the other hand, running the ToC detection immediately on the pdf2xml output is not reliable, because of the possibly wrong text flow, because of the need for a proper segmentation for instance in either line or paragraph. Also the headers/footers is a source of noise to the method mostly because a ToC running on multiple pages will be  X  X olluted X  by the headers and footers, and in a lower degree because headers/footers often repeat the textual contents of some title. 5.3 Quality metric used In this evaluation we will measure the quality of the desti-nation of the ToC links and measure the quality of the tex-tual label associated with the link. The rationale is that this information if key for structuring the document. Moreover, a wrong link source has little chance to point to a valid des-tination, so it is not generally worth measuring it explicitly.
As evaluation measure, we use:  X  For the link destination, we use the traditional precision  X  For the quality of the link label, we compute the ratio of label_accuracy = As said previously the destination of a link if defined by both a target page and a vertical offset on the page. However, our ground truth data has a certain vertical imprecision and we were forced to tolerate a certain vertical shift on the page. More precisely we tolerate 11 pixels of difference for the Open Office corpus and 60 pixels for the e-Books, which is respectively about 1 and 7% of the typical page height in these corpora. An additional evaluation ignoring the verti-cal positioning is also reported, since many methods work exclusively at the page level. In this case, we consider only the first link destination on any given page. 5.4 Evaluation of the functional approach We report here on the evaluation made on both corpora, with the following standard parameter values:  X  Jaccard minimal coefficient = 0.4  X  Tolerate at most three contiguous optional elements
In addition, in order to speed up the computation we con-sider only the 100 first and last pages of the documents or the first and last 10% pages if it is a larger set of pages. We have validated in many cases that the results are identical as if we considered also the middle part of the document. Under this setting, the method requires on average from 0.5 to 1.5 s per page, i.e. from 60 to 100s per document (Table 1 ).
The macro measure gives slightly worse values together with relatively large standard deviations. We explain this by the presence of documents poorly processed but with a rel-atively small number of ToC entries. The histogram below shows the repartition of document with a F1 measure above a certain threshold:
We observe that about half of the documents have a 100% precision and recall, while about three quarters of them have a F1 above 0.95 (Fig. 16 ).

The evaluation at the page level, i.e. ignoring the vertical position of the link target is shown in Table 2 .

We observe a significant improvement on the Open Office corpus, which cannot be explained alone by the quite small vertical tolerance chosen since putting it to a high value does not significantly improve the results. So this means that by considering only the first link destination on any page, we discard some erroneous links. 5.5 Error review for the functional approach We inspected the 13 documents of the 167 Open Office doc-uments having a F1 measure below 80. We found 5 different root causes of errors:  X  PDF creation issue: three documents where the chapter  X  Text segmentation issues: one document where the seg- X  Header/footer: six documents where the page header/  X  Ground truth errors for three documents.
 5.6 Evaluation of the functional + formal approach A formal pass is performed after the functional processing reported in the previous sections. In order to train a statis-tical model, we selected five negative links per positive and chose to apply a model only if it cross-validation lead to an F1 measure above 0.8 for the positive class.
 We report the results in Table 3 .

We observe no change whatever on the e-Books collection while a small but interesting improvement occurred for the Open Office collection, which is illustrated on the histogram below. In particular, we observe that about 10% additional document reached a F1 of 100 and that these documents were previously processed at a F1 between 85 and 95. This is inline with the intuition that when the functional step works well the formal step can in turn build a valuable model. Overall, 86% of the document have a F1 at 95% or above.

The reasons why the e-Books collection processing stay unchanged is less clear. One hypothesis is that the functional pass already did a very good job (Fig. 20 ). 6 Related work We discuss here other work on the detection or analysis of proposed a method for detecting ToC pages in a document. Lin X  X  method is based on a similar functional approach and is applied to journals. It takes advantage of a combination of text matching, layout and page numbers to determine the pages holding the ToC as well as the starting page of each referenced paper. Mandal X  X  method relies on a page-number-based heuristic and works on the page image, before segment-ing the page content. The present method offers a means to go beyond the page level. It also differs from most of the cited works in that it relies on functional properties and only uses layout information for improvement in a second step, which we see as a key advantage in term of robustness and generality.
 Regarding further analysis of the ToC itself, i.e. once the ToC is identified by automatic or manual means, we noticed the works below. Two of them aimed at attaching seman-tics to the ToC entry constituents, which we do not do here. Satoh et al. [ 13 ] first proposed an electronic library frame-work including the automatic analysis of the ToC pages of journals in order to extract bibliographic information by cat-egorizing each text block by means of a decision tree. Bela X d et al. [ 1 ] proposed a labeling approach of the ToC of scientific journal in the Calliope electronic library using dictionaries and collection-dependant contextual rules for part-of-speech tagging. The objective here is to analyze each element of the ToC, which is provided as input.

Lin et al. [ 8 ] acquired the logical structure of the ToC thanks to an in-depth analysis of its numbering scheme. Tsuruoka et al. [ 14 ] exploited the indentation and font size to classify ToC lines in different hierarchical groups. Feng et al. [ 6 ] exploited the indentation, page numbers and numbering scheme to compute the logical structure of a book. We view those approaches as less general than the one proposed here, because of the observed diversity of page/section-numbering schemes and ToC layout.

We also tested two commercial OCR engines. They can generate MS word format as final output. This output format may contain a document map section which corresponds to the document structure. They offer good results for very sim-ple documents but their use quickly reaches their limit. As far as we can analyze their results they might use font infor-mation in order to select section headings. 7 Discussion The proposed method for detecting ToCs relies on five prop-erties presented in Sect. 2 . Our evaluations show that it achieves very good results when all the five properties are respected by the ToCs. When one or more properties are not fulfilled, the results are partial or wrong. It is of course very difficult to assess the representativeness of these properties. Our experience shows that many recent documents respect them, but we have found collections where some documents have ToCs falling short of them.

It is interesting to remark that these five properties offer a way to characterize ToCs. It would be interesting to complete these functional properties with formal properties in order to have a more comprehensive way to characterize ToCs. Such classification could be valuable during the construction of ground truth datasets in order to assess a kind of the repre-sentativeness of the document set.

We will now go through these five properties and discuss how (or if) the method can be improved when a particular property is not respected.

The first property states that a ToC consists in a series of contiguous references. Therefore we consider that there is a unique ToC per document made of a single contiguous chunk. We have found technical documents having one main ToC simply referring to the different sections, each of which having its own detailed ToC, as shown in Fig. 21 . In such a case, the present method will select the longest ToC in the document. However, in the cases we observed, the documents were already split in sections (a meaningful business object) because of their large size and applying our method to each section led to good results.

The method assumes that there is a sole main ToC per document. Even if no example was found, we could also envisage a document with several sub-ToCs describing its specific part, without a global ToC for the document.
The second property states that there is a textual similarity between the ToC entry and its referred part. We have found examples (Figs. 22 , 23 , 24 ) where this textual similarity was difficult (or impossible) to compute. In these examples, which both were published in the 1830 X  X , the similarity is not textual but semantic. The property can of course be general-ized to state instead that there is a semantic similarity between the ToC entry and the referred part, but we would face an implementation issue (the use of more sophisticated Natu-ral Language Processing tools is today not realistic). Since we do not foresee any feasible and realistic improvement to overcome this problem, we believe this method does not work in this particular case.

With respect to the textual similarity, we can also add that the links having the best similarity score are not always the best ones: one example is provided by the links between a ToC entry and some page headers. The similarity can be per-fect, but nevertheless the link could be wrong. Furthermore, the similarity can be degraded by some OCR errors. This explains why a global optimization is performed with the Viterbi algorithm.

The third property imposes that the ToC entries appear in the same order as the referred titles in the body. There is a family of ToCs which does not respect this property as the one illustrating in Fig. 25 . The ToC entries are first grouped by topics or themes, and then the entries of each group appear in the same order as the referred elements in the document body. Similar ToCs can be found in contemporary newspapers or magazines. The method will provide in this case a partial solution where the longest sub-ToC will be recognized (the Articles sub-ToC). It could be possible to extend the ToC detection by looking at surrounding parts having a similar layout or being also considered as a shorter ToC candidate.
We must here highlight the importance of the fragments order of the document input. In our system, the reading order is always reconstructed before applying the ToC detector. Severe mistakes in this reading order detection can hinder the ToC detection. Figure 26 shows a ToC organized in a 2-page table structure. If the correct segmentation can be provided to our ToC detector, this latter will find the correct ToC. If not, the first page of the ToC will be considered as the ToC (the number of holes generated by the second page will be too high).

The fourth property allows some optional elements (or holes) in a ToC. These elements can be of various types depending on the document type. They can be elements struc-turing the ToC but not appearing in the document body (struc-turing the ToC in parts, or divisions, whereas these parts are not indicated in the document body). These optional elements can also be some other elements of the ToC entry such as the authors, or the revision date. The present functional charac-terization of a ToC, considering a Toc as a sequence of links and holes, can be viewed as an oversimplification: an entry can be more structured, and does not only correspond to a sin-gle element provided by the segmentation. Figure 7 , Sect. 3 , illustrates this configuration: the text segmentation generated a paragraph for each title but also for each label (SECTION X). One should logically consider one ToC entry as com-posed of two parts: its section numbering and its title. We see here that the method could be improved by performing an entry analysis, as it is done in [ 1 ] and [ 8 ], whose objectives include a better structuring of each entry (by regrouping links and optional elements into one entry) and the establishment, for each entry, of one unique link to the referred part.
It is important to highlight that the tolerance of these holes in the TOC provides some robustness when the ToC is wrongly segmented: a merge of two entries can generate a hole. An entry split into two lines can generate one or two holes (depending on the similarity) as shown in Fig. 18 .The presence of such holes will not invalidate the ToC candi-date. The number of permitted holes is a parameter of the method.

The fifth property forbids self-references. We were unable to find ToCs not respecting this property. Its purpose is indeed to filter out bad candidates.

An important yet missing step is the determination of the different hierarchical levels of the ToC entries. Some experiments have been conducted which strongly rely on the assumption that elements of the same level share the same layout aspect. Entries are clustered according to their lay-out, and the hierarchical level is assigned using a heuristic. The quality of this method highly depends on the quality of the documents, especially for documents provided by OCR: in such a case, information about layout may be noisy, and other kinds of information have to be introduced in order to constraints the clustering operation.

Finally, what about documents without a ToC? When a document does not include a ToC, our system may wrongly identify a portion of text as table of contents. Some cri-teria can be then used in order to evaluate the quality of the selected table of contents such as coverage (proportion of document referred by the ToC), the number of holes in the ToC, or the average similarity between links and section headings. In fact the coverage criterion has proved to be very efficient.

To conclude, we can say that if the method does not encom-pass all the possible types of table of contents, it offers a robust and general framework for ToC detection and links determination for many types of documents and collections. Combined with other conversion components, as described in [ 4 ], it provides a very useful piece for a document conver-sion system. References
