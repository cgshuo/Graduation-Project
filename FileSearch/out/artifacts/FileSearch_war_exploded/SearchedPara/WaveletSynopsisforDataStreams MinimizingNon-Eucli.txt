 We consider the wavelet synopsis construction problem for data streams where given n num bers we wish to estimate the data by constructing a synopsis, whose size, say B is much smaller than n . The B num bers are chosen to minimize a suitable error between the original data and the estimate deriv ed from the synopsis.

Sev eral good one-pass wavelet construction streaming al-gorithms minimizing the ` 2 error exist. For other error mea-sures, the problem is less understo od. We pro vide the  X rst one-pass small space streaming algorithms with pro vable er-ror guaran tees (additiv e appro ximation) for minimizing a va-riet y of non-Euclidean error measures including all weigh ted ` (including ` 1 ) and relativ e error ` p metrics.

In sev eral previous works solutions (for weigh ted ` 2 , ` and maxim um relativ e error) where the B synopsis coe X -cien ts are restricted to be wavelet coe X cien ts of the data were prop osed. This restriction yields sub optimal solutions on even fairly simple examples. Other lines of researc h, suc h as probabilistic synopsis, imp osed restrictions on how the synopsis was arriv ed at. To the best of our kno wledge this pap er is the  X rst pap er to address the general problem, with-out any restriction on how the synopsis is arriv ed at, as well as pro vide the  X rst streaming algorithms with guaran teed performance for these classes of error measures.
 Categories and Sub ject Descriptors: F.2 [Analysis of Algorithms and Complexit y] : Miscellaneous; G.2 [Discrete Mathematics] : Miscellaneous; H.3 [Information Storage and Retriev al] : Miscellaneous General Terms: Algorithms, Theory Keyw ords: Wavelet Synopses, Streaming Algorithms  X 
Supp orted in part by an Alfred P. Sloan Researc h Fello w-ship and by an NSF Aw ard CCF-0430376. y Sponsored by NIH Training Gran t T32HG0046
Wavelets are localized, orthogonal transforms that are ex-tremely versatile for represen ting discrete signals [3, 21]. They allo w a multi-resolution view of the data and easily extend to more than one dimension. Among these, the Haar wavelets have been used extensiv ely in synopsis construction with a variet y of uses in image analysis, signal pro cessing, and databases to name a few. The primary attraction of the Haar Wavelets is the existence of linear time forw ard and inverse transforms. The non-normalized Haar basis for n = 4 is: The vectors generalize to larger powers of 2 quite easily . The interv als over whic h the basis vectors are de X ned are powers of 2. The interv al corresp onding to the basis vec-tor  X  i is denoted by Supp (  X  i ), e.g. Supp (  X  1 ) = n and Supp (  X  3 ) = n= 2 alw ays. Moreo ver every value of the inverse transform W  X  1 ( Z ) = P i  X  i Z i in the Haar setting is the sum of only logarithmically (in the length of the signal) man y values in the transformed represen tation Z . This allo ws fast \on demand" computation for a broad spectrum of analyses tasks. Furthermore, there is a signi X can t intuitiv e meaning to the Haar wavelet coe X cien ts. The (non-normalized) coef- X cien ts denote half of the di X erence between the averages of the left and righ t halv es of the entire interv al (the supp ort).
Most applications of Wavelets consider represen ting the input in terms of the high level coe X cien ts and broader characteristics of the data, typically referred to as a syn-opsis or signature. These synopses or signatures are used subsequen tly in learning, classi X cation, and event detection among man y other applications. The synopsis is typically constructed to minimize some desired error criterion. One of the most common error criteria is the sum-of-squares cri-terion whic h is also the square of the ` 2 distance between the original signal and its represen tation. However with the emerging mining applications suc h as time series analysis other error measures (e.g. ` 1 , weigh ted ` 2 etc.) have been considered recen tly. It would be imp ossible to conduct a thorough review, however [13, 19, 1, 18, 7, 6, 4, 11] and pointers therein serv e as excellen t starting points. In this pap er we consider the follo wing problem: Problem 1. Given a set of n numb ers X = x 1 ;:::;x n ,  X nd a synopsis vector Z with at most B non-zer o entries, such that the inverse wavelet transform of Z (denote d by W  X  1 ( Z ) ) gives a good estimate of the data, i.e., minimizes k X  X W  X  1 ( Z ) k p for some inte ger p ( p = 1 corresponds to the maximum error). We will assume n is a power of 2 .
The problem also gener alizes to weighte d-` p error metrics wher e given weights  X  i  X  0 we seek to minimize For the standar d ` k norm all  X  i are set to 1 . The Relative Err or metrics have  X  i = 1 max fj x c &gt; 0 which avoids division by 0 . The relative error metrics will be denote d as k X k rel p .

In absenc e of any quali X er such as `relative' or `weighte d', the term `err or' will imply error in the ` p norm. For the weighte d-` p norm we can multiply all the  X  i 's by a constant and leave the problem unchange d. Ther efor e for weighte d ` we will assume  X  max = max i  X  i = 1 .

For the Euclidean error, i.e., minimizing unweigh ted ` 2 error k X  X W  X  1 ( Z ) k 2 . Observ e that the set of vectors  X  1 = p Supp (  X  i )  X   X  i form an orthonormal basis. In any or-thonormal basis the Euclidean length or the ` 2 norm of any vector (including X  X W  X  1 ( Z )) is preserv ed (Parsev al's The-orem). Thus the problem of minimizing k X  X W  X  1 ( Z ) k 2 is equiv alen t to minimizing P i Supp (  X  i )( W ( X ) i the best choice of Z is to store the largest B (ignoring signs) normalized, i.e. multiplied by p Supp (  X  i ), coe X cien ts. Sev-eral algorithms have been prop osed for this in the streaming con text [18, 7, 8, 10]. For the streaming mo del considered in this pap er the optim um synopsis under unweigh ted ` 2 error can be found in O ( n ) time and O ( B + log n ) space.
The simplicit y of the unweigh ted ` 2 solution breaks down in case of non-Euclidean error measures. In an early pap er Matias, Vitter and Wang [19], demonstrated a num ber of di X eren t applications for wavelet synopsis for non-Euclidean error measures and prop osed greedy algorithms. In fact, sev-eral researc hers have sho wn greedy heuristics perform quite well, but no theoretical analysis about the qualit y of the syn-opsis exists. The problem is quite non-trivial, because the Wavelet basis vectors overlap and two coe X cien ts can cancel out eac h other leaving a signi X can tly (exp onen tially) smaller con tribution. In fact, for ` k ( k &gt; 2, even weigh ted ` is no kno wn guaran tee that the solution will be over ratio-nals since the optimization minimizes an algebraic equation of degree greater than 1. This is the biggest stum bling blo ck in the synopsis construction, and has likely been one of the reasons for considering the restrictions on how the synopses are arriv ed at. We discuss some of the previous works next.
Garofalakis and Gibb ons [6] prop osed a strategy that im-pro ves upon storing the largest coe X cien ts for non-Euclidean errors. They consider probabilistic synopsis where the i 'th coe X cien t tak es the value  X  i with probabilit y W ( X ) is set to 0. They sho w the estimation using probabilistic synopses is unbiased and pro vide algorithms for  X nding the best probabilistic synopsis under di X eren t measures. How-ever, we note that,
Garofalakis and Kumar [5] avoid the problem with the space bound and give a deterministic O ( n 2 B log B ) time and O ( n 2 B ) space algorithm for maxim um error metrics for the restricted case where the i th entry of Z , Z i , is re-stricted to be 0 or W ( X ) i , the i th Wavelet coe X cien t of the input. Muth ukrishnan [20] extends the algorithm of [5] to handle weigh ted ` 2 error measures. Matias and Urieli [16] as well as [20] impro ve the running time of the algorithm in [5]. Guha [9] sho ws that all weigh ted ` k error measures, in the restricted version can be solv ed in O ( n 2 log B ) time and O ( n ) space (constan ts indep enden t of B ) using space e X -cien t dynamic programming techniques. All the algorithms for this restricted version in [5, 20, 16, 9] share the follo wing prop erties:
Matias and Urieli [17] consider the weigh ted-` 2 error and pro vide a near linear time optimal algorithm, but for a dif-feren t wavelet basis that dep ends on the weigh ts. Their algorithm also app ears to be an o X ine algorithm.
The example X = f 1 ; 4 ; 5 ; 6 g underscores that the restric-tion of a synopsis coe X cien t to be a wavelet coe X cien t of the data results in a sub optimal strategy . The problem dis-app ears in the unrestricted case and matc hes the intuitiv e notion of a synopsis. We will also sho w that the remo val of the restriction gives us a streaming computation. For the purp ose of the pap er a data stream computation is a space bounded algorithm, where the space is sub-linear in the in-put. Any input items are accessed sequen tially and any item not explicitly stored cannot be accessed again in the same pass. We discuss streaming mo dels and their relev ance to our problem at the appropriate place. 1. We prop ose the  X rst one-pass streaming Wavelet syn-2. We sho w that the general version of the problem pro-3. We also prop ose the  X rst streaming appro ximation al-4. We also prop ose a new algorithm Hybrid whic h stores While this pap er was submitted for review, Karras and Mamoulis [14] have prop osed a greedy one pass algorithm for the ` 1 and related maxim um measures for the restricted ver-sion (storing coe X cien ts of the data) whic h runs in O ( n log n ) time and O ( n ) space. The algorithm is extended to a stream-ing setting by rep eatedly adding two new coe X cien ts and discarding two old coe X cien ts. Note that the authors of [14] do not pro vide any guaran tees for the synopsis qualit y for any of the algorithms prop osed, but observ e on the ba-sis of exp erimen ts that their synopses are good. Since all of their algorithms store the coe X cien ts of the input, the example X = f 1 ; 4 ; 5 ; 6 g applies to them as well.
In Section 2 we discuss the preliminaries of Wavelet trans-forms and various terminology . In Section 3 we pro vide the basic algorithm and running time analysis without getting into the streaming or space complexit y asp ect. In Section 4, we indicate how the algorithm is adapted to a one pass data stream and analyze the space complexit y. We also include a discussion on streaming mo dels and the issue of precision. In Section 6 we pro vide some exp erimen tal results sho wing the pro of of concept of these algorithms.
We will work with non-normalize d wavelet transforms where the inverse computation is simply adding the coe X cien ts that a X ect a coordinate. For normalized wavelets the nor-malization constan t app ears both in the forw ard and inverse transforms, all the results in the pap er will carry over in that setting as well, with the introduction of the normalization constan ts at sev eral places. The wavelet basis vectors are de X ned as (assume n is a power of 2):  X  The above de X nitions ensure W  X  1 ( Z ) = P i Z i  X  i . To com-men ts as i ranges over 0 ; 1 ; 2 ; 3 ;::: . The di X erence coe X -cien ts form the last n= 2 entries of W ( X ). The pro cess is rep eated on the n= 2 average coe X cien ts -their di X er ence coe X cients yield the n= 4 + 1 ;:::;n= 2 coe X cients of W ( Z ). The pro cess stops when we compute the overall average, whic h is the  X rst elemen t of W ( Z ).

The wavelet basis functions naturally form a complete bi-nary tree, termed the coe X cient tree , since their supp ort sets are nested and are of size powers of 2 (with one ad-ditional node as a paren t of the tree). The data elemen ts corresp ond to the leaves, and the coe X cien ts corresp ond to the non-leaf nodes of the tree. Assigning a value c i to the coe X cien t corresp onds to assigning + c i to all the leaves that are left desc endants (descendan ts of the left child) and  X  c to all righ t descendan ts. The leaves that are descendan ts of a node in the coe X cien t tree are termed the supp ort of the coe X cien t.
In this section we brie X  X  describ e the algorithm frame-work prop osed in [5]. Recall that the algorithm only retains coe X cien ts of the input signal. The algorithm uses the coef- X cien t tree, and eac h node decides the best solution for the subtree given the choic es made at all anc estor nodes in the coe X cient tree . To  X nd the best solution given suc h a con- X guration of the ancestors the algorithm needs to allo cate the coe X cien ts to the two subtrees. The num ber of choices of con X gurations at a node is 2 depth (root is at depth = 0), and the num ber of ways of dividing the coe X cien ts (at most B ) is O ( B ). To  X nd the best division we need O (log B ) time (using binary searc h) and thus the time spent at eac h node in the coe X cien t tree is O (2 depth B log B ). Since the depth of any node is at most log n + 1 and there are n nodes, the to-tal time tak en is O ( n 2 log n +1 B log B ) whic h is O ( n As noted earlier, [20, 16, 9] presen t better analyses of the algorithm but the computation is  X ( n 2 ).
We now sho w how to obtain an additiv e appro ximation algorithm for the general/unrestricted wavelet synopsis con-struction problem. Recall that the wavelet synopsis problem is: Giv en a set of n num bers X = x 1 ;:::;x n ,  X nd a Z 2R with at most B non-zero entries suc h that k X  X W  X  1 ( Z ) k is minimized.
The algorithm will be bottom up, whic h is con venien t from a streaming point of view. In this section we will ignore the streaming asp ect and pro ve correctness of our algorithms and the appro ximation guaran tees.

Observ e that in case of general ` p norm error, we cannot dispro ve that the optim um solution cannot have an irra-tional value, whic h is detrimen tal from a computation point of view. In a sense we will seek to narro w down our searc h space, but we will need to preserv e near optimalit y. We will  X rst sho w that ther e exists a set R suc h that if the coef- X cien ts were dra wn from it, then ther e exists one solution whic h is close to the optim um unrestricted solution (where we searc h over all reals). In a sense the set R \rescues" us from the searc h. Alternately we can view R as a \rounding" of the optimal solution. Obviously suc h an R exists if we did not care about the error, e.g. tak e the all zero solution. We would exp ect a dep endence between the set R and the error bound we seek.

However there is a subtle twist { the existence of R is straigh tforw ard if  X  i &gt; 0 for all i . But it is unclear if the values of the largest num bers in R are bounded if some  X  are very small. For cases where there are  X  i 's that are very small we would have to allo w the algorithm to use di X er ent sets R j at eac h node j of the coe X cien t tree. We can sho w that j R j j will be bounded|but may be O ( n ). This would imply that the algorithm cannot be made small space if some of the  X  i 's are small.

In what follo ws we  X rst sho w the additiv e appro ximation algorithm for minimizing the ` p norm, k X  X W  X  1 ( Z  X  ) k Subsequen tly we sho w how to get an additiv e appro ximation for the weigh ted ` p norm and the relativ e error ` p norms.
De X nition 1. Let E [ i;v;b ] be the minim um possible con-tribution to the overall error from all descendan ts of node i using exactly b coe X cien ts, under the assumption that an-cestor coe X cien ts of i will add up to the value v at i (taking accoun t of the signs) in the  X nal solution.

The value v will obviously be set later for a subtree as more data arriv e. Note that the de X nition is bottom up and after we compute the table, we do not need to remem ber the data items in the subtree. As the reader would have guessed, this second prop erty will be signi X can t for streaming as we will see in the next section.

The overall answ er is clearly min b E [ root; 0 ;b ]|b y the time we are at the root, we have looked at all the data and no ancestors exist to set a nonzero v . A natural dy-namic program arises whose idea is as follo ws: Let i L and i
R be node i 's left and righ t children resp ectiv ely. In order to compute E [ i;v;b ], we guess the coe X cien t of node i and minimize over the error pro duced by i L and i R that results from our choice. Speci X cally , the computation is: 1. A non-ro ot node computes E [ i;v;b ] as follo ws: 2. Then root computes: Time Analysis. The size of the error table at node i , E [ i;  X  ;  X  ], ror tree (the leaves have heigh t 0). Further, computing eac h total running time is O ( j R j 2 B 2 ) for computing the root ta-other error tables. Now, where the  X rst equalit y follo ws from the fact that the num ber of nodes at level t is n 2 t . For ` 1 , when computing E [ i;v;b ] we do not need to range over all values of B . For a speci X c r 2 R , we can  X nd the value of b 0 that minimizes max f E [ i r;b 0 ] ;E [ i R ;v  X  r;b  X  b 0  X  1] g using binary searc h. The running time thus becomes,
The algorithm needs to main tain the \state" whic h is the errors for the set R , and all b s.t. 0  X  b  X  min f B; 2 t node at level t . The bottom up dynamic programming will require us to store the states along at most two leaf to root paths. Thus the required space is,
In this section, we pro ve the existence of the set R , as well as sho w how to  X nd the set. The  X rst task of the pro of of existence will be to sho w that the values in the set R are bounded by some function of the input. The pro of is based on the fact that the all zero solution is a feasible solution.
Lemma 1. For any vector Y , if max i j Y i j = M , then max i jW ( Y ) i j X  M .

Proof. The 1 st coe X cien t is the average of all values and therefore cannot exceed M . Every other coe X cien t is half the average value of the left half (of the supp ort) min us half the average value of the righ t half. Eac h cannot be more than M= 2 in absolute value.

Lemma 2. Let the optimum solution Z  X  be better than the all zero vector ~ 0 , then max i j Z  X  i j X  2 n 1 p M .
Proof. Observ e that, k X  X W  X  1 ( Z  X  ) k p &gt; n 1 p M  X  X  X k p = k X  X W  X  1 i.e., setting Z  X  as the all zero vector ~ 0 impro ves the solution. This con tradicts that Z  X  is the optim um solution. Therefore, max i jW  X  1 ( Z  X  ) i j  X  2 n 1 p M . Now we apply Lemma 1 on Y = W  X  1 ( Z  X  ) and get max i jW  X  1 ( Z  X  ) max i j Z  X  i j whic h completes the pro of.

The above lemma is one of the main reasons for choosing to work with a non-normalized basis. An analogous the-orem could be pro ven for normalized coe X cien ts, but the statemen ts of the lemma would be signi X can tly less clean.
From the above lemma max r 2 R j r j = 2 n 1 p M . The next lemma bounds the size of R . The basic intuition is that if we appro ximate the coe X cien ts the e X ect seen at a point can be bounded.

Lemma 3. If we round each non-zer o value of the opti-mum Z  X  to the nearest multiple of  X  ther eby obtaining ^ then k X  X W  X  1 ( ^ Z ) k p  X  X  X  X W  X  1 ( Z  X  ) k p +  X n 1 and j R j X  4 n 1 p M= X  .

Proof. The bound on j R j clearly follo ws from Lemma 2 and the size  X  since we are interested in searc hing in the range  X  max i j Z  X  i j . Now from the triangle inequalit y we have, k X  X W  X  1 ( ^ Z ) k p  X  X  X  X W  X  1 ( Z  X  ) k p + kW  X  1 ( Z
In what follo ws, we will argue that kW  X  1 ( Z  X  )  X W  X  1 is at most  X n 1 =p min f B; log n g whic h will pro ve the lemma. Note that if Z  X  i = 0 then ^ Z i = 0 and thus we do not increase the num ber of coe X cien ts.

For all i we have j ^ Z i  X  Z  X  i j X   X  , and eac h point in W (or W  X  1 ( ^ Z )) is a sum of at most min f B; log n g wavelet co-e X cien ts. Therefore since the rounding errors at eac h point can at most add up, we get Now we observ e that and the lemma follo ws.
 Therefore if we set  X  =  X M= ( n 1 =p min f B; log n g ) we can say that we have an additiv e appro ximation of  X M as well as lowing:
Theorem 4. We can solve the Wavelet Synopsis Con-struction problem with ` p error with an additive appr oxima-tion of  X M in time O ( B X   X  2 n 1+4 =p (min f B; log n g ) M = max i j x i j using space O ( B X   X  1 n 2 =p min f B; log n g log It is immediate that we can achiev e a tradeo X  of the error and running time. Further,
Cor ollar y 5. For ` 1 error measur e the above algorithm O ( B X   X  1 min f B; log n g log n B ) space.
The key to the analysis in Section 3.3 was bounding max i in the optimal solution Z  X  . We will pro ve a lemma analogous to Lemma 2 above. We will pro ve the result for the weigh ted ` norm; and then sho w that the result is sligh tly better for the relativ e ` p error. Recall that  X  i = 1 = max fj x i the relativ e ` p error. We begin with the follo wing de X nition:
De X nition 2. De X ne  X  min = min i  X  i . Recall  X  max = max For the weigh ted-` p norm  X  max = 1 without loss of gener-alit y. For the relativ e ` p error  X  max = 1 = max f min (whic h is at most 1 =c ) and  X  min = 1 =M . We can assume M &gt; c since otherwise relativ e ` p is almost the same as the ` norm (with a  X  i = 1 =c scaling).

Lemma 6. Let the optimum solution be Z  X  for the weighte d-` p error measur e. If max i j x i j  X  M , then max i j Z  X  2 n
Proof. The pro of of this lemma will be similar to Lemma 2 with a small twist. Observ e that if U i =  X  i x i and V i  X  We transform the problem to ordinary ` p norm over the weigh ted vectors. Note that for relativ e error j U i j  X  1 and therefore k U k p  X  n 1 =p . In case of weigh ted ` p norm k U k
If max i j V i j &gt; 2 k U k p , then Again, the all zero solution pro vides an error of k U k p we arriv e at a con tradiction of the optimalit y of Z  X  . There-fore, max i j V i j X  2 k U k p . Now from Lemma 2, we have that
For relativ e ` p error we get 2 n 1 =p  X  max i j V i j X   X  and the lemma follo ws. For the weighed ` p norm, we get (  X  min max i j Z  X  i j )  X  2 Mn 1 =p and the lemma is true. The next lemma is immediate from the pro of of Lemma 3.
Lemma 7. If we round each non-zer o value of the opti-mum Z  X  to the nearest multiple of  X  ther eby obtaining ^ then k X  X  W  X  1 ( ^ Z ) k  X ;p is at most k X  X  W  X  1 ( Z  X n 1 =p min f B; log n g sinc e  X  max = 1 . For relative ` p Based on the above we get the follo wing
Theorem 8. We can solve the Wavelet Synopsis Con-struction problem for minimizing the relative ` k error in ditive error of  X  . The running time for ` 1 reduc es by B=log
For the weigh ted-` p error the above gives an additiv e  X M M = max i j x i j . Clearly the above result is useful when  X  min &gt; 0. In what follo ws we will sho w how to handle  X  for the weigh ted-` p .
Recall the algorithm outline, E [ i;v;b ] was de X ned to be the minim um possible con tribution to the overall error from all descendan ts of i using exactly b coe X cien ts, under the assumption that ancestor coe X cien ts of i will add up to the value v at i (taking accoun t of the signs) in the  X nal solution: min Denote eac h entry E [ i;v;  X  ] as a \line" { based on the notion that the entries corresp ond to a table.

Lemma 9. At a leaf node i , for weighte d-` p error, if  X  0 then the range does not matter and we can describ e the dynamic programming table in one line.

Lemma 10. For any node i ther e exist two unique lines s.t. the entries E [ i;v;  X  ] for v 62 [  X  M i ;M i ] wher e M can be represente d by those two lines (corresponding to v &gt; M i and v &lt;  X  M i ).

Proof. The pro of is by induction on the level of i . For a leaf node with  X  i = 0 clearly we can set M i = 0. For any v;b the error is 0 and therefore one \line" su X ces. Assuming  X  i &gt; 0 for a leaf node i . Then M i = M + Mn 1 =p = X  i su X ces because any value of v outside this range will ensure that the error is at least Mn 1 =p , whic h we have seen, is more than the error of the all zero solution ~ 0. Thus for any v;b in this range we can set E [ i;v;b ] = 1 since these entries will nev er be useful for the optim um solution.
For an internal node the two children (by induction) will return tables whic h are in the range [  X  M L ;M L ] and [  X  M Let M i = max f M L ;M R g . For a v 2 [  X  M i ;M i ] the com-putation of E [ i;v;b ] is the same as before, except that if we consider storing a coe X cien t at i whose value v i is suc h that v + v i (or v  X  v i ) exceeds the range [  X  M L ;M L [  X  M R ;M R ]) then we use the unique line for the left (or righ t) hand side. The imp ortan t point is that v i cannot be cases we would be focusing on the unique lines on both sides and the optim um allo cation of the buc kets is  X xed (do es not dep end on v ). Figure 1: An example of merging tables at a node i
Now consider a v 62 [  X  M i ;M i ]. Supp ose we are looking at v 2 as sho wn in Figure 1. If we do not store a coe X cien t at node i then the minimization is between the unique lines on the left and righ t hand sides and is  X xed. If we do want to store a coe X cien t at i , for v 2 consider a wavelet whic h adds r to the left hand side (wavelet represen ted by solid line around v 2 ). The optimization varies only in the righ t hand side of the table since v 2 + r uses the unique line on the left hand side. For a  X xed b , there is exactly one line in the entire range on the righ t hand side (as v 2  X  r varies) whic h gives the optim um answ er to Lik ewise for r &lt; 0 (sho wn by the dashed line) the righ t hand side uses the unique line and for every b there is a  X xed u whic h minimizes the above equation. Therefore, for every v 62 [  X  M i ;M i ] and every 0  X  b  X  B the error E [ i;v;b ] is the minim um of three quan tities that are indep enden t of v . Hence, for all suc h v &gt; M i (and likewise v &lt;  X  M i use the same line.
 Based on the above and Lemma 7 we get the follo wing
Theorem 11. We can solve the Wavelet Synopsis Con-struction problem for minimizing the weighte d-` k error in time O ( B X   X  2 n 1+4 =p (1 = X  + min ) 2 (min f B; log n g ) error of  X M . The running time for ` 1 reduc es by B=log 2
For the purp ose of the pap er a data stream computation is a space bounded algorithm, where the space is sub-linear in the input. Any input items are accessed sequen tially and any item not explicitly stored cannot be accessed again in the same pass. In the speci X c streaming mo del we will as-sume, we are given num ber X = x 1 ;:::;x i ;:::;x n whic h corresp ond to the signal to be summarized in the increasing order of i . This mo del is often referred to as the aggregated mo del and has been used widely [12, 7, 10]. This mo del is specially suited to mo del streams of time series data [15, 2].
As noted before, our algorithms will not dep end on M , but the appro ximation guaran tee of the streaming algorithm will dep end on this parameter. This is not a very restrictiv e assumption, if stock prices rose or fell exp onen tially , or the temp erature readings from a sensor net work deplo yed in a nuclear plan t rose exp onen tially , typically there would be more radical issues at stak e. For most reasonable analysis tasks, the input has bounded precision and the guaran tee is a non-issue.

The streaming algorithm will build upon the previous sec-tion and borro w from the paradigm of reduce-merge. The high level idea will be to construct and main tain a small ta-ble of possibilities for eac h resolution of the data. On seeing eac h item x i , we will  X rst  X nd out the best choices of the wavelets of length one (over all future inputs) and then, if appropriate, construct/up date a table for wavelets of length 2 ; 4 ;::: etc.

The idea of sub dividing the data, computing some infor-mation and merging results from adjacen t divisions were used in [12] for stream clustering. The stream computation of wavelets in [7] can be view ed as a similar idea|where the divisions corresp onds to the supp ort of the wavelet basis vectors. Figure 2: The arriv al of the  X rst 3 elemen ts. Up on seeing x 2 node 1 computes E [1 ;  X  ;  X  ] and the two error arra ys asso ciated with x 1 and x 2 are discarded. Figure 3: The arriv al of x 4 triggers the computation of E [2 ;  X  ;  X  ] and the two error arra ys asso ciated with x and x 4 are discarded. Subsequen tly, E [3 ;  X  ;  X  ] is com-puted from E [1 ;  X  ;  X  ] and E [2 ;  X  ;  X  ] and both the latter arra ys are discarded. If x 4 is the last elemen t on the stream, the root's error arra y, E [3 ;  X  ;  X  ] , is computed from E [2 ;  X  ;  X  ] . Our streaming algorithm will compute the error arra ys E [ i;  X  ;  X  ] asso ciated with the internal nodes of the coe X cien t tree in a post-order fashion. Recall that the wavelet basis vectors, whic h are describ ed in Sec. 2, form a complete bi-nary tree. For example, the basis vectors for nodes 4 ; 3 ; 1 and and [0 ; 0 ; 1 ;  X  1] resp ectiv ely. The data elemen ts corresp ond to the leaves of the tree and the coe X cien ts of the synopsis corresp ond to its internal nodes. Hence, as men tioned in Sec. 2, assigning the value c to node 2 (equiv alen tly, setting z 2 = c ) for example corresp onds to adding c to W  X  1 ( Z ) and W  X  1 ( Z ) 2 , and adding  X  c to W  X  1 ( Z ) 3 and W
However, we need not store the error arra y for every inter-nal node since, in order to compute E [ i;v;b ] our algorithm from Sec. 3.2 only requires that E [ i L ;  X  ;  X  ] and E [ i kno wn. Hence, it is natural to perform the computation of the error arra ys in a post-order fashion. An example best illustrates the pro cedure. In Fig 2 when elemen t x 1 arriv es, the algorithm computes the error arra y asso ciated with x call it E x 1 . When elemen t x 2 arriv es E x 2 is computed. The arra y E [1 ;  X  ;  X  ] is then computed and E x 1 and E x 2 carded. Arra y E x 3 is computed when x 3 arriv es. Finally the arriv al of x 4 triggers the computations of the rest of the arra ys as in Fig. 3.

Note that at any point in time, there is only one error arra y stored at eac h level of the tree. In fact, the computa-tion of the error arra ys resem bles a binary coun ter. We start with an empt y queue Q of error arra ys. When x 1 arriv es, E 0 is added to Q and the error asso ciated with x 1 is stored in it. When x 2 arriv es, a temp orary node is created to store Algorithm APX ( B;M; X  ) 1. Let j R j = 4 M= X  . 2. Initialize a queue Q with one node q 0 (  X  Eac h q i con tains an arra y E q i of size  X  ) (  X  j R j min f B; 2 i g and a  X  X g isEmpty  X  ) 3. rep eat Until there are no elemen ts in the stream 4. Get the next elemen t from the stream, call it e 5. if q 0 is empt y 6. then Initialize E q 0 [ r 2 R; 0] = j r=e  X  1 j 7. else Create t 0 and Initialize E t 1 [ r 2 R; 0] = j r=e  X  1 j 8. for i = 1 until the 1 st empt y q i or end of Q 9. do Create a temp orary node t 2 . 10. Compute E t 2 [ r;b 2 B ] from t 1 and q i  X  1 11. Set t 1  X  t 2 and Discard t 2 12. Set q i : isEmtpy = true 13. if we reac hed the end of Q 14. then Create the node q i 15. Compute E q i [ r 2 R;b 2 B ] from t 1 and q i  X  1 16. Set q i : isEmtpy = false and Discard t 1 the error arra y asso ciated with x 2 . It is immediately used to compute an error arra y that is added to Q as E q 1 . Node E 0 is emptied, and it is  X lled again upon the arriv al of x When x 4 arriv es: (1) a temp orary E t 1 is created to store the error asso ciated with x 4 ; (2) E t 1 and E q 0 are used to create E 2 ; E t 1 is discarded and E q 0 is emptied; (3) E t 2 and E are used to create E q 2 whic h in turn is added to the queue; E 2 is discarded and E q 1 is emptied. Figure APX sho ws the implemen tation of our algorithm for relativ e ` 1 .
Based on the description of above, the algorithm uses the same space as men tioned in the o X ine algorithm in the pre-vious section. Therefore we conclude with:
Theorem 12. We can solve the Wavelet Synopsis Con-struction problem in a single pass over the data by providing an algorithm (assuming M = max i j x i j ) that The running time for ` 1 reduc es by B=log 2 B in all cases.
A natural question arises, if we were interested in the re-stricted synopses only can we dev elop streaming algorithms for them? The answ er rev eals a rich tradeo X  between syn-opsis qualit y and running time.

The  X rst observ ation we mak e is that if at eac h node we only consider either storing the coe X cien t or 0, then we can limit the searc h signi X can tly. Instead of searc hing over all v + r to the left and v  X  r to the righ t in the dynamic program (whic h we rep eat below) min we only need to searc h for r = c i where c i is the wavelet co-e X cien t at i |observ e that a streaming algorithm can com-pute c i (See [7]). However we have to \round" c i since we are storing the table corresp onding to the values in R and c may have higher precision. We consider the better of round-ing up or rounding down c i to the nearest multiple of  X  . No-tice the set R still performs the role of \an ticipatory values" that are being set by the rounded ancestors. The running time impro ves by a factor of R in this case since to compute eac h entry we are now looking at two values of R (round up/do wn) instead of the entire set. The overall running time is O ( j R j nB ) in the general case and O ( j R j n log the ` 1 varian ts.

The space bound and the appro ximation guaran tees re-main unc hanged. However the guar ante e is now against the synopsis which is restricte d to Z i = W ( X ) i or 0 otherwise. We cannot sho w any relationship between the qualit y of this solution and the general unrestricted case. However in the exp erimen ts we found that simply deciding between the bet-ter of rounding up or down gives a signi X can t impro vemen t in qualit y in some cases. The rounding also introduces more (but bounded) error in other cases as is exp ected from an appro ximation. We conclude with:
Theorem 13. We can solve the restricte d Wavelet Syn-opsis Construction problem in a single pass over the data by providing an algorithm (assuming M = max i j x i j ) that The running time for ` 1 reduc es by B=log 2 B in all cases.
The previous theorem sets the ground for investigating a variet y of Hybrid algorithms where we choose di X eren t searc h strategies (i.e., what set does r range over) at eac h of the nodes i . One of the simplest algorithms is to allo w r 2 R at the root node since we already have full information from the input, and locally at the root, we can choose the best constan t value to add.

Observe that this strategy already gives the optimum so-lution for B = 1 in the bad example f 1 ; 4 ; 5 ; 6 g . In fact this observ ation is our motiv ation for studying the strategy . We can sho w that just this small mo di X cation impro ves the synopsis qualit y signi X can tly.
We consider two issues in this section, namely (i) the qual-ity of the unrestricted version vis-a-vis the restricted opti-mum solution and (ii) the running times of the algorithms.
All exp erimen ts rep orted in this section were performed on a 2 CPU Pentium-I II 1.4 GHz with 2GB of main mem-ory , running Lin ux. All algorithms were implemen ted using version 3.3.4 of the gcc compiler.

Due to shortage of space we restrict ourselv es to the ` 1 and relativ e ` 1 errors for the purp oses of this section. We sho w the performance  X gures of the follo wing schemes:
We chose a syn thetic dataset to sho wcase the point made in the introduction about the sub-optimalit y of the restricted versions. Otherwise we use a publicly available real life data set for our exp erimen t.
Maxim um Relativ e Error: The maxim um relativ e er-rors as a function of B are sho wn in Figures 6 and 7. The  X  in the appro ximation algorithms UNREST, JITTER and HYBRID, was set to 1, as indicated by the discussion in Section 3.4. We sho w two  X gures for the Saw data to em-phasize that the beha vior alluded to in the introduction oc-curs at a wide range of B values and the di X erences are highligh ted since the overall range changes. The restricted version REST either has 30% more error or requires 20% more coe X cien ts compared to the general unrestricted ver-sion. The JITTER and HYBRID algorithms lie in between, HYBRID being better than JITTER as exp ected. Notice that JITTER follo ws REST and then switc hes to the bet-ter beha vior of UNREST. Observ e also that just the simple
See http://lib.stat.cm u.edu/datasets/djdc0093. power of choosing the better of round up or round down achiev es a signi X can t impro vemen t. However REST and JITTER are not great for mo derate to small B , whic h is an imp ortan t range in synopsis construction.

Maxim um Error ( ` 1 ): The ` 1 errors as a function of B are sho wn in Figure 8. The  X  in the appro ximation algorithms UNREST, JITTER and HYBRID, was set to M= min f B; log n g as describ ed in Section 3. We sho w only the Dow data since all the algorithms gave very similar syn-opsis for the Saw data and had almost the same errors. In case of the Dow data we sho w the range B = 5 onward since the maxim um value is  X  500 and the large errors for B &lt; 5 (for all algorithms) biases the scale making the di X erences in the more interesting ranges not visible. Once again REST has a 30% worse error compared to UNREST or requires a lot more coe X cien ts (as a ratio of the synopsis size of UN-REST). The HYBRID algorithm performs consisten tly in the middle.
Figure 9 sho ws the running times of the algorithms as the pre X x size n is varied for the Dow data. We rep ort the running time of the ` 1 algorithms only . As men tioned above  X  was set to 0 : 1 and  X  was set analogously .

The grid in the log-log plot helps us clearly iden tify the quadratic nature of REST. The algorithms UNREST, JIT-TER and HYBRID beha ve linearly .
From the preliminary exp erimen ts sho wn in this pap er the follo wing prop erties are immediate: We are curren tly investigating speeding up the algorithm UNREST by analyzing the searc h space and pruning the computation.
 Ackno wledgmen ts We thank Vlad Petric for help with the implemen tations, and Sampath Kannan for man y useful discussions. We thank the referees for useful feedbac k whic h impro ved the pap er signi X can tly.
Figure 7: Dow data, ` rel 1 Error, n = 16384 ,  X  = 1
Figure 8: Dow data ` 1 Error, n = 16384 ,  X  = 0 : 1 [1] K. Chakrabarti, M. N. Garofalakis, R. Rastogi, and [2] K. Chakrabarti, E. J. Keogh, S. Mehrotra, and M. J. [3] I. Daub echies. Ten Lectur es on Wavelets . SIAM, 1992. [4] A. Deligiannakis and N. Roussop oulos. Extended [5] M. Garofalakis and A. Kumar. Deterministic wavelet [6] M. N. Garofalakis and P. B. Gibb ons. Probabilistic [7] A. Gilb ert, Y. Kotadis, S. Muth ukrishnan, and [8] A. C. Gilb ert, S. Guha, P. Indyk, Y. Kotidis, [9] S. Guha. Space e X ciency in synopsis construction [10] S. Guha, P. Indyk, S. Muth ukrishnan, and M. Strauss. [11] S. Guha, C. Kim, and K. Shim. XW AVE: Optimal [12] S. Guha, N. Mishra, R. Mot wani, and L. O'Callaghan. [13] C. E. Jacobs, A. Fink elstein, and D. H. Salesin. Fast [14] P. Karras and N. Mamoulis. One pass wavelet synopis [15] E. Keogh, K. Chakrabati, S. Mehrotra, and [16] Y. Matias and D. Urieli. Man uscript, 2004. [17] Y. Matias and D. Urieli. Optimal workload-based [18] Y. Matias, J. S. Vitter, and M. Wang. Dynamic [19] Y. Matias, J. Scott Vitter, and M. Wang.
 [20] S. Muth ukrishnan. Workload optimal wavelet [21] G. Strang and T. Nguy en. Wavelets and Filter Banks .
