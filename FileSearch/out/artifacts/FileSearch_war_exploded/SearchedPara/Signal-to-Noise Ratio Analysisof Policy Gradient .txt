 Policy gradient (PG) reinforcement learning algorithms have strong (local) con-vergence guarantees, but their learning performance is typically limited by a large variance in the estimate of the gradient. In this paper, we formulate the variance reduction problem by describing a signal-to-noise ratio (SNR) for policy gradient algorithms, and evaluate this SNR carefully for the popular Weight Perturbation (WP) algorithm. We confirm that SNR is a good predictor of long-term learn-ing performance, and that in our episodic formulation, the cost-to-go function is indeed the optimal baseline. We then propose two modifications to traditional model-free policy gradient algorithms in order to optimize the SNR. First, we examine WP using anisotropic sampling distributions, which introduces a bias into the update but increases the SNR; this bias can be interpreted as following the natural gradient of the cost function. Second, we show that non-Gaussian distribu-tions can also increase the SNR, and argue that the optimal isotropic distribution is a  X  X hell X  distribution with a constant magnitude and uniform distribution in direc-tion. We demonstrate that both modifications produce substantial improvements in learning performance in challenging policy gradient experiments. which are impractical to model effectively, whether due to cost, complexity or uncertainty in the very structure and dynamics of the system (Kohl &amp; Stone, 2004; Tedrake et al., 2004). However, these algorithms often suffer from high variance and relatively slow convergence times (Greensmith et al., 2004). As the same systems on which one wishes to use these algorithms tend to have a high cost of policy evaluation, much work has been done on maximizing the policy improvement from any individual evaluation (Meuleau et al., 2000; Williams et al., 2006). Techniques such as Natural Gradient (Amari, 1998; Peters et al., 2003a) and GPOMDP (Baxter &amp; Bartlett, 2001) have become popular through their ability to match the performance gains of more basic model-free policy gradient algorithms while using fewer policy evaluations.
 As practitioners of policy gradient algorithms in complicated mechanical systems, our group has a vested interest in making practical and substantial improvements to the performance of these algo-rithms. Variance reduction, in itself, is not a sufficient metric for optimizing the performance of PG algorithms -of greater significance is the magnitude of the variance relative to the magnitude of the gradient update. Here we formulate a signal-to-noise ratio (SNR) which facilitates simple and fast evaluations of a PG algorithm X  X  average performance, and facilitates algorithmic performance im-provements. Though the SNR does not capture all facets of a policy gradient algorithm X  X  capability to learn, we show that achieving a high SNR will often result in a superior convergence rate with less violent variations in the policy.
