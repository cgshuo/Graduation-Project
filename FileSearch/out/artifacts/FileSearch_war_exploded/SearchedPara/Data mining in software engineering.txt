
Department of Digital Systems, University of Piraeus, Pira eus, Greece Department of Management Science and Technology, Athens Un iversity of Economics and Business, Athens, Greece Department of Informatics, Athens University of Economics and Business, Athens, Greece 1. Introduction
The recent advances in data management technology provide u s with the tools and methods for efficient collection, storage and indexing of data. Large amount of da ta is produced in software development that software organizations collect in hope of extracting usefu l information and obtain better understanding of their processes and products. Also a significant amount of Op en Source Software (OSS) project metadata is collected and maintained in software repositories. For i nstance, the FLOSSmole 1 project integrates data from several OSS projects and freely provides them in se veral formats. Then there is an increasing abundance of data stored in software engineering (SE) repos itories which can play a significant role in improving software productivity and quality. Specifical ly, data in software development can refer to programs versions, execution traces, error/bug reports and open source packages. Mailing lists, explosive growth of software engineers X  ability to collect and store SE data has created a need for new, scalable and efficient, tools for data analysis.
 Data mining provides the techniques to analyze and extract n ovel, interesting patterns from data. Formally, it has been defined as the process of inducing previ ously unknown and potentially useful infor-mation from data collections. Thus mining of software engin eering data has recently attracted the interest of researchers, emerging as a promising means to meet the goa l of software improvement [58]. The extracted patterns of knowledge can assist software engine ers in predicting, planning, and understanding various aspects of a project so that they can more efficiently support future development and project management activities. There are several challenges that e merge in mining software repositories [57]:  X  Complex Data . Software engineers usually use individual data types to pe rform a software engi- X  Large-scale data . Huge amount of data are collected and stored in software eng ineering repositories.
In this paper, we present an overview of approaches that aim t o connect the research areas of data mining and software engineering leading to more efficient techniqu es for processing software. In Section 2, we provide an introduction to the main data mining concepts a nd approaches while in Section 3 we describe the different types of software engineering data t hat can be mined. In the follow up, a discussion takes place in Section 4 concerning the methods that have app lied data mining techniques in the context engineering while it discusses on the main characteristics of the respective approaches. Specifically, our study is based on the following main features of the appro aches: i) data mining technique used (Section 2), ii) the software engineering data to which they are applied (Section 3), and iii) the software engineering tasks that they can help (Section 4). Thus, this paper aims to introduce practitioners to the fundamental concepts and techniques that can use in orde r to obtain a better understanding of the software engineering processes and potentially perform th em more efficiently by applying data mining. In parallel, researchers can exploit the recent techniques to better understand the project data, to identify the limitations of the current processes and define methodol ogies that facilitate software engineering tasks.

To summarize, the key contributions of this work are:  X  A classification of the approaches used to mine software engi neering data, according to the software  X  Matrix based analysis framework bridging software enginee ring with data mining approaches.  X  Bringing together data mining and software engineering res earch areas. A number of approaches 2. Data mining and knowledge discovery
The main goals of data mining are prediction and description . Prediction aims at estimating the future value or predicting the course of target variables based on s tudy of other variables. Description is focused on patterns discovery in order to aid data representation to wards a more comprehensible and exploitable manner. A good description suggests a good explanation of data behavior. The relevant i mportance of prediction and description varies for different data mining applications. However, as regards the knowledge discovery, description tends to be more important than prediction , contrary to the pattern recognition and machine learning application, for which prediction is more important.

Data mining assists with software engineering tasks by expl aining and analyzing in depth software artifacts and processes. Based on data mining techniques we can extract relations among software projects. Data mining can exploit the extracted informatio n to evaluate the software projects and/or predict software behavior. A number of data mining methods have been proposed to satisfy the requirements of different applications. All of them accomplish a set of da ta mining functionalities to identify and describe interesting patterns of knowledge extracted from a data set. Below we briefly describe the main data mining tasks and how they can be used in software enginee ring.  X  Clustering  X  Unsupervised Learning Techniques  X  Classification  X  Supervised Learning Techniques  X  Frequent Pattern Mining and Association Rules. Association rules mining has attracted considerable  X  Data Characterization and Summarization. Data characterization [23] is the summarization of  X  Change and Deviation Detection focuses on discovering the most significant changes in the da ta
Various approaches have been developed to accomplish the ab ove mentioned data mining tasks and deal with different types of data. They exploit techniques f rom different aspects of data management and data analysis, including pattern recognition, machine lea rning, statistics, information retrieval, concept and text analysis.
 information from text. Software engineering repositories , among others, include textual information like source code, mailing lists, bug reports and execution logs. The mining of textual artifacts is requisite for many important activities in software engineering: tra cing of requirements; retrieval of components from a repository; identify and predict software failures; software maintenance; testing etc. The methods deployed in text mining, depending on the application, usua lly require the transformation of the texts into an intermediate structured representation, which can be for example the storage of the texts into a database management system, according to a specific schema. In many approaches though, there is gain into also keeping a semi-structured intermediate form of th e texts, like for example the representation of documents in a graph, where social analysis and graph tech niques can be applied. Independently from the task objective, text mining requires preprocessin g techniques, usually levying qualitative and quantitative analysis of the documents X  features. In [10,2 ], several preprocessing and feature analysis techniques are discussed for text mining. In Fig. 2, the diag ram depicts the most important phases of the preprocessing analysis, as well as the most important text m ining techniques. 3. Software engineering data
The nature of the data being used by data mining techniques in software engineering can act as distinguishing means of the underlying methods, since it af fects the preprocessing as well as the post analysis. Below we present the various sources of software e ngineering data to which data mining has been applied. The presentation also tries to reflect the diffi culty of preparing the data for processing. 3.1. Documentation
Software documentation data are of high importance but in ta ndem of high complexity for being processed by data mining techniques. Application, system a dministration and source code documentation constitute a large buffer of documents and free text for soft ware analysis and mining. Among the pieces of text information that can be considered of great value for use in mining techniques are the software and compatibility issues. Besides external software docum entation, internal documentation might also types of documents, like portable document format, html, te xt only and typesetting system files. An analytical reference of all possible types of software docu mentation data can be found in [53]. Due to the large variety of document types and text data used, it i s necessary that a preprocessing module for documentation data is able to use parsers of all the afore mentioned types of documents. Another important source of information that lies in software docum entation are the multimedia data. Figures, as well as audio and video instructions, can all be considered o f added information value. In such cases, multimedia mining techniques must be incorporated to tackl e with the pre-and post-processing, raising the overall processing overhead. 3.2. Software configuration management data
Data rising from software configuration management systems (SCMs) among others may include software code, documents, design models, status accountin g, defect tracking as well as revision control data (documentation and comments escorting software versi ons in the adopted CVS). In [17] the evolution of SCMs from the early days of software development to presen t is discussed, where additionally the impact of research in the field of SCMs is depicted. Independe ntly of the underlying version control system (centralized or distributed) the amount of data avai lable from SCMs is large and thus a careful study and clean understanding of the software domain is need ed, so thus the most valuable data are kept. The majority of the SCMs data is structured text.
 Decentralized source code management (DSCM) systems show a significant growth the last years. Bird et al. have studied the main properties of theses system s in [7] and discussed the advantages and risks that decentralization brings in mining software engi neering data. DSCMs can provide software engineering researchers with new and useful data which enab le them to better understand software processes. However the DSCM data should be mined with care. B ird et al. in their work have noted potential pitfalls that one may encounter when analyzing th is data since there are differences in semantics of terms used in centralized and decentralized source code m anagement systems. 3.3. Source code Source code for data mining in software engineering can be pr oved an important source of data. Various data mining applications in software engineering h ave employed source code to aid software maintenance, program comprehension and software componen ts X  analysis. The details of these ap-proaches are discussed in section 4. An initial preprocessi ng of the available source code is always a caveat, since a parser for the respective source code langua ge must be available. Once parsed, the source code can be seen as structured text. Central aspects of apply ing data mining techniques in source code among others include prediction of future changes through m ining change history, predicting change propagation, faults from cached history, as well as predict ing defect densities in source code files. 3.4. Compiled code and execution traces
Compiled code constitutes in its form of object code one of th e alternative data sources for applying static analysis in software engineering. Compiled code has also been used as a data source from data mining techniques in order to assist malicious software det ection. Furthermore, web mining principles have been widely used in object-oriented executables to ass ist program comprehension for means of reverse engineering. When the software modules and compone nts are tested, a chain of events occurs which are recorded in an execution trace. Execution pattern mining has also been used in execution traces under the framework of dynamic analysis to assist with the ex traction of software systems X  functionalities. 3.5. Issue-tracking and bug databases
Issue-tracking or bug reporting databases constitute the m ost important cesspool of issue reporting in software systems. Structured data (database tuples) conta ining the description of an issue, the reporter X  X  details and date/time are the standard three types of inform ation that can be found in issue-tracking assignments of developers to bugs, cleaning the database fr om manifestations of the same error, or even predicting software modules that are affected at the same ti me from reported bugs. 3.6. Mailing lists
Large software systems, and especially open source softwar e, offer mailing lists as a means of bridging users and developers. Mailing lists constitute hard data si nce they contain a lot of free text. Message and author graphs can be easily pulled up from the data, but conte nt analysis is hard since probably messages constituting replies need to consider initial and/or previ ous discussions in the mailing lists. Data mining applications in mailing lists among others include but are n ot limited to text analysis, text clustering of subjects discussed, and linguistic analysis of messages to highlight the developers personalities and profiles. 4. Data mining for software engineering
Due to its capability to deal with large volumes of data and it s efficiency to identify hidden patterns of knowledge, data mining has been proposed in a number of resea rch work as mean to support industrial scale software maintenance, debugging, testing. The minin g results can help software engineers to libraries, analyze defect data, discover reused patterns i n source code and thus automate the development procedure. In general terms, using data mining practitione rs and researchers can explore the potential of software engineering data and use the mining results in or der to better manage their projects and to produce higher quality software systems that are deliver ed on time and on budget. In the following sections we discuss the main features of mining approaches t hat have been used in software engineering and how the results can be used in the software engineering li fe cycle. We classify the approaches according to the software engineering tasks that they help a nd the mining techniques that they use. 4.1. Requirement elicitation and tracing requirements. The works for requirement analysis refers to data mining in its broadest sense, including certain related activities and methodologies from statist ics, machine learning and information retrieval. Table 1 summarizes the main features of the techniques discu ssed below. 4.1.1. Classification
A recent approach, presented in [26], has focused on improvi ng the extraction of high level and low level requirements using information retrieval. More spec ifically, they consider the documents X  universe as being the union of the design elements and the individual r equirements and they map the problem of requirements tracing into finding the similarities between the vector-space representations of high level and low level requirements, thus reducing it into an IR task. As an expansion of this study, in [27], the authors focused on discovering the factors that affect an an alysts X  behavior when working with results from data mining tools in software engineering. The whole st udy was based on the verified hypothesis that the accuracy of computer-generated candidate traces affec ts the accuracy of traces produced by requirements through the use of information retrieval, aff ects the time consumed by an analyst to submit feedback, as well as her performance. Results reveal that da ta mining systems exhibiting low recall result in a time consuming feedback from the analyst. In parallel, h igh recall leads to a large number of false positive thus prompting the analyst cut down large number of requirements, dimming recall. Overall reported results reveal that the analyst tends to balance pr ecision and recall at the same levels. 4.1.2. Data summarization
From another perspective, text mining has been used in softw are engineering to validate the data from mailing lists, CVS logs, and change log files of open source so ftware. In [21] they created a set of tools, namely SoftChange 2 , that implements data validation from the aforementioned t ext sources of open source software. Their tools retrieve, summarize and v alidate these types of data of open source projects. Part of their analysis can mark out the most active developers of an open source project. The statistics and knowledge gathered by SoftChange analysis h as not been exploited fully though, since further predictive methods can be applied with regard to fra gments of code that may change in the future, or associative analysis between the changes X  importance an d the individuals (i.e. were all the changes committed by the most active developer as important as the re st, in scale and in practice?). 4.2. Development analysis
This section provides an overview of mining approaches used to assist with development process. We summarize the main features of these approaches in Table 2. 4.2.1. Clustering Text mining has also been used in software engineering for di scovering development processes. Software processes are composed of events such as relations of agents, tools, resources, and activities organized by control flow structures dictating that sets of e vents execute in serial, parallel, iteratively, or that one of the set is selectively performed. Software pro cess discovery takes as input artifacts of development (e.g. source code, communication transcripts , etc.) and aims to elicit the sequence of events software processes from open source software Web repositor ies is presented. Their method contains text extraction techniques, entity resolution and social n etwork analysis, and it is based on the process of entity taxonomies. Automatic means of evolving the taxon omy using text mining tasks could have been levied, so that the method lacks strict dependency on th e taxonomy X  X  actions, tools, resources and agents. An example could be the use of text clustering on the o pen software text resources and extraction of new candidate items for the taxonomy arising from the clus ters X  labels.

In [6], they used as text input the Apache developer mailing l ist. Entity resolution was essential, since many individuals used more than one alias. After const ructing the social graph occurring from the interconnections between poster and replier, they made a social network analysis and came to really important findings, like the strong relationship between em ail activity and source code level activity. discussions. Though graph and link analysis were engaged in the method, the use of node ranking techniques, like PageRank, or other graph processing techn iques like Spreading Activation, did not take place. 4.2.2. Classification building source code, but also provide a detailed log how the source code has evolved during development. Information regarding the evidence of source code refactor ing will be stored in the repository. Also as bugs are fixed, the changes made to correct the problem are rec orded. As new APIs are added to the source code, the proper way to use them is implicitly explain ed in the source code. Then, one of the challenges is to develop tools and techniques to automatica lly extract and use this useful information.
In [56], a method is proposed which uses data describing bug fi xes mined from the source code repository to improve static analysis techniques used to fin d bugs. It is a two step approach that uses the source code change history of a software project to assist wi th refining the search for bugs.
The first step in the process is to identify the types of bugs that are being fixed in the software. The goal is to review the historical data stored for the software project, in order to gain an understanding of what data exists and how useful it may be in the task of bug findi ngs. Many of the bugs found in the CVS history are good candidates for being detected by statistic analysis, NULL pointer checks and function return value checks.

The second step is to build a bug detector driven by these findings. The idea is to develop a function return value checker based on the knowledge that a specific ty pe of bug has been fixed many times in the past. Briefly, this checker looks for instances where the return value from a function is used in the source code before being tested. Using a return value could m ean passing it as an argument to a function, using it as part of calculation, de-referencing the value if it is a pointer or overwriting the value before it is tested. Also, cases that return values are never stored by the calling function are checked. Testing a return value means that some control flow decision relies on t he value.

The checker does a data flow analysis on the variable holding t he returned value only to the point of determining if the value is used before being tested. It simp ly identifies the original variable the returned value is stored into and determines the next use of that varia ble.

Moreover, the checker categorizes the warnings it finds into one of the following categories:  X  Warnings are flagged for return values that are completely  X  Warnings are also flagged for return values that are used in a c alculation before being tested in a
Any return value passed as an argument to a function before be ing tested is flagged, as well as any pointer return value that is de-referenced without being te sted.

However there are types of functions that lead the static ana lysis procedure to produce false positive value checked. Mining techniques for source code repositor y can assist with improving static analysis results. Specifically, the data we mine from the source code r epository and from the current version of the software is used to determine the actual usage pattern fo r each function.

In general terms, it has been observed that the bugs cataloge d in bug databases and those found by inspecting source code change histories differ in type and l evel of abstraction. Software repositories record all the bug fixed, from every step in development proce ss and thus they provide much useful information. Therefore, a system for bug finding techniques is proved to be more effective when it automatically mines data from source code repositories. 4.2.3. Frequent pattern mining and association rules
An approach is proposed in [44] that exploits association ru les extraction techniques to analyze defect data. Software defects include bugs, specification and desi gn changes. The collected defect data under analysis are nominal scale variables such as description of defect, priority to fix a defect and its status as well as interval and ratio scale variable regarding defec t correction effort and duration. An extended association rule mining method is applied to extract useful information and reveal rules associated with defect correction effort.

The problem of discovering neglected conditions (missing p aths, missing conditions, and missing discover implicit conditional rules in a code base and to dis cover rule violations that indicate neglected condition. They represent programs and conditional progra mming rules in terms of dependence graphs. Then they use frequent item set mining and frequent subgraph mining techniques to discover conditional rules involving preconditions and postconditions of funct ion calls as well as discover violations of those rules. 4.3. Testing
The evaluation of software is based on tests that are designe d by software testers. Thus the evaluation knowledge of the requirements specification.

Data mining approaches can be used for extracting useful inf ormation from the tested software which can assist with the software testing. Specifically, the indu ced data mining models of tested software can be used for recovering missing and incomplete specification s, designing a set of regression tests and evaluating the correctness of software outputs when testin g new releases of the system. A regression test library should include a minimal number of tests that cover a ll possible aspects of system functionality. To ensure effective design of new regression test cases, one has to recover the actual requirements of an existing system. Thus, a tester has to analyze system spec ifications, perform structural analysis of the system X  X  source code and observe the results of system ex ecution in order to define input-output relationships in tested software.

Table 3 summarizes the main data mining techniques that are u sed in the context of software testing. 4.3.1. Clustering
In [13] a method is proposed that exploits the cluster analys is methods to select the set of executions that will be evaluated for conformance to requirements. The prop osed approach assumes a set of execution profiles that have been defined executing the software versio n under test on a given set of program inputs. A clustering algorithm is used to filter profiles based on thei r similar characteristics. Then execution profiles are selected from the resulting clusters.

An approach that aims to analyze a collection of programs X  ex ecutions and define classifiers of software behavior is proposed in [8]. According to this work, Markov m odels are used to encode the execution of profiles of projects. Then the Markov models of individual program executions are clustered using an agglomerative clustering algorithm. The clustering pro cedure aims to aggregate the similar program execution and thus define effective classifiers of program be havior. Also a bootstrapping is used as an active learning technique so that the learning classifiers i s trained in a incremental fashion. Specifically, using the expanded set of training instances. 4.3.2. Classification
An approach that aims to automate the input-output analysis of execution data based on a data mining methodology is proposed in [35]. This methodology relies on the info-fuzzy network ( IFN ) which has an  X  X blivious X  tree-like structure. The network components i nclude the root node, a changeable number of hidden layers (one layer for each selected input) and the tar get (output) layer representing the possible output values. The same input attribute is used across all no des of a given layer (level) while each target node is associated with a value (class) in the domain of a targ et attribute. If the IFN model is aimed at predicting the values of a continuous target attribute, the target nodes represent disjoint intervals in the attribute range.

A hidden layer l , consists of nodes representing conjunctions of values of t he first l input attributes, which is similar to the definition of an internal node in a stan dard decision tree. The final (terminal) nodes of the network represent non-redundant conjunctions of input values that produce distinct outputs. Considering that the network is induced from execution data of a software system, each interconnection between a terminal and target node represents a possible out put of a test case. Figure 3 presents an IFN structure.
 A separate info-fuzzy network is constructed to represent e ach output variable.

The main modules of the IFN-based environment presented in [ 35] are:  X  Legacy system ( LS ). This module represents a program, a component or a system t o be tested in  X  Specification of Application Inputs and Outputs ( SAIO ). Basic data on each input and output variable  X  Random test generator ( RTG ). This module generates random combinations of values in th e range  X  Test bed ( TB ). This module feeds training cases generated by the RTG modu le to the LS.
The IFN algorithm is trained on inputs provided by RTG and out puts obtained from a legacy system by means of the Test Bed module. A separate IFN module is built for each output variable.

The IFN algorithm takes as input the training cases that are r andomly generated by the RTG module and the outputs produced by LS for each test case. The IFN algo rithm repeatedly runs to find a subset of input variables relevant to each output and the correspon ding set of non-redundant test cases. Actual test cases are generated from the automatically detected eq uivalence classes by using an existing testing policy. 4.4. Debugging
Program logic errors rarely incur memory access violations but generate incorrect outputs. A number of mining techniques have been used to identify logic error a nd assist with software debugging (see Table 4). 4.4.1. Classification
An approach that aims to investigate program logic errors is proposed in [36]. Liu et al. develop a data mining algorithm that can assist programmers X  manual debug ging. They introduce a statistical approach to quantify the bug relevance of each condition statement an d then develop two algorithms to locate the possible buggy functions.

The proposed model considers a test suite T n has an input d output of execution P on t subsets T instrument each condition statement in P to collect the evaluation frequencies at runtime. Specifica lly, they consider the boolean expression in each condition stat ement as one distinct boolean feature . Also assuming that X is the random variable for the boolean bias of a boolean feature B , we use f ( X/ X  and f ( X/ X  from T if its underlying probability model f ( X/ X  An open issue is the definition of a suitable similarity funct ion. In [36], they introduce an approach based on a probabilistic model to approximate the values of f ( X/ X  relevance score for a boolean feature B . Moreover they propose two algorithms ( CombineRank and UpperRank ) to combine individual bug scores of B ( s ( B ) ) in order to define a global score s(F) for a function F .

Another method that exploits data mining methods to analyze logical bugs is proposed in [37]. In this work, they treat program executions as software behavi or graphs and develop a method to integrate closed graph mining and SVM classification in order to isolat e suspicious regions of non-crashing bugs. They consider that each execution of a program is summarized as a behavior graph. Then, given a set of behavior graphs that are labeled either positive (incorr ect runs) or negative (correct runs), the goal is to train a classifier to identify new behavior graphs with unk nown labels. The proposed classification model consists of three steps:  X  define the training dataset extracting features from behavi or graphs  X  learn an SVM classifier using these features  X  classify new behavior graphs.

The graphs are represented as vectors in a feature space in or der to apply SVM in behavior graph classification. A naive representation is to consider the ed ges as features and a graph as a vector of edges. that the graph has, and  X 0 X , otherwise. The similarity betwe en two graphs is defined as the dot product of their edges. According to the above representation of gra phs, the dot product of two feature vectors is the number of common edges that two graphs have. The hyperpla ne learned in this way will be a linear combination of edges. Thus it may not achieve good accuracy w hen a bug is characterized by multiple connected call and transition structures. Liu et al. observ ed that functions in well-designed programs usually exhibit strong modularity in source code and in dyna mic executions. Also these functions are often grouped together to perform a specific task. The calls a nd transitions of these functions will be tightly related in the whole behavior graph. The buggy code m ay disturb the local structure of a run and then have an effect on its global structure. Based on this observations they propose to use recurrent local structures as features. They introduce the concept of frequent graphs and define a classification process that is based on them. Each frequent graph is treated as a separate feature in the feature vector. Hence, a behavior graph G is transformed into a feature vecto r whose i -th dimension is set to be 1 if G contains the i -th frequent graph or 0 otherwise. The authors in [37] propos ed an approach for mining closed frequent graphs from a set of behavior graphs a nd then used them as features. Based on these features a classification model is trained so that assi sts programmers with debugging non-crashing bugs. Moreover, an approach that measures incrementally th e classification accuracy changes aiming to identify suspicious regions in a software program. 4.4.1.1. Software failures classification
A semi-automated strategy for classifying software failur es is presented in [46]. This approach is based on the idea that if m failures are observed over some period during which the soft ware is executed, it is likely that these failures are due to a substantially sm aller number of distinct defects. Assume that Then F can be partitioned into k &lt; m subsets F by the same defect d we describe the main phases of the strategy for approximatin g the true failure classification: 1. The software is implemented to collect and transmit to the development either execution profiles 2. Execution profiles corresponding to reported failures ar e combined with a random sample of profiles 3. The profiles of reported failures are analyzed using clust er analysis, in order to group together 4. The resulting classification of failures into groups is ex plored in order to confirm it or refine it.
The above described strategy provides an initial classifica tion of software failures. Depending on the application and the user requirements these initial classe s can be merged or split so that the software failure are identified in an appropriate fashion.

In [19], two tree-based techniques for refining an initial cl assification of failures are proposed. Below we present the main idea of these approaches. 4.4.1.2. Refining failures classification using dendograms
One of the strategies that has been proposed for refining init ial failure classification relies on tree-like diagram (known as dendrograms). Specifically, it uses them t o decide how non-homogeneous clusters should be split into two or more sub-clusters and to decide wh ich clusters should be considered for merging. A cluster in a dendrogram corresponds to a subtree t hat represents relationships among its sub-clusters. The more similar two clusters are to each othe r, the farther away from the dendrogram root their nearest common ancestor is. For instance, based o n the dendrogram presented in Fig. 4 we can observe that the clusters A and B are more similar than the clusters C and D. A cluster X  X  largest homogeneous subtree is the largest subtree consisting of fa ilures with the same cause. If a clustering is too coarse, some clusters may have two or more large homogene ous subtrees containing failures with different causes. Such a cluster should be split at the level where its large homogeneous subtrees are connected, so that these subtrees become siblings as Fig. 6 s hows. If it is too fine, siblings may be clusters containing failures with the same causes. Such sib lings (clusters) should be merged at the level of their parent as Fig. 5 depicts.

Based on these definitions, the strategy that has been propos ed for refining an initial classification of failures using dendrograms has three phases: 1. Select the number of clusters into which the dendrogram wi ll be divided. 2. Examine the individual clusters for homogeneity by choos ing the two executions in the cluster with 3. If neither the cluster nor its sibling is split by step 2, an d the failures were examined have the same
Clusters that have been generated from merging or splitting should be analyzed in the same way, which allow for recursive splitting or merging. 4.4.1.3. Refinement using classification trees
The second technique proposed by Francis et al., relies on bu ilding a classification tree to recognize tree. Each internal node in the tree is labeled with a relatio nal expression that compares a numeric feature of the object being classified to a constant splitting value. On the other hand, each leaf of the tree is labeled with a predicted value, which is the class of interes t the leaf represents.

Given the classification tree, we have to traverse the tree fr om the root to a leaf in order to classify an object. At each step of the traversal prior to reach a leaf, we evaluate the expression at the current node. When the object reaches a leaf, the predicted value of that le af is taken as the predicted class for that object.
 In case of software failure classification problem, we consi der two classes, that is success and failure . The C lassification A nd R egression T ree (CART) algorithms was used in order to build the classific ation tree corresponding of software failures. Assume a training set of execution profiles where each x steps of building the classification tree based on L are as fol lows:  X  The deviance of a node t  X  L is defined as  X  Each node t is split into two children t  X  A node is declared a leaf node if d ( t ) 6  X  , for some threshold  X  .  X  The predicted value for a leaf is the average value of j among the executions in that leaf. 4.4.2. Frequent pattern mining and association rules
Two approaches for mining call-usage patterns from source c ode are presented in [31]. The first approach is based on the idea of itemset mining. It identifies frequent subsets of items that satisfy at least a user-defined minimum support. The results of applying this approach to source code are unordered patterns related to the function calls. On the other hand, se quential pattern mining approach produces a set of ordered patterns with a specified minimum support. In g eneral terms these approaches can assist with mining patterns of call-usage and thus identifying pot ential bugs in a software system. 4.5. Maintenance A problem that we have to tackle in software engineering is th e corrective maintenance of software. of the failures fall into small groups, each consisting of fa ilures caused by the same software defect. Recent research has focused on data mining techniques which can simplify the problem of classifying failures according to their causes. Specifically, these app roaches requires that three types of information about executions are recorded and analyzed: i) execution profiles reflecting the causes of the failures, ii) auditing information that can be used to confirm reported failures and iii) diagnostic information that can be used in determining their causes. Below we present the various data mining approaches used to facilitate software maintenance (see also Table 5). 4.5.1. Clustering
In [32] a framework is presented for knowledge acquisition f rom source code in order to comprehend an object-oriented system and evaluate its maintainabilit y. Specifically, clustering techniques are used to assist engineers with understanding the structure of sou rce code and assessing its maintainability. The proposed approach is applied to a set of elements collected f rom source code, including:  X  Entities that belong either to behavioral (classes, member methods) or structural domain (member  X  Attributes that describe the entities (such class name, sup erclass, method name etc).  X  Metrics used as additional attributes that facilitate the s oftware maintainer to comprehend more
The above elements specifies the data input model of the frame work. Another part of the framework is an extraction process which aim to extract elements and me trics from source code. Then the extracted information is stored in a relational database so that the da ta mining techniques can be applied. In the specific approach, clustering techniques are used to analyz e the input data and provide a rough grasp of the software system to the maintenance engineer. Clusterin g produces overviews of systems by creating mutually exclusive groups of classes, member data, methods based on their similarities. Moreover, it can assist with discovering programming patterns and outli er cases (unusual cases) which may require attention.

Text clustering has also been used in software engineering, in order to discover patterns in the history and the development process of large software projects. In [ 54] they have used CVSgrab to analyze the ArgoUML and PostgreSQL repositories. By clustering the related resources, they generated the evolution of the projects based on the clustered file types. U seful conclusions can be drawn by careful manual analysis of the generated visualized project develo pment histories. For example, they discovered that in both projects there was only one author for each major initial contribution. Furthermore, they came to the conclusion that PostgreSQL did not start from scr atch, but was built atop of some previous project. An interesting evolution of this work could be a mor e automated way of drawing conclusions from the development history, like for example extracting c lusters labels, map them to taxonomy of development processes and automatically extract the devel opment phases with comments emerging from taxonomy concepts.
 development and maintenance. Introducing the concepts of i nter-connectivity and intra-connectivity, they develop a clustering algorithm that aims to partition t he components of a system into compact and well-separated clusters. Specifically, they aim to apply cl ustering to the module dependency graph in order to identify significant connection among the system mo dules. The goal is to partition the software system so that it maximizes the connections between the comp onents of the same cluster and minimizes the connections between the components of distinct cluster s).

Basit et al. [4] introduced the concept of structure clone an d proposed the use of mining techniques in order to detect them in software. The procedure of detecti ng structural clones can assist with under-standing the design of the system for better maintenance and with re-engineering for reuse. According to their approach, they extract simple clones form the source c ode (similar code fragments). Then they use techniques of finding frequent closed item sets to detect rec urring groups of simple clones in different files or methods. Also clustering techniques are applied to i dentify significant groups of similar clones. Also Basit et al. implement their structural clone detectio n technique in a tool called Clone Miner . 4.5.2. Classification
In [49] they use the data coming from more than 100.000 open so urce software projects lying in the SourceForge portal, in order to build a predictive model for software maintenance using data and text mining techniques. Using SAS Enterprise Miner and SAS T ext Miner, they focused on collecting values for variables concerning maintenance costs and effo rt from OSS projects, like Mean Time to Recover (MTTR) an error. The task also entailed the removal o f projects that were under development, thus considering exclusively operational projects, as wel l as the removal of projects that did not have a bug reports database since the absence of such prohibited t he measurement of variables like MTTR. Furthermore, they clustered the remaining projects based o n their descriptions, in order to discover the most important categories of OSS projects lying in the Sourc eForge database. Finally, they used the SAS Enterprise Miner to build classifiers on the MTTR class va riable, after having transformed the later into a binary one (High or Low) using its values X  distributio n. The reported results highlight interesting correlations between features like number of downloads, us e of mail messages and project age and the class variable. For example, projects with increased age ha ve higher MTTR than younger projects.
An approach that exploits the idea of spam filtering techniqu es to identify fault-prone software modules is presented in [42]. The proposed framework is based on the f act that faulty software modules have similar pattern of words or sentences. Mizuno et al. propose d the implementation of a tool that extracts fault-probe (FP) modules and non fault-prone (NPF) modules from source code repositories. Then these set of modules are used to learn a classifier that is used to cla ssify new modules as FP or NFP.
Also an approach for classifying large commits so that under stand the rationale behind them is proposed in [25]. Though large commits are usually consider ed as outliers when we study source control repositories (SCRs), they may contain useful information a bout the projects and their evolution. Hindle etal. decided to exploit classification techniques in order to classify commits and thus identify different types of software changes. This study shows that in many case s the large commits refer to modification of the system architecture while small commits are more ofte n corrective.

Ekanayake et al. [16] propose a method to evaluate the stabil ity of a prediction model. They explore four open source projects and extract features from their CV S and Bugzilla repositories. Then they build defect prediction models using Weka X  X  decision tree learne r and evaluate the prediction quality over time. This study conclude that there are significant changes over t ime and thus it should be used cautiously. 4.5.3. Frequent pattern mining and association rules
The work proposed by Zimmerman et al. [59] exploits the assoc iation rules extraction technique to identify co-occurring changes in a software system. For ins tance, we want to discover relation between the modification of software entities. Then we aim to answer t he question when a particular source-code entity (e.g. a function A) is modified, what other entities ar e also modified (e.g. the functions with names B and C)? Specifically, a tool is proposed that parses the sour ce code and maps the line numbers to the subsequent entity changes in the repository are grouped as a transaction. An association rule mining techniques is then applied to determine rules of the form B, C  X  A .

Sartipi et al. [50] proposes the use of clustering and associ ation rules techniques in order to recover the architectural design of legacy software systems according to user defined plans. The source code of a legacy system is analyzed and a set of frequent itemsets is ex tracted from it. Using clustering and pattern matching techniques, the proposed algorithm defined the com ponents of the legacy system. Given a user query, the best matching component of the system is selected . Also a score can be associated with each possible answer (match) to the user query and thus a ranking o f design alternatives can be presented to the user for further evaluation.

An approach for identifying library reuse patterns is prese nted in [41]. The proposed approach exploits association rules techniques to identify relatio ns among classes in a library. The authors extend the concept of traditional association rules to generalize d rules so that the inheritance relationships are taken into account. Thus an automated technique is develope d for discovering reused patterns in libraries and identifying characteristic usages of a library.

An approach for analyzing instantiation code to find usage ch anges in evolving frameworks is proposed in [51]. The mining process takes as input two versions of ins tantiation code and exploiting frequent pattern mining techniques aims to find patterns describing a changed usage of the framework. At the first step, it extracts information about how the instantiat ion code uses the framework (which methods are called, which framework classes are sub-classed). Then transactions are built by combining usage algorithm is applied to those transactions to extract all po ssible change rules. 4.5.4. Change and deviation detection
The identification and fixing of bugs is one of the most common a nd costly tasks of software devel-opment. The software projects manage the flow of the bugs usin g software configuration management (SCM) systems to control the bug changes, bug tracking softw are (such as Bugzilla) to capture bug reports and then they record the SCM system that fixes a specifi c bug in the tracking system. Generally, a bug is introduced into the software when a programmer makes a change to the system, that is, to add a new functionality, to reconstruct the code or to repair an e xisting bug. When the bug is identified, it is recorded in a bug tracking system. Subsequently, a develo per could repair the bug by modifying the project X  X  source code and commit the change to the SCM system . This modification is widely called bug-fix change. The bug tracking and SCM systems are widely us ed, the most readily available data concerning bugs are the bug-fix changes. There are some appro aches that deals with mining a SCM system to find those changes that have repaired a bug. There ar e two categories of approaches that search for changes in the log messages: i) approaches [43] th at searches for keywords such as  X  X ixed X  and  X  X ug X , ii) approaches that look for references to the bug reports (e.g #9843 ). Bug-fixing informa-tion is useful for determining the location of a bug. This per mits useful analysis, such as determining per-file bug counts, predicting bugs, finding risky parts of s oftware or visually revealing the relationship between bugs and software evolution. One of the main problem s with the bug-fix data is that it does not give an indication when a bug was injected into the code an d who injected it. Also bug-fix data provide imprecise data on where a bug occurred. In order to de eply understand the phenomena related to the introduction of bugs into the code, we need access to th e actual moment and point the bug was introduced. The algorithm proposed in [52] (further referred to as SZZ algorithm) is th e first effort to identify bug-introducing changes from bug-fix changes. The main steps of SZZ can be summarized as follows: i) it finds bug-fix changes by locating bug identifier s or relevant keywords in change log text or following a recorded linkage between a bug tracking system a nd a specific SCM commit. ii) it runs a diff tool to determine what changed in the bug-fixes. The diff tool returns a list of regions further, referred to as hunks , which are different in the two files. In each hunk the deleted or modified source code is considered as a location of a bug. iii) it tracks down the orig ins of the deleted or modified source code in hunks. For this purpose it uses the built-in annotate featur e of a SCM system, which computes the most recent revision in which a line was changed and the developer who made the change. The discovered origins are identified as bug-introducing changes. However there some limitations of the SZZ algorithm which can be summarized as follows:  X  SCM annotation does not provide enough information to ident ify bug-introducing changes. Also we  X  All modifications are not fixes: There might be changes that ar e not bug-fixes. For instance, changes
An approach proposed in [34] aims to tackle the above discuss ed problems of the SZZ algorithm. The proposed approach exploits annotation graphs which contai n information on the cross-revision mappings of the individual lines. This allow us to associate a bug with its containing function or method. The proposed bug-introducing identification algorithm can be e mployed as an initial clean-up step to obtain high quality data sets for further analysis on causes and pat terns of bug formation. The accuracy of the automatic approach is determined using a manual approac h. This implies that two human manually verified all hunks in a series of bug-fix changes to ensure the c orresponding hunks are real bug-fixes. The main steps of the approach introduced in [34], which aims to r emove false positive and false negatives in identifying bug-introducing changes are the followings :  X  Use annotation graphs that provide more detailed annotatio n information  X  Ignore comments, black line, format changes, outlier bug-fi x revisions in which too many files were  X  Manually verify all hunks in the bug-fix changes 4.5.5. Mining approaches based on statistics Many of source code version repositories repositories are e xamined and managed by tools such as CVS (Concurrent Versions System) and (increasingly) its su ccessor Subversion . These tools store difference information access across document(s) version s, identifies and express changes in terms of physical attributes, i.e., file and line numbers. However, C VS does not identify, maintain or provide any change-control information such as grouping several chang es in multiple files as a single logical change. Moreover, it does not provide high-level semantics of the na ture of corrective maintenance(e.g. bug-fixes). Recently, the interest of researchers has been focused on te chniques that aim to identify relationships and trends at a syntactic-level of granularity and further asso ciate high-level semantics from the information available in repositories. Thus a wide array of approaches t hat perform mining of software repositories (MSR) have been emerged. They are based on statistical metho ds and differencing techniques, and aim to extract relevant information from the repositories, ana lyze it and derive conclusions within the context of a particular interest. 4.5.5.1. Mining via CVS annotations
One approach is to utilize CVS annotation information. Gall et al. [20] propose an approach for detecting common semantic (logical and hidden) dependenci es between classes on account of addition or modification of particular class. This approach is based o n the version history of the source code where a sequence of release numbers is maintained for each cl ass in which its changes are recorded. Classes that have been changed in the same release are compar ed in order to identify common change patterns based on author name and time stamp from the CVS annotations. Classes that are changed with the same time stamp are inferred to have dependencies.

Specifically, this approach can assist with answering quest ions such as which classes change together, how many times was a particular class changed, how many class changes occurred in a subsystem (files in a particular directory). An approach that studies t he file-level changes in software is presented in [22]. The CVS annotations are utilized to group subsequen t changes into what termed modification request (MR). The proposed approach focus on studying bug-M Rs and comment-MRs to address issues regarding the new functionality that may be added or the bugs that may be fixed by MRs, the different stages of evolution to which MRs correspond or identify the r elation between the developer and the modification of files. 4.5.5.2. Mining via heuristics
CVS annotation analysis can be extended by applying heurist ics that include information from source code or source code models. Hassan et al. [24] proposed a vari ety of heuristics (developer-based, history-based, code-layout-based (file-based)) which are then used to predict the entities that are candidates for a change on account of a given entity being changed. CVS annota tions are lexically analyzed to derive the set of changed entities from the source-code repositories. Also the research in [24,59] use source-code version history to identify and predict software changes. T he questions that they answered are quite interesting with respect to testing and impact analysis.
 4.5.5.3. Mining via differencing
Source-code repositories contain differences between ver sions of source code.Thus it would be inter-esting to mine source code repositories, identify and analy ze the actual source-code differences.
An approach that aims to detect syntactic and semantic chang es from a version history of C code is presented by Raghavan [48]. According to this approach, e ach version is converted to an abstract or deriving the semantics of an expression in a programming l anguage. A top-down or bottom-up heuristics-based differencing algorithm is applied to eac h pair of in-memory ASGs. The differencing algorithm produces an edit script describing the nodes that are added, deleted, modified or moved in order to derive one ASG from another. The edit scripts produc ed for each pair of ASGs are analyzed to answer questions from entity level changes such as how many f unctions and functions calls are inserted, added or modified to specific changes such as how many if statement conditions are changed. Also in [12] a syntactic-differencing approach, which is called meta-differencing , is introduced. It allows us to ask syntax-specific questions about differences. Accord ing to this approach the abstract syntax tree (AST) information is directly encoded into the source code v ia XML format. Then we compute the added, deleted or modified syntactic elements based on the en coded AST. The types and prevalence of syntactic changes can be easily computed. Specifically, the approach supports the following questions: i) Are new methods added to an existing class?, ii) Are there c hanges to pre-processor directives?, iii) Was the condition in an if-statement modified? 4.6. Software reuse
Systematic software reuse has been recognized as one of the m ost important aspects towards the increase of software productivity, and quality [40,45]. Th ough software reuse can take many forms (e.g., ad-hoc, systematic), and basic technical issues such as dev elopment of software repositories, and search engines for software components in various programming lan guages are on the frontier of research in the area of software reuse, recently there have been attempts to incorporate data mining techniques in an effort to identify the most important factors affecting the succes s of software reuse. The motivation behind those approaches stems partially from the fact that previou s surveys showed possibility of projects X  failure due to the lack of reuse processes introduction, as well as mo dification of non-reuse processes [45]. the software reuse process in conducted projects. More spec ifically, they collected through an interview process data from twenty-four European projects from ninet een companies in the period from 1994 to 1997. In their analysis, they defined 27 variables that are us ed to formulate the description of each project, which are nicely summarized in [40]. Among the used variables, there are ten state variables representing attributes over which a company has no control , six high-level control variables representing key high-level management decisions about a reuse program, ten low-level control variables representing specific approaches to the implementation of reuse, and a var iable indicating whether the project was successful or not. Currently, this data set, though with few examples, constitutes the largest empirical data set on software reuse at present, and in this data set sev eral data mining algorithms have been applied to identify patterns regarding the factors affecting the su ccess of software reuse. Table summarizes the main feature of mining techniques that have been used in soft ware reuse. 4.6.1. Classification
In the same study ([40]), the authors also attempted the use o f decision trees, and more specifically the J. 48 implementation of Weka, which is essentially the implement ation of the C 4 . 5 decision tree algorithm [47], in order to analyze the same data. The applic ation of the C 4 . 5 decision tree algorithm in this was made in a manner so that the authors were able to ide ntify the most important features from the 27, by conducting attribute removal experiments. More s pecifically, they studied what would be the root node of the tree in each case, if at each time the most i mportant attribute is removed (i.e., the root node), and the tree is rebuilt without considering that attribute. This methodology allowed them to identify weak attributes (i.e., attributes that appear in a ny non-root node after several removals of root node attributes), as well as barely supportive attributes ( i.e., attributes that, once root nodes, if removed and the tree is rebuilt disregarding them, the classificatio n accuracy remains the same).

In addition to the aforementioned analysis, they also appli ed a learning algorithm called treatment learning , and more specifically they applied the TAR2 algorithm [39]. The basic idea behind the treatment learner is that it selects a subset D  X  of the training set D , which contains more preferred classes and less undesired ones. The criterion according to which the subset is selected is based on the used treatment, denoted as R requires from the user to assign a numeric score to each class that represents how much a user likes that class. In this case, the authors weighted more the successfu l reuse project than an unsuccessful project, and after the conducted analysis through the application of TAR2 , they were able to discover the features and the respective value ranges that were mostly used in succ essful reuse projects. The interesting part from the application of this data mining algorithm is the fac t that the algorithm discovered features and value ranges that the empirical study in [45] had failed to un cover, showing how important the application of data mining can be in this case.

In another direction, but still using the same data set for so ftware reuse, Jiang et al. [30] applied an ensemble learning approach (i.e, an approach that combines the decisions of different classifiers and attempts to get the best of all worlds) based on the notion of r andom forests [9]. The used ensemble algorithm, RF2TREE (Random Forest to Tree), introduced in the same paper, has tw o conditions under which it can be guaranteed to work well: (a) the original trai ning data set is very small, like in the case of the data set produced in [45], and (b) the random forest is m ore accurate than single decision tree if both of them were directly trained from the original trainin g data set. The algorithm first builds a random forest from the original data set, and then the random forest is used to generate many virtual examples that are used to train a single decision tree. Based on their c onducted experiments, they discovered Process Introduced, Type of Software Production, Applicat ion domain, Top Management Commitment, and Nonreuse Processes Modified, which vary from the empiric al analysis in [45], and the data mining analysis in [40]. The differences, as well as the similariti es of the three research works with regards to the most important factors affecting software reuse are sum marized nicely in Table 8 in the work of Jiang et al. [30].
 4.6.2. Frequent pattern mining and association rules
Menzies and Di Stefano [40] worked on the aforementioned dat a set in order to examine further what conclusions may be drawn regarding the affecting factors in software reuse, and also to compare the patterns derived from applying automated data mining metho dologies with the empirical conclusions formulated in [45]. Among the methodologies they used, asso ciation rule mining was employed, in order to extract meaningful associations between the 27 features. The association rule extraction was conducted with the Apriori algorithm [1] implementation of fered by the Weka data mining platform 3 . The top 10 association rules derived, setting minimum confid ence at 90%, are presented in [40]. Among the association rules derived, there are very interesting a ssociations learned, like for example the fact that when the produced software was embedded in a product (e. g., in contrast to being embedded in a process, or itself being a stand-alone product), the use of reward policy for software reuse was not enabled (i.e., SoftwareandProduct = product = &gt; RewardsPolicy = no ).

Another important aspect of data mining in software enginee ring is the process of mining design specifically, the process of mining focuses on extracting pa tterns by analyzing the code or the design of the software system in order to trace back the design decisio ns made, which are usually buried inside the source code. Typically, during the software system desi gn, the system components are not tagged with the respective design patterns applied, and, thus, the design decisions are no longer connected with the existent system, often leading to lack of understanding of the software X  X  details. In this direction, a number of techniques and tools have been proposed in the pas t, which attempt to mine the design patterns from a software system.

In [14] the authors present a thorough overview of these appr oaches in a comparative study. Depending on the description of each design pattern, i.e., the perspec tive from which it is described, the approaches of design pattern mining can be widely classified into the one s analyzing the structural aspect only, the behavioral aspect only, or both. There are also some approac hes that attempt to analyze a combination of the above aspects including the semantic aspect as well, w hich refers to the semantic meaning of some entities in the system. From the perspective of our anal ysis, we focus only on the types of data mining methods to extract those patterns. In this direction , the primal technique used is the utilization of existing tools that transform the source code into some inte rmediate representation, e.g. Abstract Syntax Trees (AST) or Abstract Semantic Graphs (ASG), and then simp le search strategies are applied to the transformed graphs, in order for patterns to be identified. T his procedure of course implies that the patterns are somehow already defined, in order for the applic ation of the search strategy to be able to find matches. The problem of design pattern mining from sourc e code is then reduced to identify graph components of the code that match already predefined pattern s, which can be expressed for example in a XML-like format [3].

The matching itself can be usually conducted through the use of sub-graph isomorphic comparison already known), using graph comparison or similarity metri cs between the examined graphs and the graph patterns. 5. Summary and open issues in mining software repositories
The recent explosive growth of our ability to collect data du ring the software development process has created a need for new, scalable and efficient, tools for data analysis. Also there is strong requirement for mining software repositories and extracting hidden inform ation. This extracted knowledge is expected to assists the software engineers with better understandin g the development processes and predict the future of software products. The main focus of the disciplin e of data mining in software repositories is to address this need. In this paper we review the various data mining methods and techniques used in software engineering. Specifically our objective is to pres ent an overview of the different data sources in software engineering that are interesting to be mined. Also we discuss how the data mining approached can be used in software engineering and what software engine ering tasks can be helped by data mining. The main characteristics of data mining approaches used in s oftware engineering are summarized in Tables 1 X 6.

One of the main issues in software engineering is the evaluat ion of software project and the definition of metrics and model that give us an indication of the future o f a project. Though a number of mining approaches have been used to assist with software engineeri ng tasks, an open issue is if and how data mining techniques can be exploited to define novel quality me trics in software engineering.
Below we discuss challenging issues in mining software engineering reposit ories that are interesting and deserve further work.  X  Supervised learning approaches , like text classification, based on predictive modeling tec hniques,  X  Online mining . The data mining techniques that have recently been develop ed in software engineer- X  Quality project classification . A classifier will be built to categorize projects as success ful or non- X  Association rules extraction from OSS project data . There is useful information provided for Open  X  Graph mining on the mailing lists of OSS projects . Based on the information provided by the  X  Pattern mining form source code . Another interesting perspective in the category of design pattern  X  Mining bug reports . The bug report database contains useful information regar ding the quality of the Acknowledgments This work is supported by the European Community Framework P rogramme 6, Information Society Technologies key action, contract number IST-5-033331 (SQ O-OSS: Software Quality Observatory for Open Source Software).
 References
