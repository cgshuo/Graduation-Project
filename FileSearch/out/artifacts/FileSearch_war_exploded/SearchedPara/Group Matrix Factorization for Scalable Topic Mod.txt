 Topic modeling can reveal the latent structure of text data and is useful for knowledge discovery, search relevance ranking, docu-ment classification, and so on. One of the major challenges in topic modeling is to deal with large datasets and large numbers of topics in real-world applications. In this paper, we investigate techniques for scaling up the non-probabilistic topic modeling ap-proaches such as RLSI and NMF. We propose a general topic mod-eling method, referred to as Group Matrix Factorization (GMF), to enhance the scalability and e ffi ciency of the non-probabilistic approaches. GMF assumes that the text documents have already been categorized into multiple semantic classes, and there exist class-specific topics for each of the classes as well as shared top-ics across all classes. Topic modeling is then formalized as a prob-lem of minimizing a general objective function with regularizations and / or constraints on the class-specific topics and shared topics. In this way, the learning of class-specific topics can be conducted in parallel, and thus the scalability and e ffi ciency can be greatly im-proved. We apply GMF to RLSI and NMF, obtaining Group RLSI (GRLSI) and Group NMF (GNMF) respectively. Experiments on a Wikipedia dataset and a real-world web dataset, each contain-ing about 3 million documents, show that GRLSI and GNMF can greatly improve RLSI and NMF in terms of scalability and e ciency. The topics discovered by GRLSI and GNMF are coherent and have good readability. Further experiments on a search rele-vance dataset, containing 30,000 labeled queries, show that the use of topics learned by GRLSI and GNMF can significantly improve search relevance.
 Categories and Subject Descriptors: H.3.1 [Information Storage and Retrieval]: Content Analysis and Indexing General Terms: Algorithms, Experimentation Keywords: Matrix Factorization, Topic Modeling, Large Scale
Topic modeling refers to machine learning technologies whose aim is to discover the hidden semantic structure existing in a large collection of text documents. Given a collection of text documents, a topic model represents the relationship between the terms and the documents through latent topics. A topic is defined as a probability distribution over terms or a cluster of weighted terms. A document is viewed as a bag of terms generated from a mixture of latent top-ics. Many topic modeling methods, such as Latent Semantic Index-ing (LSI) [7], Probabilistic Latent Semantic Indexing (PLSI) [11], Latent Dirichlet Allocation (LDA) [5], Regularized Latent Seman-tic Indexing (RLSI) [26], and Non-negative Matrix Factorization (NMF) [13, 14] have been proposed and successfully applied to di ff erent applications in text mining, information retrieval, natural language processing, and other related fields.

One of the main challenges in topic modeling is to handle large numbers of documents and create large numbers of topics. For the probabilistic topic models like LDA and PLSI, the scalability chal-lenge mainly comes from the necessity of simultaneously updating the term-topic matrix to meet the probability distribution assump-tions. When the number of terms is large, which is inevitable in real applications, this problem becomes particularly severe. For the non-probabilistic methods of NMF and RLSI, the formulation makes it possible to decompose the learning problem into multiple sub-problems and conduct learning in parallel, and hence in general to [26] for detailed discussions.

The high scalability of non-probabilistic methods makes them easier to be employed in practice. However, to handle millions or even billions of documents, it is still necessary to further improve their scalability and e ffi ciency. In this paper, we investigate the possibilities of further enhancing the scalability and e ffi non-probabilistic methods such as RLSI and NMF.

The method, called Group Matrix Factorization (GMF), assumes that the documents have already been categorized into multiple classes in a predefined taxonomy. This assumption is practical and common in many real-world applications. For example, Wikipedia data contains a hierarchical taxonomy with 25 classes at the first layer. Each Wikipedia article falls into at least one of the classes. about 4 million web pages manually classified into the classes. The data can be used for training a classifier and other webpages can be classified into the classes by the classifier [2]. GMF further as-sumes that there exists a set of class-specific topics for each of the classes, and there also exists a set of shared topics for all of the classes. Each document in the collection is specified by its classes, class-specific topics, as well as shared topics. In this way, the large-scale learning problem can be decomposed into small-scale sub-problems. We refer to the strategy as the divide-and-conquer technique. assumption and thus it is hard to be scaled up. 2 http: // www.dmoz.org /
In GMF, the documents in each of the classes are represented as a term-document matrix. The term-document matrix is then ap-proximated as the product of two matrices: one matrix represents the shared topics as well as the class-specific topics, and the other matrix represents the document representations based on the top-ics. An objective function is defined to measure the goodness of prediction of the data with the model. Optimization of the objec-tive function leads to the automatic discovery of topics as well as topic representations of the documents.

We show that GMF can be used to improve the e ffi ciency and scalability of non-probabilistic topic models, using RLSI and NMF as examples. Specifically, we apply GMF to RLSI [26] and NMF [13, 14], obtaining the Group RLSI (GRLSI) and Group NMF (GNMF), respectively. Like in RLSI, the objective function of GRLSI con-sists of squared Frobenius norm as loss function, 1 -regularization on topics, and 2 -regularization on document representations. Sim-ilarly to NMF, GNMF also uses squared Frobenius norm as loss function and non-negative constraints on the topics and document representations. Algorithms for optimizing the loss functions of GRLSI and GNMF are given and theoretical justification of the al-gorithms is shown. Time complexity analysis show that GRLSI and GNMF can achieve P times of speedup on RLSI and NMF respec-tively, where P is the number of classes.

Experiments on two large datasets containing about 3 million documents have verified the following points. (1) Both GRLSI and GNMF can e ffi ciently handle the documents on a single machine, and the number is larger than those which can be processed by most existing topic modeling methods. (2) GRLSI and GNMF are more scalable and e ffi cient than RLSI and NMF respectively, especially when the number of topics is large. (3) In GRLSI and GNMF, the shared topics as well as the class-specific topics are coherent and meaningful. (4) Experiments on another relevance dataset show that GRLSI and GNMF can help significantly improve search rele-vance.

Exploiting the divide and conquer strategy in the non-probabilistic methods has been investigated in computer vision [16, 25]. How-ever, it was not clear whether it works for text data. As far as we know, this is the first work on large scale text data. Our main con-tributions in this paper lie in that we have empirically verified the e ff ectiveness of the divide and conquer strategy on text data, by specifically implementing and testing the GRLSI and GNMF meth-ods in large scale experiments.
The goal of topic modeling is to automatically discover the latent topics in a document collection as well as model the documents by representing them with the topics. Methods for topic modeling fall into two categories: probabilistic approaches and non-probabilistic approaches. In the probabilistic approaches, a topic is defined as a probability distribution over terms and documents are viewed as data generated from mixtures of topics. To generate a document, one first chooses a topic distribution. Then, for each term in that document , one chooses a topic according to the topic distribution, and draws a term from the topic according to its term distribu-tion. PLSI [11] and LDA [5] are two widely-used probabilistic approaches to topic modeling. Please refer to [3] for a survey on probabilistic topic models. In the non-probabilistic approaches, the term vectors of documents (term-document matrix) are projected into a topic space in which each axis corresponds to a topic. A doc-ument is then represented as a vector of topics in the space. These approaches are realized as factorization of the term-document ma-trix such that the matrix is approximately equal to the product of a term-topic matrix and a topic-document matrix under certain con-straints. LSI [7] is a representative method, which performs the factorization under the assumption that the topic vectors are or-thonormal. In NMF [13, 14], the factor matrices are assumed to be nonnegative, while in RLSI [26], the factor matrices are regular-ized with 1 and / or 2 norms.

It has been demonstrated that topic modeling is useful for knowl-edge discovery, search relevance ranking, and document classifica-tion (e.g., [19, 27, 26]). Topic modeling is actually becoming one of the most important technologies in text mining, information re-trieval, natural language processing, and other related fields.
The topic modeling approaches that we have discussed so far are completely unsupervised. Recently, researchers have also proposed supervised or semi-supervised approaches to topic modeling. For example, Supervised Latent Dirichlet Allocation (SLDA) [4] and Supervised Dictionary Learning (SDL) [18] are methods for incor-porating supervision into probabilistic and non-probabilistic topic models. In this paper, we assume that documents have already been classified into classes, and then we conduct topic modeling on the basis of the classification to enhance scalability and e ffi
Using document classes in topic modeling has been studied in previous literature. For probabilistic approaches, Zhai et al.(2004), for example, proposed incorporating class labels into a multinomial mixture model in order to more accurately discover topics,such that some topics are shared by all classes and other topics are spe-cific to individual classes [28]. The discriminatively training of LDA (DiscLDA) [12] and Partially Labeled Dirichlet Allocation (PLDA) [22] incorporate class labels into LDA to achieve similar goals. For the non-probabilistic approaches, Mairal et al. (2008) [17], Bengio et al. (2009) [1], and Wang et al. (2011) [25] proposed us-ing class labels in Sparse Coding [15, 20], a special case of RLSI, in which a dictionary for each class (i.e., topics specific to each class) is learned first, after that a common dictionary over all classes (i.e., topics shared by all classes) is learned, and finally the common and class-specific dictionaries are learned simultaneously. Group Non-negative Matrix Factorization (GNMF) [16] extends NMF in a sim-ilar way. Both extensions on non-probabilistic methods were con-ducted in computer vision. As can be seen, all the previous work was not motivated toward enhancing scalability and e ffi ciency. In this paper, we also exploit class information in topic modeling and our goal is to enhance scalability and e ffi ciency. As far as we know, this is the first time such an investigation is conducted on text data. We also note that the formulation of GNMF in this paper is di from that in [16].
We present the formulation of Group Matrix Factorization and provide a probabilistic interpretation of it.
Suppose that we are given a document collection D with size N , containing terms from a vocabulary V with size M . A document is represented as a vector d  X  R M where each entry denotes the score of the corresponding term, for example, a Boolean value in-dicating occurrence, term frequency, tf-idf, etc. Each document is associated with a class label y  X  { 1 ,  X  X  X  , P } .The N documents in can be classified into P classes according to their class labels and represented as D = { D 1 ,  X  X  X  , D P } . D p = d ( p ) 1 ,  X  X  X  , the term-document matrix corresponding to class p , in which each row stands for a term and each column stands for a document. N is the number of documents in class p such that P p = 1 N
A topic is defined as a subset of terms from V with important weights, and is also represented as a vector u  X  R M with each entry corresponding to a term. Suppose that there are K s shared topics, denoted as a term-topic matrix U 0 = u (0) 1 ,  X  X  X  , u (0) which each column corresponds to a shared topic. Also, for each class p ,thereare K c class-specific topics, which can also be repre-sented by a term-topic matrix U p = u ( p ) 1 ,  X  X  X  , u ( p ) each column stands for a class-specific topic. Then, the total num-ber of topics in the whole collection is K = K s + PK c 3 .
The documents in each class are then modeled by the shared topics as well as the topics specific to their own class. Specifi-cally, given the shared topics U 0 and the class-specific topics U document d ( p ) n in class p is approximately represented as a linear combination of these topics, i.e., matrix corresponding to class p ,and v ( p ) n  X  R K s + K tation of document d ( p ) n in latent topic space. Since a document is represented only by the shared topics and the class-specific top-ics corresponding to its own class, GMF actually decomposes the large-scale matrix operations concerning all the topics into multi-ple small-scale ones concerning only subsets of the topics, and thus reduces the computational complexity.
 matrix corresponding to class p . We denote V T p = H T p that  X  U p V p = U 0 H p + U p W p ,where H p  X  R K s  X  N topics U p . Table 1 gives a summary of notations.

Thus, given a document collection together with the class labels, represented as D = { D 1 ,  X  X  X  , D P } , GMF amounts to solving the following optimization problem: where L (  X  X  ) is a loss function that measures the quality of the ap-proximation defined in Eq. (1); R 1 (  X  ) , R 2 (  X  ) ,and ization items on shared topics, class-specific topics, and document representations, respectively; C 1 , C 2 ,and C 3 are feasible sets for shared topics, class-specific topics, and document representations, respectively;  X  1 ,  X  2 ,and  X  3 are coe ffi cients.
We give a probabilistic interpretation of GMF, as shown in Fig-ure 1. In the graphical model, shared topics u (0) 1 ,  X  X  X  , shared topics are independent from each other, with prior p u e specific topics are independent from each other, with prior p u e 3 A more general case is defining di ff erent numbers of class-specific topics for di ff erent classes, which can also be modeled by GMF. y . Documents d 1 ,  X  X  X  , d N are also observed variables. Each doc-ument is generated according to a probability distribution condi-tioned on the shared topics, the class-specific topics, the corre-sponding class label, and the corresponding latent variable, i.e., Moreover, all triplets ( y n , d n , v n ) are independent given u formulation Eq. (2) can be obtained with Maximum A Posteriori approximation.

GMF can be applied to non-probabilistic methods to further en-hance their scalability and e ffi ciency. Next, as examples, we define Group RLSI (GRLSI) and Group NMF (GNMF) under the frame-work of GMF.
GRLSI adopts the squared Euclidean distance to measure the ap-proximation quality and employs the same regularization schema as in RLSI [26], i.e., 1 -regularization on both shared and class-specific topics and 2 -regularization on document representations. The optimization problem of GRLSI is as follows: where  X  1 is the parameter controlling the 1 -regularization, and Algorithm 1 Group RLSI Require: D 1 ,  X  X  X  , D P 1: for p = 1: P do 2: U p  X  zero matrix 3: V p  X  random matrix 4: end for 5: repeat 6: U 0  X  Update U 0 D p , U p , V p 7: for p = 1: P do 8: U p  X  Update U p D p , U 0 , V p 9: V p  X  Update V p D p , U 0 , U p 10: end for 11: until convergence 12: return U 0 , U 1 ,  X  X  X  , U P , V 1 ,  X  X  X  , V P is the parameter controlling the 2 -regularization 4 . GRLSI decom-poses the large-scale matrix operations in RLSI into multiple small-scale ones and thus can be solved more e ffi ciently.
Optimization Eq. (3) is convex with respect to one of the vari-we sequentially minimize the objective function with respect to shared topics U 0 , class-specific topics U 1 ,  X  X  X  , U P representations V 1 ,  X  X  X  , V P . This procedure is summarized in Al-gorithm 1.
Holding U 1 ,  X  X  X  , U P , V 1 ,  X  X  X  , V P fixed, the update of U to the following minimization problem: where  X  F is the Frobenius norm and u (0) mk is the mk -th entry of U Eq. (4) is equivalent to where D and H are defined as D = [ D 1  X  U 1 W 1 ,  X  X  X  , D and H = [ H 1 ,  X  X  X  , H P ], respectively. Let  X  d m = ( d are those of the m th row of D and U 0 , respectively. Eq. (5) can be decomposed into M subproblems that can be solved independently, with each corresponding to one row of U 0 : for m = 1 ,  X  X  X  , M .

Eq. (6) is an 1 -regularized least squares problem, whose objec-tive function is not di ff erentiable and it is not possible to directly apply gradient-based methods. A number of techniques can be used here, such as interior point method [6], coordinate descent with soft-thresholding [9, 10], Lars-Lasso algorithm [8, 21], and feature-sign search [15]. Here we choose coordinate descent with soft-thresholding. To do so, we calculate S 0 = HH T = P p = 1 4 A more general case is setting di ff erent regularization parameters for shared topics and class-specific topics, for separately control-ling the sparsity of shared topics and class-specific topics. Algorithm 2 Update U 0 Require: D 1 ,  X  X  X  , D P , U 1 ,  X  X  X  , U P , V 1 ,  X  X  X  , 3: for m = 1: M do 5: repeat 6: for k = 1: K s do 9: end for 10: until convergence 11: end for 12: return U 0 Algorithm 3 Update U p Require: D p , U 0 , V p 3: for m = 1: M do 5: repeat 6: for k = 1: K c do 9: end for 10: until convergence 11: end for 12: return U p R and then update U 0 with the following update rule: where s (0) ij and r (0) ij are the ij -th entry of S 0 and R (  X  ) + denotes the hinge function. The algorithm for updating U summarized in Algorithm 2.
Holding the other variables fixed, the update of U p amounts to the following optimization problem: the same technique presented for optimizing Eq. (5). We calculate S = W then update U p with the following update rule: where s ( p ) ij and r ( p ) ij are the ij -th entry of S The algorithm for updating U p is summarized in Algorithm 3. Algorithm 4 Update V p Require: D p , U 0 , U p 1:  X  p  X   X  U T p  X  U p +  X  2 I 2:  X  p  X   X  U T p D p 3: for n = 1: N p do 5: end for 6: return V p Table 2: Time complexity (per iteration) of RLSI and GRLSI.
The update of V p with the other variables fixed is a least squares problem with 2 -regularization. It can also be decomposed into N optimization problems, with each corresponding to one v ( p ) be solved in parallel: for n = 1 ,  X  X  X  , N p . It is a standard 2 -regularized least squares problem and the solution is: Algorithm 4 shows the procedure.
The formulation of learning in GRLSI is decomposable and thus can be processed in parallel. Specifically, the for -loops in Algo-and Algorithm 4 (i.e., line 3 to line 5) can be processed in parallel. In this paper, we implement GRLSI as well as RLSI using multi-threaded programming and compare their time complexities. Table 2 shows the results, where Q is the number of threads,  X  the topic sparsity, and AvgDL the average document length. For GRLSI, the  X  X pdate U  X  includes the update of U 0 , U 1 ,  X  X  X  , U P and the  X  X pdate that GRLSI is approximately P times faster than RLSI in terms of time complexity. Here we suppose that 1) the documents are evenly distributed to the P classes; 2) the number of class-specific topics in each class is similar to the number of shared topics; and 3) the topic sparsity of GRLSI is similar to the topic sparsity of RLSI.
Folding-in refers to the problem of computing representations of documents that were not contained in the original training collec-tion. When a new document d , represented as d  X  R M in the term space, is given, its representation in the topic space can be com-puted under two di ff erent conditions. First, if the class label of the document is given, denoted as y d , we represent the document in the topic space as Second, if the document label is unknown, we first define the error Algorithm 5 Group NMF Require: D 1 ,  X  X  X  , D P 1: U 0  X  random matrix 2: for p = 1: P do 3: U p  X  random matrix 4: V p  X  random matrix 5: end for 6: repeat 8: for p = 1: P do 11: end for 12: until convergence 13: return U 0 , U 1 ,  X  X  X  , U P , V 1 ,  X  X  X  , V P of classifying document d into class p as and predict the class label of document d by We then represent the document in the topic space with Eq. (8).
Similarly we can define Group NMF (GNMF) by adopting the squared Euclidean distance to measure the approximation quality and employing the nonnegative constraints on shared topics, class-specific topics, and document representations, as in NMF [13, 14]. The optimization problem of GNMF is as follows: which decomposes the large-scale matrix operations in NMF into multiple small-scale ones and thus can be solved more e ffi
Optimization Eq. (9) is convex with respect to one of the vari-We again sequentially minimize the objective function with respect to shared topics U 0 , class-specific topics U 1 ,  X  X  X  , U ment representations V 1 ,  X  X  X  , V P . The procedure is summarized in Algorithm 5, where the operator  X   X   X  represents the entry-wise mul-tiplication, and the division is also entry-wise.

The multiplicative update rules in Algorithm 5 were first pro-posed in [16] and then applied in [25]. However, neither [16] nor [25] gave su ffi cient evidence to demonstrate the correctness of them. Here, we theoretically justify Algorithm 5, showing that the objective in Eq. (9) is nonincreasing under the update rules in Al-gorithm 5. We first proof Proposition 1.
P roposition 1. Given X , Y  X  R M  X  N + and S  X  R K  X  N + timization problem min A  X  0 X  X  Y  X  AS 2 F . The objective is nonin-creasing under the update rule and the division is also entry-wise.
 A proof sketch of the proposition can be found in Appendix.
Holding U 1 ,  X  X  X  , U P , V 1 ,  X  X  X  , V P fixed, the update of U to the following minimization problem: which can be rewritten as where E , F ,and H are respectively defined as E = [ D 1 ,  X  X  X  , F = [ U that the objective is nonincreasing under the update rule according to Proposition 1.
Holding the other variables fixed, the update of U p amounts to the following optimization problem: According to Proposition 1 we get the multiplicative update rule: which keeps the objective nonincreasing.
The update of V p with the other variables fixed amounts to the following optimization problem: As demonstrated in [14], V p can be updated with the following update rule: which keeps the objective nonincreasing.
The multiplicative update rules of GNMF (i.e., line 7, line 9, and line 10 in Algorithm 5) can be processed in parallel since the multiplication and division are both entry-wise. In this paper, we implement GNMF as well as NMF using multithreaded program-ming and compare their time complexities. Table 3 shows the re-sults, where Q is the number of threads and AvgDL is the average document length. For GNMF, the  X  X pdate U  X  includes the up-date of U 0 , U 1 ,  X  X  X  , U P and the  X  X pdate V  X  includes the update of Table 3: Time complexity (per iteration) of NMF and GNMF. V ,  X  X  X  , V mately P times faster than NMF in terms of time complexity. Here we also make the same assumptions as in Section 4.2.
Given a new document d  X  R M , its representation in the topic space can be computed under two di ff erent conditions. First, if the class label y d is also given, we can represent the document in the topic space as Second, if the document label is unknown, we first define the error of classifying document d into class p as and predict the class label of document d by We then represent the document in the topic space with Eq. (10).
Topic modeling can be used in a wide variety of applications. We apply GRLSI and GNMF to relevance ranking in search and eval-uate their performances in comparison to RLSI and NMF respec-tively. The use of topic modeling techniques such as LSI was pro-posed in IR many years ago [7]. Two recent works [27, 26] demon-strated that improvements on relevance ranking can be achieved by using topic modeling.

The motivation of incorporating topic modeling into relevance ranking is to reduce  X  X erm mismatch X . Traditional relevance mod-els, such as VSM [24] and BM25 [23], are all based on term match-ing. The term mismatch problem arises when the author of a docu-ment and the user of a search system use di ff erent terms to describe the same concept, and in such a case the search may not be carried out successfully. For example, if the query contains the term  X  X ir-plane X  but the document contains the term  X  X ircraft X , then there is a mismatch and the document may not be viewed as relevant. In the topic space, however, it is very likely that the two terms are in the same topic, and thus the use of matching score in the topic space may help improve the relevance ranking. In practice it is benefi-cial to combine topic matching scores with term matching scores, to leverage both broad topic matching and specific term matching.
A general way of using topic models in IR is as follows. Sup-pose that there is a pre-learned topic model. Given a query q and a document d , we first represent them in the topic space as v v respectively. Then we calculate the matching score between the query and the document in the topic space as the cosine similar-ity between v q and v d . The topic matching score s topic linearly combined with the term matching score s term ( q nal relevance ranking. The final relevance ranking score s ( q calculated as: any existing term-based model, for example, VSM and BM25.
We have conducted experiments to test the e ffi ciency and e tiveness of GRLSI and GNMF.
We tested the e ffi ciency and e ff ectiveness of GRLSI and GNMF loaded from the English version of Wikipedia and Web-I dataset which consists of webpages randomly sampled from a crawl of the Internet at a commercial search engine. The Wikipedia dataset con-tains 2,807,535 articles and the Web-I dataset contains 3,184,138 web documents. For both datasets, the titles and bodies were taken as the contents of the documents. Stop words in a standard list and terms whose total frequencies are less than 10 were removed. Table 4 lists the sizes of Wikipedia and Web-I datasets.

In the Wikipedia dataset, documents are associated with labels representing the categories of them. We adopted the 25 first-level categories in the Wikipedia hierarchy, i.e., each Wikipedia docu-ment is categorized into one of the 25 categories. The categories in-clude  X  X griculture X ,  X  X rts X ,  X  X usiness X ,  X  X ducation X ,  X  X aw X , etc. In the Web-I dataset, similarly, all documents are categorized into one of the ODP categories by a built-in classifier at the search engine. There are 204 categories from the second-level ODP categories, in-cluding  X  X rts / music X ,  X  X usiness / management X ,  X  X omputer tics of both Wikipedia and Web-I, where Min and Max stand for the minimal and maximal class sizes respectively, R is the range of class sizes, i.e., R = Max  X  Min, Mean and STD represent the mean value and the standard deviation of class sizes respectively, and CV is the coe ffi cient of variance, i.e., CV = STD / Mean. One can see that Web-I has smaller R and CV values, indicating that it has a smaller degree of dispersion in the distribution of class sizes. From the table, we can see that although these two datasets have similar data sizes, the granularities of classes, i.e., number of classes and average number of documents per class, are very di ff erent.
We tested RLSI, NMF, GRLSI and GNMF on the Wikipedia dataset and Web-I dataset under di ff erent parameter settings. We used single machine implementations of the methods. Specifically, for the Wikipedia dataset, we set the number of class-specific top-ics per class and the number of shared topics in GRLSI and GNMF as ( K s , K c ) = (10 , 4) / (20 , 8) / (50 , 20) / (100 110 / 220 / 550 / 1100 total number of topics. (Note that the total number of topics in GRLSI and GNMF is K s + 25  X  K c , where 25 is the number of classes in the Wikipedia dataset.) We set the number of topics in RLSI and NMF as 110 / 220 / 550 / 1100 for fair compari-son. For the Web-I dataset, we decided the number of class-specific Table 6: Execution time (per iteration) of RLSI on Wikipedia. Table 7: Execution time (per iteration) of GRLSI on Wikipedia. topics per class and the number of shared topics in GRLSI and GNMF as ( K s , K c ) = (10 , 5) / (20 , 10) / (40 , 20) / in K = 1030 / 2060 / 4120 / 10300 as the total number of topics. (The total number of topics in GRLSI and GNMF is K s + 204  X  K where 204 is the number of classes in the Web-I dataset.) As will be explained later, we found that it is not possible to run RLSI and NMF with such large numbers of topics on a single machine. Thus, we determined the number of topics in RLSI and NMF as 100 / 200 / 500 / 1000. Parameter  X  1 in GRLSI and RLSI, which con-trols the sparsity of topics, was selected from 0 . 01 / 0 for both datasets. Parameter  X  2 in GRLSI and RLSI was fixed to 0 . 1, following the experimental results in [26].

We also conducted search relevance experiments to test the ef-fectiveness of GRLSI and GNMF on another dataset, the Web-II dataset, which is obtained from the same web search engine. The dataset consists of 752,365 documents, 30,000 queries, and rel-evance judgments on the documents with respect to the queries. The relevance judgments are at five levels:  X  X erfect X ,  X  X xcellent X ,  X  X ood X ,  X  X air X , and  X  X ad X . There are in total 837,717 judged query-document pairs. The documents in Web-II are classified into 204 ODP categories with the same classifier as in Web-I. We randomly split the queries into validation / test sets, each has 15,000 queries. We used the validation set for parameter tuning and the test set for evaluation. We adopted MAP and NDCG at the positions of 1, 3, 5, and 10 as evaluation measures for relevance ranking. When calculating MAP, we considered  X  X erfect X ,  X  X xcellent X , and  X  X ood X  as  X  X elevant X , and the other two as  X  X rrelevant X .
 All of the experiments were conducted on a server with AMD Opteron 2.10GHz multi-core processor (2  X  12 cores), 96GB RAM. All the methods were implemented using C# multithreaded pro-gramming, with the thread number being 24. In this experiment, we evaluated the e ffi ciency improvement of GRLSI and GNMF over RLSI and NMF on the Wikipedia dataset and the Web-I dataset. We ran all the methods in 100 iterations. For each method, the average execution time per iteration was recorded.
Table 6 and Table 7 report the average execution time per itera-tion for RLSI and GRLSI on Wikipedia, under di ff erent settings of topic numbers and  X  1 values. Figure 2 further shows average time per iteration of GRLSI and RLSI versus numbers of topics when  X  = 0 . 01. Figure 3 shows the average time per iteration of GNMF over NMF on Wikipedia, versus numbers of topics. From these results, we can conclude that GRLSI and GNMF consistently out-perform RLSI and NMF, respectively, in terms of e ffi ciency. More speedup can be achieved when total number of topics increases.
Figure 2: Execution time of RLSI and GRLSI on Wikipedia. Figure 3: Execution time of NMF and GNMF on Wikipedia.
 The results indicate that GRLSI and GNMF are superior to RLSI and NMF in terms of e ffi ciency.

Table 8 and Table 9 report the average execution time per itera-tion for RLSI and GRLSI on Web-I with respect to di ff erent settings of topic numbers and  X  1 values. Figure 4 shows the average exe-cution time per iteration of GRLSI and RLSI when  X  1 equals 0.01. Figure 5 shows the results of GNMF and NMF. In fact we were not able to run RLSI and NMF on the single machine, when the num-ber of topics is larger than 1,000. The results indicate that GRLSI and GNMF have better e ffi ciency and scalability, particularly when the number of topics gets large.

From the experimental results reported above, we can conclude that applying GMF to non-probabilistic methods of RLSI and NMF can significantly improve the e ffi ciency and scalability of them. The proposed GRLSI and GNMF methods can handle much larger numbers of topics and much larger datasets.

Next, we evaluated the e ff ectiveness of GRLSI and GNMF by checking the readability of the topics generated by them. As ex-ample, we show the topics generated by GRLSI and GNMF in the setting of ( K s = 20 , K c = 8(10) , X  1 = 0 . 01 , X  2 both Wikipedia and Web-I. Table 10 and Table 11 present exam-ple topics randomly selected from the topics discovered by GRLSI and GNMF on Wikipedia and Web-I. For each of the datasets and each of the methods, 3 shared topics and 9 class-specific topics are presented. The corresponding class labels are also shown for the class-specific topics. Top 6 weighted terms are shown for each topic. From all the results (including the results in other parameter settings), we found that (1) GRLSI and GNMF can discover read-able topics. Both of the shared topics and the class-specific topics are coherent and easy to understand. (2) For each class, GRLSI and GNMF can discover class-specific topics that characterize the class. (3) GRLSI discovers compact topics (the average topic com-pactness AvgComp = 0.0032 for Wikipedia topics and AvgComp = with non-zero weights per topic.
 Table 8: Execution time (per iteration) of RLSI on Web-I. Table 9: Execution time (per iteration) of GRLSI on Web-I.
We further evaluated the shared topics discovered from Wikipedia (Table 10) and Web-I (Table 11). In the Web-I dataset, the shared topics seem to characterize general information. In the Wikipedia dataset some of the shared topics are similar to the class-specific topics in category  X  X eography X . We checked the Wikipedia dataset and found that this is because more than one third of Wikipedia ar-ticles fall into category  X  X eography X , and some geography related topics appear to be general in the document collection.
From the experimental results reported above, we can conclude that applying GMF to non-probabilistic methods of RLSI and NMF can maintain the same level of readability while significantly im-proving the e ffi ciency and scalability. The resulting methods of GRLSI and GNMF can really find coherent and meaningful topics. This is true for not only class-specific topics, but also shared topics. In this experiment, we tested the e ff ectiveness of GRLSI and GNMF by using the topics generated by them with the Web-I dataset for GRLSI, we combined the topic matching scores with the term matching scores given by BM25, denoted as  X  X M25 + GRLSI X . We took RLSI and CRLSI as baselines, denoted as  X  X M25 + RLSI X  and  X  X M25 + CRLSI X , respectively. In the former an RLSI model is trained for the whole Web-I dataset and in the latter an RLSI model is trained for each class. Similarly, for GNMF, we combined the topic matching scores with the term matching scores by BM25, de-noted as  X  X M25 + GNMF X . We took NMF and CNMF as baselines, denoted as  X  X M25 + NMF X  and  X  X M25 + CNMF X , respectively. In the former an NMF model is trained for the whole Web-I dataset and in the latter an NMF model is trained for each class.
GRLSI, RLSI, GNMF, and NMF were trained on Web-I dataset with the same parameter settings in Section 7.1. For CRLSI and CNMF, we also trained the models on Web-I dataset under the same parameter settings in Section 7.1, except parameter K s ,asthere exists no shared topic in CRLSI and CNMF.
 To evaluate the relevance performance of these topic models on Web-II, we took a heuristic method for relevance ranking. Given a query q and a document d (and its label y d ), the method as-signs the query into the same class that the document belongs to, i.e., class y d , and then calculates the matching score between the query and the document in the topic space using the techniques de-scribed above for GRLSI and CRLSI (also GNMF and CNMF). The method then ranks the documents based on their relevance scores. The relevance score of a document is calculated as a lin-ear combination of the BM25 score and the topic matching score the categories are not consistent with the categories in Web-II. between the document and the query. For RLSI (also NMF), neither document labels nor query labels were needed. We directly calcu-lated the matching score between a query and a document in the topic space using the techniques described in [26]. The trading-o parameter  X  in the linear combination was set from 0 to 1 in steps of 0 . 1 for all methods. The heuristic method of automatic assign-ment of a query into a class has the advantage of better e online prediction, given that usually the number of classes is large. Even though this is heuristic, our experimental results show that it is e ff ective.

Table 12 and Table 13 show the retrieval performance of RLSI families and NMF families on the test set of Web-II respectively, obtained with the best parameter setting determined by the valida-tion set. From the results, we can see that (1) all of these meth-ods can significantly improve the baseline BM25 (t-test, p-value &lt; 0 . 05). (2) GRLSI and GNMF perform significantly better than CRLSI and CNMF respectively (t-test, p-value &lt; 0 . 05), indicat-ing the e ff ectiveness of Group Matrix Factorization, specifically, the use of shared topics. (3) GRLSI and GNMF perform slightly worse than RLSI and NMF, but they can achieve much higher ef-ficiency and scalability, as described in Section 7.2. The decreases of accuracy by GRLSI and GNMF are very small, e.g., NDCG@1 drops only 0.0010 for GRLSI and 0.0011 for GNMF. (4) The NMF families perform better than the RLSI families. This is because we did not further tune the parameters for the RLSI families. The results in [26] show that with fine tuning RLSI can achieve high performances, and we anticipate that this is also the case for the other RLSI methods. We conclude that both GRLSI and GNMF are useful for relevance ranking with high accuracies.
In this paper, we have investigated the possibilities of further enhancing the scalability and e ffi ciency of non-probabilistic topic modeling methods. We have proposed a general topic modeling technique, referred to as Group Matrix Factorization (GMF), which conducts topic modeling on the basis of existing classes of docu-ments. Thus the learning of a large number of topics (i.e.,class-specific topics) can be performed in parallel. Although the strat-egy has been tried in computer vision, this is the first compre-Table 12: Relevance performance of RLSI families on Web-II. Table 13: Relevance performance of NMF families on Web-II. hensive study of it on text data, as far as we know. The GMF technique can be further specified in individual non-probabilistic methods. We have applied GMF to RLSI and NMF, obtaining Group RLSI (GRLSI) and Group NMF (GNMF), and theoretically demonstrated that GRLSI and GNMF are much more e ffi cient and scalable than RLSI and NMF in terms of time complexity.
We have conducted experiments on two large datasets to test the performances of GRLSI and GNMF. Both datasets contain about 3 million documents. Experimental results show that GRLSI and GNMF are much faster and scalable than existing methods such as RLSI and NMF, especially when the number of topics is large. We have also verified that GMF can discover meaningful topics and the topics can be used to improve search relevance. As future work, we plan to implement GMF on distributed systems and perform experiments on even larger datasets. [1] S. Bengio, F. Pereira, and Y. Singer. Group sparse coding. In [2] P. N. Bennett, K. M. Svore, and S. T. Dumais.
 [3] D. Blei. Introduction to probabilistic topic models. [4] D. Blei and J. McAuli ff e. Supervised topic models. In NIPS , [5] D. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet [6] S. S. Chen, D. L. Donoho, and M. A. Saunders. Atomic [7] S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Landauer, [8] B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani. Least [9] J. Friedman, T. Hastie, H. Hofling, and R. Tibshirani. [10] W. J. Fu. Penalized regressions: The bridge versus the lasso. [11] T. Hofmann. Probabilistic latent semantic indexing. In [12] S. Lacoste-Julien, F. Sha, and M. I. Jordan. Disclda: [13] D. D. Lee and H. S. Seung. Learning the parts of objects [14] D. D. Lee and H. S. Seung. Algorithms for non-negative [15] H. Lee, A. Battle, R. Raina, and A. Y. Ng. E ffi cient sparse [16] H. Lee and S. Choi. Group nonnegative matrix factorization [17] J. Mairal, F. Bach, J. Ponce, G. Sapiro, and A. Zisserman. [18] J. Mairal, F. Bach, J. Ponce, G. Sapiro, and A. Zisserman. [19] D. M. Mimno and McCallum. Organizing the oca: Learning [20] B. A. Olshausen and D. J. Fieldt. Sparse coding with an [21] M. Osborne, B. Presnell, and B. Turlach. A new approach to [22] D. Ramage, C. D. Manning, and S. Dumais. Partially labeled [23] S. E. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu, [24] G. Salton, A. Wong, and C. S. Yang. A vector space model [25] F. Wang, N. Lee, J. Sun, J. Hu, and S. Ebadollahi. Automatic [26] Q. Wang, J. Xu, H. Li, and N. Craswell. Regularized latent [27] X. Wei and B. W. Croft. Lda-based document models for [28] C. Zhai, A. Velivelli, and B. Yu. A crosscollection mixture
P roof sketch of P roposition 1. The proof will follow closely the proof given in [14] for the case Y = 0 . First note that the objective is decomposable in the rows of A . Considering the case of a single row, denoted as  X  a , leads to the objective where  X  x and  X  y are the corresponding rows of X and Y respectively. Define the auxiliary function G  X  a ,  X  a t as
G  X  a ,  X  a t = F (  X  a ) +  X  a  X   X  a t T  X   X  a F  X  a where  X   X  a t is a diagonal matrix defined as Here,  X  ij is equal to 1 if i = j and 0 otherwise. Then the update rule can be derived using the methods in [14].
