 Keywords offer an important clue for people to grasp the topic of the document, and help people easily understand what a document describes, saving a great amount of time reading the whole text. Being increasingly exposed to more and more informa-tion on the Internet, people of today have to be more selective about what to read and it X  X  impossible for people to mark keywords for every document manually. Conse-quently, automatic keyword extraction is in high demand. Meanwhile, it is also fun-damental to many other natural language processing applications, such as information retrieval, text clustering, text categorization and so on. When people mark the documents manually, they try to understand the key facts present in the documents according to their background knowledge, and then pick out the most important content words as keywords. Our keyword extraction method tries to provide the background knowledge for the target document, which make our me-thod can  X  X nderstand X  the document as human being, then  X  X dentify X  the most impor-tant words in the document. 
A straightforward solution for building the background knowledge is to make use of massive data. Patent data is the record of science and technology, which has recog-nized authority. In recently, patent data has become the main source of science and technology in the world. The United States Patent and Trademark Office publish pa-Patent. Each patent is presented in well XML format, including patent title, number, gorous description of science and technology, including abundant knowledge. It can be a very good support for background knowledge building. 
The key contribution of our work is the introduction on how to make use of a uni-versal knowledge base, patent data, to construct the background knowledge for the patent data as the external knowledge repository. 
The second contribution of our work is the introduction of novel word features for keyword extraction. We built a document X  X  background knowledge derived from patent data. To retrieve background knowledge related to the target document, we knowledge is obtained, we can mine the hidden knowledge by using the relationship between patent inventors, assignees and citations for semantic relatedness. We then nees and patent citations link in the retrieved files. Our approach utilizes all four types of information during the keyword selection process. The newly introduced word features that reflect the document X  X  background knowledge offer valuable indications on individual words X  importance in the input document and serve as nice complements to the traditional word features derivable from explicit information of a document. The results of our experiments confirm the effectiveness of our new word features for key-word extraction by considering the background knowledge of a document. There are a number of classical approaches to extracting keywords. Traditional key-word extraction methods only use information explicitly contained in a document based on lexical and syntactic features, such as the word frequency and word posi-tions etc. However, these methods were generally found to lead to poor results, and consequently other methods were explored. 
The research community investigated supervised learning methods whereby train-KEA [2] system, the candidate terms are represented using three features: TFxIDF, by the number of words in the document), key phrase-frequency (the number of times trained using the naive Bayes learning algorithm. Thus, KEA analyses only simple statistical properties of candidate terms and considers each candidate term indepen-dently from the other terms in the document. 
An alternative class of approaches applies graph-based semantic relatedness meas-ures for extracting keywords. The example is TextRank [3]: a document is modeled as a graph of candidate terms in which edges represent a measure of term relatedness. Some graph analysis technique is used to select key terms. Usually the graph is ana-lyzed using a graph-based ranking algorithm (such as PageRank [4], HITS [5], etc.) to range candidate terms. Top N terms with the highest scores are selected as keywords. 
Last but not least, in the past few years, different works have leveraged the struc-tured knowledge from Wikipedia to extract keywords. The new methods have been proposed that extend classical methods with new features computed over Wikipedia corpus. For instance, Wikify! [6] introduces keyword feature of a candidate term that is defined as the number of Wikipedia articles in which the term appears and is marked up as a link divided by the total number of Wikipedia articles where the term rules only key terms should be used as links to other articles. Maui [7] is the successor of the KEA system, and it extends the feature set used by KEA with several Wikipe-dia-based features: a) Wikipedia keyphraseness: likelihood of a term in being a link in Wikipedia; b) semantic relatedness: semantic score derived from Wikipedia asso-ciated with each candidate keyword; and c) inverse Wikipedia linkage, which is the normalized number of pages that link to a Wikipedia page. The semantic relatedness feature is calculated by linking each candidate keyword to a Wikipedia article, then comparing  X  by means of a Wikipedia-based semantic measure  X  a given candidate keyword to all the other candidates. The final value corresponds to the total of all the pairwise comparisons. Paper [8] exploits the titles of Wikipedia articles as well of the graph of Wikipedia categories, and paper [9] utilize Wikipedia as a thesaurus for can-didate keyword selection. The paper [10] uses not only information explicitly con-tained in the document such as word frequency, position, and length but also the background knowledge of the document, which is acquired from Wikipedia via ana-lyzing the inlink, outlink, category, and infobox information of the document X  X  related articles in Wikipedia. Paper [11] compares the state-of-the-art systems, and none of them make use of patent data. 
The information retrieval research community is increasingly making use of the in-formation richness in knowledge sources for improving the effectiveness of Web search applications. Wikipedia has been intensively used recently to provide back-ground knowledge for natural language processing while the patent data hasn X  X  been data as background knowledge depository on the task of keyword extraction. Further-content of the patent files but also mining the hidden knowledge by using the relation-ship between patent inventors, assignees and citations for semantic relatedness. Patent data is the main carrier of science and technology progress and innovation, which provides up-to-date and extensive coverage for a broad range of domains. Every Patent file is a strictly verified document, which has considerable authority. Patent file provides information of inventors, assignees, title, abstract, patent classifi-cation, description and citations etc., as showed in Fig 1. The information provided in patent file is good for patent understanding and searching. Also can be a good support for related domain X  X  background knowledge. 
In this section we discuss the characters of patent data, and discover the relation-ship between different patent files to build a background knowledge base for the input document. Novel word features derived from patent data that reflect the document X  X  background knowledge are described in section 3.4. Section 3.5 introduce how the keyword extraction problem can be regarded as a classification problem and how the Support Vector Machine (SVM) is used to extract the keywords. 3.1 Process Our patent based document background knowledge acquisition approach consists of following steps to acquired the novel words features and extract the keywords for the input document: trieving the document X  X  background knowledge through searching the patent data. between patent inventors, assignees and citations, and then extend the background knowledge. 3) Feature extraction. The input is a bag of words in a document. We make use of both local context information and global contex t information from background knowledge of a word in the document to define its features (see section 3.4 for a detailed defini-tion of the features). The output is the feature vectors, and each vector corresponds to a word. 4) Learning and Extraction. We construct a SVM model that can identify the keyword. In the SVM model, we view a word as an example, the words labeled with  X  X eyword X  as positive examples, and the other words as negative examples. Each word has the features value defined in step 3). We use the labeled data to train the SVM model in advance. Note that we consider only single words as candidates for training. In extrac-tion, the input is a document. We employ the preprocessing and the feature extraction on it, and obtain a set of feature vectors. We then predict whether or not a word is a keyword by using the SVM model from above. The multi-word keywords will then be eventually reconstructed. Finally, we get the extracted keywords for that document. 
The key issue here is how to obtain the background knowledge and how to define features for effectively performing the extraction task. 3.2 Pre-processing: Generating a Patent Search Query To generate a query to find the most relevant background knowledge of a document via searching the patent data, we construct the query based on the key facts carried in the input document. This is done through applying a modified version of the TextRank [3] to select the important content words from the input document. In the original Tex-tRank algorithm, pairwise sentence similarity is based on the word matching degree of two sentences. In the modification method, the way the pairwise sentence similarity is calculated using semantics of words than through mere counting of the number of overlapping characters in the spellings of two words. In our pre-processing, Qtag [12] is used to identify the part of speech of the words. Then we measure sentence similari-ty through word semantic relativeness analysis [13] using WordNet [14]. Finally we get a few key sentences selected from the input document through the above process. We then perform stop word removal and word stemming over all the words in these key sentences. The remaining words constitute our patent search query. 3.3 Knowledge Obtains: Search ing the Patent Data To obtain the background knowledge for the input document, we call on the full text search engine, Lucene [15], to retrieve files from the patent data that are related to the input document X  X  key contents. We defined two levels background knowledge for the input document. 
Using the search query for the input document generated in section 3.2, we get the search results, which are returned as a ranked list of patent files and their corresponding trieved patent files (duplicated files are discard) as It is agreed the title and abstract elaborately reflect the content of a document in brief. level background knowledge base BK( p r ). We remove the stop words and duplicated words inside, and then perform word stemming. Finally, the remaining words constitute 
In each file p r , the information of patent X  X  inventors, patent assignee and patent ci-tations is presented. It is obviously that the same patent X  X  inventor concentrate on the same/related research field. While the patent assignees also, in generally, concentrate on the same business area. The patent citation provides the most related patents, which is a great help for understanding the current patent file. Both these information provide additional related information to help the readers better understand the top-ic(s) discussed in the current patent file, they can be the second level background knowledge for the input document. 
To make use of the inventor information in our patent search result set set IL (duplicated files is discard). Then similarly, the words from title and abstract of each patent file in set IL are extracted to construct the inventor knowledge base IK( p supplies another kind of additional background knowledge from assignees for p CK( p r ) supplies additional background knowledge about related works for p
In summary, via searching the patent data, we find the most relevant patent file set  X  background knowledge of the input document. We then get the related information hidden knowledge and relationship between different patents files is used to build the second level background knowledge. The second level background knowledge is knowledge about the input document, the second level background knowledge is also helpful for understanding the input document. 3.4 Feature Extraction: Novel Word Features tion by making use of background knowledge from patent data. In the same time, tradi-tional word features based on lexical and syntactic features are also used as supplement. 1) Word Background Knowledge Feature For every word in an input document, we derive a word background knowledge feature using the first level background knowledge base as follows: 
In the above, is the query relativeness score of the patent file p r )| is the number of words in BK( p r ). By the above definition, the more seman-tically similar a word is to words in BK( p r ), the larger would be  X  X  back-ground feature value, . 2) Word Inventor Background Feature For every word in an input document, we derive a word inventor background feature 
In the above, is the query relativeness score of the patent file p the number of patent file in set IK( p r ). By the above definition, the more semantically similar a word is to words in IK( p r ), the larger would be  X  X  inventor back-ground feature value, . 3) Word Assignee Background Feature and Word Citation Background Feature Similarly, for every word in an input document, we can derive the word assignee background feature and word citation background feature using the assignee informa-compute the word inventor background feature in the above. 4) Common Word Features. The comment word features directly derivable from the input document including word frequency feature, specific name feature, word position features, relative word length feature, and conclusion sentence feature are also used as supplement. 3.5 Learning and Extraction Once all the word features introduced in the above are derived, we can then apply machine learning based approach to extract the keywords. We formalize keyword extraction as a classification problem. We take  X  X eyword X  and  X  X ot keyword X  as classes, words manually labeled with the two classes in the  X  X raining data X  as training examples, words in the  X  X esting data X  as test examples, so that keyword extraction can be automatically accomplished by predicting the class of each test example. 
We perform keyword extraction in two passes of processing: learning and keyword extraction. In learning, we construct the cl assification model that can predict the class of each word. In the classification model, we view each word as an example. For each example, we define a set of background knowledge features and the document global represents whether the word is a keyword or not as showed in table 1. Since SVM is a good way to using features without discussions, and our method is totally based on LIBSVM [16] as the classification model to obtained the classifier with small genera-lization errors. The labeled data is used to train the classification models in advance. One attempts to find an optimal separating hyper-plane that maximally separates the two classes of training examples (more precisely, maximizes the margin between the two classes of examples). The hyper-plane corresponds to a classifier. 
In keyword extraction, the input is a document. We extract a bag of words from that document. Then, for each word in that document, we employ the learned classifi-cation models to predict whether it is  X  X eyword X  or  X  X ot keyword X . We next view the words that are predicted as  X  X eyword X  as keywords. All lexical units selected as po-tential keywords in the above are marked in the document, and sequences of adjacent keywords are collapsed into a multi-word keyword. For instance, in the text  X  X t is essential for the success of grid computing X , if both  X  X rid X  and  X  X omput X  are selected keyword  X  X rid comput X . Note that we perform word stemming over all the words for simply. 4.1 Data Sets and Evaluation Measures Patent Data as Background Knowledge Depository. We download the US Patent [1] data from 2003 to 2013 as the background knowledge depository.

To evaluate the performance of our approach for keyword extraction, we first con-struct our own keyword extraction ground truth data set through collecting 300 patent files from US Patent dataset in 2013~2014 as the Patent Dataset. We also use the SemEval [17] Dataset for experiment. Patent Dataset. We choose 300 patent files (with the same classification number) published in 2013~2014 as the patent dataset. It is well agreed that the title has a simi-lar role to the key phrases. They are both elaborated to reflect the content of a docu-ment. Therefore, words in the titles are often appropriate to be keywords. However, the understanding of the document. On the other hand, some words maybe put in title to attract the readers but th ey are not exactly reflecting the content of the document. Therefore, we perform stop word removal and word stemming over all the words in title. The remaining words constitute one set of keyword answer. We then asked 10 master students in our computer science department to extract keywords manually from these files. Each patent file was analyzed by four students. Students were asked to extract 5 to 10 keywords from every patent file assigned to them. Thus we get five appears at least three times in the five sets of answer, we agree that it is the keyword of the document. After carrying out this manual keyword extraction process, we constructed a data set consisting of 300 files and their corresponding keywords. The average number of the key phrases is 7.2 per document. During the evaluation, the the rest 100 ones served for the final evaluation.
 SemEval [16] Dataset. The SemEval dataset is a standard benchmark in the keyword extraction field. It is comprised of 244 scientific articles, usually composed of 6 to 8 pages. The articles cover 4 research areas of the ACM classification related to Com-puter Science, including the Computer-Communication Networks (C), Information Storage and Retrieval (H), Artificial Intelligence (I) and Computer Applications (J). Human annotators who assigned a set of keywords to each document carried out the annotation of the gold standard. Besides the annotators X  keywords, the gold standard also considers keywords originally assigned by the authors of the papers. On average, the annotators provided 75% of the keywords and the authors provide the 25%. Dur-ing the evaluation, the corpus was divided into two parts corresponding to the training and testing stages: 144 articles (2265 keywords) were dedicated to training and the other 100 articles (1443 keywords) served for the final evaluation. Evaluation Measures for Keyword Extraction. In all the experiments, we employ widely used Precision, Recall and F-Measure to evaluate the performance of different approach for keyword extraction. The F-Measure is a standard method that is used to analyze the performance of the keyword extraction methods. The higher the F-Measure, the better will be the performance of extraction. These standard measures to address the performance are given by: 4.2 Experimental Results We evaluated the performances of our keyword extraction methods on two datasets. This section conducts two sets of experiment: 1) to evaluate the performance of our algorithm on patent dataset; 2) to compare different query generation methods on our algorithm X  X  overall keyword extraction performance; 3) to compare with other well known state-of-art keyword extraction approaches on SemEval dataset. Keyword Extraction on Patent Dataset. It is the first time that patent data being used as background knowledge serving for keyword extraction. To evaluate the per-formance of our approach, we compare the keywords of a document identified by our section 4.1. We also implement several existing keyword extraction methods includ-ing TFxIDF [18], TextRank [3] and [10](we name it as AAAI10) to compare the per-formance of these peer methods and our algorithm. Different parameter setting can influence the performance of the classifier. We change the value of parameter c and gamma for LIBSVM. The result is shown in Table 2, where P stands for precision, R stands for recall and F stands for F-Measure. We choose C=512 and gamma=8 as the best result. Different systems X  result shown in Table 3 confirms the advantage of our method for keyword extraction. 
We also consider the influence of differ ent query generation methods to the overall performance of our method for keyword extraction. To this aim, we implemented two different query generation methods and experimentally compared the overall keyword extraction performance when employing each query generation method in the first can see that the modified TextRank method, currently employed in our algorithm, optimizes the performance of our keyword extraction approach. Experiments on SemEval Dataset. Paper [11] evaluates five semantic annotators and compares them to two state-of-the-art keyword extractors, namely KP-miner [19] and Maui [20]. Experiments show that the top three systems on SemEval dataset are Alchemy Keyword(Alch_key) [21], KP-Miner and Maui. The experiment for top 15 keywords extraction is the best. The top 15 keywords were obtained based on the confidence scores returned by the systems. We compare our method with the top three systems X  best results in paper [11]. For the SemEval dataset, we used both authors and readers selected keywords as a gold standard. Table 5 indicates the results for key-words extracted by the systems on testing data provided by SemEval. 
In the experiments on SemEval data, all the systems are only able to achieve less than 30% F1-score for the keyword extraction task, which is much lower than the experiments over patent data. It is because we concentrate on the same class patent domain. While the SemEval data provide four classes of scientific articles, each do-main has only about 20~30 articles. For scientific articles, choices of words vary be-tween different domains. Especially, keywor ds are usually made up of specialized words, most of which are unique to the domain. Therefore, it influences the perfor-mance of keyword extraction. In this paper, we address the problem of automatic keyword extraction using novel word features. We investigate the patent data to serve for the problem of keyword extraction. We propose some novel word features for keyword extraction. 
These new word features are derived through background knowledge of the input document. The background knowledge is acquired via first querying patent data, and then mining the hidden knowledge by exploring the inventor, assignees, and citation from this background knowledge. Experimental results have proved that using these novel word features, we can achieve superior performance in keyword extraction to other state-of-the-art approaches. 
In future work we would like to investigate the following issues: 1) to make further improvement on the efficiency, 2) to investigate the effect of including flat text from our approach to document classification and other text mining applications. 
