 Nowadays, World Wide Web is the best environment for producing information, pub-percent of the web pages of today will disappear and 50 percent of all contents will be changed. The web graph (link structure) will change even faster: about 80 percent of all links (web graph) will have changed or will be new within a year [12]. The results this vast dynamic environment. gram for the bulk downloading of the web pages. The aim of the crawling process is to retrieve whole of the web content. Over the time frame of crawler development, the web Most crawlers will not be able to visit every possible page for three main reasons:  X 
The network bandwidth is expensive [2].  X  most crawlers will not be able to cope with all data [5].  X 
Crawling takes time, so at some point the crawler should revisit previously scanned pages to prevent index inconsistency [5]. tance of pages as a crawling priority. In this paper, we propose a new method based on FICA (Fast Intelligent Crawling Algorithm) [19], called FICA+, which has higher tween web pages. 
The remainder of this paper is structured as follows: The next section reviews the background and related work. In Section 3, we introduce our algorithm, FICA+. Ex-perimental analysis and comparison to some of the well-known algorithms are given in Section 4, and finally our conclusion and future work of research are presented in section 5. algorithms are the subject of extensive research. These studies can be categorized into the page selection category. late the page importance such as HITS [11] and PageRank [15]. In the following some of the well-known algo rithms are considered. 
PageRank is a popular ranking algorithm used by Google which measures the im-document from which it originates and the number of outlinks in the origin document. When she (PageRank) reaches to a web page that does not have any outward link, she link from the current page or jumps to a random page on the web graph. The rank of page j is then computed by following equation: from page i, and B(j) shows the set of pages that point to page j. Parameter d, damp-ing factor, is used to guarantee the convergence of PageRank and remove the effects of sink pages-pages with no outputs. variations of PageRank such as Personalized PageRank [10] and TrustRank [9] which assumes that users X  random jumps are limited to a set of specified pages. 
In [14], breadth-first algorithm is used as a crawling algorithm. They examined the downloaded pages. In [6], a comparison between some crawling algorithms including PageRank, Backlink count and breadth-first has been done. It was found that the crawling based on PageRank finds the hot (important) pages earlier than others. importance of pages online in the crawling process. In their method, each page has a value called cash. Initially all pages have the same cash equal to 1/n (n is the number of web pages). The crawler will download web pages with the higher cash and when a time. Unfortunately, the experiments were done on a synthetic web graph including at most 600,000 nodes with the power law distribution. There is no comparison between OPIC and other crawling strategies. 
A site-based method named largest site first has been proposed [4]. In this method the sites with the larger number of pending pages have higher priority for crawling. It is found that this algorithm is better than the breadth-first method. (re)downloading into a search engine repository [16]. The objective of the algorithm using users X  experiences. 
Dasgupta, Ghosh and Kumar et al. [8] proposed a new crawling algorithm in order to discover newly-arrived content on the web. They measured the overhead of disco-new page. They showed that with perfect foreknowledge of where to explore for links percent overhead and 100 percent of new content with 9 percent overhead. 
ZarehBidoki and Yazdani proposed an intelligent crawling algorithm based on explain it in more details in the following subsection. 2.1 FICA she (FICA) does not have any background (knowledge) about the web pages and selects them only by current status and over time her knowledge gradually increases. Over time, she learns more and more, with accumulating knowledge from the web environment she learns which page is more important than others and improves her selections. logarithmic distance:  X  between i and j equals to log 10 O(i) where O(i) denotes i X  X  out-degree.  X 
Definition 2. Logarithmic distance: the distance between pages i and j is the weight distance between the root and page i with d i . p and v is log2+log2. Thus, whereas both t and v are the same number of links away from tance of each of its child nodes is computed as follows: of log(O(i)) and d i are not comparable and almost the effect of current link X  X  weight forcement learning algorithm [17]. (4) shows the main formula of FICA which is based on reinforcement learning [17]. d and is the new distance of page j at time t+1. The aim of FICA is to decrease the sum of the punishments (distances) received from the web environment. she visits more pages, she slowly learns from environment (  X  decreases). 
Eq. (4) used when we reach page j for the first time. In situations that a page has different parents, the parent which produces the least distance has been chosen. Thus Eq. (4) changes to Eq. (6) which is based on Q-learning [19]. well-defined method for crawling, but it has two major weaknesses. FICA weaknesses. FICA has two main problems:  X 
Problem 1. FICA is only dependent on out-degree of web pages. As Eq. (6) shows the main operand in FICA is O(i).  X  her decisions. page i (Fig. 2), the crawler then finds page j for the second time through page k. 
Suppose page k has the distance 0.4 and an out-degree of 10. Then the distance of will be chosen as the new distance value for page j [19]. In fact, FICA chooses a par-tried to cover all of the weaknesses of the old version. important role in developing our new algorithm. The breadth-first crawling algorithm traverses the graph by following its links. The distance of each crawled page from the the average page quality during a web crawl of 328 million unique pages. They used the PageRank metric as benchmark. Fig. 3 shows the average PageRank (unnorma-lized) of all downloaded pages on each day of the crawl. three times the average score of 2.07 for crawled pages on the second day. The aver-week, and to 0.59 after the fourth week. 
According to Figure 3, we can say that the breadth-first method downloads the hot pages in the first day of the crawling process. It happens due to the large number of gradually. We use this feature of breadth-first in our algorithm. 3.1 FICA+ As was mentioned, in the initial stages of the crawling process the crawler agent has little knowledge about web environment, and it is possible it makes wrong choices in her decision. Furthermore, it is based on outward links. To solve these problems, we propose a new crawling algorithm, called FICA+ which is based on FICA, Backlink distance formula. In other words: process that crawler agent does not have any background about web structure, 1 =  X  , zero-because in the final stages she has almost compelete knowledge about web envi-following Crawling algorithms:  X 
Breadth-first: The crawling process is done in the breadth-first order. Initially, the algorithm starts with some starting URLs as the roots of the crawling tree.  X  [13], that is, pages with more input links have higher ranks.  X 
Partial PageRank: This method uses the PageRank algorithm [7] on the web pages seen so far and crawls the pages with higher PageRank first.  X 
OPIC: In this algorithm, all pages start with the same amount of cash [14]. Every time a page is crawled, its cash is distributed to its outward links. In each step the next page for crawling is the one with the highest amount of cash up to now.  X 
FICA: It is an intelligent crawling algorithm based on reinforcement learning that concept, called logarithmic distance, web pages with low logarithmic distance are more important than others. Initially, we start crawling the web with every algorithm with some starting URLs (5000 URLs as seed URLs). Every time by crawling k new web pages (k is set to 125,000 web pages), we run one of the above ranking algorithms. Afterward, we sort the web pages in algorithms and they do not require an additional ranking stage. 
The aim of the crawling is to find hot pages. To do this, we choose the PageRank al-each step as a fraction of crawled hot pages to all hot pages that can be discovered. 
We compared the aforementioned algorithms with FICA+ in Figure 4 on the web other algorithms in the tested web graph. For example, in Figure 4, when 25 percent of pages are crawled, FICA+ finds about 54 percent of hot pages whereas partial Pa-percent increase in the average throughput. In this paper we proposed a new crawling algorithm called  X  X ICA+ X . This algorithm selects each page based on its background knowledge from visited pages and the fea-ture of web environment. In fact, it makes a relationship between the obtained know-ledge by the crawler agent and the web structure feature. For evaluation of FICA+ we used the web graph of Berkeley university. main contribution of the paper is an efficient crawling algorithm that finds hot pages web graph and only a vector of web graph nodes for saving the distances of pages is enough. 
There are two directions in which we would like to extend this work. One direction is to execute FICA+ as a crawling algorithm on a dynamic web graph and the second direction is to evaluate FICA+ as a ranking algorithm. Acknowledgments. This work has been partially supported by Iran Telecommunica-tion Research Center (ITRC). 
