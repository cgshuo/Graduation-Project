 1. Introduction recognition, and data mining. The problem is also known as cluster Mimaroglu and Erdil (2011b) , Domeniconi and Al-Razgan (2009) ,and Gullo et al. (2011) .

A set of clusterings can be obtained by using multiple cluster-ing methods, or by providing multiple input parameter values to a clustering method. In some cases, human experts can also produce clusterings. Therefore, we can have multiple clusterings of an input data set, and utilize this valuable information to obtain a fi nal clustering.

Some applications of combining multiple clusterings in bioin-formatics, image processing, and in other disciplines can be found (2009) , and Mimaroglu and Erdil (2010) . Combining multiple clustering applications are more challenging than applications of and Pajares, 2009 ), since there are no prede fi ned labels.
Grouping objects into the correct clusters is a hard task as ers from many research areas like machine learning, data mining, computational biology, and bioinformatics. Among pioneering methods are k -means ( MacQueen and et al., 1967 ), DBSCAN ( Ester et al., 1996 ), AGNES ( Kaufman et al., 1990 ), and DIANA techniques make some assumptions about the underlying data set or affect the quality of clustering with user speci fi parameters; therefore possess some advantages and disadvan-same technique with varying input parameters on a data set, and quality which is novel, robust, and scalable.

The contribution of this paper is a new family of algorithms for combining multiple clusterings. Our main motivation is to develop new methods that solve the weaknesses of the related work discussed in Section 3.8 . Exhaustive experimental results on a and consume very low memory compared to the state-of-the-art.
This paper includes a novel family of algorithms for combining multiple clusterings with discussions and extensive experimental evaluations on real, synthetically generated, and gene expression data sets. The paper is structured as follows: in Section 2 ,we de fi ne the problem of combining multiple clusterings formally. Section 3 presents some well-known and recent related work.
Section 4 contains our contribution, which is a novel family of combining clusterings methods. Section 5 discusses the properties of our algorithms. An objective cluster quality measure, test data sets, and test results are presented in Section 6 . Final section contains our concluding remarks. 2. Combining multiple clusterings
In this section we formally introduce the combining multiple clusterings problem which takes a collection of clusterings and provides a fi nal clustering with better overall quality. stated as follows:  X  D  X  X f C 1 ; C 2 ; ... ; C j  X   X  D  X j g ; where C i is a cluster (block) of  X   X  D  X  , C i \ C j  X   X  i a j and
D  X   X 
Note that we may have a partial clustering (i.e. not complete)
C \ C j a  X  , for some i and j .

For a given data set D , let  X   X  D  X  X f  X  1  X  D  X  ;  X  2  X  D  X  be a collection of clusterings where  X  i  X  D  X  X f C i 1 ; C 1 r i r j  X   X  D  X j , and the collection of clusters C  X  D  X  X f C 8  X  D  X  A  X   X  D  X g . Combining multiple clusterings problem is de as fi nding a new fi nal clustering  X   X   X  D  X  X f C  X  1 ; C  X  2 ; ... ; using the information obtained from  X   X  D  X  such that  X   X   X   X  D  X  X  Z  X   X   X  i  X  D  X  X  ; 8 i ; 1 r i r j  X   X  D  X j X  1  X  where function  X  is a clustering quality measure.
 practical, since there are  X  1 = k !  X   X  k l  X  1  X  k l  X  X  1  X  objects ( Strehl and Ghosh, 2003 ).

Binary representation of a collection of clusterings can be seen istic bit vector having j D j bits.

The collection of clusterings in Fig. 1 represents a data set with eight objects and three different clusterings. 1 For example, C vector as shown below. 3. Related work
In this section some of the state-of-the-art methods for combining multiple clusterings are presented. A detailed survey of cluster ensemble methods can be found in Vega Pons (2011) . 3.1. Combining multiple clusterings using similarity graph (COMUSA)
COMUSA is a graph-based algorithm introduced in Mimaroglu and Erdil (2011b) , which uses the evidence accumulated in the collection of input clusterings and produces a very good quality fi nal clustering. A similarity graph is constructed by computing the co-associations of objects in the input. COMUSA initiates a new neighbors if they are most similar to the pivot. COMUSA correctly fi nds the number of clusters in the fi nal clustering automatically, which is a big advantage. COMUSA can be applied in many domains; Mimaroglu and Erdil (2011a) is a recent application of
COMUSA for automatically detecting arbitrary shape objects. 3.2. Link-based cluster ensemble (LCE) Link-based cluster ensemble (LCE), which is presented in
Iam-on et al. (2010) , starts with a bipartite membership graph of objects and clusters and builds up a dense graph with implied fi nal clustering on this structure by a spectral graph partitioning technique. LCE, which is designed to work on gene expression data sets, produces good results on both biological and non-biological data sets. 3.3. Cluster-based similarity partitioning algorithm (CSPA)
CSPA, which was introduced in Strehl and Ghosh (2003) ,is based on a co-association matrix, and METIS ( Karypis and Kumar, 1998 , 1999 ). Initially, CSPA constructs a co-association matrix, which can also be represented by a graph, using the input multiple clusterings. Then, METIS algorithm partitions the graph. Finally,
CSPA forms each partition as a cluster. 3.4. Hyper-graph partitioning algorithm (HGPA)
HGPA was introduced in Strehl and Ghosh (2003) as well: multiple clusterings are used to construct a hyper-graph where idea is to obtain k unconnected components of the hyper-graph by using HMETIS ( Karypis et al., 1997 , 1999 ). Combining multiple clusterings problem is formulated as partitioning the hyper-graph by cutting a minimal number of hyper-edges. A set of hyper-edges are removed and k unconnected components are obtained, which provides the fi nal clustering. 3.5. Meta-clustering algorithm (MCLA)
In meta-clustering algorithm (MCLA) ( Strehl and Ghosh, 2003 ) each cluster is represented by a hyper-edge, like HGPA.
MCLA is composed of the following steps: (1) constructing the meta-graph, (2) partitioning the meta-graph, and (3) computing cluster members. 3.6. Combining multiple clusterings using evidence accumulation (EAC)
Evidence accumulation (EAC) ( Fred and Jain, 2005 ) accumulates the evidence in each cluster to form a co-association matrix, SM . 3.7. Bipartite merger (BM) and metis merger (MM)
The input data may be distributed at different sites, in this case needed. Hore et al. (2009) propose two approaches (BM and MM) for combining clusters, represented by sets of cluster centers.
Using cluster centers (prototypes) instead of clusters reduce computation and memory requirements. BM works on several can have different number of clusters. Good results from both BM and MM were reported in Hore et al. (2009) . 3.8. Weaknesses of related work
As seen from the brief literature survey above, most of the existing methods in the literature work at object level. These methods generally suffer from slow execution times and high memory requirements; improving these shortcomings are the main motivations of this paper.

CSPA, HGPA, EAC, and COMUSA all work at object level and each method creates a dense graph of objects. CSPA and HGPA use METIS package which partitions a graph having roughly equal number of vertices.

Although CSPA and HGPA work very fast, their outcome may not be accurate when especially clusters differ in size. EAC and
COMUSA produce a fi nal clustering with a very high accuracy on a wide range of data sets. However, they easily become inef for large data sets in terms of both run times and memory. LCE not original data set, which may not be available in some cases. LCE computes a bipartite membership graph of objects and clusters with implied similarities, which needs a lot of computation. BM and MM are prototype based cluster ensemble algorithms which
For this reason, we will not consider these methods in our experimental evaluations. MCLA works at cluster level. Hence, it produces a fi nal clustering in a short amount of time and consumes low amount of memory. However, MCLA uses Jaccard measure, which only captures syntactical similarity between clusters which downgrades the accuracy of the method.
In addition to above mentioned methods, median partition and genetic methods are also used for combining multiple clusterings ( Cristofor and Simovici, 2002 ). Although median partition meth-partition is a very challenging problem ( Barth X lemy and Leclerc, 1995 ). These methods suffer from slow execution times, mainly because they work on the object level. Noisy input clusterings may considerably affect median partition based clustering ensem-ble methods which is another disadvantage. Genetic methods suffer from long execution times. In the domain of cluster ensemble, determining chromosome encoding, crossover, muta-tion, and the fi tness function are not immediate.

Our main motivation of proposing new algorithms that operate at cluster level is to solve the scalability issue of the existing methods. However, there is generally a trade-off between run time and fi nal clustering validity. Our new family of clustering algo-rithms produces a new fi nal clustering having high accuracy in a short amount of time. 4. A new family of algorithms for combining clusterings
In this section, we present a novel family of algorithms for combining multiple clusterings that work at cluster level.
Newconceptsemployedbyournewalgorithmsarede fi ned in Section 4.1 . An immediate improvement of COMUSA which Section 4.3 presents a scalable and ef fi cient algorithm that is designed to work on a diverse set of data sets with a broad range are improvements of COMUSA ( Mimaroglu and Erdil, 2011b ), both in terms of accuracy and execution time. 4.1. De fi nitions De fi nition 4.1. A similarity graph at cluster level SG  X  X  C ; undirected and weighted graph where C is the set of input clusters and E is the set of edges.

De fi nition 4.2. Edge weight between a pair of vertices in SG is de fi ned by the following function: weight  X  C ik ; C jl  X  X  ECS  X  C ik ; C jl  X  X  1 j C where sim  X  d m ; d n  X  is the number of times objects d m and C ik A C , C jl A C .

Fast computation of Formula (2) in linear time with respect to
De fi nition 4.3. The degree of freedom of a vertex C ik is the number of vertices that are connected (adjacent) to C ik and is de follows: df  X  C  X  X jf C jl j weight  X  C ik ; C jl  X  a 0 ; ik a jl gj
De fi nition 4.4. The sum of weights of a vertex C ik is the sum of all sw  X  C
In a cluster level similarity graph, each cluster (vertex) is represented by a circle and each dot in a circle corresponds to vertex C ik are represented by the pair  X  df  X  C ik  X  ; sw  X  C
Fig. 2 . High values of df  X  C ik  X  mean that the cluster C many other vertices. Similarly, large values of sw  X  C ik similarity with several other vertices.

De fi nition 4.5. The attachment of a vertex is de fi ned as attachment  X  C ik  X  X  sw  X  C ik  X  df  X  C attachment  X  C ik  X  values which indicate that the vertex C strongly connected to somewhere. Moreover, a vertex having the highest attachment generally corresponds to a point at the center of a region in a data set which is a good starting point (seed) to grow a cluster ( Mimaroglu and Erdil, 2011a , 2011b ). In our algorithms, we use the attachment information to initiate new fi nal clustering as demonstrated in the experiments.

As a convention, in SG we discard self loops (self-similarity of a node) and 0 weight edges. Note that there may be isolated nodes having 0 degree of freedom and 0 sum of weights. By convention, attachment value of such vertices are considered as 0.
Edge weights are not shown for simplicity; they can be seen from the matrix SM given in Fig. 3 a. 4.2. COMUSACL: COMUSA at cluster level
COMUSACL, which is shown in Algorithm 1 ,isbasedonCOMUSA ( Mimaroglu and Erdil, 2011b ), but unlike COMUSA it works on a similarity graph at cluster level by De fi nition 4.1 .Bythismajor number of edges are reduced substantially from  X j D j 2 j  X j
C graph). Since both COMUSA and COMUSACL run over the edges in the similarity graph, a signi fi cant improvement in execution time is obtained by COMUSACL. COMUSACL produces very good quality clearly demonstrate that COMUSACL is an important improvement over COMUSA both in execution time and quality of its output. Algorithm 1. COMUSACL: COMUSA at cluster level.
 ratio Output :  X   X   X  D  X  : Final Clustering
Initialize an empty queue Q ; // meta cluster id mcId  X  1; Construct similarity graph SG  X  X  C ; E  X  ;
Sort C in decreasing order with respect to attachment ; while there are unmarked vertices do
Add unmarked vertex, C ik , with the highest attachment  X  C to Q ;
Create an empty set MetaCluster mcId ; while Qis not empty do relaxation // Majority voting: form final clusters from meta-for each obj A D do return  X   X   X  D  X  ;
COMUSACL takes a collection of clusterings as the input and all the vertices are computed and sorted in decreasing order. a meta-cluster, 3 it is marked.

COMUSACL picks an unmarked vertex with the highest attach-ment value as the pivot, assigns it into the mcId th meta-cluster, and marks the vertex as shown in lines 6  X  11. The meta-cluster is expanded by the immediate neighbors of the pivot as follows: a vertex w is added to the pivot's meta-cluster if the edge weight between the pivot and w is maximum (lines 18  X  21) of all the edge will be an acting pivot and its immediate neighbors are considered for further expansion (line 23). Expansion of the meta cluster continues until no further expansion is possible. COMUSACL continues this procedure by fi nding a new pivot vertex having the highest attachment value among the vertices that have not been assigned into any meta-cluster. 4.2.1. Toy problem demonstration
Performing COMUSACL on the similarity graph shown in Fig. 3 b dashed pattern in Fig. 5 a, the second meta-cluster is shown in
Fig. 5 b with thick straight line, the third meta-cluster is shown with an irregular line in Fig. 5 d.
 fi nal clustering as shown in Fig. 4 . Final clusters are C  X  1
C  X  2  X f d 8 g , C  X  3  X f d 5 ; d 7 g ,and C  X  4  X f d 1 ; d 4.3. COMUSACL-DEW: COMUSACL-with dynamic edge weights
In this section we introduce a novel algorithm for combining a characteristics. Our new meth od, which we call COMUSACL-DEW (COMUSACL-with dynamic edge weights), is introduced in Algorithm 2 . Like COMUSACL, it works on a similarity graph constructed at average which makes our algorithm adjustable to diverse group of new edge weight values using min , max ,and average schemes are giveninFormula (3) : where v and w are two vertices merged into a new vertex v and k A C ; k a v ; k a w .

The setting of new edge weight values using min , max , and average schemes is shown on a toy example in Fig. 6 .
Our main motivation of proposing COMUSACL-DEW is to incorporate a fl exibility to COMUSACL for the data sets having various characteristics. While updating edge weights, COMUSACL-DEW uses min , max , and average schemes as we have mentioned above. This step is very similar to complete link, single link and clusters in agglomerative hierarchical clustering. Each different scheme used in hierarchical clustering algorithm enables the algorithm to work on the data sets having various characteristics clustering problem and achieved similar fl exibility.
COMUSACL-DEW, which is shown in Algorithm 2 , has two major differences when compared with COMUSACL. The fi rst difference is its dynamically updated edge weights. This feature of COMUSACL-DEW not only enables the algorithm to work on variety of data sets, but also speeds it up. COMUSACLDEW computes whether to expand a cluster by checking its immediate neighbors only. Therefore, COMUSACL-DEW does not have any redundant operations which improves its execution time. The second difference is shown in lines 25 and 26: when a cluster is considered as a new clustering problem so, results are not changed dynamically and they need to be calculated as shown in line 4. New pivot and cluster vertices are chosen from the rest of the similarity graph.
 Algorithm 2. COMUSACL-DEW: COMUSACL-with dynamic edge weights.

Input :  X   X  D  X  : Multiple Clusterings of a Data Set D , weight  X  v merged ; k  X  X 
Output :  X   X   X  D  X  : Final Clustering // meta cluster id mcId  X  1;
Construct similarity graph SG  X  X  C ; E  X  ; while there are unmarked vertices do
Compute attachment values for all unmarked vertices; v  X  An unmarked vertex with highest attachment in C ; Create an empty set MetaCluster mcId ;
Add v to meta-cluster MetaCluster mcId ; for each  X  v ; w  X  A E do relaxed with relaxation ratio inupdateSG mark v ; mcId  X  X  ; // remove all the edges incident to v  X  merged  X  in SG for each  X  w ; v  X  A E do //
Majority voting: Form final clusters from meta-clusters for each obj A D do ; maxNormOccr  X  1; clsId  X  0; for 1 r i r mcId do clusters in MetaCluster i
Assign obj into C  X  clsId in  X   X   X  D  X  ; return  X   X   X  D  X  ; Procedure 1. Procedure updateSG(SG  X  ( C , E), choice, v, w).
Create an isolated vertex, merged ; // Clusters in v and w are added into the merged Merge v and w into vertex merged ;
Add merged into C ; for each k A C f v ; w ; merged g do if choice  X  min then if choice  X  max do if choice  X  avg do add  X  merged ; k  X  into E; remove ( v , k ) and ( w , k ) from E; remove v and w from C ; 4.3.1. Toy problem demonstration
COMUSACL-DEW with max updating scheme is demonstrated meta-cluster and includes C 21 in the meta-cluster as shown in
Fig. 7 c. Further expansion of the meta-cluster is achieved by any more and takes its fi nal form as shown in Fig. 7 e. C cluster. Final form of the meta-clusters is presented in Fig. 7 f.
Following this step, majority voting shall be performed for computing memberships of the objects. 5. Discussion
COMUSACL does not make any assumptions about the input; by noise and outliers. COMUSACL-DEW offers three dynamically evolving edge weight adjusting schemes: min , max , and average .
COMUSACL-DEW with max is also affected by noise and outliers, but COMUSACL-DEW with min and average are resistant to noise and outliers. Like COMUSACL, COMUSACL-DEW with max weight adjusting scheme works well on arbitrary shape data sets as well.
However, COMUSACL-DEW with min weight adjusting scheme works well on globular shape data sets. This behavior is expected since it selects an edge with the minimum edge value from a set of edges incident to the cluster. Finally, COMUSACL-DEW with average weight adjusting scheme works well on globular and arbitrary shape data sets.

To point out the major differences between our proposed algorithms we demonstrate them on a simple globular data set which contains noise in between two natural clusters as shown in Fig. 8 a. Three input clusterings provided to COMUSACL and COMUSACL-DEW are shown in Fig. 8 b  X  d. COMUSACL and
COMUSACL-DEW with max detects only one cluster on this data set as shown in Fig. 9 a, therefore they cannot identify the true nature of the input data set. COMUSACL-DEW with min produces three clusters by correctly identifying two main clusters and separating the noise in its own cluster as shown in Fig. 9 b.
COMUSACL-DEW with average outputs two clusters by including the noise into the closer right cluster as presented in Fig. 9 c.
Additionally, we run our algorithms on a data set having arbitrary shape as shown in Fig. 10 a. Two input clusterings provided to COMUSACL and COMUSACL-DEW are shown in
Fig. 10 b  X  c. COMUSACL and COMUSACL-DEW with max correctly
COMUSACL-DEW with min and COMUSACL-DEW with average separate the data set into two clusters as shown in Fig. 11 b.
Results of COMUSACL and COMUSACL-DEW on Figs. 8 a and 11 are expected, and fi ts well with the working principles of these algorithms. 5.1. Relaxation ratio
We can relax the maximum constraint with a user speci fi ed
Algorithm 2 . Experimental results demonstrate that adjusting the clusters having larger size are obtained. 6. Experimental evaluation
In this section we present the objective cluster quality measure for evaluating the quality of COMUSACL and COMUSACL-DEW, experimental data sets, and test results. 6.1. Measuring quality of a clustering measures such as Adjusted Rand Index (ARI) ( Rand, 1971 ) and normalized mutual information (NMI) ( Strehl and Ghosh, 2003 ). These cluster validity measures are explained below. 6.1.1. Adjusted Rand Index (ARI)
We use Adjusted Rand Index (ARI) in order to measure the extent to which the clustering structure discovered by our algo-set D  X f d 1 ; ... ; d n g , suppose U  X f u 1 ; ... ; u r V  X f v 1 ; ... ; v p g represents a clustering of D  X  and u i \ u j  X   X  for 1 r i ; j r r and i a j . Also, v i 1 r i ; j r p and i a j .

In Table 1 , n ij  X j u i \ v j j ; n i :  X   X  p j  X  1 n ij
ARI can be formulated as follows:  X   X   X  2  X  X 
ARI takes maximum value at 1, which indicates perfect match to the external criteria. 6.1.2. Normalized mutual information (NMI)
Normalized mutual information can be used as a clustering validity measure when the original class labels of the objects are known. For a fi nal clustering  X   X   X  D  X  and original class labels NMI between  X   X   X  D  X  and  X  o  X  D  X  are de fi ned as follows:
NMI  X  where I  X  ;  X  represents the mutual information between fi of a clustering. Similar to ARI, NMI takes maximum value at 1. In our comparisons, we used ARI since these methods produce similar results ( Mimaroglu and Yagci, 2012 ). 6.2. Generating cluster ensembles
In our experiments we use three different approaches for generating cluster ensembles: manually constructing clusters, randomly constructing clusters or randomly injecting error into the original clusters (clusters whose labels are already known), and using k -means algorithm with varying k -values. Note that the diversity and quality of a cluster ensemble impacts the quality of the fi nal clustering. We choose different approaches for generating cluster ensembles because we expect them to produce a diverse experimental data sets, Tables 2 and 3 provide the number of ensemble generation methods, and information about ARIs of ings, each clustering with 2  X  5 clusters, and each clustering is generated by k -means, manually or at random with input cluster-ings having min, max, and average ARI values of 0.077, 0.525, available at http://cs.umb.edu/~smimarog/comusacl . 6.3. Test results
We have conducted experiments on a computer having 2.8 Ghz processor and 4 GB of main memory, running on Linux kernel 2.6. Our choice of implementation language for COMUSA, COMUSACL,
COMUSACL-DEW, MCLA, and EAC is Java. We obtained LCE from its corresponding authors in MATLAB. 1-spiral ( Fig. 12 a) data set is synthetically generated and it data sets are also synthetically generated and contain 200 and 118 objects, respectively. Although these are small data sets having challenging for many clustering algorithms and combining multi-ple clusterings methods. On the data set shown in Fig. 12 a, we created two partial clusterings. These clusters are shown in objects form another clustering. We created multiple clusterings from 2-half rings data set by performing k -means algorithm for different k values and obtained clusterings shown in Fig. 15 . 2D2K and 8D5K data sets are obtained from Strehl and Ghosh (2003) : 2D2K is synthetically generated and contains 500 points each of two 2-dimensional Gaussian clusters with means (0.227, 0.077) and (0.095, 0.323) and diagonal covariance matrices with 0.1 for all diagonal elements. 8D5K contains 1000 points from multivariate Gaussian distributions (200 points each) in 8-dimensional space. The clusters all have the same variance (0.1), but different means. Means were drawn from a uniform distribu-tion within the unit hypercube. Syn5K data set is an arti generated data set containing 5000 objects and 5 classes.
Iris, Glass, Breast Cancer, and Image Segmentation are real data
Learning Repository ( Bache and Lichman 2013 ). Iris is a multi-variate, real data set having 4 dimensions, 150 objects and set having 10 dimensions, 214 objects, 6 classes. Breast Cancer objects and 2 classes. Image Segmentation is also a multivariate
Letter Recognition data set has 25 attributes and 20,000 objects. 4C40K ( Fig. 16 a) is a globular data set having 4 clusters and 40,000 objects. G150C2 ( Fig. 16 b) is another globular data set shown in Fig. 17 , which are very challenging data sets for clustering.

Properties of the gene expression data sets we used for testing are presented in Table 4 .
 Table 5 represents the quality of clusterings generated by
COMUSA, COMUSACL, COMUSACL-DEW, LCE, MCLA, and EAC methods on real data sets. COMUSACL and COMUSACL-DEW are better than COMUSA on 9 out of 10 real input data sets. COMUSACL and COMUSACL-DEW produce better quality outputs than LCE and EAC on 9 and 8 inputs, respectively. Note that EAC cannot run on very large data sets due to its very high memory requirements. Although LCE is designed for gene expression data sets, on all the gene expression data sets COMUSACL and COMUSACL-DEW pro-duce better results. MCLA produces outputs having lowest ARI values among all methods that we used in our experiments.
Similar good results are obtained on arti fi cially generated data sets as shown in Table 6 .
 In Tables 5 and 6 , we reported numerical quality results of COMUSA, COMUSACL, and COMUSACL-DEW with different relaxation relaxation ratio. Empirically, the relaxation ratio which produces expected number of clusters gives the best results.

Table 7 compares the number of clusters generated by our methods with the original number of clusters. COMUSACL and COMUSACL-DEW fi nd the correct number of clusters on all the data sets with respect to relaxation rate.
 Finally, Table 8 shows the execution time results of COMUSA, COMUSACL, COMUSACL-DEW, LCE, MCLA, and EAC. Note that we did not provide execution time results with different relaxation values since we evaluate all edges regardless of relaxation ratio. On some data sets COMUSACL is 61,379 times faster than COMUSA. Similarly COMUSACL-DEW is 6,081,294 times faster than COMUSA on some other data sets. COMUSACL runs slower than LCE, MCLA, and EAC in very large data sets. Execution time results of COMUSACL-DEW and LCE are very close to each other almost in all data sets. However, LCE runs faster than COMUSACL-DEW when the size of the data set increases. COMUSACL-DEW is faster than EAC in almost all data sets. MCLA has the best execution time results among all methods in the table. Note that on gene expression data sets all three methods output results in a short amount of time, since these data sets are small. Also note that COMUSACL-DEW works much faster than COMUSACL since
COMUSACL-DEW reduces the number of edges (and vertices) which considerably improves the execution time.
 7. Conclusions
In this paper we presented a family of combining multiple clusterings algorithms that are scalable and accurate. COMUSACL and COMUSACL-DEW, which are built on COMUSA ( Mimaroglu and Erdil, 2011b ), are designed to operate on a diverse range of input data sets. Both COMUSACL and COMUSA-DEW demonstrate considerable speed advantages and much lower memory require-ments than COMUSA by operating at cluster level. It is remarkable both COMUSACL and COMUSA-DEW provide better quality output clusterings than COMUSA on top of their speed and memory advantages. Extensive experimental evaluations on some very expression data sets-establish the usefulness of our proposed algorithms.
 References
