 Computer Science Division Dimensionality reduction is a common and often necessary st ep in most machine learning appli-and FDA as well as manifold learning algorithms.
 dence assumption or low-rank assumption. The models are gen erally fit with maximum likelihood, tion of draws from a mixture model in which each mixture compo nent is known as a topic [3]. The mixing proportions provides a reduced representation of th e document. This model has been used matics [8, 1].
 The dimensionality reduction methods that we have discusse d thus far are entirely unsupervised. Another branch of research, known as sufficient dimension reduction (SDR), aims at making use of predictive power of the side information.
 As a parametric generative model, parameters in LDA are typi cally estimated with maximum like-ing criterion X  X onditional likelihood X  X o train a variant o f the LDA model. Moreover, we augment procedure and provide additional freedom for tracking the s ide information. on applying DiscLDA to model text documents. Finally, in Sec tion 5 we present our conclusions. We start by reviewing the LDA model [3] for topic modeling. We then describe our extension to based on supervised information provided in the training da ta set. 2.1 LDA distribution over words. Let the vector w is first associated with a K -dimensional topic mixing vector  X  distribution,  X  drawn from the multinomial variable, z V -dimensional multinomial variable, w Given a set of documents, { w can be done by maximum likelihood,  X   X  = arg max parameter whose columns {  X  also possible to place a prior probability distribution on t he word probability vectors {  X  a Dirichlet prior,  X  via Bayesian methods. In both the maximum likelihood and Bay esian framework it is necessary to integrate over  X  inference or Gibbs sampling [3, 8]. 2.2 DiscLDA bel y alt.atheism vs. talk.religion.misc ). To model this labeling information, we introduce linear transformation T y :  X  K  X  X  X  L a mixture of Dirichlet distributions: T y  X  of the transformed variable T y  X  text corpus is represented through  X  be placed arbitrarily, as all documents X  X hether they have t he same class labels or they do not X  Compared to standard LDA, we have added the nodes for the vari able y  X  ), the transformation matrices T y and the corresponding edges.
 An alternative to DiscLDA would be a model in which there are c lass-dependent topic parameters  X  transformation of the  X  izing out the hidden topic vector z , we get the following distribution for the word w parameters as follows: tion can be accomplished by using the following transformat ions (for binary classification): where I other. We will explore this parametric structure later in ou r experiments. Note that we can give a generative interpretation to the tran sformation by augmenting the model with a hidden topic vector variable u , as shown in Fig. 3, where u -topics.
 In the author-topic model, the bag-of-words representatio n of a document is augmented by a list document, it is crucial that we also model the content of a doc ument.
 McAuliffe [2] proposed a supervised LDA model where the empi rical topic vector z (sampled from  X  ) is used as a covariate for a regression on y (see also [6]). Mimno and McCallum [9] proposed a instead of a generative criterion. Given a corpus of documents and their labels, we estimate the parameters { T y } by maximizing the conditional likelihood P much the same way as in standard LDA models. Intuitively, the two different training objectives different classes within the corpus.
 We use the Rao-Blackwellized version of Gibbs sampling pres ented in [8] to obtain samples of z and u with  X  and  X  marginalized out. Those samples can be used to estimate the l ikelihood of that it gave reasonably stable estimates for our purposes.
 fixed  X  . The gradient can be estimated by Monte Carlo EM, with sample s from the Gibbs sampler. More specifically, we use the matching property of gradients in EM to write the gradient as:  X   X  T where q y T . We can approximate those expectations using the relevant G ibbs samples. After a few gradient updates, we refit  X  by its MAP estimate from Gibbs samples. 3.1 Dimensionality reduction P y p ( y |  X  , w , T ) E [ T Figure 4: t-SNE 2 D embedding of the
E [ T y  X  |  X  , w , T ] representation of News-groups documents, after fitting to the Dis-cLDA model ( T was fixed). be estimated using the harmonic mean estimator and the secon d term can be approximated from or for visualization purposes. We evaluated the DiscLDA model empirically on text modeling and classification tasks. Our ex-periments aimed to demonstrate the benefits of discriminati ve training of LDA for discovering a that were not trained discriminatively. 4.1 Text modeling The 20 Newsgroups dataset contains postings to Usenet newsgroups. The postin gs are organized by we investigate how DiscLDA can exploit the labeling informa tion X  X he category X  X n discovering meaningful hidden structures that differ from those found u sing unsupervised techniques. We fit the dataset to both a standard 110 -topic LDA model and a DiscLDA model with restricted forms of the transformation matrices { T y } y =20 label c is fixed and given by the following blocked matrix This matrix has ( C + 1) rows and two columns of block matrices. All but two block matr ices dimensionality of K K topic vector  X  sets: one set of K of specific characteristics of data from each class.
 Table 1: Most popular words from each group of class-depende nt topics or a bucket of  X  X hared X  topics learned in the 20 Newsgroups experiment with fixed T matrix.
 In a first experiment, we examined whether the DiscLDA model c an exploit the structure for T y MAP estimate from Gibbs samples as explained in Section 3. We then estimated a new represen-explained in Section 3.1. Finally, we then computed a 2D-emb edding of this K (MDS), using the symmetrical KL divergence between pairs of  X  a modified version of the t-SNE stochastic neighborhood embe dding presented by van der Maaten bedding computed from standard LDA, shown in Fig. 5, does not show a clear separation. In this experiment, we have set K we set K = 110 for the standard LDA model for proper comparison.
 of stop words from the documents. 4.2 Document classification SVM) to compare the features obtained by DiscLDA to those obt ained from LDA. features obtained by DiscLDA for the 20 Newsgroups problem. Specifically, we constructed mul-from the DiscLDA model were used we obtained an error rate of 20% . Clearly the DiscLDA features have retained information useful for classification.
 We also computed the MAP estimate of the class label y  X  = arg max p ( y | w ) from DiscLDA and used this estimate directly as a classifier. The error rate wa s again 20% . T estimate of the  X  vector. This was followed by the discriminative learning pr ocess in which we EM with a constant step size for 10 epochs. We then re-estimat ed  X  by sampling u conditioned validation data set. The step size was chosen by grid search.
 newsgroup alt.atheism from postings of the newsgroup talk.religion.misc , a difficult task due to the similarity in content between these two group s.
 vised LDA and DiscLDA as input features to binary linear SVM c lassifiers. We also computed the prediction of the label of a document directly with DiscLDA. As shown in the table, the DiscLDA model clearly generates topic vectors with better predicti ve power than unsupervised LDA. inative DiscLDA approach. We found that the learned T had a block-diagonal structure similar more discriminating than the topics presented in Table 1. We have presented DiscLDA, a variation on LDA in which the LDA parametrization is augmented discriminative learning experiment on the binary dataset. tion (labels) in forming these representations.
 Although we have focused on LDA, we view our strategy as more b roadly useful. A virtue of the may be more effective.
 Acknowledgements We thank the anonymous reviewers as well as Percy Liang, Iain Murray, Guillaume Obozinski and Erik Sudderth for helpful suggesti ons. Our work was supported by Grant 0509559 from the National Science Foundation and by a grant f rom Google. [1] T. L. Berg, A. C. Berg, J. Edwards, M. Maire, R. White, Y. W. Teh, E. Learned-Miller, and [4] F. Chiaromonte and R. D. Cook. Sufficient dimension reduc tion and graphics in regression. [9] D. Mimno and A. McCallum. Topic models conditioned on arb itrary features with Dirichlet-
