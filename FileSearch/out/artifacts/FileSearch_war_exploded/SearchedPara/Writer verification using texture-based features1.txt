 ORIGINAL PAPER R. K. Hanusiak  X  L. S. Oliveira  X  E. Justino  X  R. Sabourin Abstract In this work, we propose a writer verification sys-tem that takes into account texture-based features and dis-similarity representation. Textures of the handwritings are created based on the inherent properties of the writer. Inde-pendent of the writing style, the proposed method reduces the spaces between lines, words, and characters, producing a texture that keeps the main features thus avoiding the com-plexity of segmentation. We also address an important issue of verification system, i.e., the number of writers used for training. Our experiments show that the number of writers do not have an important impact on the overall error rate, but it has an important role in reducing the false acceptance of the verification system. We show that the false acceptance decreases as the number of writers increases. Finally, the ROC curves produced by different classifiers trained with different texture descriptors are combined using the maxi-mum likelihood analysis, producing a ROC combined clas-sifier. A set of experiments on a database composed of 315 writers show the efficiency of the texture-based features and the ROC combination scheme. Experimental results report an overall error rate of about 4%. This performance com-pares to the state of the art. Besides, the combination scheme is able to considerably reduce the false-positive rates while maintaining the same true-positive rates.
 1 Introduction The main goal of a writer verification system is to deter-mine whether or not a handwritten text was written by a given writer. Similar to other verification systems, like sig-nature verification, a failure is referred as type I error (false rejection), i.e., rejecting a genuine text. The system should cope also with a more challenging problem, i.e., avoiding the acceptance of forgeries as being authentic. This second error is referred as type II error (false acceptance).

The writer verification problem can be categorized into on-line and off-line. In general, on-line systems achieve bet-ter performance since they can use spatial and temporal infor-mation about the writing. Off-line systems, on the other hand, are difficult to design since the dynamic information about the writing is lost during the document acquisition. They also can be categorized into text-dependent and text-independent. As the name suggests, in the text-dependent methods, the writing samples contain a predefined text. These methods normally use the comparison between individual characters or words of known transcription and thus require the text to be recognized or segmented into characters or words prior to writer verification [ 28 ]. As pointed out by Siddiqi and Vincent [ 31 ], in this respect, text-dependent writer verifica-tion is similar to signature verification. The text-independent methods, on the other hand, use any text to establish the iden-tity of the writer; hence, they feature less constraints and are more suitable for real applications.

The writer verification problem is a binary problem by nature. Given an input feature vector x extracted from a text t and a claimed identity I , determine whether ( I , x ) belongs to class  X  1 or  X  2 . The class  X  1 indicates that the claim is true, i.e., the text has been written by the author I , and  X  2 cates that the claim is false, i.e., the text is from an impostor. Differently from the identification problem, where the task consists in determining the identity I among all the writers enrolled in the system, the verification task performs an 1:1 comparison, which makes this kind of approach suitable even when a huge number of writers should be considered.
One can deal with the writer verification problem in two different ways. The standard approach, which is often used for signature verification, consists in building a specific model for each writer. In this context, different techniques of classification have been reported in the literature, such as hidden Markov models [ 28 ], distance measures [ 6 , 7 ], grapheme Clustering [ 1 ], dissimilarity [ 3 , 30 ], and Bayes-ian classifiers [ 31 ]. In these cases, some samples of a given writer are used to model  X  1 , and some samples of other writ-ers, chosen randomly, are used to model the forgery class The main drawbacks of this approach are the need of learn-ing the model each time a new writer should be included in the system and the great number of genuine samples neces-sary to build a reliable model. In real applications, usually a limited number of samples per writer is available to train a classifier, which leads the class statistics estimation errors to be significant, hence resulting in unsatisfactory verification performance.

An alternative approach that has been successfully applied to signature verification have been presented by Bertolini et al. in [ 2 ]. It is based on dissimilarity representation, and it allows the possibility of adding new writers into the sys-tem without retraining the models. This strategy is based on a forensic document examination approach and is called writer-independent approach as the number of models does not depend on the number of writers [ 33 ]. It has been demon-strated that through this strategy, it is possible to build robust verification systems even when few samples per writer are available.

Regarding the feature extraction methods used in writer verification systems, they can be classified into local and global. Local methods [ 4 ] are based on specific features which in general involve a segmentation process, whereas global techniques [ 25 ] are based on the overall look and feel of the writing. Combining both global and local strategies has been investigated by some authors [ 1 , 30 , 31 ], which have reported interesting results.

In general, local approaches are preferred from the forensic experts perspective, since they can be based on well-known graphometric features [ 16 , 20 ]. A fundamental prob-lem with this approach lies in the difficulties of automatically segmenting the handwriting into lines, words, and characters for further feature extraction. To overcome the difficulties imposed by explicit segmentation strategies, in this work we propose a global approach for writer verification based on texture analysis. Such a texture is built based on the inherent properties of the writer. That is, regardless of the way the text has been drafted, all connected components are rearranged into a new space keeping the original slant but reducing the spaces between lines of text, words, and characters. This process creates a texture that keeps important features that enables us to use a global approach, avoiding the complexity of segmentation. Moreover, it is suitable for both text-depen-dent and text-independent verification systems.

Since our focus in this work is the texture generation rather thanexploringdifferenttextureanalysismethods,wedecided to apply a widely used approach to texture analysis, i.e., the gray-level co-occurrence matrix (GLCM), which have been extensively validated in several different domains, including writer identification [ 5 , 10 , 25 ]. In light of this, several GLCM were tried out, and those with best performance were con-sidered in this work. We also show that these results can be further improved by combining classifiers without the need for joint training. This combination is based on the maximum likelihood analysis of the receiver operating characteristics (ROC) curves of the classifiers.

Regarding the dissimilarity representation, one issue addressed in this work is the number of writers used to train the dissimilarity model, in our case, a support vector machine (SVM) trained to discriminate between genuine and forgery. We have shown that the size of the training set does not have an important impact on the overall error rate, but it has an important role in reducing the false acceptance (error type II) of the system. Through a set of experiments on a database composed of 315 writers, we demonstrate that the proposed approach is feasible. The overall error rate yielded by the system is about 4% after combining the classifiers. This performance compares to other methods reported in the literature. Besides, the combination scheme is able to con-siderably reduce the false-positive rates while maintaining the same true-positive rates.
 The remaining of this paper is organized as follows: Section 2 presents the dissimilarity framework proposed in this work. Section 3 introduces the ROC combination algo-rithm used in this work. Section 4 describes the database considered in our experiments. Section 5 shows the pro-posed texture-based feature sets and classification methods, while the experimental results and discussions are reported in Sect. 6 . Finally, Sect. 7 concludes this work and points out some future works. 2 The dissimilarity and writer identification framework In this section, we describe the proposed framework based on dissimilarity for writer verification. Our approach is inspired on forensic experts who compare a questioned sample with some references to assert whether a piece of handwriting is genuine or forgery. During this comparison, the experts extract different features to compute the level of similarity between the samples being compared [ 20 ]. In fact, the work performed by forensic experts is quite more complex in the sense they decide on the fly, based on their tacit knowledge, which features from the handwriting they should extract.
The concepts of similarity, dissimilarity, and proximity have been discussed in the literature from different perspec-tives [ 13 , 23 , 27 ]. Pekalska and Duin [ 23 ] introduce the idea of representing the relations between objects through dis-similarity, which they call dissimilarity representation. The seminal work using this concept in the field of author identifi-cation was presented by Cha and Srihari [ 7 ]. In this work, we use the idea of dissimilarity vectors presented by Bertolini et al. in [ 2 ], which combines feature-based description with the concept of dissimilarity. The idea is to extract the feature vectors from both questioned and reference samples and then compute what we call the dissimilarity feature vector. If both samples come from the same writer (genuine), then all the components of such a vector should be close to 0, otherwise (forgery), the components should be far from 0.
 Of course, this is totally true under favorable conditions. As any other feature representation, the dissimilarity feature vector can be affected by the intra-writer variability. Such a variability could generate values far from zero when measur-ing the dissimilarity of genuine writers.

The proposed framework is depicted in Fig. 1 .Aftera preprocessing step, composed basically of the Otsu thres-holding algorithm [ 22 ], a set of n genuine handwriting sam-ples, R i ( i = 1 , 2 , 3 ,..., n ) is sent to the feature extraction module and the feature vectors V i are stored into a data-base. Feature extraction is discussed in Sect. 5 . When ques-tionedsamples S i arepresentedtothesystem,theygothrough the same feature extraction process, thus generating the fea-ture vectors Q i . Then, the dissimilarity feature vectors Z |
V provides a decision for each dissimilarity feature vector. The final decision D depends on the combination of these partial decisions, which are obtained through a fusion rule. Section 6 discusses how such a combination is performed.

In the remaining of this work, we use several SVM classi-fiers trained on two features representations (directional and texture), employ traditional straightforward fusion rules, and also explore the use of a ROC-based classifier combination algorithm (described in Sect. 3 ) as an alternative combination method.
 3 Combining classifiers in the ROC space ROC curves are two-dimensional graphs in which true-positive rate (TPR) is plotted on the y axis and false-positive rate (FPR) is plotted on the x axis. Given a decision list and an instance set, a 2  X  2 confusion matrix (Fig. 2 ) can be gener-ated, representing the classification performance. From this matrix, four statistics are represented: tp , fp , fn , and tn , which stand for true-positive, false-positive, false-negative, and true-negative, respectively.

For a given classifier C , the ROC is a set of points ( fp C ( k C ), tp C ( k C ) ) where k C is the parameter that governs the decision process. A ROC graph depicts relative trade-offs betweenbenefits( tp )andcosts( fp ).Eachclassifierproduces a ( fp , tp ) pair corresponding to a single point in the ROC space. For an extensive review of ROC, please refer to [ 9 ].
Besides being an important tool to analyze and compare classifiers, ROC also have been used for combining classifi-ers. In this work, we have used the algorithm introduced by Haker et al. in [ 14 ]. It is based on the calculation of a com-bined ROC using maximum likelihood analysis to determine a combination rule for each ROC operating point.
 Let us consider the ROC for two different classifiers A and B and their respective parameters k A and k B . The per-formanceofclassifiers C A and C B arerepresentedintheROC spacebythepoints, ( fp A , tp A ) and ( fp B , tp B ) ,respectively. Given an input pattern, both classifiers will produce an out-put either positive ( + ) or negative (  X  ) , giving us a total of 4 possible cases. For each case, we have an expression of the maximum likelihood estimation (MLE) of the unknown truth T (Table 1 ).
Each inequality (logical expression) in the rightmost col-umn evaluates to either + or  X  , and the resulting value is the maximum likelihood estimate of the truth T . If conditional independence is assumed, then P ( A = 1 , B = 1 | T = 1 ) = P ( A = 1 | T = 1 ) P ( B = 1 | T = 1 ) = tp ing similarly for the other terms in the rightmost column of Table 1 , we get Table 2 .

Fromtheassumptionsdetailedabove, tp A tp B = fp A fp B and ( 1  X  tp A )( 1  X  tp B ) = ( 1  X  fp A )( 1  X  fp B ) , so when-ever A and B are in agreement, their common output is the maximum likelihood estimate of T . Thus, the middle two rows of the Table 2 need to be determined, resulting in one of four possible MLE combination schemes, which are mne-monically named schemes  X  X  AND B X  ( S A &amp; B ),  X  X  X  ( S  X  X  X  ( S B ), and  X  X  OR B X  ( S A | B ). Table 3 summarizes these schemes.
 Using again the assumption of conditional independence, Table 4 shows how to calculate the false-positive and true-positive rates for these schemes. In practice, this means that decision processes can be combined without retraining, since there is no need to estimate joint distributions for the output of A and B , nor the need to know the distribution of the underlying truth T .

As stated before, the classifiers (ROC) A and B are gov-erned by the parameters k A and k B , respectively. In other words, k A and k B are simple thresholds applied to the outputs s and s B calculated as part of the A and B decision process. Thus, A returns the estimate T = 1 if and only if s A &gt; Thesameholdsfor B .Therefore,anewdecisionruleisneces-sary for each scheme described so far. Let  X  be the combined classifier, created as described above. For a chosen operating point ( fp , tp ) on the ROC for  X  , we have associated thresh-olds k A and k B and an associated MLE combination rule. The new decision rules s for  X  are defined as function of s and s B as follows: min ( s A  X  k A , s B  X  k B ), s A  X  k and max ( s A  X  k A , s B  X  k B ) for the schemes S A &amp; B and S A | B , respectively. The combined classifier  X  will assign  X  +  X  when s &gt; 0; otherwise, it will assign  X   X   X . Algorithm 1 shows the pseudo-code for the combined ROC.

Tocombineclassifiers A and B ,wecomputeforeachvalue of the parameter pair ( k A , k B ) and corresponding 4-tuple of FPR and TPR ( fp A , tp A , fp B , tp B ) the correct ML scheme to use according to Table 2 and the resulting combined rates ( fp , tp ) for that scheme using the formula described in Table 4 . In practice, discrete values are assumed for k A k
B by sampling them evenly. Figure 3 shows an example of what would be a resulting set of points ( fp , tp )fortwo example ROCs.

The set of points ( fp , tp ) represent possible operating points for the joint process. However, we do not need to con-sider all the points since for each point in the interior (the red Algorithm 1 ROC Combination ones in Fig. 3 ) there is a point on the outer boundary of the region which is superior, and thus a better operating point. For example, there is a point on the boundary which has the same FPR and a greater TPR. Such points form the combined ROC. In practice, the outer boundary ROC can be estimated by splitting the interval [0,1] into a number of sub-intervals i.e., bins, and within each bin finding the pair ( fp , tp )having the largest value of tp [ 9 ]. 4 Database The database used in this investigation is the Brazilian Forensic Letter Database [ 11 ], which is composed of 315 writers, three samples per writer, summing up 945 images. The motivation for using such database is the growing inter-est of the Brazilian forensic experts as well as the Brazilian Federal Police in writer verification.

The samples were provided by undergraduate students in three different sessions during one month. The texts were collected on an A4 white sheet of paper with no pen-draw baseline and then scanned in gray level with 300 dpi (3760 2448). Each writer was allowed to use his/her own pen, which means that several different pens were used. The text is con-cise (131 words) and complete in the sense that it contains all characters (letters and numerals) and certain character com-binationsofinterest.Thismakesitsuitablefortext-dependent writer identification as well. Figure 4 shows (a) the letter con-tents and (b) a image sample of the database. More details about this database 1 can be found in [ 11 ].

As discussed in the introduction, in this work we argue that it is possible to identify discriminant features from the texture created by concatenating the writer X  X  handwriting. In this vein, a new database of image textures was build to sup-port further experiments. More details about the technique used to build the textures are presented in Sect. 5 . The texture database is composed of 15 (5 textures  X  3 letters) images per writer, summing 4,725 texture images.

Considering the dissimilarity-based approach adopted in this work, the classifiers should be trained to discriminate between genuine (positive) and forgeries (negative). To gen-erate the positive samples, we have computed the dissimi-larity vectors among three genuine samples of each writer (one segment of texture extracted from each letter), which results into three different combinations. The negative sam-ples were generated by computing the dissimilarity between one sample of one author against one sample of three other authors picked randomly. This also results into three different combinations.

The 315 writers of the database were divided into training and testing. Four different partitions for training were con-sidered: 25, 50, 100, and 200 writers. The idea is to analyze the impact of the number of writers used for training in the overall performance. The remaining 115 were used for test-ing. It is important to remark that different writers were used for training and testing. Considering 50 writers and three textures segments for training, we would have 150 (3  X  50) positive samples and 150 (3  X  50) negative samples. Figure 5 exemplifies this process.

In Fig. 5 a, V a , V b , and V c are the reference feature vectors extracted from the reference images (e.g., texture segments) for a given writer. Based on these three vectors, three dis-similarity vectors ( Z 1 , Z 2 , and Z 3 ) are computed. These are positive (genuine) dissimilarity vectors, which are expected to have components close to 0. A similar process is depicted in Fig. 5 b to create the negative (forgery) dissimilarity vec-tors. In this case, the reference feature vectors are compared with feature vector of other authors picked randomly, and it is expected that they have components far from 0.

Regarding the testing set, we have picked 115 writers randomly touseas randomforgeries. Tobuildabalancedtest-ing set, two letters from each author were selected randomly. Therefore,eachwriterofthetestingsethasonegenuineletter, represented by 5 pieces of texture, and one random forgery, also represented by 5 pieces of texture. 5 Features It is well known that the choice of the features is a crucial aspect for any pattern recognition problem. For writer verifi-cation, it is not different. However, when dealing with hand-writing, one must rely on features that have well-established scientific basis. This is an important issue especially if the writer verification system is used to support a legal decision in a court of justice.

In light of this, forensic document examiners (FDE) usually rely on graphology to perform writer verification. Graphology can be defined as the study and analysis of hand-writing especially in relation to human psychology. A branch of the graphology is the psychometrical graphology or graph-ometry. This is the term used to describe the technique of picking up psychic impressions about a person from a spec-imen of their handwriting [ 21 ]. The graphometrical features can be classified into genetic and generic. The genetic fea-tures are: minimal graphics (i dots, commas, cedillas, tildes, etc.), pressure, speed, entry/exit strokes, and movement. The generic features are: caliber, spacing between characters and words, proportion, slant, and alignment to baseline.
Gobineau and Perron [ 12 ] elaborated a theory of graph-ometry or, more exactly, a statistical method of the graphic elements. In their work, they propose more than 60 features but choose 14 which they deem essential and easy to extract.
Some of the concepts of graphology have been intrinsi-cally used to build automatic signature verification systems by several different authors [ 17 , 18 ], but few works [ 26 ]have applied the graphology concepts to writer verification. This is because the extraction of graphometric features in the context of writer verification is not straightforward. The main prob-lems are related to the difficulties of segmenting the hand-writing into lines, words, and characters for further feature extraction. Consider the examples depicted in Fig. 6 where problems such as (a) overlapping and (b) skew are impor-tant issues for any segmentation algorithm. Besides, Fig. 6 c shows a piece of a illegible text which is not so difficult to segment, but it is not so rich in terms of graphometric features.

To surpass these problems, in this work, we proposed a segmentation strategy based on the inherent properties of the writer. That is, regardless of the way the text has been drafted, the segmentation process will rearrange all connected com-ponents into a new space keeping the original slant but reduc-ing the spaces between lines of text, words, and characters. In the end, the segmentation process will produce a texture image that still contains the main characteristics of the writ-ing style.

The segmentation process is straightforward. First, the im-age is binarized using Otsu algorithm and then scanned top-down,left-right,todetectalltheconnectedcomponentsofthe image. The 8 adjacency was considered in this work. Small components such as periods, commas, strokes, and noise are discarded at this time. The bounding box of the remaining components are then used to extract the original components of the gray-level image. The components in gray level are then aligned with the new image using the center of mass of the bounding box. Figure 7 a exemplifies this process.
After fillingthefirst line, wecomputetheaverageheight of all connected components used in such a process. This value is used to define the y -coordinate of the next line, which is given by new_ y = previous_ y + where previous_ y is the y -coordinate used to fill the previous line (in the case of the first line a constant k = 150 was used) and h is the average height of all connected components used to fill the previous line. Reducing the gap between the lines by dividing h by two allows us to build more representative textures; otherwise, the texture will contain too much blank spots, like in Fig. 7 b. This denominator was found empiri-cally. Figure 8 shows an example of the texture created from the original gray-level letter. As mentioned previously, the final texture image representing the writer X  X  handwriting is finally segmented into equal blocks of 256  X  256 pixel.
The letters of the database used in this work allow us to generate up to nine blocks of texture. However, if smaller documents were considered, it will be impossible to get the same number of blocks. To make the proposed method less dependent of the size of the document, we decide to use only five blocks per letter.

This segmentation schemes differs from the ones pre-sented in the literature [ 5 , 25 ] in the sense that there is no preprocessing such as slant correction which are necessary for line segmentation. Besides making the segmentation sim-pler, the proposed texture generation method also keeps some features such as skew and slant. Figure 9 shows that different textures can be created for different handwriting styles.
In Fig. 9 a, different calibers, which is the relationship between height and width, can be observed. Words with smaller caliber produce less-overlapped texture than those with larger caliber. Figure 9 b shows the progression where slow writers usually yield more legible texts than fast writers. In Fig. 9 c, the proportion among ascenders, descenders, and the main body of the words is presented. Alike words with big caliber, high ascenders and descenders will create a more dense texture.

Pressure is related to the changing width of a line as pen pressure on paper varies. High pressure will produce darker textures, while low pressure will produce lighter textures, as depicted in Fig. 9 d. Entry/End points are related to how a writer starts of finishes a character or word. This feature is known as characteristic gesture and usually very difficult to find by means of a computer program. In a texture (Fig. 9 e), bigger entry/end points (strokes) may produce more white spaces. Finally, Fig. 9 f shows different slants produced by different writers, which are clearly captured by the texture. 5.1 Feature extraction After producing the textures from the handwritings, the next step is to choose a suitable descriptor that is capable of pro-viding measures such as smoothness, coarseness, and regu-larity. As stated before, our choice was to apply a widely used approach to feature analysis, the GLCM. A GLMC is the joint probability occurrence of gray-level i and j within a defined spatial relation in an image. That spatial relation is defined in terms of a distance d and an angle  X  . Given a GLMC, some statistical information can be extracted from it. The most common descriptors were proposed by Haralick [ 15 ] and have been successfully used in several application domains, including writing verification/identification [ 5 , 10 , 25 ]. In this work, we have used the following five descriptors, which pro-vided the best preliminary results: Entropy = X  Homogeneity = Dissimilarity = Inverse variance = Energy =
In that case, p ( i , j ) is the probability of co-occurrence of gray-level i and j observing consecutive pixels at distance d and angle  X  .

Based on these measures, five different feature sets were extracted. Each of them contains 20 components, which are the combination of five distances ( d = 1 , 2 , 3 , 4 , 5) and four angles (  X  = 0 , 45 , 90 , 135). Longer distances and different angles were tried out during our experiments, but these val-ues brought us the best results. The features are normalized between 0 and 1 using the min-max rule. 6 Experiments and discussion Our experiments are divided into three parts. First, we ex-plore the use of directional features extracted from both original and texture images. Then, we discuss different texture descriptors extracted from the GLCM and the impacts of the number of writers on the overall error rate. Finally, we report the experiments concerning the combination of clas-sifiers using the ROC-based classifier combination strategy described in Sect. 3 .

In all experiments, support vector machines (SVM) were used as classifiers. Training is done using fivefold cross-validation. Different kernels were tried, but the best results were achieved using a Gaussian kernel. Parameters C and  X  were determined through a grid search. The overall error rate that has been used for evaluation purposes in this work is given by Eq. 7 .
 Overall Error Rate =
One of the limitations with SVMs is that they do not work in a probabilistic framework. There are several situations where it would be very useful to have a classifier producing a posterior probability P ( class | input ) . In our case, as depicted in Fig. 1 , we are interested in the estimation of probabili-ties because we want to try different fusion strategies like sum, max, min, average, and median. Due to the benefits of having classifiers estimating probabilities, many researchers have been working on the problem of estimating probabilities with SVM classifiers [ 24 , 32 ]. In this work, we have adopted the strategy proposed by Platt in [ 24 ]. 6.1 Experiments with directional features Before reporting the experiments using texture-based fea-tures, first we present some results using one of the tradi-tional characteristics used by FDEs. According to the experts [ 16 , 20 ], a given writer takes some directions more frequently and more intensively than others. Therefore, the cumulative distribution of such directions is an important and discrim-inative feature of the writer. The directional features have been implemented as suggested by Crettez [ 8 ]. The feature vector is composed of 17 components containing a histogram of 17 directions ranging from 0 to 180 degrees.

The experimental protocol aforementioned, using 50 writ-ers for training, was used into two different experiments. In the first case, the letters were segmented into 5 blocks of 800  X  600 pixels and then the features were extracted from these sub-images. In the second experiment, we extracted the features from five blocks of textures. The size 800  X  was the maximum size that allowed us to created five distinct pieces with handwriting information in all of them. In spite of the large size of the original image (3760  X  2448), some writers use just part of the sheet with the handwriting, which makes it impossible to extract more blocks of handwriting.
Using five blocks in both cases allows us to keep the same experimental protocol, i.e., combining five outputs to pro-duce a final decision. However, it is fair to argue that this is not a complete honest comparison since the five original blocks of handwriting contain less information than the tex-ture blocks. With this in mind, we increased the size of the original blocks to 1200  X  600 pixels, thus reducing the num-ber of blocks to three. Experimental results show that using larger blocks has almost no impact in the overall error rate. Table 5 compares these experiments.

It is worth of noticing that when the directional features were extracted from the texture images, the overall error rate was about 13% points smaller than the features computed on the original images. This can be explained by the fact that the texture images (Fig. 10 a) contain more information so more representative histograms can be created. From Fig. 10 b, it is easy to observe that segmenting the original letter can pro-duce images with several blank spots (Fig. 10 a), thus pro-ducing less discriminative feature vectors. 6.2 Experiments with texture features Concerning the experiments using the texture descriptors introduced in Sect. 5.1 , the first (baseline) experiment we have performed used 50 writers for training and the entire testing set, i.e., 115 writers, for testing. The original texture images with 16 gray levels were considered. Regarding the fusion rule responsible for combining the results produced by the SVM on the three dissimilarity feature vectors, in our experiments the sum rule outperformed all the other meth-ods. For this reason, it was used in all remaining experiments discussed in this section. Table 6 reports the results for the baseline system.

As we can observe from Table 6 , those descriptors that look for the homogeneity of the texture such as entropy and homogeneityachievedsmaller overall error rates. Inall cases, though, the type II error is higher than type I Error, which is not interesting since in a verification system, it is desirable to reduce as much as possible the false acceptance.
After analyzing the baseline results and the co-occurrence matrices created with 16 gray-level images, we observed that the elevated number of gray levels did not bring enough dis-criminant information for the handwriting textures. With this in mind, we investigated the impact of reducing the number of gray levels until we get binary images. Table 7 shows the results of these experiments.

From Table 7 , we can conclude that binary images are the best representation for handwriting textures when using co-occurrence matrices. In fact, the gray-level information contained in the letter images are not relevant. All the fea-tures discussed previously are available in the binary texture, which has been demonstrated through these experiments. Figure 11 showstheROCcurvesforallthedescriptorstrained with binary images. As we can see, their performance is quite similar differing only for some operational points in the ROC space. The area under the curve (AUC), which is used very often to compare classifiers, is practically the same for all the classifiers (AUC 0 . 98).

Another experiment we performed in this work regards the number of writers used in the training set. It is worth of remembering that in the dissimilarity approach the writers used for training are not used for testing. This is one of the benefits of this approach since we can add new writers into the system without retraining the classifier. This being said, this experiment aims at verifying the impacts of increasing the number of writers in the training set. In other words, is 50 writers enough to train a robust machine learning model based on dissimilarity? For these experiments, we have used the same protocol used so far but considering only the binary images, which achieved the best results in the previous exper-iments. Since we have 200 writers available in the training set, we doubled the number of writers for each experiment starting from 25 writers. Table 8 reports the results.
The experiments reported in Table 8 show that increasing the number of writers does not necessarily reduce the overall error rate. However, if we take a closer look, we will notice a trade-off between the number of writers used for training and the performance of error types I and II. By increasing the number of writers for training, we sure build more robust models against false acceptance. From the ROC depicted in Fig. 12 , it is possible to notice that the classifier trained with 200 writers dominates all the others for low FPR, but after a certain level, it is surpassed by the others. Table 9 shows the trade-off between the size of the training set and error types I and II for the classifier trained with the entropy feature set. The same phenomenon was observed for all other feature sets. 6.3 Combining classifiers in the ROC space A final experiment concerns the combination of classifiers using the strategy described in Sect. 3 . The objective here is to improve the trade-off between false rejection and false acceptance by combining the classifiers through the max-imum likelihood analysis of the ROC classifiers. We have done different experiments, but the setup that brought more improvement was the combination of classifiers trained with entropy and homogeneity descriptors. Figure 13 shows all the ( fp , tp ) pairs generated by Haker X  X  algorithm and the combined ROC classifier as well.

As discussed in Sect. 3 , the red dots in Fig. 13 represent all possible operating points for the joint process. As stated before, we do not need to consider all the points, since for each point in the interior there is a point on the outer bound-ary of the region which is superior and thus a better operating point.

It is clear from Fig. 13 that such a combination can con-siderably reduce the FPR while maintaining the same TPR. Using the combined ROC, for a TPR of 0.95 the correspond-ing FPR is 0.008. For the same TPR, entropy and homo-geneity produce a FPR of 0.056 and 0.087, respectively. It is important to remember, though, that this algorithm re-lies on the assumption of conditional independence. In other words, the combined ROC is the upper-bound that could be achieved if both entropy and homogeneity classifiers were conditionally independents. In terms of recognition rate, the upper-bound would be 97.1%, since the pair ( fp , tp ) that maximizes performance is (0.008, 0.95). Considering that the testing set contains 575 (115  X  5 references) positive and 575 negative samples, 97 . 1% = (( 575  X  0 . 95 ) + ( ( 575  X  0 . 008 ))/ 1150 )
However, in practice, the performance observed during the experiments was 96.1%, i.e., an overall error rate of 3.9%. This corroborates to the fact that the classifiers we have used in this work do not hold the assumption of conditional inde-pendence. Nevertheless, the algorithm still brought about 1% point of improvement. This proves that it can be applied even when there is no guarantee of conditional independence.
Still, in the context of combining classifiers, we have used both classifiers (entropy and homogeneity) and combined them at the feature level. In this case, the SVM classifiers were trained with a feature vector composed of 40 texture descriptors, and the same experimental protocol described so far has been followed. The best overall error rate we got using this combination strategy was 5.2%.

To have a better insight into the performance of the results reported so far, Table 10 summarizes some works we found in the literature on writer verification/identification. Of course, a direct comparison is not possible since different databases, number of writers, features, classifiers, and samples per writer were considered. Our approach, for example, uses a full page of handwriting instead of segmented words and paragraphs. In spite of all that, it is possible to observe that the performance reported in this work compares to the state of the art. 7 Conclusion In this work, we have proposed a writer verification system that takes into account texture-based features and a dissim-ilarity representation. The texture of the handwriting was created based on the inherent properties of the writer. Inde-pendent of the writing style, the proposed method reduces the spaces between lines, words, and characters, producing a texture that keeps important characteristics of the writing style. This enables us to use a global approach, avoiding the complexity of segmentation. The overall error rate achieved by the system trained with texture descriptors was about 13 percentage points smaller than the system trained with clas-sical characteristics such as directional features. Besides, the handwriting texture can be used together with other features, e.g., directional features, improving their results.
We also demonstrate the impacts of increasing the number of writers for training in the proposed system. We have seen that the size of the training set does not have an important impact on the overall error rate, but it has an important role in reducing the false acceptance of the verification system. Based on these findings, it is reasonable to suggest a bigger training set when the requirements of the verification system demand small FPR. On the other hand, the number of writer in the training set can be relaxed, thus reducing the false rejection. Finally, we have demonstrated that it is possible to considerably reduce the FPR for a fixed TPR by combining the ROC produced by two different classifiers through the maximum likelihood analysis.

These results suggest that some kinds of writer selection process could be interesting to build better dissimilarity mod-els for writer verification. Using those writers that feature excessive variability in his/her handwriting may not be help-ful to build a stable machine learning model. Another aspect worth of investigation is the impact of the number of refer-ences used to perform the verification. Both issues will be subject of further investigation.
 References
