 f kathy,rambow g @cs.c olumbia.edu Relations such as Cause and Contrast, which we call rhetorical-semantic relations (RSRs), may be sig-naled in text by cue phrases lik e because or how-ever which join clauses or sentences and explicitly express the relation of constituents which the y con-nect (Example 1). In other cases the relation may be implicitly expressed (2). 1 Example 1 Because of the recent accounting scan-dals, ther e have been a spate of executive resigna-tions.
 Example 2 The administr ation was once again be-set by scandal. After sever al key resignations ...
In this paper , we examine the problem of detect-ing such relations when the y are not explicitly sig-naled. We dra w on and extend the work of Marcu and Echihabi (2002). Our baseline model directly implements Marcu and Echihabi' s approach, opti-mizing a set of basic parameters such as smoothing weights, vocab ulary size and stoplisting. We then focus on impro ving the quality of the automatically-mined training examples, using topic segmenta-tion and syntactic heuristics to lter out training instances which may be wholly or partially in-valid. We nd that the parameter optimization and segmentation-based ltering techniques achie ve sig-nicant impro vements in classication performance. Rhetorical and discourse theory has a long tradition in computational linguistics (Moore and Wiemer -Hastings, 2003). While there are a number of dif fer -ent relation taxonomies (Hobbs, 1979; McK eown, 1985; Mann and Thompson, 1988; Martin, 1992; Knott and Sanders, 1998), man y researchers have found that, despite small dif ferences, these theories have wide agreement in terms of the core phenom-ena for which the y account (Ho vy and Maier , 1993; Moser and Moore, 1996).

Work on automatic detection of rhetorical and dis-course relations falls into two cate gories. Marcu and Echihabi (2002) use a pattern-based approach in mining instances of RSRs such as Contrast and Elaboration from lar ge, unannotated corpora. We discuss this work in detail in Section 3. Other work uses human-annotated corpora, such as the RST Bank (Carlson et al., 2001), used by Soricut and Marcu (2003), the GraphBank (W olf and Gib-son, 2005), used by Wellner et al. (2006), or ad-hoc annotations, used by (Girju, 2003; Baldridge and Lascarides, 2005). In the past year , the ini-tial public release of the Penn Discourse TreeBank (PDTB) (Prasad et al., 2006) has signicantly ex-panded the discourse-annotat ed corpora available to researchers, using a comprehensi ve scheme for both implicit and explicit relations.

Some work in RSR detection has enlisted syntac-tic analysis as a tool. Marcu and Echihabi (2002) l-ter training instances based on Part-of-Speech (POS) tags, and Soricut and Marcu (2003) use syntac-tic features to identify sentence-internal RST struc-ture. Lapata and Lascarides (2004) focus their work syntactically , analyzing temporal links be-tween main and subordinate clauses. Sporleder and Lascarides (2005) extend Marcu and Echihabi' s ap-proach with the addition of a number of features, including syntactic features based on POS and ar-gument structure, as well as lexical and other sur -face features. The y report that, when working with sparse training data, this richer feature set, combined with a boosting-based algorithm, achie ves more ac-curate classication than Marcu and Echihabi' s sim-pler , word-pair based approach (we describe the lat-ter in the next section). We model two RSRs, Cause and Contrast, adopt-ing the denitions of Marcu and Echihabi (2002) (henceforth M&amp;E) for their Cause-Explanation-Evidence and Contrast relations, respecti vely . In particular , we follo w their intuition that in building an automated model it is best to adopt a higher -le vel vie w of relations (cf. (Ho vy and Maier , 1993)), collapsing the ner -grained distinctions that hold within and across relation taxonomies.

M&amp;E use a three-stage approach common in cor -pus linguistics: collect a lar ge set of class instances ( instance mining ), analyze them to create a model of dif ferentiating features ( model building ), and use this model as input to a classication step which determines the most probable class of unkno wn in-stances.

The intuition of the M&amp;E model is to apply a set of RSR-associated cue phrase patterns over a lar ge text corpus to compile a training set without the cost of human annotation. For instance, Example 1 will match the Cause-associated pattern  X Because of W 1 ,
W 2 . X , where W 1 and W 2 stand for non-empty strings containing word tok ens. In the aggre gate, such instances increase the prior belief that, e.g., a text span containing the word scandals and one containing resignations are in a Cause relation. A critical point is that the cue words themselv es (e.g., because ) are discarded before extracting these word pairs; otherwise these cue phrases themselv es would lik ely be the most distinguishing features learned.
More formally , M&amp;E build up their model through the three stages mentioned abo ve as fol-lows: In instance mining , for each RSR r the y com-pile an instance set I match a set of patterns associated with r . In model building , features are extracted from these in-stances; M&amp;E extract a single feature, namely the frequenc y of tok en pairs deri ved from taking the cartesian product of W 1 = f w 1 ...w f w n +1 ...w m g = f ( w 1 , w n +1 ) ... ( w n , w m ) g each span pair instance ( W 1 , W 2 ) 2 I ; these pair frequencies are tallied for each RSR into a frequenc y table F lation r between two unkno wn-relation spans W 1 and W 2 can be determined by a na X ve Bayesian classier as argmax probability P ( r j W 1 , W 2 ) is simplied by assum-ing the independence of the indi vidual tok en pairs to: Q counts F tors of P (( w TextRels is our implementation of the M&amp;E frame-work, and serv es as our platform for the experiments which follo w.

For instance mining , we use a set of cue phrase patterns deri ved from published lists (e.g., (Marcu, 1997; Prasad et al., 2006)) to mine the Giga word corpus of 4.7 million newswire documents 2 for re-lation instances. We mine instances of the Cause and Contrast RSRs discussed earlier , as well as a NoRel  X relation X . NoRel is proposed by M&amp;E as a def ault model of same-topic text across which no specic RSR holds; instances are extracted by tak-ing text span pairs which are simply sentences from the same document separated by at least three inter -vening sentences. Table 1 lists a sample of our ex-traction patterns and the total number of training in-stances per relation; in addition, we hold out 10,000 instances of each type, which we divide evenly into development and training sets.

For model building , we compile the training in-stances into tok en-pair frequencies. We implement several parameters which control the way these fre-quencies are computed; we discuss these parameters and their optimization in the next section.
For classication , we implement three binary classiers (for Cause vs Contrast, Cause vs NoRel and Contrast vs NoRel) using the na  X  ive Bayesian frame work of the M&amp;E approach. We implement several classication parameters, which we discuss in the next section. Our rst set of experiments examine the impact of various parameter settings in TextRels, using classi-cation accurac y on a development set as our heuris-tic. We nd that the follo wing parameters have strong impacts on classication: ming slightly impro ves accurac y and also reduces model size.
 Turing, but is simpler to implement. Our experi-ments nd peak performance with 0.25  X  value, i.e. the frequenc y assumed for unseen pairs.
 mance; tok ens which are not in the most frequent 6,400 stems (computed over Giga word) are replaced by an UNK pseudo-tok en before F is computed. we nd that even the most frequent tok ens contrib ute useful information to the model; a stoplist size of zero achie ves peak performance.
 card from F tok en pair counts with a frequenc y of &lt; 4; results degrade slightly belo w this value, and discarding this long tail of rare pair counts signi-cantly shrinks model size.
 Table 2: Classier accurac y across PDTB, Auto and Auto-S test sets for the parameter -optimized classier ( X Opt X ) and the same classier trained on segment-constrained instances ( X Se g X ). Accurac y from M&amp;E is reported for reference, but we note that the y use a dif ferent test set so the comparison is not exact. Baseline in all cases is 50%.

To evaluate the performance of our three binary classiers using these optimizations, we follo w the protocol of M&amp;E. We present the classier for , e.g., Cause vs NoRel with an equal number of span-pair instances for each RSR (as in training, any pattern text has been remo ved). We then determine the ac-curac y of the classier in predicting the actual RSR of each instance; in all cases we use an equal num-ber of input pairs for each RSR so random baseline is 50 %. We carry out this evaluation over two dif-ferent test sets.
 The rst set ( X PDTB X ) is deri ved from the Penn Discourse TreeBank (Prasad et al., 2006). We ex-tract  X Implicit X  relations, i.e. text spans from adja-cent sentences between which annotators have in-ferred semantics not mark ed by any surf ace lexi-cal item. To extract test instances for our Cause RSR, we tak e all PDTB Implicit relations mark ed with  X Cause X  or  X Consequence X  semantics (344 to-tal instances); for our Contrast RSR, we tak e in-stances mark ed with  X Contrast X  semantics (293 to-tal instances). 3 PDTB marks the two  X Ar guments X  of these relationship instances, i.e. the text spans to which the y apply; these are used as test ( W 1 , W 2 ) span pairs for classication. We test the perfor -mance on PDTB data using 280 randomly selected instances each from the PDTB Cause and Contrast sets, as well as 280 randomly selected instances from our test set of automatically extracted NoRel instances (while there is a NoRel relation included in PDTB, it is too sparse to use in this testing, with 53 total examples).

The second test set ( X Auto X ) uses the 5,000 test instances of each RSR type automatically extracted in our instance mining process.

Table 2 lists the accurac y for the optimized ( X Opt X ) classier over the Auto and PDTB test sets 4 . (The  X Se g X  columns and  X Auto-S X  test set are ex-plained in the next section.)
We also list for reference the accurac y reported by M&amp;E; howe ver, their training and test sets are not the same so this comparison is ine xact, al-though their test set is extracted automatically in the same manner as ours. In the Cause versus Contrast case, their reported performance exceeds ours sig-nicantly; howe ver, in a subset of their experiments which test Cause versus Contrast on instances from the human annotated RSTBank corpus (Carlson et al., 2001) where no cue phrase is present, the y re-port only 63% accurac y over a 56% baseline (the baseline is &gt; 50% because the number of input ex-amples is unbalanced).

Since we also experience a drop in performance from the automatically deri ved test set to the human-annotated test set (the PDTB in our case), we fur -ther examined this issue. Our goal was to see if the lower accurac y on the PDTB examples is due to (1) the inherent dif culty of identifying implicit rela-tion spans or (2) something else, such as the corpus-switching effect due to our model being trained and tested on dif ferent corpora (Giga word and PDTB, respecti vely). To informally test this, we tested against explicitly cue-phrase mark ed examples gath-ered from PDTB. That is, we used the M&amp;E-style method for mining instances, but we gathered them from the PDTB corpus. Interestingly , we found that (1) appears to be the case: for the Cause vs. Contrast (68.7%), Cause vs. NoRel (73.0%) and (Contrast vs. NoRel (71.0%) classiers, the performance patterns with the Auto test set rather than the results from the PDTB Implicit test set. This bolsters the argument that  X synthetic X  implicit relations, i.e. those created by stripping of originally present cue phrases, can-not be treated as fully equi valent to  X or ganic X  ones annotated by a human judge but which are not ex-plicitly indicated by a cue phrase. Sporleder and Lascarides (To Appear) recently investigated this is-sue in greater detail, and indeed found that such syn-thetic and organic instances appear to have impor -tant dif ferences. In our experiments with topic segmentation, we aug-mented the instance mining process to tak e account of topic segment boundaries. The intuition here is that all sentence boundaries should not be treated equally during RSR instance mining. That is, we would lik e to mak e our patterns recognize that some sentence boundaries indicate merely an orthographic break without a switch in topic, while others can separate quite distinct topics. Sometimes the latter type are mark ed by paragraph boundaries, but these are unreliable mark ers since the y may be used quite dif ferently by dif ferent authors.

Instead, we tak e the approach of adding topic seg-ment boundary mark ers to our corpus, which we can then inte grate into our RSR extraction patterns. In the case of NoRel, our assumption in our original patterns is that the presence of at least three inter -vening sentences is a suf cient heuristic for nding spans which are not joined by one of the other RSRs; we add the constraint that sentences in a NoRel re-lation be in distinct topical segments, we can in-crease model quality . Con versely , for two-sentence Cause and Contrast instances, we add the constraint that there must not be an interv ening topic segment boundary between the two sentences.
Before applying these segment-augmented pat-terns, we must add boundary mark ers to our cor -pus. While the concept of a topic segment can be dened at various granularities, we tak e a goal-oriented vie w and aim to identify segments with a mean length of approximately four sentences, rea-soning that these will be long enough to exclude some candidate NoRel instances, yet short enough to exclude a non-tri vial number of Contrasts and Cause instances. We use an automatic topic segmentation tool, LCSe g (Galle y et al., 2003) setting parame-ters so that the deri ved segments are of the approx-imate desired length. Using these parameters, LC-Seg produces topic segments with a mean length of 3.51 sentences over Giga word, as opposed to 1.54 sentences for paragraph boundaries. Using a sim-ple metric that assumes  X correct X  segment bound-aries always occur at paragraph boundaries, LCSe g achie ves 76% precision.

We rerun the instance mining step of TextRels over the segmented training corpus, after adding the segment-based constraints mentioned abo ve to our pattern set. Although our constraints reduce the overall number of instances available in the corpus, we extract for training the same number of instances per RSR as listed in Table 1 (our non-se gment-constrained training set does not use all instances in the corpus). Using the optimal parameter set-tings determined in the pre vious section, we build our models and classiers based on these segment-constrained instances.

To evaluate the classiers built on the segment-constrained instances, we can essentially follo w the same protocol as in our Parameter Optimization ex-periments. Ho we ver, we must choose whether to use a held-out test set tak en from the segment-constrained instances ( X Auto-S X ) or the same test set as used to evaluate our parameter optimization, i.e. the ( X Auto X ) test set from unse gmented training data. We decide to test on both. On the one hand, segmentation is done automatically , so it is realistic that given a  X real world X  document, we can compute segment boundaries to help our classication judg-ments. On the other hand, testing on unse gmented input allo ws us to compare more directly to the num-bers from our pre vious section. Further , for tasks which would apply RSR models outside of a single-document conte xt (e.g., for assessing coherence of a synthesized abstract), a test on unse gmented input may be more rele vant. Table 2 sho ws the results for the  X Se g X  classiers on both Auto test sets, as well as the PDTB test set.

We observ e that the performance of the classi-ers is indeed impacted by training on the segment-constrained instances. On the PDTB test data, per -formance using the segment-trained classiers im-pro ves in two of three cases, with a mean impro ve-ment of 1.2%. Ho we ver, because of the small size of this set, this mar gin is not statistically signicant.
On the automatically-e xtracted test data, the segment-trained classier is the best performer in all three cases when using the segmented test data; while the mar gin is not statistically signicant for a single classier , the overall accurate-inaccurate im-pro vement is signicant ( p &lt; . 05 ) using a Chi-squared test. On the unse gmented test data, the segment-trained classiers are best in two of three cases, but the overall accurate-inaccurate impro ve-ment does not achie ve statistical signicance. We conclude tentati vely that a classier trained on ex-amples gleaned with topic-se gment-augmented pat-terns performs more accurately than our baseline classier . Whether or not we use topic segmentation to con-strain our training instances, our patterns rely on sentence boundaries and cue phrase anchors to de-marcate the extents of the text spans which form our RSR instances. Ho we ver, an instance which matches such a pattern often contains some amount of text which is not rele vant to the relation in ques-tion. Consider: Example 3 Wall Str eet investor s, citing a drop in oil prices because weakness in the automoti ve sector , sold off shar es in GM today .

In this case, a syntactically informed analysis could be used to extract the constituents in the cause-effect relationship from within the boldf aced nomi-nal clause only , i.e. as  X a drop in oil prices X  and  X weakness in the automoti ve sector . X  Ho we ver, the output of our instance mining process simply splits the string around the cue phrase  X because of X  and extracts the entire rst and second parts of the sen-tence as the constituents. Of course, this may be for the best; in this case there is an implicit Cause rela-tionship between the NP headed by drop and the sold VP which our pattern-based rules inadv ertently cap-ture; our experiments here test whether such noise is more helpful than hurtful.

Recognizing the potential comple xity of using syntactic phenomena, we reduce the dimensions of the problem. First, we focus on single-sentence in-stances; this means we analyze only Cause and Con-trast patterns, since NoRel uses only multi-sentence patterns. Second, within the Cause and Contrast in-stances, we narro w our investigation to the most pro-ducti ve pattern of each type (in terms of training in-stances extracted), given that dif ferent syntactic phe-nomena may be in play for dif ferent patterns. The two patterns we use are  X  W 1 because W 2  X  for Cause (accounts for 54% of training instances) and  X  W 1 , but W 2  X  for Contrast (accounts for 41% of train-ing instances). Lastly , we limit the size of our train-ing set because of parsing time demands. We use the Collins parser (Collins, 1996) to parse 400,000 instances each of Cause and Contrast for our -nal results. Compared with our other models, this is approximately 43% of our total Cause instances and 13% of our total Contrast instances. For the NoRel model, we use a randomly selected subset of 400,000 instances from our training set. For all rela-tions, we use the non-se gment-constrai ned instance set as the source of these instances. 7.1 Analyzing and Classifying Syntactic Err ors To analyze the possible syntactic bases for the type of over-capturing beha vior sho wn in Example 3, we create a small development set of 100 examples each from Cause and Contrast training examples which t the criteria just mentioned. We then manually iden-tify and cate gorize any instances of over-capturing, labeling the relation-rele vant and irrele vant spans. We nd that 75% of Cause and 58% of Contrast examples contain at least some over-capturing; we observ e several common reasons for over-capturing that we characterize syntactically . For example, a matrix clause with a verb of saying should not be part of the RSR. Using automatic parses of these in-stances created by we then design syntactic ltering heuristics based on a manual examination of parse trees of several examples from our development set.
For Contrast, we nd that using the coordinat-ing conjunction (CC) analysis of but , we can use a straightforw ard rule which limits the extent of RSR spans captured to the conjuncts/children of the CC node, e.g. by capturing only the boldf aced clauses in the follo wing example: Example 4 For the past six months, mana gement has been revamping positioning and strategy , but also scaling back operations .

This heuristic successfully cuts out the irrele vant temporal relati ve clause, retaining the rele vant VPs which are being contrasted. Note that the heuris-tic is not perfect; ideally the adv erb also would be ltered here, but this is more dif cult to generalize since contentful adv erbials, e.g. str ate gically should not be ltered out.

For the because pattern, we capture the right-hand span as any text in child(ren) nodes of the be-cause IN node. We extend the left-hand span only as far as the rst phrasal (e.g. VP) or nite clause (e.g. SB AR) node abo ve the because node. Analyz-ing Example 3, the heuristic correctly captures the right-hand span; howe ver, to the left of because , the heuristic cuts too much, and misses the key noun drop . 7.2 Err or Analysis: Ev aluating the Heuristics The rst question we ask is, how well do our heuristics work in identifying the actual correct RSR extents? We evaluate this against the Penn Discourse TreeBank (PDTB), restricting ourselv es to discourse-annotated but and because sentences which match the RSR patterns which are the sub-ject of our syntactic ltering. Since the PDTB is annotated on the same corpus as Penn Tree-Bank (PTB), we separately evaluate the perfor -mance of our heuristics using gold-standard PTB parses ( X PDTB-Gold X ) versus the trees generated by Collins' parser ( X PDTB-Prs X ). We extract our test data from the PDTB data corresponding to section 23 of PTB, i.e. the standard testing section, so that the dif ference between the gold-standard and real parse trees is meaningful. Section 23 contains 60 annotated instances of but and 52 instances of be-cause which we can use for this purpose. We dene the measurement of accurac y here in terms of word-level precision/recall. That is, the set of words l-tered by our heuristics are compared to the  X correct X  Table 3: Precision/Recall/F-measure of syntactic heuristics under various data sets and settings as de-scribed in Section 7.2. words to cut, i.e. those which the annotated RSR ex-tents exclude. The results of this analysis are sho wn in Table 3.
 We performed an analysis of our heuristics on Section 24 of the PDTB. In that section, there are 74 rele vant sentences: 20 sentences with because , and 54 sentences with but . Exactly half of all sentences (37) have no problems in the application of the heuristics (7 because sentences, 30 but sentences). Among the remaining sentences, the main source of problems is that our heuristics do not always remo ve matrix clauses with verbs of saying (15 cases total, 8 of which are because sentences). For the but clauses, our heuristics remo ved the subject in 12 cases where the PDTB did not do so. Additionally , the heuristic for but sentences does not correctly identify the sec-ond conjunct in ve cases (choosing instead a paren-thetical, for instance).
 In looking at our syntactic heuristics for the Cause relationship, we see that the y indeed elimi-nate the most frequent source of discrepancies with the PDTB, namely the false inclusion of a matrix clause of saying, resulting in 15 out of 20 perfect analyses.

We also evaluate the dif ference in performance between the PDTB-Gold and PDTB-Prs perfor -mance to determine to what extent using a parser (as opposed to the Gold Standard) degrades the per -formance of our heuristics. We nd that in Sec-tion 24, 13 out of 74 sentences contain a parsing error in the rele vant aspects, but the effects are typ-ically small and result from well-kno wn parser is-sues, mainly attachment errors. As we can see in Ta-ble 3, the heuristic performance using an automatic parser degrades only slightly , and as such we can ex-pect an automatic parser to contrib ute to impro ving RSR classication (as indeed it does).
 Table 4: Classier accurac y for the Unltered (U), Syntactically Filtered (Syn), and POS (P) models described in Section 7.3, over PDTB and Auto test sets. Baseline in all cases is 50%. 7.3 Classication Ev aluation We evaluate the impact of our syntactic heuristics on classication over the Auto and PDTB test sets using the same instance set of 400,000 training instances per relation. Ho we ver, each applies dif ferent lters to the instances I before computing the frequencies F (all other parameters use the same values; these are set slightly dif ferently than the optimized val-ues discussed earlier because of the smaller train-ing sets). In addition to an Unltered baseline, we evaluate Filtered models obtained with our syntac-tic heuristics for Cause and Contrast. To pro vide an additional point of comparison, we also evaluate the Part-of-Speech based ltering heuristic described by Marcu and Echihabi, which retains only nouns and verbs. Unlik e the other lters, the POS-based lter -ing is applied to the NoRel instances as well as the Cause and Contrast instances. Table 4 summarizes the results of the classifying the PDTB and Auto test sets with these dif ferent models.

Before we examine the results, we note that the syntactic heuristic cuts a lar ge portion of training data out. In terms of the total sum of frequencies in F instances, the syntactic ltering cuts out nearly half.
With this in mind, we see that while the syntac-tic ltering achie ves slightly lower mean accurac y as compared to the Unltered baseline on the Auto test set, the pairs it does keep appear to be used more ef-ciently (the dif ferences are signicant). Ev en with this reduced training set, the syntactic heuristic im-pro ves performance in two out of three cases on the PDTB test set, including a 2.7 percent impro vement for the Cause vs NoRel classier . Ho we ver, due to the small size of the PDTB test set, none of these dif ferences is statistically signicant.

We posit that bias in the Auto set may cause this dif ference in performance across training sets; spans in the Auto set are not true arguments of the rela-tion in the PDTB sense, but nonetheless occur reg-ularly with the cue phrases used in instance mining and thus are more lik ely to be present in the test set.
Lastly , we observ e that the POS-based ltering described by M&amp;E performs uniformly poorly . We have no explanation for this at present, given that M&amp;E' s results with this lter appear promising. In this paper , we analyzed the problem of learning a model of rhetorical-semantic relations. Building on the work of Marcu and Echihabi, we rst optimized several parameters of their model, which we found to have signicant impact on classication accurac y. We then focused on the quality of the automatically-mined training examples, analyzing two techniques for data ltering. The rst technique, based on au-tomatic topic segmentation, added additional con-straints on the instance mining patterns; the sec-ond used syntactic heuristics to cut out irrele vant portions of extracted training examples. While the topic-se gmentation ltering approach achie ves sig-nicant impro vement and the best results overall, our analysis of the syntactic ltering approach indi-cates that rened heuristics and a lar ger set of parsed data can further impro ve those results. We would also lik e to experiment with combining the two ap-proaches, i.e. by applying the syntactic heuristics to an instance set extracted using topic segmenta-tion constraints. We conclude that our experiments sho w that these techniques can successfully rene RSR models and impro ve our ability to classify un-kno wn relations.

