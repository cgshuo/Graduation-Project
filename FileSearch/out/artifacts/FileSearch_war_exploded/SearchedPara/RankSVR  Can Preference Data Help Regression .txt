 In some regression applications (e.g., an automatic movie scor-ing system), a large number of ranking data is available in addi-tion to the original regression data. This paper studies whether and how the ranking data can improve the accuracy of regression task. In particular, this paper first proposes an extension of SVR (Support Vector Regression), RankSVR , which incorporates rank-ing constraints in the learning of regression function. Second, this paper proposes novel sampling methods for RankSVR, which se-lectively choose samples of ranking data for training of regres-sion functions in order to maximize the performance of RankSVR. While it is relatively easier to acquire ranking data than regression data, incorporating all the ranking data in the learning of regression doest not always generate the best output. Moreoever, adding too many ranking constraints into the regression problem substantially lengthens the training time. Our proposed sampling methods find the ranking samples that maximize the regression performance. Ex-perimental results on synthetic and real data sets show that, when the ranking data is additionally available, RankSVR significantly performs better than SVR by utilizing ranking constraints in the learning of regression, and also show that our sampling methods improve the RankSVR performance better than the random sam-pling.
 H.4 [ Data mining algorithms ]: Miscellaneous Algorithms, Performance Sampling, Ranking, Regression  X 
This work was supported by the Brain Korea 21 Project in 2010 and the Korea Research Foundation Grant funded by the Korean Government(KRF-2008-314-D00483).

Regression, estimating the target value (represented as a real number y  X  X  ) of an input object (often represented as a vector x ), has been a challenging research problem having numerous ap-plications. Sometimes regression task involves ranking data. For example, consider an automatic movie scoring system, which pre-dict a score of how much a user will like a new movie, based on the user X  X  previous records on old movies. The user X  X  records could be a list of scores on movies (e.g., scaled from 1 to 10) or preference orderings (or ranking) on some movies. Or, the user may simply list up some favorite or nonfavorite movies in his blog. Even though the problem of estimating movie scores is generally a regression prob-lem, the available data from the user includes the ranking data as and their scores). Pairwise rankings can also be extracted from a list of favorite and nonfavorite movies. In practice, judging prefer-ence orderings or ranks of movies is easier for users than judging absolute scores on objects.

Thus, this paper studies whether and how the ranking data can improve the accuracy of regression task. In this regard, we first propose RankSVR, an extension of 2-norm SVR (Support Vector Regression), which incorporates ranking constraints in the learn-ing of SVR function. From the representation theory [13], we can easily show that the prediction function of RankSVR is the lin-ear combination of two well-known max-margin methods  X  SVR and RankSVM, expressed by kernel functions on input training data. Thereby, RankSVR naturally inherits the advantages of ker-nel method. Experimental results show that, when ranking data is available, RankSVR performs better than SVR.
 Second, we propose novel sampling methods for RankSVR, i.e., ClosePair , PClosePair ,and SampleMST . While it is relatively easier to acquire ranking data than regression data, a large amount of ranking data does not always generate the best output. Moreover, adding too many ranking constraints into the regression problem substantially lengthens the training time. Our proposed sampling methods find the ranking samples that maximize the regression per-formance.

Our sampling methods are motivated by the fact that the uncer-tainty of the difference of output values for closer input vectors is less than that for farther input vectors. The uncertainty informally indicates the possible range of the difference of output values be-tween two input vectors of an ordered pair. As the distance of an ordered pair becomes large, there is much more number of feasible regression curves that satisfy the given ranking constraints. When the distance between an ordered pair is short, the number of fea-sible curves is highly restricted within the space. This concept of number of feasible curves is similar to that of the version space http://en.wikipedia.or g/wiki/Version_space concept learning. Our sampling methods are to select the sample reducing the version space.

Experimental results on synthetic and real data sets show that, our RankSVR with ranking and regression data significantly per-forms better than the SVR with regression data only, and the sam-pling methods improve the RankSVR performance better than the random sampling.

This paper is organized as follows. Section 2 overviews related work. Section 3 reviews the SVR for regression. Section 4 presents our RankSVR for regression with ranking data. Section 5 then presents the sampling methods for RankSVR. Section 6 experimen-tally evaluates our methods. Section 7 concludes our study.
Rank learning or learning to rank has gained much attention in the machine learning and information retrieval communities for the last dacade. RankSVM is known as the first successful method for rank learning applied to various applications and extensively researched since the year of 2000 [7, 8, 11, 3, 22, 24, 23]. Many other rank learning methods have been developed after RankSVM such as RankBoost [5], RankNet [2], AdaRank [18], and Listwise approaches [17].

On the other hand, there is only one previous work on resolving regression problems with ranking data [25]. In this work, Zhu and Goldberg proposed a variant of 1-norm SVR by adding a regular-ization term from ranking data into the original regression function, and have shown that the additional use of ranking data can signif-icantly reduce the regression error. However, their methods are based on the formulation of 1-norm SVM, which typically takes much longer to optimize than 2-norm SVM, as 1-norm SVMs rely on conventional linear programming for optimization while 2-norm SVMs have many scalable algorithms for optimization such as se-quential minimum optimization (SMO) or cutting plane algorithms [21, 9, 10]. Our work formulates the problem based on the 2-norm SVR and also proposes new sampling methods for further improv-ing the performance of regression.

Selective sampling, also known as active learning , has been ex-tensively researched in the machine learning community. The ac-tive learning aims at reducing the labeling effort by selecting only the most informative instances to be labeled. SVM selective sam-pling techniques have been developed and proven effective in achiev-ing a high accuracy with fewer instances [12, 15]. Such techniques are also applied to conduct effective binary relevance feedback for image retrieval [4]. There is also an effort to extend the selective sampling to ranking [1, 6, 19, 20]. However, all of these techniques are proposed within the context of binary classification or rank-ing, and they are running iteratively in an incremental way. On the other hand, our sampling methods are distinguished in two aspects. First, our sampling methods are to find a subset of ranking data that maximizes the regression performance, which is the first according to our knowledge. Second, while existing active learning requires learning of a function in each iteration in order to select samples, our sampling methods do not require learning ahead because they select samples based on characteristics of input data.
In this section we review SVR (Support Vector Regression) [16, 14]. Regression training set D consists of ( x i ,y i ) . where x i is a d -dimensional vector, and y i is a scalar real value. From D , SVR learns a regression function f ( x ) as follows. where w is the weight vector trained by SVR.  X  (  X  ) indicates a high dimensional feature map. Function f ( x ) , a linear function in the feature space  X  , becomes nonlinear in the original input space. This paper considers the standard -SVR using -insensitive loss func-tion. The primal form of -SVR is given as follows. -SVR re-gression makes the following constraint for each regression data  X  ( x where f n is f ( x n ) .

Since most of regression problems are not linearly separable, these hard constraints are relaxed by introducing non-negative slack variables  X  n and  X   X  n for each n th sample, respectively, as follows:
Based on these constraints, the goal of SVR is to find weight vector w that minimally violates hard constraints and maximizes the margin, leading to the following loss function.
 From this, we can make the primal form of SVR as follows:
Definition 1. (SVR: Primal Form) Find weight vector w such that minimizes subject to
We can derive the dual form of the SVR using the Lagrangian multipliers as follows.

Definition 2. (SVR: Dual Form) Find weight vector w such that maximize
L (  X ,  X   X  )=  X  subject to where  X  and  X   X  are [  X  X  X   X  i  X  X  X  ] T and [  X  X  X   X   X  i  X  X  X 
Once  X  and  X   X  are computed from the dual form, the regression score of a new vector x is computed as follows. where k is a kernel function and x n are support vectors whose  X  is larger than zero.
This section presents RankSVR which utilizes ranking data in regression. We first motivate RankSVR by illustrating an example.
Example 1. (SVR vs. RankSVR) Figure 1(a) and (b) draw three regression functions respectively  X  (1) the target function to be learned (i.e., black line), (2) the function learned by SVR using by RankSVR using the regression and ranking data (i.e., red line). In both figures, the SVR functions are highly disparate from the target function, since there is not enough training data. When three pairs of ranking data (Figure 1(a)) is additionally given, the RankSVR utilizes the additional information and draws the function more closely to the target function. When ten pairs of ranking data (Fig-ure 1(b)) is given, the RankSVR draws the function even more closely to the target function.

Now, let us formulate RankSVR. The basic procedure for ex-tending SVR to RankSVR is to add constraints for additional rank-ing data. Assume that R is a set of given ranking data. Each el-ement of R is an ordered pair ( i, j ) , indicating the event that i object is preferred to j th object, where i and j are members of col-lection of m objects  X  Z .Let z i and f be i th vector in target regression function respectively. Then, ( i, j ) , an element of R
That is, in fact, the same as the constraints of RankSVM for learning from ranking data [7]. Just as in RankSVM, this hard con-as follows:
We integrate the additional loss term of constraints of ranking data to the loss function of SVR, and we can obtain the loss function for RankSVR as follows: where C is an additional parameter to control the importance of the constraints of ranking data. From this, the primal form for RankSVR is given as follows:
Definition 3. (RankSVR: Primal Form) Find weight vector w that minimizes subject to
A complete derivation of the RankSVR prediction function is shown in the Appendix A. According to the representation theorem [13], we can show that the regression function of RankSVR is the linear combination of the regression terms of SVR and RankSVM as follows.

From Eq. (17) in the Appendix,
We can also derive the dual form of RankSVR. Refer to Ap-pendix B for the complete derivation.
 Definition 4. (RankSVR: Dual Form) Find weight vector w that maximizes  X  1  X  1  X   X  subject to where  X  and  X   X  is a vector which consists of  X  i and  X   X  spectively (both are m -dimensional vectors), and m i and m defined as follows: where m i and m  X  j indicate the number of ranking data when i object is preferred to other objects, and when i th object is less-preferable, respectively. presented by solid circles and each pair is connected by a solid line.
While ranking data is relatively easy to acquire in practice, it is often not practical to add a large number of ranking constraints to the optimization problem since the training time of RankSVR quadratically increases as the size of data increases. Moreover, when a large amount of ranking data is available, many of them does not always help to improve the regression accuracy. This sec-tion proposes methods for selecting ranking samples based on their usefulness on the regression prediction. With properly selected ranking data, the regression performance can be significantly im-proved without a substantial increment of training time. We pro-pose three sampling methods  X  ClosePair (Section 5.1), PClose-Pair (Section 5.2), and SampleMST (Section 5.3).
We first motivate our sampling heuristics by illustrating an ex-ample.

Example 2. (Sampling for RankSVR) Figure 2 shows regression curves learned from SVR and RankSVR. The red curves estimated by RankSVR in both Figure 2(a) and (b) are trained from three re-gression points and three pairs of ranking data. However, the curve in Figure 2(b) is drawn more closely to the target function than that in Figure 2(a). The main difference between two figures is that the data pairs in the right figure have shorter lines (distances) than that in the left figure. Intuitively, when a data pair is distant from each other, their ranking does not help much to improve the estimation of regression function because there could exist a large number of functions satisfying the ranking constraint. On the other hand, as the data pair is closer to each other, the number of regression func-tions satisfying the ranking decreases.

Let us see another example illustrating the relationship between the distance of ranking data pairs and the range of possible regres-sion functions.

Example 3. (Sampling for RankSVR for linear functions) As-sume the target function f ( x ) is ax + b , and one regression train-ing data is given by (0,1). From this, we can infer b =1 and f ( x )= ax +1 . Assume that an additional ranking data pair is presented by ( z 1 ,z 2 ) , from which we can infer a range of a .Let | z 1  X  z 2 | be d . Now, we will see how a feasible range of a is related to the distance d . From the ranking data pair, f ( z 1 ) where  X  is some positive value. From this, we can easily show that a restricted as d is small.

Based on the intuitions, our sampling methods select the input vector pairs ( i, j )  X  Z whose Euclidean distances are minimized in the feature space. The Euclidean distance of two vectors in the feature space is computed as: ||  X  ( z i )  X   X  ( z j ) || 2 2  X  ( z i )  X  ( z j )+  X  ( z j )  X  ( z j )= k ( z i , z i z is normalized, finding K pairs that minimize the kernel distance is equal to finding those that maximize the kernel function as follows. Thus, our first sampling algorithm, ClosePair , is simply described as Algorithm 1.
 Algorithm 1 ClosePair Input: data set Z Output: Kpairsof ( z i , z j ) 1: for each pair ( z i , z j )  X  Z , compute k ( z i , z j 2: return K pairs whose k ( z i , z j ) are the highest;
While applying ClosePair can substantially reduce the training time of RankSVR by reducing the size of training data, running ClosePair takes a quadratic time w.r.t. | Z | since it needs to com-pute the kernel function for every pair of data in Z . Thus, we here propose a probabilistic sampling method, PClosePair , which pro-vides a constant response time (= O (1) ) independent of the size of Z and is still more accurate th an the random sampling.

The key idea of the PClosePair is to pick a pair such that the pair generates the kernel output of top p % with q % of confidence. To provide such probabilistic guarantee, we only need to examine a constant number of pairs regardless of the size of data set.
The probability (or confidence) that at least one pair of s ran-domly chosen pairs generates the kernel output of top p % is 1 each pair is connected by a solid line. (1  X  p %) s .Let q % be the confidence. Then, Given a fixed probability p and a confidence q , the size of sample s is also fixed regardless of the size of data set. For example, accord-ing to Eq.(10), if we want to have at least one of s pairs generates the kernel output of top 5% with 95% confidence, we only need to sample 59 ( = log( . 05) / log( . 95) ) pairs randomly regardless of thesizeofdataset.

Based on the  X 59 X  sample rule, we can sample K pairs such that each of the pairs generates the kernel output of top 5% with 95% confidence as follows: (1) randomly sample 59 pairs, (2) compute the kernel functions on the pairs, (3) pick the pair whose kernel output is the highest, and (4) repeat the process K times.
However, while repeating the process K times in this method, the same pair could be chosen mu ltiple times. We avoid the dupli-cation as follows: 1. randomly sample 59  X  K pairs 2. compute the kernel functions on the pairs 3. pick the top K pairs such that their kernel outputs are the PClosePair is thus described in Algorithm 2.
 Algorithm 2 PClosePair Input: data set Z Output: Kpairsof ( z i , z j ) 1: randomly sample 59  X  K pairs 2: compute the kernel functions on the pairs 3: return top K pairs such that their kernel outputs are the highest.
While closest data pairs are helpful for improving regression ac-only from the most dense region, as the distances among data are typically the shortest in the dense region. If sampling is biased toward the most dense region, it could degrade the regression ac-curacy, because the other regions than the most dense region may not have enough training data to draw an accurate regression func-tion. Figure 3 illustrates an example of the sampling bias problem: The regression curve of (a) is more accurate than that of (b), as the ranking sample of (a) is more spreaded.

Because of the sampling bias problem, ClosePair may not work well especially when the data is highly skewed. In order to avoid the bias while sampling the closest pairs, we present the third sam-pling algorithm, SampleMST , Sampling based on MST (Mini-mum Spanning Tree). SampleMST first constructs a minimum spanning tree (MST) from the data set and randomly selects K pairs of adjacent nodes from the tree. This strategy is a reason-able compromise between finding the closest pairs and avoiding the bias, because adjacent nodes in an MST are relatively close to each other than random pairs, and the random selection of adjacent pairs from the MST will avoid sampling only from the most dense region. SampleMST is described in Algorithm 3.
 Algorithm 3 SampleMST Input: data set Z Output: Kpairsof ( z i , z j ) 1: construct an MST 2: randomly select K pairs of adjacent nodes from the MST
The time complexities of MST and thus SampleMST are O ( E log N ) where N and E are the number of nodes and edges respectively. Thus, as our experiments in Section 6 show, SampleMST runs much faster than ClosePair while the accuracy of SampleMST is better or at least as good as that of ClosePair .
We perform experiments on synthetic and real data set to exam-ine the following questions. by solid circles and each pair is connected by a solid line. ranking data pairs; Y-axis: MAE
For evaluation metric, we use the mean absolute error ( MAE ) between target outputs and regression outputs on test examples:
This section summarizes our experiments on synthetic data. First, to generate synthetic data, we randomly generated n vectors of d dimensions, some of which are used for training and the others for testing. Let X be the data set. We then randomly generated d -dimensional vector, and the target regression function f ( parametric function of w with the kernel k () as follows:
We consider three kinds of kernels for f ( x ) . 1. Linear kernel: f ( x )= w T x 2. Polynomial kernel: f ( x )=( w T x +1) p 3. RBF kernel: f ( x )=exp
Given a regression function, we can construct target output vec-p =3 and  X  =1 .

There are two types of training data: For SVR, we select l train-ing vectors from X , denoted by X svr . For RankSVR, we addition-ally select m vectors from X , and generate m C 2 pairwise orderings from it, denoted by R . We used the other vectors for testing, de-noted by T . For this experiment, we fixed the soft margin param-eters C = C =1 , since the data is noise-free and the regression performance is not sensitive to the soft margin parameters when the data is noise-free.

Figure 4 shows the mean regression accuracies when |X svr and increasing numbers of ranking data pairs with varying dimen-sions of data. The results are averaged over 30 runs. ClosePair and SampleMST show the best performance overall.
This section performs experiments on real data sets of various sizes that we acquired from the UCI machine learning repository and the Statlib from CMU as described in Table 1. We used the RBF kernel for this experiment and tuned the soft margin param-eters C, C X , and the kernel parameter  X  from 10  X  3 to 10 ported the best accuracy. The results were averaged over 30 runs. Dataset SVR Random ClosePair PClose SMST Abalone 2.1651 2.1160 2.1344 2.1202 2.0989 Bodyfat 0.2944 0.0171 0.0116 0.0171 0.0137 Concrete 0.3888 0.3855 0.3857 0.3860 0.3706 Cpusmall 8.8299 8.4818 8.7048 8.0568 8.0660 Forest fires 0.5674 0.0298 0.0298 0.0299 0.0287 Housing 6.2472 6.1232 6.1825 6.1677 6.0173 Space_ga 0.1558 0.1536 0.1476 0.1539 0.1446 Table 2: MAE (three regression points and five ranking data pairs). PClose:PClosePair; SMST:SampleMST.
 Dataset Random ClosePair PClosePair SampleMST Abalone 1.01 711.859 18.939 15.218 Bodyfat 0.018 3.162 1.871 0.233 Concrete 0.604 43.258 5.156 1.371 Cpusmall 1.061 2743.776 35.327 50.447 Forest fires 0.299 11.416 3.2 1.025 Housing 0.248 10.743 3.012 0.898 Space_ga 0.828 388.654 14.199 8.378 Table 3: Sampling time (in seconds). The time for sampling five pairs six times
Table 2 reports the MAE of each method trained from three re-gression points and five ranking data pairs selected by each sam-pling method. Figure 5 draws the MAEs of each method on in-creasing numbers of ranking pairs. SampleMST have shown the best performance overall and ClosePair also performs as well on some data sets. Table 3 shows the sampling time of each method, and SampleMST runs much faster than ClosePair ,as ClosePair takes quadratic time w.r.t. the number of data points.
This paper proposes RankSVR, a method that learns a regression function from both regression and ranking data. This method is useful for the applications where regression labeling is hard while ranking data is relatively easy to acquire. We also propose several sampling methods for ranking data that maximize the performance of RankSVR. The sampling methods are based on the idea of min-imizing the number of feasible curves that satisfy the ranking con-straints. Extensive experiments show that RankSVR improves the regression accuracy by utilizing the ranking constraints in learn-ing regression functions, and the proposed sampling methods per-form better than the random sampling by selecting more informa-tive samples that can further improve the regression performance.
To resolve the problem, let us introduce  X  ij , the Lagrangian mul-tiplier for the constraint of ranking data ( i, j ) ,and  X  grangian multiplier for  X  ij  X  0 . Then, we obtain Lagrangian func-tion L rank for the error function.

L where L reg is the Lagrangian for SVR.

Now we set the derivatives of the Lagrangian with respect to w , b ,  X  n ,  X   X  n ,and  X  ij to zero, giving
By using the above equation of weight vector w , f ( x ) becomes
For simplification, let us define r i and r  X  i as follows. indicates the degree of the preference for z i over other j = i ), whereas r  X  j indicates the degree of the non-preference for z over other z i (where i = j ). From the definition of r i Eq. (16) becomes f ( x )=
Using kernel function k , Eq. (17) becomes f ( x )=
To derive the dual form, we construct the Lagrangian dual func-tion in terms of dual variables by removing primal variables of parts.
 where L 3 can be reformulated as follows: Using Eq. (11) and Eq. (12) that w is z ) ,and And, L 2 + L 3 becomes
L 2 + L 3 =  X  w 2  X 
By applying Eq. (13), Eq. (14) and Eq. (15), L 2 + L 3 becomes
L 2 + L 3 =  X  w 2  X  Then, L 1 + L 2 + L 3 is L 1 + L 2 + L 3 =  X 
Note that w is rewritten in terms of  X  i and  X   X  i as follows:
Using Eq. (19), w 2 is expressed in terms of dual variables as follows:
Eq. (20) is expressed in terms of kernel function.
From this, we can obtain the RankSVR dual from of Definition 4 by eliminating the remaining primal variables w .
 [1] K. Brinker. Active learning of label ranking functions. In [2] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, [3] Y.Cao,J.Xu,T.-Y.Liu,H.Li,Y.Huang,andH.-W.Hon.
 [4] E. Chang and S. Tong. Support vector machine active [5] Y. Freund, R. Iyer, R. E. Schapire, and Y. Singer. An efficient [6] J. Furnkranz and E. Hullermeier. Pairwise preference [7] R. Herbrich, T. Graepel, and K. Obermayer, editors. Large [8] T. Joachims. Optimizing search engines using clickthrough [9] T. Joachims. Training linear svms in linear time. In Proc. [10] J. Platt. Fast training of support vector machines using [11] F. Radlinski and T. Joachims. Query chains: Learning to rank [12] G. Schohn and D. Cohn. Less is more: Active learning with [13] B. Scholkopf, R. Herbrich, A. J. Smola, and R. C. [14] A. J. Smola and B. Scholkopf. A tutorial on support vector [15] S. Tong and D. Koller. Support vector machine active [16] V. Vapnik. Statistical Learning Theory . John Wiley and [17] F. Xia, T. Liu, J. Wang, W. Zhang, and H. Li. Listwise [18] J. Xu and H. Li. Adarank: A boosting algorithm for [19] H. Yu. SVM selective sampling for ranking with application [20] H. Yu. Selective sampling techniques for feedback-based [21] H. Yu, C. Hsieh, K. Chang, and C. Lin. Large linear [22] H. Yu, S.-W. Hwang, and K. C.-C. Chang. Enabling soft [23] H. Yu, T. Kim, J. Oh, I. Ko, S. Kim, and W. Han. Enabling [24] H. Yu, Y. Kim, and S. Hwang. RV-SVM: An efficient method [25] X. Zhu and A. Goldberg. Kernel regression with order
