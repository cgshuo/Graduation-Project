 Recently, the forum system of Web 2.0 is more and more popular and interesting. People can share or seek any information from any place in the world. One of the most popular and useful forum system is Community-based Question-Answering (CQA) portals. For example, the typical CQA portals such as Yahoo! Answers 1 in English, online forums. In this paper, We choose Yahoo!Answers where approximately over one hundred million resolved questions in English for our research . 
On account of the increasing progressively increasing questions and answers, user works in CQA service are to improve their functionalities like question ranking, ques-tion search, and question recommendation. However, the output of the prior work do not consider the expertise (or authority) of users, i.e., for question ranking, the user may be an amateur in this area and the outputs of the search engine are so hard to understand results are too easy to contain rich information for them. Thus, in this paper, we con-cerned with how to rank questions by their difficulty levels . 
Moreover, the work of expert finding is highly associated with our task. The expert can be considered a user who is familiar with the particular topic or category, and the expert can solve most of questions in the particular category. In short, the difference between an easy question and a hard question is the ratio of non-experts and experts participating in this question respectively, and we can separate the task as the two parts: expert finding and question ranking. In this paper, the  X  X xpert X  we want to find is dif-swer-hard-question-frequently expert . That is to say, we want to find the user that not only is an expert but also focuses on answering hard questions more than other users. 
However, the fact in the real world is that the experts not only answer hard questions easy or hard. We observe a phenomenon called the knowledge gap existing in the CQA service. The knowledge gap is a phenomenon which associates with the expertise of users. There are two main principles on the knowledge gap. First, the non-experts have no ability to answer the question that over their knowledge. Second, the experts have paper, we investigate the knowledge gap how to effect a CQA service and we propose a knowledge-gap-based difficulty rank (KG-DRank) algorithm calculating in probability to tackle the task that determine the question is easy or hard. 
The rest of this paper is structured as follows. In Section 2, we briefly discuss related approach called KG-DRank algorithm to solve this task. Next, experimental results are reported in Section 4. Section 5 concludes this work and our future work. There are several researches focusing on how to find expert by modeling and compu-ting user-user graph such as [6,11,14]. McCallum [11] et al. utilize the user-user graph to find the experts for particular topics and then Zhang et al. [14] use the network-based ranking algorithms HITS [9] and PageRank [2] to identify users with high expertise. Their results show high correlation between link-based metrics and the answer quality. finding the best answers for a given question. Then [6,7] use HITS algorithm to find the authority of users in question/answering network that an asker is linked to an answerer if the answerer replies the question that as ker has been asked. The experiments show that the obtained authority score is better than simply counting the number of answers an answerer has given. Although the results demonstrate that HITS algorithm is a good questions and replying answers may reduce the performance. Unlike [6] and [14], Zhou content-based probability model to find the experts for the particular question. 
Question ranking is to rank the results for purpose of browsing-time decreasing, the typical approach is using the information of Q&amp;A content such as best ratings such as [3].Jeon et al. [8] addressed the answer quality problem in a community QA portal and expands on [8] by exploring a larger range of features including both structural, textual, significantly. Bian et al. [3] proposed to solve collaborative QA by considering both answer quality and relevance and also used content-based quality answers without considering user expertise. Because of th e answer quality problem on [3,8,13], Su-ryanoto et al. [12] propose a quality-aware framework that considers both answer re-levance and answer quality derived from answer features and expertise of answerers. 3.1 Problem Definition probability model. Given a question q, the probability of the question q being hard is estimated as follows:  X   X   X   X  is the probability of a question generated by users, and the two variables are the question q by  X   X   X  |  X   X  . 3.2 The KG-DRank Algorithm The progress of our system only has iterations of the two steps: the expert finding and the difficulty degree detecting. The expert finding concludes the expertise model and reinforcement model, while the difficulty degree detecting concludes the difficulty degree model and question ranking model. 3.2.1 Expertise Model We cite the prior work [6] as our expertise model. 3.2.2 Difficulty Degree Model taking the summation across the users of the two parts on the question: how much hard in answering. In asking, we capture an estimate of  X  X  X   X   X   X  X  X  by means of three types of architecture of CQA service, and it means how much expertise asker has in asking. It can be expressed as:  X  3.2.1 referred and we will discuss the change of the next value later in this section. malize it by the square root such as the following:  X  cording to above estimation, the estimation of  X   X   X  |  X   X  is defined by: to compute  X   X   X  |  X   X  . It is reasonable that the hard questions must be using more words to ask and the easy questions instead. For the same asker, the question which the asker uses more words to describe has stronger probability of being hard question than the other questions the same asker asked. 
In answering, we also divided it into two parts: the best answerer and the other rep-liers, so how much difficulty given by the answerers can be obtained as follows: 3.2.3 Reinforcement Model The main idea of reinforcement model is that the more hard questions user participates in, the more expertise user obtains and the expertise can be divided into the asking and the answering. In this model, we define two types of association between difficulty and users. One of them is defined as : Where  X |  X  X  is the number of questions that user has asked, while  X |  X  X  is the number of questions that user has replied. And  X |  X  | X  X  represents the number of hard questions that user has asked, while  X |  X  | X  X  represents the number of hard questions that user probability model. The other association between difficulty degree and user is that how much ratio of hard questions user participates in his/her all participating questions, and it can be expressed as: Where |  X  | is the total number of hard questions in category C, and we called this eq-uation as global difficulty probability module. Therefore, the global KG-DRank ( GKG-DRank ) of  X   X  X  X  X   X   X  |  X  X   X  and  X   X  X  X   X   X  |  X  X   X  is represented as:  X  ity that base HITS algorithm computes in the prior section, and  X 0,1 X  X  X  . The other way is local KG-DRank ( LKG-DRank ) is represented as : We crawled 40,000 resolved questions from Yahoo! Answers service in English for our experiments, and these questions from five categories respectively such as Martial arts , Cycling , Health , Pets , and Software, and there are 8,000 questions among each cate-gory. For each category, ten colleague students volunteer to be our assessors and each assessor also has interesting and basic knowledge at one or more categories. In general, we let assessors choose questions from each category randomly and label the question people consider the question as easy or hard is subjective, we do our best to label it and omit the ambiguous questions, and Table 1 is our answer set for each category. 4.1 Baseline approach: 1. Eigenrumour [4] algorithm: the linked based algorithm for ranking blogs, and the 3. Base Hits: there are two steps in this appr oach. First, calculate the auth score and 4. #Words: the number of words in the question included answers 5. #Answerers: the number of answerers in the question 4.2 Evaluation Metrics In order to evaluate the performance of our system, we use the four evaluation metrics such as the precision, the recall, the F-measure. In general, we set a variable  X  X  X  as a threshold, i.e., if the rank of the question is more than N, the question is regarded as a  X  X ard question X  and  X  X asy question X  separately, and both are measured when the ex-periment threshold N is set to maximize the F-measure. We will use these evaluating strategies to show how the performance of our methodology. 4.3 Comparison to Baseline comparison of methods for each category is represented later. 4.3.1 F-measure of Easy Question and Hard Question approach with the LKG-DRank exhibits the best performance among all the other methods in detecting the hard question. The performance of the method we presented with the GKG-DRank is similar to the Base probability and the two methods show the second to the best performance. The #Answerers showed the worst performance and it Although EigenRumor also utilize the relationship between users and questions, the method shows poor performance due to the phenomenon of the experts can answer easy questions. F-measure of hard question. Compare with Table 3, the performance with all methods are increasing, but our approach with LKG-DRank also exhibits the best performance for all categories. It is reasonable that the part of the expert computed in KGS is higher than the part of the non-expert among all categories. In Table 4, the method that the performance is most increasing compared to Table 3 is #Answerers. It represents that the easy question is identified easier than the hard question via #Answerers. 4.3.2 The Examples of the Outputs Compared with the Basic Approaches In order to compare the effectiveness of our approach, we give the particular query for searching questions in the particular category with cosine similarity. For example, we give the query  X  X arate X  in the category Martial arts and there are 358 questions in the output. For the purpose of more showing the effectiveness of our approach, we list the other questions and rank the questions with different methods and the outputs is shown in Table 5 . The numbers in the brackets in Table 5 represent the corresponding methods. For example, the number 2,768 in No1 question represents there are 2,768 words in the question including asking and answering and the number 161 represents the order of the question is 161 of 358 by ranking with #words. The top three questions in Table 5 are detected as hard questions by our approach and the order of the rank is 36, 39, and order of the same questions is last by the other two methods due to the less words and number of answers in the questions. On the contrary, the last two questions in Table 5 questions. But the two easy questions attract some non-experts to answer it and it bring about the high order in the methods with number of words and answers. In addition, the question of No2 and the question of No4 are similar in the particular viewpoint, how-ever; the different type of asking may make question be easy or hard. The question of No4 is the popular question of the amateur, but the question of No2 repeats asking the same thing by the different viewpoints and it would make this question hard. The two examples can prove the two similar questions may not be same degree of difficulty. In this paper we defined the problem of detecting the question is easy or hard in YA and addressed it as a probability model and then utilize the phenomenon called knowledge gap in CQA service to solve this task. The contributions of this paper include: 1. We observe the unreasonable relationship between users from knowledge gap 2. We present the approach called KG-DRank algorithm combining the relationship 3. We demonstrate that the phenomenon of knowledge gap in our experiments and 
