
How closely related are two nodes in a graph? How to compute this score quickly, on huge, disk-resident, real graphs? Random walk with restart (RWR) provides a good relevance score between two nodes in a weighted graph, and it has been successfully used in numerous settings, like automatic captioning of images, generalizations to the  X  X onnection subgraphs X , personalized PageRank, and many more. However, the straightforward implementations of RWR do not scale for large graphs, requiring either quadratic space and cubic pre-computation time, or slow response time on queries.

We propose fast solutions to this problem. The heart of our approach is to exploit two important properties shared by many real graphs: (a) linear correlations and (b) block-wise, community-like structure. We exploit the linearity b y using low-rank matrix approximation, and the community structure by graph partitioning, followed by the Sherman-Morrison lemma for matrix inversion. Experimental results on the Corel image and the DBLP dabasets demonstrate that our proposed methods achieve significant savings over the straightforward implementations: they can save several orders of magnitude in pre-computation and storage cost, and they achieve up to 150x speed up with 90%+ quality preservation.
Defining the relevance score between two nodes is one of the fundamental building blocks in graph mining. One very successful technique is based on random walk with restart (RWR). RWR has been receiving increasing interest from both the application and the theoretical point of view (see Section (5) for detailed review). An important researc h challenge is its speed. especially for large graphs.
Mathematically, RWR requires a matrix inversion. There are two straightforward solutions, none of which is scal-able for large graphs: The first one is to pre-compute and store the inversion of a matrix ( X  PreCompute  X  method); the second one is to compute the matrix inversion on the fly, say, through power iteration ( X  OnTheFly  X  method). The first method is fast on query time, but prohibitive on space (quadratic on the number of nodes on the graph), while the second is slow on query time.
 Here we propose a novel solution to this challenge. Our approach, B LIN, takes the advantage of two prop-erties shared by many real graphs: (a) the block-wise, community-like structure, and (b) the linear correlations across rows and columns of the adjacency matrix. The pro-posed method carefully balances the off-line pre-processi ng cost (both the CPU cost and the storage cost), with the re-sponse quality (with respect to both the accuracy and the response time). Compared to PreCompute , it only requires pre-computing and storing the low-rank approximation of a large but sparse matrix, and the inversion of some small size matrices. Compared with OnTheFly , it only need a few matrix-vector multiplication operations in on-line respo nse process.

The main contributions of the paper are as follows:  X  A novel, fast, and practical solution (B LIN and its  X  Theoretical justification and analysis, giving an error  X  Extensive experiments on several typical applications,
The proposed method is operational, with careful de-sign and numerous optimizations. Our experimental results show that, in general, it preserves 90%+ quality, while (a) saves several orders of magnitude of pre-computation and storage cost over PreCompute , and (b) it achieves up to 150x speedup on query time over OnTheFly . Figure (1) shows some results for the auto-captioning application as in [22].

The rest of the paper is organized as follows: the pro-posed method is presented in Section 2; the justification and the analysis are provided in Section 3. The experimental re-sults are presented in Section 4. The related work is given in Section 5. Finally, we conclude the paper in Section 6. and D Automatic image captioning. The proposed method and OnTheFly output the same result within 0.04 seconds and 4.5 seconds, respectively.
 Table 1 gives a list of symbols used in this paper.
Random walk with restart is defined as equation (1) [22]: consider a random particle that starts from node i . The par-ticle iteratively transmits to its neighborhood with the pr ob-ability that is proportional to their edge weights. Also at each step, it has some probability c to return to the node The relevance score of node j wrt node i is defined as the steady-state probability r at node j [22].
Equation (1) defines a linear system problem, where ~r is determined by:
The relevance score defined by RWR has many good properties: compared with those pair-wise metrics, it can capture the global structure of the graph [14]; compared with those traditional graph distances (such as shortest pa th, maximum flow etc), it can capture the multi-facet relation-ship between two nodes [26].

One of the most widely used ways to solve random walk with restart is the iterative method, iterating the equatio n (1) until convergence, that is, until the L estimates of ~r ation step m is reached. In the paper, we refer it as OnThe-Fly method. OnTheFly does not require pre-computation and additional storage cost. Its on-line response time is li n-might be undesirable when (near) real-time response is a tion of [25] is that the distribution of ~r Based on this observation, combined with the factor that many real graphs has block-wise/community structure, the authors in [25] proposed performing RWR only on the par-tition that contains the starting point i (method Blk ). How-ever, for all data points outside the partition, r set 0 . In other words, Blk outputs a local estimation of
On the other hand, it can be seen from equation (2) that the system matrix Q defines all the steady-state probabil-compute and store Q  X  1 , we can get ~r this method as PreCompute ). However, pre-computing and
On the other hand, linear correlations exist in many real graphs, which means that we can approximate  X  W by low-rank approximation. This property allows us to approximate Q mation of ~r However, due to the low rank approximation, such kind of estimation is conducted at a coarse resolution.
In summary, the skewed distribution of ~r global/coarse resolution estimation. In this paper, we com -bine these two properties in a unified manner. The proposed algorithm, B LIN is shown in table (2). 2.3 Normalization on W B LIN takes the normalized matrix  X  W as the input. There are several ways to normalize the weighted ma-trix W . The most natural way might be by row nor-malization [22]. Complementarily, the authors in [27] propose using the normalized graph Lapalician (  X  W = D izing the famous nodes before row normalization for social network.
Input: The normalized weighted matrix  X  W and the
Output: The ranking vector ~r
Pre-Computational Stage(Off-Line): p1. Partition the graph into k partitions by METIS [19]; p2. Decompose  X  W into two matrices:  X  W =  X  W p3. Let  X  W p4. Compute and store Q  X  1 p5. Do low-rank approximation for  X  W p6. Define Q  X  1
Query Stage (On-Line): q1. Output ~r
It should be pointed out that all the above normalization methods can be fitted into the proposed B LIN. However, in this paper, we will focus on the normalized graph Lapla- X  For real applications, these normalization methods of- X  Unlike the other two methods, normalized graph  X  The normalized graph Laplacian is symmetric, and it  X  It might be difficult to develop an error bound for 2.4 Partition number k : case study
The partition number k balances the complexity of  X  W and  X  W periment section. Here, we investigate two extreme cases of k .

First, if k = 1 , we have  X  W B LIN is just equivalent to the PreCompute method.
On the other hand, if k = n , we have  X  W  X  W simplified version of B LIN as in table(3). We refer it as NB LIN.

One natural choice to do low-rank approximation on  X  W where each column of U is the eigen-vector of  X  W is a diagonal matrix, whose diagonal elements are eigen-values of  X  W
The advantage of eigen-value decomposition is that it is  X  X ptimal X  in terms of reconstruction error. Also, since V = U
T in this situation, we can save 50% storage cost. How-ever, one potential problem is that it might lose the spar-sity of original matrix  X  W eigen-value decomposition itself might be time-consuming .
To address this issue, in this paper, we also propose the following heuristic to do low-rank approximation as in ta-ble (4). Its basic idea is that, firstly, construct U titioning  X  W sub-space spanned by the columns of U as the low-rank ap-proximation. Table 4. Low Rank Approximation by Partition
Input: The cross-partition matrix  X  W
Output: Low rank approximation of  X  W 1. Partition  X  W 2. Construct an n  X  t matrix U . The i th column of U is 3. Compute S = ( U T U )  X  1 ; 4. Compute V = U T  X  W
Here, we present a brief proof of the proposed algorithms 3.1.1 B LIN Lemma 1 If  X  W =  X  W actly the same result as PreCompute .
 Proof: Since  X  W equation (3) and (4), we have
Then, based on the Sherman-Morrison lemma [23], we have: ( I  X  c  X  W )  X  1 = ( I  X  c  X  W 1  X  c USV )  X  1 which completes the proof of Lemma 1. It can be seen that the only approximation of B LIN comes from the low-rank approximation for  X  W
We can also interpret B LIN from the perspective of la-tent semantic/concept space. By low-rank approximation on  X  W S . Furthermore, if we treat the original  X  W as an n  X  n node space, U and V actually define the relationship between these two spaces ( U for node-concept relationship and V for concept-node relationship). Thus, it can be seen that, instead of doing random walk with restart on the original whole node space, B LIN decomposes it into the following simple steps: (1) Doing RWR within the partition that contains the start-(2) Jumping from node-space to latent concept space (3) Doing RWR within the latent concept space (multiply (4) Jumping back to the node space(multiply the result of (5) Doing RWR within each partition until convergence 3.1.2 NB LIN Lemma 2 If  X  W = USV holds, NB LIN outputs exactly the same result as PreCompute .
 Proof: Taking  X  W 1, we directly complete the proof of Lemma 2.
In this section, we make a brief analysis for the proposed algorithms in terms of computational and storage cost. For the limited space, we only provide the result for B LIN. 3.2.1 On-line computational cost It is not hard to see that, at the on-line query stage of B LIN (table 2, step q1), we only need a few matrix-vector mul-tiplication operations as shown in equation (7). Therefore , B LIN is capable of meeting the (near) real-time response requirement. 3.2.2 Pre-computational cost The main off-line computational cost of the proposed algo-rithm consists of the following parts: (1) partitioning the whole graph; (2) inversion of each I  X  c  X  W (3) low-rank approximation on  X  W (4) inversion of ( S  X  1  X  VQ  X  1
Thus, instead of solving the inversion of the original n  X  n matrix, B LIN1) inverses k + 1 small matrices ( Q  X  i=1,...,k, and  X   X  ); 2) computes a low-rank approximation of a sparse n  X  n matrix (  X  W graph. 3.2.3 Pre-storage cost In terms of storage cost, we have to store k + 1 small matri-ces ( Q  X  1 one t  X  n matrix ( V ). Moreover, we can further save the storage cost as shown in the following:  X  An observation from all our experiments is that many  X  The normalized graph Laplacian is symmetric, which
Developing an error bound for the general case of the proposed methods is difficult. However, for NB LIN (table 3), we have the following lemma: Lemma 3 Let ~r and  X  ~r be the ranking vectors 6 by PreCom-pute and by NB LIN, respectively. If NB LIN takes eigen-value decomposition as low-rank approximation, k ~r  X   X  ~r k 2  X  (1  X  c ) eigen-value of  X  W .
 Proof: Taking the full eigen-value decomposition for  X  W where  X  corresponding eigen-vector of  X  W , respectively. U = [ u 1 , ...u n ]
Note u
By Lemma 2, we have: Thus, we have k ~r  X   X  ~r k 2 = k (1  X  c ) which completes the proof of Lemma 4. 4.1.1 Datasets  X  CoIR
This dataset contains 5,000 images. The images are cate-gorized into 50 groups, such as beach, bird, mountain, jew-elry, sunset, etc. Each of the categories contains 100 image s of essentially the same content, which serve as the ground truth. This is a widely used dataset for image retrieval. Two kinds of low-level features are used, including color mo-ment and pyramid wavelet texture feature. We use exactly the same method as in [14] to construct the weighted graph matrix W , which contains 5 , 000 nodes and  X  774 K edges  X  CoMMG
This dataset is used in [22], which contains around 7,000 captioned images, each with about 4 captioned terms. There are in total 160 terms for captioning. In our experiments, W is constructed exactly as in [22], which contains 54 , 200 nodes and  X  354 K edges.  X  AP
The author-paper information of DBLP dataset [4] is used to construct the weighted graph W as in equation ( ?? ): every author is denoted as a node in W , and the edge weight is the number of co-authored papers between the corre-sponding two authors. On the whole, there are  X  315 K nodes and  X  1 , 834 K non-zero edges in W .
 All the above datasets are summarized in table(5): 4.1.2 Applications As mentioned before, many applications can be built upon random walk with restart. In this paper, we test the follow-ing applications:  X  Center-piece subgraph discovery (CePs) [26]  X  Content based image retrieval (CBIR) [14]  X  Cross-modal correlation discovery (CMCD), including  X  neighborhood formulation (NF) [25]
The typical datasets for these applications in the past years are summarized in table(4.1.2)
Table 6. Summary of typical applications with different datasets 4.1.3 Parameter Setting The proposed methods are compared with OnTheFly , Pre-Compute and Blk . All these methods share 3 parameters: c m and  X  1 . we use the same parameters for CBIR as [14], that is c = 0 . 95 , m = 50 and  X  tions, we use the same setting as [22] for simplicity, that is c = 0 . 9 , m = 80 and  X  1 = 10  X  8 .

For B LIN and NB LIN, we take  X  sify Q evaluate different choices for the remaining parameters. F or clarification, in the following experiments, B LIN is further referred as B LIN( k , t , Eig/Part), where k is the number of partition, t is the target rank of the low-rank approxima-tion, and  X  X ig/Part X  denotes the specific method for doing low-rank approximation  X   X  X ig X  for eigen-value decompo-sition and  X  X art X  for partition-based low-rank approxima-tion. Similarly, NB LIN is further referred as NB LIN( Eig/Part), and Blk is further referred as Blk ( k ).
For the datasets with groundtruth (CoIR and CoMMG ), we use the relative accuracy RelAcu as the evaluation cri-terion: where d Acu and Acu are the accuracy values by the evalu-ated method and by PreCompute , respectively.
 Another evaluation criterion is RelScore , where d tScr and tScr are the total relevance scores captured by the evaluated method and by PreCompute , respectively.
All the experiments are performed on the same machine with 3.2GHz CPU and 2GB memory. 100 images are randomly selected from the original dataset as the query images and the precision vs. scope is reported. The user feedback process is simulated as fol-lows. In each round of relevance feedback (RF), 5 images pointed out that the initial retrieval result is equivalent to that for neighborhood formulation (NF). RelAcu is evalu-ated on the first 20 retrieved images, that is, the precision within the first 20 retrieved images. In figure (2), the result s are evaluated from three perspectives: accuracy vs. query time (QT), accuracy vs. pre-computational time (PT) and accuracy vs. pre-storage cost (PS). In the figure, the QT, PT and PS costs are in log-scale. Note that pre-computational time and storage cost are the same for both initial retrieval and relevance feedback, therefore, we only report accuracy vs. pre-computational time and accuracy vs. pre-storage cost for initial retrieval.
 It can be seen that in all the figures, B LIN and NB LIN always lie in the upper-left zone, which indi-cates that the proposed methods achieve a good balance between on-line response quality and off-line processing cost. Both B LIN and NB LIN 1) achieve about one order of magnitude speedup (compared with OnTheFly ); and 2) save one order of magnitude on pre-computational and storage cost. For example, B LIN( 50 , 300 , Eig) pre-serves 95%+ accuracy for both initial retrieval and rel-evance feedback, while it 1) achieves 32x speedup for on-line response (0.09Sec/2.91Sec), compared with On-TheFly ; and 2)save 8x on storage (21M/180M) and 161x on pre-computational cost (90Sec/14,500Sec), compared with PreCompute . NB LIN(600,Eig) preserves 93%+ ac-curacy for both initial retrieval and relevance feedback, while it 1) achieves 97x speedup for on-line response (0.03Sec/2.91Sec), compared with OnTheFly ; and 2)saves 10x on storage(17M/180M) and 48x on pre-computational cost (303Sec/14,500Sec), compared with PreCompute . 7 . For this dataset, we only compare NB LIN with On-TheFly and PreCompute . The results are shown in fig-ures, which means that NB LIN achieves a good bal-ance between on-line quality and off-line processing cost. For example, NB LIN(100, Eig) preserves 91.3% quality, while it 1) achieves 154x speedup for on-line response (0.029/4.50Sec), compared with OnTheFly ; 2) saves 868x on storage (281/243,900M) and 479x on pre-computational cost (46/21,951Sec), compared with PreCompute .
This dataset is used to evaluate Ceps as in [26]. B LIN is used to generate 1000 candidates, which are further fed center-piece subgraphs. We fix the number of query nodes to be 3 and the size of the subgraph to be 20 . RelScore is measured by  X  X mportant Node Score X  as in [26]. The result is shown in figure (4).

Again, B LIN lies in the upper-left zone in all the fig-ures, which means that B LIN achieves a good balance between on-line quality and off-line processing cost. For example, B LIN(100, 4000, Part) preserves 98.9% qual-ity, while it 1) achieves 27x speedup for on-line response (9.45/258.2Sec), compared with OnTheFly ; 2) saves 2264x on storage (269/609,020M) and 214x on pre-computational cost (8.7/1875Hour), compared with PreCompute .
In this Section, we briefly review related work, which can be categorized into three groups: 1) random walk re-lated methods; 2) graph partitioning methods and 3) the methods for low-rank approximation.

Random walk related methods. There are sev-eral methods similar to RWR, including electricity-based method [28], graph-based Semi-supervised learn-usually requires the inversion of a matrix which is often di-agonal dominant and of big size. Other methods sharing this requirement include regularized regression, Gaussian pro -cess regression [24], and so on. Existing fast solution for RWR include Hub-vector decomposition based [16]; block structure based [18] [25]; fingerprint based [9], and so on. Many applications take random walk and related methods as the building block, including PageRank [21], personalized PageRank [13], SimRank [15], neighborhood formulation in bipartite graphs [25], content-based image retrieval [1 4], cross modal correlation discovery [22], the BANKS sys-tem [2], ObjectRank [3], RalationalRank [10], and so on.
Graph partition and clustering. Several algorithms have been proposed for graph partition and clustering, e.g. METIS [19], spectral clustering [20], flow simulation [8], co-clustering [6], and the betweenness based method [11]. It should be pointed out that the proposed method is orthog-onal to the partition method.

Low-rank approximation: One of the widely used techniques is singular vector decomposition (SVD) [12], tent semantic index (LSI) [5], principle component analysi s (PCA) [17], and so on. For symmetric matrices, a comple-mentary technique is the eigen-value decomposition [12]. More recently, CUR decomposition has been proposed for sparse matrices [1].
In this paper, we propose a fast solution for computing the random walk with restart. The main contributions of the paper are as follows:  X  The design of B LIN and its derivative, NB LIN.  X  The proof of an error bound for NB LIN. To our  X  Extensive experiments are performed on several real
Sherman-Morrison Lemma [23]: if X  X  1 exists, then: where  X   X  = ( S  X  1  X  VX  X  1 U )  X  1
