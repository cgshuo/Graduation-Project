 Attribution of climate change to causal factors has been based pre-dominantly on simulations using physical climate models, which have inherent limitations in describing such a complex and chaotic system. We propose an alternative, data centric, approach that re-lies on actual measurements of climate observations and human and natural forcing factors. Specifically, we develop a novel method to infer causality from spatial-temporal data, as well as a procedure to incorporate extreme value modeling into our method in order to address the attribution of extreme climate events, such as heat-waves. Our experimental results on a real world dataset indicate that changes in temperature are not solely accounted for by solar radiance, but attributed more significantly to CO2 and other green-house gases. Combined with extreme value modeling, we also show that there has been a significant increase in the intensity of extreme temperatures, and that such changes in extreme tempera-ture are also attributable to greenhouse gases. These preliminary results suggest that our approach can offer a useful alternative to the simulation-based approach to climate modeling and attribution, and provide valuable insights from a fresh perspective.
 I.2.6 [ Artificial Intelligence ]: Learning Algorithms Climate modeling, Climate change attribution, Spatio-temporal mod-eling, Causal modeling Granger causality
Climate change is one of the most critical socio-technological issues mankind faces in the present century [1]. Though it is re-garded primarily as an energy related problem, computing technol-ogy will play an important role in devising potential solutions in a variety of ways. One that particularly interests us is that of applying data modeling to the climate data in order to better understand and quantify the causal effects of various parameters involved. There is a clear need for an effective methodology of data modeling that will allow us to analyze the large amount of time series data on the climate and climate forcing agents and draw conclusions on how these factors affect each other and which parameters are to be con-troled for the best environmental results.

It is well recognized that climate is a chaotic system, and hence it is difficult to reliably model it as a whole. Nonetheless, there are reasons to believe we can meaningfully characterize causal or statistical relationships that exist among parameters of interest, and make assertions about the presence or absence of such relationships and quantify them. (Recently, there have been a number of articles published in prominent scientific journals that carry out studies of this type. [11, 15, 5]) Fundamentally, our goal is to focus on  X  X li-mate change detection and attribution X  (i.e., identification, quantifi-cation and prioritization of the effects of controllable forcing fac-tors on climate), rather than on  X  X limate projection X  (i.e., prediction of the evolution of the global climate system in the next decades).
The climate system comprises complex relationships between a large number of variables. Hence, the factors of interest involve many dimensions, including measurements of climate parameters, anthropogenic factors, and regional factors [2]. Fortunately, many of these data are publicly available in forms that are well suited for data modeling  X  e.g. Climate Research Unit (CRU) dataset, NOAA NESDIS data set, Carbon Dioxide Info analysis Center (CDIAC).
Considerable amount of scientific investigations have been car-ried out to date in the community of climate change study, to ad-dress these very questions [11]. The dominant existing approach in the community, however, is based on forward simulation with cli-mate models built using fundamental physical laws. These models are used to estimate the expected space-time pattern (fingerprints) of the response to individual anthropogenic or natural forcing fac-tors on the observed climate. The task of detection and attribution is then performed by estimating the factors by which these model-simulated patterns have to be scaled to be consistent with the ob-served change (optimal fingerprinting), and by applying standard statistical significance tests for isolated hypotheses on the value of the estimated factors. As these existing approaches rely heavily on the employed climate models, they are subject to the models X  shortcomings (e.g. models X  uncertainties, simplifications, and dis-crepancies from observed data).

Given the understanding of the existing approaches and their lim-itations, what we propose is an alternative approach based on data modeling, with special attention paid to address unique character-istics of climate modeling. First has to do with our emphasis on attribution, rather than forecasting, of climate change, motivating us to look to techniques that aim at modeling causality. Secondly, the climate data are spatio-temporal in nature, where both the cli-mate observations and forcings are associated with specific points in space and time, and these aspects will be critical for conducting informed analyses on climate change over time and across regions over the globe. Thirdly, there is a particular interest in modeling the extreme climate events, such as the frequency and severity of heatwaves and floods, beyond just the change in the mean climate behavior [15, 5].

To address the modeling challenge described above, we develop and employ methods of  X  X patial temporal causal modeling, X  which allow us to model causal relationships between time and space-persistent features, given spatio-temporal data. More specifically, we develop a spatio-temporal version of the so-called  X  X raphical Granger modeling methods, X  which is an emerging collection of methods that combine the graphical modeling techniques with the notion of  X  X ranger causality X  to derive effective methods for causal modeling based on time series data. Here  X  X ranger causality X  is an operational definition of causality from econometrics, which is based on the premise that  X  X  cause necessarily precedes its effects, X  and its adoption to graphical modeling allows us, to model causal relationships between a large number of time series variables.
Specifically, we develop a novel method we call  X  X roup Elas-tic Net X , which can address the spatio-temporal aspect of climate modeling, and use it as our primary modeling methodology. This algorithm incorporates the spatio-temporal structure in the data in the variable selection process of the regression procedure under-lying graphical Granger modeling. That is, the lagged variables from different time steps for the same feature are grouped together and the penalty function used in variable selection is modified so as to enforce sparsity at the group level, rather than at the level of the individual lagged variables. Additionally, the spatial smooth-ness is enforced by an additional penalty term that encourages sim-ilarity between coefficients for spatial neighbors. This formulation leads to a grouped version of the so-called  X  X lastic net X  problem, for which we devise an efficient solution.

One potential weakness of a data centric approach to climate modeling is the lack of sufficient past data on extreme events, which may pose difficulties in modeling and attributing such events. Here we develop a dynamic modeling method by applying the theory of extreme event and value modeling. Extreme-value theory [3] provides a natural family of probability distributions for model-ing the magnitude of the largest or smallest of a large number of events, and a canonical stochastic process model ([7], sec. 7.3) for the occurrence of rare events, those whose magnitude exceeds a very high (or very low) threshold. The stochastic process model involves three parameters, which specify the rate of occurrence of extreme events and the distribution of the magnitude of events that exceed a threshold. We treat these parameters as varying over space and time and we model their variation by means of a Bayesian hi-erarchical model in which the parameters are regarded as random variables. The outputs of the model are a posteriori estimates of the parameters at potentially all locations in space and time. From these outputs we can estimate the spatial and temporal variation of properties of the distribution of annual extremes. In particular we look for evidence of climate change in the temporal variation of our estimates of the  X  N -year event X , the event magnitude that occurs on average once every N years.

The relationship between extreme event modeling and graphi-cal Granger modeling has been underexplored in the literature to date. In the present work, we employ a relatively simple approach to combining the two: using our Bayesian hierarchical model we estimate the N -year event magnitudes associated with the climate metrics of interest, and we incorporate these estimated variables as additional variables in causal modeling and attribution in the spatio-temporal modeling with the grouped elastic net algorithm described above. The choice of N -year event magnitudes as a proxy of ex-treme temperature is, in part, motivated by the fact that they are typ-ically approximated using normal distributions , which is consistent with our causal modeling method, using linear Gaussian models as component models of conditional distributions.
 We evaluate our proposed approach with two sets of experiments: In the first set of experiments, we use simulated spatio-temporal data to demonstrate the advantage of the proposed spatial-temporal modeling method based on group elastic net, as compared to meth-ods that do not take advantage of the spatial aspects of the data. In the second, and main, set of experiments, we use our developed methods to model real climate data, focusing on the data for the last couple of decades in the North American region. We collected and processed a wide range of climate related data for these space and time ranges, including the climatological observations, natural forcings (e.g. solar radiance), as well as greenhouse gas measure-ments. The results we obtained to date include: 1) Spatio-temporal causal modeling attributes the change in the temperature signifi-cantly to that of CO 2 and other greenhouse gases, even in the pres-ence of solar radiance; 2) Extreme value modeling confirms that the intensity of extreme weather events, such as unseasonably hot summer days and warm winter days, have significantly increased between the years of 1982 and 2001; 3) The combination of the two approaches indicate that, for the N -year return level of temperature as well, CO 2 and other greenhouse gases are attributed even in the presence of, and with greater significance than, the solar radiance.
We briefly review the notion of  X  X ranger Causality X  [9], which was introduced by the Nobel prize winning economist, Clive Granger, and has proven useful as an operational notion of causality for time series analysis in econometrics. It is based on the idea that if a time series variable causally affects another, then the past values of the former should be helpful in predicting the future values of the latter, beyond what can be predicted based only on their own past values.
More specifically, a time series x is said to  X  X ranger cause X  an-other time series y , if regressing for y in terms of past values of y and x is more accurate with statistical significance, as compared to regressing just with past values of y . Let { x t } T t =1 series variables for x and { y t } T t =1 the same for y . The so-called Granger test first performs the following two regressions: where L is the maximum  X  X ag X  allowed in past observations, and then applies a statistical test to determine whether or not (1) is more accurate than (2), with statistical significance.

The notion of Granger causality, as reviewed above, was de-fined for a pair of time series variables. Now in the context of cli-mate modeling, we are actually interested in cases in which there are many variables present as opposed to a pair, and each one is a spatio-temporal variable as opposed to a time series variable; and we wish to determine the causal relationships between them. Hence, the notion of Granger causality needs to be appropriately extended to incorporate the spatial dimension. Let us, for any mea-surement or feature over time and space (e.g. temperature, CO etc), use variables (e.g. x ) to refer to the entire spatio-temporal se-ries, and use indexed variables (e.g. x t,s ) to denote the associated individual spatially and temporally lagged variables.

For convenience, we assume that the measurements are sampled along a regular spatial grid. Similarly to the notion of maximum temporal lag, one may consider a maximum  X  X patial lag X  and sup-pose that each point is influenced by a finite neighborhood around it. Let N ( s ) denote the set of points in the neighborhood of s . We assume that the neighborhood structure is identical for each grid point, and thus consider neighborhoods of the form N ( s ) = s + X  , where  X  = {  X  1 , . . . ,  X  K } is a set of  X  X elative locations X .
Now, the extended Granger causality notion is defined in terms of the following two regressions: The above, simplified, scheme is symmetric with respect to time and space, but there is a difference between space and time that calls for a refinement of this formulation.

For applying Granger causality to many variables, or measure-ments, there is a collection of methods, known as graphical Granger modeling, which combines methods of graphical modeling with the notion of Granger causality. A particularly relevant approach is that of applying regression algorithms with variable selection to determine the causal links for each variable. Lasso [14] is a prime example, which trades off the minimization of the sum of squared errors and that of the sum of the absolute values of the regression coefficients in the penalty term.

Consider N measurements x i ( i = 1 , . . . , N ) (e.g. tempera-ture, pressure, etc.). For each such measurement x i , denote by x its sample at time t and location s. For any given measurement x one can view the variable selection process in the regression for x an application of the Granger test on x i against x 1 , . . . , x extending the pairwise Granger test to one involving an arbitrary number of spatial-temporal series, it makes sense to say that x Granger causes x i if x i t  X  l,s +  X  is selected for any time and spatial lags l,  X  in the above variable selection process.

A critical aspect that is worth emphasizing, and is overlooked in most of the existing methods in the literature, is that the ques-tion we are interested in is whether an entire series { x { 1 , . . . , L } ,  X   X   X  } provides additional information for the pre-diction of x i t,s , and not whether for specific time and spatial lags l,  X  x j t  X  l,s +  X  provides additional information for predicting x Therefore, a faithful instantiation of Granger causal modeling, in the context of spatial-temporal modeling, should take into account the group structure imposed by the spatial-temporal series into the fitting criterion that is used in the variable selection process.
The foregoing discussions naturally lead to the proposal of our novel method,  X  X roup elastic net X , which addresses both the issue of  X  X rouping X  the lagged variables for the same feature, and that of the smoothness desired for the spatial dimension.
 Figure 1: Generic Spatial-Temporal Causal Modeling Method
The generic spatio-temporal causal modeling method we described in the foregoing section is given in Figure 1. We now describe the variable selection procedure which we propose to use as an in-stance of the REG procedure in Step 3 of our algorithm. We as-sume  X  X patial stationarity X , i.e., that the same model applies to each point on the grid (relaxing this assumption will be the object of fu-ture work). More precisely, we consider regression coefficients of the form  X  k l, X  where k is the measurement (e.g. temperature), l is the time lag,  X  is the relative location between the point considered and a point in its neighborhood.
 Let S be the set of (interior) locations s such that for each  X   X   X  s +  X  is a point of the grid (and not outside the grid). Let, t = 1 , . . . , T be the time points considered.

For a given measurement x i , we propose to use the following penalized regression model to determine which spatial-temporal se-ries x j ( j = 1 , . . . , N ) Granger-cause x i .  X   X  = arg min
The role of the  X  X patial penalty X  is to enforce spatial regular-ization. Specifically the matrix  X   X  j is meant to enforce spatial smoothness as well as some form of distance-based coefficient de-cay. Namely the regression coefficients are penalized more as they correspond to increasingly distant neighborhood locations. For in-stance,  X   X  j could be a diagonal matrix such that the diagonal entry corresponding to  X  j l, X  equals k  X  k .

The  X  X parsity penalty X  is a group Lasso penalty [16], which im-poses sparsity across measurements. More precisely, the regression coefficients corresponding to spatial-temporal samples of the same measurement are penalized as a group, namely through k  X  j Then l 1 norm of the coefficients corresponding to a given measurement are either included as a group in the model or excluded. Note that the depen-dence in j of  X   X  j and  X  j is due to the fact that we may consider different regularization matrices for different measurements.
Let Y be the vector of length ( T  X  L +1) | S | formed by x ( L, . . . , T ) , s  X  S. Consider the spatially and temporally lagged matrix X of dimension (( T  X  L + 1) | S | )  X  ( NL |  X  | ) such that the row corresponding to the pair ( t, s ) is the vector formed by x corresponding vector of regression coefficients, i.e.  X  is of length  X  . Denote by  X  j the restriction of  X  to the elements correspond-ing to measurement x j , i.e  X  j is the vector formed by  X  (1 , . . . , L ) ,  X   X   X  . Then Eq. 5 can be rewritten as  X   X  = arg min
Notice that the above formulation resemble a  X  X roup version X  of the Elastic net problem [17], hence we call it the Group Elastic Net.
The following proposition states that the Group Elastic Net prob-lem can be transformed into a group Lasso problem, and hence can be efficiently solved using existing algorithms.

P ROPOSITION 2.1. Assume that  X  j ( j = 1 , . . . , N ) is positive definite, let  X  j = S T j S j . Let A j = ( S T j S j )  X  1
The Group Elastic Net problem solution  X  GEN = arg min  X  k Y  X  X X  k 2 +  X  2 can be obtained by solving the Group Lasso problem and where p is the number of columns of X , and setting  X 
P ROOF . Let  X  X = XC and  X   X  j = S j  X  j . Then solving Eq. 6 is equivalent to solving equivalent to solving min k Y  X  1 p Let p = NL |  X  | , i.e, p is the number of columns of  X  X. Let q = ( T  X  L +1) | S | , i.e, q is the length of Y and also the number of rows of  X  Then the problem is equivalent to solving which is the Group Lasso formulation.

Similar to [17], the penalty parameters are tuned as follows. We consider a set of candidate parameters  X  2 for  X  2 , (for instance  X  2 = (0 , 0 . 01 , 0 . 1 , 1 , 10 , 100) ). For each  X  2 equivalent Group Lasso algorithm for  X   X   X  , where  X  is a set of candidate parameters for  X  (e.g.  X  = (0 , 0 . 01  X  max , 0 . 1  X  where  X  max is a value which is so high that no group gets selected.) Then we pick the pair (  X   X  2 ,  X   X  ) = arg min BIC(  X  2 ,  X  ) , where
BIC(  X ,  X  ) = k where df GL is the degrees of freedom estimate for Group Lasso as proposed by [16], i.e., where  X   X  =  X   X  GL (  X ,  X  ) and  X   X  OLS is the ordinary least squares solu-tion when using all the variables.
We now give a brief review of extreme value theory [7]. We will show that a natural statistical model for the occurrence of extreme events is a Poisson point process that yields a generalized extreme value (GEV) distribution for the magnitude of the largest event in a fixed time period and a generalized Pareto distribution (GPD) for the amounts by which the magnitudes of extreme events exceed a specified threshold.

Let X 1 ,  X  X  X  , X n be a sequence of independent and identically distributed random variables, and let M n = max { X 1 ,  X  X  X  , X If there exist sequences of constants a n &gt; 0 and b n such that for some nondegenerate distribution function G , then G is a gener-alized extreme value distribution, with distribution function and  X  X  X  &lt;  X  &lt;  X  .

If the limiting distribution (11) exists, then, for a large thresh-old u , the exceedance Y = X  X  u , conditional on X &gt; u , is well approximated by a generalized Pareto distribution defined on { y : y &gt; 0 and (1 +  X y/  X   X  ) &gt; 0 } , where The parameters of the generalized Pareto distribution of threshold excesses are uniquely determined by those of the associated GEV distribution of block maxima.
These results provide two approaches for statistical modeling of extreme values. The block maxima (e.g. annual maxima of mete-orological variables) can be modeled as independent observations from a GEV distribution, or the excesses over a high threshold can be modeled by a GPD. Both approaches have weaknesses. The GEV approach uses only one observation per block, which may be wasteful if more data than just the block maxima are available. In the GPD approach, the probability of exceeding the threshold is not available. These weaknesses can be overcome by formulat-ing the behaviour of extreme events using a Poisson point process. This encompasses the GEV and GPD models and the process is completely defined by the same parameters that describe GEV dis-tribution of block maxima. The model leads directly to a likelihood that enables a natural formulation of nonstationarity in threshold excesses, for example by including spatio-temporal correlation.
A point process on a set A is a stochastic rule that describes the occurrence and position of point events. For a set A  X  X  , we define the non-negative integer-valued random variable N ( A ) to be the number of points in the set A . In a Poisson process the occurrence of events at different points a  X  A is statistically independent and N ( A ) has a Poisson distribution, with where the intensity function  X  ( a ) , a  X  A , indicates the relative frequency of occurrence of events at different locations in A . In extreme-value modeling the set A has the form (  X  X  X  , +  X  )  X  [ u,  X  ) , the two components respectively indicating time and event magnitude. The intensity function is which yields distributions of block maxima and of excesses over threshold u that have the forms (11) and (12) respectively.
Since the statistical characteristics of extreme climate data vary over space and time, the model specified by (13) X (15) cannot be used directly. We have therefore developed a more general version of the model, a spatio-temporal point process in which the location parameter  X  and scale parameter  X  are permitted to vary over space and time, and the threshold u varies over space.

To incorporate spatial and temporal correlation among the data, we build a hierarchical Bayesian spatio-temporal dynamic model [4]. This modeling strategy involves three stages. The first stage is the data model which models only observation process given a latent process. Stage 2 specifies the latent process; in our case, this is a Poisson point process and incorporates spatio-temporal de-pendence structures that are much more complicated than could be specified directly. In stage 3 we specify prior distributions for the parameters occurring in stage 2; here we can include external knowledge and expert opinion.

Let X i s,t be the i th exceedance over threshold u s at location s In the observation process, the likelihood function of the Poisson point process can be written as where  X  s,t and  X  s,t are varied over space and time.

In the process model, we model the location parameter  X  s,t a dynamic linear model and  X  s,t is modeled in the same procedure: where  X  t = (  X  1 ,t , . . . ,  X  S,t ) 0 at time t .  X   X  t called state vector. B  X  s in (17), is a S  X  K matrix which can re-duce the spatial dimension from S to K ( K &lt; S ). We choose B as a Matern kernel with fixed smoothness parameter [12].  X  random Gaussian process to include systematic error. where  X   X  in the transition equation is specified through an AR(1) process.
 In stage 3, we assign noninformative priors to all the parameters. Given the data model (16), the process model (17) and (18), and the prior process, we can derive the posterior distribution. Markov Chain Monte Carlo (MCMC) algorithm can be used to draw sam-ple from the full conditional distributions. The full conditional dis-tributions of the variance parameters which characterize the ran-dom process  X   X  t and  X   X  t are inverse gamma distributions and can be drawn through Gibbs sampler. Some full conditional distribu-tions of the parameters, such as  X  t and the temporal correlation pa-rameters in  X   X  , are hard to sample directly, and hence Metropolis-Hasting algorithm is used.  X  t are jointly sampled by forward filter-ing backward sampling (FFBS) algorithm [6]. After obtaining the MCMC samples, we can make inferences for the parameters in the model. We drew 15,000 samples and discarded the first 5,000. The chains were thinned by choosing every 10th samples to reduce the correlation. So 1,000 samples for each chain were left for analysis. Convergence was checked on trace plots of posterior samples.
It is usually more convenient to interpret extreme value models in terms of return levels, rather than individual parameter values. Let z m be the return level associated with the return period m years; z m is the level exceeded by the annual maximum in any particular year with probability 1 /m . Statistically, the return level is the 1 /m upper quantile of generalized extreme distribution. Let n be the number of observations in a year, and z m satisfies the equation where if [1 +  X  ( z m  X   X  s,t ) / X  s,t ] &gt; 0 , otherwise p = 1 . Here  X  and  X  are the parameters of the point process for year t and location s . This equation can be solved for z m using standard methods.
The mere amount of publicly available climate data is outright staggering. There are a large number of governmental and scien-tific institutions who publish measurements for a given geograph-ical range on a multitude of relevant variables on the Web. This being said, it is nevertheless a major challenge to obtain consistent longitudinal records that cover with comparable temporal and spa-cial resolution all relevant variables. Another problem is the large variety of formats in which data are available.
We compiled a comprehensive set of relevant variables for cli-mate modeling in North America. Aside from the primary climate variables that we eventually wish to explain, the literature distin-guishes human and natural agents or forcings that are known to affect the climate. These include solar irradiance and volcanic ac-tivities, greenhouse gases and aerosols (small particles dispersed in air). Figure 2 shows a schematic view of the data collection and preparation process. Table 1 lists the variables that we used in our analysis. We note that the  X  X emperature extreme X  variable is to be distinguished from all the others, in that they are estimated , using the extreme value modeling technique described in the previous section. We used for this study data from the following 5 sources: 1) CRU: Climate Research Unit provides monthly climatology data at http://www.cru.uea.ac.uk/cru/data for 11 surface variables including precipitation, wet-day frequency, mean, max, min tem-perature, vapor pressure, relative humidity, sunshine percent, cloud cover, frost frequency, wind speed from 1901 to present on a 0.5 degree latitude and longitude resolution. This grid data was inter-polated from station data as a function of latitude, longitude, and elevation using thin-plate splines by New et al.[13] 2) NOAA: The data center http://www.cdc.noaa.gov/data/gridded/ of the National Oceanic and Atmospheric Administration is consid-ered the  X  X orld X  X  largest archive of climate data X . We downloaded the greenhouse data from 170 worldwide stations from http:// www.esrl.noaa.gov/gmd/dv/ftpdata.html. 3) NASA: NASA uses satellite images to estimates of the ambi-ent aerosol optical thickness based on the resulting ultra-violet irra-diation. We collected this data from http://iridl.ldeo.columbia.edu/ SOURCES/.NASA/ .GSFC/.TOMS/.NIMBUS7/. 4) NCDC: The National Climate Data Center was our source for the different solar radiation measurements in 997 different lo-cations at http://rredc.nrel.gov/solar/old_data/nsrdb/. 5) CDIAC: Daily temperature data are obtained from U.S. his-torical climatology network (http://cdiac.ornl.gov/epubs/ndp/ushcn/ usa.html). The data of daily maximum temperature were collected from year 1948 to 2005 at 351 stations in U.S. We cleaned the data by removing invalid temperature observations.
The preparation of the data for the modeling involved a number of steps: 1) Normalization: Initially we transformed each dataset into monthly observations in a standard format including longitude, lat-itude, altitude, date, variable, value, unit, and source. 2) Interpolation and Smoothing: We interpolated the data from NOAA and NCDC into a common 2.5x2.5 degree grid for North America to allow us to join multiple data sources. For this process we used thin plate splines on the monthly data to be consistent with the interpolation method used for the CRU data. Since the data from NASA and CRU were provided for a finer resolution grid, we performed spatial averaging to get data on the common 2.5x2.5 degree grid. 3) De-seasonalization: We performed de-seasonalization by re-moving seasonal averages.
As we noted in Introduction, we conduct two sets of experiments, one involving generic spatio-temporal data that are simulated from an artificial model, and the other involving the actual climate data we described in the previous section. The experiments involving real climate data consist of the following steps: 1) Using spatio-temporal extreme value modeling technique to estimate the 1 -year return levels ( 1 -year event magnitudes) of temperature; 2) Incor-porating the estimated 1 -year return levels as a proxy for extreme temperature in the spatio-temporal causal modeling using Group Elastic Net.

In the subsequent subsections, we describe the details of these experimental procedures and their results.
We performed two sets of experiments on synthetic data to eval-uate the performance of  X  X roup Elastic Net X  (which takes into ac-count spatial interactions through spatial lagging and appropriate penalization in the regression), against that of a method that ne-glects such interactions and considers instead that a measurement at location s is only affected by variables at the same location. Specif-ically, the comparison method solves the following group Lasso problem for each measurement x i . We generated synthetic spatial-temporal data using a spatial-temporal vector autoregressive (VAR) model as generative model. More specifically, we considered N = 10 measurements x 1 , . . . x taken on a 15  X  15 spatial grid. For each (interior) point s = ( s 1 , s 2 ) we consider the neighborhood structure  X  = { (  X  L = 3 . Let x t , s denote the vector formed by all the measurements x t,s , i = 1 , . . . , N. We considered the following generative model. The matrices A l, X  where generated as follows. We first generated an N  X  N adjacency matrix A , where the entry A [ i, j ] = 1 indi-cates that x i causes x j , and A [ i, j ] = 0 otherwise. The value of each entry was chosen by sampling from a binomial distribution, where the probability that an entry equals to one was set to 0.2.
For the first set of experiments, we use a setup we call  X  X an-dom coefficient weighting. X  That is, for each pair ( l,  X  ) and each i, j , we set A l, X  [ i, j ] = c l, X  ( i, j )  X  A [ i, j ] , where c Unif (  X  0 . 1 , 0 . 1) . For the second set of experiments, we use a dif-ferent setup we refer to as  X  X ecaying coefficient weighting X , which is meant to represent situations where the influence decays with the distance. Formally, we first focus on the central location  X  (0 , 0) , and for each l and each i, j , we set A l, X  0 [ i, j ] = c ( l,  X  6 =  X  0 ) , for each i, j we set A l, X  [ i, j ]= where  X   X  is some random noise  X  Uniform (  X  0 . 01 , 0 . 01) .
For both sets of experiments the noise  X  was sampled according to a normal distribution N (0 , 0 . 01) . For each setup, we generated 10 models, and for each model simulated data for 100 time points. For Group Elastic Net, we used  X   X  j = I for the first set of experi-ments, and  X   X  j = diag(exp( k  X  k / 2) ,  X   X   X ) for the second (since in practice one may not know the exact type of distance based de-cay, e.g polynomial, exponential).

We measure the accuracy of each method with respect to their ability to correctly identify the underlying adjacency matrix A . We report the average F 1 score along with standard deviation. The F score is defined as F 1 = 2 PR P + R , where P is the precision and R the recall. The results are reported in Table 2. Under both settings, Group Elastic Net exhibits higher accuracy than the comparison method, and the difference in accuracy is greater for the  X  X ecaying coefficient weighting X . This illustrates the importance of taking spatial interactions into account in the modeling.
 Table 2: The accuracy (F1) of two comparison methods: Group Lasso (no spatial interaction) and Group Elastic Net for spatial temporal VAR models with random and decaying coefficient weighting. Figure 3: A comparison of average return levels from 1948 to 1980
We used the daily temperature data from CDIAC for modeling extreme temperature. To obtain the exceedances over a thresh-old required for the modeling, we calculated the 95 th quantile of the temperature distribution over the 58 years at each location and chose the observations which exceed the location-specific thresh-old. We removed the stations which have very few years with at least 2 exceedances. Thus the data used for our model are the daily maximum temperature exceedances for 58 years at 254 stations.
As discussed in Section 2.2.2, we interpret the extreme value analysis through return levels. We obtained the return level for years from 1948 to 2005 and each of the 254 stations. To investigate the evidence of global warming, for each station, we calculated the average return levels from 1948 to 1980 and from 1981 to 2005 and compared if there is any increase in terms of return level for these two periods. Figure 3 gives the return level difference which indi-cates the return levels increase over the past 58 years in midwest, western and eastern coastal areas of the United States. We observe a clear trend that the difference is mostly positive, with some of the regions exhibiting as much as 5 degrees Fahrenheit increase during this period.
We applied our spatio-temporal causal modeling method on two datasets: one monthly, the other yearly. Both contain data for 1990-2002 on a 2.5x2.5 degree grid for latitudes in (30 . 475 , 50 . 475) , and longitudes in (  X  119 . 75 ,  X  79 . 75) . The monthly dataset con-tains the first 19 variables listed in Table 1. The annual dataset contains in addition the estimated return levels for the extreme tem-perature. Having two different time resolutions allows us to inves-tigate short term and longer term influences. Note that since the return levels were estimated yearly, we did not incorporate them into the monthly dataset (Estimating monthly return levels will be the object of future work.)
For the spatial-temporal causal modeling, we used a 3x3 spatial neighborhood structure and a maximum time lag of 3 months for the monthly data, and 3 years for the yearly data. In our modeling, we considered the temperature variables as a group (TMP), as well as the solar variables (SOL), in addition to the natural grouping structure by spatial temporal series.
Figure 4 shows the results of attributing the changes in return level for extreme temperatures using the yearly dataset, while Fig-ures 5 and 6 show the results on attributing the changes in temper-ature, using respectively the yearly and the monthly dataset. In as-sessing the strength of causal links identified in our outputs, we use two separate metrics. One is the l 2 -norm of the regression coef-ficients corresponding to the variable group in question, which co-incides with its contribution to the spatio-temporal penalty term in the Group Elastic Net modeling. The other is the point at which the causal link in question appears in the output graph, as the parame-ter dictating how much emphasis is placed on the model complexity penalty in BIC is varied. This is done by multiplying the estimated noise variance in the penalty term (  X  2 in Equation 9) by a varying constant, which determines the trade-off between the model fit and model complexity. (The noise variance estimate is known to add a certain degree of arbitrariness to BIC with finite samples.) Each of the figures exhibits several causal graphs corresponding to differ-ent values of this parameter, with models becoming denser going from left to right and top to bottom. Also in the figures the edge thickness represents the l 2 -norm of the regression coefficient. It is apparent that the two measures coincide for the most part (order of appearance and edge thickness), and in particular, CO2 and other greenhouse gases are judged to have greater causal strength than solar radiance, according to both of these measures.
 Figure 4: Attributing the change in 1-year return level for tem-perature extremes using annual data. Output causal structures for decreasing degrees of sparsity. Edge thickness represents the causality strength.
In the present paper we initiated a data-centric approach to cli-mate change attribution. The results to date are preliminary but en-couraging, and in the future we plan to refine them, validate them with the domain experts, and explore ways in which they can pro-vide assistance to the dominant, simulation-based, approach to cli-mate modeling.
 We would like to thank the following people for their contribu-tions to this work in a variety of ways: Huijing Jiang, Elena No-vakovskaia, Cezar Pendus, Saharon Rosset and Lloyd Treinish. [1] Climate change 2007 -the physical science basis IPCC [2] Barnett, T.P., Pierce,D.W. and Schnur, R. (2001). Detection [3] Beirlant, J., Goegebeur, Y., Segers, J., and Teugels, J. (2004). [4] Banerjee, S., Carlin, B., and Gelfand, A. (2004).
 [5] Christidis, N., Peter, S.A., Brown, S., Office, M. and Hegerl, [6] Carter, C. K. and Kohn, R. (2001). On Gibbs sampling for [7] Coles, S. (2001). An Introduction to Statistical Modeling of [8] Gillett, N.P., Zwiers,F.W., Weaver,A.J. and Stott, P.A. (2003). [9] Granger, C. (1980). Testing for causlity: A personal [10] Karoly, D. J., Braganza, K., Stott, P. A., Arblaster, J.M. [11] Luo, L. Wahba, G. and Johnson, D.R. (1998) [12] Matern, B. (1960). Spatial Variation . New York: Springer. [13] New, M., Hulme, M. and Jones, P.D. (1999) Representing [14] Tibshirani, R. (1996). Regression shrinkage and selection via [15] P.A. Stott, D.A. Stone, and M.R. Allen. (2004) Human [16] Yuan, M. and Lin, Y. (2006) Model selection and estimation [17] Zou, H., Hastie T. (2005) Regularization and variable thickness represents the causality strength.
 thickness represents the causality strength.
