 Conventional studies on data analysis, image classification, pattern recognition and speech recognition have used competitive algorithms based on k-means algorithm[1] and Self-Organizing Map (SOM)[2]. Gaussian Mixture Models (GMMs) and maximum or minimum-likelihood classifiers are used for modeling and classification, respectively. To cluster the GPDF data in GMMs, the con-ventional k-means algorithm and SOM clustering algorithms are widely used[1]. However, because of the selection of par ameters such as lea rning rates and to-tal number of iterations and the initialized conditions, the k-means algorithm and SOM algorithms often give unstable results. Park proposed a competitive clustering algorithm called the Centroid Neural Network (CNN)[3]. Compared with the conventional k-means algorithm and SOM, the CNN converges stably to suboptimal solutions[3].

In order to improve the recognition accuracy in GPDF data clustering prob-lems, the maximum likelihood (ML) estimation is one of the empirical ap-proaches. To utilize the full information contained in data as specified by the probability density function, an alternative method is the cross-entropy[4]. The Kullback-Leibler and the Bhattacharyya distance measures are the represen-tative examples of the cross-entropy. The Centroid Neural Network with the Bhattacharyya distance (B-CNN) was first proposed by Park and Kwon[5]. In this paper, we propose a new algorithm for clustering of GPDF data called Centroid Neural Network with Bhattacharyya Kernel (BK-CNN). The proposed BK-CNN is based on the CNN algorithm and employs a kernel method for data projection. Though the kernel met hod has been successfully applied in various fields such as Support Vector Machine[6] and Fuzzy Clustering [7], it was designed for clustering of deterministic data because of its Eu clidean distance. In this paper, the Bhattacharyya kernel is used to measure the distance between two probability distributions for clustering probability data[8].

The remaining of this paper is organi zed as follows: In Section 2, we briefly review the conventional Centroid Neural Network (CNN) and the Bhattacharyya distance as a distance measure between two GPDFs distributions. Section 3 in-troduces the proposed BK-CNN algorit hm. Section 4 shows experiments and results including performance comparison of the BK-CNN with some conven-tional algorithms. Finally, conclusions are provided in Section 5. 2.1 Centroid Neural Network The CNN algorithm has been shown excellent results as an unsupervised compet-itive algorithm based conventional k-means algorithm[3,5]. It finds the centroids of clusters at each presentation of data vector. Unlike conventional unsupervised algorithms such as k-means algorithm and Self-Organizing Map, the CNN al-gorithm updates its weights only when the status of the output neuron for the current data has changed.

The following equations show the weight update equations for winner neuron j and loser neuron i when an input vector x is presented to the network at epoch n . where w j ( n )and w i ( n ) represent the weight of the winner neuron and the loser neuron, respectively while N i and N j describe the number of data vectors in cluster i and j , respectively.

More detailed description on CNN can be found in [3,5]. 2.2 Clustering in GPDF Data with Divergence Measure The conventional k-means algorithm and its variants have been most widely used in practice for clustering GPDF data. In order to exploit entire information in-cluding the mean and covariance information in the GPDF data for clustering, the divergence measure is employed as si milarity distance between two proba-bility distributions. The popular Bhattacharyya measure distance is adopted as divergence measure in this paper. The Bhattacharyya distance is a separability measure between 2 Gaussian distributions and is defined as follows: where  X  i and  X  i denote the mean vector and covariance matrix of a Gaussian distribution G i , respectively. T denotes the transpose matrix. 3.1 Updating Cluster Prototypes The energy function with a kernel can be written in feature space with the mapping function  X  : x ( j ) denotes the data j in the cluster i.
 Through the kernel substitution in Eq.(4), we obtain In the case of the Gaussian kernel function, we have K ( x i ( j ) , x i ( j )) = 1 and K ( w i , w i ) = 1, and the objective function becomes:
In order to minimize the objective function with a kernel, we use the steepest gradient descent algorithm. The learning rule can be summarized as follows:
In the case of the Gaussian kernel function, the objective function in Eq.(5) can be rewritten as: From Eq.(7), we obtain: In the BK-CNN, ( N i +1)  X  1 is used instead of 4  X  like the CNN, 3.2 CNN with the Bhattacharyya Kernel(BK-CNN) Recently, the kernel method has been us ed in various clust ering algorithms [9,10,11]. The kernel method is based on mapping data from the input space to a feature space of a higher dimensionality, and then solving a linear problem in that feature space. It has been successfu lly employed in many traditional clus-tering algorithms such as Support Vector Machine[6], Fuzzy Kernel Perceptron [12]. In order to calculate the kernel be tween two GPDF data,the Bhattacharyya kernel is employed. The Bhattacharyy a kernel is an extension of the standard Gaussian kernel. The Bhattacharyya kernel function between two GPDF data is defined as follows: where BK ( x ( n ) , w j ( n )) is the Bhattacharyya with a kernel distance between two Gaussian distributions x ( n )and w j ( n ). In this paper, the Bhattacharyya distance with a kernel as shown in Eq. (10) is employed for GPDF data. Unlike the CNN, we should consider the variance,  X  as well as the mean,  X  . The rule of the weight update for the mean is similar to the CNN: that is, the  X  of the winner weight go close to the input data vectors while the one of the looser weight goes away from the input vectors at every iteration. Therefore, the update rule is defined as follows: The performance of the proposed BK-CNN is evaluated and compared with other conventional clustering algorithms by applying to the Caltech image data set. The Caltech image data set consists of different image classes (categories) in which each class contains different vi ews of an object. The Caltech image data were collected by the Computational Vision Group and can be downloaded at http://www.vision.caltech.edu/html-files/archive.html
From these classes, we selected the 4 most easily confused classes: airplane, car, bike, and motorbike for experiments. Each class consists of 200 images with different views resulting in a total of 800 images in the data set. From this data set, 100 images were randomly chosen for training while the remaining images were used for testing. The entire images a re converted to grey scale and the same resolution. Fig. 1 shows an example of 4 image categories used in the experiments. Figs. 1(a), 1(b), 1(c), and 1(d) are examples of car, airplane, motorbike, and bike, respectively. For the localized represen tation, the images are transformed into a collection of 8  X  8 blocks. The block is then shifted by an increment of 2 pixels horizontally and vertically. The DCT coefficients of each block are then computed and return in 64 dim ensional coefficients. Only the 32 lowest frequency DCT coefficients that are visible to the human eye were kept. Therefore, the feature vectors that are obtained from each block have 32 dimensions. In order to calculate the GPDF for the image, the mean vector and the covariance matrix are estimated from all blocks obtained from the image. Finally, a GPDF with 32-dimensional mean vectors and 32  X  32 covariance matrixes is used to represent the content of images.

After mixtures are built, the minimum-likelihood classifier is adopted for choosing the class that the tested image belongs to: where x is the tested image represented by a Ga ussian distribution feature vector with mean vector,  X  , and covariance matrix,  X  .  X  ik and  X  ik represent for the mean vector and covariance matrix of cluster k in class C i , respectively. w ik is the weight component of cluster k in class C i . N i is the number of clusters in the class C i .

Fig. 2 shows the classification accuracy of the classification model using SOM with a Bhattacharyya measure (B-SOM), the k-means algorithm with a Bhat-tacharyya measure(Bk-means), the CNN with a Bhattacharyya measure (B-CNN) and the proposed BK-CNN. In this figure, the number of code vector is varied from 3 code vectors to 7 code vect ors in order to determine a sufficient number of code vectors to represent for the number of mixtures in GMMs. As can be seen from Fig. 2, the most algorithms tend to saturate at the point of 5 or 6 code vectors, while the proposed algorithm doesn X  X . The BK-CNN shows the better results than the other algorithms.

Table 1 shows the confusion matrix that describes the classification results of the proposed classification model in detail. As can be inferred from Table 1, cars can be well discriminated from the o thers while bikes and motorbikes are easily confused. These are logical results because motorbikes and bikes are quite similar even to the human eye while the cars are significantly different. A new clustering algorithm for clustering of GPDF data called Centroid Neural Network with a Bhattacharyya Kernel (BK-CNN) is proposed in this paper. The proposed BK-CNN is formulated by a incorporation of the Kernel method, the Bhattacharyya distance and the competitive learning algorithm. The kernel method adopted in the proposed BK-CNN is used for transformation of data from input space into feature space of higher dimensionality to obtain nonlinear solutions. By using the Bhattacharyya divergence distance, the BK-CNN can be used for clustering the GPDF data to utilize entire the mean values and covariance information of the GPDF data. The proposed BK-CNN is applied to cluster the GPDF data in the images. These encroaching results imply that the proposed BK-CNN can be used as an efficient clustering tool for GPDF data in other practical applications.

