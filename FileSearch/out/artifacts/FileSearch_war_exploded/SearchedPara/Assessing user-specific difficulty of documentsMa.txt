 1. Introduction
The Internet has become a major source of information in almost any domain. The huge amount of information makes it possible to have a full range of target audiences with levels of knowledge that are required to understand the information. techniques have been proposed for the assessment of the difficulty level, ranging from manually calculated readability formulas to automatic machine learning approaches.

Text may be perceived difficult by a particular reader for a number of reasons. Some of the most common reasons are the unfamiliarity of the terminology used and the background knowledge required. Crossley, Greenfield, and McNamara (2008) list representation skills the reader needs: world knowledge, knowledge of the text genre and the discourse model employed. Further they note that text comprehensibility includes measures of text cohesion, meaning construction, decoding, syntactic parsing, propositional density, complexity and the amount of working memory ( Crossley et al., 2008; inadequate (e.g., too descriptive) language, ambiguous words and phrases, and structural problems.  X 
All users have their own background knowledge and vocabulary, and thus there is a need for user modelling in the text modelling with web content difficulty assessment and document personalisation ( Elhadad, 2006b; Jones et al., 2006; Peru-gini, 2010; Nanas, Vavalis, &amp; Houstis, 2010 ).

In this article, we review the existing work related to difficulty assessment and propose a novel approach for assessing document difficulty by doing it in a user-specific way. The proposed difficulty measure uses automatically extracted key-phrases and compares their frequencies in a document to a user model. We propose two approaches to obtain the user mod-el: (1) a direct approach in which users are asked about the difficulty of certain terms and (2) a light-weight indirect approach in which the difficulty is automatically calculated based on texts users have written.

The main contribution of this article is the proposed difficulty measure. Most of the difficulty measures in the literature involve general difficulty, but our method assesses also subjective difficulty. Moreover, out of the two user modelling ap-proaches, the indirect approach is novel, whereas the direct approach is a more straightforward one, in which users are di-rectly asked to rate the difficulties of terms. The proposed method is fast and can handle documents in any language or domain. The experiments have been conducted for a corpus of medical documents in the Finnish language, intended for ficult than texts targeted at lay people.

Then we introduce our proposed method for user-specific difficulty assessment in Section 3 . We conduct the experiments with a set of professional and lay texts in Section 4 . Finally, we discuss possible applications and conclude the work in
Section 5 . 2. Related work matic readability formulas are discussed first and then we move to more automatic methods. Since our own method is based on the assumption that most of the difficulty in understanding text comes from difficult terminology, we review research on expert-domain terminology understanding by lay people. Finally we briefly discuss approaches for user modelling. 2.1. Readability formulas means that the text is easier to read than the other texts. A wide range of readability formulas have been proposed in the literature. Most of them are intended for measuring text difficulty for pupils and students and in those cases the outcome are language-specific because, e.g., average word length differs between languages. Some of the best-known, traditional readability formulas for English include the Coleman X  X iau index ( Coleman &amp; Liau, 1975 ), Flesch Reading Ease ( Flesch, and adverbs, long and complicated sentences, and large numbers of subordinate clauses ( Wiio, 1968, 2000 ).
While the outcome of the formulas is usually an elementary school grade level, they are not directly applicable to diffi-culty assessment of texts targeted at adults with normal reading skills and background knowledge. Furthermore, many read-ability formulas need manual work and thus are not suitable for automatic processing of text. 2.2. Automatic approaches for text difficulty assessment extensive language-specific preprocessing or word lists ( Feng, Elhadad, &amp; Huenerfauth, 2009; Fran X ois, 2009; Heilman, ing the best performance. The sets of features included both unigrams and features used in the traditional readability formulas. Watters, Zheng, and Milios (2002) classified news articles into three classes: non-medical, medical for experts and medical for other audiences. They used both decision trees and the naive Bayes classifier. The feature set was built by extracting keyphrases from the articles, filtering them by comparisons to medical vocabularies and having a medical expert rate the difficulty of all the resulting terms.
 The classifications between expert and lay texts have been conducted also for other languages than English. Poprat,
Mark X , and Hahn (2006) classified German, English and Spanish medical documents into lay and expert audiences by using medical web pages into those targeted at experts and non-experts. Their features were related to the style and genre of the texts.

Miller, Leroy, Chatterjee, Fan, and Thoms (2007) came to the conclusion that consumer health web pages do not use the nature of the features cannot guarantee independence of text source, topic and genre.

The reviewed methods measure general difficulty and cannot adapt to each user X  X  subjective perception of difficulty. Clas-matic versions of the readability formulas that use language-specific preprocessing or specific word lists are difficult to extend to new languages and domains. Moreover, automatic readability formulas face the same restriction with the tradi-tional ones: they are usually intended for students and thus cannot be directly applied for assessment of expertise domain texts that are intended for adults. 2.3. Lay understanding of expert terminology
Lay understanding of expert domain terminology has been studied in many qualitative works. Asking for the definition of concept can be difficult, even though one might have a vague idea of what it is about ( Gittelman, Mahabee-Gittens, &amp; (active vocabulary), although receptive knowledge (passive vocabulary) is more central to text understanding.
A number of studies have investigated lay use of medical terminology and identified medical terms in lay-produced text educated people with no medical training. A term was judged difficult if it had a low frequency in both non-expert targeted medical texts and a general language corpus. 2.4. User modelling
Even though the goal of readability and text difficulty measures is to analyse the difficulty of a text to the reader, very ity features, readability indices and n -gram frequencies.

The use of machine learning algorithms has attracted much attention for user modelling purposes in web-related tasks sonalised information retrieval by re-ranking the search engine results according to a user model. The system weighted ex-panded query terms with their existence in user X  X  personal content, such as visited web pages, e-mail messages, calendar items and stored documents on the client machine.

In the question X  X nswering task, Quarteroni and Manandhar (2006, 2007) used extracted keywords to model the user interests in their web-based YourQA system. The relevance of a document for a user was calculated by comparing a ranked the submitted question was selected to fit the group by using the extracted keyphrases. 3. Methods
In this section, we introduce our methodology for assessing user-specific conceptual difficulty of a document. For simplic-ity, we make an assumption that most of the difficulty perceived in a document comes from the difficult vocabulary it con-tains and the other sources of difficulty play a minor role. A document vector d unigram terms c j  X  t i  X  ; t i 2T ; i 2f 1 ; ... ; F g , as its elements where F is the number of features in the document set feature space T .

We assume that each user has their own knowledge of terminology and thus the difficulty of terms or documents varies ument vectors. The values for each term t i in the user profile vector correspond to perceived difficulty h ( t by the user 3.1. The difficulty measures
Given the document vector d j and user vector u k , where d in the following way: where the document vector is normalised with M j , the length of the document j , i.e., the total number of words in the document.

The difficulty of a set of documents d for a certain user u where D is the total number of documents in the document set.

The general difficulty of document d j for a set of users u is defined as where U is the number of users who participate in the term difficulty rating.

The general difficulty of document set d for a set of users u is defined as In this article, we propose two ways for creating the user profile vectors: direct and indirect. In the direct approach in of the user vectors we show how the method performs with artificial data. 3.2. Examples with artificial data
To show how the proposed difficulty measure performs with different documents and users, we first analyse some arti-ficial data. Let us consider a simple term set of three keywords or terms { t difficulty of terminology varies from 0 ( very easy )to1( very difficult ).

Example 1. We have three documents { d 1 , d 2 , d 3 } that contain a varying number of terms t addition, some easy words. Since document d 1 does not contain any of the terms t words, its difficulty d du ( d 1 , u k ) = 0 for all users u the difficulty score for documents that have also non-difficult words in addition to the terms t number of words varies between 3 and 20. For user u 1 the difficulties of the documents are d this user considers all terms in both the documents very easy.

Document d 2 of length 3 in Fig. 1 a contains nothing else than three times term t maximum difficulty d du ( d 2 ) = 1 for users u 3 and u 6 length of 4 words in Fig. 1 b is maximally difficult for users u than 1, as they are for term t 3 for users u 2 , u 4 and u document difficulty scores are lower d du ( d 3 , u 2 )= d
Example 2. Documents that contain also terms t 1 and t 2 besides t users u 5 and u 6 can be seen in Fig. 2 . The users consider the difficulty of terms t for user u 6 than for user u 5 . This can be seen as higher overall difficulty scores for user u 3.3. Direct user representation using human evaluators documents to be assessed and ask the difficulty of the terms directly from the users. We use automatic keyphrase extraction to select keyphrases and further, using support vector machine (SVM), select a subset of keyphrases that best distinguish is shown in Fig. 3 . 3.3.1. Document vector: keyphrase extraction and feature selection
Automatic keyphrase extraction is a task in which the contents of a document are represented with a few keywords or longer phrases known as keyphrases. Keyphrases are supposed to be available in the processed documents themselves, language-independent Likey method ( Paukkeri, Nieminen, P X ll X , &amp; Honkela, 2008; Paukkeri, P X rez Garc X a-Plaza, Fresno,
Mart X nez Unanue, &amp; Honkela, 2012 ). The method compares the ranks of the word and phrase frequencies to the ranks in a large reference corpus that is seen as a sample of the general language. The resulting keyphrases are those phrases that are common in the document but not in the reference corpus and thus the words with low information value (including stop words) are filtered out.

After keyphrase extraction we apply an additional dimensionality reduction step, feature selection, because the manual not separate well different sets of documents, i.e., they are common in more than one of the document sets. A subset of the terms T N T is selected with the Recursive Feature Elimination Support Vector Machine (RFE-SVM) ( Guyon, Weston, Barnhill, &amp; Vapnik, 2002 ). The document vectors d j are created in the reduced term space T of each term in each document. Any suitable feature selection method could have been applied here. Feature selection step could have also been skipped if the size of the vocabulary were not an issue. 3.3.2. User vector: human keyphrase rating
We conducted a survey in order to obtain human ratings of term difficulties. The terms were presented as a list for survey respondents. The task was to tell whether an otherwise clear sentence containing the listed term would remain unclear without finding out the meaning of the term from, e.g., an encyclopedia. There were four possible answers for the rating term difficulties h ( t ) for the user vectors u k . 3.4. Indirect user representation using texts written by the users
The indirect approach to obtain the user vectors is based on the assumption that the information about knowledge of ter-minology can be collected automatically from texts the user has written and no manual rating is needed. The approach of terminology is rated using the frequencies of terms in texts written by the users. 3.4.1. Document vector: keyphrase extraction
A percentage u terms per 100 words are extracted with the Likey keyphrase extraction method from the document set in d . 3.4.2. User vector: word frequency calculations
User vectors are created from large text collections written by individual users. The frequency of each extracted key-an individual may know many more terms that there happen to be in the specific text collection. We selected a simple con-tinuous squashing function to transform term frequencies c ( t ) 2 {0,1,2, ... } into difficulty rating h ( t ) similar decreasing sigmoid function could have been used as well. 3.5. Relation to existing methods
The existing automatic methods for text difficulty assessment usually use supervised approaches, such as text categori-training data. The assumption that each user has their own knowledge related to the difficulty of terminology has not been considered earlier in automatic text difficulty assessment approaches as far as we are aware. The proposed method deter-mines a user-specific difficulty of a document, whereas the others determine a general difficulty level. General difficulty by Watters et al. (2002) is close to ours since they also extract keyphrases from documents. However, they have a medical expert who manually rates the resulting terms to obtain a general difficulty level. The YourQA system ( Quarteroni &amp; user profile vector is rather limited, containing 3 X 9 keyphrases per user, whereas our method exploits all extracted key-phrases from every document. Their system is also intended for students instead of adults as in our case.
Unlike many other studies, our approach does not use much language-related information and is thus easily portable to new domains and languages. Nevertheless, besides the sample of general language for Likey keyphrase extraction, our ap-ment of one document only. Also, since the method concentrates on the terminology of the documents, it cannot assess dif-ficulty arising from other features of text, such as complex structure or ambiguity. 4. Experiments
In this section we describe the conducted experiments for user-specific difficulty assessment. We first experiment with ducted by comparing difficulties between data sets intended for either professional or lay use. 4.1. Direct approach: human rating of terminology difficulty of a document.
 4.1.1. Data and preprocessing
The direct approach requires three data sets: the documents to be assessed, an independent reference corpus and survey results of terminology difficulty rating. online medical documents. The documents are from four sources and they were all freely available at the time of examina-sion of the recommendations 2 ( ProLay ). The two other sources are also targeted at lay people: L X  X k X rikirja Duodecim
Poliklinikka.fi 4 ( Lay2 ) contain articles on specific medical topics.
The document collections as html pages were preprocessed in the following way: the text contents were extracted, and disclaimers and other repeating parts were removed if they were obvious. The texts were lowercased, references to footnotes were removed, numbers and problematic characters were replaced by tags and punctuation marks were separated from the surrounding words. Documents that were less than 4 kB in size after preprocessing were removed from the material. Table 5 shows the numerical characteristics of the data sets.

The total number of medical documents is 316, and the total number of word tokens in them is 511,450. Although the sizes of the document collections are relatively small, they provide a good example of texts on the same topic that are in-tended for readers of different expertise level. We used the Finnish stemmer in the Snowball software package and fast stemming all the preprocessed texts. However, similar results could have been obtained with language-independent morpheme segmentation. used as a reference corpus that contains general language. This reference corpus was used by Likey to extract medical terms from the analysed medical documents. The same preprocessing steps were carried out for Europarl as for the medical corpus.
The total number of word tokens in the preprocessed Europarl corpus is 18,553,177. 4.1.1.3. Survey data. We conducted a survey in order to get human ratings for difficulties of medical terminology. Twenty people with different levels of medical expertise were recruited to respond to the survey which was distributed via e-mail.
The survey consisted of 128 keyphrases extracted from the medical documents. 4.1.2. Experimental setup
In the direct approach, 30 keyphrases were extracted with Likey from each medical document, having Europarl as the reference corpus. A linear RFE-SVM method implemented in the Weka machine learning software ( Witten &amp; Frank, 2005 ) was used to select a subset of the keyphrases. The binary classification algorithm was applied for multiple classes by com-total of about 4,800 keyphrases.

The lemmas of each of the 300 keyphrases were manually restored to their original forms to make the human rating eas-ier. The terms were examined independently by three people to remove terms that were in all regards too simple to test in tween the difficulty levels of different document groups were calculated using Welch X  X  t test for samples of unequal vari-ances at 5% significance level. 4.1.3. Results
We evaluate the performance of the direct approach for our difficulty assessment method by first listing the terms con-sidered the most difficult by the human evaluators. Then we compare the difficulty assessment of professional medical doc-uments to the difficulty of lay documents. 4.1.3.1. Term difficulty ratings. Of the total of 300 terms in the reduced term space T cated by any of the human evaluators as being even somewhat incomprehensible. There were 22 terms that scored an aver-age of ^ h P 1 in human-rated difficulty ( ^ h  X  t  X  X  1 for possibly not unclear ,
These terms and their scores are listed in Table 6 . ler. The differences between the professional ProPro set and the lay sets are statistically significant ( p &lt; 0.05). As baseline methods, we implemented several simple readability measures: Automated Readability Index (ARI) ( Senter &amp; formulas for other languages. The results show that five out of six baselines agree with the proposed difficulty measure, specific difficulties d u .

We calculated the general difficulties of individual documents d descending difficulty. The histograms of documents for professional audience on the ordered list are shown in Fig. 5 a and documents for lay audiences in Fig. 5 b. The professional ProPro documents are concentrated at the beginning (the most are very evenly distributed between the four document sets. This means that all the document sets have some documents 20 X 30 documents are from the Lay2 document set only. The differences between the professional ProPro set and the lay sets are statistically significant ( p &lt; 0.05).
The histograms of documents ordered according to the user-specific difficulty of individual documents d side of the ordered list. 4.2. Indirect approach: automatic difficulty assessment
In the indirect approach for user representation, texts written by users are analysed automatically. 4.2.1. Data and preprocessing
We use the medical and reference corpora which were used in the previous experiments, and, in addition, have texts writ-ten by lay users and professional users. The sizes of the texts are shown in Table 9 . steps mentioned in Section 4.1.1.1 , also punctuation was removed. The average length of the documents in the ProPro data set is substantially larger than the length of the other documents. In order to ensure fair comparison between professional and lay documents, we reduced the length of the ProPro documents into the average of the averages of the other document sets by removing a percentage of words from the ends of the documents. preprocessed by removing punctuation. and as such they are assumed to contain professional terminology. 4.2.1.4. Lay users: blogs. Finnish blog texts were collected from blogger.com from one person, and the lengths of the documents are selected to correspond to the dissertation data set. 4.2.2. Experimental setup
Keyphrases of the length of one word were extracted from the medical documents. The frequencies of the terms t quencies were transformed to term difficulty ratings using Eq. (7) . 4.2.3. Results 4.2.3.1. General difficulty of documents. First we show how the percentage of extracted keyphrases u affects the difficulty user groups, professionals and lay people alike. Smaller percentage u &lt; 2 would anyway yield to few keyphrases per docu-ment and thus very weak representation of the document. The differences between ProPro  X  ProLay and ProPro  X  Lay2 are sta-
The results with u = 3 are given in Table 10 . The document set ProPro intended for professional users was more difficult difficult than did professional users.

The professional documents in the ProPro set are more common in the beginning of an ordered list of documents accord-good as with manual rating of terminology in Fig. 5 . 4.3. Discussion
The size of the medical document collection is relatively small. The direct approach for user modelling needs human rat-ing of terms, and this could have not been conducted with a much larger document set. Nevertheless, in the indirect ap-proach, the data sets for user modelling are substantially larger than the assessed medical documents.
In the indirect user modelling approach, we use a large set of texts from each user but, nevertheless, the data contain far passive vocabulary. To obtain also passive vocabulary for the user models, documents the users have recently read could be examined. This would also make the user models more accurate and bring the indirect user modelling approach performance closer to the direct approach. 5. Conclusions
In this article, we reviewed related work about readability formulas and automatic text difficulty assessment. We pro-posed two ways to obtain the required user models: directly by using a survey and indirectly by collecting the information method was applied to four document sets in the medical domain. As a result, documents intended for professionals were assessed to be more difficult than documents for lay people.

The proposed method has many possible applications. One application would be an automatic categorisation of web doc-method to assess documents retrieved by a general-purpose search engine. The search results could include a personalised difficulty assessment of each retrieved document.

The experiments were carried for the Finnish language, but the applicability of the method to other languages than Finn-ish could be shown in future work. The extension would be very simple with the indirect user modelling approach. The direct approach needs the ratings of difficulty for the terminology. The same applies also for new domains beyond medicine. The lower the generality of the approach.

This article takes a step towards a so-called within-language translation system. The idea of within-language translation using keyphrase extraction methods, such as Likey.
 Acknowledgments
This work has been financially supported by Langnet, the Finnish Graduate School in Language Studies, and the Academy of Finland through the Centre of Excellence Programme. The work has also been funded by Tekes, the Finnish Funding
Agency for Technology and Innovation Agency, and partners through the KULTA project on Modeling Changing Needs of Con-sumers. Furthermore, we acknowledge the opportunity to increase our domain understanding through the participation in the MedIEQ project which was funded by the European Union under the Programme of community action in the field of Pub-lic Health (2003 X 2008). We thank our colleagues Sami Virpioja, Jaakko V X yrynen, Oskar Kohonen and Mehmet G X nen for their valuable comments during this study, and the anonymous reviewers for their comments that helped us to improve the final version of the article. We also would like to warmly thank the medical experts and lay people who participated in rating the difficulty of terms.
 References
