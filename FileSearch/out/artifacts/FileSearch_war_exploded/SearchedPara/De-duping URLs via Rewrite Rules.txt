 A large fraction of the URLs on the web contain dupli-cate (or near-duplicate) content. De-duping URLs is an extremely important problem for search engines, since all the principal functions of a search engine, including crawl-ing, indexing, ranking, and presentation, are adversely im-pacted by the presence of duplicate URLs. Traditionally, the de-duping problem has been addressed by fetching and examining the content of the URL; our approach here is dif-ferent. Given a set of URLs partitioned into equivalence classes based on the content (URLs in the same equivalence class have similar content), we address the problem of min-ing this set and learning URL rewrite rules that transform all URLs of an equivalence class to the same canonical form. These rewrite rules can then be applied to eliminate dupli-cates among URLs that are encountered for the first time during crawling, even without fetching their content.
In order to express such transformation rules, we propose a simple framework that is general enough to capture the most common URL rewrite patterns occurring on the web; in particular, it encapsulates the DUST ( D ifferent U RLs with s imilar t ext) framework [5]. We provide an efficient algorithm for mining and learning URL rewrite rules and show that under mild assumptions, it is complete, i.e., our algorithm learns every URL rewrite rule that is correct, for an appropriate notion of correctness. We demonstrate the expressiveness of our framework and the effectiveness of our algorithm by performing a variety of extensive large-scale experiments.
 Categories and Subject Descriptors: I.7.0 [Comput-ing Methodologies] : Document and Text Processing X  General General Terms: Algorithms, Performance Keywords: URL normalization, de-duping, rewrite rules
Several previous studies [12, 17, 15] have established that a large fraction of the web consists of duplicate URLs  X  syntactically distinct URLs having similar content. These duplicate URLs adversely affect the performance of com-mercial search engines in various ways. In crawling, they waste valuable bandwidth, affect refresh times, and impact politeness constraints; in indexing, they consume unneces-sary disk space; in link-based ranking, they impart dispro-portionate authority to undeserving URLs; in presentation, they pollute displayed search results and lead to a poor user experience. De-duping URLs is thus an extremely important problem in end-to-end web search, and enormous resources are invested by search engines for this task. The traditional approach to de-duping has been to fetch the content of the URL and then apply standard fingerprinting methods on the content to eliminate duplicates [7, 8, 9]. However, it is de-sirable to identify duplicate URLs as early in the workflow as possible, ideally even prior to crawling [14, 15].
Duplicate URLs occur on the web due to a multitude of reasons beyond blatant plagiarism. These include hosting the same set of URLs on different mirrors that are typi-cally done for load balancing and fault tolerance, e.g., http: //www-1.ibm.com and http://www-2.ibm.com . Often these are simple web-server based canonicalizations of URLs, e.g., dropping index.html from the website name, or other sim-ple syntactic modifications such as removing the trailing slash, interchanging upper and lower cases etc. Dynamic scripts frequently encode session-specific identifying infor-mation in the URL that is used to track the user and the session but has no impact on the content of the page. The presence of such content-neutral parts in a URL is an im-portant reason for the proliferation of duplicates. Further-more, even the order of dynamic parameters is mostly in-consequential with respect to the content of a URL, e.g., the URLs http://domain/show.php?a=10&amp;b=20 and http: //domain/show.php?b=20&amp;a=10 are the same. Unlike pla-giarized content, these are structured transformations on the URL string that mostly happen due to server software. With a proper understanding of these transformations, it is pos-sible to detect whether two URLs have similar content even without explicitly examining their content. This is the mo-tivation behind our work.

Suppose we have a large collection of URLs along with their duplicate information, i.e., for every URL, which other URLs are duplicates (or near-duplicates) of this URL. Is it then possible to mine this collection and learn whether two URLs are duplicates of one another by examining only the URL strings? Equivalently, can we learn a set of rewrite rules that, given any two duplicate URLs, canonicalizes them to the same URL?
If these rules are indeed learnable from an offline compu-tation, they can be deployed in conjunction with the crawler to de-dup URLs prior to crawling them, thus ensuring that duplicate URLs are not even crawled! This is a significant departure from traditional de-duping approaches [7, 8, 9] that require the content of URLs in order to identify dupli-cates. Considering that duplicate URLs constitute a large portion of the web, learning URL rewrite rules and deploy-ing them in the crawler can tremendously improve the effi-ciency of not only the crawler but also subsequent steps in the processing pipeline.

Given that URLs appear and disappear at a rapid rate, the value of mining and learning URL rewrite rules, espe-cially in an offline manner, is not immediately apparent. Note however that the rewrite rules are typically specific to a particular web server, and more specifically, to the soft-ware used by the web servers. As a result, these rules are likely to be more stable and have a longer life than the actual URLs themselves.
 URL rewriting and DUST. Bar-Yossef et al. [5] were the first to address the problem of learning URL to URL rewrite rules. They formalize the rewrite rules as simple string substitutions; for instance, the dropping of index. html from a URL is captured by the DUST rule X  index.html  X   X   X , i.e., the string index.html is replaced by the empty string. They give an algorithm to generate a list of possible DUST rules from a set of URLs, without explicitly looking at the content of the URLs. Subsequently, they employ various heuristics to prune candidate rules that are expected to have lower precision. In the rest of the paper, when we refer to a DUST rule, we mean a simple string-substitution rule, of the kind explained above.

Although DUST rules form an important part of the most common URL rewrite rules, they do not capture all possible rules. To understand the limitations of DUST rules, it is most illustrative to look at an example. Consider the fol-lowing URLs that point to the same content: http://www.amazon.com/Deathly-Hallows/dp/0545010225, http://www.amazon.com/Harry-Potter/dp/0545010225, http://www.amazon.com/Any-string/dp/0545010225.
 The fact that one of the components in the URL path is irrelevant  X  and can be replaced by any string without changing the content  X  is a rule that cannot be captured by simple substring substitutions, and therefore by DUST. In effect, the DUST algorithm applied here would gener-ate a number of rules, one for each pair of distinct values that the irrelevant path component takes, and consequently, none of the generated DUST rules would be of use in canon-icalizing any other URL that has a yet unseen value for the irrelevant path component. Rather, this particular rule is more intuitively captured by storing as context the pre-fix http://www.amazon.com/ and the suffix /dp/0545010225 of the particular path component, along with the informa-tion that the path component between the prefix and the suffix is irrelevant. Furthermore, even this particular rule is generalizable beyond this particular context. We can learn that for any product number ASIN , under the pre-fix http://www.amazon.com/ and suffix /dp/ASIN , the path component between the prefix and the suffix is irrelevant. Our contributions. (1) We present a formalization of URL rewrite rules that forms an efficiently learnable class, while at the same time is expressive enough to capture the most common URL transformations on the web, including DUST. (2) We present an efficient data mining algorithm for learn-ing URL rewrite rules from a set of URLs with similarity in-formation, and show that under mild assumptions about the set of URLs, the algorithm is complete, i.e., the algorithm learns all URL rewrite rules that are correct. (3) We demonstrate the expressiveness of our formula-tion and the effectiveness of our algorithm and the overall de-duping approach through a number of large-scale exper-iments on real data. In particular, we show that the URL rewrite rules that we learn are able to reduce the number of duplicate URLs by 60% , and of the rules we learn, DUST rules only reduce the number of duplicate URLs by 22% ; thus, our algorithm is able to learn rules that identify sig-nificantly more duplicate URLs than DUST.
 Organization. The rest of the paper is organized as fol-lows. We discuss related work in Section 2. In Section 3, we describe the representation of URLs, present the seman-tics of URL rewrite rules, and formally define the problem statement. The various algorithms for learning URL rewrite rules are presented in Section 4. The experimental results are discussed in Section 5.
The related work falls into three categories. The first is the work on de-duping URLs that has been done in the web research community. The second is the work on con-tent de-duping that is studied in the database community. The third is the work on rule rewriting and the inductive learning framework, performed in the artificial intelligence community.

Our work on de-duping via URL rewrite rules is closest in spirit to Bar Yossef et al. [5] who present a structured frame-work to learn simple string substitution rules, and demon-strate the savings effected by URL canonization using these rules on four websites. We go significantly beyond their work by identifying more general URL rewrite rules, includ-ing session-id parameters, irrelevant path components, and more complicated URL token transpositions (Section 3.3). Our experiments demonstrate that non-DUST rules identify a much higher number of duplicate URLs. As we mentioned earlier, early work of de-duping using content can be found in [7, 8, 9]. See [14, 15] for more recent large-scale experi-mental evaluations of these techniques. Najork [16] describes a system for generating URL rewrite rules that is very simi-lar to ours; however, his focus was not on the formal aspects of rewriting.

De-duping is a well-studied problem in the database com-munity. The usual problems addressed include: eliminating duplicate records in a database or a multitude of databases, resolving correspondence between seemingly different-looking but similar entities (e.g., same author with the full name and only the last name), performing disambiguation of identical-looking but different entities (e.g., different authors with same names), and so on. These problems becomes especially challenging when some of the fields are either erroneous, par-tial, or missing; see [1, 10, 13, 11] for some sample work in this area. Our goal is somewhat different from this line of work: we wish to achieve de-duping of previously unseen URLs through a set of rewrite rules learned by mining the currently known set of URLs.

Our work on learning rules from examples also relates to learning abstract rewriting rules [6]. Stated in its full gener-ality, the problem of finding the smallest set of rewrite rules that explains a given collection of examples is intractable, but we benefit by the constrained structure of the rewrite rules that we intend to discover. The inductive learning framework [4] formalizes the process of learning concepts from examples by building successive hypotheses through a process of X  X east general generalization X . Our method of gen-eralizing rewrite rules can easily be seen as an instance of this framework. A specific example of such inductive learn-ing is to learn a regular expression from a certain finite set of examples [4]. Again, without any constraints on the type of regular expressions, finding the minimal such expression is intractable. However, for most such previous work [3, 2], the assumptions made on the allowable regular expressions render them unrelated to our problem.
In this section we formalize the URL rewrite rules. First, we abstract our representation of a URL.
Given a URL, we represent it as a function from a set of keys to a set of values. The set of key and values for the URL is defined as follows: first we split the URL into the static part comprising of the protocol, hostname, and the static path components, and the dynamic parts composed of the parameters and their values. We assume that the set of sep-arator tokens is known and fixed apriori; for the static part, we use the separator  X / X  and for the dynamic part, we use the tokens  X ? X ,  X &amp;, X  and  X ; X  for identifying key-value pairs, and the token  X = X  for separating a key from a value. We represent the static portion with the static keys { k 1 , . . . , k corresponding to the ` components in the static part. For the dynamic part, each parameter in the URL is defined as a key. For instance, the URL http://shopping.yahoo.com/electronics/cameras/digicams/ product.php?prodid=1000&amp;sort=price has the following keys: { k 1 , . . . , k 6 , k prodid , k sort } and the values corresponding to these keys are Let K denote the universe of all keys for all the URLs. Let V denote the set of all tokens from all possible URLs, aug-mented with the token  X  that denotes the empty value. In what follows, in describing any URL u , we assume that u maps the keys in K to the default empty value  X  unless otherwise specified. With this premise, we define a URL as follows
Definition 1 (URL). A URL u is a map u : K  X  V from the key-set K to the value-set V . The set of all possible URLs will be denoted by U .
 As an example, in our formalization, the above URL will be represented as the following map u , with u ( k 1 ) = http , u ( k 2 ) = shopping . yahoo . com , u ( k 3 ) = electronics , u ( k cameras , u ( k 5 ) = digicams , u ( k 6 ) = product . php , u ( k 1000 , and u ( k sort ) = price . All the remaining elements of K are mapped by u to the token  X  .
We characterize a rule in terms of two properties: its con-text , i.e., the set of URLs to which the rule applies, and its transformation , i.e., the changes to URLs the rule effects.
Define W to be set of values extended with the regular expression characters. For simplicity, in this paper we only consider W = V  X  X  ? } where ? represents the wildcard char-acter 1 that can match any non-empty value in V .

Define a context as a function c : K  X  W , mapping each key in K to a value in W . An example of a context that captures only the transfer protocol and the website is c ( k 1 ) = http and c ( k 2 ) = shopping . yahoo . com . We say that a URL u matches the context c , written as u ` c , if for every key k , either c ( k ) = ? , or u ( k ) = c ( k ) . Let C be the set of all possible contexts.
 Define the extended key set to be the set  X  K = K  X  W . Define a transformation a to be a map K  X   X  K . The in-tuition is that the function a intends to describe the target URL in terms of the key-value pairs of the source URL, in addition to having new key-value pairs. Let A be the set of all transformations.

In order to define the action of a rewrite rule on a URL, we extend the definition of a URL. Define the family of extended URL functions  X  U :  X  K  X  W as follows. For every u  X  U , the extended version of u is defined to be  X  u  X   X  U where Define the set of canonical URLs U c to be the set of maps K  X  W .

Definition 2 (Rewrite rule). A rewrite rule r is a mapping from the set of URLs U to the set of canonical URLs U c . Each rule r is a tuple consisting of two parts, a context c and a transformation a . Given such a tuple ( c, a ) , the rewrite rule r ( c,a ) : U  X  U c is defined as follows. Here, recall that  X  u  X   X  U is the extended URL correspond-ing to u . For notational simplicity, we will denote rules as context-transformation tuples, e.g., r = ( c, a ) . We define a partial order on the rules as follows.
Definition 3 (Rule ordering). A rule r 0 = ( c 0 , a 0 ) is less general than r = ( c, a ) , written as r 0  X  r , if for all tokens k , c ( k ) =  X  =  X  c 0 ( k ) =  X  or c 0 ( k ) = c ( k ) or c ( k ) = ? .
 Define r 0 to be the child of r if there does not exist any rule r ( 6 = r, r 0 ) such that r 0  X  r 00  X  r .
Before proceeding further, we show how the formulation can capture a variety of common URL rewrites.
 DUST rules. DUST rules are simple substring substitu-tions, that can be captured easily in our framework. Con-sider the following example, where for any value of the token
We are implicitly limiting the generality of the possible contexts we will learn by having only one wildcard character. However, the framework is easily extensible for more general wildcards, for example, [0 X 9] ? and [a X  X ] ? ANYTHING , the following two URLs point to the same con-tent: Suppose we arbitrarily decide that the static URL should be the canonical form, i.e., the rewrite rule is should transform the former to the latter. This rule can be captured by a simple substitution of the substring ?title by the substring wiki , and hence is a DUST rule  X  ?title  X  wiki  X . In our formalization, the context c for this rule is c ( k 1 ) = http , k , c ( k 0 ) =  X  . The transformation a is defined as a ( k and a ( k 2 ) = k 2 , i.e., the first two path components of the target URL are the same as the first two path components of the source URL. Furthermore, a ( k 3 ) = wiki , a ( k 4 ) = k i.e., the value of third path component is the string wiki and the value of fourth is the the value of the parameter title in the source URL. Thus the function  X  u  X  a is defined by the variable title in the source URL.
 Session ids. Our framework can capture session-ids, or more generally, any content-neutral parameter. Suppose the parameter sid in the following set of URLs is content-neutral. The canonical URL for all the URLs above is http://www. xyz.com/show.php?sid= ? and the rule performing this canon-icalization can be written as follows. The context c is defined as c ( k 1 ) = http , c ( k 2 ) = www . xyz . com , c ( k c ( k sid ) = ? and the transformation a is defined as a ( k for i  X  X  1 , 2 , 3 } and a ( k sid ) = ? . Note that both the context and the transformation functions have a wildcard.
 Irrelevant path components. Irrelevant path compo-nents can be thought of as session-id parameters that occur in the static part of a URL. Consider the example from Sec-tion 1 where we had the following set of duplicate URLs. http://www.amazon.com/Deathly-Hallows/dp/0545010225, http://www.amazon.com/Harry-Potter/dp/0545010225, http://www.amazon.com/Any-string/dp/0545010225.
 Suppose the canonical URL for this set of URLs is where ASIN can take any value. The rule performing this canonicalization can be written as follows. The context c is defined as c ( k 1 ) = http , c ( k 2 ) = www . amazon . com , c ( k c ( k 4 ) = dp , and c ( k 5 ) = ? . The transformation a is defined as a ( k i ) = k i for i  X  X  1 , 2 , 4 , 5 } and a ( k 3 ) = ? .
In the rest of the paper, we jointly refer to session id pa-rameters and irrelevant path components as irrelevant path (IRP) rules.
 Complicated rewrites. More complicated rewrites that capture token transpositions, converting the parameter val-ues to static components, can be captured by suitably defin-ing the transformation function. For instance, suppose for multiple values of the parameter page , the following two URLs point to same content: That is, the value of the parameter page is being used as a path component. The rule performing this transformation can be written as follows. The context c of the rule is defined as c ( k 1 ) = http , c ( k 2 ) = www . xyz . com , c ( k c ( k page ) = ? . The transformation a is defined as a ( k for i  X  X  1 , 2 } , a ( k 3 ) = k page and a ( k 4 ) = index . html .
In the rest of our paper, we refer to these complicated rewrites as regular-expression substitution (REGX) rules.
Our aim is to learn URL rewrite rules by mining a given set of URLs that have been partitioned into clusters, such that each cluster contains URLs with similar content. There are various ways of performing this initial clustering, the most well known ones [7, 9] involve creating a small finger-print of the content of a URL, and then hashing URLs with identical fingerprints into the same cluster. In our formu-lation, we assume that we are given a set of URLs U and similarity equivalence relation  X  between the URLs in U ; i.e., given two URLs u 1 , u 2  X  U , u 1 and u 2 have similar con-tent if u 1  X  u 2 . We do not concern ourselves with how the similarity equivalence relation is computed.

Given a set of URLs U and a similarity equivalence rela-tion  X  between them, we can now define the support, and the false positive rate of a rule in U .

Definition 4 (Support). The support of r in ( U,  X  ) , denoted supp( r, U ) , is the number of URL pairs ( u, v )  X  U  X  U such that v = r ( u ) .

Definition 5 (False-positive rate). The false-positive rate of r in ( U,  X  ) , denoted fpr( r, U ) , is the number of URL pairs ( u, v )  X  U  X  U such that v = r ( u ) but u 6 X  v . The false-positive rate of r , denoted fpr( r, U ) = fp( r, U ) / supp( r, U ) .
Given a sample of URLs and their similarity information, our aim is to find the set of rules that have high support in the sample, and low overall false-positive rate.

Problem 6. Given thresholds minS , maxP &gt; 0 , a subset of URLs U , and a similarity relation  X  between them, find the list of rules R such that  X  r  X  R , supp( r, U )  X  minS and fpr( r, U )  X  maxP .
 The problem is hard if instead of having a minimum sup-port threshold, we try looking for the minimum set of such rules. In fact, since each such rule can be seen as  X  X xplain-ing X  or  X  X overing X  a number of duplicate pairs, the problem of finding the minimum set of such rules becomes a general-ization of the NP-hard set-cover problem. Given an instance of the set-cover problem, with sets { S 1 , . . . S n } and universe { e 1 , . . . , e m } , we construct an instance of our problem as fol-lows. For each e j we construct a URL pair ( e 1 j , e 2 j e and e 2 j are duplicates, i.e., e 1 j  X  e 2 j . Also, for j two URLs for e j are both different from the URLs for e j 0 The URLs have keys corresponding to each S i , and two other keys, b and x . The values are as follows: for i  X  [ n ] , j  X  [ m ] ,  X  = 1 , 2 , e  X  j ( S i ) = [ e j  X  S i ] , e  X  j ( b ) = i , and e set S i is represented by a rule r i = ( c i , a i ) . The context c has c i ( S i ) = 1 , and maps every other key to ? . The action a corresponding to set S i just maps a i ( S i ) = value( b ) , i.e., to the value corresponding to key b , and every other key to ? . Thus, the rule ( c i , a i ) has a false positive rate 0 since it collapses only URL pairs that are duplicates. Now, it is easy to check that no rule more general than these rules has a zero false positive rate. Thus, setting maxP = 0 , choos-ing a smallest subset of rules such that for every duplicate pair we have a rule corresponding to it, is exactly the same as choosing the minimum number of subsets to cover the universe.

For tractability reasons, we make the following assump-tion about the existence of URL pairs in the given sample U that enables an inductive construction of the rules from the set U . We define an exhaustive sample set as follows.
Definition 7 (Exhaustive set). A set of URLs U is said to be exhaustive if for any rule r with supp( r, U ) &gt; 0 and fpr( r, U )  X  maxP , r has at least two children r 0 , r the following properties: (i) there exists at least one key k such that r 0 and r 00 differ on k , (ii) supp( r 0 , U ) &gt; 0 , and (iii) supp( r 00 , U ) &gt; 0 .

The basic intuition behind this characterization and our algorithm is simple. Given a list of URLs and their similarity information, we are going to learn the underlying rules by trying to compare pairwise URLs that are duplicates, and then trying to create rules by generalizing one token at a time. If the given sample set is exhaustive, then our algo-rithm will actually succeed in discovering the rules. We now describe the algorithm and the associated claims.
In this section we describe an algorithm, Offline , for gen-erating URL rewrite rules from clusters of duplicate URLs. Define d to be a difference function between maps in K  X  V . Namely, for any two functions u , v , both u, v  X  K  X  V , define the difference d ( u, v ) to be the set of keys k such that u ( k ) 6 = v ( k ) . We say that the rule r = ( c, a ) and r = ( c 0 , a 0 ) differ in key k if either c ( k ) 6 = c 0 a ( k ) . Define the complementary notation  X ( r, k ) to be the set of all rules r 0 that differ from rule r only at key k , i.e.,  X ( r, k ) = { r 0 | d ( r, r 0 ) = { k }} .

The Offline algorithm uses two modules. One is the sub-routine GenerateRule that given a pair of URLs u and v , that are duplicates, creates a basic rule that maps u to v . The context of this rule is essentially defined by the key-value map of the URL u itself, and the transformation is intended to capture how many of the values of v are essentially iden-tical to values of u .

After we have generated rules for each pair of URLs in all the duplicate clusters, we then invoke the subroutine Gen-eralizeRules . This subroutine is intended to look at all ex-isting rules r and find out keys k such that rule r can be generalized on key k . The set of candidate for this general-ization are all the pairs ( r, k ) such that  X ( r, k ) is non-zero. The subroutine first filters the rule-key tuples using an or-acle Filter that keeps only the candidates ( r, k ) that have a low false-positive rate when generalized. These rules r are then generalized on the corresponding keys and stored with appropriate support. In what follows, we present only the description of the ideal Filter in the algorithm 4. In Section 4.1 we then discuss practical methods of realizing this ideal filter.

The following lemma is intuitively obvious, and serves to show that our definition of partial order of the rules is con-sistent with the semantic notion.
 Algorithm 1 Filter ideal Input: T : set of ( r, k ) rule-key pairs.
 Output: Return only the of rule-key pairs that have low 1: for all ( r, k )  X  T do 2: r 0 be the rule obtained by generalizing r on key k . 3: Estimate r 0 fp = min r 00  X  r 0 fpr( r 00 , U ) to be the mini-4:  X  R =  X  R  X  X  ( r, k ) } if r 0 fp &lt; maxP . 5: end for 6: return  X  R .
 Algorithm 2 GenerateRule ( u, v ) Input: Pair of duplicate URLs u, v .
 Output: A rewrite rule that transforms u to v . 1: Define condition c as  X  k  X  K : c ( k ) = u ( k ) . 2: Define transformation function a as follows. 3: if  X  k 0  X  K : u ( k 0 ) 6 =  X   X  u ( k 0 ) = v ( k ) then 4: a ( k ) = k 0 . 5: else 6: a ( k ) = v ( k ) . 7: end if 8: return r = ( c, a ) .
 Algorithm 3 GeneralizeRules Input: R : A set of specific rewrite rules, minS : Threshold Output:  X  R : Set of generalized rewrite rules. 1:  X  R = R . 2: while true do 3: D = { ( r, k ) | r  X   X  R, k  X  K :  X ( r, k ) 6 =  X  X  . 4:  X  D = Filter ( D ) . { Filter the candidate generalizations 5: for all ( r, k )  X   X  D do 6: Suppose r = ( c, a ) . Define new rule r 0 = ( c 0 , a 7: if k  X  c then 8: a 0 = a . 9: c 0 = c { k 7 X  ? } . { c 0 generalizes c only at key k } 10: else 11: c 0 = c . 12: a 0 = a { k 7 X  ? } . { a 0 generalizes a only at key k } 13: end if 14:  X  R =  X  R  X  X  r 0 } with supp( r 0 ) = 15: end for 16: end while 17: return  X  R = { r  X   X  R : | supp( r ) | &gt; minS } .
Lemma 8. If r 0  X  r then for any subset of URLs ( S,  X  ) , supp( r 0 , S )  X  supp( r, S ) .
 Proof. Suppose r = ( c 0 , a 0 ) and r = ( c, a ) . For any URL pair ( u, v ) , in supp( r 0 , S ) , we have that u ` c v ` a 0 . Since c 0  X  c , we have that for any key k , c ( k ) 6 = ? =  X  c ( k ) = c 0 ( k ) . Thus, for every key k , one of the three cases occurs: u ( k ) =  X  , and hence c ( k ) =  X  and c ( k ) =  X  , or c 0 ( k ) = ? , or u ( k ) = c ( k ) = c 0 u ` c . Similarly, v ` a 0 and a 0  X  a , resulting in v ` a . Hence ( u, v )  X  supp( r, S ) . The proof is complete.

Recall that an exhaustive list of URLs is one for which for every rule with non-zero support, there exists at least two children that have non-zero support.

Theorem 9 (Completeness). Given an exhaustive set of URLs U , the algorithm Offline ( S, minS , maxP , Filter finds out all, and only, the set of rules that have support at least minS and false-positive rate at most maxP .
Proof. First we prove that if rule r has either support smaller than minS or false-positive rate larger than maxP , then rule r is not returned by the algorithm. This is trivial, as if the support of r is smaller than minS then the subrou-tine GeneralizeRules prunes r in step 17. Also, the oracle Filter ideal filters out all generalizations that would generate such a rule r with false-positive rate more than maxP . Hence we have this direction of the implication.

Next we prove that if rule r 0 has support at least minS and false-positive rate at most maxP then r 0 is generated and stored by the algorithm. In fact, we prove the following using induction: if rule r is such that there exists r 0  X  r with be generated. We show this, by induction on the structure of the rules. The base case is when we consider rules r that not have any ? , and that have a r 0  X  r that satisfies the above property. Then, we have to show that r is contained in the set found by the algorithm Offline . First, by our definition of r there does not exist any rule r 00 with r 00  X  r . Since r is in the subtree of r 0 , r has non-zero support, and so there exist pairs ( u, v ) with u  X  v and ( u, v ) ` ( c, a ) , and hence u ` c and v ` a . Thus, it must be that for each key k , c ( k ) = u ( k ) and a ( k ) = v ( k ) . Hence, by considering the pair ( u, v ) , the subroutine GenerateRule ( u, v ) will create the rule r . Also, r is not removed from the set of rules found as it has a rule r  X  r with false-positive rate at most maxP .

Now, consider any rule r = ( c, a ) , and let k be a key such that c ( k ) = ? or a ( k ) = ? . Without loss of generality, we consider only the case c ( k ) = ? . By our assumption about the exhaustiveness of the set U , r has at least two children r = ( c 0 , a 0 ) and r 00 = ( c 00 , a 00 ) , such that there exist two values v 1 6 = v 2 , with c 0 ( k ) = v 1 and c 00 ( k ) = v and r 00 have non-zero support in U . Then, by our inductive hypothesis, both r 0 and r 00 will be generated. Also, the Filter ideal oracle, when presented with ( r 0 , k ) and ( r not filter them out since they have a generalization with low false positive rate. Thus, r will also be generated by the algorithm.

Corollary 10. If there exist DUST rules that have sup-port at least minS and false-positive rate at most maxP , then the algorithm Offline ( S, minS , maxP , Filter ideal ) will find them.
The main ingredient in the algorithm Offline is the pres-ence of the Oracle Filter . We have shown that if the ideal oracle Filter ideal is provided to the algorithm, then we can learn all the URL-rewrite rules that have a large support and low false-positive rate in the URL sample. In practice, however, getting a reasonable estimate of the false-positive rate of a particular generalization is extremely expensive, both in terms of computation and data. Thus, the following heuristics can be used in order to filter the set of candidates for generalization.

Large fan-out filter ( Filter fanout ). The intuition is to generalize the rule r at key k only if the fan-out of the re-sulting rule, i.e., the set of values that k assumes in the rules of  X ( r, k ) , is large.

High entropy filter ( Filter entropy ). We only generalize the rule r at key k if the distribution of values that k as-sumes for rules in  X ( r, k ) is close to uniform, i.e., have a high entropy. The intuition is that statistically, a distribution of values that is close to uniform gives us a higher confidence about the key k being generalizable to a ? .

Estimated false-positive filter ( Filter efpr ). We esti-mate the false-positive rate of the rule r 0 that would re-sult from generalizing the rule r at key k . Specifically, if r = ( c 0 r , a 0 r ) , let S r 0 be the set of all URLs u such that u ` c r . Define T r 0 =  X  r timated false-positive ratio e r fpr = 1  X  X  T r 0 | / | S to generalize tuple ( r, k ) if e r fpr &lt; maxP . Algorithm 4 GenerateSpecificRules Input: D : Set of pairs of duplicate URLs.
 Output: R : Set of specific rewrite rules. 1: R =  X  . 2: while All URL pairs in D have not been picked do 3: Randomly pick a pair of URLs ( u, v )  X  D . 4: r = GenerateRule ( u, v ) . 5: if r  X  R then 6: supp( r ) = supp( r )  X  X  ( u, v ) } . 7: else 8: R = R  X  X  r } , with supp( r ) = { ( u, v ) } . 9: end if 10: end while 11: return R .
 Algorithm 5 Offline Input: U : Set of URLs along with minS : Threshold (mini-Output:  X  R : Set of generalized rewrite rules. 1: R = GenerateSpecificRules ( D ) 2:  X  R = GeneralizeRules ( R, minS , maxP , Filter ) 3: return  X  R .

Let ` max be the maximum number of key-value pairs in a URL. Then, we have the following bound on the running time.

Lemma 11. The algorithm Offline with the filter Filter fanout or the Filter entropy can be implemented to run in time
Proof. Comparing all the duplicate URLs pairwise takes time O ( ` max number of possible rules generated by GenerateSpecificRules . Name this set of rules R . Then the number of all pos-sible rules that can result from generalizing from this set R is O ( ` 2 max j  X  [1 , ` max ] the number of rules such that exactly j keys mapped to ? , and are obtained by generalizing from the set R is at most | R | . Thus, the number of iterations of the subroutine GeneralizeRules is at most O ( ` 2 max
Now, for each tuple ( r, k ) , we need to compute the two quantities  X ( r, k ) , and the fan-out or entropy only once, and it takes time O ( | R | ` max ) for all the tuples. So amor-tizing, the total cost of each iteration of subroutine Gener-ateSpecificRules is a constant. Thus the total complexity is
We implemented an optimized version of the Offline algo-rithm and measured its performance on several data sets. We first describe out experimental methodology and the metrics that we use to measure the performance of our ap-proach.
 Training and test data. The input data to our algo-rithm is a set of URLs with their similarity information. We crawled 25.6 million URLs from a set of 360 randomly chosen hosts with at least 1,000 URLs. Using standard du-plicate detection techniques [7, 9], we partitioned the URLs into clusters of duplicate URLs. Because we are really in-terested in measuring the performance of our algorithm in canonicalizing duplicate URLs, we only keep the URLs that are in duplicate clusters of size at least 2, that is, we re-moved all URLs that have no duplicates. We are thus left with 7.8M URLs. We then did a 50-50 test-train split by randomly assigning each cluster of duplicate URLs to either the Training set or to the Test set. Table 2 gives the ac-tual number of URLs and the number of duplicate clusters for the Training and Test sets.
 Metrics. We used the following metrics in order to measure the coverage of our algorithm. (1) False positive rate. The false positive rate of a rule is defined in Section 3.4. The false positive rate of a rule with respect to given set of duplicate clusters is a measure of the number of non-duplicate pairs of URLs that are rewritten to the same URL by the rule. Rules that have lower false positive rates are more accurate. (2) Reduction ratio. Given a set of rules R and a set of URLs U , let T be the set of URLs obtained after applying the rules R to the URLs in U . The reduction ratio of R is ratio thus covers a larger set of duplicate URLs.

False-positive rate is thus indicative of the precision of the rules, whereas the reduction ratio is indicative of how effec-tive are the rules. There is an obvious tradeoff between the following two metrics, the higher we set the required thresh-old for the false-positive rate, the lower is the reduction ratio that we achieve.
We first generated rules by applying the procedure Offline described in Section 4 to the URLs in the Training set. We used the Filter fanout heuristic with a threshold of 10 when generalizing rules. In order to study the effect of duplicate cluster sizes on the characteristics of the generated rules, we split the Training set into two parts (i) clusters of size 2 to 9 and (ii) clusters of size greater than 9. In the rest of this section, we denote the rules generated from duplicate clus-ters of size 2 to 9 by R 1 , the rules generated from duplicate clusters of size greater than 9 by R 2 . The set R 3 denotes the set of rules R 1  X  R 2 . Table 1 summarizes the number of rules in sets R 1 , R 2 , and R 3 .
 We measured the coverage of the generated rules on the Test and Training data sets. We first present the coverage ratios for the aggregated rule sets, and then do a finer anal-ysis of the effectiveness of various rule-types by computing the coverage numbers for rules of each type.
 Table 3 gives the reduction ratios for the rule sets R 1 , R , and R 3 when applied to clusters of various sizes in the Training set, the Test set and in the union of the Training and Test sets, denoted as the Training + Test set. Note that the rules were generated using only the duplicate clusters in the Training set. As the results in Table 3 demonstrate, by using the rule set R 3 , i.e., rules generated from the entire training set, we can reduce the number of URLs in the Test set by 55% and the number of URLs in the overall Train-ing + Test set by 60% . Thus, over half the duplicate URLs are identifies by the rules generated by our algorithm.
Rule set R 1 that is generated from Training set from clus-ters of sizes 2-9 has a reduction ratio of 33% on clusters of size 2-9, but a reduction ratio of only 16% on clusters of size &gt; 9 . This indicates that smaller-sized clusters generate dif-ferent kind of rules compared to larger-sized clusters. The intuition is that rules in set R 1 are generated from smaller clusters and should primarily be of the type DUST and gen-eral regular-expression transformations, whereas the rules R 2 that are generated from the clusters of larger size should mostly be ones that identify content-neutral parameters and path components. In our next set of experiments, we inves-tigate in details the different types of rules generated by clusters of different sizes, and the coverage ratios for each of these type.
We classified the rules in the rule set R 3 into the three categories described in Section 3.3: (i) DUST  X  simple string substitution rules, (ii) IRP  X  irrelevant path rules, including session-id parameters and irrelevant path components and (iii) REGX  X  general regular-expression substitution rules that cannot be expressed as DUST rules. In Table 4 we on the Training and Test data sets.
 Table 2: Description of the Training and Test data sets. give the distribution of the number of rules of each type for the sets R 1 and R 2 . As expected, 81% of the rules in R of type DUST , whereas 91% of the rules in R 2 are IRP and REGX type rules.

Table 5 gives the reduction ratios for each rule category in the rule set R 3 when applied to different sized dup clusters in the Training + Test data set. As expected, DUST rules have a higher reduction ratio for smaller sized clusters ( 29% ) compared to larger-sized clusters ( 14% ). Rules of type IRP do better on larger-sized clusters ( 56% ) compared to smaller-sized clusters ( 41% ). Over all clusters, DUST has a reduction ratio of 22% whereas IRP has a reduction ratio of 48% , and DUST + IRP + REGX has a reduction ratio of 60% . This indicates that there is a a lot of value in learning rules that go beyond just DUST .
 Table 4: Distribution of R 1 , R 2 , and R 3 into rule categories.
We investigate the effectiveness of the Filter fanout heuristic and present the trade-off between precision and coverage of the generated rules.

In the previous experiments, we used the rule set R 3 that was generated from the Training set using a fan-out thresh-old of 10. The number of rules in the set R 3 is 240,260. For each rule in R 3 , we computed its false-positive rate (as de-scribed in Section 3.4) against the Test set. We filtered the Table 5: Reduction ratio on Training + Test set per rule category for R 3 . rules by using different thresholds of the false-positive rate ( fpr  X  5% , fpr  X  10% and fpr  X  20% ) and for each false-positive rate threshold, we computed the reduction ratio of the filtered set of rules against the Training + Test set. Ta-ble 6 gives the reduction ratios for different false-positive rate thresholds.
 Table 6: Trade-off between false-positive rate and reduction ratio with fan-out threshold 10.

Next, we generated another set of rules from the Training set using a fan-out threshold of 30. We call this set of rules R . The number of rules in the set R 4 is 31,363. For each rule in R 4 , we computed its false positive rate against the Test set and for different false-positive rate thresholds, we filtered the rules, and computed the reduction ratio of the filtered rules against the Training + Test set. Table 7 gives the reduction ratios for different false-positive rate thresh-olds for the rule set R 4 .

Note that compared to fan-out threshold of 10, the num-ber of rules generated with fan-out threshold 30 is much less: 31 K rules in R 4 compared to 240 K rules in R 3 . On filtering with a false-positive threshold of  X  5% , fan-out 10 gives around 166 K rules, whereas fan-out 30 gives only 21 K rules (8 times less). But the reduction ratio with fan-out 30 is 16% which is not much lower than the reduction ratio of 21% that fan-out 10 gives. This indicates that the reduction per rule is much better with fan-out 30 compared to fan-out 10, and fan-out 30 is more effective at learning rules that have a higher impact. Table 7: Trade-off between false-positive rate and reduction ratio with fan-out threshold 30.

A web crawler can use the generated rules to compute a canonical id for each URL, and store the downloaded con-tent of a URL indexed by the canonical id. This allows the crawler to avoid crawling duplicate URLs with the same canonical id. In this mode of operation, an incorrect rule (a false positive) can cause unique URLs to be incorrectly clas-sified as duplicates, which in turn, can cause the crawler to not crawl these unique URLs. The higher the false-positive rate of a rule, the higher is the loss of potential unique URLs. On the other hand, from the tables 6 and 7, it is clear that as the false-positive threshold decreases, the coverage of rules also decreases, thus reducing the desired benefit of not crawl-ing duplicate URLs. A crawler can resolve this trade-off according to its own constraints and requirements. For ex-ample, if a crawler has a very large set of URLs to crawl, but is resource-constrained (i.e., does not have enough ma-chines/network to crawl all the URLs), or if there is a large set of URLs to crawl from a politeness-constrained website, then the crawler can use a higher false-positive threshold (say fpr  X  10% ) to select a larger set of rules to de-dup URLs. Thus, even if the crawler does not crawl some unique URLs due to incorrect rules, the impact is reduced since the crawler has enough URLs to crawl. On the other hand, if the crawler is not resource-constrained, then it could use a lower false-positive threshold (say fpr  X  3% ) to ensure that it does not lose unique URLs by applying incorrect rules. In web-search, crawlers typically are resource-constrained; there is a far larger set of URLs available for crawling than there are resources to crawl them all. So, crawlers are more tolerant of higher false positive thresholds ( fpr  X  5% ) and worry less about losing unique URLs.
To summarize our results, we achieve reduction ratios of up to 60% using all the rules generated by our algorithm. If we restrict the rules to ones that have high-precision ( &gt; 95% ), then we can still achieve a 21% reduction ratio. DUST rules are responsible for only a quarter of this reduction, being more effective only for the clusters of smaller size. The remaining three-quarters of the reduction are actually effected by the non-DUST rules in our formulation. We also show that by using a high fan-out in the Filter fanout heuristic, we can cut down the number of rules without significantly worsening the reduction ratios.
In this paper we present a different approach to the URL de-duping problem based on automatically generating URL rewrite rules by mining a given collection of URLs with content-similarity information. These rewrite rules can then be applied to eliminate duplicates among URLs that are en-countered for the first time during crawling, even without fetching their content. This has the huge advantage of trap-ping duplicates much earlier in a search-engine workflow, improving the efficiency of entire processing. Our frame-work is simple and has provable guarantees, and is shown to be effective in a large-scale experiment.

In our formulation, we use a fixed set of delimiters; it will be useful to study the effect of a more flexible tokenization on our algorithm. For instance, can our method be used to detect site mirrors, by segmenting the hostname using  X . X  as the delimiter? Extending our formalization to capture a wider set of rules, while still being efficiently learnable is also an interesting research direction.
 We thank Rajat Ahuja, Krishna Prasad Chitrapura, Raghu Ramakrishnan, and the Yahoo! Bangalore team for useful discussions and suggestions. We also thank the anonymous reviewers for their comments.
