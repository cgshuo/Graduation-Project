 Any given Web search engine may provide higher qual ity results than others for certain queries. Therefore, it is in users X  best inter-est to utilize multiple search engines. In this pap er, we propose and evaluate a framework that maximizes users X  sear ch effective-ness by directing them to the engine that yields th e best results for the current query. In contrast to prior work on me ta-search, we do not advocate for replacement of multiple engines wi th an aggre-gate one, but rather facilitate simultaneous use of individual en-gines. We describe a machine learning approach to s upporting switching between search engines and demonstrate it s viability at tolerable interruption levels. Our findings have im plications for fluid competition between search engines. D.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval  X  search process, selection process .
 Measurement, Experimentation, Human Factors. Search engine switching. Web search engines such as Google, Yahoo!, and Live Search provide users with keyword access to Web content. A ccording to statistics aggregated by audience measurement and a nalysis firms such as Nielsen-NetRatings 1 and comScore Media Metrix though users occasionally use multiple search engin es, they are typically loyal to a single one even when it may no t satisfy their needs, despite the fact that the cost of switching engines is rela-tively low ( e.g. , [19]). While most users appear to be content wit h their experience on their engine of choice, it is c onceivable that many users dislike the inconvenience of adapting to a new engine, may be unaware how to change the default settings i n their Web browser to point to a particular engine, or may eve n be unaware of other Web search engines that exist and may provide better ser-vice. Performance differences between Web search e ngines may be attributable to ranking algorithms and index siz e, among other factors. It is well understood in the Information Retrieval (IR) community that different search systems perform wel l for some queries and poorly for others [2,10], which suggest s that excessive loyalty to a single engine may actually hinder sear chers. To address this problem, this paper describes a mac hine learning approach that allows users to leverage multiple sea rch engines by unobtrusively recommending the most effective engin e for a given query. The approach relies on a classifier to sugge st the top-performing engine for a given search query, based o n features derived from the query and from the properties of s earch result pages, such as titles, snippets, and URLs of the to p-ranked docu-ments. We seek to promote supported search engine switching operations where users are encouraged to temporaril y switch to a different search engine for a query on which it can provide better results than their default search engine. Unsupport ed switching, whereby users navigate to other engines on their ow n accord, is a phenomenon that may occur for a number of reasons: users may be dissatisfied with search results or the interfac e, they may be lured to the engine by advertising campaigns or wor d of mouth, or they may switch by accident. 3 Results of a log-based study that we present in the paper show that only around 10% o f search ses-sions currently involve more than one search engine . We conjec-ture that by proactively encouraging users to try a lternative en-gines for appropriate queries (hence increasing the fraction of sessions that contain switching) we can promote mor e effective user searching for a significant fraction of querie s. Empirical re-sults presented in this paper support this claim. We structure the remainder as follows. Section 2 de scribes related work and provides some evidence which motivates thi s research. Section 3 demonstrates the importance and potential benefit of search engine switching using large-scale behaviora l datasets. Section 4 describes the machine learning approach t o supporting switching behavior, which is empirically evaluated in Section 5. In Section 6 we discuss the implications of this re search and fu-ture work, followed by conclusions in Section 7. Prior work in search engine switching has sought to characterize the behavior with the goal of developing metrics fo r competitive analysis of engines in terms of estimated user pref erence and user engagement [16], or switching prediction [13]. Oth er work has focused on building conceptual and economic models of search engine choice. Telang et al. [24] proposed a qualit ative model of search engine choice that is a function of the sear ch engine brand, the loyalty of a user to a particular search engine at a given time, user exposure to banner advertisements, and the lik elihood of a within-session switch from the engine to another en gine. Mukho-padyay et al. [18] develop an economic model of sea rch engine http://www.nielsen-netratings.com http://www.comscore.com For example, when a query is typed into a browser X  s address bar, most browsers forward it to the default search engi ne. competition assuming that the switching cost betwee n engines is very low. These studies have focused on understandi ng and cha-racterizing existing switching behaviors in Web search. Although we provide summary statistics on the nature of swit ching from our observations, our objective is not to characterize switching beha-vior. Instead, we demonstrate that the utilization of multiple search engines can be advantageous to users and pro pose a framework that proactively promotes switching. Commercial meta-search engines such as Clusty 4 and Dogpile attempt to provide access to multiple engines. Giv en the ranked lists of documents returned by multiple search engi nes in response to a given query, the objective of meta-search engi nes is to com-bine these lists in a way which optimizes the perfo rmance of the combination. The IR community has studied meta-sear ch in great detail, with the emphasis on how to merge results f rom multiple engines ( e.g., [7,21,23]), rather than on encouraging people to switch engines as we do in this work. Proactive swi tching support is an attractive alternative to meta-search for the following rea-sons: (i) strong brand loyalty may discourage users from migrat-ing to a meta-search engine, (ii) meta-search engin es merge search results and obliterate the benefits of interface fe atures of the indi-vidual engines, and (iii) meta-searching may be dis couraged by search engines as it can negatively impact brand aw areness and advertising revenue. We propose an approach whereby users can use their favorite engine but have an alternate eng ine suggested to them when it is expected to perform better for thei r current query. In some respects, this is similar to distributed IR (c.f. [4]), al-though we are interested in directing users to the best engine ra-ther than the best collection of documents, and do not merge the search results, as is common practice in that sub-d iscipline. Supporting engine switching in real-time requires c omputationally efficient estimation of relative search result qual ity across several engines. Measuring quality of search results via me trics such as precision and recall has been central in driving re search in IR algorithm design, particularly in the Text REtrieva l Conference (TREC) community [11]. Hawking et al. [12] employed a metho-dology similar to TREC to compare the performance o f multiple Web search engines. Others, such as Rorvig [22] and Cronen-Townsend et al. [8], have looked at techniques for predicting the quality of results using the dispersion of the top documents or computing the entropy between the language model fo r the results and the collection as a whole. Leskovec et al. [17] used properties of search result sets projected onto the Web graph to estimate result quality. Despite their effectiveness at comp uting result qual-ity, some of techniques depend on relevance judgmen ts, meaning that they cannot scale to unseen queries, and some are computa-tionally expensive, meaning that real-time computat ion is unfeasi-ble. One key distinction of our work from these ap proaches is that we directly model relative quality of multiple search result sets instead of the quality of any individual resul t set. Our framework relies on a classifier to estimate th e differences in search result quality between the engines using fea tures computed based on the query and the result pages. Yom-Tov et al. [27] have proposed estimating query difficulty using a machin e learning approach based on query-only features, validating i t for a distri-buted IR setting with several collections of newswi re documents, rather than Web search as we do in this work. Capti on features have already been shown to be important to users in determining which search results to select [5], and query-capti on features have http://www.clusty.com http://www.dogpile.com been used in the development of ranking algorithms to improve search [1]. As our empirical results demonstrate, utilizing mul-tiple diverse feature sources is beneficial over qu ery-only features, and is a key performance differentiator for accurat e prediction of the most appropriate search engine for a given quer y in real-time. At the outset of our studies, we pursued general st atistical clues that could provide insight into the extent to which users switched engines and the potential benefit to them of switch ing engines. To do so, we used the interaction logs of a large samp le of consenting Web users. We begin by describing the statistical p roperties of search sessions extracted from the logs. We used the interaction logs of over five million c onsenting Web users over a five-month period from May 2007 to Sep tember 2007. These logs were anonymized, and all personal ly identifia-ble information, including IP addresses, was remove d. The logs gave us access to user interactions with all search engines. From these logs, we extracted search sessions that began with a query to Google, Yahoo!, or Live Search and terminated after 30 minutes of browsing inactivity. 6 A similar threshold has been used to demarcate search sessions in previous work on searc h engine switching [16] and in related studies of user searc h behavior [20,26]. These sessions are used to analyze switchi ng behavior and give insight into the potential benefit of supp orting switching. Our analysis showed that 36.4% of searchers used mo re than one search engine in the duration of the logs. 7 The findings also showed that 6.8% of all sessions and 12.0% of sessi ons containing more than one query involved a switch between two o r more search engines. Although the aim of the paper is no t to character-ize the nature of search engine switching, a visual examination of search engine usage patterns in the logs revealed t hree salient classes of switching behavior: within-session , between-session , and long-term . We now describe these classes and provide sum-mary statistics:  X  Within-session switching: Users switch between Web search  X  Between-session switching: Users switch engines for indi- X  Long-term switching: Users switch from one search engine At the time of writing, together these engines han dle over 80% of worldwide Web search queries according to comSco re. These users submitted five or more queries to at l east two search engines. If we vary this threshold between one and ten queries the proportion of users that switch engines ranges between 54.0% and 26.7%. Of these three classes, our component aims to suppo rt within-session switches, where it might be in a user X  X  int erest to change search engines for the current query. While the abo ve statistics demonstrate that search engine switching is a strat egy employed by some users, the majority of users remain loyal t o a single en-gine. Prior to describing our method for supporting search engine switching, the next section analyzes the potential benefit to users brought by utilizing multiple search engines. To motivate our approach, we first quantify the pot ential benefit of multiple search engine use. That is, if a user i s searching on a given engine, what is the likelihood that they woul d obtain better quality results if they were to issue the same quer y on a different engine. This is important, since encouraging users to switch when it is not in their interests to do so could lead to user dissatisfaction and ultimately distrust for our classifier. To quantify the potential benefit from switching, w e studied user search behavior in the interaction logs described i n the previous section. We used two measures to evaluate engine pe rformance for a given query: relevance score and result click-through rate :  X  Relevance score : The Normalized Discounted Cumulative  X  Click-through rate : The proportion of searches on an engine From each of the search sessions described in Secti on 3.1, we extracted the queries that users issued. We identi fied a set of 4,921 queries that were submitted at least five tim es to each of the three engines in this study: Google, Yahoo!, and Li ve Search. These queries were originally drawn from a larger s et of queries obtained by randomly sampling by frequency a one mo nth query log of one of the search engines ( i.e. , each query had a chance of being selected proportional to its frequency). For each of these queries, trained human assessors assigned judgments to result pages from the live Web (on a six point scale) base d on their per-ceived relevance to the query. This judged set prov ided the basis for evaluation. We computed the relevance scores (NDCG) and the cli ck-through rates on all three engines for each of these querie s. For each query in this set, we ranked the three engines base d on the relev-ance score and their click-through rate to give us two independent rankings for each query. The direct comparison of quality bet-ween these three engines is beyond the scope of thi s paper. None-theless, in Table 1 we present the number (and perc entage) of queries in our query set where each of the three en gines  X  represented in random order as X, Y, and Z to prese rve anonymity  X  outperformed the two other engines in terms of re levance and result click-through rate. The criterion for a long-term switch was a switch followed by no further queries on the prior engine. More relaxed v ariants would lead to the identification of more long-term switch ers. Table 1. Number of queries for which engine perform s best. Search engine Relevance (NDCG) Result click-through X 952 (19.3%) 2,777 (56.4%) Y 1,136 (23.1%) 1,226 (24.9%) Z 789 (16.1%) 892 (18.1%) No difference 2,044 (41.5%) 26 (0.6%) These findings demonstrate that engine choice for a particular query is important, and that a classifier to help u sers select the most effective engine for each of their search quer ies is likely to improve a user X  X  overall search effectiveness, sinc e all engines perform best at some subset of the query set. To estimate the potential for supported search engi ne switching, we once again used the search sessions described in Section 3.1. We extracted all instances of the 4,921 queries fro m these ses-sions and computed the proportion of all query inst ances for which the relevance score or click-through rate wer e higher on an engine other than that selected by the user. Since it considers all query instances this more accurately captures the p otential benefit of switching than the findings presented in Table 1 . Results show that users could benefit from switching engine for around half of their queries ( i.e. , click-through rate higher on alternate engine for 54.5% of query instances, relevance score higher on alternate engine for 52.3% of query instances). To ensure th at we were not simply advocating a switch to a single dominant eng ine, we com-puted the distribution of search engines recommende d across all query instances. This analysis showed that all thr ee engines were recommended approximately equally, alleviating our concerns. These results quantify the benefits of switching, d emonstrating that any given engine performs best for at least so me fraction of search queries. As loyalty and familiarity may dis courage users from switching, our aim is to automatically determi ne when it is in users X  interest to try another search engine. Th e principal chal-lenge for a generic solution to this problem lies i n achieving real-time accurate performance for previously unseen que ries. In the next section, we present our proposed machine learn ing methodo-logy that utilizes features of the query, the resul ts, and the titles/snippets/URLs of the top-ranked pages. As the results in the previous section demonstrate, search engine switching was detected for around half of our five million users, and in 10% of all search sessions. The analysis sh ows that around 50% of all searches may have more accurate results if the query is issued on a different engine. Therefore, a user X  X  s earch experience could be enhanced if they were notified when an alt ernate search engine is likely to provide better results or diffe rent results of same quality, obviating the need to attempt the que ry on an alter-nate engine manually and broadening awareness of ot her engines. Achieving this requires automatically detecting whe ther the re-sults for the current query on an alternate engine are better (or equivalently good but different) than the results f or the currently used engine. The following subsections describe our approach for solving this prediction problem. Comparison of search result sets from any two engin es can be modeled in several ways. One approach is to predict the quality of the results for each engine independently and subse quently com-pare the two scores. An alternative is to consider the two engines simultaneously, where the single prediction objecti ve is to deter-mine whether one engine produces results of better or equal quali-ty than the other engine. Since the underlying prob lem facing the user is a decision task based on the pair of result sets, this  X  X oupled X  approach is a more appropriate abstractio n, and hence is the direction we pursue. Modeling the difference in quality between two sets of search results can be viewed as a regression task (predict ing the real-valued difference in quality between the two result sets), or as binary classification (where the prediction is equi valent to decid-ing whether switching to a different engine is wort hwhile, without directly learning to quantify the expected differen ce in result qual-ity). Among these options, binary classification is a more suitable choice, since it most closely mirrors the switching decision task. The actual utility of switching for a given user de pends on such factors as the relative costs of interruption and b enefits of obtain-ing better and/or different search results, which c an be incorpo-rated into the classification task via the concept of a margin in quality between the two result sets (by assigning  X  positive X  labels to pairs of results sets where the difference in qu ality is above the minimum margin corresponding to switching utility).
 Formally, let a given problem instance consist of a query and two search engine result pages: from the current search engine, and  X  for an alternative search engine. Let query have a hu-man-judged result set  X  = ( , ,...,( , } consisting of ordered URL-judgment pairs, where each judgment ref lects how well the URL satisfies the information need express ed in the engine for the query can be represented as the NDCG score of the returned results set: ( =  X  ( and (  X  = port if the alternative search engine provides util ity that is higher by at least  X  0 . Then, a dataset of queries = (,, ,  X  } yields a set of training instances, = ( , } , where each in-stance = ( , , is comprised of features derived from the query and result pages as described in Section 4.2, and the binary label encodes whether the alternative search engine prov ides performance that is higher than that for the curren t engine by at least : = 1 "  X  (  X   X  ( + ). While any binary classifier can be used for this ta sk, minimizing computational and memory costs is a key considerati on for select-ing an appropriate algorithm. Upon every search exe cuted in the browser, the switching support framework must execu te the same search on alternative engines in the background, su bsequently computing features for the classifier, which then p redicts whether alternative engines should be suggested. Furthermor e, users X  inte-raction with the switching support system may provi de additional training information for the classifier, which call s for classifiers that can be trained in online fashion, where learni ng is performed using a continuously incoming stream of instances w ith labels derived from user interaction ( e.g. , using such indicators of user satisfaction as click-through on the search results page or dwell time on result pages). In this work, we employ maxi mum-margin averaged perceptron [6] as the classifier, since it readily satisfies the constraints above and has previously shown exce llent empiri-cal performance in many domains from natural langua ge to vision. For each query submitted to a current search engine , the classifier must predict in real-time whether the user would be nefit from utilizing a different search engine based on featur es derived from the query and the two sets of results from the two engines. Thus, features can be separated into three broad categori es: (i) features derived from the two result pages, (ii) features ba sed on the query, and (iii) features based on the matching between th e query and the results page. The subsequent sections describe each of these fea-ture sets in detail, while Table 2 provides a compr ehensive list of all features. We employ only generic text-based fea tures that can be obtained for any search result page; the space o f features was determined before running any experiments, and we d id not per-form any feature selection. In Section 5.1.3, we me asure the utility of each feature category to determine their relativ e contributions to the classification task. Each engine X  X  result page contains a ranked list of search results, where each result is described by a title, a snippe t (a short sum-mary), and its URL. The results page features captu re the follow-ing properties of each result:  X  Textual statistics for the title, URL, and the snip pet, such as  X  Properties of the URL, such as whether it comes fro m a .com Furthermore, there are features of the results page not captured by the result lists themselves. For example, search en gines typically inform the user how many total pages in their index contain the given query terms ( e.g. ,  X  Results 1-10 of 64,500  X ). This number is also a feature. Other features encode such results page properties as whether spelling correction was engaged, feature s of any query-alteration suggestions, and features based on any adver-tisements also found on the page. Different search engines may have ranking algorithm s that per-form particularly well (or particularly poorly) on certain classes of queries. For example, one engine may focus on answe ring rare ( X  X ong-tail X ) queries, while another may focus on c ommon que-ries. Thus, features can also be derived from query properties, such as the length of the query, presence of stop-w ords (common terms like  X  the  X ,  X  and  X , etc.), presence of named entities, etc. We designed the third set of features to capture ho w well the re-sults page matches the query. For example, these fe atures encode how often query words appear in the title, snippets , or result URLs, or how often does the entire query, or bigram s within the query appear in these segments. Since search engine s attempt to create a snippet that represents the most relevant piece of a docu-ment, one expects that snippets that contain many m atches of the query are indicative of a relevant result, while fe w or no matches likely correspond to a less relevant result. Following a common practice in machine learning app lications, we provide non-linear transforms of each feature to the learner, so it can directly utilize the most appropriate featur e representation. In this paper, we add the logarithm and the square of each feature value as two additional features. Another group of meta-features are based on combinations of feature values for the two engines, e.g. , a binary feature indicating whether the number of results that contain the query is at least 50% greater in the al ternative engine than in the current engine. Note that simple differ ences between features ( e.g. , the number of results on the alternate engine min us the number on the current engine) are unnecessary, as the percep-tron can model such features by assigning a higher positive weight to the first component of the difference, and a hig her negative weight to the second. These features can all be computed at run time and are all readily available with minimal overhead, and are only a sub set of all fea-tures that could be used. If efficiency constraint s were relaxed, the feature set could be enhanced to leverage the h yperlink struc-ture of top documents (as done in [17]), search res ult clickthrough logs, and search engine response times, among many others. As demonstrated by the analysis in Section 3, no ma tter what search engine is employed by a given user, there ar e always some queries for which other engines provide better resu lts. The objec-tive of providing switching support is to identify such queries automatically using the machine learning methodolog y described in Section 4. Therefore, our goal is to evaluate t he accuracy of the proposed switching support mechanism independently of popular-ity or absolute accuracy of individual search engin es to assess the viability of recognizing queries for which an alter native engine provides better performance. To evaluate the proposed approach for recognizing q ueries for which switching search engines is beneficial, we em ploy a labeled corpus of queries randomly sampled from search engi ne logs. For each query, a panel of human judges evaluated sever al dozen top-ranked results returned by the three most popular s earch engines, assigning them relevance scores on a six-point scal e that range from Bad to Perfect . Human evaluation is performed without any information that may identify engines to remove any individual biases that judges may have. Table 3 below provides some sum-mary statistics for the labeled dataset. Total number of queries 17,111 Total number of judged pages 4,254,730 Total number of judged pages labeled Fair or higher 1,378,011 Given the labeled dataset, the quality of results r eturned by search engines for each query can be evaluated by computin g NDCG against the human judgments as described in Section 3.3. To eva-luate the machine learning approach to switching su pport de-scribed in Section 4.1, we transform the corpus of queries, judg-ments and search engine results into multiple label ed datasets of feature vectors and labels as described in Section 4.2. For every pair of search engines and any predefined margin in NDCG scores required to justify switching for the user, the sampled data-set includes an equal number of positive and negati ve instances corresponding to queries for which switching is ben eficial, and queries for which it is not. Classification experiments are performed using 10-f old cross-validation by separating the dataset for every pair of search en-gines into ten folds of equal size, and repeatedly computing accu-racy on one (testing) fold after training on the re maining folds. The process is repeated over 100 runs with randomiz ed fold as-signment. There are fundamental trade-offs between recall, in terruption, and error cost to the user that switching support must address. If the confidence threshold is low, the user will be infor med of possibly better results provided by the alternative engine m ore frequently, however some suggestions may be erroneous, which co upled with the increased interruption cost is likely to upset the user. There-fore, it is preferable to interrupt the user less f requently, while providing high-accuracy suggestions. Evaluation ca n reflect these considerations by employing precision-recall curves in place of single-point accuracy measurements, where precision and recall are defined as the proportion of true positives (qu eries for which switching is desirable) among (1) all predicted pos itives for preci-sion, and (2) all true positives for recall. We co nstruct precision-recall curves by varying the confidence threshold o f the classifier, starting with a high value, where switching is advi sed in very few cases, resulting in high precision (few erroneous s uggestions) but low recall. Through lowering the confidence thresh old, it is poss-ible to suggest switching for more queries at the c ost of more errors and increased interruption to the user. Figure 1 shows the precision-recall curves that sum marize the performance of the classifier-based approach with r espect to NDCG@3 with = 0 (in other words, equally or more accurate but different results on a different engine compris e a positive ex-ample for predicting switching). These results demo nstrate that the proposed approach can attain very high precisio n at lower recall levels, which are most important if the cost s of user inter-ruption are viewed as non-negligible. Precision dec reases sharply at higher recall levels, eventually dropping to the random prior, which is above 50% because with = 0 , queries on which en-gines obtain equally accurate but different results are viewed as 
Results Page Features 10 binary features indicating whether there are 1-1 0 results Number of results 
For each title and snippet: # of characters in URL # of characters in domain (e.g.,  X  apple.com  X ) # of characters in URL path (e.g.,  X  download/quicktime.html  X ) # of characters in URL parameters (e.g.,  X  ?uid=45&amp;p=2  X ) 3 binary features: URL starts with  X  http  X ,  X  ftp  X , or  X  https  X  5 binary features: URL ends with  X  html  X ,  X  aspx  X ,  X  php  X ,  X  htm  X  # of  X / X  in URL path (i.e., depth of the path) # of  X &amp; X  in URL path (i.e., number of parameters) # of  X = X  in URL path (i.e., number of parameters) # of matching documents (e.g.,  X  results 1-10 of 2375  X ) 
Query Features # of characters in query # of words in query # of stop words ( a , an , the , ...) 8 binary features: Is i th query token a stopword 8 features: word lengths (# chars) from smallest to largest 8 features: word lengths ordered from largest to sm allest Average word length Match Features 
For each text type (title, snippet, URL): # of domains containing the query in the top-1, top -2, top-3 switch-worthy, since the alternative search engine provides the user with novel results of equal quality. The sharp decline in precision at higher recall lev els demonstrates that discriminating between search engines using on ly the query and their result pages is a very difficult learning task. However, since the goal is to only suggest alternative searc h engines when they are likely to provide additional value over th e current search engine, high performance at low recall levels is st ill highly valua-ble as it allows the provision of accurate suggesti ons to the user for a number of queries, while not interrupting the m too often. Table 4 summarizes precision at recall of 0.05 for all engine pairs. These results demonstrate that the machine learning approach we propose for supporting search engine switching can achieve high accuracy, and therefore can be used for providing u seful search engine suggestions to users. The table shows that there are signif-icant distinctions in performance between different engine pairs: e.g. , performance is much higher when identifying queri es on which users of engine X will benefit when switching to engine Z versus switching to engine Y. These differences ar e caused by two factors: (i) the degree to which ranking algor ithms employed by the engines differ, and (ii) the prior probabili ty of obtaining better performance on the alternate engine when swi tching from the given default engine. Because both of these factors can be controlled by varying the margin parameter, , which specifies the minimum difference in result quality considered acceptable for providing the user with a switching suggestion. We investigate the effect th at has on accuracy by varying the value of , and correspondingly changing the classification task to have fewer/more positive examples. Fig-ure 2 demonstrates the precision values at 0.05 rec all averaged over all search engine pairs, for different values of alongside the prior probability of obtaining better results on th e alternate engine. The two values at = 0 denote either labeling queries on which the engines produce different but equally accurate results as posi-tive (switching is beneficial for novel results), o r negative (switch-ing is only desired for higher-quality results). Figure 2. Prediction accuracy for different margin values. These results demonstrate that while our approach i s able to im-prove over the baseline for all margin values, the task becomes significantly harder for larger margin values, sinc e the number of queries for which one engine is better than another by a large margin decreases with margin size. So far, we have seen that we can, with reasonably h igh precision, suggest alternative search engines to users for app ropriate queries. Doing so requires not only analyzing the content of the current result page, but also querying the alternate search engine in the background. For some users and/or search engines, t he resulting extra network traffic may be undesirable. One way t o avoid this is by classifying whether a switch would be beneficial , but using only features based on the current engine X  X  result page. The re-sults for this are given in Figure 3, averaged acro ss both alternate engines for each of X, Y, and Z. From Figure 3. Prediction accuracy using only one engine 's features. As expected, the accuracy is lower than when using the results from both engines . Whether the networking cost to the user and the alternate search engine is worth the boost in a ccuracy is an empirical question that would have to be answered b y the particu-lar user and search engine in question. One intere sting approach would be to use the single-engine classifier as a f ilter to exclude queries least likely to be served better by the alt ernate engine. In order to better understand the utility of variou s features to the overall task performance, we conducted an ablation study in which we removed each of the three feature sets, re trained the classifier, and observed the decrease in performanc e. In Figure 4, we show the results of these ablations. Results pag e features are denoted as R, query-based features are denoted as Q , and match features are denoted as M. As can be seen from thes e results, every set of features is contributing to the overal l accuracy to some degree. However, it is clear from the figure t hat features obtained from results pages are providing the most benefit, since performance decreases most substantially when they are removed. We also tested the performance of the classifier wh en it is pro-vided with only one feature group at a time; result s of these expe-riments are shown in Figure 5, averaged across all search engine pairs. Confirming the ablation study, we again obse rve that fea-tures based on results pages yield most accurate pr edictions among the three groups. The features we used were designed to be efficient in terms of computational and memory requirements, so there is little to gain by removing any of them from the classifier, especi ally given that the above results demonstrate that each of the feat ure groups im-proves performance. The analysis here mostly serves as a guide for investigating new features, and while most bene fit comes from analysis of the results page itself, investigating the utility of addi-tional, possibly server-based features, is interest ing future work. We have described a method for automatically determ ining when user X  X  interests would be best served by switching engines for a given query. The precision and recall values provid ed in the pre-vious section demonstrate that it is possible to ac curately predict whether another engine has better quality results b ased solely on features of the query and search results from alter nate engines. Additional analysis revealed that if an oracle swit cher could per-fectly predict which engine had better results for the queries in our test set, a ten-point gain in NDCG would be observe d. This im-provement would significantly impact users X  search effectiveness. The machine learning framework we proposed in this paper could be implemented as a browser plug-in that would noti fy users in real time when they should consider switching engin es. The tool would alert users whenever another engine could pro vide a better set of search results, or when the user appeared di ssatisfied with the current result set (per negative interaction be haviors such as requesting the next page of search results, not cli cking on any results, or reformulating their current query). It is important to emphasize that our approach can be implemented comp letely client-side without the need for server-side link-g raphs or log-based information that would make meeting the real-time perfor-mance constraints difficult. As part of the study of switching behavior describe d earlier, we identified three classes of search engine switching : within-session, between-session, and long-term. Although our emphas is has been on supporting users who may be willing to switch be tween en-gines within a single search session, it is also im portant to consid-er how to support users in selecting a different en gine for different search sessions. Automatic detection of search task intent and switches between search tasks has been studied exte nsively in the human factors community [3,9], leaving it as an exc iting chal-lenge for future research to develop techniques tha t would provide users with the best multi-engine support at search task level. While we have found that for approximately half of all searches users could retrieve more accurate search results i f they switched search engines, we did not impose any constraints o n the accuracy margin beyond analyzing performance for different m argin values in Section 5.1.1. Thus, we established an upper bo und for accura-cy improvements enabled by switching engines. Prev ious work has shown that users may not notice small differenc es in quality of search results, even though these have been dete cted by evalua-tion metrics [25]. We hope to investigate the rel ative benefits of accuracy increases versus the cost of user interrup tion in future user studies, so that our methodology could provide maximum value to users. As well as predicting the existenc e of higher-quality search results on alternative engines, fact ors that must be considered include understanding users X  focus of at tention, work-load, and willingness to be interrupted, so as to p resent recom-mendations at an appropriate time [14]. It is worth noting that the objective measures of s witching utility do not consider the additional cognitive burden and associated temporal costs on users of this activity. Web sear ch engines exhi-bit differences in their user interfaces, the query syntax they sup-port, and the collection of Web pages they index. T hese distinc-tions may adversely affect users X  ability to locate relevant infor-mation when changing engines. Further work is requi red to under-stand the cognitive costs to users in multiple sear ch engine use. The research described in this paper has shown that it is possible to facilitate switching between search engines in r eal-time; the next step is develop methods that will make the tra nsition between engines maximally smooth for the user. We hope tha t future user studies will help to evaluate the performance of th e classifier with human subjects engaged in realistic task scenarios and quantify the above factors in computing switching utility. In this paper, we advocated for the use of multiple search engines to empower users to search more effectively. We des cribed a log-based study of Web search behavior with a particula r emphasis on multiple search engine use, which demonstrated that search en-gine switching can substantially improve retrieval effectiveness. We proposed a machine learning-based approach for s upporting switching that estimates in real time whether more accurate results exist on alternate search engines. Estimation is ba sed on features of the query, the result set, and the titles, snipp ets, and URLs of the top-ranked search results. An empirical analysi s of classifica-tion performance demonstrates that it is accurate a t predicting when users would benefit from switching between eng ines at low recall levels. The promotion of multiple search eng ine use through application components such as that described has t he potential to improve the retrieval experience for users of all s earch engines. We wish to thank Resa Roth for her invaluable suppo rt and con-structive comments on drafts of this paper. [1] Agichtein, E., Brill, E. &amp; Dumais, S. (2006). I mproving web [2] Buckley, C. &amp; Walz, J. (1999). Overview of the TREC-8 [3] Budzik, J. &amp; Hammond, K.J. (2000). Users intera ctions with [4] Callan, J.P., Lu, Z. &amp; Croft, W.B. (1995). Sear ching distri-[5] Clarke, C., Agichtein, E., Dumais, S. &amp; White, R.W. (2007). [6] Collins, M. (2002). Discriminative training me thods for hid-[7] Craswell, N., Hawking, D. &amp; Thistlewaite, P. (1 999). Merg-[8] Cronen-Townsend, S., Zhou, Y. &amp; Croft, W.B. (20 02). Pre-[9] Czerwinski, M., Horvitz, E. &amp; Wilhite, S. (2004 ). A diary [10] Gordon, M. &amp; Pathak, M. (1999). Finding inform ation on the [11] Harman, D. (1993). Overview of the first text retrieval confe-[12] Hawking, D., Craswell, N., Bailey, P. &amp; Griffi ths, K. (2001). [13] Heath, A.P. &amp; White, R.W. (2008). Defection de tection: [14] Horvitz, E. &amp; Apacible, J. (2003). Learning an d reasoning [15] J X rvelin, K. &amp; Kek X l X inen, J. (2000). Informa tion retrieval [16] Juan, Y.-F. &amp; Chang, C.-C. (2005). An analysis of search [17] Leskovec, J., Dumais, S.T. &amp; Horvitz, E. (200 7). Web pro-[18] Mukhopadhyay, T., Rajan, U. &amp; Telang, R. (2004 ). Competi-[19] Pew Internet &amp; American Life Project. (2005). Search En-[20] Pitkow, J.E. &amp; Pirolli, P. (1999). Mining long est repeating [21] Rasolofo, Y., Abbaci, F. &amp; Savoy, J. (2001). A pproaches for [22] Rorvig, M. (2000). A new method of measurement for ques-[23] Si, L. &amp; Callan, J. (2003). A semi-supervised learning me-[24] Telang, R., Mukhopadhyay, T. &amp; Wilcox, R. (199 9). An [25] Turpin, A. &amp; Hersh, W. (2001). Why batch and u ser evalua-[26] White, R.W. &amp; Drucker, S.M. (2007). Investigat ing beha-[27] Yom-Tov, E., Fine, S., Carmel, D. &amp; Darlow, A. (2005). 
