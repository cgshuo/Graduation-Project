 Traditional ad hoc retrieval models do not take into account the closeness or proximity of terms. Document scores in these models are primarily based on the occurrences or non-occurrences of query-terms considered independently of ea ch other. Intuitively, documents in which query-terms occur closer together should be ranked higher than documents in which the query-terms appear far apart.

This paper outlines several term-term proximity measures and develops an intuitive framework in which they can be used to fully model the proximity of all query-terms for a particular topic. As useful proximity functions may be con-structed from many proximity measures, we use a learning approach to combine proximity measures to develop a useful proximity function in the framework. An evaluation of the best proximity functions show that there is a significant im-provement over the baseline ad hoc retrieval model and over other more recent methods that employ the use of single proximity measures.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Retrieval models, Search Process Experimentation, Performance Information Retrieval, Learning to Rank, Proximity
Ad hoc retrieval functions typically use occurrences of the query-terms in the document and collection to determine the usefulness (or weight) of each term in a particular doc-ument. These weights are aggregated and normalised for a specific document so that a final score can be assigned to the document for a particular topic. Vector space models [17], probabilistic models [9] and language models [12] all sim-plify their approaches by adopting the term-independence assumption. This assumption ignores the relationship be-tween individual terms with regard to proximity , position and synonimity . We tackle the problem of incorporating the first of these characteristics, that of the proximity betwee n terms, with regard to document scoring in the traditional ad hoc retrieval task in an intuitive information retrieval (I R) model.

The contributions of this paper are four-fold:
The remainder of the paper is organised as follows: Sec-tion 2 summarises previous work into proximity in IR. We outline the strengths and limitations of previous work. Sec -tion 3 outlines an extensive list of possible term-term prox -imity measures. We also perform an analysis of each of the independent measures with respect to relevance. Sec-tion 4 details the framework into which a possible proximity function may be placed in order to complete the IR model. Section 5 presents the experimental setup and retrieval fun c-tions used. Section 6 presents the experimental results. Ou r conclusions and future work are detailed in section 7.
There has been many recent attempts to incorporate prox-imity into IR models [4, 3, 18, 2, 1, 11]. The relatedness of terms in a semantic sense can be mapped to a proximity measure [3]. Proximity in IR has been researched for some time and semantic proximity (or related-ness between terms ) has been incorporated into retrieval systems using query ex -pansion techniques to help overcome the problem of term mismatch.

However, as we wish to explore the actual closeness of terms in a segment of text, we define proximity as the sim-ilarity of query-terms within a sample of text. Thus, if the proximity between two terms is 0, they are not in the same grouping. This same grouping may be defined at different levels of granularity, for example, at the document level. This paper deals with proximity within documents only. The underlying hypothesis being that documents in which query-terms appear closer together are more useful to the user (i.e . have a higher degree of relevance).

Boolean (or set-based) retrieval models often have opera-tors to measure the proximity of certain terms within doc-uments. Work has been presented [2] that makes use of a fuzzy set theoretic measure of proximity in a Boolean model of retrieval. This framework is elegant and presents result s for a number of different system parameter settings. Previ-ous research [14] has used a window or passage method to determine proximity within a certain threshold. The work shows that proximity can increase performance on small col-lections.

Proximity information is incorporated into an existing ad hoc retrieval function (used as an underlying framework) to improve the performance of short queries [15]. They create a proximity function at the sentence level, whereby if two query terms appear within the same sentence the document score will be increased. This score is also subject to the distance between the terms in the sentence. They show that modest improvements on larger ad hoc retrieval TREC data can be achieved. More recently some approaches have been successful in employing proximity into a number of keyword based retrieval functions [18]. The work shows that a search of the space of proximity functions is non-trivial. They sho w significant improvements for short queries.

However, we have identified several limitation of the re-search conducted to date. Some of the research looks at only the closest pair of query-terms in a document and modifies the document score accordingly, while ignoring other query -terms in the document [18]. A complete proximity function should include the relationships between all query-terms. In much, if not all, of the literature, only short queries are us ed. It has also been shown that longer queries are more difficult to improve using a proximity measure [1]. The use of shorter queries simplify the number of term-term proximity relatio n-ships, as there may be more complex interactions when there are several term matches between a query and document. It is not clear if significant increases in retrieval effectiven ess can be achieved using longer queries. This question has been largely unexplored.

Most of the research to-date has only constructed prox-imity functions from independent proximity measures. It is a simpler task to measure independently the benefit of individual proximity measures to retrieval performance in a specific framework. However, a better proximity function may well consist of a combination of proximity measures and normalisation factors. A proximity function which uses mul -tiple measures of proximity may outperform these simpler approaches. In much of the research, the models adopted augment a traditional keyword-based retrieval model with a proximity function in an unmotivated, ad-hoc and non-intuitive manner.

This paper aims to address all of the aforementioned is-sues. We aim to create a proximity function which deals with all possible query-terms, without the need for arbi-trary thresholds or parameters and which fits neatly into an intuitive framework for retrieval.
This section outlines several individual term-term proxim -ity measures, measures which capture proximity of all terms in the query and also outlines some normalisation measures. For the term-term proximity measures outlined, it is neces-sary that the measure is symmetrical. For a specific term-term proximity measure ( pm ( a, b )) which measures the prox-imity between term a and b , we wish to find measures where pm ( a, b ) = pm ( b, a ). This definition of proximity is intuitive for proximity as defined in this work (although it may not be a valid assumption if one is to define proximity in terms of synonimity or semantic relatedness).

We wish to search for proximity functions which incorpo-rate relationships between all query-terms and as such we first consider the pairwise similarity between terms. There -fore, when considering two terms a and b occurring in a document D , we consider measures which can be calculated using both terms X  position vectors. We define a position vec-tor as the list of integer positions in the document D where a term occurs. The following sample document D will be used to explain how each of the proximity measures can be calculated for a query Q with query-terms a and b : D = {a b c d a b d e f g h a i j} Q = {a b} For simplicity, we ignore paragraph and sentence bound-aries. Therefore, the positions of each term reflect the ac-tual ordering in which the terms occur in the document. Let pos D a denote the vector of integer positions of a in document D and let tf D a be the term-frequency of a in document D . Therefore, pos D a = { 1 , 5 , 12 } , pos D b = { 2 , 6 } , tf b = 2. In this work, we are aiming to develop measures which capture proximity information from all of the query terms.

The 12 measures in this section relate to proximity ei-ther implicitly or explicitly. Many of these measures can be easily calculated using the position vectors of each term . The term-term proximity measures 1 to 6 deal with the dis-tance between the positions of a pair of terms in a document. These measures can be seen as explicitly capturing proxim-ity in some way. Measures 7 and 8 may capture proximity implicitly by combining the term-frequencies of each term in the document. Measures 9 and 10 capture information regarding the terms in the entire query. Measures 11 and 12 are potentially useful normalisation measures. 1. min dist ( a , b , D ) is defined as the minimum distance 2. diff avg pos ( a , b , D ) is defined as the difference be-3. avg dist ( a , b , D ) is defined as the average distance 4. avg min dist ( a , b , D ) is defined as the average of the 5. match dist ( a , b , D ) is defined as the smallest distance 6. max dist ( a , b , D ) is the maximum distance between 7. sum ( tf ( a ) , tf ( b )) is defined as the sum of the term-8. prod ( tf ( a ) , tf ( b )) is defined as the product of the 9. fullcover ( Q , D ) is defined as the length of the doc-10. mincover ( Q , D ) is defined as the length of the doc-11. dl ( D ) is defined as the length of the document and is 12. qt ( Q , D ) is defined as the number of unique terms that
It can be noted that some of these proximity measures are similar to those in [18]. However, all the measures in that work are query specific, meaning that they only calculate the proximity for the closest pair of terms in a query or use a type of span measure to capture the minimum portion of text which covers occurrences of the query terms. In fact, of the 12 measures outlined here, only mincover and f ullcover are the same measures used previously [18].
In this section, we perform an analysis of each of the inde-pendent proximity measures to predict which measures may be most useful when incorporated into a proximity measure. For our analysis and subsequent experiments, we use the FBIS, FT, FR collections from TREC disks 4 and 5 as test collections and topics. Some characteristics of the collec -tions are indicated in Table 1. For each set of topics we create a short query set, consisting of the title field of the topics and a medium length query set, consisting of the title and description fields. We also use the OHSUMED collec-tion and its topics. Table 1 shows some of the characteristic s of the collections used in this research. We stemmed the col-lections using Porter X  X  algorithm [13] and removed standar d stop-words.
Our aim is to incorporate a proximity function into an existing term-weighting scheme. Therefore, we can view the problem as performing re-ranking on the top N documents from an initial ranked list using a proximity function on the query-terms. This also ensures that these N documents have an ample supply of query-terms. Consequently, we perform an analysis of the top 1000 documents from a retrieval run and examine the correlation between the measures outlined in the previous section and the relevant and non-relevant documents in this set of documents.

Table 2 shows the average values of the individual mea-sures per query across all of the test collections (for relev ant and non-relevant documents respectively). For example, fo r short queries, the average min dist between query terms in the relevant documents is 39 . 2, while it is 172 . 4 in the non-relevant documents. The percentage of queries for which this inverse relationship holds is also indicated. For exam -ple, in 83 . 5% of short queries, the min dist is a smaller value in the relevant documents than in the non-relevant documents across all of the collections. However, on the collections used we have determined that the consistency of the correlation and the average proximity measure in the relevant and non-relevant document does not always agree. For example, consider the avg dist proximity measure for the medium length queries. We can see that the measure seems directly correlated with relevance as the avg dist av-eraged over all queries is higher in the relevant documents (i.e. 250.0 compared to 235.1). However, we can see that in fact it is inversely correlated with relevance in 68% of the queries. Thus, there is a difference between the strength of the difference of each proximity measure in the relevant and non-relevant documents and the degree to which they are correlated with relevance. The asterisk (*) indicates that there are certain collections in which the consistency of correlation does not agree with the average difference in magnitude of the measure in the relevant and non-relevant document sets.

Of the six pairwise proximity measures, we can see that min dist , avg min dist and match seem to have the strongest inverse correlation with relevance. Conversely, we can see that the qt measure is directly correlated with relevance. In only 18 . 5% of queries, the number of distinct query terms ( qt ) is smaller in the relevant documents than in the number of non-relevant documents for short queries.

However, it should be noted that some of the proxim-ity measures may be correlated with each other implicitly and a combination of such measures may not increase per-formance. Some measures may be correlated to relevance because they are highly correlated with the original rankin g (which is correlated to relevance). Since ranking function s tend to promote shorter documents with more occurrences of query terms, many measures of proximity will be implic-itly correlated to relevance. As a result, incorporating th em in a ranking function may bring about no increase in per-formance ( MAP ). Due to the complexity of the problem, the potential number of measures and multitude of combi-nations that may exist, it is difficult to perform an in-depth analysis of the potential benefit of each proximity measure in isolation.
Our goal is to create a model of retrieval which incorpo-rates proximity into an intuitive model. Therefore, we ex-tend a vector (or bag of words) model into one which exploits proximity or closeness between pairs of terms. As most tra-ditional keyword models of retrieval can be viewed as vector type approaches, we extend this model so that documents and queries are viewed as matrices.

Consider the following 3  X  3 matrix representing a docu-ment D matching 3 query-terms ( Q ). The document may have many terms but we only consider terms that match the query in the scoring process. Let the diagonals be the relatedness between a term and itself and the non-diagonal entries be the proximity (or closeness) between a pair of dis -tinct terms. The query representation is kept simple due to the relative size of the query compared to the document. Thus, the following document representation is equivalent to a vector based model as term-independence is assumed (i.e. the proximity between two distinct terms is 0 and w ( t 1 some tf -idf type score for each term).

If we assume that the weights of the diagonal entries are weights in a traditional retrieval model (e.g. tf -idf or BM 25), the model can be simply extended by defining the non-diagonal elements as a useful proximity function ( p ()). Thus, the proximity function is some combination of the set of 12 proximity measures introduced earlier that define term-ter m closeness or proximity as defined in this work. Again, we can assume a simple weighting on the query-terms due to the rel-ative size of the query in comparison to the document (even for a query of 10 or so terms). Thus, the representation of a document in our model is as follows: where w () is a standard term-weighting scheme and gives us the weight of term in a document and p () is a proxim-ity function. Usually, a document can be represented by a vector and the final score is some aggregation of those weights. Therefore, the entire score of the document can now be defined as the sum of all combinations of term-term relationships: which aggregates all of the elements of the document ma-trix (as we use simple weighting on the query terms). As all measures of proximity introduced earlier are symmetrical, p ( i, j ) will equal p ( j, i ) and each term-term proximity score in the framework is counted twice. We allow this as the framework can then also be used to model semantic simi-larity (i.e. where p ( i, j ) 6 = p ( j, i )) using a different set of semantically related term-term measures.

While this framework, like many others, has no theoretical basis, it is an intuitive extension of a vector based approac h. Indeed, there is no theoretical basis for mapping documents into a Euclidean space at all. If we assume that proxim-ity has no effect or that term occurrence is independent we can set p ( i, j ) = 0  X  i 6 = j and recover a standard vector based retrieval model. Now, assuming that proximity is an important measure in a retrieval model, we need to instan-tiate the proximity function p () in our framework. Previ-ous research has indicated that this is a non-trivial proble m and developed a constraint which helped to guide the search for a useful function [18]. However, this constraint is only useful when using the individual measures for proximity in isolation. We wish to find a useful proximity function, by combining some or all of the 12 proximity measures identi-fied earlier. In order to achieve this, we employ the use of genetic programming (GP) [10] and search the space of prox-imity functions in a guided manner, as GP has previously been used in IR to search for useful ranking functions [19, 5]. The next section introduces this GP process and exper-iments which aim to instantiate useful proximity functions in our framework.
In this section, we outline the benchmarks and the ini-tial term-weighting schemes into which we can incorporate a proximity function. We also provide a brief description of the GP process before outlining the settings for the GP process for which we use to create proximity functions.
As a benchmark against which to test our approach we use the traditional BM25 scheme.
BM 25( Q, D ) = X where tf D t is the frequency of term t in document D , dl is the document length, df t is the document frequency of term t and dl avg is the average document length in the en-tire collection. k 1 is the term-frequency influence parameter which is set to 1.2 by default. The query term weighting used here ( tf Q t ) is slightly different to the original weight-ing method proposed [16] but has been used successfully in many studies [7]. b is the document normalisation influence tuning parameter and has a default value of 0.75.
We use the BM 25 matching function and the proximity function previously developed by Tao [18] as another bench-mark. The proximity function t () is as follows: where min dist ( Q, D ) is the minimum distance between is stated that  X  set to 0.3 provides stable performance and thus we use this setting for our experiments [18].
The term-weighting scheme upon which we base the prox-imity functions in these experiments is based on a previousl y learned formula [6] and is as follows:
ES ( D, Q ) = X where cf t is the frequency of t in the entire collection. The choice of underlying ranking function is not crucial for the development of a proximity function. Although the perfor-mance of a specific instantiation of a proximity functions is dependent on the underlying ranking function, it is our intent to show that the process can find useful proximity functions and that these are indeed general across different test collections (not that the proximity functions are gene ral for a multitude of ranking functions). The process outlined here can be repeated for different underlying ranking func-tions, like BM 25, but this experimentation lies outside the scope of this paper.

The benchmark proximity functions can be instantiated by simply adding t () to the score of the original ranking function. We will label these functions BM 25 + t () and ES + t (). In this section, we have outlined two ranking functions ( BM 25 and ES ) and a baseline proximity function ( t ()) which can be used with each.
GP is a heuristic stochastic search algorithm, inspired by natural selection, and is useful for navigating large compl ex search spaces. Initially, a population of solutions is crea ted randomly (although other approaches seed the initial popu-lation with known solutions). The solutions are encoded as trees. Each tree (genotype) contains nodes which are either functions (operators) or terminals (operands). Each solu-tion is rated based on how it performs in its environment. This is achieved using a fitness function. Having assigned 1 This is different that the min dist ( a, b, D ) defined in this work which specifies which pair of query-terms to use. the fitness values, selection can occur. Individuals are se-lected for reproduction based on their fitness value. There are various different methods used to select individuals but all are based in some way on the fitness of the individual. As a result, fitter solutions will be selected more often. Once selection has occurred, recombination can start. Re-combination creates new solutions for the next generation by use of crossover and mutation. Recombination occurs until the population is replaced by new solutions. The pro-cess usually ends when a predefined number of generations is complete. Some important parameters in the GP process are the population size, the number of generations for which to run the GP, the function set and the terminal set. Genetic programming has been shown to be useful for finding solu-tions in complex search spaces such as combining sources of evidence in IR. One of the big advantages conferred by the GP approach over other machine learning solutions is that the resulting learned solutions are available in reada ble format rendering them amenable to further analysis.
We used a subset of the Financial Times collection as a training collection for the GP (FT-TRAIN in Table 1). We took 69,500 documents and 55 queries (in the range of topics 301-400). We used 25 title only queries and 30 title and description queries. The slightly longer queries will help the GP to learn general term-term proximity functions as there are more pairwise interactions in these longer querie s (( n 2  X  n ) / 2 possible relationships for each query). The fitness function used in the experiments is MAP as it is a stable measure of IR performance. In fact, we learn a function which is twice the value produced by a proximity function for a pair of terms in the the framework outlined earlier (i.e . 2  X  p ()). Therefore, we can score the proximity part of each document in ( n 2  X  n ) / 2 steps.

We ranked the documents using the ES scheme and calcu-lated all the proximity measures for the top 3000 documents. We used all of the proximity measures previously introduced as input terminals to the GP (which was developed using ex-isting software [8]) along with three constants for used for scaling (i.e. { 1 , 10 , 0 . 5 } ) . We also used the following func-tions as inputs to the GP : +,  X  ,  X  , / ,  X  , sq (), log (). We then ran the GP six times with an initial random pop-ulation of 2000 for 30 generations using an elitist strategy (i.e. where the best individual is automatically copied to the next generation). We then chose the best function from each of the six runs as a proximity function (labelled p 1() to p 6() in our experiments). In this section, we outline the results of our experiments. We conducted a Wilcoxon signed-rank test on the proximity functions when compared to the underlying ranking func-tion. For the following results,  X  and  X  indicate that the results are significant at the 0 . 05 and 0 . 01 level respectively. The best result on a collection is in bold text. In all cases we are testing the significance with respect to the underlyin g ranking function. For example, BM 25 + t () is compared to the BM 25 function, while ES + t () is compared to the ES function.
Table 3 shows the results of the best three runs of the GP on the training data. The only two formula which are significant on the training data are p 5() and p 6(). On the training data, the t () function does not lead to a significant increase although a small increase is seen when compared to both underlying ranking functions.

The three best functions produced by the GP are as fol-lows: 2  X  p 2() = log ( 10 2  X  p 5() = (((((( log ( f ullcover ) 2  X  p 6() = ((3  X  log ( 10
From inspection, it can be seen that min dist appears in all of the functions found by the GP. avg dist also appears in all of the best functions suggesting that there is also benefi t in incorporating this measure into proximity functions. Th e best two functions ( p 5 () and p 6 ()) also make use of the sum and prod measures. However, as is the case with any ma-chine learning approach, it is possible to have over trained , or over-fitted, for the data. In order to properly test the usefulness of the learned functions, we need to test over un-seen data. The next section compares the chosen functions on unseen test data.
The results on the test data show that measures of prox-imity and the information contained therein are useful in increasing performance of IR systems. We attain a notable improvement over existing benchmarks by learning a suit-able means to combine intuitive proximity measures. This is reflected in the increased MAP scores obtained (Table 4). We can see that the performance of p 6() in terms of MAP is significantly better than that of its underlying function on most test collections. p 5() also shows significant improve-ment on many of the collections. We can see that there is little or no improvement for p 2() which mirrors the results of the significance tests on the training data.

The t () function also performs poorly on many of the col-lections especially for longer queries. This shows that it i s important to incorporate the proximity relationships of al l the query-terms into one function for these longer queries. The t () function only incorporates the relationship between the closest query-term pair in the document to augment the retrieval score. For longer queries, this becomes problem-atic as the closest pair of query-terms may be less important terms with respect to the entire query.

We also compare the benchmarks with the best evolved proximity function ( p 6()) in terms of the precision achieved at 10 documents (Table 5). Again, an improvement is achieved over the standard ranking functions.
We have outlined an extensive list of measures that may be used to capture the notion of proximity in a document. We have indicated the potential correlation between each of the individual measures and relevance. From this analysis, we have indicated that min dist is highly correlated with relevance which corresponds with previous work [18]. We outline an IR framework which incorporates the term-term similarities of all possible query-term pairs. This framew ork leads to a ranking function into which a proximity function can be placed. We adopt a population based learning tech-nique (GP) which learns useful proximity functions in the framework developed. Using this learning approach, we de-velop six functions, three of which are presented here.
Finally, we evaluate three of our learned proximity func-tions on test data and show that they outperform previous benchmarks, particularly for longer queries. It is interes t-ing to note that the proximity functions presented in this paper achieve an improvement for both short and longer queries. Previous approaches have failed to demonstrate an improvement using proximity measures for longer queries. We believe this is due to limitations of the chosen proximity measures used in previous research; previously used mea-sures did not take into account evidence provided by all the proximity relationships among query terms. We have also included measures which consider more than just a limited subset of the query terms proximity scores. Such measures (e.g avg dist ) take into account proximity scores between all occurrences of query terms.

The research described in this paper demonstrates that it is possible to use combinations of proximity measures to improve the performance of IR systems for both short and long queries. Future work will involve further exploration of the learned functions and other types of proximity that could be used in the framework.
The work presented in this paper has been funded by Sci-ence Foundation Ireland under Grant No. SFI/08/CE/I1380 (Lion-2). [1] Jing Bai, Yi Chang, Hang Cui, Zhaohui Zheng, [2] Michel Beigbeder and Annabelle Mercier. An [3] M. P. S. Bhatia and Akshi Kumar Khalid. Contextual [4] Stefan B  X  uttcher, Charles L. A. Clarke, and Brad [5] Ronan Cummins and Colm O X  X iordan. Evolving local [6] Ronan Cummins and Colm O X  X iordan. An axiomatic [7] Hui Fang and ChengXiang Zhai. An exploration of [8] Adam Fraser. Genetic programming in c++. Technical [9] K. Sparck Jones, S. Walker, and S. E. Robertson. A [10] John R. Koza. Genetic Programming: On the [11] Seung-Hoon Na, Jungi Kim, In-Su Kang, and [12] Jay M. Ponte and W. Bruce Croft. A language [13] M.F. Porter. An algorithm for suffix stripping. [14] Bruno P X ossas, Nivio Ziviani, and Jr. Wagner Meira. [15] Yves Rasolofo and Jacques Savoy. Term proximity [16] Stephen E. Robertson, Steve Walker, Micheline [17] G. Salton, A. Wong, and C. S. Yang. A vector space [18] Tao Tao and ChengXiang Zhai. An exploration of [19] Andrew Trotman. Learning to rank. Inf. Retr. ,
