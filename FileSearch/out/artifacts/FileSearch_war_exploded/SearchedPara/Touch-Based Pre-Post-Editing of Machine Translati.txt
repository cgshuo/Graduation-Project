 As shown by oracle studies (Wisniewski et al., 2010; Turchi et al., 2012; Marie and Max, 2013), Statistical Machine Translation (SMT) systems produce results that are of significantly lower qual-ity than what could be produced from their avail-able resources. As a pragmatic solution, human intervention is commonly used for improving au-tomatic draft translations, in so-called post-editing (PE), but is also studied earlier in the translation process in a variety of interactive strategies, in-cluding e.g. completion assistance and local trans-lation choices (e.g. (Foster et al., 2002; Koehn and Haddow, 2009; Gonz  X  alez-Rubio et al., 2013)). Al-though interactive machine translation does facil-itate the work of the SMT system in certain situa-tions by allowing it to make efficient use of knowl-edge contributed by the human translator, post-editing has been shown to remain a faster alter-native (Green et al., 2014). Nevertheless, this ac-tivity usually requires complex intervention from an expert translator (Carl et al., 2011).

In this work we reduce interaction with an SMT system to its most basic form: similarly to what a human translator is likely to do when first reading a draft translation to post-edit, we require a user to simply spot those segments of a draft transla-tion that can participate in an acceptable transla-tion. The corresponding information is then used by a SMT system in a soft way to improve the draft translation. This process may be iteratively repeated as long as enough improvements are ob-tained, and terminates with classical post-editing on the obtained translation, hence we dub it pre-post-editing (PPE). We resort to simulated pre-post-editing and post-editing, as in other works (Carl et al., 2011; Denkowski et al., 2014), to measure translation performance on some avail-able reference translation using both classical met-rics and a variant of the TER metric (Snover et al., 2006), where, essentially, the cost of a token matching operation is a parameterized fraction of the cost of the other token edit operations. With the implementation of appropriate strategies in the SMT system, we show under reasonable assump-tions that this approach has the potential to signifi-cantly reduce the amount of human effort required to obtain a final translation.

In the remainder of this article, we describe the technical details of pre-post-editing (Section 2), report experiments conducted on two translation directions and two domains (Section 3), and fi-nally discuss our proposal and introduce our future work (Section 4). In our PPE framework, the human pre-post-editor has to mark n -grams from a translation hypoth-The annotated n -grams are counted, as an n -gram can appear more than once in the same sentence, and a  X  X ositive X  6-gram language model (LM) ( positive-lm ) is trained on these counts 2 . A  X  X egative X  LM ( negative-lm ) is also trained on the counted n -grams left unannotated. Then, all bi-phrases from the SMT system X  X  phrase ta-ble that match an annotated n -gram, according to the source token alignments provided by the de-coder, are removed from the main phrase table and stored in a separate  X  X ositive X  phrase table ( positive-pt ). Conversely, n -grams contain-ing at least one token left unannotated are consid-ered as incorrect, and the set of bi-phrases match-ing these n -grams are removed and stored in a  X  X egative X  phrase table ( negative-pt ).

As source tokens can appear more than once in a source text, they are located : an identifier is con-catenated to each token to make it unique in the source text. Tokens of the source phrases in the phrase table are accordingly also located, so each bi-phrase is duplicated as needed to cover all lo-cated tokens. Using located tokens allows our PPE framework to treat differently source tokens that are correctly translated from incorrectly translated ones in the same sentence or text. Figure 1 shows an example of phrase table extraction, using lo-
If an n -gram is annotated as correct, all its in-ner n -grams of lower order are also deemed cor-rect. Although annotating translations of high quality may be less expensive by explicitely anno-tating incorrect n -grams instead of correct ones, such annotations would not permit to identify cor-rect n -grams inside incorrect ones, as illustrated in Figure 2. PPE can thus be worded as a simple problem for the pre-post-editor: which sequences of tokens should appear in the final translation? along with the remainder of the original phrase table and the original LM, are used to re-decode the source text in a first iteration of PPE. A new PPE annotation can then be performed on the new translations. The newly extracted  X  X ositive X  and  X  X egative X  phrase tables are merged with the cor-responding phrase table of the previous iteration. The extracted n -gram counts from the current iter-ation and the counts of the previous iterations are summed, and the LMs are re-trained with the up-dated counts. A new iteration of PPE is then per-Figure 1: Examples of some of the bi-phrases and n -grams extracted for phrase tables and language models according to a reference translation. Figure 2: Annotation example for two correct to-kens forming an incorrect n -gram. At the first PPE iteration a reordering is performed and the new hy-pothesis now matches the reference translation. formed with the updated models. The weights for all, old or new, models in the log-linear combina-tion are found by tuning on a development set for tions of PPE from an initial translation hypothesis assuming a given target reference translation. 3.1 Data and systems We ran experiments on two translation tasks of different domains: the WMT X 14 Medical trans-lation task ( medical ) and the WMT X 11 news translation task ( news ) for the language pair en-fr on both translation directions. For both tasks we trained two competitive phrase-based SMT sys-tems using Moses (Koehn et al., 2007) and WMT including our iteration-specific PPE systems, was performed with kb-mira (Cherry and Foster, 2012). 3.2 An adapted evaluation metric: TER PPE Classical MT evaluation metrics cannot take into account the interactive cost of PPE, and thus do not allow us to make direct comparisons with PE. We thus adapt the TER (Snover et al., 2006) metric, which typically uses 4 types of token edits: substitution ( s ), insertion ( i ), deletion ( d ) and shift ( f ). While these edit types all have a (debatable) uniform cost of 1, the operation of match ing ( m ) a correct token is ig-nored. We posit that this operation is in fact per-formed by a human translator during PE (at the minimum, by recognizing and skipping tokens), and that it can be directly compared to our touch-based selection of tokens for PPE. However, we cannot at this stage of our work provide a realistic cost value for this operation, and so we introduce a match cost parameter  X  , and use the following as our PPE-aware metric: where r is the number of tokens in the reference translation. Note that a null value for  X  makes TER PPE correspond to TER, while a value of 1 would indicate that a token matching / touch ( m ) is e.g. as costly as a token rewriting ( s ). We antic-ipate that a realistic value for  X  given a reasonably skilled user should be rather small, but we will provide TER PPE results for the full range [0 , 1] . 3.3 Experimental results To validate our approach, we initially used a sim-ulated post-editing paradigm (Carl et al., 2011; Denkowski et al., 2014) in which non-post-edited reference translations are used in lieu of human post-editions. Results on TER (Snover et al., 2006) and BLEU (Papineni et al., 2002), tuning on both metrics, are provided in Tables 2 ( news ) and 3 ( medical ).

First, we observe that whatever the metric and the task, the first iteration of PPE always yields a significant improvement over the Moses initial system (e.g. up to +9.8 BLEU and -8.2 TER for news fr  X  en). Unsurprisingly, tuning on a met-ric yields better results for the same metric for the first iteration; however, we note that this is not always true for the TER metric at later itera-tions (cf. news en  X  fr). More generally, tuning on the TER metric results in lower improvements for news , which are mostly concentrated on the first iterations; as systems tuned on BLEU have been found to produce better translations than sys-tems tuned on TER (Cer et al., 2010), only BLEU tuning was used for medical . 7
Improvements follow an interesting pattern over PPE iterations: for instance, on news fr  X  en, BLEU scores steadily increase after each new touch-based iteration and reach a gain of +21.1 BLEU and -12.3 TER over the initial Moses translation after 5 PPE iterations. Re-sults are very comparable on both language pairs and both domains, e.g. gains of +12.1 BLEU and -9.7 TER are obtained on fr  X  en medical . The lesser amplitude of the gains obtained after 5 iterations may be attributed to the higher ini-tial quality of the translations in the medical task (e.g. 37.1 BLEU vs 28.6 BLEU in fr  X  en for Moses with BLEU tuning).

Figures 4 and 5 show how our TER PPE metric varies for different values of our  X  parameter (re-call that  X  = 0 corresponds to TER). Essentially, whatever the value of  X  , we observe that any it-eration of PPE dominates PE ( Moses 1-best ), but with a tendency to become as costly as PE for high, but probably unrealistic values of  X  . Tuning with BLEU allows us to bring regular improve-ments as the number of iteration increases, while tuning with TER makes the amplitude of the gains decrease faster.

Furthermore, results shown in Table 4 point out the complementarity between negative models ( negative-lm and negative-pt ) and positive models ( positive-lm and positive-pt ), with a drop of almost 10 BLEU points compared to the corresponding config-uration using all models when removing one type of models on both translation directions. The language models ( negative-lm and positive-lm ) seem to play a more impor-tant role during PPE than the phrase tables ( negative-pt and positive-pt ), with a drop of 9.6 BLEU points on news fr  X  en when removing the language models against a significantly lower drop of 4.4 BLEU points when removing the phrase tables.
Figure 4: PPE results on the en  X  fr news task. We have introduced pre-post-editing , a minimalist interactive machine translation paradigm where a user is only asked to spot text fragments that may be used in the final translation. Our approach is quite comparable to the two-pass procedure de-scribed by Luong et al. (2014) using word-level confidence estimation (e.g. (Bach et al., 2011)) to update the cost of the search graph hypotheses. However, contrarily to Luong et al. X  X  work, our PPE framework is efficiently multi-pass, updates the models over iterations and relies on more in-formative annotations made at n -gram-level. Our evaluation based on simulated post-editing has re-vealed a large potential for translation improve-ment. Interestingly, the type of interaction defined
Figure 5: PPE results on the fr  X  en news task. is very different from that expected of a post-editor or in existing interactive translation modes, and lends itself nicely to touch-based interaction. Fur-thermore, our proposal may in fact define a new role in Computer-Assisted Translation, with PPE being performed on-the-go on mobile devices by more people than available human translators, and even possibly by monolinguals of the target lan-guage whose contribution may be more efficiently exploited than that of monolinguals of the source language (e.g. (Resnik et al., 2010)).

In terms of usability, our future work will fo-cus on two important questions: ( a ) study the actual use of PPE in an interactive setting and tune the  X  parameter for our TER PPE metric on HTER (Snover et al., 2006) traces, and ( b ) study whether PPE alters in any positive way the work of the human translator performing the resid-ual post-editing, hoping that PE could become a less tedious task by nature . We further an-ticipate that some additions would improve our approach, including dealing early with out-of-vocabulary phrases, proposing local drop-down options (e.g. (Koehn and Haddow, 2009)), possi-bly clustered by senses, allowing the user to eas-ily fix reordering issues, and adapting PPE to be discourse-aware (e.g. (Ture et al., 2012)). The authors would like to thank the anonymous reviewers for their helpful comments and sugges-tions. The work of the first author is supported by a CIFRE grant from French ANRT.

