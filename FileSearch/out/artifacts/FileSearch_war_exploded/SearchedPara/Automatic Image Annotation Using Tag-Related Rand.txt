 In this paper, we propose a novel image auto-annotation model using tag-related random search over range-constrained visual neighbors of the to-be-annotated image. The pro-posed model, termed as TagSearcher, observes that the an-notating performances of many previous visual-neighbor-based models are generally sensitive to the quantity setting of visual neighbors, and the probabilities for visual neigh-bors to be selected is better to be tag-dependent, meaning that each candidate tag can have its own trustworthy part of visual neighbors for score prediction. And thus TagSearcher uses a constrained range rather than an identical and fixed number of visual neighbors for auto-annotation. By per-forming a novel tag-related random search process over the graphical model made up of range-constrained visual neigh-bors, TagSearcher can find the trustworthy part for each candidate tag, and further utilize both visual similarities and tag correlations for score prediction. With the range constraint for visual neighbors and the tag-related random search process, TagSearcher can not only achieve satisfac-tory annotating performances, but also reduce the perfor-mance sensitivity. Experiments conducted on benchmark Corel5k well demonstrate its rationality and effectiveness. H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing Algorithms, Experimentation, Verification.
 image annotation, TagSearcher, random search
With the prevalence of social network and digital pho-tography, the number of web images has exploded in recent years, which necessitates effective techniques to manage and retrieve such a large-scale image database. As revealed by recent studies, automatic semantic annotation for unlabelled images can be a potential approach to tackling this problem, which aims to objectively and effectively assign images with tags that can well describe corresponding semantic content.
In this paper, we propose a novel auto-annotation model termed as TagSearcher, which adopts tag-related random search over range-constrained visual neighbors of the to-be-annotated image for predicting tag scores. Experiments con-ducted on benchmark dataset well demonstrate that TagSear-cher is rational and effective, and it presents a promising way to reduce the performance sensitivity. The main motivations of our work are based on the following observations:
The annotating performances of many previously proposed models, which utilize an identical and fixed number of vi-sual neighbors for label propagation, are generally sensitive to the quantity setting of neighbors, since insufficient neigh-bors cannot provide enough information for mining while redundant neighbors probably introduce much noise, and each image may even has its own optimal quantity setting. To tackle the problem, we propose to utilize both strongly-related and weakly-related ranges of visual neighbors for all to-be-annotated images, i.e. a strong upper bound and a weak upper bound for constraining the number of visual neighbors, as illustrated in Fig. 1. Images in the stronly-related range are supposed to be more reliable for knowledge mining, and those out of the weakly-related range are as-sumed to be unrelated. Moreover, the proposed tag-related random search over range-constrained visual neighbors can find out the trustworthy part for each candidate tag, and then enhance the robustness of annotating performances.
When predicting tag scores for a to-be-annotated image, previous auto-annotation models usually assume that the probabilities for visual neighbors to be selected remain con-stant for all candidate tags. In this paper, however, we pro-pose that they are better to be tag-dependent, and denote them as the  X  X rust degrees X  of neighbors w.r.t the candidate tag. It is because that in the process of predicting score for a specific candidate tag, visual neighbors associated with it are supposed to be more likely to be selected, which is analo-g ous to using both the target image and the candidate tag to re-select the tag-specific trustworthy visual neighbors. Note that in the same case, the assumption of invariant probabil-ities for visual neighbors to be selected in previous models will indirectly weakens the positive contributions of trust-worthy neighbors and strengthens the negative effects of less trustworthy ones, resulting in  X  X ag-specific X  noise.
The contributions of our work can be summarized as fol-lows: (1) For reducing performance sensitivity, we propose to utilize a constrained range rather than an identical and fixed number of visual neighbors. (2) An effective and robust image auto-annotation model is proposed, which considers weights of visual neighbors, votes for candidate tags and tag-specific trust degrees of visual neighbors for predicting tag scores. (3) To estimate the trust degrees of visual neighbors w.r.t a candidate tag, we propose a novel optimization algo-rithm for graphical model named tag-related random search.
The remainder of this paper is organized as follows: Sec-tion 2 gives an overview of related work. Section 3 elabo-rates on the proposed model. Section 4 presents the details of our experiments, including experimental settings, results and analyses. Finally we conclude the paper in Section 5.
Automatic image annotation has been studied for years, resulting in various models. Among them, a large part adopts the strategy of propagating tags from nearest visual neighbors [1, 2, 3, 4, 5, 6], categorized as visual-neighbor-based (VNB) models. Such models generally assume that visually similar images probably share common tags. In re-cent years, due to the rapidly increasing image data, VNB models tend to be more preferable. In [1, 3, 4, 5], re-searchers utilized the visual neighbors of target images to build up real-time annotating frameworks and exploit help-ful knowledge from large-scale web images for performance improvements. In general, VNB models will derive weight distribution among visual neighbors and then utilize them for predicting tag scores. F. Wang et al. [2] proposed a graph-based semi-supervised approach for label propagation that estimates weights of neighbors through linear recon-struction. M. Guillaumin et al. [6] adopted metric learning methods for better weight distribution and proposed the so-phisticated TagProp, which maintains the state-of-the-art annotating performance.
 By surveying previous researches, we conclude that most VNB models utilize an identical and fixed number of visual neighbors for all to-be-annotated images, and the quantity setting of visual neighbors is vital to the annotating perfor-mance, thus keeping high the performance sensitivity, espe-cially for models relying heavily on the whole range of visual neighbors.
Following previous VNB models, we target at estimating the conditional probability of a candidate tag t i given the to-be-annotated image I , i.e. P ( t i | I ). With the conditional independence assumption between t i and I , we can derive where VN ( I ) is the set of visual neighbors and P ( I j sents the probability for I j to be selected. The conditional probability P ( I | I j ) and P ( t i | I j ) are also respectively de-noted as the weight of visual neighbor I j and the vote for candidate tag t i . In previous VNB models, P ( I j ) is gener-ally assumed to be a uniform prior probability and assigned with a constant value. Then formula (1) is further simplified as P ( t i | I )  X  ing to the observations described formerly, it can be more appropriate to make P ( I j ) non-constant and tag-dependent, which in this paper is denoted as the trust degree of I j t . Therefore, the proposed TagSearcher predicts tag scores (i.e. estimated P ( t i | I )) by considering and estimating three factors: weights of visual neighbors, votes for candidate tags and tag-specific trust degrees of visual neighbors. Specifi-cally, we rewrite formula (1) as: where s ( I; t i ) is the predicted score, U ( I ) is the weakly-related range of visual neighbors, w ( I; I j ) represents the es-timated weight of I j (i.e. estimated P ( I | I j )), v ( I estimated vote for t i from I j (i.e. estimated P ( t i | P ( I j )). Since the former two factors have been extensively studied in various heuristic ways, we will focus much on the last one in this paper.
Assuming that images ranked after the weak upper bound are unrelated, when estimating weights of visual neighbors, only images within the weakly-related range are considered and others will be directly assigned with zero weights. In the proposed model, weights of visual neighbors are estimated via the following formula: where U and j are respectively the weak upper bound and the rank position of the visual neighbor I j , and d ( I; I visual distance between I j and I . Apparently the formula is both distance-based and rank-based, which assigns larger weights to neighbors ranked in the front.
In TagSearcher, when estimating vote for a candidate tag, visual neighbors containing the tag are to return 1, and oth-ers will take tag correlations into consideration and give a soft vote. Here we adopt a conditional probability model as following to estimate the soft vote. v ( I j ; t i )  X  P ( t i |{ t j 1 ;t j 2 ;:::;t jn } ) s:t: t where { t j 1 ; t j 2 ; : : : ; t jn } is the associated tag set of I the assumption of tag correlations, and the observation that resort to an approximation scheme as following: where n is the number of tags associated with I j , and P ( t is the conditional probability between tags, which is approx-imated with tag frequencies (i.e. f ( { t i ; t jk } ) and f ( t Fi gure 1: Illustration of the proposed tag-related random search process.
As illustrated in Fig. 1, in TagSearcher, for each candi-date tag a random search process starting from the to-be-annotated image is performed over its weakly-related range of visual neighbors (i.e. the dashed box) for finding the trustworthy part, and estimating corresponding trust de-grees w.r.t the tag. Note that after the first step, the to-be-annotated image is left out in following random search steps. At each step of subsequent random search, the process will determine the probability for each vertex of the graphical model to move forward, which is tag-dependent and varies with the depth of search step. For moving forward, each vertex will choose one of its strongly-related neighbors as a successive vertex and continue. It should be noticed that each visual neighbor also has its own strongly-related neigh-bors that is constrained within the weakly-related range of the to-be-annotated image. As the random search process moves on, it will finally converge and the probability for the process to stay at each vertex can be utilized to estimate the corresponding trust degree.
In TagSearcher, the constructed graphical model is di-rected, where each vertex V a represents a visual neighbor I , and the weight on a directed edge means the probabil-ity for one to choose the other as a successive vertex for a further step. Note that in the graphical model, all vertices except for the to-be-annotated image can be the successive vertices of others. Hence the to-be-annotated image can be denoted as the source of the graphical model. Regarding the weight on a directed edge, we estimate it as following: where s a;b is the weight on the directed edge from vertex V a to V b , U is the weak upper bound for visual neighbors, V ( I a ) represents the strongly-related visual neighbor set of I sition of I b among the neighbors of I a , and is normalizing factor to ensure all the weights on edges from a vertex to sum up to 1. Here we conservatively utilize the more re-liable neighbor range for avoiding too much noise. Then a successive matrix S U  X  U representing the successive relation-ships between visual neighbors can be derived, of which the element S ij is the weight on the directed edge from V i to V . Assuming at first only the source of the graphical model is assigned with 1 while others with zero, we can then get the expectation values of other vertices after the first step, denoted as the initial value vector p . Apparently, p i equals the weight on the edge from the source to V i , and p sums up to 1. Then tag-related random search can be further performed for achieving trust degrees of visual neighbors.
At a specific step of the proposed tag-related random search process, the expectation value of a vertex at the k th step is calculated as following: where p i is the initial value of the vertex V i , p ( k  X  pectation value of V j at the ( k  X  1)th step, f ( k  X  1) are respectively the probability of moving forward on V j the probability for V i to be chosen as a successive vertex of V j at the ( k  X  1)th step, and is a weighting parameter be-tween 0 and 1. The successive matrix S ( k  X  1) ji is constructed in nearly the same way as formula (6), while the number of candidate successive vertices decreases by a ratio and re-mains no less than 1 as the step increases in order to avoid reaching too many less related neighbors. In formula (7), the forward probability of each vertex, i.e. f ( k  X  1) j to finding trustworthy visual neighbors for a candidate tag, which is estimated as following: where j is the conditional probability of the candidate tag b t given image I j . For simplicity, here we use the vote for from I j , i.e. v ( I j ; b t ), as an approximation. j is the expec-tation value of the conditional probability at a further step, which can be estimated as j = the law of total probability. As implied by formulas above, it is more likely for tag-related random search to stay at a vertex which contains the specific tag or strongly-related tags, and the staying probability increases with the depth of search step. By constructing a diagonal matrix F ( k ) forward probabilities of all vertices, formula (7) can be fur-ther rewritten with matrix notations as following:
According to formula (9), we can derive that where p is the final value vector as the step of tag-related random search tends to positive infinity (i.e. p = lim n  X  X  X  It can be demonstrated that the proposed tag-related ran-dom search process will finally converge, and the second part of formula (10) tends to zero. Then formula (10) can be fur-ther simplified as: By normalizing p to make it sum up to 1, the trust degree of visual neighbor I j w.r.t the candidate tag t i , i.e. c ( I in formula (2), can be estimated as the j th element of p .
In the basic TagSearcher presented above, there exist some drawbacks w.r.t non-frequent tags, and thus we further pro-pose refining approaches. Firstly, since non-frequent tags rarely co-occur with other tags in sufficient images, the re-turned votes for them tend to be smaller, resulting in their lower predicted tag scores. Hence, we resort to the widely-used WordNet for completing the tagging matrix. Specifi-cally, for each non-frequent tag, all the tagging vectors of other tags will be summed up with their corresponding se-mantic similarities to the specific tag as weights. Thus zero positions in the tagging vector of the non-frequent tag will be changed to 1 if they are above some predefined threshold in the summed vector, and then tag correlations and votes for tags are re-estimated. Secondly, for a to-be-annotated image the weight distribution among visual neighbors in TagSearcher is better to be different between frequent and non-frequent candidate tags, since in most cases a non-frequent tag just appears as an unrelated bundled attachment in vi-sual neighbors. Hence we propose that the weight distribu-tion for non-frequent tags should be more insensitive to the rank positions of neighbors. Specifically, for non-frequent tags, we adjust formula (3) into w ( I; I j ) = 1 wh ere is a smoothing factor between 0 and 1, and  X  X  X  X  is a ceiling function. Namely, we utilize an echelon decline curve to estimate the weight distribution for any non-frequent tag.
Our experiments were conducted on the widely-used well-known benchmark dataset Corel5k for making fair compar-isons. Corel5k is one of the most important evaluation benchmarks in the community of image auto-annotation, containing around 4,999 images that are manually annotated with 1 to 5 tags. A fixed set of 499 images is split out for test, and the remaining works as the training set (i.e. the labelled database). There are totally 260 tags existing in both training and test sets. With accurate manual annota-tions, the dataset contains little noise and acts as an ideal evaluation benchmark.

To retrieve visual neighbors of to-be-annotated images, we use the open-source Lire [7] project for feature extrac-tion and visual similarity measurement. In our experiments, eleven kinds of features 1 are extracted for each image, and Th e features include: Color Correlogram, Color Layout, CEDD, Edge Histogram, FCTH, HSV Color Histogram, JCD, Jpeg Coefficient Histogram, RGB Color Histogram, Scalable Color, SURF with bag-of-words model. distances between feature vectors are calculated with Lire, then normalized and merged with equal weights for measur-ing visual distance. Following previous researches, we anno-tate each test image with the top 5 tags, and calculate the average precision p and recall r for all tags to measure the annotating performances. Additionally, the number of tags with non-zero recall, denoted as N +, is also a widely-used important measurement.
Firstly we investigate whether the range constraint for vi-sual neighbors and the proposed tag-related random search can reduce the performance sensitivity to the quantity set-ting of visual neighbors. We respectively keep either the strong upper bound (i.e. 10) or weak upper bound (i.e. 60) invariant, and investigate the performance variations as the other bound changes. Here we introduce a simple but typical baseline denoted as VNvote, which predicts tag scores with formula (2) except for c ( I j ; t i ) (i.e. trust degrees of visual neighbors w.r.t the candidate tag), similar to many previ-ous VNB models. Hence the comparisons between both can well reflect the effects of the range constraint of visual neigh-bors and the tag-related random search. Fig. 2 illustrates the performance variations of TagSearcher with the strong or weak upper bound varying, in terms of average precision, recall and N +. From Fig. 2, we can draw the following con-clusions. (1) The annotating performances of TagSearcher is much less sensitive to the bound settings of visual neigh-bors, which is attributed to the range constraint for visual neighbors and the proposed tag-related random search. (2) The annotating performances of TagSearcher remain com-parable to or even better than the best performance of the baseline, though the settings of the strong and weak up-per bounds vary in quite a large range, which well demon-strates the rationality of introducing trust degrees of visual neighbors for auto-annotation and the effectiveness of the proposed tag-related random search. (3) The strong upper bound for visual neighbors has a relatively more significant effect on annotating performance than the weak one. It is because that in the proposed model we rely much more on the strongly-related range of visual neighbors. (4) Though the performance insensitivity is significantly improved, both strong and weak upper bounds can still slightly affect the annotating performance as a convex curve.
Table 1 gives an overview of the annotating performances in terms of average precision, recall and N + of the pro-posed model and those reported in other remarkable earlier En -CRF[15] researches on the benchmark Corel5k. JEC* is our imple-mentation of JEC using our eleven kinds of features, and TagProp* refers to the corresponding model with published implementation by M. Guillaumin [6] and our features. Re-garding TagSearcher, we empirically set the strong and weak upper bound as 10 and 60 respectively, and decreasing ra-tio as 2, weighting factor as 0 for reducing compu-tation complexity. Here we denote the refined variant of TagSearcher with tagging matrix completion using WordNet as TS+WN, and that with weight distribution adjustment for non-frequent tags as TS+WDA ( = 0 : 5). Furthermore, we merge both refining strategies and denote it as TS+Both.
From table 1 we can make several observations. (1) The annotating performance of JEC* on Corel5k is similar to those reported in previous researches, making it relatively fair to make comparisons with their reported results. It can be seen that the proposed TagSearcher and its refined variants outperform most previous remarkable models, and achieve comparable annotating performances to the state-of-the-art TagProp [6]. (2) When comparisons are strictly made with the same kinds of features, TagSearcher and its refined variants outperform the state-of-the-art TagProp. Both observations above well demonstrate the effectiveness of TagSearcher for image auto-annotation. (3) The refined variants of TagSearcher achieve more satisfactory annotating performances, especially TS+Both, which demonstrates the rationality and effectiveness of our proposed refining strate-gies for non-frequent tags.
In this paper, we propose a novel model named TagSearcher for image auto-annotation, using tag-related random search over range-constrained visual neighbors. TagSearcher pro-poses to use a constrained range of visual neighbors for la-bel propagation, and utilizes tag-related random search pro-cesses to find out the trustworthy part for each candidate tag. By merging weights of visual neighbors, votes for can-didate tags and tag-specific trust degrees of visual neighbors in score predictions for candidate tags, TagSearcher can not only achieve satisfactory annotating performances but also effectively reduce the performance sensitivity.
This research was supported by the National Basic Re-search Project of China (Grant No. 2011CB707000), the National Natural Science Foundation of China(Grant No. 60972096, 61005045). [1] X. Li, L. Chen, L. Zhang, F. Lin, and W. Ma. Image [2] F. Wang and C. Zhang. Label propagation through [3] J. Li and J.Z. Wang. Real-time computerized [4] X. Wang, L. Zhang, X. Li, and W. Ma. Annotating [5] X. Li, C.G.M. Snoek, and M. Worring. Annotating [6] M. Guillaumin, T. Mensink, J. Verbeek, and [7] M. Lux and S.A. Chatzichristofis. Lire: lucene image [8] G. Carneiro, A.B. Chan, P.J. Moreno, and [9] S.L. Feng, R. Manmatha, and V. Lavrenko. Multiple [10] J. Liu, M. Li, Q. Liu, H. Lu, and S. Ma. Image [11] C. Wang, S. Yan, L. Zhang, and H. Zhang. Multi-label [12] A. Makadia, V. Pavlovic, and S. Kumar. A new [13] Xiao Ke, Shaozi Li, and Donglin Cao. A two-level [14] S. Zhang, J. Huang, Y. Huang, Y. Yu, H. Li, and D.N. [15] X. Xu, Y. Jiang, L. Peng, X. Xue, and Z. Zhou.
