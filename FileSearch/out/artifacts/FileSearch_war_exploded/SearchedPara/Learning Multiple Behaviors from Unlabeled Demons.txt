 I3A, Universidad de Zaragoza, Spain Manuel Lopes manuel.lopes@inria.fr Inria Bordeaux Sud-Ouest, France Learning behaviors from multiple unlabeled data has applications in surveillance, monitoring systems, robotics and virtual agents, among others. In surveil-lance the number of behaviors is unknown and the learned model has to predict and interpret new activ-ities and detect anomalous behaviors. In robotics and virtual agents it is common to use large time-series of human behavior to program robots or virtual charac-ters. By simultaneously inferring the number of be-haviors and how the motion primitives generate them, we can bootstrap a set of controllers for many activi-ties.
 There are several difficulties to learn multiple behav-iors from unlabeled time-series. First, the number of underlying behaviors is usually unknown. Second, it may be costly and difficult to label the data by hand based solely on observations due to perceptual aliasing, different reference frames or without knowing the rel-evant features of the problem. In this paper we argue that, given an unlabeled dataset of possibly multiple behaviors, there exist a latent representation of the motion primitives that allows to generate the differ-ent behaviors (see (Taylor et al., 2007) for an example of learned latent representations of dynamical behav-ior). A natural way to group these behaviors is to search common motion primitives that generate them accurately. Therefore, our objective is to cluster a col-lection of time-series and, at the same time, learn the latent control structure that generates them.
 The contribution of the paper is a hierarchical model that combines a DP mixture of linear models with a Laplacian distribution as base measure to simultane-ously cluster trajectories into behaviors and perform feature selection in a latent representation of the mo-tion primitives. The motion primitives are represented as potential functions generated by linear combina-tions of features. We show that it is possible to derive an efficient sampler, despite non-conjugacy, by com-puting in closed form the marginal of each observation under the base measure of the DP. We also present an experimental validation of the proposed method in two applications: clustering robot behaviors and human trajectories. The results show that the approach ro-bustly recovers meaningful behaviors by grouping com-mon sparse latent representations and performs better than other methods such as a sparse EM algorithm or learning and clustering the motion primitives indepen-dently. Few works have considered the problem of learning multiple behaviors from unlabeled time-series. (Babes et al., 2011) introduced an EM algorithm to cluster time-series in the space of rewards functions using Bayesian inverse reinforcement learning (Ramachan-dran &amp; Amir, 2007). In (Dimitrakakis &amp; Rothkopf, 2011) multi-task priors are defined on the policy and the rewards of finite MDPs to learn a different reward function per trajectory without clustering them. A similar approach considers a prior directly on the pol-icy (Doshi-Velez et al., 2010). (Choi &amp; Kim, 2012) simultaneously perform clustering and task learning using a Bayesian formulation of IRL and a DP prior on rewards. As in our case, trajectories can be very mixed in the observation space, but have a compact represen-tation in terms of rewards. Our approach differs from these works in two important aspects. First, they all relied on finite state-action representations while we consider continuous spaces for the actions and the la-tent space. Second, we incorporate sparsity into the parameter space allowing for more freedom when de-signing the features, for instance, when the relevant features of each controller are unknown.
 Another different but complementary perspective models each time-series using a set of simpler local models. Several authors approached this problem as a pure regression problem adding local models as needed (Schaal &amp; Atkeson, 1998) or relying on non-parametric methods (Hannah et al., 2011; Rasmussen &amp; Ghahra-mani, 2002; Shahbaba &amp; Neal, 2009; Wood et al., 2008). Recently, more complex models have been con-sidered such as switching systems (Fox et al., 2008; Chiappa &amp; Peters, 2010). In contrast with the first type of methods, the difficulty here is the segmenta-tion of the time series into an unknown number of local models rather than finding an unknown number of global ones. In this case, sparsity is usually im-posed on the number of local models and not on their features.
 The closest work is (Fox et al., 2008) which proposed a model to extract the local behaviors from multiple unlabeled sequences. Sparsity is enforced using a Beta Process that allows to share local behaviors between the different time series. By using more data it is pos-sible to better estimate these local behaviors and the possible transitions among them within each time se-ries. Instead of sharing local behaviors, our aim is to cluster global behaviors using a DP prior. Another difference is that we enforce sparsity on the number of features defining each global behavior, instead of enforcing a sparse number of local behaviors. Nev-ertheless, this body of work is complementary to the one proposed here. An interesting avenue for future work would be to cluster global behaviors while mod-eling each behavior as a combination of local models instead of the global one used in this paper.
 Finally, it is worth to discuss few works that have in-cluded sparsity at the feature level within a DP based clustering algorithm. Gaussian DPMM with variable relevance determination has been recently addressed in (Yau &amp; Holmes, 2011). This was achieved by im-posing a hierarchical prior that shrinks the mean of each cluster to a common value. (Qi et al., 2008) pro-poses a sparse model for Multi-Task compressive sens-ing using linear models based on a DP with a Gamma distribution as prior over each parameter. Due to non conjugacy, the authors used variational inference to approximate the posterior with a truncated number of possible clusters. This section describes the latent controller space parametrization at the core of the clustering algo-rithm. We are interested in representations of mo-tions with the following properties: i) can be learned efficiently from data; ii) can be used not just for dis-criminating motions but also to generate motions; and iii) can represent a large variety of motions. We will represent motion as the movement generated by a particle moving along a potential field V ( X ) (Brillinger, 2007). In other words, the velocity of the particle is given by the gradient of the potential field: where x  X  R d and V is the potential function gen-erating the motion. If V is a smooth function then the observations can be approximated using the model x ( t +  X  t )  X  x ( t ) =  X  X  X  V ( x ( t ))  X  t + an additive noise term,  X   X  N (0 , X  2 I d ), and  X  t is the sampling interval. In order to learn this function, we will consider a linear parametric model for V of the form V ( x ) =  X  ( x ) T  X  with  X  ( x )  X  R p a vector of a priori given features and  X   X  R p a vector of param-eters to be estimated. Rearranging the terms of the previous equation one obtains: The advantage of this representation over other com-mon approaches, such as (Ijspeert et al., 2003; Jetchev &amp; Toussaint, 2011; Khansari-Zadeh &amp; Billard, 2010), is that the global solution can be found efficiently even when including sparsity constraints and this will be exploited in next section to derive an efficient sampler for the DPMM. Also, it is a simple but general rep-resentation equivalent to representations widely used in robotics (Howard et al., 2009; Jetchev &amp; Toussaint, 2011; Chaumette, 2004) and similar to a Lagrangian from physics. Note that this representation allows to encode any type of information within the features, for instance, it is possible to create large sets of non-linear features (e.g. combinations of attractors, polynomials or basis functions) to generate complex motions. We can now construct a linear system for the whole trajectory x 1: T of the form Y = X X  , where matrices Y and X are obtained simply by stacking for all the points of a trajectory the corresponding terms y t = In this section, we introduce our algorithm for cluster-ing trajectories through the use of a linear representa-tion of each behavior. Given a set of m observed tra-jectories, we can compute the corresponding input and feature gradients { Y i ,X i } i =1 ..m . We assume that they are generated from a mixture model of K ( K &lt; m ) linear dynamical systems (namely a potential function V (  X  ) with parameters  X  k as described in Eq.2). The mixture model generating the trajectories is given by: where K is the number of components of the mixture,  X  k are the mixing weights with P k  X  k = 1, p ( Y i | X , X  k ) is the distribution generating the observations for mixture component k with parameters  X  k .
 To estimate this mixture we use a Dirichlet Process (DP) (Ferguson, 1973) over the  X  parameters of each controller. A DP is a probability measure on the space of probability measures parameterized by the scale fac-tor  X  0 and the base distribution G 0 , The base distribution is a continuous density that de-fines the prior over the distributions of the parameters. The DP induces a discrete probability over the samples drawn from the base measure and provides a clustering effect. The scale factor  X  0 , on the other hand, controls the effective number of classes.
 Each cluster corresponds to a behavior with its own latent representation. In order to automatically select relevant features and recover compact representations, we use a sparse prior for the parameters  X  k of the linear model. For a single behavior, using a Laplacian distribution as prior for a given  X  k = (  X  1 ,k  X  X  X   X  p,k results in the Bayesian Lasso (Park &amp; Casella, 2008). Since each behavior may have its own relevant fea-tures, the sparse prior is directly incorporated as the base distribution G 0 of the DP. The proposed hierar-chical model, shown in Fig. 1, uses the  X  X tick break-ing X  construction representation of a DP to extend the Bayesian Lasso to an unknown number of models: where the indicator variables c i  X  X  1 ..K } assign each trajectory to one of the K components of the mix-ture. The model uses the representation of a Laplacian as a scale mixture of normals through the variables  X  dependently each mixture component and, therefore, features selected in one component do not affect fea-tures selected in another component.
 Under this model, the conditional distributions can be computed due to the conjugacy properties exploited in the Bayesian Lasso. In particular, the full conditionals of  X  k ,  X   X  2 j,k ,  X  2 and  X  2 are respectively where X | k and Y | k denote the stacked matrices of all trajectories assigned to cluster k and IG denotes an Inverse-Gamma (see (Park &amp; Casella, 2008) for a com-plete description of the parameterizations of the Inv-Gaussian, Inv-Gamma and Gamma distributions).
 In order to implement the Gibbs sampler for the DPMM we also need to compute the marginal of each trajectory under the base measure G 0 . In our case, this measure is the prior distribution defined on the space of the  X  and  X  parameters as defined in Eqs.5 and 7. The corresponding likelihood function is de-fined in Eq.4. To compute this marginal we note that since integrating out the  X  2 = (  X  2 1  X  X  X   X  2 p ) T the conditional Laplace prior defined in Eq. 3. Due to the use of a Laplacian prior, the model does not preserve conjugacy. However, it is possible to an-alytically compute q 0 (see Appendix A). The deriva-tion requires to decompose the covariance matrix of the predictors to decorrelate the features and, due to the Laplacian prior, to consider the integral on the positive and negative values of each  X  j independently. We provide the full derivation in the supplementary materials. The proposed method is summarized in Al-gorithm 1.
 Algorithm 1 Sparse Dirichlet Process Mixture Model (DPMM) Given a set of trayectories { Y i ,X i } , i = 1  X  X  X  m 5.1. Robot Behaviors We consider a task where a mobile robot has to col-lect different types of objects from the environment and take them to a base. The behavior of the robot is defined by the type of objects it has to collect and the ones it has to avoid. For two classes of objects, we define five different controllers depending on which objects to pick (one type of object, the other type or both) and on the need for collision avoidance to get away of objects while fetching and carrying others (see Figure 2(a)). Note that, depending on the location of the objects, trajectories of two behaviors may be iden-tical. For instance, consider two behaviors that differ in their obstacle avoidance policy. If they do not find an obstacle, they will generate identical trajectories. The trajectories are difficult to visualize because they are in a 6D space (the 2D locations p = ( p u ,p v the robot and the objects). It is clear that any clus-tering made on the observed time-series can not work because the trajectories are context dependent making any comparison among observed trajectories meaning-less. The controller of each behavior is implemented using servoing (Chaumette, 2004) to map each point of the 6D input space into the speed in each axis of the plane ( v u ,v v ).
 At a given time step we define the following features for the relative location of the robot p w.r.t. another feature h indicates whether the robot is holding an object or not. The full feature vector is then: where o t represents the target location and o 1 and o 2 are the locations of the objects of each class. For two classes and one object per class, this amounts to a total of 12 features. We note that  X  o corresponds to simple, generic, features not including any special domain knowledge. On the other hand, the feature h does include some domain knowledge, namely the fact that the robot controller has two modes of oper-ation depending on whether the robot already picked up the object or not. However, we believe this is a very high level information that can be easily encoded in the algorithm. Furthermore, when comparing the algorithm to other approaches, all the methods ben-efit from this domain knowledge. For each behavior we simulated 200 time steps trajectories from 12 dif-ferent initial configurations. Every time an object was taken to the base it reappeared in a random location. The measurements were corrupted with noise sampled from N (0 , 0 . 05 I 6 ). Figure 2(b) shows the trajectories of all five behaviors.
 5.1.1. Clustering behaviors We first analyze the clustering obtained by the DPMM algorithm for different number of classes. We show our results using a correspondence matrix where each component i,j of the matrix represents the frequency of assignment of trajectories i and j to the same clus-ter. The results shown are averaged over 500 samples after a burning period of 500. Figure 3 shows that the DP usually finds the correct number of clusters and as-signs the trajectories consistently to the same group. As mentioned before, there exist some behaviors which may generate indistinguishable trajectories and this is observed in the off diagonal blocks, specially for classes 2 and 3 and classes 4 and 5. Figure 4 shows the poste-rior distribution over the number of clusters provided by the DPMM algorithm. This figure shows that most of the probability is assigned to the right number of classes, although some behaviors were split into more than one cluster.
 We will now compare our DPMM method with two baseline algorithms, both assuming that the number of clusters is known. The first one is a sparse EM algorithm that uses the L1-norm in the minimization step to compute the  X  parameters for each cluster and allows us to illustrate the benefits of using the DP prior. The second method corresponds to a naive so-lution to our problem, denoted Trajectory Reconstruc-tion and Clustering (TRC). TRC estimates first a con-troller (i.e. the  X  parameters) for each trajectory and then clusters the resulting controllers using Kmeans (Macqueen, 1967). In this case, we analyze the ben-efits of simultaneously clustering and discovering the sparse latent space of the controller. We do not report results of a direct clustering on the observation and feature spaces since they results were systematically much worse.
 To analyze the properties of the proposed method, we consider three cases using 50, 100 and 200 time steps, respectively. We perform this comparison be-cause longer trajectories are more informative about the underlying behavior. The TRC should be able to estimate more accurately the parameters of each single trajectory making easier the clustering of the estimated controllers. Figure 5 shows the correspon-dence matrices obtained by the DPMM, EM and TRC methods for five different behaviors with trajectories of different durations. For the DPMM, the results do not vary significantly. It is always able to recover the right number of clusters. The only variation is that the ambiguities between classes diminish for longer trajec-tories because the probability of having multiple indis-tinguishable behaviors decrease. Despite the number of clusters is known, the EM algorithm is not able to discriminate properly between similar behaviors 1 . It behaves better with longer trajectories (assignments concentrate in three main behaviors), but still fails to distinguish classes 2,3 and 4,5 . Although we used multiple initializations, the EM seems to get stuck in a local minima with three dominating clusters instead of five independently of the trajectory length. The results for the TCR algorithm show that shorter trajectories make the estimated  X  less stable and more difficult to cluster. If trajectories are long enough, the TCR bet-ter estimates the controller parameters for each single trajectory and the confusion matrix is closer, but still not as good, as the EM one (and much worse than the DPMM).
 The assignment matrix shows if trajectories belonging to the same class were grouped together. This is useful in applications where the behaviors need to be iden-tified unambiguously, e.g. surveillance or robot con-trol. Other applications will be more interested in the quality of the reconstructed trajectories rather than assignments, e.g. computer graphics. Thus, we also compare the prediction error on a test set using the behaviors learned for the different clusters.
 We created 12 test trajectories for each behavior start-ing from random points. We computed the root mean square error (RMSE) of the test trajectories on each of the estimated controllers using Eq. 2. The RMSE measure how well the potential created by the esti-mated parameters  X  k matches the gradients from the test trajectories. To solve the correspondence prob-lem, we selected the minimum RMSE. Figure 6 shows the RMSE obtained by the different algorithms. The DPMM behaves better than the other algorithms due to the better clustering properties. For 200 time step trajectories, the EM algorithm is close to the DPMM, but it degrades its performance with less informative trajectories (50 or 100 time steps). Note that the changes in RMSE per trajectory will not be very large even if the behavior is not correct since obstacle avoid-ance is a short and local maneuver that does not accu-mulate over time. However, not avoiding an obstacle may be a fatal failure for a robot controller. We con-clude that DPMM provides a more accurate behavior assignment, is able to find the correct number of clus-ters, has better reconstruction properties and is more robust when few data is available. 5.1.2. Feature selection We next analyze the feature selection properties of the DPMM. Table 1 shows that the parameters found by the DPMM algorithm were quite similar to the sparse original ones. Furthermore, the correlation between both parameters was always over .98 since the DPMM recovered the right coefficients. However, the Lapla-cian prior did not shrink to zero all parameters not needed by the controller and some of them kept small values (under 0 . 05). Removing the Laplacian prior (  X  = 0) resulted in much larger L1-norms and the recovered parameters did not match the true parame-ters. However, it did correctly group the trajectories and the reconstruction error increased only slightly. EM also found sparse parameters but with a worse reconstruction and assignment matrix.
 We further tested the ability of the DPMM algorithm by expanding the feature vector with 78 correlated fea-tures resulting of cross products of the original ones. Table 2 shows the L1-norm of the estimated param-eters and the correlation with the original ones. The first row shows the results for the sampled values of  X  , while in the other two  X  was fixed manually to a lower value. The Laplacian prior has a strong effect as the L1-norm increases and the correlation with the original parameters decrease. Also, the reconstruction error is better which indicates that sparsity avoids over-fitting. However, there were no differences in terms of cluster-ing and correspondence matrices. Therefore, for this dataset sparsity helps to recover a compact and inter-pretable controller and to reduce the reconstruction error but does not play a crucial role in grouping tra-jectories together.
 5.2. Edinburgh Informatics Forum Pedestrian We now use the Edinburgh Informatics Forum Pedes-trian Database (Majecka, 2009) to cluster the trajec-tories of people in a open public space (see Fig. 7(a)). The input data is a set of 474 trajectories in 2 D . Each trajectory was generated by sampling 22 points from a six point spline approximation of each tracked per-son in normalized pixel coordinates ( u,v ). Two type of features were computed from the trajectory points using: 1) Gaussian attractors as in Section 5.1 over a 8  X  8 grid on the image plane located at o t ,t = 1 ... 64, and 2) a polynomial of degree two. The dimension of the feature space is therefore 64 + 7. To evaluate the algorithm, we used the hand-made clustering provided in (Majecka, 2009) where trajec-tories were grouped in 51 classes based solely on man-ually selected entry and exit points. The correspon-dence matrix is shown in Fig. 7(b). The reconstruc-tion error (RMSE) obtained with the potential func-tions estimated from the manually labeled trajectories was 1.95. We also ran a KMeans with 51 classes, the number of classes provided by manual labeling, on the trajectories to assess the difficulty of the clustering on the observed trajectories space. The correspondence matrix for KMeans is shown in Fig. 7(c). Although the matrix has some structure similar to the ground truth, the assignments are more randomly distributed and the reconstruction error is higher (3.04). The DPMM algorithm was run with 1000 samples and a burn-in period of 300. Figure 8 shows the posterior distribution for the number of clusters. The average number of clusters is 28 . 7, but there is no clear peak indicating multiple possible groupings. The number of cluster was less than the ground truth partition, due to the grouping of different entry/exit points when their trajectories could be explained by a single potential function. The correspondence matrix is shown in Fig. 7(d). It shows more structure than the KMeans and recovers a more similar pattern to the hand-made clus-tering. The reconstruction error was 1 . 58, lower than the one provided by the ground truth. This improve-ment is due to the fact that the DPMM was able to consider the whole trajectory and fit functions that ap-proximated the global behavior measured in the image, not just entry and exit points. Figure 7(e) shows the reconstructed trajectories from each initial point given the parameters obtained with the most likely particle. We see that they represent very accurately the aver-age behavior per class of which each individual demon-stration was a noisy observation. This shows how the reconstructed trajectories can indeed represent the be-havior of each class and be used to generate new mo-tions and potentially be used for behavioral predic-tion. We did more experiments to test the sensitivity with the choice of features. Training the DPMM only with Gaussian attractors resulted in a slightly higher RMSE of 1.69 and a mean number of clusters of 26. For polynomials, the RMSE decreased to 1 due to a higher number of clusters (mean of 61 . 2) and a cor-respondence matrix with less structure. These results show that the algorithm is able to deal with different feature spaces with good accuracy in terms of RMSE . Depending on the problem domain, different choices of features will lead to different interpretations of the re-sulting behavior models.
 We also evaluated the effect of the sparsity prior on the clustering process by fixing different sparsity levels. Contrary to the robot example, sparsity decreased the total number of clusters. The mean of the estimated  X  was 1 . 573 and the sum of the L1-norm of the estimated parameters of 10 . 41 for the whole feature vector (0.236 and 68.64 with only Gaussian attractors). When  X  was fixed to 10  X  4 , the mean number of clusters was 38, the RMSE was 1 . 37 and the sum of the L1-norm of the estimated parameters was 1 . 22  X  10 5 . This is due to the fact that the number of clusters is not so clearly defined by the data and a sparse  X  regularize trajectories and make their clustering less costly. In this paper we presented a novel algorithm for learn-ing an unknown number of behaviors from a set of unlabeled trajectories which can be highly mixed in the measurement space. It estimates the number of behaviors in a latent controller space and selects the relevant dimensions for each one. By selecting a lin-ear representation, in this paper a potential function generated by a linear combination of features, it is pos-sible to derive a sparse version of the DPMM based on Laplacian priors over the parameters and compute the marginal of the observations in closed form.
 The results show that the latent controller space al-lows to reliably recover groups of behaviors with a sparse representation that can be used to generate new instances of the learned behaviors without losing in terms of reconstruction error. The effect of the spar-sity prior on the clustering depends on the problem at hand and the selected features. Although it may not always change the clustering, the recovered representa-tion is always simple and more interpretable with sim-ilar accuracy. The results also show that the proposed method behaves better than a sparse EM version for mixture models and naive approaches such as cluster-ing directly on the feature space or fitting a controller for each trajectory and then cluster the estimated pa-rameters.
 There are some interesting future directions. First, the method can be applied to any linear representa-tion. The potentials used in this paper do not directly model sequential data and it would be interesting to extend this to models that do so, for instance, linear auto-regressive models. Second, it may be possible to combine this type of clustering of global trajecto-ries with recent work on learning multiple behaviors within a single sequence using multiple sequences as in (Fox et al., 2008) or (Chiappa &amp; Peters, 2010). This work has been supported by Spanish projects DPI2011-2589, and IPT-2012-1309-430000, and the Flowers Team (INRIA/ENSTA-Paristech).
 This appendix provides the expression for q 0 = R f (  X |  X  ) p (  X  |  X  2 ) d X  when the likelihood model is a Multivariate Normal and the prior is a Laplacian dis-tribution. After resolving the integral over  X  we get the following expression
T
T where  X  p is the rank of matrix A = 1  X  2 X T X and d i are the eigenvalues of matrix A . The integral requires to decompose matrix A = SDS  X  1 to compute the vector E where matrix R is defined as the component wise ab-solute value of matrix S .
 Babes, M., Marivate, V., Littman, M., and Subrama-nian, K. Apprenticeship learning about multiple in-tentions. ICML, 2011.
 Brillinger, D.R. Learning a potential function from a trajectory. Signal Processing Letters, IEEE , 14(11): 867 X 870, 2007.
 Chaumette, F. Image moments: a general and useful set of features for visual servoing. Robotics, IEEE Transactions on , 20(4):713 X 723, 2004.
 Chiappa, S. and Peters, J. Movement extraction by detecting dynamics switches and repetitions. Ad-vances in neural information processing systems , 23: 388 X 396, 2010.
 Choi, Jaedeug and Kim, Kee-Eung. Nonparametric bayesian inverse reinforcement learning for multiple reward functions. In Neural Information Processing Systems (NIPS) , 2012.
 Dimitrakakis, C. and Rothkopf, C. Bayesian multi-task inverse reinforcement learning. Arxiv preprint arXiv:1106.3655 , 2011.
 Doshi-Velez, Finale, Wingate, David, Roy, Nicholas, and Tenenbaum, Joshua. Nonparametric Bayesian Policy Priors for Reinforcement Learning. In Neural Information Processing Systems (NIPS) , 2010.
 Ferguson, T. A bayesian analysis of some nonparamet-ric problems. The annals of statistics , 1:209 X 230, 1973.
 Fox, E.B., Sudderth, E.B., Jordan, M.I., and Willsky,
A.S. Nonparametric bayesian learning of switching linear dynamical systems. Advances in Neural In-formation Processing Systems , 21:457 X 464, 2008. Hannah, Lauren A., Blei, David M., and Powell, War-ren B. Dirichlet process mixtures of generalized lin-ear models. J. Mach. Learn. Res. , pp. 1923 X 1953, July 2011.
 Howard, M., Klanke, S., Gienger, M., Goerick, C., and Vijayakumar, S. A novel method for learning policies from variable constraint data. Autonomous Robots , 27:105 X 121, 2009.
 Ijspeert, A. J., Nakanishi, J., and Schaal, S. Learning attractor landscapes for learning motor primitives. In Neural Information Processing Systems (NIPS) , Cambridge, MA, 2003.
 Jetchev, N. and Toussaint, M. Task space retrieval using inverse feedback control. In Intl. Conf. on Machine Learning (ICML) , 2011.
 Khansari-Zadeh, S.M. and Billard, A. Bm: An itera-tive algorithm to learn stable non-linear dynamical systems with gaussian mixture models. In Robotics and Automation, IEEE Inter. Conf. on , 2010.
 Macqueen, J. B. Some methods for classification and analysis of multivariate observations. In Berke-ley Symposium on Math, Statistics, and Probability . University of California Press, 1967.
 Majecka, B. Statistical models of pedestrian behaviour in the forum. Master X  X  thesis, School of Informatics, University of Edinburgh, 2009.
 Neal, R. Markov chain sampling methods for dirichlet process mixture models. Journal of computational and graphical statistics , 9:249 X 265, 2000.
 Park, T. and Casella, G. The Bayesian Lasso. J. Am.
Stat. Assoc. , 103(482):681 X 686, June 2008. ISSN 0162-1459.
 Qi, Y., Liu, D., Dunson, D., and Carin, L. Multi-task compressive sensing with dirichlet process priors. In
International conference on Machine learning , pp. 768 X 775, 2008.
 Ramachandran, Deepak and Amir, Eyal. Bayesian inverse reinforcement learning. In 20th Int. Joint Conf. Artificial Intelligence , India, 2007.
 Rasmussen, C. E. and Ghahramani, Z. Infinite Mix-tures of Gaussian Process Experts. In Advances in Neural Information Processing Systems 14 , 2002. Schaal, S. and Atkeson, C. G. Constructive incre-mental learning from only local information. Neural Comput. , 10(8):2047 X 2084, 1998.
 Shahbaba, Babak and Neal, Radford. Nonlinear mod-els using dirichlet process mixtures. J. Mach. Learn. Res. , pp. 1829 X 1850, August 2009.
 Taylor, G.W., Hinton, G.E., and Roweis, S.T. Mod-eling human motion using binary latent variables.
Advances in neural information processing systems , 19:1345, 2007.
 Wood, F., Grollman, D. H., Heller, K. A., Jenkins,
O. C., and Black, M. Incremental nonparametric bayesian regression, 2008.
 Yau, C. and Holmes, C. Hierarchical bayesian non-parametric mixture models for clustering with vari-able relevance determination. Bayesian Anal , 6:329 X 
