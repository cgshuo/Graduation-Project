 Contextual Advertising is a type of Web advertising, which, given the URL of a Web page, aims to embed into the page (typically via JavaScript) the most relevant textual ads available. For static pages that are displayed repeat-edly, the matching of ads can be based on prior analysis of their entire content; however, ads need to be matched also to new or dynamically created pages that cannot be processed ahead of time. Analyzing the entire body of such pages on-the-fly entails prohibitive communication and latency costs. To solve the three-horned dilemma of either low-relevance or high-latency or high-load, we propose to use text sum-marization techniques paired with external knowledge (ex-ogenous to the page) to craft short page summaries in real time. Empirical evaluation proves that matching ads on the basis of such summaries does not sacrifice relevance, and is competitive with matching based on the entire page content. Specifically, we found that analyzing a carefully selected 5% fraction of the page text sacrifices only 1% X 3% in ad rel-evance. Furthermore, our su mmaries are fully compatible with the standard JavaScript mechanisms used for ad place-ment: they can be produced at ad-display time by simple additions to the usual script, and they only add 500 X 600 bytes to the usual request.
 Categories and Subject Descriptors: H.3.3 [ Informa-tion Storage and Retrieval ]: Information Search and Retrieval X  Query formulation, Selection process ; H.3.4 [ In-formation Storage and Retrieval ]: Systems and Soft-ware X  Performance evaluation (efficiency and effectiveness) General Terms: Algorithms, Experimentation, Measure-ment, Performance Keywords: Text classification, text summarization
The total Internet advertiser spending in 2006 in the US alone is estimated at over 17 billion dollars, with a growth rate of almost 20% year over year. A large part of this market consists of textual ads ,thatis,shorttextmessages usually marked as  X  X ponsored links X  or similar. Today, there are two main types of textual Web advertising: sponsored search , which serves ads in response to search queries, and content match , which places ads on third-party Web pages. In the former case, ads are matched to the (short) query issued by the user, and in the latter case ads are matched to the entire page content. In both cases, it has been shown that the response of the users to the advertising is related to how relevant the ads are to the query or to the page (respectively).

In this paper, we study a very common content match scenario, where Web site owners (called publishers )provide the  X  X eal-estate space X  (i.e., a reserved portion of their page) for placing ads, and the ad server or ad network ,anentirely different commercial entity, returns the ads that are most suitable for the page content. Typically, this is done via JavaScript: the display of the page on a user X  X  screen results in calls being made to the ad server for the supply of suitable textual ads. These calls provide the URL of the page being displayed, and potentially other data.

When a user requests to view a page, the ad selection en-gine has only a couple hundred milliseconds to provide the ads. In most cases this low latency requirement does not al-low for pages to be fetched and analyzed online. Instead, the pages are fetched and analyzed offline, and the results are applied in subsequent ad serving for the same page. This ap-proach works well for static content pages that are displayed repeatedly.

However, a significant amount of the Web is not static: some pages are dynamic by definition, such as personalized pages, and the front pages of news sites, forums, and blogs are constantly changing. Some pages cannot be accessed in advance because they belong to the  X  X nvisible Web, X  that is, they do not exist, except as a result of a user query. Yet other pages are not independently accessible since they require authorizations and/or cookies that are present on the user X  X  computer but not on the ad server X  X  platform. In all of these examples, ads need to be matched to the page while it is being served to the end-user , thus critically limiting the amount of time allotted for its content analysis.
Thus, our challenge is to find relevant ads while main-taining low latency and communication costs. We propose a two-pronged approach to solve it: 1. We employ text summarization techniques to extract 2. In line with our previous work on full pages [5], we
The volume of pages in contextual advertising systems follows the long tail (power law) model, where a relatively small number of pages are seen numerous times and the ma-jority of pages are seen only a few times. In addition to eliminating the need for re-crawls of static pages, our ap-proach also reduces the need for crawling  X  X ail X  pages that are rarely seen by the system. If the page content can be analyzed using a serving-time summary, it might not be nec-essary (nor economically viable) to crawl the page ahead of time. This would limit the crawling only to the pages in the head and the torso of the volume curve, and therefore save additional networking and processing resources both for the ad server and for the publisher.

Previous studies have explored content match based on different ad parts (see Section 5 for a full discussion). While selecting the right ad parts to perform the match is certainly important from the relevance point of view, ads are available beforehand, and so their leisurely analysis has no impact on latency. Here we focus on analyzing the information con-tent of the different page parts, at ad-display time ,when communication and processing time are at a premium.
The main contributions of this paper are threefold. First, we describe a novel method that enables online contextual matching of pages and ads. We create a concise page sum-mary on the fly, and match ads based on this summary rather than the entire page. Empirical evaluation confirms that matching ads based on dynamically created page sum-maries yields ads whose relevance is on par with that of the full page analysis. Second, we analyze the role and the fea-sibility of semantic match of the page and the ads based on text classification of page excerpts and ads. Third, our findings imply that frequent repeated crawling of publisher pages can be avoided by analyzing page summaries just in time for actual page display. Consequently, our method re-duces system load by making it unnecessary to crawl numer-ous  X  X ail pages, X  and allows to serve relevant ads for dynam-ically changing pages.

The rest of the paper is organized as follows. Section 2 provides background on current practices in Web advertis-ing. Section 3 outlines our methodology for robust page analysis. Empirical evaluation of our methodology is pre-sented in Section 4. We survey the related work in Section 5. We discuss our findings and draw conclusions in Section 6.
In this section we give a brief overview of the current prac-tices in Web advertising, which is based on a longer presen-tation in our earlier work [5].

A large part of the Web advertising market consists of tex-tual ads , which are distributed through two main channels: 1. Sponsored Search or Paid Search Advertising ,which 2. Content Match (CM) or Contextual Advertising ,which
Contextual advertising is an interplay of the following four entities:
Given a page, instead of placing generic ads, it is prefer-able to have ads related to the page content in order to provide a better user experience and to increase the proba-bility of clicks. This intuition is supported by the analogy to conventional publishing, where a number of very successful magazines (e.g., Vogue ) have a majority of the pages devoted to topical advertising (fashion in the case of Vogue) .Anum-ber of user studies also confirmed that improved relevance increases the number of ad-clicks [8, 27].

Contextual advertising usually falls into the category of direct marketing (as opposed to brand advertising ), that is, advertising whose aim is a  X  X irect response, X  where the ef-fect of a campaign is measured by the user reaction (e.g., purchase of advertised goods or services). Compared to the traditional media, one of the advantages of online advertis-ing in general and contextual advertising in particular is that it is relatively easy to measure the user response. Usually the desired immediate reaction is for the user to follow the link in the ad and visit the advertiser X  X  Web site.
The prevalent pricing model for textual ads is that the ad-vertisers pay a certain amount for every click on the adver-tisement (pay-per-click or PPC). There are also other mod-els, such as pay-per-impression, where the advertiser pays for the number of exposures of an ad, and pay-per-action, where the advertiser pays only if the ad leads to a sale or similar completed transaction. In this paper we deal with the PPC model.
Non-transactional sites are those that do not sell anything directly.
Content match advertising has grown organically from sponsored search advertising. In most networks, the amount paid by the advertiser for each sponsored search click is de-termined by an auction process. The advertisers place bids on a search phrase, and their position in the tower of ads dis-played on the search results page is determined by their bid. Thus,eachadisannotatedwithoneormore bid phrases . The bid phrase has no direct bearing on the ad placement in content match. However, it is a concise description of tar-get ad audience as determined by the advertiser, and it has been shown to be an important feature for successful CM ad placement [19]. In addition to the bid phrase, an ad is also characterized by a title usually displayed in bold font, and an abstract or creative , which is the few lines of text, usually shorter than 120 characters, displayed on the page. Naturally, each ad contains a URL to the advertised Web page, called landing page .

The ad-network model aligns the interests of the publish-ers, advertisers and the network. In general, clicks bring benefits to the publisher and the ad network by providing revenue, and to the advertiser by bringing traffic to the tar-get web site. The revenue of the network, given a page p , can be estimated as where k is the number of ads displayed on page p and price ( a i ,i ) is the click-price of the given ad a i at position i . The price in this model depends on the set of ads presented on the page. Several models have been proposed to deter-mine the price, most of them based on generalizations and variants of second price auctions. In this paper, we ignore the pricing model for simplicity, and concentrate on finding ads that will maximize the first term of the product, that is, we search for Furthermore, we assume that the probability of a click for a given ad and page is determined by the ad X  X  relevance score with respect to the page, thus ignoring the positional effect of the ad placement on the page. We assume that this is an orthogonal factor to the relevance component and could be easily incorporated in the model.
In this section we first define in more detail the problem of efficiently matching ads to pages, and then develop the proposed solution. The typical content match approach for displaying ads on Web pages is outlined in Figure 1. Upon a request initiated by the user X  X  browser (HTTP get request), the Web server returns the requested page. As the page is being displayed, a JavaScript code embedded into the page (or loaded from a server) sends to the ad server a request for ads that contains the page URL and possibly some additional data.

When the page contents is static (that is, the content as-sociated to the given URL is not generated on-the-fly and changes infrequently), the ad server can invest computation resources in a one-time offline process that involves fetching the entire page and performing deep analysis of the page content to facilitate future ad matches. However, ads need to be matched also to new or dynamically created pages that cannot be processed ahead of time, and analyzing the entire body of such pages at display-time entails prohibitive communication and latency costs.

If the page content cannot be analyzed in advance, we are facing a three-horned dilemma:
Thus, our challenge is to produce highly relevant ads with-out any pre-crawling of Web pages, using only a modest amount of processing and communication resources at ad-display time.
Our solution is to use text summarization techniques pair-ed with external knowledge (exogenous to the page) to craft short page summaries in real-time. The summaries are pro-duced within the standard JavaScript mechanisms used for ad placement and they only add 500 X 600 bytes to the usual request. Thus, our approach balances the two conflicting requirements: analyzing as much page content as possible for better ad match vs. analyzing as little as possible to save transmission and analysis time.

For summaries, we use several techniques [7, 15] to extract short but concise page excerpts that are highly informative of the entire page content.

To supplement the page summary, we also use external knowledge from a variety of sources, namely: 1. URL. We tokenize the page URL into individual words, 2. Referrer URL. We also analyze the referrer URL, 3. Page classification. More importantly, we classify
One often used source of external knowledge about Web pages is anchor text of incoming links [3]. However, we do not use such anchor text in this work since in many cases advertisement pages are dynamic, and therefore have no an-chor text. Furthermore, our just-in-time approach can also be used to put relevant ads on new pages, for which little or no anchor text is available.

In the experiments reported in Section 4, our baseline cor-responds to matching ads by analyzing the full text of the page (including the page and referrer URLs, as well as the classification information). We use a variety of text summa-rization techniques to achieve substantial reduction in pro-cessing time while demonstrating matching relevance that is on par with (or even better than) full page analysis.
We now explain our methodology in more detail.
Text summarization techniques are divided into extrac-tive and non-extractive approaches. The former approach strives to summarize the document by taking carefully se-lected terms and phrases that are already present in the doc-ument. The latter approach analyzes the entire document as a whole and rewrites its content in a more concise way; this option is usually very resource-and computation-intensive, hence we adopt the extractive approach.
 Since our input is an HTML document, we rely on the HTML markup that provides hints to the relative impor-tance of the various page segments. This allows us to avoid time-consuming analysis of the text by taking cues from the document structure. When the u ser X  X  browser displays the Web page, it actually performs HTML parsing prior to ren-dering, hence the JavaScript code embedded into the page has easy access to the DOM 2 representation of the parsed document.

Following prior works [7, 15], we evaluate the role of the following page components in constructing summaries:
In the next section, we evaluate the individual contribu-tion of each of the above-listed page segments as well as their combinations for serving a page proxy for ad matching. To tokenize URLs into words, we used a dynamic programming tool developed in-house, which relied on a language model built from a corpus of several million documents.
Using a summary of the page in place of its entire content can ostensibly eliminate some information. To alleviate pos-sible harmful effect of summarization, we study the effects of using external knowledge by means of classifying page sum-maries with respect to an ela borate taxonomy. Prior stud-ies found that text summarization can actually improve the accuracy of text classification [15, 23]. A recent study also found that features constructed with the aid of a knowledge-based taxonomy are beneficial for text classification [11]. Consequently, we classify both page excerpts and ads with respect to a taxonomy, and use classification-based features to augment the original bag of words in each case.
Our choice of taxonomy was guided by a Web advertis-ing application. Since we want the classes to be useful for matching ads, the taxonomy needs to be elaborate enough to facilitate ample classification specificity. For example, classifying all medical queries into one node will likely result
Document Object Model (DOM) is a standard approach to representing HTML/XML documents [26]. in poor ad matching, as both  X  X ore foot X  and  X  X lu X  queries will end up in the same node. The ads appropriate for these two queries are, however, very different. To avoid such sit-uations, the taxonomy needs to provide sufficient discrimi-nation between common commercial topics. Therefore, we employed a large taxonomy of approximately 6 , 000 nodes, arranged in a hierarchy with median depth 5 and maximum depth 9. Human editors populated the taxonomy with la-beled bid phrases of actual ads (approx. 150 phrases per node), which were used as a training set; a small fraction of queries have been assigned to more than one category. We used the same taxonomy in our earlier work [4], where it is described in more detail.

Few machine learning algorithms can efficiently handle so many different classes and about an order of magnitude more of training examples. Suitable candidates include the near-est neighbor and the Naive Bayes classifier [9], as well as pro-totype formation methods such as Rocchio [21] or centroid-based [12] classifiers.

We used the latter method to implement our text clas-sifier. For each taxonomy node we concatenated all the phrases associated with this node into a single meta-docu-ment. We then computed a centroid for each node by sum-ming up the TFIDF values of individual terms, and normal-izing by the number of phrases in the class: where c j is the centroid for class C j and p iterates over the phrases in a particular class.

The classification is based on the cosine of the angle be-tween the document and the centroid meta-documents: where F is the set of features, and c i and d i represent the weight of the i th feature in the class centroid and the doc-ument, respectively. The scores are normalized by the doc-ument and centroid lengths to make the scores of differ-ent documents comparable. These weights are based on the standard  X  X tc X  TFIDF function [22].

We classified each page summary and each ad with re-spect to the taxonomy, retaining the 5 top-scoring classifica-tions for each text fragment. Following [11], we constructed additional features based on these immediate classifications as well as their ancestors in the taxonomy (the weight of each ancestor feature was decreased with a damping factor of 0.5). Each page and ad were represented as a bag of words (BOW) and an additional vector of classification fea-tures. Finally, the ad retrieval function was formulated as a linear combination of similarity scores based on both BOW and classification features: score ( page, ad )=  X   X  sim BOW ( p, a )+  X   X  sim class ( p, a ) , where sim BOW ( p, a )and sim class ( p, a ) are cosine similarity scores between page p and ad a using BOW and classification features, respectively.
We start with the description of the dataset and the met-rics used, and then proceed to discuss the experimental re-sults. Unless specified otherwise, all the experiments below employ both text summarization and text classification tech-niques; the effect of text classification in isolation is studied in Section 4.7.
To evaluate the effects of text summarization and classi-fication for efficient ad matching, we used two sets of Web pages, which have been randomly selected from a larger set of around 20 million pages with contextual advertising. Ads for each of these pages have been selected from a large pool of about 30 million ads. We preprocessed both pages and ads by removing stopwords and one-character words, followed by stemming. We collected human judgements for over 12,000 individual page-ad pairs, while each pair has been judged by three or more human judges on a 1 to 3 scale: 1. Relevant The ad is semantically directly related to 2. Somewhat relevant The ad is related to the sec-3. Irrelevant The ad is unrelated to the page. For exam-
To obtain a single score for a page-ad pair, we averaged the human judgments. We then used these judgments to evaluate how well our methods distinguish the positive (rel-evant) and the negative (irrelevant) ad assignments for each page. An ad is considered relevant if its score is below some threshold, otherwise it is irrelevant. We experimented with several different thresholds (ranging between 1.7 X 2.4), and found that they did not affect the conclusions. In all the graphs presented below we used the threshold of 2 . 4 (i.e., most of the judges considered the ad somewhat relevant). Based on human judgments, we eliminated pages for which the judged ads were all relevant or all irrelevant (after the thresholding procedure), as they provide little information in judging different algorithmic ad rankings.
 The two sets of pages we used are inherently different. Dataset 1 consists of Web pages that are accessible through a major search engine, and have actually appeared in the first 10 results for some query; consequently, they tend to be of better quality with more textual content. On the other hand, Dataset 2 consists of pages from publishers that are not found in the search engine index, and therefore are gen-erally of lower quality with less text and more images and advertising. Having these two datasets allows us to evaluate our methodology in a more comprehensive way. The statis-tics for the two datasets are given in Table 1. The pages in Dataset 1 have more textual content than in Dataset 2. In addition to the amount of text, visual inspection of the pages indicates that the content on the pages in Dataset 1 is much more consistent around the page topic. Furthermore, pages in Dataset 1 have on average twice as many judgments as in Dataset 2 (28 vs. 11.7). For these reasons, and due to space paucity, we emphasize Dataset 1 in our evaluation.
Dataset 1 consisted of 200 Web pages of various types, ranging from Amazon.com query result pages to medical doc-uments, Wikipedia articles, online tutorials, and so on. Upon eliminating pages for which all judged ads had identical scores (as explained above), we ended up with a set of 105 pa-ges that were used in the experiments. There were 2,680 unique ads and 2,946 page-ad scores (some ads have been scored for more than one page). Inter-judge agreement in scoring was 84%. We classified the pages and ads as ex-plained in Section 3.3.2; the classification precision was 70% for the pages and 86% for the ads.
Dataset 2 is a larger dataset, consisting of 1,756 Web pages, which are also of various types, from online merchant pages to forum pages. After the aforementioned elimination procedure, there remained 827 pages that we used in our experiments. There were 5,065 unique ads and a total of 9,748 judgments.

Table 1 provides average sizes of the individual page frag-ments defined in Section 3.3.1. The rightmost column shows the number of pages in which each fragment was available. Noteworthy are M , H and R , which were not available for all the pages in both datasets (and hence their overall use-fulness should be considered accordingly). The page and referrer URLs ( U and R ) were not available for Dataset 2.
The standard practice of evaluating IR systems is to per-form pooling of judged documents for each query/topic [13]. However, the pooling practice assumes most relevant doc-uments have been judged, and hence considers non-judged documents to be irrelevant. Given the multitude of relevant ads for each page in our case, this solution is inadequate since judged ads constitute only a tiny fraction of all the ads available for retrieval. When each page has numerous relevant ads, it can happen that the top N retrieved ads contain a single judged ad or even none at all. We address this problem in two different ways.

First, Buckley and Voorhees [6] have recently introduced a new evaluation metric, bpref-10 , which allows to over-look non-judged documents and does not require to consider them to be irrelevant (the metric is computed by analyzing the relative rankings of the relevant and irrelevant docu-ments). To the best of our knowledge, our work is the first study in contextual ad matching that makes use of this new metric in evaluating different matching algorithms.
Second, to compute the standard metrics such as preci-sion or mean average precision (MAP), in our evaluation for each page we consider only those ads for which we have judgments. Each summarization method was applied to this set and the ads were ranked by the score. The relative effec-tiveness of the methods was determined by comparing how well they separated the ads with positive judgments from those with negative judgments. We present precision at var-ious levels of recall within this set. As the set of judged ads per page is relatively small, this evaluation reports precision that is somewhat higher than it would be with a larger set of negative ads. However, these numbers still establish the relative performance of the algorithms. In Section 4.8 we revisit this issue in greater detail, and for reference conduct an evaluation where we consider non-judged ads to be irrel-evant. We demonstrate that in both cases, i.e., whether the non-judged ads are ignored or are considered irrelevant, the performance metrics ar e highly correlated.
We now compare the relevance of ad matching when using the entire page vs. the summary of the page.

We examine the performance of different ad matching al-gorithms that use the following parts of the page:
As we can see in Figures 2 and 3, even using the page title alone ( T ) yields matching relevance that is competitive with using all of the page information. The U-R-T-M-H method ( T-M-H for Dataset 2) appears to be the most cost-effective option, as it achieves high relevance scores by analyzing only a few short page excerpts.
Figure 4 shows the contributions of individual page frag-ments, that is, when the page summary is based on each fragment alone. The fragments are ordered from left to right in the decreasing order of their average size (cf. Table 1). Re-call that some fragments (notably M , H and R ) are available only in some of the pages. Consequently, we evaluated the contribution of each fragment first for all the pages, and then only for pages for which it was available (the corresponding Figure 2: The effect of text summarization (Dataset 1) Figure 3: The effect of text summarization (Dataset 2) graphs are labeled  X  X o zeros X  in Figure 4). Predictably, the difference is quite pronounced for H and R , implying that these components should be used whenever they are avail-able in the page. Figure 5 shows the results for Dataset 2.
The performance of summaries based on the anchor text of outgoing links ( A ) might seem surprising. Intuitively, an-chor text characterizes the pages that the current page links to rather than the page itself. However, the anchor text often makes a very good summary of the page itself. For example, a page about high blood pressure might link to pages about heart attacks or medication descriptions that contain relevant information, while pages with lists of items (products, events, etc.) often include links to longer item descriptions. We do not advocate using anchor text in sum-marization as its size is often quite large (cf. Table 1), but we report this finding because it appeared interesting.
Throughout the paper, we report the results for P500 , i.e., the initial prefix of the first 500 bytes of the page text. Figure 6 shows the contribution of prefixes of various length. We show a standard precision-recall graph in Figure 7. Each data point corresponds to the value of precision cal-culated at a certain percentage of recall. We observe that in all the curves the precision declines gracefully across the entire range of recall levels. We also observe that summaries provide a very good approximation of the full page content over the entire recall range.
Figure 6: Prefixes of various length (Dataset 1)
Figure 8 plots the performance of increasingly longer sum-maries, as we progressively incorporate additional page con-stituents. We add fragments in the increasing order of their length (cf. Table 1). We start with the U-R combination, which encompasses external information gathered from the page and referrer URLs, and then add information from the different page parts.

As we can see, even extremely short fragments such as U-R or T carry enough information for successful match-ing. We also observe that beyond some point using longer summaries becomes unwarranted, as we gain small improve-ments in relevance in exchange for considerably larger com-munication and computation load.
Figure 9 shows the effect of using text classification. We compare ad matching using the following feature sets:
We observe that the representation based on classification features is surprisingly powerful, and is consistently better than using the words alone. Merging the BOW and the clas-sification features together has a small positive effect, but it might be worth the added complexity, since the number of classification features (5 classes + their ancestors per sum-mary) is much smaller than the BOW.

Previous studies [15, 23] found that text summarization can improve the results of subsequent classification. Al-though we did not directly evaluate the accuracy of text clas-sification based on summaries, our findings show the benefits of classifying page summaries for ad matching.
Figure 9: The effect of classification (Dataset 1)
The experiments reported above ignored the non-judged ads for each page for the reasons explained in Section 4.2. However, IR practice often considers non-judged documents to be irrelevant, so for the sake of completeness we experi-mented with this assumption as well. Figure 10 shows the effect of considering non-judged ads as irrelevant. Obviously, the absolute numbers are lower than when non-judged ads are not used. However, the conclusions regarding the utility of text summarization for matching ads still hold. Figure 10: Considering non-judged ads as irrelevant (Dataset 1)
There are several lines of prior research that are relevant to the work reported herein, including online advertising and text summarization.
Online advertising in general and contextual advertising in particular are emerging areas of research, so the published literature is quite sparse. A recent study [27] confirms the intuition that ads need to be relevant to the user X  X  interest to avoid degrading the user X  X  experience and increase the probability of reaction.

Ribeiro-Neto et al. [19] examined a number of strategies for matching pages to ads based on extracted keywords. They used the standard vector space model to represent ads and pages, and proposed a number of strategies to improve the matching process. The first five strategies proposed in this work match pages and ads based on the cosine of the angle between their respective vectors. To find the impor-tant parts of the ad, the authors explored using different ad sections (e.g., bid phrase, title and body) as a basis for the ad vector. The winning strategy required the bid phrase to appear on the page, and then ranked all such ads by the cosine of the union of all the ad sections and the page vec-tors. While both pages and ads are mapped to the same space, there is a discrepancy (called  X  X mpedance mismatch X ) between the vocabulary used in the ads and in the pages. For example, the plain vector space model cannot easily ac-count for synonyms, that is, it cannot easily match pages and ads that describe related topics using different vocab-ularies. The authors achieved improved matching precision by expanding the page vocabulary with terms from similar pages, which were weighted based on their overall similarity to the original page.

In their follow-up work [16], the authors proposed a method to learn the impact of individual features using genetic pro-gramming to produce a matching function. The function is represented as a tree composed of arithmetic operators and functions as internal nodes, and different numerical features of the query and ad terms as leaves. The results show that genetic programming finds matching functions that signifi-cantly improve the matching compared to the best method (without page-side expansion) reported in [19].

Another approach to contextual advertising is to reduce it to the problem of sponsored-search advertising by extract-ing phrases from the page and matching them with the bid phrase of the ads. Yih et al. [28] described a system for phrase extraction that uses a variety of features to deter-mine the importance of page phrases for advertising pur-poses. The system is trained with pages that have been hand-annotated with important phrases. The learning algo-rithm takes into account features based on TFIDF ,HTML meta data, and search query logs to detect the most impor-tant phrases. During evaluation, each phrase up to length 5 is considered a potential result and evaluated against the trained classifier. In our recent work [5] we experimented with a phrase extractor developed by Stata et al. [24]; how-ever, while slightly increasing the precision, it did not change the relative performance of the explored algorithms.
Langheinrich et al. [17] studied customization techniques for matching ads to users X  short-term interests. To capture short-term interests, the authors used search queries as well as visited URLs, which could then be looked up in Web directories.

With the exception of the study by Yih et al. [28], all prior works mostly experimented with the different parts of the ad, assuming the publisher X  X  page is given in its entirety. The latter study did take into account the different page parts (e.g., title, meta data, and specific location of the text on the page), but they used them for a completely different task, namely, identifying good advertising keywords. In con-trast, in this work we study the importance of the different parts of the page for the process of contextual ad matching, while our primary aim is to make the matching process as computationally efficient as possible without sacrificing the matching quality.
An important research direction in web advertising is pre-dicting the clickthrough rate (CTR), that is, the number of clicks a given ad is likely to solicit if displayed on a given page.

Regelson and Fain [18] estimated the CTR by clustering ads by their bid phrases. The clickthrough rate was aver-aged over each cluster, and the CTR estimate for new ads was obtained by finding the nearest cluster. More recently, Richardson et al. [20] estimated the clickthrough rate by analyzing the different parts of the ads (e.g., bid phrases, landing page, and title). Again, both works focused on the ad side of the matching problem, while we study the role of the different parts of the page to which ads are matched.
Our analysis of parts of the page instead of the entire page for ad matching relies on the findings of prior stud-ies in Web page summarization. The latter is different from general text summarization in two important aspects. First, it relies on markup and other clues that are typically found on Web pages but not in plain text documents. Second, Web pages are often more noisy and generally do not qual-ify as Standard Written English, which is often assumed in mainstream text summarization.

Buyukkokten et al. [7], and later Alam et al. [1] stud-ied summarization of Web pages for presentation on hand-held devices. Sun et al. [25] summarized Web pages by using clickthrough data from a search engine, which allowed them to associate pages with queries that retrieved them. The authors argued that when users click on a search re-sult retrieved for a given query, the words of a query can be viewed as highly characteristic of the page content, and thus useful in its summary. Jatowt and Ishizuka studied the effect of the dynamic nature of Web pages on their summa-rization [14]. The authors proposed to collectively analyze historic versions of the page to gain insights into the terms that are most characteristic of this page. Berger and Mit-tal [2] argued that Web pages often lack coherent text and well-defined discourse structure, and consequently extrac-tive summarization techniques are not applicable to them. To address the peculiar nature of Web page summarization, they proposed to perform non-extractive summarization by  X  X ranslating X  a page using techniques based on statistical machine translation.

Several works studied the synergy between text summa-rization and text classification. Kolcz et al. [15] used sum-maries to perform feature selection, assuming that terms that occur in the summary are more informative for cate-gorization. Shen et al. [23] also found that carefully crafted summaries of pages can notably increase the precision of text classification by eliminating less important and more noisy parts of the page. Both these works found that page title, first paragraph and meta fields (keywords/description) carry a significant amount of information about the page.
We presented a new methodology for contextual Web ad-vertising in real time. Prior works in the field explored the relative importance of the different constituent part of ads. In this work, we focused on the contributions of the different fragments of the pages. Extracting small but informative parts of pages is important because often page content is not available for analysis ahead of time, as is the case for dynamically created or frequently updated pages.

Our approach allows to match ads to pages in real time, without prior analysis of the page content. Our solution is easy to implement within the standard JavaScript mecha-nisms used for ad placement, and adds only 500 X 600 bytes to the usual request for ads. We employ text summarization techniques to identify short but informative page fragments that can serve as a good proxy for the entire page. We also use two source of external knowledge. First, we extract information from the page and referrer URLs, which often contain words pertinent to the page topic. Second, we use text classification techniques to classify the page summary with respect to a large taxonomy of commercial topics.
Experimental findings confirm that using only a small por-tion of the page text can yield highly relevant ads, and the quality of summary-based ad matching is competitive with that of using the full page. For example, for Dataset 1 we observed that using only 5% of the page text can still yield 97% X 99% of the full-text-based relevance (94% X 99% for Dataset 2). We identified the various key parts of the page, and analyzed their contributions collectively and indi-vidually. Our results also confirmed that page-ad matching can be improved by classifying page summaries, and match-ing pages and ads in the augmented space of words and classification-based features.

In our experiments, we observed that in some cases merely taking the first few hundred bytes of the page text also yields reasonable results. However, using the page prefix rather than the page structure entails some caveats: it raises higher privacy concerns (if the page is personalized) and it is easier to spam. Further observation and experimentation is nec-essary, in particular for long pages. In future work, we also plan to experiment with different weighting of the various page fragments, using machine learning techniques to deter-mine the optimal weights. We also plan to examine ways of constructing the summary based on the page type (e.g., for a blog page, the prefix information might be useful as it is likely to contain the most recent postings, while for a concert listing, the anchor text might be of crucial importance). We thank our colleagues Bo Pang for the text tokenization module, Prashanth Bhat for the ad indexer, and Kishore Papineni for suggesting the idea of extracting words from URLs. We also thank Arkady Estrin for JavaScript advice.
