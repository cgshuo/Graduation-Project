 Summarization is a common task in many a pplications. For example, in a Web search system, the summarization can return a concise and informative summary of the result documents for each query. A precise summary can greatly help users to digest the large number of returned documents in the traditional search engine. 
Unlike the generic summarization task, which aims at extracting a summary about general ideas of document(s), query-focused summarization tries to extract a summary related to a given query. Therefore, the summarization results of different queries might be different, for the same document collection. 
Previously, much effort has been made for dealing with this problem. For example, methods based on word frequency (e.g., TF) have been proposed to score each sen-tence and sentences with the highest scores have been extracted as the summary. However, the word-based methods tend to be overly specific in matching words of the query and documents. For example, it cannot discover semantic similarity between different terms such as  X  X ata mining X  and  X  X nowledge discovery X . In addition, with a Query-Focused Summarization by Combining Topic Model and Affinity Propagation 175 query, the user may want to know information about different perspectives of the query, not just documents containing words in the query. 
Therefore, how to discover topics, more accurately, how to discover the query-related topics from the document collection has become a challenging issue. Recently, probabilistic topic models have been successfully applied to multiple text mining tasks [18] [21] [22] [23] [24]. In a topic model, documents are represented in a latent topic space. Some methods also consider usin g the estimated latent topical aspects for each document to help extract the generic summary [2] [5] [12]. However, such methods are inapplicable to query-focused summarization, because the existing topic general on all documents. 
From another perspective, how to determine the length of the summary is an open issue in all document(s) summarization tasks. It would be more serious in the query-documents may vary largely (from a few documents to thousands of documents). Previously, the length of summary is predefined with a fix value (e.g., a fixed number of sentences/words). However, it is obviously not reasonable to specify a same length value to the summary of several document and that of thousands of documents. 
In this paper, we aim at breaking these limitations by combining statistical topic models and affinity propagation for query-focused summarization. We first identify the major tasks of query-focused summarization and propose a probabilistic topic model called query Latent Dirichlet Allocation (qLDA). The qLDA represents each document as a combination of (1) a mixture distribution over general topics and (2) a model, we treat the query as a kind of prior knowledge of documents. Thus the learned topic distribution of each document depends on the query. Based on the mod-sentences from documents. 
Experimental results on the DUC data show that our proposed method outperforms the baseline methods of using term frequency and LDA. Experimental results also the human labeled summary. 
The remainder of the paper is organized as follows. In Section 2, we formalize the major tasks in query-focused summarization. In Section 3, we present our approach and conclude the paper in Section 6. In this section, we first present several necessary definitions and then define the tasks of query-focused summarization. 
We define notations used throughout this paper. Assuming that a query q consists quence of N d words, denoted as w d , where each word is chosen from a vocabulary of summarizes the notations. Definition 1 (Summary and Key Sentence). A summary is comprised of multiple center sentence in affinity propagation, which will be described in Section 3.4. Definition 2 (Query-focused Summarization). Given a document cluster C and a query q, the task of query-focused summarization is to identify the most representative sentences that are relevant to the query, from the document cluster.
 The query represents the information need of the user. With different queries, the user asks for summaries of the document cluster from different perspectives. Following [3] [11] [18], we can define topical aspects of a document cluster as: Definition 3 (Topical Aspects of Document Cluster). A document cluster contains information of different aspects, each of which represents one topic. Hence, a docu-ment cluster can be viewed as a combination of different topics.
 For example, in people comments of  X  X loba l climate change X , the topical aspects may include  X  X ollution X ,  X  X isaster X ,  X  X xtinction of bio-species X , etc. Discovery of the topi-cal aspects from the document cluster can be helpful to obtain an overview under-standing of the document cluster. Definition 4 (Query-focused Topica l Aspects of Document Cluster). Different from general topical aspects, query-focused topical aspects mainly include topics related to the query.
 For the example of  X  X lobal climate change X , suppose the query is about the  X  X mpacts X   X  X he sea level rise X  and  X  X he extinction of bi o-species X , and treat the other topics in the second place. Discovery of the query-focused topical aspects thus can be greatly help-ful for extracting the query-focused summary. 
Based on these definitions, the major tasks of query-focused summarization can be defined as: Query-Focused Summarization by Combining Topic Model and Affinity Propagation 177 1) Modeling query-focused topical aspect s. Given a document cluster and a 2) Identifying key sentences for the query from the document cluster. Based on the general topical aspects in the document cluster, but cannot capture the query-focused topical aspects. It is not clear how to balance the generality and specificity in a principled way. Second, it is unclear ho w to determine how many sentences should be included in the summary. 3.1 Overview At a high level, our approach primarily consists of the following three steps: 1) We propose a unified probabilistic topic model to uncover query-focused 2) After building the topic model, we calculate similarities between sentences 3) We take the  X  X enter X  sentences as the summary. The number of sentences in 
We see from above discussions that our main technical contributions lie in the first and second steps. In the first step, we use qLDA to capture the dependencies between document contents and the query, in other words, to obtain the query-focused topical propagation to identify the key sentences . In the remainder of this section, we will first introduce the baseline method based on an existing topic model: LDA [3]. We will then describe our proposed topic model in detail and explain how we make use of the affinity propagation to identify the key sentences. 3.2 Baseline Model Latent Dirichlet Allocation (LDA). LDA [3] is a three-level Bayesian network, which models documents by using a latent topic layer. In LDA, for each document d , a multinomial distribution  X  d over topics is first sampled from a Dirichlet distribution with parameter  X  . Second, for each word w di , a topic z di is chosen from this topic dis-tribution. Finally, the word w di is generated from a topic-specific multinomial Accordingly, the generating probability of word w from document d is: 
For scoring a sentence, one can sum up the probability P ( w | d ) of all words in a sentence. 3.3 Modeling Documents and Query Our topic model is called query Latent Dirichlet Allocation (qLDA). The basic idea in this model is to use two correlated generative processes to model the documents and the query. The first process is to model the topic distribution of the query. The process is similar to the traditional topic model LDA [3]. The second process is to model the from a document-specific distribution or from a query-specific distribution. 
Formally, the generative process can be described as follows: 
For inference in the qLDA model, the task is to estimate the two sets of unknown parameters: (1) the distribution  X  of M document-topics, the distribution  X  q of query-topics, the distribution  X  of M document-Bernoulli, and the distribution  X  of T topic-words; (2) the corresponding coin x di and topic z di for each word w di in the document sampling [9] for parameter estimation due to its ease of implementation. Additionally, instead of estimating the model parameters directly, we evaluate (a) the posterior process; (b) the posterior distribution on x and z , and then use the sampling results to infer  X  ,  X  and  X  for the second generative process. (Due to space limitation, we omit-ted the derivation of the posterior probability. Details can be referred to [22].) 
During the Gibbs sampling process, the algorithm keeps track of an M x T (docu-ment by topic) count matrix, a T x V (topic by word) count matrix, an M x2 (document topic given a document  X  dt , the probability of a word given a topic  X  zv by: Query-Focused Summarization by Combining Topic Model and Affinity Propagation 179 where n dz (or n qz ) is the number of times that topic z has been sampled from the multi-has been sampled from the multinomial specific to topic z ; the superscript  X  q  X  denotes that we count the numbers in all the queries. Query Expansion. Sometimes the query may be so short that the qLDA model can-not yield good results. We employ a method to expand the query. Specifically, we preprocess a query by word stemming and stop-words filtering. After that, we obtain a co-occurring words from the document cluster and add them into the query. We con-sider words appearing in a window-size of the word w i as its co-occurrence words, i.e. words before and after the word w i . We set the window size as 1. 3.4 Key Sentences Extraction We use affinity propagation [8] to discover key sentences from the document cluster. Specifically, we first employ the learned topi c model from the above step to calculate an asymmetric similarity between sentences; we then use all sentences and similarities between them to generate a graph; next, we perform propagations for links on the graph. The basic idea is that if a sentence has a high  X  X epresentative X  score for other sentences, then the sentence is likely to be selected as a key sentence in the summary. In the rest of this section, we will introduce how we generate the sentence graph and describe how we conduct the propagation on the graph. Sentence Graph Generation. From the topic modeling step (cf. Section 3.3), we obtain a topic dist ribution for each sentence. Based on the distribution, we calculate a similarity matrix between sentences. Specifically, we define an asymmetric similarity between two sentences according to the negative KL -divergence [14]: where p ( z | S 1 ) is the probability of topic z sampled from sentence S 1 . 
Another optional input of the affinity propagation is a  X  X reference X  score for each sentence, which can be taken as the priori information of how likely a sentence can be selected as the key sentence. Note that the input is optional, which provides a flexible way to integrate the prior information for summarization, thus the method has the flavor of semi-supervised learning. In this work, we do not combine the human pro-similarity with the query, i.e., cording to the topic model;  X  is a parameter to balance the two terms and it is empiri-cally set as 0.5. 
We combine the preference scores into the similarity matrix. They are put in the s ( i,k ). 
Following [8], we define two types of links between sentences  X  X esponsibility X  and how well-suited sentence k is to serve as the center of sentence i , by considering the from sentence k to i , represents how appropriate the sentence i to choose sentence k as the center, by considering for what sentences that sentence k can be the center. Fig. 1. shows an example of the generated sentence graph. Affinity Propagation. Different from other propagation algorithms (e.g. PageRank [20]), which usually propagate scores of nodes along with links on a graph; the affinity propagation used in this paper propagates score of links. Two scores, i.e. r ( i , process (as shown in algorithm 1).
 Query-Focused Summarization by Combining Topic Model and Affinity Propagation 181
After affinity propagation, we identify key sentences by combining the  X  X vailabil-key sentences are selected without a predefined length of the summary. 4.1 Experimental Setup Data Sets. Document Understanding Conference (DUC), now moved to Text Analy-sis Conference (TAC), provides a series of benchmarks for evaluating approaches for document summarization. We conducted experiments on data sets of DUC2005 and DUC2006. Each of them contains 50 document clusters, accordingly 50 query-focused summarization tasks. Documents are from Financial Times of London and Los Angeles Times. For each summarization task, given a query and a document cluster containing 25-50 documents, the objective is to generate a summary for an-swering the query. (In DUC, the query is called  X  X arrative X  or  X  X opic X ). Evaluation measures. We conducted evaluations in terms of ROUGE [17]. The meas-ure evaluates the quality of the summarization by counting the numbers of overlapping units, such as n-grams, between the automatic summary and a set of reference summa-ries (or human labeled summaries in DUC). In our experiment, we calculated the macro-average score of ROUGE-N on all document sets. We utilized the tool ROUGE 1.5.5 with the parameter setting  X -n 4 -w 1.2 -m -2 4 -u -c 95 -r 1000 -f A -p 0.5 -t 0 -a X . Summarization methods. We define the following summarization methods: TF: it uses only term frequency for scoring words and sentences. The basic idea is to to [17], except that [17] also adjusts the scoring measure to reduce redundancy. 
LDA + TF: it combines the LDA based scores with the TF based scores. qLDA + TF: it combines the qLDA based scores with the TF based scores.. 
LDA+AP: it combines the LDA model with affinity propagation. qLDA+AP: it combines the qLDA model with affinity propagation. For both LDA and qLDA, we performed model estimation with the same setting. accurately, by minimizing the perplexity). One can also use some solutions like [4] to automatically estimate the number of topics. 4.2 Results on DUC All experiments were carried out on a server running Windows 2003 with two Dual-Core Intel Xeon processors (3.0 GHz) and 4GB memory. For each dataset, it needs about 55 minutes for estimating the qLDA model. For each query, it needs about 40 seconds to identify the key sentences (including the I/O time). Because the qLDA model estimating results will be shared for all summarization tasks, and only the time spent on identifying the key sentences is concerned for online applications, then if we do qLDA modeling offline and cache the model parameters to save the I/O time, our solution will be efficient enough for online summarizations. 
Table 2 shows the performance of summarization using our proposed methods ( T=60 ) and the baseline methods on the DUC 2006 data set. We see from Table 2 that, in terms of Rouge1, qLDA+AP results in the best performance; while in terms of Rouge2 and RougeSU4, qLDA+TF obtains the highest score. 
We also compared our results with the three best participant systems in DUC2005 and DUC2006. Table 3 shows the evaluation results. We see that our proposed approaches are comparable with the three best systems. In terms of Rouge1, qLDA+AP achieves the best results (0.39089 and 0.41433) on both data sets. In terms of Rouge2, qLDA+TF achieves the best performance (0.07144) in DUC2005. In terms of RougeSU4, qLDA+TF achieves the best perform-ance (0.15800) in DUC2006. We need to note that our approach does not make use of any external information; while the systems usually (heavily) depend on some exter-nal knowledge, for example System 15 [25] and System 17 [15]. We can also see that qLDA+AP performs better than LDA+AP on almost all measures, which confirms us that qLDA for the query-focused summarization task is necessary. 
We also evaluated the length of summary by comparing the sentence numbers ex-tracted by different methods with that in the referred (human labeled) summary. Table 4 shows the comparison result. We see that the sentence number extracted by our proposed approach is closer to that of the referred summary. 
We conducted a detailed analysis on the summarization task D307 (DUC2005), which talks about  X  X ew hydroelectric proj ects X . The query is  X  X hat hydroelectric Query-Focused Summarization by Combining Topic Model and Affinity Propagation 183 projects are planned or in progress and wh at problems are associated with them? X . Four human summaries (A, B, C, and D) are given for this task. They mainly focus on problems of projects, like environmental problems, social problems, and so on. The summary obtained by TF contains many sentences about the financial problem and the power of the projects. It does not cover so many problems. Our summary covers more aspects. Table 5 shows some sentence samples. 
Fig. 2. shows a snippet of the sentence graph after affinity propagation on the summarization task D0641 (DUC2006). Circles with the gray color are selected as key sentences. The sentences are listed in the right table. Most of extraction-based document summarization methods aim to rank sentences by different scoring schemes and select ones with the highest scores as summaries. Fea-tures such as term frequency [19] [26], cue words, stigma words, topic signature [16] and topic theme [10] are used to calculate the importance score of words. A composi-tion function is then utilized to score the sentences. 
Nenkova et al. [19] uses term frequency as the important factor for choosing words to be included in summaries and utilize an average sum of the word score to compute the score of a sentence. 
Conroy et al. [6] propose an oracle score based on the probabilistic distribution of unigrams of human summaries. They utilize the query words and the topic signature terms to approximate the probabilistic distribution. Our qLDA model is relevant to this model to some sense, except that this model does not consider the topical aspects in the document cluster. 
Over the last several years, several methods have been proposed for documents summarization using latent topics in the document cluster. For example, Barzilay and Lee [1] use the Hidden Markov Model to learn a V-topic. Their method requires a labeled training data to learn the probability. Topic models like pLSI has been applied to spoken document summarization [12] and text document summarization [2]. The word topical mixture model [5] in spoken document summarization resembles LDA. However, all of the aforementioned topic-based methods do not consider the query information or integrate the query information in a heuristic way. 
More recently, some research effort has been made for incorporating the query in-formation into the topic model. For example, Daum X  and Marcu [7] propose a hierar-chical Bayesian model to compute the relevance of a sentence to a query. However, the method needs a predefined length for the generated summary. Kummar et al.[13] proposed a method to uncover storylines of multi-documents based on graph theo-retic. But the results they get are bags of words instead of natural sentences. 
Frey and Dueck [8] propose the notion of affinity propagation, which can cluster the method for query-focused summarization by combining a query-focused topic model and the affinity propagation. Different from existing methods, our method can discover multiple topics related to the query. Moreover, our method can automatically discover the number of sentences for each summarization task. In this paper we investigate the problem of query-focused summarization on multiple documents. We propose a novel method to deal with the problem by combining a statistical topic model (i.e. qLDA) with the affinity propagation. The method can automatically identify key sentences from the document cluster without a predefined summary length. Experiments on DUC data show that our proposed method can out-perform the general topic-based methods as well as a word-based method (TF) for query-focused summarization tasks. Experiment results also indicate that the length of the summary extracted by our approach is close to that of the referred summary. 
The proposed solution is quite general. It can be applied to many other applica-more accurate prior information, in particular, query expansion when the query is considered during the affinity propagation process. Query-Focused Summarization by Combining Topic Model and Affinity Propagation 185
