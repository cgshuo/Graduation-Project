 Incomplete data sets have become almost ubiquitous in a wide variety of application domains. Common examples can be found in climate and image data sets, sensor data sets and medical data sets. Tb.e incompleteness in these data sets may arise from a number of factors: in some cases it may simply be a reflection of certain measurements not be-ing available at the time; in others the information may be lost due to partial system failure; or it may simply be a result of users being unwilling to specify attributes due to privacy concerns. When a significant fraction of the entries are missing in all of the attributes, it becomes very diffi-cult to perform any kind of reasonable extrapolation on the original data. For such cases, we introduce the novel idea of conceptual reconstruction, in which we create effective con-ceptual representations on which the data mining algorithms can be directly applied. The attraction behind the idea of conceptual reconstruction is to use the correlation structure of the data in order to express it in terms of concepts rather the original dimensions. As a result, the reconstruction pro-cedure estimates only those conceptual aspects of the data which can be mined from the incomplete data set, rather than force errors created by extrapolation. We demonstrate the effectiveness of the approach on a variety of real data sets. 
In recent years, a large number of data sets which "are available for data mining tasks are incompletely specified. 
An incompletely specified data set is one in which a certain percentage of the values are missing. This is because the data sets for data mining problems are usually extracted from real world situations in which either not all measure-ments may be available or not all the entries may be relevant to a given record. In other cases, where data is obtained from users directly, many users may be unwilling to specify *Both authors contributed equally to this work. personal or classroom use is granted without fee provided that copies requires prior specific permission and/or a fee. KDD 01 San Francisco CA USA 
Copyright ACM 2001 1-58113-391-x/01/08...$5.00 all the attributes because of privacy concerns [2]. In many cases, such situations result in data sets in which a large percentage of the entries are missing. This is a problem since most data mining algorithms assume that the data set is completely specified. 
Common solutions to the missing data problem include the use of imputation, statistical or regression based proce-dures [5, 8] in order to estimate the entries. Unfortunately, these techniques are also prone to estimation errors with in-creasing dimensionality and incompleteness. This is because when a large percentage of the entries are missing, each at-tribute can be estimated to a much lower degree of accuracy. Furthermore, some attributes can be estimated to a much lower degree of accuracy than others, and there is no way of knowing a-priori which estimations are the most accurate. A discussion and examples of the nature of the bias in using direct imputation based procedures may be found in [3]. 
We note that any missing data mechanism would rely on the fact that the attributes in a data set are not independent from one another, but that there is some predictive value from one attribute to another. If the attributes in a data set are truly uncorrelated, then any loss in attribute entries leads to a true loss of information. In such cases, missing data mechanisms cannot provide any estimate to the true value of a data entry. Fortunately, this is not the case in most real data sets, in which there are considerable redun-dancies and correlations across the data representation. 
In this paper, we discuss the novel concept of conceptual reconstruction, in which we express the data in terms of the salient concepts of the correlation structure of the data. This conceptual structure is determined using techniques such as Principal Component Analysis [4]. These are the directions in the data along which most of the variance occurs, and are also referred to as the conceptual directions. We note that even though a data set may contain thousands of di-mensions, the number of concepts in it may be quite small. For example, in text data sets the number of dimensions (words) are over 100,000 but there are often only 200-400 salient concepts [7]. In this paper, we will provide evidence of the fact that even though predicting the data along ar-bitrary directions (such as the original set of dimensions) is fraught with errors, the components along the conceptual directions can be predicted quite reliably. This is because the conceptual reconstruction method uses these redundan-cies in an optimum way so as to estimate whatever concep-tual representations are reliably possible rather than force extrapolations on the original set of attributes. Such a strat-egy is advantageous, since it only tries to derive whatever information is truly available in the data. 
This paper is organized as follows. The remainder of thi,; section provides a formal discussion of the contributions of this paper. In the next section we will discuss the basic con-ceptual reconstruction procedure, and provide intuition on why it should work well. In section 3, we provide the imple-. mentation details. Section 4 contains the empirical results. The conclusions and summary are contained in section 5. 
This paper discusses a technique for mining massively in-. complete data sets by exploiting the correlation structure of data sets. We use the correlation behavior in order to cre-ate a new representation of the data which predicts only as much information as can be reliably estimated from the data set. This results in a new full dimensional representation of the data which does not have a one-to-one mapping with the original set of attributes. However this new represen-tation reflects the available concepts in the data accurately and can be used for many data mining algorithms, such as clustering, similarity search or classification. 
In order to facilitate further discussion, we will define the percentage of attributes missing from a data set as the in-completeness factor. The higher the incompleteness factor, the more difficult it is to obtain any meaningful structure from the data set. The conceptual reconstruction technique is tailored towards mining massively incomplete data sets for high dimensional problems. As indicated earlier, the at-tributes in high dimensional data are often correlated. This results in a natural conceptual structure of the data. For instance, in a market basket application, a concept may con-sist of groups or sets of closely correlated items. A given cus-tomer may be interested in particular kinds of items which are correlated and may vary over time. However, her con-ceptual behavior may be much clearer at an aggregate level, since one can classify the kinds of items that she is most interested in. In such cases, even when a large percentage of the attributes are missing, it is possible to obtain an idea of the conceptual behavior of this customer. 
A more mathematically exact method for finding the ag-gregate conceptual directions of a data set is Principal Com-ponent Analysis (PCA) [4]. Consider a data set with N points and dimensionality d. In the first step of the PCA technique, we generate the covariance matrix of the data set. The covariance matrix is a d * d matrix in which the (i, j)th entry is equal to the covariance between the dimen-sions i and j. In the second step we generate the eigen-vectors {E~'... ~} of this covariance matrix. These are the directions in the data, which are such that when the data is projected along these directions, the second order corre-lations are zero. Let us assume that the eigenvalue for the eigenvector ~ is equal to 2i. When the data is transformed to this new axis-system, the vhlue A~ is also equal to the variance of the data along the axis ~. The property of this transformation is that most of the variance is retained in a small number of eigenvectors corresponding to the largest values of A~. We retain the k &lt; d eigenvectors which cor-respond to the largest eigenvalues. An important point to understand is that the removal of the smaller eigenvalues for highly correlated high dimensional problems results in a new data set in which much of the noise is removed [6], and the qualitative effectiveness of data mining algorithms such as similarity search is improved. This is because these few eigenvectors correspond to the conceptual directions in the data along which the non-noisy aspects of the data are preserved. One of the interesting results that this paper will show is that these relevant directions are also the ones along which the conceptual components can be most accu-rately predicted by using the data in the neighborhood of the relevant record. We will elucidate this idea with the help of an example. Let Q be a record with some missing attributes denoted by B. Let the specified attributes be denoted by A. Note that in order to estimate the conceptual component along a given direction, we find a set of neighborhood points based on the known attributes only. These points are used in order to estimate the corresponding conceptual coordinates. Corre-spondingly, we define the concept of an (e, A)-neighborhood of a data point Q. is the set of points from the data set D such that the distance of each point in it from Q based on only the attributes in A is at most e. We shall denote this neighborhood by S(Q, e, A). Once we have established the concept of (e, A)-neighborhood, we shall define the concept of (e, A, g)-predictability along the eigenvector g. Intuitively, the predictability along an eigenvector g is a measure of how closely the value along the eigenvector g can be predicted using only the behavior of the neighborhood set S(Q, e, A). 
DEFINITION 2. For a given eigenveetor -d, let .hf be the coordinates along e in the transformed domain for the set S(Q, e, A). Let p be the mean of the elements in A/" and a be the standard deviation. The (e, A, e)-predietability of a data point Q is defined as the ratio I#/~1. Since the above ratio measures the mean to standard devi-ation ratio, greater amount of certainty in the accuracy of the prediction is obtained when the ratio is high. We shall 
Figure 1: Predictability for a Simple Distribution now illustrate with the help of an example, why (e, A, E)-predictability of eigenvector E is higher when the correspond-ing eigenvalue is larger. In Figure 1, we have shown a two-dimensional example for the case when a data set is drawn from a uniformly distributed rectangular distribution cen-tered at the origin. We also assume that this rectangle is banked at an angle 0 from the X-axis and the sides of this rectangle are of lengths a and b respectively. Since the data isuniformly generated within the rectangle, if we were to perform PCA on the data points, we would obtain eigen-vectors parallel to the sides of the rectangle. The corre-sponding eigenvalues would be proportional to a 2 and b 2 re-spectively. Without loss of generality, we may assume that a &gt; b. Let us assume that the eigenvectors in the corre-sponding directions are e-'T and ~ respectively. Since the variance along the eigenvector ~'T is larger, it is clear that the corresponding eigenvalue is also larger. Let Q be a data point for which the X-coordinate x is shown in Figure 1. 
Now, the set 8(Q, e, {X}) of data points which is closest to the point Q based on the coordinate X = x is in a thin strip of width 2e centered at the segment marked with a length of c in Figure 1. In order to make an intuitive analysis  X  without edge effects, we will assume that e -~ 0. Therefore, in the diagram for Figure 1, we have just used a vertical line which is a strip of width zero. Then, the standard de-viation of the records in S(Q, e, {X}) along the Y axis is given by c/x/~ = b. secant(O)/v/-~ using the formula for a uniform distribution along an interval of length c. The cor-responding components along the eigenvectors ~ and ~ are respectively. The corresponding means along the eigenvec-tots ~T and ~-~ are given by Ix  X  sec(O)l and 0 respectively. 
Now we can substitute for the mean and standard deviation values in Definition 2 in order to obtain the following results: 1. The (~,{X},~T)-predictability of the data point Q is 
Ix/b. sine(O)l. 2.The (e, {X}, ~'~)-predictability of the data point Q is 0. 
Thus, this example illustrates that predictability is much better in the direction of the larger eigenvector ~. Fur-thermore, with reduced value of 0, predictability along this eigenvector (which has an angle 0 with the specified at-tribute) improves. We will now proceed to formalize some of these intuitive results. for'~i, the greater the relative predictability of the conceptual component along ~7. 
This intuition summarizes the implications of the example discussed in the previous section. In the previous example, it was also clear that the level of accuracy with which the con-ceptual component could be predicted along an eigenvector was dependent on the angle with which the eigenvector was banked with the axis. In order to formalize this notion we introduce some additional notations. Let (bl,... , bn) corre-spond to the unit direction vector along a principle compo-nent (eigenvector) in a data set with n attributes. Clearly the larger the value of bi, the more the variance of the projec-tion of attribute i along the principle component i and vice versa. Given our above notions of A and B let us further define the weight ratio WA/1Z =  X ~'~iEA b2i / ~/~iEB 2 b i  X  
INTUITION 2. For a given vector Ei, the larger the weighted ratio W A/ S, the greater the relative predictability of the con-ceptual component along ~7. 3. DETAILS OF THE CONCEPTUAL RE-
In this section we outline the overall conceptual recon-struction procedure along with key implementation details. More specifically, two fundamental problems with the im-plementation need to be discussed. In order to find the conceptual directions, we first need to construct the covari-ance matrix of the data. Since the data is massively in-complete, this matrix cannot be directly computed but only estimated. This needs to be carefully thought out in order to avoid bias in the process of determining the conceptual directions. Second, once the conceptual vectors (principal components) are found, we will work out the best meth-ods for finding the components of records with missing data along these vectors. 
The overall conceptual reconstruction algorithm is illus-trated in Figure 2. For the purpose of the following descrip-tion, we will assume without loss of generality that the data set is centered at the origin. 
The goal in Step 1 is to compute the covariance matrix M from the data. Since the records have missing data, the co-variance matrix cannot be directly constructed. Therefore, we need methods for estimating this matrix. In a later sub-section, we will discuss methods for computing this matrix M. Next, we compute the eigenvectors of the covariance matrix M. The covariance matrix for a data set is positive semi-definite and can be expressed in the form M = PNP T, where N is a diagonal matrix containing the eigenvalues A1... Ad. The columns of P are the eigenvectors el ... ed, which form an orthogonal axis-system. We assume without loss of generality, that the eigenvectors are sorted so that AI &gt; A2 &gt; ... Aa. To find these eigenvectors, we rely on the popular Householder reduction to tridiagonal form and then apply the QL transform [4], which is the fastest known method to compute eigenvectors for symmetric matrices. 
Once these eigenvectors have been determined, we decide to retain only those which preserve the greatest amount of variance from the data. Well known heuristics for decid-ing the number of eigenvectors to be retained may be found in [4]. Let us assume that a total of m &lt; d eigenvectors el... em are retained. Next we set up a loop for each re-tained eigenvector ~7 and incompletely specified record Q in the database. We assume that the set of known attributes in 
Q are denoted by A, whereas the set of unknown attributes axe denoted by B. We first find the projection of the speci-fied attribute set A onto the eigenvector ~7. We denote this projection by Y~, whereas the projection for the unspeci-fied attribute set B is denoted by Y~. Next, the K nearest records to Q are determined using the euclidean distance on the attribute set A. The value of K is a user-defined pa-rameter and should typically be fixed to a small percentage of the data. For the purposes of our implementation, we set the value of K consistently to about 1% of the total number of records, subject to the restriction that K was at least 5. 
This representative set of records is denoted by C in Fig-ure 2. Once the set C have been computed we estimate the missing component Y~ of the projection of Q on ~. For each record in the set C we compute its projection along ei using the attribute set B. The average value of these projections is then taken to be the estimate Y~ for Q. Note, that it is possible that the records in C may also have missing data for the attribute set B. For such cases, only the components from the specified attributes are used in order to calculate the Y$ values for that record. The conceptual coordinate of the record Q along the vector ~ is given by yi = y~ + y~. 
Thus, the conceptual representation of the record Q is given by (y1 ... y,-~). 
At first sight, a natural method to find the covariance be-tween a given pair of dimensions i and j in the data set is to simply use those entries which are specified for both di-mensions i and j and compute the covariance. However, this would often lead to considerable bias, since the entries which are missing in the two dimensions are also often correlated with one another. Consequently, the covariance between the specified entries is not a good representative of the overall covariance in a real data set. This is especially the case for massively incomplete data sets in which the bias may be considerable. By using dimensions on a pairwise ba,~is only, such methods ignore a considerable amount of infor-mation that is hidden in the correlations of either of these dimensions with the other dimensions for which fully speci-fied values are available. In order to harness this hidden information, we use a pro-cedure in which we assume a distribution model for the data and estimate the parameters of this model in terms of which the covariances are expressed. Specifically, we use the tech-nique discussed in [5], which assumes a Gaussian model for the data, and estimates the covariance matrix for this Gaus-sian model using an Expectation Maximization (EM) algo-rithm. Even though some inaccuracy is introduced because of this modeling assumption, it is still better than the vanilla approach of pairwise covariance estimation. To highlight some of the advantages of this approach, we conducted the following experiment. 
We used the Musk data set from the UCI data set repos-itory to create an incomplete data set in which 20% of the attribute values were missing. We computed the conceptual directions using both the model based approach 1 and the simple pairwise covariance estimation procedure. We com-of the conceptual directions under both estimation methods and compared these direction vectors with the correspond-ing unit vectors constructed from the fully specified data 1Note that we did not run the EM algorithm to convergence but only for 30 iterations for this experiment. set and the actual vector will be in the range [0,1], 1 indicat-ing coincidence (maximum accuracy) and 0 indicating the two vectors are orthogonal (minimal accuracy). Figure 3 de-scribes the results of this experiment on the first 30 eigen-vectors. Clearly, the EM estimation method outperforms the pairwise estimation method. The absolute accuracy of the EM-estimation method is also rather high. For example, for the first 13 eigenvectors (which covers more than 87% of the variance in the data set) the accuracy is typically above 0.94. Figure 3: Comparing EM and Pair-Wise Estimation Once the conceptual vectors have been identified the next step is to estimate the projection of each record Q onto each conceptual vector. In the previous section, we discussed how a set C of close records are determined using the known at-tributes in order to perform the reconstruction. We defined 
C to be the set of records in the neighborhood of Q using the attribute set A. The Y~ value for Q is estimated us-ing the records in set C. It is possible to further refine the performance using the following observation. The values of lib for the records in C may often show some clustering behavior. We cluster the YB values in C in order to create the sets C1 .,. Cr, where U~=ICi = C. For each set 
Ci, we compute the distance of its centroid to the record Q using the known attribute set A. The cluster that is closest to Q is used to predict the value of Ys. The intuition behind this method is obvious. In order to perform the testing, we used several com-pletely specified data sets (Musk(1 &amp; 2), BUPA, and Letter-
Recognition) in the UCI 2 machine learning repository. The incomplete records were generated by randomly removing some of the entries from the records. We introduce a notion of incompleteness in these data sets by randomly eliminating values in records of the data set. One of the advantages of this method is that since we already know the original data set, we can compare the effectiveness of the reconstructed data set with the actual data set to validate our approach. We use several evaluation metrics in order to test the effec-tiveness of the reconstruction approach. These metrics are designed in various ways to test the robustness of the re-constructed method in preserving the inherent information from the original records. Direct Error Metric: Let Yistimated(~ ) be the estimated value of the conceptual component for the eigenvector i us-ing the reconstruction method. Let Y2ct~az(Q) be the true value of the projection of the record Q on to eigenvector i, if we had an oracle which knew the true projection onto eigenvector i using the original data set. Obviously, the closer Y~c~,at(Q) is to Y~st~mated(Q), the better the quality of the reconstruction. We define the relative error 3 along the eigenvector i as follows: Clearly, lower values of the error metric are more desirable. In many cases even when the absolute error in estimation is somewhat high, emperical evidence suggests that the corre-lations between estimated and actual values continue to be quite high. This indicates that even though the estimated conceptual representation is not the same as the true rep-resentation, the estimated and actual components are cor-related so highly that the direct application of many data mining algorithms on the reconstructed data set is likely to continue to be effective. To this end, we computed the co-variance and correlation of these actual and estimated pro-jections for each eigenvector over different values of Q in the database. A validation of our conceptual reconstruction procedure would be if the correlations between the actual and estimated projections are high. Also, if the magnitude of the covariance between the estimated and actual com-ponents along the principal eigenvectors were high it would provide further validation of our intuitions that the principle eigenvectors provide the directions of the data which have the greatest predictability. 
Indirect Error Metric: Since, the thrust of this paper is to compute conceptual representations for indirect use on data mining algorithms rather than actual attribute recon-3Note that this error metric only takes into account records that have missing data. Complete records (if any) play no role in the computation of this metric. struction, it is also useful to evaluate the methods with the use of an indirect error metric. In this metric, we build and compare the performance of a data mining algorithm on the reconstructed data set. To this effect, we use classifier trees generated from the original data set and compare it with the performance of the classifier trees generated from the reconstructed data set. Let CAo be the classification accu-racy with the original data set, and CAr be the classification accuracy with the reconstructed data set. This metric also referred to as Classification Accuracy Metric (CAM) mea-sures the relative change in classifier accuracy, over the case where the original data set was actually available to the user. 
Due to lack of space we present results here for the Musk dataset alone on two incompleteness factors 20% and 40%. The results are shown in Figure 4. In all cases, we plot the results as a function of the eigenvectors ordered by their eigenvalues where eigenvector 0 corresponded to the one with the largest eigenvalue. Figure 4a offers some empirical evidence for Intuition 1. Clearly, the predictability is better on eigenvectors with larger variance. In this data set we note that the error rapidly increases for the eigenvectors with small variance. For eigenvectors 145-165 the relative error is larger than 3. This is because these are the noise directions in the data along which there are no coherent correlations among the different dimensions. For the same reason, these eigenvec-tots are not really relevant even in fully specified data sets, and are ignored from the data representation in dimension-ality reduction techniques. The removal of such directions is often desirable even in full specified data sets, since it leads to the pruning of noise effects from the data [6]. 
To further validate our approach, we calculated the co-variances and correlations between the actual and estimated components along the different eigenvectors. The results are illustrated in Figures 4b, and 4c. For this data set the largest eigenvectors show a very strong correlation and high covari-ance between the estimated and actual projections. The correlation value for the largest 20 eigenvectors is greater than 0.95. For the first five eigenvectors, there is about an 8% drop in the average error, while the correlation continues to be extremely significant (around 0.99). 
As expected, the average errors are higher for 40% in-completeness factor when compared to 20% incompleteness factor. However, the general trend of variation in error rate with the magnitude of the variance along a principal compo-nent is also retained in this case. The correlations between the true and estimated values continue to be quite high. These results are encouraging, and serve to validate our key intuitions, especially given the high level of incompleteness of this data set. 
Similar trends were observed for the other data sets. In general, our observation across a wide variety of data sets was that the correlation between the actual components and re-constructed components was quite high. This robustness of the correlation metric indicates that for a particular eigen-vector, the error is usually created by either a consistent underestimation or a consistent overestimation of the con-ceptual component. This consistency is quite significant, since it implies that a simple linear translation of the origin along the eigenvector, could reduce the error rate further. Of course, the direction of translation is not known apriori. However, for typical data mining tasks such as clustering and similarity search where the relative position of the data points with respect to one another is more relevant, it is not necessary to perform this translation. In such cases, the reconstructed data set would continue to be highly reliable. Dataset CAo CAM(20%) CAM(40%) BUPA 62.4 0.963 0.927 Musk (1) 76.2 0.943 0.92 Musk (2) 95.0 0.96 0.945 Letter Recognition 84.9 0.825 0.62 
Since the purpose of the conceptual reconstruction method is to provide a new representation of the data on which data mining algorithms can be directly applied, it is useful to test the effects of using the procedure on one such algorithm. To this effect, we use a decision tree classifier [8], which we ap-ply both to the original (complete) representation and the conceptual representation of the missing data. 
In Table 1, we have illustrated 4 the accuracy of the clas-sifter on a conceptual representation of the data, when the percentage of incomplete entries varies from 20 % to 40 % respectively. We have also illustrated the accuracy on the original representation in the same Table. For the case of the BUPA, Musk(l) and Musk(2) data sets, the C4.5 classi-fier was at least 92% as accurate as the original data set even with 40% incompleteness. In most cases, the accuracy was significantly higher. This is evidence of the robustness of the technique and its applicability as a procedure to trans-form the data without losing the inherent information avail-able in it. Out of the four data sets tested, only the letter recognition data set did not show as effective a classification performance as the other three data sets. This difference 4Note that the original classification task for both Musk (1) and Musk (2) is to classify the original molecules into Musk and non-Musk. These data sets represents a multiple-instance classification problem with the total number of instances significantly exceeding the original number of molecules. The classification accuracies reported here are for the case where each instance is treated as an indepen-dent entity and is therefore different from the original clas-sification problem, since C4.5 does not support the multiple instance problem. is especially noticeable at the 40% incompleteness factor. There are three particular characteristics of this data set and the classification algorithm which contribute to this. The first reason is because correlation structure of the data set was not strong enough to account for the loss of infor-mation created by the missing attributes. This tends to amplify the errors of the reconstruction approach. We note that any missing data mechanism needs to depend upon inter-attribute redundancy, and such behavior shows that this dataset is not as suitable for missing data mechanisms as the other datasets. Second, on viewing the decision trees that were constructed we noticed that for this particular dataset, the classifier happened to pick the eigenvectors with lower variance first, while selecting the splitting attributes. These lower eigenvectors also are the ones where our es-timation procedure results in larger errors. This problem may not however, occur in a classifier in which the higher eigenvectors are picked first (as in PCA-based classifiers). Finally, in this particular data set, several of the classes are inherently similar to one another, and are distinguished from one another by only small variations in their feature values. Therefore, removal of data values has a severe effect on the retainment of the distinguishing characteristics among dif-ferent classes. This tends to increase the misclassification rate. 
In this paper, we introduced the novel idea of concep-tual reconstruction for mining massively incomplete data sets. The key motivation behind conceptual reconstruction is that by choosing by predict the data along the conceptual directions, we use only that level of knowledge as can be reliably predicted from the incomplete data. This is more flexible than the restrictive approach of predicting along the original attribute directions. We show the effectiveness of the technique on a wide variety of real data sets. Our re-sults indicate that even though it may not be possible to reconstruct the original data set for an arbitrary feature or vector, the conceptual directions are very amenable for re-construction. Therefore, it is possible to reliably apply data mining algorithms on the conceptual representation of the reconstructed data sets. [1] C. C. Aggarwal, S. Parthasarathy. Mining Massively Incomplete Data Sets by Conceptual Reconstruction. 
IBM Research Report, 2001. [2] R. Agrawal, R. Srikant. Privacy Preserving Data 
Mining. In ACM SIGMOD Conference, 2000. [3] Z. Ghahramani, M. I. Jordan. Learning from incomplete data. Department of Brain and Cognitive 
Sciences, Paper No. 108, MIT, 1994. [4] I. T. Jolliffe. Principal Component Analysis, 
Springer-Verlag, New York, 1986. [5] It. Little, D. Rubin. Statistical Analysis with Missing 
Data Values. Wiley Series in Prob. and Star., 1987. [6] C. H. Papadimitriou, P. Raghavan, H. Tamaki, S. Vempala: Latent Semantic Indexing: A Probabilistic 
Analysis. In ACM PODS Conference, 1998. [7] K. V. Ravikanth, D. Agrawal, A. Singh. Dimensionality Reduction for Similarity Searching in Dynamic 
Databases. In ACM SIGMOD Conference, 1998. [8] J. R. Quinlan. C,~.5: Programs for Machine Learning. 
Morgan Kaufman, 1993. 
