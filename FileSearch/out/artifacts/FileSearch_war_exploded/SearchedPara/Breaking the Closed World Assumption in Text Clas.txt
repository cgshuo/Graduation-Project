 With the rapid growth of online information, text classifiers have become one of the most important tools for people to track and organize information . And t he emergence of social media platforms has brought increas ing diversity and dynamics to the Web. M any social scien ce researchers rely on the collected online user generated content to carry out research on different social phenomenon . In this case, m ulticlass text classifiers are widely used to gather information of several topics of interest. However, most existing research on multiclass text classification makes the close d world assumption , meaning that all the test classes have been seen in training. However, in a more realistic scenario where people use a multiclass classifier to collect in formation of several topics from a data source that cover s a much broader range of topics , it is normal to break the closed world assumption and to see the arrival of documents from unknown classes that have never been seen in training. In this case, a multiclass classifier should not always assign a document to one of the known classes. I n-stead, it should identify unknown classes of doc u-ments and label them as un known or reject. This is called open ( world ) classification.

More precisely, in the traditional multiclass classification setting, the learner assumes a fixed set of classes Y = { C 1 , C 2 , ... , C m } , and the task is t o construct a  X  -class classifier using the training data . The resulting classifier is tested/applied on the data from only the m classes . While in open classification, we allow the classifier to predict l a-bels /classes from the set of C 1 , C 2 , ... , C m classes , where the ( m+ 1 ) th class C m + 1 represents the unknown which covers documents of all unknown or unseen classes or topics. In other words, every test instance may be predicted to belong to either one of the known class es y i  X   X  , or C m +1 (u n-known) .

I t is thus not sufficient for a classifier to just r e-tur n the most likely class label am ong the m known classes . An option to reject must be provided. An obvious approach to predict ing the class label  X   X   X   X  {  X  ! ! ! } for a n n -dimensional data point  X   X   X  ! is to incorporate a posterior probability e s-timator  X  (  X  |  X  ) and a decision thresh old into an e x-ist ing multiclass learning algorithm ( Kwok, 1999; Fumera and Roli, 2002 ; Huang et al., 2006; Bravo et al., 2008 ) . There are many reasons this tec h-nique would not achieve good results in open cla s-sification. As we will discuss in the following se c-tions, one of the most important reason s is that the underlying classifier is not robust or is not i n-formed enough to reject unseen classes of doc u-ments due to its significant open space risk.

T raditional multiclass learner s optimize only on the known classes under the closed world assum p-tion , w hile a potential learner for open classific a-tion has to optimize for both the known classes and for the unknown classes . Some recent research i n the field of computer vision studied the problem , which they call o pen set recognition (Scheirer et al., 2013; 2014; Jain et al., 2014) for facial reco g-n i tion. Classic learners define and opti mize over empirical risk , which is measured on the training data. F or open classification, it is crucial to consi d-er how to extend the model to capture the risk of the unknown by preventing overgeneralization or overspecialization . In order to tackle this problem, Scheirer et al. (2013 ) introduce d the concept of open space ri sk and formulated a n extension of e x-isting one -class and binary SVM s to address the open classification problem. However, as we will see in section 3 , the ir proposed method is weak as the positively labeled open space is still an infinite area. 
In this work, we propose a solution to reduce the open s pace risk while also balancing the empirical risk for open classification. Intuitively, given a positive class of documents, our open space for the positive class is considered as the space that is su f-ficiently far from the center of the positive doc u-ments . In the multiclass classification setting, each of the m target classes is surrounded by a ball co v-ering the positively labeled (the target class) area, while any document falling outside of all the m balls is considered belonging to the unknown class .
Recent work by Fei and Liu (2015) propose d a new learning strategy called c enter -based similar i-ty space learning ( CBS learning ) to deal with the problem of covariate shift in binary classification . We found that it is also suitable for open classific a-tion. Instead of conduct ing learning in the trad i-tional document space (or D -space ) with n -gram features , CBS learning learn s in a similarity space. U nlike SVM learning in D -space that bounds the positive class only by a n infinite half -space formed with the decision hyperplane , which has a huge open space risk , CBS learning finds a closed boundary for the positive class covering only a f i-nite area, which is a spherical area in the original D -space and thus reduce the open space risk signi f-ica ntly . While discussing CBS learning, w e will a l-so describe the underlying assumptions made by it which w ere not state d in our earlier paper ( Fei and Liu , 2015) . Our final multiclass classifier is called cbsSVM (based on SVM). 
To the best of our knowledge, this is the first a t-tempt to stud y multiclass open classification in text from the open space risk management perspective. Our experiments show that cbsSVM for multiclass open classification produces superior classifiers to ex isting state -of -the art methods. Compared to research on multiclass classification with the closed world assumption, there is relativ e-ly less work on open classification. In this section, we review related work on one -class classification, SVM decision score calibration , and others . 
One -class classifiers, which only rely on pos i-tive training data, are natural starting solutions to the multiclass open classification task. One -class SVM ( Scholkopf et al., 2001 ) a nd SVDD (Tax and Duin, 2004) are two representative one -class cla s-sifiers. One -class SVM treats the origin in the fe a-ture space as the only member of the negative class, and maximizes the margin with respect to it. SVDD tries to place a hypersphere with th e min i-mum radius around almost all the positive training points. It has been shown that the use of Gaussian kernel makes SVDD and One -class SVM equiv a-lent, and the results reported in (Khan and Madden, 2014) demonstrate that SVDD and One -class SVM are comparable when the Gaussian kernel is a p-plied. However, as no negative training data is used, one -class classifiers have trouble producing good separation s . W e wil l see in S ection 4 that their results are poor. 
T his work is also related to using threshol d ed probabilities for rejection. As the decision score produced by SVM is not a probability distribution , s everal techniques have been proposed to convert a raw decision score to a calibrated probability ou t-put ( Platt, 2000; Zadrozny and Elkan, 2002; Duan and Keerthi, 2005; Huang et al., 2006 ; Bravo et al., 2008 ) . Usually a parametric distribution is a s-sumed for the underlying distribution, and raw scores are mapped based on the learned model. A variation o f Platt X  X  (2000) approach is the most widely used probability estimator for SVM score calibration. It fits a sigmoid function to the SVM scores during training. Provided with a threshold, a test instance can be rejected if the highest probabi l-ity of this in stance belonging to a class is lower than the threshold in multiclass open classification settings.

Recent ly, research ers in computer vision ( Scheirer et al. , 2013 ; 2014 ; Jain et al. , 2014 ) made some attempts to solve open classification (which they call open set recognition ) for visual learning from new angles . Scheirer et al. ( 2013 ) introduced the concept of open space risk , and define d it as a relative measure. The proposed model reduces the open space risk by replacing the half -space of a b i-nary linear classifier with a positive region boun d-ed by two parallel hyper planes. While the positiv e-ly labeled region for a target class is reduced co m-pared to the half -space in the traditional linear SVM, the ir open space risk is still infinite. In ( Jain et al., 20 14 ), the authors proposed to use Extreme Value Theory (EVT) to estimate the unnormalized posterior probability of inclusion for each class by fit ting a Weibull distribution over the positive class scores from a 1 -vs -r est multiclass RBF SVM cla s-sifier . Sche irer et al. ( 2014 ) introduced the Co m-pact Abating Probability (CAP) model , which e x-plains how thresholding the probabilistic output of RBF One -class SVM manages the open space risk. Using the probability output from RBF one -class SVM as a conditioner, the authors combine RBF One -class SVM and a Weibull -calibrated SVM similar to the one in (Jain et al., 2014). For both methods (Jain et al., 2014 ; Scheirer et al. , 2014) , decision thresholds need to be chosen based on the prior knowledge of the ratio of unseen classes in testing, which is a weakness of the methods.
Dalvi et al. (2013) proposed Exploratory Lear n-ing in the multiclass semi -supervised learning (SSL) setting. In th eir w ork, an  X  X xploratory X  ve r-sion of expectation -maximization (EM) is pr o-posed to extend traditional multiclass SSL met h-ods, which deals with the scenario when the alg o-rithm is given seeds from only some of the classes in the data. It automatically explores di fferent numbers of new classes in the EM iterations. The underlying assumption is that a new class should be introduced to hold an instance  X  when the pro b-ability of  X  belonging to the existing classes is close to uniform. This is quite different from ou r work. First, it works i n the semi -supervised setting and assumes that test data is available during trai n-ing . S econd, it only focus es on improving accuracy on the classes with seed examples. In this section, we propose our solution for the open classification problem. First we discuss our strategy to reduce the open space risk while ba l-an c ing the empirical risk of the training data . Then we apply a recently proposed SVM -based learning strategy (Fei and Liu, 2015), which yields the same r isk management strategy. W e will also discuss its underlying assumptions , which was not addressed in the original paper of Fei and Liu (2015) . Lastly, we will show why the proposed solution works for open classification . 3.1 Open S pace Risk Formulation Consider the risk formulation by Scheirer et al. ( 2013 ) , where apart from the empirical risk , there is risk in labeling the open space (space away from positive training examples) as  X  X ositive X  for any known class. Due to lack of information on a cla s-sific ation function on the open space, open space risk is approximated by a relative Lebesgue mea s-ure ( Shackel, 2007 ) . Let  X  ! be a large ball of radius  X  that contains both the positively labeled open space  X  and all of the positive training examples; and let  X  be a measurable classification function where  X  !  X  = 1 for recognition of class  X  of i n-terest and  X  !  X  = 0 otherwise. The probabilistic open s pace r isk  X  !  X  of function  X  for a class  X  is defined as the fraction (in terms of Lebesgue measure) of positively labeled o pen space co m-pared to the over all measure of positively labeled space (which includes the space close to the pos i-tive examples ) .
The above definition indicates that the more we label open space as positive, the greater open space risk is. However, it does not suggest how to spec i-fy the positively labeled open sp ace  X  .

In this work, we formulate  X  as the positively labeled area that is sufficiently far from the center of the positive training examples . Let  X  ! be a closed ball of radius  X  ! centered around the center of posi tive class  X  (  X  X  X  X  ! ), which ideally contains all positive examples of class  X  ;  X  ! be a larger ball  X  ! the best in almost all settings (6 out of 7) except for 20 -newsgroup with 20 classes . In this case, it lost to ExploratoryEM by 1.12%. In fact, it is u n-fair to compare cbsSVM with ExploratoryEM b e-cause ExploratoryEM uses the test data in training. 
We also analyzed the cases where our technique does not perform well. By compari ng Table 1 and Table 6 , we see that our method loses to 1 -vs -set -linear , wsvm -linear and P i -svm -linear on both d a-tasets when training on 2 classes (25%) and testing on 10 classes, though in other cases training on 25% known classes can still yield good results. By inspecting the results, we found that in both se t-tings our technique achieves very high recall but low precision on the known classes, while achieves high precision but low recall on the unknown cla s-ses. After careful investigation, we found th is is caused by the relatively poor approximation of r a-dius  X  ! when positive and negative training exa m-ples are far apart.

To verify the cause, we conduct ed more exper i-ments on the 20 -newsgroup data using the same setting (10 classes for test and 2 for training). T he 10 classe s are listed in Table 8 . We show the r e-sults for two sets of experiments . In each set of the experiments, we keep one known c lass unchanged in training and select different classes as the s e-cond class . W e show how the results change on the unchanged class as well as the unknown (reject) classes. Table 9 gives the precision, recall, and F1 score for comp.windows.x and for the un known class es . Similarly, Table 1 0 gives the results for rec.motorcycles and for the unknown class es. The first column in both tables are the different second class es used in training. We can see that in both sets of experiments, the precision and F1 score on the unchanged known classes ( comp.windows.x and rec.motorcycles ) are better when a more sim i-lar class (closer in distance) is selected in training . In particular , comp.windows.x achieves the best r e-sult when comp.os.ms -windows.misc is the second known class, and rec.motorcycles achieves the best result when rec.autos is the second known class . This is because the radius  X  ! for each positively labeled space is determined based on the distance between the positive and negative training exa m-ples. As rela ted classes are closer in distance, a tighter boundary with smaller  X  ! can be learned. However, our results show in the cases when only 2 known classes are available, a tight boundary is hard er to achieve for either class for cbsSVM . In this paper, we proposed to study the problem of multiclass open text classification. In particular, we investigated the problem via reducing the open space risk, and proposed a solution based on ce n-
