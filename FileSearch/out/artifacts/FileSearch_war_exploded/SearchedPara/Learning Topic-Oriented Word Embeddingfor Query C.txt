 How people who seek information interact with the search engine is an important research problem. What is the user intent? How to understand their information needs? A good way of starting to pick apart the puzzle is to classify the query types.
 In order to understand what a user truly desires when searching for informa-tion, the intuitive idea is to ask each user what it is they are after. While this scenario may work for the offline mom and pop store, it definitely is not feasible for a search engine. Therefore, an automated approach, instead of the manual interaction, is necessary to be proposed.
 Query classification is to map the queries to a list of predefined topic cate-gories. However, most queries are short and ambiguous [ 14 ]. For example, accord-ing to our statistics in the experiments, we have: (1) around 20% queries contain three words; (2) queries having no more than four keywords are as frequent as almost 80%; (3) many keywords such as  X  X ava X  have multiple aspects. to represent the queries through the word embedding method. Then, the query embedding can be treated as the features to classify the queries.
 lary with a low-dimensional real-value vector [ 13 ]. Then, words with similar con-texts are mapped to close vector space. Although existing word embedding, such as Word2Vec[ 10 ]andC&amp;W[ 5 ] models, has yielded the state-of-the-art results in many natural language processing (NLP) tasks, current research does not show their effectiveness on query classification. Another problem for word embedding is that it models the syntactic contexts of words [ 17 ], but ignores the topic cate-gory information of queries. This results in misclassifying the queries into topic categories such that the categories lose their discriminative ability. roach to address the following problems as: (1) mapping short and ambiguous queries into right categories; (2) adopting explicit/implicit category information, especially the topic information of categories. The main contribution is that four neural network language models, namely as TOWE e , TOWE and TOWE iu , are presented based on the Word2Vec model. Here TOWE TOWE eu encode explicit category information of categorised queries into word embedding as a supervised learning. TOWE i and TOWE iu incorporate user click-through information. Finally, we evaluate the proposed approach on two datasets which are widely published by two famous commercial search engine companies. The experimental results show that our approach is promising and outperforms Word2Vec[ 10 ].
 the related work on query classification , word embeddings and its applications. Then, we present our methodologies in Section 3, followed by the experimental design in Section 4. After that, we show our experimental results in Section 5 and discuss them in Section 6. Finally, we draw the conclusions and future work in Section 7. Here we mainly review the related studies on query classification, word embed-dings and its applications. 2.1 Query Classification Mapping web-user posted queries to a predefined taxonomy with a reasonable degree of accuracy is the heart of search engine and also particularly challenging. Since web-user queries are typically short, they yield few keywords features per query based on traditional bag-of-words representations.
 efforts are devoted to enrich query features. The wining solution[ 14 ] of the 2005 KDD Cup associates a set of web pages by sending queries to search engines. Then, the titles of Web pages, snippets and their contents are used as features to build classifiers based on document taxonomy.
 However, with the coming of big data era, especially in the search engine domain, building a query classification system using queries expansion approaches are com-putationally infeasible, since fetching external text from large quantities of data is expected to be very time consuming and heavily depends on the quality of search engines.
 To address the above problem, Broder et al.[ 4 ] propose to classify all of the web pages and then group the queries by voting methods using those pre-classified web pages. There is still a big problem which is to classify all of web pages in the search engine, since the huge number of web pages. If the predefined taxonomy has been changed, re-classify all of the web pages is required. There are situations that queries are typically short, but terms in the vocab-ulary are extremely large. Therefore, large corpus of labeled training data is required for a query classification system[ 6 ]. But categorized queries are limited. Many studies focus on leveraging unlabeled queries to improve query classifica-tion performance using semi-supervised learning algorithm.
 Beitzel et al.[ 1 ] exploit both labeled and unlabeled queries using several classification approaches. They emphasize on an application of computational linguistics, named selectional preference, to automatically generate some associ-ation rules. Then using those generated rules to label large numbers of training data from the vast amount of unlabeled web query logs. Li et al.[ 9 ] propose two semisupervised learning methods to infer the class memberships of unlabeled queries using click-through data. 2.2 Word Embeddings How to represent text is central to many text classification tasks and determines the performance of tasks.
 Bag-of-words representation is the most common and popular fixed-length vector representation for texts owing to its simplicity and efficiency. However, bag-of-words representation treats each term in vocabulary as an atomic unit. Such representation suffers from the problem of high dimensionality and sparsity. It considers very little in the semantic connection between words. The problem is more acute in the query classification task. To overcome this shortcoming, numerous studies have been done to learn other word rep-resentations, such as Latent Semantic Analysis (LSA)[ 18 ] and Latent Dirichlet Allocation (LDA)[ 3 ].
 With the revival of deep learning, word embedding, also referred as continu-ous distributed word representations, has been proved to be invaluable resource for many NLP tasks.
 Word embedding is introduced by Hinton[ 7 ] to solve the problems of high dimensionality and sparsity in their work. Bengio et al.[ 2 ] propose a feed-forward neural network language model to predict the next word based on its previous contextual words. But the time complexity is very high.
 language mode. Collobert and Weston[ 5 ] propose a ranking-type word embed-ding learning algorithm (C&amp;W). Mikolov et al.[ 11 ] introduce the Recurrent neu-ral network language models (RNNLMs). In 2013, Mikolov[ 10 ] et al. propose Word2Vec to learn word embedding.
 Chinese word segmentation[ 16 , 19 ], POS tagging[ 5 ], sentiment analysis[ 15 ], name entity recognition[ 5 ]etc.
 feature embedding instead of human crafted feature and achieved the state-of-the-art result. Sun et al.[ 16 ] propose Radical-enhaced model based on C&amp;W model to learn enhanced word embedding by exploiting Chinese word radical information and utilize neural-CRF to the Chinese word segmentation task. ing the explicit topic information of categorized queries and implicit topic infor-mation of the very large click-through data. We represent queries through topic-oriented word embedding. Then, the query embedding is treated as features for the query classification task. In this section, the details of learning topic-oriented word embedding (TOWE) is proposed for web user query classification. Four neural networks based on Word2vec 1 are proposed to learn TOWE. In the following sections, we describe the Word2Vec model firstly and then present the details of the four proposed methods. 3.1 Word2Vec Model The goal of Word2Vec is to associate each term in the vocabulary w with a unique d dimensional real value vector v w  X  R d . Words with similar syntactic context are assigned to close vector space. We use the query  X  X pple retina macbook air 13 X  as the demonstration.
 tic context c w  X  the set of words in the window of size ws centered at w ( w excluded). For ws = 2 , the syntactic context of w is c w In our example the context of macbook are apple , retina , air , 13 . And the detail architecture of Word2Vec is shown in Figure 1 (a). In this framework, every word in the syntactic context w i  X  c w is mapped to a unique vector. And the sum of the vectors is used as features for prediction Huffman code H w . The negative maximum log likelihood loss function of softmax layer is Where d j is the j th Huffman code H w value. And f w 2 v ability of d j = 0, which is calculated as given Equation ( 2 ). Where v of vectors in context c w which is calculated as v c w = w  X  c parameter of huffman tree node. g ( x )= 1 1+ e  X  x is the sigmoid function. 3.2 Topic-Oriented Word Embedding In this section, we incorporate the explicit or implicit topic information into the Word2Vec model to learn topic-oriented word embedding. We develop four neural networks with different strategies to integrate the topic information of queries.
 Model 1 ( TOWE e ). The Word2Vec model does not capture the explicit topic category information of categorized queries. For this reason, explicit topic-oriented word embedding ( TOWE e ) is proposed to integrate topic category information by predicting the topic category distribution of text based on input ngram. We assume that if the query q belongs to topic category k , the syntactic context c w in the query q also belongs to topic category k . Therefore, we predict the topic category based on each categorized syntactic context.
 Assuming there are K topic categories. We modify the top softmax layer by predicting its topic distribution instead of the target word. The detail neural network framework ( TOWE e ) is shown in Figure 1 (b). Let td distribution of c w .If c w  X  q and the category of q is k then td td k = 0. Therefore the negative maximum log likelihood error of softmax layer is computed as: Where f t ( c w ) is the predicted topic distribution, and td distribution mentioned above.
 Model 2 ( TOWE eu ). The Word2Vec model learns word embedding by mod-eling syntactic contexts but disregard the topic category information of text. And TOWE e learn topic-oriented word embedding by exploiting explicit topic distribution of text but ignore the syntactic contexts of words. In this part, we develop a unified model ( TOWE eu ). The details neural network framework is shown in Figure 1 (c). And the loss function is computed as: Where Loss w 2 v ( w, c w ) is the loss of the context part, Loss loss of topic category part, and  X  is the linearly weights of the two parts. If context cw is labeled as k then  X  = 1, otherwise  X  =0.
 Model 3 ( TOWE i ). Large corpus of categorized queries are not available for training TOWE e and TOWE eu in some case. In this section, implicit topic-oriented word embedding ( TOWE i ) is proposed to conquer this problem. TOWE topic information from large corpus of click-through data instead of categorized queries. Click-through data can be extracted easily form search log. Because the length of queries are variant, so we do not use entire queries as input. An assumption is made that if a user submit query q and click web page u in the corresponding search results, the syntactic context c w in query q is likely to click url u .In TOWE framework, the vector of syntactic context is used as features to predict the Huffman code of clicked URL. The detail of neural network framework architecture is shown in Figure 1 (d). And the loss function is similar to Word2Vec, and is calculated as: Where H url is the Huffman code of clicked URL, d j is the j th value of H f j ( c w ) is the predict value of d j which is calculated same as Equation ( 2 ). Model 4 ( TOWE iu ). Model 3 TOWE i learns word embedding by modeling implicit topic relevance of click-through data but ignore the syntactic context of queries. So unified implicit topic-oriented word embedding ( TOWE posed by linear combination. The detail of neural network architecture is shown in Figure 1 (e). The loss function is: Loss Where  X  is the linear weight of two loss parts. And  X  =1if c  X  =0. 3.3 Query Embedding We apply TOWE for query classification under a supervised learning framework. Instead of hand-crafting feature and feature enrichment, we represent queries based on word embedding. Owing to the short length and simple structure of queries, we represent a query by considering it as the sum of all words with ignoring word orders. This can be expressed by the following equation: Where v q is a the embedding of query q ,and v w is the embeddings of word in query q . We use this d dimension vector as features of query to predict its topic category. And linear SVM in Scikit-learn[ 12 ] is employed to build classifier. To make a comprehensive comparison between our proposed TOWE models and the Word2Vec model, we utilize two real-word query datasets to evaluate the effectiveness of TOWE provided by Baidu 2 and Sogou 3 popular Chinese search engines. Same statistical information of Baidu and Sogou datasets is given in Table 1 .
 In Baidu dataset, we randomly sample 80% of categorized queries as training data and the remained 20% as test data. And exact category names are not pro-vided in Baidu dataset. In Sogou dataset, predefined taxonomy and categorized queries are not available. So we manually label some queries which randomly select from query log for training and testing. At the same time, we also cre-ate a large corpus of categorized queries using rule based classifier for training TOWE e and TOWE eu . Taxonomy and statistical information of Sogou dataset is shown in Table 2 .
 In this paper, we apply the standard precision, recall and F1 measure as eval-uation metrics. And the open source toolkit Word2Vec 4 and four our proposed neural networks is used to train word embeddings. The same parameter settings are used for Word2Vec and TOWE models. The context window size is 5; the learning rate is set as 0.05. We present the experimental results in Table 3 : (1) Bag-of-Word is the Tradi-tional text representation; (2) Word2Vec, as a stat-of-the-art word embedding learning algorithm, is the baseline; (3) TOWE e is one of our proposed algorithms for learning word embedding by modeling explicit topic of labeled queries; (4) TOWE eu is another proposed word embedding learning algorithm, which learns both syntactic context information and explicit topic; (5) TOWE embedding by mimic user search behavior using large corpus of click-through data; (6) TOWE iu learns word embedding by modeling user search behavior and syntactic contexts of user submitted queries. (7)  X +label X  means that only categorized queries are used to train word embedding. 6.1 TOWE VS Word2Vec We compare topic-oriented word embedding ( TOWE e , TOWE eu TOWE iu ) with the baseline of Word2Vec by only using query embedding as features for query classification. We are comparable to Word2Vec model as it has achieved great success in many NLP tasks. And the embeddings of Word2Vec and TOWE are trained with same datasets and same parameters.
 Table 3 shows the performance of query classification on the Baidu and Sogou datasets. From Table 3 , we can see that the performance of TOWE is obviously better than Word2Vec as feature for query classification. The reason is that Word2Vec do not capture the topic information, resulting in that the words with different topics are mapped to neighboring word vectors space. The classifi-cation performance is affected since the discriminative ability of topic words are weakened when such word embeddings are fed as features. TOWE effectively separate words with different topic category to different ends of the spectrum and perform better compare with Word2Vec model in both datasets. TOWE i and TOWE iu outperform Word2Vec model by exploiting more user click information from large corpus of click-through data. 6.2 Word Embedding VS Bag-of-Word We compare word embedding with the traditional bag-of-word query represen-tation for query classification. Classification performance is showed in Table 3 . We can see that the performance of TOWE is obviously outperform bag-of-word and Word2Vec representation using same svm classifier with same parameters. Word2Vec model perform slightly worse than bag-of-word in Baidu dataset and better in Sogou dataset. TOWE capture the syntactic information of text but also the explicit topic and implicit topic information from categorized queries or large number of user-click-through data. 6.3 Effect of  X  in TOWE eu We tune the hyper-parameter  X  of TOWE eu model. As given in Equation ( 4 ),  X  is the weighing score of syntactic context loss part and explicit topic category loss part.
 Figure 2(a) shows the precision of TOWE eu on query topic category classi-fication on Baidu and Sogou datasets. The performance of TOWE when  X  in the range of [0 . 3 , 0 . 8]. The TOWE ue model with  X  = 1 stands for the Word2Vec model, which learns word embedding by modeling syntactic context of queries. The importance of topic information in learning word embedding for query classification can be verified by the sharp decline at  X  =1. 6.4 Effect of  X  in TOWE iu We also tune the hyper-parameter  X  of TOWE iu to learn TOWE for query classification. As gave in Equation ( 6 ),  X  is the weighing score of syntactic loss implicit topic loss.
 different  X  on Sogou test sets. We can see that TOWE iu performs better when  X  is in the range of [0 . 3 , 0 . 5]. The model with  X  = 1 stands for the standard Word2Vec model. The sharp decline at  X  = 1 reflects the importance of implicit topic relevant information in learning word embeddings for query classification. We draw our conclusions as follows. First of all, we propose a topic oriented word embedding approach, configuring with four neural network strategies. Sec-ond, the features we learn for query classification is under a supervised learning framework, which makes the proposed model duplicable in other data sets. Third, compared to the traditional word embedding, we fully adopt the implicit and explicit topic information. Fourth, we suggest the parameters for the proposed TOWE model. Finally, our experiments confirm that TOWE with four strategies is successful on both data sets. Furthermore, we achieve the high performance as 95.73% in terms of Precision, 97.79% in terms of F1.
 mation such advertiser information, URL category. And study how to composi-tion more effective query embedding.

