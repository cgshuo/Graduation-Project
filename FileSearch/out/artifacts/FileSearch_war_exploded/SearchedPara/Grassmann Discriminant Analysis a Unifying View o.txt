 Jihun Hamm jhham@seas.upenn.edu Daniel D. Lee ddlee@seas.upenn.edu We often encounter learning problems in which the ba-sic elements of the data are sets of vectors instead of vectors. Suppose we want to recognize a person from multiple pictures of the individual, taken from differ-ent angles, under different illumination or at different places. When comparing such sets of image vectors, we are free to define the similarity between sets based on the similarity between image vectors (Shakhnarovich et al., 2002; Kondor &amp; Jebara, 2003; Zhou &amp; Chel-lappa, 2006).
 In this paper, we specifically focus on those data that can be modeled as a collection of linear subspaces. In the example above, let X  X  assume that the set of images of a single person is well approximated by a low di-mensional subspace (Turk &amp; Pentland, 1991), and the whole data is the collection of such subspaces. The benefits of using subspaces are two-fold: 1) compar-ing two subspaces is cheaper than comparing two sets directly when those sets are very large, and 2) it is more robust to missing data since the subspace can  X  X ill-in X  the missing pictures. However the advantages come with the challenge of representing and handling the subspaces appropriately.
 We approach the subspace-based learning problems by formulating the problems on the Grassmann manifold, the set of fixed-dimensional linear subspaces of a Eu-clidean space. With this unifying framework we can make analytic comparisons of the various distances of subspaces. In particular, we single out those distances that are induced from the Grassmann kernels , which are positive definite kernel functions on the Grassmann space. The Grassmann kernels allow us to use the usual kernel-based algorithms on this unconventional space and to avoid ad hoc approaches to the problem. We demonstrate the proposed framework by using the Projection metric and the Binet-Cauchy metric and by applying kernel Linear Discriminant Analysis to clas-sification problems with real image databases. 1.1. Contributions of the Paper Although the Projection metric and the Binet-Cauchy metric were previously used (Chang et al., 2006; Wolf &amp; Shashua, 2003), their potential for subspace-based learning has not been fully explored. In this work, we provide an analytic exposition of the two metrics as examples of the Grassmann kernels, and contrast the two metrics with other metrics used in the literature. Several subspace-based classification methods have been previously proposed (Yamaguchi et al., 1998; Sakano, 2000; Fukui &amp; Yamaguchi, 2003; Kim et al., 2007). However, these methods adopt an inconsistent strategy: feature extraction is performed in the Eu-clidean space when non-Euclidean distances are used. This inconsistency can result in complications and weak guarantees. In our approach, the feature ex-traction and the distance measurement are integrated around the Grassmann kernel, resulting in a simpler and better-understood formulation.
 The rest of the paper is organized as follows. In Sec. 2 and 3 we introduce the Grassmann manifolds and de-rive various distances on the space. In Sec. 4 we present a kernel view of the problem and emphasize the advantages of using positive definite metrics. In Sec. 5 we propose the Grassmann Discriminant Analysis and compare it with other subspace-based discrimination methods. In Sec. 6 we test the proposed algorithm for face recognition and object categorization tasks. We conclude in Sec. 7 with a discussion. In this section we briefly review the Grassmann man-ifold and the principal angles.
 Definition 1 The Grassmann manifold G ( m, D ) is the set of m -dimensional linear subspaces of the R D . The G ( m, D ) is a m ( D  X  m )-dimensional compact Rie-mannian manifold. 1 An element of G ( m, D ) can be represented by an orthonormal matrix Y of size D by m such that Y 0 Y = I m , where I m is the m by m iden-tity matrix. For example, Y can be the m basis vectors of a set of pictures in R D . However, the matrix rep-resentation of a point in G ( m, D ) is not unique: two matrices Y 1 and Y 2 are considered the same if and only if span( Y 1 ) = span( Y 2 ), where span( Y ) denotes the subspace spanned by the column vectors of Y . Equiva-lently, span( Y 1 ) = span( Y 2 ) if and only if Y 1 R 1 = Y for some R 1 , R 2  X  X  ( m ). With this understanding, we will often use the notation Y when we actually mean its equivalence class span( Y ), and use Y 1 = Y 2 when we mean span( Y 1 ) = span( Y 2 ), for simplicity. Formally, the Riemannian distance between two sub-spaces is the length of the shortest geodesic connecting the two points on the Grassmann manifold. However, there is a more intuitive and computationally efficient way of defining the distances using the principal angles (Golub &amp; Loan, 1996).
 Definition 2 Let Y 1 and Y 2 be two orthonormal matrices of size D by m . The principal an-gles 0  X   X  1  X  X  X  X  X  X   X  m  X   X / 2 between two subspaces span( Y 1 ) and span( Y 2 ) , are defined recursively by cos  X  k = max In other words, the first principal angle  X  1 is the small-est angle between all pairs of unit vectors in the first and the second subspaces. The rest of the principal angles are similarly defined. It is known (Wong, 1967; Edelman et al., 1999) that the principal angles are re-lated to the geodesic distance by d 2 G ( Y 1 , Y 2 ) = P (refer to Fig. 1.) The principal angles can be computed from the Singu-lar Value Decomposition (SVD) of Y 0 1 Y 2 , where U = [ u 1 ... u m ], V = [ v 1 ... v m ], and cos  X  is the diagonal matrix cos  X  = diag(cos  X  1 ... cos  X  m ). The cosines of the principal angles cos  X  1 , ... , cos  X  are also known as canonical correlations .
 Although the definition can be extended to the cases where Y 1 and Y 2 have different number of columns, we will assume Y 1 and Y 2 have the same size D by m throughout this paper. Also, we will occasionally use G instead of G ( m, D ) for simplicity. In this paper we use the term distance as any assign-ment of nonnegative values for each pair of points in a space X . A valid metric is, however, a distance that satisfies the additional axioms: Definition 3 A real-valued function d : X X X X  R is called a metric if 1. d ( x 1 , x 2 )  X  0 , 2. d ( x 1 , x 2 ) = 0 if and only if x 1 = x 2 , 3. d ( x 1 , x 2 ) = d ( x 2 , x 1 ) , 4. d ( x 1 , x 2 ) + d ( x 2 , x 3 )  X  d ( x 1 , x 3 ) , for all x 1 , x 2 , x 3  X  X  .
 A distance (or a metric) between subspaces d ( Y 1 , Y 2 ) has to be invariant under different representations In this section we introduce various distances for sub-spaces derivable from the principal angles. 3.1. Projection Metric and Binet-Cauchy We first underline two main distances of this paper. 1. Projection metric . 2. Binet-Cauchy metric As the names hint, these two distances are in fact valid metrics satisfying Def. 3. The proofs are deferred until Sec. 4. 3.2. Other Distances in the Literature We describe a few other distances used in the liter-ature. The principal angles are the keys that relate these distances. 1. Max Correlation 2. Min Correlation 3. Procrustes metric 3.3. Which Distance to Use? The choice of the best distance for a classification task depends on a few factors. The first factor is the dis-tribution of data. Since the distances are defined with particular combinations of the principal angles, the best distance depends highly on the probability dis-tribution of the principal angles of the given data. For example, d Max uses the smallest principal angle  X  1 only, and may be robust when the data are noisy. On the other hand, when all subspaces are sharply concen-trated on one point, d Max will be close to zero for most of the data. In this case, d Min may be more discrimi-native. The Projection metric d P , which uses all the principal angles, will show intermediate characteristics between the two distances. Similar arguments can be made for the Procrustes metrics d CF and d C 2 , which use all angles and the largest angle only, respectively. The second criterion for choosing the distance, is the degree of structure in the distance. Without any struc-ture a distance can be used only with a simple K-Nearest Neighbor (K-NN) algorithm for classification. When a distance have an extra structure such as tri-angle inequality, for example, we can speed up the nearest neighbor searches by estimating lower and up-per limits of unknown distances (Farag  X o et al., 1993). From this point of view, the max correlation is not a metric and may not be used with more sophisticated algorithms. On the other hand, the Min Correlation and the Procrustes metrics are valid metrics 2 . The most structured metrics are those which are in-duced from a positive definite kernel. Among the met-rics mentioned so far, only the Projection metric and the Binet-Cauchy metric belong to this class. The proof and the consequences of positive definiteness are the main topics of the next section. We have defined a valid metric on Grassmann mani-folds. The next question is whether we can define a kernel function compatible with the metric. For this purpose let X  X  recall a few definitions. Let X be any set, and k : X X X X  R be a symmetric real-valued function k ( x i , x j ) = k ( x j , x i ) for all x i , x Definition 4 A real symmetric function is a (resp. conditionally) positive definite kernel function, if P i,j c i c j k ( x i , x j )  X  0 , for all x 1 , ..., x n ( x c , ..., c n ( c i  X  R ) for any n  X  N . (resp. for all c , ..., c n ( c i  X  R ) such that P n i =1 c i = 0 .) In this paper we are interested in the kernel functions on the Grassmann space.
 Definition 5 A Grassmann kernel function is a pos-itive definite kernel function on G .
 In the following we show that the Projection metric and the Binet-Cauchy are induced from the Grass-mann kernels. 4.1. Projection Metric The Projection metric can be understood by associ-ating a point span( Y )  X  X  with its projection matrix Y Y 0 by an embedding: The image  X  P ( G ( m, D )) is the set of rank-m or-thogonal projection matrices. This map is in fact an isometric embedding (Chikuse, 2003) and the projection metric is simply a Euclidean distance in R
D  X  D . The corresponding innerproduct of the space Proposition 1 The Projection kernel is a Grassmann kernel.
 Proof The kernel is well-defined because k P ( Y 1 , Y 2 ) = k ( Y 1 R 1 , Y 2 R 2 ) for any R 1 , R 2  X  X  ( m ). The positive definiteness follows from the properties of the Frobe-nius norm. For all Y 1 , ..., Y n ( Y i  X  X  ) and c 1 , ..., c R ) for any n  X  N , we have We can generate a family of kernels from the Projec-tion kernel. For example, the square-root k Y 0 i Y j k F also a positive definite kernel.
 4.2. Binet-Cauchy Metric The Binet-Cauchy metric can also be understood from an embedding. Let s be a subset of { 1 , ..., D } with m elements s = { r 1 , ..., r m } , and Y ( s ) be the m  X  m matrix whose rows are the r 1 , ... , r m -th rows of Y . If s , s 2 , ..., s n are all such choices of the subset s ordered lexicographically, then the Binet-Cauchy embedding is defined as  X 
BC : G ( m, D )  X  R n , Y 7 X  det Y ( s 1 ) , ..., det Y where n = D C m is the number of choosing m rows out of D rows. The natural innerproduct in this case is P Proposition 2 The Binet-Cauchy kernel is a Grassmann kernel.
 Proof First, the kernel is well-defined because k
BC ( Y 1 , Y 2 ) = k BC ( Y 1 R 1 , Y 2 R 2 ) for any R 1 O ( m ). To show that k BC is positive definite it suffices From the Binet-Cauchy identity, we have Therefore, for all Y 1 , ..., Y n ( Y i  X  X  ) and c 1 , ..., c R ) for any n  X  N , we have X We can also generate another family of kernels from the Binet-Cauchy kernel. Note that although det Y 0 1 Y 2 is a Grassmann kernel we prefer using k to principal angles det( Y 0 1 Y 2 ) 2 = Q cos 2  X  i , whereas det Y 0 1 Y 2 6 = Q cos  X  i in general. 3 Another variant arcsin k BC ( Y 1 , Y 2 ) is also a positive definite kernel and its induced metric d = (arccos(det Y 0 1 Y 2 )) 1 / 2 a conditionally positive definite metric. 4.3. Indefinite Kernels from Other Metrics Since the Projection metric and the Binet-Cauchy metric are derived from positive definite kernels, all the kernel-based algorithms for Hilbert spaces are at our disposal. In contrast, other metrics in the previ-ous sections are not associated with any Grassmann kernel. To show this we can use the following result (Schoenberg, 1938; Hein et al., 2005): Proposition 3 A metric d is induced from a positive definite kernel if and only if is conditionally positive definite.
 The proposition allows us to show a metric X  X  non-positive definiteness by constructing an indefinite ker-nel matrix from (11) as a counterexample.
 There have been efforts to use indefinite kernels for learning (Ong et al., 2004; Haasdonk, 2005), and sev-eral heuristics have been proposed to make an in-definite kernel matrix to a positive definite matrix (Pekalska et al., 2002). However, we do not advocate the use of the heuristics since they change the geome-try of the original data. In this section we give an example of the Discriminant Analysis on Grassmann space by using kernel LDA with the Grassmann kernels. 5.1. Linear Discriminant Analysis The Linear Discriminant Analysis (LDA) (Fukunaga, 1990), followed by a K-NN classifier, has been success-fully used for classification.
 Let { x 1 , ..., x N } be the data vectors and { y 1 , ..., y be the class labels y i  X  X  1 , ..., C } . Without loss of generality we assume the data are ordered according to the class labels: 1 = y 1  X  y 2  X  ...  X  y N = C . Each class c has N c number of samples.
  X  = 1 /N P i x i be the overall mean. LDA searches for the discriminant direction w which maximizes the Rayleigh quotient L ( w ) = w 0 S b w / w 0 S w w where S and S w are the between-class and within-class covari-ance matrices respectively: The optimal w is obtained from the largest eigenvec-C  X  1-number of local optima W = { w 1 , ..., w C  X  1 } . By projecting data onto the space spanned by W , we achieve dimensionality reduction and feature extrac-tion of data onto the most discriminant subspace. 5.2. Kernel LDA with Grassmann Kernels Kernel LDA can be formulated by using the kernel trick as follows. Let  X  : G X  X  be the feature map, and  X  = [  X  1 ...  X  N ] be the feature matrix of the train-ing points. Assuming w is a linear combination of the those feature vectors, w =  X   X  , we can rewrite the Rayleigh quotient in terms of  X  as
L (  X  ) = where K is the kernel matrix, 1 N is a uniform vector [1 ... 1] 0 of length N , V is a block-diagonal matrix whose c -th block is the uniform matrix 1 N c 1 0 N and  X  2 I N is a regularizer for making the computation stable. Similarly to LDA, the set of optimal  X   X  X  are computed from the eigenvectors.
 The procedures for using kernel LDA with the Grass-mann kernels are summarized below:
Assume the D by m orthonormal bases { Y i } are already computed from the SVD of sets in the data.
Training: 1. Compute the matrix [ K train ] ij = k P ( Y i , Y j ) or 2. Solve max  X  L (  X  ) by eigen-decomposition. 3. Compute the ( C  X  1) -dimensional coefficients
Testing: 1. Compute the matrix [ K test ] ij = k P ( Y i , Y j ) or 2. Compute the ( C  X  1) -dim coefficients F test = 3. Perform 1-NN classification from the Eu-Another way of applying LDA to subspaces is to use the Projection embedding  X  P (7) or the Binet-Cauchy embedding  X  BC (9) directly. A subspace is repre-sented by a D by D matrix in the former, or by a vector of length n = D C m in the latter. However, us-ing these embeddings to compute S b or S w is a waste of computation and storage resources when D is large. 5.3. Other Subspace-Based Algorithms 5.3.1. Mutual Subspace Method (MSM) The original MSM (Yamaguchi et al., 1998) performs simple 1-NN classification with d Max with no feature extraction. The method can be extended to any dis-tance described in the paper. There are attempts to use kernels for MSM (Sakano, 2000). However, the kernel is used only to represent data in the original space, and the algorithm is still a 1-NN classification. 5.3.2. Constrained MSM Constrained MSM (Fukui &amp; Yamaguchi, 2003) is a technique that applies dimensionality reduction to bases of the subspaces in the original space. Let G = P i Y i Y 0 i be the sum of the projection matrices and { v 1 , ..., v D } be the eigenvectors corresponding to the eigenvalues {  X  1  X  ...  X   X  D } of G. The authors claim that the first few eigenvectors v 1 , ..., v d of G are more discriminative than the later eigenvectors, and they suggest projecting the basis vectors of each sub-space Y 1 onto the span( v 1 , ..., v l ), followed by normal-ization and orthonormalization. However these proce-dure lack justifications, as well as a clear criterion for choosing the dimension d , on which the result crucially depends from our experience. 5.3.3. Discriminant Analysis of Canonical DCC (Kim et al., 2007) can be understood as a non-parametric version of linear discrimination analysis us-ing the Procrustes metric (6). The algorithm finds the discriminating direction w which maximize the ratio L ( w ) = w 0 S B w / w 0 S w w , where S b and S w are the nonparametric between-class and within-class  X  X ovari-ance X  matrices: where U and V are from (1). Recall that tr( Y i U  X  Y V )( Y i U  X  Y j V ) 0 = k Y i U  X  Y j V k 2 F is the squared Procrustes metric. However, unlike our method, S b and S w do not admit a geometric interpretation as true covariance matrices, and cannot be kernelized ei-ther. A main disadvantage of the DCC is that the algorithm iterates the two stages of 1) maximizing the ratio L ( w ) and of 2) computing S b and S w , which results in computational overheads and more parame-ters to be determined. This reflects the complication of treating the problem in a Euclidean space with a non-Euclidean distance. In this section we test the Grassmann Discriminant Analysis for 1) a face recognition task and 2) an object categorization task with real image databases. 6.1. Algorithms We use the following six methods for feature extraction together with an 1-NN classifier. 1) GDA1 (with Projection kernel), 2) GDA2 (with Binet-Cauchy kernel), 3) Min dist , 4) MSM, 5) cMSM, and 6) DCC.
 For GDA1 and GDA2, the optimal values of  X  are found by scanning through a range of val-ues. The results do not seem to vary much as long as  X  is small enough. The Min dist is a simple pairwise distance which is not subspace-based. If Y i and Y j are two sets of basis vectors: Y i = { y i 1 , ..., y im i } and Y j = { y j 1 , ..., y jm d DCC, the optimal dimension l is found by exhaus-tive searching. For DCC, we have used two nearest-neighbors for B i and W i in Sec. 5.3.3. Since the S w and S b are likely to be rank deficient, we first reduced the dimension of the data to N  X  C using PCA as recommended. Each optimization is iterated 5 times. 6.2. Testing Illumination-Invariance with Yale The Yale face database and the Extended Yale face database (Georghiades et al., 2001) together consist of pictures of 38 subjects with 9 different poses and 45 dif-ferent lighting conditions. Face regions were cropped from the original pictures, resized to 24  X  21 pixels ( D = 504), and normalized to have the same variance. For each subject and each pose, we model the illumi-nation variations by a subspace of the size m = 1 , ..., 5, spanned by the 1 to 5 largest eigenvectors from SVD. We evaluate the recognition rate of subjects with nine-fold cross validation, holding out one pose of all sub-jects from the training set and using it for test. The recognition rates are shown in Fig. 2. The GDA1 outperforms the other methods consistently. The GDA2 also performs well for small m , but performs worse as m becomes large. The rates of the others also seem to decrease as m increases. An interpreta-tion of the observation is that the first few eigenvec-tors from the data already have enough information and the smaller eigenvectors are spurious for discrim-inating the subjects. 6.3. Testing Pose-Invariance with ETH-80 The ETH-80 (Leibe &amp; Schiele, 2003) database con-sists of pictures of 8 object categories ( X  X pple X ,  X  X ear X ,  X  X omato X ,  X  X ow X ,  X  X og X ,  X  X orse X ,  X  X up X ,  X  X ar X ). Each cat-egory has 10 objects that belong to the category, and each object is recorded under 41 different poses. Im-ages were resized to 32  X  32 pixels ( D = 1024) and normalized to have the same variance. For each cate-gory and each object, we model the pose variations by a subspace of the size m = 1 , ..., 5, spanned by the 1 to 5 largest eigenvectors from SVD. We evaluate the classification rate of the categories with ten-fold cross validation, holding out one object instance of each cat-egory from the training set and using it for test. The recognition rates are also summarized in Fig. 2. The GDA1 also outperforms the other methods most of the time, but the cMSM performs better than GDA2 as m increases. The rates seem to peak around m = 4 and then decrease as m increases. This results is consistent with the observation that the eigenvalues from this database decrease more gradually than the eigenvalues from the Yale face database. In this paper we have proposed a Grassmann frame-work for problem in which data consist of subspaces. By using the Projection metric and the Binet-Cauchy metric, which are derived from the Grassmann ker-nels, we were able to apply kernel methods such as kernel LDA to subspace data. In addition to having theoretically sound grounds, the proposed method also outperformed state-of-the-art methods in two experi-ments with real data. As a future work, we are pur-suing a better understanding of probabilistic distribu-tions on the Grassmann manifold.

