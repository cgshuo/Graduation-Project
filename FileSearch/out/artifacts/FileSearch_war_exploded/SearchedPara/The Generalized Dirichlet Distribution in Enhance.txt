 We present a new, robust and computationally efficient Hier-archical Bayesian model for effective topic correlation mod-eling. We model the prior distribution of topics by a Gen-eralized Dirichlet distribution (GD) rather than a Dirichlet distribution as in Latent Dirichlet Allocation (LDA). We de-fine this model as GD-LDA. This framework captures cor-relations between topics, as in the Correlated Topic Model (CTM) and Pachinko Allocation Model (PAM), and is faster to infer than CTM and PAM. GD-LDA is effective to avoid over-fitting as the number of topics is increased. As a tree model, it accommodates the most important set of topics in the upper part of the tree based on their probability mass. Thus, GD-LDA provides the ability to choose significant topics effectively. To discover topic relationships, we per-form hyper-parameter estimation based on Monte Carlo EM Estimation. We provide results using Empirical Likelihood (EL) in 4 public datasets from TREC and NIPS. Then, we present the performance of GD-LDA in ad hoc information retrieval (IR) based on MAP, P@10, and Discounted Gain. We discuss an empirical comparison of the fitting time. We demonstrate significant improvement over CTM, LDA, and PAM for EL estimation. For all the IR measures, GD-LDA shows higher performance than LDA, the dominant topic model in IR. All these improvements with a small increase in fitting time than LDA, as opposed to CTM and PAM. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Document Filtering ;G.3[ Probability and Statistics ]: Statistical Computing Algorithms, Experimentation Statistical Topic Modeling, Document Representation  X  Main contact.

Topic modeling has been widely studied in Machine Learn-ing and Text Mining as an effective approach to extract latent topics from unstructured text documents. The key idea underlying topic modeling is to use term co-occurrences in documents to discover associations between those terms. The development of Latent Dirichlet Allocation (LDA) [3, 8] enabled the rigorous prediction of new documents first time. Consequently, variants and extensions of LDA have been an active area of research in topic modeling. This research has 2 main streams in document representation: 1) The explo-ration of super and subtopics as in Pachinko Model Alloca-tion (PAM) [9, 10]; and 2) the correlation of topics [2, 9, 10]. However, despite the improvement of these approaches over LDA, they are rarely used in applications that handle large datasets, such as Information Retrieval. Major gaps in these models include: modeling correlated topics in a computa-tionally effective manner; and a robust approach to ensure good performance for a wide range of document numbers, vocabulary size and average word length per document. In this paper we develop a new model to meet these needs. We use the Generalized Dirichlet (GD) distribution as a prior distribution of the document topic mixtures, leading to GD-LDA. We show that the Dirichlet distribution is a special case of GD. As a result, GD-LDA is deemed to be a generalized case of LDA. Our goal is to provide a more flexible model for topics while retaining the conjugacy prop-erties, which are desirable in inference. The features of the GD-LDA model include: 1) An effective method to represent sparse topic correlations in natural language documents. 2) A model which handles global topic correlations with time complexity O ( KW ), adding minimal computational cost re-spect to LDA. This results in a fast and robust approach compared to CTM and PAM. 3) A hierarchical tree struc-ture that accommodates the most significant topics, based on probability mass, at the upper levels allowing us to reduce the number of topics efficiently. Note that GD is a special case of Dirichlet Trees [13, 5]. This distribution has been used previously in topic modeling to add domain knowledge to the probability of words given a topic [1]. In contrast, we use the GD distribution to model topic correlations. Thus, this approach is complementary to GD-LDA.

To validate our model, we use Empirical Likelihood (EL) in four data sets with different characteristics of document length, vocabulary size, and total number of documents. GD-LDA outperforms CTM, PAM and LDA consistently in all the datasets. In addition, we test the performance of GD-LDA in adhoc Information Retrieval, obtaining superior results to those in the literature. We show a significant dif-ference in running times between GD-LDA, CTM and PAM which makes GD-LDA as viable as LDA for large data sets.
This paper is organized as follows: In section 2, we present the definition and features of the GD distribution. We will use this definition to develop the methodology of the GD-LDA model. Section 3 depicts our proposed approach with the proper derivations. Validation criteria, experimental set-tings, and results are presented in section 4. Finally the discussion and conclusion are presented in section 5.
The Generalized Dirichlet (GD) distribution was intro-duced by Connor and Mosimann in [4]. The GD distribution is motivated by the limitations of the Dirichlet distribution in modeling covariances. In the case of the Dirichlet dis-tribution, all the entries of the random vector must share a common variance, and they must sum to one. When we use the Dirichlet distribution as a prior for the multinomial distribution we have only one degree of freedom, the total prior sample size, to incorporate our confidence in the prior knowledge. As a consequence, we can not add individual variance information for each entry of the random vector. In addition, all entries are always negatively correlated. In other words, if the probability of one entry increases each of the other probabilities must either decrease or remain the same to sum to one. Despite these limitations, the Dirich-let distribution is widely used given that this is a conjugate prior of the multinomial distribution.

The GD distribution allows us to sample each entry of the random vector of proportions from independent Beta distri-butions. This independence is the key property that pro-vides more flexibility than the Dirichlet distribution. For-mally this distribution is defined as: where  X  1 +  X  2 + ... +  X  K  X  1 +  X  K =1,  X  j =  X  j  X   X  j +1 for 1  X  j  X  K  X  2and  X  K  X  1 =  X  K  X  1  X  1.

To illustrate the properties of the GD distribution we de-fine Z 1 =  X  1 and Z k =  X  k /V k for k =2 , 3 ,...,K  X  1where V k =1  X   X  1  X  X  X  X  X  X   X  k  X  1 .Let T k bethediscreterandomvari-able with multinomial distribution with parameter  X  1 ... X  for K different categories. We start at node V 1 ,andatthis node we sample T 1 with probability Z 1 and V 2 with proba-bility 1  X  Z 1 . Conditional on V 2 ,wesample T 2 with prob-ability Z 2 and V 3 with probability 1  X  Z 2 . In the general case, conditional on V k ,wesample T k with probability Z and V k +1 with probability 1  X  Z k for k =1 ...K  X  1. If we now add a prior Beta distribution with parameters  X  k , X  k each conditional Binomial distribution of the nodes V k ,we have a GD distribution where this set of Beta distributions is conjugate to the set of Binomial distributions. where N is the total number of observations and N k is the number of observations remaining from previous categories in the tree[20].

The GD distribution is a special case of the Dirichlet Tree distribution [13, 5] where a cascade hierarchy is employed in the generative process of the distribution. To interpret the parameters of the GD distribution we refer to its tree representation shown in Fig 1. Conditional on V k ,  X  k is the sample size assigned to the discrete output T k ,and  X  k is the sample size assigned to the next level in the tree. If  X  k is too small compared to  X  k , we could discard the rest of the tree. This is a desirable property as it facilitates dimensionality reduction in the number of topics. By set-ting  X  k =  X  k +1 +  X  k +1 ,  X  K  X  1 =  X  K ,weobtainaDirichlet distribution. This is why GD distribution  X  X eneralizes X  the Dirichlet distribution [4]. Although a general Dirichlet Tree structure that is inferred by data is highly appealing, in practice this structure determines the number and proper-ties of the parameters to fit. Thus, the tree must be fixed before inferring the model [13, 14]. A key property of the GD cascade structure is that it facilitates topic reduction. This is because the tree can be pruned based on the conditional probability of going to the following level.

The GD distribution is a conjugate prior distribution to the Multinomial distribution, then we can integrate out the parameter of the Multinomial distribution leading to: where  X  k =  X  k + T k ,  X  k =  X  k + T k +1 + ... + T K . The ability to estimate this integral in closed form is crucial for GD-LDA model because this enables us to perform Gibbs sampling. This is the product of Beta-Binomial distributions for Z k independent random variables. As a result, this expression is factorized into independent ratios of Gamma functions , which allows us to find the Maximum Likelihood Estimation (MLE) of  X ,  X  in a simple manner (independently) which is not the case of the Dirichlet distribution.
In natural language documents, most of the times only a few topics co-occur leading to sparse topic correlations [19]. This implies that a very few random topics may suffice, rather than the full joint distribution including all the topics.
When considering useful distributions for topic modeling, a desirable feature is to model only a few topics in a doc-ument. For instance, the topic domestic politics could fre-quently co-occur with middle east politics and health care reform . This implies that if we observe domestic politics , the conditional probabilities of observing middle east politics and health care reform will increase. However, if a document contains domestic politics and middle east politics ,itcould very rarely contain health care reform .

GD has the computational advantage of only having 2 K  X  1 parameters. This also implies that the distribution covari-ances are constrained. The question is whether it is possible that these constraints imply that only a few topics co-occur Algorithm 1 GD-LDA Generative Model Figure 2: Graphical Model for GD-LDA.  X  and  X  are in a document. In this respect, the covariance properties de-scribed in Appendix A.1, particularly the third one, indicate that the covariances are dependent on the expected value of two topics, then several covariances might be close to zero if the right hand side ratio is small. This is an important feature of the GD distribution which we propose to exploit in topic modeling. A detailed discussion of the covariance constraints of GD distribution is provided in Appendix A.
In this section, we depict the parameter estimation pro-cess to fit the GD as a prior distribution for topics. We show that GD-LDA can be computed with a computational cost similar to that of LDA. Fig 2 shows the graphical model for GD-LDA and its generative model is described in Algo-rithm 1. We follow a Monte Carlo Expectation Maximiza-tion (MCEM) approach to fit our model. Conditional on the hyper-parameters, we develop a Gibbs sampling approach to infer the topic assignments to each word in the corpus. Then, assuming that the topic assignment expectations as given, we optimize the hyper-parameters of the model.
Let K be the number of topics, D the number of docu-ments, and V the vocabulary size. We use the indices: i to denote a word or term index in the vocabulary, k to denote a specific topic, j to refer to a document, and w to identify a specific observed word. N i,k defines the number of observed words which correspond to term i and have been assigned to topic k . N j,k is the frequency of topic k in document j . We refer to p ( w,z | X  ) as the joint probability of all the words w in the corpus and their topic assignments z .  X  represents avectorofsize V .  X  and  X  are vectors of size K  X  1.
We define the joint probability associated with the graph-ical model defined in Fig 2 with joint distribution: This expression allows us to decompose the problem into two hierarchical models that can be treated and optimized sep-arately based on these conditional probabilities. The prob-ability of words given topics is: For the probability of topics, we have a GD prior distribution for the topic mixtures in each document which is assumed to be Multinomial. Thus we have:
The topic assignments z are not observed. Then, we define a Gibbs sampling method to infer z as follows: Here, z  X  wj represents the topic assignments for all the words except the word w from document j . This analysis leads us to the following distributions for Gibbs sampling:
In LDA, the topic distribution depends on  X  k alone for the current topic k . Intuitively, if a new sample is assigned to a topic k in LDA, there is no effect on the sampling distri-bution of other topics. However, if a new sample is assigned to a topic k in GD-LDA, it will affect other topics through the evaluation of the product in Eq 9. The impact of this assignment will depend on the parameters  X ,  X  .

Sampling from the distribution in Eq 9 results in a high computational cost due to the cumulative product defined there. Since this distribution is not standard, we need to estimate its normalization constant. To perform this task with a computational cost comparable to collapsed Gibbs sampling in LDA [8], we compute the cumulative product for each k&gt; 1 iterations and pass it to the next iteration of the evaluation. This is illustrated in Algorithm 2. From this pseudo code, O ((2 K +1) W ) operations are required for each Gibbs sampling draw for the whole corpus, where W is the total number of words. Thus, the time complexity for each iteration is O ( KW ) as in the case of LDA [15]. This com-plexity is possible due to the cascade structure of the GD Algorithm 2 GD-LDA Gibbs Sampling distribution, which facilitates the calculation of the cumula-tive factor in Algorithm 2. In contrast, the time complexity of a Gibbs draw for PAM depends on the number of super-topics, S , and sub-topics K as O ( SKW ) [11]. The general recommendation is to set S = K/ 2 1 leading to O ( K 2 W ).
Previous research has assumed uniform priors for the topic mixtures  X  and the vocabulary distribution for topics  X  with-out estimating them (constant values for  X  and  X  ) [8]. Wal-lach et al. in [16] concludes that parameter estimation with asymmetric Dirichlet prior probability of topics provides an improvement in the fitting. In GD-LDA, we estimate the parameters  X ,  X  of the GD distribution to discover topic cor-relations. We estimate the parameters for the prior distri-bution of words given topics  X  . Ideally, we should maximize the likelihood p ( w |  X ,  X ,  X  ) for observations w and hyper-parameters  X ,  X ,  X  directly. Unfortunately, this distribution is intractable for this model. To solve this issue, we aug-ment the likelihood to p ( w,z |  X ,  X ,  X  ) and use Monte Carlo Expectation Maximization (MCEM) [18]. Conditional on hyper-parameters  X ,  X ,  X  , we use Gibbs sampling to esti-mate the posterior topic assignment distribution for each word (E-step). Then, given the expected topic assignments and words, we optimize p ( w,z |  X ,  X ,  X  ) (M-step). Algorithm 3 describes these iterations.
 To fit  X  , we maximize the joint distribution described in Eq 5 conditional on the expected topic assignments, z = E ( z |  X ,  X ,  X  ), estimated from Gibbs sampling. Then we have the following optimal function: where: N k,i = f ( z )
We follow the Newton-based approach proposed by Minka in Eqs 56-60 of [12]. Here, we have a DCM distribution to fit from K observed vectors of dimension V . To initialize the search, we use the method of moments based on the
Similarly, we estimate the parameters of the GD distribu-tion by maximizing the joint distribution:
K/ 2 super-topics is the recommendation by Mallet toolbox, http://mallet.cs.umass.edu/ Algorithm 3 Monte Carlo EM
We develop a Newton-based method in Appendix B. To initialize the search, we use the method of moments based on the conditional beta distributions from the tree represen-tation of the GD distribution [21]. The proportions p k,i N k,i / V i =1 N k,i are employed for this initialization.
One key component of this optimization is that each pair  X  , X  k is optimized separately. Thus, the time complexity for this optimization is linear in time with K . Parameter fitting in PAM is performed using the method of moments due the high model complexity and computational cost of the optimization [10, 11]. In addition, the number of parameters to fit for CTM and PAM is K 2 [2] [19], assuming K/ 2super-topics for PAM. As a result, these methods are highly prone to over-fitting, as many parameters are being fit.
Given the optimal parameters (  X   X  , X   X  ) obtained from Al-gorithm 3, and the word-topic observations ( w,z ) the pre-dictive distribution for document j ,  X   X  j , is estimated as: for the topics k =1 ...K and the documents j =1 ...D .
The predictive distribution for the probability of words given topics,  X   X  k is estimated as follows [8]: Notice that the probability of topics  X   X  j is document depen-dent. On the other hand, the probability of words given their topic  X   X  k is topic dependent. This implies that for an unseen document, we need to estimate its predictive distri-bution of topics while the probability of words given their topics remains the same.
The main challenge to validate statistical topic models is the lack of reliable observed topic labels for each word of a corpus. The standard approach is to estimate the likelihood of held-out data [17]. In addition, model performance in a Algorithm 4 EL Estimation for model M supervised task has also been used [22]. Here, we validate the GD-LDA performance addressing these two forms. We estimate the likelihood of completely unseen documents us-ing Empirical Likelihood, and we compare the performance of topic models in the ad hoc Information Retrieval as de-veloped in [19].
There has been a debate about the evaluation of topic models with methods ranging from the harmonic mean of complete likelihood for topic assignments, to perplexity and empirical likelihood. As discussed in [17], perplexity falls into the category of document completion where a portion each document must be observed to estimate the likelihood of the remaining content. Similarly, a  X  X eft to right X  evalua-tion has been proposed to estimate the probability of words in a test document incrementally [17]. For validation, we use Empirical Likelihood (EL) criterion. Our intent is to predict the likelihood of fully unseen documents. We do not use perplexity or  X  X eft to right X  evaluation because they are based on the word order inside the document, in contrast to  X  X ag of words X . In addition, EL has been shown to be a more pessimistic approximation for the probability of held-out documents than  X  X eft to right X  method [17].

We estimate EL as described in [11]. Here, we generate a set of pseudo documents,  X  s using the estimated prior topic distribution of the model being tested using training data. Then, a word distribution is estimated for each  X  s based on  X   X  . This is used to estimate the probability of seeing the test set. We define this process by means of Algorithm 4. Here, D t represents the number of documents for a test set, d t is the document t =1 ,...,D t , w t are the N t words of d t N s is the number of pseudo documents given model M .We use the generative approach based on the conditional Beta distributions of the tree representation for GD as in Fig 1. The main limitation of EL estimation is that the number of pseudo documents should be sufficiently large to cover the parameter space of  X  given the trained model. Then, N s determines the accuracy of the approximation.
Information Retrieval represents a hard problem where the gains of new models are often small or nil. This applica-tion represents a good test of the power of our approach. We compare the performance of different topic models in ad hoc Information Retrieval (IR). We use the approach proposed in [19] and incorporate our model by replacing the LDA model described in the method. Based on the predictive distribu-tion estimators for topics  X   X  , and for each document  X   X  provide a topic-based language model for each document, Table 1: Features of the datasets analyzed. Mean doc-Table 2: Number of Gibbs samples per EM iteration P TM ( w |  X  j ), as follows: This is augmented with the maximum likelihood estimate for the language model based on document terms D j , P ML ( w and for the language model based on the corpus C , P ML ( w leading to: where  X  is a smoothing parameter, N j is the number of words in document j ,and  X  is a parameter for the linear combination. Therefore, the ranking function for query Q with terms q is given by: We use the parameter values:  X  =1000,  X  =0 . 7. These are the values recommended in [19].
We use 4 different datasets to test our model. NIPS con-ference papers dataset 2 which contains long documents (8-10 pages). Two news datasets, NYT and APW, obtained from TREC-3. These collections contain shorter documents (1 page) with different vocabulary size. A fourth dataset, OHSUMED from TREC-9, consists of abstracts from med-ical papers. In contrast to the other datasets, the number of documents is much larger (more than ten times). Table 1 shows the features of these datasets. We remove standard stop words and perform stemming in all datasets.
We test 4 variants of the GD-LDA algorithm. In two of these cases we fix the value of  X  and optimize the parameters  X  and  X  as in Algorithm 3 based on two variants: 1) using the empirical expected topic assignments  X  z [18], GD-LDA Fixed Gamma Mean ; and 2) using the empirical mode z  X  [7] in the M-step, GD-LDA Fixed Gamma Max . In the other two cases, we optimize all the three parameters (  X  ,  X  and  X  ) based on the expected topic assignments, GD-LDA Mean , and the topic assignment mode GD-LDA Max .
 We compare GD-LDA, LDA with parameter optimization, CTM and PAM. For LDA, we use our own implementation. We follow the collapsed Gibbs sampling approach [8], with asymmetric Dirichlet prior distributions for both, the prob-ability of words given a topic, and for the probability of topics. These parameters are optimized as discussed by au-thors in [6]. For optimization, we follow the Newton-based approach proposed by Minka in Eqs 56-60 [12]. For both
Available at: http://cs.nyu.edu/~roweis/data.html Figure 3: Qualitative example of GD-LDA for the NYT GD-LDA and LDA, we set a maximum of 1 , 000 Newton iterations for parameter optimization. The schedule of the Gibbs sampling is detailed in Table 2. We initialize the val-ues  X  k =2 /K for k =1 ,...,K  X  1, and  X  K  X  1 =2 /K in GD-LDA to obtain the special case of Dirichlet distribution; we start with LDA as prior model, and allow the data to adapt the model in the MCEM iterations to the more general GD. For CTM 3 , we use the default settings with parameter estimation: a maximum of 1 , 000 EM iterations with con-vergence of 10  X  5 and maximum of 20 variational iterations with a convergence rate of 10  X  6 .ForPAM 4 we use 1000 Gibbs samples and K/ 2 super topics. This implementation supports multi-threading; to enable a fair comparison we use only one thread. We modify the implementation to obtain the fitted parameters since they are not provided by default. For EL estimation, we perform 10-fold cross-validation and we sample N s =10 , 000 pseudo documents.
Fig 3 shows a qualitative example of GD-LDA in the NYT dataset. The figure represents the empirically estimated probability of topics for a subset of the topics. Each box represents a topic and the terms displayed are the ones with highest posterior predictive probability  X   X  k . Given the fitted GD-LDA model, we calculate the point estimate of the to-tal probability for each topic (in blue) and its conditional probability given a parent node (red). Here, conditional on thecurrentnode,wemovedeeperintothetreeorobserve a word from the topic at that level. As we move down into the tree, the conditional probability of picking the left hand topic increases since the probability mass of the remaining topics is less. This is a desirable property which facilitates
We use the CTM implementation provided by Blei at http: //www.cs.princeton.edu/~blei/ctm-c/ .
We use the implementation provided by Mallet http:// mallet.cs.umass.edu/ .
 Figure 4: Correlation graph of a subset of topics inferred dimensionality reduction. If this probability is large com-pared with the probability of exploring further the tree, we can discard the remaining tree.

Fig 4 shows some of the positive and negative correlations between different topics inferred by GD-LDA. We observe that Health is positively correlated with Technology and Fi-nancial . Y2K is positively correlated with Technology and Arts but negatively correlated with Foreign Politics .Note that by default LDA assumes negative correlations, thus the main value of topic correlated models is to discover positive correlations. Fig 5(a) shows the decomposition of a sports document from the NYT dataset into topics using PAM with 10 super-topics and 20 sub-topics, CTM and GD-LDA with 20 topics. This shows that CTM and GD-LDA assign a high probability to a single topic (sports). On the other hand, PAM provides 3 sub-topics and most of the super-topics with a significant probability mass. This is not de-sirable since an important objective with topic modeling is to cluster documents based on the topic mixture. Fig 5(b) shows a comparison between CTM and GD-LDA for a docu-ment about the Y2K problem and airport functionality .We observe that GD-LDA provides better segmentation of the document based on the topics proportions and content.
In addition, we calculate the distribution of the number of significant topics in a document. We rank the topics based on their topic probability mass in each document. Then, we estimate the number of topics which accounts for 95% of this probability mass. Fig 6 shows the distribution of this number of topics inferred by LDA, CTM, PAM, and GD-LDA. For PAM, we consider the number of sub-topics. Here, we observe that CTM favors high number of topics for Figure 5: Qualitative comparison of the topic distribu-Figure 6: Distribution of the number of significant each documents which introduces noise when the goal is to characterize documents as in Information Retrieval. In PAM the number of topics per document is highly dependent of the number of super topics used. In order to handle topic correlation sparsity, PAM prunes the relationships between a super topic and subtopics [11]. This reduces the amount of correlations that can be modeled. In contrast, GD-LDA favors smaller number of topics per documents and without any constraint in the inference. The key property for this behavior is the cascade structure of the distribution. As dis-cussed in section 2.2, this validates empirically the intuition of few significant topics for natural language documents, and sparse correlations. Fig 7 shows EL estimations for the 4 variants of GD-LDA, CTM, PAM and LDA with asymmetric prior and parame-ter optimization in the four datasets. For the NIPS dataset, LDA is not shown since its predictive likelihood is extremely low. Similarly, PAM performance for the OHSUMED dataset is not shown for the same reason. The CTM model would not run for the OHSUMED dataset (mid-size dataset) based on the number of documents and vocabulary size.

We observe that optimizing  X  hasalowimpactasmore data becomes available. Conceptually, a prior distribution represents the prior knowledge with a given sample size. Consequently adding more data decreases the impact of the prior information. This is consistent with the conclusions discussed in [16]. We then compare the use of the topic assignments,  X  z (posterior mean), and the topic assignment mode, z  X  (maximum a posteriori MAP). We find that GD-LDA mean performs better, when the number of documents and the vocabulary size are relatively small (NIPS dataset). In contrast, GD-LDA Max shows the highest performance for the other datasets, Fig 7(b)-(d). In particular, this method performance is clearly superior for all the topics when tested for the largest dataset (OHSUMED). GD-LDA Mean requires fewer topics to achieve a superior performance compared to LDA, CTM and PAM. Moreover, EL decreases more smoothly than these methods. It also remains fairly constant when the number of topics attains a high value.
An important difference between the datasets analyzed is the average document length and number of unique terms. The worst performance of CTM, as well as PAM, is found in the case of APW dataset. This dataset has a larger vocab-ulary size, and significantly shorter documents than NIPS dataset, where both CTM and PAM show a similar perfor-mance to that of GD-LDA Mean . This suggests over-fitting by CTM where a full topic covariance matrix is estimated, and by PAM where a matrix of K subtopics by K/ 2super-topics is estimated. In addition, PAM uses the method of moments, instead of optimization, for parameter estimation. This is a limitation of PAM when EL based evaluation is performed since the fitted parameters do not optimize the likelihood. As discussed in section 4.3.1, CTM favors larger number of topics for each document than the other meth-ods. This behavior introduces noise in EL because correla-tions are sparse, and only a few topics are present in natural language documents.
We show the application of GD-LDA in ad hoc IR using the OHSUMED dataset. As discussed above, we use the ap-Table 3: Results of ad hoc IR using GDLDA, LDA and proach from [19] as a benchmark for comparison. We train the GD-LDA Max using K =50and K = 75 topics, and compare its performance with LDA and PAM. We use stan-dard IR measures: Precision at 10 (P@10), Mean Average Precision (MAP) and Discounted Gain (DG), to compare these methods. The OHSUMED dataset is considered to be a medium size dataset in the IR literature. This contains 63 topical queries with 35 , 000 relevant labels.

We have not modified the retrieval model of [19] to ex-ploit the power of the new GD-LDA model. Despite this, the performance improvement for all the measures is signif-icant as we observe in Table 3. Here, the best performance is for K = 50 topics, where EL estimation peaks in Fig 7(d). For this case, there is agreement between the adhoc IR and EL performance. Respect to LDA, GD-LDA shows improvement of: 6.3% for P@10, 5.5% for MAP, and 1.1% for DG. Note that PAM shows lower performance than LDA in IR for all the measures. This is consistent with the IR performance of PAM reported in [22].
One advantage of GD-LDA over CTM and PAM is that its computational complexity is linear in the number of topics. This is not the case for CTM and PAM which scale quadrat-ically with the number of topics. As discussed in section 3, GD-LDA should add minimal computational cost to LDA. Fig 8 shows the computational time in minutes to fit the model for LDA, CTM, PAM and GD-LDA in the datasets we consider. A non-linear increase in time is observed after 50 topics for CTM in NYT and APW datasets, and after 80 topics in NIPS dataset. We observe that the computa-tional cost of PAM grows quadratically after 20 topics. In general, the computational cost of GD-LDA is comparable to that of LDA and is less than CTM and PAM. This is a significant advantage since GD-LDA provides a more flex-ible model structure when compared with LDA. Moreover, variances and a number of covariances are modeled more ef-fectively, and with minimal increase in computational cost. This property makes GD-LDA more suitable than CTM or PAM for larger scale applications, such as IR.
An open problem is how to select the optimal number of topics to train a model. In general, an exhaustive experimen-tation needs to be performed to select the optimal number of topics. Due to the tree structure of the GD distribution, GD-LDA tends to accommodate the most relevant topics, based on probability mass, at the upper levels of the tree.
Fig 9 shows the cumulative probability vs the number of topics of the expected topic mixture of documents given the fitted parameters for GD-LDA and CTM. We observe that GD-LDA favors a smaller number of topics. Notice that, after a certain number of topics, the expected contri-bution of the remaining ones is not significant. This prevents GD-LDA from over-fitting. When comparing GD-LDA with CTM, we observe that CTM favors uniform topic mixtures in each document. Thus, if we fit CTM for larger number of topics, we would observe the same linear behavior as seen in graphs from Fig 9. This prevents CTM from discarding any topic easily or suggesting an optimal topic range as op-posed to GD-LDA. In addition, this behavior makes CTM less effective in handling over-fitting. Figure 9: Cumulative distribution of the topic mix-
We have introduced the use of the GD distribution in probabilistic topic modeling. The advantages of the GD over the Dirichlet distribution, and the benefits when compared with the estimation of the full covariance matrix in CTM have been described. The apparent constraints on covari-ances in GD actually results in modeling better sparse topic correlations in natural language documents, as our empirical validation indicates. This results in better performance in empirical likelihood and IR measures. We have developed an efficient Gibbs sampling model which uses the conjugacy property of GD with the Multinomial distribution. We have demonstrated that the running time of GD-LDA is compa-rable to LDA and less than CTM and PAM. This provides a model computationally competitive and with better per-formance than these methods.

We have shown that the impact of optimizing the vocab-ulary parameter  X  decreases when the vocabulary size and the number of documents in the corpus is large. Due to the tree structure of the GD distribution, GD-LDA proves to be powerful in handling over-fitting with a large number of topics as its performance remains fairly high even when the number of topics is increased. This is not the case for CTM, PAM, and LDA. As a consequence, we can reduce the number of topics, by using the conditional probability of the remaining topics when we are moving down into the tree. A natural extension of the model is to allow the tree struc-ture to be fitted. A direction of improvement is to modify the Dirichlet tree to have a comparable notion of subtopics and super topics as in PAM with the same conjugacy and computational cost as GD-LDA.

We have shown that the use of GD-LDA in adhoc IR in-creases the performance significantly, in contrast to earlier incorporations of topic models. Future directions include the use of topic mixture instead the probability of words in the ranking function. In addition, we plan to explore the impact of GD-LDA in Interactive Information Retrieval.
This work is partially f unded by CONAC YT grant 207751 and CONACYT-MEXUS grant 194880.
We derive the first and second moments of the GD distri-bution based on the tree representation of Eq 2. We write: Let S k = E ( Z k ), R k = E ( Z 2 k ). Since Z k  X  X  are independent: Similarly for the crossproducts where k&lt;j : Therefore, we have for the variance and covariance: for k =1 ,...,K in Eq 20, and for k =1 ,...,K  X  1and j = k +1 ,...,K in Eq 21. We estimate S k ,R k based on the independent Beta distributions of Z k . From the moment generating function for the Beta distribution we have: where  X  K =1 , X  K = 0 [4]. Note that the variances and the expected value are not constrained.

Three key properties can be derived from Eqs 20-22: 1. For j&gt;k , Cov (  X  k , X  j ) &gt; 0 if and only if Var ( V 2. For j&gt; 1, Cov (  X  1 , X  j )=  X  Var (  X  1 ) E (  X  j ) /E (1 3. For j&gt;k +1, Cov (  X  k , X  j )= Cov (  X  k , X  k +1 ) E (  X 
The GD distribution models the Cov (  X  k , X  k +1 )forcon-secutive tree levels without constraints. The covariances for deeper levels in the tree Cov (  X  k , X  j ) ,j &gt; k + 1 are con-strained. Since  X  1 is used as base category, it is always negatively correlated with the other categories. This is a typical constraint of the Logistic Normal distribution used in CTM[2]. The GD distribution constrains the sign of Cov (  X  k , X  j ) to be the same as that of Cov (  X  k , X  k +1 k +1. These constraints imply that probability of co-occurrence of these topics is very small.

To estimate the parameters of the GD distribution, the log-likelihood of Eq. 6 is optimized using the Newton method. By taking the derivative with respect to  X ,  X  we have: Recall that  X  j k =  X  k + N j,k ,  X  j k =  X  k + N j,k +1 + ... + N j,K . Notice that the derivative of L (  X ,  X  )withrespectto  X  k just depends on  X  k and  X  k as opposed to the Dirichlet distribution. For the second derivative we have: Therefore, the Hessian matrix can be written as: where H k is the Hessian matrix for  X  k , X  k . Following a simi-lar logic as in the case of a Dirichlet distribution described in Eqs (56-60) of [12], we have the following Newton iteration: where g k 1 = dL (  X ,  X  ) /d X  k and g k 2 = dL (  X ,  X  ) /d X  24.
