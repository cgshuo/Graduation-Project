 Learning from massive datasets, such as text, images, and high throughput biological data, has often demands high, sometimes prohibitive, computational cost. To address this issue, an obvious approach is to parallelize learning methods with multiple processors. While large CPU clusters are commonly used for parallel computing, Graphics Processing Units (GPUs) provide us with a powerful alternative platform for developing parallel machine learning methods.
 A GPU has massively built-in parallel thread processors and high-speed memory, therefore provid-ing potentially one or two magnitudes of peak flops and memory throughput greater than its CPU counterpart. Although GPU is not good at complex logical computation, it can significantly reduce running time of numerical computation-centric applications. Also, GPUs are more cost effective and energy efficient . The current high-end GPU has over 50x more peak flops than CPUs at the same price. Given a similar power consumption, GPUs perform more flops per watt than CPUs. For large-scale industrial applications, such as web search engines, efficient learning methods on GPUs can make a big difference in energy consumption and equipment cost. However, parallel computing on GPUs can be a challenging task because of several limitations, such as relatively small memory size.
 In this paper, we demonstrate how to overcome these limitations to parallel computing on GPUs with an exemplary data-intensive application, training Latent Dirichlet Allocation (LDA) models. LDA models have been successfully applied to text analysis. For large corpora, however, it takes days, even months, to train them. Our parallel approaches take the advantage of parallel computing power of GPUs and explore the algorithmic structures of LDA learning methods, therefore significantly reducing the computational cost. Furthermore, our parallel inference approaches, based on a new data partition scheme and data streaming, can be applied to not only GPUs but also any shared memory machine. Specifically, the main contributions of this paper include: We briefly review the LDA model and two inference algorithms for LDA. 1 LDA models each of D documents as a mixture over K latent topics, and each topic k is a multinomial distribution over a word vocabulary having W distinct words denoted by  X  k = {  X  kw } , where  X  k is drawn from a symmetric Dirichlet prior with parameter  X  . In order to generate a document j , the document X  X  mixture over topics,  X  j = {  X  jk } , is drawn from a symmetric Dirichlet prior with parameter  X  first. For the i th token in the document, a topic assignment z ij is drawn with topic k chosen with probability  X  jk . Then word x ij is drawn from the z ij th topic, with x ij taking on value w with probability  X  z ij w . Given the training data with N words x = { x ij } , we need to compute the posterior distribution over the latent variables.
 Collapsed Gibbs sampling [4] is an efficient procedure to sample the posterior distribution of topic variable z ij , the conditional distribution of z ij is where n wk denotes the number of tokens with word w assigned to topic k , n jk denotes the number of tokens in document j assigned to topic k and n  X  ij k = X variable is calculated as if token x ij is removed from the training data.
 CGS is very efficient because the variance is greatly reduced by sampling in a collapsed state space. Teh et al. [9] applied the same state space to variational Bayesian and proposed the col-lapsed variational Bayesian inference algorithm. It has been shown that CVB has a theoretically tighter variational bound than standard VB. In CVB, the posterior of z is approximated by a fac- X  bound L ( q ) = X approximation. The updating formula for  X  ij is similar to the CGS updates 3.1 Parallel Collapsed Gibbs Sampling A natural way to parallelize LDA training is to distribute documents across P processors. Based on this idea, Newman et al. [8] introduced a parallel implementation of CGS on distributed machines, called AD-LDA. In AD-LDA, D documents and document-specific counts n jk are distributed over P processors, with D P documents on each processor. In each iteration, every processor p inde-pendently runs local Gibbs sampling with its own copy of topic-word count n p kw and topic counts n global counts n kw and n k . AD-LDA achieved substantial speedup compared with single-processor CGS training without sacrificing prediction accuracy. However, it needs to store P copies of topic-word counts n kw for all processors, which is unrealistic for GPUs with large P and large datasets due to device memory space limitation. For example, a dataset having 100 , 000 vocabulary words pacity of current high-end GPUs. In order to address this issue, we develop parallel CGS algorithm that only requires one copy of n kw .
 Algorithm 1 : Parallel Collapsed Gibbs Sampling z ij whose document index is j  X  J p and word index is x ij  X  V p  X  l . The  X  is the modulus P addition operation defined by and all processors run the sampling simultaneously without memory read/write conflicts on the n , which are used as local counts in the next epoch. Our parallel CGS can be regarded as an extension to AD-LDA by using the data partition in local guarantees that any two processors access neither the same document nor the same word in an epoch, the synchronization of n wk in AD-LDA is equivalent to keeping n wk unchanged after the sampling step of the epoch. Becasue P processors concurrently sample new topic assignments in parallel CGS, we don X  X  necessarily sample from the correct posterior distribution. However, we can view it can be found in [8]. 3.2 Parallel Collapsed Variational Bayesian The collapsed Gibbs sampling and the collapsed variational Bayesian inference [9] are similar in their algorithmic structures. As pointed out by Asuncion et al. [2], there are striking similarities between CGS and CVB. A single iteration of our parallel CVB also consists of P epochs, and each epoch has an updating step and a synchronization step. The updating step updates variational pa-rameters in a similar manner as the sampling step of parallel CGS. Counts in CGS are replaced by expectations and variances, and new variational parameters are computed by (2). The synchro-nization step involves an affine combination of the variational parameters in the natural parameter space.
 Since multinomial distribution belongs to the exponential family, we can represent the multinomial distribution over K topics defined by mean parameter  X  ij in natural parameter  X  ij = (  X  ijk ) by  X  from all processors  X  sync =  X  (1) =  X  new . If (quasi)concavity [3] holds in sufficient large neighborhoods of the doesn X  X  guarantee global convergence, but we shall see that  X  (1) can produce models that have almost the same predictive power and variational lower bounds as the single-processor CVB. 3.3 Data Partition In order to achieve maximal speedup, we need the partitions producing balanced workloads across processors, and we also hope that generating the data partition consumes a small fraction of time in the whole training process.
 CGS, r jw is the number of occurrences of word w in document j ; for parallel CVB, r jw = 1 if w V . The optimal data partition is equivalent to minimizing the following cost function The basic operation in the proposed algorithms is either sampling topic assignments (in CGS) or updating variational parameters (in CVB). Each value of l in the first summation term in (5) is associated with one epoch. All R mn satisfying m  X  l = n are the P submatrices of R whose entries are used to perform basic operations in epoch l . The number of these two types of basic operations on each unique document/word pair ( j, w ) are all r jw . So the total number of basic operations in R m,n is C mn for a single processor. Since all processors have to wait for the slowest processor to complete its job before a synchronization step, the maximal C mn is the number of basic operations for the slowest processor. Thus the total number of basic operations is C . We define data partition efficiency ,  X  , for a given row and column partitions by where C opt is the theoretically minimal number of basic operations.  X  is defined to be less than or equal to 1 . The higher the  X  , the better the partitions. Exact optimization of (5) can be achieved through solving an equivalent integer programming problem. Since integer programming is NP-hard in general, and the large number of free variables for real-world datasets makes it intractable to solve, we use a simple approximate algorithm to perform data partitioning. In our observation, it works well empirically.
 Here we use the convention of initial value j 0 = w 0 = 0 . Our data partition algorithm divides row index J into disjoint subsets J m = { j ( m  X  1) , . . . , j m } , where j m = arg min arg min for several random permutations of J or V , and take the partitions with the highest  X  . We empirically obtained high  X  on large datasets with the approximate algorithm. For a word token x large numbers and the central limit theorem also give C mn  X  C opt P and the distribution of C mn is approximately a normal distribution. Although independence and i.i.d. assumptions are not true for real data, the above analysis holds in an approximate way. Actually, when P = 10 , the C mn of NIPS and NY Times datasets (see Section 4) accepted the null hypothesis of Lilliefors X  normality test with a 0 . 05 significance level. 3.4 GPU Implementation and Data Streaming We used a Leatek Geforce 280 GTX GPU (G280) in this experiment. The G280 has 30 on-chip multiprocessors running at 1296 MHz, and each multiprocessor has 8 thread processors that are responsible for executing all threads deployed on the multiprocessor in parallel. The G280 has 1 GBytes on-board device memory, the memory bandwidth is 141.7 GB/s. We adopted NVidia X  X  Compute Unified Device Architecture (CUDA) as our GPU programming environment. CUDA programs run in a Single Program Multiple Threads (SPMT) fashion. All threads are divided into equal-sized thread blocks . Threads in the same thread block are executed on a multiprocessor, and a multiprocessor can execute a number of thread blocks. We map a  X  X rocessor X  in the previous algorithmic description to a thread block. For a word token, fine parallel calculations, such as (1) and (2), are realized by parallel threads inside a thread block.
 Given the limited amount of device memory on GPUs, we cannot load all training data and model parameters into the device memory for large-scale datasets. However, the sequential nature of Gibbs sampling and variational Bayesian inferences allow us to implement a data streaming [5] scheme which effectively reduces GPU device memory space requirements. Temporal data and variables, x ij , z ij and  X  ij , are sent to a working space on GPU device memory on-the-fly. Computation and data transfer are carried out simultaneously, i.e. data transfer latency is hidden by computation. We used three text datasets retrieved from the UCI Machine Learning Repository 2 for evaluation. Statistical information about these datasets is shown in Table 4. For each dataset, we randomly extracted 90% of all word tokens as the training set, and the remaining 10% of word tokens are the CVB, and this setting works well in all of our experiments. 4.1 Perplexity We measure the performance of the parallel algorithms using test set perplexity. Test set perplexity We report the average perplexity and the standard deviation of 10 randomly initialized runs for the parallel CVB. The typical burn-in period of CGS is about 200 iterations. We compute the likelihood Two small datasets, KOS and NIPS, are used in the perplexity experiment. We computed test per-(right). We used the CPU to compute perplexity for P = 1 and the GPU for P = 10 , 30 , 60 . For a fixed number of K , there is no significant difference between the parallel and the single-processor algorithms. It suggests our parallel algorithms converge to models having the same predictive power in terms of perplexity as single-processor LDA algorithms.
 Perplexity as a function of iteration number for parallel CGS and parallel CVB on NIPS are shown variational lower bound is computed using an exact method suggested in [9]. Figure 2 (c) shows the per word token variational lower bound as a function of iteration for P = 1 , 10 , 30 on a sampled subset of KOS ( K = 64 ). Both parallel algorithms converge as rapidly as the single-processor LDA algorithms. Therefore, when P gets larger, convergence rate does not curtail the speedup. We surmise that these results in Figure 2 may be due to frequent synchronization and relative big step the result became significantly worse. The curve  X   X  =1/P, P=10 X  in Figure 2 (right) was obtained by step size. Figure 2: (a) Test set perplexity as a function of iteration number for the parallel CGS on NIPS, K = 256 . (b) Test set perplexity as a function of iteration number for the parallel CVB on NIPS, K = 128 . (c) Variational lower bound on a dataset sampled from KOS, K = 64 . 4.2 Speedup The speedup is compared with a PC equipped with an Intel quad-core 2.4GHz CPU and 4 GBytes memory. Only one core of the CPU is used. All CPU implementations are compiled by Microsoft C++ compiler 8.0 with -O2 optimization. We did our best to optimize the code through experiments, such as using better data layout and reducing redundant computation. The final CPU code is almost twice as fast as the initial code.
 Our speedup experiments are conducted on the NIPS dataset for both parallel algorithms and the large NYT dataset for only the parallel CGS, because  X  ij of the NYT dataset requires too much memory space to fit into our PC X  X  host memory. We measure the speedup on a range of P with or without data streaming. As the baseline, average running times on the CPU are: 4 . 24 seconds on NIPS ( K = 256 ) and 22 . 1 seconds on NYT ( K = 128 ) for the parallel CGS, and 11 . 1 seconds ( K = 128 ) on NIPS for the parallel CVB. Figure 3 shows the speedup of the parallel CGS (left) and the speedup of the parallel CVB (right) with the data partition efficiency  X  under the speedup. We note that when P &gt; 30 , more threads are deployed on a multiprocessor. Therefore data transfer between the device memory and the multiprocessor is better hidden by computation on the threads. As a result, we have extra speedup when the number of  X  X rocessors X  (thread blocks) is larger than the number of multiprocessors on the GPU.
 Figure 3: Speedup of parallel CGS (left) on NIPS and NYT, and speedup of parallel CVB (right) on NIPS. Average running times on the CPU are 4 . 24 seconds on NIPS and 22 . 1 seconds on NYT for the parallel CGS, and 11 . 1 seconds on NIPS for the parallel CVB, respectively. Although using data streaming reduces the speedup of parallel CVB due to the low bandwidth between the PC host memory and the GPU device memory, it enables us to use a GPU card to process large-volume data. The synchronization overhead is very small since P N and the speedup is largely determined by the maxi-mal number of nonzero elements in all partitioned sub-matrices. As a result, the speedup (when not using data streaming) is proportional to  X P . The bandwidth be-tween the PC host memory and the GPU device mem-ory is  X  1 . 0 GB/s, which is higher than the computa-tion bandwidth (size of data processed by the GPU per second) of the parallel CGS. Therefore, the speedup with or without data streaming is almost the same for the parallel CGS. But the speedup with or without data streaming differs dramatically for the parallel CVB, because its computation bandwidth is roughly  X  7 . 2 GB/s for K = 128 due to large memory usage of  X  ij , higher than the maximal bandwidth that data stream-ing can provide. The high speedup of the parallel CVB without data streaming is due to a hardware supported exponential function and a high performance implementation of parallel reduction that is used to normalize  X  ij calculated from (2). Figure 3 (right) shows that the larger the P , the smaller the speedup for the parallel CVB with data streaming. The reason is when P becomes large, the data streaming management becomes more complicated and introduces more latencies on data transfer. NIPS.  X  X urrent X  is the data partition algorithm proposed in section 3.3,  X  X ven X  partitions documents and word vocabulary into roughly equal-sized subsets by setting j m = b mD P c and w n = b nW P c .  X  X andom X  is a data partition obtained by randomly partitioning documents and words. We see that the proposed data partition algorithm outperforms the other algorithms.
 More than 20x speedup is achieved for both parallel algorithms with data streaming. The speedup of the parallel CGS enables us to run 1000 iterations (K=128) Gibbs sampling on the large NYT 30-hour training on a CPU. Our work is closely related to several previous works, including the distributed LDA by Newman et al. [8], asynchronous distributed LDA by Asuncion et al. [1] and the parallelized variational EM algorithm for LDA by Nallapati et al. [7]. For these works LDA training was parallelized on distributed CPU clusters and achieved impressive speedup. Unlike their works, ours shows how to use GPUs to achieve significant, scalable speedup for LDA training while maintaining correct, accurate predictions.
 Masada et al. recently proposed a GPU implementation of CVB [6]. Masada et al. keep one copy of n wk while simply maintaining the same algorithmic structure for their GPU implementation as Newman et al. did on a CPU cluster. However, with the limited memory size of a GPU, compared to that of a CPU cluster, this can lead to memory access conflicts. This issue becomes severe when one performs many parallel jobs (threadblocks) and leads to wrong inference results and operation failure, as reported by Masada et al. Therefore, their method is not easily scalable due to memory the proposed partitioning scheme and data streaming. They can also be used as general techniques to parallelize other machine learning models that involve sequential operations on matrix, such as online training of matrix factorization.
 Acknowledgements We thank Max Welling and David Newman for providing us with the link to the experimental data. We also thank the anonymous reviewers, Dong Zhang and Xianxing Zhang for their invaluable inputs. F. Yan conducted this research at Microsoft Research Asia. F. Yan and Y. Qi were supported by NSF IIS-0916443 and Microsoft Research. [1] A. Asuncion, P. Smyth, and M. Welling. Asynchronous distributed learning of topic models. In [2] A. Asuncion, M. Welling, P. Smyth, and Y. W. Teh. On smoothing and inference for topic [3] S. Boyd and L. Vandenberghe. Convex Optimization . Cambridge University Press, March 2004. [4] T. L. Griffiths and M. Steyvers. Finding scientific topics. Proceedings of the National Academy [5] F. Labonte, P. Mattson, W. Thies, I. Buck, C. Kozyrakis, and M. Horowitz. The stream vir-[6] T. Masada, T. Hamada, Y. Shibata, and K. Oguri. Accelerating collapsed variational bayesian [8] D. Newman, A. Asuncion, P. Smyth, and M. Welling. Distributed inference for latent Dirichlet [9] Y. W. Teh, D. Newman, and M. Welling. A collapsed variational Bayesian inference algorithm
