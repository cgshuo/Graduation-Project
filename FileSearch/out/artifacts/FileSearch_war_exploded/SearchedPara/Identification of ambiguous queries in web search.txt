 1. Introduction
Given the impact of search engines on the web users X  experience, some in-depth analysis of diverse information needs has results in an unsatisfactory retrieval. Some technologies like personalized web search and search results clustering aim to improve user satisfaction towards ambiguous queries. However, there are no sufficient studies on ambiguous queries iden-tification. Questions like  X  X  X an we determine an ambiguous query? X  and  X  X  X hat is the proportion of ambiguous queries? X  are still open. If we can estimate the percentage of ambiguous queries, we would know how many queries may be potentially influenced by query-ambiguity-oriented technologies. If we can further determine ambiguous queries automatically, it is in this paper.

Identifying ambiguous queries is challenging for three reasons. First, there is no acknowledged definition and taxonomy
Townsend, Zhou, and Croft (2002) proposed using the relative entropy between a query and a collection to quantify query them in an automatic way?
Based on findings that human annotators are in majority agreement on 90% of queries as being ambiguous or not, we take a supervised learning approach to automatically identify ambiguous queries based on the evidence source of a web collection.
Experimental results show that our approach achieves 85% precision and 81% recall in identifying ambiguous queries. Final-ly, we estimate that about 16% of queries in the sampled search log are ambiguous.

Our main contributions include (1) In our investigations, an ambiguous query is differentiated from a broad topic query. An ambiguous query has more (2) A user study is designed to evaluate the feasibility of automatic identification. We found that human annotators have (3) We propose to learn a model of identifying ambiguous queries based on the query and its top returned documents. We (4) By applying the learned model, we estimate the proportion of ambiguous queries in a real query log is about 16%. That
In the next section, we define three query types and three sources that affect the judgment of query type. In Section 3,we describe a user study with human annotators in detail. Then in Section 4, we use a supervised learning approach to classify whether a query is ambiguous. Evaluations of ambiguous query classifiers and an estimation of the proportion of ambiguous and discuss some possible future work. 2. Taxonomy of queries By surveying the literature, we found the following three types of queries from being ambiguous to specific.
Type A (ambiguous query): a query that has more than one meaning. produced in 1956),  X  X  X iant Bike X  (a bicycle manufacturer), or  X  X  X an Francisco Giants X  (National League baseball team).
Type B (broad query): a query that covers a variety of subtopics, and a user might look for one of the subtopics by issuing another query. e.g.  X  X  X ongs X , which covers some subtopics such as  X  X  X ong lyrics X ,  X  X  X ove songs X ,  X  X  X arty songs X , and  X  X  X ownload songs X . In practice, a user often issues such a query first, and then narrows down to a subtopic.

Type C (clear query): a query that has a specific meaning and covers a narrow topic. several nice results in the first result page.

Is this classification adequate? In other words, is it possible to determine the type of a query by human beings based upon the above descriptions? To answer this question, we test with human subjects. Four researchers in the areas of Natural Lan-guage Processing, User Interface and Web search, and one professional editor volunteered to classify some queries based on the above descriptions by using any resources. Then, we ask for their feedback in interviews. One problem is remarkable in the interviews:
People may take into account various sources, such as dictionaries, Web search results, and user behaviors, when judging a query X  X  type. Different resources may lead to conflicting conclusions.
 disorder X , dominates the web. Thus, the query  X  X  X ipolar X  tends to an ambiguous query in terms of dictionary, whereas it would be a clear query in terms of the web.

Therefore, we propose to discuss query types with respect to a particular source. We consider three sources that may affect the judgment of a query X  X  type: (1) Dictionary/thesaurus: A dictionary, such as the Merriam-Webster Dictionary, contains an alphabetical list of words, (2) Web search results: As argued in Cronen-Townsend et al. (2002), relevant documents provide rich evidence on a (3) User behaviors: Given a query, user behaviors like click-through data and other related queries provide information to
We investigate how these three sources affect human judgments in Section 3. 3. User study
The purpose of user study is to answer whether it is ever possible to associate a query with a certain type by looking at a and 3.3 provide more details on design of user studies and on the analysis of results. 3.1. User study setup
The queries used in our investigations are sampled from a 12-day Live Search (Live Search) http://www.live.com/ query
Query Frequency (i.e. how many times a query is searched) and clicked URLs with Click Frequency (i.e. how many times a URL is clicked by the users). To avoid frequency bias, we split the queries into 12 buckets in terms of Query Frequency .[ n used to represent a bucket of queries with Query Frequency between n
The length of each bucket is determined by taking account of Zipf-Law distribution of Query Frequency . We randomly sample 100 queries from each bucket, and get 1200 queries as a whole. After removing non-English and porn queries, here are 989 queries. All the 989 queries are eventually used for the estimation experiment in Section 5.4. However, due to the high cost of user study, we use 60 queries only in the user study. From each bucket, five queries are selected.
We involve five human subjects, including two males and three females. Among them, two participants are native English speakers and the other three are Chinese graduate students who major in English. 3.2. User study design We ask the participants to classify queries independently according to the three sources described in Section 2.
First, a participant would look up a query in dictionaries and choose one of three options: (1)  X  X  X he query has a single meaning X , (2)  X  X  X he query has more than one meaning X , and (3)  X  X  X he query cannot be found in the thesauruses X . A choice would be directly inferred from the number of interpretations listed in the dictionary. Close interpretations would not be merged. We use TheFreeDictionary (TheFreeDictionary) http://www.thefreedictionary.com/ as thesaurus in our user study because it contains both a classical dictionary and a Wikipedia (Wikipeida) http://www.wikipedia.org/ . Compared with a classical dictionary, Wikipedia contains more new words. As Fig. 1 shows, all the interpretations are listed in the center of the page. Wikipedia can be reached by clicking the tab of  X  X  X ikipedia encyclopedia X  at the top.

Second, by using Web search results as the source, a participant would judge the type of a query or choose  X  X  X ot applica-ble X  otherwise. It is tough for a person to survey a query X  X  meanings by going through hundreds of returned documents.
Thus, we use clustered search results generated by Vivisimo (Vivisimo) http://www.vivisimo.com/ to facilitate the under-top hundreds of searched documents. A participant is required to click all groups and understand the interpretation of the query in each group. Non-relevant documents or sponsored links are not taken into account.
 lated searches X  lists a set of related queries. Such information most likely reminds a participant whether the query covers several entities or subtopics. 3.3. Results and analysis Results of our user study are presented in this section. We also provide some analysis of the results. In terms of the source of dictionaries, queries are split into three groups: Single-Sense, Multi-Sense and Not-Found. As the other is the count of queries with more than one term.
We find that the well-organized thesauruses have low coverage of queries. In Fig. 4 a, more than one-third of queries can-nor a phrase in a dictionary. For other nine one-term but not-found queries, we find some of them are compound words like website names, such as  X  X  X llhiphop X , and some others are none-word strings like  X  X  X indow98SE X .

In annotations based upon web search results and user behaviors, no one chooses  X  X  X ot applicable X  in the user study, and types. Table 1 shows the results measured by majority agreement, whereas Table 2 shows the agreement measured by Fleiss X  kappa ( Fleiss, 1971 ).

In our analysis, majority agreement is achieved only when no less than four of five annotators agree on a query X  X  type. We queries are of Type B/C. The annotators cannot agree on the types of only six queries. Actually, majority agreement is achieved in judging whether a query is of Type A or not for 90% queries, no matter based upon web search results or user behaviors. This means it is easy for human annotators to distinguish Type A queries from the other two types of queries. ries based upon web search results, and for 17 Type B/C queries based upon user behaviors. In addition, compared to web search results, user behaviors induce less queries as Type A, and more queries as Type C. One possible explanation is that although some queries have more than one interpretation, one interpretation is much more popular than the others on the web. Most users often search for the dominant interpretation of the queries.

We use Fleiss X  kappa, a statistical measure for assessing the reliability of agreement between a fixed number of raters,
Cohen X  X  kappa that only works when assessing the agreement between two raters (Cohen, 1960; Fleiss &amp; Cohen, 1973 ). In this measure, the degree of agreement is scored as a number between 0 and 1. The larger is a score, the more agreement
Types A, B, and C. Row one shows the Fleiss X  kappa over three categories. The agreement is not high based upon both web search results and user behaviors. Then, we merge any two query types to get two categorical ratings. For example, when two categories is shown in the last three rows. We find the agreement is the highest when we merge Types B and C. The ings in terms of majority agreement in Table 1 .

When we merge Types B and C as Type (BC), we show the number of one-term and multi-term queries in each labeled majority of Type A queries are one-term queries. It means that ambiguous queries tend to have a single term only. Second,
Type (BC) queries may be one-term or multi-term queries with a similar chance. Thus, the number of terms in a query is useful but far from enough in identifying ambiguous queries. Third, the queries that the annotators have different opinions on also tend to be one-term. For example,  X  X  X ackgrounds X  is one of the disagreed queries. Some users label the word as ambiguous while some others argue that it is clear to be the ground picture located behind something on the web. 4. Learning query vagueness models
We propose to take a machine learning approach to identify ambiguous queries based upon a web collection automati-and time consuming. In some applications, e.g. ranking, we need an automatic way to do runtime query classification instead of offline manual methods. Second, the user study results indicate that human beings are in general agreement in judging whether a query is ambiguous or not, while it is difficult to distinguish Type B from Type C. Thus, we are more confident to learn a model of classifying ambiguous queries or not rather than a three-class model. Third, we take a web collection as our evidence because for any query it is easy to retrieve some related documents from the web, whereas user behaviors are not available or adequate for some queries.

In this paper, we utilize a query q and a set of top n search results D with respect to the query in modeling query ambi-guity. We formulate the problem of identifying ambiguous queries as a classification problem uous queries) and A (broad or clear queries). Support Vector Machines (SVMs) developed by Vapnik (Vapnik, 1992 ) with RBF
Sequential Minimal Optimization algorithm (Platt, 1998 ). We have tried both linear SVM and nonlinear SVM with RBF kernel, but we find that the nonlinear SVM with RBF kernel always outperforms the linear SVM on our dataset when parameters are tuned as the best for both. Therefore, we report the results produced by the nonlinear SVM only to avoid confusions.
We will describe how to represent a document in our approach in Section 4.1. Then, our main idea on extracting effective of features. Thus, we propose a solution on filtering irrelevant documents in Section 4.3. 4.1. Document representations
Documents are frequently represented by a bag of words in Web information retrieval, while some personalized Web In this paper, we represent a document by the categories that it belongs to.
 These documents compose a set D where d i is a retrieved document. We set n = 200 because one experiment in Zeng et al. (2004) shows that 200 is optimal in covering different interpretations of ambiguous queries.

Then, a text classifier similar to that used in Shen et al. (2005) is applied to classify each Web document into predefined the confidence of predicting a document to be in a certain category.

Finally, we get a vector of categories to represent a document d p is the confidence with respect to category, and P K k  X  1 For instance, the page http://www.billie-holiday.net/ is classified into the following categories: Entertainment n Music Entertainment n Celebrities Work&amp;Money n Company Entertainment n Movies with confidences 0.7261, 0.2392, 0.0124, 0.0043, 0.0037, etc. The corresponding second-level category vector is
By aggregating confidences of the sub-categories in a first-level category, we get the first-level category vector as 4.2. Query representations
Our main idea of identifying an ambiguous query is that relevant documents with different interpretations may belong to several different categories. To illustrate this assumption, we project documents into a three-dimensional (3D) space, and show three example queries of Types A, B, and C, respectively. As Section 4.1 describes, we could represent a document as a select three dimensions to draw a 3D graph instead. Given a query, we select three dimensions/categories to which docu-ments most likely belong. Now, the documents are represented as three-dimension vectors. Then in Fig. 5 , we project the documents as points in a 3D space, three axes of which correspond to the selected three categories. For instance,  X  X  X iant X  (an internet security software developer) in Computing category, and  X  X  X aint Food supermarket X  or  X  X  X iant bike manufactur-being scattered and gathered is observed in Fig. 5 b.

Twelve features are derived to quantify the distribution of D . For the set D , we first calculate the centroid document d
Then, three functions of distance are used in this paper. (1) Euclidean distance: (2) Jensen X  X hannon Divergence (JSD) (Lin, 1991 ) distance:
As stated in Carmel et al. (2006) , the square root of Jensen-Shannon divergence is used because the divergence does not obey the triangle inequality but its square root does. (3) Cosine distance:
We add a minus in the equation because the cosine is known to measure similarity rather than distance. If the angle be-tween two documents is zero, its cosine gets the maximum, i.e. 1.
 Finally, the following features are extracted: Euc-Diameter, JSD-Diameter, and Cos-Diameter:
Three kinds of distances are applied to calculate diameters as follows: Euc-Mean, Euc-SD, JSD-Mean, JSD-SD, Cos-Mean, and Cos-SD:
Mean and standard deviation are calculated on the array of Dis  X  d Cat-Entropy: We normalize the centroid document d c and treat it as a probability distribution. Entropy is calculated by the formula: Clstr-entropy: K -means algorithm ( MacQueen, 1967 ) is used to cluster documents into L clusters.

We count the number of documents in each cluster, and form a vector of L dimensions. After normalizing each dimension with the total number of documents, we calculate the entropy of the normalized vector. L is set as 10 in our experiments because Vivisimo (Vivisimo) http://www.vivisimo.com/ generally shows ten clusters for a query.
 NumTerm:
Number of terms in the query q . 4.3. Irrelevant documents filter
In practice, search engines may occasionally return irrelevant documents for query q . For example, a user who is inter-are related to the magazine, we find one snippet is Why Use Time Warner Cable X  X  Road Runner Service?
Our World class customer service is local and available when you need us. Customer satisfaction is ... Triple the speed of
DSL claim is based on based on Road Runn er X  X  standard maximum download speed of 5.0 Mbps ... which is irrelevant to the query. Another example is the snippet for the query  X  X  first man on moon  X :
Moon Glow ... to go on-line and read additional background information related to the Moon (Solar System section) and man ... one-quarter of the way around Earth, this phase is referred to as a first -quarter Moon . which has nothing to do with Armstrong or the Apollo program.
 racy, we propose a filter to remove noise in D . We use D  X  to denote the new document set after filtering.
Our filter first calculates a relevance score based on the title and snippet of a document to a given query. We use a prox-more unique query terms, the fragment gets a higher relevance score. If query terms are disordered in a fragment, the frag-ment will be penalized. Finally, the scores of the fragments are combined into one relevance score for the document.
The maximum relevance score among all the documents is used to normalize scores into [0, 1]. The filter uses a prede-fined threshold to remove the document with normalized score lower than s . As a result, D  X  is generated. Features will be extracted upon D  X  if the filter is turned on. 5. Experiments
We conduct some experiments to examine the effectiveness of our proposed automatic methods, and also to estimate the percentage of ambiguous queries in a real query log. 5.1. Experimental setup
To collect more data for learning a model, we hired four assessors to label another 400 queries that are selected from the whole set of 989 queries as described in Section 3.1. Since the task is labor consuming, we use a more efficient way. Each query is assigned to two annotators. The query would be used for learning only if both annotators agree on the label based on a web collection. Finally, we get 253 valid queries, among which 94 queries are labeled as Type A, 41 queries as Type B, and 118 queries as Type C. We take queries with Type A labels as positive samples, and merge queries with Type B and Type C labels into one negative sample set.
 We conduct the experiments of learning query ambiguity models on 253 labeled queries in Sections 5.2 and Section 5.3 . it is the optimal when s = 0.7. The experiment in Section 5.4 is done on the whole set of 989 queries. 5.2. Query ambiguity classification
The best classifier in our experiments achieves the precision of 85.4% and the recall of 80.9%. Such performance verifies that ambiguous queries can be identified automatically.

Fig. 6 shows a classification performance with a filtering threshold s increasing from 0 to 1. When s = 0, i.e. none of the documents in D is filtered out, the F1 measure of 76% is achieved. As s increases, more irrelevant documents are removed, and thus the performance is improved. The best performance is achieved when s = 0.7, with 82.6% F1 measure. If the crite-useful when we need lower false alarm.

Overall, Table 3 compares the performance of classifiers with filter and without filter. We found that precision and recall 5.3. Feature contributions
We investigated the contribution of each individual feature in this experiment. Table 4 ranks the 12 features in decreasing order of F1 measure.

Euc-SD , NumTerm and Cat-Entropy are the three most significant features, with their F1 measure larger than 70%. Euc-SD and Cat-Entropy successfully capture how diverse the documents are distributed in various categories. The performance of
NumTerm indicates that short queries are more likely to be ambiguous. From the result, we see Euclidean distance performs the best among the three types of distances, and Standard Deviation is more effective than Mean and Diameter.
We also notice that Cos-Diameter performs poorly, with zero precision and recall. When closely checking the feature val-the category vectors are sparse. When we calculate the cosine distance between any two documents among the document set, it is usually easy to find two documents with the angle 90 degree in-between. Thus the Cos-Diameter , i.e. the maximum cosine distance of any two documents, always values 0. This means the feature is totally useless for distinguishing ambig-uous queries from others. 5.4. Ambiguous query estimation
We try to estimate what percentage of queries is ambiguous in the query set sampled from Live Search (Live Search) http://www.live.com/ logs. We do not know whether 989 queries are enough to get a reliable estimation, so we show the evenly. Second, our newly learned query ambiguity model is used to do prediction for all the queries. Third, we count the percentage of ambiguous queries above ten query sets, whose size ranges from one part to ten parts. We find that the per-centage vibrates between 15% and 18% when the number of queries for estimation is less than 500. As the size increases, the 6. Discussion
Ambiguous query classifiers can play a significant role in Web search. First, the classifier can identify ambiguous queries from query logs and set up a dataset of ambiguous queries. Such datasets can be used to measure the ability that a search us which documents cover which meanings in additional to relevance judgments. Then some metrics, such as S-Precision and query types automatically, we could employ different strategies intelligently in personalized Web search or search results clustering. For example, when a query is a clear query, we can keep the original search result and simple user interface, whereas for an ambiguous query, we can apply some advanced technologies to improve user satisfaction. Third, we could go ahead to distinguish what the meanings are for an ambiguous query, so that a summary of search results can be generated competition, the purpose of which is to support and further research within the information retrieval). A user could browse more information in the first page, and then narrow down to search results about a particular meaning. We will investigate these interesting applications as our future work. 7. Related work
Query ambiguity is related to several research areas: personalized Web search, query difficulty, query classification, word sense disambiguation.
 Personalized web search
A strong motivation of personalized Web search is that queries are inherently ambiguous. The query  X  X  X ava X  is frequently or categories that are mined from users X  surfing behavior and history. However, most of these technologies are generally used for all queries, and few studies have tried to answer how many queries can benefit from personalization. Our proposed query taxonomy is relevant to this question. We could imagine that personalization may be helpful in searching ambiguous queries but not necessary for clear queries. Our proposed automatic method of identifying ambiguous queries also provides an opportunity to use personalization intelligently.
 Query Difficulty ative entropy between a query and the collection in language models, to quantify a query X  X  ambiguity. Experiments with
TREC data show positive correlation between clarity score and query performance. Yom-Tov, Fine, Carmel, and Darlow (2005) suggested an alternative approach, based on machine learning, for estimating query difficulty. Mothe and Tanguy (2006) , touched a fundamental question:  X  X  X hat makes a query difficult? X  They addressed a novel model that comprises three main components of a topic: queries, the relevant document set, and the collection. Using the model-induced distances between any two components, they attempted to estimate the average precision of the topics, topic aspect coverage, and topic  X  X  X indability X . Our work is inspired by these studies, but we aim to understand more on query ambiguity in semantics before applying the outcome in search.
 Query classification
There is a recent interest in identifying users X  goals during a search. Broder (2002) and Rose and Levinson (2004) came up with a taxonomy of search goal types: navigational, informational, and transactional/resources. Both works have reported the percentage of Web queries that belong to each category from the manual inspection process. For automatic identification of query goals, Kang and Kim (2003) proposed to explore the occurrence patterns of query terms in Web pages, while Lee, a query as either navigational or informational.

Another interesting research area is on which categories of information a user is seeking. In the KDD-Cup 2005 Compe-titles, and Web pages, for a query, and apply text categorization methods to build up models. Dai et al. (2006) focused on capturing commercial intention from search queries and Web pages. They found 38% of the sampled search queries have commercial intention.

Our work uses the machine learning-based methodology as some of these investigations did, but our target is to learn a different taxonomy. The proposed features are unique to identify ambiguous queries. Although we classify documents into the categories used in the KDD-Cup Competition (Li et al., 2005 ), we cannot derive that a query is ambiguous from that the Word sense disambiguation Word sense disambiguation (WSD) is the problem of selecting a sense for a word from a set of predefined possibilities.
Sense inventory usually comes from a dictionary or a thesaurus. On the one hand, knowledge intensive methods, supervised learning, and bootstrapping approaches are well studied to address the problem. On the other hand, unsupervised tech-niques were proposed to divide the usages of a word into different meanings, regardless of any particular existing sense inventory, which is also called word sense discrimination. A rather comprehensive review of such studies can be found in
Mihalcea and Pedersen (2005) . Identifying ambiguous queries is related, but different from WSD. For one thing, identifying ambiguous queries distinguishes not the usage of a word but the topic of a query. For another thing, the query usually con-sists of more than one word.
 Miscellaneous
Zhai et al.(2003) presented a problem called subtopic retrieval. If finding documents that cover many different subtopics of a query is preferred, the assumption that a document in a ranking is independent on other documents is violated. Chal-uation metrics are proposed. Chen and Karger (2006) developed a greedy optimization algorithm to approximately optimize the metrics that consider diverse information needs. However, these studies do not differentiate ambiguous queries from broad queries in discussion.
 A number of information retrieval interface ideas are presented to help the user deal with ambiguous queries. Lawrie and
Croft (2000), Sanderson and Croft (1999) worked on the concept hierarchies that provide a hierarchical map of words and their relationships. The concept hierarchies are dynamic, and not aligned with the existing directories. Clustering the re-2004; Zeng et al., 2004 ). They group documents by topic and generate descriptive names for clusters. The problem is the one aspect of a meaning. Therefore, we cannot infer how many meanings a query has directly from the number of clustered groups. 8. Conclusion
In this paper, we try to define and differentiate ambiguous query, broad query, and clear query. User studies show that people are in general agreement on whether a query is ambiguous or not, while there is a gray field between broad queries and clear queries. Thus, we propose to learn a model for identifying ambiguous queries based on search results from the
Web. Our main idea is to represent a document with a vector of semantic categories. Then we assume relevant documents with different meanings may scatter in the category space. Some features are proposed, and one further improvement is made by filtering noise that is brought by irrelevant returned documents. In the experiments, the best classifier achieves an 85% precision and 81% recall. Applying the query ambiguity classifier, we estimate that 16% sampled queries are ambig-uous in a real query log.
 Acknowledgement We are grateful to Dwight Daniels for edits and comments on writing the paper.
 References
