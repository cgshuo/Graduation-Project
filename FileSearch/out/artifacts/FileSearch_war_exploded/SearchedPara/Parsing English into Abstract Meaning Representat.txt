 Abstract Meaning Representation (AMR) is a compact, readable, whole-sentence semantic an-notation (Banarescu et al., 2013). It includes en-tity identification and typing, PropBank semantic roles (Kingsbury and Palmer, 2002), individual en-tities playing multiple roles, as well as treatments of modality, negation, etc. AMR abstracts in nu-merous ways, e.g., by assigning the same concep-tual structure to fear (v), fear (n), and afraid (adj). Figure 1 gives an example.

AMR parsing is a new research problem, with only a few papers published to date (Flani-gan et al., 2014; Wang et al., 2015) and a publicly available corpus of more than 10,000 can be tackled either by developing new algo-rithms/techniques (Flanigan et al., 2014; Wang et al., 2015) or by adapting existing algo-rithms/techniques to the problem at hand. In this paper, we investigate the second approach.

The AMR parsing problem bears a strong for-mal resemblance to syntax-based machine transla-tion (SBMT) of the string-to-tree variety, in that Figure 1: An Abstract Meaning Representation (AMR) with several English renderings. a string is transformed into a nested structure in both cases. Because of this, it is appealing to ap-ply the substantial body of techniques already in-an SBMT inference engine instead of creating cus-tom inference procedures, we lose the ability to embed some task-specific decisions into a custom transformation process, as is done by Flanigan et al. (2014) and Wang et al. (2015). However, we reap the efficiency gains that come from work-ing within a tested, established framework. Fur-thermore, since production-level SBMT systems are widely available, anyone wishing to generate AMR from text need only follow our recipe and retrain an existing framework with relevant data to quickly obtain state-of-the-art results.

Since SBMT and AMR parsing are, in fact, distinct tasks, as outlined in Figure 2, to adapt the SBMT parsing framework to AMR parsing, we develop novel representations and techniques. Some of our key ideas include: 1. Introducing an AMR-equivalent representa-Figure 2: Differences between AMR parsing and syntax-based machine translation (SBMT). 2. Proposing a target-side reordering technique 3. Introducing a hierarchical AMR-specific lan-4. Integrating several semantic knowledge 5. Developing tuning methods that maximize By applying these key ideas, which constitute lightweight changes on a baseline SBMT system, we achieve state-of-the-art AMR parsing results. We next describe our baseline, and then describe how we adapt it to AMR parsing. Our baseline SBMT system proceeds as follows. Given a corpus of (source string, target tree, source-target word alignment) sentence translation training tuples and a corpus of (source, target, score) sentence translation tuning tuples: 1. Rule extraction: A grammar of string-to-2. Local feature calculation: Statistical and in-3. Language model calculation: A Kneser-4. Decoding: A beamed bottom-up chart de-5. Tuning: Feature parameters are optimized Table 1: Data splits of AMR 1.0, used in this work. Tokens are English, after tokenization.
We initially use this system with no modifica-tions and pretend that English X  X MR is a language pair indistinct from any other. We use English X  X MR data from the AMR 1.0 cor-pus, LDC Catalog number 2014T12. In contrast to narrow-domain data sources that are often used in work related to semantic parsing (Price, 1990; Zelle, 1995; Kuhlmann et al., 2004), the AMR cor-pus covers a broad range of news and web forum data. We use the training, development, and test splits specified in the AMR corpus (Table 1). The training set is used for rule extraction, language modeling, and statistical rule feature calculation. The development set is used both for parameter optimization and qualitatively for hill-climbing. The test set is held out blind for evaluation. We preprocess the English with a simple rule-based tokenizer and, except where noted, lowercase all data. We obtain English X  X MR alignments by us-ing the unsupervised alignment approach of Pour-damghani et al. (2014), which linearizes the AMR and then applies the models of Brown et al. (1993) with an additional symmetrization constraint.
All parsing results reported in this work are obtained with the Smatch 1.0 software (Cai and Knight, 2013). We compare our results to those of Flanigan et al. (2014) on the AMR 1.0 data splits; we run that work X  X  JAMR software according to results to published scores in the recent work of Wang et al. (2015). Their work uses slightly dif-have not seen significant variation in results. et al., 2004; Galley et al., 2006) and yields good performance. In this section we discuss various transformations to our AMR data. Initially, we concern ourselves with converting AMR into a form that is amenable to GHKM rule extraction and string to tree decod-ing. We then turn to structural transformations designed to improve system performance. Fig-ure 3 progressively shows all the transformations described in this section; the example we follow is shown in its original form in Figure 3a. We note that all transformations are done internally; the in-put to the final system is a sentence and the output is an AMR. We further observe that all transfor-mations are data-driven and language agnostic. 4.1 Massaging AMRs into Syntax-Style Trees The relationships in AMR form a directed acyclic graph (DAG), but GHKM requires a tree, so we must begin our transformations by discarding some information. We arbitrarily disconnect all but a single parent from each node (see Figure 3b). This is the only lossy modification we make to our AMR data. As multi-parent relationships oc-cur 1.05 times per training sentence and at least once in 48% of training sentences, this is indeed a regrettable loss. We nevertheless make this mod-ification, since it allows us to use the rest of our string-to-tree tools.

AMR also contains labeled edges, unlike the constituent parse trees we are used to working with in SBMT. These labeled edges have informative content and we would like to use the alignment procedure of Pourdamghani et al. (2014), which aligns words to edges as well as to terminal nodes. So that our AMR trees are compatible with both our desired alignment approach and our desired rule extraction approach, we propagate edge labels to terminals via the following procedure: 1. For each node n in the AMR tree we create 2. For each outgoing role edge we insert two un-3. For the outgoing concept edge we insert an 4. Since SBMT expects trees with preterminals,
The complete transformation can be seen in Fig-ure 3c. Apart from multiple parent ancestry, the original AMR can be reconstructed deterministi-cally from this SBMT-compliant rewrite. 4.2 Tree Restructuring While the transformation in Figure 3c is accept-able to GHKM, and hence an entire end-to-end AMR parser may now be built with SBMT tools, the resulting parser does not exhibit very good per-formance (Table 3, first line). The trees we are learning on are exceedingly flat, and thus yield rules that do not generalize sufficiently. Rules pro-duced from the top of the tree in Figure 3c, such as that in Figure 4a, are only appropriate for cases where fear-01 has exactly three roles: ARG0 (agent), ARG1 (patient), and polarity .

We follow the lead of Wang et al. (2010), who in turn were influenced by similar approaches in monolingual parsing (Collins, 1997; Charniak, 2000), and re-structure trees at nodes with more than three children (i.e. instances with more than one role), to allow generalization of flat structures.
However, our trees are unlike syntactic con-stituent trees in that they do not have labeled non-terminal nodes, so we have no natural choice of an intermediate ( X  X ar X ) label. We must choose a meaningful label to characterize an instance and its roles. We initially choose the concept label, resulting in trees like that in Figure 3d, in which a chain of fear-01 nodes is used to unflatten the root, which has instance fear-01 .

This attempt at re-structuring yields rules like that in Figure 4b, which are general in form but are tied to the concept context in which they were extracted. This leads to many redundant rules and blows up the nonterminal vocabulary size to ap-proximately 8,000, the size of the concept vocabu-lary. Furthermore, the rules elicited by this proce-dure encourage undesirable behavior such as the immediate juxtaposition of two rules generating ARG1 .

We next consider restructuring with the imme-diately dominant role labels, resulting in trees like that in Figure 3e and rules like that in Figure 4c. The shape of the structure added is the same as in Figure 3d but the bar nodes now take their labels from their second children. This approach leads to more useful rules with fewer undesirable proper-ties. 4.3 Tree Relabeling AMR strings have an effective preterminal label of  X  X , X  which allows them to compete with full AMR instances at decode time. However, whether or not a role is filled by a string or an instance is highly dependent on the kind of role being filled. The polarity and mode roles, for instance, are nearly always filled by strings, but ARG0 and ARG1 are always filled by instances. The quant role, which is used for representation of numerical quantities, can be filled by an instance (e.g. for approximate quantities such as  X  X bout 3 X ) or a string. To capture this behavior we relabel string preterminals of the tree with labels indicating role identity and string subsumption. This relabeling, replaces, for exam-ple, one  X  X  X  preterminal in Figure 3c with  X  X po-larity, X  as shown in Figure 3f. 4.4 Tree Reordering Finally, let us consider the alignments between English and AMR. As is known in SBMT, non-monotone alignments can lead to large, unwieldy rules and in general make decoding more diffi-cult (May and Knight, 2007). While this is often an unavoidable fact of life when trying to trans-late between two natural languages with different syntactic behavior, it is an entirely artificial phe-nomenon in this case. AMR is an unordered repre-sentation, yet in order to use an SBMT infrastruc-ture we must declare an order of the AMR tree. This means we are free to choose whatever order is most convenient to us, as long as we keep role label edges immediately adjacent to their corre-sponding role filler edges to preserve conversion back to the edge-labeled AMR form. We thus choose the order that is as close as possible to that of the source yet still preserves these con-straints. We use a simple greedy bottom-up ap-proach that permutes the children of each internal node of the unrestructured tree so as to minimize crossings. This leads to a 79% overall reduction in crossings and is exemplified in Figure 3g (before) and Figure 3h (after). We may then restructure our trees, as described above, in an instance-outward manner. The final restructured, relabeled, and re-ordered tree is shown in Figure 3i. We now turn to language models of AMRs, which help us prefer reasonable target structures over un-reasonable ones.

Our first language model is unintuitively simple X  X e pretend there is a language called AMRese that consists of yields of our restruc-tured AMRs. An example AMRese string from Figure 3i is  X  ARG0 soldier polarity -fear-01 ARG1 die-01 ARG1 *. X  We then build a standard n-gram model for AMRese.

It also seems sensible to judge the correctness of an AMR by calculating the empirical probability of the concepts and their relations to each other. This is the motivation behind the following model
We define an AMR instance i = ( c, R ) , where c is a concept and R is a set of roles . We define an AMR role r = ( l, i ) , where l is a role label, and i is an AMR instance labeled l . For an AMR instance i let  X  c i be the concept of i  X  X  parent instance, and  X  l i be the label of the role that i fills with respect to its parent. We also define the special instance and role labels ROOT and STOP. Then, we define P the conditional probability of AMR instance i given its ancestry as P P ( c |  X  l i ,  X  c i ) P as empirical conditional probabilities, Witten-Bell interpolated (Witten and Bell, 1991) to lower-order models by progressively discarding context event per instance. We define the probability of a full-sentence AMR i as P ROOT in this case serves as both parent concept and role label. Table 2: The effect of AMRese n-gram and AMR LMs on Smatch quality.

As an example, the instance associated with concept die-01 in Figure 3b has  X  l i = ARG1 and  X  c i = fear-01 , so we may score it as P ( die-01 | ARG1 , fear-01 )  X  P ( ARG1 | die-01 )  X  P ( STOP | die-01 )  X  P ( * | ARG1 , die-01 ) In Table 2 we compare the effect of varying LMs on Smatch quality. The AMR LM by itself is inferior to the AMRese n-gram LM, but com-bining the two yields superior quality. Although we are engaged in the task of semantic parsing, we have not yet discussed the use of any semantic resources. In this section we rectify that omission. 6.1 Rules from Numerical Quantities and While the majority of string-to-tree rules in SBMT systems are extracted from aligned parallel data, it is common practice to dynamically generate addi-tional rules to handle the translation of dates and numerical quantities, as these follow common pat-terns and are easily detected at decode-time. We follow this practice here, and additionally detect person names at decode-time using the Stanford Named Entity Recognizer (Finkel et al., 2005). We use cased, tokenized source data to build the decode-time rules. We add indicator features to these rules so that our tuning methods can decide how favorable the resources are. We leave as fu-ture work the incorporation of named-entity rules for other classes, since most available named-entity recognition beyond person names is at a granularity level that is incompatible with AMR (e.g. we can recognize  X  X ocation X  but not distin-guish between  X  X ity X  and  X  X ountry X ). 6.2 Hierarchical Semantic Categories In order to further generalize our rules, we mod-ify our training data AMRs once more, this time replacing the identity preterminals over concepts Figure 5: Final modification of the AMR data; se-mantically clustered preterminal labels are added to concepts. with preterminals designed to enhance the appli-cability of our rules in semantically similar con-texts. For each concept c expressed in AMR, we consult WordNet (Fellbaum, 1998) and a curated set of gazetteers and vocabulary lists to identify a hierarchy of increasingly general semantic cat-egories that describe the concept. So as not to be overwhelmed by the many fine-grained distinc-tions present in WordNet, we pre-select around 100 salient semantic categories from the WordNet ontology. When traversing the WordNet hierarchy, counts when paths meet. For each selected se-mantic category s encountered in the traversal, we calculate a weight by dividing the propagated ex-ample count for c at s by the frequency s was pro-posed over all AMR concepts. We then assign c to the highest scoring semantic category s . An ex-ample calculation for the concept computer is shown in Figure 7.

We apply semantic categories to our data as replacements for identity preterminals of concepts. This leads to more general, more widely-applicable rules. For example, with this transformation, we can parse correctly not only contexts in which  X  X oldiers die, X  but also contexts in which other kinds of  X  X killed workers die. X  Figure 5 shows the addition of semantic preterminals to the tree from Figure 3i. We also incorporate semantic categories into the AMR LM. For concept c , let s c be the semantic category of c . Then we reformu-Knight, 2013).
 Table 4: Lexical conversions to AMRese form due to the morphological normalization rules de-scribed in Section 6.3. late P P ( s c |  X  l i , s  X  c P ( STOP | s c , c ) , where P P ( l | s c , c )  X  P 6.3 Morphological Normalization While we rely heavily on the relationships be-tween words in-text and concept nodes expressed in parallel training data, we find this is not suf-ficient for complete coverage. Thus we also in-clude a run-time module that generates AMRese base forms at the lexical level, expressing relation-ships such as those depicted in Table 4. We build these dictionary rules using three resources: 1. An inflectional morphological normalizing 2. Lists of derivational mappings (e.g.  X  X uietly X  3. PropBank framesets, which we use, e.g., to 6.4 Semantically informed Rule-based For our final incorporation of semantic resources we revisit the English-to-AMR alignments used to extract rules. As an alternative to the unsupervised approach of Pourdamghani et al. (2014), we build alignments by taking a linguistically-aware, super-vised heuristic approach to alignment:
First, we generate a large number of poten-tial links between English and AMR. We attempt to link English and AMR tokens after conver-sion through resources such as a morphological analyzer, a list of 3,235 pertainym pairs (e.g. adj- X  X ubernatorial X   X  noun- X  X overnor X ), a list of 2,444 adverb/adjective pairs (e.g.  X  X umbly X   X   X  X umble X ), a list of 2,076 negative polarity pairs (e.g.  X  X llegal X   X   X  X egal X ), and a list of 2,794 known English-AMR transformational relationships (e.g.  X  X sleep X   X  sleep-01 ,  X  X dvertiser X   X  person ARG0-of advertise-01 ,  X  X reenwich Mean Time X   X  GMT ). These links are then culled based on context and AMR structure. For example, in the sentence  X  X he big fish ate the little fish, X  ini-Figure 6: B LEU of AMRese and Smatch correlate closely when tuning. tially both English  X  X ish X  are aligned to both AMR fish . However, based on the context of  X  X ig X  and  X  X ittle X  the spurious links are removed.

In our experiments we explore both replacing the unsupervised alignments of Pourdamghani et al. (2014) with these alignments and concatenat-ing the two alignment sets together, essentially doubling the size of the training corpus. Because the different alignments yield different target-side tree reorderings, it is necessary to build separate ing both alignment sets together, we also use both AMRese language models simultaneously. We would like to tune our feature weights to max-imize Smatch directly. However, a very con-venient alternative is to compare the AMRese yields of candidate AMR parses to those of ref-erence AMRese strings, using a B LEU objective and forest-based MIRA (Chiang et al., 2009). Fig-ure 6 shows that MIRA tuning with B LEU over AMRese tracks closely with Smatch. Note that, for experiments using reordered AMR trees, this requires obtaining similarly permuted reference tuning AMRese and hence requires alignments on the development corpus. When using unsuper-vised alignments we may simply run inference on the trained alignment model to obtain devel-opment alignments. The rule-based aligner runs one sentence at a time and can be employed on the development corpus. When using both sets of alignments, each approach X  X  AMRese is used as Figure 7: WordNet hierarchy for computer . Pre-selected salient WordNet categories are boxed. Smoothed sense counts are propagated up the hierarchy and re-combined at join points. Scores are calculated by dividing propagated sense count by count of the category X  X  prevalence over the set of AMR concepts. The double box indicates the selection of artefact as the category label for computer . a development reference (i.e. each development sentence has two possible reference translations). Our AMR parser X  X  performance is shown in Ta-ble 3. We progressively show the incremental im-provements and compare to the systems of Flani-gan et al. (2014) and Wang et al. (2015). Purely transforming AMR data into a form that is com-patible with the SBMT pipeline yields suboptimal results, but by adding role-based restructuring, re-labeling, and reordering, as described in Section 4, we are able to surpass Flanigan et al. (2014). Adding an AMR LM and semantic resources in-creases scores further, outperforming Wang et al. (2015). Rule-based alignments are an improve-ment upon unsupervised alignments, but concate-nating the two alignments is even better. We com-pare rule set sizes of the various systems in Ta-ble 5; initially we improve the rule set by remov-ing numerous overly brittle rules but then succes-sive changes progressively add useful rules. The parser is available for public download and use at http://amr.isi.edu . The first work that addressed AMR parsing was that of Flanigan et al. (2014). In that work, mul-tiple discriminatively trained models are used to identify individual concept instances and then a minimum spanning tree algorithm connects the concepts. That work was extended and improved upon by Werling et al. (2015). Recent work by Wang et al. (2015) also uses a two-pass approach; dependency parses are modified by a tree-walking algorithm that adds edge labels and restructures to resolve discrepancies between dependency stan-dards and AMR X  X  specification. In contrast to these works, we use a single-pass approach and re-use existing machine translation architecture, adapting to the AMR parsing task by modify-ing training data and adding lightweight AMR-specific features.

Several other recent works have used a ma-chine translation approach to semantic parsing, but all have been applied to domain data that is much narrower and an order of magnitude smaller than that of AMR, primarily the Geoquery cor-pus (Zelle and Mooney, 1996). The WASP sys-tem of Wong and Mooney (2006) uses hierarchi-cal SMT techniques and does not apply semantic-specific improvements; its extension (Wong and Mooney, 2007) incorporates a target-side reorder-ing component much like the one presented in Section 4.4. Jones et al. (2012a) cast semantic parsing as an instance of hyperedge replacement grammar transduction; like this work they use an IBM model-influenced alignment algorithm and a GHKM-based extraction algorithm. Andreas et al. (2013) use phrase-based and hierarchical SMT techniques on Geoquery. Like this work, they per-form a transformation of the input semantic repre-sentation so that it is amenable to use in an exist-ing machine translation system. However, they are unable to reach the state of the art in performance. Li et al. (2013) directly address GHKM X  X  word-to-terminal alignment requirement by extending that algorithm to handle word-to-node alignment.
Our SBMT system is grounded in the theory of tree transducers, which were applied to the task of semantic parsing by Jones et al. (2011; 2012b).
Semantic parsing in general and AMR parsing specifically can be considered a subsumption of many semantic resolution sub-tasks, e.g. named entity recognition (Nadeau and Sekine, 2007), se-mantic role labeling (Gildea and Jurafsky, 2002), word sense disambiguation (Navigli, 2009) and re-lation finding (Bach and Badaskar, 2007). By restructuring our AMRs we are able to convert a sophisticated SBMT engine into a baseline se-mantic parser with little additional effort. By fur-ther restructuring our data to appropriately model the behavior we want to capture, we are able to rapidly achieve state-of-the-art results. Finally, by incorporating novel language models and external semantic resources, we are able to increase quality even more. This is not the last word on AMR pars-ing, as fortunately, machine translation technology provides more low-hanging fruit to pursue.
 Thanks to Julian Schamper and Allen Schmaltz for early attempts at this problem. This work was sponsored by DARPA DEFT (FA8750-13-2-0045), DARPA BOLT (HR0011-12-C-0014), and DARPA Big Mechanism (W911NF-14-1-0364).
