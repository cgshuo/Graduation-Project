 In this paper we consider the problem of using the block structure of a Web page to improve ranking results when searching for information on Web sites. Given the block structure of the Web pages as input, we propose a method for computing the importance of each block (in the form of block weights) in a Web collection. As we show through experiments, the deployment of our method may allow a significant improvement in the quality of search results. We ran experiments to compare the quality of search results when using our method to the quality obtained when us-ing no structure information. When compared to a ranking method that considered pages as monolithic units, our block-based ranking method led to improvements in the quality of search results in experiments with two sites with heteroge-neous structures. Further, our method does not increase the cost of processing queries when compared to the systems us-ing no structural information.
 H.3.3 [ Information Storage and Retrieval: Informa-tion Search and Retrieval ]: Retrieval models Algorithms, Experimentation Page Segmentation, Block Importance, Block Class
Web pages can be sub-divided into non-overlapping struc-tural blocks, which can be easily identified by manual inspec-tion or by automatic analysis of their content [4, 13]. Pre-vious work has shown that block information can be used to improve the representation of Web pages in information retrieval tasks such as searching and classification [1, 5, 16]. In [1] the authors show that such structure is particularly useful to the so called data-intensive Web sites, such as dig-ital libraries, Web forums, news Web sites, electronic cat-alogs, or institutional sites, whose main focus is providing access to a large quantity of data and services [7]. In these sites, search is an important way of accessing data, given the large amount of information usually provided to their users.
Information retrieval systems assign a weight to the oc-currence of each term in a page (or document), creating a term-document matrix. This matrix is one of the key pieces of evidence used for computing similarities between queries and documents (other key pieces of evidence come from in-formation provided by link analysis, such as the page rank or authority of a Web page). In the case of a collection of Web pages restricted to a particular domain, composed of pages in a single site or in a pre-selected set of sites, link-analysis is not of great help and the term-document matrix becomes the central foundation for the ranking system. In this context, we argue that the block structure of Web pages may provide valuable information for attaining better rank-ing. This can be accomplished by recognizing that pages are frequently composed of non-overlapping blocks and by as-signing weights to each block (which can then be factored in into the ranking formula). Our basic assumption is that the occurrence of a term on each block has itself an importance factor in the ranking process that should be taken into ac-count when computing the overall weight of a term in a page.
For instance, there are types of blocks , such as menus, advertisements, and blocks containing information not asso-ciated with the main topics or themes of a Web page, whose contents should be considered of less importance in the rank-ing formula. This is one piece of information that we can factor into the ranking by applying the approach we propose here. Further, we argue that each block of a Web page has a different degree of usefulness for IR tasks, which we call the importance value of the block. Therefore, we assume that we can assign to each block b of a Web page p an impor-tance factor value i b , which estimates the importance of b for performing retrieval tasks.

Experiments presented in [1] indicate that a manual as-signment of importance weights for blocks may result in an improvement on the quality of the search results. However, the task of manually assigning such weights requires spe-cialized knowledge about the ranking function adopted in the search system, as well as a complex and expensive hu-man effort to evaluate the relative importance of all block types found in the target Web collection. For Web collec-tions containing a large number of block types, which is a common scenario, this manual assignment of block impor-tance may be unfeasible in practice. For instance, in IG , www.ig.com.br , one of the sites adopted in our experiments, we found 242 different block types in the whole collection. In CN N , www.cnn.com , the second site adopted in the ex-periments, we found 101 different block types. These two examples illustrate how complex would be to manually as-signment weights to the blocks of these two sites. Notice that the decision about an appropriate weight assignment for each block type found in a Web collection would be dif-ficult, even in sites with a small number of block types. For instance, it is difficult to determine in which degree the title of a news is more important than its body text in a Web site.
To avoid these and other practical problems, we propose a new method for automatically computing the block im-portance factors in a Web collection. Our method uses sta-tistical information available in the collection to assign an importance value to all blocks of each page found in the col-lection. It presents the advantage of not requiring a learning process nor any other type of manual intervention, as it hap-pens in previous work [1, 16].

We performed experiments with two distinct Web sites to illustrate the advantages of our method. In both cases, the results obtained indicate that the block importance values found by our method were capable of improving the quality of the results. For IG , one of the Web sites experimented, an improvement from 0.77 to 0.84 in the bpref-10 [3] scores was achieved when compared to a system that did not take the division of a page in blocks into account. For CN N , the second Web site, the system resulted in an improvement from 0.66 to 0.75 in bpref-10.

The remaining of this paper is organized as follows. Sec-tion 2 covers related work. Section 3 presents basic defini-tions required to better understand the problem addressed here. In Section 4 we present two alternative methods for computing the importance of blocks in order to improve search results in Web search tasks. In Section 5 we describe experiments that illustrate the process of computing block importance in a Web site and show how such process may be useful for improving searching results. Finally, Section 6 presents our conclusions.
Before computing the importance of block types, it is nec-essary to identify blocks occurring in a Web collection. As suggested in [1], this identification can be performed by the Web site designer. Such a task can be considered a sim-ple one when compared to the weight assignment problem. The process of analyzing Web sites to detect the structural division of its pages in logical blocks is by itself a research problem, with some solutions proposed in literature [10, 8, 13, 4, 2] and that we do not address here. One of the best solutions found in literature to this problem is presented in Cai et al [4], where the authors propose a Vision-based Page Segmentation technique (VIPS) to divide a Web page into blocks. We here have used the VIPS to guide the segmenta-tion of pages. However, other techniques or even a manual segmentation could be adopted, since we assume that the division of each Web pages of a site into logical blocks is provided as input for our work.

Several authors have presented methods that aim at sub-dividing Web pages into distinct blocks of information and use such information to improve the representation of these pages in information retrieval systems. For instance, Bar-Yossef et al. [2] proposed a mechanism for segmenting Web pages into blocks, represented there as Pagelets [9], and then classifying them into useful or noise content. In their work, authors considered any information present on templates of the pages as noise and proposed to disregard such infor-mation when processing the content of Web pages. Their experiments show that template elimination can improve the precision of the search results. Other template removal methods are proposed in literature [17, 19].

Experimental results in all the previous work show that the template removal methods can improve results of clus-tering, classification and search of pages on the Web. Since the removal of templates can be considered as an structural information, we included a search system that does not index template contents as one of the baselines in our experiments.
In [18] a technique is proposed for eliminating informa-tion considered as noisy from pages of a given Web site. Distinctly, in our method, we are interested in assigning a degree of importance for page blocks. Their method relies on the notion that noisy blocks usually share some common content and formatting styles, while the main content blocks of pages are often diverse. A similar notion is also considered as part of criteria we rely on to determine block importance in our method. Most of the noise information removed by this method in fact belong to templates, and in [17] it is shown that other template removal methods produce sim-ilar, or even better improvements in information retrieval tasks.

In Cai et al [5], authors proposed a method for taking ad-vantage of the segmentation of pages into blocks in a search task. They present a ranking strategy where the blocks are processed as semantic passages and then apply previ-ously proposed passage level ranking strategies [6] to eval-uate the possible improvements obtained in ranking quality with their method. Two important conclusions are: the VIPS [4] produced results superior to other previously stud-ied passage segmentation, such as paragraphs or fixed size windows. Further, the best ranking was obtained with a combination of similarity scores that considers the whole documents as units with the ones that considers the blocks as units. We adopted this ranking strategy as one of the baselines in our experiments.

In Song et al. [16] a vision based block division is also adopted. The authors use such a division to compute an importance rank of blocks found in a page through a learn-ing approach. Spatial features, such as position and size of blocks, and content features, such as the number of im-ages and links on each block, are extracted from each block. Then, two learning algorithms, neural network and SVM, are used to rank the blocks according to their importance. We here propose a more generic goal of assigning an impor-tance degree to each block, instead of just ranking them. Further, our proposed method can also be used to also rank blocks if necessary, with the advantage of not requiring a training phase.
In this Section we present some basic definitions which are useful for better characterizing the problem addressed here and for supporting the explanation of our method.
Definition 1 . We model a Web page as a set of non-overlapping logical blocks B = { b 1 , ..., b n } , each of them composed of a pair ( l, c ), where l is the label of the block, represented as an string, and c is its textual content, which is a portion of the text of the Web page. Such portion is not present in any other block of the page, thus there is no overlap between block contents.

The division of a Web page in a set of non-overlapping blocks is performed according to the perception of users about the possible logical division of a page. We adopted the VIPS segmentation method described in Cai et al [4] to guide the segmentation of the Web pages into blocks. We chose VIPS because it produces results close to our percep-tion of how the pages of the experimented sites could be seg-mented. Since VIPS does not assign a label to each block, we adopted the path of the block in the DOM tree of the page segmented as its label. The DOM tree is a tree that repre-sents the structure of a Web page and is usually adopted in research related to the analysis of page structure [13, 10].
Based on definition 1, we can now define the concept of structural equivalence of two pages as follows:
Definition 2 . Two Web pages p and q are structurally equivalent if: (1) they have the same number of blocks, (2) for every block in p there is a block in q with the same label, and (3) for every block in q there is a block in p with the same label.

Figure 1 illustrates an example of a set of pages which are structurally equivalent. It is important to remember that the labels of blocks in such page are assigned either automatically or manually and that these labels are given as input to our methods. The DOM tree path heuristic we adopted here to derive block labels produces the same labels when processing different Web pages that share the same structure. This property allows us to automatically find structural equivalences between Web pages segmented with VIPS.

Definition 3 . A page class is a set of pages in which all of them are structurally equivalent.

Definition 4 . A block class is a set of blocks { b l,p , b l,z . . . } that belong to distinct pages of the same class in a Web site and that share the same label, i.e, all the blocks of a class have exactly the same label.

Figure 1 shows an example where the pages A, B and C are structurally equivalent, thus composing a page class. In this example, the blocks labeled as 1 in the three pages com-pose a block class. Other block classes can be derived simi-larly.

Block class is a key concept in our work. We here make the assumption that statistics about the block class are useful for obtaining information that could not be obtained when analyzing blocks as individual units, as it is proposed in [5].
In this section we describe our method for computing the importance weight of each block of each Web page in a collec-tion. We compute block importances by analyzing the block classes found in the collection. Thus, given a block class C , we associate an unique importance value to all blocks in C . We call this unique importance value as the block class importance value of C .

Our method uses statistics about the occurrence of terms in each class of pages found in the collection in order to compute the block class importance values. We adopted ideas similar to the ones proposed in the successful vector space model [15], to compute the weight of occurrence of each term in each block class. Then, we use these weights to compute the block class importance. Two new concepts for computing such importance, derived from the vector space model, are introduced below.

Definition 5 Given a block class C = { b 1 , ..., b B C } con-taining B C elements and a term t that occurs in at least one block of C , the Inverse Class Frequency of a Term t in C is defined as where B ( t,C ) is the number of blocks of C in which t oc-curs.

Notice that the ICF is similar to the IDF concept but considers each block class as a separate  X  X ollection of docu-ments X . As in the IDF , the intuition behind the ICF values is to quantify the significance is the occurrence of a term on a block of a given block class.
 Definition 6 The Average ICF of a block class C , AICF ( C ), is the average value of ICF of all terms that occur in C and is computed by the formula: where V C is the size of vocabulary of class C i.e., the number of distinct terms that occurs at least once in C .
As the IDF measure, ICF is a measure of the amount of information that carries an occurrence of t in the block class C . It assigns a high value for terms that are rare in a block class and is low for terms that are common. If all the blocks of a block class contain equal or very similar content, the ICF of terms that block class will be low. Thus, when we compute the AICF ( C ), we obtain a measure of how frequent is the content of different blocks in the block class. A similar notion was used before in literature to find noise on Web pages [18]. This measure is useful when computing the block class importance value. We argue that block classes that have too much repetition of content (low AICF ) are less important, and block classes where the blocks have more diverse content, which increases AICF , are more important. For instance, block classes that contain menus, where all the blocks tend to have equal content, will have AICF values closer to zero.

Another new concept we introduced to measure the im-portance of a block class is the block class spread . We argue that the importance weight of a block class should also be account for the similarity of each of its blocks b to the re-maining blocks in the page b belongs to. Our rationale is that blocks which have terms in common with other blocks of the same page tend to be related to the subject of the page.

Using this intuition, we propose another measure for tak-ing into account when computing the importance value of a block class. We start by defining the term spread of a term t in a page p ( termSpread ( t, p )) as the number of blocks in page p in which t occurs.

Our assumption is that, given a document d and a term t , the greater the number of blocks of d where t occurs, the more related t is to the main subject of d . We use the term spread measure to compute the block spread as the average spread of all terms in that block, using the following formula: where P ( b ) is the document that block b belongs to and N umT erms ( b ) is the total number of terms in b .
The blockSpread indicates how related is the content of a block to the remaining blocks of the page and will be used to compute the degree of spread of the content of a block class in the collection. We compute the spread of a block class as the average block spread of all blocks belonging to it, according to the formula below.
 Definition 7 The block class spread of a block class C , Spread ( C ), is a function that expresses how the blocks of a block class are related to the remaining blocks of the pages in the collection. Given a block class C , the block class spread is computed as where B C is the number of blocks in block class C .
As an example of the intuition behind the concept of block class spread, most of the terms found in the title of news usually have a high number of terms that appear on the news text body. Thus, the spread of a block class that contain titles of news would be considered high, since it contains a high block class spread. Blocks containing the body text of the news usually will have most of their terms not present on the remaining blocks of the page, but some of them will be present. Thus these blocks probably should produce a block class with intermediate spread value according to our definition. Finally, blocks containing menus or advertising banners are examples of blocks that may have no terms in common with the remaining blocks of the documents they belong to. Thus, such types of block classes tend to have low spread values.

Finally, we use the values of block class spread and aver-age inverse block class frequency to compute the block class importance of each block class. We propose to compute such importance as the product of the two measures: classImportance ( C ) = classSpread ( C )  X  AICF ( C ) (5)
A practical problem that could arise when applying the proposed block importance computation method is that some page classes found in a Web site may have a small number of instances (a low number of structuraly equivalent pages). The number of elements may be not sufficient to allow an ac-curate statistical information about the importance of blocks present on these page classes. For instance, when dealing with a page class with only 1 or 2 elements, any speculation about how common is a term in a block class of such a page class will probably not work properly.

We are interested here in data-intensive Web sites, where the large number of pages and the regularity of page struc-tures minimizes this problem. In data-intensive Web sites it is expected a large number of pages sharing a few page structures. However, still some page classes may have a small number of elements in these sites and we need an al-ternative solution to deal with these pages.

Our solution to this problem is to assign to all block classes of these pages classes an equal weight value. Such strategy is similar to consider that pages of small classes have no struc-tural information. The assignment of equal weights to their blocks is equivalent to process these pages as plain texts, since we in fact maintain their usual plain text representa-tion, considering all their blocks as equal in the information retrieval model. We have decided to adopt this alternative weight computation only to pages classes containing one el-ement, since in such cases our method would not produce meaningful results, assigning all blocks a weight zero. For the remaining page classes, we directly applied our block importance method.
To assert the usefulness of our method, we performed search experiments on real Web sites and evaluated the im-pact of computing block importance on the quality of results.
The information on block importance information was in-troduced in the computation of the ranking by using the method described in [14], which presents an adaptation of the BM25 ranking formula for dealing with documents that have multiple fields (e.g., title, body, anchor text) with dif-ferent weights.

In that work, the authors present a discussion of several alternative weight combination and concluded that the best alternative is to adjust the term frequency of each document, in accordance with the weights of the fields in which these terms occur. In this scheme, a structured document with a title whose weight value is two is mapped to an unstructured document with the title content repeated twice (in fact, its frequency could by multiplied by any real number). This more verbose unstructured document is then ranked in the usual way. In our case, blocks will play the role of fields. We will use this method to evaluate the quality of the block important values assigned in our method. Figure 2: Number of elements in each of the 17 page classes found on the IG web site. We adopted two document collections in our experiments: IG and CNN. From IG , one of the largest Brazilian Web portals, we crawled 33561 pages, containing 17 page classes and 241 different block classes, including a news site, a recipe site and a forum site. The set of queries used was extracted from a log of queries submitted to the IG Web search ser-vice. From this log, we selected the 45 most frequent queries. Relevance assessments for these queries were made by 85 volunteers from 5 different Brazilian universities. A pooling method [11, 12] was used to collect these judgments. The relevance evaluations were gathered by presenting the fol-lowing question to the volunteers:  X  X f you were writing a report on the subject of the proposed theme (a query ex-tracted from the log), this document would be useful for writing your report? X . In case of a positive answer of the the volunteer, the document was classified as relevant to the query.

Figure 2 shows the distribution of the number of elements on the 17 page classes found on IG . As it can be seem, most of the pages are grouped into classes with a large num-ber of elements. This is an expected characteristic on data-intensive Web sites.
 The second collection was obtained by crawling the CNN Web site. It contains 17241 Web pages with a total of 8 page classes and 101 block classes. Since we did not have enough information about queries submitted to the CNN site, we asked the users to perform spontaneous queries and evaluate the answers obtained. The queries in this site were formulated by 45 users who were able to read and under-stand English texts. We had a total of 45 queries, and, for each query submitted, we computed the union of the top 20 results of all the experimented methods and presented them in a random order to the users. Although these queries were not submitted to the CNN Web site, we believe they are useful to evaluate the impact of our method on the search task over the CNN collection.

Figure 3 shows the distribution of number of elements on the 9 page classes found on CNN. As in IG , there are a few classes. The small number of classes when compared to the number of pages in both collections is a characteristic expected in data-intensive Web sites, since these sites usu-ally use publishing tools and fixed templates to create large number of pages with similar structure.

To provide examples of how the Web sites were structured we present details of common structures found in the IG and CNN Web sites. The screenshot in Figure 4 shows a typical page of the first sample structure, extracted from the IG Figure 3: Number of elements in each of the 8 page classes found on the CNN web site. forum Web site. Below we list all block classes present in pages structurally equivalent to this sample page.
Another sample structure is shown in Figure 5, which shows a typical page of the CNN news Web site. We list below all block classes present in pages structurally equiva-lent to this second example page. Figure 4: A typical page present on the forum Web site of IG .
These two sets of classes illustrate only a small sample of the classes found in IG and CNN. The weights assigned to them will be used in our experiments to illustrate the results of the block importances automatically assigned by our method.
We adopted 3 ranking quality measures to evaluate the proposed methods. The first measure is the traditional mean average precision (MAP). The second is the p@10, which measures the amount of relevant documents in the top 10 answers provided on each system.

The third one is bpref-10 [3], a metric designed to com-pare information retrieval systems when only a partial set of the query answers is evaluated. The authors show that this measure is similar to MAP for collections where the full set of documents is evaluated and the relative compar-isons is kept stable when taking only partial results. This means that using bpref-10 with partially evaluated answers the conclusions about the relative performance of the sys-tems tends to be the same that would be obtained with the full evaluation of the answers. The main difference between Figure 5: A typical page present on the most com-mon class found in the CNN collection bpref-10 and other evaluation metrics is that it does not take the non-evaluated answers into account.

Bpref-10 is computed by the formula: where R is the number of documents judged as relevant, Irrel R ( r ) is the number of documents judged as irrelevant ranked higher than r among the top R + 10 documents judged as non-relevant retrieved by the system.

The reason for adopting these three measures is to provide a better insight about the differences in ranking quality ob-tained by our method when compared to the baseline. For each evaluation measure we applied a statistical test (t-test) to check the significance of the comparative results obtained.
We adopted 3 different baseline methods for asserting the impact of our method in the search results. The first adopts the BM25 using weight 1 for all blocks in all pages, which is similar to consider the collection as a plain text collection. This baseline method will be referred to as BM25 in our experiments.

In the second baseline method, we manually removed all the templates of the collection, and then applied the same plain text BM25 in the remaining content. This second base-line method will be referred to as BM25-NT (BM25 with no template). Experiments with the collection without the template content are important to check whether the im-provements achieved by the use of structural information are a consequence of the assignment of lower weights to tem-plates or not. Previous studies in literature advocate that the removal of templates may improve the results of search systems in Web collections.

The third baseline method an strategy proposed in [5] that computes two scores, one for the whole document, and another considering the blocks as units. The two scores are then combined by using the function: score ( d, q ) =  X  rank doc ( d, q ) + (1  X   X  ) rank bestblock where rank doc is the rank given by BM25 to the docu-ment d for query q and rank bestblock ( d, q ) is the rank given by BM25 to the best ranked block of d when considering blocks as retrieval units. The parameter  X  should be ad-justed through a training process, which is a disadvantage of this approach when compared to our method. We have performed experiments to find the best  X  value for each of the collections adopted in the experiments, the value was 0 . 5 for IG and 0 to CNN. The results in the experimental sec-tion adopt these best  X  values. This method will be referred in the experiments as CDB (Combined scores of document and best block). All the parameter settings in this case were equal to the ones used for BM25.

This approach was chosen because authors have experi-mented their method with a segmentation process similar to the ones adopted here, using VIPS, and also because it is an alternative strategy to take advantage of page structure.
Tables 1 shows the weights assigned by our method to the 8 example block classes of CNN. Our method assigned low weights to classes He (Head), FN (Footnote), Me (Menu) and LN (Latest News), which have content obviously not related to the main subject of the pages they belong to. On the other hand, the classes Ti (title) , SH (Story High-lights) and FT (Full text) received the higher weights in this order. This classification of blocks according to their importance would probably be easily derived by a human. However, it would be difficult, even for a specialist, to man-ually derive the weights obtained for each block. Further, it is important to notice that there are 101 block classes in this site and 241 in IG , which shows how difficult would be a manual assignment of weights for the blocks classes of these sites.

Table 2 presents the weights obtained for the block classes examples extracted from the IG web site. Again for this site, the weights assigned agree with what we expected. However, such class weights are even more difficult to be derived by a human. For instance, the decision of who is most important when comparing FT and TE is not trivial, which again rein-force the necessity of an automatic approach for the weight computation.

These examples are useful to illustrate that the weight assignment given by our method is meaningful to humans. However, the number of classes to analyze in the experi-ments is too high to report, since we have several hundred of classes. Further, this analysis is too subjective to eval-uate the quality of our method. The best alternative we found to evaluate the final result was to perform search ex-periments and check the impact of our method in the search task, which is our target application in this paper.
Table 3 shows the results obtained by our method and the three baselines when processing the queries submitted to the IG Web site. As it can be seem, our method achieved gain in all metrics considered. When using the metrics bpref-10 and p@10, the best baseline was the BM25-NT (BM25 with the collections indexed after removing templates). These results reinforce conclusions presented on previous work that tem-Table 3: Comparison of results obtained by our method (BI) to the baselines methofs. All the meth-ods were applied to the web site IG Table 4: Comparison of results obtained by our method (BI) to the baselines all the methods are applied to the Web site CNN plate removal may improve the quality of search results [17, 19]. For the MAP metric, the best baseline was CDB, which combines the rank of blocks with the rank of documents.
The overall gain in ranking quality of our method in IG web site when compared to the best baselines was 4.40% in P@10, 5.52% in MAP and 3.52% in bpref-10. When com-paring to the plain text representation of the documents (BM25) the improvements were quite high, being 9.00% in P@10, 7.72% in MAP and 8.65% in bpref-10. Applying t-test, we concluded that most of these gains were significant, with p &lt; 0 . 01. The exception is the comparison to BM25-NT for the P@10 metric, where the p value obtained was 0 . 022.

To check whether the final improvements were uniform in the sites of IG , we also studied the impact of the methods over the news, forum and recipe web sites of IG . The ex-periments in this case considered only relevant answers from each of the three web sites for each query. The improvements obtained by our method were similar to the ones obtained in the whole collection in the three cases, which lead us to conclude that the block importance weight assignment uni-formly improved the representation of pages in the three sites of IG.

Table 4 shows the results obtained when applying the methods to the CNN web site. Again, our method pro-duced superior results in all metrics, which indicates the use of block importance weights may significantly improve the quality of search results on web sites. The best baseline in CNN was CDB in all metrics and the gain achieved by our method were 14.28% in P@10, 9.93% in bpref-10 and 7.09% in MAP for the CNN web site. The higher improvements in P@10 and bpref-10 indicate our method obtains higher precision at the top of the ranking, which is an important feature in web search tasks. Applying t-test, we concluded that all these gains were significant, with p &lt; 0 . 01 in all cases.

Again the improvements were higher when comparing our method to the plain text representation of the documents. The results with the two Web sites show the potential im-provements that can be obtained when considering the struc-ture of documents in information retrieval models.
In this paper, we proposed a method to compute the im-portance of blocks in a Web page by means of the assignment of weights to classes of structurally similar blocks. We com-pute these weights based on the estimated importance of the class of the block in conducting the main subject or message of the Web pages where the blocks occur. The method uses two types of information to determine these block weights. One of them measures how diverse is the content of blocks in a class, adopting a metric we call AICF . More diversity implies in more relation to the main content of the pages and therefore higher weights. The second information cap-tures how spread are the terms of blocks of a class in their corresponding pages, adopting a metric we call classSpread . Blocks with terms spread also in other blocks have a higher chance of being more related to the main subject of their re-spective pages, implying also in higher weights. These two measures are then combined for determining the importance of blocks in a Web page.

Experiments with our method were performed in two rich and heterogeneous collections against three baselines. The first one did not consider any structural (block) information. The second one applied a template removal pre-processing. And the third one used a training process to optimize the combination of ranks when considering the whole documents and the blocks as retrieval units. These experiments have shown that our method consistently outperforms all base-lines in both collections, obtaining gains up to 14.28% in P@10, 9.93% in bpref-10 and 7.09% in MAP for the CNN web site.

We foresee some work ahead of us. Our model works only with flat block divisions in the Web page collection. As a future direction, we plan to expand our method to process Web page collections where the page structure is modelled as a hierarchy, including the possibility of nested blocks. Another future research direction is to create a method to automatically determine when the examples of pages in a page class are sufficient to a good estimation of the block importance weights on such class. This feature would allow us to apply our method to web collections of large scale web search engines, where the crawling process usually does not provide a good coverage of each site, and check the impact of our method in this new scenario.

Finally, we are also interested in studying the impact of the use of block importance information on other informa-tion retrieval tasks, such as clustering, filtering tasks and classification of documents. [1] K. Ahnizeret, D. Fernandes, J. M. B. Cavalcanti, E. S. [2] Z. Bar-Yossef and S. Rajagopalan. Template detection [3] C. Buckley and E. M. Voorhees. Retrieval evaluation [4] D. Cai, S. Yu, J. Wen, and W. Ma. Vips: a visionbased [5] D. Cai, S. Yu, J. Wen, and W. Ma. Block-based web [6] J. P. Callan. Passage-level evidence in document [7] S. Ceri, M. Matera, F. Rizzo, and V. Demalde. [8] S. Chakrabarti, M. Joshi, and V. Tawde. Enhanced [9] S. Chakrabarti. Integrating the document object [10] W. ching Wong and A. W.-C. Fu. Finding structure [11] D. Hawking and N. Craswell. Overview of TREC-7 [12] D. Hawking, N. Craswell, P. Thistlewaite, and [13] S.-H. Lin and J.-M. Ho. Discovering informative [14] S. Robertson, H. Zaragoza, and M. Taylor. Simple [15] G. Salton, A. Wong, and C. S. Yang. A vector space [16] R. Song, H. Liu, J.-R. Wen, and W.-Y. Ma. Learning [17] K. Vieira, A. S. da Silva, N. Pinto, E. S. de Moura, [18] L. Yi, B. Liu, and X. Li. Eliminating noisy information [19] X. Yin and W. S. Lee. Using link analysis to improve
