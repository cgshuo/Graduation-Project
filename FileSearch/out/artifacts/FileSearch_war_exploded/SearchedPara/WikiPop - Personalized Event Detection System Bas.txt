 In this paper, we describe WikiPop service, a system designed to detect significant increase of popularity of topics related to users X  interests. We exploit Wikipedia page view statistics to identify con-cepts with significant increase of the interest from the public. Daily, there are thousands of articles with increased popularity; thus, a personalization is in order to provide the user only with results re-lated to his/her interest. The WikiPop system allows a user to define a context by stating a set of Wikipedia articles describing topics of interest. The system is then able to search, for the given date, for popular topics related to the user defined context.
 Categories and Subject Descriptors: H.4 [Information Systems Applications]: Miscellaneous General Terms: Algorithms.
 Keywords: Wikipedia, recommendation, knowledge base.
Wikipedia has received large attention from the computer sci-ence research community in recent years. It has been used for solving natural language processing tasks, for enriching informa-tion retrieval systems, for ontology building, and for text mining tasks [3]. However, not only the textual content of Wikipedia is of interest. It has also a unique structure, where each article is dedicated to a single topic and articles are densely linked among themselves. In this work, we exploit the the Wikipedia link struc-ture together with its page view statistics in order to make possible a personalized news detection system. By analyzing the trends in the development of page view statistics (number of visits of distinct Wikipedia pages per day), we identify concepts with significantly increased popularity for a given time period. Our hypothesis is that events described in public sources trigger an increase in the num-ber of visits of Wikipedia articles corresponding to the concepts related to those events. Several existing services provide infor-mation on which Wikipedia articles received the highest increase in page views for a given date. We, on the other hand, want to identify all articles (concepts) that received a significant increase in page views and present to the user those that are related to his/her interests (user defined context). To identify concepts related to a given context, we use the SA technique over the Wikipedia link graph. The main challenge is how to pre-process the Wikipedia link graph and assign weights to its edges, so that the SA algo-rithm can retrieve semantically related concepts. The main con-tribution of this work lies in the proposed edge weighting strategy histogram of page views. In addition, if the popularity increase of a Wikipedia article is also supported by high page views, it is often easy to identify possible source causing increased interest in particular Wikipedia articles (e.g. by using a web search engine, composing the query with the article title and the given date). As the number of article page views gets lower, the success rate drops, even thought the relative increase is high.

In the following, we provide an overview of our approach to the retrieval of topics related to the given context. The context is a set of Wikipedia articles describing topics of user interest. We use the Wikipedia link graph as a knowledge base and the Spreading Activation (SA) algorithm as a method to identify topics related to a context (SA is a method for associative retrieval from a graph data structure [2]). We perform the SA algorithm using concepts from the given context as initially activated nodes. The result of this operation is a set of concepts related to the given context. The final result is the intersection of the set of concepts with in-creased popularity for a given date and the set of concepts related to defined context (obtained by SA). A straightforward approach, in which the SA is applied on the Wikipedia link graph with con-stant weights on the edges, produces very large result sets. This is because the Wikipedia link graph is a small-world network with power-law degree distribution and small average distance between nodes. It contains hub nodes with very high indegrees (which usu-ally corresponds to very general topics), through which the activa-tion is spread to a large portion of the network. Thus, we would like to use weights expressing a strength of a semantic relationship between linked concepts. We have proposed a weighting method which has two components. The first function is Indegree Square Ratio (ISR) [1], designed to overcome the problem with the wide spread of the activation through the hub nodes. Let indegree ( i ) be the indegree of the node i . ISR can be defined as follows: if indegree ( i ) &gt; indegree ( j ) then ISR ( e ( i; j )) = 1 , other-weighting method is intended to provide the information on the se-mantic relatedness between topics. It is based on the observation that semantically related articles have similar development trends in page views in the periods of the peak popularity. We argue that this is caused by following reasons: 1) articles targeted by a hyper-link from an article with increased popularity are likely to obtain an increase in page views, and 2) when a topic receives a hight at-tention, the public tends to look for information on related topics as well. An example is depicted in Figure 2, where we can see the correlation in page view development for the topics a (topic  X  X alloween X ) and b (topic  X  X ombie X ) in the selected period (around the Halloween day), while the third histogram (representing topic Linux) shows different trend in page views development. To mea-sures the relatedness of topics, we define Popularity Development Correlation (PDC) function. For a link e ( i; j ) between concepts i and j PDC is computed as the Pearson correlation coefficient for the values of page views of i and j for 5 days around the peak popularity of i (PDC is zero iff Pearson correlation &lt; 0). The final weight W ( e ( i; j )) = ISR ( e ( i; j )) P DC ( e ( i; j )) . The following experiment was set up to evaluate the weighting and re-trieval of related concepts: from a randomly chosen concept with increased popularity we did a two-hops random walk following the links that can be found in the first paragraph of related Wikipedia articles leading to the node with the same or lower order of mag-nitude of indegree. From the node where random walk ended, we performed SA algorithm on the link graph with uniform weights on edges and compared it with results of SA algorithm on the link graph weighted by the proposed method. The experiment showed that results obtained on the weighted network were in size 18% of
