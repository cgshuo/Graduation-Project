
Selecting the best document from a set (i.e. Best docu-ment selection) is a common problem with many real world applications. On a news commenting web site like Digg.com, a major task is to promote one comment onto a highly-visible sidebar to entice visitors to participate in the discussion. On a web search engine like Google, a widely used functional-ity is the  X  X  X  X  feeling lucky X  button that leads user directly to the highest ranked URL. On a content web site like ny-times.com, a single ad needs to be selected and displayed on the banner for each visitor.

Best document selection is not a well studied problem by itself. One may think we can just cast it as a binary classi-fication problem [1]. However, this approach lacks the abil-ity to distinguish between multivariate levels of quality of the information. Another simple solution is just treat it as a ranking problem and use existing ranking algorithms to rank all documents (e.g. [2, 4], etc.). Then we can select only the first element from the sorted list. However, because ranking models optimize for all ranks, the model may sacri-fice accuracy of the top rank for the sake of overall accuracy. This is an unnecessary trade-off.

We describe an alternative approach to handle the best document selection problem. We do this by first defining an appropriate objective function for the domain, then create a boosting algorithm that explicitly targets this function. Be-cause of the comparative simplicity of the objective function and the special characteristics of the best document selection problem, we can use a stronger and tighter approximation to optimize the objective function than existing approximated ranking solutions. Based on experiments on a benchmark retrieval data set and Digg.com news commenting data set, we find that even a simple algorithm built for this specific problem gives better results than baseline algorithms that were designed for the more complicated ranking tasks. Categories and Subject Descriptors: H.3.3 [Informa-tion Search and Retrieval]: Retrieval models General Terms: Algorithms, Experimentation, Measure-ment Keywords: Best Document Selection, Boosting, Learning to Rank
For the task of selecting the best document(s), we mea-sure the utility as the average ratio between the selected document X  X  relevance score and the best one:
U = 1 | Q | where Q is the set of queries, and r k,s and r k,o are the rele-vance scores of the s elected and o ptimal documents for the k th query, respectively. d k,i is a i th candidate document for the k th query, and r k,i is its relevance score. [] is the Iverson bracket that returns 1 for when the condition in the bracket is true, 0 otherwise. This measure is equivalent to Winner Take All (WTA), NDCG@1, and Precision@1 for binary labels. Otherwise, U can be seen as a multivariate case of Precision@1 as well as a non-exponentiated version of NDCG@1.
The utility measure in Equation 1 can not be optimized easily using gradient descent methods, since it is not dif-ferentiable. Similar to what people has done for optimiz-ing ranking measures such as MAP or NDCG, we find an approximate solution by constructing a new approximate objective function that is differentiable. To do so, we ap-proximate the Iverson bracket [] with a softmax function, which is commonly used in machine learning and statistics, for mathematical convenience. Thus the approximated ob-jective function is:
M ( ~ H ) = 1 where H is the model (e.g. learner or hypothesis) to be learned, which can predict a relevance score for each doc-ument query pair, j is the index for candidate documents, H k,i is the retrieval score of document i for query k es-timated by hypothesis H .  X  is a coefficient that controls the tightness of the approximation. Note that the function approaches the indicator as  X  approaches infinite. l 2 introduced as a regularizer to control model complexity and avoid the overfitting problem, and  X  is a pre-set parameter that controls the degree of regularization.
The approximated objective function in Equation 2 can be optimized via gradient boosting. We employ a stage-wise gradient technique `a la AdaBoost [3]. Its overview is presented in Fig. 1. We call this algorithm CommentBoost or CBoost@1. It is a fast, list-wise boosting algorithm for best document selection.

CBoost@1 alternates between finding the gradient vector of the objective function given the current hypothesis, find-ing a weak learner that correlates well with the gradient vector, then finding an appropriate alpha coefficient. 2 If bi-nary classifiers are used as weak learners, then this results in setting each label as y k,i = sign (  X   X  X  that may output real values (e.g. point-wise rankers) can be accepted as long as it is well correlated with the gradient.
The algorithm is tested on a user comments data set col-lected from Digg.com, where the best document selection problem is well motivated. Labels are gathered as the com-munity X  X  vote on each comment, and features are gathered from various lexical tests (e.g. word count, SMOG scores) as well as user profiles (account age, friend counts, etc.). Features are then query-level normalized. We also com-pared our algorithm against previously published baselines on the LETOR3.0 data set 3 . For CBoost@1, we use decision stumps as our weak learners and selected the best parame-ters as rated against the validation set. For the purpose of comparison, we report NDCG figures instead of U on the Letor3.0 data set, and we report U on the Digg.com data set.

On both data sets, we see across-the-board improvements for NDCG@1 with our method. For NDCG@2 and beyond, our algorithm is comparable to the baselines, however, rarely improving upon them. On the Digg.com data set, our ap-proach yields a utility score of 0.393, much better than a tuned svm rank[4] (0.365).
This paper studies the problem of best document selection using a machine learning algorithm. Instead of using exist-ing ranking algorithms, we propose a boosting algorithm that optimizes explicitly for the top rank. The strength of this approach is that it greatly simplifies the required ob-jective function and allows us to use a tighter, more accu-rate approximation than before. We also demonstrate that our algorithm does out-performs a number of baselines on a benchmark ranking data set LETOR3.0 and a new Digg.com data set where best document selection problem is well mo-tivated. As part of future work, we plan on investigating scalable, distributed best document selection algorithms. [1] Eugene Agichtein, Carlos Castillo, Debora Donato, [2] Yoav Freund, Raj Iyer, Robert E. Schapire, and Yoram [3] Yoav Freund and Robert E. Schapire. A [4] Thorsten Joachims. Optimizing search engines using [5] Ruslan Salakhutdinov, Sam Roweis, and Zoubin
