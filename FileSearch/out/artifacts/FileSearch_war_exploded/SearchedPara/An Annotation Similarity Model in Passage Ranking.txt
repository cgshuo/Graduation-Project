 State-of-the-art question answering (QA) systems employ passage retrieval based on bag-of-words similarity models with respect to a query and a passage. We propose a com-bination of a traditional bag-of-words similarity model and an annotation similarity model to improve passage ranking. The proposed annotation similarity model is generic enough to process annotations of arbitrary types. Historical fact val-idation is a subtask to determine whether a given sentence tells us historically correct information, which is important for a QA task on world history. Experimental results show that the combined model gains up to 7.7% and 4.2% im-provements in historical fact validation in terms of precision at rank 1 and mean reciprocal rank, respectively.
 H.3.3 [ Information Storage and Retrieval ]: Informa-tion Search and Retrieval; H.3.4 [ Information Storage and Retrieval ]: Systems and Software; J.m [ Computer Applications ]: Miscellaneous passage retrieval; question answering; text annotation
Passage retrieval is a core component for question answer-ing (QA) systems [18, 5, 6, 10]. Many passage retrieval approaches commonly used in QA systems cannot check lin-guistic and semantic types annotated in passages at query time [1, 2]. We believe that a main reason for the inability of passage retrieval is the lack of a general ranking scheme to incorporate annotations in retrieval processes along with traditional retrieval models.

As a first step toward the goal to embody such ranking scheme, we propose a three-stage passage ranking approach to effectively integrate a range of annotations in passage re-trieval for QA. Our approach has two advantages. First, it can rank passages using those annotations in an unsu-pervised manner. Second, it can deal with annotations of arbitrary types. The rest of this paper is organized as fol-lows. In Section 2, we discuss past work on passage retrieval for QA. We describe our approach in Section 3, and show and discuss experimental results in Section 4. We make a conclusion and describe future work in Section 5.
There is a considerable amount of work on passage re-trieval. Passage retrieval was initially explored to overcome the shortcomings of document retrieval [15, 7, 8]. It was fur-ther investigated and integrated into QA [14, 4, 18]. This is primarily because a passage is a more appropriate unit to rank for answering questions than a document.

In retrieval processes for QA, researchers often augment information sources with various types of annotations. For instance, Prager et al., 2000 [14] used named entities to improve the performance of passage retrieval. Tiedemann, 2005 [19] integrated dependency relations into a multi-layer index in passage retrieval for Dutch question answering. Bilotti et al. 2007 [2] leveraged named entities and semantic roles to perform sentence-level structural retrieval for QA. Our approach is different from theirs in that we do not pre-annotate any corpora, which is a very expensive process. Shen and Lapata, 2007 [17] examined the effectiveness of semantic roles in factoid QA with a graph matching tech-nique. Our model is different from theirs in that ours can easily incorporate annotations of other types.

More recently, much work showed QA performance im-provements using supervised learning models for (re-)ranking passages with linguistic and knowledge-based features [5, 9, 1, 16, 20]. As compared to these learning-based stud-ies, there is much less work on utilizing such linguistic and knowledge-based resources in passage ranking with retrieval models in an unsupervised manner.
NTCIR-11 set up a shared task called QA Lab 1 . In this task, participants are expected to collaboratively develop module-based QA systems for solving real-world university entrance exam questions. One of the exams is from a stan-dardized test created by the National Center Test for Uni-versity Admissions in Japan. The original exam corpus was http://ntcir.nii.ac.jp/QALab/ . . . , most of those who excelled in culture and the arts were those who had passed the Imperial examinations, but in the (2) Ming period , there was a shift toward . . .

Question 2. From 1-4 below, choose the most ap-propriate sentence concerning events that occurred during the period referred to in the underlined portion (2). 1. Japanese silver circulated in China. 2. A Buddhist sect called Zen was created. 3. The play  X  X he Story of the Western Wing (Xixi-angji) X  was created. 4. The capital was established in Lin X  X n (present-day Hangzhou).
 Figure 1: An illustrative question in the exam cor-pus. The correct answer is 1. created in Japanese, but this work uses an English trans-lation version of the corpus. We use a set of 26 true-false questions from the 2009 exam on world history. All the ques-tions are multiple-choice questions with four answer choices, and one of them is the correct answer. Each answer choice is given in a single sentence.

Figure 1 shows an example of the true-false questions in the corpus, and the correct answer to this question is 1. Ex-aminees are instructed to read introductory text with some contextual information, and to solve questions following the text. The questions are strongly or weakly dependent on their corresponding introductory text. A degree of the de-pendency varies among individual questions, but in any case the correct answer does not appear anywhere in that text or in the entire corpus. Therefore, examinees must rely solely on their knowledge on world history in their brains to an-swer the questions. In the case of Figure 1, for example, the introductory text has a strong dependency on Question 2 since the underlined portion (2) provides an indispensable piece of temporal information for examinees to solve the question. There is no answer-bearing sentence in the intro-ductory text, including the  X . . .  X  portions that we omitted to save space in Figure 1.
The focus of this paper is not on QA but rather on pas-sage ranking as a system module for QA. To evaluate our passage ranking system, we first collect historical facts from the exam corpus. In this work, we define a historical fact as a sentence that tells us historically correct information. We ensure the historical correctness by a reference to informa-tion sources that we rely on.

The process of collecting historical facts is a sequence of the following manual steps. We first select the 26 true-false questions from the corpus. We then divide the 26 into two groups: (A) a subset of questions whose answer choices com-prise one historically correct sentence (the correct answer) and three historically incorrect sentences, and (B) the other subset of questions whose answer choices are one historically incorrect sentence (the correct answer) and three historically correct sentences. Each question in group (A) produces one historical fact, and each in group (B) produces three. Since the 26 questions consist of 21 questions in group (A) and 5 questions in group (B), we collect 36 historical facts in total.
In collecting historical facts, we possibly make a modi-fication to raw answer-choice sentences when a question is strongly dependent on its introductory text. For instance, Question 2 in Figure 1 is strongly dependent on the intro-ductory text, since solving the question requires the tempo-ral phrase specified with the underlined portion (2), as de-scribed in Section 3.1. In such cases, we append such phrase to answer-choice sentences in order to make the sentences as historically specific as possible. As a result, from Ques-tion 2 we create a history fact  X  X apanese silver circulated in China during the Ming period. X  We observe that we do not need to make such modification when questions are weakly dependent on their introductory text, because answer-choice sentences of these questions are historically specific enough to be a complete historical fact by themselves.

We define historical fact validation as a subtask to de-termine whether or not a given sentence is a historical fact and output a binary value (i.e., true or false) about it. It is clear that a system component to perform the subtask is directly useful to QA on the 26 true-false questions. For historical fact validation, we employ passage retrieval. The basic idea is that if a system taking a given sentence as a query (historical hypothesis) retrieves and ranks a passage (historical evidence) with a reasonably high score, then the system regards the sentence as a historical fact.
We choose Wikipedia as information sources for two rea-sons. First, it is abundant of historical facts and highly likely to cover historical topics of posed questions in the exam cor-pus. Second, Wikipedia is text-based, and thus makes it eas-ier for us to incrementally add desirable annotations using natural language processing or knowledge base tools than structured resources such as DBpedia. We use the  X 2014-02-03 X  dump of English Wikpedia articles 2 to construct an index. Table 1 shows the number of articles in the dump. We index only non-redirect articles with a Main namespace (the third row of Table 1), excluding the other articles be-cause they do not have any textual contents to be effectively retrievable for historical fact validation. As another prepro-cess to obtain plain text, we clean up Wikipedia markups in the dump using Bliki 4 . After the preprocess, we build up the index using Apache Solr 5 .
For historical fact validation, we propose a three-stage passage ranking approach shown in Figure 2. All the re-http://dumps.wikimedia.org/enwiki/20140203/enwiki-20140203-pages-articles.xml.bz2 http://en.wikipedia.org/wiki/Wikipedia:Namespace https://code.google.com/p/gwtwiki/ http://lucene.apache.org/solr/ trieval processes are done at a query time. The first stage is document retrieval. We assume that a query comprising raw words in an answer-choice sentence can retrieve doc-uments with substantially high recall. Therefore, we take a simple approach to formulate a query using just all raw words in the sentence. We restrict the number of retrieved documents to N d for further processing. Our another as-sumption here is that a properly tuned N d gives us a set of retrieved documents with relatively high recall.

The second stage is passage retrieval without involving any annotations except sentence segmentation. In this stage, we segment a retrieved document into passages by a fixed-length windows [8, 3]. The effectiveness of fixed-length arbi-trary passages is not particularly sensitive to passage length in the standard information retrieval setting [8, 11]. Al-though it is not clear that this is the case with passage re-trieval for historical fact validation, we follow the approach and use a sliding window of N s sentences to obtain passages. We use Stanford CoreNLP 6 for sentence segmentation. We also restrict the number of retrieved passages to N p for sub-sequent processes. Our assumption here is that we can also tune N p properly so it gives us a set of retrieved passages with moderately high recall. We use TF-IDF [12] for re-trieval in the both first and second stage.
 Algorithm 1 Annotation Similarity Model.
 Input: G 1 = ( E 1 = ( te 1 ) ,R 1 = ( tr 1 ) ,T ) Input: G 2 = ( E 2 = ( te 2 ) ,R 2 = ( tr 2 ) ,T ) Input: T c  X  T 1: E 0 1  X  ( te 1 ) where te 1  X  T c 2: E 0 2  X  ( te 2 ) where te 2  X  T c 3: R 0 1  X  ( tr 1 ) where tr 1  X  T c 4: R 0 2  X  ( tr 2 ) where tr 2  X  T c Output: sim ( G 1 ,G 2 ,T c )
The third stage is passage ranking with a range of anno-tations. The underlying idea of this stage is to improve pas-sage ranking by combining a bag-of-words similarity model ( sim BOW ) and an annotation similarity model ( sim ANN ). Following [1], we represent a set of anntations as an an-ntation graph. More specifically, an annotation graph G is represented as G = ( E,R,T ) where E is a set of elemental annotations and R is a set of relational annotations specified under a type system T . Algorithm 1 shows our algorithm to http://nlp.stanford.edu/software/corenlp.shtml output the annotation similarity using vertex/edge overlap [13]. T c is a subset of T used for the similarity calculation. In this work, we determine the final score with respect to sentence s and passage p by a multiplication of the TF-IDF score and the annotation similarity score as follows. where  X  is a weight on the annotation similarity model, G and G p are annotation graphs of s and p , respectively. We intend that sim ANN gives a small amount of similarity ad-justment to sim BOW . Thus, we seek relatively small val-ues when tuning  X  . With respect to annotations, we used Stanford CoreNLP for named entities and dependencies and ClearNLP 7 for semantic roles.
Since there is no gold standard for a ranked list of pas-sages as output of passage ranking, we judge the relevance of ranked passages manually by ourselves. More specifi-cally, the judgment process consists of the following man-ual steps. We first figure out a full set of semantic com-ponents (e.g., key entities, and temporal and geographical information) in a given sentence to constitute a historical fact. We then examine whether a ranked passage contains the necessary pieces of information to determine whether it is an answer-bearing passage, i.e., whether it can vali-date the correctness of a historical fact. To measure the performance of passage ranking, we use precision at rank 1 (P@1) and mean reciprocal rank (MRR). P@1 is the per-centage of historical facts where an answer-bearing passage is ranked at the first position. MRR is computed as follows: a query in Q , and rank ( q ) is the rank of the first answer-bearing passage in ranked passages retrieved from q .
We conducted an experiment to investigate the impact of the annotation similarity model in passage ranking using the 26 true-false questions. Table 2 shows our experimental re-sults. The first row of this table shows the performance of a baseline where we run the system up to the second stage. We found out that named entities of a person type is the most useful annotation. The annotation obtained 7.7% and 4.2% gains in terms of P@1 and MRR, respectively. This http://clearnlp.com Table 2: Comparison in the performance of passage ranking between passage annotations. In this exper-iment, the maximum number of documents is 1000, the maximum number of passages is 10, the window size is 3 sentences, and the weighting parameter  X  is 0.1.
 result indicates that names of historical figures are a key el-ement to amplify TF-IDF effects well, and the named entity annotation of persons effectively boost the performance of passage ranking.

For dependency relations, we used two relations  X  X subj X  and  X  X obj X . The former means a predicate-subject relation, and the latter a predicate-object relation. We observed that these relations also gave us performance gains comparable to named entities. We also examined two semantic arguments  X  X 0 X  and  X  X 1 X , which mean an agent and a patient, respec-tively. They achieved a slightly better performance than the baseline, but the performance improvement was quite small. This is mainly due to a sparseness problem of semantic ar-guments. The system produced a relatively small number of semantic role annotations. Consequently, it is rather rare that a sentence and a passage exhibit the same argument structure over the same tokens.
We proposed a three-stage passage ranking algorithm for historical fact validation, which is an important subtask for on world history. To our knowledge, this is the first work on an annotation similarity model that can process annotations of any type, along with traditional bag-of-words models, in an unsupervised manner. The model showed performance gains in passage ranking in terms of both P@1 and MRR.
Our future work is to refine the model so it can bene-fit from a combination of different annotations, including WordNet synsets and temporal relations. We also plan to implement a true-false judgment component to be integrated with our passage ranking component for an end-to-end eval-uation of a world history QA system.
We would like to thank the three anonymous reviewers for their valuable comments. The first author is supported by the Funai Overseas Scholarship. [1] M. W. Bilotti, J. Elsas, J. Carbonell, and E. Nyberg. [2] M. W. Bilotti, P. Ogilvie, J. Callan, and E. Nyberg. [3] J. P. Callan. Passage-Level Evidence in Document [4] C. L. A. Clarke, G. V. Cormack, and T. R. Lynam. [5] H. Cui, R. Sun, K. Li, M.-Y. Kan, and T.-S. Chua. [6] D. A. Ferrucci, E. W. Brown, J. Chu-Carroll, J. Fan, [7] M. A. Hearst and C. Plaunt. Subtopic Structuring for [8] M. Kaszkiel and J. Zobel. Passage Retrieval Revisited. [9] J. Ko, E. Nyberg, and L. Si. A Probabilistic Graphical [10] E. Krikon, D. Carmel, and O. Kurland. Predicting the [11] X. Liu and W. B. Croft. Passage Retrieval Based on [12] C. D. Manning, P. Raghavan, and H. Sch  X  utze. [13] P. Papadimitriou, A. Dasdan, and H. Garcia-Molina. [14] J. Prager, E. Brown, A. Coden, and D. Radev. [15] G. Salton, J. Allan, and C. Buckley. Approaches to [16] A. Severyn, M. Nicosia, and A. Moschitti. Building [17] D. Shen and M. Lapata. Using Semantic Roles to [18] S. Tellex, B. Katz, J. Lin, A. Fernandes, and [19] J. Tiedemann. Integrating Linguistic Knowledge in [20] W. Yih, M. Chang, C. Meek, and A. Pastusiak.
