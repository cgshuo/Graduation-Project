 Effective representation of images is crucial in image recognition and retrieval tasks. One popular method to represent images in vectors is bag of visual words(BoVW)[ 7 , 9 ], after the introduction of BoVW, a lot of notable contri-butions have been made to this field, including descriptors enhancement [ 10 , 11 ], vector quantization(VQ)[ 9 ], sparse coding(SC)[ 12 ], locality-constrained linear coding (LLC) [ 14 ], fisher vector(FV) [ 15 ], vector of locally aggregated descrip-tors(VLAD) [ 5 , 6 ], and encoding with convolutional neural networks(CNN) [ 3 , 4 ] etc. Among this kind, VLAD is an efficient super vector encoding method that provides successful representations for image retrival task, especially large scale image search given an instance.
 Super vector based encoding methods mainly follow a pipeline of multiple stages, which are  X  X ocal feature extraction X ,  X  X odebook learning X ,  X  X ector qua-tizing X  and  X  X ector pooling X . lows the aforementioned pipeline, and it can be regarded as the non probabilistic version of Fisher Vector. VLAD differs from the BoVW method by aggregat-ing the differences of each local descriptor and corresponding visual word, while BoVW records the number of descriptors around each centroid. VLAD is demon-strated to be able to generate compact vectors while preserving high retrieval accuracy, however, it suffers from the  X  X isual burstiness X  problem, which has been discussed in [ 1 ], and local coordinate system(LCS) is proposed to address the problem.  X  X isual burstiness X  X  8 ], caused by visual elements that appear more frequently in an image than statistics, will dominates the similarity measure. retrieval task, this paper makes some further modifications to VLAD procedure at two key steps of original VLAD.
 dinate rotation algorithm, which ameliorates the objective and timing while performing coordinate rotation independently for each visual word, and benefits the subsequent power-law normalization. Secondly, the classical power-law nor-malization introduces non-linearity to the final VLAD representation with only one parameter  X  , which is easy to use but limits the positive effects on allevi-ating  X  X isual burstiness X  for naive parameterization. This paper introduces the adaptive power-law normalization, which weights and restrains bursts on each dimension respectively. 2.1 VLAD The vector of locally aggregated descriptors is an encoding and aggregating tech-nique theoretically based on the locality of feature space, and can likewise be regarded as the non probabilistic version of Fisher Vector. Similar to BoVW model, a codebook D =[ d 1 , d 2 , ..., d K ]  X  X  d  X  K is first learnt from train-ing samples, subsequently the encoding and aggregation of local descriptors are conducted. Typically, d = 128 for SIFT descriptor is used to describe regions extracted from an image using an affine invariant detector.
 Aggregation. Given a set of local descriptors X =[ x 1 , x from an imagem, for each visual word d j ,j  X  [1 , ..., K ], a d -dimensional sub-vector vsub j is yielded by accumulating the differences between assigned local descriptor ( x for convenience) and the corresponding visual word where NN ( x ) denotes the nearest visual word of x in D . The final VLAD representation is the concetentation of all the d -dimensional subvectors and therefore becomes a d  X  K dimensional vector v =[ vsub Normalization. The VLAD vector is usually L 2 -normalized by the VLAD representation is encoded by accumulating the differences between local descriptors and their corresponding visual words, leading to that the local descriptors do not contribute equally to the final VLAD representation. Resid-ual normalization (RN) [ 1 ] is another normalization method for balancing the contribution of each local descriptor in an image. RN will normalize every resid-ual vector by L 2 -normalization prior to accumulation, thus, each d -dimensional subvector vsub j is normalized to: then, each block is L 2 -normalized reprectively in the final VLAD representation: Experiments show that a codebook of large size is required for good perfor-mance while BoVW is adopted, however, VLAD with a vocabulary of size 16 to 256 leads to good enough results. Besides, there are some variants of VLAD. [ 1 ] projects all local descriptors of an image on the 64 first principal directions of a offline learned PCA basis prior to accumulation. [ 2 ] performs intra-normalization on the final VLAD representation. Several methods brings a substantial perfor-mance boosting for negligible additional computational cost. 2.2 LCS Previous work [ 6 ] has shown that performing a PCA of local descriptors boosts VLAD. LCS provides a better handling of burstiness by adapting independently the coordinate system for each visual word. LCS achieves this goal by simply learning a  X  X ocal X  PCA for the partitioned feature space corresponding to each visual word.
 To elaborate on the process, for each visual word d j , which is a cluster center among the training descriptors, a rotation matrix Q j is offline learned to map from training descriptors to this word. The learned rotation matrices ( K in total) are then applied to the normalized residual vectors prior to accumulated to the final VLAD representation. Thus, each aforementioned d -dimensional subvector vsub j is normalized to: Under the observation that PCA-based processing of local descriptors improves VLAD representation, we need to dig out the fundamental cause. Since power-law normalization introduces a subsequent non-linearity before generat-ing the final VLAD representation, the basis counts. And the basis produced by PCA boosts the performance. [ 1 ] shows that the during PCA, it is the rotation and its interplay with power-law normalization that improves the results, rather than the dimensionality reduction, which is proved harmful. Although the local coordinate system(LCS) is a simple solution, it benefits the processing of raw local descriptors for avoiding the arbitrariness when power-law normalization is applied during the original VLAD process. This method boosts the performance of VLAD in conjunction with power-law normalization. Inspired by [ 1 ], this paper proposes a coordinate rotation algorithm based on local aggregated vectors, which improves on upon the local coordinate system by modifying the objective and timing while applying local coordinate rotation, which links up with the subsequent power-law normalization, and thus enhance the improvement of performance by this synergy.
 achieve a better handling by adapting independently the coordinate system for each visual word, and related experiments have shown gains in coordination with power-law normalization. However, the encoding function (Eq. 4 )doesnot explain well for timing the coordinate rotation. In order to explicitly describe the problems exist in LCS, this paper mainly made the following observations and will then propose an improved method to address the problem.
 offline learned by performing PCA on the feature space composed by the subset of local descriptors mapped to this word. Residual vectors from the testing set in correlation with the same centroid are then transformed with these rotation matrices. For all that, the rotation matrix Q j is learned from training descriptors in the original feature space but applied to normalized residual vector space, the inconformity can not be stated reasonably.
 tor coordinate system has no impact on similarity measure between resulting vectors in absence of power-law normalization. The PCA-based processing of local descriptors is able to improves VLAD by working with power-law. In other words, PCA does not contribute to the suppression of burstiness directly, but by increasing the effect of power-law normalization. However, the encoding proce-dure of LCS flaws for performing normalized residual vectors of local descriptors and cluster centers while applying power-law normalization to the representation vector accumulating all the residual vectors.
 rotation algorithm based on locally aggregated vectors, to aggregate the local descriptors of an image. The encoding procedure in VLAD pipeline can be for-mulized as: Equation 5 mainly differs from Eq. 4 in two aspects. First, in the modified formulation, each rotation matrix Q j is offline trained with a fixed-size( n ) subset of training images by performing PCA on the aggregated local residual vectors AR Besides, In Eq. 5 , the rotation is performed posterior to accumulating the resid-uals, which means the rotation for each segment of representation vector occurs after accumulating residuals, in accordance with the subsequent power-law nor-malization, further enhanced the synergistic effect between them.
 As illustrated in Fig. 1 , the coordinate rotation works effectively along with power-law normalization. A representative image with  X  X isual burstiness X  is singled out and the representation vectors obtained with diverse meth-ods are graphed, thereinto VLAD indicates the traditional VLAD method and the proposed coordinate rotation method in this paper is referred to as aggrLCS. Through comparison and analysis, we can come to the following two conclusions: First, Fig. 1 (a) and (b) show that the coordinate rotation algorithm based on local aggregated vectors cannot be applied independently. As a matter of fact, the fundamental purpose of rotation of local descriptors is to capture the principle directions while acculating a large variety of similar residual vectors, and rotate coordinate system in accord to these principle directions afterwards. The rotation magnifies the impact of  X  X isual burstiness X  instead of suppressing it, which brings convinience to the subsequent power-law normalization. restraining burstiness in conjunction with power-law normalization. Although the application of coordinate rotation temporarily blows up the  X  X isual bursti-ness X  phenomenon, as shown in Fig. 1 (b), while Fig. 1 (c) and (e) show that the tunning of power-law normalization parameter  X  can ultimately provide a final representation with overall balance. Power-law normalization works on a simple principle, when  X &lt; 1, the normaliza-tion transforms the vector elements by introducing a non-linearity, which adjusts the overall absolute values of vector elements more evenly, and further to some extent, alliviates the similarity measurement problem caused by  X  X isual bursti-ness X  in representation vectors component-wise. However, the design concept of power-law is not complicated, and has certain limitations in real world appli-cations. Based on the traditional power-law normalization process, this paper makes some extension hypotheses and proposes a novel adaptive power-law nor-malization method. This paper suspects that there are two major aspects of limitations of traditional power-law normalization, and would like to make some extensional assumptions to address the problem in this section.
 sentation vector with non-linear transformation, without fully taking advantage of the locality of burstiness. Aiming at this thoughtless, this paper makes an extensional assumption, that the positive impact of the power-law normalization can be magnified by applying it to each visual word segment of the representa-tion vector, rather than the overall one, and customizing different adjustment policy on each segment respectively.
 malization, which limits the flexibility of its application. This paper observes that the absolute values of the representation vector components vary greatly, and considers bringing in additional adjustable parameters. As can be inferred from the property of power-law normalization, the closer the value of  X  is to 0, the stronger effect power-law normalization has on balancing the components of representation vectors, and vice versa. Performing segment-wise power-law nor-malization to respectively with different parameters, i.e. applying minor para-meter  X  1 to vector elements with greater absolute values, and a major  X  others, will maglify the detraction of visual burstiness.
 law normalization in the first place, which is applied subsequently after perform-ing rotation to the coordinate systems and obtained the subvector Thereinto, RankInd j denotes the indices of elements in vsub the descending order of absolute values of the elements. Equation 7 achieves 2-phase power-law normalization to vsub j : for the top 10 % elements with greater absolute values, a smaller adjustment parameter  X  1 is utilized to enhance the restraining of power-law normalization to visual burstiness, a bigger parameter  X  is meanwhile used to descrese tensile strength for the last 90 %, thus, bal-ancing the absolute values of elements in adjusted representation vectors as a whole.
 The choice of segmented percentage mentioned in Eq. 7 is heuristical parame-ters for spotting the segmentation position. Its universal effectiveness on other datasets cannot be guaranteed. On the basis of this, this paper further proposes an adaptive 2-phase power-law normalization algorithm, the improved algorithm segments the representation vector by the particularities of subvectors. rank(i, j) denotes the rank of the i th vector element in indicates the segmentation position on the j th subvector, which can be deter-mined adaptively, it is the first index position that meet the following condition: The adaptively determined segmentation position divides the rearranged vec-tor elements into two sections: the first section with greater absolute values and large variety scope, which gathered peeks caused by burstiness and should be strongly suppressed, and the tensile strength should be decreased for the sec-ond part. There are some heuristical parameters for spotting the segmentation position, experiments have shown that these parameter are only related to the choice of image features, but nothing on the variety of datasets, and then would not influence the adaptivity of segmentation.
 Since the rotation of coordinate systems maginifies visual burstiness in advance, it is a common phenomenon that there are a few (around 10 %) ele-ments with large absolute values, and traditional power-law normalization will never deal effectively with these elements. As shown in Fig. 1 (c) and (d), a big  X  (=0.5) leads to insufficient suppression of burstiness while a small one (0.1) destroy the original representation shape. Besides, the utilizing of naive 2-phase power-law normalization can greatly alliviate the inflexibility with the adjust-ment of only one parameter. In Fig. 1 (e) the peeks are suppressed effectively and keep the distribution of original representation vector well at the same time. Fur-thermore, there are still some peeks that naive 2-phase power-law normalization fails to handle, the comparison of Fig. 1 (e) and (f) shows the effectiveness of the proposed adaptive power-law normalization method. The effectiveness of our proposed method has been verified on two standard and publicly availabe image retrival benchmarks along this line, which are the Oxford Buildings [ 19 ]and Holidays [ 20 ]. In this section, we begin with describ-ing the datasets, with the following experiment settings, evaluation procedure, experiment results and discussion in subsequent order. 5.1 Benchmark Datasets In this section, we will take a brief overview on the datasets.
 Oxford Buildings. 1 The Oxford Buildings, also referred as Oxford5k , is widely used in content based image retrieval, which consists of 5,062 images of build-ings downloaded from Flickr. Along with the images, there are 55 query images corresponding to 11 distinct buildings in Oxford, each query is specified by a rectangular region of interest.
 Paris Buildings. 2 The Paris Buildings [ 21 ] dataset, often referred to as Paris6k , consists of 6,412 images. It is usually used for unsupervised learning of the para-meters when evaluating the results on Oxford5k and its extension Oxford105k . INRIA Holidays. 3 The INRIA Holidays( Holidays ) contains 1,491 high reso-lution images from personal holiday photos of different locations and objects, 500 of which are used as queries. For large scale retrieval evaluation, another 1 million Flickr images ( Flickr1M [ 20 ]) are taken into account.
 Flickr 60k. Similar to the Paris6k dataset, Flickr60k is usually used to deter-mine the parameters while evaluating Holidays . It is also provided by INRIA and contains another 60k images downloaded from Flickr. 5.2 Experiment Settings and Evaluation Dataset Preprocessing. Since images in Holidays dataset and Flickr60k dataset are with high resolution, in order to avode negative impact on effec-tiveness for mass amount of local descriptors, we make proper resizing to images in these two datasets and ensure there are no more than 786,432 pixels in each image.
 Local Descriptors. Normalized RootSIFT [ 10 ] descriptors extracted with Hessian-Affine detectors are used as local descriptors of images, and as stan-dardly done in the literature, in Sect. 5 , we report both the results of normalized SIFT descriptors and RootSIFT descriptors.
 Experiment Parameters. Parmeter tuning exists only in the adaptive power-law normalization, due to experimental experience, parameters are within limits that  X  1  X  [0 . 1 , 0 . 3] and  X  2  X  [0 . 3 , 0 . 8].
 Comparison Methods. The comparison methods are the pairwise combination of aggregating methods for local descriptors, which includes BoW, VLAD, Fisher Vector and LCS, and normalization methods for image representation vectors, including L 2 -normalization, power-law normalization and whitening [ 13 ]. Evaluation Protocol. Following the standard experimental scenario, retrieval performance is measured in terms of mean average precision(mAP) and the distance measurement is Euclidean distance. In particular, the query image itself is left out while calculating the mAP of specific query in Holidays . 5.3 Results and Discussion Overall Performance. Table 1 reports the retrieval mAP on both Oxford5k and Holidays using SIFT and RootSIFT descriptors. The second and third column denotes the size of visual vocabulary and representation vector respectively. The proposed coordinate rotation method is referred as aggrLCS and Whitening denotes performing whitening process on the subvector of each visual word. Besides, the naive and adaptive 2-phase power-law normalization proposed in this paper are referred as Naive PL and Adaptive PL respectively.
 with adoptive power-law normalization provides the best performance which is superior to all the others, thus it is reasonable to infer that the coordination of these two processes can better suppress the negative impact of visual bursti-ness on image representation. The effectiveness of aggrLCS is verified on both the Oxford5k and Holidays datasets, with 2 % relative performance gain on Oxford5k with aspect to LCS and 5 % to initial VLAD, and on Holidays , that is +6 % and +10 % respectively. As for the normalization methods, the whitening process and our proposed 2-phase power-law normalization show their superiority, fur-thermore, the Adaptive PL is the best of all.
 under most of the local descriptors aggregating methods.
 each dimension of the final representation vectors produced by different methods on Oxford5k , from the point of energy, reflects the suppression of visual bursti-ness. RootSIFT descriptor is utilized and the size of vocabulary is fixed to 64 for a fair comparison. The Adaptive PL clearly shows its flexibility and effec-tiveness when it come to the preservation of details in the representation vectors and restrain the peeks of absolute values at the same time. Vocabulary Size. We then conduct experiments to show the comparison results when different sizes of vocabulary are used. As a normal practice, RootSIFT descriptor is utilized in all the methods and the adjustment parameter  X  =0 . 5 is chosed in traditional power-law normalization and in adaptive ones, we use  X  1 =0 . 1and  X  2 =0 . 5. We can observe the following points from results in Fig. 3 . First, the image representation ability boosts with the expansion of visual vocabulary, for all the involved methods. Besides, the trends of the performance present a consistancy, that is, methods utilizing coordinate rotation, such as LCS or aggrLCS , signifi-cantly outperform the initial VLAD method, and 2-phase power-law normalization methods shows their superiority. At last, the improvements are out of proportion to the size of vocabulary, indicating the infeasibility of using vocabularies with unlimited scales, which increases computing resource overhead.
 Parameter Tuning. Experiment on investigating how the variation of para-meters in adaptive power-law normalization will affect the final performance is also conducted. The result is presented in Table 2 . We can observe that as for Adaptive PL , when parameters are tuning in the empirical range (  X  normalization results are basically achieved. Besides, the empirical parame-ter setting can be adapted to different benchmarks as we can observe with  X  1 =0 . 1 , X  2 =0 . 4. What X  X  more, the impact of adjusting  X  due to its depression effect on the elements with greater absolute values in the subvector corresponding to each visual word.
 Projected Representation. Last but not the least, we carried out experiments for applying dimensionality reduction to generated representation vectors. Con-clusions can be drawn from Table 3 . First of all, compression final representation vectors to reasonable size cause little loss to the performance, the mAP drops only 0.3 % on Oxford5k and 2 % on Holidays when the 8,192-D representation vectors are compressed to 256-D. Second, there is no distinctive relationship between the reduced dimensionality and the final performance. In addition, per-forming another power-law normalization after dimensionality reduction won X  X  make any positive contribution to the retrieval performance. This paper introduces two complementary techniques based on VLAD, which are the aggregated coordinate rotation algorithm and adaptive power-law nor-malization. In comparison with most existing local descriptors aggregating and vector normalization methods, the proposed methods take full advantage of the locality of visual burstiness by performing flexible transformation of the subvec-tors corresponding to each visual word, to better handle the visual burstiness and enhance the expression of final aggregated vectors. The sufficient experiments have shown the superiority of the proposed methods on standard benchmarks.
