 One of the challenges in statistical language mod-eling are words that appear in the recognition task at hand, but not in the training set, so called out-of-vocabulary (OOV) words. Especially for produc-tive language it is often necessary to at least reduce the number of OOVs. We present a novel approach based on morphological classes to handling OOV words in language modeling for English. Previous work on morphological classes in English has not been able to show noticeable improvements in per-plexity. In this article class-based language mod-els as proposed by Brown et al. (1992) are used to tackle the problem. Our model improves perplex-ity of a Kneser-Ney (KN) model for English by 4%, the largest improvement of a state-of-the-art model for English due to morphological modeling that we are aware of. A class-based language model groups words into classes and replaces the word transition probability by a class transition probability and a word emission probability: Brown et al. and many other authors primarily use context information for clustering. Niesler et al. (1998) showed that context clustering works better than clusters based on part-of-speech tags. How-ever, since the context of an OOV word is unknown and it therefore cannot be assigned to a cluster, OOV words are as much a problem to a context-based class model as to a word model. That is why we use non-distributional features  X  features like mor-phological suffixes that only depend on the shape of the word itself  X  to design a new class-based model that can naturally integrate unknown words.

In related work, factored language models (Bilmes and Kirchhoff, 2003) were proposed to make use of morphological information in highly inflecting languages such as Finnish (Creutz et al., 2007), Turkish (Creutz et al., 2007; Yuret and Bic  X ici, 2009) and Arabic (Creutz et al., 2007; Vergyri et al., 2004) or compounding languages like German (Berton et al., 1996). The main idea is to replace words by sequences of factors or features and to apply statistical language modeling to the resulting factor sequences. If, for example, words were seg-mented into morphemes, an unknown word would be split into an unseen sequence, which could be rec-ognized using discounting techniques. However, if one morpheme, e.g. the stem, is unknown to the sys-tem, the fundamental problem remains unsolved.
Our class-based model uses a number of features that have not been used in factored models (e.g., shape and length features) and achieves  X  in con-trast to factored models  X  good perplexity gains for English. first character of w is an uppercase letter The feature vector of a word consists of four parts that represent information about suffixes , capitaliza-tion , special characters and word length . For the suffix group, we define a binary feature for each of the 100 most frequent suffixes learned on the training corpus by the Reports algorithm (Keshava, 2006), a general purpose unsupervised morphology learning algorithm. One additional binary feature is used for all other suffixes learned by Reports, in-cluding the empty suffix.

The feature groups capitalization and special characters are motivated by the analysis shown in Table 2. Our goal is to improve OOV modeling. The table shows that most OOV words ( f = 0 ) are numbers (CD), names (NP), and nouns and adjec-tives (NN, NNS, JJ). This distribution is similar to hapax legomena ( f = 1 ), but different from the POS distribution of all tokens. Capitalization and special character features are of obvious utility in identify-ing the POS classes NP and CD since names in En-glish are usually capitalized and numbers are writ-ten with digits and special characters such as comma and period. To capture these  X  X hape X  properties of a word, we define the features listed in Table 1.
The fourth feature group is length. Short words often have unusual distributional properties. Exam-ples are abbreviations and bond credit ratings like Aaa. To represent this information in the length part of the vector, we define four binary features for lengths 1, 2, 3 and greater than 3. The four parts of the vector (suffixes, capitalization, special char-acters, length) are weighted equally by normalizing the subvector of each subgroup to unit length.
We designed the four feature groups to group word types to either resemble POS classes or to in-duce an even finer sub-partitioning. Unsupervised POS clustering is a hard task in English and it is vir-tually impossible if a word X  X  context (which is not available for OOV items) is not taken into account. For example, there is no way we can learn that  X  X he X  and  X  X  X  are similar or that  X  X hild X  has the same re-lationship to  X  X hildren X  as  X  X id X  does to  X  X ids X . But as our analysis in Table 2 shows, part of the benefit of morphological analysis for OOVs comes from an appropriate treatment of names and numbers. The suffix feature group is useful for categorizing OOV nouns and adjectives because there are very few ir-regular morphemes like  X  X en X  in children in English and OOV words are likely to be regular words.
So even though morphological learning based on the limited information we use is not possible in gen-eral, it can be partially solved for the special case of OOV words. Our experimental results in Section 5 confirm that this is the case. We also testes prefixes and features based on word stems. However, they produced inferior clustering solutions. As mentioned before in the literature, e.g. by Mal-tese and Mancini (1992), class-based models only outperform word models in cases of insufficient data. That is why we use a frequency-based ap-proach and only include words below a certain to-ken frequency threshold  X  in the clustering process. A second motivation is that the contexts of low fre-quency words are more similar to the expected con-texts of OOV words.

Given a training corpus, all words with a fre-quency below the threshold  X  are partitioned into k clusters using the bisecting k-means algorithm (Steinbach et al., 2000). The cluster of an OOV word w can be defined as the cluster whose centroid is closest to the feature vector of w . The formerly removed high-frequency words are added as single-ton clusters to produce a complete clustering. How-ever, OOV words can only be assigned to the orig-inal k-means clusters. Over this clustering a class-based trigram model can be defined, as introduced by Brown et al. (1992). The word transition proba-bility of such a model is given by equation 1, where c transition probability P ( c the unsmoothed maximum likelihood estimate. The emission probability is defined as follows: P ( w 3 | c 3 ) = where c ( w ) is the frequency of w in the training set.  X  is estimated on held-out data. The morphologi-cal language model is then interpolated with a modi-fied Kneser-Ney trigram model. In this interpolation the parameters  X  depend on the cluster c tory word w
P ( w 3 | w 1 w 2 ) =  X  ( c 2 )  X  P M ( w 3 | w 1 w 2 ) This setup may cause overfitting as every high fre-quent word w grouping of several words into equivalence classes could therefore further improve the model; this, however, is beyond the scope of this article. We es-timate optimal parameters  X  ( c described by Bahl et al. (1991). We compare the performance of the described model with a Kneser-Ney model and an interpolated model based on part-of-speech (POS) tags. The relation be-tween words and POS tags is many-to-many, but we transform it to a many-to-one relation by labeling every word  X  independent of its context  X  with its most frequent tag. OOV words are treated equally even though their POS classes would not be known in a real application. Treetagger (Schmid, 1994) was used to tag the entire corpus.
 The experiments are carried out on a Wall Street Journal (WSJ) corpus of 50 million words that is split into training set (80%), valdev (5%), valtst (5%), and test set (10%). The number of distinct fea-ture vectors in training set, valdev and validation set (valdev+valtst) are 632, 466, and 512, respectively. As mentioned above, the training set is used to learn suffixes and the maximum likelihood n-gram esti-mates. The unknown word rate of the validation set is  X   X  0 . 028 .

We use two setups to evaluate our methods. The first uses valdev for parameter estimation and valtst for testing and the second the entire validation set for parameter estimation and the test set for testing. All models with a threshold greater or equal to the fre-quency of the most frequent word type are identical. We use  X  as the threshold to refer to these models. In a similar manner, the cluster count  X  denotes a clustering where two words are in the same cluster if and only if their features are identical. This is the finest possible clustering of the feature vectors. Table 3 shows the results of our experiments. The KN model yields a perplexity of 88 . 06 on valtst (top row). For small frequency thresholds overfitting ef-fects cause that the interpolated models are worse than the KN model. We can see that a clustering of the feature vectors is not necessary as the differ-ences between all cluster models are small and c is the overall best model. Surprisingly, morphologi-cal clustering and POS classes are close even though 10 2 85.92 87.06 85.92 85.91 85.89 10 3 84.43 86.88 84.83 84.77 84.56 10 4 85.22 87.59 85.89 85.73 85.26 10 5 86.82 87.99 87.44 87.32 86.79 0 813.50 813.50 813.50 813.50 813.50 1 181.25 206.17 182.78 183.62 184.43 5 152.51 185.54 154.52 152.98 153.83 10 147.48 186.12 149.34 147.98 147.48 50 146.21 203.10 142.21 140.67 140.46 10 2 149.06 215.54 143.95 142.48 141.67 10 3 173.91 279.02 164.22 159.04 150.13 10 4 239.72 349.54 221.42 208.85 180.57 10 5 317.13 373.98 318.04 297.18 236.90  X  348.76 378.38 366.92 357.80 292.34 the POS class model uses oracle information to as-sign the right POS to an unknown word. The optimal 84.43 and 84.56; that means that only 1 . 35% of the word types were excluded from the morphological clustering ( 86% of the tokens). The improvement over the KN model is 4% .

In a second evaluation we reduce the perplexity calculations to predictions of the form P ( w where w the KN model has to back off to a bigram or even unigram estimate, which results in inferior predic-tions and higher perplexity. The perplexity for the KN model is 813 . 50 (top row). A first observation is that the perplexity of model c value, but worsens with rising values for  X   X  10 . The reason is the dominance of proper nouns and cardinal numbers at a frequency threshold of one and in the distribution of OOV words (cf. Table 2). The c words after unknown nouns and cardinal numbers and two thirds of the unknown words are of exactly that type. However, with rising  X  , other word classes get a higher influence and different probability dis-tributions are superimposed. The best morphologi-cal model c to 140.46 (bolded), an improvement of 83% .

As a final experiment, we evaluated our method on the test set. In this case, we used the entire validation set for parameter tuning (i.e., valdev and valtst). The overall perplexity of the KN model is 88 . 28 , the perplexities for the best POS and c  X  clus-ter model for  X  = 1000 are 84 . 59 and 84 . 71 respec-tively, which corresponds again to an improvement of 4% . For unknown histories the KN model per-plexity is 767 . 25 and the POS and c perplexities at  X  = 50 are 150 . 90 and 144 . 77 . Thus, the morphological model reduces perplexity by 81% compared to the KN model. We have presented a new class-based morphological language model. In an experiment the model outper-formed a modified Kneser-Ney model, especially in the prediction of the continuations of histories con-taining OOV words. The model is entirely unsuper-vised, but works as well as a model using part-of-speech information.

Future Work. We plan to use our model for do-main adaptation in applications like machine trans-lation. We then want to extend our model to other languages, which could be more challenging, as cer-tain languages have a more complex morphology than English, but also worthwhile, if the unknown word rate is higher. Preliminary experiments on German and Finnish show promising results. The model could be further improved by using contex-tual information for the word clustering and training a classifier based on morphological features to as-sign OOV words to these clusters.
 Acknowledgments. This research was funded by DFG (grant SFB 732). We would like to thank Hel-mut Schmid and the anonymous reviewers for their valuable comments.
