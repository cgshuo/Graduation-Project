 Data clustering process groups a set of data into similar and dissimilar subsets. Important applications of data clustering can be found in areas such as group-ing, decision-making, and machine learning, including data mining, document retrieval, image segmentation, and pattern classification [1,5,6]. Traditionally, clustering method can be broadly classified into the partition and hierarchical clustering algorithms. The k-means algorithm is one of the most famous parti-tional clustering algorithms because of its simplicity and easy implementation. Although it is widely used, the k -means algorithm suffers from some drawbacks [9]. Firstly, there is no efficient method for identifying the initial partitions and the number of clusters k . Second, it often terminates at a local optima and, it is also sensitive to the noisy data. On the other hand, the well-known hierarchi-cal clustering algorithms are single-lin k, complete-link, average-link, and so on [4]. These hierarchical clustering algorithms give incorrectness results when the clusters are close to one another even by noises, or when their shapes and sizes are not hyper-spherical.

To alleviate these drawbacks, the hybrid idea or two-phase clustering is intro-duced. Firstly, this strategy partitions the data set into the pre-specified number of sub-clusters and, then, continuously merges the sub-clusters based on simi-larity criteria in a hierarchical manner. This process continues until the exactly desired number of clusters is determined. Some of the hybrid clustering methods are: Balanced Iterative Reducing and Clustering using Hierarchies  X  BIRCH [10], Clustering Using representatives  X  CURE [3] and Cohesion-based Self-Merging  X  X SM[7].

At present, the development of the exist ing clustering algorithms concentrates on the computational resources rather than the quality of clustering result. In general, the development of clustering technique for a good quality result should consider the following requirements [2]: (1) minimal requirements of domain knowledge to determine input parameters, (2) discovery of clusters with arbi-trary shapes, (3) ability to deal with the noisy clusters. As aforementioned, these well known clustering algorithms offer no solution to the combination of cluster-ing requirements. Most of these methods do not produce a suitable estimation of the number of output clusters by themselves. Since they require pre-specified pa-rameters, these algorithms may be unsuccessful if the choice of static parameters is incorrect with respect to the data set be ing clustered. Alt hough the concept of noise removal allows these clustering algorithms to find clusters of arbitrary shapes and sizes in some data sets, these algorithms still have some troubles when clusters have different densities in the data set.

This work is focused on developing a new method for clustering data which is based on hybrid approach named Self-Partition and Self-Merging (SPSM) algo-rithm. Our idea is, first, to partition data points into several small sub-clusters using self-partition method (Phase 1). Each of sub-cluster may be all noisy data or all non-noisy data. In order to achieve a better clustering result and less noisy effects, the sub-clusters are separated into normal sub-clusters and noisy sub-clusters based on our density threshold in noise removal algorithm (Phase 2). Thereafter, those normal sub-clusters are continuously combined into larger clus-ters based on the homogeneity within eac h cluster and the closeness between two clusters in self-merging algorithm (Phase 3). With our hybrid approach, SPSM algorithm is capable to find clusters with different shapes and sizes as well as different densities without knowing the number of clusters in advance. Besides, it can handle any large data sets and successfully handles the noisy data sets. SPSM algorithm can conform with all clustering requirements to obtain a good clustering result. The rest of the paper is organized as follows. Section 2 presents the background material related to the proposed method. Section 3 introduces the self-partition algorithm. Section 4 addresses the noise removal process. The self-merging procedure is d escribed in Section 5. The experimental results and discussion are given in Section 6. The conclusion is in Section 7. The major focus of this section is to show all the measurements used in SPSM algorithm. We begin by defining the centroid and homogeneity for one cluster. in cluster c p .
 Definition 1. The average intra-cluster distance of the cluster c p , denoted by D avgintra ( c p ) , is the average distance of all nearest neighbor pairwise distances within cluster c p defined as where  X  is the Euclidean norm.
 and m q of two clusters c p and c q , respectively, is defined as Definition 3. The maximum intra-cluster distance of the cluster c p ,denoted pairwise distances within cluster c p defined as: Definition 4. The minimum inter-cluster distance D mininter ( c p ,c q ) is the max-imum of minimum distance chosen from the pairwise distances between two clus-ters c p and c q defined as where bx i and bx j are the boundary data points of the clusters c p and c q , respectively.
 Definition 5. The maximum inter-cluster distance D maxinter ( c p ,c q ) is the maximum distance selected from the pairwise distances between two clusters c p and c q defined as The main idea of this process is to partition the data set to form a number of small sub-clusters by recursively subdividing the data set into four sub-clusters according to data distribution. The partitioning process can be carried out by the proposed self-partitioned clustering model called Dynamic Tree-Structured Self-Organizing Map (DTS-SOM) algorithm. 3.1 Architecture of the DTS-SOM Algorithm The initial topology of DTS-SOM is a two-dimension grid-tree structure. The tree used in DTS-SOM is a 4-ary tree. Each node, accepting the leaves, has four descendants. Each leaf node represents each prototype vector of each sub-cluster. These leaves are hierarchically clustered into groups of four nodes. Each w i in the network has a counter  X  i denoting the number of times this node has been the best matched unit during the DTS-SOM training. Each node has the lateral connections among neighboring nodes defined as the direct left, right, top, bottom, top-left, bottom-left, top-right, and bottom-right neighboring nodes as shown in Fig. 1.
 3.2 Training of the DTS-SOM The DTS-SOM algorithm starts by taking a single node from the available set of data vectors in a random manner. Then, the node is split to generate four new nodes and their counters are initialized. During the DTS-SOM training, the neural nodes are arranged in a tree topology and allowed to grow when any given branch receives a lot of times being the best matching unit from the training data vectors. The search for the best matching unit and its neighbors is conducted along the tree. The DTS-SOM algorithm is repeated for every vector on the training set until the stopping crit erion is reached. The size of the tree is examined at the end of each epoch and co mpared with the size of the tree from the previous epoch. If their difference is less than a small threshold value, then the training is completed. Algorithm. DTS-SOM Training 1. Initialize the DTS-SOM network as described at the beginning of Sec. 3.2. 2. repeat 3. Select an input datum x j ( t ) randomly from input space. 4. Find the best matching unit b using the tree search. 5. Update weight vectors of the best matching unit b and its direct neigh-6. Update their parents layer by layer by using w i ( t +1) = 1 nc i  X  k w k ( t ). 7. Decrease the counter  X  b of the best matching unit b by 1. 8. if the counter  X  b decreases to zero then 9. Generate four new neurons of original neuron b . 10. Initialize their new weights and their counter  X  i . 11. until the tree has grown less than a small threshold value. After Phase 1, the sub-clusters with low density can be considered as noisy sub-clusters while the others are considered as normal sub-clusters. The main process of Phase 2 is to filter out a majority of the noisy sub-clusters based on density distribution. The whole process of noise removal is composed of the following three steps. 4.1 Density Computation Before finding the sub-cluster density, it is observed that the expected normal sub-clusters have some deviated points. These points will affect the correctness of density computation. Thus, all the data points in each sub-cluster are examined. The detail of of finding sub-cluster density is described in the following algorithm. Algorithm. DensityFinding 1. for each sub-cluster c p 2. do if the number of data points in cluster c p ( | c p | ) &gt; 1 then 3. Compute all eigenvalues  X  k and eigenvectors v k of sub-4. Compute Volume ( c p )= 5. for each point x i  X  c p 6. do Compute l = m p  X  x i ,where m p is the cluster 7. for each eigenvector v k 8. do Compute angle  X  k between v k and x i . 9. Compute projected line segment l = l  X  10. if l&gt; 11. Remove the point x i as the noisy 13. if | c p | X  1 14. then Remove x i as nx i and set Density ( c p )=0 15. else Remove x i as nx i and set Density ( c p )=0 4.2 Cluster Separation The purpose of this stage is to separate the sub-clusters without the deviated data points into two classes X  the normal sub-clusters and the noisy sub-clusters  X  based on the density threshold value. The question is how to set this den-sity threshold value. The problem can be solved by sorting all the sub-cluster densities in ascending order and arranging them in terms of a logarithm scale with each scale ranging in 10 times. Presentation of densities on a logarithmic scale can be used to analyze the characteristic of the densities. Fig. 2 shows the example of density arrangement in terms of a logarithm scale. From the den-sity arranging experiments of several d ata sets, it is not necessary to consider all density ranges to set the threshold value. Therefore, some density ranges can immediately identify to the densities of n ormal sub-clusters or noisy sub-clusters because the number of densities in such ranges is less with respect to the number of densities in the other ranges. As the result in Fig. 2, density range 1-10 can be set as the densities of noisy sub-clusters, and density range 1000-10000 can be considered as the densities of normal s ub-clusters. From this observation, we consider the density range having the mo st number of densities to determine the threshold value and, then, we set this range as RangeI . Then, we select the an-other range, named RangeII , whose density range is less than the density range of RangeI. In this work, the threshold value thres density will separate all den-sities in RangeI and RangeII into two set s: the first set (densities of the noisy sub-clusters) and the second set (densitie s of the normal sub-clusters), based on the coefficient of variation ( CV ) [8]. Thus, the density threshold value should be set at the position such that the CV of the first set is greater than the CV of the second set. This implies that the variation of the first set is more than the second set. Algorithm. ClusterSeparating 1. for each sub-cluster c p 2. do if Density ( c p ) &lt;thres density 3. then Set sub-cluster c p as noisy sub-cluster nc x . 4. else Set sub-cluster c p as normal sub-cluster c p . 5. Let density mean be the average density of normal sub-clusters c p . 6. Let density std be the standard deviation of normal sub-clusters c p . 7. Compute density mean . 8. Compute density std . 9. for each normal sub-clusters c p 10. do if Density ( c p ) &lt; ( density mean  X  density std ) 11. then Set normal sub-cluster c p as noisy sub-cluster nc x . The self-merging approach is the process of iterative joining neighboring sub-clusters into larger clusters according to similarity constraints. To obtain the complete results, the algorithm consists of the multiple sequential merging steps. 5.1 Neighboring Merging Merging any two neighboring sub-clusters depends upon the noisy points lying in between them. Therefore, in this merging step, all removed noisy points must be placed back to their original locations. Then, an imaginary region called partial region overlapping these two sub-clusters is created to deter mine the merging possibility. We define the partial region as the hyper-box whose side lengths depend on how side lengths of two normal sub-clusters overlap. In our approach, each side length of any normal sub-clusters is the data range of each dimension. Fig. 3(a) shows the partial region occurred when the side lengths of two normal sub-clusters do not overlap but either side length of two normal sub-clusters in Fig. 3(b) overlaps. After that, the neighboring merging algorithm is performed in the following algorithm. 5.2 Local Merging I The objective of this stage is to further aggregate the closest clusters. This step is shown as follws: 5.3 Local Merging II Before performing further merging process, the large clusters based on the thresh-old value are temporarily removed. Once the number of data points for each cluster is sorted in descending order, the threshold value can be set as the num-ber of data points such that the difference between the number of data points in clusters c g and c g +1 is maximum. After removing the large clusters, the further merging steps are shown in the LocalMergingII algorithm.
 5.4 Refinement Merging final clusters are obtained, each the remaining noisy point is assigned to be the member of the cluster. However, there ar e still some noisy points which cannot belong to any clusters since these poi nts are very deviated from the others. Thus, these points are set as noise. The algorithm of this stage is described in the RefinementMerging algorithm.
 The purposes of this section is to measu re the performance of the proposed SPSM algorithm in the complicated data sets and compare them to the clus-tering algorithms such as the single-link, complete-link, CURE and CSM al-gorithm. These data sets are consisted of DS1, DS2 and DS3 obtained from http://glaros.dtc.umn.edu/gkhome/cluto/cluto/download. The clustering results of SPSM algorithm are shown in Fig. 4. Our results show that SPSM algorithm is able to correctly identify the genuine cl usters in all three data sets. Besides, these results illustrate that SPSM algorithm is very effective in finding clusters of arbitrary shapes and orientation, and is tolerant to noisy data. These data sets have been tested with CURE algorithm obtained from public domain at http://www.cs.cas.cz/  X  petra/Liter-Odkazy-shluk.html.

Some of unsuccessful clustering results acquired from CURE algorithm are shown in Fig. 5. CURE fails to find the right clusters on all these data sets, although we have carefully chosen CURE X  X  parameters. As these results, CURE fails because CURE is also hierarchical clustering algorithm that may produce the wrong results at the desired level. In the cases of DS1, DS2 and DS3, CURE selects the wrong pair of clusters to merge it together before forming a final clustering results. Besides, CURE algorithm is not a suitable method for the long piece shapes in data sets. Since the shrinking method causes such clusters to be split if shrinking factor is set too large. On the other hand, if the shrinking factor is chosen too small, some clusters will be merged by noise links.
We also implemented the CSM algorithm with the best attempt because the suggestion of choosing the parameters of CSM algorithm is not provided. Then, SPSM algorithm has been evaluated the performance against CSM algorithm. Fig. 6(a) shows the results of algorithm CSM with the number of sub-clusters, the parameter m = 16, respectively. Fig. 6(b) shows the results of algorithm CSM with the parameter m = 50, respectively. The number in the parenthesis is the desired number of clusters specified by user. From the results, the CSM algorithm fails to identify the clusters b ecause of the paramete r selection. One of the parameters affects the co rrectness of the clusterin g results is the parameter m specified by users. If the sub-clusters produced in Phase 1 are be too many then the actual noisy sub-clusters will become dense sub-clusters. This may lead to CSM algorithm makes a mistake by merging the wrong pair of clusters. On the other hand, when the value of parameter m is too small, the number of sub-clusters obtained from Phase 1 will be too low. So many noisy sub-clusters may exist. These noisy sub-clusters may merge with other clusters and affect the clustering results. Besides, the clustering results of CSM algorithm also depend on the user-specified parameters used to compute the threshold value in the noise removal mechanism. In practice, it is hard to set with the proper parameters. Thus, algorithm CSM cannot precisely remove those noisy sub-clusters. In this paper, the 3-phase clustering called SPSM algorithm is proposed. SPSM algorithm is not only able to find the clusters of arbitrary shapes and size, but also able to tolerate noise. Furthermore, SPSM algorithm automatically determines the number of clusters. This is more appropriate when the user has no a priori information on the number of clusters presented in the data sets. In addition, SPSM algorithm concentrates on producing the good quality of clustering results rather than to speed up the computational resources. Thus, the experimental results on several varying data sets show that SPSM algorithm provide better clustering results than those clustering algorithms. Besides, SPSM algorithm can discover the clusters of varied densities which most of existing algorithms fail to find.
 This work was supported in part by the Development and Promotion for Science and Technology talents project of Thailand (DPST).

