 Web search services can create web databases (a collection of web pages) that enable browsers tend to maintain web databases to cache web pages internally, providing users web pages without downloading the same pages repeatedly. As web pages are created, modified, and destroyed dynamically, users are likely to see out-of-date pages instead of seeing up-to-date ones. Administrators should frequently update web databases to keep the databases fresh. Understanding the change behavior of web pages certainly helps the administrators manage their web databases. They found that pages in the  X  X om X  domain changed more frequently than those in download 151 million pages 11 times over 11 weeks. They found that the average pages change more often than small ones do. Ntoulas et al. [8] picked 154 sites in the five top-ranked pages in each topical category of the Google directory. They crawled about 4.4 million pages from the sites weekly for one year. They found that every week 8% of downloaded pages were new, and that 25% of all crawled links were new. phenomenon draws little attention in research communities. To our best knowledge, research that considers downloading issues of URLs. In the real world, we cannot always download a page with a given URL. An approach that makes a distinction between URLs and successfully downloaded pages (possibly in a single page basis) is recommended. Third, even though previous researchers computed the average change intervals of web pages in one way or another, there has been no technical attempt to investigate what portion of all the web pages change at uniform rates and what portion of them do not. modification rate, and the coefficient of age variation to represent the change behavior of web pages. These metrics take into account not only the unsuccessful download states of web pages, but also the creation of new URLs and new web pages. From the 34,000 Korean sites we selected, we have monitored approximately 1.8 million to 3 million URLs at two-day intervals for 100 days. Using the metrics we propose, we analyze the collected URLs and web pages. This paper presents our findings from the analysis. Also, we propose a method that computes the probability that a page will be successfully downloaded in the next crawls. Our experiment shows that the method estimates the probability well. of metrics to represent the changes of web pages in section 2. Section 3 analyzes the collected URLs and web pages. Section 4 proposes a formula that predicts the change behavior of the web pages. Section 5 contains the closing remarks. In this section, we define metrics that show the change behavior of web pages. We are going to use an illustrative example (see Fig. 1) to explain these metrics. There are 16 crawls with six URLs. In each crawl, the web robot [7] starts off by placing an initial set of URLs in a queue that stores URLs to be visited. From this queue, the robot gets a URL, downloads a page, extracts (detects) any URLs in the downloaded page, and URLs in the queue or the robot visits a predetermined number of pages. corresponding URL is not available at that point. A black circle ( X   X   X ) means that we fail to download a web page. The content of a downloaded page is denoted as a circled character (here , , etc.). For example, on the first crawl where four URLs (A, B, E, and F) are available, we can download two pages from URL A and B successfully, and we denote the contents of the downloaded pages as and , respectively. numbers on which a URL is detected first and last, respectively. The detection rate of a URL is a value produced by the following equation: rate of URL C is 4/(9-5+1)=0.8. The more frequently a URL is detected between the first and last detections, the higher the detection rate value becomes. The low detected. Because the URLs with low detec tion rates are not informative enough to be studied, we should analyze the change behavior of web pages with URLs whose detection rates are higher than a given threshold. equation: (4/8)=0.5. A download rate value shows how consistently a web page is downloaded. equation: crawls, the download recall of URL D is (8/16)=0.5. Suppose a URL is detected only once and a page with the URL is downloaded successfully. Then, even though the download rate of the URL becomes 1.00, we cannot accept this number for granted 
URL A B ----C ------------D --------E ----F because there is only one download attempt in 16 crawls. We should consider the URLs whose download recalls are higher than a given threshold. Definition 4: Suppose that a page with URL u is downloaded on crawl number i , and that there is at least one download success with URL u prior to crawl number i . The current content of URL u on crawl number i is defined as the content of a page that is downloaded on crawl number i . The previous content of URL u on crawl number i is number i are different from each other, the page of URL u is defined to be modified on crawl number i . 
For example, the current content of URL E on the 10 th crawl is and the previous content is . The page for URL E is modified on the 10 th crawl. On the 12 th crawl, the modified on the 12 th crawl. equation: Definition 6: The modification recall of a page is a value computed by the following equation: implies that eight comparisons take place. The page is modified twice on the 7 th and 10 th crawls. The modification rate and modifica tion recall of the page are computed as (2/(9-1)) = 0.25 and (8/15)=0.53, respectively. the page contents. Because the content of a first page cannot be compared, we do not denominator of the modification rate computation is ( the number of successful downloads  X  1). As before, we also should analyze the modification rates of the pages whose modification recalls are higher than a given threshold. Definition 7: Suppose we have a page with content c that is successfully downloaded c of the page of URL u is successfully downloaded, respectively. If there is no crawl URL u is successfully downloaded, then the content age of the page is defined to be ( j  X  i + 1 ). following equation: equation: how long a page keeps its content unchanged. The average of the content ages is the coefficient of age variation of URL E is (2.52/4.7) = 0.54. The coefficient of age modified regularly, the content ages of u become more or less uniform, and the standard deviation of the content ages becomes small. As before, we need to compute given threshold. 3.1 Experimental Setups from two separate categories: famous sites and random sites. The famous sites are composed of the top 4,000 sites offered by RankServ (http://www.rankserv.com/), which ranks all Korean sites in terms of popularity, as of November 2003. The random sites are composed of the 30,000 sites that were randomly selected from 1.2 million Korean sites we crawled in October 2003. A single site could belong to both the categories. pages of all the monitored sites. Our robot visits all the URLs in the list periodically, and visits a predetermined number of pages that are reachable from the root pages in a breadth-first fashion. As web pages are created and destroyed dynamically, the crawled pages are different on each visit. This scheme is superior to one that simply pages. depth (i.e., the number of hops from a root page) was limited to nine. The robot was not allowed to crawl URLs containing a question symbol ( X ? X ) because these URLs mean the pages are not static. The page comparison was made character by character. pages, to see if the pages are of the same. case, we also tried to crawl a new page with the redirected URL. 3.2 Experimental Results In this subsection, we analyze the change behavior of the web pages and describe our findings. Fig. 2 shows the number of the detected URLs, downloaded pages, and accumulated URLs for each crawl. The robot detected approximately 0.8 million URLs and downloaded 0.62 million pages (about 78% of the 0.8 million URLs) from the famous sites. From the random sites, the robot detected one million URLs and downloaded 0.83 million pages (about 83%). After all the 50 crawls, we accumulated 1.32 million URLs from the famous sites and 1.69 million URLs from the random sites. 40% of the accumulated URLs, which is much greater than expected, were not detected on the first crawl. On a weekly basis, about 5% of URLs detected were new in our experiment. These figures imply that any approaches to indexing, searching, and maintaining web databases should consider newly generated URLs (and pages) in an appropriate way. It should be emphasi zed that new web pages or URLs are created dynamically in the real world. of the detection rates were 0.9 or more. For exactly 9 URLs out of 10 URLs, we were Simply speaking, once a URL is detected, it continues to be detected. We also learned that there were few URLs with the detection rates less than 0.9, which implies that a URL is hardly detected again once it ceases to be detected. From here, we are going to analyze the change behavior of web pages with URLs whose detection rates are higher than 0.9, because URLs with the low detection rates are simply unstable. (1.93 million URLs from both set of the sites) satisfying the condition that the rates of 19% of all the URLs were zero, which means that approximately one out of five URLs with detection rates at least 0.9 was not downloadable. two-fold. First, if the first download of a URL is successful, the URL continues to be downloadable (precisely speaking, at least 9 out of 10 download attempts are downloads are very likely to be unsuccessful. administrators may have an expectation that the page will be downloaded someday. They may keep the URL and request the page later, in a hope that the page is restored, meaningless to keep and request such a page. the modification recalls were at least 0.2 and the detection rates were at least 0.9. 73% of the pages were never modified (i.e., their modification rates were zero) during our experiment. Roughly speaking, the figure means that if pages are downloaded in a row, at least 73% contents of them are the same as before. The rest 27% of the URLs pages change on each crawl. Recalling that we visited a site every other day, we do not know how many times the pages change in our visiting interval. We also learned that most of the pages were not frequently modified and a small number of pages were modified very frequently. modification cycles and 57% URLs did not. 58% URLs of the random sites had modification cycles and 42% URLs did not. These results show clearly that we cannot assume that web pages are modified with fixed cycles. downloaded in the next crawls. The appr oach postulates that the future change behavior of web pages is strongly related to the past one. rates of URLs that were monitored for 100 days at two-day intervals in our experiment. We calculate the download rates to the two places of decimals. Based on Table 1, we estimate the probability of successful downloading of a web page. 0.99. For instance, P(DR(0.55)) is 0.07%/10 = 0.007%. ( c+d ) download attempts. Before computing the probability, we have two number of crawls (50 in this paper). Second, intervals of ( a+b ) and ( c+d ) download probability is computed by the following formula: sites. Second, we have attempted to download pages with the selected URLs five times for 10 days at a two-day interval. With the number of successful downloads in the five past crawls, we estimate the number of successful downloads in the five future crawls. There are all the six cases such that a page will be downloaded 0, 1, 2, estimated how many pages belonged to each case, and investigated how many pages actually did. Over 581,608 URLs, 715 URLs (323, 86, 39, 64, 175, and 92, respectively, in each case) wrongly estimated is very trivial. We have defined a number of new metrics (including the download rate, the modification rate, and the coefficient of age variation) that shows the change behavior of web pages, and we have monitored 34,000 sites at two-day intervals for 100 days. Following are what we have learned in our experiment. First, 1.3% of the URLs detected at a crawl were not detected at th e previous crawls (new pages are created at next download attempts are to be successful. Third, the 73% web pages that were downloaded successfully at least 10 times were unmodified and the less than 5% web modified at least 10 times did not follow a fixed change interval. After observing the change behavior of the web pages, we presented the method estimating the change behavior in terms of downloading issues. issues in a single page basis. We believe that the findings in our experiment will help web administrators maintain web databases fresh (such as which pages to re-crawl, the interval of re-crawling, the order of pages to re-crawl, and so on). pages. A more intensive experiment on the change model of web pages is currently pages affect the future change behavior. 
