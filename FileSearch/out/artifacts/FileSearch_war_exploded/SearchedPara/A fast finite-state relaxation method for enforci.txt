 } @cs.jhu.edu Many tasks in natural language processing involve sequence labeling. If one models long-distance or global properties of labeled sequences, it can be-come intractable to find ( X  X ecode X ) the best labeling of an unlabeled sequence.

Nonetheless, such global properties can improve the accuracy of a model, so recent NLP papers have considered practical techniques for decod-ing with them. Such techniques include Gibbs sampling (Finkel et al., 2005), a general-purpose Monte Carlo method, and integer linear program-ming (ILP), (Roth and Yih, 2005), a general-purpose exact framework for NP-complete problems.

Under generative models such as hidden Markov models, the probability of a labeled sequence de-pends only on its local properties. The situation improves with discriminatively trained models, such as conditional random fields (Lafferty et al., 2001), which do efficiently allow features that are functions of the entire observation sequence. However, these features can still only look locally at the label se-quence. That is a significant shortcoming, because in many domains, hard or soft global constraints on the label sequence are motivated by common sense:  X  For named entity recognition, a phrase that  X  In bibliography entries (Peng and McCallum,  X  In seminar announcements, a given field  X  For semantic role labeling, each argument
A popular approximate technique is to hypothe-size a list of possible answers by decoding without any global constraints, and then rerank (or prune) this n -best list using the full model with all con-straints. Reranking relies on the local model being  X  X ood enough X  that the globally best answer appears in its n -best list. Otherwise, reranking can X  X  find it.
In this paper, we propose  X  X onstraint relaxation, X  a simple exact alternative to reranking. As in rerank-ing, we start with a weighted lattice of hypotheses proposed by the local model. But rather than restrict to the n best of these according to the local model, we aim to directly extract the one best according to the global model. As in reranking, we hope that the local constraints alone will work well, but if they do not, the penalty is not incorrect decoding, but longer runtime as we gradually fold the global constraints into the lattice. Constraint relaxation can be used whenever the global constraints can be expressed as regular languages over the label sequence.

In the worst case, our runtime may be exponential in the number of constraints, since we are consider-ing an intractable class of problems. However, we show that in practice, the method is quite effective at rapid decoding under global hard constraints.
The remainder of the paper is organized as fol-lows: In  X  2 we describe how finite-state automata can be used to apply global constraints. We then give a brute-force decoding algorithm (  X  3). In  X  4, we present a more efficient algorithm for the case of hard constraints. We report results for the semantic role labeling task in  X  5.  X  6 treats soft constraints. Previous approaches to global sequence labeling X  Gibbs sampling, ILP, and reranking X  X eem moti-vated by the idea that standard sequence methods are incapable of considering global constraints at all.
In fact, finite-state automata (FSAs) are powerful enough to express many long-distance constraints. Since all finite languages are regular, any constraint over label sequences of bounded length is finite-state. FSAs are more powerful than n -gram mod-els. For example, the regular expression  X   X  X X   X  Y X   X  matches only sequences of labels that contain an X before a Y . Similarly, the regular expression  X  ( O  X  ) requires at least one non-O label; it compiles into the FSA of Figure 1.

Note that this FSA is in one or the other of its two states according to whether it has encountered a non-O label yet. In general, the current state of an FSA records properties of the label sequence prefix read so far. The FSA needs enough states to keep track of whether the label sequence as a whole satisfies the global constraint in question.

FSAs are a flexible approach to constraints be-cause they are closed under logical operations such as disjunction (union) and conjunction (intersec-tion). They may be specified by regular expressions (Karttunen et al., 1996), in a logical language (Vail-lette, 2004), or directly as FSAs. They may also be weighted to express soft constraints.

Formally, we pose the decoding problem in terms of an observation sequence x  X  X   X  and possible la-bel sequences y  X  X   X  . In many NLP tasks, X is the set of words, and Y the tags. A lattice L : Y  X  7 X  R maps label sequences to weights, and is encoded as a weighted FSA. Constraints are formally the same X  any function C : Y  X  7 X  R is a constraint, includ-ing weighted features from a classifier or probabilis-tic model. In this paper we will consider only con-straints that are weighted in particular ways.
Given a lattice L and constraints C , we seek We assume the lattice L is generated by a model M : X  X  7 X  ( Y  X  7 X  R ) . For a given observation se-quence x , we put L = M ( x ) . One possible model is a finite-state transducer, where M ( x ) is an FSA found by composing the transducer with x . Another is a CRF, where M ( x ) is a lattice with sums of log-To find the best constrained labeling in a lattice, y  X  , according to (1), we could simply intersect the lat-tice with all the constraints, then extract the best path.

Weighted FSA intersection is a generalization of ordinary unweighted FSA intersection (Mohri et al., 1996). It is customary in NLP to use the so-called tropical semiring, where weights are represented by their natural logarithms and summed rather than multiplied. Then the intersected automaton L  X  C computes L  X  C 1  X  C 2  X  X  X  X  using the Viterbi algorithm, or Dijkstra X  X  algorithm if the lattice is cyclic. This step is fast if the intersected automaton is small. The problem is that the multiple intersections in L  X  C 1  X  C 2  X  X  X  X  can quickly lead to an FSA with an intractable number of states. The intersection of two finite-state automata produces an automaton with the cross product state set. That is, if F has m states and G has n states, then F  X  G has up to mn states (fewer if some of the mn possible states do not lie on any accepting path).

Intersection of many such constraints, even if they have only a few states each, quickly leads to a com-binatorial explosion. In the worst case, the size, in states, of the resulting lattice is exponential in the number of constraints. To deal with this, we present a constraint relaxation algorithm. The simplest kind of constraint is the hard con-straint. Hard constraints are necessarily binary X  either the labeling satisfies the constraint, or it vi-olates it. Violation is fatal X  X he labeling produced by decoding must satisfy each hard constraint.
Formally, a hard constraint is a mapping C : Y  X  7 X  { 0 ,  X  X  X } , encoded as an unweighted FSA. If a string satisfies the constraint, recognition of the string will lead to an accepting state. If it violates the con-straint, recognition will end in a non-accepting state.
Here we give an algorithm for decoding with a set of such constraints. Later (  X  6), we discuss the case of binary soft constraints. In what follows, we will assume that there is always at least one path in the lattice that satisfies all of the constraints. 4.1 Decoding by constraint relaxation Our decoding algorithm first relaxes the global con-straints and solves a simpler problem. In particular, we find the best labeling according to the model, ignoring all the constraints in C .

Next, we check whether y  X  straints. If so, then we are done X  y  X  not, then we reintroduce the constraints. However, rather than include all at once, we introduce them only as they are violated by successive solutions to the relaxed problems: y  X  for some constraint C that y  X  y and so on. Eventually, we find some k for which y  X  satisfies all constraints, and this path is returned.
To determine whether a labeling y satisfies a con-straint C , we represent y as a straight-line automa-ton and intersect with C , checking the result for non-emptiness. This is equivalent to string recognition.
Our hope is that, although intractable in the worst case, the constraint relaxation algorithm will operate efficiently in practice. The success of traditional se-quence models on NLP tasks suggests that, for nat-ural language, much of the correct analysis can be recovered from local features and constraints alone. We suspect that, as a result, global constraints will often be easy to satisfy.
 Pseudocode for the algorithm appears in Figure 2. Note that line 2 does not specify how to choose C from among multiple violated constraints. This is discussed in  X  7. Our algorithm resembles the method of Koskenniemi (1990) and later work. The difference is that there lattices are unweighted and may not contain a path that satisfies all constraints, so that the order of constraint intersection matters. The semantic role labeling task (Carreras and M ` arques, 2004) involves choosing instantiations of verb arguments from a sentence for a given verb. The verb and its arguments form a proposition . We use data from the CoNLL-2004 shared task X  X he PropBank (Palmer et al., 2005) annotations of the Penn Treebank (Marcus et al., 1993), with sections 15 X 18 as the training set and section 20 as the de-velopment set. Unless otherwise specified, all mea-surements are made on the development set.

We follow Roth and Yih (2005) exactly, in order to compare system runtimes. They, in turn, follow Hacioglu et al. (2004) and others in labeling only the heads of syntactic chunks rather than all words. We label only the core arguments ( A0  X  A5 ), treating (b) adjuncts and references as O .

Figure 3 shows an example sentence from the shared task. It is marked with an IOB phrase chunk-ing, the heads of the phrases, and the correct seman-tic role labeling. Heads are taken to be the rightmost words of chunks. On average, there are 18.8 phrases per proposition, vs. 23.5 words per sentence. Sen-tences may contain multiple propositions. There are 4305 propositions in section 20. 5.1 Constraints Roth and Yih use five global constraints on label se-quences for the semantic role labeling task. We ex-press these constraints as FSAs. The first two are general, and the seven automata encoding them can be constructed offline:
The last three constraints require information about the example, and the automata must be con-structed on a per-example basis: 5.2 Experiments We implemented our hard constraint relaxation al-gorithm, using the FSA toolkit (Kanthak and Ney, 2004) for finite-state operations. FSA is an open-source C++ library providing a useful set of algo-rithms on weighted finite-state acceptors and trans-ducers. For each example we decoded, we chose a random order in which to apply the constraints.
Lattices are generated from what amounts to a unigram model X  X he voted perceptron classifier of Roth and Yih. The features used are a subset of those commonly applied to the task.
 Our system produces output identical to that of Roth and Yih. Table 1 shows F-measure on the core arguments. Table 2 shows a runtime comparison. The ILP runtime was provided by the authors (per-sonal communication). Because the systems were run under different conditions, the times are not di-rectly comparable. However, constraint relaxation is more than sixteen times faster than ILP despite run-ning on a slower platform. 5.2.1 Comparison to an ILP solver
Roth and Yih X  X  linear program has two kinds of numeric constraints. Some encode the shortest path problem structure; the others encode the global con-straints of  X  5.1. The ILP solver works by relaxing to a (real-valued) linear program, which may obtain a fractional solution that represents a path mixture instead of a path. It then uses branch-and-bound to seek the optimal rounding of this fractional solution to an integer solution (Gu  X  eret et al., 2002) that repre-sents a single path satisfying the global constraints.
Our method avoids fractional solutions: a relaxed solution is always a true single path, which either satisfies or violates each global constraint. In effect, we are using two kinds of domain knowledge. First, we recognize that this is a graph problem, and insist on true paths so we can use Viterbi decoding. Sec-ond, we choose to relax only domain-specific con-straints that are likely to be satisfied anyway (in our domain), in contrast to the meta-constraint of inte-grality relaxed by ILP. Thus it is cheaper on aver-age for us to repair a relaxed solution. (Our repair strategy X  X inite-state intersection in place of branch-and-bound search X  X emains expensive in the worst case, as the problem is NP-hard.) 5.2.2 Constraint violations
The y  X  satisfy most of the global constraints most of the time. Table 3 shows the violations by type.

The majority of best labelings according to the local model don X  X  violate any global constraints X  a fact especially remarkable because there are no label sequence features in Roth and Yih X  X  unigram model. This confirms our intuition that natural lan-guage structure is largely apparent locally. Table 4 shows the breakdown. The majority of examples are very efficient to decode, because they don X  X  require intersection of the lattice with any constraints X  y  X  is extracted and is good enough. Those examples where constraints are violated are still relatively effi-cient because they only require a small number of in-tersections. In total, the average number of intersec-tions needed, even with the naive randomized con-straint ordering, was only 0.65. The order doesn X  X  matter very much, since 75% of examples have one violation or fewer. 5.2.3 Effects on lattice size
Figure 6 shows the effect of intersection with vi-olated constraints on the average size of lattices, measured in arcs. The vertical bars at k = 0 , k = 1 , . . . show the number of examples where con-straint relaxation had to intersect k contraints (i.e., y k = 3 shows how the average lattice size for that subset of examples evolved over the 3 intersections. The X at k = 3 shows the final size of the brute-force lattice on the same subset of examples.

For the most part, our lattices do stay much smaller than those produced by the brute-force algo-rithm. (The uppermost curve, k = 5 , is an obvious exception; however, that curve describes only the seven hardest examples.) Note that plotting only the final size of the brute-force lattice obscures the long trajectory of its construction, which involves 10 in-tersections and, like the trajectories shown, includes longer runtime of the brute-force method (Table 2).
Harder examples (corresponding to longer trajec-tories) have larger lattices, on average. This is partly just because it is disproportionately the longer sen-tences that are hard: they have more opportunities for a relaxed decoding to violate global constraints.
Hard examples are rare. The left three columns, requiring only 0 X 2 intersections, constitute 96% of examples. The vast majority can be decoded without much more than doubling the local-lattice size. The gold standard labels  X  y occasionally violate the hard global constraints that we are using. Counts for the development set appear in Table 5. Counts for violations of N O DUPLICATE A  X  do not include discontinous arguments, of which there are 104 in-stances, since we ignore them.

Because of the infrequency, the hard constraints still help most of the time. However, on a small sub-set of the examples, they preclude us from inferring the correct labeling.

We can apply these constraints with weights, rather than making them inviolable. This constitutes a transition from hard to soft constraints. Formally, a soft constraint C : Y  X  7 X  R  X  is a mapping from a label sequence to a non-positive penalty.

Soft constraints present new difficulty for decod-ing, because instead of eliminating paths of L from contention, they just reweight them.

In what follows, we consider only binary soft constraints X  X hey are either satisfied or violated, and the same penalty is assessed whenever a violation occurs. That is,  X  C  X  C ,  X  w  X  y ,C ( y )  X  X  0 ,w C } . 6.1 Soft constraint relaxation The decoding algorithm for soft constraints is a gen-eralization of that for hard constraints. The differ-ence is that, whereas with hard constraints a vio-lation meant disqualification, here violation simply means a penalty. We therefore must find and com-pare two labelings: the best that satisfies the con-straint, and the best that violates it.

We present a branch-and-bound algorithm (Lawler and Wood, 1966), with pseudocode in Figure 7. At line 9, we process and eliminate a currently violated constraint C  X  C by considering two cases. On the first branch, we insist that C be satisfied, enqueuing L  X  C for later exploration. On the second branch, we assume C is violated by all paths, and so continue considering L unmodified, but accept a penalty for doing so; we immediately explore the second branch by returning to the start of the for loop. 3
Not every branch needs to be completely ex-plored. Bounding is handled by the P RUNE func-tion at line 4, which shrinks L by removing some or all paths that cannot score better than Score ( y  X  ) , the score of the best path found on any branch so far. Our experiments used almost the simplest possi-ble P RUNE : replace L by the empty lattice if the best path falls below the bound, else leave L unchanged. 4
A similar bounding would be possible in the im-plicit branches. If, during the for loop, we find that the test at line 12 would fail, we can quit the for loop and immediately move to the next branch in the queue at line 3.

There are two factors in this algorithm that con-tribute to avoiding consideration of all of the expo-nential number of leaves corresponding to the power set of constraints. First, bounding stops evaluation of subtrees. Second, only violated constraints re-quire branching. If a lattice X  X  best path satisifies a constraint, then the best path that violates it can be no better since, by assumption,  X  y ,C ( y )  X  0 . 6.2 Runtime experiments Using the ten constraints from  X  5.1, weighted naively by their log odds of violation, the soft con-straint relaxation algorithm runs in a time of 58.40 seconds. It is, as expected, slower than hard con-straint relaxation, but only by a factor of about two.
As a side note, softening these particular con-straints in this particular way did not improve de-coding quality in this case. It might help to jointly train the relative weights of these constraints and the local model X  X .g., using a perceptron algorithm (Freund and Schapire, 1998), which repeatedly ex-tracts the best global path (using our algorithm), compares it to the gold standard, and adjusts the con-straint weights. An obvious alternative is maximum-entropy training, but the partition function would have to be computed using the large brute-force lat-tices, or else approximated by a sampling method. For a given task, we may be able to obtain further speedups by carefully choosing the order in which to test and apply the constraints. We might treat this as a reinforcement learning problem (Sutton, 1988), where an agent will obtain rewards by finding y  X  quickly. In the hard-constraint algorithm, for ex-ample, the agent X  X  possible moves are to test some constraint for violation by the current best path, or to intersect some constraint with the current lattice. Several features can help the agent choose the next move. How large is the current lattice, which con-straints does it already incorporate, and which re-maining constraints are already known to be satis-fied or violated by its best path? And what were the answers to those questions at previous stages?
Our constraint relaxation method should be tested on problems other than semantic role labeling. For example, information extraction from bibliography entries, as discussed in  X  1, has about 13 fields to ex-tract, and interesting hard and soft global constraints on co-occurrence, order, and adjacency. The method should also be evaluated on a task with longer se-quences: though the finite-state operations we use do scale up linearly with the sequence length, longer sequences have more chance of violating a global constraint somewhere in the sequence, requiring us to apply that constraint explicitly. Roth and Yih (2005) showed that global constraints can improve the output of sequence labeling models for semantic role labeling. In general, decoding un-der such constraints is NP-complete. We exhibited a practical approach, finite-state constraint relax-ation, that greatly sped up decoding on this NLP task by using familiar finite-state operations X  X eighted FSA intersection and best-path extraction X  X ather than integer linear programming.

We have also given a constraint relaxation algo-rithm for binary soft constraints. This allows incor-poration of constraints akin to reranking features, in addition to inviolable constraints.
 This material is based upon work supported by the National Science Foundation under Grant No. 0347822. We thank Scott Yih for kindly providing both the voted-perceptron classifier and runtime re-sults for decoding with ILP, and the reviewers for helpful comments.

