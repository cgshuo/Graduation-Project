 Online social networks (OSNs) like Facebook, and Myspace recommend new friends to registered users based on local features of the graph (i.e. based on the number of common friends that two users share). However, OSNs do not exploit the whole structure of the network. Instead, they consider only pathways of maximum length 2 between a user and his candidate friends. On the other hand, there are global approaches, which detect the overall path structure in a net-work, being computationally prohibitive for huge-size social networks. In this paper, we define a basic node similar-ity measure that captures effectively local graph features. We also exploit global graph features introducing transitive node similarity. Moreover, we derive variants of our method that apply in signed networks. We perform extensive ex-perimental comparison of the proposed method against ex-isting recommendation algorithms using synthetic and real data sets (Facebook, Hi5 and Epinions). Our experimen-tal results show that our FriendTNS algorithm outperforms other approaches in terms of accuracy and it is also time efficient. We show that a significant accuracy improvement can be gained by using information about both positive and negative edges.
 G.2.2 [ Graph Theory ]: Graph algorithms Algorithms, Experimentation Online social networks (OSNs) such as Facebook.com 1 , Myspace 2 , Hi5.com 3 , etc. contain gigabytes of data that can http://www.facebook.com http://www.myspace.com http://www.hi5.com be mined to make predictions about who is a friend of whom. OSNs gather information on users X  social contacts, construct a large interconnected social network, and recommend other people to users based on their common friends.

In this paper, we focus on recommendations based on links that connect the nodes of an OSN, known as the Link Pre-diction problem, where there are two main approaches that handle it. The first approach is based on local features of a network, focusing mainly on the nodes structure; the second approach is based on global features, detecting the overall path structure in a network.

Facebook.com and Hi5.com, as shown in Figure 1, have adopted a local method for recommending new friends to a target user v 8 :  X  X eople you may know : (i) users v 2 , v because you have one common friend (user v 1 ) (ii) user v because you have one common friend (user v 9 ) . . .  X . The list of recommended friends is ranked based on the number of common friends each candidate friend has with the target user. However, in the aforementioned example, the list of recommended friends cannot be ranked, because the number of common friends is the same for all recommended friends. Thus, user v 8 gets as friend recommendation user v 2 or v or v 4 or v 5 or v 6 with equal probability.

Although Facebook.com and Hi5.com provide friend rec-ommendations, they do not exploit effectively the similari-ties between the social graph nodes. Instead, they consider only pathways of maximum length 2 between a target user and his candidate friends. For example, according to exist-ing OSNs, in Figure 1, user v 8 would get as friend recom-mendation with equal probability user v 2 or v 3 or v 4 or v or v 6 .

However, if we take into account the  X  X trong X  connection between v 8 and v 9 (due to the fact that v 9 does not share many edges with others) then v 4 should have a higher prob-ability to be recommended as a friend to v 8 . In contrast, other candidate friends (e.g. v 2 , v 3 , v 5 , v 6 ) should have a lower probability to be recommended as friends to v 8 be-cause of the  X  X oose X  connection between v 8 and v 1 (due to the fact that v 1 shares many edges with other nodes).
Compared to existing approaches, our method takes into account the local and the global features of a graph. In particular, we define a basic local similarity measure that captures effectively the proximity between neighbor graph nodes. We also exploit global graph features by introduc-ing transitive node similarity. Thus, two persons that are connected with a path have a high probability to know each other, proportionally to: (i) the length of the path they are connected with, and (ii) the degree of similarity between the neighbor nodes that form that pathway.

Compared to the bulk of research on social networks that has focused almost exclusively on positive interpretations of links between people, we also study the interplay between positive and negative relationships. We connect our anal-ysis to theories of signed networks, such as the Structural Balanced theory [8], and the Status theory [10, 11]. More details about these theories can be found later in Section 4.3.
The rest of this paper is organized as follows. Section 2 summarizes the related work, whereas Section 3 briefly re-views preliminaries in graphs employed in our approach. Section 4 defines a node similarity measure in OSNs. A mo-tivating example, the proposed algorithm and an algorithm variation for signed networks are described in Section 4.2. Experimental results are given in Section 5. Finally, Sec-tion 6 concludes this paper.
The research area of link prediction in social networks, tries to infer which new interactions among members of a social network are likely to occur in the near future. There are two main approaches [12] that handle the link prediction problem. The first approach is based on local features of a network, focusing mainly on the nodes structure; the second approach is based on global features, detecting the overall path structure in a network.
 There is a variety of local similarity measures [12] (i.e. Adamic/Adar index, Jaccard Coefficient, Common Neigh-bors index, etc.) for analyzing the  X  X roximity X  of nodes in a network. Among these indices, Adamic/Adar [1] index is re-ported [12] to attain the best performance in predicting new links in a social network. Adamic/Adar index, which is sim-ilar to Jaccard Coefficient (a commonly used similarity met-ric in information retrieval), measures how strongly X  X elated X  two web pages are. Common Neighbors index, also known as Friend of a Friend algorithm (FOAF) [2], is adopted by many popular OSNs, such as facebook.com and hi5.com for the friend recommendation task. FOAF is based on the com-mon sense that two nodes v x and v y are more likely to form a link in the future, if they have many common neighbors. Finally, other local similarity measures are based on prefer-ential attachment [12]. These similarity measures are based on the sum or product of nodes degree. The basic premise of preferential attachment is that the probability that a new edge involves a node is proportional to the current number of its neighbors.
 There is a variety of global approaches [12] (i.e Shortest Path algorithm, RWR algorithm, SimRank algorithm etc.). Liben and Kleinberg [12] claimed that the identification of the shortest path between any pair of nodes in a graph can be used for link prediction (friend recommendation). The computation of the shortest path between two nodes, can be made using any well-known shortest path algorithm [4, 6]. RWR algorithm [15] (Random Walk with Restart algo-rithm) is based on a Markov-chain model of random walk through a graph. RWR considers a random walker that starts from node v x who chooses randomly among the avail-able edges every time, except that, before he makes a choice, with probability c he goes back to node v x (restart). Thus, the relevance score of node v x with respect to node v y is defined as the steady-state probability r v x ,v y that the ran-dom walker will finally stay at node v y . SimRank [9] also computes a global similarity measure based on the struc-tural context of a network that says  X  X wo objects are similar if they are related to similar objects X . Recently, Clauset et al. [3] proposed an algorithm based on the hierarchical network structure.

The novelty of our approach compared to existing ap-proaches is as follows:
In contrast to global algorithms, such as the Random Walk with Restart (RWR) algorithm [15], the Shortest Path [4, 6] algorithm etc., our method also takes into account lo-cal graph features (i.e. the weighted similarity between nodes that may share many edges with others). We selected RWR (reported to present good accuracy results in [12]) and Shortest Path algorithms as representatives of the global al-gorithms and compared them with our method. As will be shown experimentally later, our method outperforms RWR and Shortest Path. The reason is, they traverse globally the social network, missing to capture adequately the local graph characteristics.

In contrast to local similarity measures, such as FOAF [2] algorithm (also known as the Common Neighbors index [13]), the Adamic/Adar [1] index etc., we take into account also global graph features (i.e. paths connecting any pair of nodes in an OSN). We have compared our method against FOAF algorithm and Adamic/Adar index, as representa-tives of the local-based measures. As will be shown experi-mentally later, our method outperforms FOAF and Adamic-Adar index. The reason is, we do not take into account only pathways of length 2 to compute similarity between a pair of nodes in an OSN. Instead, we use an extensive similarity measure that takes into account transitive node similarity.
Apart from the aforementioned link prediction algorithms, which are based solely on the graph structure, there are al-ternative methods [14, 7] that also exploit other data sources such as users messages, users ratings, co-authored papers, common tagging etc. However, we focus only on recommen-dations based on the link structure of an OSN, and thus, we will exclude them from our experimental comparison.
In this section, we present the most important notations and the corresponding definitions used throughout the rest of the paper.
 Let G be a graph with a set of nodes V and a set of edges E . Every edge is defined by a specific pair of graph nodes ( v , v j ), where v i , v j  X  V . We assume that the graph G is undirected and un-weighted, thus the graph edges do not have any weights, plus the order of nodes in an edge is not important. Therefore, ( v i , v j ) and ( v j , v i ) denote the same edge on G . We also assume that the graph G has no multiple edges, thus if two nodes v i , v j are connected with an edge of E , then there is no other edge in E also connecting them. Finally, we assume that there are no loop edges on G (i.e. a node can not be connected to itself). The graph expressing friendships among users of an OSN, which can be seen in Figure 1, will be used as our running example throughout the rest of the paper. For our calculations, we will use well-known representations, such as the adjacency matrix A n  X  n and the incidence matrix R m  X  n .
In this section, we define a basic node similarity mea-sure to determine the proximity between any pair of neigh-bor nodes in a graph G . Therefore, if v i and v j are two neighbor connected nodes of G , we define a specific function sim ( v i , v j ) that expresses their corresponding similarity in the range [0,1] and has all the required properties (i.e. pos-itivity, reflexivity, symmetry etc.) of a well-defined mea-sure. The more similar the nodes are, the more the value of sim ( v i , v j ) will be close to 1. On the contrary, the more dissimilar the nodes are, the more the value of sim ( v i will be close to 0.
 To capture proximity between node vectors, we apply the Jaccard Coefficient, which is able to measure the degree of overlap between node vectors, in contrast to other measures (i.e. dot product, Euclidean distance etc.), which cannot measure it. In particular, we use an extension of the Jac-card Coefficient that contains the cosine similarity metric as we have binary vectors. This extension is also called the Tanimoto coefficient [16], and for two binary vectors r i defined as: By substitution of vector operations between r i , r j in the previous equation with the corresponding values of the inci-dence matrix R , we derive the following equivalent equation: sim ( v i , v j ) = = Note that the term nal derived equation, expresses the number of edges that the nodes v i , v j share, whereas the terms P spectively.

The basic node similarity measure satisfies the positivity property, returning values into the interval [0,1]. Note that the maximum value of similarity (equal to 1) can be reached, when the two nodes are connected with only one edge and have no connections with other nodes. Moreover, Equation 1 can be simplified by using Theorem 1.

Theorem 1. If the basic node similarity measure of Equa-tion 1 is applied in a graph G satisfying all mentioned as-sumptions of Section 3, then it is equivalent with the follow-ing Equation: sim ( v i , v j ) = where deg ( v i ) and deg ( v j ) are the degrees of nodes v v , respectively.

Proof. The fact that ( v i , v j ) /  X  X  and ( v j , v i that nodes v i , v j do not share any edges. Thus the term P h =1 R [ e h , v i ]  X  R [ e h , v j ] in Equation 1 is equal to 0 and sim ( v i , v j ) = 0 .

If nodes v i , v j share one edge, then they can not share any other edge as explained in Section 3. Thus, the term P over, the terms to the degrees of nodes v i , v j , respectively. In that case we have: and the theorem has been proved.

Henceforth, Theorem 1 will be used in defining our ba-sic similarity measure, which is based on the inverse sum of node degrees. However, someone could suggest the usage of any other local-based similarity measure [12] as described in Section 2. For this reason, our basic measure will be later ex-perimentally compared with other measures, which are also based on the nodes degree and the preferential attachment process [12]: the sum of nodes degree and the product of nodes degree.

Now, let us calculate some similarity values on the graph of Figure 1 using Equation 1. The similarity between nodes v 1 and v 2 is: sim ( v 1 , v 2 )= 1 5+2  X  1 = 1 6 =0.16. The similarity between nodes v 2 and v 4 is: sim ( v 2 , v 4 )= 1 2+3  X  1 Thus, the similarity score between nodes v 1 , v 2 is less than that of v 2 , v 4 because the degree of node v 1 is greater than that of v 4 , whereas v 1 shares only one of its total 5 edges.
Collecting all similarity values between the nodes of a graph G , we construct the basic node similarity matrix S of G , which is an n  X  n matrix having n rows and n columns labeled by the graph nodes. The basic node similarity ma-trix values are defined as follows: In our running example, the basic node similarity matrix is depicted in Figure 2, where all values are rounded to the third decimal digit. As shown, user v 9 is more similar with user v 8 than user v 4 . This is reasonable, because user v connected with 2 other nodes ( v 2 and v 3 ), while user v connected with only 1 other node ( v 1 ).

Based on Theorem 1, the similarity values between all non-neighbor nodes in a graph G are zero. For instance, in our running example, the similarity value between nodes v and v 4 is zero, because they do not share any edge. However, users v 1 and v 4 have both user v 2 as a common friend, and thus they could be related in some way.

By using a transitive similarity we can efficiently solve this problem. In our method, we define a transitive node similarity, between two nodes v i and v j , denoted as extended similarity. Extended similarity is calculated by the product of the basic similarities between the nodes of the shortest path from v i to v j .

This shortest path expresses the minimum number of edges required to connect the two nodes, as all edges of graph G are not weighted. Therefore, we define the following extended node similarity measure for any two nodes of G : esim ( v i , v j ) = are all the intermediate nodes that the shortest path from v to v j passes through. Note that, in case that v i , v j are neighbor nodes, the shortest path between them is the single edge connecting them, and this explains why esim ( v i , v sim ( v i , v j ).

In our running example, according to the previous defini-tion, the extended similarity between nodes v 1 and v 4 using Equation 2 equals: sim ( v 1 , v 4 ) = sim ( v 1 , v 2 )  X  sim ( v 2 , v 4 ) = as the shortest path between v 1 , v 4 is: v 1  X  v 2  X  v 4 alternative path v 1  X  v 3  X  v 4 has the same length and the same similarity score since nodes v 3 and v 2 have equal degrees). Note that the extended similarity score between nodes v 1 , v 4 is less than the basic similarity score of v (0.167) and v 2 , v 4 (0.25).

Collecting all the extended similarity values between the nodes of a graph G , we construct the extended node simi-larity matrix ES of G . It is a matrix which has the same dimensionality and structure with the basic node similarity matrix S . Its values are defined as follows:
In our running example, the extended node similarity ma-trix is depicted in Figure 3, where all values are rounded to the third decimal digit.

It is important to note that using the extended node sim-ilarity in a connected graph, such as the graph G of our running example, all values of ES will be positive numbers (non-zero values). This is due to the fact that there is al-ways a shortest path between any pair of node of a connected graph.
In this section, we present the proposed algorithm, de-noted as FriendTNS (Friend Transitive Node Similarity), we analyze its steps, provide implementation details and discuss its time and space complexity.

The basic task of the FriendTNS algorithm is simple: to compute the similarities from a specific node (user) v 0 to all other nodes (users) in a graph, using our basic and ex-tended node similarity measures. The algorithm input is the graph G , the node v 0 , which represents the target user that will take friend recommendations and the number r of friends that will be recommended to him. The output is the recommendations array recom [ r ].

Firstly, FriendTNS initializes the arrays and computes all node degrees from the graph data. Then, it computes all similarities for the target user v 0 in an array s [ n ]. It uses the formula of Theorem 1, if the examined node v i is a neighbor of v 0 . Otherwise, it uses Equation 2. Friends can be recom-mended to v 0 according to their weights in s [ n ]. Therefore, we sort the similarity list s [ n ], we keep an index ind [ n ] for the corresponding node ID X  X , and we recommend the top-r nodes (users), which are not already friends of v 0 .
In our running example, user v 8 would receive user v 4 as friend recommendation, because his similarity score (0.083) is greater than the similarity score of users v 2 , v 3 , v (0.028). Note that the similarity values of the neighbor nodes of v 8 (and v 8 itself) are ignored as these are already friends of the target user v 8 . The resulting recommenda-tion is reasonable, due to the fact that user v 9 (which is re-sponsible for recommending user v 4 to target user v 8 ) does not share many edges with others. In contrast, user v 1 (which is responsible for recommending users v 2 , v 3 , v to target user v 8 ) shares many edges with others. Thus, our FriendTNS algorithm is able to capture the associations among the graph nodes.

FriendTNS keeps the graph nodes and edges in memory using an adjacency list representation, which requires an O ( n + m ) space, where n is the total number of nodes and m is the total number of edges. All other arrays ( s, ind, recom ) require O ( n ) space. Therefore, our FriendTNS total space complexity is O ( n + m ).

For FriendTNS X  X  similarity calculations, we use the one-to-all nodes shortest path algorithm of Fredman-Tarjan [6], which has a complexity of O ( m + n log n ), and when occurs an update into the shortest path tree of the graph, we im-mediately update the transitive similarity values on-the-fly. Therefore, the computational complexity of FriendTNS is O ( m + n log n ).
In this Section, we derive variants of FriendTNS that ap-ply to directed networks and networks with weighted edges, including the case of edges with negative weights (signed networks).

In signed networks edges have positive (+1) as well as negative (-1) weights. Such signed graphs arise for instance in social networks (i.e. Epinions.com, Shashdot Zoo, etc.) where negative edges denote enmity instead of friendship. In such signed graphs, FriendTNS X  X  basic similarity measure of Theorem 1, which is the inverse of the sum of nodes X  degree, can be adjusted accordingly based on the Status theory [10, 11].

Based on Status theory [10, 11], the positive nodes X  in-degree deg + in ( x ) and the negative nodes X  in-degree deg of a node x increase its status. In contrast, the positive nodes X  out-degree deg + out ( x ), and the negative nodes out-degree deg  X  in ( x ) decrease its status. In the following, our basic similarity measure is transformed, so that it can take into account the aforementioned properties of Status The-deg  X  out ( x )  X  deg + out ( x )  X  deg  X  in ( x ).
As already stated, in networks with negative edge weights the concept of transitivity has to take into account nega-tive values. Thus, for our extended similarity measure of Theorem 2, if some edges have negative weight, the total weight of a shortest path can be calculated as the product of the edges X  X  weights, based on the assumption of multi-plicative transitivity of the structural balance theory [8, 11], as formulated in the graph-theoretic language by Hage and Harary (1983).

Structural balance theory considers the possible ways in which triangles on three individuals can be signed. Triangles with three positive signs exemplify the principle that  X  X he friend of my friend is my friend X , whereas those with one positive and two negative edges capture the notions  X  X he enemy of my friend is my enemy X ,  X  X he friend of my enemy is my enemy X , and the  X  X nemy of my enemy is my friend X .
In this section, we compare experimentally our approach with existing friend recommendation algorithms. Hence-forth, our proposed approach is denoted as FriendTNS. We use in the comparison the Random Walk with Restart [15] algorithm, the Shortest Path[4] algorithm, the Adamic and Adar [1] algorithm and the Friend of a Friend [5] algorithm, denoted as RWR, Shortest Path, Adamic/Adar and FOAF, respectively. Our experiments were performed on a 3 GHz Pentium IV, with 2 GB of memory. All algorithms were implemented in C. To evaluate the examined algorithms, we have generated synthetic data sets and chosen three real data sets from the Facebook, Hi5, and Epinions web sites.
We used the Epinions 4 data set, which is a who-trusts-whom social network. In particular, users of Epinions.com express their Web of Trust, i.e. reviewers whose reviews and ratings they have found to be valuable. It contains 49K users and 487K edges among pair of users. Moreover, we crawled the Facebook website on the 30th of October, 2009. Our data crawling method was the following: For each user u , we traverse all his friends and then traverse the friends of each of u  X  X  friends etc. We created a data set with 3694 users, denoted as Facebook 3.7K. Moreover, from the Hi5 http://www.trustlet.org/wiki/Downloaded Epinions dataset web site, we crawled 63329 users and all of their friends, denoted as Hi5 63K 5 , available from the Hi5 site on the 15th of April, 2009. Finally, we also use in our comparison the extended Epinions 132K data set, which consist of positive and negative edges. A positive edge implies friendship/trust whereas a negative edge implies enmity/distrust.
In contrast to purely random (i.e., Erdos-Renyi) graphs, where the connections among nodes are completely inde-pendent random events, our synthetic model ensures depen-dency among the connections of nodes, by characterizing each node with a ten-dimensional vector with each element a randomly selected real number in the interval [  X  1 , 1]. This vector represents the node X  X  intrinsic features such as the profile of a person. Two nodes are considered to be similar and thus of high probability to connect to each other if they share many close attributes. Given a network size N and the degree k of each node, we start with an empty network with N nodes. At each time step, a node with the smallest degree is randomly selected (there is more than one node having the smallest degree). Among all other nodes whose degrees are smaller than k , this selected node will connect to the most similar node with probability 1  X  p , while a ran-domly chosen one with probability p . This process will be terminated when all nodes are of degree k . The parameter p  X  [0 , 1] represents the strength of randomness in generat-ing links, which can be understood as noise or irrationality that exists in almost every real system. Based on the above procedure, we have created 2 synthetic data sets based on different network sizes N (1000, 100000), with k nodes de-gree equal to 10 for the first synthetic data set and with k equal to 100 for the second synthetic data set, whereas p is equal to 0.2 for both data sets.

We calculated several topological properties of the syn-thetic and real data sets which are presented in Figure 4. Figure 4: Topological properties of the real and syn-thetic data sets.

As shown in Figure 4, Epinions 49K and Facebook 3.7K present (i) a large clustering coefficient (LCC), and (ii) a small average shortest path length (ASD). These topological features can be mainly discovered in small-worlds networks. Small-world networks have sub-networks that are character-ized by the presence of connections between almost any two nodes within them (i.e. high LLC). Moreover, most pairs of nodes are connected by at least one short path (i.e. small ASD).
Our Facebook and Hi5 data sets are available in our web site: http://delab.csd.auth.gr/  X  symeon
In contrast, as also shown in Figure 4, Hi5 63K has a very small LLC ( 0 . 02) and a quite big ASD ( 7 . 18). In other words, Hi5 data set can not be considered as a small-world network, since (i) most of its nodes can not be reached from every other by a small number of hops or steps and (ii) does not have sub-networks that are a few edges shy of being cliques.
Our evaluation considers the division of friends of each target user into two sets: (i) the training set E T is treated as known information and, (ii) the probe set E P is used for testing and no information in the probe set is allowed to be used for prediction. It is obvious that, E = E T  X  X  P and E
T  X  X  P =  X  . Therefore, for a target user we generate the recommendations based only on the friends in E T .
Each experiment has been repeated 30 times (each time a different training set is selected at random) and the pre-sented measurements, based on two-tailed t-test, are statis-tically significant at the 0.05 level. All algorithms predict the friends of the target users X  in the probe set.
We use the classic precision/recall metric as performance measure for friend recommendations. For a test user receiv-ing a list of k recommended friends (top-k list), precision and recall are defined as follows:
Precision is the ratio of the number of relevant users in
Recall is the ratio of the number of relevant users in the
Moreover, we use the AUC statistic to quantify the accu-racy of prediction algorithms and test how much better they are than pure chance, similarly to the experimental protocol followed by Clauset et al. [3]. AUC is equivalent to the area under the receiver-operating characteristic (ROC) curve. It is the probability that a randomly chosen missing link (a link in E P ) is given a higher similarity value than a ran-domly chosen non-existent link (a link in U  X  X  T , where U denotes the universal set). In the implementation, among n times of independent comparisons, if there are n 0 times the missing link having higher similarity value and n 00 times the missing link and nonexistent link having the same similarity value, we define AUC, as follows: AUC = ( n 0 +0 . 5  X  n 00 If all similarity values are generated from an independent and identical distribution, the accuracy should be about 0.5. Therefore, the degree to which the accuracy exceeds 0.5 in-dicates how much better the algorithm performs than pure chance.
In this Section, we study the sensitivity of FriendTNS in terms of accuracy performance. In particular, we test how the performance of FriendTNS is affected, when (i) it is combined with different basic similarity metrics, (ii) it runs on synthetic data sets with different controllable sparsity, and (iii) it runs with different graph model randomness.
As the basic similarities of our proposed algorithms are calculated using the inverse sum of node degrees (Theorem 1), it is very interesting to compare the precision of our basic similarity measure with the corresponding precision of two other similarity measures, which are based on preferential attachment process [12]: the sum of node degrees and the product of node degrees. The basic premise of preferential attachment is that the probability that a new edge involves node v i is proportional to current number of neighbors of v . Thus, we used the three aforementioned measures to calculate the extended similarities of FriendTNS algorithm for the synthetic 1K data set and then, we computed the precision attained by each measure vs. the fraction of ob-served links used in the training set ( p parameter is fixed to 0.2). Figure 5(a) depicts the results.

We observe that the inverse sum of nodes degree measure outperforms both other measures. The same result holds for the 100K synthetic data set, but we do not present it due to lack of space. In the following, we adopt the inverse sum of nodes degree measure as the default basic similarity measure of FriendTNS. Notice also that, as we increase the fraction of observed edges, the precision of all algorithms is increased too. This is reasonable, since every prediction algorithm is expected to give higher accuracy for a denser network.
In our synthetic model, the parameter p  X  [0 , 1] repre-sents the strength of randomness in generating links. Next, we test FriendTNS X  X  sensitivity with different graph model randomness. As shown in Figure 5(b), when the strength of randomness is weak, the inverse sum of nodes degree performs better than the other metrics. However, as the strength of randomeness becomes high all metrics cannot perform better than pure chance. Facebook 3.7K, (b) Epinions 49K and (c) Hi5 63K data sets.
In this section we compare FriendTNS with other meth-ods in terms of accuracy. The performance of all tested algorithms should be at least better than the case, where the friend recommendations would be performed by pure chance. Thus, to more meaningfully represent the friend rec-ommendation algorithms X  accuracy performance, we also use as baseline algorithm a pure chance predictor which simply randomly selects pairs of nodes in graph G to be friends. We use the AUC statistic, which looks at an algorithm X  X  over-all ability to rank all the missing connections over nonex-isting ones. For the Epinions 49K data set, as shown in Figure 5(c), we plot a curve for AUC vs. the fraction of ob-served links used in the training set. As shown, FriendTNS does far better than pure chance, indicating that FriendTNS is a strong predictor of missing structure. The main reason is that FriendTNS captures effectively the local and global graph features. Notice that we have also verified the same results for the other real networks (Hi5 and Facebook), but we do not present them due to lack of space.
 Next, we proceed with the comparison of FriendTNS with RWR, Shortest Path, Adamic-Adar, and FOAF algorithms, in terms of precision and recall. This reveals the robustness of each algorithm in attaining high recall with minimal losses in terms of precision. We examine the top-k ranked list, which is recommended to a target user, starting from the top friend. In this situation, the recall and precision vary as we proceed with the examination of the top-k list.
For the Facebook 3.7K data set, in Figure 6(a) we plot a precision vs. recall curve for all algorithms. As expected, all algorithms X  precision falls as k increases. In contrast, as k increases, recall for all algorithms increases as well. FriendTNS attains the best results with impressive high pre-cision. The reason is that FriendTNS exploits global and local features of the social graph by combining the basic with the extended similarity measure. In contrast, RWR traverses only globally the social network, missing to cap-ture adequately the local characteristics of the graph. More-over, Shortest Path does not take into account the reduced similarity between connected nodes that do not share many edges with others. Furthermore, Adamic/Adar and FOAF algorithms exploit only local features of the social network.
The precision of FriendTNS is impressive in this specific data set. The main reason is the topological characteristics of Facebook 3.7K data set. It presents (i) a large cluster-ing coefficient (LCC) equal to 0.32, and (ii) a small average shortest path length (ASD) equal to 3.73. Thus, Facebook 3.7K can be considered as a small-world network .
For the Epinions 49K data set, as shown in Figure 6(b), we also plot precision vs. recall curve for all algorithms. FriendTNS again attains the best results. The precision of FriendTNS is again quite high. Based on its topological features, Epinions 49K can be also considered as a small-world network, since it presents high LLC and small ASD.
For the Hi5 63K data set, as shown in Figure 6(c), we plot precision vs. recall curve for all algorithms. However, the overall performance of FriendTNS, RWR and Shortest Path algorithms is significantly decreased compared with the results in both previous data sets. The main reason is the high sparsity (i.e. very small ADEG equal to 2.78) of the Hi5 data set. Moreover, it has very small LLC and quite big ASD ( 7 . 18). In other words, Hi5 data set can not be considered as a small-world network. In this section, we compare FriendTNS against the RWR, Shortest Path, Adamic/Adar, and FOAF algorithms in terms of time complexity. We have created 2 synthetic data sets based on different network sizes N (1000, 100000), where N is the total number of nodes in the network. For the first synthetic data set the k nodes degree is equal to 10, whereas k is equal to 100 for the second one. All recorded times are refer to the total required time for calculating sim-ilarities for a target node with all other nodes in a graph. Each algorithm X  X  performance is obtained by averaging over 30 independent realizations. Figure 7 depicts the results. As shown, RWR present the worst results because it calcu-lates the inverse of an n  X  n matrix. As expected, FOAF and Adamic/Adar algorithms, outperform the other algo-rithms due to their simpler complexity since they are local-based similarity measures. However, as already shown in Section 5.5, both methods perform the worst results in terms of accuracy. In this section, we present the accuracy performance of FriendTNS when we take into account positive and nega-tive links of a signed network, i.e. extended Epinions 132K data set. We have two different variants of FriendTNS: The first variation considers only positive links and is denoted as FriendTNS + . The second variation considers both pos-itive and negative links and is denoted as FriendTNS +  X  . Figure 8 presents the precision and recall diagram for both versions of FriendTNS. As shown, FriendTNS +  X  outper-forms FriendTNS + . The reason is that FriendTNS +  X  ex-ploits positive and negative links. This means that if we use information about negative edges for predicting the pres-ence of positive edges we get an accuracy improvement of FriendTNS predictions. These results clearly demonstrate that there is, in some settings, a significant improvement to be gained by using information about negative edges, even to predict the presence or absence of positive edges. Figure 8: Accuracy performance of FriendTNS in terms of precision/recall.
In this paper, we proposed the FriendTNS algorithm to provide more accurate friend recommendations. We defined a transitive node similarity measure in OSNs by taking into account local and global features of a social graph. We also derived variants of our method that apply to signed net-works. We performed an extensive experimental comparison using the three real social networks (Facebook, Epinions and Hi5 data sets). We have shown that our FriendTNS algo-rithm provides more accurate friend recommendations com-pared to existing approaches. In future, we want to improve friend recommendations based on other features that OSNs offer, such as photos and video tagging, and common ap-plications. The combination of such features can provide valuable information and therefore yield to more accurate friend recommendations. [1] L. Adamic and E. Adar,  X  X ow to search a social [2] J. Chen, W. Geyer, C. Dugan, M. Muller, and I. Guy, [3] A. Clauset, C. Moore, and M. E. J. Newman, [4] T. Cormen, C. Leiserson, R. Rivest, and S. Stein, [5] Facebook, Official blog, [6] M. Fredman and R. Tarjan,  X  X ibonacci heaps and their [7] J. Golbeck,  X  X ersonalizing applications through [8] P. Hage and F. Harary,  X  X tructural models in [9] G. Jeh and J. Widom,  X  X imrank: a measure of [10] J. Leskovec, D. Huttenlocher, and J. Kleinberg, [11] J. Leskovec, D. Huttenlocher, and J. Kleinberg, [12] D. Liben-Nowell and J. Kleinberg,  X  X he link [13] L. Liben-Nowell, J. Novak, R. Kumar, P. Raghavan, [14] P. Massa and P. Avesani,  X  X rust-aware collaborative [15] J. Pan, H. Yang, C. Faloutsos, and P. Duygulu, [16] T. Tanimoto, IBM Internal Technical Report , 1957.
