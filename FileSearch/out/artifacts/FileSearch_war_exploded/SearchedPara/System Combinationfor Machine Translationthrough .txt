 In the past several years, many machine transla-tion (MT) combination approaches have been developed. Word -level combination approaches, such as the confusion network decoding model, have been quite successful (Matusov et al., 2006; Rosti et al., 2007a; He et al. 2008; Karakos et al. 2008; Chen et al. 2009 a ; Narsale 2010; Leusch 2011 ; Freitag et al. 2014 ). 
I n addition to word -level combination ap-proaches, some phrase -level combination ap-proache s have also recently been developed; the goal is to retain coherence and consistency be-tween the words in a phrase. The most common phrase -level combination approaches are re -decoding methods: by constructing a new phrase table from each MT system X  X  source -to -target phrase alignments, the source sentence can also be re -decode d using the new translation table (Rosti et al., 2007b; Huang and Papineni, 2007; Chen et al., 2007; Chen et al., 2009 b ). One prob-lem with these approaches is that, just with a new phrase table , existing infor mation about word ordering present in the target hypotheses is not utilized ; thus the approaches are likely to make new mistakes of word reordering which do not appear in the target hypotheses of MT engines . Huang and Papineni ( 2007) attacked this issue through a reordering cost function that encour-ages search along with decoding paths from all MT engines X  decoders. 
Another phrase -level combination approach relies on a lattice decoding model to carry out the combination (Feng et al 2009; Du and Way 2010 ; Ma and McKeown 2012 ). In a lattice, each edge is associated with a phrase ( a single word or a sequence of words ) rather than a single word. The construction of the lattice is based on the extraction of phrase pairs from word alig nments between a selected best MT system hypothesi s (the backbone) and the other translation hypothe-ses. One challenge of the lattice decoding model is that it is difficult to consider structural consen-sus among target hypotheses from multiple MT engines , i.e, the consensus among occurrences of discontinuous word s .

I n this paper, w e propose a nother phrase -level combination approach  X  a paraphrasing model using hierarchical para phras e s (para phras es con-tain subpara phras es ) , to fuse target hypotheses. W e dynami cally learn hierarchical para phras es from target hypotheses without any syntactic an-no tations and form a s ynchronous c on text -f ree g rammar (SCFG) (Aho and Ullman 1969) to guide a serie s of transformations of target hy-potheses in to fused translation s. Through these structural transformations , t he paraphrasing model is able to exploit phrasal and structural system -weighted consensus and also able to uti-lize existing information about word ordering present in the target hypotheses . In addition, to conside r a diverse set of plausible fused transla-tions , we develop a hybrid combination architec-t ure , where we paraphrase every target hypothe-sis using different fusing techniques to obtain fused translations for each target , and then make the final selection among all fused translations through a sentence -level selection -based model. 
In short, compared with other related work, our approach features the following advantages: 1. It can consider structural system -weighted 2. It can utilize existing information about 3. It can r etain coherence and consistency 4. The h ybrid combination a rchitecture ena-I n the context of system combination, discrimi-native reranking or post editing , MT researchers ( Rosti et al., 2007a ; Huang and Papineni , 2007 ; Devlin and Matsoukas, 2012, Matusov et al., 2008; Gimpel et al., 2013) have recently show n many positive results if more diverse translations are considered. Inspired by them , we develop a hybrid combination architecture in order to con-sider more diverse fused translations . We para-phrase every target hypothesis to obtain the cor-responding fused translation, and then make the final selection among all fused translations through a sente nce -level selection -based model, shown in Figure 1. In the architecture, different fusing techniques can be used to generate fused translation s for the further sentence -level selec-t i on , enabling us to exploit more sophisticated information of the whole sentence.
 I n th is section, we introduce our paraphrasing model. F or each single target hypothesis, we ex-tract a set of hierarchical para phras es from m on-olingual word alignments between the hypothesis and other hypothes es. Each set of hierarchical para phras es form s a synchronous context -free grammar to guide a serie s of transformations of that target hypothe s i s in to a fused translation . produce the monolingual word alignments . In our system, w e adopt TERp (Snover et al. 2009) , one of the state -of -the -art alignment tools, to serve this purpose. TERp is an ext ens ion of TER (Snover et al. 2006). Both TERp and TER are automatic evaluation metric s for MT , based on measuring the ratio of the number of edit opera-tions between the reference sentence and the MT system hypothesis. The edit operations of TER p include TER X  X  Matches, Insertions, Deletions, Substitutions and Shifts  X  as well as three new edit operations: Stem Matches, Synonym Match-es and P araphrases. A valuable side product of TERp is the monolingual word align ment. A constructed example is shown in Figure 2. 3.1 Hierarchical Paraphrase Extraction We first introduce our notation. For a given sen-sis from MT system h , u se i attached with related word positions , use i e attached with related word positions . For in-For a given sentence i , a MT system h and a MT system k , w e use a SCFG denoted by i represent the set of h ierarchical paraphrases 2007), w e design the follow ing rules to obtain obtained by a aligner, such as TERp .
Please note that for each extracted h ierarchical para phrase - X   X  X   X   X  , X ,  X  would in clude information of word positions while  X  would not.
For a certain target hypothe s i s -i is to paraphrase it to get the fusion output by us-ing a set of hierarchical para phras es , denoted by Q . Thus we create the union of all related hier-archical para phras es learned from i target hypothe s e s . Two special  X  X lue X  rules -
Q . The process can be represented formally in the following: where N is the total number of MT systems. 3.1.1 An Example We use a Chinese -to -English example in Figure 2 to illustrate the extraction process . T he e xtract-ed hierarchical para phrases to paraphrase i EP Because of limited space, only part of the para-phrases , i.e, part of the rules of i Q
Note that, in Table 1 , the rule s (j), (k) and (l) can be regarded as structural paraphrases , and they utilize existing information about word or-dering present in the target hypotheses . S ince rule (l) is included in both i Q say that rule (l) has more structural consensus than rule (j) and (k). And rule (l) also models the word reordering through reversing the order of X 1 and X 2 . By the example, we can see the rea-son why our model is able to exploit structural consensus and also to utilize existing information about word ordering present in the target hypoth-eses . 3.2 Decoding Given a certain target hypothe s i s -i set of hierarchical para phras es -i aims to paraphrase i search for the single most probable derivation via the CKY algorithm with a Viterbi approximation . The derivation is the paraphrased result, i.e, the fusion result indicated in Figure 1 . The single most probable derivation can be represented as generate i h E , J is the number of para phrases used to generate tems. of the system -weighted consensus . penalty. penalty. All weights are trained discriminatively for Bleu score using Minimum Error Rate Train-ing (MERT) procedure (Och 2004) .

T he ideal result of para phras ing i EP in the following, which is supposed to be gener-ated with a higher chance if, regardless of system weights. That is because of the use of the rule s with higher degree of structural consensus , such as (l) and (e) . For a given sentence i and its M multiple fusion phrasing model or the lattice decoding model , the goal here is to select the best one among them , as shown in Figure 1 (For the case shown in the figure, M is 2 N ). The idea is to compare system -weighted consensus among all fusion outputs and transla tions from all MT systems, and then select the one wi th the highest consensus. We adopt Minimum Bayes Risk (MBR) decoding ( Kumar and Byrne , 2004; Sim et al., 2007 ) to serve our purpose and develop the following TER -based MBR : w here TER is Translation Tdit Ratio. fusion weight specific to a certain MT system and a certain fusion model , MT system k and weights are trained discriminatively for Bleu score using MERT . Our experiments are conducted and reported on three datasets: The first dataset includes Chinese -English system translations and reference trans-lations from DARPA GALE 2008 (GALE Chi -Eng). The second dataset includes Chinese -English system translations and reference trans-lations and from NIST 2008 (NIST Chi -Eng). And the third dataset includes Arabic -English system translations and reference translations and from NIST 2008 (NIST Ara -Eng).

Table 3 lists distinguishing machine transla-tion approach es of top five MT of GALE Chi -Eng Dataset . And  X  X wth -pbt -sh X  performs the best in B leu score.

Two combination baselines are implemented for comparison: one is an implementation based on c onfu sion n etwork decoding , and the oth er is L at tice D ecoding from ( Ma and McKeown 2012 ) , both of which are using TERp to obtain word alignments between a selected backbone hypoth-esis and other target hypothe ses. The former uses these word alignments to construct a c onfusion n etwork while the latter extract s phr ases which are consistent with these word alignments to construct a lattice. For both baselines, backbone hypotheses are selected sentence by sentence based on system -weighted consensus among translation of all MT systems. 5.1 Result s In Table 4 , CN represents confusion network; LD represents Lattice Decoding (Ma and McKe-own 2012); PARA represents paraphrasing mod-el proposed in this paper; Backbone_* represents that * is carried out on selected backbones, in contrast with the hybrid combination architecture. Arch_LD represents that only lattice decoding is carried out using hybrid combination architecture. Arch_PARA represents that only paraphrasing model is carried out using hybrid combination architecture. Arch_LD_PARA represents that LD and PARA are both carried out using hybrid combination architecture, which is the examp le shown in Figure 2.

F ro m Table 4 , we can first observe that, f or the three datasets, B ackbone_PARA and B ack-bone_LD out perform B ackbone_ CN, which shows the advantage of using phrases over words in combination. However, B ackbone_PARA does not show improvement over B ackbone_LD . The reason could be that selected backbone s al-ready have a high level of quality and fewer words need to be replaced or re -ordered in con-tra s t with other target hypotheses.
 We find that Arch_PARA performs better than B ackbone_PARA , and Arch_LD performs better than B ackbone_LD . Th is observa tion support s our claim that it is beneficial to consider more diverse set s of plausible fused translations . 
Arch_LD_PARA achieves the best perfor-mance among all techniques used in this paper . I t not only sup port s our claim, but also bring s a conclusion that the paraphrasing model and lat-tice decoding can compensate for the weaknesses of the other in our architecture. 
S ince the paraphrasing model uses hierarchical para phras es to carry out the fusion, it is able to make a big ger degree of word -reordering or structural change on the input hypoth esis in comparison with lattice decoding. We suppose that w hen more word -reordering and structural change s are needed, paraphrasing model can bring more benefits than lattice decoding . Be-cause t he quality of a given translation hypothe-sis is highly related to word reorderin g and struc-tural change, it can be expected that when a poorly translated hypothesis is paraphrased, par-aphrasing model can bring more benefits than lattice decoding . In order to obtain the evidence to support this hypothesis, we carried out the fol-lowing experiment on NIST Chi -Eng Dataset .
For each MT system from the selected top 5 system A -E , we paraphrase its translations using the paraphrasing model and lattice decoding sep-arately, aiming to compare the performances of the two models on each MT system. In other words, we do not first do backbone selection. Every MT system X  X  translation is regarded as a backbone. The results are shown in Table 5.
Among the five MT systems,  X  X ys C  X  and  X  X ys D  X  perform poorer than the other three MT systems. When we paraphrase the two systems, we find that par aphrasin g model outperforms lattice decoding . T hese re sults support our hy-pothesis that w hen more word -reordering and structural changes are needed, paraphrasing model can bring more benefits than lattice de-coding . We view MT combination as a paraphrasing pro-cess using a set of hierarchical para phrases , in which more complicated paraphrasing phenome-na are able to be modeled, such as phrasal and structural consensus . E xisting information about word ordering present in the target hypotheses are also considered. The experiment al results show that our approach can achieve a significant improvem ent over combination baseline s.

T here are many possibilities for enriching the simple framework. Many ideas from recent translation developments can be borrowed and modified for combination. Our f u ture work aims to incorporate syntactic or semantic infor mation into our paraphrasing framework.
 We would like to thank the anonymous review-ers for their valuable comments an d suggestions . This work is supported by the National Science Foundation via Grant No. 0910778 entitled  X  X icher Representations for Machine Transla-tion X . 
