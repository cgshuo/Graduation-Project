 Many real-world graphs have complex labels on the nodes and edges. Mining only exact patterns yields limited in-sights, since it may be hard to find exact matches. However, in many domains it is relatively easy to define a cost (or dis-tance) between different labels. Using this information, it becomes possible to mine a much richer set of approximate subgraph patterns, which preserve the topology but allow bounded label mismatches. We present novel and scalable methods to efficiently solve the approximate isomorphism problem. We show that approximate mining yields interest-ing patterns in several real-world graphs ranging from IT and protein interaction networks to protein structures. Categories and Subject Descriptors: H.2.8 [Database Management]: Database Applications -Data Mining Keywords : approximate graph mining; label costs; ap-proximate subgraph isomorphism
Graphs are a natural way to model many of the mod-ern complex datasets that typically have interlinked entities connected with various relationships. Examples include so-cial, biological and technological networks. Tools for rapidly querying and mining graph data are therefore in high de-mand. Our focus is on graph pattern discovery methods that can simultaneously consider both the structure and content (e.g., node labels).

Most of the prior work on frequent graph mining has fo-cused on exact pattern discovery, which involves two main steps. The first step is to generate non-duplicate candidate patterns, and the second is to compute the frequency of each candidate. The former requires graph isomorphism testing, whereas the latter requires subgraph isomorphism checking, since we need to count all the occurrences of a smaller graph within a much larger graph (or a set of graphs). Many efficient methods have been proposed for mining exact la-beled graph patterns, including both complete search and sampling based approaches [9, 11, 15, 16, 19]. These meth-ods require an exact match between the labels of nodes in the candidate pattern and in the database graph. This can potentially miss many patterns where nodes may share a high label similarity, but may not match exactly. This is  X  specially true for more complex labels (e.g., text data), or in cases where the nodes represent some real-world objects (e.g., proteins, IT infrastructure nodes), where it may be possible to easily design a meaningful cost or distance ma-trix between node  X  X abels X . Unfortunately, exact isomor-phism based methods cannot leverage the rich information from the cost matrix. What is required is a new class of algo-rithms that can mine frequent approximate patterns via ap-proximate subgraph isomorphism that satisfies some bound on the overall cost of the match between a candidate and the database graph(s). Only a few methods have tackled this problem [5,13,20], but they typically enumerate all iso-morphisms, and are therefore not scalable to large graphs due to the combinatorial explosion in the number of isomor-phisms.

In this paper we present a new approach to mine frequent approximate patterns from a single large graph in the pres-ence of a cost matrix between the labels . The method can also be extended to mine approximate patterns from multi-ple graphs by appropriately defining the frequency of a pat-tern. In particular we make the following contributions: i) We propose a novel approach to effectively prune the space of approximate labeled isomorphisms. Instead of enumerating all the possible isomorphisms, we maintain a set of represen-tatives (mappings of the pattern vertex in the database) that is linear in the database and pattern size. Pruning is applied on this set to narrow down the search to only viable map-pings. ii) We propose label based iterative pruning methods to compute the representative sets efficiently. These meth-ods are based on k -hop labels and neighbor concatenated labels. iii) Our method handles both arbitrary as well as binary cost matrices. iv) We place our work within the pat-tern sampling paradigm, thereby avoiding complete search, which can be infeasible in real-world graphs.

We study the effectiveness of the proposed methods on three real-world datasets. The first is a configuration man-agement database graph, where the nodes represents enti-ties comprising the IT infrastructure and the links represent relationships between them; approximate mining yields a richer set of de-facto IT policies in the company. The second dataset is a graph dataset representing 3D protein struc-tures; mined patterns represent approximate motifs. The last dataset comes from a protein interaction network, where the nodes are proteins and edges indicate whether they in-teract physically (i.e., they may bind together or they may be part of the same protein complex); the mined approxi-mate patterns represent protein complexes that take part in important cellular processes. An undirected labeled graph G is represented as a tuple G = ( V G , E G , L ) where V G is the set of vertices, E of edges and L : V G  X   X  is a function that maps vertices to their labels. The neighbors of a vertex v are given as N ( v ) = { u | ( v, u )  X  E G } . A walk in a graph is a sequence of vertices v , . . . , v k such that there is an edge between adjacent pairs of vertices i.e., ( v i , v i +1 )  X  E G and its length is k . A walk is a path if a vertex appears at most once in the sequence.We w rite u k  X  X  X  v if there is a path of length k between u and v . Cost matrix: We assume that there is a cost matrix C :  X  2  X  R  X  0 . The entry C [ l i ][ l j ] denotes the cost of matching the labels l i and l j . Usually C is symmetric and the diagonal entries are 0.
 Approximate subgraph isomorphism: A graph S = ( V
S , E S , L ) is a subgraph of G, denoted S  X  G , iff V S and E S  X  E G . Given a database graph G and a pattern graph P = ( V P , E P , L ), a function  X  : V P  X  V G is called an unlabeled subgraph isomorphism provided  X  is an injec-tive (or one-to-one) mapping such that  X  ( u, v )  X  E P , we have (  X  ( u ) ,  X  ( v ))  X  E G . That is,  X  preserves the topology of P in G . Define the cost of the isomorphism as follows: of matching the node labels in P to the corresponding node labels in G . We say that  X  is an approximate subgraph iso-morphism from P to G provided its cost C (  X  )  X   X  , where  X  is a user-specified cost threshold. In this case we also call P an approximate pattern in G . Note that if  X  = 0, then  X  is an exact subgraph isomorphism between P and G . From now on, isomorphism refers to approximate subgraph isomorphism unless specified otherwise.
 (a) Database Graph G C A B C A 0 0 . 2 0 . 6 B 0 . 2 0 0 . 4
C 0 . 6 0 . 4 0 F igure 1: (a): database graph G , (b): approximate pattern P . (c): cost matrix. (d): approximate embeddings of P . Pattern support: Given a (large) database graph G , a pattern graph P , and the set of all approximate subgraph isomorphisms  X  from P to G , the support of P is some anti-monotonic function sup ( P,  X ), i.e., sup ( P,  X  P )  X  sup ( Q,  X  for all subgraphs Q of P . We discuss some of the common support definitions in Sec 4.2. A pattern P is called fre-quent if sup ( P )  X  minsup , where minsup is a user defined support threshold. P is maximal iff P is frequent and there does not exist any supergraph of P that is frequent in G . Representative set: Given a node u  X  V P , its representa-tive set in the database graph G is the set R ( u ) = { v  X  V G | X   X , such that C (  X  )  X   X  and  X  ( u ) = v } That is, the representative set of u comprises all nodes v in G that u is mapped to in some isomorphism  X  . Fig-ure 1 shows an example database, a cost matrix, an ap-proximate pattern, and its approximate subgraph isomor-phisms for  X  = 0 . 5. There are only two possible approx-imate isomorphisms from P to G , as specified by  X  1 and  X  . For example, for  X  1 , we have  X  1 (1)  X  30,  X  1 (2)  X  10,  X  (3)  X  60, and  X  1 (4)  X  40, as seen in Table 1d. The cost of the isomorphism is C (  X  1 ) = 0 . 4, since C [ L (1)][ L (30)] + representative set for node 1  X  V P is R (1) = { 30 , 40 } . Outline of our Approach: The two main steps in approx-imate graph mining are candidate generation and support computation. Candidate generation explores frequent pat-tern search space. For each candidate we can check whether it is frequent by computing its support. Given a pattern with k vertices, the maximum number of possible isomorphisms is k !  X  | V G | k . It is therefore infeasible to either enumer-ate or store the complete set of isomorphisms. Computing and storing the representative sets is a compromise that will enable us to decide efficiently if a candidate is frequent.
Representative vertex v of a pattern vertex u implies that there exists an isomorphism  X  for which  X  ( u ) = v . One way to interpret it is that the neighborhood of u matches that of v . By comparing the neighborhoods we can find ver-tices that are not valid representatives of u without trying to find an isomorphism exhaustively. Therefore, to com-pute the representative sets we will start with a candidate representative set denoted by R  X  ( u ) and iteratively prune some of the vertices if the neighborhoods cannot be matched. The candidate set is a super set of the representative set, R ( u )  X  R ( u ). An example of a candidate set is R  X  ( u ) = the single vertex pattern with label L ( u ). In this section, we will describe different notions of neighborhood and show how they help us in computing the final representative set of vertices in a pattern. Note that the problem of checking whether a vertex v  X  R ( u ) is an NP-Hard problem, since it involves the approximate subgraph isomorphism problem. The pruning methods typically do not prune all the invalid vertices. So, we use a final enumeration-based verification step to prune the remaining invalid vertices and reduce R to the true R ( u ) (as described in Sec. 3.3). k-hop label is defined as the set of vertices that are reach-able via a path of length k . In other words, k-hop label contains all vertices that are reachable in k hops starting from u and by visiting each vertex at most once. Note that, we use the word label even though we refer to a set of ver-tices. Formally, the k-hop label of a vertex u in graph G , h ( u, G ) = { v | v  X  G, u k  X  X  X  v } . We simply write it as h when the graph is evident from the context. For example, for pattern P in Fig. 2a, the 0-hop label of vertex 5 is h 0 (5) = 5, its 1-hop label is h 1 (5) = 2 , 4 , 6 (we omit the set notation for convenience) and its 2-hop label h 2 (5) = 1 , 3. The minimum cost of matching k-hop labels h k ( u ) and h k ( v ) is where the minimization is over all injective functions f : h the vertex labels. In other words, it is the minimum total cost of matching the vertices present in the k-hop labels. The following theorem places an upper bound on the min-imum cost of matching the k-hop label of a pattern vertex and any of its representative vertices.
 Theorem 1. Given any pattern vertex u , a representative vertex v  X  R ( u ) and cost threshold  X  , then Proof. Consider any isomorphism  X  such that  X  ( u ) = v . It is enough if we can show an injective function f : h k ( u )  X  h ( v ) with a cost  X   X  (as defined in equation 1). We show that  X  on the restricted domain h k ( u ) is one such function f . First, we know that P u  X  V an isomorphism. Second, let u k  X   X  u  X  then  X  ( u  X  )  X  h because for every edge ( u 1 , u 2 ) on a path between u and u in P , (  X  ( u 1 ) ,  X  ( u 2 ))  X  E G . Therefore the cost of matching the k-hop labels using  X  is upper bounded by  X  . Hence, the minimum cost of matching C k [ h k ( u )][ h k ( v )]  X   X  .
Based on the above theorem, a vertex v i s not a repre-However, in practice, it enough to check the condition only for k  X | V P | X  1 because h k ( u ) is the null set  X  k  X | V
Figure 2 shows an example for the k-hop label based prun-ing of the candidate representative set where the threshold  X  = 0 . 5. Consider vertex 2  X  V P and vertex 20  X  V G , we have, C 0 [ h 0 (2)][ h 0 (20)] = 0, since the cost of matching vertex labels C [ L (2)][ L (20)] = 0, as per the label matching matrix C in Fig. 2c. The k-hop labels for k = 1 , 2 , 3 and the minimum of cost matching them are as shown in Table 1, and it can be verified that the minimum cost is within the threshold  X  . Thus far, we cannot prune node 20 from R  X  (2). However, h 4 (2) = 4 , 6 and h 4 (20) = 30 , 60 and the minimum cost of matching them is 0 . 6 &gt;  X  (since C [ L (4)][ L (60)] + from Theorem 1 we conclude that 20 /  X  R  X  (2). This example illustrates that k-hop labels can help prune the candidate representative sets.
 2
F igure 2: Pattern (a), db graph (b), and cost matrix (c).
In neighbor concatenated label (NL), the information re-garding the candidates of a neighbor that were pruned in the previous iteration is used along with the current k-hop label to prune candidates in the current iteration. In con-trast, the k-hop label pruning strategy for a vertex u works independently of the result of k-hop label pruning of other vertices in the pattern. This leads us to the following recur-sive formulation for NL.
 The initial NL label for a vertex is given as  X  0 ( u ) = u . The NL label for the k th iteration, with k  X  1, is then defined as the tuple  X  k ( u ) = ( {  X  k  X  1 ( u  X  ) | u  X   X  N ( u ) } , h first element (denoted X ) of the tuple is the set of NL labels of the neighbors of the vertex u in the previous iteration k  X  1, and the second element (denoted Y ) is the k-hop label defined in the Sec. 3.1. We say that  X  k ( u ) is dominated by  X  k ( v ), denoted as  X  k ( u ) = ( X, Y )  X  k ( v ) = ( X iff i) C k [ Y ][ Y  X  ]  X   X  , i.e., the minimum cost of matching the k-hop labels is within  X  , and ii) there exits an injective function g : X  X  X  X  such that x g ( x ) for all x  X  X i.e., there is a one to one mapping between the NL labels (of the previous iteration k  X  1) of neighbors of u and v . For For example, with  X  = 0 . 5 in Fig. 2,  X  1 (2)  X  1 (20) because C [ h 1 (2)][ h 1 (20)]  X   X  and the NL labels of vertices 1 , 3 , 5 are dominated by the NL labels of vertices 10 , 50 , 30 or 10 , 50 , 60, respectively. The following theorem states that the NL of a pattern vertex u is dominated by the NL of any of its representative vertices v  X  R ( u ).
 Theorem 2. Given any pattern vertex u , a representative vertex v  X  R ( u ) , and cost threshold  X  , then Proof. Let  X  be any isomorphism such that  X  ( u ) = v . We prove the theorem by using induction on k .
 Base case:  X  0 ( u )  X  0 ( v )  X  X  X  C [ L ( u )][ L ( v )]  X   X  is true because v  X  R ( u ) .
 Inductive Hypothesis: Assume that  X  k ( u )  X  k ( v ) holds true for all u  X  V P and v  X  R ( u ) . Now consider  X  k +1 ( X, Y ) and  X  k +1 ( v ) = ( X  X  , Y  X  ) . From theorem 1 we know that C [ Y ][ Y  X  ]  X   X  , for all k  X  0 . Let u  X   X  N ( u ) and v  X  ( u  X  ) . From inductive hypothesis,  X  k ( u  X  )  X  k ( v v  X  N ( v ) because ( u, u  X  )  X  E P =  X  (  X  ( u ) = v,  X  ( u v )  X  E G . Therefore, the injective function  X  maps the ele-ments a  X  X to  X  ( a )  X  X  X  . The theorem follows by NL label definition.

Based on the above theorem, a vertex v c an be pruned from R  X  ( u ) if  X  k ( u ) 6  X  k ( v ) for some k  X  0. In Fig. 2, consider the vertices 3  X  P , 50  X  G and let  X  = 0 . 5. For k = 0, we have,  X  0 (3)  X  0 (50) as C [ B ][ B ] = 0  X   X  . Similarly it is also true for the pairs (2 , 20), (4 , 40), etc. It follows that  X  1 (3)  X  1 (50) as the neighbors 2 , 4 , 6 can be mapped to 20 , 40 , 60 respectively and the minimum cost of matching the 1-hop label is 0 . 4 which is less than the  X  threshold. In the next iteration the NL labels of vertices 3 and 50 are But  X  2 (3) 6  X  2 (50) because the NL label  X  1 (6) is not domi-nated by any of  X  1 (20) ,  X  1 (40) ,  X  1 (60). So, there is no map-ping between the neighbors of vertices 3 and 50 in the second iteration. Hence, 50 /  X  R (3). Note that the k-hop label by it-self cannot prune 50 because the minimum cost of matching the k-hop labels is within  X  as shown in Table 2. There-fore, NL label is more effective compared to k-hop label as it subsumes the latter.
The pruning methods based on k-hop and NL labels start with a candidate representative set R  X  ( u ) and prune some of the candidate vertices based on Theorems 1 and 2. The verification step reduces R  X  ( u ) to R ( u ) by retaining only those vertices v for which there exists an isomorphism  X  in which  X  ( u ) = v . Informally, it does this by checking if the pattern P can be embedded at v such that total cost of label mismatch is at most  X  .
A vertex v  X  R ( u ) iff for any walk W p = u 0 , u 1 , . . . , u (with u = u 0 ) that covers all the edges in pattern P , there exists a walk W g = v 0 , v 1 , . . . , v m (with v = v 0 database graph G , which satisfies the following conditions: i) u i = u j =  X  v i = v j , ii) ( v i , v i +1 )  X  E G , and iii) P C [ L ( u i )][ L ( v i )]  X   X , u i  X  { W p } , i.e., the summation is over each unique vertex u i . Unlike the NL label condition, the above conditions are necessary and sufficient and follow directly from the definition of approximate isomorphism.
To check whether v  X  R ( u ), we first map u to v and subtract the cost of C [ L ( u )][ L ( v )] from the threshold  X  . We then try to map the remaining vertices in P by following W p one edge at a time. In any step ( u i , u i +1 ), if u u i +1 are mapped to x and y in G , respectively, then we ensure that ( x, y )  X  E G (condition ii). If on the other hand, u i +1 is not mapped then we map it to some vertex y  X  R ( u i +1 ) and subtract the cost C [ L ( u i +1 )][ L ( y )] from the remaining  X  threshold. We backtrack if any of the conditions is violated. The vertex v  X  R ( u ) iff we can complete the walk W p satisfying the above three conditions. Note that we have to find one isomorphism that maps u to v , which is much better than enumerating all possible isomorphisms. Consider whether vertex 30  X  R (1) for the pattern in Fig. 1b, and let  X  = 0 . 5. The sequence W p = 1 , 2 , 4 , 3 , 1 is a walk in the pattern that covers all the edges. In gen-eral, finding a walk that covers all the edges in a graph is a special case of Chinese postman problem [6] when the edge weights are one. We first map 1 to 30 an subtract the cost 2 is not mapped we map it to some vertex, say 20  X  R (2). The cost of the mapping is 0 . 2 and the remaining threshold is 0 . 3  X  0 . 2 = 0 . 1. It can be verified that these mappings can-not complete the walk W p . So we backtrack and map 2 to another vertex say 10  X  R (2). This walk can be completed with the mappings as in  X  1 in Table 1d and the remaining cost is 0 . 1. The mappings of the pattern vertices not only implies that 30  X  R (1), it also tells us that 10 , 60 , 40 rep-resent vertices 2 , 3 , 4 respectively. The above procedure can be easily extended to enumerate all the isomorphisms of the pattern, if needed.
Candidate representative vertices are pruned by check-ing for dominance relation between the NL label of a pat-tern vertex and that of a candidate vertex in the database. Comparing the NL labels requires i) computing the cost of matching the k-hop labels ii) matching the neighbors of a pattern vertex with neighbors of a candidate vertex such that the NL of the candidate vertex dominates that of a pattern vertex. The first problem can be formulated as a minimum cost maximum flow in a network, and the second as maximum matching in a bipartite graph.
 Computing k-hop label cost : The minimum cost of match-ing the k-hop labels h k ( u ) and h k ( v ) is equal to minimum cost maximum flow in a network F defined as follows. Each edge in F is associated with a maximum capacity and a cost for sending one unit of flow across it. The network contains a vertex for each label l u = L ( u  X  ) where u  X   X  h k ( u ) and a vertex for each label l v = L ( v  X  ) where v  X   X  h k ( v ). There is a directed edge between source vertex ( s ) and each l u with zero cost and a capacity equal to the multiplicity of the l i.e., the number of vertices in h k ( u ) that have the label l Similarly there is a directed edge between l v and the sink node ( t ). In addition, there is a directed edge from l u with a cost equal to C [ l u ][ l v ] and a capacity equal to the mul-tiplicity of l u . The cost between the k-hop labels is equal to the minimum cost for maximum flow if the maximum flow is equal to | h k ( u ) | and  X  otherwise.
Fig. 3 shows the flow network required to compute the minimum cost of matching the k-hop labels h 2 (2) = 4 , 6 and h (20) = 40 , 50 , 60 as shown in Table 1. The labels of ver-tices in the k-hop labels are C, D and B, B, A respectively. There is an edge from s to each of C, D with zero cost and maximum capacity of one. Similarly, there is an edge from each of A, B to the sink vertex t with zero cost and maxi-mum capacity of one and two respectively. The capacity of the edge between B and t is two because both the vertices 40 and 50 have the same label B . There is an edge from C, D to each of A, B with cost equal to the corresponding entry in the cost matrix C . The maximum flow in the net-work is two and the minimum cost of sending two units of flow 0 . 4 is achieved by pushing a unit flow along the paths s, C, B, t and s, D, A, t . Therefore, the cost of matching the labels h 2 (2) and h 2 (20) is 0 . 4. Thus, vertex 4 with label C can be matched to either 40 or 50 and the vertex 6 to 60. Dominance check : Consider the NL labels  X  k +1 ( u ) = ( X, Y ) and  X  k +1 ( v ) = ( X  X  , Y  X  ). The cost of matching the k-hop labels Y and Y  X  can be computed using the above the network formulation. Finding an injective function f : X  X  X  X  such that x f ( x ), is equivalent to finding a matching of size | N ( u ) | in the bipartite graph with edges ( x, x all x  X  X and x  X  X  X  . The NL label  X  k ( u ) is dominated by  X  ( v ) if the cost between the k-hop labels is within  X  and the size of maximum bipartite matching is | N ( u ) | . Optimization : The candidate pattern may contain groups of symmetric vertices that are indistinguishable with respect to the k-hop label. In such a scenario, the candidate repre-sentative sets of all these vertices are exactly the same. Uti-lizing the symmetry, we can apply the label pruning strat-egy only on one vertex per symmetry group and replicate the results for all other vertices in the group. For example, vertices 1 and 4 in Fig. 1b are symmetric and the represen-tative sets R (1) and R (4) are exactly the same. In abstract algebra terms such groups are called orbits of the graph and can be computed by using the Nauty algorithm [17]. Even though computing the orbits is expensive, we can avoid ( | g | X  1)  X | R  X  ( u ) | NL dominance checks ( g is the size of an orbit) due to the fact that NL dominance checks are per-formed only on one vertex in each group. Note that we find the orbits only for the pattern which is usually very small compared to the database graph.
The k-hop label of a database vertex is independent of the candidate pattern. Also, the flow network to compute the cost of matching the k-hop labels requires only aggregate in-formation about the multiplicity of the vertex label. Hence, we can precompute the k-hop label of database vertices and store them in memory. The following theorem proves that computing k-hop label is expensive.
 Theorem 3. Given a graph G , k , and u  X  V G , then com-puting h k ( u ) is NP-Hard.
 Proof Sketch: We can prove this by using a polynomial time reduction from the Hamiltonian path problem which is NP-Complete [14]. We omit the complete details due to lack o f space.

To compute k-hop label of a vertex u , we check for each vertex v whether u k  X  X  X  v by enumerating all possible k length paths until a path is found. This procedure is ex-ponential, we therefore fix a maximum value k max and use the NL label based pruning only for values of k  X  k max . It takes only a small amount of time to compute the k-hop label for k  X  6 for all the vertices in the database graph; significantly less than the overall run time of the mining al-gorithm. Once h k ( u ) is computed we store in memory only the tuples ( l, m ) where m is the multiplicity of the label l = L ( u  X  ), where u  X   X  h k ( u ). The total amount of main memory required to store the precomputed k-hop labels is O( | V G | X |  X  | X  k max ).
Having described the key contributions of label based prun-ing and candidate representative set verification, we now briefly describe our algorithm for mining approximate sub-graphs in the presence of a label cost matrix. The main steps of the mining algorithm include candidate generation and support computation. The representative set for each pattern vertex comprises a compact view of all the isomor-phisms of the pattern in the input graph. We now show how the representative sets can be used in conjunction with dif-ferent candidate generation and support computation tech-niques to yield approximate graph mining algorithms with different properties.
The search space of the frequent patterns forms a partial order. It can be explored in a depth first or breadth first order but doing so requires computing the canonical code to avoid duplicates. Since the search space is exponential, sampling methods have gained traction recently [4,9]. In our algorithm we employ the depth-first random edge extension strategy we proposed in [2], i.e., we employ random walks over the chains of the frequent subgraph partial order. Each random walk starts with an empty pattern and repeatedly adds a new edge to a new vertex, or connects two existing vertices in the pattern, to generate a new candidate. More precisely, at any stage of the walk let Q be the current fre-quent pattern. A candidate pattern P is generated from Q either by adding a new vertex with label l or by connect-ing two existing vertices u, v  X  V Q . For any vertex u , if u  X  V P  X  V Q then the candidate representative set R  X  ( u ) in P is the same as the representative set R ( u ) verified for Q . Otherwise u  X  V P \ V Q , and the candidate representative set the current representatives if the vertex is already present, otherwise it is the set of vertices in G whose label matching cost is within  X  . Using the label pruning and verification mechanism we compute the representatives of P . Then we decide if the pattern is frequent using the support function that we will define in Sec. 4.2. If the candidate pattern P is frequent, then we continue the walk by extending P . Oth-erwise, we try another random edge extension from Q . If no extension of Q leads to a frequent pattern then by defi-nition Q is maximal and we terminate the current random walk. Using an input parameter K , our algorithm performs K random walks (by default), or outputs K distinct maxi-mal approximate patterns (if desired). Furthermore, if the application requires a complete set of maximal patterns an ordered exploration of the search space may be employed.
The support of a pattern is an anti-monotonic function on the set of isomorphisms of the pattern. The anti-monotonicity means that the support of a pattern cannot be greater than the support of any of its subgraphs. Therefore, if a can-didate pattern is found to be infrequent we can prune the entire subtree under it from the search space. This helps in pruning the otherwise exponential search space.

When mining from a database of graphs, a function as simple as the total number of graphs having at least one isomorphism is anti-monotonic. This approach cannot be used when mining from a single graph as it leads to a bi-nary support function which is not very informative. On the other hand, counting the number of isomorphisms is not anti-monotonic because a graph can have more isomor-phisms compared to its subgraph.

An anti-monotonic support function for a single graph is the maximum number of vertex disjoint isomorphisms. How-ever, this requires computing the maximum independent set (MIS) in a graph where a vertex represents an isomorphism, and an edge exists if the isomorphisms share a vertex in common. This is called the MIS support of the pattern. Clearly, it is not feasible to compute the MIS support when the input graphs are large and patterns have large number of isomorphisms. An easy upper bound on the MIS support is the size of the smallest representative set of a vertex in the pattern [3]. Thus, we define the support of pattern P in a database graph G as That is, the minimum cardinality over all representative sets of vertices in P . The size of representative sets constructed from the disjoint isomorphisms is equal to the MIS support. Hence, sup ( P ) is at least as large as MIS support. Other upper bounds for the MIS value have been proposed in gAp-prox [5] and CMDB-Miner [2] algorithms. The support func-tion used in gApprox can be computed from the representa-tive sets by enumerating the isomorphisms as described in the Sec. 3.3. The support function used by the CMDB-Miner algorithm can also be used by constructing the appropriate flow network on the representative sets. In conclusion, we can mix and match different techniques for candidate gen-eration and support computation to produce different ver-sions of the approximate graph mining algorithm since the isomorphisms are stored as representative sets. Space Complexity : At any given stage of the mining pro-cess, we need to store the candidate representative sets and the precomputed k-hop labels. For a pattern with m ver-tices, the total amount of memory is O( m  X | V G | + k max ( |  X  | X  X  V G | )). The first term corresponds to the representa-tive sets and the second to the precomputed k-hop labels. k max is the maximum value of k for which we compute k-hop labels.
 Time Complexity : The cost of matching the k-hop labels requires at most | h k ( u ) | augmentations in the flow network F , which is an upper bound on the minimum cost, assuming the cost on each edge is at most one. Each augmentation involves cycle detection which takes O( |  X  | 3 ) time because the number of vertices in F is O( |  X  | ). Therefore, the time for minimum cost flow is O( | h k ( u ) | X |  X  | 3 ). The time for the bipartite matching is proportional to the number of vertices and the edges. Since, we try to match the neighbors of a pat-tern and candidate vertex, the number of vertices is bounded by the maximum degree d max of the pattern and candidate vertices. Therefore, the total time for each dominance check is O( | h k ( u ) | X |  X  | 3 + d 3 max ). The number of dominance checks performed per candidate are O( k max  X  n g  X | V G | ) where n is the number of orbit groups in the pattern vertex.
W e ran experiments on several real world datasets to eval-uate the performance of our algorithm. All the experiments were run on an 4GB Intel Core i7 machine with a clock speed of 2 . 67 GHz running Ubuntu Linux 10.04. The code was written in C++ and compiled using g++ version 4 . 4 with -O3 optimization flag. The default number of random walks is K = 500.
 Dataset | V | | E | |  X  | Pre processing time CMDB 10466 15122 84 329 . 3 1s
A CMDB is used to manage and query the IT infrastruc-ture of an organization. It stores information about the so-called configuration items (CIs)  X  servers, software, run-ning processes, storage systems, printers, routers, etc. As such it can be modeled as a single large multi-attributed graph, where the vertices represent the various CIs and the edges represent the connections between the CIs (e.g., the processes on a particular server, along with starting and ending times). Mining such graphs is challenging because they are large, complex, multi-attributed, and have many repeated labels. We used a real-world CMDB graph for a large multi-national corporation (name not revealed due to non-disclosure issues) from HP X  X  Universal Configuration Management Database (UCMDB). Table 3 shows the size of the CMDB graph, and also the time for precomputing the k-hop labels.
 Cost Matrix : The set of labels in a CMDB form a hierarchy which can be obtained from HP X  X  UCMDB. In the absence of domain knowledge, one way to obtain a cost matrix is by assigning low costs for pairs of labels that share many ancestors in the hierarchy and high costs otherwise. The algorithm is general in that it doesn X  X  depend on how the label matching costs are assigned, the range of these val-ues or whether the cost matrix is symmetric. Consider any two labels l 1 , l 2 and their corresponding paths p 1 , p the root vertex in the hierarchy. We first define the sim-ilarity between the labels to be proportional to the num-ber of common labels in p 1  X  p 2 , as follows sim ( l 1 , l C [ l 1 ][ l 2 ] = 1  X  sim ( l 1 , l 2 ). minsup Fwd Fwd Success Back Back Success Table 5: CMDB: Number of extensions and successes R esults : Table 4 shows the time for K = 500 random walks for different values of minsup and  X  = 0 . 5. The average time per random walk is also shown. Somewhat counter-intuitively, the time increases for higher minimum support values. This can be explained by the fact that the CMDB graph contains many relatively small subgraphs with low support, and few relatively large subgraphs with high sup-port. Thus, when minsup is high, more random edge ex-tensions have to be tried to find the frequent ones, whereas when minsup is low fewer random edge extensions are re-quired to locate the frequent patterns. This trend is verified in Table 5, which shows the total number of forward (adding a new label) and backward (connecting two existing vertices in the pattern) edge extensions tried by our algorithm, and also the number of extensions tried that result in a success (i.e., a frequent pattern). We can see that higher minsup in general requires more extensions for the CMDB graph. E xample Patterns : Figure 4 shows a maximal approximate pattern found in the CMDB graph, representing a typical  X  X e-facto X  X onfiguration of the IT infrastructure in this com-pany. It shows the connection between some services run-ning on an NT server, and also the web/ftp services. The node with label 9  X  process indicates that there are nine nodes in the maximal pattern with label process all of which are connected to the nt node. This is an example where the run time for computing the representative sets is signif-icantly reduced by the optimization proposed in section 3.4. All of the nine nodes belongs to the same orbit and hence their representative sets are identical.
T o show the effectiveness of the pruning based on labels, we compared the time taken to enumerate a single maximal pattern in the CMDB graph. We compared the time with and without label-based pruning. Both the methods ter-minated the random walk with the maximal pattern shown in Figure 5. However, the total time taken to enumerate the pattern without using any derived label is 18306 secs whereas by using the NL label the total time reduced to only 15 . 58 secs. The huge difference between the times arises due to the multiplicity (labels with many occurrences) effect in CMDB graphs.
SCOP ( scop.mrc-lmb.cam.ac.uk/scop/ ) is a hierarchi-cal classification of proteins based on structure and sequence similarity. The four levels of hierarchy in this classification are: class, fold, superfamily and family. The 3D structure of a protein can be represented as an undirected graph with the vertex labels being the amino acids, with an edge connecting t wo nodes if the distance between the 3D coordinates of the two amino acids (their  X  -Carbon atoms) is within a thresh-old (we use 7 Angstroms). We constructed a database of 100 protein structures belonging to 5 different families with 20 proteins from each family. We chose the proteins from different levels in the SCOP hierarchy, and we also focused on large proteins (those with more than 200 amino acids). The 3D protein structures were downloaded from the pro-tein data bank ( http://www.rcsb.org/pdb ). The database can be considered as a single large graph with 100 connected components. The graph characteristics and k-hop label pre-computation times are shown in Table 3. For the SCOP dataset, the support is redefined as the number of proteins containing the pattern, i.e., even if a protein contains multi-ple isomorphisms we count them only once for the support. Cost Matrix : Since there are 20 different amino acids, we need a 20  X  20 cost matrix. BLOSUM62 [10] is a commonly used substitution matrix for aligning protein sequences. The i, j entry in BLOSUM denotes the log-odd score of sub-stituting the amino acids a i and a j , defined as: B [ i ][ j ] = be substituted by a j ; f i , f j denote the prior probabilities for observing the amino acids; and  X  is a constant. We compute f i and f j from the database, and then reconstruct p ij = f i f j e  X  B [ i ][ j ] . Next, we define the pair-wise amino acid cost matrix as C [ i ][ j ] = 1  X  p ij p agonal entries are C [ i ][ i ] = 0.
 Results : Table 6 shows the time taken for enumerating ap-proximate maximal patterns for different values of  X  (with fixed minsup = 20). The table shows the time for K = 500 random walks and the average time per walk, with and with-out label pruning. It can be seen that by using the label-based pruning the time for random walks reduces signifi-cantly (by over 100%). As expected, the time increases as the values of  X  increases, since the number of isomorphisms clearly increases for a more relaxed (larger) cost threshold.
Table 7 compares the time taken to mine K = 500 max-imal patterns from the SCOP dataset using the NL label algorithm and two other algorithms, with minsup = 20 and  X  = 1 . 0. The no pruning algorithm computes the represen-tative sets from the candidate representative sets directly us-ing the verification procedure described in section 3.3. The gApprox algorithm is based on [5] and stores all isomor-phisms during the course of enumerating a maximal pattern. It can be seen that the run time for the NL based algorithm is significantly less as it prunes invalid candidates without performing an expensive verification procedure or storing a large number of isomorphisms.

Table 8 shows the time taken for K = 500 random walks for various values of minsup , with  X  = 1 . 0. The table shows the time spent in the k-hop matching, NL matching, and pattern verification steps, and the total time. We see a sim-ilar trend compared to the CMDB graph in terms of the run time, i.e., as minsup increases the time also increases. From Table 9 we can see that the number of forward extensions tried increases with higher minsup , though the number of backward extensions tried decreases slightly. However, the increase in time with higher minsup is also a result of in-creased cost of NL label pruning and the verification steps, both in terms of time (as seen in Table 8) and in terms of the number of such checks (as seen in Table 10).

Table 11 compares the effectiveness of NL and k-hop la-bels for different values of the threshold  X  . For each value of  X  , the top row shows the time with NL label, whereas the bottom row shows the time using only the k-hop label. The NL label clearly reduces the time taken. In fact, it re-minsup k-hop label NL label Verification Total Table 8: SCOP: Time (sec) for Different Steps vs. m insup minsup Fwd Fwd Success Back Back Success minsup neighbor checks k-hop checks Verify checks Table 10: SCOP: Number of matching neighbors, k-hop dis-t ance and verification computations.
Table 11: SCOP: Effectiveness of labels (Time in sec) d uces the time for both the k-hop matching and the pattern verification steps, since NL is very effective in pruning the representative set. This effect is best seen for  X  = 0 . 75, where the total time for verification reduces even though matching the neighbors takes more time compared to k-hop matching. This shows the effectiveness of the NL label ver-sus k-hop label in isolation.
 Example Patterns : Figure 6 show an example approximate protein graph pattern and its corresponding 3D structure extracted from the SCOP dataset. For example, the graph in 6a has support 19, and the structure of one of its occur-rences, in protein PDB:1R2E, is shown in 6b. The common motif comprises the black colored amino acids some of whom are far apart in sequence but are spatially close in 3D. It is important to note that the cost of this isomorphism is C (  X  ) = 0 . 4541, indicating that exact isomorphism cannot find the motif.
F igure 6: SCOP: Approximate Pattern and its Structure We ran experiments on a yeast (Saccharomyces cerevisiae) PPI network. The list of interacting proteins for yeast was downloaded from the DIP database ( http://dip.doe-mbi. ucla.edu ). As seen in Table 3, the PPI network has 4950 proteins and 16,515 interactions. Unlike the other datasets, each node in the PPI network essentially has a unique label, which is the protein name. One of the differences for the PPI graph is that we do not utilize the k-hop labels. The com-plexity of matching the k-hop labels depends on the num-ber of literals in the k-hop label. As each label (protein) is unique in the PPI graph, the number of literals in the k-hop label of a vertex v in a PPI network is equal to the num-ber of vertices reachable in k-hops. This increases the run time for the k-hop label matching. Therefore, for mining PPI networks we use only the neighbor mapping component of NL labels (thus, the pre-processing time for PPI is not applicable in Table 3).
 Cost Matrix : To construct the cost matrix for the protein network we consider the similarity between the protein se-quences for any two adjacent nodes. Sequence similarity is obtained via the BLAST alignment score ( http://blast. ncbi.nlm.nih.gov/ ), that returns the expected value (E-value) of the match. A low E-value implies high similarity, thus we create a binary cost matrix between the proteins by setting C [ p i ][ p j ] = 0 iff the proteins p i and p similarity, i.e., iff E-value ( p i , p j )  X   X  . We empirically set  X  = 0 . 003. Once the binary cost matrix is constructed the algorithm is run with  X  = 0 since the label mismatch is handled by the cost matrix.
Table 12: Time (sec) for random walks in PPI Dataset minsup Fwd Fwd Success Back Back Success R esults : Table 12 shows the time for K = 500 random walks in the yeast PPI network for different values of minsup . It can be seen that the time for random walks decreases as the support value increases, which is opposite to the trend seen for CMDB and SCOP graphs. We verify in Table 13 that for PPI the number of forward extensions is drastically lower for higher minsup values.
 Example Patterns : Figure 7a shows a mined maximal fre-quent approximate pattern (using minsup = 5). The pro-teins are labeled with their DIP identifiers (e.g., DIP-2818N);
Figure 7: Approximate PPI Pattern and GO Enrichment the last number in the label is just a sequential node id. It is worth emphasizing that exact subgraph isomorphism would not yield any patterns in this dataset, since each la-bel is unique. However, since we allow a protein to be re-placed by a similar protein via the cost matrix C , we ob-tain interesting approximate patterns. To judge the qual-ity of the mined patterns we use the gene ontology (GO; www.geneontology.org ), which comprises three structured, controlled vocabularies (ontologies) that describe gene prod-ucts in terms of their associated biological processes (BP), molecular functions (MF), and cellular components (CC). For each of the mined approximate patterns we obtain the set of all the GO terms common to all proteins in the pat-tern. This serves as an external validation of the mined results, since common terms imply meaningful biological re-lationships among the proteins. Figure 7b shows the com-mon GO terms for the pattern in Figure 7a. This subgraph comprises proteins involved in proteolysis as the biological process, i.e., they act as enzymes that lead to the breakdown of other proteins into amino acids. Their molecular function is endopeptidase activity, i.e., breakdown of peptide bonds of non-terminal amino acids, in particular the amino acid Threonine. These proteins are located in the proteasome storage granule, and most likely comprise a protein complex (proteasome)  X  a molecular machine  X  that digests proteins into amino acids.
In the past, many algorithms have been proposed to mine subgraphs from a given database of graphs. These algo-rithms can be mainly divided into two classes depending on how the candidate patterns are generated. Algorithms like those in [11, 12, 15] are Apriori based methods, i.e., a can-didate pattern of size k + 1 is generated by combining two frequent graphs of size k that have a common k  X  1 sized subgraph. Algorithms like those in [19], on the other hand, belong to the class of pattern growth algorithms in which a candidate pattern is generated by extending a frequent pattern with another edge. Sampling approaches like those proposed in [2, 9, 20] mine a representative set of maximal patterns from a database of graphs or a single graph; they are especially effective in large real-world graphs where com-plete graph mining is practically infeasible.

Mining subgraphs from a single graph is a related prob-lem which is surprisingly difficult compared to mining from a database of graphs. In [16], they defined the support of a pattern in a single graph as the maximum number of edge disjoint isomorphisms, which is itself an N P -Hard problem. In [7], they proposed a definition of support based on over-lapping ancestor isomorphisms. In [2], we proposed CMDB-M iner to mine frequent patterns from a single large graph. Support of a pattern is defined as the maximum flow in an appropriately constructed flow network with capacities. This method estimates the support of a pattern without enumerating its isomorphisms.

There has been little work in approximate subgraph min-ing. In [5], they proposed gApprox to mine approximate frequent subgraphs. The degree of approximation between a pattern and its isomorphism includes label mismatches and missing edges. The search space is explored in a depth first order and the support of a pattern is computed by enu-merating all its isomorphisms. This approach is not feasible for large graphs with label multiplicities as there are poten-tially an exponential number of isomorphisms [2]. In [13], they proposed APGM to mine approximate frequent sub-graphs from a database of graphs. The method is similar to the gApprox method, with the main difference being that the entire 1-hop neighborhood of the current embeddings is explored to enumerate all extensions of the frequent pat-tern and their corresponding embeddings, whereas gApprox enumerates the embeddings for a single extension in each step. However, they store the complete set of approximate embeddings of the current frequent pattern, which can be a problem. In [1], the authors proposed strategies to speed up the existing approximate mining algorithms, by limit-ing the number of candidates and also the number of du-plicate checks performed. They assume that the underlying algorithm takes care of the label and/or edge mismatches. In [20], they proposed a randomized algorithm to mine ap-proximate patterns from a database of graphs. This method only handles edge mismatches and not label costs.
Graph querying is another problem that is related to sub-graph mining. The goal is to find matches of a given query graph in a single graph or database of graphs. In [18], they proposed an indexing method to extract the approximate oc-currences of a given graph query in large graph databases. In [8], they proposed a polynomial time algorithm for detect-ing isomorphism between spectrally distinguishable graphs. An isomorphism, if it exists, is obtained by matching the steady state vectors of Markov chains in both the graphs. The problem with the indexing approaches is that they are efficient in retrieving a single match for the query graph but fail at retrieving all matches, and thus are not suited to mine frequent patterns. Furthermore, they assume that the query is given, and thus they do not perform pattern enumeration as required in graph mining.
We presented an effective approach to mine approximate frequent subgraph patterns from a single large graph database in the presence of a label cost matrix.

There are two main parameters in our method: K , the number of random walks, and  X  the cost threshold. The value of K is directly proportional to the number of maxi-mal approximate patterns we desire, and is relatively easy to set. On the other hand, choosing an appropriate value of  X  is very important as it affects the quality of patterns mined. Depending on the application domain and the pur-pose of graph mining, let t be the number of vertices in the pattern for which we allow label mismatches in the subgraph isomorphism. One reasonable value of  X  is t  X  IM Q where IM Q is the inter-quartile mean, i.e., the mean of the entries between the first quartile (25 th percentile) and the third quartile (75 th percentile) of the entries in the cost matrix arranged in sorted order. t can be chosen by first enumer-ating maximal patterns with  X  = 0 and then computing the average size m of the maximal patterns mined from the graph. The value of t then is a fraction of the average size m . Care has to be taken not to choose a very large  X  as it leads to patterns of poor quality and also increases the run time significantly as can be seen in Table 6.

In terms of future work, we plan to increase the efficiency of our method by exploiting parallelism. Obviously different walks can be carried out in parallel. However, more inter-esting is the parallelization of the approximate isomorphism generation and label-based pruning steps, including verifica-tion. We also want to explore the idea of label based pruning for more general definitions of approximate isomorphism in-cluding edge mismatches.
