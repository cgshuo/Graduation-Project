 leozhu@csail.mit.edu Language and image understanding are two major tasks in artificial intelligence. Natural language researchers have formalized this task in terms of parsing an input signal into a hierarchical represen-tation. They have made great progress in both representation and inference (i.e. parsing). Firstly, they have developed probabilistic grammars (e.g. stochastic context free grammar (SCFG) [1] and beyond [2]) which are capable of representing complex syntactic and semantic language phenom-ena. For example, speech contains elementary constituents, such as nouns and verbs, that can be recursively composed into a hierarchy of (e.g. noun phrase or verb phrase) of increasing complex-ity. Secondly, they have exploited the one-dimensional structure of language to obtain efficient polynomial-time parsing algorithms (e.g. the inside-outside algorithm [3]).
 By contrast, the nature of images makes it much harder to design efficient image parsers which are capable of simultaneously performing segmentation (parsing an image into regions) and recogni-tion (labeling the regions). Firstly, it is unclear what hierarchical representations should be used to model images and there are no direct analogies to the syntactic categories and phrase structures that occur in speech. Secondly, the inference problem is formidable due to the well-known complexity and ambiguity of segmentation and recognition. Unlike most languages (Chinese is an exception), whose constituents are well-separated words, the boundaries between different image regions are usually highly unclear. Exploring all the different image partitions results in combinatorial explo-sions because of the two-dimensional nature of images (which makes it impossible to order these partitions to enable dynamic programming). Overall it has been hard to adapt methods from natural language parsing and apply them to vision despite the high-level conceptual similarities (except for restricted problems such as text [4]).
 Attempts at image parsing must make trade-offs between the complexity of the models and the complexity of the computation (for inference and learning). Broadly speaking, recent attempts can be divided into two different styles. The first style emphasizes the modeling problem and develops stochastic grammars [5, 6] capable of representing a rich class of visual relationships and conceptual knowledge about objects, scenes, and images. This style of research pays less attention to the com-plexity of computation. Learning is usually performed, if at all, only for individual components of the models. Parsing is performed by MCMC sampling and is only efficient provided effective pro-posal probabilities can be designed [6]. The second style builds on the success of conditional random fields (CRF X  X ) [7] and emphasizes efficient computation. This yields simpler (discriminative) mod-els which are less capable of representing complex image structures and long range interactions. Efficient inference (e.g. belief propagation and graph-cuts) and learning (e.g. AdaBoost, MLE) are available for basic CRF X  X  and make these methods attractive. But these inference algorithms become less effective, and can fail, if we attempt to make the CRF models more powerful. For ex-ample, TextonBoost [8] requires the parameters of the CRF to be tuned manually. Overall, it seems hard to extend the CRF style methods to include long-range relationships and contextual knowledge without significantly altering the models and the algorithms.
 In this paper, we introduce Hierarchical Image Models (HIM) X  X  for image parsing. HIM X  X  balance the trade-off between model and inference complexity by introducing a hierarchy of hidden states. In particular, we introduce recursive segmentation and recognition templates which represent com-plex image knowledge and serve as elementary constituents analogous to those used in speech. As in speech, we can recursively compose these constituents at lower levels to form more complex constituents at higher level. Each node of the hierarchy corresponds to an image region (whose size depends on the level in the hierarchy). The state of each node represents both the partitioning of the corresponding region into segments and the labeling of these segments (i.e. in terms of objects). Segmentations at the top levels of the hierarchy give coarse descriptions of the image which are refined by the segmentations at the lower levels. Learning and inference (parsing) are made efficient by exploiting the hierarchical structure (and the absence of loops). In short, this novel architecture offers two advantages: (I) Representation  X  the hierarchical model using segmentation templates is able to capture long-range dependency and exploiting different levels of contextual information, (II) Computation  X  the hierarchical tree structure enables rapid inference (polynomial time) and learning by variants of dynamic programming (with pruning) and the use of machine learning (e.g. structured perceptrons [9]).
 To illustrate the HIM we implement it for parsing images and we evaluate it on the public MSRC image dataset [8]. Our results show that the HIM outperforms the other state-of-the-art approaches. We discuss ways that HIM X  X  can be extended naturally to model more complex image phenomena. 2.1 The Model We represent an image by a hierarchical graph defined by parent-child relationships. See figure 1. The hierarchy corresponds to the image pyramid (with 5 layers in this paper). The top node of the hierarchy represents the whole image. The intermediate nodes represent different sub-regions of the image. The leaf nodes represent local image patches ( 27  X  27 in this paper). We use a to index nodes of the hierarchy. A node a has only one parent node denoted by Pa ( a ) and four child nodes denoted by Ch ( a ) . Thus, the hierarchy is a quad tree and Ch ( a ) encodes all its vertical edges. The image region represented by node a is denoted by R ( a ) . A pixel in R ( a ) , indexed by r , corresponds to an image pixel. The set of pairs of neighbor pixels in R ( a ) is denoted by E ( a ) . A configuration of the hierarchy is an assignment of state variables y = { y a } with y a = ( s a , c a ) at each node a , where s and c denote region partition and object labeling, respectively and ( s, c ) is called the  X  X egmentation and Recognition X  pair, which we call an S-R pair . All state variables are unobservable. More precisely, each region R ( a ) is described by a segmentation templates which is selected from a dictionary D S . Each segmentation template consists of a partition of the region into K non-overlapping sub-parts, see figure 1. In this paper K  X  3 , | D s | = 30 , and the segmentation templates are designed by hand to cover the taxonomy of shape segmentations that happen in images, such as T-junctions, Y-junctions, and so on. The variable s refers to the indexes of the segmentation one sub-part as  X  X orse X  another as  X  X og X  and another as  X  X rass X ). Hence c a is a K-dimension vector whose components take values 1 , ..., M where M is the number of object classes. The labeling of a pixel r in region R ( a ) is denoted by o r a  X  { 1 ..M } and is directly obtained from s a , c a . Any two pixels belonging to the same sub-part share the same label. The labeling o r a is defined at the level of node a . In other words, each level of the hierarchy has a separate labeling field. We will show how our model encourages the labelings o r a at different levels to be consistent.
 A novel feature of this hierarchical representation is the multi-level S-R pairs which explicitly model both the segmentation and labeling of its corresponding region, while traditional vision approaches [8, 10, 11] use labeling only. The S-R pairs defined in a hierarchical form provide a coarse-to-fine representation which captures the  X  X ist X  (semantical meaning) of image regions. As one can see in figure 2, the global S-R pair gives a coarse description (the identities of objects and their spatial layout) of the whole image which is accurate enough to encode high level image properties in a compact form. The mid-level one represents the leg of a horse roughly. The four templates at the lower level further refine the interpretations. We will show this approximation quality empirically in section 3.
 The conditional distribution over all the states is given by: where x refers to the input image, y is the parse tree,  X  are the parameters to be estimated, Z ( x ;  X  ) is the partition function and E i ( x, y ) are energy terms. Equivalently, the conditional distribution can be reformulated in a log-linear form: on potential functions defined over the hierarchical structure. There are six types of energy terms defined as follows.
 The first term E 1 ( x, s, c ) is an object specific data term which represents image features of regions. We set E 1 ( x, s, c ) =  X  levels of the hierarchy, and  X  1 ( x, s a , c a ) is of the form: F (  X  ,  X  ) is a strong classifier output by multi-class boosting [12]. The image features used by the classifier ( 47 in total) are the greyscale intensity, the color (R,G, B channels), the intensity gradient, the Canny edge, the response of DOG (difference of Gaussians) and DOOG (Difference of Offset Gaussian) filters at different scales (13*13 and 22*22) and orientations (0,30,60,...), and so on. We use 55 types of shape (spatial) filters (similar to [8]) to calculate the responses of 47 image features. There are 2585 = 47  X  55 features in total.
 The second term (segmentation specific) E 2 ( x, s, c ) =  X  the segmentation templates in which the pixels belonging to the same partitions (i.e., having the same labels) have similar appearance. We define: graph-cut algorithms [13] as edge potentials (only in one level) to favors pixels with similar colour having the same labels.
 The third term, defined as E 3 ( s, c ) =  X  levels are considered and b is the parent of a ) is proposed to encourage the consistency between defined by the Hamming distance: hamming function ensures to glue the segmentation templates (and their labels) at different levels together in a consistent hierarchical form. This energy term is a generalization of the interaction energy in the Potts model. However, E 3 ( s, c ) has a hierarchical form which allows multi-level interactions.
 The fourth term E 4 ( c ) is designed to model the co-occurrence of two object classes (e.g., a cow is unlikely to appear next to an aeroplane): where  X  4 ( i, j, c a , c b ) is an indicator function which equals one while i  X  c a and j  X  c b ( i  X  c a means i is a component of c a ) hold true and zero otherwise.  X  4 is a matrix where each entry  X  4 ( i, j ) encodes the compatibility between two classes i and j . The first term on the r.h.s encodes the classes in a single template while the second term encodes the classes in two templates of the parent-child nodes. It is worth noting that class dependency is encoded at all levels to capture both short-range and long-range interactions. The fifth term E 5 ( s ) =  X  the segmentation template. Similarly the sixth term E 6 ( s, c ) =  X   X  ( s a , j ) = log p ( s a , j ) , models the co-occurrence of the segmentation templates and the object classes.  X  5 ( s a ) and  X  6 ( s a , j ) are directly obtained from training data by label counting. The pa-rameters  X  5 and  X  6 are both scalars.
 Justifications. The HIM has several partial similarities with other work. HIM is a coarse-to-fine representation which captures the  X  X ist X  of image regions by using the S-R pairs at multiple levels. But the traditional concept of  X  X ist X  [14] relies only on image features and does not include segmen-tation templates. Levin and Weiss [15] use a segmentation mask which is more object-specific than our segmentation templates (and they do not have a hierarchy). It is worth nothing that, in contrast to TextonBoost [8], we do not use  X  X ocation features X  in order to avoid the dangers of overfitting to a restricted set of scene layouts. Our approach has some similarities to some hierarchical models (which have two-layers only) [10],[11]  X  but these models also lack segmentation templates. The hierarchial model proposed by [16] is an interesting alternative but which does not perform explicit segmentation. 2.2 Parsing by Dynamic Programming Parsing an image is performed as inference of the HIM. More precisely, the task of parsing is to obtain the maximum a posterior (MAP): The size of the states of each node is O ( M K | D s | ) where K = 3 , M = 21 , | D s | = 30 in our case. Since the form of y is a tree, Dynamic Programming (DP) can be applied to calculate the best parse tree y  X  according to equation 7. Note that the pixel label o a is determined by ( s, c ) , so we only need consider a subset of pixel labelings. It is unlike flat MRF representation where we need to do exhaustive search over all pixel labels o (which would be impractical for DP). The final output of the model for segmentation is the pixel labeling determined by the ( s, c ) of the lowest level. It is straight forward to see that the computational complexity of DP is O ( M 2 K | D s | 2 H ) where H is the number of edges of the hierarchy. Although DP can be performed in polynomial time, the huge number of states make exact DP still impractical. Therefore, we resort to a pruned version of DP similar to the method described in [17]. For brevity we omit the details. 2.3 Learning the Model Since HIM is a conditional model, in principle, estimation of its parameters can be achieved by any discriminative learning approach, such as maximum likelihood learning as used in Conditional Random Field (CRF) [7], max-margin learning [18], and structure-perceptron learning [9]. In this paper, we adopt the structure-perceptron learning which has been applied for learning the recursive deformable template (see paper [19]). Note that structure-perceptron learning is simple to imple-ment and only needs to calculate the most probable configurations (parses) of the model. By con-trast, maximum likelihood learning requires calculating the expectation of features which is difficult due to the large states of HIM. Therefore, structure-perceptron learning is more flexible and compu-tationally simpler. Moreover, Collins [9] proved theoretical results for convergence properties, for both separable and non-separable cases, and for generalization.
 The structure-perceptron learning will not compute the partition function Z ( x ;  X  ) . Therefore we do not have a formal probabilistic interpretation. The goal of structure-perceptron learning is to learn a mapping from inputs x  X  X to output structure y  X  Y . In our case, X is a set of images, with Y being a set of possible parse trees which specify the labels of image regions in a hierarchical form. It seems that the ground truth of parsing trees needs all labels of both segmentation template and pixel labelings. In our experiment, we will show that how to obtain the ground truth directly from the segmentation labels without extra human labeling. We use a set of training examples { ( x i , y i ) : i = 1 ...n } and a set of functions  X  which map each ( x, y )  X  X  X  Y to a feature vector  X  ( x, y )  X  R d . The task is to estimate a parameter vector  X   X  R d for the weights of the features. The feature vectors  X  ( x, y ) can include arbitrary features of parse trees, as we discussed in section 2.1. The loss function used in structure-perceptron learning is usually of form: where y is the correct structure for input x , and y is a dummy variable.
 The basic structure-perceptron algorithm is designed to minimize the loss function. We adapt  X  X he averaged parameters X  version whose pseudo-code is given in figure 3. The algorithm proceeds in a simple way (similar to the perceptron algorithm for classification). The parameters are initialized to zero and the algorithm loops over the training examples. If the highest scoring parse tree for input x is not correct, then the parameters  X  are updated by an additive term. The most difficult step of the method is finding y  X  = arg max y  X  ( x i , y )  X   X  . This is precisely the parsing (inference) problem. Hence the practicality of structure-perceptron learning, and its computational efficiency, depends on the inference algorithm. As discussed earlier, see section 2.2, the inference algorithm has polynomial computational complexity for an HIM which makes structure-perceptron learning practical for HIM. The averaged parameters are defined to be  X  = is the number of epochs, NT is the total number of iterations. It is straightforward to store these averaged parameters and output them as the final estimates. Dataset. We use a standard public dataset, the MSRC 21-class Image Dataset [8], to perform exper-imental evaluations for the HIM. This dataset is designed to evaluate scene labeling including both image segmentation and multi-class object recognition. The ground truth only gives the labeling of the image pixels. To supplement this ground truth (to enable learning), we estimate the true labels (states of the S-R pair ) of the nodes in the five-layer hierarchy of HIM by selecting the S-R pairs which have maximum overlap with the labels of the image pixels. This approximation only results in 2% error in labeling image pixels. There are a total of 591 images. We use the identical splitting as [8], i.e., 45% for training, 10% for validation, and 45% for testing. The parameters learnt from the training set, with the best performance on validation set, are selected.
 Implementation Details . For a given image x , the parsing result is obtained by estimating the best configuration y  X  of the HIM. To evaluate the performance of parsing we use the global accuracy measured in terms of all pixels and the average accuracy over the 21 object classes (global accuracy pays most attention to frequently occurring objects and penalizes infrequent objects). A computer with 8 GB memory and 2 . 4 GHz CPU was used for training and testing. For each class, there are around 4 , 500 weak classifiers selected by multi-class boosting. The boosting learning takes about 35 hours of which 27 hours are spent on I/O processing and 8 hours on computing. The structure-perceptron learning takes about 20 hours to converge in 5520( T = 20 , N = 276) iterations. In the testing stage, it takes 30 seconds to parse an image with size of 320  X  200 (6s for extracting image features, 9s for computing the strong classifier of boosting and 15s for parsing the HIM). Results. Figure 4 (best viewed in color) shows several parsing results obtained by the HIM and by capture different shaped segmentation boundaries (see the legs of the cow and sheep in rows 1 and 3, and the boundary curve between sky and building in row 4). Table 1 shows that HIM improves the results obtained by the classifier by 6 . 9% for average accuracy and 5 . 3% for global accuracy. In particular, in rows 6 and 7 in figure 4, one can observe that boosting gives many incorrect labels. It is impossible to correct such large mislabeled regions without the long-range interactions in the HIM, which improves the results by 20% and 32% .
 Comparisons. In table 1, we compare the performance of our approach with other successful meth-ods [8, 20, 21]. Our approach outperforms those alternatives by 6% in average accuracy and 4% in global accuracy. Our boosting results are better than Textonboost [8] because of image features. Would we get better results if we use a flat CRF with our boosting instead of a hierarchy? We argue that we would not because the CRF only improves TextonBoost X  X  performance by 3 percent [8], while we gain 5 percent by using the hierarchy (and we start with a higher baseline). Some other methods [22, 11, 10], which are worse than [20, 21] and evaluated on simpler datasets [10, 11] (less than 10 classes), are not listed here due to lack of space. In summary, our results are significantly better than the state-of-the-art methods.
 Diagnosis on the function of S-R Pair. Figure 5 shows how the S-R pairs (which include the segmentation templates) can be used to (partially) parse an object into its constituent parts, by the correspondence between S-R pairs and specific parts of objects. We plot the states of a subset of S-R pairs for some images. For example, the S-R pair consisting of two horizontal bars labeled  X  X ow X  and  X  X rass X  respectively indicates the cow X  X  stomach consistently across different images. Similarly, the cow X  X  tail can be located according to the configuration of another S-R pair with vertical bars. In principle, the whole object can be parsed into its constituent parts which are aligned consistently. Developing this idea further is an exciting aspect of our current research. This paper describes a novel hierarchical image model (HIM) for 2D image parsing. The hierarchical nature of the model, and the use of recursive segmentation and recognition templates, enables the HIM to represent complex image structures in a coarse-to-fine manner. We can perform inference (parsing) rapidly in polynomial time by exploiting the hierarchical structure. Moreover, we can learn the HIM probability distribution from labeled training data by adapting the structure-perceptron algorithm. We demonstrated the effectiveness of HIM X  X  by applying them to the challenging task of segmentation and labeling of the public MSRC image database. Our results show that we outperform other state-of-the-art approaches. The design of the HIM was motivated by drawing parallels between language and vision processing. We have attempted to capture the underlying spirit of the successful language processing approaches  X  the hierarchical representations based on the recursive composition of constituents and efficient inference and learning algorithms. Our current work attempts to extend the HIM X  X  to improve their representational power while maintaining computational efficiency. This research was supported by NSF grant 0413214 and the W.M. Keck foundation.

