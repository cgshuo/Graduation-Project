 Clustering techniques have been used to retrieve, filter and categorize docu-ments available on the World Wide Web for their increasing size and dynamic content[2][22][18]. Document clusters can organize large bodies of text for ef-ficient browsing and searching. Clustering techniques have also been used to cluster web users via web log mining [16][21]. Web pages in the discovered clus-ters that have not been explored by the user may serve as navigational hints for the user to follow. User queries have also seen a need to apply clustering tech-niques [20]. The discovered frequently asked questions and most popular topics on a search engine facilitate the query answering substantially.
 example, users and web pages they visit, queries and the documents related to, etc. These kinds of relationships may serve as valuable information to be explored. Users of the same interest are likely to browse the pages on the same topic. Similar queries submitted may reveal the same information need of the users. Hence, the search engine may respond by feeding these users with the documents that highly overlap. A simple way to use these kinds of relationships is to view the links among them as additional features in clustering process. However, such a solution makes the dimensionality even higher without fully exploring the mutual reinforcement between the objects.
 proposes a unified framework for clustering heterogeneous data by using an iter-ative algorithm. This framework uses a two-layered graph with nodes in different layers representing different types of objects as a basis for clustering. However, this approach simply identifies the links by 0 or 1, which will lead to information loss during the process of link merging. Moreover, the link merging process will probably make the error clustering result in one layer be introduced to the other layer and degrade the clustering performance.
 for clustering correlated heterogeneous data objects, which differs from [23] by employing probabilistic model for clustering. We introduce two latent clustering layers, which serve as the mixture probabilistic models to generate the data objects. In each iteration we first use EM algorithm to estimate the parameters of the mixture model in a latent layer. The experimental results show that our algorithm converges after several iterations and the clustering performance is improved during the iterations. The comparative experiment presents that our approach outperforms this method proposed in [23].
 related work on current clustering algorithms. In Section 3, we describe how we estimate the mixture densities parameters via EM algorithm, which is the basis of our framework. In Section 4, a novel unified framework for clustering correlated heterogeneous objects is introduced. We show the experimental results of the proposed approach in Section 5. Finally, we conclude in Section 6. Data clustering is a well studied problem in machine learning [8]. For cluster-ing the highly correlated objects, diverse approaches have been proposed. Some clustering methods cluster objects solely based on content features while other methods treat link information as additional features [14][6][17]. These algo-rithms incorporate the link information as a side-effect and have not fully ex-plored the mutual reinforcement between the web data.
 heterogeneous data. [13] introduces mixture models to cluster the co-occurrence data. [14] presents a systematic, domain-independent framework for unsuper-vised learning from dyadic data by statistical mixture models. Based on as-pect models, probabilistic latent semantic analysis(PLSA) models are proposed [9][11], which provide a probabilistic approach for the discovery of latent vari-ables. Due to its great flexibility, PLSA has been widely and successfully used in variety of application domain, including information retrieval [10], text learning [3][7], co-citation analysis [4] and Web usage mining [15].
 mation is utilized. [19] [12] presents a formal statistical model of collaborative filtering, which characterizes the link information between the web objects in probabilistic manner. Variations of K-means clustering and Gibbs Sampling in-stead of EM algorithm are used to estimate the model parameters. The mixture model framework defines a probabilistic generative model for the data. The model probability distribution, which is defined by a set of parameters  X  , consists of a mixture of components c s  X  C = { c 1 , ..., c | C | } . Each component is parameterized by a disjoint subset of  X  . A data x is created by first selecting a component according to the mixture cluster prior probabilities p ( c s ), then having this selected component generate a data according to its own parameters, with distribution p ( x | c s ). Thus, we can characterize the likelihood of data x with a sum of total probability over all mixture components: and each p ( x | c s ) is the density function of each component parameterized by  X  s . mum likelihood estimation in problems with incomplete data [6]. The mixture-density model parameters estimation problem is probably one of the most widely used applications of the EM algorithm.
 as incomplete and posit the existence of unobserved data items Y = { y s } N c s =1 ,whose values inform us which component density generated each data item. The incomplete-data log-likelihood expression for the data X is given by: Using EM algorithm we can get the expression for p ( c s ), and for some model, it is possible to get analytical expressions for  X  s . [1] introduces how to estimate the parameters of Gaussian Mixture Model using EM algorithm. Following the approach mentioned in [1], we can also estimate the parameters of other mixture models, such as Na  X   X ve Bayes Model and Multinomial Mixture Model.
 4.1 The Data Object Structure We are given two different object layers P and U . Each data object has a content vector, which describes its content feature. Data objects from two different lay-ers are related through weighted links, which can be described by a N p  X  N u Matrix( N p and N u are the object numbers in layer P and U respectively). The weight of the links reveals the association extent between two layer ob-jects. We normalize this matrix and name it as  X  X bject conjunction matrix X  { 4.2 Probabilistic Model with Two Latent Clustering Layers In this subsection, a probabilistic model with two latent clustering layers is pro-posed. Firstly, two mixture models of the latent clustering layers are introduced. Then we introduce the model of the content feature and the link feature, and how the link vectors are calculated. Finally, we describe how the content and link features are combined by the probabilistic function in the mixture model. The calculated parameters of one layer mixture model will be propagated to the other layer by the  X  X omponent Conjunction Matrix X .
 Two Latent Clustering Layers. We propose a probabilistic model to gen-erate the data objects in two layers. We assume the data objects in layer P (or U ) can be clustered into N c (or N g ) clusters and introduce two latent clustering layers C and G . Each latent clustering layer is a mixture model, which generates not only the content feature but also the link feature of its corresponding layer data objects. Latent clustering layer C (or G ) consists of a mixture of N c (or N g ) components. We make an assumption that one layer X  X  data objects are indepen-dent of its opposite latent clustering layer. Figure 2 illustrates the framework, P and U are two object layers , C and G are two latent clustering lay-ers . c s ( s =1 , ..., N c ) are N c components in the mixture model of layer C and g ( t =1 , ..., N g ) are N g components in the mixture model of layer G . weighted links referred to as  X  X omponent links X , which can be viewed as the relationship between the components. We denote it by a N c  X  N g normalized ma-trix, which is referred to as  X  X omponent Conjunction Matrix(CCM) X  { p ( c s ,g t ) } ( s =1 , 2 , ..., N c ; t =1 , 2 , ..., N g ). CCM can be viewed as the prior associated probability of the component link ( c s ,g t ). Thus the prior component probabili-ties p ( c s ) and p ( g t ) can be calculated: p ( u | g t ), according to Bayesian Theorem we can get the probability of p i being generated by component c s and the probability of u j being generated by g t : Each component can be viewed as a virtual cluster, therefore the probability of p being generated by component c s can be viewed as the probability of object p i belonging to cluster c s . Hence, the clustering of objects in layer P can be fulfilled, so can the clustering of objects in layer U .
 Content and Link Features of Data Objects. We now describe how the data object of one layer is generated by its corresponding mixture model. Each data object has both content features and link features. Content features are the intensional attributes of each data object, denoted by a content vector. We can assume these content features of all data objects are generated by a content probabilistic model. For example, if the data objects in layer P are web pages, they can be represented by a keyword vector of term frequency, which can be assumed to be generated by Na  X   X ve Bayes generative model.
 mapped to those between the objects of one layer and the components of the opposite latent clustering layer. For a certain object p in layer P , its weighted links to all objects in layer U are mapped to the links to the components in latent layer G . We call these links the link feature of the data object and denote it by a link vector, which be calculated by the following expression: Here p ( g t | u k ) is the probability that u k belongs to the component g t , and can be calculated by equation 4. p ( u k | p ) can be calculated from  X  X bject conjunction weighted value of the link vector of p . Then we normalize this vector so as that the sum of each item equals to 1.
 page/user clustering, we want to cluster pages into page classes and cluster users into user groups. If two users are interested to a similar extent in the same classes of pages, these two users will have a high probability of belonging to the same user group; on the other hand, if two pages interest the same groups of users to a similar extent, these two pages will also have a high probability belonging to the same page class. So we can regard the link feature as a useful guide in object clustering.
 tion, here we denote by P ) can be viewed as points in a space. Because of assembled together on the hyperplane, they will have a high probability of being in the same cluster. The nearer the region is to the cluster center, the more dense the points are distributed. As mentioned above, the objects in layer P is gener-ated by the mixture model of latent layer C , we assume the assembled points are generated by the component c s . We need a suitable component model to generate them. Commonly, we will select the Gaussian Model. However, the points gen-erated by a Gaussian model are not distributed on a hyperplane. In this paper, we take the Multinominal Model since the sample points generated by it are dis-tributed on a hyperplane. However, each dimension value of the points generated by the Multinomial Model is discrete, so we make a modification by multiplying each dimension value of the link vectors p ( g t | p ) by a observation number N and rounding it to an integer. So N g t =1 N  X  p ( g t | p )= N , the amplified link vectors of all objects in layer P are generated by a N c -Multinomial Mixture Model. We think that our assumption on the distribution model is reasonable by taking into account all the properties of the points mentioned above.
 Combination of Content and Link Features. The data object of one layer is generated by its corresponding mixture model. A data object p in object layer P is created by first selecting a mixture component according to the component prior probabilities, p ( c s ), then having this selected mixture component generate a data object according to its own parameters, with distribution p ( p | c s ). In this paper, we combine the content feature model and link feature model together by The probabilistic distribution for component c s is defined as: where  X  is a predefined parameter. Thus, we can characterize the likelihood of data object p with a sum of total probability over all mixture components: where the parameters are  X  c = { p ( c s ) , X  c c = 1 and each p c ( p | c s ) is the density function of each component of the content feature model parameterized by  X  c c each component of the link feature model parameterized by  X  l c previous part, here we assume that p l ( p | c s ) is a multinomial distribution. content feature obeys. For example, if objects in layer P are web pages, the content feature can be assumed to be generated by Na  X   X ve Bayes model. i.e.  X  1 , ..., N c ; t =1 , ..., N g ) by the following expressions: As mentioned above, p ( c s ) is the prior probability of the component c s . Because all components in the opposite latent layer (see Figure 4), and they also can be viewed as the means of multinomial distribution p l ( p | c s ). i.e.  X  l c for it to be generated by a component c s is: where N is the observation number of the multinomial distribution, here we set it to be N g . Thus we can use EM algorithm to estimate the parameters  X  c of the mixture model.
 a mixture model. We also combine the content feature model and link feature model together. The probabilistic distribution for component g t is defined as: where  X  is the same predefined parameter as equation (5). The likelihood of data object u with a sum of total probability over all mixture components: Where the parameters are  X  g = { p ( g t ) , X  c g = 1 and each p c ( u | g t ) is a density function of each component of the content feature model parameterized by  X  c g component of the link feature model parameterized by  X  l g ( s =1 , 2 , ..., N c ; t =1 , 2 , ..., N g ) by the following expressions: They can be viewed as the parameters of the multinomial distribution which the link feature of u obeys. i.e.  X  l g algorithm to estimate the parameters  X  g of the mixture model.
 part of both the parameters  X  c and  X  g . Now we can see that this is an iterative process. We alternately estimate  X  c and  X  g using EM algorithm and update the CCM each time we get the estimation. 4.3 An Iterative Clustering Algorithm Here we present the details of the iterative clustering algorithm. We can see how to use one equivalent form of CCM to update the other equivalent form, and actually update the CCM itself. Thus we iteratively update the CCM until it converges. The parameters of our algorithm are: where  X  l c And we let:
 X  c = { p ( c s ) , X  c c so we can see  X  =  X  c  X  g .
 1. Random the parameters  X  , including the parameters of probabilistic distri-2. Calculate { p ( c s ) } and { p ( g t | c s ) } according to equation 8. 3. Let  X  c = { p ( c s ) , X  c c 4. Update CCM according to the equation: p ( c s ,g t )= p ( g t | c s )  X  p ( c s ). 5. Calculate the link vectors of data objects in layer U according to the equa-6. Calculate { p ( g t ) } and { p ( c s | g t ) } according to equation 12. 7. Use EM algorithm to estimate the parameters  X  g , which are the parameters 8. Update CCM according to the equation: p ( c s ,g t )= p ( c s | g t )  X  p ( g t ). 9. Calculate the link vector of the data objects in P layer according to the 10. Calculate { p ( c s ) } and { p ( g t | c s ) } according to equation 8. 11. Use EM algorithm to estimate the parameters  X  c , which are the parameters 12. Update CCM according to the equation p ( c s ,g t )= p ( g t | c s )  X  p ( c s ). 13. Go to step 5 until the parameters  X  converge.
 data objects in layer P only according to their content features. Step 5 utilizes the parameters  X  c to calculate the link vectors of objects in layer U . Then step 7 clusters the data objects in layer U according to both their content and link features by estimating the parameters  X  g with initial values calculated at step 6. Similarly step 9 utilizes the parameters  X  g to calculate the link vectors of objects in layer P . Then step 11 clusters the data objects in layer P according to both features by estimating the parameters  X  c with initial values calculated at step 10.  X  c is updated by  X  g and vice versa until the process converges. CCM is viewed as the bridge between parameters  X  c and  X  g . Our first experiment is based on semi-synthetic data which simulates the docu-ments having hyper-links pointing to other ones. The second experiment is based on real query log data, which shows that our algorithm also performs well in the real application. 5.1 Semi-synthetic Data We conduct an experiment based on semi-synthetic data. First we select two groups of topics from Reuters Corpus Volume 1. The first group contains 12 topics and second one contains 8 topics. For each topic, we randomly select 100 to 300 documents to form a collection. The links between the documents of the two groups are randomly generated from a virtual probabilistic model. The similarity between the two collections is computed using the cosine function. The larger the similarity between the two collections is, the higher probability the two documents selected from these collections respectively will be considered as related, that is, to have a link. The content feature of documents in both layers is assumed to be generated by the Na  X   X ve Bayes Model.
 accuracy based on the entropy in information theory [5], which measures the uniformity or purity of a cluster. Assume n objects are clustered into N c clusters, let S denotes the set of objects in a cluster, and the class label of each object x  X  S (where i =1 , ..., | S | ) is denoted by label ( x i ), which takes values c j (where j =1 , ..., N c ). The entropy of cluster S is defined by: tropies, and it is the expected purity calculated on all clusters. The smaller the Entropy avg is, the more accurate the clustering result is.
 the one proposed in [23], respectively. Figure 5 shows the variance of the Entropy avg in different iterations. The horizontal axis denotes the outer iter-ation times and the vertical axis denotes the Entropy avg of each iteration. The solid line and the dashed line denotes our approach and the approach in [23], respectively. As shown in the figure, the result entropy of our approach is lower than theirs and decreases during iterations, which indicates that the combina-tion of the content and link feature by our approach does take effect in clustering the two layer objects. However, the fluctuation of the dashed line demonstrates that the result of the approach in [23] is not satisfactory. We can notice that the polyline depicted in our figure is different from that in [23]. We deduce that the reason is in [23] the inner iteration number of k-means is set to three and it is not guaranteed to converge. For comparability, we modify it by clustering the objects to convergence in each inner iteration. The results also show that our approach performs well when the parameter  X  ranges from 0.3 to 0.5. 5.2 Real Data We conducted experiments on a real data set, the MSN query log in Nov, 2003. After preprocessed, the MSN query log contains 5537 pages, 5163 queries and 16587 links between them. The cluster numbers of pages and queries are both set to 10. Figure 6 shows the convergence of the algorithm. The vertical axis denotes the value |  X  g +1  X   X  g | , which is the closeness of the value  X  between the current and the previous iteration. We can notice it is converged to a low value after 4 to 6 iterations.
 two parts, with 3/4 being the training data and 1/4 being the test data. Our clustering algorithm is run on the training data to train the model, and the log-likelihood of test data is calculated and depicted in Figure 7. The likelihood is the probability of all the test data being generated by the model trained based on the training data. The vertical axis denotes the log-likelihood of test data at each iteration with clustering from that without clustering. We can see that the log-likelihood is increased during each iteration, which indicates that the model trained based on the training data is becoming more and more accurate. In this paper we propose a novel framework for clustering correlated objects. In the framework we combine the content and link feature of the data objects effectively. The iterative clustering algorithm uses one equivalent form of CCM to update the other equivalent form, and actually update the CCM itself. Finally we perform the experiments to demonstrate the effectiveness of our framework and the iterative clustering algorithm.

