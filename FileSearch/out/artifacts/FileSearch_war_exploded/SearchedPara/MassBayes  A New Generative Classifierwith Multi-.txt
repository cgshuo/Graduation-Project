 The learning task in classification is to learn a model from a labelled training set that maps each instance to one of the predefined classes. The model learned is then used to predict a class label for e ach unseen test instance. Each instance x is represented by a d -dimensional vector x 1 ,x 2 ,  X  X  X  ,x d and given a class label y  X  X  y a collection of labelled instances { ( x ( i ) ,y ( i ) ) } ( i =1 , 2 ,  X  X  X  ,N ).
The generative approach of classifier learning models the joint distribution p ( x ,y ) and predicts the most probable class as: Using the product rule, the joint probability can be factorised as: Generative classifiers learn either the joint distribution p ( x ,y ) or the likelihood data modelling techniques is difficult. Density estimators such as Kernel Density Estimation [1], k -Nearest Neighbour [1] and Density Estimation Trees [2] are impractical in large data sets due to their high time and space complexities. The research has thus focused on learning one-dimensional likelihood to approximate p ( x ,y ) in different ways.

Existing generative classifiers allow limited probabilistic dependencies among attributes and assume some kind of conditional independence. Different gener-ative classifiers make different assumptions and allow different level of depen-dencies. They learn a network (or its simplification) of probabilistic relationship between the attributes and estimate the likelihood at each node given its par-ents from D (i.e., one-dimensional likelihood estimation). The joint distribution p ( x ,y ) is estimated as the product of likelihood of each attribute given their parents in the network: where  X  i is parent ( x i )and  X  y is parent ( y ).

Though these one-dimensional likelihood generative classifiers have been shown to perform well [3,4,5,6,7], we hypothesize that a multi-dimensional like-lihood generative classifier will produce even better results.

In this paper, we propose an ensemble approach to estimate multi-dimensional likelihood without making any explicit assumption about attribute indepen-dence. The idea is to construct an ensemble of t multi-dimensional likelihood estimators using random sub-samples D i  X  D ( i =1 , 2 ,  X  X  X  ,t ). Each estimator estimates the multi-dimensional likelihood using a random subset of d attributes from D i . The average estimation from t estimators provides a good approxima-tion of p ( x | y ). We call the resulting generative classifier MassBayes .Ithascon-stant space complexity and constant training time complexity because it employs a fixed-size training subset to build each of the t estimators.

The rest of the paper is structured a s follows. Section 2 provides a brief overview of well-known generative classi fiers. The proposed method is described in Section 3 followed by the implementation details in Section 4. The empirical evaluation results are presented in Sectio n 5. Finally, we provide conclusions and directions for future r esearch in Section 6. Naive Bayes (NB) [3] is the simplest generative approach that estimates p ( x ,y ) by assuming that the attributes are statistically independent given y : Despite the strong independence assumption, it has been shown that NB pro-duces impressive results in many application domains [3,4]. Its simplicity and clear probabilistic semantics have motivated researchers to explore different ex-tensions of NB to improve its performance by relaxing the unrealistic assumption.
BayesNet [5] learns a network of probabilistic relationship among the at-tributes including the class attribute from the training data. Each node in the network is independent of its non-descendants given the state of its parents. At each node, the conditional probabilities with respect to its parents are learned from D . The joint probability p ( x ,y ) is estimated as: Learning an optimal network requires searching over a set of every possible network, which is exponential in d . It is intractable in high-dimensional problems [8]. NB is the simplest form of a Bayesian network, where each attribute is dependent on y only.

In another simplification of BayesNet, A n DE [7] relaxes the independence assumption by allowing dependency between y and a fixed number of privileged attributes or super-parents. The other attributes are assumed to be independent given the n super-parents and y .A n DE with n =0,A0DE,isNB.A n DE avoids the expensive searching in learning probabilistic dependencies by constructing an ensemble of n -dependence estimators. The joint probability p ( x ,y )isestimated as: where S n is the collection of all subsets of size n of the set of d attributes { 1 , 2 ,  X  X  X  ,d } ;and x s is a n -dimensional vector of values of x defined by s .
It has been shown that A1DE and A2DE produce better predictive accuracy than the other state-of-the-art generative classifiers [6,7]. However, it only allows dependencies on a fixed number of attributes and y . Because of the high time complexity of O N d n +1 1 and space complexity of O c d n +1 v n +1 ,where v is the average number of values for an attribute [7], only A2DE or A3DE is feasible even for a moderate number of di mensions. Further more, selecting an appropriate value of n for a particular data set requires a search.
A n DE and many other implementations of B ayesNet require all the attributes to be discrete. The continuous-valued attributes must be discretised using a discretisation method be fore building a classifier. Rather than aggregating an ensemble of n -dependence single-dimensional like-lihood estimators, we propose to aggregate an ensemble of t multi-dimensional likelihood estimators where each likelihood is estimated using different random subsets of d attributes from data. The likelihood p ( x | y ) is estimated as: where G t is a collection of t subsets of varying sizes of d attributes; and x g is a | g | -dimensional vector of values of x defined by g ;and1  X | g | X  d .

Each p ( x g | y ) is estimated using a random subset of training instances D X  D , where |D| =  X &lt;N . where |D y, x g | is the number of instances having attribute values x g belonging to class y in D and |D y | is the number of instances belonging to class y in D .
Rather than relying on a specific discretisation method in the preprocess-ing step, we propose to build a model directly from data, akin to an adaptive multi-dimensional histogram, to determine x g which adapts to the local data distribution. The feature space partitioning we employed (to be discussed in Section 4) produces large regions in sparse area and small regions in the dense area of the data distribution.

Let T (  X  ) be a function that divides the feature space into non-overlapping regions and T ( x ) be the region where x falls. In a multi-dimensional space, each instance in D can be isolated by splitting only on few dimensions i.e., only a the probability of region T ( x ) when only class y instances in D are considered. The new generative classifier, called MassBayes , estimates the joint distribution as:
The average probability of t different regions T i ( x )( i =1 , 2 ,  X  X  X  ,t ), con-structed using D i  X  D , provides a good estimate for p ( x | y )asitestimates the multi-dimensional likelihood by considering the distribution in different lo-cal neighbourhood of x in the data space. An illustrative example is provided in Figure 1. Note that, the estimator employed in MassBayes is not a true density estimator as it does not integrate to 1.

MassBayes has the following characteristics in comparison with A nDE : 1. In each estimator, A n DE estimates one-dimensional likelihood given a 2. In A n DE, the ensemble size is fixed to d n . But, MassBayes allows the flex-3. A n DE requires continuous-valued attributes to be discretised before build-4. Each model in MassBayes is built with training subset of size  X &lt;N which 5. A n DE is a deterministic algorithm wher eas MassBayes is a randomised al-6. Like A n DE, MassBayes is a generative classifier without search. In order to partition the feature space to define the regions T i (  X  ), we use the implementation described by Ting and Wells (2010) using a binary tree called h : d -tree [9]. A parameter h defines the maximum level of sub-division. The maximum height of a tree is h  X  d .
 Let the data space that covers the instances in D be  X  . The data space  X  is adjusted to become  X  using a random perturbation conducted as follows. For each dimension j , a split point v j is chosen randomly within the range max j (  X  )  X  min j (  X  ). Then, the new range  X  j along dimension j is defined as [ v all dimensions defines the adjusted work space for the tree building process.
A subset D is constructed from D by sampling  X  instances without replace-ment. The sampling process is restarted with D when all the instances are used. The random adjustment of the work space and random sub-sampling, as de-scribed earlier, ensure that no two trees are identical.

The dimension to split is selected from a randomised set of d dimensions in a round-robin manner at each level of a tree. A tree is constructed by splitting the work space into two equal-volume half spaces at each level. The process is then repeated recursively on each non -empty half-space. The tree building process stops when there is only one in stance in a node or the maximum height is reached.

At the leaf node, the number of instan ces in the node belonging to each class is stored. Figure 2 shows a typical example of an implementation of T (  X  )asan h : d -tree for h =2and d = 2. The dotted lines enclosed the instances in D and the solid lines enclosed the adjus ted work space which has ranges  X  1 and  X  2 on x 1 and x 2 dimensions. R 1 ,R 2 ,R 3 ,R 4and R 5 represent different regions in T ( depending on the data distribution in D . Region R 1 is defined by splitting the work space in x 1 dimension only, g = { 1 } , whereas the other four regions use dimensions x 1 and x 2 , i.e., g = { 1 , 2 } .

In the original implementation by Ting and Wells (2010) for mass estimation, each tree is built to the maximum height of h  X  d resulting in equal-size regions regardless of the data distribution [9]. In our implementation, in order to adapt to the data distribution, the tree building stops early once the instances are separated. We use the same algorithm as used by Ting and Wells (2010) to generate h : d -trees to represent T i (  X  ) in [9] with the required modification. The procedures to generate t trees from a given data set D are provided in Algorithms 1 and 2.

The maximum height of each tree is hd ,and  X  instances have to be assigned to either of the two child nodes at each level of a tree. Hence, the total training time complexity to construct t trees is O ( thd X  ). There are a maximum of  X  (as  X &lt; 2 hd in general) leaf nodes in each tree. The total space complexity is O ( t ( d + c )  X  ).

The time and space complexities of two variants of NB (NB-KDE that es-timates p ( x i | y ) through kernel density estimation [4]; and NB-Disc that esti-mates p ( x i | y ) through discretisation [10]), A n DE and MassBayes are presented in Table 1. Both training time complexity and space complexity of MassBayes are independent of N . Note that the complexities for NB-Disc and A n DE do not include the additional discretisation needed in the preprocessing.
 Algorithm 1. BuildTrees( D,t, X ,h ) Algorithm 2. SingleTree( D ,min,max,,A ) This section presents the results of the experiments conducted to evaluate the performance of MassBayes against seven well known contenders: two variants of NB (NB-KDE and NB-Disc), BayesNet, three variants of A n DE (A1DE, A2DE, A3DE) and decision tree J48 (i.e., the WEKA [11] version of C4.5 [12]).
MassBayes was implemented in Java using the WEKA platform [11] which also has implementations of NB, BayesNet, A1DE and J48. For A2DE and A3DE, we used the WEKA implementations provided by the authors of A n DE.
All the experiments were conducted using a 10-fold cross validation in a Linux machine with 2.27 GHz processor and 100 GB memory. The average accuracy (%) and the average runtime (seconds) over a 10-fold cross validation were re-ported. A two-standard-error significance test was conducted to check whether the difference in accuracies of two classi fiers was significant. A win or loss was counted if the difference was significant; otherwise, it was a draw.
Ten data sets with N&gt; 10000 were used. All the attributes in the data sets are numeric. The properties of the data sets are provided in Table 2. The RingCurve, Wave and OneBig data sets were three sy nthetic data sets and the rest were real-world data sets from UCI Machine Learning Repository [13]. RingCurve and Wave are subsets of the RingCurve-Wave-TriGaussian data set used in [9] and OneBig is the data set used in [14].

For A n DE, BayesNet and NB-Disc, data sets were discretised by a supervised discretisation technique based on minimum entropy [15] as suggested by the authors of A n DE before building the classification models.
 Two variants of MassBayes were used: MassBayes with (  X  = 5000) and MassBayes (  X  = N ). The other two parameters t and h were set as default to 100 and 10, respectively.

For BayesNet, the parameter  X  X aximum number of parents X  was set to 100 to examine whether a large number of parents produces better results; and the parameter  X  X nitialise as Naive Bayes X  was set to  X  X alse X  to initialise an empty network structure. The default values were used for the rest of the parameters. All the other classifiers were executed with the default parameter settings. Table 3 shows the average classi fication accuracies of MassBayes and Mass-Bayes in comparison to the other contenders. The results of the two-standard-error significance test in Table 4 show that both MassBayes and MassBayes produced better classification accuracy than the other contenders in most data sets.
 MassBayes produced slightly poorer results than A2DE, A3DE, BayesNet and J48 in CoverType. This was because the default sample size was not enough to yield a good estimate. The accura cy was increased up to 84.62% with  X  = 20000 and 88.66% with  X  = 50000. More samples are required to grow the trees further to model the distributions well if the class distributions in the feature space are complex. Figure 4(a) shows the imp rovement in accuracy of MassBayes in CoverType when the sample size was increased.

Table 5 presents the average runtime. In terms of runtime, MassBayes was an order of magnitude faster than A2DE in MiniBooNE; BayesNet in Cover-Type, MiniBooNE and OneBig; NB-KDE in MiniBooNE and OneBig; and J48 in CoverType and MiniBooNE. It was of the same order of magnitude as A3DE, A2DE, BayesNet, NB-KDE and J48 in many cases and an order of magnitude slower than NB-Disc and A1DE. MassBayes was an order of magnitude slower than the other contenders in many data sets. However, it was of the same order of magnitude as A3DE in Letters; A2DE in MiniBooNE; BayesNet and NB-KDE in MiniBooNE and OneBig; and J48 in CoverType and MiniBooNE.

Note that the reported runtime results for A n DE, BayesNet and NB-Disc did not include the discretisation time that must be done as a preprocessing step, which give the existing generative classifiers (except NB-KDE) an unfair advantage over MassBayes. The discretisation time can be substantially large in large data sets. For example, the discretisation took 52 seconds in the largest data set, CoverType. This discretisation time alone was more than the total runtime of MassBayes. Thus, MassBayes in effect runs faster than all existing generative classifiers on equal footing.

In order to examine the scalability of the classifiers in terms of training time and space requirements with the increase in training size N , we used the 48-dimensional (42 irrelevant attributes with constant values) RingCurve-Wave-Tri-Gaussian data set previously employed by Ting and Wells (2010) in [9]. The training data size was increased fr om 7000 to 70000, half-a-million, 1 million and 10 million by a factor of 1, 10, 75, 150 and 1500, respectively. Figure 3 shows the increase in classification model building time and memory space required to store the classification model for different generative classifiers. Note that the discreti-sation time was not included in the presented results. The discretisation time increases linearly with the increase in training data size. This additional time for discretisation will increase the training time of A n DE, BayesNet and NB-Disc. MassBayes had constant training time and constant space requirements.
In order to examine the sensitivity of the parameters  X  , t and h in classifica-tion accuracy and runtime of MassBayes, we conducted a set of experiments by varying one parameter and fixing the other two to the default values. The result of the experiment varying  X  and t in the largest data set (CoverType) is shown in Figure 4. The increase in runtime was plotted as a ratio to show the factor of runtime increased when the p arameters were increased.

In general, accuracy increased up to a certain point and remained flat when each of the three parameters was increased. This indicates that the parameters of MassBayes are not too sensitive in term s of classification accuracy if they are set to sufficiently high values. The r untime increased linearly with t and sub-linearly with  X  .Withfixedsamplesize(  X  = 5000), increase in h after a certain point did not affect the runtime because the tree building process stopped before reaching the maximum level h once the instances are separated. In this paper, we presented a new generative classifier called MassBayes that approximates p ( x | y ) by aggregating multi-dimensional likelihoods estimated us-ing varying size subsets of features from random subsets of training data. In contrast, existing generative classifiers make assumptions about attribute inde-pendence and estimate single-dimensional likelihood only. Our empirical results show that MassBayes produced better cla ssification accuracy than the existing generative classifiers in large data sets.

In terms of runtime, it scales better than the existing generative classifiers in large data sets as it builds models in an ensemble using fixed-size data subsets. The constant training time and space complexities make it an ideal classifier for large data sets and data streams.

Future work includes applying the proposed method in data sets with discrete and mixed attributes and investigating t he effectiveness of MassBayes in the data stream context. In this paper, we have rigorously assessed MassBayes with the state-of-the-art Bayesian classifiers. In the near future, we will assess its perfor-mance against some well-known discriminative classifiers and their ensembles. The feature space partitioning can be implemented in various ways. It would be interesting to investigate a more intelligent way of feature space partitioning rather than dividing at mid-point of a randomly selected dimension. Acknowledgement. This work is supported by the Air Force Research Labo-ratory, under agreement# FA2386-11-1-4112. Sunil Aryal is partially supported by Monash University Postgraduate Publications Award to write this paper. We would like to thank Geoff Webb and the anonymous reviewers for their helpful comments.

