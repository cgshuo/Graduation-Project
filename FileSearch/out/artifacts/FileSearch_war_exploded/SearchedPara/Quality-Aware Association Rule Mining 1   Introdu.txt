 Quality in data mining results critically depends on the preparation and on the quality of analyzed datasets [10]. Indeed data mining processes and applications require vari-ous forms of data preparation, correction and consolidation combining complex data transformation operations and cleaning techniques [11], because the data input to the mining algorithms is assumed to conform to  X  X ice X  data distribu tions, containing no missing, inconsistent or incorrect values [15]. This leaves a large gap between the available  X  X irty X  data and the available machinery to process and analyze the data for discovering added-value knowledge and decision making [9]. Data quality is a multi-dimensional, complex and morphing concep t [4]. Since a decade, there has been a significant amount of work in the area of information and data quality management initiated by several research communities (database, statistics, workflow management, knowledge management), ranging from techniques in assessing information quality [13] to building large-scale data integration systems over heterogeneous data sources with different degrees of quality and trust. In error-free data warehouses or database-backed information systems with perfectly clean data, knowledge discovery tech-niques (such as clustering, mining association rules or visualization) can be relevantly used as decision making processes to automatically derive new knowledge patterns and new concepts from data. Unfortunately, most of the time, these data are neither rigorously chosen from the various heterogeneous sources with different degrees of quality and trust, nor carefully controlled for quality [9]. Deficiencies in data quality still are a burning issue in many application areas, and become acute for practical applications of knowledge discovery and data mining techniques [5]. We illustrate this idea with the following example in the context of association rule mining. Among traditional descriptive data mining techniques, association rule mining identifies intra-transaction patterns in a database and describes how much the presence of a set of attributes in a database's record ( i.e. , a transaction) implicates the presence of other distinct set of attributes in the same reco rd (respectively the same transaction). The quality of discovered association rules is commonly evaluated by interestingness measures (namely support and confidence). The support of a rule measures the occur-rence frequency of the pattern in the rule while the confidence is the measure of the strength of implication. The problem of mining association rules is to generate all association rules that have support and confidence greater than the user-specified minimum support and confidence thresholds. Besides support and confidence, other interestingness measures have been proposed in the literature for knowledge quality evaluation with the purpose of supplying subsidies to the user in the understanding and use of the new discovered knowledge [12], [7]. But, to illustrate the impact of low-quality data over discovered association rule quality, one might legitimately wonder whether a so-called ''interesting'' rule noted LHS  X  RHS is meaningful when 30 % of LHS data are not up-to-date anymore, 20% of RHS data are not accurate, and 15% of LHS data come from a data source that is well-known for its bad credibility. Our assumption is that interestingness measures are not self-sufficient for represent-ing association rule quality. Association rule quality should also integrate the meas-ures of the quality of data the rule is computed from with considering the probability that the deficiencies in data quality may be adequately detected. The twofold contri-bution of this paper is to propose a method for scoring association rule quality and a probabilistic cost model that predicts the cost of low-quality data over the quality of discovered association rules. This model is used to select so-called  X  X egitimately in-teresting X  rules. We evaluate our approach using the KDD-Cup-98 dataset. 
The rest of the paper is organized as follows. Section 2 gives a brief overview on data quality characterization and management. In Section 3, we present our decision model for estimating the cost of low-quality data on association rule mining. In Sec-tion 4, we evaluate our approach using the KDD-Cup-98 dataset. Section 5 provides concluding remarks and guidelines for future extensions of this work. Maintaining a certain level of quality of data is challenging and can not be limited to one-shot approaches addressing simpler abstract versions the real problems of dirty or low-quality data [4]. Solving them requires highly domain-and context-dependent information and also human expertise. Classically, the database literature refers to data quality management as ensuring: i) syntactic correctness (e.g., constraints en-forcement, that prevent  X  X arbage data X  from being entered into the database) and ii) semantic correctness ( i.e. , data in the database truthfully reflect the real world situa-tion). This traditional approach of data quality management has lead to techniques such as integrity constraints, concurrency control and schema integration for distrib-uted and heterogeneous information systems. But since a decade, literature on data and information quality across different research communities (including databases, statistics, workflow management and knowledge engineering) proposed a plethora of: -Data quality dimensions and classifications with various definitions depending -Data quality metrics [4], -Conceptual data quality models [6], [1], -Frameworks and methodologies for cleaning data [11], for improving or assess-The most frequently mentioned data quality dimensions in the literature are accuracy, completeness, timeliness and consistency [1]. Our initial assumption is that the quality of an association rule depends on the quality of the data which the rule is computed from. This section will present the formal defi-nitions of our model that introduces data quality indicators and combines them for determining the quality of association rules. 3.1 Preliminary Definitions for Association Rule Quality LHS  X  RHS where LHS  X  I , RHS  X  I and LHS  X  RHS =  X  . LHS and RHS are con-junctions of variables such as the extension of LHS is: g(LHS)= x 1  X  x 2  X  ...  X  x n and the extension of Y is g(RHS)= y 1  X  y 2  X  ...  X  y n' . 
Let j ( j=1, 2, ..., k ) be the dimensions of data quality (e.g., data completeness, max ij ] be a scoring value for the dataset I i on the quality dimension j ( I i  X  I ). The vec-tor, that keeps the values of all quality dimensions for each dataset I i (normalized in [0,1]) is called quality vector and noted q(I i ) . The set of all possible quality vectors is called quality space Q . Definition 1. Association Rule Quality The quality of the association rule R is defined by a fusion function denoted " o j " specific for each quality dimension j that merges the components of the quality vec-tors of the datasets constituting the extension of the right-hand and left-hand sides of the rule . The quality of the rule R is k -dimensional vector such as: The average quality of the association rule R denoted q(R) can be computed by the weighted sum of the quality dimensions of the quality vector components of the rule: with w j the weight of the quality dimension j . We assume the weights are normalized: Definition 2. Fusion Function per Quality Dimension Let T be the domain of values of the quality score q(I i ) for the dataset I i on the quality dimension j. The fusion function denoted " o j " is commutative and associative such as o : T  X  T  X  T. The fusion function may have different definitions depending on the considered quality dimension j in order to suit the properties of each quality criterion. Table 1 presents several examples of definition for the fusion function allowing the combination of quality scores per quality dimension for two datasets noted x and y over the four quality dimensions; freshness, accuracy, completeness, consistency. We consider that selecting an association rule is a decision that designates the rule as legitimately interesting (noted D 1 ), potentially interesting ( D 2 ), or not interesting ( D 3 ) based both on good interestingness measures and on the actual quality of the datasets composing the left-hand and right-hand sides of the rule. Consider the item x  X  LHS  X 
RHS of a given association rule, we use P CE (x) to denote the probability that the item x will be classified as  X  X rroneous X  (or  X  X olluted X  and  X  X ith low-quality X ), e.g., classified as  X  X orrect X  ( i.e. ,  X  X ith correct quality X  in the range of acceptable values for each pre-selected quality dimension). Also, P AE (x) represents the probability that the item x is  X  X ctually erroneous X  ( AE ) but detected correct, and P AC (x) represents the probability that it is  X  X ctually correct X  ( AC ) but detected erroneous (see Figure 1).
For an arbitrary average quality vector q  X  Q on the datasets in LHS  X  RHS of the rule, we denote by P(q  X  Q | CC) or f CC (q) the conditional probability that the average quality vector q corresponds to the datasets that are classified as correct ( CC ). Simi-larly, we denote by P(q  X  Q | CE) or f CE (q) the conditional probability that the average quality vector q corresponds to the datasets that are classified erroneous ( CE ). We denote by d the decision of the predicted class of the rule ( i.e. , legitimately interesting D , potentially interesting D 2 , or not interesting D 3 ), and by s the actual status of qual-ity of the datasets upon which the rule has been computed. Let us also denote by P(d=D i , s=j) and P(d=D i | s=j) correspondingly, the joint and the conditional prob-CE, AE, AC ) is j . We also denote by c ij the cost of making a decision D i for classify-ing an association rule with the actual data quality status j of the datasets composing the two parts of the rule. Based on the example presented in Table 3 where we can see how the cost of decisions could affect the result of selection among interesting asso-ciation rules, we need to minimize the mean cost c that results from making such a decision. In Table 3, c 10 is the cost of a confident decision ( D 1 ) for the selection of a decision ( D 3 ) of selecting a rule based on low-quality data but actually detected as correct ( AC ). The corresponding mean cost c is written as follows: From the Bayes theorem, the following is true: where i=1,2,3 and j= CC,CE,AE,AC . Let us also assume that q is the average quality vector drawn randomly from the space of all quality vectors of the datasets of the rule. The following equality holds for the conditional probability P(d=D i | s=j) : where i=1,2,3 and j=CC,CE,AE,AC . f j is the probability density of the quality vectors when the actual data quality status is j. We also denote the a priori probability of CC or else P(s=CC) as  X  0 , the a priori probability of P(s=AC)=  X  0 AC , the a priori probabil-The mean cost c in Eq. (4) based on Eq. (5) is written as follows: and by using Eq. (6) and dropping the dependent vector variable q , Eq. (7) becomes: For the sake of simplicity for the following of the paper, let's now consider the case of Without misclassification region P(s=CE) could be simplified as 1- X  0 . Every point q that its contribution to the mean cost is minimum. This will lead to the optimal selec-observation, a point q that represents the quality of a rule defined in Eq. (2) is as-signed to one of the three optimal areas as follows: The inequalities of Eq. (9) give rise to three different threshold values L, P and N (respectively for legitimately, potentially and not interesting rules) in the decision space as defined in Eq. (10): In order to validate and evaluate our decision model, we built an experimental system. The system relies on a data generator that automatically generates data quality meta-data with a priori known characteristics. This system also allows us to perform con-trolled studies so as to establish data quality indicators and quality variations on data-sets and on discovered association rules which are assigned to the decision areas D 1 , D or D 3 . In the set of experiments that we present, we make use the KDD-CUP-98 1 dataset from the UCI repository. The KDD-Cup-98 dataset contains 191,779 records about individuals contacted in the 1997 mailing campaign. Each record is described by 479 non-target variables and two target variables indicating the  X  X espond X / X  X ot respond X  classes and the actual donation in dollars. About 5% of records are  X  X e-spond X  records and the rest are  X  X ot respond X  records. The KDD-Cup-98 competition task was to build a prediction model of the donation amount. The participants were records with predicted donation greater than the mailing cost $0.68 (see [14] for de-tails). Because we ignored the quality of the data collected during this campaign, we generated synthetic data quality indicators with different distributions representative of common data pollutions. In this experiment, our goal is to demonstrate that data quality variations may have a great impact on the significance of KDD-Cup-98 results ( i.e. , the top ten discovered  X  X espond X  rules) and we use different assumptions on data quality indicators that do not affect the top ten list of discovered association rules but that significantly change the reliability (and quality) of this mining result and also the cost of the decisions relying on these rules. The variable names, definitions, estimated probabilities and average quality score per attribute are given in Table 2. For the sake of simplicity, we suppose that the quality dimension scores are uniformly representa-tive of the quality of the attribute value domain. The average quality per attribute in Table 2 is computed from the equi-weighted function given in Eq. (2). f CC (q(I i )) (also noted f CC in Table 2) is the probability density that the dataset I i is  X  X orrect X  when the I is  X  X rroneous X  when the average quality score of I i is q(I i ). Table 3 shows tentative unit costs developed by the staff of the direct marketing department on the basis of consideration of the consequences of the d ecisions on selecting and using the discov-c correct data is $500. Based on the values assigned to the various costs in Table 2, we also assume that the a priori probability that a certain quality vector belongs to CC equals the a priori probability that the same vector belongs to CE . For this reason, the ratio the values of the three decision thresholds for rule selection for the a priori probability = 0.200 without misclassification and we obtain: L =0.125 , P = 0.0131579 and N = 2.25 . In order to be consistent with the conditional independency of the quality vector com-ponents we also need to take the logarithms of the thresholds values. By doing this we these thresholds, we can assign the rules to one of the three decision areas. The top 10 a priori association rules discovered by Wang et al. [14] are given in Table 4 with the confidence, the support (in number of records), and the profit. Table 4 also shows the score per quality dimension, the average quality and the cost of selecting the associa-tion rule. The scores are computed from the definitions of the quality dimensions given in Table 1. The costs are computed from Eq. (8). It X  X  very interesting to notice that the predicted profit per rule may be cons iderably affected by the cost of the rule computed from low-quality data (e.g., the second best rule R2 whose predicted profit is $61.73 has a cost of $109.5 and thus is classified as  X  X ot interesting X  due to the bad quality of its datasets). Let us now introduce different variations on the average qual-ity of the datasets composing the rules. Based on the cost Table 3, Figure 2 shows the behavior of the decision cost of rule selection when data quality varies from the initial average quality down to -10%, -30%, and -50% and up to +10%, +30% and +50% for a priori probability 0  X  =0.200 and without misclassification. In Figure 2 we observe that the quality degradation of the datasets composing the rules increases the cost of these rules with variable amplitudes. 
Data quality amelioration implies a stabilization trend of the decision cost for le-gitimately interesting rule selection. Another interesting result is shown in Figure 3 where the decisions for rule selection change simultaneously with the data quality variations. Among the top 10 interesting rule discovered by Wang et al. [14] with the initial data quality (noted Init Qual ), 5 rules (R1, R5, R7, R9 and R10) are poten-tially worth being selected based on their average data quality. Increasing data quality up to +30%, 3 rules were legitimately interesting (R5, R7 and R9). This observation offers two (among others) interesting research perspectives for both association rule mining and data quality management: first, for proposing a post-filtering rule process based on data quality indicators and decision costs for rule selection and secondly, for the optimal scheduling of data quality improvement activities (e.g., cleaning) driven and tuned by the rule pruning step. Additionally to the interestingness measures the three thresholds can be used as a predictive technique for quality awareness in asso-ciation rule mining for the appropriate selec tion of legitimately interesting rules based on the data quality indicators. The original contribution of this paper is twofold: first, we propose a method for scor-ing the quality of association rules that combines and integrates measures of data quality; secondly, we propose a probabilistic cost model for estimating the cost of selecting  X  X egitimately (or not) interesting X  association rules based on correct-or low-quality data. The model defines the thresholds of three decision areas for the predicted class of the discovered rules ( i.e. , legitimately interesting, potentially interesting, or not interesting). To validate our approach, our experiments on the KDD-Cup-98 data-set consisted of: i) generating synthetic data quality indicators, ii) computing the aver-age quality of the top ten association rules discovered by Wang et al. [14], iii) com-puting the cost of selecting low-quality rules and the decision areas they belong to, iv) examining the cost and the decision status for rule selection when the quality of un-derlying data varies. Our experiments confirm our original assumption that is: inter-estingness measures are not self-sufficient and the quality of association rules depends on the quality of the data which the rules are computed from. Data quality includes various dimensions (such as data freshness, accuracy, completeness, etc.) which should be also considered for effective and quality-aware mining. Our future plans regarding this work, are to study the optimality of our decision model, to propose error estimation and to validate the model with experiments on large biomedical data-sets (see [2]) with on-line collecting and computing operational data quality indicators with the aim to select high-quality and interesting association rules. 
