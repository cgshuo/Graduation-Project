 We introduce an asynchronous distributed stochastic gradi-ent algorithm for matrix factorization based collaborative filtering. The main idea of this approach is to distribute the user-rating matrix across different machines, each having ac-cess only to a part of the information, and to asynchronously propagate the updates of the stochastic gradient optimiza-tion across the network. Each time a machine receives a parameter vector, it averages its current parameter vector with the received one, and continues its iterations from this new point. Additionally, we introduce a similarity based reg-ularization that constrains the user and item factors to be close to the average factors of their similar users and items found on subparts of the distributed user-rating matrix. We analyze the impact of the regularization terms on Movie-Lens (100K, 1M, 10M) and NetFlix datasets and show that it leads to a more efficient matrix factorization in terms of Root Mean Square Error (RMSE) and Mean Absolute Er-ror (MAE), and that the asynchronous distributed approach significantly improves in convergence time as compared to an equivalent synchronous distributed approach.
 Recommender Systems, Asynchronous Distributed Optimiza-tion, Similarity-based Regularization
The immense growth of e-commerce in the last decade has resulted in an increasing popularity for Recommender Systems, helping customers to identify products that best fit their personal tastes. Many Recommender Systems ap-proaches were proposed in the recent years among which collaborative filtering based algorithms have been found to be very effective [6]. The underlying hypothesis H of these algorithms is that if two users share the same opinion over a set of items, they are likely to share the same opinion over new items. Two popular collaborative filtering techniques that have been developed based on this idea are 1) Neigh-borhood methods [5], which generate prediction for unseen items using the past ratings of similar users or items; and 2) Matrix Factorization techniques [3], that approximate the matrix of ratings with the product of two matrices corre-sponding to users and items in some latent space.

In this study, we tackle this problem by splitting the rating matrix across different machines of a network and propose a novel distributed asynchronous framework for matrix fac-torization using the Stochastic Gradient Optimization ( SGO ) method for recommender systems. The rational is that ma-chines connected in a network are generally loaded differ-ently and synchronous updates may become very slow as every machine has to wait for the slowest machine to finish before updating. Moreover, in order to enhance the under-lying hypothesis H stated above, we add to the classical RMSE objective two similarity-based regularization terms that constrain respectively user and item factors to be close to the factors of their most similar users and items found in the subpart of the rating matrix stored in the machine. In our asynchronous framework, each machine performs SGO in parallel on different parts of the rating matrix and sends its updated weights to the other machines in the network. Once a machine receives an updated weight, it averages the parameter weights with its own updates and continues ap-plying SGO from the new averaged point. This guarantees that each machine keeps an overall view over the whole data.
Using Movielens and Netflix datasets, we show that the addition of the proposed user and item regularization terms lead to a more effective matrix factorization in terms of RMSE and MAE. We also demonstrate the attractive con-vergence properties of our asynchronous distributed update scheme compared to an equivalent synchronous distributed approach [8, 4].
 In the remainder of the paper, we define in Section 2 the Matrix Factorization problem with our proposed similarity-based regularization. Then, in Section 3, we describe the proposed distributed asynchronous framework. Finally, we discuss the experimental settings and results in Section 4.
In this section, we introduce basic definitions and the ob-jectives that we address in our setting of matrix factoriza-tion. Users and items are respectively represented by the sets U and I , where u i and i j are respectively the i th and the j th item. Furthermore, we denote by r ij the rat-ing of item i j by user u i and we define the ratings matrix R as the matrix of the r ij whenever they exist. The user |U| X  K factor matrix (resp. item |I| X  K ) in a latent space of dimension K is denoted by P (resp. Q ), and p i (resp. q is the size-k vector corresponding to the i th user/line (resp. j th item/line).

Matrix factorization with Stochastic Gradient Optimiza-tion ( SGO ) has been proved to be a successful approach for recommender systems, notably after winning the Netflix prize [3]. The premise behind this approach is to approximate the large rating matrix R with the multiplication of low-dimensional factor matrices PQ T . These factor matrices try to model user preference for items with small number K of implicit factors. For a pair of user and item ( u for which a rating r ij exists, this approach is based on the minimization of the ` 2 -regularized quadratic error: where  X   X  0 is the regularization parameter. The whole matrix factorization problem thus writes Note that the error ` ( u i ,i j ,P,Q ) depends only on P and Q through p i and q j ; however, item i j may also be rated by user u 0 so that the optimal factor q j depends on both p i and p The Stochastic Gradient Optimization ( SGO ) method thus proceeds as follows: at each iteration t , i) select a random user/item pair ( u i t ,i j t ) for which a rating exists; ii) perform a gradient step on ` ( u i t ,i j t ,P,Q ).

In order to enhance the assumption that similar users have similar tastes, we impose that the factor vector of each user (resp. item) should be close to the average factor vector of its similar users (resp. items). For computing the most similar users (or items) we considered a modified version of Pearson correlation coefficient [2] which for two users u u j writes: sim ( u i ,u j ) = Where, I c is the items co-rated by both users, r i. denote the average ratings for u i and u j respectively.
Hence, we are able to find the N most similar users for u , denoted by N i (resp. N j for items similar to i j now propose a slight modification of the individual ratings objective function ` of Eq. (1) above by adding another reg-ularization term. For a pair of user and item ( u which a rating r ij exists, the similarity-regularized individ-ual objective writes: +  X  u p i  X  1 | N where  X  u  X  0 and  X  i  X  0 are the regularization parameters linked to the similar-user and similar-item regularizations re-spectively. Performing the same updates as the conventional SGO but replacing ` by ` 1 , we get Algorithm 1 for minimizing the whole matrix factorization problem (Eq. 2) where ` is Algorithm 1 Similar based regularization 1: procedure Modified SGO 2: Input: R, X , X  u ,  X  i 3: Initialize: P and Q randomly 4: while not converged do 5: Choose randomly ( u i ,i j )  X  R 6: N i = GetSimilarUsers ( i,N ) 7: N j = GetSimilarItems ( j,N ) 8: Update p i and q j by a gradient step on replaced by ` 1 i.e. the similarity-regularized problem:
Even though stochastic gradient optimization offers a high prediction accuracy on recommender system datasets, there are some computational challenges associated with it. Per-forming SGO sequentially on a single machine takes unaccept-ably large amount of time to converge. So, there is a need to perform SGO in a distributed manner for large datasets. However, parallelizing SGO is not trivial. A drawback of a straightforward implementation is that updates on factor matrices might not be independent. For example, for the training points that lie on same rows (or columns), they update the same corresponding row (or column) of P and Q matrices simultaneously. So, efficient communication be-tween the computing nodes is necessary to synchronize the updates on factor matrices.

One common approach is to divide the rating matrix into several blocks and run SGO on each of the blocks on distinct machines. Factor matrices are updated on each machine for the corresponding ratings. Even though the rating matrix parts on each machine are different, the factor matrix up-dates are not independent. So, after each epoch the factor matrices present in each machine are synchronized. We refer to this method as  X  X ynchronized SGO  X , as all machines syn-chronize their factor matrices after every epoch (in [4] this method is referred to as Asynchronous SGD because the pa-rameters are communicated asynchronously, but however, the epochs are synchronized as aforementioned). But, such approaches might not be efficient mainly because the compu-tation time for one epoch of SGO is different depending on the machine it is performed. So, this approach of synchronous propagation between machines after each epoch can consid-erably be slowed down because the faster machines have to wait for the slower ones in order to synchronize.

We propose here a distributed algorithm for SGO with asynchronous propagation of parameter matrices after each epoch. Each of the machines contain a part of the rating matrix corresponding to clusters of similar users, that could be found efficiently offline with large-scale graph clustering techniques [7]. Factor matrices are hence updated on each machine using only the part of the rating matrix it contains. As opposed to the synchronous method, we broadcast the updated factor matrices as soon as the machine finishes one epoch. So, whenever a new epoch begins, it collects the most recent factor matrices from other machines and after finish-ing one epoch it broadcasts its updated factor matrices to all other machines and begins another epoch immediately, independently of the state of other machines. In this way, all the machines are running independently of each other. So, the faster machines will perform their epochs faster, whereas the slower slower machines will be lagging on time but af-ter finishing each epoch they will receive the most updated factor matrices from the faster machines.

In our setting, we cluster the rating matrix R into m blocks of lines as in [4]. Where m is the number of machines (or processes) available. The P matrix is also blocked in the same way. At each node we keep: one block of R , the corre-sponding block of P , and a local copy of Q . At each epoch, we update that block of P and local Q matrix based on the block of R and the individual loss functions ( ` or ` 1 ). The overall procedure is depicted in Figure 1, showing an exam-ple distributed network of three machines. R 1 , R 2 and R represent the rating blocks on each machine. ep represents the end of previous epoch and beginning of new epoch on different machines. As shown, after finishing one epoch each machine broadcasts its updated Q matrix to all other ma-chines, whereas at the beginning of new epoch each machine averages its Q update with the received updated Q matri-ces from other machines (if any). In this way the updates from faster machines help the slower machines to converge faster, thus enhancing the overall convergence time of the distributed network.
 Figure 1: Block Diagram of Asynchronous Distributed SGO
We conducted a number of experiments aimed at evalu-ating what are the impacts of similar user and item regu-larization terms on the matrix factorization, and how the proposed asynchronous framework can help to speed up the convergence of the SGO algorithm.

Datasets: We performed experiments on MovieLens data-sets (ML-100K, ML-1M and ML-10M) and a subset of the NetFlix collection (see Table 2). For ML-100K and ML-1M we used both sets (ra and rb, or ua and ub), whereas for ML-10M and Netflix we used a single dataset.
 Table 2: Characteristics of Datasets used in our experi-ments. |U| and |I| denote respectively the number of users and items.
 Dataset |U| |I| K training size test size sparsity ML-100K 943 1682 20 90570 9430 93.7 % ML-1M 6040 3952 40 939809 60400 95.8 % ML-10M 71567 10681 100 9301274 698780 98.7 % NF-Subset 28978 1821 40 3255352 100478 93.7 %
Implementation: We implemented the distributed frame-work using PySpark version 1.5.1. To demonstrate the com-munication between disparate machines, we connected five servers with different computational loads.
 Platform and Parameters: Three of the machines have Intel Xenon E5-2640 2.60 GHz processors and 256 GB mem-ory each. The other two machines have Intel Xenon E5-2643 3.40 GHz processors and 128 GB memory each. Even though some of the machines have identical configuration, they were running different workloads on them. One of the machines dispatches the rating matrix across all the others as well as on itself. We stop the running algorithm on each machine, when the RMSE difference between two epochs on a valida-tion set falls below a predefined threshold,  X  , fixed as 10 in our experiments. The convergence of the proposed asyn-chronous approach is set when the slowest machine finishes its job, while for the synchronous SGO all machines finish at the same time.

Following [1], the number of latent factors for dataset ML-10M was fixed to 100, and for the other smaller datasets, it was chosen experimentally for best result/speed compromise as 40, 40 and 20 respectively. The learning rate and the reg-ularization parameter  X  were fixed to 0.005 and 0.05 as in [1]. For our proposed similarity-based regularization  X  u ,  X  i the number of similar users/items N were chosen with values that led to the best RMSE on validation sets for each collec-tion chosen among { 10  X  1 , 5 . 10  X  2 , 10  X  2 , 5 . 10 10  X  4 } for  X  u ,  X  i and { 10 , 20 , 30 , 40 , 50 } for N . Evaluation Measures: In our experiments, we used the Mean Absolute Error (MAE) and the Root Mean Square Er-ror (RMSE) as performance measures. Also, we compared the convergence time for each of the algorithms in the syn-chronous and the asynchronous distributed settings.
First, we compare the results between the traditional SGO method and the proposed Modified SGO with Similarity Based Regularization. The difference here is solely on the objective function that is minimized (Pbs. (2) and (4) respectively). similar user and item based regularization for matrix factorization. R MSE R MSE W e tested two scenarios: i) where only the users are regu-larized with similarity (  X  i = 0); and ii) when both users and items are regularized with the same parameter (  X  u =  X  i Table 1 shows the complete results of our experiments.
It comes out that forcing the vectors of users and items to lie within the centroids of their most similar users and items found by the Pearson similarity measure is effective as the final RMSE and MAE with Algorithm 1 are always better than with classical SGO . Thus, there is a significant benefit to use this regularization in terms of learning. We also report results by looking at the effect of the similar user regularization and not items (  X  u &gt; 0 , X  i = 0). As shown in Table 1, this user-only regularization also gives uniformly better results than traditional SGO , and even better than the user and item regularization on one dataset.
Now, we evaluate the proposed asynchronous distributed method by comparing its convergence time with the syn-chronous distributed SGO method. We evaluate the conver-gence speed on the problem with user and item similarity regularizations in order to show its scalability. The evolu-tions of RMSE with respect to the number of epochs of our proposed algorithm and the synchronous SGO on the ML-10M dataset and the subset of the Netflix collection are shown in Figure 2. In the case of the asynchronous approach, we show the convergence time of the slowest machine. As expected, the proposed approach converges in less epochs as the slowest machine gets updated values from the faster machines, which helps it to converge faster, whereas in the case of the synchronous method, the faster machines have to wait for the slower ones after every epoch.
In this work, we presented an asynchronous distributed framework for SGO . Though generic, the framework was ap-plied to matrix factorization for recommender systems. Fur-thermore, we proposed two additional regularization terms for this task. Experimental results on different MovieLens and NetFlix collections tend to validate the interest of this similarity-based regularization as well as the convergence speedup of the proposed asynchronous approach. This work has been partially supported by the LabEx PERSYVAL-Lab (ANR-11-LABX-0025-01) funded by the French program Investissement d X  X venir. [1] W.-S. Chin, Y. Zhuang, Y.-C. Juan, and C.-J. Lin. A [2] J. L. Herlocker, J. A. Konstan, A. Borchers, and [3] Y. Koren, R. Bell, and C. Volinsky. Matrix [4] F. Makari, C. Teflioudi, R. Gemulla, P. Haas, and [5] P. Melville, R. J. Mooney, and R. Nagarajan.
 [6] B. Sarwar, G. Karypis, J. Konstan, and J. Riedl. [7] J. J. Whang, X. Sui, and I. S. Dhillon. Scalable and [8] M. Zinkevich, M. Weimer, L. Li, and A. J. Smola.
