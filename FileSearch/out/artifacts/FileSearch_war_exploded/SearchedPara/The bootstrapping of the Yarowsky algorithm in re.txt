 1. Introduction
Onespecificprobleminthefieldofcomputationallinguisticsisthatofdeterminingwhichparticularsenseofawordisbeing used in a given context. This problem is often referred to as word sense disambiguation (WSD). WSD is often viewed as a classi-ficationproblem:themanyoccurrencesofagivenwordforminsomecorpusaregroupedintotheclassesrepresentedbyeachof its possible senses. The set of possible senses of the word can be extracted from a generic dictionary, knowledge base or ontol-ogy; from an application-specific inventory; or it may even be a priori inexistent. Usually the context of each occurrence of the wordinthecorpusisusedastheclassifyingcriterion.WSDhasbeenconsideredinmany naturallanguageprocessing (NLP)appli-cations, both as a generic and as an integrated component. The most prominent include machine translation (MT), information retrieval (IR), information extraction (IE) and text mining , and modern lexicography (Agirre &amp; Edmonds, 2006 ).
It is to be expected that in some highly constrained forms of texts, such as patents or some summaries, WSD will be of little or no relevance. However, in a wide range of less constrained texts WSD has a prominent role to play, particularly in important applications such as machine translation and for many kinds of information retrieval and information extraction.
There are three main approaches to the WSD problem. One of these approaches, referred to as knowledge-based or dictio-nary-based , relates to methods that use some kind of knowledge-rich resource such as a dictionary, thesaurus, concept hier-archy or lexical database. However, these sources of knowledge are in general independent of the target corpus being disambiguated. On the other hand, supervised corpus-based methods use (sense-tagged) corpus training material related in some way to the target corpus, instead of an independent external resource. Finally, unsupervised corpus-based methods cluster words in non-annotated raw corpora without any kind of training or independent source of knowledge evidence.
While knowledge-based and supervised corpus-based methods categorize words based on some pre-existing sense inventory, unsupervised corpus-based methods do not. Frequently they are only told the total number of senses of a word form to be discriminated ( Pedersen, 2006 ). Combinations exist of the knowledge-based approach with either of the other two, but not between these.

The supervised corpus-based (and also the knowledge-based) methods need hand-built resources like sense-tagged train-ing corpora. These training data are difficult and expensive to produce. This problem is often referred to as the knowledge acquisition bottleneck (M X rquez, Escudero, Mart X   X  nez, &amp; Rigau, 2006 ) and it prevents many supervised methods from being applied to foreign language corpora, for example.

The number of senses distinguished by a WSD system is often called its granularity. While many systems, including those tested in the Senseval competition, use a fine-grained approach i.e. a high number of sense distinctions for each target word, it has been argued that systems used in real NLP applications only need a coarse-grained granularity with two sense distinc-polysemy. We will refer to these issues in more detail below.

The Yarowsky (1995) algorithm is a semi-supervised WSD method  X  i.e., it does not suffer the knowledge acquisition bot-tleneck and thus it can be easily ported to foreign unknown language corpora, or to corpora written in languages for which no specific resources are available, something very useful in a multi-lingual Web context. Moreover it resolves the homo-graph-level sense disambiguation to the accuracy level required by real NLP applications (above 95%). However, we will show that the Yarowsky algorithm is affected by the domain fluctuations present in real corpora, something that signifi-cantly reduces its accuracy in practice. We will also show that we can use a bootstrapping methodology to increase its accu-racy in such real ambiguous corpora. 2. The Yarowsky bootstrapping algorithm
The Yarowsky (1995) algorithm uses a bootstrapping method for disambiguating homographs in non-annotated raw cor-pora. For this reason it is not strictly supervised, but neither is it an unsupervised method. Thus, it is often considered a semi-supervised algorithm. It uses a small seed set of labelled examples which are representative of each of the homograph senses. 2.1. Knowledge sources The Yarowsky algorithm uses two different sources of linguistic knowledge. In relation to the set of knowledge sources for
WSD listed in Agirre and Stevenson (2006) the Yarowsky algorithm uses one syntactic ( collocation , KS 3) and one pragmatic/ topical ( topical word association , KS 10) source. In terms of the original Yarowsky (1995) paper it uses two  X  X  X roperties of hu-man language X : one-sense-per collocation and one-sense-per discourse .

The one-sense-per-collocation property states that word forms placed near the target word, called collocations, provide strong indication about its sense (Yarowsky, 1993 ). This effect varies depending on the type of collocation. It is strongest for immediately adjacent collocations (97%), and weakens with distance. It is much stronger for words in a predicate X  X rgument relationship than for arbitrary associations at equivalent distance, and very much stronger for collocations with content words than for those with function words.

The one-sense-per-discourse property states that words show a strong tendency to exhibit only one-sense in any given document ( Yarowsky, 1995 ), discourse ( Gale, Church, &amp; Yarowsky, 1992 ) or, we would say, domain. In Yarowsky (1995) a measure of this property is provided by means of its accuracy (if a word occurs more than once in a discourse, how often it takes on the majority sense for the discourse) and its applicability (how often the word occurs more than once in a dis-course) for a set of 37,232 instances of 10 different homographs. Results show average 99.8% accuracy and 50.1% applicabil-ity. However, the Yarowsky algorithm uses one-sense-per-discourse in a flexible way; if there is probabilistic evidence regarding one-sense-per-collocation, it may be overridden. 2.2. The learning algorithm
The main idea behind the Yarowsky algorithm is to begin with a small set of correctly tagged seed examples represen-tative of the two (or more) senses of a word and, using a combination of the one-sense-per-collocation and the one-sense-per-discourse tendencies in those examples, augment them with additional examples of each sense. After several iterations of this process (most) instances of the word in the original (untagged) corpus will be assigned their (probably cor-rect) corresponding sense. Of course, the application of one-sense-per-collocation requires the availability of the context (represented by the +/ k -word window) of each instance of the target word in the original corpus.

There are several strategies for identifying the initial training seeds. A very simple one is hand-tagging a small subset of the instance contexts. Another (semi-automatic) procedure is to identify a small number (maybe only one for each sense) of word collocations representative of each sense and then to tag all instance contexts containing these collocates with the seed X  X  sense label. An automatic procedure would be to extract collocate words from a dictionary X  X  entry for the target sense.
These words would occur with significantly greater frequency in the entry relative to the entire dictionary, and would there-fore appear in the most reliable collocational relationships (Yarowsky, 1993 ) with the target word.
Once the training set is tagged, a supervised classification algorithm can be trained with the two senses on it. The decision list algorithm (Yarowsky, 1994 ) is used here. This algorithm is able to become self-trained by identifying new collocations that reliably partition the seed-training examples, rank them according to the purity of their distribution and classify the most evident contexts out of the whole corpus into their corresponding senses. This means that when new examples are reliably classified, the new collocations appearing in their contexts can be used in turn to train the decision list for classifi-cation in the next iteration. The ranking of collocations in the decision list is based on the value of the log-likelihood of their frequencies on the training examples of each different sense.

Different kinds of collocations may be considered. The most common three are: a word immediately to the left of the tar-get word, a word immediately to the right, and a word inside a window of  X  k words around the target word.
Note that the algorithm is able to  X  X  X orrect X  its own previous tagging decisions since a new decision list is built in each iteration, and this new classifier is applied to the whole sample set.

Also note that every training-tagged context will exhibit multiple collocations indicative of the same sense and that these will be ordered (by their log-likelihood value). At the same time every non-tagged context under classification will exhibit multiple collocations (of which some will be in the decision list and others will not). The decision list algorithm resolves any not a combination of all matching collocations. This circumvents many of the problems associated with non-independent evidence sources (Yarowsky, 1995 ). 2.2.1. Using one-sense-per-discourse
If several instances of the target word (contexts) have already been assigned a particular sense, this sense tag may be ex-tended to all examples in the same discourse (document), conditional on the relative numbers and the probabilities associ-ated with the tagged examples. This serves both to augment with new tags and to correct erroneously labelled examples.
The one-sense-per-discourse constraint may be used after each iteration, or only once at the end of the algorithm. 2.3. Results
Yarowsky reports in his original 1995 paper the accuracy results and baselines shown in Table 1 . The target words were randomly selected from those previously studied in the literature. The data were extracted from a 460 million-word corpus containing news articles, scientific abstracts, spoken transcripts and novels.

Column 5 illustrates results using only two word collocations (one for each sense) as triggers of seed contexts. Column 6
Columns 7 and 8 illustrate the use of one-sense-per-discourse only at the end of the algorithm or at the end of each iteration. 2.4. The algorithm as a semi-supervised learning method
The Yarowsky algorithm can be viewed as a meta-learner in which any supervised machine learning (ML) method can be nested  X  in the original algorithm this was the decision list classifier. This property, which is the foundation behind its semi-supervised character, has made it very attractive, and several variations, improvements and applications have followed it.
Blum and Mitchell (1998) propose a semi-supervised algorithm called co-training which iteratively builds two classifiers, instead of just one, and uses each to improve the other. Abney (2002) showed that co-training and the Yarowsky algorithm are based on different statistical independence assumptions  X  view independence and precision independence respectively, which are entirely distinct so that the Yarowsky algorithm is not a special case of co-training. Nigam and Ghani (2000) and Ng and Cardie (2002) compared the two methods in experiments with equivalent models and report results in favour of the  X  X  X ingle-view X , i.e., the Yarowsky algorithm.

Abney (2004) analyzes the Yarowsky algorithm mathematically and shows that some variants of it proposed by himself optimize either likelihood or a closely related objective function which he calls K .

Eisner and Karakos (2005) show that it is possible sometimes to eliminate the last bit of supervision from bootstrapping methods, by trying many candidate seeds and selecting the one with the most plausible outcome. They call this technique  X  X  X trapping X  and report results better than the standard method of picking seeds by hand proposed by Yarowsky (1995) .
Traupman and Wilensky (2003) report three attempts to improve the accuracy of the Yarowsky algorithm. First they used the classifier X  X  output from one iteration as its training input in the next; second they pre-processed the training and test corpora with a part of speech (POS) tagger and used these tags to filter possible senses and improve the predictive power of word X  X  contexts; and third they replaced the usual assumption of a uniform distribution of the senses of a word with a more realistic one using the frequencies of use of senses in a dictionary. They report the second experiment as the most ben-eficial, outperforming the standard algorithm X  X  accuracy, the third experiment as providing a slight additional improvement, and the first as not being successful, and even hurting performance.

Ueffing, Haffari, and Sarkar (2008) use a modified version of the Yarowsky algorithm as a semi-supervised learning (SSL) method to build a statistical machine translation (SMT) system. His system is checked against a baseline supervised phrase-based SMT system on a dataset consisting of the EuroParl corpus from the SMT shared task 2006. The supervised baseline system is trained with a set of 25,000 aligned English (target) and French (source) sentence pairs. He shows that a semi-supervised system that uses an additional unlabeled set of source French text (500 sentences) together with 25,000 aligned sentence pairs provides an improvement in the Bleu score (performance score against 4 X 10 human translations for each sen-tence) almost equivalent to doubling the training data in the supervised baseline system from 25,000 to 50,000 sentence pairs.

Other applications of algorithms similar to Yarowsky X  X  include parsing, morphology learning, grammatical gender predic-tion, named entity recognition and bilingual lexicon induction (Smith, 2006 ). 3. The knowledge acquisition bottleneck
The knowledge-based and supervised corpus-based methods need resources that must be hand-built by humans and therefore are expensive to acquire and maintain. These include dictionaries, thesauri, ontologies and other lexical databases for the former approach and sense-annotated corpora for the latter. This requirement leads to the so-called knowledge acqui-sition bottleneck . This problem affects WSD in at least three important aspects (Pedersen, 2006 ): size, when attempting to handle larger amounts of text; domain, when changing the genre/topic of the corpora; and language, when intending to ap-ply a single method to a range of (foreign) language corpora. 3.1. The relevance of semi-supervised learning algorithms
Researchers and developers in NLP are confronted more and more frequently with a need to develop language technology components in new languages ( Hwa, Resnik, Weinberg, Cabezas, &amp; Kolak, 2005 ). The recent success of corpus-based ap-proaches has exacerbated the knowledge acquisition bottleneck problem. This has led to the development of weakly super-vised algorithms, such as active learning (Baldridge &amp; Osborne, 2003; Hermjakob &amp; Mooney, 1997; Hwa, 2004; Tang, Luo, &amp;
Roukos, 2002 ), self-and co-training ( Sarkar, 2001; Steedman et al., 2003 ) and bootstrapping via projection across parallel texts ( Merlo, Stevenson, Tsang, &amp; Allaria, 2002; Yarowsky &amp; Ngai, 2001; Yarowsky, Ngai, &amp; Wicentowski, 2001 ).
Active learning (AL) attempts to reduce the cost of annotating labelled datasets by selecting the best new examples to tag: those examples on which the learner is most uncertain (uncertainty sampling, Cohn, Ghahranami, &amp; Jordan, 1995; Osborne &amp;
Baldridge, 2004 ). However, contrary to expectations, creating labelled material for some model using this technique and then reusing it with another model can produce negligible or even negative gains (Baldridge &amp; Osborne, 2004 ). Other techniques such as active annotation (Vlachos, 2006 ) have been proposed to address the reusability of the data obtained through active learning. While the reductions in annotation cost are comparable to those of active learning and the corpus produced is more portable, this technique needs the manual intervention of a human annotator.

Bootstrapping via projection across parallel texts attempts to tag a new corpus in a given language by projecting linguistic knowledge from a (tagged) parallel text corpus in a resource-rich language (such as English). This is called annotation pro-jection using parallel text and has been accomplished for many different standard NLP tasks ( Hwa et al., 2005 ). The need for parallel texts in both languages is an evident shortcoming of this approach.

Self-training and co-training refer to the bootstrapping algorithms introduced in ( Yarowsky, 1995 ) and ( Blum &amp; Mitchell, 1998) as described in the preceding section. The semi-supervised bootstrapping algorithms have received much attention in recent years and have been applied to many different NLP techniques, sometimes as case studies.

Pierce and Cardie (2001) used a base noun phrase bracketing task as a case study to investigate the learning behaviour of co-training (CT). They checked the bootstrapping algorithm against a fully supervised algorithm performing at an accuracy of 95.17%. The bootstrapping algorithm reached 93.3% accuracy, but this result depended on the parameter L (initial amount of labelled data for the training algorithm). They found that the CT algorithm is quite sensitive to the parameter settings ( L and number of iterations among others). They also found that the accuracy obtained did not continue as co-training progressed, i.e., as the number of iterations grew, and hypothesized that this decline was due to degradation in the quality of the labelled data (produced by the algorithm as it progressed). They suggested combining semi-supervised learning methods with active learning methods and introduced corrected co-training as a  X  X  X oderately supervised X  method that simulated a human anno-tator by automatically correcting each newly labelled instance as it was added to the labelled data. Corrected co-training reached 94.5% accuracy and achieved the accuracy of the supervised algorithm after adjusting (globally) only one of its parameters.

The work in ( Pierce &amp; Cardie, 2001 ) illustrates two important characteristics of semi-supervised algorithms: their suscep-tibility to parameter adjustment and to mislabelled examples. Mihalcea (2004) used WSD with self-training and co-training and obtained an average precision of 65.61% for the former and 65.75% for the latter in a lexical sample-like task with many senses per word. The supervised algorithm embedded inside the semi-supervised algorithms only reached 53.84% accuracy.
However, these results correspond to an optimal setting of the three algorithms parameters, i.e. the setting with the best precision in each experiment, something which is hardly realistic. With a global setting the algorithms reached 55.67% accu-racy. Again this is due to the mislabelled examples. Mihalcea proposed a new bootstrapping scheme based upon majority produced a benefit in both aspects: it yielded a larger interval of higher performance (58.35%). Ng and Cardie (2004) used simple majority voting (Breiman, 1996 ) on a task of noun phrase co reference resolution with self-training and co-training and concluded that the former outperformed the latter and was also comparatively less sensitive to parameter tuning.
Steedman et al. (2003) applied self-and co-training to the problem of bootstrapping statistical parsers. A standard sta-tistical parser can perform at 89% F-measure with a large enough number of training sentences. Steedman et al. reported an F-measure of 79.0% in their bootstrapping experiments and claimed that their low result was due to parsing being a much harder task than many others. However, they also projected their result to a much higher number of training examples (400,000) and their result was 90.4%, suggesting that a bootstrapping method might improve the performance of a statistical parser beyond the current state-of-the-art. 3.2. Semi-supervised learning and domain dependence
Steedman et al. (2003) also point to a third problem in bootstrapping algorithms  X  namely the possible domain variation in corpora (see also Section 6). This problem was the reason why for instance their experiments yielding an F-measure of 79.0% reported in the preceding subsection dropped down to 76.8% if training took place in a different corpus.
He and Gildea (2004) referred to this problem when dealing with self-and co-training for the task of semantic role label-ling. They also pointed to the fact that it is impractical to have annotated data for every domain. However, this is not a prob-lem specific to semi-supervised learning algorithms, but it also affects supervised learning algorithms. In fact, He and Gildea claimed that the initial motivation behind their research was that by using weakly supervised learning algorithms they ex-pected to tune the classifier trained on data from annotated domains to data from new domains, hypothesizing the potential of this kind of learning algorithms to deal with the domain fluctuation problem. 3.3. Other possible approaches to the knowledge acquisition bottleneck
A slightly different approach to the knowledge acquisition bottleneck would be to try to use the available resources for one specific task to solve the problem posed by another (related) task. We could for instance try to solve the WSD problem using available material from the semantic role tagging problem.

We have explored this possibility using material from the last Semeval competition held in 2007. Using training (tagged) material from task 17 (English Lexical Sample WSD and Semantic Role Labeling) (Pradhan, Loper, Dligach, &amp; Palmer, 2007 ) we have investigated the results that would be obtained by using a semantic role tagger for disambiguating the senses of space and drug (which we will be investigating again later in this paper). The senses tagged in the training examples in the WSD part of the task for these two words were perfectly mapped to the sense tags used for the same words throughout all this paper, and the tags used in the semantic role part of the task were the usual semantic roles agent , theme , experiencer , cause , etc.

Our results showed that for homograph space a perfect (manually constructed) semantic role tagger used as a WSD meth-od over the WSJ corpus with a most frequent sense baseline of 64.0% would obtain a disambiguation accuracy (at 100% re-call) of 80.0%. For word drug the baseline was 82.7% and the accuracy reached 85.3%. The number of semantic role tags used in each experiment was of 11.

It is important to note that the target corpus was the WSJ, where the WSD Yarowsky algorithm and many of the WSD supervised methods reach near 95% accuracy. In addition, if in the preceding WSD experiment a practical semantic tagger were used, i.e. a non-perfect semantic tagger, the results of precision would be lower.

This result suggests that using the tagged material from the semantic role field would not constitute a full solution to the disambiguation of homographic word senses.

Other kinds of semantic taggers like named entity recognizers that distinguish between locations, persons, organizations, months, etc. would not seem to outperform the results yielded by the previous semantic role tagger, at least for the words space and drug in the WSJ corpus.

Another possibility would be to use existing knowledge-rich lexical databases as a source of semantic senses and to try to use this large amount of knowledge to first tag the target corpus and then try to disambiguate among the different possible senses of a given word. While this possibility has not yet been put into practice (Rayson, Archer, Piao, &amp; McEnery, 2004 )it should be noted that it would resolve a problem very different from the semi-supervised learning methods from the stand-point of the knowledge acquisition bottleneck, since for instance all this lexical knowledge would need to be ported to a for-eign language system. This is by no means a straightforward task, even when there exist abundant (automatic) linguistic resources in both languages ( L X fberg et al., 2003 ). 4. The sense granularity level
Most WSD methods assign sense labels drawn from a pre-defined sense inventory (not only a traditional dictionary or thesaurus, but also system-specific codes). In the last decade, the sense inventory most commonly used by the WSD com-munity has been WordNet (Miller, 1990 )  X  X  X ne of the lexical resources most used in NLP applications, ( ... ) developed by the Cognitive Science Laboratory at Princeton University ( ... ) organized by semantic relations, providing a hierarchy and net-work of word relationships X  (Agirre &amp; Edmonds, 2006 ). WordNet was used as the sense inventory in English in Senseval-2 and Senseval-3 ( Kilgarrif &amp; Palmer, 2000; Mihalcea &amp; Edmonds, 2004; Resnik &amp; Yarowsky, 1999 ), primarily because it is freely available for research purposes.

The most debated problem arising from the use of the WordNet inventory is that its sense distinctions are too fine-grained. As early as 1993 it was shown (Kilgarrif, 1993 ) that human annotators cannot distinguish well between some of the finer-grained senses delineated in the LDOCE (the Longman Dictionary of Contemporary English). This fact has been re-established in (Edmonds &amp; Kilgarrif, 2002 ), at a ceiling of approximately 80% inter-tagger agreement (ITA) for English.
ITA is computed  X  X  X y looking at how two or more people who have been given the same tagging guidelines annotate the same data; when multiple tags are allowed, agreement may be measured as exact match or as overlap. Agreement may be re-ported as the percentage of times that the annotators assigned the same sense tag to each instance X  ( Palmer, Ng, &amp; Dang, 2006). In recent Senseval exercises several fixes to the  X  X  X ordNet problem X  have been adopted, like a  X  X  X oarse-grained X  scor-ing scheme, where sub-senses are collapsed to their highest parent.

As early as 1994 (Dagan &amp; Itai, 1994 ) it was argued that sense distinctions at the homograph-level, i.e., crane -as-bird-or-machine level,  X  X  X re the ones actually used for most WSD and therefore those needed, by definition, for NLP X  ( Ide &amp; Wilks, 2006). In its strict definition, the word homograph refers to etymologically unrelated words which through historical accident have the same form, like the senses of crane , bank or calf . However, there are many instances where etymologically related senses are as distinct as strict homographs  X  for example, the word paper in its sheet of paper or in its newspaper senses. This fact has been stated in psycholinguistic experiments (Klein &amp; Murphy, 2001, 2002; Rodd, Gareth, &amp; Marslen-Wilson, 2002, 2004). It suggests that some senses of grossly polysemous words, although not unrelated etymologically, are as distinct in the mind of the hearer as strict homographs, and therefore they may be just as relevant for NLP.

These sense distinctions can be identified by other means. One is by using multi-lingual criteria  X  i.e., identifying senses of the same homograph that have different translations in some significant number of other languages. For example, the two senses of paper cited above are translated in French as papier and journal and in Spanish as papel and peri X dico , respectively. Sometimes more than two translations are needed  X  i.e., when the same historical process of sense  X  X  X haining X  (Cruse, 1986;
Heine, 1992; Lakoff, 1987; Malt, Sloman, Gennari, Shi, &amp; Wang, 1999 ) has occurred in different languages and their transla-tions are indistinguishable.

A third and important source of information about relevant sense distinctions is domain,  X  i.e., if senses of a given word are distinguished by their use in particular domains  X  they could be distinguishable at the homograph-like level. Conversely for all practical purposes, as a single homograph-level sense.

Therefore  X  X  X eal NLP applications, when they need WSD, seem to need homograph-level disambiguation, involving those senses that psycholinguists see as represented separately in the mental lexicon, are lexicalized cross-linguistically, or are domain-dep endent. Finer-grained distinctions are rarely needed, and when they are, different kinds of processing are re-quired. For the purposes of NLP, work on the problem of WSD should focus on the broader distinctions that can be determined reliably from context  X (Ide &amp; Wilks, 2006 ). 4.1. A word on performance
Performance achieved by state-of-the-art WSD systems shows that aspect polysemy, i.e. not homograph-level or full polysemy, cannot be resolved reliably from context. Results from the Senseval competitions since 1997 show improvements over time and by the 2004 edition ( Mihalcea &amp; Edmonds, 2004 ) the best-performing systems were performing at human lev-els: 72.9% accuracy compared to an inter-tagger agreement of 67%. But  X  X  X his could be their ceiling X  (Ide &amp; Wilks, 2006 ) and WSD systems seem to need near-100% accuracy in order to be useful in real applications.

This level of accuracy (above 95%) is currently achieved by homograph-level WSD systems. In the introduction to their edited book, Agirre and Edmonds (2006) report two examples of such systems: the Yarowsky (1995) algorithm (96.5%) and the experiments by Stevenson and Wilks (2001) (94.7%). However, these results are not strictly comparable, since the latter uses part of speech from the LDOCE to differentiate between homograph senses, and this information would be unable to differentiate the senses used in the former. Moreover, these sense distinctions (most relating to the same part of speech) are the ones distinguished using the domain and cross-language translation criteria cited above. As an example, the word plant both in its vegetable and its industrial sense belongs to the same part of speech, i.e. noun.

With respect to the knowledge-acquisition bottleneck problem, in the last Senseval competition all the top performing systems were supervised corpus-based systems. This means that they are prone to this problem if applied to a foreign lan-guage or just in a different domain. Similarly, the homograph-level experiments by Stevenson and Wilks use a part of speech tagger (POS) (Brill, 1995 ) (and other knowledge sources) which requires the use of a (standard) dictionary in the target language. 5. The dependence of collocations on domain variation
The main knowledge source used by the Yarowsky algorithm is collocation (KS 3, in terms of Agirre &amp; Stevenson, 2006 )or one-sense-per collocation in terms of Yarowsky (1995) . This is the property by which the words near the target word are as-sumed to be strong indicators of its sense or, in other words, the target word tends to have the same neighbouring words in each of its sets of instance homographic senses.

Martinez and Agirre (2000) ask themselves the following question: does the collocation hypothesis i.e. the one-sense-per-collocation property, hold across corpora; that is, across genre and topic variations (compared to a single corpus, probably with little genre and topic variations)? In order to answer it, they carry out several experiments using the DSO collection.
The DSO collection ( Ng &amp; Lee, 1996 ) is a hand-tagged corpus focusing on 191 frequent and polysemous words (nouns and names). It contains around 1000 sentences (contexts) per word summing up to 192,874 occurrences of the target words.
These occurrences are hand-tagged with WordNet senses, i.e. it is much more fine-grained than the homograph-level sense distinctions. The DSO collection was built with examples from the Wall Street Journal (WSJ) and the Brown corpus (BC). The
Brown corpus is  X  X alanced X , and its texts are classified (rather informally) according several predefined categories that mix topic and genre variations.

In their experiments, they used several kinds of collocations (included those used in the original 1995 Yarowsky paper) organized as decision lists, arguing that these algorithms X  results correlate closely to entropy measures. They trained the decision lists using a subset of the DSO corpus and tested the results on another subset. They also adapted decision lists to n-way ambiguities, instead of the two-way ambiguities used at the homograph-level (Martinez &amp; Agirre, 2000 ).
First, they extracted the collocations in the Brown corpus section of the DSO corpus and tagged the same corpus. The same procedure was followed for the WSJ section. From their results they drew the conclusion that  X  X  X ollocations are stronger in the WSJ, surely due to the fact that the BC is balanced, and therefore includes more genres and topics. This is a first indicator that genre and topic variations have to be taken into account X . They also concluded from this experiment that collocations are  X  X  X ensibly weaker X  for fine-grained word senses than for two-way, i.e. homograph-level, ambiguous words: 70% precision (they use this term as a synonym for accuracy) versus 99% reported by Yarowsky (1993) .
 Next they performed several cross-corpora experiments: they trained on the BC and tagged the WSJ corpus and vice versa. They report a  X  X  X ignificant drop X  in precision (16%) and coverage relative to the in-corpora experiments.

They remark that the training and test examples from the in-corpora experiments were taken at random and therefore could have been drawn from the same document whereas in the cross-corpora experiments training and test examples come without doubt from different documents. In order to ascertain if this might be the reason for the lower cross-corpora results, they repeated the in-corpora experiments ensuring that the training and test examples came (also at random) from different documents. For the BC experiment, precision and coverage were degraded significantly relative to the original in-corpus experiment while results for the WSJ remained nearly the same as in the original in-corpus experiment. This could be ex-plained by the  X  X  X reater variation in topic and genre between the files in the BC corpus X  (Martinez &amp; Agirre, 2000 ).
After observing that the category press:reportage in the BC is related to the genre/topics of the WSJ they tagged each cat-egory in the BC with the decision list trained in the WSJ, and also with the decision list trained on the rest of the categories in the BC. They found that the best precision and coverage attained by the decision list trained on WSJ was for the press:report-age category and that these results were also better than those obtained by the decision lists trained on the remaining cat-egories in the BC.

They conclude that collocations vary from one corpus to the other following genre and topic variations. 6. The Yarowsky algorithm and domain variation
If collocations are domain-dependent and are also the main source of knowledge used by the Yarowsky algorithm a ques-tion arises about the domain-dependency of this algorithm.

In order to explore this relationship we carried out several bootstrapping experiments i.e., executions of the algorithm, on different corpora. Specifically, we used the 1989 subset of the Wall Street Journal (WSJ) corpus and a section of a balanced corpus like the British National Corpus (BNC).

The WSJ corpus is a collection of approximately 30 million words in 98,732 stories appearing in the Wall Street Journal newspaper in the years 1987, 1988 and 1989. It was developed by the Brown Laboratory for Linguistic Information Process-ing (BLLIP) and contains a complete parsing of the corpus.

The BNC is a general-purpose corpus of English language material. It contains 4054  X  X  X exts X  (samples not exceeding 45,000 words) with over 100 million words in over six million sentences.

The BNC is a balanced corpus: texts were chosen for inclusion according to three selection features : domain (subject field), time (within certain dates) and medium (book, periodical, etc.) and target percentages were set for each class. Half of the books in the  X  X  X ooks and Periodicals X  class were selected at random from Whitaker X  X  Books in Print 1992 . In this category, a target sample size of 40,000 words was chosen.

Classification according to domain is listed in Table 2 (all non-imaginative sources are treated as informative). Table 3 lists the classification according to medium. The  X  X iscellaneous published X  category includes brochures leaflets, manuals, advertisements. The  X  X iscellaneous unpublished X  category includes letters, memos, reports, minutes, essays. The  X  X ritten-to-be-spoken X  category includes scripted television material, play scripts, etc. 6.1. Bootstrapping experiments
We carried out two executions of the Yarowsky algorithm on the 1989 subset of the WSJ corpus and on the non-imag-inative subset of the books category of the BNC. We used the word forms drug , plant and space as target homographic words.
The initial seed selection method was the  X  X wo words X  option introduced by Yarowsky (1995) . In this method a single defining collocate is manually identified for each sense and only those contexts containing one of these two words are used as seeds. This is one of the simplest methods and no attempt was made to apply the one-sense-per-discourse (OSPD) con-straint at any point in the algorithm.
 The results as reported by Yarowsky in his original 1995 work are presented in Table 1 , column 5.
 Our results of the WSJ and BNC experiments are shown in Tables 4 and 5 , respectively. We can see that our results on the
WSJ in Table 4 are very similar to Yarowsky X  X  results in Table 1 : we obtain slightly better results for homographs drug and space (1.2% and 1.1% higher, respectively) and lower results for homograph plant (5.3% lower). However, as can be seen in
Tables 4 and 5 the WSJ corpus yields a much higher accuracy (16.5% higher on average) than the BNC experiment, which falls far from the near-100% accuracy (74.7% on average). This could be due to the  X  X  X reater variation in topic and genre X   X  using Martinez and Agirre X  X  terminology -of the BNC with respect to the WSJ corpus. It confirms their results as discussed in the preceding section, and we can conclude that collocations in general are domain-dependent and in particular their appli-cation in the Yarowsky algorithm makes this also domain-dependent.

The question arises as to why Yarowsky obtained his original high-accuracy results. Given the sample sizes used in our two experiments (2498 and 1013 contexts, in the case of the word form drug for instance) versus the sizes of the corpora (10 million and 40 million words, approximately) we can only conclude that the target word drug is much more frequent in a news than in a real corpus and this makes the news  X  and maybe also other sections  X  of Yarowsky X  X   X  X 460 million word corpus containing news articles, scientific abstracts, spoken transcripts and novels X  the most significant contributors of word form drug occurrences. 7. The bootstrapping of the Yarowsky algorithm in a general-purpose corpus
The Yarowsky algorithm resolves the WSD problem at the homograph-level (near 100% accuracy: by analogy with Table 1, 3.5% could be added by using the fuller Yarowsky procedure) yielding 95.06% accuracy for target word  X  X rug X , the sense level and accuracy required for real NLP applications. At the same time, being an almost unsupervised algorithm i.e. requir-ing virtually no training, it also resolves the knowledge-acquisition bottleneck problem. This means, for example, that it can be easily used with foreign language corpora.

However, the Yarowsky algorithm does depend on domain variations. As shown in the previous section, its accuracy falls significantly if the target corpus exhibits domain fluctuations, as in the case of the BNC. In this section we investigate a meth-od of bootstrapping the Yarowsky algorithm on such balanced general-purpose corpora.

In principle, the algorithm could be bootstrapped in a general purpose corpus like the BNC by bootstrapping each differ-ent document of the corpus separately in the usual way. However, this approach has serious drawbacks: if we select the ini-tial seeds for each document manually we are in fact using a supervised method, given the high number of documents; but neither can we do this automatically in the usual way. This is due to several different reasons: first, there is a large number of domain changes between different documents, even within the same target homograph word sense; second, there is also a very different degree of intrinsic ambiguity among documents; and third, the original Yarowsky algorithm is very susceptible to the proportion of initial correct seeds, and so a slightly low number in this ratio can cause a dramatic fall in the algorithm X  X  accuracy.

This means that we cannot simply apply the same seed selection collocations to the whole corpus, even if we try to boot-strap each document separately. We need a procedure to generate bootstrapping collocations that change dynamically through the different documents/domains of the corpus. Here the Yarowsky algorithm has the advantage of using the deci-sion list machine-learning algorithm which is flexible enough to perform this adaptation. Also, we need a procedure to val-idate the decision list trained on one document in order that it may be applied to other documents, thereby preventing the creation of false degenerate decision lists. This procedure uses the original algorithm X  X  one-sense-per-discourse property and is explained in the next subsection. 7.1. The application of the one-sense-per-discourse property
The overwhelming accuracy of the one-sense-per-discourse (OSPD) property on two-way homograph-level ambiguities, yielding an accuracy of 99.8% and an applicability of 50.1% in Yarowsky X  X  1995 paper, has encouraged us to use a  X  X igital X  approach to the first part of the bootstrapping algorithm. Specifically, the standard Yarowsky algorithm is applied to each separate BNC document. This means that each target word context in that document gets labelled with a sense tag and a score or else it does not get labelled at all because its original score does not reach some corresponding predetermined threshold. Then all the scores in that document corresponding to each labelled sense are added up and the following formula is applied:
In this formula, n 0 and n 1 represent the total number of labelled contexts corresponding to each sense (senses 0 and 1) in the document; and s stands for the score of one labelled context. If the resulting number th reaches some predefined threshold then all the contexts of the document are labelled with the corresponding sense tag. As a result, we are determining the homo-graph sense for each different document or, in other words, the theme of each document, at least from the viewpoint of the two different target homograph senses. This can be very useful, at least from an IR perspective. 7.2. The bootstrapping algorithm
The bootstrapping algorithm only needs two initial correct manual context sense tags in two initial documents. The homographic themes (see previous subsection) of these documents should be different, i.e. the majority homographic sense of each document should correspond to one of the two different senses. These two documents are compound (juxtaposed) into a single document, and only two contexts in the compound document must be (correctly) labelled, one with each dif-ferent sense tag. The size of these documents need not be very large  X  some 10 contexts each are enough.
 An initial decision list is trained on the compound document. This means that one whole performance of the standard
Yarowsky algorithm must be executed on this document and it must succeed, i.e., the vast majority of the sense 0 contexts corresponding to the original sense 0 single file should be labelled with that tag and the same should apply to the sense 1 contexts in the other original single document. It is very easy to check this condition automatically and its success is a pre-condition for the success of the whole bootstrap.

This decision list, or classifier, is applied to each separate document in no particular order. This means that we are per-forming the Yarowsky algorithm on each separate document without bootstrapping it from originally (manually) tagged context seeds  X  we are using an already-built decision list. If this application succeeds in the establishing the homographic theme of the document, as outlined in the previous subsection, the decision list is updated with the new collocations arising from this document, i.e. as a result of performing the Yarowsky algorithm using that decision list, it gets updated with new collocations arising from the new document. Note that we are only interested in the new decision list if the algorithm suc-ceeds in deciding the homographic sense of the document. This guarantees the correctness of the collocations in the new decision list.

However, only the highest scoring collocations, i.e. the several first elements of the new decision list, are kept. This en-sures that the decision list is changing very quickly. One of the reasons for the failure of the original Yarowsky algorithm on a domain-changing corpus is that the collocations get mixed on a single corpus-scoped decision list due to changing contexts.
Making the decision list change with every (different) document has the effect of not mixing contexts whilst bootstrapping similar context (or domain) documents. As a result the algorithm needs to be applied to all not-yet-categorised documents of the corpus several times until a stable state is reached, i.e. no more documents succeed to decide its theme. This state is reached after several iterations on the whole corpus.

Another consequence of this is that once a new decision list is generated it must be applied to every not-yet-categorised document before it gets updated by any other previous of these . This means that some data structure needs to be used in order to keep each decision list from being updated before it is tried with all the not-yet-decided documents of the corpus, and that once a new decision list is generated it is immediately stored into that data structure. This maintenance policy suggests that this data structure should be a FIFO queue.

However, since when the algorithm succeeds in a given document i.e. when the homographic theme of the document is determined, it is likely that the decision list is biased towards one of the two word senses  X  the majority sense corresponding to that document, two different FIFO queues are actually used  X  one for each sense  X  as shown in Fig. 1 . The two lists at the front of each queue are merged into one single decision list  X  see Fig. 2  X  in an all-to-all fashion until both queues become empty. This combined list is the decision list applied to every not-yet-categorised document in the corpus. Decision lists are removed from the queue once they have been tried on the whole corpus, and the process ends when one of the two queues becomes empty, i.e., no single decision list can be built. Fig. 3 shows a pseudo-code-like formalization of the algorithm.
Application of this algorithm to the  X  X on-imaginative X  and  X  X ooks X  intersection subset of the BNC corpus using target words drug , plant and space yields typically a number of documents labelled (decided) after several loops of the whole cor-pus, resulting in very high (95 X 100%) accuracy and low (10%) recall  X  i.e., labelled contexts ratio. Subsequent applications of the algorithm to the remaining documents yield similar accuracy but lower recall. Moreover, further applications after three or four times do not produce new labelled documents. This means that there are a number of documents ambiguous enough to be incapable of being classified by any decision list trained on (almost) any other document.

Table 6 shows the accuracy results obtained after application of the bootstrapping algorithm four times with each of the three homographs as target words. Non-categorised contexts (these include all single-context documents and several multi-ple-context documents) were assigned the sense label yielded by the application of the standard Yarowsky algorithm to the same whole corpus, as in Table 5 . This information is taken into account in the results shown in Table 6 and in Fig. 3 from the start, i.e., beginning at the first bootstrap. As in Tables 4 and 5 one column shows the major sense frequency baseline for each target word in the corpus.

As can be seen in Fig. 4 , the behaviour of the accuracy values after several bootstraps is considerably different depending on the homograph under disambiguation. Accuracy with drug gets better almost linearly with further bootstraps, while in the case of plant the final result after four bootstraps is almost reached after the first bootstrap. The homograph space pre-sents an intermediate behaviour. This could be explained by the fact that plant would be, at least in the BNC subset under study, the least ambiguous homograph among the three and so only one bootstrap would be sufficient to decide the homo-graphic sense of almost all the documents. In the other extreme would stand drug , the most ambiguous; relatively few doc-uments are decided in the first bootstrap, meaning that drug documents are intrinsically more ambiguous and thus harder to decide. However, for the same reason, many more documents are decided in subsequent bootstraps. Space represents a mid-dle case, in which an intermediate number of documents get decided after the first bootstrap and as a consequence also an intermediate number get decided in the following bootstraps.

This hypothesis is indeed confirmed by the results of the standard Yarowsky algorithm on the whole corpus using the three homographs presented in Table 5 . Those results confirm that drug is the most ambiguous homograph and presents the lowest accuracy, plant is the least ambiguous with the highest accuracy and space is an intermediate ambiguous homo-graph that gets an intermediate level of accuracy under the Yarowsky algorithm. This would suggest that the better the per-formance of the standard Yarowsky algorithm, i.e., the less ambiguous is the homograph/corpus, the less necessary is the new bootstrapping methodology.

These results are reinforced by those of Tables 1 and 4 on the WSJ. The results on the BNC ( Table 5 ) show that space is inter-mediately ambiguous between plant and drug , but actually it is closer to drug than to plant . Table 1 on the WSJ gives the poor-is worse than drug in the WSJ could be explained by the very different nature of the two kinds of corpora  X  a journalistic news corpus (WSJ) and a balanced general text corpus (BNC). In any case the difference in that corpus is very low  X  around 1.4%.
The hypothesis thus suggests that there exists an intrinsic degree of ambiguity in a document as represented by a set of target word occurrences and its contexts, i.e., a degree of ambiguity related to a specific document/homograph pair. How-ever, we could argue that a fraction of this ambiguity is inherent to the homograph/corpus and is located in some especially ambiguous contexts, whereas another fraction is attached to the bootstrapping methodology itself, and thus could be im-proved. This fraction of non-decided documents would be composed of those documents that do not necessarily have espe-cially ambiguous contexts, but do indeed have a domain change  X  or homographic theme change  X  inside them. This domain change would make the methodology unable to decide their homographic theme since it is mixed. Further work needs to be done in order to separate these documents into their smaller uniform theme parts, thus perhaps improving the level of accu-racy of the algorithm.
 7.3. Significance of the bootstrapping algorithm
As pointed out in Sections 3.1 and 3.2 , semi-supervised learning algorithms suffer from two self-specific problems, parameter tuning and mislabelled examples, and one generic problem, domain fluctuation. We believe that the bootstrap-ping algorithm introduced in this paper constitutes a solid methodology to overcome these three withdrawals.
The use of a binary approach makes the system self-correcting: the binary module is able to decide if a newly labelled example in a document will be kept or if it will be discarded. In reality, it will not be the tagged example which will be kept or discarded, but the collocations generated by it on its decision list. This makes the system free from the proliferation of mislabelled examples problem, probably the most important defect of semi-supervised algorithms, and it does it automat-ically, without any external or manual active learning procedure.

This approach also can help in the parameter tuning issue, since the proportion of successes in the binary decisions serves as automatic feedback for the calibration of how good the current parameter setting is, and thus provides a way to automat-ically tune it. This number of successes must not be too high, because that means that wrong decisions are being taken, nor too small, which means that the system is being too strict and many documents are not being decided with the consequent waste of resources. Further work needs to be done in this sense to determine the correct proportion of document successes given only the invariant inputs to the algorithm.

Finally, as suggested in ( He &amp; Gildea, 2004 ), semi-supervised algorithms seem to be good candidates to tackle the domain fluctuations present in general text corpora. In previous unpublished work the authors have shown that this is a generic problem affecting all general text (non-journalistic) corpora and not only semi-supervised, but also supervised corpus-based
WSD algorithms. The semi-supervised algorithm proposed in this paper is an OSPD-perfect algorithm in the sense that if the documents of the corpus are thematically well segmented, the algorithm is able to produce results at least as good as those permitted by the OSPD property, which reaches an accuracy of 99.8% and an applicability of 50.1%. The only obstacle here is due to applicability: those documents where the target word occurs only once; and a very little number of especially ambig-uous documents that, even when they are monothematic, they exhibit a strong ambiguity which makes the algorithm unable to decide their (homographic) theme.

It should be noted that due to its binary nature, no other algorithm can outperform it over the OSPD constraint level, pro-vided that the thematic segmentation of the documents is correct, and with the only possible exception of an hypothetic very efficient WSD algorithm that would be able to disambiguate (dynamically) those few especially ambiguous documents that seem to appear in almost every corpus. 8. Conclusions
We have shown that the Yarowsky algorithm only performs at above-95% accuracy in the context of a roughly homoge-neous corpus like a journalistic news corpus. In a general-purpose, i.e. general text, corpus the algorithm performs at a much lower level of precision (about 70%) due to domain fluctuations. This phenomenon has its roots in the domain-dependence of syntactic collocations which constitute the main source of knowledge used by the algorithm.
 These domain fluctuations have the effect of mixing up the collocations used by the decision list classification algorithm. As a result, this algorithm becomes unable to determine correctly the sense of each homographic instance.

We have shown that a new bootstrapping algorithm that uses a rapidly changing decision list, as opposed to a single-iter-ation-wide decision list, succeeds in not mixing up domain-dependent collocations and thus maintains the algorithm X  X  near-100% accuracy. However, there still remains an intrinsic level of mixed ambiguous collocations represented by a number of especially ambiguous documents which the changing decision list algorithm is unable to classify. These documents may still be classified by the standard Yarowsky algorithm, but at a lower level of accuracy due to their inherent ambiguity. The over-all compounded effect is an increase in the algorithm X  X  accuracy from 74.7% to 85.3% (at 100% recall) against a major-sense baseline of 59.8% (see Tables 5 and 6).
 References
