 information but only aims at predicting the labels of the kno wn unlabeled examples. The problem of transduction inference was originally formu lated and analyzed by Vapnik [1982] with classification problems.
 Vapnik [1998], this function must be  X  X abulated by a compute r X . when the number of training points m is small compared to the number of unlabeled points, using algorithms and that it can scale to large data sets.
 and the experimental results of our study are reported in Sec tion 5. the labels of a random subset of X of size m which serves as a training sample: The remaining u unlabeled examples, x problem that we consider consists of predicting accurately the labels y that information and achieve a better result than via the sta ndard induction. tion. For a hypothesis h  X  H , we denote by R b R ( h ) its error on the training data, and by R ( h ) the error of h on the test examples: R 0 ( h ) = For convenience, we will sometimes denote by y This section presents explicit generalization error bound s for transductive regression. general explicit bounds for transductive regression.
 Our first bound uses the function  X   X  defined as follows. Let  X (  X , k ) be defined by: where I ( m, u, k,  X  ) is the set of integers r such that: k  X  r  X  between the training and test set when the total number of err ors is k (see [Cortes and Mohri, 2006]). Then  X   X  is defined as  X   X (  X  ) = max fication bound of Vapnik [1998] (see [Cortes and Mohri, 2006] [Theorem 2]). [Cortes and Mohri, 2006][Corollary 2] gives an upper bound on  X   X  .
 denote the fraction of the points x the value zero if ( h ( x )  X  y such classifiers, we will denote that number by N ( m + u ) .
 Theorem 1 Let  X  &gt; 0 , and let  X  assume that the loss function is bounded: for all h  X  H and x  X  X  , ( h ( x )  X  y B  X  R + . Then, with probability at least 1  X   X  , for all h  X  H , Proof. For any h  X  H , let R By the Cauchy-Schwarz inequality, for all x  X  X  . Let Pr according to D . By definition of R R 0 ( h ) = Similarly, setting X In view of Equation 7, Inequality 6 can be rewritten as: R 2006][Theorem 2], for all  X  &gt; 0 and for any t  X  0 , Then, the convergence of the Riemann sums to the integral ens ures that Let  X  &gt; 0 and select  X  =  X  probability at least 1  X   X  , Plugging in the following expression of R and solving the second-degree equation in R ( h ) yields directly the statement of the theorem. the number of equivalence N ( m + u ) or the VC-dimension d , and the sample sizes m and u . bounded: for all h  X  H and x  X  X  , ( h ( x )  X  y least 1  X   X  , for all h  X  H , with  X  = and Mohri, 2006][Corollary 2], log N ( m + u )  X   X (  X  )  X  log N ( m + u )  X  1 This gives the upper bound on  X  in terms of the VC-dimension.
 algorithm inspired by these concepts is described in the nex t section. This section presents an algorithm for the transductive reg ression problem. space H , that minimizes the following optimization function parameters, and where the minimum is taken over all possible labels y  X  setting. Indeed, let h the induction problem. For the particular choice y  X  vanishes. Thus, h h ( x m + i ) is not in { 0 , 1 } .
 stage is based on the position of unlabeled points. For each u nlabeled point x a local estimate label  X  y and the estimate labels  X  y less vulnerable to noise. 4.1 Local Estimates Let  X  be a feature mapping from X to a vector space F provided with a norm. We fix a radius neighborhoods to limit the number of parameters for the algo rithm. Labeled points x  X  X  images  X ( x ) fall within the neighborhood of  X ( x  X  ) , x  X   X  X  labeled point exists in the neighborhood of x  X   X  X  disregarded in both training stages of the algorithm.
 There are many possible ways to define the estimate label of x  X   X  X  borhood labels y which is what we used in most of our experiments.
 In practice, with a relatively small radius r , the computation of an estimated label  X  y on a limited number of labeled points and their labels, and is quite efficient. 4.2 Global Optimization objective function 4.2.1 Primal solution Let N be the dimension of the feature space and let W  X  R N  X  1 denote the column matrix whose labels y estimated labels  X  y X then be rewritten as: G is convex and differentiable and its gradient is given by The matrix W minimizing G is the unique solution of  X  G = 0 . Since ( I is invertible, it is given by the following expression When the dimension N of the feature space is small compared to the number of exampl es m + u , vector associated to x is the m -dimensional vector  X ( x ) = [ K ( x, x u This computational advantage is not shared by other methods such as the manifold regularization see Section 5) since it requires among other things the inver sion of a matrix in R u  X  u . Once W is computed, prediction can be done by computing X  X  X  X  W in time O ( uN ) . 4.2.2 Dual solution dimensional feature spaces. Let M by: Then, Equation 21 can be rewritten as: W = ( I solution, observe that where I from a series expansion of ( M where K is the Gram matrix K = M  X  the sub-matrices of the Gram K defined by: K Then, predictions can be made using kernel functions alone s ince X  X  X  X  W can be computed by: ples, this can lead to a faster computation of the solution. ( I examples u  X   X  u determines the computational cost. original paper. With the notation used in that paper, it can b e shown that training and test sets.
 The kernels used with all algorithms were Gaussian kernels. To measure the improvement produced values for the width of the Gaussian  X  and the ridge 1 of labeled points.
 For our algorithm, we experimented both with the dual soluti on using Gaussian kernels, and the with very large u , e.g. u  X  10 , 000 , where the dual method was too time-consuming. improvement in mean squared error (MSE) with respect to the b aseline averaged over the random Housing data set (both absolute and relative MSE).
 This matches many real-world situations where amount of unl abeled data is orders of magnitude larger than that of labeled data. transductive regression.
 for dealing with these and other similar transduction regre ssion problems.
