 instances [7]. In most traditional active learning methods, an expert is required to provide ground truths to the queried instances and the model will be learned based on the labeled data. The learned model is applied to the unlabeled data again and another subset of unlabeled data is selected to ask for the expert X  X  labeling. This procedure is iterated many rounds until some criteria are met.

Although active learning only requires a subset of instances to be labeled, it is still not a simple task for that the selected data set is still large and the active learning iterations can last for quite long time. Moreover, since the model will be learned only based on a subset of the entire data set, the labeling quality for the selected instances is extremely cru-cial for the model X  X  performance. Thus, the labeling task in active learning is still expensive in many cases.
Recently, researchers resort to committees of nonexpert la-belers, who are cheaper but can only provide weak and noisy labels, for data labeling. Some works based on this idea have been proposed, such as [8, 5], which only consider the label-ing issue in supervised learning scenarios. However, such weak and noisy labels may not be helpful in active learning scenarios for two reasons: (1) Since only a small subset of critical instances are selected for labeling, the labeling qual-ity in active learning is more sensitive to the model X  X  per-formance than that in plain supervised learning. (2) Since active learning is consisted of multiple learning iterations, the errors induced in each round will be passed onto the following rounds and will be amplified. Thus, asking nonex-pert labelers to directly provide noisy class labels may not be appropriate in active learning.

In this paper, we propose a new active learning paradigm, in which a nonexpert labeler is only asked  X  X hether a pair of instances belong to the same class X . The proposed paradigm is based on the following assumption: For a nonexpert la-beler, it is difficult to tell the definite class label of an instance by linking its content to a conceptual label (which needs domain knowledge), while, it is easier to directly compare the content of two instances and tell their label homogeneity (which only needs intu-ition). Based on this assumption, the underlying queries for active learning is to generate pairwise constraints be-tween labels, which will be incorporated into the learning procedure. Figure 1 illustrates the difference between the traditional active learning paradigm and the proposed one. Compared to a specific label in the traditional strategy, our strategy only acquires a pairwise label homogeneity informa-tion ( X  X es/no X ) in each query, which is much easier/cheaper to implement for a labeler. labels in supervised learning [8, 5]. However, since the la-beling quality is much more sensitive to the model X  X  per-formance in active learning than that in plain supervised learning due to a limited number of labeled data. Asking nonexpert labelers to directly provide noisy class labels may not be appropriate in active learning. Otherwise, the er-rors induced in each query round will be passed onto the following rounds and will be amplified.

In contrast to the above methods, the proposed active learning paradigm employs nonexpert labelers to perform labeling in each round of active learning iterations, but only request them to provide pairwise label homogeneity informa-tion (answer  X  X es/no X ). Compared to specifying class labels, answering  X  X hether two instances belong to the same class X  is much simpler for nonexpert labelers and the queried re-sults are more reliable for training prediction models. Given a data set D , which comprises a labeled subset D L , an unlabeled subset D U , and a test set D T . At first, D L D U is casted into a graph G =( V , E ) based on some similarity measures, where V = D L D U ,and E X  X  X V . Each edge e has a weight w ( e ). Each node v i is denoted by v = { v i, 1 ,  X  X  X  ,v i,m ; y i } ,where v i,j denotes the j th attribute value of the node and y i denotes v i  X  X  class label. If v i is un-labeled, we denote it by v i = { v i, 1 ,  X  X  X  ,v i,m ;? } . All labeled nodes form an labeled vertices subset V L and the remaining unlabeled nodes in G form an unlabeled vertices subset V U , with V = V L  X  X  U and V L  X  X  U =  X  .

Assume a nonexpert labeler exists to answer queries on whether a pair of samples ( v i , v j ) belong to the same class or not, our aim is to query the labeler for a number of times, through which we can infer labels for a batch ( i.e. asubset  X ) of instances from V U ( i.e. , V U = V U \  X and V L = V L  X   X  ), such that when the requested number of nodes are labeled, a classifier trained on V L has the maximum prediction accuracy in classifying test set D T .
In order to train a classifier on V L with maximum pre-diction accuracy, a commonly employed principle for active learning is to label a batch of most informative data. Fol-lowing this principle, we employ MinCut algorithm to infer the unlabeled data memberships. We then query all the unlabeled edges in the max-flow paths with a pairwise la-bel homogeneity question. According to the query results, our algorithm updates the weights of edges whose two ver-tices are queried pairs. This process iterates k times, with k MinCut classifiers generated. All the unlabeled nodes can be resorted according to the prediction confidence on the k MinCut classifiers. Therefore, our goal is to select an opti-mal subset with the highest confidence values. The overall algorithm procedure is described in Algorithm 1. Given a binary classification task with a set of labeled data D L and a set of unlabeled data D U ,weuse L p to indicate the set of positive data in D L ,and L n to denote the set of negative data in D L . The graph construction is as follows. where u  X  X \{ s, t } . Therefore the flow from source to sink is denoted by where s is source vertex. The maximum flow problem is to maximize f s  X  t with as much flow from s to t as possible.
The maximum flow through a series of paths relies on the smallest flow of the edge on each path, which is also the bottleneck of each path. That means if such bottleneck edges are removed from the network, it leads to no flow can pass from the source to the sink. As a result, we can determine a minimum cut using maximum flow algorithm.
In general, a graph may have many mimimum cuts, whereas max-flow algorithm exits when a minimum cut is found. This may generate an extremely imbalanced cut, that is, a small cluster associated with a big one. Furthermore, an initial small amount of labeled subset could not generate a cut closed to the genuine distributions of the underlying data. To this end, we employ active learning to obtain extra information by querying unlabeled pairs on maximal flow paths. Here we just query  X  X hether the pair belong to the same class X . According to the query results,we adjust weight of the edges formed by queried pairs. This weight updating scheme is defined as follows: where  X  is an adjustment factor, which determines weight updating scale(0 &lt; X &lt; 1). When the pair has the same class label, the corresponding edge weight increases by  X  ,other-wise, the relevant edge weight decreases by  X  . We iteratively use MinCut algorithm to update edges weights, avoiding im-balanced cut issue and refining decision boundary.
When a MinCut algorithm is instantiated, the class labels of all the unlabeled vertices are inferred by the cut.During weight updating circle, a community of MinCut classifiers is generated.We report the prediction on each classifier for each unlabeled vertex in the graph, such that an ouput dis-tribution can be calculated for each unlabeled vertex. We treat the maximal probability output value as the final pre-diction result, and the value is considered as the confidence of each unlabeled vertex. We sort the unlabeled vertices basedontheir confidence values. The top budget vertices formed an optimal labeling set  X , with the final prediction as their labels. We add  X  into labeled training set to train a model and make prediction on the test set D T .
We implement the proposed method and a number of baseline methods using Java and WEKA data mining tools. For fair comparisons, all experiments are based on 10 times 10-fold cross validation, with all methods compared using the same training and test data sets (the initial randomly labeled samples are also the same for all methods). For per-formance analysis, we compare our method with supervised learning methods and semi-supervised methods. If our al-gorithm X  X  accuracy is higher than the other two kinds of
The results in Figure2(b) show that cotraining algorithm is superior to supervised method and TransductiveSVM. This is mainly because by employing a data selection scheme, important samples can be selected to help improve the un-derlying learning model. Our results also show that Trans-ductiveSVM outperforms supervised method for all percent-ages of selected samples, which asserts that incorporating unlabeled sample to the learning process can provide addi-tional information to benefit the model training.
Comparing Figures 2(a) and 2(b), it is clear that the per-formance of ALQPM will continuously improve with more samples to be labeled, whereas the performance of cotraining algorithm remains relatively stable most of times. This as-serts that using multiple independent views to label samples, like Co-Training does, has very limited contributions, com-pared to the proposed pair-wise homogeneity query strategy.
In this paper, we formulated a new active learning paradigm, in which a nonexpert labeler is asked to answer the label ho-mogeneity of a pair of instances instead of a specific label for an individual instance. To solve the problem, we proposed to incorporate queried results into a MinCut based active learner by updating unlabeled pair weights on the max-flow paths in the graph. Then we select an optimal unlabeled subset of nodes according to their prediction confidence to extend the labeled data set for model training. Experiment results demonstrated that our new active learning paradigm can achieve good performance compared to various state-of-the-art methods.
 This work is supported by Australian Research Council X  X  Future Fellowship under project number FT100100971 and Centre for Quantum Computation &amp; Intelligent Systems. [1] A. Blum, J. Lafferty, M. R. Rwebangira, and R. Reddy. [2] D. Cohn, Z. Ghahramani, and M. Jordan. Active [3] G. Druck, G. Mann, and A. McCallum. Learning from [4] J. Du and C. C. Ling. Asking generalized queries to [5] V. C. Raykar, S. Yu, L. Zhao, A. Jerebko, C. Florin, [6] B. Settles. Active learning literature survey. Technical [7] B. Settles and M. Craven. An analysis of active [8] V.S.Sheng,F.Provost,andP.G.Ipeirotis.Get
