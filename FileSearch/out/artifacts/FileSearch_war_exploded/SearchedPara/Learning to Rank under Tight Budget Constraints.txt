 This paper investigates the influence of pruning feature lists to keep a given budget for the evaluation of ranking meth-ods. We learn from a given training set how important the individual prefixes are for the ranking quality. Based on there importance we choose the best prefixes to calculate the ranking while keeping the budget.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithms, Performance, Measurement Learning to Rank, Constraints
Efficiently finding the best results for a query is an im-portant issue in many precision-oriented, interactive tasks. Consequently, a large number of top-k or dynamic pruning algorithms have been proposed (see [2,6] for an overview). In many applications, results need to be provided within a con-trolled amount of time; however, only very few algorithms have been proposed that explicitly support execution under a limited budget of processing time or disk access costs. One of the few examples from the database community, proposed by Shmueli-Scheuer et al. [3], makes heuristic decisions at runtime based on the documents seen so far. A more fun-damental approach that integrates budget constraints into a learning-to-rank framework was recently proposed by Wang et al. [5]; however, they focus on selecting features and do not consider evaluating features for some documents only, resulting rather long processing times of a few seconds per query.

We propose an algorithm that combines the best of both worlds: It selects, for each feature of the ranking function, a subset of documents to be evaluated before the actual query processing starts, extending the method of [5]. We demonstrate in experiments with TREC GOV2 and queries from the TREC Terabyte track that our method can achieve almost perfect precision while scoring only as few as 20% of the documents.
We focus on the family of linear ranking functions that come with a family of features X  X unctions F = { f 1 ,...,f assigning real values to (document,query) pairs X  X nd model parameters  X  = {  X  1 ,..., X  N } . Our current work uses only unigram (i.e., term) features, but can be extended to bigram features. For simplicity, we assume that each term t i cor-responds to exactly one feature f i , which can be its BM25 score (used in our experiments), a Dirichlet score, etc, and write f i ( d ) instead of f i ( d,q ); again, extension to multiple features per term is simple. The score of a document d with respect to a query q = { t 1 ,...,t n } is computed as Following existing work [1], we define the feature weights as weighted sum of a number of document independent meta features (we use the unigram meta features from [5]), i.e.,  X  t = P j  X  j g j ( t ), where the  X  i are learned from training queries.
We assume that for each feature f i , a corresponding in-verted list L i exists that provides all pairs ( d,f i ( d )) of doc-uments d with their feature values where f i ( d ) 6 = 0, and that is sorted by descending feature value. Processing a query q without budget constraints would read all entries from the lists for q and compute the top-k result documents. The problem considered in this paper is computing good ap-proximate results for q when the budget, i.e., the aggregated number of entries read from all lists, is limited by B . Under the realistic assumption that documents with high feature values are more likely relevant than documents with low fea-ture values, we will read only prefixes of lists. Representing by m i the maximal feature value of any document in L i , we denote by L b i i = { ( d,f i ( d ))  X  L i | f i ( d ) &gt; b of L i where the score is more than b i  X  m i , for b We denote by | L | the number of entries in a list or a list prefix. We now want to compute an access plan P  X  [0; 1] that denotes, for each L i , which prefix L P i i should be ac-cessed. The execution cost of such an access plan is given as | P | = P | L P i i | . Given such an access plan P , the score of a document now is
Our goal is now to determine an access plan P for query q that keeps the given budget B , i.e., where | P |  X  B , and yields a good result quality. We leverage a general idea from [5] for selecting features under budget constraints: In-tuitively, the weight  X  i of a feature indicates how important that feature is for the final ranking. We extend this approach by assigning a weight  X  b i i to each prefix L b i i , and select an access plan P  X  that maximizes the sum of the prefix weights while keeping the budget: This is an instance of the multiple-choice knapsack prob-lem [4].For tractability, we restrict possible choices of P a relatively small number of values (we use { 0 , 0 . 1 ,..., 0 . 9 } ); this makes problem instances usually small enough to solve them exactly.

However, this just shifted the problem towards comput-ing the  X  P i i . We believe that these weights should be derived from the weight  X  i of the corresponding list L i in the uncon-strained case, and examined the following two derivations (others could be possible as well):
We evaluate our method using the TREC GOV2 collection and topics from the TREC Terabyte tasks 2004 X 2006, which we split into training (100 topics from 2005+2006) and test (49 topics from 2004) topics. Our features are BM25 scores with k 1 = 1 . 2 and b = 0 . 5, and we use the same meta features used in [5]. We first learned optimal list weights (  X  i ) on the training topics without budget constraints. We considered a set of relative budgets B = { 0 . 2 , 0 . 25 ,..., 1 . 0 } , where a relative budget of 0.2 means the system can access at most 20% of the entries in all index lists for that query. We learned the  X  P i on the training topics for precision@20 assuming uniform distribution of the budgets.

We then measured precision@20 for the test topics for the same selection B of relative budgets, solving the multiple-choice knapsack problem with the two different list prefix Figure 1: Precision@20 for different budgets, rela-tive to unconstrained execution weights introduced in the previous section. Figure 1 shows the result of this experiment, where we report relative pre-cision compared to the precision with full feature lists. It is evident that both methods provide good results (relative precision &gt; 90%) over a large range of budgets. For small budgets below 35% of the full index lists, the learned weights outperform the simple cost-based weights, providing excel-lent performance even at 20% budget.
The results show the benefit of using the introduced pre-fixes when facing budget constraints. Under very tight con-straints we could show that learning good prefix combina-tions can keep good ranking quality while skipping larger parts of the index lists. Here the effect to even tighter bud-gets will be examined in the future. A comparison to existing budget-aware top-k techniques, such as [3], will clarify if our proposed method is significantly better, while not requiring expensive bookkeeping at query execution time and addi-tional statistics. Further we plan to additionally integrate multiple features per term and multi-term features to raise the ranking quality and better compare to [5]. [1] Michael Bendersky, Donald Metzler, and W. Bruce [2] Ihab F. Ilyas et al. A survey of top-query processing [3] Michal Shmueli-Scheuer et al. Best-effort top-k query [4] Prabhakant Sinha and Andris A. Zoltners. The [5] Lidan Wang, Donald Metzler, and Jimmy Lin. Ranking [6] Justin Zobel and Alistair Moffat. Inverted files for text
