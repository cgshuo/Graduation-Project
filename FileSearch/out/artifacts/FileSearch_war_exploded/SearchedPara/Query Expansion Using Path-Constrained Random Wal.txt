 This paper exploits Web search l ogs for query expansion (QE) by presenting a new QE method based on path-constrained random walks (PCRW), where the search logs are represented as a labeled, directed graph, and the probabilit y of picking an expansion term for an input query is computed by a learned combination of con-strained random walks on the graph. The method is shown to be generic in that it covers most of the popular QE models as special cases and flexible in that it provides a principled mathematical framework in which a wide variet y of information useful for QE can be incorporated in a unified way. Evaluation is performed on the Web document ranking task usi ng a real-world data set. Re-sults show that the PCRW-based method is very effective for the expansion of rare queries, i.e. , low-frequency queries that are unseen in search logs, and that it outperforms signi ficantly other state-of-the-art QE methods. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval; I.2.6 [ Artificial Intelligence ]: Learning Algorithms, Experimentation Search Log, Query Expansion, Random Walk, Path Ranking Al-gorithm, Web Search Term mismatch is one of the fundamental challenges in Web search, where a query and its relevant documents are often com-posed using different vocabularies and language styles. Query expansion (QE) is an effective strategy to address the challenge. It expands a query issued by a user with additional related terms, called expansion terms , so that more releva nt documents can be retrieved. (IR). The methods based on automa tic relevance feedback (e.g., explicit feedback and pseudo relevance feedback (PRF)) have been proved to be useful for improving the performance of IR on TREC datasets [10, 11, 34, 36, 36, 45, 48]. However, these meth-ods cannot be applied directly to a commercial Web search engine because the relevant documents are not always available and gen-erating pseudo-relevant documents requires multi-phase retrieval, which is prohibitively expensive. logs (i.e., clickthr ough data) for QE [7, 14, 15, 19, 22, 41, 42]. These methods, called log-based QE , also derive expansion terms for a query from its (pseudo-)relevan t document set. But, different from the methods based on automatic relevance feedback, the relevant document set is identified by user clicks recorded in search logs. For example, the se t of (pseudo-)relevant documents of an input query can be formed by including the documents which have been previously clicked for the query or its similar queries [1, 7, 46]. Most state-of -the-art log-based QE methods use a global model that is pre-comput ed from search logs [14, 19]. The model captures the correlation between query terms and doc-uments terms and can be used to generate expansion terms for the input query on the fly. Despite th e effectiveness of log-based QE methods, they suffer from two problems. First is data sparseness. A large portion of queries have very few or no click in search logs, as stated by Zipf X  X  law. Second is the ambiguity of search intent. For example, a term correlation model may fail to distinguish the search intent of the query term  X  X ook X  in  X  X chool book X  from that in  X  X otel booking X . Although the pr oblem can be partially allevi-ated by using the improved corr elation models based on phrases and concepts [19], there are plenty of cases where the search in-tent can only be identified correctly via global context. For exam-ple, the query  X  X hy 6 bottles in one wrap X  is about package, and the intent of the query  X  X cme bake d bread X  is to look for the bak-ery in CA. In such cases, a (pseudo-)relevant document set of an input query, if available, is more likely to preserve the original search intent than any pre-computed global correlation model. log-based QE method based on path-constrained random walks [32]. We represent the search l ogs, consisting of billions of clicked query-document pairs, as a labeled, directed graph, where there are three types of nodes, representing respectively queries, documents, and words (i.e., candi date expansion terms), and the edges between nodes are labeled by relations. An example graph, which will be described in detail in Section 3.1, is shown in Fig-ure 1. For each path in the graph that links the input query  X  to a candidate expansion term  X  , there is a path type  X  , defined by a sequence of edge labels. Each path type can be viewed as a par-ticular process of generating  X  from  X  , and the generation proba-bility  X  X , X | X  X  X  is computed by random walks along the paths that instantiate the path type  X  , as known as path-constrained random walks (PCRW). Many log-based QE models proposed previously can be formulated in the framework of PCRW by de-fining particular path types. For example, the method based on relevance feedback, where th e pseudo-relevant documents  X  are defined as the ones that have clicks for the input query  X  or its similar queries  X  X  , can be presented using the following path type This is a three-step random walk. The first step retrieves similar queries by a random walk on edges labeled by the relation  X  X 2 X _ X  X  X  X  X  X  X  . The second follows any edges labeled by  X  X 2 X _ X  X  X  X  X  . The third follows any edges labeled by  X 2 X _ X  X  X  X  X  X  X  X  . These relations are summarized in Table 1, and will be described in detail in Section 3. modeling framework in that it no t only covers most of the popular log-based QE models as special cases , but also allows us to devise new QE models that can potentially use much richer information than that of previous models. Fo r example, we can define a rich set of walk behaviors that support a variety of labeled edges where different information can be used at different stages of the walk. Some examples will be presented in Section 3.2. ent sources and are potentially complimentary, it is desirable to combine them to address data sp arseness and help disambiguate search intent. For example, while the automatic feedback methods using (pseudo-)relevant documents ar e good to retain search intent but suffer from data sparseness es pecially for rare queries, the methods based on global term co rrelation models can be applied equally well to both common and rare queries but, due to the lim-ited context information it captures, may lead to an unexpected shift of search intent. We will show that PCRW provides a flexi-ble mathematical framework in which different QE features, spec-ified by path types  X  , can be incorporated in a unified way. For-mally, in the PCRW-based QE method the probability of picking  X  for a given  X  ,  X  X  X  X  X  X  , is computed by a learned combination of path-constrained random walks on the graph, i.e.,  X   X   X  |  X   X   X   X   X   X   X   X   X  |  X , X   X   X  X  X  X  , where  X   X   X  X  are the combination weights learned on training data. only makes QE robust to data spar seness but also help disambigu-ate search intents, leading to a significant improvement over pre-vious state-of-the-art QE methods. tion 3 describes in detail the PC RW-based QE method. Sections 4 and 5 present experiments and related work, respectively. The paper is concluded in Section 6. This section briefly reviews th e path-constrained random walk model. Readers are referred to [ 32] for a more detailed treatment. The model used in this study is a variant of the one described in [32, 33]. Modifications ar e made for the QE task.  X  X  X  is the set of labeled edges (also known as triples )  X  X  X , X , X  X  . Each triple represents an instance  X , X  X  X   X   X  of the relation  X  X  X  . For the QE task considered in this study it will be useful to intro-duce for each relation  X  a separate probabilistic model  X  of reaching  X  X  from  X  with a one-step random walk with edge type  X  ,  X , X | X  X  X  X  X   X   X  . In Section 3 we will see how the use of relation-specific models allows us to bui ld significantly more expressive QE models. the path type is sequence of nodes  X   X   X ,...,  X  such that  X  Each path type specifies a real-value feature. For a given node of the feature  X  is  X  X , X | X  X  X  , i.e., the probability of reaching  X  from  X  by a random walk that instantiates the path type, also known as a path-constrained random walk . Specifically, suppose that the random walk has just reached  X   X  by traversing edges la-beled  X   X   X ,...,  X  with  X  X  X   X  . Then  X   X  X  X  is drawn at random, ac-cording to  X   X  A path type  X  is active for pair  X  X , X  X  if  X   X   X  |  X , X   X   X 0 . greater than  X  that occur in the graph together with the dummy type  X  , which represents the bias feature. It is convenient to set  X   X   X  |  X , X   X   X 1 for any nodes  X  ,  X  . The score of whether target node  X  is related to source node  X  is given by where  X   X  is the weight of feature  X  . The model parameters to be learned are the vector  X  X  X   X   X   X   X   X  X  X  X  . The construction of  X  and the estimation of  X  are application specific. For the QE task source node is the input query to be expanded  X  and target node is a candidate expansion term  X  . Thus, Equation (1) gives the proba-bility of whether  X  is a good expansion term of  X  . This is the QE model we will describe in detail in Section 3. The search logs used in this st udy consist of a list of query-document pairs, also known as clickthrough data . Each pair con-tains a query and a document which has one or more user clicks for the query. We represent the search logs as a graph  X  X , X  X  X  X  X  , as shown in Figure 1. We define three types of nodes to represent respectively queries, documents, and words that occur in queries and documents. While a query in the search logs, denoted by  X  X  , always has clicked document(s), an input query to be expanded, denoted by  X  , could be a new, low-frequency query without clicked documents. Such a query is called a rare query in this paper.  X  and  X  X  are treated as different nodes in  X  . using a relation-specific model  X   X  . The edge score is the probabil-ity of reaching target node  X  from source node  X  with a one-step random walk with edge type  X  ,  X , X | X  X  X   X   X  . The set of relations  X  and their corresponding scoring functions  X  X  X  X  X   X  are used in this study, are summarized in Table 1. To ensure that edge score is a probability,  X , X | X  X  X   X   X  is computed via softmax as Note that in the original PCRW model [32] there is no  X   X  edge score is computed by where  X  X  X  X , X  X  X  X  X  is an indicator function that takes value 1 if there exists an edge with type  X  that connects  X  to  X  . Introducing  X  allows us to easily incorporate we ll-established mode ls that have been developed for QE and document ranking models in the IR community. The scoring functions in Table 1 lie in four categories. The first is the functions for the  X _ X  X  X  X  X  X  X  relation (e.g., #1), and is based on the BM25 model [39]. The second, including func-tions for the relations of  X _ X  X  X  X  X  X  X  X  (e.g., #5), uses unigram language models with Bayesian smoothing using Dirichlet priors [49]. The third, including functions for  X _ X  X  X  X  X  (e.g., #3), uses a click model [13]. The last category, including functions for  X _ X  X  X  X  X  X  X  X  X  X  X  (e.g., #6), uses translation models [5, 17, 19], where, if clickthrough data is av ailable for model training, the word translation probabilities  X   X  X  X  are estimated on query-document pairs by assuming that a query is parallel to the docu-ments clicked on for that query [17]. Given a graph, any path type  X  that starts with the input query node  X  and ends with a word node  X  defines a real-value feature, which can be viewed as a QE model (or QE feature). The feature  X   X   X  |  X , X   X  by PCRWs of type  X  . In what follows we illustrate the capability of the PCRW model using examples in Table 2. We focus our discussion on three categories of QE features: (1) TM features, which perform QE using translation models (i.e., the corresponding path types are specified by IDs from TM1 to TM5 in Table 2), (2) SQ features, which perform QE using similar queries (i.e., SQ1 to SQ6 ), and (3) RD features, which perform QE using (pseudo-)relevant documents (i.e., RD1 to RD10 ). tion models where term correlati ons are pre-computed using que-ry-document pairs extracted from clickthrough data [14, 19, 22]. Compared to the methods that are based on thesauri either com-piled manually [38] or derived from document collections [28], such log-based methods are superior in that the translation models explicitly capture the correlation between query terms and docu-ment terms. One example is the word translation model described in [19], which can be encoded by the path type TM1 ,  X   X 2 X _ X  X  X  X  X  X  X  X  X  X  X   X  . In case there is not (enough) clickthrough data for model training, Lafferty and Zhai [31] present a method using Markov chains, where the tr anslation probability between two words is computed by random walks on a document-word graph. The method can be encoded by the path types of TM2 and TM3 in Table 2. expansion of a rare query  X  is often performed by adding terms from common queries  X  X  which are similar to  X  [45]. The PCRW model achieves this by a random walk that instantiates the path type SQ1 ,  X   X  X 2 X _ X  X  X  X  X  X  X  X , X  X 2 X _ X  X  X  X  X  X  X   X  . [6, 21, 35] show that (more) similar queries can be retrieved by performing random walks on a query-document click graph. Thus, rare query expan-sion could be improved using a larg er set of similar queries identi-fied by repeatedly applying random walks following the edges with types  X 2 X _ X  X  X  X  X  and  X  X  X  X  X   X 2 X _ . SQ3 and SQ4 in Table 2 are two examples of such improved models. in the search logs can be formed by collecting all the documents that have clicks for that query. Thus, the relevance feedback QE method can be represented as e.g., RD8 , If the input query is a rare query, we can form the set of pseudo-relevant documents through its similar queries  X  X  that are in the search logs, e.g., RD1 , To conquer the data sparseness problem, more pseudo-relevant documents can be retrieved by performing random walks on a query-document click graph, such as RD4 and RD5 in Table 2. random walks are implemented as matrix multiplication. As an example, we consider the task of retrieving similar queries by repeatedly applying random walks following  X 2 X _ X  X  X  X  X  and  X 2 X _ X  X  X  X  X  . Let  X  be the number of query nodes in  X  and  X  be the number of document nodes. Let  X  be the  X  X  X  matrix with entries  X   X , X   X  X  X  X  X  X  X  X  , called query-document transition matrix , where the probability is calculated from clicks as in #3 in Table 1. Also, let  X  be the  X  X  X  matrix with entries  X   X , X  where the probability is calculated from clicks as in #4 in Table 1.  X  and  X  are called transition matrices . It is easy to see that using  X  X  X  X  X  we can compute the probabil ity of walking from an ini-tial query  X   X  to any other query  X  in  X 2 steps, and the corre-sponding probability, which is used to measure query-to-query similarity, is given by  X   X   X  |  X   X   X   X  X   X  and  X  are sparse, the matrix product  X  X  X  X  X  can be computed efficiently. As  X  increases,  X   X  quickly becomes dense and the powers cannot be computed efficiently. However, as  X  increases, the search intent shifts from the initial query, as the probability quickly spreads out over all queries. Thus, in our experiments we limit  X  to 1 and 2. For QE, we rewrite the PCRW model of Equation (1) as which is weighted linear combination of path features  X  in  X  . Thus, the PCRW model performs QE by ranking a set of com-bined paths, each for one pair of  X  and  X  (i.e., a candidate expan-sion term). This section presents the way  X  is constructed and the next two sections present the way parameters  X   X  are estimated. nentially with the increase of pa th length. To make the computa-tion feasible, in our experiments we set the maximum length to 7, and only consider a small set of re lations that are highly selective, as shown in Table 1. Given a path type  X  , due to the large number of nodes in  X  , even with a length limit, the total number of paths that instantiate  X  could be extremely large. For example, since a word could translate to any other word based on a smoothed trans-lation model, any node pair  X  X   X   X  X , would have a non-zero-score relation  X  X 2 X _ X  X  X  X  X  X  X  X  X  (#2 in Table 1), thus making the transi-tion matrix extremely dense. For efficiency, we keep the (multi-plication of) transition matrices sparse by retaining only top-1000 (partial) paths after each step of random walk. T he training data used for the estimation of parameters  X  Equation (3) is denoted as  X  X  X  X   X   X ,  X   X  X  X  , where  X   X  is a vector of all the  X  is  X  X  X   X   X |  X   X ,  X   X  , and  X   X  is a Boolean variable indicating whether  X  is a good expansion term for  X   X  . In our experiments  X  is gen-erated using a method similar to [10], which will be described below. consists of a set of que ries. Each query is associated with a set of documents. Each query-document pa ir has a relevant label. The effectiveness of a document ranking model  X  X , X  X  X  X  X  X  X  can be evaluated on the set. We determine whether a word  X  is a good expansion for a query  X  by examining whether expanding  X  with  X  leads to a better document rank ing result. Specifically, we use the following ranking model  X log X  X  X  X  X , X  X  X  X  X  X  X   X   X  |  X   X   X   X  X  X  X  X  X  X  X   X   X | X  X  X  X  X log  X   X  where  X  is the expansion term under consideration,  X  is its weight,  X  is a term in the original query  X  , and  X   X  document models, respectively. The query model  X  X  X  X  X  timated via MLE (maximum likelihood estimation) without smoothing as where  X  X ; X  X  X  X  is the number of times  X  occurs in  X  , and | X | is the query length. The document model, e.g.,  X | X  X  X   X   X  , is estimat-ed via MLE with Dirichlet smoothing as where  X  X ; X  X  X  X  is the number of times  X  occurs in  X  , | X | is the document length,  X  is the Dirichlet prior (set to 2000 in our exper-iments), and  X  X | X  X  X  is the probability of  X  on the collection  X  , estimated via MLE without smoothing. single term. It is used to label whether  X  is a good expansion term for  X  . To simplify the training data generation process, we assume that  X  acts on the query independently from other expansion terms, and each expansi on term is added into  X  with equal weight, i.e.,  X 0.01  X  or  X  X  X 0.01  X  . the relevance judgment set, a se t of candidate expansion terms  X  X   X   X  is formed by collecting all terms that occur in the documents that are paired with  X  but do not occur in  X  . Then  X  a good expansion term for  X  if it improves the effectiveness of ranking document when  X 0.01  X  and hurt the effectiveness when  X  X  X 0.01  X  .  X   X  is labeled as bad if it produces an opposite effect or produces si milar effect when  X 0.01  X  or  X  X  X 0.01  X  . Given training data  X  X  X  X   X   X ,  X   X  X  X  , the model parameters  X  X  X   X   X  can be optimized by maximizing the following objective [32]  X   X   X   X   X  X  X   X   X   X ; X , X   X  where  X   X  and  X   X  respectively control the strength of the L regularization, which helps with structure selection, and L regularization which helps prevent overfitting.  X   X   X ; X , X   X  is the log-likelihood of the training sample  X   X , X   X  , and is defined as  X   X   X ; X , X   X   X log X  X   X   X , X   X   X   X   X 1 X   X   X log X 1 X   X   X , X   X   X  (8) and is the model-predicted probabilit y. In our experiments the maxi-mization is performed using the OWL-QN algorithm [2], which is a special version of L-BFGS designed to deal with non-differentiable L 1 norm. type a weight. Such a parameterization is called one-weight-per-path-type . An alternative way of parameterizing the model is one-weight-per-edge-label [11, 37]. [32] argue th at the former is supe-rior in that it takes into acc ount the context in which a relation appears. In our experiments we compare these two parameteriza-tion options. Following [32], we use the same objective function and optimization procedure for the parameter estimation of the one-weight-per-edge-label model. Because the model can be seen as the combination of all the PCRWs with each path having its weight set to the product of all the edge weights along the path, we can calculate the gradient of edge weights by first calculating the gradient with respect to the paths, and then applying the chain rule of derivative. In this study the effectiveness of a QE method is evaluated by issuing a set of queries which ar e expanded using the method to a search engine and then measuring the Web search performance. Better QE methods are supposed to lead to better Web search results using the correspondi ngly expanded query set. duct experiments on standard test collections such as the TREC data because they do not contain related search logs we need. Therefore, following previous studies of log-based QE [e.g., 14, 19, 40], we used the proprietary da tasets that have been developed for building a commercial search engine, and demonstrated the effectiveness of our methods by comparing them against several state-of-the-art QE methods that are originally developed using TREC data [45, 34, 36]. For comparison, we also reproduced on our datasets the results of severa l previous state-of-the-art log-based QE methods [14, 19, 41]. English. On average, each query is associated with 33 Web docu-ments (URLs). Each query-document pair has a relevance label. The label is human generated and is on a 5-level relevance scale, 0 to 4, with 4 meaning document  X  is the most relevant to query  X  and 0 meaning  X  is not relevant to  X  . the rare queries are sampled from one day of search engine logs. Adult, spam, and bot querie s are all removed. To reflex a natural distribution of rare queries, we do not try to control the quality of these queries. We fou nd that in comparison with com-mon queries, rare queries are l onger and contain more spelling errors. For example, in our rare query set, the average query length is 5 (vs. 3-word for a common query set), and there are around 20% misspelled queries (vs. 12% for a common query set). Second, for each query, we collect Web documents to be judged by issuing the query to several popular search engines (e.g., Google, Bing) and fetching top-10 retrieval results from each. Finally, the query-document pairs are judged by a group of well-trained assessors. In this study all the queries are prepro-cessed as follows. The text is white-space tokenized and lower-cased, numbers are retained, and no stemming/inflection treatment is performed. Since all the docume nt ranking and QE models test-ed in our experiments contain free parameters that must be esti-mated empirically on data, we us ed two-fold cross validation to report results: a set of results on one half of the relevance judg-ment set is obtained using the pa rameter settings optimized on the other half, and global retrieval results are combined from those of the two sets. billion query-document pairs sampled from the search logs of a commercial search engine. The Web document collection consists of around 730 million Web pages. In the retrieval experiments we use the index based on the content fields (i.e., body and title text) of each Web page. malized Discounted Cumulative Gain (NDCG) [26]. We report NDCG scores at truncation levels 1, 3, and 10. We also per-formed a significance test, i.e., a t-test with a significance level of 0.05. A significant difference shoul d be read as significant at the 95% level. We constructed the graph  X  using the search logs described in Section 4.1.  X  consists of 730 million document nodes  X  , 1.8 billion query nodes  X  X  , and 100 million word nodes  X  . To repre-sent the rare queries in the re levant judgment set, we extend  X  by generated 12,000 input query nodes  X  , each for one rare query. The edges between nodes are labeled using the relations defined in Table 1. Since rare queries are unseen in search logs, the edges between  X  and  X  , as in Figure 1, have a zero score, and all path types that include zero-score edges are inactive, such as RD8 , RD9 and RD10 in Table 2. and the node of an input query  X  , we perform random walks in  X  following all possible paths that instantiate the path types defined in  X  . We then generate a list of candidate expansion term nodes  X  together with their scores  X  X  X  X  X  X  , as computed by Equation (3). We sort all the predictions  X  X , X  X  by the scores in descending order, and pick the top- X  words that are not in the input query for QE. In our experiments we set | X | X 10 X  X  , where | X | is the length of the input query. The terms in the expanded query are weighted using the method described in [45]. The weights of the terms in the original query are set to 2, and the weight of a new term is set to  X / X 1.0 X 0.9 X  , where  X  the rank of the term in the sorted list of top- X  candidates. to perform document ranking [49]. The model is defined as the second term on the right-hand-side of Equation (4). Table 3 summarizes the main results using different QE methods, evaluated on the relevance judgment set described in Section 4.1. input queries without expansion. Rows 2 to 6 are the QE methods proposed previously. For fair comparison, the number of expan-sion terms for a query  X  is set to | X | X 10 X  X  for all QE methods. relevance model [34]. LCA and RM are state-of-the-art PRF methods, developed respectively for the vector space and lan-guage modeling IR frameworks. LCE (Row 4) is latent concept expansion [36], which is a generalization of RM in that it explicit-ly models term dependencies for QE. Unfortunately, the generali-zation does not lead to any signi ficant improvement in our exper-iments (Row 4 vs. Rows 2 and 3). mentation of the term correlati on model [14]. We see that both TC and PRF methods improve the effectiveness of Web search significantly, and the log-base d method outperforms significantly the RPF methods that do not use query logs. The results confirm the conclusion of [14]. QE system. Following Riezler et al. [41], the system is an imple-mentation of a standard phrase-based SMT system with a set of features derived from a transla tion model and a language model, combined under the log-linear model framework [29]. To apply the system to QE, expansion terms of a query are taken from those terms in the 10-best translations of the query that have not been seen in the original query string. The results show that SMT is also effective (Row 6 vs. Row 1), outperforming significantly TC in NDCG at 3 and 10 (Row 6 vs. Row 5). This result is more or less consistent with what is reported in Riezler et al. [41], despite the difference in training data we used, (i.e., Riezler et al. used query-snippet pairs while we used query-title pairs). translation model [17, 19]. TM and TC models are trained on the same clickthrough data that consists of 3 billion query-title pairs. The result that TM outperforms TC confirms the conclusion of [19] that a translation model trained using the EM algorithm [8, 16] is better than a correlation model estimated purely based on frequency counting as in TC . have been frequently used for comparison in related studies. outperforms significantly the base line (Row 1) and the other QE methods we used for comparison (Rows 2 to 6). Since the PCRW model combines a wide variety of features, each encoded by a path type, it is instructive to investigate the QE performance of individual features and the impact of how these features are com-bined. Recall that in Section 3.2 we group path types into three catego-ries: (1) TM features, (2) SQ features, and (3) RD features. They generate expansion terms using di fferent data sources, and thus are expected to be complimentary. Table 4 presents QE results using individual features, where the best feature in each category is in bold and italic . Comparing the results of individual features with that of the PCRW model reveals that combining features significantly improves the QE performance. where different QE features give complimentary expansion terms and the combined achieves the best result. The query  X  X cme baked bread X  in Figure 2 is issued to search for the homepage of a bak-ery company in Berkeley, CA. The expanded query based on clickthrough-based translation model ( TM1 in Table 2) leads to worse document ranking results than that of NoQE because the model generates expansion terms from query terms in a word-by-word fashion, e.g., generating  X  X akery X  or  X  X read X  from  X  X aked X . But, without knowing that the entire query refers to an entity (i.e., company), it cannot generate e xpansion terms relating to the properties of the entity (e.g., the location of the company). How-ever, using features base d on similar queries ( SQ1 ) location names such as  X  X an francisco X  and  X  X erkeley X  are selected as ex-pansion terms. It is also encourag ing to see that its relevant docu-ment (www.acmebread.com) is ranked top in its pseudo-relevant document set obtained via similar queries ( RD1 ). two common terms  X  X aterfall X  and  X  X lass X . The search intent suggested by the two terms when they occur in the same query is very different from that when on ly one of them occurs. As ex-pected, the QE method using a word translation model ( TM1 ) fails to improve the search performance. Neither do the similar queries retrieved via random walks ( SQ1 and SQ3 ) provide very useful expansion terms since most of the sim ilar queries are simp-ly different permutations of the same set of terms. # QE Methods NDCG@1 NDCG@3 NDCG@10 1 NoQE 0.2648 0.2985 0.3905 2 LCA (PRF) 0.2742  X  0.3107  X  0.4075 3 RM (PRF) 0.2689  X  0.3077  X  0.4068 4 LCE 0.2695  X  0.3069  X  0.4098 5 TC 0.2811  X   X  0.3198  X   X  0.4132  X   X  6 SMT 0.2803  X   X  0.3230  X   X   X  0.4199 7 TM 0.2837  X   X  0.3212  X   X  0.4183 8 PCRW 0.2959  X   X   X  0.3302  X   X   X  0.4265 Table 3: Document ranking results using different QE meth-ods. The superscripts  X , X , and  X  indicate statistically signifi-cant improvements (  X  X  X . X  X  X  X  ) over NoQE, LCA, and TC, respectively. 
Figure 2: QE results of  X  X  X  acme baked bread. (a), (b) and (c) are top expansion terms and their scores  X  X  X | X  X  X  X  generated using features TM1, SQ1 and RD1, respectively; (d) are top similar queries generated using SQ1; (e) are top pseudo rele-vant documents generated using RD1. Features are defined in Table 2. 
Figure 3: QE results of  X  X  X  waterfall glass in dallas tx. (a), (b) and (c) are top expansion terms their scores  X  X  X | X  X  X  X  generated using features TM1, SQ1 and RD1, respectively; (d) are top similar queries generated using SQ1; (e) are top pseudo relevant documents generated using RD1. Features are defined in Table 2. Fortunately, as shown in Figure 3 (e ), these permuted queries lead to a set of clicked documents ( RD1 ) from which effective expan-sion terms are generated. tures, some of which have not been studied previously. TM1 , which is also reported in Row 7 in Table 1, is th e best among all TM features. Although the tran slation probabilities in TM2 , TM3 , TM4 and TM5 are estimated via random walks, rather than on query-document pairs as in TM1 , these models still improve the baseline NoQE , although not as effective as TM1 , thus providing an alternative way of obtaining translation models without train-ing data. to retrieve semantically similar queries for QE, and this can be achieved either by applying a translation model ( SQ2 vs. SQ1 ) or by applying random walks on query-document graph ( SQ3 vs. SQ1 ); and (2) taking a 2-step random walk is useful but taking longer steps is not (e.g., SQ3 vs. SQ1 and SQ4 ). use a translation model to retrieve similar queries ( RD2 vs. RD1 ) or to generate expansion term s from pseudo-relevant documents ( RD3 vs. RD1 ); and (2) random walks cannot significantly im-prove the quality of the pseudo-relevant document set (e.g., RD6 and RD7 vs. RD2 ). Table 5 compares the PCRW model parameterized using one-weight-per-path-type with two baselines. We see that (1) the trained models outperform the untrained model; and (2) one-weight-per-path-type is slightly better than one-weight-per-edge-label, but the difference is not statistically significant, except for NDCG score at 1, indicating that capturing context information between relations in a path is useful, although the impact on QE is marginal. Our work is a significant extens ion to the random walk models described in [11] in two aspect s. First, while we use a PCRW model, [11] uses a more traditional Markov chain model, similar to [23, 37, 43], where random walks are not constrained by path types and their models are parame terized as one-weight-per-edge-label. As discussed in Section 3.5, paths in  X  provide more useful features for QE than edges since the former captures more context information. Although it is difficult for us to perform a direct comparison between our model and the model in [11] due to the dramatically different data sources that these two models are based on respectively, th e result in Table 5 suggests that given the same search logs as thesauri, our model is likely to perform better. Second, while  X  in our model is constructed on search logs,  X  in [11] is constructed on thesauri that are compiled manually or de-rived from document collections. Our design decision of using search logs rather than pre-complied thesauri is motivated by those studies that show that log-based QE methods [e.g., 15, 19, 22, 42] often lead to a superior performance to the QE methods that use human-compiled thesauri [e .g., 24, 38] largely due to the fact that models trained on search logs explicitly capture the cor-relation between query terms and document terms, thus bridging the lexical gap between them more effectively. On the other hand, Web scale thesauri such as ConceptNet and Wikipedia have re-cently been explored for QE, leading to some promising results [30, 47]. The graph representatio n and the PCRW-based inference we proposed provide a flexible framework to incorporate such new thesauri. We leave it to future work. plied on other Web search applications, such image search [12], query suggestion [6, 35], query translation for cross-lingual IR [9], click model smoothing [21], and email search [37]. Our method bears some resemblance to all these previous works. gories: the automatic relevance f eedback methods [10, 11, 34, 36, 40, 45, 48] developed mainly on TREC data and the log-based methods [14, 15, 19, 22, 41, 42] where the correlation between query terms and document terms is learned from clickthrough data. Most of the features used in our PCRW model, as in Table 2, are inspired by these QE models. many Web search tasks. In addition to QE, they have also been used for document ranking [1, 20, 27], query processing and spelling correction [18, 25], user query clustering [3, 4, 44], etc. This paper exploits search logs for QE for Web search ranking. We present a QE method based on path-constrained random walks, where the search logs are represented as a labeled, directed graph, and the probability of selecting an expansion term for an input query is computed by a learned combination of constrained ran-dom walks on the graph. We show that our method is generic and flexible in that it not only repres ents most of popular QE models as features, but also allows us to easily devise new features, which can potentially use much richer information than previous QE models, by defining path types with a rich set of walk behaviors. The PCRW model also provides a principled mathematical framework in which different QE models, i.e., defined as path types or features, can be incorporated in a unified way, thus mak-ing it less susceptible to the spar seness issue of clickthrough data and ambiguous search intent of us er queries. The evaluation on a real-world data set shows that the PCRW-based method signifi-cantly outperforms other state-of-the-art QE methods. for Web document ranking directly . For example, we might model the relevance score of a query  X  and a document  X  as the proba-bility, computed by a learned combination of path-constrained random walks from  X  to  X  , where different document ranking models can be incorporated as pa th types. In addition to click-through data, we need to incorporate other data source to con-struct  X  , such as link graphs and the category structure of Web documents. [1] Agichtein, E., Brill, E., and Dumais, S. 2006. Improving web [2] Andrew, G., and Gao, J. 2007 . Scalable training of L1-[3] Baeza-Yates, R. 2007. Graphs fro m search engine queries. In [4] Baeza-Yates, R., and Tiberi, A. 2007. Extracting semantic [5] Berger, A., and Lafferty, J. 1999. Information retrieval as [6] Boldi, P., Bonchi, F., Castillo , C., Donato, D., and Vigna, S. [7] Broder, A., Ciccolo, P., Gabrilovich, E., Josifovski, V., [8] Brown, P. F., Della Pietra, S. A., Della Pietra, V. J., and 
Parameterization NDCG@1 NDCG@3 NDCG@10 untrained (  X   X  =1) 0.2910 0.3263 0.4233 one-weight-per-edge-label 0.2924 0.3277 0.4249 one-weight-per-path-type 0.2959 0.3302 0.4265 
Table 5: Comparison with different parameterization methods. [9] Cao, G., Gao, J., Ni e, J-Y., and Bai, J. 2007. Extending query [10] Cao, G., Nie, J-Y., Gao, J., a nd Robertson, S. 2008. Selecting [11] Collins-Thompson, K., and Call an, J. 2005. Query expansion [12] Craswell, N., and Szummer, M. 2007. Random walks on the [13] Craswell, N. Zoeter, O., Taylor, M. J., and Ramsey, B. 2008. [14] Cui, H., Wen, J-R., Nie, J-Y. and Ma, W-Y. 2002. [15] Cui, H., Wen, J-R., Nie, J-Y. and Ma, W-Y. 2003. Query [16] Dempster, A., Laird, N., and Rubin, D. 1977. Maximum [17] Gao, J., He, X., and Nie, J-Y. 2010. Clickthrough-based [18] Gao, J., Li, X., Micol, D., Quirk, C., and Sun, X. 2010. A [19] Gao, J., Nie, J-Y. 2012. Towards concept-based translation [20] Gao, J., Toutanova, K., an d Yih, W-T. 2011. Clickthrough-[21] Gao, J., Yuan, W., Li, X., Deng, K., and Nie, J-Y. 2009. [22] Gao, J., Xie, S., He, X., and Ali, A. 2012. Learning lexicon [23] Haveliwala, T. H. 2002. T opic-sensitive pagerank. In WWW , [24] Hovy, E., Gerber, L., Hermjakob, U. Junk, M., and Lin, C-Y. [25] Huang, J., Gao, J., Miao, J., Li, X., Wang, K., and Behr, F. [26] Jarvelin, K. and Kekalainen, J. 2000. IR evaluation methods [27] Joachims, T. 2002. Optimizing search engines using click-[28] Jing, Y., and Croft., B. 1994. An association thesaurus for [29] Koehn, P., Och, F., and Marcu, D. 2003. Stat istical phrase-[30] Kotov, A., and Zhai, C. 2012. Tapping into knowledge base [31] Lafferty, J., and Zhai, C. 2001. Document language models, [32] Lao, N., and Cohen, W. W. 201 0. Relational retrieval using a [33] Lao, N., Subramanya, A., Pere ira, F., Cohen, W. W. 2012. [34] Lavrenko, V., and Croft, B. 2001. Relevance-based language [35] Mei, Q., Zhou, D., and Church, K. 2008. Query suggestion [36] Metzler, D., and Croft, B. 2007. Latent concept expansion [37] Minkov, E., Cohen, W. W., and Ng, A. Y. 2006. Contextual [38] Prager, J., Chu-Carroll, J., and Czuba, K. 2001. Use of [39] Robertson, S., and Zaragoza, H. 2009. The probabilistic [40] Rocchio, J. 1971. Relevance feedback in information [41] Riezler, S., Liu, Y. and Vasserman, A. 2008. Translating [42] Riezler, S., and Liu, Y. 2010. Query rewriting using [43] Toutanova, K., Manning, C. D., and Ng, A. Y. 2004. [44] Wen, J., Nie, J-Y., and Zha ng, H. 2002. Query clustering [45] Xu, J., and Croft, B. 1996. Query expansion using local and [46] Xu, J., and Xu, G. 2011. Learning similarity function for rare [47] Xu, Y. Jones, G.J.F., and Wa ng, B. 2009. Query dependent [48] Zhai, C., and Lafferty, J. 2001a. Model-based feedback in the [49] Zhai, C., and Lafferty, J. 2001b. A study of smoothing 
