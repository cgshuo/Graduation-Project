 ORIGINAL PAPER Joost van Beusekom  X  Faisal Shafait  X  Thomas M. Breuel Abstract In this paper, an approach for forgery detection using text-line information is presented. In questioned doc-ument examination, text-line rotation and alignment can be important clues for detecting tampered documents. Measur-ing and detecting such mis-rotations and mis-alignments are a cumbersome task. Therefore, an automated approach for verification of documents based on these two text-line fea-tures is proposed in this paper. An in-depth evaluation of the proposed methods shows its usefulness in the context of document security with an area under the ROC curve (AUC) score of AUC = 0.89. The automatic nature of the approach allows the presented methods to be used in high-volume envi-ronments.
 Keywords Document security  X  Text-line alignment  X  Text-line orientation 1 Introduction and related work In every day life, document verification is an important task as many documents present a potential value. A typical exam-ple is bank notes. When handling bank notes, most people quickly control their genuineness by verifying easy to detect security signs as, e.g., holograms, structured print, and spe-cial inks. This is seamlessly done by many people as they have got used to the bank notes and their security features.
A person handling an unseen bank note will not know exactly what features to look for. This person might either just trust the source or look for features it might know from previously encountered bank notes: it might, e.g., look for a watermark or a metallic stripe inside the paper. Thus, even unknown bank notes can be checked for signs of forgery.
For less secured documents, a verification is only feasi-ble if the characteristic features of the document are known. Assume, e.g., the following scenario: a perpetrator has signed a contract that he wants to modify in order to gain advantages over the other contracting party. He therefore adds an addi-tional clause in some whitespace area of the document. This will have as effect that the security features as, e.g., the gen-uine signatures and the genuine paper are genuine. However, this type of forgery can detected by detecting verifying the consistency of the text-lines: small variations in the rotation and the alignment of the added text-lines compared to the previously printed, genuine text-lines can often be seen.
This forging approach is only one among others. Unfortu-nately, no public statistical data are available that shows how forgers generate their documents, especially for this scenario. However, text-line features are used in questioned document examination [ 1 ]. The authors also conducted a user study to find out what forgery methods are being used by people who want to defraud money from the insurance company. This user study showed that three different approaches are used:  X  Print, Paste and Copy (PPC) forgeries: out of 25 forger- X  Reverse Engineered Imitations (REI) Forgeries: three  X  Scan, Edit and Print (SEP) Forgeries: the remaining Details about this user study can be found in Appendix 6 . The PPC forgeries represent the class of forgeries that the proposed method can be applied too. In this work, we con-sider single column documents without tables. These char-acteristics can often be found in contracts. Also, only the manipulation of entire text-lines is considered. Tampering single characters or digits is most likely to be done by other means, as e.g., done for the SEP forgeries class.

To the authors X  best knowledge, there is no related work trying to automate text-line alignment and orientation mea-surement for forgery detection. Different approaches from optical document security exist. However, these cannot be used in the presented use case: as normal documents do not contain any extra security features, so called intrinsic features have to be used. Intrinsic features are features that are inte-grated into the printout by the normal document generation process, in contrast to extrinsic features that are added solely for the task of securing a document. This excludes the use of most of the features used in the optical document security domain, as e.g., watermarks, holographic images [ 2 ], special-ized printing techniques [ 3 ], and other physical and chemical signatures [ 4 ]. Many other types of extrinsic features can be found in literature [ 5  X  7 ].

However, this is not the first attempt to use intrinsic doc-ument features for document security applications. Several uses of intrinsic features have been presented in previous publications: printer identification, the process of assigning a printout to a unique printer or a printer type, has been inten-sively studied by different groups. Mikkilineni et al. [ 8  X  10 ] present a set of gray scale features that is used to determine the type of laser printer that was used to print a document. Schreyer et al. [ 11  X  13 ] worked on detecting the printing tech-nique used to print a document. Also, classification between printed and copied documents has been analyzed. Using dis-crete cosine transform features, good performance could be shown even when scanning with relatively moderate resolu-tions of 400 dpi.

The remaining sections of the paper are organized as follows: the automatic method for verification of printed documents using text-line skew and alignment measures is presented. The features as well as the statistical models to perform the plausibility check are presented in Sect. 2 .Eval-uation and results are presented in Sects. 3 and 4 . The paper concludes with Sect. 5 . 2 Text-line features for document security For checking the plausibility of a document, the following two features are used: the skew angle of text-lines and the alignment of text-lines. Other features exist that could be used for detecting anomalies in text-lines as, e.g., the text-line and word spacing. Measurements on a set of pages from the same document have however shown that the variation of these features in genuine documents is too high, and that their expected discriminative power will be very poor.
Measurement of the text-line X  X  skew angle is straightfor-ward using the method presented by Breuel [ 14 ]. The reason for using this text-line extraction algorithm lies first of all in the fact that it has proven good results in the task of page segmentation, as shown by Shafait et al. [ 15 ]. The second main advantage is that an open source implementation of the algorithm is available in the OCRopus 1 OCR system. For the sake of completeness, a short overview of the text-line extraction method is given in Sect. 2.1 .

To measure the alignment of a text-line, the following approach is used: first, the left and right alignment lines are computed (Sect. 2.3.1 ). These lines are defined as the left and right margin lines where justified, left-and right-aligned text start or end on. A visualization of the alignment lines is depicted in Fig. 1 .

After having extracted the alignment lines, the distance between the start and end point of a text-line to the respec-tive alignment line is computed. These two distances are used as features to perform a plausibility check.

In Sect. 2.2 , the use of the skew angles of text-lines for the proposed application is explained. Section 2.3 explains the details of the method using the alignment of text-lines for plausibility checks of documents. In Sect. 2.4 , both features are combined into one framework. 2.1 Text-line extraction Breuel proposed a parameterized model for a text-line with parameters ( r , X , d ) , where r is the distance of the baseline from the origin,  X  is the angle of the baseline from the hor-izontal axis, and d is the distance of the line of descenders from the baseline. The text-line extraction consists in find-ing parameter triples that maximize a quality function that is highest when all reference points are positioned exactly on the line. This is done using a branch-and-bound search. In the first step, the parameter space P is defined: P =[ r
The branch-and-bound algorithm then works as follows: 0: initialize the search space and insert it into priority queue 1: stop if Q is empty, else get the top search space S from 2: if S is a solution: save S and discard all image points con-3: split S into two subspaces S 1 and S 2 . 4: compute upper bounds for the quality of S 1 and S 2 5: put S 1 and S 2 on Q . Continue with Step 1.

The computation of the upper bound for the quality of the text-line defined by the parameter subspace is done using interval arithmetic on the reference points. The set of refer-ence points { x 1 , x 2 ,..., x n } is obtained by taking the middle of the bottom line of the bounding boxes of the connected components in a document image.
A priority queue is used to keep track of the subspaces that need to be analyzed. The number of lines to be extracted is used in Step 6 to define the stopping criterion. A space is considered as a solution if it is small enough to identify the unique line in the image. The parameters of the solution represent the detected line.

This algorithm guarantees an optimal solution and is thus a good candidate for the proposed task. The necessary accuracy of the estimated rotation angle can be obtained by changing the stopping criterion to only accept solution regions that are very narrow for the angle dimension. The skew angle of the text-line is then defined as the center of the angle interval of the solution region. 2.2 Plausibility check using skew angles Using the skew angle of text-lines for identifying suspicious documents is a well-known technique in questioned docu-ment examination [ 1 ]. Questioned document examiners do this step mostly manually using standard image manipula-tion software. In this section, the first approach to automate this process is presented.

The main idea of the process is sketched in Fig. 2 .The binarized document is correctly oriented and deskewed using the method presented in [ 16 ]. Next, the text-line skew angles are extracted. These are checked if they are within the  X  X atu-ral X  variation of text-line skew. Then, the text-line is consid-ered as valid. Else, the line is reported as an  X  X mplausible X  line.

In the next section, the statistical modeling of the skew angle variations and its application to the plausibility check are explained. 2.2.1 Text-line skew variation model In order to define what  X  X ormal X  and  X  X bnormal X  skew angles are, the natural variation of text-line skew angles has to be measured. Although in the electronic representation, all text-lines are exactly parallel, the transfer to the paper medium and again back to an electronic image format adds text-line skew variations that have to be considered before being able to make a decision on the skew angle:  X  printing: variations introduced by printing can be speck- X  digitization: digitization of a paper-based document  X  typographic enhancements: optical correction is often
All these effects can lead to text-line skew variations that are frequently observed and that should thus not be consid-ered as  X  X bnormal X .
 In Fig. 4 , a histogram of measured skew angles is shown. It can be seen that the distribution is peaked around 0  X  variation is quite low, and that its shape should be reasonably well approximated by a Gaussian distribution with parame-ters N ( X   X  , X   X  ) being the mean and the standard deviation of the skew angle  X  .

Using this model, a simple threshold-based method to decide whether a text-line skew angle is suspicious or not could be applied: using the confidence intervals, the thresh-old could be set to  X  3  X   X   X  , so every skew angle outside the 99 . 7% confidence interval will be reported.

This approach has two drawbacks:  X  no prior: due to the missing prior, forged text-lines are  X  text-line length: for longer text-lines, the skew angle can
To solve the first problem, a Bayesian formulation of the problem is proposed: P ( f |  X ) = p where P ( f |  X ) is the posterior of having a forged text-line given the observation of its skew angle  X , p ( X  | f ) is the like-lihood of observing the skew angle  X  knowing it is a forged text-line. P ( f ) is the prior for observing a forged text-line, and p ( X ) is the probability of observing the skew angle  X  (for forged as well as for original text-lines).

In the ideal case, statistics on the parameters could be used to extract reliable estimates. This, however, is only possible for genuine documents, as in a practical setup, no training data for forged documents are available. Even worse, to the authors best knowledge, there is no public dataset with forged documents that could be used to extract the necessary infor-mation. For original documents, the information can partially be computed. Thus, the posterior of having a genuine text-line given the observed skew angle  X  is written as: P (  X  f |  X ) = p In this case, p ( X  | X  f ) is estimated from training data consist-ing of genuine documents. As mentioned above, it is modeled as a Gaussian distribution N ( X   X  , X   X  ) . P (  X  f ) is considered as a sensitivity parameter of the system that can be tuned by the operator according to his needs. Still, p ( X ) is not known due to the missing data from forged documents. The infor-mation of interest is which of both posteriors are higher and thus, the normalizing factor can be ignored. Consequently, a text-line is classified as a forged one if:  X  P ( f |  X ) &gt;  X  P (  X  f |  X ) (4) where  X  P (  X  f |  X ) = p ( X  | X  f )  X  P (  X  f ) and  X  P ( f p ( X  | f )  X  P ( f ) .

As no measurements can be taken on the skew angles of forged documents, an assumption of the likelihood term has to be made: the assumption is that the observed skew angles for forged text-lines can be well modeled using a uniform distribution in a symmetric interval around the mean of 0 As a forging person tries to obtain perfect rotation angle, expecting the mean to be 0  X  is a reasonable choice. High rotation angles that can easily be detected with the bare eye are not to be expected, as in this case, the forging person will most likely try to generate a better forgery. Therefore, a uniform distribution is more representative of rotation angles for forged text-lines than a normal distribution.

To solve the second problem concerning the measurement accuracy for lines of different length, the following solution is adopted: instead of estimating the parameters  X   X  and  X  over all the text-lines, the parameters are estimated for a certain text-line length interval. Ideally, the interval would have the size of one pixel. This would require very large amounts of data to obtain reasonable estimates of the parameters. Instead, awindowofsize2  X  W is defined for which the parameters for a given line length l are computed around in the interval [ l  X  W , l + W ] . The size of the interval has to be fixed by the operator. In principal, it is best to have small-sized intervals. But this is only possible if enough training data are available. If only few samples are available, a higher value has to be chosen in order to have robust model parameter estimates. Finally, the two terms that need to be compared are: P (  X  f |  X ) = and P ( f |  X ) = where l is the text-line length,  X   X , l , X   X , l the estimated mean angle and standard deviation for the text-lines with length [ l  X  W , l + W ] , and where  X  w  X , bound of the interval uniform distribution for the forged skew angles. Ignoring the normalizing factor, a text-line is classi-fied as forged if U ( X ,  X  w  X , or as genuine in the other case. 2.3 Plausibility check using text-line alignment The examination of the alignment property of text-lines has also been previously used in questioned document exami-nation [ 1 ]. However, prior to this work, there is no method to evaluate this feature automatically. The main idea of the automated process is sketched in Fig. 5 .

In the first step, the text-lines are extracted from the binarized document image. Then, the left and right align-ment lines are computed. These are finally used to examine whether the text-lines are normally aligned or not. In the next section, the alignment detection is explained. 2.3.1 Alignment line computation After the text-line extraction, the alignment lines have to be detected. Four different alignment types are commonly dis-tinguished in typesetting:  X  left aligned: text-lines start at the left margin  X  right aligned: text-lines end at the right margin  X  justified: text-lines start at the left margin and end at the  X  centered: text-lines do neither touch the right nor the left
In practical document security applications, centered lines are unlikely to be forged, as they are not frequent and they do usually not contain any valuable information. Therefore, the focus in this work lies on the left and right alignment of the text-lines.

In order to find the alignment lines, the start and the end points of the text-lines have to be analyzed. The left align-ment line is defined as a vertical line where left-aligned and justified text-lines have their starting point. The right align-ment line is analogously defined as the vertical line where right-aligned and justified text-lines have their end point. Finding these lines is done using RAST line finding [ 18 ], The polar representation  X  = ( | is the L2-norm of the vector that is normal to the line and that points to the origin, and  X  is the rotation angle of
Consider feature points { x 1 , x 2 ,..., x n } : the quality func-tion that is being optimized to return the best alignment lines is written as:  X   X  := arg max where Q and q to the line defined by parameters ( |
The line finding is run twice, each time with different fea-ture points: once with the starting points and once with the end points of text-lines. The left and right alignment lines are considered as the respective resulting highest quality lines, thus the lines where most start or end points of text-lines lie on. 2.3.2 Text-line alignment model The alignment feature of a text-line is measured by the dis-tance of the text-line X  X  start and end point to the corresponding alignment line. A visualization can be found in Fig. 6 .For each text-line, two different distances are computed. To sim-plify the explanation, in the following, only the left alignment distance is considered. The modeling and computation of the right alignment distance are done analogously.

Again, just as for the text-line skew angle variance mod-eling, there is the problem of missing training data for forged documents. Also, the modeling of genuine documents fea-tures is not as straightforward as in case of the skew angle-based method: for the skew angle, it is clear that the angles to be encountered are close to 0  X  . For the alignment distances, this is not true. Indented text-lines, e.g., will lead to measure-ments that are significantly off from the normal variations for left-aligned text-lines and these should not be considered as potentially forged lines.

Therefore, instead of assuming a Gaussian distribution as done for the skew angles, the distribution of the alignment distances is measured from a training set of genuine text-lines. A Bayesian classification approach is chosen: P (  X  f | d where p ( d l | X  f ) is the likelihood of observing the distance d of the left alignment line to the left end of the text-line d given the information that the text-line is original. As dis-cussed above, assuming a Gaussian distribution would be a bad choice. Therefore, the distribution is measured on a training set.
 For the forged case this becomes: P ( f | d
The problem to be solved is how to model the likeli-assumption is that the variability around the correct align-ment distance is higher for forged lines than for original ones. However, this distribution is not Gaussian, as, just as for gen-uine text-lines, forged text-lines may start or end somewhere further away from the corresponding alignment line. The distances are therefore modeled as a mixture of a Gaussian and a uniform distribution: P ( d l | f ) = P ( j )  X  p ( d l | f , j ) + ( 1  X  P ( j ))  X  p ( d l | f , j ) the likelihood of observing distance d l given the fact that the text-line is forged and justified, and p ( d the likelihood of observing distance d l given the fact that the text-line is forged and not justified.

As argued before, p ( d l | f , j ) is modeled as being normally distributed N ( X  d l , d l ) . For the observed distance in a case of a non-justified text-line, a uniform distribution is assumed U ( d , 0 , L max ) where L max is the longest expected text-line. expected to be the same for forged and genuine text-lines.
So far, the modeling of one of the two alignment distances has been explained. Next, these two measures have to be com-bined into one common framework. Two different measures are generated for each text-line: d l and d r , representing the distances of the left and right text-line point to the left and right alignment line respectively. In the end, one decision is needed. In order to combine these three features into one decision, the presented model is extended: P (  X  f | d and P ( f | d where d l and d r are the distances of the left and right text-line point to the left and right alignment line, respectively, p ( d l , d r | X  f ) is the likelihood of observing d l , d r the text-line is not forged and p ( d l , d r | f ) is likelihood of observing the aforementioned distances knowing that the text-line is forged.
 For classification, the normalization term is discarded. Assuming independence of d l and d r , the two terms are rewritten as:  X  P (  X  f | d and  X  P ( f | d where now p ( d l | X  f ) and p ( d r | X  f ) are measured from a training set and p ( d l | f ) and p ( d r | f ) are approximated by the mixture distribution described above. The prior is used as a sensitivity parameter to tune the system. 2.4 Integration of skew and alignment In the previous two sections, the skew angle and the align-ment features were considered separately for detecting implausible text-lines. In this section, the combination of both features into a combined framework is presented.
Two different combination schemes have been tested: the first scheme is a simple voting based on the two separate classification results: a text-line is classified as forged if one of the two classifiers presented in the previous two sections  X  X ires. X  This is a restrictive approach that should prevent that many forged text-lines are missed.

The second combination scheme extends the statistical model presented in the previous section to include the text-line skew angle  X  and the alignment distances d l and d r P (  X  f |  X , d and P ( f |  X , d
The normalization factor is discarded and independence of the observations is assumed, leading to the following sim-plification:  X  P (  X  f |  X , d and  X  P ( f |  X , d Thus, a text-line is classified as forged if  X  P ( f |  X ,  X  P (  X  f |  X , d 3 Evaluation Evaluation of the proposed approaches showed to be a chal-lenging task. On the one hand, no public real-world dataset could be found to do a meaningful evaluation on. On the other hand, apart from the observations made during the forgery experiment in Sect. 6 , no statistics could be found on the methods used by amateur document forgers. For these rea-sons, new datasets had to be generated.  X  X he Two-pass Print 300 dpi (TP300) dataset contains 43  X  X he Print, Paste and Copy 300 dpi (PPC300) data- X  X he Two-pass Print LaserJet (TPLJ) dataset contains  X  X he Two-pass Print Color LaserJet (TPCLJ) dataset  X  X he Originals dataset contains 30 document images that  X  X he Distorted Text-Lines (DTL) consists of three main This dataset is used to analyze the effect of varying fonts, font sizes, rotation angles, and line lengths on the pro-posed method. The dataset was generated automatically using python to generate scalable vector graphic (SVG) files that were then converted into PDF files using the open source software inkscape . Ground truth, consisting of a map show-ing the area containing the distorted text-lines was also auto-matically generated in the same process. The dataset has been made public and can be downloaded 2 .

All images have been binarized and deskewed before fur-ther processing. The deskewing method from [ 16 ] has been modified to use the median angle instead of the angle of the text-line with highest quality, to avoid deskewing the page by an angle of a forged text-line. It is reasonable to assume that for a given page, the number of forged text-lines is lower than the number of genuine text-lines. In the first part (Sect. 4.1 ), it will be shown that the preprocessing of the document (scan-ning, binarization, and deskewing) is stable and that it is thus likely that the distortions present in the measurements have been introduced by the forging procedure. The hypothesis about the distributions of the features for original text-lines are also verified. Therefore, the text-line alignment distances and the skew angles have been extracted automatically from the Originals dataset.

The second part (Sect. 4.2 ) consists of evaluating the skew angle of text-lines for the plausibility check. The tests were run on all four test sets. For the TP300 and the PPC300 data-set, an extensive evaluation on the influence of both param-eters  X  d  X   X  X  0 . 5 , 8 . 0 ] and P ( f )  X  X  0 . 0 , 1 . The method has also been evaluated on the TPLJ and TPCLJ dataset.

The third test setup (Sect. 4.3 ) repeats the tests from the second setup for the alignment feature. The parameters of interest are  X  d  X  X  1 . 0 , 30 . 0 ] and P ( f )  X  X  0 . 0 tests, the prior was sampled in with higher density (step size 0.005) in the start ( [ 0 . 0 , 0 . 1 ] ) and end ( [ 0 . the interval and with a step size of 0.02 for the middle part of the interval ( ] 0 . 1 , 0 . 9 [ ).

The fourth part (Sect. 4.4 ) deals with the evaluation of the combination of both features into a combined frame-work by using reasonable parameter settings found in the iso-lated case. Both methods of classifier combination have been tested. For the statistical approach, the parameters  X   X  = and  X  d = 20 . 0 were fixed according to the settings giving good performance from the previous experiments and sole the parameter P ( f ) was varied. Tests were done on all four test datasets.

For the evaluation of the alignment distances, centered text-lines are being ignored as they do not fit the proposed model. In the case of the combination of both approaches, if a centered text-line is encountered, only the skew angle is considered to make a decision. Each line that does not start or end within a fixed threshold of six pixels from an alignment line and that has its middle point on the center alignment line is being considered as a centered text-line.

As evaluation measures, the receiver operating character-istic (ROC) [ 19 ] curves and the area under the ROC curve (AUC) are used.

The last evaluation focusses on the influence of differ-ent fonts, font sizes, and line lengths on the forgery detection performance. For these tests, the DTL dataset was generated. As a large amount of data is needed for these tests, synthetic data were chosen. Also, the parameter grid was coarser to keep the computational effort reasonable. For the prior, val-ues { 0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0 } were tested. Values for sigma were set up { 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0 } in the case of the rota-tion measurement and { 1.0, 2.0, 3.0, 5.0, 10.0, 20.0, 30.0 the case of the alignment measure. The results of these tests are given in Sect. 4.5 .
 Finally, the error analysis and discussion are presented in Sect. 4.6 . 4 Results In the following section, the results of the different evalua-tion steps are presented. Hypothesis validation is presented in Sect. 4.1 . Evaluation of the different features and the com-bination of both features are given in Sects. 4.2 , 4.3 , and 4.4 . 4.1 Hypothesis validation In this part, the hypothesis concerning the distributions of the skew and alignment text-line features are analyzed. The fea-tures were extracted on the Originals dataset containing 30 deskewed and genuine document images. The histogram of the extracted text-line skew angles is shown in Fig. 7 .Itcan be seen that the vast majority of the extracted skew angles lies around 0  X  .

The histograms of the left and right alignment distances are shown in Fig. 8 . As expected, the majority of the text-lines start at the left and end at the right alignment line. Also, peaks coming from indented text-lines are observed. It becomes clear that a frequency measurement-based statistics for com-puting the likelihood p ( d l | r | X  f ) is a reasonable choice.
A last test was done to test whether the variations that are being measured do not come from the scanning or the pre-processing steps. Therefore, a small sample set of 21 doc-ument pages has been printed once. These pages have been repeatedly scanned, in total 15 times on the same scanner. For each run, the preprocessing pipeline has been run to obtain deskewed images. Also, the mean and standard deviation of the text-line skew angles and alignment distances have been measured. The variance of these parameters over all runs has been measured. Results can be found in Table 1 . It can be seen that the variation between the different runs is quite low. It can thus be concluded that the preprocessing steps are stable and that the variations measured in the test datasets are likely to be induced by document generation process rather than by the digitization process. 4.2 Results on text-line skew angles Results on the TP300 and the PPC300 dataset are presented in Fig. 9 . It can be seen that in both cases, the best results in terms of area under the ROC curve (AUC) is found for the parameter  X   X  = 2 . 5. Also, small values of  X   X  give bad per-formance. This is due to the high number of false positives (original text-line detected as a forged one), as the threshold for identifying a skew angle as suspicious is really low. Vice versa, high values of  X   X  &gt; 3 lead to a high number of false negatives (forged text-lines detected as original ones). It can also be seen that for these high values, there are practically no points in the center and end part of the ROC curve, leading to the conclusion that for these parameters settings, the prior influences the outcome only for extreme values.

It can also be seen that the performance on the two-pass printing forgeries is slightly lower compared to the manual forgeries. This goes along with the observation that the skew angle variations on the manually forged text-lines are much higher than for the two-pass printed text-lines.

The results on the TPLJ and TPCLJ dataset are shown in Fig. 10 . The overall performance is comparable to the performance obtained for the TP300 dataset. Also, the characteristic gap of graph points in the middle of the curve is observed. 4.3 Results on text-line alignment The results on the TP300 and the PPC300 dataset can be found in Fig. 11 . The figures show different ROC curves for different values of  X  d and a varying prior P ( f )  X  X  0 . For reasonable values of  X  d , the performance is good. For values of  X  d &gt; 10, the performance does not improve sig-nificantly anymore.

In contrast to the previous results on the skew angle-based approach, the results on the TP300 dataset are better than for the PPC300 dataset. This is likely due to the increased amount of control over the alignment in the manual forgery process, than in the case of the two-pass print forgeries, where the alignment is only partially controllable by adjusting the doc-ument page in the paper tray accordingly.

The results on the TPLJ and TPCLJ dataset can be found in Fig. 12 . Again, the performance is better than on the man-ual forgeries, although slight differences between the two printers can be observed. 4.4 Results on the combination of both features The results for the combination using the voting scheme that classifies a text-line as forged if at least one of the two sepa-rate classifiers does so can be found in Fig. 13 a. It can be seen that in three of the four cases, the combination outperforms using the AUC as performance measure.

For the test on the combination using the statistical model, the parameters  X   X  and  X  d where fixed with values that proved to perform well in the previous tests:  X   X  = 2 . 5 and  X  d 20 . 0. The results can be found in Fig. 13 b.

In comparison with the results of the single features, using the AUC as a measure, the combination of the two features does not significantly change the performance. One can also see that for some datasets, the combination slightly outper-forms the best single feature (PPC300, TPCLJ) but for the other two cases, using only the alignment would have given a slightly better result in terms of AUC. 4.5 Results on the DTL dataset The results for the orientation test on the DLT dataset can be found in Fig. 14 . The figures show boxplots of the accuracy for different fonts (Fig. 14 a), font sizes (Fig. 14 b), rotation angles (Fig. 14 c), and line lengths (Fig. 14 d) for all differ-ent parameter sets. The distribution of the line lengths in the dataset is shown in Fig. 15 d.

It can be seen that the accuracy neither depends on the font nor on the font size. Furthermore, smaller angles of devia-tion lead to lower recognition rates as can be seen in Fig. 14 c. Also, shorter text-lines tend to have lower accuracy, which is most likely due to the fact that short text-lines cannot be measured as accurately as long ones.

The results of the test for the text-line alignment measure-ment can be found in Fig. 15 . The boxplot (Fig. 15 a) of the accuracy compared to the font (for all different parameter pairs from the parameter sets) shows that for most fonts, the performance of the method is similar. It can thus be concluded that the method works well on many widely used fonts.
Concerning the performance in relation to the mis-align-ment, it can be seen in Fig. 15 c that for small mis-alignments, the accuracy is higher. This is due to the higher true positive rate, whereas true negative rate stays relatively constant. This result coincides with our model definition that assumes that high mis-alignments are less likely to come from the forging process than small mis-alignments.

Finally, in Fig. 15 b, it can be noted that the accuracy drops with increasing font size. In this case, this is due to the decreasing true negative rate: one possible expla-nation could be that with increasing font sizes, the natu-ral mis-alignments due to optical corrections increase and thus many unforged lines are classified as potentially forged ones. 4.6 Error analysis and discussion An analysis of the errors made by the proposed text-line skew examination gave the following results:  X  line splits: for the manually generated forgeries of the  X  errors in deskewing: in rare cases, when more forged  X  limited discriminative power of the feature: in several
The first two error types reduce the accuracy of the pro-posed method in the current evaluation setup, but in a real-world scenario, these problems would be less important, as in both cases suspicious lines would be reported to the human operator who can easily see that there are issues related to that document page. For the last problem, no solution can be found relying on the skew angle alone. Other features have to be added to detect these forgeries.

An inspection of the errors made by the system in the examination of the alignment feature showed the following reasons for failure:  X  typographic enhancement: problems, as e.g. italics at  X  wrong alignment line: when more indented text-lines  X  errors in text-line finding: in rare cases text-lines extend  X  indented text-lines: text-lines from enumerations are  X  limited discriminative power of the feature: in some
The problem with the optical corrections and with enu-merations could be solved by integrating optical character recognition information to the process. Text-lines starting ( a ) with numbers, italic characters or ending with characters pre-senting these problems could be treated differently, e.g., by allowing more variation for these text-lines.

The problem of extracting the wrong alignment line could be encountered by extending the method to not only extract one alignment line but to extract more and compare them pairwise by computing their distance. This feature could then be used to detect suspicious alignment line constellations.
Taking in consideration all the results so far, one can say that for simple documents, the proposed method could be applied on real-world documents, if such a dataset would become available. Several limitations, however, do exist: first of all, only simple document layouts have been used so far. Principally, application to more complex documents is also possible. The main challenge in this case is to extract the correct text-lines. This is a hard problem considering, e.g., multi-column and tabular content.

Another drawback is that the method can only detect forg-eries if there is a mis-orientation or mis-alignment present. If the forgery method does not produce any of these distortions ( a ) cannot be used to distinguish between forged and genuine documents.

It should be noted that modifications of single charac-ters or words cannot be detected with this approach. If the manipulated words are long enough, there is a chance that these could be detected. However, this is not the focus of this method. Modifications of single words or characters are more likely to appear in SEP -type forgeries. To detect these type of forgeries, other methods have to be applied as, e.g., [ 20 ]. 5 Conclusion and future work In this paper, the first approach for automatic plausibility checks for forgery detection on printed documents has been presented. The text-line skew angle and alignment features have been integrated into a statistical framework for auto-matically detecting implausible skew angles or alignment distances. Extensive evaluation of the proposed methods on different datasets has been done to show the usefulness of the approach. (b) ( a ) (c)
An important aspect for future work is the extension of the evaluation using real-world datasets or at least datasets that have been generated by a wide variety of different persons and methods, in order measure the impact of the method in the different forging scenarios.

The overall goal of this and related work is to obtain a set of methods verifying different aspects of documents in order to find forgeries. The combination of these can hope-fully help to reduce the error rate, especially the false positive rates, as these are often the most costly errors in high-volume applications. 6 Appendix: methods for generating forgeries One major problem in the development of techniques for forgery detection is that it is not known how the common person would forge a document. Also, no public statistical data are available giving an insight into how people forge documents. It is reasonable to assume that he will use stan-dard hard-and software to solve this task, but even with this restriction, there are many possible scenarios leading to var-ious kinds of defects.

In an effort to get an overview on how people would forge documents using standard hard-and software, a small exper-iment was set up. In this experiment, people were given two invoices and the following description:
Have a look at the invoice and consider the follow-ing scenario: a person with criminal energy, Frau Gaby
Musterfrau, cannot have enough money and thinks of defrauding money from her insurance company. She recently had a technical problem with the car that is covered by her car insurance company. The idea she came up with is to somehow forge the original invoice ( a ) (b) (c) ( d ) and increase the total amount of money stated in the invoice. She will then send the invoice to the insurance company and get more money back than she actually paid to have the car repaired.

Frau Gaby Musterfrau knows that the insurance com-pany does not check the invoice with the repair shop.
But that when something looks suspicious, the insur-ance company will do so. In that case, Gaby Musterfrau would be in trouble because she knows it is illegal to defraud the insurance company.

Your Task: Play the role of Frau Gaby Musterfrau and forge the invoice with the goal of increasing the total amount of money.

The candidates were not given any information about how they should or could forge the document. As the candidates did not perform the forgery under supervision, they were asked not to discuss any ideas with their colleagues in order to avoid biasing the results into one direction. However, it could not be verified if the candidates sticked to this request.
The invoices that they were given were generated by the author using a word processing software. Genuine bills from previous car repairs were taken as a basis in order to have a realistic layout and to avoid copyright issues in case of pub-lishing the dataset. Figure 18 shows the two genuine invoices. The  X  X nid X  invoice was printed on normal white paper. The  X  X FKI X  invoice was printed on old stationary from the DFKI with a logo on top and business-related information at the bottom right part of the page.

As the scenario implies that the test candidates are given a paper version of the invoice and hand back a forged paper document, diffusing the task by e-mail was not pos-sible. Therefore, only a limited number of people could be asked for participation. Approximately 40 sample sets (cover letter, DFKI invoice and Anid invoice) have been distributed. A small award was advertised for the best 3 forgery of each invoice.
 Unfortunately, only 14 candidates delivered forgeries. In total, 25 forgeries were obtained, as some candidates submitted several forgeries using different approaches. The approaches can be divided into the following categories:  X  Print, Paste and Copy (PPC) Forgeries: these forgeries  X  Reverse Engineered Imitations (REI) Forgeries: this  X  Scan, Edit and Print (SEP) Forgeries: in this category
An overview of the frequencies of the different approaches is given in Table 2 . The first line gives the number of samples that have been received for each class of forgery. The second line contains the number of candidates that provided a forgery of that type. It can be noted that the SEP forgery approach is by far the most commonly used approach. An analysis of the background of the test candidate gives a possible explanation: out of the 14 candidates, ten are computer scientists or study computer science, with an image processing background. The other four have a less technical background. In the table, this information is broke down to the number forgery types per background. This information is given in the last two rows of the table. It can be seen that the computer scientists tend to make REI and SEP forgeries, whereas PPC forgeries are mainly produces by persons with non-technical backgrounds.
Manual inspection of the documents has been done to see what peculiarities do forgeries show. Distortions, color, and font variations could be observed. For all the PPC cop-ies, it could be observed that the text-line skew and align-ment variations were significantly higher than for the other forgery methods. Alignment inconsistencies for single char-acters could be seen in some SEP forgeries. Thus, it can be concluded that the text-line alignment and skew angle are valuable measures for detecting potential forgeries. This observation is also supported by the fact that these features are used by questioned document examination experts [ 1 ]. References
