 Recently, the general framework of learning to rank has attracted much research in-terests and even found its use in some of the popular commercial search engines. The supervised learning approaches explored in learning to rank entail the generation and collection of labeled training data. For example, labels or relevance grades such as  X  X erfect X ,  X  X ood X , or  X  X ad X  are assigned to documents indicating their degrees of rele-vance with respect to a query. We can collect such labeled data for a set of queries and their associated documents and use them to train a ranking function that can rank doc-uments for new queries [Zheng et al. 2007]. Acquiring large quantities of such training data, however, can be very costly since it requires a large amount of time and efforts for human editors to judge the relevance of documents to queries manually. There are studies that explore user interactions such as user clicks as the training signals, but they tend to be noisy and require much care for their use [Joachims et al. 2005].
One natural idea to tackle the training data scarcity problem is to leverage the train-ing data that have been collected for existing search engines with different purposes (for the auxiliary tasks), and use them to enhance the training process of the ranking function for a target search engine (the target task). Transfer learning is the appropri-ate conceptual framework for dealing with this type of problems [Dai et al. 2007, 2008; Pan and Yang 2010; Raina et al. 2007; Wu and Dietterich 2004]. It is useful to utilize training data of the existing search engines together with training data for the target task for a wide range of applications in Web search such as the deployment in newly born search markets and novel vertical search engines and services for new groups of users, since these target tasks often suffer the acute issue of lack of labeled training data.

A prerequisite for transfer learning to be advantageous is that the tasks share some common characteristics [Pan and Yang 2010]. This is the case of Web search engines. Consider search engine markets involving different languages and regions of the world as an example. Search engines rely on features such as term occurrences and co-occurrences in text and anchor texts, url addresses, and so on for learning ranking functions. On the other hand, due to the diversity in the cultures and backgrounds of users of search engines, the shared knowledge between target and the auxiliary train-ing data of the search engine can be very complicated. For example, although search engines rely on link analysis features such as PageRank to judge the quality of Web pages, the impact of PageRank can be very different among search engine markets. Therefore, the transfer learning algorithm should be flexible enough to capture the so-phisticated forms of knowledge and transfer them from the auxiliary task to the target task. However, existing transfer learning methods usually assume that the transfer-able features have some particular parametric forms, such as hidden layers of neural networks [Caruana 1997] or linear combinations of the original features [Argyriou et al. 2007; Wang et al. 2009]. These assumptions do not hold in general, especially in the case of learning to rank, where the shared information can be quite complex. Furthermore, these methods may even reduce the performance of ranking functions by transferring irrelevant knowledge to the target task.

In this article, we propose a principled method of transfer learning for learning ranking functions whereby labeled training data of the target task is utilized together with the training data of an auxiliary task in order to produce more accurate ranking functions for the target task. In particular, we propose a boosting framework that adaptively learns transferable representations or super-features from the training data of both the target task and the auxiliary task. Unlike previous methods, the super-features in our framework are not restricted to any particular forms. Specifically, the transferable features can be learned by any weak learners from the training data. Therefore, the proposed framework is sufficiently flexible to deal with the application of transfer learning for search engines.

Our core contribution is the development of an algorithm that adaptively learns the super-features among both tasks in a stage-wise manner similar to that used in gradient boosting [Friedman 2001]. The ranking function for the target task is then learned as a linear combination of those super-features. At each iteration, one super-feature is constructed based on the training data from both the target task and the auxiliary task and the corresponding coefficients are then learned with respect to each task independently. We evaluate our methods over the Letor data collection as well as a dataset from a commercial search engine. Our experimental studies show that the proposed transfer learning algorithm can improve the target ranking functions through utilizing training data from the auxiliary tasks. Also, comparisons with exist-ing transfer learning methods confirm that learning a nonparametric super-feature is more superior for learning to rank.

Next, we will first briefly describe some previous studies about learning to rank and transfer learning. Then our method will be described in detail together with a summary of the convergence properties of the algorithm. Experiments using Letor data as well as search engine data will be presented to show the effectiveness of the methods. Finally, conclusions and future work will be summarized. In recent years, the ranking problem is frequently formulated as a supervised machine learning problem [Burges et al. 2005, 2007; Cortes et al. 2007; Freund et al. 1998; Fung et al. 2006; Guiver and Snelson 2008; Joachims 2002a; Li et al. 2007; Xu and Li 2007; Yue et al. 2007; Zheng et al. 2007]. These learning to rank methods are capable of combining different kinds of features to train ranking functions. There are generally two approaches to these methods. The first approach formulates problem of ranking as that of learning a ranking function from pair-wise preference data and the ranking function is constructed through advanced machine learning techniques such as SVM [Joachims 2002a], neural network [Burges et al. 2005], and boosting [Freund et al. 1998; Zheng et al. 2007]. Another approach of learning a ranking function addresses the problem of optimizing the list-wise performance measures of information retrieval, such as precision, mean average precision, and Discount Cumulative Gain (DCG) [Cao et al. 2007; Joachims 2005; Xia et al. 2008; Xu and Li 2007; Xu et al. 2008; Yue et al. 2007]. Recent results of the Yahoo! Learning to Rank contest show that the meth-ods based on the gradient boosting trees are quite effective for constructing ranking functions [Burges 2010]. However, the previous approaches are all proposed to learn a single ranking function for a market in supervised learning. In the case where the training data is limited, they cannot produce a good ranking function. Several stud-ies have been focusing on learning ranking functions in a semi-supervised learning framework [Amini et al. 2008; Duh and Kirchhoff 2008].

Recently, several studies propose to construct ranking functions with transfer learn-ing [Chen et al. 2008a, 2008b; Wang et al. 2009]. The work of Wang et al. [2009] adapts the ideas of Argyriou et al. [2007] to construct a linear combination of features to trans-fer knowledge. This work focuses on transfer learning from a different domains with different feature spaces, while we propose to estimate nonparametric common struc-tures from the same feature space with different distributions.
 The goal of transfer learning [Pan and Yang 2010] is to enhance the learning process of a target task utilizing the knowledge obtained from learning other tasks. Multitask learning is a type of transfer learning that has been studied by the machine learning researches [Ando and Zhang 2005; Bakker and Heskes 2003; Caruana 1997; Evgeniou and Pontil 2004; Heskes 2000; Lawrence and Platt 2004; Obozinski et al. 2007; Xue et al. 2007; Yu et al. 2005]. The goal of multitask learning is to improve the generalization performance of multiple tasks through transferring knowledge among these tasks. Recently, several studies have been proposed for classification and clus-tering using auxiliary data sources [Dai et al. 2007, 2008; Raina et al. 2007; Wu and Dietterich 2004]. There are two main approaches in these studies: The first one is to select auxiliary training data that is effective for the target task. The second approach is to learn transferable representations using the auxiliary data. Previous transfer learning works focus on learning transferable representations in parametric forms, such as linear combinations of the original features [Argyriou et al. 2007; Evgeniou and Pontil 2004], and hidden layers of neural networks [Caruana 1997]. However, learning to rank usually requires to capture more complex transferable features and therefore expects to learn more flexible transferable representations. In this study, we propose a boosting framework that can estimate transferable nonparametric representations from both the target task and the auxiliary task. Our method is based on gradient boosting trees for regression [Friedman 2001]: We is the response space. For example, y i can be a numeric coding of the relevance grade of the corresponding document. The training process aims to find a function h such that For simplicity we use the square loss function, which is shown to be an upper bound to the Discount Cumulative Gain(DCG) that is widely used to evaluate information retrieval systems [Cossock and Zhang 2006]. More complex loss functions, such as pair-wise losses, can be adapted naturally. Formally, we measure the discrepancy to solve the following minimization problem where H is a predefined function class. We can apply gradient descent in the function space to minimize the functional R ( h ), that is, compute the gradient of R ( h )with respect to h at the current iterate h ( m ) ( x ) and form the next iterate as Here, w ( m ) is the step size. We can obtain an approximation of  X  R ( h ( m ) ( x )) by finding a function that interpolates/approximates the following conditions. ALGORITHM 1: GBT (Gradient Boosting Trees) For example, we can perform the interpolation/approximation by fitting a regression tree to the sample values. We summarize the preceding in Algorithm 1 and we label it as GBT (Gradient Boosting Trees) [Friedman 2001]. There are two parameters M , the number of regression trees and  X  , and these are the shrinkage factor that needs to be chosen by the user. In general, we can use cross-validation for choosing the two parameters. GBT has been successfully applied to learning ranking functions (i.e., the learned regression function h is used as the ranking function [Zheng et al. 2007]), and next we discuss how to adapt it to the transfer learning context. We consider the problem of constructing a ranking function for a target task. Assume Generally, these two training data are governed by different distributions. Our goal is to construct a ranking function h T ( x ) for the target task utilizing both the training set S T of the target task and the training set S A of the auxiliary task.
 In general, we assume that a loss function L T ( y ,  X  y ) is defined for the target task. Hence, the empirical risk R T ( h T ) is defined to be the sum of loss over the training set of the target task.
 Similar, empirical risk R A ( h A ) is defined by where L A ( y ,  X  y ) is the loss function of the auxiliary task.

In order to transfer knowledge from the auxiliary task to the target task, we need to model the relatedness between the target task and the auxiliary task properly. In our approach, we assume that some common internal representations are shared among both the target and the auxiliary tasks. Specifically, we assume that both h T ( x ) ..., g ( M ) ( x )) which can themselves depend on the original features in some complex ways. Formally, where the vectors w T and w A contain the coefficients of the super-features for the tar-get and the auxiliary ranking functions, respectively. The super-features g ( m ) : X  X  R are shared among all tasks and enable us to transfer knowledge from the auxiliary task to the target task. As we can see in Section 5, the transferable features can be learned by any weak learners from the training data from both the target and the aux-iliary tasks. Therefore, these transferable features make our framework sufficiently flexible to transfer complicated knowledge among different tasks in Web search. In transfer learning, the empirical risk of both the target task and the auxiliary task are optimized simultaneously. We have where the parameter c  X  (0 , 1) controls the amount of knowledge transfered from the auxiliary task to the target task. Considering the super-features g ( x ), the previous problem is equivalent to In Eq. (1), we combine estimating the function h T ( x ) for the target task with learning the super-features g (1) , g (2) ,..., g ( M ) that are shared by both the target task and the auxiliary task. In particular, the super-features g are estimated utilizing training data from both the target task and the auxiliary task. Hence, they can be more accurately estimated than using the training data from the target task only. Our goal is to find h T ( x ) for the target task and h A ( x ) for the auxiliary task (as a byproduct) that minimize the objective function defined in Eq. (1). Generally, the prob-lem in Eq. (1) is difficult to optimize directly. We propose to learn the super-features g ( m ) ( x ) and their coefficients w ( m ) a super-feature g ( m ) and its coefficient w ( m ) T and w ( m ) A at each iteration such that where C is the set of all super-features which is determined by the weak learner. Through optimizing Eq. (2), we fit a super-feature g ( m )  X  C that reduces the objective function in Eq. (1) as large as possible at each iteration. In this sense, the super-feature obtained at each iteration is effective for all the tasks and abstracts knowledge from all tasks, allowing the knowledge from auxiliary tasks to be transfered to the target task through sharing these super-features.
 ALGORITHM 2: Transfer Boosting
The problem in Eq. (2) can be solved through alternating optimization. The algo-rithm optimizes Eq. (2) by alternatively performing the following two steps. We first following problem.
Then, w ( m ) T and w ( m ) A are obtained by optimizing Eq. (2) with g ( m ) fixed. Since the and w ( m ) A with respect to each task separately. These two steps are performed alternatively until convergence. In practice, we observe that these two steps converge quite fast. In particular, we usually perform a few itera-tions (  X  5) and obtain quite good results. We summary our transfer learning algorithm in Algorithm 2.

We now apply Algorithm 2 to the square loss L ( y ,  X  y )=( y  X   X  y ) 2 for both target and the auxiliary tasks. Substitute L ( y ,  X  y ) into Eq. (1), we have the following problem. Under square loss, Eq. (3) now becomes From the preceding equation, we can see that the g ( m ) is obtained by solving a weighted regression problem. The g ( m ) is obtained by fitting the training data. Given the current estimation of g ( m ) , the linear coefficient w ( m ) T can be determined by solving In this case, we have a closed-form solution for w ( m ) T . The coefficient w ( m ) A can be calculated in a similar approach.

In principle, any weighted regression algorithm can be applied to fit g ( m ) . Currently, we use regression trees as is done in GBT [Friedman 2001]. We follow the techniques proposed by Zhang and Yu [2005] and Zheng et al. [2008] to analyze the algorithm proposed in Section 5.

Definition 1. The function class C is called scale-invariant if  X  g  X  C and a  X  R ,we have ag  X  C .
 the following condition. We have where m &gt; 0 is the tolerance constant 1 .
 We have the following theorem that ensures the convergences of our algorithm.
T HEOREM 1. Assume that R T ( h T ) and R A ( h A ) are convex functions of h T and h A , respectively. Moreover, assume that C is scale-invariant. If Eq. (9) holds, then under some technical conditions that are described in the Appendix, we have Due to the space limitations, we present the proof of the previous theorem in the Appendix. In this section, we evaluate the proposed transfer learning algorithms over the publicly available Letor data collection. In addition, we also use a dataset from a commercial search engine consisting of training data from several search markets which motivated our initial investigation of the problem. In order to evaluate the performance of the proposed algorithm, three evaluation mea-sures will be used: Precision, Mean average precision, and (Normalized) Discount Cu-mulative Gain.

Precision. In our experimental studies, precision at position n (P@n) is defined to be the fraction of the top-n retrieved documents that are relevant.

Mean Average Precision. The average precision of a query is the average of the preci-sion scores after each relevant document retrieved. Formally, average precision (AP) is calculated by the following equation. We have where rel i is the indicator function whether the i -th document of the ranking list is relevant to the query. Mean Average Precision (MAP) is obtained by the mean of the average precision over a set of queries. (Normalized) Discount Cumulative Gain. The NDCG value of a ranking list is calcu-lated by where r i is the grade assigned to the i -th document of the ranking list. For datasets with binary judgments, r i is set to 1 if the document is relevant and 0 otherwise. The constant Z n is chosen so that the perfect ranking gives an NDCG value of 1. Without normalization constant Z n , we obtain DCG [J  X  arvelin and Kek  X  al  X  ainen 2002]. We use the Letor 3.0 2 data collection developed by Microsoft [Liu et al. 2007]. In our experimental studies, TREC2003 and TREC2004 datasets are used since they contain multiple types of queries which we will use as different tasks. For a comprehensive comparison, we also evaluate our method over a dataset, namely Web, from a commer-cial search engine.

TREC2003 and TREC2004. The two datasets are provided for the Web information re-trieval track of TREC2003 and TREC2004 3 . The dataset contains three different tasks corresponding to three different types of queries: Topic Distillation (TD), HomePage Finding (HP), and Named Page Finding (NP). For each query, the human assessors de-cide the degree of relevance of a Web page with two labels: relevant and not relevant . The readers are referred to Liu et al. [2007] for details of the data.

Web. This dataset is from a commercial search engine. Each query-document pair is labeled by five grades 0 X 4 representing: perfect , excellent , good , fair ,and bad , re-spectively. The dataset contains three different search engine markets in two different languages.

The datasets are partitioned randomly into five folds and the performance is evalu-ated through averaging the performance over each fold. 7.3.1. General Performance. In order to evaluate the proposed algorithm, we use TD2003 and TD2004 datasets in the Letor data collection as the target tasks. For each target task, each of the other five datasets are used as the auxiliary task to help the construction of ranking functions. For example, for the target task TD2003, we will use HP2003, HP2004, NP2003, NP2004, and TD2004 as auxiliary tasks. Thus, we obtain 10 pairs of target tasks and auxiliary tasks. We use TD2003 as the target task and HP2003 as the auxiliary task for the row labeled by TD03+HP03 in Table I. For each pair of target and auxiliary task, we randomly sample 20% queries of the training set from the target task and construct ranking functions using the sampled training set of the target task and the whole training set of the auxiliary task. Each dataset in the Letor data collection is partitioned on queries to perform 5-fold cross-validation. For each fold, there are three subsets of data: training, test, and validation set. The validation set is used to determine the parameters, such as the number of iterations and the factor c in our transfer learning algorithm. Several methods are included for comparison.

GBTree: Gradient boosting trees are applied to the 20% sampled training data of the target task.

SVMRank: Rank SVM is applied to the 20% sampled training data of the target task [Joachims 2002b].

Mix: For each pair of a target task and an auxiliary task, we first combine the 20% sampled training data of the target task and all the training data of the auxiliary task as a training set. Then the gradient boosting trees are applied to construct the ranking function.
Mix2: Similar to Mix , we combine 20% training data of the target task and 20% training data of the auxiliary task to form the training set. Then the gradient boosting trees are used to construct the ranking function. The main difference between Mix and Mix2 is that Mix2 uses less training data from the auxiliary task but avoids the imbalance issue in Mix .

Combine: Based on the idea proposed in Li [2010], we train the two ranking func-tions h T and h A based on the training set of the target task and the auxiliary task respectively. Then we predict the ranking scores by their linear combination h T + ch A , where c =0 . 8 in our experiments.
 MFL: We also compare our method to the transfer learning method proposed in Argyriou et al. [2007], which learns common features among the tasks in linear forms.
GBTree-ALL: In order to have a comprehensive evaluation, we also report the perfor-mance of the gradient boost trees trained over the complete training set of the target task.
 SVMRank-ALL: Rank SVM is applied to the complete training set of the target task. The result is reported by the Letor data collection [Liu et al. 2007].
 Trans: This is the transfer learning method proposed in this article.

For comparisons of the different methods, we report the performance over the test set of the target task measured by both the precision and NDCG at positions 1, 3, and 5 as well as the Mean Average Precision (MAP). We average these performance measures over 5 folds for each dataset. The results over TD2003 and TD2004 datasets are reported in Table I and Table II, respectively. Our experiments are carried over a server with four 2 GHz cores and 2G memory, and the training process for each model takes about 1 hour using one of the four cores.

We can observe from the first four rows of Table I and Table II that the performance of GBTree and SVMRank is decreased when using 20% of the training data. This sug-gests that the amount of training data is important for constructing an accurate rank-ing function. When the amount of training data is not enough, the performance of the learned ranking functions will be reduced.

The performance of Trans is generally superior to the performance of the correspond-ing Mix , Mix2 ,and Combine . Hence, we conclude that Trans can utilize the knowledge of the auxiliary task to enhance the construction of the ranking function of the target task. We conduct a t-test over the improvements of NDCG@5 for statistical signifi-cance. The statistical significance improvements over Mix and MFL are marked by  X   X  in Table I and Table II. Moreover, by comparing Mix and Mix2 , we find that in some cases Mix2 performs better then Mix even if it utilizes less training data from the aux-iliary task. This is because the data distributions between different tasks can be quite different from each other. Therefore, adding more training data from the auxiliary task can be viewed as including more training data from a different distribution, which will reduce the performance.

Another observation is that the performance of the Mix models is inferior to the performance of the model trained using only the training data from the target task in some cases. For example, when the ranking function is constructed over the combined training data of TD2003 and NP2004, the performance is reduced with respect to the model with TD2003 data only. On the other hand, the performance of Trans is always superior to the model constructed with training data of the given task. Moreover, in several cases, the performance of Trans is superior or comparable to the model with the complete training set of the target task (training data increased five fold). For example, the performance of Trans model with auxiliary task HP03 is superior to the performance of SVMRank-ALL on the TD2003 dataset. These observations confirm that the knowledge from the auxiliary task is useful for improving the ranking functions of the target task.

We can also observe that MFL does not perform very well in most cases, which con-firms that the super-features cannot be represented by linear combinations of the orig-inal features, especially in ranking applications. Moreover, we can observe that MFL is not as good as GBTree with only the target training data in a lot of cases, which suggests that enforcing the linear common features may even reduce the performance when the linearity assumption does not hold. On the other hand Trans is more robust; it improves the performance whenever an auxiliary task is provided.

It is also interesting to observe that the performance of Trans varies when con-structed with different auxiliary tasks. For example, the performance of sf Trans on TD03+HP03 is superior to Trans on TD03+HP04 by a large margin. We think the rea-son is that the similarities between the target task and the auxiliary tasks are quite different. Specifically, when the target task and auxiliary task are similar, Trans can utilize more knowledge from the auxiliary task. In this case, the similarity between TD2003 and HP2003 is much more higher than the similarity between TD2003 and HP2004. 7.3.2. The Impact of Parameters. In this section, we investigate the impact of the pa-rameters on the performance of our methods. In this part of evaluation, we fix the target task to be TD2003 and the auxiliary task to be NP2003. First, our method has a parameter c , which controls the amount of information shared between the target task and the auxiliary task. The performance measured by NDCG@5 over the test set with respect to different values of c is represented in Figure 1(a). We also report the performance of Mix , Mix2 ,and Combine with respect to the values of c . It can be observed that the performance of Trans first grows and then is reduced when the value of c changes from 0 to 1. Generally, less information is transfered from the auxiliary task to the target task when c is close to 0. On the other hand, the training process is dominated by the auxiliary data when c is close to 1. We can also observe that the performance of Mix , Mix2 ,and Combine can be slightly improved when c is set properly. However, their performance is not as good as Trans , which indicates different ranking functions should be constructed for different tasks.
 We also investigate the influence of the amount of training data in the target task. To this end, we fix the amount of training data from the auxiliary task and vary the fraction of the training data from the target task from 10% to 100%. The performance measured by NDCG@5 over the test set is shown in Figure 1(b) with respect to differ-ent fractions of training data from the target task. The performance of GBTree is also reported in Figure 1 for comparison. It can be observed from Figure 1(b) that the per-formance of both Trans and GBTree grows when the amount of training data from the target task increases. When the amount of training data from the target task is small, Trans outperforms GBTree with a large margin. Hence, Trans is more robust to the decrease of training data. From Figure 1(b), we can also observe that Trans with 70% of the training data from the target task can archive comparable performance with GBTree with 100% of the training data, which suggests that the amount of training data can be reduced significantly through utilizing the transfer learning method. We also consider the impact of the amount of training data from auxiliary task. The performance over different fractions of training data from the auxiliary task is reported in Figure 1(c). It can observed that the performance of the target task grows in most cases when the amount of auxiliary data increases. However, in some cases, the performance is reduced when more training data from the auxiliary task are in-cluded. In our experiments, similar phenomenons are also observed on other datasets. We think that this is because some queries in the auxiliary task are very dissimilar with the target task. When these queries are included as auxiliary training data, the performance decreases since the relatedness between the target task and the auxiliary task is reduced. This is related to concept of n egative transfer, that is, the auxiliary data reduce the performance. However, to the best of our knowledge, it is quite difficult to design transfer learning methods that are robust to negative transfer in general. We will leave this interesting topic as a future direction. We also evaluate the transfer learning over a dataset from a commercial search engine. There are three different tasks in two different languages in this data: Task1 and Task2 are from two markets with the same language, Task1 and Task3 are from two markets in different languages. The goal is to construct ranking functions for Task1 which is viewed as the target task, while Task2 and Task3 are viewed as the auxiliary tasks, respectively. The relevance judgments for each task are organized into six sets with each set containing 600 queries. In our experiments, we use up to 4 sets of them as the training data and the other 2 sets as the testing data.

We apply DCG [J  X  arvelin and Kek  X  al  X  ainen 2002] to evaluate the performance of the obtained ranking functions, which is the standard metric used to evaluate this search engine. In our case, we use a five-grade labeling schema for editorial judgement,  X  X er-fect X ,  X  X xcellent X ,  X  X ood X ,  X  X air X  or  X  X ad X , which are mapped to values  X 10 X , X 7 X , X 3 X , X 1 X , X 0 X .
We report the results of using one set of relevance judgments of Task1 as the train-ing set for the target task. The training dataset of the auxiliary task is formed by 4 sets of judgments from Task2 or Task3, respectively. We compare Trans to two baselines, GBTree and Mix as described in Section 7.3.1. We do not include MFL for comparison since it does not work well in ranking applications. The performance measured by DCG at the 5-th position is reported in Figure 2(a). We can observe that the trans-fer learning method always outperforms the two baselines whenever the auxiliary task is included. The results of the t-test also show the improvements are statistically significant.

Another observation is that the performance improvement with Task2 as auxiliary task is superior to that with Task3 as the auxiliary task. We think the reason is that Task1 and Task2 are from two markets of the same language, while Task1 and Task3 are from two markets of different languages. As a result, the differences between Task1 and Task3 are larger than the difference between Task1 and Task2. Therefore, less information is transfered from Task3 to Task1.

We also investigate the influence of the amount of the training data from both the target task and the auxiliary task. First, we use 4 sets of training data from the auxiliary task and vary the training data from the target task from 1 set to 4 sets. From Figure 2(b), we can see that the performance measured by DCG5 grows when the amount of the training data in the target task increases. We can also observe that the performance of Task1+Task2 is always superior to the performance of Task1+Task3, which confirms our conclusion that the differences between Task1+Task2 are larger than the difference between Task1+Task3.

In Figure 2(c), we use 1 set of the training data from the target task and report the performance with respect to the amount of training data from the auxiliary task. We can observe that the performance sometimes reduces when the amount of auxiliary data increases. This confirms the fact that the effectiveness varies a lot for different queries from the auxiliary tasks. In the article, we propose a boosting framework for transfer learning, taking advantage of the training data from an auxiliary task in order to enhance the ranking function of a target task. In particular, the proposed framework estimates a set of transferable super-features based on training data from both the target task and the auxiliary task. This allows us to discover the common structures among the auxiliary and target task and transfer knowledge from the auxiliary task to enhance the construction of rank-ing functions for the target task. In our method, the super-features can be learned by any weak learners from the combined training data. This important flexibility prop-erty of the method makes it more suitable for applications such Web search where the transferred structures can be complicated and may not well captured by predefined parametric forms. Our evaluation results also show that the proposed transfer learn-ing method can utilize the knowledge from the auxiliary task to improve the ranking function of the target task.

There are several directions for future work. Since there are only a subset of the queries from the auxiliary task that carry useful knowledge to improve the target task, we plan to integrate the query selection into our transfer learning framework. This issue can be viewed as active learning in transfer learning scenarios, which relies on the trade-off between selecting informative queries and preventing unrelated queries in an auxiliary task. Another direction is to extend our framework to incorporate more complex loss functions for ranking, such as the pair-wise loss and list-wise loss functions [Cao et al. 2007; Pahikkala et al. 2009]. We also plan to consider the case where the target task and auxiliary task have different feature spaces, where we will need to learn a mapping between the two feature spaces.
 P
ROOF OF T HEOREM 1. We introduce some definitions that are proposed in Zheng et al. [2008]. Let C be a function class.
 Definition 2. For g  X  C , we define g = 1 N span( C ), h =inf { j | a j | : h = j a j g j g
Definition 3. Given a function R ( h ), a real number W is called a global upper bound of its Hessian if the constant W satisfies that for all h , g ,  X  ,wehave R ( h +  X  g )  X  R ( h )+  X   X  R ( h ) T g +  X  2 2 g 2 W .

Based on the preceding definitions, we first analyze the convergence bound for one-condition. We have where m &gt; 0 is the tolerance constant. For the simplicity of notations, we define The following lemma gives the bound for one-step convergence.

L EMMA 1. Assume that R T ( h T ) and R A ( h A ) are convex functions of h T and h A , re-spectively. Let W be an upper bound of the Hessian of R T ( h T ) and R A ( h A ) . Moreover, assume that C is scale-invariant. If Eq. (12) holds, then we have that for some constant N P ROOF .If R ( h ( m ) )  X  R (  X  h ) &lt; 0, we have which implies the lemma. Hence, we will assume R ( h ( m ) )  X  R (  X  h )  X  0 in the following. we can find a finite set S m  X  C that satisfies the following conditions. And for all &gt; 0
Let b g = |  X  T g  X  w T g | g . Then, we have Then, it is easy to verify that for all g  X  S m and t  X  X  T , A } , The definition of upper bound of the Hessian implies that for all a t , g and g  X  S m : Multiple both sides with b g and sum up over g  X  S m . in the previous inequality.

Divide both sides with g  X  S Therefore, we have Then, replace g  X  S lemma.
 Next, we restate Theorem 1 as follows.
 T HEOREM 2. Under the assumptions of Lemma 1 , we have Moreover, assume that  X  j =1  X  j  X  X  X  and  X  j =1 w j =  X  , then we have the following results. P ROOF .Let R ( h ( m ) )= R ( h ( m ) )  X  R (  X  h ). By recursively utilizing Lemma 1, we have Replace the previous inequality into Eq. (16), we obtain Eq. (15) of the theorem.
The second part of Theorem 2 is a direct result of Eq. (15). The readers are referred to Zhang and Yu [2005] for complete proof.

