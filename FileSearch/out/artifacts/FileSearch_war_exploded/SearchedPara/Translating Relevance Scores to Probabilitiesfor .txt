 Information retrieval systems conventionally assess docu-ment relevance using the bag of words model. Consequently, relevance scores of documents retrieved for different queries are often difficult to compare, as they are computed on dif-ferent (or even disjoint) sets of textual features. Many tasks, such as federation of search results or global thresholding of relevance scores, require that scores be globally comparable. To achieve this aim, we propose methods for non-monotonic transformation of relevance scores into probabilities for a contextual advertising selection engine that uses a vector space model. The calibration of the raw scores is based on historical click data.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  retrieval models Algorithms, Experimentation, Measurement, Performance Relevance scores, probability of relevance, logistic regres-sion, online advertising
Conventional information retrieval (IR) systems compute document relevance scores based on the bag of words model, where tf.idf term weighting considers word occurrence fre-quencies in individual documents and in the entire corpus. Heuristic tf.idf weighting works well in practice when doc-uments need to be ranked by their scores for a given query. However, in many cases, it is also necessary to consider ab-solute values of document scores, in addition to being able to Research performed during internship at Yahoo! Research. compare their relative values. For example, in meta-search (or more generally, federated search), scores assigned by dif-ferent search engines need to be reconciled. In some retrieval scenarios (notably, in online advertising) the final decision on which results are selected is based on their revenue po-tential, which is estimated as a function of the probability of a click and the bid amount submitted by the advertiser. In these scenarios, a principled way of comparing relevance scores is important to select the final set of results shown to the user. One way to address this problem is to calibrate the relevance scores using evidence from another knowledge source. Previous works focused on training regression mod-els using human relevance judgments (cf. [8]), while ignoring the score of the initial retrieval step. As opposed to learning the probability of click directly from the query and docu-ment features, in this work we rely on the IR score to cap-ture the information used in the first phase of the retrieval and transform that score into the probability of a click by using historical click data.

We focus on contextual advertising , where ads are selected for a given Web page based on its content [4]. True assess-ment of relevance probabilities is particularly important for ad matching as current models of ad selection often simulta-neously optimize relevance and revenue. The expected rev-enue of an ad is computed by multiplying the estimated ad relevance by some function of the bid , which is the amount of money the advertiser pays if the ad gets clicked. Producing accurate probability estimates of ad relevance is therefore crucial for reliable ad ranking, as well as for determining whether or not to show ads for each particular Web page or query [3].

We translate the scores produced by a vector space model into probabilities using logistic regression over click data. We further enhance our click prediction through additional query and ad features and examine a  X  X ixture of experts X  model, where each expert is implemented using logistic re-gression. The resulting transformation is non-monotonic (that is, it reorders the set of retrieved ads) and gives sig-nificant cumulative gain improvement over the vector space model ordering when measured by the historical click data.
Determining the probability of relevance has been an ac-tive field of study in IR for the last few decades. Both gen-erative and discriminative models have been proposed to estimate the probability of relevance [12, 9], however, these do not use existing relevance scores. Another approach to estimating the probability of a click is to use an aggregate of the click-through rate (CTR) over a period of time at different level of aggregation [1].
In this section we give a brief overview of the current prac-tices in Web advertising based on [4].

Contextual advertising is an interplay of four entities: The publisher rents space on its Web page in return for revenue; its utility is usually measured by the revenue and user re-tention. The advertiser provides the supply of ads. The ad network selects the ads that are placed on publisher X  X  Web pages. It acts as a mediator and shares the advertising rev-enue with the publisher. Finally, the users visit the Web pages of the publisher and interact with the ads. Studies has shown that users perceive as more relevant ads that re-late to the content of the publisher page [5]. The vector space model has been proposed to compare the content of the web page and ads to select relevant ads [11, 4].
The content match model aligns the interests of the pub-lishers, advertisers and the network itself. In general, clicks bring benefits to the publisher and the ad network by pro-viding revenue, and to the advertiser by bringing traffic to the target Web site. The revenue of the network, given a page p , can be estimated as: where k is the number of ads displayed on page p , and price ( a i ,i ) is the click-price of the ad a i at position i . Note that P ( click | p,a i ) needs to be normalized for position, tak-ing into account that ads in higher positions are more visible and tend to be clicked much more often. A common model of determining how much an advertiser is charged is based on the intuition that the charge should be equal to the min-imum amount that the advertiser needs to bid to retain the given position. As the ads are ordered by expected revenue, this can be estimated using the bid of the next ad in the revenue-ordered list of ads: Note that this computation requires to estimate the ratio of the click probabilities.
We propose models for estimating click probabilities P ( c | r, x ) based on the vector space relevance score r and other fea-tures x . We explore three models, all based on logistic regression. The first model fits a single global logistic re-gression to the entire data. Our second model partitions the data by publisher ids and fits a separate local logistic regres-sion to heavy hitters , i.e., the publishers who make a large number of ad calls. For the tail publishers, we fall back on the global logistic regression. Our third model is based on a committee logistic regression that performs clustering and fits a local logistic regression for each member of the com-mittee. To avoid the cold-start problem (new page-ad pair showing up in the test data), the cluster assignment proba-bility of a pair is also modeled as a function of features. In the next sections, we provide a description of logistic regres-sion model (along with features used) and committee based logistic regression. Throughout, we will assume a training set of page-ad pairs with the i th pair having features x relevance score r i , each pair is accompanied by a binary vari-able y i indicating whether the ad was clicked or not when shown on the page.
 Logistic regression is a well known technique to estimate conditional probabilities associated with a binary outcome variable. We assume Here,  X  are unknown weight parameters associated with fea-tures and the function f (unknown) quantifies the relation-ship between relevance and CTR after adjusting for features. Empirical analysis indicates that f is well approximated by a quadratic function, however, we obtained more robust per-formance by approximating f through a piecewise constant function, i.e., B k is the k th bin and  X  k is the associated weight parameter. For our experiments, we obtained B k by dividing [0 , 1] into 20-30 equi-spaced intervals. Maximum likelihood estimates of the unknown parameters (  X  ,  X  ) is a well-studied prob-lem [2], it can be obtained through several numerical meth-ods like conjugate gradient, L-BFGS, Iteratively re-weighted least squares[10]. Since most of the features in our scenario are binary with a small number of them  X  X urned on X  for a given page-ad pair i , we use the L-BFGS (CG applies as well) method that exploits the sparse structure.
 Global logistic regression defined in Equations 1 and 2 as-sumes the unknown weight parameters (  X  ,  X  ) are constant for all page-ad pairs. This maybe a limiting assumption, especially in our application where extreme heterogeneity is expected to exist due to differences in publishers, ad cam-paigns, user population. We relax this assumption by fitting a mixture of logistic regressions, i.e., Here,  X  i = (  X  i 1 ,  X  X  X  , X  iK ) are cluster membership probabil-ities for the i th pair, (  X  c ,  X  c ) are cluster specific regression weights assigned to the features. The main advantage of such a feature based mixture allocation strategy is to avoid the cold start problem; we can assign a new pair to an ap-propriate cluster based on features alone. To complete our model specification, we provide the functional relationship between  X  i and binary features w i = ( x i , { 1( r i  X  B 1 ,  X  X  X  ,M } ). There are several possibilities here, we explore the simplest one that is based on a Naive Bayes assumption for each co-ordinate  X  ic , i.e., where  X  c,j  X  X  are unknown constants estimated from data. We use an EM algorithm[6] to fit the model described by Equations 3 and 4 to our training data. This is done by introducing a  X  X atent X  allocation variable z i to each pair i which is indicative of the cluster to which i is assigned. The incomplete data log-likelihood based on Y is now optimized by working with the complete data log-likelihood ( Y , Z ); this splits the log of sums arising due to Equation 3 into sum of logs, usual trick employed in likelihood maximiza-tion with mixture models. The E-step computes  X  X esponsi-bilities X   X  i ( c ) for each pair i , i.e., the weight with which pair i belongs to cluster c . Note that where  X   X  i,c is the estimated allocation probability based on estimates  X   X  c,j , and  X  p i,c is the estimated Bernoulli success probability based on estimates of parameters (  X   X  c the M-step, the parameters  X  c  X  X ,  X  c  X  X  are updated by run-ning separate weighted logistic regressions in each cluster, the weight assigned to the i th pair in cluster c being given by the estimated responsibility in the E-step. The alloca-tion parameters {  X  c,j }  X  X  are updated by fitting Naive Bayes models in each cluster to the estimated responsibilities. We iterate the E and M steps until convergence. To ensure scalable model fitting to large amounts of click-log data, we exploit grid computing. In addition to the relevance score, we used the following features.
We now present an extensive evaluation of our methods on a sample of data obtained from an actual content match system of a major US search engine. To demonstrate the effectiveness of converting relevance scores to probabilities, we use historical data to compare retrieval results using con-ventional tf.idf scores and using our probability estimates. Our training data consisted of about 2 million contextual ad-vertising ad slates spanning 15 days. All models were fitted using the training data, and results are reported by comput-ing metrics on the test data, which consists of approximately 1 million slates spanning 7 days. Overall, our data consisted of approximately 400 publishers with the top-20 accounting for 70% of the total number of clicks.

We refer to the different models as follows. We use VSM to denote the baseline system that uses a vector space model with tf.idf weights (similar to the approach proposed by Broder et al. [4]). Global logistic regression without word-in-common features will be called Global , while GlobalW is a global logistic regression that includes the word-in-common features. The variants of committee logistic regressions with and without word-in-common would be denoted by EMW and EM , respectively. Finally, PART denotes the model that runs local logistic regression for the top 20 publishers that account for approximately 70% of clicks, and falls back to Global for the tail publishers that obtain the remaining 30% of clicks.

We compare the ad ranking of the VSM model to the rank-ing based on P ( click | p,a i ) for the various models using the Discounted Cumulative Gain (DCG) metrics. For a given slate with l positions ordered by priority (position 1 being the best), the DCG is defined by where r i is the relevance of ad at position i and w position-specific weight, assumed to be 1 /log 2 ( i + 1) in the standard literature. The relevance r i is binary and takes the value 1 if the ad at slate position i is clicked, and 0 otherwise. Normalized DCG (NDCG) is defined as DCG/IDCG, where IDCG is the ideal DCG attained using the best relevance ranking.

Logarithmic decay by positions is a reasonable assumption when ads are presented in a list format (e.g., in Web Search). This is not the case in our application where the presentation of ad slates depends critically on the page layout, which can vary greatly. An ideal estimate of w i should be based on CTR differential, that is, the drop in CTR when a typical ad is moved from position 1 to i after randomizing over all other factors. This is intractable to be calculated at a granularity of a page, hence we make a simplifying assumption that decay is constant across all pages of one publisher and use the per-publisher decay curve through historic data. In our data we observed clear heterogeneity in CTR in different publishers due to the positional effects. We note that the bidding mechanism automatically induces a certain degree of randomness into the system, hence we believe such global decay estimates of positional effects are reasonable for the purpose of evaluation.
 More specifically, we let w i = CTR( i ) / CTR(1), where CTR( i ) denotes the global CTR at position i . We refer to the modified NDCG formula that uses these weights as Emp-NDCG, which stands for NDCG with empirically esti-mated decay weights. We refer to the standard formulation of NDCG as Log-NDCG. Figure 1 shows the weight curve obtained through logarithmic weighting and through global positional CTR estimates. The weights estimated empiri-cally from the actual data are further smoothed through an isotonic regression to ensure monotonicity. The CTR-based decay tapers off much faster than the standard logarithmic weighting resulting in higher relative reward for Emp-NDCG at the premium positions (i.e., positions 1 and 2).
Table 1 provides the overall NDCG numbers for two dif-ferent choices of decay (Logarithmic and Empirical) for all methods. We note that Global , GlobalW , EM , EMW all have similar performance. PART has the best performance Figure 1: Decay weights for Log-NDCG and Emp-NDCG. Empirical refers to Emp-NDCG with weights estimated from the data; these are smoothed to be monotonic through Isotonic regres-sion.
 Table 1: Overall NDCG for different models com-pared to the vector space model (VSM) baseline and has a relative improvement of 13% in Log-NDCG, 2% in Emp-NDCG over the VSM baseline. The improvement of PART over Global is marginal on Log-NDCG scale (1%) but relatively better on Emp-NDCG scale (0.9%). This is largely because PART provides separate estimates of po-sitional effects for top publishers, and is more effective in re-ranking ads that get clicked at the bottom to the top of the list. In fact, if we just confine the evaluation to the top 20 publishers where PART differs from Global , the Emp-NDCG scores for PART and Global are 0.564 and 0.556, respectively, a 1 . 4% relative improvement.

To test statistical significance, we conducted a bootstrap procedure [7], i.e., we computed average NDCG by taking a random sample of size n (we used n = 100 , 000) from the clicked slates with replacement. We took B such bootstrap replications (we used B = 50) and computed empirical dis-tributions of relative improvements between the NDCG X  X . The improvements reported above between ( PART , VSM ) and ( PART , Global ) are statistically significant. In Fig-ure 2, we look at the distribution of average NDCG per pub-lisher for PART and VSM . Here again, the NDCG X  X  are significantly better for the best probabilistic model across publishers.

The experiments show that modeling using the relevance score and the four features described above produces a signif-
Figure 2: Model performance across publishers icant improvement of the ad ordering. One of the questions posed by this result is what contributes to the improvement -the prediction modeling or the extra information provided by the features used in the model. Of the four sets of features used in the modeling, two (words in common and taxonomy features) are already used in the vector space model and do not contribute new information to the re-ranking process. The other two (publisher domain and ad position) pertain only the page and its layout and are not used in ad selection. While it is feasible to assume that ad selection could be influ-enced by the ad position on the page (.e.g. ads in all capital letters do better at the bottom of a page), this is an unlikely cause of the improvement of ad ranking. Therefore we can conclude that the NDCG improvements are brought by the prediction modeling having better differentiation than the cosine similarity used in the VSM.
We described a method for converting vector space rel-evance scores into probabilities for contextual advertising. The transformation function uses the original (VSM-based) score as well as other features of the page to achieve non-monotonic transformation that significantly improves the NDCG over the click data. In fact, we obtain a 13% rel-ative gain in NDCG over the vector space model using our best probabilistic model. [1] D. Agarwal, A. Z. Broder, D. Chakrabarti, D. Diklic, [2] C. M. Bishop. Pattern Recognition and Machine [3] A. Broder, M. Ciaramita, M. Fontoura, [4] A. Broder, M. Fontoura, V. Josifovski, and L. Riedel. [5] P. Chatterjee, D. L. Hoffman, and T. P. Novak. [6] A. P. Dempster, N. M. Laird, and D. B. Rubin.
 [7] B. Efron and R. Tibshirani. An Introduction to the [8] F. C. Gey. Inferring probability of relevance using the [9] J. M. Ponte and W. B. Croft. A language modeling [10] W. H. Press, S. A. Teukolsky, W. T. Vetterling, and [11] B. Ribeiro-Neto, M. Cristo, P. B. Golgher, and E. S. [12] S. Robertson and K. Sparck Jones. Relevance
