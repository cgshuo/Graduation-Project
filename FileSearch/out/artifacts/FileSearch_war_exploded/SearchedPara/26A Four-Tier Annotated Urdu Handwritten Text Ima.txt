 Over the past few years, a lot of advancements have been made in the field of hand-written text recognition. Linguistic resources such as annotated corpus are playing a significant role and are the most demanding platform for computational linguistic research. A machine-readable corpus has more capability to explore and identify all features of natural language including the characteristics of the desired texts such as lexical, textual, semantic, and syntactic attributes. Corpus Linguistics is an approach for investigating the diversity of a language using a large collection of real-life natural language text samples. This approach has been used in a number of research areas for ages, like in the study of the language, writing style of the language, and development and benchmarking of various OCR algorithms.

Researchers developed various datasets/databases of different standards based on requirements, such as a database of isolated digits/characters, text lines/words, or paragraphs to evaluate the performance of various OCR algorithms. In the field of handwritten text document image analysis, many highly cited algorithms exist in the literature such as those of Alaei et al. [2011b], Gatos et al. [2009], Stamatopoulos et al. [2013], Margner and El Abed [2009], Gatos et al. [2010], Likforman-Sulem et al. [2007], Li et al. [2008], Louloudis et al. [2009], and Yin and Liu [2009], which have used and have shown the need in standard databases for training and testing their method X  X  performance.

Two axes of database development have been identified for handwritten text recogni-tion systems based on the input mode: online and offline. Researchers have developed online handwritten databases [Liu et al. 2011; Guyon et al. 1994; Viard-Gaudin et al. 1999; Kumar 2010; Indermhle et al. 2010; Nakagawa et al. 1997] and [Nethravathi et al. 2010]. Bhaskarabhatla and Madhvanath [2004] collected handwritten data for online handwriting recognition in different Indic scripts. A brief overview of the online handwritten databases for English, Chinese, Japanese, and Indic scripts is shown in Table I.

Guyon et al. [1994] designed a platform for data exchange and recognizer benchmark-ing. This format includes various online hand-printed and cursive alphabets including Latin and Chinese, signatures, and pen gestures. The database CASIA (Institute of Au-tomation of Chinese Academy of Sciences) built by NLPR (National Laboratory of Pat-tern Recognition) [Liu et al. 2011] introduces both modes of online and offline Chinese handwriting databases, containing samples of isolated characters and handwritten texts. The dataset has 3 . 9 million isolated character samples produced by 1,020 writers using Anoto pen on paper for obtaining both online trajectory data and offline images.
The database TAUT [Nakagawa et al. 1997] is another online handwritten database made of 10,000 character patterns by selecting the 1,227 most frequently appear-ing character categories from a sequence of newspaper sentences. The dataset IRESTE [Viard-Gaudin et al. 1999] is a dual handwriting database; it has 4,086 isolated digits, 10,685 isolated lowercase letters, 10,679 isolated uppercase letters, and 410 EURO signs. It also contains 31,346 isolated words (28,657 French and 2,689 English words).
 ples from 36 character classes obtained from 25 native writers. Each writer was asked to provide two samples per class. In 2010 , Nethravathi et al. [2010] developed an online handwritten database of 200,000 words for two Indic scripts, Tamil and Kannada, by collecting 100,000 words for each script from 600 users to capture the variations in writing style. Bhaskarabhatla and Madhvanath [2004] collected handwritten data for online handwriting recognition in different official Indic scripts.

The second category of handwritten datasets is the offline handwritten database where standard databases of isolated characters/digits or sentences have been de-veloped and proposed in the literature. Some of the most widely used handwritten databases for some scripts such as French, English, Korean, Chinese, and Indic script are IRESTE, CEDAR, IAM, CMATER, NIST, PE92, IFN/ENIT, KHTD, FHT, HIT-MW, and PBOK. A brief overview of these databases is shown in Table II.
 NIST [Wilkinson 1992], MNIST [Deng 2012], IRESTE [Viard-Gaudin et al. 1999], IAM [Marti and Bunke 2002], RIMES [Grosicki et al. 2006], and CEDAR [Hull 1994] are the most frequently used standard English text databases.

The NIST database is composed of handwritten characters/digits and running En-glish texts. The data samples were extracted from 2,100 filled forms. The MNIST database is a large database of handwritten digits extracted from the NIST database. The IAM database is a collection of 1,539 handwritten text pages. Besides text pages, it also contains images of text lines and words with ground-truth labels.

The CEDAR [Hull 1994] database is a collection of digital images of city names, state names, and zip codes from the postal addresses. Images have been segmented from the addresses by a semiautomatic process. It has been very widely used in the experimentation of a wide number of OCR techniques in ICDAR and ICFHR .

The databases IRESTE [Viard-Gaudin et al. 1999] and CEMTAR [Sarkar et al. 2012] were developed for more than one language simultaneously. IRESTE is a dual hand-writing database of English and French scripts. It includes images of isolated digits and letters and 410 EURO signs. The database also contains 31,346 isolated words (French: 28,657 and English: 2,689). The CEMTAR database contains 150 handwrit-ten document pages, among which 100 pages were written purely in Bangla script and the rest of the 50 pages were written in Bangla texts mixed with English words.
The database PE92 [Kim et al. 1993] is a collection of handwritten Korean character images where the authors collected 100 sets of KS (Korean Script) 2,350 handwritten Korean character images. The first 70 sets were generated by more than 500 different writers, and the same person wrote each of the remaining 30 sets.

A Chinese handwriting database named HIT-MW [Su et al. 2007] was developed by including 853 handwritten forms, where forms were produced under unconstrained conditions without preprinted character boxes. ETL9 [Saito et al. 1985] is a set of hand-printed characters in JIS Chinese with its analysis in Japanese.
 corpus, developed by the NECTEC (National Electronics and Computer Technology Center). The corpus consists of more than 44 , 000 images of online and offline hand-written characters, including isolated, touching, and cursive handwritten characters.
For the Arabic language, most of the available databases are a collection of isolated characters/digits or words, while the database AHTD [Mahmoud et al. 2011] is a col-lection of handwritten text pages. The IFN/ENIT [Pechwitz et al. 2002] database was developed for training and testing of Arabic handwriting recognition systems. It con-tains more than 2,200 binary images of handwritten forms written by 411 writers. A ground-truth file for each word in the database has been compiled. This file contains information about the word, such as the position of the word base line, and information on the individual characters used in the word.

AHDB [Al-Ma X  X deed et al. 2002] introduced a database for offline Arabic handwriting recognition, together with an analysis of the database and its associated preprocessing operations. The database contains a sample image of Arabic words and free handwrit-ing text pages. Alamri et al. [2008] introduced a database of isolated Indian digits, numerical strings, Arabic isolated letters, and a collection of 70 Arabic words. It also includes a free format sample for Arabic date.
 recognition of handwritten Arabic cheques. It is composed of real-life Arabic legal amounts, Arabic subwords, courtesy amounts, Indian digits, and Arabic cheques.
The database Al ISRA [Kharma et al. 1999] describes the methodology for the devel-opment of a comprehensive database including handwritten Arabic words, numbers, and signatures. AHTD [Mahmoud et al. 2011] is a database for offline Arabic hand-written text recognition. The database is composed of images of the handwritten text at various resolutions, and it also provides ground-truth metainformation for written text at the page, paragraph, and line levels.

For the Farsi language, there exist a few databases. FHT [Ziaratban et al. 2009] is an unconstrained Farsi handwritten text database of 1 , 000 forms with contributions from 250 participants in different age groups and with varied education levels. These characteristics of the database make it suitable for many OCR applications. Khosravi and Kabir [2007] introduced a very large dataset of handwritten Farsi digits. The database includes binary images of digits extracted from about 12 , 000 registration forms of two types, filled out by BSc and senior high school students.

A new large-scale multipurpose CENPARMI Farsi handwritten dataset [Haghighi et al. 2009] consists of 432 , 357 images of dates, words, isolated letters, isolated dig-its, numeral strings, special symbols, and documents. The forms were collected from 400 native Farsi writers. The IfN/Farsi [Mozaffari et al. 2008] database consists of 7 , 271 binary images of Iranian province/city names. The HaFT [Safabaksh et al. 2013] database contains 1 , 800 grayscale images of unconstrained texts.

The generation of corpus methodology for Indian scripts was initiated in 1991. To date, very few datasets are available for Indian scripts. Some of the notable works are as follows: The Kannada handwritten text database (KHTD) [Alaei et al. 2011a] is an unconstrained dataset, containing 204 handwritten documents of four different categories written by 51 native speakers of Kannada. The total number of text lines and words in the dataset are 4,298 and 26,115 respectively.
 database of Indian scripts. The database includes isolated handwritten numeral sam-ples of real-life situations for Devanagari and Bangla scripts. CEMTAR [Sarkar et al. 2012] is a database of unconstrained Bangla  X  English mixed script handwritten docu-ment images. The database contains 150 handwritten document pages, among which 100 pages are written purely in Bangla script and the rest of the 50 pages are written in Bangla text mixed with English words.

The standard database PBOK [Alaei et al. 2012] of four different scripts includes text pages of three Indic scripts, Kannada, Bangla, and Oriya. The Kannada part of the database has 228 text pages of four different domains written by 57 writers. The Kannada section contains a total of 4 , 850 handwritten text lines, 29 , 306 words, and 213 , 147 characters. It also contains 199 and 140 handwritten text pages of Bangla and Oriya, respectively. The database provides pixel-and content-based ground truthing for all the text pages. This database contains text pages written from both directions, and most of the samples are either overlapping or touching text lines.
 For the Urdu language, so far only two handwritten databases exist: CEN-PARMI [Sagheer et al. 2009] is the Urdu offline handwriting database, which in-cludes isolated digits, numeral strings with/without decimal points, five special sym-bols, 44 isolated characters, 57 financial related words, and a collection of Urdu dates in different formats. Another available offline Urdu handwritten database is CENIP-UCCP [Raza et al. 2012], which is an unconstrained offline sentence database com-posed of 400 digitized forms produced by 200 different writers. The database has been labeled/marked up at the text-line level only.

From the literature review, it can be summarized that there exists a sufficient num-ber of standard databases for scripts like English, Chinese, and Japanese, while very few standard databases are available for Arabic and Farsi. Compared to these lan-guages, very little attention has been given to the Urdu language. The Urdu handwrit-ten database developed by CENPARMI [Sagheer et al. 2009] focuses only on isolated characters and digits and some selected words. Only CENIP-UCCP [Raza et al. 2012] includes 400 images of handwritten sentences.

Urdu script is more complicated and elaborate compared to Arabic and Persian. The main reason for the Urdu script getting less attention and its slower development in the OCR s field is the lack of a standard database for Urdu script. The availability of resources for data collection is much less for Urdu as compared to scripts like Persian and Arabic. It is difficult to use Urdu script in automation, as a single character entry needs two to three keystroke combinations. To bypass this data entry step, we need to develop machine vision systems for automatically converting handwritten Urdu characters into their transcripted counterpart. To develop such intelligent systems, we need a large corpus to train the system for recognizing handwritten Urdu characters. These issues motivated us to develop an Urdu handwritten text database, which is a much-needed platform for training, testing, and benchmarking of handwritten text recognition systems for the Urdu script.
 This article describes the detailed methodology of developing an annotated corpus, CALAM, in a scientific way, including a large volume of unconstrained handwritten text images in Urdu script and their corresponding transcripted texts in a Unicode text file or in an XML file format. The corpus consists of of 1 , 200 handwritten images written by 725 writers belonging to different geographical regions. The number of handwritten text lines varies from two to six lines in a form. The average number of words varies from 20 to 80 in a text form/image. The text page also includes the demographic in-formation of the writer like name, age, gender, education, address, and signature. The selection of texts is distributed within six categories and 14 subcategories to achieve the maximum variations in the words as texts. The corpus is designed to support a large number of computational linguistic research, such as identifying writing styles and grammatical information and developing machine-readable platforms. The corpus consists of an aligned transcription for image, line by line, phrase by phrase, or word by word. The corpus is completely marked up for content information to support content detection and evaluation of systems like linguistic handwriting recognition, signature verification, and writer identification. The database was experimented for the bench-marking of handwritten text recognition algorithms by generating an XML file of annotated handwritten text images. Experimental results in the form of quantitative analysis of four handwritten text-line segmentation techniques are also reported.
The article first introduces the experimental setup for the collection and distribution of data in a systematic manner, and then reports the process of information fetching and feeding in both the handwritten text image and its corresponding XML file. The article is organized as follows: Section 2 describes the characteristics of Urdu script. Section 3 introduces the process of data collection and gives an overview of the statistics of the database. Section 4 describes the functionality and annotation of the scanned handwritten image in a hierarchical manner with the generation of an XML file for ground truth. Section 5 does the comparative analysis of the structure with the existing document annotation tools. Section 6 provides experimental evidence in terms of text-line segmentation and distribution-of-words frequency in the proposed corpus. Finally, conclusions are presented in Section 7. Urdu script belongs to the Indo-Aryan family of scripts and is historically related with India from the time of the Mughal Empire. The present shape of Urdu script is significantly influenced by languages like Persian, Arabic, Turkish, Punjabi, and other indigenous languages of the Indian subcontinent. It is the national language of Pakistan and is one of the 22 scheduled languages in the Constitution of India. India has a large number of native Urdu speakers in its five states: Andhra Pradesh, Jammu and Kashmir, Bihar, Uttar Pradesh, and New Delhi. Urdu is the official language of Jammu and Kashmir state, and recently Urdu was also approved as the second official language of Uttar Pradesh. The population of Hindi-Urdu speakers is the fourth-largest community in the world after Mandarin, Chinese, English, and Spanish. According to Government of India 2001 census data [Census 2001], in India, more than 50 million people speak Urdu as their native language.

The Urdu script is written from right to left and is an extension of the Persian alphabet, which is itself an extension of the Arabic alphabet. The Urdu alphabet set contains 38 characters and 10 digits, as shown in Figure 1. Urdu is associated with the Nastaleeq style of Persian calligraphy, whereas Arabic is written in the Naskh style. As shown in Figure 1, the  X  X iamond shape X  on the top of characters indicates the extended characters for Urdu from Persian. In Unicode, Arabic and its associative languages like Urdu, Punjabi, and Sindhi have been allocated 1 , 200 code points as (0600h -06FFh, FB50h -FEFFh).

At the time of writing, individual characters are joined together according to rules for every consecutive pair of characters in order to form groups of characters called ligatures. A word consists of one or more ligatures written next to each other. Ligatures in Urdu are composed of one or more characters; Table III shows examples of seven valid ligatures formed with a combination of two to eight Urdu characters. Urdu characters typically attain different shapes according to their placement in forming a ligature. Both the meaning and shape of the characters change depending on their positions (at beginning, middle, and last). The problem is further aggravated by the cursive nature of the script. Thus, the shape assumed by a character in a word is context sensitive, decided by its placement.

Furthermore, the uses of the dots(.) and diacritic during the writing makes it more complicated for the recognition process. Dots play a significant role in the Urdu alpha-bet; a single dot can make a big difference. The placement of a dot can change one letter into a different letter. For example, as shown in Table IV, the letter [be] has its basic shape in common with three other letters, [pe] , [te], and [se] , with only some dots differentiating them.

One of the challenges for Urdu OCR is to characterize the differences between these very similar-looking letters. Table IV shows the differences between these very similar-looking letters using the dots, and Table V shows the differences between very similar-looking letters using the diacritic.
 The process of design and development of an Urdu corpus starts with the raw data collection and ends with appropriate tagging and labeling of the collected texts in the database. In our methodology, we used a higher-level (sentences)-based approach rather than collecting a list of isolated characters, digits, and words that combines different units of writings in a single trial. The collection of data has been done mostly from the news channels like BBC Urdu and ETV Urdu, Urdu blogs, historical-ancient documents, and textbooks. In some categories like history, architecture, and biography, the printed documents are entered manually in Unicode text due to nonavailability of Urdu Unicode texts for some words that were used earlier and are not in use now. To capture the maximum words for the corpus, we have used a long time period for data collection, starting from 1901 to the present, among six different categories.
In order to be representative of all the phenomena of a particular language, the corpus contains a large variety of text samples. The domain of the corpus is a data collection of six different categories that are further divided into 14 subcategories to capture the maximum variance in word collection and make the corpus more significant in terms of a balanced corpus. Although there are no specific criteria for a balanced corpus, the criteria we have chosen for a balanced corpus are topics (category) of text selection and time span of data collection. The advantage of the balanced corpus is that texts are selected in such a way that the phenomena of searching become more efficient compared to the imbalanced corpus. It also provides additional facilities such as classification of texts as per research requirement, filtering of texts, and statistical analysis of data based on various terms like age, gender, educational qualification, region, and category.

The list of categories and their corresponding subcategories with denoted keywords for data collection is as follows: (1) History -H (2) Literature -L (3) Science -S (4) News -N (5) Architecture -A (6) Politics -P The form layout has been designed in a specific way to collect a large amount of significant information on a single form and make the corpus available in the multidisciplinary research areas of document image analysis, such as writer identifica-tion, signature verification, segmentation of printed and handwritten text, evaluation of OCR algorithms and technology, and training of a system for automatic data entry. The layout of the handwritten form is separated into four parts with a horizontal line for convenience in the segmentation of machine-printed text followed by handwritten text and demographic information of the writer. The design of the A4 size form is split into four parts as shown in Figure 2; each part is separated from each other by a horizontal line and organized as follows: (1) Part 1: This part of the form comprises the title for a language in the database (2) Part 2: This part of the form consists of two to four lines of printed text, which are (3) Part 3: The third part of the form is left blank where the writers replicate the (4) Part 4: The fourth part of the form is optional to collect demographic information
The filled-out forms were scanned at a resolution of 300dpi at a gray level. Each form was completely scanned, including the printed texts, handwritten texts, and de-mographic information, and its corresponding transcripted texts of the scanned image were stored in a Unicode UTF-8 text file. The database contains 1 , 200 handwritten text forms, filled out by 725 writers from different age groups and with different educational qualifications. Text pages were written by both males and females; 65% of the writers were males and 35% were females. Information about name, age, and address was collected on each page. Seventy-five percent of the 725 writers were younger than 26 years, and 79% were graduate students. Each writer was asked to write forms in an unconstrained environment in his or her natural handwriting with different pen styles and inks.

To capture the maximum variance in data collection, the domain of data collection is divided into six categories and 14 subcategories. The statistics of the data collection according to the categories are shown in Figure 3.

The database contains 3 , 403 Urdu handwritten text lines, 46 , 664 Urdu words, and 101 , 181 Urdu ligatures. On average, each filled-out handwritten text page comprises 2 . 84 text lines, 38 . 89 text words, and 84 . 31 ligatures. The database also contains 33 , 162 unique words, which are 71 . 07% of the total words present in the database. Besides this, the database contains 2 , 353 Urdu printed text lines.

The domain-wise distribution of lines, words, and ligatures in the database is shown in Figure 4. The database contains ligatures of one to six characters, and the dis-tribution of the ligatures with various character combinations is shown in Figure 5. Statistics of the demographic distribution of the dataset are tabulated in Table VI. Availability of a large annotated ground-truth database is a significant advancement for handwritten text recognition techniques. Corpus annotation is a useful process for making the corpus available in the broad areas of computational linguistic research by associating it with some additional information and providing support for machine learning. Corpus annotation plays a significant role for automatic evaluation of seg-mentation and recognition results.

Annotation is a time-consuming and error-prone task, so it requires the utmost care as highlighted in literature work related to online Indic script annotation [Kumar et al. 2006; Belhe et al. 2009; Jawahar et al. 2009; Bhaskarabhatla et al. 2004]. Messaoud and Abed [2010] have designed a structure to annotate the handwritten historical docu-ments, while Alaei et al. [2012] and Alaei et al. [2011a] annotate an offline handwritten documents database. We have developed a structure (CALAM) that highly annotates a large volume of offline handwritten text documents in a systematic and scientific way and also reduces the time of annotation and takes care of the data validation.
Apart from pure text-page annotation, CALAM provides some additional linguistics features such as aligned transcription for segmented lines and words. In addition to handwritten text-page annotation, the database also accumulates the demographic information at the form level related to the writer of the text page in Unicode. The in-formation includes the writer X  X  name, education, age/gender, address, and geographical information.

The annotation of a handwritten text form was done in standard encoding Unicode (UTF-8) for two reasons: (1) to ensure the compatibility with a non-Urdu operating system and character set and (2) to make the corpus, language, and operating system independent and compatible with other corpus access Unicode-based tools.

The next section describes the step-by-step process of designing a corpus after the generation of scanned handwritten text forms and the four levels of annotation along with an XML standard file generation for ground truth as shown in Figure 6. The structural mapping provides the facilities of corpus creation and navigation through the stored information of handwritten images, segmented lines, words, and components very easily through the database, along with a broad view of the input data and transcription of Unicode text. It also provides additional support for inser-tion, modification, and searching of data for direct access to the needed attributes and their annotated information.

The Unique Id configuration of each handwritten form is as follows: (1) The file name is the concatenation of the language (2 bits), category (3 bits), (2) The index of the form id is 16 bits: Total number of forms (maximum) = 2 16 = (3) There can be a maximum of eight categories, and hence 2 , 048 forms in each cate-(4) The structures reserves 2 bits for language to further extend and support other
To achieve the automatic consistency checking throughout the database, all the handwritten text images stored in the database get the same unique id that was gen-erated during the auto-indexing. The UID of each uploaded image was automatically indexed according to the selected language, category, and subcategory as shown in Figure 7. At the time of insertion of a new form, the user selects a particular script language and category of the handwritten text form, and the id field is appended ac-cordingly, For example, for the Urdu script and Literature category, UID of a form will be URD-L-GS-005, as shown in Figure 8. Automatic indexing is also applicable for the UID of the segmented lines and words of the handwritten image that is the extension of the form UID with a symbol of -. According to the image ID, the line UID is automatically generated. Similarly, according to the line UID, the word UID is automatically generated. For example, the first image of the Literature category and Poetry/Religion subcategory of the database is named as URD-L-PR-001. The images are stored in PNG format, so the first image file of the database has the name of URD-L-PR-001.PNG.
 The structure provides the functionality of mapping the accurate location of hand-written texts in the corresponding scanned images, lines, words, and ligatures. These textual region coordinates are conversely indexed in the database as well as in the XML file. That is useful for proper benchmarking of segmentation techniques for hand-written text recognition. Selected segmented images of lines and words are stored in a separate folder, while all the manually entered ground-truth transcription data of images, lines, and words are directed toward the respective fields in the database. A bounding box is displayed over the selected textual region for better visibility, so that one can recognize the path of the image components. A mapping has been done for the window screen and the viewport. When the cursor points at the unique id of lines, words, and ligatures, a rectangular bounding box appears on the corresponding image of the line, word, or ligature in the viewport. A sample of the structural markup of a handwritten image is shown in Figure 8.

Visualization of the image and corresponding information on the same viewport makes it useful for validation of context information and its visual review of annotated data. As a result, we create a database using this structure where all information stored in the database and images of text pages, segmented lines, and words are stored separately in the system with their corresponding UID as the name of the image in PNG format.

Validation checks are crucial to maintain the integrity of any database structure and is also helpful in ensuring the system operates on clean, correct, and useful data. They are equipped in our corpus by using auto-indexing and cross-indexing routines using validation and data normalization rules. In a nutshell, data needs to be validated at the same stage/level where it is most likely to be erroneous. The different types of data validations applied are form-level validation, search criteria validation, field-level validation, and range validation for every field. An Extensible Mark-up Language (XML)-based set of rules was used for encoding documents in a format that is both human readable and machine readable, as XML provides a standard representation that is logically related in a hierarchical way that is better suited for document analysis tasks. An XML is the most commonly used file format to generate ground-truth annotation results of the corpus. CALAM provides the functionality of creating an XML representation based on the data entry description for each handwritten text form of the database. The user can select an image to generate a corresponding XML formatted file and then download or directly view the XML file of that image.

The heart of the CALAM is the image database that includes 1 , 200 scanned images of handwritten sentences. In addition to image files, each image is accompanied by a rich XML metainformation file that is encoded at five levels of hierarchical metain-formation as shown in Table VII. There is a hierarchical record in each XML file for categorization of handwritten text image data into different levels such as lines, words, and components to describe its specification. The XML schema also encapsulates writ-ers X  demographic information like name, age, education, and address as other elements of the corpus. Standard CES (Character Encoding Scheme) [Ide 998b] under the guide-lines of TEI (Text Encoding Initiative) [Sthrenberg 2012] is used for electronic data encoding and an XML file X  X  metainformation.

As a result, the structure generates an XML file for each text page including the data information of lines, words, and ligatures of the respective page, based on the data entries. The XML file contains the same information as was provided dur-ing the data entries (with a five-level hierarchy, suffixed with UID). For example, the XML file obtained for the handwritten form with UID  X  X RD-U-UA-001.png X  will be  X  X RD-U-UA-001.xml. X  The comparative analysis of the proposed corpus CALAM with existing structures like Pix Labeler [Saund et al. 2009], GTLC [Yin et al. 2009], Truthing Tool [Elliman and Sherkat 2001], APTI [Slimane et al. 2009], MAST [Kasar et al. 2011], and LabelMe [Russell et al. 2008] for a handwritten text image corpus is illustrated in Table VIII.
The comparative analysis of Table VIII shows the functionality of the existing tools of annotation. PixLabler and Truthing Tools provide a way to annotate English-language documents. APTI and GTLC are available for offline handwritten document annota-tion in Arabic and Chinese scripts, respectively. APTI has been designed to annotate handwritten images excluding lines X  and words X  annotation. MAST and LabelMe were designed for annotation of camera-based images. LabelMe provides the functionality of object recognition in a scene image, and MAST can be used for annotation of multiscript scenic images for printed text.

Compared to the previous structures, CALAM provides the display of the handwrit-ten Urdu text image file and the transcription material of the corresponding image on the same screen in a collaboration context. CALAM is a simple way for annota-tion and collection of a large volume of information for Urdu script, such as digits, paragraphs, lines, words, machine-printed text, and handwritten text on the same platform. CALAM automatically generates an XML file of annotated metainformation that would be useful to ground truth of the image (bounding box coordinates of lines, words, and ligatures) for benchmarking and evaluation of various OCR techniques like segmentation and handwritten text recognition. All the structural markups are done with the pixel-level precision.

The corpus structure can be used for different classification criteria as required in multidisciplinary research, such as searching, filtering, statistics analysis on data, and the study of data distribution in terms of sex, name, education, region, domain, and other parameters. To strengthen our claim for the applicability of the proposed dataset for Urdu linguistic resources, we have also conducted the experimentations of some handwritten text segmentation algorithms and the Zipf X  X  [Piantadosi 2014] test on the dataset to observe the behavior of the word frequency distribution. To provide insight to other researchers for evaluation and comparison of their results of text-line segmentation/recognition techniques on the proposed dataset, we have tested four different text-line segmentation algorithms on the CALAM dataset. Each tech-nique was tested on 400 images taken from the proposed CALAM Urdu handwritten dataset [Choudhary et al. 2015]. We have selected 200 and 100 images from the News and Politics categories, respectively, and the remaining 100 images were a combination of the first 25 images from each of the four categories. (1) We tested a technique proposed by Alaei et al. [2011b] to segment handwritten text (2) The second technique tested is proposed by Godara et al. [2014] for handwritten (3) The third technique used in our experimentation has been proposed by Khanduja (4) Panwar et al. [2013] and Panwar and Nain [2014] proposed a line segmentation To strengthen our claim for the applicability of the proposed dataset for Urdu linguistic resources, we have also conducted the Zipf X  X  [Piantadosi 2014] test on the dataset to ascertain that it caters to the universality of a language principle. In 1949, Zipf [Piantadosi 2014] proposed a rule to analyze the distribution and behavior of words in a corpus that is significant in statistical linguistics analysis. According to Piantadosi [2014], every natural language follows Zipf X  X  rule for the frequency distribution of words. Zipf X  X  rule states that if f is the frequency of a word in a corpus and r is the rank of the word, then the frequency of words in a large corpus of natural language is inversely proportional to the rank of words as shown in Equation (2):
Zipf X  X  rule states that if words are arranged from the corpus in descending order of frequency ( w 1 ,w 2 ,...,w n ), then the occurrence frequency of the second word w 2 is w 1 2 , half times the first word w 1 , and the third word w 3 occurred roughly w 1 3 , one-third as often as the first word, and so on.

From this, it can be concluded that with the multiplication of the rank of a word r (rank one being the most frequent) by its frequency f (how many times the word occurred in the text), the product C would remain approximately the same for each word as shown in Equation (3):
From Equation (3), we can derive a generalization of this rule stating that the frequency of words decreases very rapidly with rank. This can also be written as Equation (4): By taking the log of Equation (4), we get Equation (5): where k = X  1and C is a constant. So a log( f ) and log( r ) graph drawn between frequency and rank of a corpus must be linear with slope as  X  1. Figure 9 shows the Zipf X  X  curve for the proposed Urdu corpus words. The resultant log( f ) and log( r ) Zipf X  X  curve graph validates that the proposed corpus follows Zipf X  X  rule for frequency distribution of words.
 In this article, we have presented an Urdu handwritten text image corpus CALAM along with its annotation structure with pixel-level precision. The uniformity of the structure provides an appropriate way for annotation of handwritten text images. The balancing in the data collection stage makes the corpus useful for researchers to control the proportion of values according to different usages of the corpus. We described an XML-based handwritten text image corpus and the annotation methodology that has the potential to provide researchers all the facilities for document image processing research, on a single platform, such as writer identification; signature verification; segmentation/recognition of text pages at line, word, and ligature levels; and separation of handwritten and printed texts. The database would be helpful in the design of an automatic intelligent system for direct processing of massive handwritten forms collected for census data.

Also, it can be very widely used for language transcription and transliteration appli-cations acting as an information exchange center. To date, only two datasets are avail-able for handwritten Urdu script. The aim of this work is to build a resource that would provide ground-truth annotation for handwritten text images. We propose floating the dataset as an open source on cloud storage free for academic use, where permissions for usage would be given on request.

