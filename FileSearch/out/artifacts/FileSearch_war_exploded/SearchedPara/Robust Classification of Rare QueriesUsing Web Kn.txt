 We propose a methodology for building a practical robust query classification system that can identify thousands of query classes with reasonable accuracy, while dealing in real-time with the query volume of a commercial web search en-gine. We use a blind feedback technique: given a query, we determine its topic by classifying the web search results retrieved by the query. Motivated by the needs of search ad-vertising, we primarily focus on rare queries, which are the hardest from the point of view of machine learning, yet in ag-gregation account for a considerable fraction of search engine traffic. Empirical evaluation confirms that our methodology yields a considerably higher classification accuracy than pre-viously reported. We believe that the proposed methodology will lead to better matching of online ads to rare queries and overall to a better user experience.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  relevance feedback, search process Algorithms, Measurement, Performance, Experimentation Query classification, Web search, blind relevance feedback
In its 12 year lifetime, web search had grown tremen-dously: it has simultaneously become a factor in the daily life of maybe a billion people and at the same time an eight billion dollar industry fueled by web advertising. One thing, however, has remained constant: people use very short queries. Various studies estimate the average length of a search query between 2.4 and 2.7 words, which by all ac-counts can carry only a small amount of information. Com-mercial search engines do a remarkably good job in interpret-ing these short strings, but they are not (yet!) omniscient. Therefore, using additional external knowledge to augment Copyright 2007 ACM 978-1-59593-597-7/07/0007 ... $ 5.00. the queries can go a long way in improving the search results and the user experience.

At the same time, better understanding of query mean-ing has the potential of boosting the economic underpinning of Web search, namely, online advertising, via the sponsored search mechanism that places relevant advertisements along-side search results. For instance, knowing that the query  X  X D450 X  is about cameras while  X  X c4200 X  is about laptops can obviously lead to more focused advertisements even if no advertiser has specifically bidden on these particular queries.
In this study we present a methodology for query classifi-cation , where our aim is to classify queries onto a commercial taxonomy of web queries with approximately 6000 nodes. Given such classifications, one can directly use them to pro-vide better search results as well as more focused ads. The problem of query classification is extremely difficult owing to the brevity of queries. Observe, however, that in many cases a human looking at a search query and the search query re-sults does remarkably well in making sense of it. Of course, the sheer volume of search queries does not lend itself to human supervision, and therefore we need alternate sources of knowledge about the world. For instance, in the example above,  X  X D450 X  brings pages about Canon cameras, while  X  X c4200 X  brings pages about Compaq laptops, hence to a human the intent is quite clear.

Search engines index colossal amounts of information, and as such can be viewed as very comprehensive repositories of knowledge. Following the heuristic described above, we pro-pose to use the search results themselves to gain additional insights for query interpretation. To this end, we employ the pseudo relevance feedback paradigm, and assume the top search results to be relevant to the query. Certainly, not all results are equally relevant, and thus we use elab-orate voting schemes in order to obtain reliable knowledge about the query. For the purpose of this study we first dis-patch the given query to a general web search engine, and collect a number of the highest-scoring URLs. We crawl the Web pages pointed by these URLs, and classify these pages. Finally, we use these result-page classifications to classify the original query. Our empirical evaluation confirms that using Web search results in this manner yields substantial improvements in the accuracy of query classification.
Note that in a practical implementation of our method-ology within a commercial search engine, all indexed pages can be pre-classified using the normal text-processing and indexing pipeline. Thus, at run-time we only need to run the voting procedure, without doing any crawling or classi-fication. This additional overhead is minimal, and therefore the use of search results to improve query classification is entirely feasible in run-time.

Another important aspect of our work lies in the choice of queries. The volume of queries in today X  X  search engines follows the familiar power law, where a few queries appear very often while most queries appear only a few times. While individual queries in this long tail are rare, together they account for a considerable mass of all searches. Furthermore, the aggregate volume of such queries provides a substantial opportunity for income through on-line advertising. 1
Searching and advertising platforms can be trained to yield good results for frequent queries, including auxiliary data such as maps, shortcuts to related structured informa-tion, successful ads, and so on. However, the  X  X ail X  queries simply do not have enough occurrences to allow statistical learning on a per-query basis. Therefore, we need to aggre-gate such queries in some way, and to reason at the level of aggregated query clusters. A natural choice for such ag-gregation is to classify the queries into a topical taxonomy. Knowing which taxonomy nodes are most relevant to the given query will aid us to provide the same type of support for rare queries as for frequent queries. Consequently, in this work we focus on the classification of rare queries, whose correct classification is likely to be particularly beneficial.
Early studies in query interpretation focused on query augmentation through external dictionaries [22]. More re-cent studies [18, 21] also attempted to gather some ad-ditional knowledge from the Web. However, these stud-ies had a number of shortcomings, which we overcome in this paper. Specifically, earlier works in the field used very small query classification taxonomies of only a few dozens of nodes, which do not allow ample specificity for online ad-vertising [11]. They also used a separate ancillary taxonomy for Web documents, so that an extra level of indirection had to be employed to establish the correspondence between the ancillary and the main taxonomies [18].

The main contributions of this paper are as follows. First, we build the query classifier directly for the target taxon-omy, instead of using a secondary auxiliary structure; this greatly simplifies taxonomy maintenance and development. The taxonomy used in this work is two orders of magni-tude larger than that used in prior studies. The empiri-cal evaluation demonstrates that our methodology for us-ing external knowledge achieves greater improvements than those previously reported. Since our taxonomy is consider-ably larger, the classification problem we face is much more difficult, making the improvements we achieve particularly notable. We also report the results of a thorough empiri-cal study of different voting schemes and different depths of knowledge (e.g., using search summaries vs. entire crawled pages). We found that crawling the search results yields deeper knowledge and leads to greater improvements than mere summaries. This result is in contrast with prior find-ings in query classification [20], but is supported by research in mainstream text classification [5].
Our methodology has two main phases. In the first phase,
In the above examples,  X  X D450 X  and  X  X c4200 X  represent fairly old gadget models, and hence there are advertisers placing ads on these queries. However, in this paper we mainly deal with rare queries which are extremely difficult to match to relevant ads. we construct a document classifier for classifying search re-sults into the same taxonomy into which queries are to be classified. In the second phase, we develop a query classifier that invokes the document classifier on search results, and uses the latter to perform query classification.
In this work we used a commercial classification taxonomy of approximately 6000 nodes used in a major US search en-gine (see Section 3.1). Human editors populated the taxon-omy nodes with labeled examples that we used as training instances to learn a document classifier in phase 1.
Given a taxonomy of this size, the computational effi-ciency of classification is a major issue. Few machine learn-ing algorithms can efficiently handle so many different clas-ses, each having hundreds of training examples. Suitable candidates include the nearest neighbor and the Naive Bayes classifier [3], as well as prototype formation methods such as Rocchio [15] or centroid-based [7] classifiers. A recent study [5] showed centroid-based classifiers to be both ef-fective and efficient for large-scale taxonomies and conse-quently, we used a centroid classifier in this work.
Having developed a document classifier for the query tax-onomy, we now turn to the problem of obtaining a classifi-cation for a given query based on the initial search results it yields. Let X  X  assume that there is a set of documents D = d 1 . . . d m indexed by a search engine. The search engine can then be represented by a function ~ f = similarity ( q, d ) that quantifies the affinity between a query q and a docu-ment d . Examples of such affinity scores used in this paper are rank  X  X he rank of the document in the ordered list of search results; static score  X  X he score of the goodness of the page regardless of the query (e.g., PageRank); and dy-namic score  X  X he closeness of the query and the document.
Query classification is determined by first evaluating con-ditional probabilities of all possible classes P ( C j | q ), and then selecting the alternative with the highest probability C max = arg max C j  X  C P ( C j | q ). Our goal is to estimate the conditional probability of each possible class using the search results initially returned by the query. We use the following formula that incorporates classifications of individual search results: P ( C j | q ) =
We assume that P ( q | C j , d )  X  P ( q | d ), that is, a probabil-ity of a query given a document can be determined without knowing the class of the query. This is the case for the majority of queries that are unambiguous. Counter exam-ples are queries like  X  X aguar X  (animal and car brand) or  X  X p-ple X  (fruit and computer manufacturer), but such ambigu-ous queries can not be classified by definition , and usually consists of common words. In this work we concentrate on rare queries, that tend to contain rare words, be longer, and match fewer documents; consequently in our setting this as-sumption mostly holds. Using this assumption, we can write P ( C j | q ) = ability of a classification for a given document P ( C j | d ) is estimated using the output of the document classifier (sec-tion 2.1). While P ( d | q ) is harder to compute, we consider the underlying relevance model for ranking documents given a query. This issue is further explored in the next section.
In order to describe a formal relationship of classification and ad placement (or search), we consider a model for using classification to determine ads (or search) relevance. Let a be an ad and q be a query, we denote by R ( a, q ) the relevance of a to q . This number indicates how relevant the ad a is to query q , and can be used to rank ads a for a given query q . In this paper, we consider the following approximation of relevance function:
The right hand-side expresses how we use the classifica-tion scheme C to rank ads, where s ( c, a ) is a scoring function that specifies how likely a is in class c , and s ( c, q ) is a scor-ing function that specifies how likely q is in class c . The value w ( c ) is a weighting term for category c , indicating the importance of category c in the relevance formula.
This relevance function is an adaptation of the traditional word-based retrieval rules. For example, we may let cate-gories be the words in the vocabulary. We take s ( C j , a ) as the word counts of C j in a , s ( C j , q ) as the word counts of C j in q , and w ( C j ) as the IDF term weighting for word C With such choices, the method given by (1) becomes the standard TFIDF retrieval rule.

If we take s ( C j , a ) = P ( C j | a ), s ( C j , q ) = P ( C w ( C j ) = 1 /P ( C j ), and assume that q and a are indepen-dently generated given a hidden concept C , then we have
That is, the ads are ranked according to P ( q | a ). This rel-evance model has been employed in various statistical lan-guage modeling techniques for information retrieval. The in-tuition can be described as follows. We assume that a person searches an ad a by constructing a query q : the person first picks a concept C j according to the weights P ( C j | a ), and then constructs a query q with probability P ( q | C j ) based on the concept C j . For this query generation process, the ads can be ranked based on how likely the observed query is generated from each ad.

It should be mentioned that in our case, each query and ad can have multiple categories. For simplicity, we denote by C a random variable indicating whether q belongs to category C . We use P ( C j | q ) to denote the probability of q belonging to category C j . Here the sum to one. We then consider the following ranking formula:
We assume the estimation of P ( C j | a ) is based on an existing text-categorization system (which is known). Thus, we only need to obtain estimates of P ( C j | q ) for each query q .
Equation (2) is the ad relevance model that we consider in this paper, with unknown parameters P ( C j | q ) for each query q . In order to obtain their estimates, we use search results from major US search engines, where we assume that the ranking formula in (2) gives good ranking for search. That is, top results ranked by search engines should also be ranked high by this formula. Therefore given a query q , and top K result pages d 1 ( q ) , . . . , d K ( q ) from a major search engine, we for i = 1 , . . . , K . It is worth mentioning that using this method we can only compute relative strength of P ( C j | q ), but not the scale, because scale does not affect ranking. Moreover, it is possible that the parameters estimated may be of the form g ( P ( C j | q )) for some monotone function g (  X  ) of the actually conditional probability g ( P ( C j | q )). Although this may change the meaning of the unknown parameters that we estimate, it does not affect the quality of using the formula to rank ads. Nor does it affect query classification with appropriately chosen thresholds. In what follows, we consider two methods to compute the classification informa-tion P ( C j | q ).
We would like to compute P ( C j | q ) so that R C ( d i ( q ) , q ) are high for i = 1 , . . . , K and R C ( d, q ) are low for a ran-dom document d . Assume that the vector [ P ( C j | d )] C random for an average document, then the condition that P averaged over d . Thus, a natural method is to maximize P i =1 w i R C ( d i ( q ) , q ) subject to small, where w i are weights associated with each rank i : where we assume regularization parameter. The optimal solution is just take  X  = 0 . 5 to align the scale. In the experiment, we will simply take uniform weights w i . A more complex strategy is to let w depend on d as well: where g ( x ) is a certain transformation of x .

In this general formulation, w ( d, q ) may depend on factors other than the rank of d in the search engine results for q . For example, it may be a function of r ( d, q ) where r ( d, q ) is the relevance score returned by the underlying search en-gine. Moreover, if we are given a set of hand-labeled training category/query pairs ( C, q ), then both the weights w ( d, q ) and the transformation g (  X  ) can be learned using standard classification techniques.
We can treat the problem of estimating P ( C j | q ) as a classification problem, where for each q , we label d i ( q ) for i = 1 , . . . , K as positive data, and the remaining documents as negative data. That is, we assign label y i ( q ) = 1 for d when i  X  K , and label y i ( q ) =  X  1 for d i ( q ) when i &gt; K .
In this setting, the classification scoring rule for a docu-ment d i ( q ) is linear. Let x i ( q ) = [ P ( C j | d i [ P ( C j | q )], then values P ( C j | d ) are the features for the linear classifier, and [ P ( C j | d )] is the weight vector, which can be computed us-ing any linear classification method. In this paper, we con-sider estimating w using logistic regression [17] as follows: P (  X | q ) = arg min w In this section, we evaluate our methodology that uses Web search results for improving query classification.
Our choice of taxonomy was guided by a Web advertis-ing application. Since we want the classes to be useful for matching ads to queries, the taxonomy needs to be elabo-rate enough to facilitate ample classification specificity. For example, classifying all medical queries into one node will likely result in poor ad matching, as both  X  X ore foot X  and  X  X lu X  queries will end up in the same node. The ads appro-priate for these two queries are, however, very different. To avoid such situations, the taxonomy needs to provide suf-ficient discrimination between common commercial topics. Therefore, in this paper we employ an elaborate taxonomy of approximately 6000 nodes, arranged in a hierarchy with median depth 5 and maximum depth 9. Figure 1 shows the distribution of categories by taxonomy levels. Human edi-tors populated the taxonomy with labeled queries (approx. 150 queries per node), which were used as a training set; a small fraction of queries have been assigned to more than one category.
To discuss our set of evaluation queries, we need a brief in-troduction to some basic concepts of Web advertising. Spon-sored search (or paid search ) advertising is placing textual ads on the result pages of web search engines, with ads be-ing driven by the originating query. All major search en-gines (Google, Yahoo!, and MSN) support such ads and act simultaneously as a search engine and an ad agency. These textual ads are characterized by one or more  X  X id phrases X  representing those queries where the advertisers would like to have their ad displayed. (The name  X  X id phrase X  comes from the fact that advertisers bid various amounts to secure their position in the tower of ads associated to a query. A discussion of bidding and placement mechanisms is beyond the scope of this paper [13].

However, many searches do not explicitly use phrases that someone bids on. Consequently, advertisers also buy  X  X road X  matches, that is, they pay to place their advertisements on queries that constitute some modification of the desired bid phrase. In broad match, several syntactic modifications can be applied to the query to match it to the bid phrase, e.g., dropping or adding words, synonym substitution, etc. These transformations are based on rules and dictionaries. As advertisers tend to cover high-volume and high-revenue queries, broad-match queries fall into the tail of the distri-bution with respect to both volume and revenue.
We used two representative sets of 1000 queries. Both sets contain queries that cannot be directly matched to adver-tisements, that is, none of the queries contains a bid phrase (this means we eliminated practically all popular queries).
The first set of queries can be matched to at least one ad using broad match as described above. Queries in the second set cannot be matched even by broad match, and therefore the search engine used in our study does not currently dis-play any advertising for them. In a sense, these are even more rare queries and further away from common queries. As a measure of query rarity, we estimated their frequency in a month worth of query logs for a major US search en-gine; the median frequency was 1 for queries in Set 1 and 0 for queries in Set 2.

The queries in the two sets differ in their classification difficulty. In fact, queries in Set 2 are difficult to interpret even for human evaluators. Queries in Set 1 have on average 3.50 words, with the longest one having 11 words; queries in Set 2 have on average 4.39 words, with the longest query of 81 words. Recent studies estimate the average length of web queries to be just under 3 words 2 , which is lower than in our test sets. As another measure of query difficulty, we measured the fraction of queries that contain quotation marks, as the latter assist query interpretation by meaning-fully grouping the words. Only 8% queries in Set 1 and 14% in Set 2 contained quotation marks.
The two sets of queries were classified into the target tax-onomy using the techniques presented in section 2. Based on the confidence values assigned, the top 3 classes for each query were presented to human evaluators. These evalu-ators were trained editorial staff who possessed knowledge about the taxonomy. The editors considered every query-class pair, and rated them on the scale 1 to 4, with 1 mean-ing the classification is highly relevant and 4 meaning it is irrelevant for the query. About 2.4% queries in Set 1 and 5.4% queries in Set 2 were judged to be unclassifiable (e.g., random strings of characters), and were consequently ex-cluded from evaluation. To compute evaluation metrics, we treated classifications with ratings 1 and 2 to be correct, and those with ratings 3 and 4 to be incorrect.
 We used standard evaluation metrics: precision, recall and F1. In what follows, we plot precision-recall graphs for all the experiments. For comparison with other published stud-ies, we also report precision and F1 values corresponding to complete recall ( R = 1). Owing to the lack of space, we only show graphs for query Set 1; however, we show the numerical results for both sets in the tables.
We compared our method to a baseline query classifier that does not use any external knowledge. Our baseline classifier expanded queries using standard query expansion techniques, grouped their terms using a phrase recognizer, boosted certain phrases in the query based on their sta-tistical properties, and performed classification using the nearest-neighbor approach. This baseline classifier is actu-ally a production version of the query classifier running in a major US search engine.

In our experiments, we varied values of pertinent parame-ters that characterize the exact way of using search results. In what follows, we start with the general assessment of the effect of using Web search results. We then proceed to ex-ploring more refined techniques, such as using only search summaries versus actually crawling the returned URLs. We also experimented with using different numbers of search results per query, as well as with varying the number of classifications considered for each search result. For lack of space, we only show graphs for Set 1 queries and omit the graphs for Set 2 queries, which exhibit similar phenomena.
Queries by themselves are very short and difficult to clas-sify. We use top search engine results for collecting back-ground knowledge for queries. We employed two major US search engines, and used their results in two ways, either only summaries or the full text of crawled result pages. Figure 2 and Table 1 show that such extra knowledge con-siderably improves classification accuracy. Interestingly, we found that search engine A performs consistently better with full-page text, while search engine B performs better when summaries are used.

Table 1: The effect of using external knowledge
There are two major ways to use search results as addi-tional knowledge. First, individual results can be classified separately, with subsequent voting among individual clas-sifications. Alternatively, individual search results can be bundled together as one meta-document and classified as such using the document classifier. Figure 3 presents the results of these two approaches When full-text pages are used, the technique using individual classifications of search results evidently outperforms the bundling approach by a wide margin. However, in the case of summaries, bundling together is found to be consistently better than individual classification. This is because summaries by themselves are too short to be classified correctly individually, but when bundled together they are much more stable.
To summarize the two preceding sections, background knowledge for each query is obtained by using either the full-page text or only the summaries of the top search re-sults. Full page text was found to be more in conjunction with voted classification, while summaries were found to be useful when bundled together. The best results overall were obtained with full-page results classified individually, with subsequent voting used to determine the final query classifi-cation. This observation differs from findings by Shen et al. [20], who found summaries to be more useful. We attribute this distinction to the fact that the queries we used in this study are tail ones, which are rare and difficult to classify.
We also varied the number of classifications per search re-sult, i.e., each result was permitted to have either 1, 3, or 5 classes. Figure 4 shows the corresponding precision-recall graphs for both full-page and summary-only settings. As can be readily seen, all three variants produce very similar re-sults. However, the precision-recall curve for the 1-class ex-periment has higher fluctuations. Using 3 classes per search result yields a more stable curve, while with 5 classes per result the precision-recall curve is very smooth. Thus, as we increase the number of classes per result, we observe higher stability in query classification.
We also experimented with different numbers of search results per query. Figure 5 and Table 2 present the results of this experiment. In line with our intuition, we observed that classification accuracy steadily rises as we increase the number of search results used from 10 to 40, with a slight drop as we continue to use even more results (50). This is because using too few search results provides too little external knowledge, while using too many results introduces extra noise.
 Using paired t -test, we assessed the statistical significance
Figure 4: Varying the number of classes per page Figure 5: Varying the number of results per query of the improvements due to our methodology versus the baseline. We found the results to be highly significant ( p &lt; 0 . 0005), thus confirming the value of external knowledge for query classification.
As explained in Section 2.2, one may use several methods to classify queries from search engine results based on our relevance model. As we have seen, the voting method works quite well. In this section, we compare the performance of voting top-ten search results to the following two methods:
Method B requires a training/testing split. Neither voting nor method A requires such a split; however, for consistency, we randomly draw 50-50 training/testing splits for ten times, and report the mean performance  X  standard deviation on the test-split for all three methods. For this experiment, instead of precision and recall, we use DCG-k ( k = 1 , 5), popular in search engine evaluation. The DCG (discounted cumulated gain) metric, described in [8], is a ranking mea-sure where the system is asked to rank a set of candidates (in our case, judged categories for each query), and computes for each query q : DCG k ( q ) = where C i ( q ) is the i -th category for query q ranked by the system, and g ( C i ) is the grade of C i : we assign grade of 10 , 5 , 1 , 0 to the 4-point judgment scale described earlier to compute DCG. The decaying choice of log 2 ( i + 1) is con-ventional, which does not have particular importance. The overall DCG of a system is the averaged DCG over queries. We use this metric instead of precision/recall in this ex-periment because it can directly handle multi-grade output. Therefore as a single metric, it is convenient for comparing the methods. Note that precision/recall curves used in the earlier sections yield some additional insights not immedi-ately apparent from the DCG numbers.

Results from our experiments are given in Table 3. The oracle method is the best ranking of categories for each query after seeing human judgments. It cannot be achieved by any realistic algorithm, but is included here as an absolute up-per bound on DCG performance. The simple voting method performs very well in our experiments. The more com-plicated methods may lead to moderate performance gain (especially method A, which uses discriminative training in Section 2.5). However, both methods are computationally more costly, and the potential gain is minor enough to be neglected. This means that as a simple method, voting is quite effective.

We can observe that method B, which uses quality score returned by a search engine to adjust importance weights of returned pages for a query, does not yield appreciable improvement. This implies that putting equal weights (vot-ing) performs similarly as putting higher weights to higher quality documents and lower weights to lower quality doc-uments (method B), at least for the top search results. It may be possible to improve this method by including other page-features that can differentiate top-ranked search re-sults. However, the effectiveness will require further inves-tigation which we did not test. We may also observe that the performance on Set 2 is lower than that on Set 1, which means queries in Set 2 are harder than those in Set 1.
We scrutinized the cases when external knowledge did not improve query classification, and identified three main causes for such lack of improvement. (1)Queries containing random strings, such as telephone numbers  X  these queries do not yield coherent search results, and so the latter cannot help classification (around 5% of queries were of this kind). (2) Queries that yield no search results at all; there were 8% such queries in Set 1 and 15% in Set 2. (3) Queries corre-sponding to recent events, for which the search engine did not yet have ample coverage (around 5% of queries). One notable example of such queries are entire names of news articles X  X f the exact article has not yet been indexed by the search engine, search results are likely to be of little use.
Even though the average length of search queries is steadi-ly increasing over time, a typical query is still shorter than 3 words. Consequently, many researchers studied possible ways to enhance queries with additional information.
One important direction in enhancing queries is through query expansion. This can be done either using electronic dictionaries and thesauri [22], or via relevance feedback tech-niques that make use of a few top-scoring search results. Early work in information retrieval concentrated on manu-ally reviewing the returned results [16, 15]. However, the sheer volume of queries nowadays does not lend itself to manual supervision, and hence subsequent works focused on blind relevance feedback, which basically assumes top returned results to be relevant [23, 12, 4, 14].

More recently, studies in query augmentation focused on classification of queries, assuming such classifications to be beneficial for more focused query interpretation. Indeed, Kowalczyk et al. [10] found that using query classes im-proved the performance of document retrieval.

Studies in the field pursue different approaches for ob-taining additional information about the queries. Beitzel et al. [1] used semi-supervised learning as well as unlabeled data [2]. Gravano et al. [6] classified queries with respect to geographic locality in order to determine whether their intent is local or global.

The 2005 KDD Cup on web query classification inspired yet another line of research, which focused on enriching queries using Web search engines and directories [11, 18, 20, 9, 21]. The KDD task specification provided a small taxon-omy (67 nodes) along with a set of labeled queries, and posed a challenge to use this training data to build a query classi-fier. Several teams used the Web to enrich the queries and provide more context for classification. The main research questions of this approach the are (1) how to build a doc-ument classifier, (2) how to translate its classifications into the target taxonomy, and (3) how to determine the query class based on document classifications.

The winning solution of the KDD Cup [18] proposed us-ing an ensemble of classifiers in conjunction with searching multiple search engines. To address issue (1) above, their so-lution used the Open Directory Project (ODP) to produce an ODP-based document classifier. The ODP hierarchy was then mapped into the target taxonomy using word matches at individual nodes. A document classifier was built for the target taxonomy by using the pages in the ODP tax-onomy that appear in the nodes mapped to the particular target node. Thus, Web documents were first classified with respect to the ODP hierarchy, and their classifications were subsequently mapped to the target taxonomy for query clas-sification.

Compared to this approach, we solved the problem of doc-ument classification directly in the target taxonomy by us-ing the queries to produce document classifier as described in Section 2. This simplifies the process and removes the need for mapping between taxonomies. This also stream-lines taxonomy maintenance and development. Using this approach, we were able to achieve good performance in a very large scale taxonomy. We also evaluated a few alter-natives how to combine individual document classifications when actually classifying the query.

In a follow-up paper [19], Shen et al. proposed a frame-work for query classification based on bridging between two taxonomies. In this approach, the problem of not having a document classifier for web results is solved by using a training set available for documents with a different taxon-omy. For this, an intermediate taxonomy with a training set (ODP) is used. Then several schemes are tried that estab-lish a correspondence between the taxonomies or allow for mapping of the training set from the intermediate taxonomy to the target taxonomy. As opposed to this, we built a doc-ument classifier for the target taxonomy directly, without using documents from an intermediate taxonomy. While we were not able to directly compare the results due to the use of different taxonomies (we used a much larger taxonomy), our precision and recall results are consistently higher even over the hardest query set.
Query classification is an important information retrieval task. Accurate classification of search queries is likely to benefit a number of higher-level tasks such as Web search and ad matching. Since search queries are usually short, by themselves they usually carry insufficient information for ad-equate classification accuracy. To address this problem, we proposed a methodology for using search results as a source of external knowledge. To this end, we send the query to a search engine, and assume that a plurality of the highest-ranking search results are relevant to the query. Classifying these results then allows us to classify the original query with substantially higher accuracy.

The results of our empirical evaluation definitively con-firmed that using the Web as a repository of world knowl-edge contributes valuable information about the query, and aids in its correct classification. Notably, our method ex-hibits significantly higher accuracy than methods described in prior studies 3 Compared to prior studies, our approach does not require any auxiliary taxonomy, and we produce a query classifier directly for the target taxonomy. Fur-thermore, the taxonomy used in this study is approximately 2 orders of magnitude larger than that used in prior works.
We also experimented with different values of parameters that characterize our method. When using search results, one can either use only summaries of the results provided by
Since the field of query classification does not yet have es-tablished and agreed upon benchmarks, direct comparison of results is admittedly tricky.
 the search engine, or actually crawl the results pages for even deeper knowledge. Overall, query classification performance was the best when using the full crawled pages (Table 1). These results are consistent with prior studies [5], which found that using full crawled pages is superior for document classification than using only brief summaries. Our findings, however, are different from those reported by Shen et al. [19], who found summaries to yield better results. We attribute our observations to using a more elaborate voting scheme among the classifications of individual search results, as well as to using a more difficult set of rare queries. In this study we used two major search engines, A and B. Interestingly, we found notable distinctions in the quality of their output. Notably, for engine A the overall results were better when using the full crawled pages of the search re-sults, while for engine B it seems to be more beneficial to use the summaries of results. This implies that while the quality of search results returned by engine A is apparently better, engine B does a better work in summarizing the pages.
We also found that the best results were obtained by us-ing full crawled pages and performing voting among their individual classifications. For a classifier that is external to the search engine, retrieving full pages may be prohibitively costly, in which case one might prefer to use summaries to gain computational efficiency. On the other hand, for the owners of a search engine, full page classification is much more efficient, since it is easy to preprocess all indexed pages by classifying them once onto the (fixed) taxonomy. Then, page classifications are obtained as part of the meta-data associated with each search result, and query classification can be nearly instantaneous.

When using summaries it appears that better results are obtained by first concatenating individual summaries into a meta-document, and then using its classification as a whole. We believe the reason for this observation is that summaries are short and inherently noisier, and hence their aggregation helps to correctly identify the main theme. Consistent with our intuition, using too few search results yields useful but insufficient knowledge, and using too many search results leads to inclusion of marginally relevant Web pages. The best results were obtained when using 40 top search hits.
In this work, we first classify search results, and then use their classifications directly to classify the original query. Alternatively, one can use the classifications of search results as features in order to learn a second-level classifier. In Section 3.6, we did some preliminary experiments in this direction, and found that learning such a secondary classifier did not yield considerably advantages. We plan to further investigate this direction in our future work.

It is also essential to note that implementing our method-ology incurs little overhead. If the search engine classifies crawled pages during indexing, then at query time we only need to fetch these classifications and do the voting.
To conclude, we believe our methodology for using Web search results holds considerable promise for substantially improving the accuracy of Web search queries. This is par-ticularly important for rare queries, for which little per-query learning can be done, and in this study we proved that such scarceness of information could be addressed by leveraging the knowledge found on the Web. We believe our findings will have immediate applications to improving the handling of rare queries, both for improving the search results as well as yielding better matched advertisements. In our further research we also plan to make use of session information in order to leverage knowledge about previous queries to better classify subsequent ones.
