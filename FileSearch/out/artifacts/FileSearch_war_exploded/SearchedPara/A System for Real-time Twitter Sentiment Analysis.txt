 Social media platforms have become an important site for political conversations throughout the world. In the year leading up to the November 20 12 presidential election in the United States, we have developed a tool for real -time analysis of sentiment expressed through Twitter, a m icro -blogging service , toward the incumbent President, Barack Obama, and the nine republican challengers -four of who m remain in the running as of this writing. With this analysis, we seek to explore whether Twitter provides insights into the unfolding of the campaign s and indications of shifts in public opinion.

Twitter allows users to post tweets, messages of up to 140 character s , on its social network. Twitter usage is growing rapidly. The company reports over 100 million active users worldwide, together sending over 250 million tweets each day ( Twitter, 2012 ) . It was actively used by 13% of on -line American adults as of May 2011, up from 8% a year prior ( Pew Research Center, 2011 ) . More than two thirds of U.S. congress members have created a Twitter account and many are a ctively using Twitter to reach their constituents ( Lassen &amp; Brown, 2010 ; TweetCongress, 2012 ) . Since October 12, 2012, we have gathered over 36 million tweets ab out the 2012 U.S. presidential candidates , a quarter million per day on average. During one of the key political events, the Dec 15, 2011 primary debate in Iowa, we collected more than half a million relevant tweets in just a few hours . This kind of  X  X ig data X  vastly outpaces the capacity of traditional content analysis approaches, calling for novel computational approaches.

Most work to date has focused on post -facto analysis of tweets, with results coming days or even months after the collection time. H owever, because tweets are short and easy to send, they lend themselves to quick and dynamic expression of instant reactions to current events. We expect automated real -time sentiment analysis of this user -generated data can provide fast indications of cha nges in opinion, showing for example how an audience reacts to particular candidate  X  s statements during a political debate. The system we present here, along with the dashboards displaying analysis results with drill -down ability , is precisely aimed at ge nerating real -time insights as events unfold .
Beyond the sheer scale of the task and the need to keep up with a rapid flow of tweets, we had to address two additional issues. First, the vernacular used on T witter differs significantly from common language and we have trained our sentiment model on its idiosyncrasies. Second, tweets in general , and political tweets in particular , tend to be quite sarcastic, presenting significant challenges for computer models ( Gonz X lez -Ib X  X ez et al., 2011 ) . We will present our approaches to these issues in a separate publication . Here, we focus on presenting the overall system and the visualization dashboards we have built. In section 2, we begin with a review of related work; we then turn in section 3 to a description of our system X  X  architecture and its com ponents (input, preprocessing, sentiment model, result aggregation, and visualization); in section s 4 and 5 we evaluate our early experience with this system and discuss next steps. In the last decade, interest in mining sentiment and opinion s in text has grown rapidly, due in part to the large increase of the availability of documents and messages expressing personal opinions ( Pang &amp; Lee, 2008 ) . In particular, sentiment in T witter data has been used for prediction or measurement in a variety of domains, such as stock market, politics and social movements ( Bollen et al., 2011 ; Choy et al., 2011 ; Tumasjan et al., 2010 ; Zeitzoff, 2011 ) . For example, Tumasjan (2010) found tweet volume about the political parties to be a good predictor for the outcome of the 2009 German election, while Choy et al. (2011) failed to pr edict with Twitter sentiment the ranking of the four candidates in Singapore  X  X  2011 presidential election .
Past studies of political sentiment on social networks ha ve been either post -hoc and/or carried out on small and static sample s . To address these i ssues, w e built a unique infrastructure and sentiment model to analyze in real -time public sentiment on Twitter toward the 2012 U .S. presidential candidates . Our effort to gauge political sentiment is based on bringing together social science scholarship w ith advanced computational methodology: our approach combines real -time data processing and statistical sentiment model ing informed by, and contributing to, an understanding of the cultural and political practices at work through the use of Twitter. For accuracy and speed, we built our real -time data processing infrastructure on the IBM X  X  InfoSphere Streams platform ( IBM, 2012 ) , w hich enables us to write our own analysis and visualization modules and assemble them into a real -time processing pipeline. Streams applications are highly scalable so we can adjust our system to handle higher volume of data by adding more servers and by d istributing processing tasks. Twitter traffic often balloons during big events (e.g. televised debates or primary election days) and stays low between events, making high scalability strongly desirable. Figure 1 shows our system X  X  architecture and its modu les. Next, we introduce our data source and each individual module.
 3.1 Input/Data Source We chose the micro -blogging service Twitter as our data source because it is a major source of online political commentary and discussion in the U.S. People comment on an d discuss politics by posting messages and  X  X e -tweeting X  others X  messages . It played a significant role in political events worldwide, such as the Arab Spring Movement and the Moldovian protests in 2009 . In response to events, Twitter volume goes up sharpl y and significantly. For example, during a r epublican debate, we receive several hundred thousand to a million tweets in just a few hours for all the candidates combined.

Twitter X  X  public API provides only 1% or less of its entire traffic (the  X  X irehose X ), without control over the sampling procedure, which is likely insufficient for accurate analysis of public sentiment. Instead, we collect all relevant tweets in real -time from the entire Twitter traffic via Gnip Power Track, a commercial Twitter data provider . To cope with this challenge during the later stage s of the campaign, when larger Twitter traffic is expected, our system can handle huge traffic bursts over short time periods by distributing the processing t o more servers , even though most of the times its processing load is minimal.

Since our application target s the political domain (specifically the current Presidential election cycle), we manually construct rules that are simple logical keyword combination s to retrieve relevant tweets  X  those about candidates and events (including common typos in candidate names) . For example, our rules for Mitt Romney include Romney, @MittRomney, @PlanetRomney, @MittNews, @believeinromney, #romney, #mitt, #mittromney, and #mitt2012 . Our system is track ing the tweets for nine Republican candidates (some of whom have su s pended their campaign ) and Barack Obama using about 200 rules in total . 3.2 Preprocessing The text of tweets differs from the text in articles, books, or even spo ken language. It includes many idiosyncratic uses, such as emoticon s , URLs, RT for re -tweet, @ for user mentions, # for hashtags, and repetitions. It is necessary to preprocess and normalize the text.

As standard in NLP practices, the text is tokenized for later processing. We use certain rules to handle the special cases in tweets. We compa red several Twitter -specific tokenizers, such as T weet M otif ( O'Connor et al., 2010 ) and found Christopher Potts X  basic Twitter tokenizer best suited as our base. In summary, our tokenizer correctly handles URLs, common emoticons, phone numbers, HTML tags, twitter mention s and hashtag s , numbers with fractions and decimals, repetition of symbols and Unicode characters (see Figure 2 for an example). 3.3 Sentiment Model The design of the sentiment model used in our system was based on the assumption that the opinions expressed would be highly subjective and contextualize d. Therefore, for generating data for model training and testing, we used a crowd -sourcing approach to do sentiment annotation on in -domain political data.
 To create a baseline sentiment model, we used Amazon Mechanical Turk (AMT) to get as varied a popul ation of annotators as possible. We designed an interface that allowed annotators to perform the annotations outside of AMT so that they could participate anonymously. The Turkers were asked their age, gender, and to describe their political orientation. Then they were shown a series of tweets and asked to annotate the tweets' sentiment (positive, negative, neutral, or unsure), whether the tweet was sarcastic or humorous , the sentiment on a scale from positive to negative, and the tweet auth or's political orientation on a slider scale from conservative to liberal. Our sentiment model is based on the sentiment label and the sarcasm and humor labels. Our training data consists of nearly 17000 tweets ( 16% positive, 56% negative , 18% neutral, 10% unsure ) , including nearly 2000 that were multiply an notated to calculate inter -annotator agreement. About 800 Turkers contributed to our annotation.

T he statistical classifier we use for sentiment analysis is a na X ve Bayes model on unigram features. Our features are calculated from tokeni zation of the tweets that attempts to preserve punctuation that may signify sentiment (e.g., emoticons and exclamation points) as well as twitter specific phenomen a (e.g., extracting intact URLs). Based on the data we collected our classifier performs at 5 9% accuracy on the four category classification of negative, positive, neutral, or unsure. These results exceed the baseline of classifying all the data as negative, the most prevalent sentiment category (56%). The choice of our model was not strictly moti vated by global accuracy, but took into account class -wise performance so that the model performed well on each sentiment category. 3.4 Aggregation Because our system receives tweets continuously and us es multiple rules to track each candidate X  X  tweets, our display must aggregate sentiment and tweet volume with in each time period for each candidate. For volume, the system ou tputs the number of tweets every minute for each candidate. For sentiment, the system outputs the number of positive , negative , neutral and unsure tweets in a sliding five -minute window. 3.5 Display and Visualization
We designed an Ajax -based HTML dashboard ( Figure 3 ) to display volume and sentiment by candidate as well as trending words and system statistics. The dashboard pulls updated data from a web server and refresh es its display every 30 seconds. In Figure 3 , the top -left bar graph shows the number of p ositive and negative tweets about each candidate ( right and left bars, respectively) in the last five minutes as an indicator of sentiment towards the candidates. We chose to display both positive and negative sentiment, instead of the difference between t hese two, because events typically trigger sharp variation s in both positive and negative tweet volume. The top -right chart displays the number of tweets for each candidate every minute in the last two hours . We chose this time window because a live -broadc ast primary debate usually lasts about two hours. The bottom -left shows system statistics, including the total number of tweets, the number of seconds since system start and the average data rate. The bottom -right table shows trending words of the last fiv e minutes, computed using TF -IDF measure as follows: tweets about all candidates in a minute are treated as a single  X  X ocument X ; trending words are the tokens from the current minute with the highest TF -IDF weights when using the last two hours as a corpus ( i.e., 120  X  X ocuments X ). Qualitative examination suggests that the simple TF -IDF metric effectively identifies the most prominent words when an event occurs.

The dashboard gives a synthetic overview of volume and sentiment for the candidates, but it is of ten desirable to view selected tweets and their sentiments. The dashboard includes another page (Figure 4 ) that displays the most positive, negative and frequent tweets, as well as some random neutral tweets. It also shows the total volume over time and a tag cloud of the most frequent words in the last five minutes across all candidates. Another crucial feature of this page is that clicking on one of the tweets brings up an annotation interface, so the user can provide his/her own assessment of the sentime nt expressed in the tweet. The next section describes the annotation interface. 3.6 Annotation Interface The online annotation interface shown in Figure 5 lets dashboard (Figure 4 ) users provide their own judgment of a tweet. The tweet X  X  text is displayed at the top, and users can rate the sentiment toward the candidate mentioned in the tweet as positive , negative or neutral or mark it as unsure . There are also two options to specify whether a tweet is sarcastic and/ or funny . This interface is a simplified ver sion of the one we used to collect annotations from Amazon Mechanical Turk so that annotation can be performed quickly on a single tweet. The online interface is designed to be used while watching a campaign event and can be displayed on a tablet or smart phone.

The feedback from users allows annotation of recent data as well as the ability to correct misclassifications. As a future step, we plan to establish an online feedback loop between users and the sentiment model, so users X  judgment serves to train the model actively and iteratively. In Section 3.3, we described our preliminary sentiment model that automatically classifies tweets into four categories: positive, negative, neutral or unsure. It copes well with the negative bias in po litical tweets. In addition to evaluating the model using annotated data, we have also be gun conduct ing correlational analysis of aggregated sentiment with political events and news, as well as indicators such as poll and election results. We are explor ing whether variations in twitter sentiment and tweet volume are predictive or reflective of real -world events and news. While this quantitative analysis is part of ongoing work, we present below some quantitative and qualitative expert observations indicativ e of promising research directions.

One finding is that tweet volume is largely driven by campaign events. Of the 50 top hourly intervals between Oct 12 , 2011 and Feb 29 , 2012, ranked by tweet volume, all but two correspond either to President Obama X  X  Stat e of the Union address , televised primary debates or moments when caucus or primary election results were released. Out of the 100 top hourly intervals, all but 18 correspond to such events. The 2012 State of the Union address on Jan 24 is another good exa mple. It cause d the biggest volume we have seen in a single day since last October , 1.37 million tweets in total for that day. Both positive and negative tweets for President Obama increased three to four ti mes comp ar ing to an average day.
During the Republican Primary debate on Jan 19, 2012 in Charleston, NC one of the Republican candidates, Newt Gingrich, was asked about his ex -wife at the beginning of the debate. Within minutes , our dashboard showed his negative sentiment increas e rapidly  X  it became three times more negative in just two minutes. This illustrates how tweet volume and sentiment are extremely responsive to emerging events in the real world ( Vergeer et al., 2011 ) .

These examples confirm our assessment that it is especially relevant to offer a system that can provide real -time analysis during key moments in the election cycle. As the election continues and culminates with the presidential vote this November, we hope that our system will provide rich insights into the evolution of public sentiment toward the contenders. W e pre sented a system for real -time Twitter sentiment analysis of the ongoing 2012 U . S . p residential election. We use the Twitter  X  firehose  X  and expert -curated rules and keywords to get a full and accurate picture of the online political landscape. Our real -time data processing infrastructure and statistical sentiment model evaluates public sentiment changes in response to emerging political events and news as they unfold . The architecture and method are generic, and can be easily adopted and extended to other do mains (for instance, we used the system for gauging sentiments about films and actors surrounding Oscar nomination and selection).

