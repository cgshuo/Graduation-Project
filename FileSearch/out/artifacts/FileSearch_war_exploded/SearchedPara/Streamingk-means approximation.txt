 nailon@google.com the specific application. The problem with many heuristics designed to implement some notion of algorithms have been designed with the k -means objective in mind, very few have approximation guarantees with respect to this objective.
 In this work, we give a one-pass streaming algorithm for the k -means problem. We are not aware in the batch setting. They define a seeding procedure which chooses a subset of k points from a approximation algorithm, in expectation. 2 We modify k -means++ to obtain a new algorithm, k -approximation algorithm, our modified seeding procedure is very simple to analyze. [GMMM+03] defines a divide-and-conquer strategy to combine multiple bi-criterion approximation algorithms for the k -medoid problem to yield a one-pass streaming approximation algorithm for k -median. We extend their analysis to the k -means problem and then use k -means++ and k -means# points in the stream and M is the amount of work memory available to the algorithm. Empirical evaluations, on simulated and real data, demonstrate the practical utility of our techniques. 1.1 Related work There is much literature on both clustering algorithms [Gon85, Ind99, VW02, GMMM+03, KMNP+04, ORSS06, AV07, CR08, BBG09, AL09], and streaming algorithms [Ind99, GMMM+03, rithms that operate in the streaming setting [Ind99, GMMM+03, CCP03]. Our work is inspired by that of Arthur and Vassilvitskii [AV07], and Guha et al. [GMMM+03], which we mentioned above analyzed by [ORSS06], under special assumptions on the input data.
 well in practice. There do exist constant approximations to the k -means objective, in the non-[LV92, CG99, Ind99, CMTS02, AGKM+04] give constant approximation algorithms for the re-work for the k -means problem without too much degredation of the approximation, however there to the k -means objective.
 data that admits a clustering with well separated means e.g. in [VW02, ORSS06, CR08]. Recent to any constant approximation of the clustering objective. In contrast, we prove approximation 1.1.1 Preliminaries The k -means clustering problem is defined as follows: Given n points X  X  R d and a weight will use the term  X  X enter X  to refer to any c  X  C . and let  X  C might become competitive when it is allowed to output more centers.
 means problem if it outputs a clustering C with ak centers with potential  X  C such that  X  C the worst case. Where a&gt; 1 , b &gt; 1 .
 we assume a point in R d can be stored in O (1) space. means++ algorithm: 1. Choose an initial center c 1 uniformly at random from X . 2. Repeat ( k  X  1) times: 3. Choose the next center c i , selecting c i = x #  X  X with probability D ( x ! ) 2 P In the original definition of k -means++ in [AV07], the above algorithm is followed by Lloyd X  X  algorithm. The above algorithm is used as a seeding step for Lloyd X  X  algorithm which is known this seeding step. The running time of the algorithm is O ( nkd ) .
 following definition first: is denoted by  X  C ( A ) and is defined as  X  C ( A )= ! point x from the nearest point in C .
 with just one center, chosen uniformly at random from A . Then Exp [  X  C ( A )] = 2  X   X  C center, which is chosen uniformly at random from A . Then, Pr [  X  C ( A ) &lt; 8  X  C Proof. The proof follows from Markov X  X  inequality.
 clustering. If we add a random center to C from A , chosen with D 2 weighting to get C # , then Exp [  X  C ! ( A )]  X  8  X   X  C 32  X   X  C We will use k -means++ and the above two lemmas to obtain a ( O (log k ) ,O (1)) -approximation algorithm for the k -means problem. Consider the following algorithm: 1. Choose 3  X  log k centers independently and uniformly at random from X . 2. Repeat ( k  X  1) times. above algorithm is clearly O ( ndk log k ) .
 We call this subset of clusters, the  X  X overed X  clusters. Let A i clusters. The following simple lemma shows that with constant probability step (1) of k -means# picks a center such that at least one of the clusters gets covered, or in other words, |A 1 call this event E .
 Lemma 2.6. Pr [ E ]  X  (1  X  1 /k ) .
 Proof. The proof easily follows from Corollary 2.3.
 Let X i Lemma 2.7. If event E occurs ( |A 1 64 Proof. We get the main result using the following sequence of inequalities:  X  C i =  X  C i ( X i  X  Lemma 2.8. If for any i  X  1 ,  X  C i ( X i centers x chosen in round ( i + 1) satisfies  X  C i  X  x ( A )  X  32  X   X  C  X  We use the above two lemmas to prove our main theorem.
 Theorem 2.9. k -means# is a ( O (log k ) ,O (1)) -approximation algorithm.
 Proof. From Lemma 2.6 we know that event E (i.e., |A i 64 1 In this case from Lemma 2.8, we have  X  C =  X  C k  X  32  X   X  C We have shown that k -means# is a randomized algorithm for clustering which with probability at least 1 / 4 gives a clustering with competitive ratio 64 . rithm is a divide and conquer strategy defined by [GMMM+03] which uses bi-criterion approxima-develop some of the tools needed for the above. 3.1 A streaming (a,b)-approximation for k -means We will show that a simple streaming divide-and-conquer scheme, analyzed by [GMMM+03] with respect to the k -medoid objective, can be used to approximate the k -means objective. First we present the scheme due to [GMMM+03], where in this case we use k -means-approximating algo-rithms as input.
 Inputs: (a) Point set S  X  R d . Let n = | S | . 1. Divide S into groups S 1 ,S 2 , . . . , S # 2. For each i  X  { 1 , 2 , . . . , # } 3. Run A on S i to get  X  ak centers T i = { t i 1 ,t i 2 , . . . } 4. Denote the induced clusters of S i as S i 1  X  S i 2  X   X  X  X  5.

S w  X  T 1  X  T 2  X   X  X  X   X  T # , with weights w ( t ij )  X  | S ij | 6. Run A # on S w to get  X  a # k centers T 7. Return T First note that when every batch S i has size memory. Now we will give an approximation guarantee.
 Theorem 3.1. The algorithm above outputs a clustering that is an ( a # , 2 b +4 b # ( b + 1)) -approximation to the k -means objective.
 remains to show the approximation of the k -means objective. The proof, which appears in the Appendix, involves extending the analysis of [GMMM+03], to the case of the k -means objective. Using the exposition in Dasgupta X  X  lecture notes [Das08], of the proof due to [GMMM+03], our extension is straightforward, and differs in the following ways from the k -medoid analysis. 3.2 Using k -means++ and k -means# in the divide-and-conquer strategy streaming algorithm. We now have two randomized algorithms, k -means# which with probability approximation algorithm (the approximation factor being in expectation). We can now use these two algorithms in the divide-and-conquer strategy to obtain a single pass streaming algorithm. We use the following as algorithms as A and A # in the divide-and-conquer strategy (3): Weighted versus non-weighted. Note that k -means and k -means# are approximation algorithms integers. Note that both k -means and k -means# can be easily generalized for the weighted case memory required remains logarithmic in the input size, including the storing the weights. Analysis. With probability at least step (3) of Algorithm 3 we run A on batches of data. Since each batch is of size batches is batches is at least gives a O (log k ) -approximation algorithm. The memory required is O ( log ( k )  X  logarithm of the input size. Moreover, the algorithm has running time O ( dnk log n log k ) . 3.3 Improved memory-approximation tradeoffs was O ( a is broken up in turn into two levels, and so on. This idea was used in [GMMM+03]. Here we make a more precise account of the tradeoff between the different parameters.
 i t M we must also have t 1 = n/M 1 , implying n = M 1  X  X  X  M r ! M 1 =  X  X  X  = M r , or in other words M i = rn Assume now that we are in the realistic setting in which the available memory is of fixed size M  X  k . We will choose r (below), and for each i =1 ..r  X  1 we choose to either run k -means++  X  c n k -means++ for level r . Summarizing, we get: k -means++ is chosen. The resulting algorithm is a randomized one-pass streaming approximation to time of the algorithm is O ( dnk 2 log n log k ) .
 We should compare the above multi-level streaming algorithm with the state-of-art (in terms of memory vs. approximation tradeoff) streaming algorithm for the k -median problem. Charikar, Callaghan, and Panigrahy [CCP03] give a one-pass streaming algorithm for the k -median problem would be more interesting from a practical point of view. lowing manner: we choose 25 random vertices from a 15 dimensional hypercube of side length 500. dataset consists of cloud cover data [AN07]. Here n = 1024 and d = 10 . (3) The UCI Spambase dataset is data for an e-mail spam detection task [AN07]. Here n = 4601 and d = 58 . To compare against a baseline method known to be used in practice, we used Lloyd X  X  algorithm, this algorithm to form a baseline. We also compare to an online version of Lloyd X  X  algorithm, b) Cloud dataset, c) Spambase dataset.
 n = 2048 ,k = 25 , c) Spambase dataset, k = 10 . The memory size decreases as the number of levels of the hierarchy increases. ( 0 levels means running batch k-means++ on the data.) 10 random restarts for the randomized algorithms: all but Online Lloyd X  X ) for these algorithms: (2) OL : Online Lloyd X  X . (3) DC-1 : The simple 1-stage divide and conquer algorithm of Section 3.2. (4) DC-2 : The simple 1-stage divide and conquer algorithm 3 of Section 3.1. The sub-algorithms context, k-means++ and k-means# are only the seeding step, not followed by Lloyd X  X  algorithm. of k , and lower cost than Batch Lloyd X  X  for most settings of k (including the correct k = 25 , in than the one-pass streaming problem. The performance of DC-1 and DC-2 is comparable. more optimistic outcome in which adding levels (and limiting memory) actually improves the out-Acknowledgements. We thank Sanjoy Dasgupta for suggesting the study of approximation algo-
