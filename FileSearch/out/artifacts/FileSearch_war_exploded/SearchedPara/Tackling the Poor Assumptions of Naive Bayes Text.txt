 Jason D. M. Rennie jrennie@mit.edu Lawrence Shih kai@mit.edu Jaime Teevan teevan@mit.edu David R. Karger karger@mit.edu Naive Bayes has been denigrated as  X  X he punching bag of classifiers X  (Lewis, 1998), and has earned the dubi-ous distinction of placing last or near last in numer-ous head-to-head classification papers (Yang &amp; Liu, 1999; Joachims, 1998; Zhang &amp; Oles, 2001). Still, it is frequently used for text classification because it is fast and easy to implement. Less erroneous algorithms tend to be slower and more complex. In this paper, we investigate the reasons behind Naive Bayes X  poor performance. For each problem, we propose a sim-ple heuristic solution. For example, we look at Naive Bayes as a linear classifier and find ways to improve the learned decision boundary weights. We also better match the distribution of text with the distribution assumed by Naive Bayes. In doing so, we fix many of the classifier X  X  problems without making it slower or significantly more difficult to implement.
 In this paper, we first review the multinomial Naive Bayes model for classification and discuss several sys-temic problems with it. One systemic problem is that when one class has more training examples than an-other, Naive Bayes selects poor weights for the decision boundary. This is due to an under-studied bias effect that shrinks weights for classes with few training ex-amples. To balance the amount of training examples used per estimate, we introduce a  X  X omplement class X  formulation of Naive Bayes.
 Another systemic problem with Naive Bayes is that features are assumed to be independent. As a re-sult, even when words are dependent, each word con-tributes evidence individually. Thus the magnitude of the weights for classes with strong word dependencies is larger than for classes with weak word dependencies. To keep classes with more dependencies from dominat-ing, we normalize the classification weights.
 In addition to systemic problems, multinomial Naive Bayes does not model text well. We present a simple transform that enables Naive Bayes to instead emulate a power law distribution that matches real term fre-quency distributions more closely. We also discuss two other pre-processing steps, common for information retrieval but not for Naive Bayes classification, that incorporate real world knowledge of text documents. They significantly boost classification accuracy. Our Naive Bayes modifications, summarized in Ta-ble 4, produces a classifier that no longer has a genera-tive interpretation. Thus, common model-based tech-niques to uncover latent classes and incorporate unla-beled data, such as EM, are not applicable. However, we find the improved classification accuracy worth-while. Our new classifier approaches the state-of-the-art accuracy of the Support Vector Machine (SVM) on several text corpora while being faster and easier to implement than the SVM and most modern-day classifiers. The Naive Bayes classifier is well studied. An early description can be found in Duda and Hart (1973). Some of the reasons the classifier is so common is that it is fast, easy to implement and relatively effective. Domingos and Pazzani (1996) discuss its feature in-dependence assumption and explain why Naive Bayes performs well for classification even with such a gross over-simplification. McCallum and Nigam (1998) posit a multinomial Naive Bayes model for text classifica-tion and show improved performance compared to the multi-variate Bernoulli model due to the incorporation of frequency information. It is this multinomial ver-sion, which we call  X  X ultinomial Naive Bayes X  (MNB), that we discuss, analyze and improve on in this paper. 2.1. Modeling and Classification Multinomial Naive Bayes models the distribution of words in a document as a multinomial. A document is treated as a sequence of words and it is assumed that each word position is generated independently of every other. For classification, we assume that there are a fixed number of classes, c  X  { 1 , 2 ,...,m } , each with a fixed set of multinomial parameters. The parameter vector for a class c is ~  X  c = {  X  c 1 , X  c 2 ,..., X  cn n is the size of the vocabulary, P i  X  ci = 1 and  X  ci is the probability that word i occurs in that class. The likelihood of a document is a product of the parameters of the words that appear in the document, where f i is the frequency count of word i in docu-ment d . By assigning a prior distribution over the set of classes, p ( ~  X  c ), we can arrive at the minimum-error classification rule (Duda &amp; Hart, 1973) which selects the class with the largest posterior probability, where b c is the threshold term and w ci is the class c weight for word i . These values are natural parame-ters for the decision boundary. This is especially easy to see for binary classification, where the boundary is defined by setting the differences between the positive and negative class parameters equal to zero, The form of this equation is identical to the decision boundary learned by the (linear) Support Vector Ma-chine, logistic regression, linear least squares and the perceptron. Naive Bayes X  relatively poor performance results from how it chooses the b c and w ci . 2.2. Parameter Estimation For the problem of classification, the number of classes and labeled training data for each class is given, but the parameters for each class are not. Parameters must be estimated from the training data. We do this by selecting a Dirichlet prior and taking the expectation of the parameter with respect to the posterior. For details, we refer the reader to Section 2 of Heckerman (1995). This gives us a simple form for the estimate of the multinomial parameter, which involves the number of times word i appears in the documents in class c ( N ci ), divided by the total number of word occurrences in class c ( N c ). For word i , a prior adds in  X  i imagined occurrences so that the estimate is a smoothed version of the maximum likelihood estimate, where  X  denotes the sum of the  X  i . While  X  i can be set differently for each word, we follow common practice by setting  X  i = 1 for all words.
 Substituting the true parameters in Equation 2 with our estimates, we get the MNB classifier, l
MNB ( d ) = argmax c where  X  p (  X  c ) is the class prior estimate. The prior class probabilities, p (  X  c ), could be estimated like the word estimates. However, the class probabilities tend to be overpowered by the combination of word probabilities, so we use a uniform prior estimate for simplicity. The weights for the decision boundary defined by the MNB classifier are the log parameter estimates, Naive Bayes has many systemic errors. Systemic er-rors are byproducts of the algorithm that cause an inappropriate favoring of one class over the other. In this section, we discuss two under-studied systemic er-rors that cause Naive Bayes to perform poorly. We highlight how they cause misclassifications and pro-pose solutions to mitigate or eliminate them. 3.1. Skewed Data Bias In this section, we show that skewed data  X  X ore train-ing examples for one class than another X  X an cause the decision boundary weights to be biased. This causes the classifier to unwittingly prefer one class over the other. We show the reason for the bias and propose to alleviate the problem by learning the weights for a class using all training data not in that class.
Class 1 Class 2 p (data)  X   X  1  X   X  2 Label  X  = 0 . 25  X  = 0 . 2 for H Table 1 gives a simple example of the bias. In the ex-ample, Class 1 has a higher rate of heads than Class 2. However, our classifier labels a heads occurrence as Class 2 more often than Class 1. This is not because Class 2 is more likely by default. Indeed, the classi-fier also labels a tails example as Class 1 more often, despite Class 1 X  X  lower rate of tails. Instead, the ef-fect, which we call  X  X kewed data bias, X  directly results from imbalanced training data. If we were to use the same number of training examples for each class, we would get the expected result X  X  heads example would be more often labeled by the class with the higher rate of heads.
 Let us consider the more complex example of how the weights for the decision boundary in text classifica-tion, shown in Equation 5, are learned. Since log is a concave function, the expected value of the weight estimate is less than the log of the expected value of the parameter estimate, E [  X  w ci ] &lt; log E [  X   X  ci training data is not skewed, this difference will be ap-proximately the same between classes. But, when the training data is skewed, the weights will be lower for the class with less training data. Hence, classification will be erroneously biased toward one class over the other, as is the case with our example in Table 1. To deal with skewed training data, we introduce a  X  X omplement class X  version of Naive Bayes, called Complement Naive Bayes (CNB). In estimating weights for regular MNB (Equation 4) we use train-ing data from a single class, c . In contrast, CNB esti-mates parameters using data from all classes except c . We think CNB X  X  estimates will be more effective be-cause each uses a more even amount of training data per class, which will lessen the bias in the weight es-timates. We find we get more stable weight estimates and improved classification accuracy. These improve-ments might be due to more data per estimate, but overall we are using the same amount of data, just in a way that is less susceptible to the skewed data bias. CNB X  X  estimate is where N  X  ci is the number of times word i occurred in documents in classes other than c and N  X  c is the total number of word occurrences in classes other than c , and  X  i and  X  are smoothing parameters, as in Equa-tion 4. As before, the weight estimate is  X  w  X  ci = log and the classification rule is l
CNB ( d ) = argmax c The negative sign represents the fact that we want to assign to class c documents that poorly match the complement parameter estimates.
 Figure 1 shows how different amounts of training data affect (a) the regular weight estimates and (b) the complement weight estimates. The regular weight es-timates shift up and change their ordering between 10 examples of training data and 1000 examples. In particular, the word that has the smallest weight for 10 through 100 examples moves up to the 11th largest weight (out of 18) when estimated with 1000 examples. The complement weights show the effects of smooth-ing, but do not show such a severe upward bias and retain their relative ordering. The complement esti-mates mitigate the problem of the skewed data bias. CNB is related to the one-versus-all-but-one (com-monly misnamed  X  X ne-versus-all X ) technique that is frequently used in multi-label classification, where each example may have more than one label. Berger (1999) and Zhang and Oles (2001) have found that one-vs-all-but-one MNB works better than regular MNB. The classification rule is l OVA ( d ) = argmax c log p ( ~  X  c ) + X This is a combination of the regular and complement classification rules. We attribute the improvement (a) (b) with one-vs-all-but-one to the use of the complement weights. We find that CNB performs better than one-vs-all-but-one and regular MNB since it eliminates the biased regular MNB weights. 3.2. Weight Magnitude Errors In the last section, we discussed how uneven train-ing sizes could cause Naive Bayes to bias its weight vectors. In this section, we discuss how the indepen-dence assumption can erroneously cause Naive Bayes to produce different magnitude classification weights. When the magnitude of Naive Bayes X  weight vector ~w c is larger in one class than the others, the larger-magnitude class may be preferred. For Naive Bayes, differences in weight magnitudes are not a deliberate attempt to create greater influence for one class. In-stead, the weight differences are partially an artifact of applying the independence assumption to depen-dent data. Naive Bayes gives more influence to classes that most violate the independence assumption. The following example illustrates this effect.
 Consider the problem of distinguishing between docu-ments that discuss Boston and ones that discuss San Francisco. Let X  X  assume that  X  X oston X  appears in Boston documents about as often as  X  X an Francisco X  appears in San Francisco documents (as one might expect). Let X  X  also assume that it X  X  rare to see the words  X  X an X  and  X  X rancisco X  apart. Then, each time a test document has an occurrence of  X  X an Francisco, X  Multinomial Naive Bayes will double count X  X t will add in the weight for  X  X an X  and the weight for  X  X ran-cisco. X  Since  X  X an Francisco X  and  X  X oston X  occur equally in their respective classes, a single occurrence of  X  X an Francisco X  will contribute twice the weight as an occurrence of  X  X oston. X  Hence, the summed con-tributions of the classification weights may be larger for one class than another X  X his will cause MNB to prefer one class incorrectly. For example, if a doc-ument has five occurrences of  X  X oston X  and three of  X  X an Francisco, X  MNB will label the document as  X  X an Francisco X  rather than  X  X oston. X  In practice, it is often the case that weights tend to lean toward one class or the other. For the problem of identifying  X  X arley X  documents in the Reuters-21578 corpus, it is advantageous to choose a threshold term, b = b +  X  b  X  , that is much more negative than one chosen by counting documents. In testing different smoothing values, we found that  X  i = 10  X  4 gave the most extreme example of this. With a threshold term of b =  X  94 . 6, the classifier achieved as low an error rate as any other smoothing value. However, the thresh-old term calculated via the prior estimate by count-ing training documents was log p ( ~  X  + ) threshold yielded a somewhat higher rate of error. It is likely Naive Bayes X  independence assumption lead to a strong preference for the  X  X arley X  documents. We correct for the fact that some classes have greater dependencies by normalizing the weight vectors. In-stead of assigning  X  w ci = log  X   X  ci , we assign We call this, combined with CNB, Weight-normalized Complement Naive Bayes (WCNB). Experiments in-dicate that WCNB is effective. Alternately, one could address this problem by optimizing the threshold terms, b c . Webb and Pazzani give a method for doing this by calculating per-class weights based on identi-fied violations of the Naive Bayes classifier (Webb &amp; Pazzani, 1998).
 Since we are manipulating the weight vector directly, we can no longer make use of the model-based as-pects of Naive Bayes. Thus, common model-based techniques to incorporate unlabeled data and uncover latent classes, such as EM, are not applicable. This is a trade-off for improved classification performance. 3.3. Bias Correction Experiments We ran classification experiments to validate the tech-niques suggested here. Table 2 gives classification performance on three text data sets, reporting ac-curacy for 20 Newsgroups and Industry Sector and precision-recall breakeven for Reuters. See the Ap-pendix for a description of the data sets and experi-mental setup. We compared Weight-normalized Com-plement Naive Bayes (WCNB) with standard multi-nomial Naive Bayes (MNB), and found that WCNB resulted in marked improvement on all data sets. The improvement was greatest for data sets where training data quantity varied between classes (Reuters and In-dustry Sector). The greatly improved Reuters macro P-R breakeven score suggests that much of the im-provement can be attributed to better performance on classes with few training examples. WCNB also shows an improvement (small, but significant) on 20 Newsgroups even though the distribution of training examples is even across classes.
 In comparing, we note that our baseline, MNB, is sim-ilar to the MNB results found by others. Our 20 Newsgroups result closely matches that reported by McCallum and Nigam (1998) (85% vs. our 84.8%). The difference in Ghani (2000) X  X  Industry Sector re-sult (64.5% vs. our 58.2%) is likely due to his use of feature selection. Zhang and Oles (2001) X  X  result on Industry Sector (84.8%) is significantly higher be-cause they optimize the smoothing parameter. When we optimized the smoothing parameter for MNB via cross-validation, in experiments not reported here, our MNB results were similar. Smoothing parameter op-timization also further improved WCNB. Our micro and macro scores on Reuters are reasonably similar to Yang and Liu (1999) (79.6% vs. our 73.9%, 38.9% vs. our 27.0%), with the differences likely due to their use of feature selection, a different scoring metric (F1), and a different pre-processing system (SMART). So far we have discussed systemic issues that arise when using any Naive Bayes classifier. MNB uses a multinomial to model text, which is not very accurate. In this section we look at three transforms to better align the model and the data. One transform affects frequencies X  X erm frequency distributions have a much heavier tail than the multinomial model expects. We also transform based on document frequency, to keep common terms from dominating in classification, and based on length, to keep long documents from domi-nating during training. By transforming the data to be better suited for use with a multinomial model, we find significant improvement in performance over using MNB without the transforms. 4.1. Transforming Term Frequency In order to understand if MNB would do a good job classifying text, we looked at empirical term distri-butions of text. We found that term distributions had heavier tails than predicted by the multinomial model, instead appearing like a power-law distribution. Using a simple transform, we can make these power-law-like term distributions look more multinomial.
 To measure how well the multinomial model fits the term distribution of text, we compared the empirical distribution to the maximum likelihood multinomial. For visualization purposes, we took a set of words with approximately the same occurrence rate and created a histogram of their term frequencies in a set of docu-ments with similar length. These term frequency rates and those predicted by the best fit multinomial model are plotted in Figure 2 on a log scale. The figure shows the empirical term distribution is very different from what a multinomial model would predict. The empiri-cal distribution has a much heavier tail, meaning mul-tiple occurrences of a term is much more likely than expected for the best fit multinomial. For example, the multinomial model predicts the chance of seeing an average word occur nine times in a document is p ( f i = 9) = 10  X  21 . 28 , so low that such an event in un-expected even in a collection of all news stories ever written. In reality the chance is p ( f i = 9) = 10  X  4 . 34 very rare for a single document, but not unexpected in a collection of 10,000 documents.
 This behavior, also called  X  X urstiness X , has been ob-served by Church and Gale (1995) and Katz (1996). While they developed sophisticated models to deal with term burstiness, we found that even a simple heavy tailed distribution, the power law distribution, could better model text and motivate a simple trans-form to the features of our MNB model. Figure 2(b) shows an example empirical distribution, alongside a power law distribution, p ( f i )  X  ( d + f i ) log  X  , where d has been chosen to closely match the text distribution. The probability is also proportional to  X  log( d + f i ) . Be-cause this is similar to the multinomial model, where the probability is proportional to  X  f i , we can use the multinomial model to generate probabilities propor-tional to a class of power law distributions via a sim-ple transform, f 0 i = log( d + f i ). One such transform, f = log(1 + f i ), has the advantages of being an iden-tity transform for zero and one counts, while pushing down larger counts as we would like. The transform allows us to more realistically handle text while not giving up the advantages of MNB. Although setting d = 1 does not match the data as well as an optimized d , it does produce a distribution that is much closer to the empirical distribution than the best fit multino-mial, as shown by the  X  X ower law X  line in Figure 2(a). 4.2. Transforming by Document Frequency Another useful transform discounts terms that occur in many documents. Common words are unlikely to be related to the class of a document, but random variations can create apparent fictitious correlations. This adds noise to the parameter estimates and hence the classification weights. Since common words ap-pear often, they can hold sway over a classification de-cision even if their weight differences between classes is small. For this reason, it is advantageous to down-weight these words.
 A heuristic transform in the Information Retrieval (IR) community, known as  X  X nverse document fre-quency X , is to discount terms by their document fre-quency (Jones, 1972). A common way to do this is where  X  ij is 1 if word i occurs in document j , 0 other-wise, and the sum is over all document indices (Salton &amp; Buckley, 1988). Rare words are given increased term frequencies; common words are given less weight. We found it to improve performance. 4.3. Transforming Based on Length Documents have strong word inter-dependencies. Af-ter a word first appears in a document, it is more likely to appear again. Since MNB assumes occurrence in-dependence, long documents can negatively effect pa-rameter estimates. We normalize word counts to avoid this problem. Figure 3 shows empirical term frequency distributions for documents of different lengths. It is not surprising that longer documents have larger prob-abilities for larger term frequencies, but the jump for larger term frequencies is disproportionally large. Doc-uments in the 80-160 group are, on average, twice as long as those in the 0-80 group, yet the chance of a word occurring five times in the 80-160 group is larger than a word occurring twice in the 0-80 group. This would not be the case if text were multinomial. To deal with this, we again use a common IR transform that is not seen with Naive Bayes. We discount the influence of long documents by transforming the term frequencies according to yielding a length 1 term frequency vector for each doc-ument. This transform is common within the IR com-munity because the probability of generating a docu-ment within a model is compared across documents; in such a case one does not want short documents dom-inating merely because they have fewer words. For classification, however, because comparisons are made across classes, and not across documents, the benefit of such normalization is more subtle, especially as the multinomial model accounts for length very naturally (Lewis, 1998). The transform keeps any single docu-ment from dominating the parameter estimates. 4.4. Experiments We have described a set of transforms for term frequen-cies. Each of these tries to resolve a different prob-lem with the modeling assumptions of Naive Bayes. The set of modifications and the procedure for ap-plying them is shown in Table 4. When we apply those modifications, we find a significant improve-ment in text classification performance over MNB. Ta- X  Let ~ d = ( ~ d 1 ,..., ~ d n ) be a set of documents; d  X  Let ~y = ( y 1 ,...,y n ) be the labels.  X  TWCNB( ~ d,~y ) ble 3 shows classification accuracy for Industry Sector and 20 Newsgroups and precision-recall breakeven for Reuters. In tests, we found the length normalization transform to be the most useful, followed by the log transform. The document frequency transform seemed to be of less import. We show results on the Support Vector Machine (SVM) for comparison. We used the transforms described in Section 4 for the SVM since they improved classification performance.
 We discussed similarities in our multinomial Naive Bayes results in Section 3.3. Our Support Vector Ma-chine results are similar to others. Our Industry Sec-tor result matches that reported by Zhang and Oles (2001) (93.6% vs. our 93.4%). The difference in God-bole et al. (2002) X  X  result (89.7% vs. our 86.2%) on 20 Newsgroups is due to their use of a different multi-class schema. Our micro and macro scores on Reuters differ from Yang and Liu (1999) (86.0% vs. our 88.7%, 52.5% vs. our 69.4%), likely due to their use of fea-ture selection, a different scoring metric (F1), and a different pre-processing system (SMART). The larger difference in macro results is due to the sensitivity of macro calculations, which heavily weighs small classes. We have described several techniques, shown in Ta-ble 4, that correct deficiencies in the application of the Naive Bayes classifier to text data. A series of trans-forms from the information retrieval community, Steps 1-3 in Table 4, improves the performance of Naive Bayes text classification. For example, the transform described in Step 1 converts text, which can be closely modeled by a power law, to look more multinomial. Training with the complement class, Step 4, solves the problem of uneven training data. Normalizing the classification weights, Step 6, improves upon the Naive Bayes handling of word occurrence dependen-cies. These modifications better align Naive Bayes with the realities of bag-of-words textual data and, as we have shown empirically, significantly improve its performance on a number of data sets. The modified Naive Bayes is a fast, easy-to-implement, near state-of-the-art text classification algorithm.
 Acknowledgements We are grateful to Yu-Han Chang and Tommi Jaakkola for their input. We also thank the anonymous reviewers for valuable com-ments. This work was supported by the MIT Oxygen Partnership, the National Science Foundation (ITR) and Graduate Research Fellowships from the NSF. For our experiments, we use three well X  X nown data sets: 20 Newsgroups, Industry Sector and Reuters-21578. Industry Sector and 20 News are single-label data sets: each document is assigned a single class. Reuters is a multi-label data set: each document may have many labels. Since Reuters is multi-label, it is handled differently than described in the paper. For MNB, we use the standard one-vs-all-but-one (usu-ally misnamed  X  X ne-vs-all X ) on each binary problem. For CNB, we use all-vs-all-but-one, thus making the amount of data per estimate more even.
 Industry Sector and Reuters-21578 have widely vary-ing numbers of documents per class, but no single class dominates. The distribution of documents per class for 20 Newsgroups is even at about 1000 exam-ples per class. For 20 Newsgroups, we ran 10 ran-dom splits with 80% training data and 20% testing data per class. There are 9649 Industry Sector doc-uments and 105 classes; the largest category has 102 documents, the smallest has 27. For Industry Sec-tor, we ran 10 random splits with 50% training data and 50% testing data per class. For Reuters-21578 we use the  X  X odApte X  split and use only topics with at least one training document and one testing document. This gives 7770 training documents and 90 classes; the largest category has 2840 training documents.
 For the SVM experiments, we used SvmFu 1 and set C = 10. We use one-vs-all to produce multi-class la-bels for the SVM. We use the linear kernel since it performs as well as non-linear kernels in text classifi-cation (Yang &amp; Liu, 1999).
