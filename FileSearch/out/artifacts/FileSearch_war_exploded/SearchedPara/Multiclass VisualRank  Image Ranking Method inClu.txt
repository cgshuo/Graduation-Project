 This paper proposes Multiclass VisualRank, a method that expands the idea of VisualRank into more than one category of images. Multiclass VisualRank divides images retrieved from search engines into several categories based on distin c-tive patterns of visual features, and gives ranking within t he category. Experimental results show that our method can extract several different image categories relevant to give n keyword and gives good ranking scores to retrieved images. H.3.3 [ Information Systems ]: Information Search and Re-trieval; I.4 [ Image Processing and Computer Vision ]: Miscellaneous Algorithms, Theory Ranking, Clustering, Visual Feature
Image search engines widely available on the Web retrieve images sorted in descending order of ranking scores that are calculated from text information around the images. How-ever, the search engines sometimes give high scores to image s irrelevant to queried keywords, or vice versa. Image rankin g often fails because the meaning of text information does not always correspond to the meaning of images.

Jing et al.[2] has proposed VisualRank that uses visual fea-tures instead of the text information to refine ranking score s of images retrieved from an image search engine. While Vi-sualRank achieves high retrieval precision, the top result s tend to be occupied by similar images as shown in figure 1(B). There is not always one representative image for a queried keyword. It is preferable that the user can obtain a diverse set of images.

In this paper, we propose Multiclass VisualRank, a method that expands the idea of VisualRank into more than one category of images. Multiclass VisualRank divides images retrieved from search engines into several categories base d Figure 1: Top 5 results of a keyword,  X  X otre Dame X , in each ranking method. on distinctive patterns of visual features, and gives rank-ing within the category. This method displays the images in multiple sequences. Each of the sequences contains cat-egorized images that are sorted by their ranking scores as shown in figure 1(C). Our method works as a post-filtering for existing image search engines. This helps users to grasp the entire results retrieved from image search engines.
Section 2 describes our algorithm in detail. Section 3 shows our experimental results using retrieved images from Google Image Search.
Multiclass VisualRank is composed of following three steps : obtaining visual similarity, clustering and ranking. As we ll as VisualRank, SIFT key points [3] and PageRank [1] are used in the step of obtaining visual similarity and the step of ranking, respectively. The principal contribution of th is paper is that the clustering algorithm is incorporated to th e framework of VisualRank in order to extract different image categories related to given keywords.
The visual similarity w ij between two images I i , I j is cal-culated by using SIFT key points. An ratio C ij is defined as the number of sharing key points between I i and I j devided by the mean number of key points extracted from I i , I j
While original VisualRank uses the ratio C ij as the vi-sual similarity, in our method a sigmoid function is applied to C ij to weaken a large value. Because the image search engines sometimes retrieve exactly the same images for a given keyword. In those cases, the value C ij becomes too large compared to the other visual similarities, and probab ly worsens the performance of clustering. The insertion of the sigmoid function helps to avoid this issue.
The images connected with their visual similarities can be regarded as a weighted graph. In particular, similar im-ages are mutually connected with high visual similarity. Th e graph contains several clusters that correspond to differen t image categories.

Normalized cuts [4], that is a representative method of spectral clustering, is useful to extract each cluster from the graph. Normalized cuts is formulated as generalized eigenvalue problem as follows: where W is an adjacency matrix whose elements are w ij , D is a degree matrix,  X  is the eigenvalue and v is the eigen-vector. The eigenvector corresponding to the second least eigenvalue provides optimal two-way partitioning that min -imizes normalized cuts criteria NCut defined in [4]. This two-way partitioning is recursively repeated until the val ue NCut exceeds a predefined threshold N th . The number of clusters is automatically determined depending on N th . In our experiment, N th is set to 0.4.
According to [2], VisualRank inspired by PageRank is for-mulated as follows: where r = ( r 1 ,  X  X  X  , r N )  X  is a vector of the ranking scores, p is a uniform vector that models random walk of Web brows-ing and  X  is a balancing factor that is set to 0.15 in our experiment. The ranking score vector r is updated by the procedure in (2) repeatedly.

In the case of multiclass, the adjacency matrix W is mod-ified as follows: w Instead of W , the modified adjacency matrix W  X  is used to calculate ranking scores. The equation (3) means that the visual similarities between different categories are ignor ed. It is preferable that a image does not receive ranking scores from images belonging to different categories. In this way, the more similar to the canonical appearance of each cate-gory an image is, the higher ranking score it obtains. We tested three sets of keywords. The part of keywords Table 1: Evaluation results of extracting categories Sightseeing spots 1.9 1.8 (R:0.6, U:1.2) 0.1 Product names 2.3 2.3 (R:0.2, U:2.1) 0.0 (a) Sightseeing spots : Tokyo-tower, Notre Dame, etc. (b) Artists : Rembrandt, Leonardo da Vinci, Klimt, etc. (c) Product names : Wii, Xbox, iPhone, Gameboy, etc. Each set includes 10 keywords. For each keyword in each set, top 250 images were downloaded from Google Image Search. Large images were resized to 300K pixels keeping their aspect ratio.

The precision in the top 10 re-ranked images in all of the extracted categories was 0.949. According to [2], the preci -sion of original VisualRank is 0.953. As well as VisualRank, our method achieved high retrieval precision.

Table 1 shows the evaluation results of extracting image categories. Our method provided 2.3 relevant categories pe r query. The keyword set of artists tended to give more cate-gories than the other sets. In the case of artists, several re p-resentative paintings by the artists were extracted as imag e categories. For instance, the keyword,  X  X eonardo da Vinci X  , provided 5 categories. This would help users to grasp their representative paintings.

The same objects taken from different view points or un-der different lighting conditions were appeared in the re-trieved images. These images were occasionally divided int o two or more categories. If these categories were visually similar, they were regarded as redundant categories. The average numbers of redundant categories of the sightseeing spots and artists were relatively high compared to product names.

An average number of irrelevant categories per query proved to be small. The precision of obtaining relevant categories among all the keywords was 0.95. The extracted categories were mostly related to the queries.

In conlcusion, the experimental results revealed that rele -vant yet various categories can be automatically extracted , and the images belonging to each of the categories were sorted by their ranking score at high precision. This method would provide better usability for image search engines. into English in this paper.
