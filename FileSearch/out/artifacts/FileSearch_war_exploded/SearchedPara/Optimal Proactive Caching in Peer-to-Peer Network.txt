 As a promising new technology with the unique properties like high efficiency, scalability and fault tolerance, Peer-to-Peer (P2P) tech-nology is used as the underlying network to build new Internet-scale applications. However, one of the well known issues in such an application (for example WWW) is that the distribution of data popularities is heavily tailed with a Zipf-like distribution. With consideration of the skewed popularity we adopt a proactive caching approach to handle the challenge, and focus on two key problems: where (i.e. the placement strategy: where to place the replicas) and how (i.e. the degree problem: how many replicas are assigned to one specific content)? For the where problem, we propose a novel approach which can be generally applied to structured P2P networks. Next, we solve two optimization objectives related to the how problem: MAX_PERF and MIN_COST. Our solution is called PoPCache , and we discover two interesting properties: (1) the number of replicas assigned to each content is proportional to its popularity; (2) the derived optimal solutions are related to the entropy of popularity. To our knowledge, none of the previous works has mentioned such results. Finally, we apply the results of PoPCache to propose a P2P base web caching, called as Web-PoPCache. By means of web cache trace driven simulation, our ex-tensive evaluation results demonstrate the advantages of PoPCache and Web-PoPCache.
 H.3.3 [ Information storage and retrieval ]: Information Search and Retrieval -Search process; C.2.4 [ Computer-Communication Networks ]: Distributed Systems -Distributed applications Design, Performance Peer-to-Peer, Web Caching, Placement Strategy
As a promising new technology, Peer-to-Peer (P2P) systems have the potential to build applications at a very large scale. Existing P2P applications like Gnutella and KazaA connect millions of ma-chines to provide Internet-scale file sharing services. Due to P2P X  X  unique distributed, autonomous, and heterogenous characteristics, efficient search algorithms are essential to improve the usability of a P2P system. Therefore, many research works have been con-ducted on addressing the search efficiency issue, including Chord [23], Pastry[21], Tapestry[28], CAN[11], etc. In these approaches each peer and its stored contents are organized using a distributed hash table (DHT), and they are examples of structured P2P systems. For such a system with N nodes, the search cost (i.e. number of lookup hops) is bounded by O (log N ) . For the current search solu-tions in the structured P2P, all peers are assumed to submit queries to uniformly search the contents stored in all nodes. However, this assumption is not valid in practice. Often, the popularities of the contents (measured with respect to the proportion of all submit-ted queries that can be satisfied by the content [17, 8, 29]) are quite skewed. For example, web requests on the Internet space are highly skewed with a Zipf-like distribution. Therefore, in the reality, this skewed popularity will cause the workload of whole network unbal-anced, moreover, due to the limited bandwidth, though the search cost to some  X  X ot X  peers (the peers have content that can satisfy more queries) is still bounded to O (log N ) in terms of the number of hops, there will be a long latency for getting the data.
So far, we have listed the challenges brought by the skewed pop-ularity distribution. In fact, this skewed popularity distribution also brings opportunity. If we can replicate these popular contents, there will be substantial reduction on the search cost for popular contents and the workload of the whole network can be balanced as well. Therefore, in this paper, in order to make use of the skewed pop-ularity distribution for proper replication, we propose an analysis model to handle two key problems related to caching the replicates in structured P2P systems: where and how to cache the replicas of popular contents in the P2P nodes? For the first problem involv-ing where to place the replicas of the contents, i.e. the placement strategy problem, we propose a novel approach which can be gen-erally applied in the structured P2P network by placing the replicas in the so-called k -ary tree T i . Then with such placement strategy, we handle the problem that how many replicas are assigned for one object, i.e. the degree of replication, and we consider two optimiza-tion criteria:  X  MAX_PERF: given a constant number of replicas, how to min-imize the request latency measured by the average lookup hops in P2P network (i.e. maximize the performance)?  X  MIN_COST: given a targeted threshold of the request lookup hops, how to minimize the copy number of replicas (i.e. minimize the cost)? Replicas, as the paid cost to diminish the average lookup hops of P2P systems, will consume the bandwidth by shipping the copies to multiple destination nodes; on the other hand, the average lookup hops of P2P, as the gained benefit from replication, is one of the most important factors to measure the performance of P2P sys-tems and useful to diminish the request latency in P2P applications. Consequently, how to tradeoff the gained performance(i.e. average lookup hops) and the paid cost(number of replicas) is the key prob-lem during the design of P2P based Caching system. In this pa-per, we formate such tradeoff as two orthogonal optimization prob-lems: MAX_PERF and MIN_COST; then, the system designers can choose either one criterion based on the system design objec-tive with our closed-form solutions for both optimization criteria. We refer our proposed replica placement strategy together with the optimal closed form solutions as the popularity-based P2P based proactive caching , PoPCache in short.

In order to demonstrate the real usage of PoPCache, we develop a P2P-based web caching scheme, called as Web-PoPCache. We have shown through the extensive experiments with respect to the feasibility and optimization of Web-PoPCache based on our pro-posed analysis model. In summary, we make the following contri-butions in this paper:
Among all contributions, two are interesting: (1) For both the objectives of MAX_PERF and MIN_COST, the optimal number of replicas is proportional to the object popularity p x . It is known that for unstructured P2P, the random walk based technique is op-timized by the square-root principle [7, 17, 29, 29]; here we arrive at a different optimal function of the popularity for a structured P2P system. (2) For both optimization problems MAX_PERF and MIN_COST, the derived closed form solutions are related to en-tropy of popularity p x . To our knowledge, none of previous P2P works has mentioned such a relationship. Intuitively it makes sense since we have expected that our approach can accelerate the search for popular nodes, then the skew of the popularity distribution will play an important role in the optimization of the search perfor-mance, and taking the popularity as a probability function of the query targets, entropy is a sound measure of the skew of the distri-bution.

The rest of this paper is organized as follows: Section 2 intro-duces the background of web caching system. In Section 3 we present the related work in this area. In Section 4 we present PoP-Cache X  X  novel replica placement strategy and the closed form solu-tion for the optimization objectives. In Section 5 we present Web-PoPCache, a P2P based proactive web caching scheme based on PoPCache. Section 6 evaluates the performance of PoPCache and Web-PoPCache. Finally, Section 7 is a conclusion.
As an Internet application, web caching is a widely utilized tech-nology to reduce the content request latency, to decrease the amount of aggregate network traffic between the client side and the server side where the requested contents are hosted, and to balance the workload by distributing the heavy workload of the busy web servers. Given a request from the web browser of a client side to the origi-nal web server of the server side, web caching can be implemented in various locations: (1) in the local directory of client side web browser ; (2) at the origin web server (for example, the contents, or portions of contents, can be stored in a server-side cache to reduce the server load); (3) at the intermediate the proxy servers located between the client side and the original web server, including client side proxy servers (the organization proxy, and the forward proxy cache of client side ISP:Internet Service Provider), and the server side proxy servers (reverse proxy cache of server side ISP, and such a network is called a content delivery network: CDN).

Web contents can be cached while passing to the client side web browser from client side proxy servers (the organization proxy, and the forward proxy cache of client side ISP), hence there can be caching only for those contents which are already requested. This kind of caching is typically called passive caching at client side. On the other hand, proactive caching means that the replicas of web contents are proactively cached by the original web servers and the server side CDN to achieve the goals of improved performance and load balancing. The proactive caching technique is widely uti-lized for the content providers like Google or the third-party CDN provider like Akamai. Though supporting load balancing and hav-ing better performance including reduced request latency and de-creased bandwidth consumption, such caching involves expensive costs, which include the expensive dedicate hardware devices (for example high performance servers and network devices), the opera-tional or administrative cost and the associated network bandwidth consumption.

To avoid the expensive cost to construct the server side caching and CDN, P2P technology is an alternative to connect a large num-ber of volunteered nodes with low costs (for example desk top machines) to construct a cooperative web caching system. Squir-rel [13] is an example of such a web caching system and it shares the local contents by Pastry [21] to form an efficient and scalable web caching. However, the passively web caching technique, such as the one applied by Squirrel, only stores the web content to the node with node ID numerically closest to the hash ID of the URL of such web content. There is no consideration about the popularity skewness of the contents. Consequently, the workload of the whole network may still unbalanced.

Cooperative Web caching is the most common solution for aug-menting the low cache hit rates due to single proxies. There has been extensive work on cooperative web caching system as a tech-nique to reduce request latency and increase the hit rate. The de-sign of cooperative web caching systems can be hierarchical like Harvest [10] and Squid [1], hash-based [15],directory-based [12], and multicast based [24]. Different from these cooperative web caching systems which still require a dedicated proxy infrastruc-ture, P2P based web caching systems completely eliminate the need of proxy servers. Kache [16], a cooperative web caching system built over Kelips [16], can perform a lookup in one hop, however, with the cost of maintaining a list of O ( Squirrel [13], a peer-to-peer web cache targeted at replacing central demand-side web caches within a local area network, is a passive, opportunistic cache with no consideration of the skewed popularity and performance optimization.
In this section we briefly review the related works in three as-pects: structured P2P systems, replica placement strategy, and the degree of replication.

Structured Peer-to-Peer System : A number of peer-to-peer rout-ing protocols have been proposed recently, including CAN [11], Chord [23], Tapestry [28] and Pastry [21]. These self-organizing, decentralized systems provide the functionality of a scalable dis-tributed hash-table (DHT), by reliably mapping a given object key to a unique live node in the network. The systems have the de-sirable properties of high scalability, fault tolerance and efficient routing of queries. Besides these DHT-based P2P, randomized P2P networks like Symphony [18], SkipGraph [6],and more structured P2Ps like BATON [14], P-Grid [4] etc are all examples of struc-tured P2P. In these structured P2P protocols, each node is regularly assigned with equal number of neighbors, and the average lookup hop number is guaranteed with O (log N ) .

Replica Placement Strategy : Some works in P2P network pro-pose to place the replica or cached objects to smooth the load in the hot-spot nodes caused by popular queries. CFS [9], a coopera-tive file system over Chord [23], caches the popular objects along the lookup path towards the nodes where the popular objects are stored. In PAST [22],the storage system over Pastry [21], the search for some object is redirected to the nearest replicas of the targeted object. Such placement strategy [22] is a random placement strat-egy. Based on the replication level l , Beehive [20] replicates the object content copies to all the nodes that have at least ing prefixes with the object. Unlike the heuristical schemes like CFS [9] and PAST [22], Beehive [20] and our work can provide optimal solutions based on the analytical model. Compared with Beehive [20], our proposed solution for an optimization problem MIN_COST makes use of less replicas to achieve the same targeted performance and provide better granularity to control the number of replicas; furthermore, there is no assumption on the popularity distribution in our optimization, while Beehive [20] only provides the analytical guarantee for the Zipf distribution. Different from our work, under the proposed cost model, [5] presented the repli-cation of dynamic XML documents in the context of the ad-hoc connected web services which can be treated as unstructured P2P, and does not provide the global optimality.

The Degree of Replication : In unstructured P2P networks, [7] proposes to optimize search efficiency by replication, where the number of replicas of an object is proportional to the square-root of the object X  X  popularity. [8] presents a square-root topology for unstructured P2P networks where the degree of a peer machine is proportional to the square root of the node popularity. Their results show that for random walk search, the square-root principle can achieve optimal performance. Compared with these related works, our work and Beehive [19, 20] solve the MIN_COST problem with closed form solution, however, Beehive [20] only arrives at the ap-proximate numeric results for MIN_COST under the assumption of the Zipf popularity distribution. Furthermore, for the optimization problem MAX_PERF, we derive the proportional principle, in con-trast to the square-root principle un-structured P2P network in the above.

In this section, we first present our novel replica placement strat-egy in Section 4.1, then propose the analytical solutions of PoP-Cache to two optimization objectives related to the replication de-gree problem in Section 4.2. Table 1 summarizes the main symbols used in this section.
Our proposed placement strategy is a general framework which can be applied in structured P2P networks like DHT based P2P(Chord [23], Pastry [21], Tapestry [28], CAN [11]), randomized structured P2P ( Symphony [18], SkipGraph [6]), or BATON [14]. This is difference from the work in Beehive [20], which is limited to the prefix matching based DHT like Pastry[21].

In general, each node in structured P2P network is regularly as-signed with k links for routing. In Chord [23], the value of size of the finger table; in Pastry [21] (Tapestry [28]) the value of k can be treated as the number of rows (levels) in the routing table (neighbor set); in Symphony [18], the value of k is the number of long links. A similar situation holds for other structured P2P net-work. With k -regular links for routing in each node, the content object c x stored in the structured P2P network, can be found with the average O (log N ) number of hops, where N is the total number of nodes.

The key observation in structured P2P is that each node n be treated as the root of a k -ary tree with the k direct neighbors of n connected by k links as the 1st level children, the neighbors of neighbors of n added with k 2 links as the 2nd-level children... As a result, given total N nodes in the structured P2P network, any node n i can be treated as the root of a k -ary tree, which has at most log k N levels with k remote neighbors as the -level children where =1 , 2 , ..., log k N . Such a tree with node n i as the root is denoted as T i . The search from some node n j to n i is the process of greedily approaching the root n i along the bottom-up path of T towards n i .

For simplicity, we take Chord [23] as example. In Figure 1(a), each node has a finger table with size equal to k =3 (Note: in Fig-ure 1(a) we omit the arrows between one node and its successor, for example n 0 to n 1 , for the clear figure). Based on the finger table allocation principle, the i -th node with nodeID n i is connected di-rectly to nodes with nodeID equal to ( n i +2 0 ) MOD N , ( n MOD N , and ( n i +2 2 ) MOD N , where N =13 . For example node n 0 with nodeID equal to 0 in Figure 1(a) has 3 links respec-tively pointing to nodes n 1 , n 2 , and n 4 , which alternatively means nodes n 1 , n 2 , and n 4 are connected by n 0 . With the same situation, node n 9 is connected by nodes n 8 , n 7 , and n 5 ; n 7 is connected by n , n 5 , and n 3 . The same situation holds for other nodes. Then in Figure 1(a) is the root of T 9 , with the direct neighbors of and n 5 as the 1st-level children, the direct neighbors of n 5 as 2nd-level descendants... The lookup for n 9 from any source node can be treated as the process of greedily approaching the root of T 9 , i.e. n 9 in the bottom-up manner. The routing from any node towards the root of T i always finds the closest path to greedily reach the root based on the local routing information of n . For example, to find n 9 , n 4 can look up the local routing information ( n ) and consider n 8 ,instead of n 4 or n 5 , as the closest intermedi-ate forwarder to reach n 9 . As a result, any node located at the -th level of the tree of n i will consume on average average O ( ) to reach the root n i . For other structured P2P network, such kind of k -ary tree can also be conceptually constructed for each node.
Now we show how to utilize the k -ary tree T i for replica place-ment. For a structured P2P network with k regular links in each node, to place the replicas for some content object c x , we can first find the home node n i of the content object c x , i.e. the node where the content object c x is stored in the structured P2P network. The tree T i with n i as the root can be utilized for replica placement. Suppose l x replicas of the content object c x are required. First, at most (due to the fact that one node can appear in multiple locations of the k -ary tree T i ) k replicas are placed in the 1st level children of n i in the tree T i , next at most k 2 replicas are placed in the 2nd level children of n i in the tree T i ,..., until all l i consumed. The corresponding l x descendant nodes of n i to store the replicas of c X are called the delegate nodes, and the area with these l x delegate nodes is called the acceleration area of Figure 1(b), if 5 replicas are allocated for node n 9 for caching, first the 1st level children ( n 8 , n 7 , and n 5 ) are respectively placed with one replica; then the two more replicas are placed in two 2nd level nodes ( n 6 and n 4 ). During the greedy search towards the desig-nation node, the nodes with less distance to the destination node have a higher probability to be visited as the intermediate nodes to-wards the destination node than those node with larger distance to destination node. As a result, n 6 and n 4 , instead of n chosen to place the replicas for node n 9 . Based on our placement strategy we have the following theorem: Theorem 1 In structured P2P with N nodes and k regular links for routing in each node, for content object c x with l x placed along levels of the tree T i with the home node n i the root, the ave rage hop number H x to lookup c x is O (log N log k l x ) .

P ROOF . Since the content object c x is stored in the home node n , c x is located at the root n i of the k -ary tree T i . Based on our proposed replica placement strategy, suppose the 1-st level chil-dren of n i are placed with at most k replicas of c x , the search for c x from any source can be reduced on average by 1 hop; again if both the 1-st and 2-nd levels descendants of n i are assigned at most ( k 1 + k 2 ) replicas, then the search for c x can be reduced on average by 2 hops,..., if the 1-st level, the 2-nd level,..., the -th level descendants of n i are in total assigned ( k 1 + k 2 replicas of c x , the search for c x can be reduced on average by hops. Given l x replicas for c x , they are placed at the 1-st, 2-nd,..., log k (1 + l x (1  X  1 /k )) -th level descendants of n for c x can be reduced on average by log k (1 + l x (1  X  1 /k )) hops.
For any request to lookup c x , once it reaches any one of egate nodes of n i , it can be terminated and the rest of the search from the current node to n i is skipped. Consequently, the average hop number to lookup c x will be reduced by at most log k the average hop number H x to lookup c x is H x = O (log N log
As discussed in the above section, given content object c copy number equal to l x , the average hop number to lookup H x = O (log N  X  log k l x ) . The choice of the optimization crite-rion MAX_PERF or MIN_COST is dependant upon the system de-sign principle, the optimization objective can be either oriented to the best performance or the cost minimization. In this section, we proactively place the replicas of popular contents with the place-ment strategy in Section 4.1, and provide our solutions for both optimization criteria MAX_PERF and MIN_COST based on the analytical model.
Given the total number of L replicas in the structured P2P net-work, the optimal objective of MAX_PERF is to maximize the per-formance by minimizing the average hop number to lookup all content objects in the structured P2P network. In this paper, given the content object c x , the content popularity of c x can be defined as the proportion of all submitted queries that can be satisfied by This definition is consistent with [8, 29]. Obviously, 0  X  in the system. Since the queries follow the popularity distribution, the average hop number to lookup all content objects, H ,isgiven by: Then we achieve the following proportional principle: Theorem 2 Given C x L .

P ROOF . This is an optimization problem of H with the sub-jective C x to solve for the optimal value of l x in terms of p x . We find the Lagrange multiplier  X  that satisfies  X  H =  X   X  X  f where f = x =1 l x  X  L =0 . First, treating p x , log N and k as the constants, where u x is a unit vector. Next, Since  X  H =  X   X  X  f , then
Solving for l x gives
Substituting the above equation into f = C x gives since C x
By substituting the above equation back to Equation 5, we arrive at Theorem 2.

If we substitute l x = p x  X  L in Theorem 2 into Equation 1, then we get
Compared with the well-known square-root principle in unstruc-tured P2P network, our replica placement strategy in structured P2P network arrives at a different proportional principle. In unstruc-tured P2P with the random walk based search scheme, the aver-age hop number to search c x is proportional to the reverse of the number of n i  X  X  replicas, while we achieve the average search cost as shown in Theorem 1. Hence for the optimization problem MAX_PERF, we derive the proportional principle.

Furthermore, in Equation 9, we find that  X  C x is in fact the entropy definition of p x . Intuitively it makes sense since we have expected that our approach can accelerate the search for popular nodes, then the skew of the popularity distribution will play an important role in the optimization of the search perfor-mance, and taking the popularity as a probability function (of the query targets) entropy is a sound measure of the skew of the dis-tribution. To our knowledge, no previous P2P work has mentioned such a relationship.
In contrast to the problem of MAX_PERF, the optimization prob-lem of MIN_COST is to minimize the total number of replicas, to achieve the targeted constant result  X  for the average search cost H given in Equation 1. We derive the following theorem to solve the optimization problem of MIN_COST: Theorem 3 With the targeted constant result  X  to make H =  X  E x is the entropy of p x with the value of E p x =
P ROOF . : Similar to the proof of Theorem 2, we use the La-grange multiplier method to achieve the same result of l x rem 2:
Then substituting the result of l x by Equation 10 into H =  X  give: Since C x =1 p x =1 , by the similar steps in the proof of Theorem 2 we have By substituting the result of  X  into Equation 10, we arrive at Theorem 3.

The benefits of our optimal solution for MIN_COST is that the value of  X  is independent of the value of node count N . In the very skewed case, the optimal allocation of l x for content object in Theorem 3 can achieve O (1) average search hops. Compared with Beehive [20], our optimal solution from Theorem 3 has the following the advantages: (1) the distribution of popularity be the general probability distribution, unlike Beehive which only gives the numeric result for the Zipf like distribution; (2) our op-timal solution can be applied to the general structured P2P; while Beehive [20] is limited to the prefix matching based DHT like Pas-try [21].

As in Theorem 2, Theorem 3 also give rise to the proportional principle where l x  X  p x , and the entropy term of p x appears as well. Thus, the proportional principle and the relationship to the en-tropy of p x are related to the optimal solutions to both MAX_PERF and MIN_COST in structured P2P.
In this section, we apply the results of PoPCache to a real In-ternet application, web caching, by proposing a P2P-based web caching scheme, which can optimally tradeoff the cost and perfor-mance gain. We shall call this web caching system Web-PoPCache.
The target environment of our system is a large scale cooper-ate network typically with 100 to 10,000 or more client desktop machines. All of the clients are assumed to access the Internet web server through a direct connection or through a firewall. In each node Web-PoPCache runs as a daemon program. There are three components in Web-PoPCache: proactive cache proxy, local cache store, and the underlying P2P operation unit(see Figure 2). The web browser in each node is configured to use the proactive cache proxy to access the web objects. The proactive cache proxy is responsible for: (1)intercepting the http request from the web browser; (2) caching the requested web contents, (3) optimally replicating the popular web contents based on the replica place-ment strategy; and (4) maintaining the consistency of local caches. The cache store, used to locally store the cached web contents, is limited to a fixed storage size and it uses an LRU cache replacement policy. Finally the P2P operation unit provides the get/put opera-tions by finding the home node with node ID numerically closest to the hash ID of a web content URL. Note that web contents are typically of a reasonable size. In [25] there has been a study of the sizes of web contents of a real data set taken from the internet. In their study, the mean size was 4.4KB, the median size was 2.0KB, and the maximum size was 1.6MB.

When a client node submits a http request, the client node it-self, intermediate nodes, and home node can sever the http re-quest when the local cache stores of the client node, intermediate nodes or the home node contain the replicas of the requested web contents. Even if no replicas of such contents are found, Web-PoPCache redirects the requests to the original web server. As a result, Web-PoPCache utilizes the underlying P2P overlay to co-operatively serve the http request with the replicated web contents. The key point of Web-PoPCache is to adopt the replica placement strategy of PoPCache (Section 4.1) to setup the caching location, and the proportional principle(Section 4.2) to place the replicas of the web content. Therefore, our system can optimally tradeoff the cost(i.e. the number of replicas) and the performance(i.e. the aver-age search hop number). Through Figure 2, we illustrate the use of Web-PoPCache as following: 1. The client user sends the http request by submitting one URL 2. If there is a replica in the local cache store, such content is 3. Otherwise if no replica of such a URL exists , the proactive 4. When all intermediate nodes towards home node do not con-
With no careful consideration of the load balancing scheme, the skewed popularity of web content request(i.e the well-known Zipf distribution) can aggravate the load of the node hosted with the most popular web contents. However, since the replicas of these popular web content are placed along the levels of neighbors based on the results of Section 4, the requests for such web contents can be terminated at the levels of neighbors, which reduce the work-load. However, to handle a sudden burst of request for some pop-ular contents, each node can set up a threshold for the query rate based on its capacity. Before the load of the client is out of its capacity, the client can progressively copy the requested web con-tents to its direct neighbors. Similar situation holds for these direct neighbors. As a result, the popular web contents are progressively replicated throughout the levels of nodes from its home node.
Following the idea in [27], the total number of queries issued in the system in a time interval can be estimated by gossip based sam-pling protocol or [26] like deterministic protocol. The number of queries received at each local node n i divided by the total number of queries is the popularity p x of such a node. For each node, the number of received queries can be measured by counting at regu-lar interval. However a sudden burst of queries can rapidly change the rate, hence we measure the query arrival time to respond to the change in query rate. Then, the popularity p x can be estimated by the query rate against the total query rate. The evaluation includes two parts: (1) PART-A (Section 6.2): Comparison of our proposed cache placement strategy in Section 4.1 and several current cache placement strategies including CFS [9], Pastry [22](in spirit, [22] is a random strategy), and Beehive [20]; then further present the experimental results to evaluate our solu-tions for MAX_PERF and MIN_COST. For this part of the evalu-ation, we adopt the average search hops as the metric to evaluate the lookup efficiency. (2) PART-B (Section 6.3): Comparison of Web-PoPCache with Squirrel [13] to optimally tradeoff the better performance and load balancing against the cost. In this part, we adopt three main metric types: performance (request latency, ex-ternal bandwidth and hit ratio), load balancing (query count per machine), and overhead (cache storage per machine).

The underlying structured P2P is implemented with a Chord [23] based DHT protocol. All the experimental results are obtained by a trace-driven event simulator. To model the skewed popularity distribution, we have downloaded several publicly available web request trace files. For the evaluation of PART-A, we directly store all distinct URLs that appeared in the web request trace files to the home nodes; based on the request count for each URL we calcu-late the request popularity for such URL, then apply the result of Section 4 to place the copies of URLs in the delegate nodes. Then to model the skewed request popularity, for each row of the trace file, one request for the URL that appeared in the row is sent to lookup the URL; For PART-B, we build the web cache application upon the underlying P2P simulator. Instead of statically allocated before the requests, the URL copies are progressively placed in its delegate nodes based on the incoming requests(see Section 5.2).
Table 2 lists the Web traces we have used for the performance evaluation.
To clearly present the distribution of requests, we respectively show the count of requests per URL ordered by the ranking of pop-ularities of the trace file NLANR(pa.sanitized-access.20070109) and trace file BU(the contents in subdirectory condensed/272 of BU-www-client-traces.tar.gz) in Figure 3 and Figure 4. The traces can be publicly downloaded from IRCache [2] and BU Trace [3]. The ranking sequence presented in the x-axis of Figure 3 is ordered in a descending manner. To plot the points with a zero hit, we add the original requests by 1 so that the points with zero value can be shown on the y-axis with log -scale. From this figure, we can find that the requests count basically follows the well-known Zipf like distribution. Furthermore, based on the definition of entropy E compute the entropy for NLANR trace file and BU trace file:6.2579 and 2.7637. Since entropy is used to measure the randomness of the popularity distribution, we can find the popularity distribution of BU trace file is more skewed than NLANR trace file. In addi-tion, the larger value of k also reduces the entropy of popularity distribution.
We compare the placement strategy of PoPCache with other three strategies: the CFS strategy [9], Beehive strategy [20] and the ran-dom strategy. Following the strategy described in CFS [9] which is a cooperative file system based on Chord [23], we place replicas along the lookup path towards the destination. For random place-ment, which essentially is the strategy taken by Pastry [21], repli-cas are placed at randomly chosen nodes. In the experiment setting, N = 4000 and k =8 . For each web content uniquely identified by a URL, we adopt the proportional principle to assign the number of replicas based on p x and place the replicas by four strategies as de-scribed above. By varying the total number of replicas, we measure the average lookup hops H for each strategy.

Figure 5 plots the average lookup hops for the above four cases by varying the value of D , it can be found that PoPCache outper-forms other three strategies. This can be explained that: (1) the PoPCache placement achieves the least average lookup hops be-cause the replicas placed in the the k -ary tree T i can benefit search-ing from any source nodes; (2)in the Chord or random strategy, only the searches from some specific source nodes can be acceler-ated by the alink s; (3) for Beehive [20], only roughly half of the cache copies can be helpful to the lookup due to the limitation that it is target for only the prefix based DHT. In Figure 6 we compare the our proposed optimal solution for MAX_PERF under two trace files NLANR and BU, we also plot the results of no caching in Figure 6. By no caching we mean that no replicas is utilized in PoPCache. Theoretically the adoption of the proportional principle in PoPCache can achieve less lookup hops for the same number of replicas. From this figure, we can find that the average lookup hop number for trace file NLANR is larger than that for trace file BU. This result can be consistently explained by Equation 9 in Section 4.2.1, because the entropy of p x for popularity in trace file NLANR is larger than that in trace file BU.
Figure 7 plots the total copy number C with the goal to satisfy the target lookup hops  X  respectively for N = 800 and N = 8000 Naturally, to satisfy a smaller  X  , more copies will be consumed. From Figure 7, for N = 8000 , to achieve less than 1 hops from  X  =8 to  X  =7 , about 4 . 5  X  10 7 more replicas are needed; while to achieve below 1 hops from  X  =3 to  X  =2 , about 3 . 1  X  more replicas are needed: the same reduction of the average lookup hops consumes nearly 100 times of replicas. Similar situation holds for N = 800 , though when N grows from 800 to 8000, to achieve the average lookup hops H , more replicas are required for N=8000 than N=800. Also due to the different entropy values for BU trace and NLANR trace, more replicas are consumed for NLANR trace than BU trace for both N=800 and N=8000. This experiment pro-vides the guideline for system administrator to tradeoff the perfor-mance gain and cost.
In this section, we evaluate Web-PoPCache for cooperative web caching in terms of the load balancing, the consumed external band-width and hit ratio. Each participated node in Web-PoPCache is assumed to be a desktop machine with similar capacity. For sim-plicity, Web-PoPCache adopts the LRU policy.
For a node n i , we set the load of n i , L i , to be the the number of incoming http requests with URLs that are served by n i ures 8 and 9 respectively plot the load distribution of NLANR and BU trace data for Web-PoPCache and Squirrel [13]. The x-axis of Figure 8 and Figure 9 present the load (number of requests to server in each node) boundary values, where the left most point on the x-axis corresponds to a load (request count in some node) of less than 10 both for the NLANR and BU trace, and the right most point refers to a load larger than 1500 and 1000 respectively for the NLANR and BU trace; and y-axis represents the count of all nodes with load L i within the load boundary values.
 From Figures 8 and 9, we can find that the load distribution of Squirrel [13] is heavily tailed and is consistent with the Zipf distri-bution given in Figures 3 and 4. It is because Squirrel [13] does not consider the problem of load balancing. For example in Fig-ure 9, the rate of nodes with load within [10,250] among all nodes is 98.039%. When overloading is considered, for one threshold load value(=250 in Figure 9), 7 . 698% of the nodes in Squirrel jave loads greater than the threshold value and they have to serve 49 the requests! While in Web-PoPCache, only 0 . 35% of the nodes exceed the load threshold and they only serve 1 . 862% of the re-quests. It is found that Web-PoPCache can effectively balance the load. A similar situation holds for the NLANR trace file seen in Figure 8.
We calculate the external bandwidth as the bytes transferred be-tween Web-PoPCache and the external origin servers. In this sec-tion, we compare Web-PoPCache with Squirrel [13] on the hit ra-tio and external bandwidth savings by measuring the impact of the per-node cache storage size. Since the cache replacement strategy is implemented with LRU algorithm, the rarely requested contents in the local cache store are always replaced with the new incoming popular contents which are more frequently requested to be placed in the local cache store. Obviously the larger storage size of lo-cal cache store can accommodate more replicas and can increase the hit ratio; also a large value of node count N can provide more nodes to accommodate the replicas and can also increase the hit ratio. Both Figures 10 and 11 show the impact of local cache stor-age size and node count N . When N grows, the total size of all local storage in PoPCache is also increased. As a result, the hit rate grows. Since the data size varies from web object to web object, the x-axis in Figures 10 and 11 is represented as the cache storage size (i.e. size of the priority queue to implement the local cache store), instead of the total data size of cached web contents. In our experiments, the first time to request a URL always is required to visit the original web server outside the Web-PoPCache, as a result, the infinite hit ratio appears in Figure 10 and Figure 11 is different in that of the computed hit ratio in table 2 where the downloaded trace file only records the requested URL within some interval, and this could miss some of the first-time requests.

When the requests for some URLs are missed in Web-PoPCache, the original web servers will have to requested via the Internet through firewall. As a result, the less hit ratio means that more external bandwidth will be consumed, as plotted in Figure 12 and Figure 13.
Since Web-PoPCache is targeted to a large corporate intranet en-vironment by connecting a large number of desktop machines to provide the cooperative web caching. The communication laten-cies between the desktop machines within the Web-PoPCache are of the order of a few milliseconds, while the latencies between one desktop machine within the Web-PoPCache and the external web severs are at least an order of magnitude larger than the internal la-tency. Consequently, the request latency between issuing the HTTP request and receiving the response is determined by two factors: (1) hit ratio; (2) the external latency. If the HTTP requests are hit by the cached web objects placed in Web-PoPCache, the request la-tency is within several milliseconds; if missed, the request latency is dominated by the external latency. Figures 14 and 15 respec-tively plot the average request latency for NLANR trace and BU trace.

It can be found that the trends of the average latency in Fig-ure 14 and Figure 15 follows that of Figure 12 and Figure 13, and are verse to that of Figure 10 and Figure 11. From Figure 10, 11, 12, 13, 14 and 15, we can demonstrate the benefits achieved by Web-PoPCache with increased hit rate, reduced bandwidth and smaller request latency.
Since the nodes in Web-PoPCACHE could leave or fail suddenly, the replicated web contents in those nodes will be lost. In this ex-periment, we assume the nodes fail randomly with a given failure rate. Figure 16 shows the effect of hit ratio by node failure rate. From this figure we can find that the hit ratio is non-nearly reduced as the node failure ratio grows. It can be explained as following. Due to the proportional replication principle, the more popular con-tents are placed to more nodes and less popular contents are placed to less nodes. With a given rate of node failure, the more popular contents can survive with a higher probability than the less popular contents, i.e. the hit ratio for popular contents is higher than the hit ratio of un-popular contents. Simultaneously, the http requests are issued by with skewed popularity, the high popular contents receive more request than those less popularity. As a result, the hit ratio curve vs node failure rate is the convex shape. Moreover, we can also find that with the same node failure ratio, the hit ratio for N=4000 is higher than the hit ratio for N=2000. For N=2000, each node are placed with more contents than N=4000; and more contents are lost due to the node failure. Consequently, the larger number of nodes in Web-PoPCACHE can help the high hit ratio un-der node failure. The reason that we chose hit ratio as the measure metric is that when the failure happens in the intermediate lookup path towards the destination node instead of no failure of the des-tination node, more hops will be consumed but no effect of the hit ratio, and the replicated web contents can be always found in the destination node. As a result, the missed requests caused by node failure will visit the original web servers by Internet, and then we focus on the effect of hit ratio caused by node failure.
In this paper, with consideration of the skewed popularities of data contents in a P2P environment, we propose a proactive caching method PoPCache with a novel placement strategy based on the closed form solutions for two optimization objectives: MAX_PERF and MIN_COST. In the optimal solutions, there are the following interesting properties: (1) the copy number assigned to each spe-cific content is proportional to its popularity, deviating from the well-known square-root result achieved in unstructured P2P; (2) the achieved optimal results are related to the entropy of popular-ity. Next based on these results we propose an optimal proactive web caching scheme, Web-PoPCache, which can optimally trade-off the performance gain and cost. By means of web cache trace driven simulations, our extensive evaluation results demonstrate the advantages of PoPCache and Web-PoPCache. [1] In http://squid.nlanr.net . [2] In ftp://ircache.nlanr.net/Traces/DITL-2007-01-09 . [3] In http://ita.ee.lbl.gov/html/contrib/BU-Web-Client.html . [4] Karl Aberer, Philippe Cudr X -Mauroux, Anwitaman Datta, [5] Serge Abiteboul, Angela Bonifati, Gregory Cobena, Ioana [6] James Aspnes and Gauri Shah. Skip graphs. In SODA , 2003. [7] Edith Cohen and Scott Shenker. Replication strategies in [8] Brian F. Cooper. An optimal overlay topology for routing [9] Frank Dabek, M. Frans Kaashoek, David R. Karger, Robert [10] Fred Douglis and Thomas Ball. Tracking and viewing [11] Sylvia Ratnasamy etc. A scalable content-addressable [12] Li Fan, Pei Cao, Jussara M. Almeida, and Andrei Z. Broder. [13] Sitaram Iyer, Antony I. T. Rowstron, and Peter Druschel. [14] H. V. Jagadish, Beng Chin Ooi, and Quang Hieu Vu. Baton: [15] David R. Karger, Alex Sherman, Andy Berkheimer, Bill [16] Prakash Linga, Indranil Gupta, and Ken Birman. Kache: [17] Qin Lv, Pei Cao, Edith Cohen, Kai Li, and Scott Shenker. [18] Gurmeet Singh Manku, Mayank Bawa, and Prabhakar [19] Venugopalan Ramasubramanian and Emin G X n Sirer.
 [20] Venugopalan Ramasubramanian and Emin G X n Sirer. The [21] Antony I. T. Rowstron and Peter Druschel. Pastry: Scalable, [22] Antony I. T. Rowstron and Peter Druschel. Storage [23] Ion Stoica, Robert Morris, David R. Karger, M. Frans [24] Jia Wang. A survey of web caching schemes for the internet. [25] Allison Woodruff, Paul M. Aoki, Eric Brewer, Paul Gauthier, [26] Praveen Yalagandula and Mike Dahlin. A Scalable [27] Venugopalan Ramasubramanian Yee Jiun Song and [28] Ben Y. Zhao, John Kubiatowicz, and Anthony D. Joseph. [29] Ming Zhong and Kai Shen. Popularity biased random walks
