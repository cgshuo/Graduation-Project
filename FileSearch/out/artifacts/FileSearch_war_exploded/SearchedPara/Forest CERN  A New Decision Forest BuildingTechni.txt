 From 2005 to 2020, the  X  X igital Universe X  will expand by a factor of 300, from 130 Exabytes to 40,000 Exabytes, or 40 trillion Gigabytes (more than 5,200 Gigabytes for every man, woman, and child in 2020) [ 1 ]. Thus it is easily under-standable that we need highly efficient automated means if we want to capitalize these data. Data mining is the method of automatically discovering useful infor-mation from large data sets [ 21 ]. Classification and clustering are two widely used data mining tasks that are applied for knowledge discovery and pattern recognition.
 Classification aims to generate a function (commonly known as a classifier) that maps a set of non class attributes m = { A 1 ,A 2 , ..., A class attribute C from an existing data set D [ 21 ]. A data set D can be regarded as a two dimensional table with columns/attributes ( { A 1 rows/records ( { R 1 ,R 2 , ..., R n } ). A data set generally has two types of attributes such as numerical (e.g. Age) and categorical (e.g. Gender). Out of all categorical attributes, one is chosen to be the class attribute. All other attributes are termed as non class attributes. A classifier is then built from an existing data set (i.e. training data set) where the values of the class attribute are present and then applied on unseen/test records in order to predict their class values. years [ 2 , 18 , 20 ]. Interestingly, an ensemble of classifiers is found to be effective for unstable classifiers such as decision trees [ 21 ]. Decision trees are considered to be an unstable classifier because a slight change in a training data set can induce significant differences between the decision trees generated from the original and modified data sets. A decision forest is an ensemble of decision trees where an individual decision tree acts as a base classifier. The classification is performed by taking a vote based on the predictions made by each decision tree of the decision forest [ 21 ].
 accurate and diverse (in terms of classification errors) individual decision trees as base classifiers [ 11 , 18 ]. An accurate decision tree can be generated by feeding a training data set to a decision tree algorithm such as CART [ 7 ]. Nevertheless, a single decision tree can discover only one set of logic rules and thus may wrongly predict the class value of a test record which could have been predicted correctly by a more appropriate logic rule. A different decision tree can be obtained from a differentiated data set which may include a more appropriate logic rule for the given test record. If a decision forest contains a set of decision trees which are different from each other then some of the trees may discover appropriate logic rules for a set of test records while some other trees may discover appropriate logic rules for another set of test records resulting in better generalization per-formance for the forest. However, to establish the scope of generating too diverse trees can be the cause of generating less accurate trees as optimization on the two conflicting objectives cannot be attained simultaneously [ 11 ]. Thus, decision forest algorithms need to draw a balance between how different/diverse as well as how accurate trees they need in order to increase the ensemble accuracy. and diverse decision trees. In Sect. 2 , we briefly introduce some of the prominent and recent algorithms and their limitations. Apparently, there is room for further improvement in achieving higher ensemble accuracy for decision forests. In this paper, we propose a novel decision forest algorithm called  X  ously Excluding Root Node (Forest CERN)  X  X hat aims to build a set of highly accurate decision trees by exploiting the strength of all non class attributes available in a data set, unlike some existing techniques that use a subset of non class attributes. At the same time to promote strong diversity, emphasizes to exclude attributes that participated in the root nodes of previous trees by imposing penalties on them to deter them appear in some subsequent trees. Penalties are gradually lifted in such a manner that those attributes can reappear after a while.
 of the well-known decision forest algorithms. Section 3 explains the proposed Forest CERN algorithm in detail. Section 4 discusses the experimental results. Finally, we offer some concluding remarks in Sect. 5 . In literature we find many forest building algorithms that differentiate training data set in different ways to generate diverse decision trees. We introduce some of the prominent forest building algorithms as follows. (a) Bagging [ 5 ]: Bagging generates a new training data set D of D i are chosen randomly from the original training data set D . A new training data set D i contains the same number of records as in D .Thus some records of D can be chosen multiple times and some records may not be chosen at all. This approach of generating a new training data set is known as bootstrap sampling. On an average 63.2 % of the original records are typically chosen in a bootstrap sample [ 10 ]. Bagging generates a predefined number ( T ) of bootstrap samples D 1 , D 2 , ..., D T using the above approach. A decision tree building algorithm is then applied on each bootstrap sample
D i ( i =1 , 2 ,...,T ) in order to generate T number of trees for the forest. (b) Random Subspace [ 11 ]: The Random Subspace algorithm randomly draws a subset of attributes (subspace) f from the entire attribute set m in order to determine the splitting attribute for each node of a decision tree. The
Random Subspace algorithm is applied on the original training data set in building every decision tree. (c) Random Forest [ 6 ]: Random Forest is technically a fusion of Bagging and
Random Subspace algorithms. In Random Forest, the Random Subspace algorithm is applied on bootstrap samples instead of the original training data set. (d) MDMT [ 12 ]: In MDMD (Maximally Diversified Multiple Decision Tree), each decision tree tests a completely different set of attributes than the set of attributes tested in any other decision tree. MDMT builds the first decision tree using a traditional decision tree building algorithm such as
CART [ 7 ]. All non class attributes that have been tested in the first tree are then removed from the data set and the decision tree building algorithm is again applied on the modified data set to build the second tree. The process continues until either the user defined number of trees is generated or all non class attributes of the data set are removed. (e) CS4 [ 15 ]: To build the decision forest, CS4 (Cascading and Sharing Trees) first ranks all attributes of a training data set according to their classification capacities (e.g. Gain Ratios [ 19 ]). Then in a cascading manner, the attribute with the highest Gain Ratio value is selected as the root node of the first tree; the attribute with the second highest Gain Ratio value is selected as the root node of the second tree and so on. (f) SysFor [ 13 ]: SysFor (Systematically Developed Forest of Multiple Decision
Trees) takes a user input to determine the number of decision trees to be generated. Then a set of  X  X ood attributes X  X nd corresponding split points are determined based on a user defined  X  X oodness X  X hreshold and  X  X epara-tion X  X hreshold. SysFor then starts building decision trees by placing the good attributes one by one as the root attribute (at Level 1) of a tree, and thereby Bootstrap samples are the only source of diversity in Bagging. Theoretically, a bootstrap sample contains  X  63 . 2 % of the original records of a training data set; the remaining  X  36 . 8 % records are repeated [ 10 ]. This sampling ratio that theoretically can occur in bootstrap samples is not optimal for every data set [ 17 ] and thus may not extract strong diversity specially for data sets with redun-dant/similar (i.e. difference is low) records.
 attributes (subspace) f from the entire attribute set m in order to determine the splitting attribute for each node splitting event of a decision tree. In effect, individual tree accuracy and diversity among decision trees depend on the size of f .If f is sufficiently small then the chance of having the same attribute in different subspace becomes low. Thus the trees in a forest tend to become more diverse. However, a sufficiently small f may not guaranty the presence of the adequate number of good attributes (i.e. the attributes with high classification capacity) resulting in decreased individual accuracy. Thus the value of a strong role in determining the ultimate efficiency of a decision forest and com-monly known as the hyperparameter [ 4 ]. In Breiman X  X  original Random Forest [ 6 ], | f | is chosen to be int (log 2 | m | )+1.
 at the same rate to the increase of | m | . For example, let us assume that we have a low dimensional data set consisting of 4 attributes. Thus a splitting attribute is determined from a randomly selected subspace of 3 attributes ( int (log encompassing 75 % of the total attributes. As a result, the chance of appearing similar attributes in different subspaces becomes high, resulting in decreasing diversity among the trees. On the other hand, when | m | is large say, 150 then | f | contains 8 randomly chosen attributes ( int (log 5 % of the total attributes. Hence, if the number of good attributes is not high enough in m then the chance of containing adequate number of good attributes in a subspace f becomes low, which is supposed to cause low individual accuracy of the trees as described before.
 of trees based on the non class attributes available in a training data set. CS4 can generate at most | m | trees for the forest. For example, from a low-dimensional data set such as Balance Scale with four non class attributes [ 16 ] CS4 can build only four trees. SysFor tries to eliminate this constraint by placing alternative good attributes in Level 2. However, for Balance Scale data set SysFor can gen-erate only 12 trees which is still a small number in ensemble standard [ 20 ]. This problem is more severe for MDMT. It can generate only one tree from the Balance Scale data set as no non class attributes are left to generate the second tree. Besides, both CS4 and SysFor just change the root nodes. In CS4, each non class attributes are placed in the root node once in a cascading manner according to their Gain Ratios. As a result, some attributes with vary low clas-sification capacity can be placed in the root node resulting in not generating some trees entirely. SysFor places only good attributes in the root node but the good attributes are determined by a user defined  X  X oodness X  X hreshold. If this user input is not tuned correctly, the number of good attributes can be too high or too low. Furthermore, just changing the root nodes should not be sufficient in rendering strong diversity as trees may be taken over by some attributes with relatively higher classification capacity just below the root node. In order to address the issues raised in Sect. 2 , we propose a new method for decision tree ensemble construction called Forest CERN . The main feature of our technique is that it strives to exclude attributes that participated in the root nodes of previous trees by imposing penalties on them to deter them appear in some subsequent trees. But, what is the impact of excluding attributes that appeared in root nodes? For example, let A i be the attribute that was placed in the root node of the first tree. As a result, all logic rules generated from the first tree starts with attribute A i . Thus, to generate different logic rules from the next tree, it is preferable to exclude A i from that tree. In order to facilitate the exclusion of A i (in this case), we propose our novel penalty/weight imposing strategy that works as follows: Let, we have a data set D with m = { A 1 ,A 2 , ..., A m } the weight values for all attributes are set to 1.0 (default weights). To determine the splitting attribute at first the classification capacity such as Gain Ratio [ 19 ] or Gini Index [ 7 ] of each attribute is calculated. Then a new merit value is obtained by multiplying the classification capacity with respective weight for each attribute. After the merit values of all attributes are calculated, the attribute with the highest merit value is selected as the splitting attribute. We generate the first tree from a bootstrap sample of D using the default weights. Thus, the first tree is generated in the same way as Bagging [ 5 ]. To generate the second tree, we first isolate the attribute that appeared in the root node of the first tree. Let A i be the attribute that appeared in the root node of the first tree. Then, we calculate the weight of A i as follows (Eq. 1 ): Here, | m | is the number of non class attributes in D . For example, when | m | = 25, the weight of A i is 1 25 . The weights of other attributes remain as 1.0. The next (second) tree is generated from another bootstrap sample of D with the updated weight values. Due to it X  X  disadvantageous weight, A chance to appear in the entire second tree (let alone in the root node) compared to other attributes. Let, A j be the attribute that appeared in the root node of the second tree. Thus, the weight of A j will be reduced after generating the second tree. If the weight imposing approach is continued in the same manner, all the attributes qualified to appear in the root node will acquire disadvantageous weights. As a result, trees may not be generated at all. To prevent this scenario, we increase the weights of all attributes having weight &lt; 1 . 0 other than the current root attribute. For example, after generating the second tree the weight of A j is reduced to 1 | m | and at the same time the weight of A adding 1 | m | to it X  X  current weight value. In this way, when of A i can reach to the default weight 1.0 if it misses to appear in the root nodes of all subsequent 24 trees. But, whenever A i reappears in the root node of any tree, the weight is again set to 1 | m | and at the same time the weights of all other attributes with weight &lt; 1 . 0 are increased by 1 | m | can be further illustrated in Table 1 .
 tageous weights relative to other non class attributes. In the same way, a set of attributes will obtain disadvantageous weights when building the This phenomenon is clearly different form either of CS4 or SysFor; where each attribute comes iteratively in the root node and thus attributes with relatively higher classification capacity retain their presence just below the root nodes. In fact, the proposed technique exhibits slightly similar effect of MDMT where the proposed technique can take out a set of attributes with severely disadvanta-geous weights from participating in some subsequent trees. However, in the pro-posed technique attributes can regain weights to be able to reappear in the root node. This phenomenon helps preventing non-deserving attributes (attributes with very low classification capacity) to appear in the root node. Furthermore, even when the same attribute reappears in the root node the proposed technique strives to generate different tree through different bootstrap sample and different combination of disadvantageous attributes. We performed an elaborate experimentation on fifteen (15) well known data sets that are publicly available from the UCI Machine Learning Repository [ 16 ] representing a variety of areas. The data sets used in the experimentation are described in Table 2 . For example, the Chess data set has 36 non class attributes, 3196 records with two (02) distinct class values. For our experimentation, we remove records with missing values (Table 2 shows the number of records with no missing values) and identifier attributes such as Transaction ID applicable data set. We generate 100 trees for every decision forest since the number is considered to be large enough to ensure convergence of the ensemble effect [ 2 , 4 , 9 ]. We use Gini Index [ 7 ] as a measure of classification capacity in accordance with Random Forest [ 6 ]. The minimum Gini Index/merit value is set to 0.01 for any attribute to qualify for splitting a node. Each leaf node of a tree contains at least two records and no further post-pruning is applied. We apply majority voting to aggregate results for forest classification [ 6 , 18 ]. The experimentation is conducted by a machine with Intel(R) 3.4 GHz processor and 8 GB Main Memory (RAM) running under 64-bit Windows 7 Enterprise Operating System. All the results reported in this paper are obtained using 10-fold-cross-validation (10-CV) [ 3 , 14 , 15 ] for every data set. In 10-CV, a data set is divided randomly into 10 segments and from the 10 segments each time one segment is regarded as the test data set (out of bag samples) and the rest 9 segments are used for training decision trees. In this way, 10 training and 10 corresponding testing data sets are generated. In our experimentation, we generate 100 trees from each training data set (thus 1000 trees in total) for each decision forest algorithm and then evaluate their performance with the corresponding testing data sets. All the performance indicators reported in this paper are the average values obtained from the 10 testing data sets and the best results are distinguished through bold-face .
 cators for any decision forest algorithm [ 1 ]. In Table 3 we present the EA (in percent) along with Ranks (1 for the best to 4 for the worst) of Bagging, Ran-dom Subspace, Random Forest and the proposed Forest CERN sets considered. We do not include the results generated from CS4, SysFor and MDMT as they are not able to generate 100 trees for most of the data sets used in the experimentation and as a result may not perform to the level of the considered algorithms.
 EA Rank: 2.67), Random Subspace for 1 data set (Avg. EA Rank: 3.27), Ran-dom Forest for 3 data sets (Avg. EA Rank: 2.33) whereas the proposed CERN becomes the best for 10 out of 15 data sets with Avg. EA Rank of 1.40 . We already know that better ensemble accuracy of a decision forest is a con-sequence of a better balance between individual accuracy and diversity among the trees. Thus, to explain the reason behind this outcome, we first compute individual accuracies (in percent) of each tree of a forest to compute the Aver-age Individual Accuracies (AIA) for the forest [ 2 ]. Kappa typically estimates the diversity between two trees T i and T j . Diversity among more than two trees is typically computed by first computing the Kappa ( K ) value of a single tree T with the ensemble of trees except the tree in consideration [ 2 ]. The combined prediction of the ensemble (excluding T i ) can be regarded as a single tree T Then Kappa is computed between T i and T j as shown in Eq. 2 , where Pr ( a ) is the probability of the observed agreement between two classifiers T and Pr ( e ) is the probability of the random agreement between T the Kappa for every single tree T i of a forest is computed we then compute the Average Individual Kappa (AIK) for the forest [ 2 ].
 From Table 4 we see that the proposed Forest CERN delivers most diverse decision trees (lower AIK value indicates higher diversity) with retaining the second highest AIA value (higher AIA value indicates better quality individual trees). Consequently, Forest CERN outperforms other contending algorithms in terms of EA. Now, to access the significance of the improvement we conduct a non-parametric (EA do not follow a normal distribution and thus do not sat-isfy the conditions for any parametric tests) one-tailed Wilcoxon Signed-Ranks Test for n = 15 (number of data sets used) with the significance level  X  =0 . 05 (thus the critical value is: 30) [ 22 ]. Wilcoxon Signed-Ranks Test is said to be more preferable to counting only significant wins and losses for comparison between two classifiers [ 8 ]. Here we see from Fig. 1 that Forest CERN cantly better (in terms of EA) than all three contending algorithms on 15 widely used data sets as the test values remain lower than the critical value for every head-to-head comparisons.
 we compare them on smaller 10-tree ensemble [ 20 ] as SysFor may fall short of generating a large ensemble for every data set. The results shown in Table 5 clearly depict the distinction between SysFor and Forest CERN Forest CERN generates more diverse trees than SysFor for every data set as a result of excluding root attributes in some subsequent trees. On the contrary, SysFor generates more accurate trees for every data sets with less diversity ver-ifying the fact the trees generated from SysFor are more similar to each other as a result of probable presence of attributes with high classification capacity in each tree. Ultimately, Forest CERN outperforms SysFor in terms of EA. is the effect of ensemble size on EA for Forest CERN (i.e. when is applied in the scale of larger 100-tree ensemble and smaller 10-tree ensemble). We see, the average EA is higher for 100-tree Forest CERN Forest CERN ( 88.30 vs 85.45) even when the average of AIA/AIK values are very competitive (74.67/0.5840 for 100-tree Forest CERN vs. 76.23 / 0.5814 for 10-tree Forest CERN ) as smaller Forest CERN may not ensure convergence of the ensemble effect for many data sets. In this paper, we propose a new decision forest building algorithm which strives to exclude attributes that participated in the root nodes of pre-vious trees by imposing penalties (disadvantageous weights) on them to deter them appear in some subsequent trees. Penalties are eventually lifted in such a manner that those attributes can reappear in root nodes after a while. We find that trees generated from Forest CERN are more diverse compared to other contending algorithms which can be helpful for discovering interesting knowl-edge. Also, Forest CERN is fully independent of any parameter value unlike Random Subspace, Random Forest and SysFor to be more evenly suitable for a wide range of data sets.
 However, there is an apparent limitation of Forest CERN specially when the number of non class attributes is high and a few of them have high classification capacity. In this case, after receiving drastic disadvantageous weights (due to high dimension) attributes with high classification capacity may disappear for a large number of intermediate trees leading to lowering AIA for the forest. One obvious solution to this problem is to reduce the attribute space. Another possible solution is probably improving Forest CERN to address the problem. In future, we intend to extend our work by including data sets with more number of attributes.

