 Dian Gong, Xuemei Zhao, G X rard Medioni DIANGONG , XUEMEIZ The concept of manifold has been extensively used in almost all aspects of machine learning such as non-linear dimension reduction ( Saul &amp; Roweis , 2003 ), visu-alization ( Lawrence , 2005 ; van der Maaten , 2009 ), semi-supervised learning ( Singh et al. , 2009 ), multi-task learn-ing ( Agarwal et al. , 2010 ) and regression ( Steinke &amp; Hein , 2009 ). Related methods have been applied to many real-world problems in computer vision, computer graphics, web data mining and more. Despite the success of mani-fold learning (in this paper, manifold learning refers to an y learning technique that explicitly assumes data have mani-folds structures), people find there are several fundamenta l challenges in real applications, (1) Multiple manifolds with possible intersections: in man y circumstances there is no unique (global) manifold but a number of manifolds with possible intersections. For in-stance, in handwritten digit images, each digit forms its own manifold in the observed feature space. For human motion, joint-position (angle) of key points in body skele-ton form low dimensional manifolds for each specific ac-tion ( Urtasun et al. , 2008 ). In these situations, modeling data as a union of (linear or non-linear) manifolds rather than a single one can give us a better foundation for many tasks such as semi-supervised learning ( Goldberg et al. , 2009 ) and denoising ( Hein &amp; Maier , 2007 ). (2) Noise and outliers: one critical issue of manifold learn -ing is whether the method is robust to noise and outliers. This has been pointed out in the pointer work on nonlin-ear dimension reduction ( Saul &amp; Roweis , 2003 ). For non-linear manifolds, since it is not possible to leverage all da ta to estimate local data structure, more samples from mani-folds are required as the noise level increases. (3) High curvature and local linearity assumption: typi-cal manifold learning algorithms approximate manifolds by the union of locally linear patches (possibly overlapped). These local patches are estimated by linear methods such as principal component analysis (PCA) and factor analysis (FA) ( Teh &amp; Roweis , 2003 ). However, for high curvature manifolds, many smaller patches are needed, but this often conflicts with the limited number of data samples. In this paper, we investigate the problem of robustly es-timating data structure under the multiple manifolds as-sumption. In particular, data are assumed to be sam-pled from multiple smooth submanifolds of (possibly dif-ferent) low intrinsic dimensionalities with noise and out-liers. The proposed scheme named Robust Multiple Man-ifolds Structure Learning (RMMSL) is composed of two stages. In the local stage, we estimate local manifolds structure taking into account of noise and curvature. The global stage, i.e., manifold clustering and outlier detec-tion, is performed by constructing a multiple kernel sim-ilarity graph based on local structure learning results by i n-troducing a novel curved-level similarity function. Thus, issue (1) is explicitly addressed, and (2) and (3) are par-tially investigated. A demonstration of the proposed ap-types of global learning tasks such as dimension reduc-tion, denoising and semi-supervised learning can be han-dled based on each individual manifold cluster by ex-isting methods ( Lawrence , 2005 ; van der Maaten , 2009 ; Hein &amp; Maier , 2007 ; Sinha &amp; Belkin , 2010 ). Our problem statement is rigorously expressed as follows. n ) and outliers { x j } ( n 1 + 1  X  j  X  n 1 + n 2 ). Inlier points { x } n 1 i = 1  X  R D  X  n 1 are assumed to be sampled from multiple submanifolds M c (1  X  c  X  n c ) as follows, function that maps latent variable  X  i from latent space R to ambient space R D for manifold M c . d c ( &lt; D ) is the in-trinsic dimensionality of M c and it can vary for different manifolds. The task of local structure learning is to esti-mate the tangent space T x sionality d C J (  X  . Details are given in section 3. The tasks of global struc-ture learning addressed in this paper are, to detect outlier s { x j } ( n 1 + 1  X  j  X  n 1 + n 2 ) and to assign manifolds cluster label c i  X  { 1 , 2 ,..., n c } for inliers { x i } (1  X  i This is given in section 4. Experimental results are shown in section 5 and followed by conclusion in section 6. Recent works on manifold learning and latent vari-able modeling focus on low-dimensional embedding and label propagation for high dimensional data, which are usually assumed to be sampled from a single manifold ( Saul &amp; Roweis , 2003 ; Sinha &amp; Belkin , 2010 ; Lawrence , 2005 ; van der Maaten , 2009 ). To address the multiple manifolds problem, ( Goldberg et al. , 2009 ) gives theoretical analysis and a practical algorithm for semi-supervised learning with the multi-manifold assumption. As a complementary approach to previous works, RMMSL mainly focuses on unsupervised learning with the multi-manifold assumption and it can be combined with existing approaches 1 .
 In the global structure learning stage, RMMSL focuses on clustering and outlier detection. Clustering is a long stan d-ing problem and we only review manifold related cluster-ing methods. If data lie on a low-dimensional submanifold, distance on the manifold is used to replace the Euclidean distance in the clustering process. This leads to spectral clustering ( Ng et al. , 2002 ; Zelnik-Manor &amp; Perona , 2005 ; Maier et al. , 2009 ), one of the most popular modern cluster-ing algorithms. We refer ( von Luxburg , 2007 ) as a survey for spectral clustering. However, most of these works do not explicitly consider multiple intersecting manifolds. State-of-the-art multiple subspace learning methods such as ( Elhamifar &amp; Vidal , 2009 ; Vidal et al. , 2010 ; Yan &amp; Pollefeys , 2006 ) acquire good performance on multiple linear (intersecting) subspace segmentation wit h the assumption that the intrinsic manifolds have linear structures. However, real data often have nonlinear intrin -sic structures, which brings difficulty for them. Nonlinear manifold clustering is investigated in ( Goh &amp; Vidal , 2007 ), which mainly focuses on separated manifolds. As an extension of ISOMAP, ( Souvenir &amp; Pless , 2005 ) proposes an EM algorithm to perform multiple manifolds clustering, but results are sensitive to initializations and the E-step is heuristic. Recently, ( Wang et al. , 2011 ) proposes an elegant solution of manifolds clustering by constructing the affinity matrix based on estimated local tangent space. Correctly and efficiently estimating local data structure i s a crucial step for data analysis. As explained before, prob-lems like noise and high curvature make local structure es-timation challenging. This section addresses the issue of how to model and represent local structure information on manifolds. We start from local Taylor expansion, and lo-cal manifold tangent space is represented by the Jacobian matrix under the local isometry assumption.
 Local Taylor Expansion. Without additional assump-tions, the model in eq. 1 (section 1) is not well defined. For instance, if f ( ) and point set {  X  i } satisfy eq. 1 , then f ( g  X  1 ( )) and point set { g (  X  i ) } satisfy it too, where g any invertible and differential mapping function from R d to
R d . Thus, the local isometry assumption is enforced at point  X  i  X  R d , where  X  is in the  X   X  neighborhood of  X  i . The above con-matrix ( Smith et al. , 2009 ), i.e., J T ( f ( ) ;  X  i ) From eq. 1 , by using Taylor expansion to incorporate both Taylor approximation error and inlier noise we get, where x i we have, Assume there are m i points {  X  i neighborhood of {  X  i , x i } , the local data matrix X i as ordinate matrix T i for X i is denoted as [  X  i R  X  m i , the corresponding local Taylor approximation error matrix E i is denoted as [ e i the local inlier noise matrix [ n , i the short notation of J ( f ( ) ;  X  i ) .
 geneous Gaussian distribution N ( 0 ,  X  2 n I D ) (1  X  j Furthermore, we treat errors as independent Gaussian ran-dom vectors with different covariance matrices, i.e., e i N ( 0 ,  X  2 I D  X  i grated error vector  X  i where  X  2 n indicates the scale of the noise X  X  covariance and  X  2 indicates the scale of the error.  X  i geneous property of different error of Taylor expansion on points x i considered. Instead of treating error e i across different points, we argue the error is proportional to the relative distance ||  X  i erty of Taylor expansion.
 Inference. To reflect the inhomogeneous property of er-ror  X  i noise), we propose the following objective function to esti -mate J i , where S  X  R m i  X  m i is the diagonal weight matrix with s s ( x . Intuitively, we emphasize the error minimization for x i more than x i Then we discuss how to choose  X  i ingly. First, we choose  X  i monotonically non-decreasing function in the non-negativ e domain. Supported by the fact of local-isometry (eq. 2 ), we have  X  i Eq. 7 immediately suggests a weight function, s j j =  X   X   X  i k 2 ) = 1 / (  X  2 n +  X  2  X  ( || x i j  X  x i || 2 ))  X  ( ) , s ( ) can be determined and we get the weight matrix S . Then, eq. 6 can be effectively solved by the following optimization framework Essentially, the solution of eq. 8 is just the largest d eigen-vectors of the matrix e X i SS T e X T i ( e X = ( X i  X  x called local structure matrix T i  X  R D  X  D . The local intrin-sic dimensionality d can be estimated by finding the largest gap between eigenvalues of T i ( Mordohai &amp; Medioni , 2010 ).
 Analysis. By modeling inhomogeneous error e i effect (Hessian) is implicitly considered without high ord er terms. Furthermore, for a large range of  X  ( ) (such as high order polynomial), weight s j j is quite small when || x x || 2 is large. Thus, outliers will not affect the estimation results much.
 It is also interesting to compare the local learning meth-ods with different kernel functions  X  ( ) . For standard low-rank matrix approximation by Singular Value Decomposi-tion (SVD),  X  ( ) is a constant function. This is because SVD assumes data lie on a linear subspace without consid-ering curvature or outliers. SVD can be improved to Robust SVD, by introducing the robust influence function to han-dle outliers ( De la Torre &amp; Black , 2003 ). However, Robust SVD is still constrained by the linear model and the com-putational cost is high because of the iterative computatio n. On the other hand, Tensor Voting (TV) uses Gaussian ker-nel ( Mordohai &amp; Medioni , 2010 ), which can be viewed as a special case of s ( ) when  X  n = 0. This is because standard Tensor Voting does not consider inlier noise. In this global stage of RMMSL, we focus on multiple smooth manifolds clustering based on local structure learn -ing results.
 In contrast to previous works, the clustering stage in RMMSL can handle multiple non-linear (possibly inter-secting) manifolds with different dimensionalities and it explicitly considers outlier filtering, which is addressed as one step in the clustering process. Compared to the stan-dard assumptions in clustering, i.e., the intra-class dist ance should be small while the inter-class distance should be large, we further argue that each cluster should be a smooth manifold component. As shown in Fig. 2 , when two man-ifolds intersect, there exist multiple possible clusterin g so-lutions, while only the rightmost is the result we want. The underlying assumption we make is local manifold has rela-tively low curvature, i.e. it changes smoothly and slowly as spatial distance increases. In order to get the flattest man-ifold clustering result, it is natural to incorporate a flatness measure into the clustering objective function.
 Curved-Level Measure. As an approximation of (continu-ous) Laplacian-Bertrami operator ( Hein et al. , 2005 ), (dis-crete) graph Laplacian can measure the smoothness of the underlying manifold. However, computing graph Lapla-cian is a global process and most of the theoretical results can not be easily adapted to the multi-manifold setting. On the other side, curvature is a local measurement to indi-cate the curved degree of a geometric object. As a gener-alization of the curvature for 1D curve and 2D surface, the Ricci curvature tensor is proposed to represent the amount by which the volume element of a geodesic ball in a curved (Riemannian) manifold deviates from that of the standard ball in Euclidean space.
 Inspired by the idea of curvature, the following curved-level measurement R ( x ) is considered,  X  (
J i , J ) measures the principal angle between the tangent is the geodesic distance between x i and x . N ( x ) is the spatial neighborhood points set for x . Intuitively, R ( analogous (up to a constant variation) to the integration of the unsigned principal curvatures along different directi ons at x . The theoretical analysis on the connection between R ( x ) and curvature (such as mean curvature) as well as the asymptotic behavior when the number of data samples goes to infinity is left for future investigation. Based on eq. 9 , we can measure the (approximate) total curved level on one manifold cluster M k by summing up R ( ) on all data sam-ples x i belonging to this cluster,
R ( M k ) =  X  where G is the undirected neighborhood graph built on data samples (  X  -neighborhood or K -nearest neighborhood graph).
 Objective Function. Eq. 10 provides an empirical way to measure the total curved degree on one particular manifold cluster M k . This can be viewed as one type of the intra-cluster dissimilarity in the standard clustering framewor k. In order to get the balanced clustering results, we also con-sider the inter-cluster dissimilarity function as follows , In practice, the value of ||  X  ( J i , J ) || / d ( x i , bounded and numerically unstable. Thus, it is straightfor-ward to compound eq. 9 , 11 and the standard similarity kernel function (such as Gaussian) to get the normalized curved measurement. In particular, we use the standard minimization framework by putting ||  X  ( J i , J ) || / d the similarity function with an additional distance simila r-ity function, where W ( ) is the contrary version of the curved measure function R ( ) , i.e., the flatter the manifold is the larger value of W ( ) is. M k is the complementary set of M k The formulation of W ( ) is, where w 1 ( ) and w 2 ( ) are similarity kernel functions that can be chosen as Gaussian or other standard formulations (similar for W ( M k , M l ) ). Due to the shrinkage effect of kernel, d ( ) is further approximated by Euclidean distance. Intuitively, W ( M k ) is one type of the intra-class similarity function on one manifold cluster and W ( M k , M l ) is the inter-class similarity function between two clusters. The optimal n c -classes clustering results are obtained by mini-mizing eq. 12 .
 Algorithm. Indeed, eq. 12 can be viewed as a multi-class normalized-cut with novel similarity measurement provided by local curved similarity and distance similarit y functions ( von Luxburg , 2007 ). Directly minimizing eq. 12 is an NP-hard problem, but it can be relaxed by graph spec-tral clustering in the following procedure.
 Step 1 . Before (global) manifold clustering, local structure learning of RMMSL in sec. 3 is performed to estimate the local tangent space J i  X   X  D  X  d i at each point x i . d locally estimated from the local learning stage of RMMSL or chosen as a fixed value in advance. Also, the neigh-borhood graph G is built on all input data samples { x i } ( n = n 1 + n 2 ).
 Step 2 . Constructing the similarity matrix W = [ w first kernel is the pairwise distance kernel, which is widely used in graph spectral clustering and defined as w use the idea from self-tuning spectral clustering to select the local bandwidth  X  i and  X  j ( Zelnik-Manor &amp; Perona , 2005 ). The second one is curved level kernel w 2 ( x i , exp { X  (  X  ( J i , J j )) 2 / ( || x i  X  x j || 2 (  X  2 c used to control the effect of this curved similarity. Then, w ( x i , x j ) is set as Step e 3 (optional) . Based on W , outlier detection (filtering) can be done as described later.
 Step 4 . Once we have the similarity matrix W , the stan-dard spectral clustering technique can be applied. Specif-ically, we compute the (unnormalized) Laplacian matrix L = D  X  W , where D is a diagonal matrix whose elements equal to the sum of W  X  X  corresponding rows. We select the first n c (number of clusters) eigenvectors of the generalized eigenproblem Le =  X  De . Finally, K-means algorithm is ap-plied on the rows of these eigenvectors. After identifying manifold cluster labels, many tasks such as embedding and denoising can be performed on each manifold cluster. Outlier Detection. We formulate the outlier detection problem as a manifold saliency ranking problem by the random walk model proposed in ( Zhou et al. , 2004 ). We define a random walk graph on { x i } N i ing transition probability matrix P = I  X  L rw = D  X  1 W . It can be shown that, if W is symmetric, then the stationary probability  X  of this random walk can be calculated directly without complex eigen-decomposition ( Zhou et al. , 2004 ), where 1 n is an n  X  1 column vector with all elements as 1 and |||| 1 is the entry-wise 1-norm of a matrix. It is straight-forward that, ranking data according to  X  is the same as the un-normalized distribution 1 T n D . After ranking, bottom points can be filtered out as outliers. The ratio of the out-liers can be given as a prior or be estimated by performing K-means on 1 T n D . Analysis. The key idea of this algorithm is to present a novel way to construct the similarity matrix W in a multi-ple kernel setting (based on local structure estimations) t o encourage flatter clustering results. It is worth noting that if two points have different tangent spaces, the similarity (eq. 14 ) becomes smaller when two points get closer (in a range). This can be viewed as an intuitive explanation that why RMMSL can handle multiple intersecting mani-folds. From the high level point of view, the pairwise simi-larity incorporates the information on two local points set s rather than two single points only. Similar ideas were pro-posed in ( Elhamifar &amp; Vidal , 2009 ; Goldberg et al. , 2009 ; von Luxburg et al. , 2011 ). We evaluate the performance of RMMSL on synthetic data, USPS digits, CMU Motion Capture data (MoCap) and Mo-torbike videos. These data are chosen to demonstrate the general capability of RMMSL as well as the advantages on nonlinear manifolds. We investigate the performance of manifold clustering and further applications such as human action segmentation and motion flow modeling. We also perform quantitative comparisons of the local tangent spac e estimations. In particular, the weighted low-rank matrix d e-composition (local structure learning in RMMSL) is com-pared with the local SVD (or local PCA ( Teh &amp; Roweis , 2003 )) and ND-TV ( Mordohai &amp; Medioni , 2010 ). Results show that RMMSL is more robust to curvature and outliers than other methods. Also, if the manifold has relatively low curvature and is outlier free, then RMMSL and local SVD have similar results. Details are omitted due to space limit . 5.1. Multiple Manifolds Clustering In this task, quantitative comparisons of manifolds clus-tering are provided on three data sets. We compare RMMSL (global structure learning) to K-means, spec-tral clustering (NJW algorithm ( Ng et al. , 2002 )), Self-tuning spectral clustering ( Zelnik-Manor &amp; Perona , 2005 ), Generalized PCA (GPCA) ( Vidal et al. , 2010 ) and Sparse Subspace Clustering (SSC) ( Elhamifar &amp; Vidal , 2009 ). These methods (except K-means) are chosen because they are related to manifold learning or subspace learn-ing. We also perform comparisons with other spec-tral clustering and subspace clustering methods such get the same conclusion, but results are omitted due to space limit. Rand Index score is used as the evalua-tion metric. The kernel bandwidth  X  in spectral clus-tering is tuned in { 1 , 5 , 10 , 20 , 50 , 100 , 200 } data, MoCap and videos, and { 100 , 500 , 1000 , 2000 , 5000 for USPS. The value of the K th neighborhood in self-tuning spectral clustering and RMMSL is chosen from { 5 , 10 , 15 , 20 , 30 , 50 , 100 } .  X  c in the global stage of RMMSL is chosen from { 0 . 2 , 0 . 5 , 1 , 1 . 5 , 2 } cal stage, quadratic  X  ( ) is used and  X  n is set as 1. The sparse regularization parameter of SSC is tuned in { 0 random noise, the parameters for all methods are selected on 5 trials and then the average performance on another 50 trials is reported. For real data, parameters are selected b y picking the best Rand Index. For all methods containing K-means, 100 replicates are performed.
 Synthetic Data. Since most clustering algorithms do not consider outliers explicitly, we first perform a comparison on 3 outlier free synthetic data, while each contains 2000 noisy samples from two manifolds in  X  3 ( d = 2 and D = 3). Results are shown in Fig. 3 . Rand index scores are given in the first three rows of Table 1 . For all methods, the num-ber of clusters n c is fixed as 2. Results (Table 1 ) show that RMMSL achieves comparable and often superior perfor-mance than other candidate methods. In particular, when two manifolds are nonlinear and have intersections, such as two intersecting spheres (second row), the advantage of RMMSL is clearest.
 To verify the robustness, we further evaluate RMMSL on synthetic data with outliers. We add 100 outliers and a 2D plane (1000 samples) on the Swiss roll (2000 samples) to generate two intersecting manifolds in  X  3 . The results of RMMSL are shown in Fig. 4 . RMMSL effectively fil-ters out outliers and achieves 0 . 96 Rand score ( n c = 2) and 0 . 99 F-measure for outlier detection when the ratio is given. Also, the Rand score is reduced to 0 . 78 if we do clustering without outlier filtering ( n c = 3). This fact suggests that the outlier detection step is helpful if outliers exist. It is wo rth noting that spectral clustering methods cover broader case s than RMMSL, which mainly has advantage on multiple low-dimensional manifolds embedded in high dimensional space. For instance, in the case of two 2D Gaussian dis-tributed clusters in  X  2 , RMMSL is reduced to self-tuning spectral clustering (all local tangent spaces are ideally i den-tical). Compared with multiple subspace learning methods such as GPCA and SSC, which are the state-of-the-art for linear manifold clustering, our approach is better when un-derlying manifolds are nonlinear .
 USPS Digits. We choose two subsets of USPS hand writ-ten digits images. The first contains 1100 samples for dig-its 1 and 2 each (USPS-2200) and the second contains 1100 samples for digits 1 to 5 each (USPS-5500). D is reduced from 256 (size 16  X  16 images) to 50 by PCA and d is fixed as 5. Due to the highly nonlinear image structure, results of subspace clustering methods are not reported. For USPS data, the possible high intrinsic dimensionality v.s. the l im-ited number of samples bring difficulties for data structure learning, especially the local learning stage of RMMSL. Nevertheless, RMMSL achieves comparable results.
 MoCap Data. The automatic clustering of human motion sequences into different action units is a necessary step for many tasks such as action recognition and video annota-tion. Usually this is referred as temporal segmentation . In order to make a fair comparison among different cluster-ing methods, we focus on the non-temporal setting, i.e., the temporal index is removed and sequences are treated as collections of static human poses. We choose 5 mixed action sequences from subject 86 in the CMU MoCap. We use the joint-position (45-dimensional representation fo r 15 human body markers in  X  3 ) features which are centralized to remove the global motion. The average Rand scores are reported in Table 1 . It shows that RMMSL achieves higher clustering accuracy than other candidate methods.
 One motion sequence (500 frames) and the corresponding results are visualized in Fig. 5 . The subject walks, then slightly turns around and sits down. By combining the local learning results from RMMSL and ( Teh &amp; Roweis , 2003 ), joint-position features (  X  45 ) from this sequence are visual-ized in  X  3 . This figure supports the assumption that there are low-dimensional manifolds in the joint-position space . In fact, this MoCap sequence can be viewed as three con-nected nonlinear motion manifolds, corresponding to walk-ing, turn-around and sit-down respectively.
 Motion Flow Modeling. Unsupervised motion flow mod-eling is performed on videos ( Lin et al. , 2011 ). The goal is to analyze coordinated movements formed by multiple objects, extract semantic level information, and understand what X  X  happening in the scene. Given motorbike videos as shown in Fig. 6 (from YouTube), global motion pattern is learned from low level motion features. This is an impor-tant task for video analysis and can be served as a foun-dation for many applications such as object tracking and abnormal event detection ( Lin et al. , 2010 ; 2011 ). Differ from the probabilistic approaches in ( Lin et al. , 2011 ), we formulate motion flow learning as a manifold clustering problem. In the experiments, optical flows on salient feature points are estimated by Lucas-Kanade algorithm. Every feature point has 4D information of ( x , y , v x , v y ) . Then motion direction  X  is calculated, and ev-ery point is embedded to ( x , y ,  X  ) space. We observe that coordinated group movements form into manifold struc-tures in ( x , y ,  X  ) space. Therefore, the points are used as the input. The first video contains n = 9266 points and the second one contains n = 8684 points. We use RMMSL to learn the global motion manifolds by doing manifold clustering ( n c = 2) and get the best Rand scores which are reported in Table 1 . Since optical flow results are noisy, outlier filtering is performed before clustering. The mo-tion manifold learning results of two motorbike videos are shown in Fig. 6 , where motion manifolds are visualized on images after kernel density interpolation. From the re-sults we can see that the clustered manifolds have clear semantic meanings, since each manifold corresponds to a coordinated movement formed by a group of motorbikes. Therefore, RMMSL correctly learns global motion to help understand the video scenes. Robust Multiple Manifolds Structure Learning is proposed to effectively learn the data structure by considering nois e, curved-level and multiple manifolds assumption. In partic -ular, the estimated local structure is used to assist the glo bal structure learning tasks of clustering and outlier detecti on. The algorithm is evaluated and compared to other state-of-the-art clustering methods. The results are encouraging: o n both synthetic and real data, the proposed algorithm yields smaller clustering errors in challenging cases, especiall y when multiple nonlinear manifolds intersect. Furthermore , the results on action segmentation and motion flow model-ing demonstrate RMMSL X  X  capability for broad challeng-ing applications in real world.
 This work was supported in part by NIH Grant EY016093 and DE-FG52-08NA28775 from the U.S. Department of Energy. We thank Fei Sha for helpful discussions.

