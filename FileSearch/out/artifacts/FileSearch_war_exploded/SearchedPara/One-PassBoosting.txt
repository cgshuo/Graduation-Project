 In such situations, optimizing at each round may be prohibiti vely expensi ve. that if AdaBoost is run with a sequence of base classiers b ror 1 xing an order b base classier b one-pass setting.
 are not redundant. Each of these k classiers is assumed to have error 1 dence on in the order b under the initial distrib ution D and b rst n 1 stages of AdaBoost can cause the distrib utions D in such a way that when b its adv antage under D version of AdaBoost would pass up the opportunity to use b too small) and thus be able to reap the full benet of using b is nally considered the distrib ution D is still D (i.e ., err or at most 1 are mutually independent under D : follo ws. It uses three parameters, , T and . The idea behind step 2.b .ii is that if Z how good this weak hypothesis is, so we simply use a constant hypothesis. as Z any probability distrib ution Q suc h that Q ( x; y ) ther e is a g 2 G suc h that distrib ution, there are many good weak hypotheses available. ` &lt; k: For any probability distrib ution Q suc h that Pr oof: Fix any distrib ution Q satisfying (2). Let g of g (1). This yields the lemma.
 the performance of the weak learner is not important in rounds for which Z sifier s h for a random element of D , a majority vote over h e No w we give our analysis.
 Pr oof: We pro ve that = = 4 , T = k= 64 , and = 3 k required. We will establish the follo wing claim: that D ( f ( x ) 6 = y ) is at most The nal inequality holds since G are encountered in sequence as the algorithm proceeds through h For each j 2 f 1 ; :::; k g , let X in the ordering b martingale, i.e. that E [ S countered. The value w Consequently , we have that yet been seen have adv antage at least w.r.t. D that S Since j S e 1 e O ( k ) , which completes the proof. illustrate why the pick y variant may be needed.
 for T stages with weak hypotheses h with by the algorithm (the rst distrib ution D error rate of the nal combined classier H is at most exp( 2 P T here. The fact that the denition of D Given a source D of random examples.
 base classier h then Pick yAdaBoost abstains in that round and does not include h D aBoost proceeds to use the weak hypothesis just lik e AdaBoost, i.e. it adds described in (3) and adjusts D 0 to obtain the next distrib ution. (briey , because ln 1 = ln AdaBoost is to require the magnitude of the adv antage to be lar ge. 3.1 The construction We consider a sequence of n + 1 base classiers b is simply b the bit y is chosen uniformly from f +1 ; 1 g . Each bit x y with probability 1 which x 3.2 Base classifiers in order b holds for all n 2 : Pick yAdaBoost. In the case where &lt; 1 Pick yAdaBoost ( ) after one pass through the base classiers in the order b each of b is the same as b with probability ( 1 Lemma 7 For &lt; 1 err or rate precisely ( 1 in the order b of boosting (so D = D base classiers b Claim 8 For eac h 1 t n we have that It follo ws that the coef cients The next claim can be straightforw ardly pro ved by induction on t : Claim 9 Let D fier s b Claim 9 immediately gives one pass over the n + 1 classiers b x ; : : : ; x n disa gree with y; wher e A = n 2 + ln(2 n 1) For ( x; y ) dra wn from source D , we have that each of x probability 1 Then the AdaBoost final hypothesis err or rate is Pr [ B ( n; 1 greater error than Pick yAdaBoost.
 the error rate of Pick yAdaBoost ( ) is ( 1 ( 2 0 : 38) ) n i (1 = 2 + ) i : This can be lower bounded by as a x ed constant, we have where = 1 sour ce D is 2 ( n ) times that of Pic kyAdaBoost ( ) . 3.3 Base classifiers in an arbitrary ordering considered in the particular order b n order suc h that b 2 ( n ) times greater than the err or of Pic kyAdaBoost ( ) . can be fairly lar ge, e.g. for = 0 : 45 it is greater than 1 = 5 : Bayes; (ii) one-pass Adaboost; and (iii) Pick yAdaBoost. voting weight Pick yAdaBoost( 0 : 1 ) against multinomial Nai ve Bayes is 5-1-4. x independently to be equal to y with probability 4 = 5 . The features x be independently equal to z with probability p . So each such x probability (4 = 5) p + (1 = 5) (1 p ) .
 better , and the dif ferences between algorithms shrinking some what. mone y-fx 0.043 0.042 0.041 0.041 0.048 19288 19288 2288 576 62 thresholds: 0.001, 0.01 and 0.1. not pick any base classiers.

