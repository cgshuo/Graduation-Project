
Downward-entailing operators are an interest-ing and varied class of lexical items that change the default way of dealing with certain types of inferences. They thus play an important role in understanding natural language [6, 18 X 20, etc.].
We explain what downward entailing means by first demonstrating the  X  X efault X  behavior, which is upward entailing . The word  X  X bserved X  is an example upward-entailing operator: the statement (i)  X  X itnesses observed opium use. X  implies (ii)  X  X itnesses observed narcotic use. X  but not vice versa (we write i  X  ( 6 X  ) ii). That is, the truth value is preserved if we replace the argument of an upward-entailing operator by a su-perset (a more general version); in our case, the set  X  X pium use X  was replaced by the superset  X  X arcotic use X .

Downward-entailing (DE) (also known as downward monotonic or monotone decreasing ) operators violate this default inference rule: with DE operators, reasoning instead goes from  X  X ets to subsets X . An example is the word  X  X ans X : Although DE behavior represents an exception to the default, DE operators are as a class rather com-mon. They are also quite diverse in sense and even part of speech. Some are simple negations, such as  X  X ot X , but some other English DE opera-tors are  X  X ithout X ,  X  X eluctant to X ,  X  X o doubt X , and  X  X o allow X . 1 This variety makes them hard to ex-tract automatically.

Because DE operators violate the default  X  X ets to supersets X  inference, identifying them can po-tentially improve performance in many NLP tasks. Perhaps the most obvious such tasks are those in-volving textual entailment, such as question an-swering, information extraction, summarization, and the evaluation of machine translation [4]. Re-searchers are in fact beginning to build textual-entailment systems that can handle inferences in-volving downward-entailing operators other than simple negations, although these systems almost all rely on small handcrafted lists of DE operators [1 X 3, 15, 16]. 2 Other application areas are natural-language generation and human-computer interac-tion, since downward-entailing inferences induce greater cognitive load than inferences in the oppo-site direction [8].

Most NLP systems for the applications men-tioned above have only been deployed for a small subset of languages. A key factor is the lack of relevant resources for other languages. While one approach would be to separately develop a method to acquire such resources for each lan-guage individually, we instead aim to ameliorate the resource-scarcity problem in the case of DE operators wholesale: we propose a single unsuper-vised method that can extract DE operators in any language for which raw text corpora exist.
 Overview of our work Our approach takes the English-centric work of Danescu-Niculescu-Mizil et al. [5]  X  DLD09 for short  X  as a starting point, as they present the first and, until now, only al-gorithm for automatically extracting DE operators from data. However, our work departs signifi-cantly from DLD09 in the following key respect.
DLD09 critically depends on access to a high-quality, carefully curated collection of negative polarity items (NPIs)  X  lexical items such as  X  X ny X ,  X  X ver X , or the idiom  X  X ave a clue X  that tend to occur only in negative environments (see  X  2 for more details). DLD09 use NPIs as signals of the occurrence of downward-entailing operators. However, almost every language other than En-glish lacks a high-quality accessible NPI list.
To circumvent this problem, we introduce a knowledge-lean co-learning approach. Our al-gorithm is initialized with a very small seed set of NPIs (which we describe how to generate), and then iterates between (a) discovering a set of DE operators using a collection of pseudo-NPIs  X  a concept we introduce  X  and (b) using the newly-acquired DE operators to detect new pseudo-NPIs. Why this isn X  X  obvious Although the algorith-mic idea sketched above seems quite simple, it is important to note that prior experiments in that direction have not proved fruitful. Preliminary work on learning (German) NPIs using a small list of simple known DE operators did not yield strong results [14]. Hoeksema [10] discusses why NPIs might be hard to learn from data. 3 We cir-cumvent this problem because we are not inter-ested in learning NPIs per se; rather, for our pur-poses, pseudo-NPIs suffice. Also, our prelim-inary work determined that one of the most fa-mous co-learning algorithms, hubs and authorities or HITS [11], is poorly suited to our problem. 4 Contributions To begin with, we apply our al-gorithm to produce the first large list of DE opera-tors for a language other than English. In our case study on Romanian (  X  4), we achieve quite high precisions at k (for example, iteration achieves a precision at 30 of 87%).

Auxiliary experiments explore the effects of us-ing a large but noisy NPI list, should one be avail-able for the language in question. Intriguingly, we find that co-learning new pseudo-NPIs provides better results.

Finally (  X  5), we engage in some cross-linguistic analysis based on the results of applying our al-gorithm to English. We find that there are some suggestive connections with findings in linguistic typology.
 Appendix available A more complete account of our work and its implications can be found in a version of this paper containing appendices, avail-able at www.cs.cornell.edu/  X cristian/acl2010/. In this section, we briefly summarize those aspects of the DLD09 method that are important to under-standing how our new co-learning method works. DE operators and NPIs Acquiring DE opera-tors is challenging because of the complete lack of annotated data. DLD09 X  X  insight was to make use of negative polarity items (NPIs) , which are words or phrases that tend to occur only in negative con-texts. The reason they did so is that Ladusaw X  X  hy-pothesis [7, 13] asserts that NPIs only occur within the scope of DE operators . Figure 1 depicts exam-ples involving the English NPIs  X  X ny X  5 and  X  X ave a clue X  (in the idiomatic sense) that illustrate this relationship. Some other English NPIs are  X  X ver X ,  X  X et X  and  X  X ive a damn X .

Thus, NPIs can be treated as clues that a DE operator might be present (although DE operators may also occur without NPIs). DLD09 algorithm Potential DE operators are collected by extracting those words that appear in an NPI X  X  context at least once. 6 Then, the potential DE operators x are ranked by which compares x  X  X  probability of occurrence conditioned on the appearance of an NPI with its probability of occurrence overall. 7
The method just outlined requires access to a list of NPIs. DLD09 X  X  system used a subset of John Lawler X  X  carefully curated and  X  X oderately complete X  list of English NPIs. 8 The resultant rankings of candidate English DE operators were judged to be of high quality.
 The challenge in porting to other languages: cluelessness Can the unsupervised approach of DLD09 be successfully applied to languages other than English? Unfortunately, for most other lan-guages, it does not seem that large, high-quality NPI lists are available.

One might wonder whether one can circumvent the NPI-acquisition problem by simply translating a known English NPI list into the target language. However, NPI-hood need not be preserved under translation [17]. Thus, for most languages, we lack the critical clues that DLD09 depends on. In this section, we develop an iterative co-learning algorithm that can extract DE operators in the many languages where a high-quality NPI database is not available, using Romanian as a case study. 3.1 Data and evaluation paradigm We used Rada Mihalcea X  X  corpus of  X  1.45 million sentences of raw Romanian newswire articles.
Note that we cannot evaluate impact on textual inference because, to our knowledge, no publicly available textual-entailment system or evaluation data for Romanian exists. We therefore examine the system outputs directly to determine whether the top-ranked items are actually DE operators or not. Our evaluation metric is precision at k of a given system X  X  ranked list of candidate DE oper-ators; it is not possible to evaluate recall since no list of Romanian DE operators exists (a problem that is precisely the motivation for this paper).
To evaluate the results, two native Romanian speakers labeled the system outputs as being  X  X E X ,  X  X ot DE X  or  X  X ard (to decide) X . The la-beling protocol, which was somewhat complex to prevent bias, is described in the externally-available appendices (  X  7.1). The complete system output and annotations are publicly available at: http://www.cs.cornell.edu/  X cristian/acl2010/. 3.2 Generating a seed set Even though, as discussed above, the translation of an NPI need not be an NPI, a preliminary re-view of the literature indicates that in many lan-guages, there is some NPI that can be translated as  X  X ny X  or related forms like  X  X nybody X . Thus, with a small amount of effort, one can form a min-imal NPI seed set for the DLD09 method by us-ing an appropriate target-language translation of  X  X ny X . For Romanian, we used  X  X reo X  and  X  X reun X , which are the feminine and masculine translations of English  X  X ny X . 3.3 DLD09 using the Romanian seed set We first check whether DLD09 with the two-item seed set described in  X  3.2 performs well on Romanian. In fact, the results are fairly poor: for example, the precision at 30 is below 50%. (See blue/dark bars in figure 3 in the externally-available appendices for detailed results.)
This relatively unsatisfactory performance may be a consequence of the very small size of the NPI list employed, and may therefore indicate that it would be fruitful to investigate automatically ex-tending our list of clues. 3.4 Main idea: a co-learning approach Our main insight is that not only can NPIs be used as clues for finding DE operators, as shown by DLD09, but conversely, DE operators (if known) can potentially be used to discover new NPI-like clues, which we refer to as pseudo-NPIs (or pNPIs for short). By  X  X PI-like X  we mean,  X  X erve as pos-sible indicators of the presence of DE operators, regardless of whether they are actually restricted to negative contexts, as true NPIs are X . For exam-ple, in English newswire, the words  X  X llegation X  or  X  X umor X  tend to occur mainly in DE contexts, like  X  denied  X  or  X  dismissed  X , even though they are clearly not true NPIs (the sentence  X  X  heard a ru-mor X  is fine). Given this insight, we approach the problem using an iterative co-learning paradigm that integrates the search for new DE operators with a search for new pNPIs.

First, we describe an algorithm that is the  X  X e-verse X  of DLD09 (henceforth rDLD ), in that it re-trieves and ranks pNPIs assuming a given list of DE operators. Potential pNPIs are collected by ex-tracting those words that appear in a DE context (defined here, to avoid the problems of parsing or scope determination, as the part of the sentence to the right of a DE operator, up to the first comma, semi-colon or end of sentence); these candidates x are then ranked by
Then, our co-learning algorithm consists of the iteration of the following two steps:  X  ( DE learning ) Apply DLD09 using a set N  X  ( pNPI learning ) Apply rDLD using the set D Here, N is initialized with the NPI seed set. At each iteration, we consider the output of the al-gorithm to be the ranked list of DE operators re-trieved in the DE-learning step. In our experi-ments, we initialized n to 10 and set n r to 1. Our results show that there is indeed favorable synergy between DE-operator and pNPI retrieval. Figure 2 plots the number of correctly retrieved DE operators in the top k outputs at each iteration. The point at iteration 0 corresponds to a datapoint already discussed above, namely, DLD09 applied to the two  X  X ny X -translation NPIs. Clearly, we see general substantial improvement over DLD09, al-though the increases level off in later iterations. (Determining how to choose the optimal number of iterations is a subject for future research.)
Additional experiments, described in the externally-available appendices (  X  7.2), suggest that pNPIs can even be more effective clues than a noisy list of NPIs. (Thus, a larger seed set does not necessarily mean better performance.) pNPIs also have the advantage of being derivable automatically, and might be worth investigating from a linguistic perspective in their own right. Applying our algorithm to English: connec-tions to linguistic typology So far, we have made no assumptions about the language on which our algorithm is applied. A valid question is, does the quality of the results vary with choice of appli-cation language? In particular, what happens if we run our algorithm on English?
Note that in some sense, this is a perverse ques-tion: the motivation behind our algorithm is the non-existence of a high-quality list of NPIs for the language in question, and English is essen-tially the only case that does not fit this descrip-tion. On the other hand, the fact that DLD09 ap-plied their method for extraction of DE operators to English necessitates some form of comparison, for the sake of experimental completeness.
 We thus ran our algorithm on the English BLLIP newswire corpus with seed set {  X  X ny X  } . We observe that, surprisingly, the iterative addi-tion of pNPIs has very little effect: the precisions at k are good at the beginning and stay about the same across iterations (for details see figure 5 in in the externally-available appendices). Thus, on English, co-learning does not hurt performance, which is good news; but unlike in Romanian, it does not lead to improvements.

Why is English  X  X ny X  seemingly so  X  X owerful X , in contrast to Romanian, where iterating beyond the initial  X  X ny X  translations leads to better re-sults? Interestingly, findings from linguistic typol-ogy may shed some light on this issue. Haspel-math [9] compares the functions of indefinite pro-nouns in 40 languages. He shows that English is one of the minority of languages (11 out of 40) 9 in which there exists an indefinite pronoun series that occurs in all (Haspelmath X  X ) classes of DE con-texts, and thus can constitute a sufficient seed on its own. In the other languages (including Roma-nian), 10 no indirect pronoun can serve as a suffi-cient seed. So, we expect our method to be vi-able for all languages; while the iterative discov-ery of pNPIs is not necessary (although neither is it harmful) for the subset of languages for which a sufficient seed exists, such as English, it is essen-tial for the languages for which, like Romanian,  X  X ny X -equivalents do not suffice.
 Using translation Another interesting question is whether directly translating DE operators from English is an alternative to our method. First, we emphasize that there exists no complete list of En-glish DE operators (the largest available collec-tion is the one extracted by DLD09). Second, we do not know whether DE operators in one lan-guage translate into DE operators in another lan-guage. Even if that were the case, and we some-how had access to ideal translations of DLD09 X  X  list, there would still be considerable value in us-ing our method: 14 (39%) of our top 36 highest-ranked Romanian DE operators for iteration 9 do not, according to the Romanian-speaking author, have English equivalents appearing on DLD09 X  X  90-item list. Some examples are:  X  X bt  X inut X  (ab-stained),  X  X riticat X  (criticized) and  X  X eact  X ionat X  (re-acted). Therefore, a significant fraction of the DE operators derived by our co-learning algorithm would have been missed by the translation alterna-tive even under ideal conditions. We have introduced the first method for discov-ering downward-entailing operators that is univer-sally applicable. Previous work on automatically detecting DE operators assumed the existence of a high-quality collection of NPIs, which renders it inapplicable in most languages, where such a re-source does not exist. We overcome this limita-tion by employing a novel co-learning approach, and demonstrate its effectiveness on Romanian. Also, we introduce the concept of pseudo-NPIs . Auxiliary experiments described in the externally-available appendices show that pNPIs are actually more effective seeds than a noisy  X  X rue X  NPI list.
Finally, we noted some cross-linguistic differ-ences in performance, and found an interesting connection between these differences and Haspel-math X  X  [9] characterization of cross-linguistic vari-ation in the occurrence of indefinite pronouns. Acknowledgments We thank Tudor Marian for serving as an annotator, Rada Mihalcea for ac-cess to the Romanian newswire corpus, and Claire Cardie, Yejin Choi, Effi Georgala, Mark Liber-man, Myle Ott, Jo  X  ao Paula Muchado, Stephen Pur-pura, Mark Yatskar, Ainur Yessenalina, and the anonymous reviewers for their helpful comments. Supported by NSF grant IIS-0910664.
 [1] Roy Bar-Haim, Jonathan Berant, Ido Da-[2] Eric Breck. A simple system for detecting [3] Christos Christodoulopoulos. Creating a nat-[4] Ido Dagan, Oren Glickman, and Bernardo [5] Cristian Danescu-Niculescu-Mizil, Lillian [6] David Dowty. The role of negative polar-[7] Gilles Fauconnier. Polarity and the scale [8] Bart Geurts and Frans van der Slik. Mono-[9] Martin Haspelmath. Indefinite Pronouns . [10] Jack Hoeksema. Corpus study of negative [11] Jon Kleinberg. Authoritative sources in a hy-[12] Wilfried K  X  urschner. Studien zur Negation im [13] William A. Ladusaw. Polarity Sensitivity as [14] Timm Lichte and Jan-Philipp Soehn. The re-[15] Bill MacCartney and Christopher D. Man-[16] Rowan Nairn, Cleo Condoravdi, and Lauri [17] Frank Richter, Janina Rad  X  o, and Manfred [18] V  X   X ctor S  X  anchez Valencia. Studies on natural [19] Johan van Benthem. Essays in Logical Se-[20] Ton van der Wouden. Negative contexts:
