 Dmitriy Fradkin  X  Fabian M X rchen Abstract While a number of efficient sequential pattern mining algorithms were developed over the years, they can still take a long time and produce a huge number of patterns, many of which are redundant. These properties are especially frustrating when the goal of pattern mining is to find patterns for use as features in classification problems. In this paper, we describe BIDE-Discriminative, a modification of BIDE that uses class information for direct mining of predictive sequential patterns. We then perform an extensive evaluation on nine real-life datasets of the different ways in which the basic BIDE-Discriminative can be used in real multi-class classification problems, including 1-versus-rest and model-based search tree approaches. The results of our experiments show that 1-versus-rest provides an efficient solution with good classification performance.
 Keywords Sequential pattern mining  X  Sequence classification  X  Information gain 1 Introduction Temporal data mining exploits temporal information in data sources in the context of data mining tasks such as clustering or classification. Many scientific and business data sources are dynamic and thus promising candidates for application of temporal mining methods. For an overview of methods to mine time series, sequence, and streaming data, see [ 13 , 20 ].
One particular type of temporal data consists of sequences of (sets of) discrete items asso-ciated with time stamps. Examples of such data include medical records [ 4 , 5 ], histories of transactions of customers in an online shop, log messages emitted by machines or telecom-munication equipment during operation [ 43 , 49 ], and discretized or abstracted time series or sensor data. A common task is to mine for local regularities in this data by looking for sequential patterns [ 2 ] that represent a sequence of itemsets, possibly with gaps, embedded in the observation sequences. Another common task is sequence classification X  X iven a set of example sequences belonging to two or more classes, to learn a predictive model capable of correctly assigning class labels to previously unseen sequences. In this paper, we  X  propose BIDE-Discriminative X  X  modification to the highly efficient BIDE algorithm  X  propose approaches ( BIDE-D and BIDE-DC ) for using BIDE-Discriminative for direct  X  propose a combination of BIDE-Discriminative and model-based tree (MbT) classifier  X  conduct an extensive experimental evaluation of all of the proposed methods against 2 Related work Two types of approaches for classification on itemset and symbolic sequential data have been proposed. These can be thought of as indirect and direct approaches [ 11 ]. The indirect approaches consist of three stages. First, candidate pattern is mined in an unsupervised fashion or separately for each class, and then feature selection [ 19 ] is applied, usually using the remaining features. Many papers utilize such indirect approaches for classification, e.g., [ 8 , 10 ].

The drawbacks of indirect approaches are clear and have been noted in multiple publica-tions [ 11 ]: (i) Generation of all candidate patterns can be computationally very expensive, and (ii) the overwhelming majority of the patterns thus found turn out to be useless for before classification to avoid loss in predictive performance. Note that utilizing more restric-tive definitions of patterns, such as closed or margin-closed itemset [ 32 , 34 ] or sequential some extent the first of these problems, but does not address the second.

Direct methods address both of these problems by utilizing class label information in the pattern mining stage, and possibly combining/interleaving the steps of pattern mining and pattern evaluation, thereby generating fewer but better patterns. Furthermore, they can be faster due to pruning of the pattern search space. 2.1 Classification with itemset data Multiple indirect pattern mining algorithms exist for itemset data, staring with Apriori [ 1 ]. Examples include CHARM [ 53 ], FP-tree [ 21 ], and FPclose [ 18 ]. Any of these can be used to generate features for classification.

In [ 10 ], a minimum support threshold is set based on information gain bounds for mining discriminative itemsets. FPclose is used to find all itemsets with support greater than this threshold.MaximalMarginalRelevance[ 9 ]isthenappliedtofurtherreducethesetofitemsets to be used as features. SVM and decision trees are used as classifiers. This approach can be seen as falling between indirect and direct approaches.

There have also been some real direct approaches. In [ 11 ], a branch-and-bound search is performed for discriminative itemset patterns. These patterns are generated sequentially on a shrinking FP-tree [ 22 ] by eliminating training instances that are sufficiently covered, as a way to reduce redundancy in the set. This requires multiple runs of the mining algorithm to generate one pattern at a time. Model-based search tree ( M b T ) approach is proposed in [ 15 ] and evaluated on itemset and on graph datasets. Here, a single most discriminative pattern is selected, and the dataset is split into two, based on whether instances have this pattern. Each set is then recursively processed, until all instances in a set belong to the same class, or the set is too small. In this way, a tree for classifying unseen examples is constructed.
None of these approaches have been evaluated on sequential data, which is what we do in the present paper. Our approaches SBMT and SMBT-FS are analogous to those of [ 11 ] except for sequential patterns, but we also consider mining top k informative patterns in a single run (BIDE-D and BIDE-DC approaches). 2.2 Classification with sequential data Just as for itemsets, there are many indirect sequential pattern mining methods. GSP [ 44 ], SPADE [ 52 ], PrefixSpan [ 41 ], GSpan [ 50 ], and BIDE [ 46 ] are just some of them. Examples of using Symbolic Aggregate Approximation [ 29 ]. Sequential patterns (motifs) are then mined using GSP-/Apriori-like approach with taxonomy. These patterns (motifs) are used as features for construction of SVMs and Bayesian Networks. In [ 31 ], sequential patterns in software traces are mined using a custom algorithm, then feature selection based on Fisher score is applied, and finally, a classifier is applied.
 We focus below on more direct approaches.

Text and biologic sequences can be considered sequential data, so we mention an approach that was described in [ 23 , 24 ]. Logistic regression models are built in the space of all pos-sible n-grams (of words or symbols) by directly constructing predictive n-grams. Gaps of predefined maximum size (i.e., n-grams with wildcards) can also be handled. This approach can therefore be thought of as discovering predictive sequential patterns, though it is quite different from other sequential pattern mining work.

In [ 28 ], the approach of [ 10 ] (discussed above in Sect. 2.1 ) was used to set a minimum support threshold based on information gain bound for mining sequential patterns. Partial sequential patterns, i.e., of limited length, were mined, and then feature selection using F-score was performed before applying classification methods. This approach is thus more of an indirect than a direct method.

In [ 30 ], dyadic sequential patterns are mined using a BIDE-like [ 46 ] algorithm. The instances are pairs of sequences, and the task is to determine whether elements in a pair match or not. While the high-level idea is similar to BIDE-Discriminative that we propose here, the data, the patterns, and the problem are different.

In [ 7 ], multiple pattern languages, including sequences, trees, and graphs, were compared for their usefulness in a classification task. The patterns were mined using branch-and-bound methods like those described in Sect. 2.3 with a modification of GSpan [ 50 ](no details provided) and  X  2 as the discriminative measure. In our work, we focus on evaluating alternative approaches to finding discriminative sequential patterns, rather than evaluating different pattern languages. Also, we base our discriminative pattern mining algorithm on BIDE, rather than on GSpan. 2.3 Branch-and-bound methods How can discriminative patterns be discovered without examining all the candidates? The solution, utilized in some form in all of the above-mentioned direct approaches, is to use branch-and-bound search in the space of patterns while maintaining an upper bound on the utility of the search subtree at a pattern.

Pruning based on statistical metrics such as IG or  X  2 was proposed in [ 37 ] for finding predictive itemsets. An Apriori-type algorithm was used to traverse the search space, and upper bound on the feature quality was used for pruning. In [ 39 ], such approach was used for discriminative subgraph mining, in [ 30 ] X  X or classification of dyadic sequences.
Relation between a pattern X  X  support and discriminative power was discussed in [ 10 ], but only in the context of setting appropriate minimum support for frequent itemset pattern mining. 3 Upper bounds on discriminative measures Suppose we have a dataset D of labeled examples ( x i , y i ), i = 1 ,..., N where y i  X  C and c  X  X n other words, it is the number of examples of class c j that contains/matches pattern P . Also, let s ( P ) be the total support of P  X  X he sum of support over all classes c  X  C .Table 1 shows the relationships between these values.

Information gain (IG) has long been used for feature selection in classification [ 51 ]. For convenience, let l ( p ) = p log 2 ( p ) . Then, IG is defined (for a fixed dataset) as:
Note that the first term in Eq. 1 is fixed for any given dataset, since it depends only on class distribution. Therefore, for a fixed dataset IG ( P ) is a function of conditional distribution p ( c | x ) which can be fully represented by a vector of class supports s c ( P ) . We can write IG ( P ) = IG ( s 1 ( P ),..., s k ( P )) .

For a two-class problem, IG is a convex function. Morishita and Sese [ 37 ]havecomeup with an upper bound on it: where r = s + ( P ) and q = s  X  ( P ) .

Analysis in [ 37 ] shows that for two-class problems other discriminative measures such as chi-square and correlation can also be bounded in a similar fashion. For multi-class problems, bounds (in the same form as above) have been found for chi-squared [ 38 ] and inter-class variance [ 42 ]. Both papers used these bounds in analysis of itemset data. Note, however, that these results apply to any pattern language, such as itemsets, sequential patterns, and graphs.
We are not aware of similar upper bounds for IG in multi-class problems. Theorem 7.4 one is fixed (extending a pattern changes both), and so IG appears to be neither convex nor concave for more than 2 classes.

Let us state clearly what existence of such upper bounds implies. Given a labeled dataset, patterns P and P such that P occurs in P (or P matches or extends P  X  X e will define this formally in Sect. 2.3 ) and a discriminativeness measure F with a known upper bound, if F ub ( P ) = x ,then F ( P )  X  x . This enables construction of branch-and-bound algorithms for pattern mining, such as those we mentioned in Sect. 2.3 and those we present in this paper, using any discriminative measure with a defined upper bound. Basically, if we already observe a pattern with discriminative score x ,and F ub ( P ) = y &lt; x , then we know that no extension of P is going to have a higher discriminative score, and this part of pattern space does not need to be explored. The discriminative measure itself does not need to be (and usually is not) anti-monotone.

In this paper, we focus on IG, while noting that the same methods are applicable to chi-square, correlation, and other discriminativeness measures where an upper bound can be formulated. 4 BIDE Family of algorithms 4.1 Definitions An event sequence over a set of events is a sequence of pairs ( t i , s i ) of event sets s i  X  t .The length of the event sequence is n .

For the present discussion (and in much of the sequential pattern mining literature), the exact values of the time stamps are not as important as the ordering that they impose. We will therefore treat event sequences as just an ordered set of event sets S ={ s i } . A sequence database ,SDB,ofsize N is a collection of event sequences S i , i = 1 ,..., N .
A sequence S ={ s i } , i = 1 ,..., k matches a sequential pattern P ={ p j } , j = 1 ,..., m 1 ,..., m ,
Support of a pattern, support ( P ) , is the number of sequences where the pattern occurs. A pattern is frequent if its support is above some predefined minimum support  X  . A pattern is closed if none of the patterns that include it have the same support.

In order to describe the algorithms in the following sections, we need to introduce the notion of projected databases , which is extremely useful in constructing efficient algorithms for sequential pattern mining.

Given a pattern P ={ p i } , i = 1 ,..., | P | and a sequence S ={ s j } , j = 1 ,..., | S | , S | P ={ s t } ,where t = k m + 1 ,..., | S | . We refer to k m as an offset.

Given a pattern P ={ p i } , i = 1 ,..., | P | andanSDB D ={ S j } , j = 1 ,..., | D | ,a projection of D on P is a projected database D | P , consisting of projected sequences S j | P , obtained by projecting S j onto P . Note that if a sequence does not match a pattern, it does not appear in the projected database. Projected database D | P can be efficiently represented 4.2 BIDE Details of BIDE implementation can be found in [ 17 , 46 , 47 ]. BIDE is initially called with the full sequential database D , minimum support  X  , and an empty pattern P = X  . It returns a list of frequent closed sequential patterns. BIDE operates by recursively extending patterns, and while their frequency is above the minimum support, checking closure properties of the extensions.
 Consider a frequent pattern P ={ p i } , i = 1 ,..., n . There are two ways to extend pattern Pforward with item j :  X  Appending the set p n + 1 ={ j } to P obtaining P = p 1 ... p n p n + 1 , called a forward- X  Adding j to the last itemset of P : P = p 1 ... p n , with p n = p n  X  j , assuming j /  X  p n , Similarly, a pattern can be extended backward  X  Inserting the set p x ={ j } into P anywhere before the last set obtaining P =  X  Adding j to any set in P obtaining P = p 1 ... p i ... p n , with p i = p i  X  j , assuming
AccordingtoTheorem3of[ 47 ], a pattern is closed if there exists no forward-S-extension item, forward-I-extension item, backward-S-extension item, nor backward-I-extension item with the same support.

Furthermore, if there is a backward extension item, then the resulting extension and all of its future extensions are explored in a different branch of the recursion, meaning that it can be pruned from current analysis. These insights are combined in BIDE, leading to a very memory-efficient algorithm, because the patterns found do not need to be kept in memory while the algorithm is running.
 Algorithm 1 BIDE Algorithm
Specifically, consider pseudo-code for BIDE (Algorithm 1). In Lines 3  X  4 , items that can be used in forward extension of the current pattern are found. If there is no forward extension with the same support (Line 5 ), the backward closure is checked (Line 6 ) using function backscan. If the pattern is also backward-closed, it can be added to the set of closed frequent patterns (Line 7 ).

Then, we check every item in forward S and I extensions (in the two for-loops) to see 14 ). If not, then we project the database on the extension and call BIDE recursively on the extension and the new projected database. We will omit additional details as they are not relevant to our discussion. 4.3 BIDE-Discriminative In order to select only discriminative patterns, we need to keep track of the class label information. We assume that each sequence in dataset D is associated with a class label, and thus we know class distribution in the dataset. When we search for potential S and I extensions, we also keep track of class distribution of sequences where they occur. This gives us all the necessary information for determining discriminative scores and their upper bounds for any new pattern that we discover. Let d ( P , c ) denotes discriminative score for pattern P for class c ,andlet d UB ( P , c ) denotes the upper bound on discriminative score for any extension P  X  X f P for class c . When discussing two-class problems, c can be omitted from the notation.

The pseudo-code for BIDE-Discriminative is given in Algorithm 2. We have a new user-specifiedparameter k  X  X henumberofpatternstoextract.Weintroducevariable dt ,athreshold on discriminative score, which is initially set to 0.
 Algorithm 2 BIDE-Discriminative Algorithm
In Lines 3  X  4 , we check the upper bound of pattern P . If the upper bound is below the threshold dt , then the pattern and all of its extensions can be pruned X  X he function returns. Otherwise, algorithm proceeds. In Line 8 , the score of the pattern itself is checked against the threshold, to determine if it should be added to the list. If the size of the set F exceeds k , the pattern with the lowest score is removed, and threshold dt is updated accordingly examined like in regular BIDE, since the upper bound was above the threshold.

Note that with minor modifications, we can have BIDE-Discriminative output, all patterns with discriminative score above some value. All that is needed is to remove parameter k and make dt a fixed parameter instead. 4.4 Handling multi-class problems As mentioned before, the upper bounds for IG or  X  2 hold for two-class problems only. Multi-class problems can be handled in two ways.

BIDE-D: Run BIDE-Discriminative on the whole training data once, but redefine discrim-inative scores and upper bounds of patterns to be maximums over all binary one-versus-rest problems. Specifically:
BIDE-DC: Mine discriminative patterns on the whole training data separately for each of | C | one-versus-rest binary problems. (This straightforward idea has been mentioned in [ 38 ] in context of itemset pattern mining, but not implemented). The sets of top-k patterns produced for each class can be merged, resulting in at most k | C | patterns. Note that this could be done in a single run, by maintaining separate sets of patterns and thresholds for each class and expanding a pattern as long as it could be informative for at least one class ( BIDE-DC-1R ). Alternatively, the same set of patterns can be discovered by performing | C | one-versus-rest runs of two-class discriminative BIDE ( BIDE-DC-CR ).
 These direct approaches can be contrasted with two indirect approaches:
BIDE: Run regular BIDE on all training data and use all closed patterns found. This approach is fully unsupervised.

BIDE-C: Run BIDE separately on training data from each class and merge the sets of patterns found. Note that this is the only approach of the four that does mining on subsets of the whole dataset. This approach is indirect but does make some use of class labels. 4.5 Computational efficiency A single run of BIDE-Discriminative has the same complexity as BIDE, since it may still potentially generate all frequent closed sequential patterns, in exactly the same fashion. However, the pruning, depending on the values of k or dt , may remove a significant number of patterns and their extensions from consideration. The cost involved in computing the upper bounds for each pattern is independent of the size of the data (at most proportional to number of classes) with appropriate bookkeeping and so does not affect computational complexity.
The reduction in the number of patterns produced can also lead to savings in I/O time and space/memory to store them. Furthermore, since the discriminative scores for the generated patterns are already known, there is no need to separately compute them, as in indirect approaches.
The two variants of BIDE-DC allow us to compare the costs of deeper expansion and of maintaining multiple pattern sets, against the costs of doing | C | runs of BIDE-Discriminative.
Meanwhile, BIDE-C requires | C | runs of BIDE, but each is only on a fraction of the dataset, which is likely to be much faster than a single run on the full dataset. We explore these trade-offs in the experimental section. 5 Model-based tree algorithms A method for direct construction of a tree-based predictive model (model-based tree or MbT) was proposed in [ 15 ] and evaluated on itemset and on graph data. We combine BIDE-Discriminative algorithm with the MbT idea for efficiently building a predictive model for sequential data. We will refer to this as SMBT . The pseudo-code is given in Algorithm 4. BIDE-Discriminative (Line 4 ) is used to find the most informative pattern. The database is then split into two sets of sequences: those that match this pattern, and those that don X  X . The two sets are processed recursively in the same fashion until they become too small or until the class purity of a node exceeds a predefined threshold.
 The resulting tree can be used to make prediction on test data in a straightforward fashion. for the node). If the node is not a leaf, we check if the node pattern occurs in the sequence and depending on the result descent to the left or to the right child.

We expect the SMBT to include only a small fraction of the patterns that would have been produced by BIDE-Discriminative alone, significantly easing interpretation of the classifier and of the extracted patterns.
 Algorithm 3 BIDE-DC-1R: Discriminative Algorithm for Multiclass Problems Algorithm 4 SMBT: MBT Algorithm for sequential data
While SMBT approach directly constructs a classifier, it can also be viewed as a form of feature selection ( SMBT-FS ): The patterns in the SMBT can be treated as individual features, to be combined using methods such as support vector machines or neural nets. 6 Experiments So far, we have described the following approaches:  X  two indirect (i.e., unsupervised) sequential pattern mining approaches (BIDE and BIDE- X  two variants of BIDE for directly mining discriminative patterns, specifically BIDE-D  X  a sequential model-based tree approach, SMBT, which is a direct mining method and a  X  SMBT-FS, which uses SMBT purely as a feature Selection/direct mining step and builds
The goals of our experiments are to examine and compare the behavior of the above methods, under different parameter choices, in terms of (i) accuracy, (ii) number of patterns produced, and (iii) time required to mine the patterns. Our expectations are that direct mining methods produce fewer pattern and require less time than indirect and unsupervised methods without sacrificing accuracy. Specifically, because of their respective designs, we expect BIDE-D and BIDE-DC to produce fewer patterns and run faster than BIDE, while SMBT will be faster still with even fewer patterns. The behavior of BIDE-C is more difficult to predict.

We use information gain as the discriminativeness measure with all of these algorithms. 6.1 Data While unlabeled sequential data are relatively common, e.g., web log dataset [ 3 ], labeled data needed for our evaluation are much harder to come by. The nine datasets used in our exper-iments are summarized in Table 2 . Two (Unix and WW3D) are simple sequential datasets. The remaining seven, while technically databases of intervals, can be interpreted as sequen-tial databases by treating start and end boundaries of an interval as separate events [ 48 ]. Specifically, each symbolic interval, a triple ( t s , t e , X ) with event  X   X  and time stamps t points with the same time stamp are aggregated into itemsets, resulting in a standard event sequence.
The advantage of this collection is that class labels are available for each sequence. This allows an automated evaluation of patterns using a classifier, while the categorical sequential data available in the UCI Machine Learning Repository [ 3 ], such as web log data, are largely unlabeled.

ASL-BU 1 TheintervalsaretranscriptionsfromvideosofAmericanSignLanguageexpres-sions provided by Boston University [ 40 ]. It consists of observation interval sequences with labels such as head mvmt: nod rapid or shoulders forward that belong to one of 7 classes like yes X  X o question or rhetorical question .

ASL-GT The intervals are derived from 16-dimensional numerical time series with fea-tures derived from videos of American Sign Language expressions [ 45 ]. The numerical time series were discretized into 2 X 4 states, each using Persist [ 35 ]. Each sequence represents one of the 40 words such as brown or fish .

Auslan2 The intervals were derived from the high-quality Australian Sign Language dataset in the UCI repository [ 3 ] donated by Kadous [ 25 ]. The x , y , z dimensions were discretized using Persist with 2 bins, and five dimensions representing the fingers were discretized into 2 bins using the median as the divider. Each sequence represents a word such as girl or right .

Blocks 2 The intervals describe visual primitives obtained from videos of a human hand stacking colored blocks provided by [ 16 ]. The interval labels describe which blocks touch and the actions of the hand ( contacts blue red , attached hand red ). Each sequence represents one of 8 different scenarios from atomic actions ( pick-up ) to complete scenarios ( assemble ).
Context 3 The intervals were derived from categoric and numeric data describing the context of a mobile device carried by humans in different situations [ 33 ]. Numeric sensors were discretized using 2 X 3 bins chosen manually based on exploratory data analysis. Each sequence represents one of five scenarios such as street or meeting .
 Pioneer The intervals were derived from the Pioneer-1 datasets in the UCI repository [ 3 ]. The numerical time series were discretized into 2 X 4 bins by choosing thresholds manually based on exploratory data analysis. Each sequence describes one of three scenarios: gripper , move ,or turn .

Skating The intervals were derived from 14-dimensional numerical time series describing muscle activity and leg position of six professional in-line speed skaters during controlled tests at seven different speeds on a treadmill [ 36 ]. The time series were discretized into 2 X 3 bins using Persist and manually chosen thresholds. Each sequence represents a complete movement cycle and is labeled by skater or speed.
 Unix The dataset consists of sanitized command histories of 9 users [ 3 ].

WW3D This dataset was collected from Wubble World 3D (ww3d), a virtual environment with simulated physics in which softbots, called wubbles, interact with objects [ 26 ]. 6.2 Experiment setup For each method and parameter setting, we repeated fivefold cross-validation 3 times, each time with a different random split. Thus, all the measures (accuracy, number of patterns, run times, etc) are averages taken over 3  X  5 = 15 test sets. Each experimental run consisted of two parts: pattern mining and classification. Pattern mining is done on the training folds. Then, sequences in both training and test folds are converted to binary vectors based on the presence of the patterns. These vectors are written to files. (This part is programmed in Java). The vectors from training folds are then used to build classifiers, which are then applied to the test folds.

For direct BIDE approaches (BIDE-D and both variants of BIDE-DC), we considered k = 10 , 20 , 30 , 40 , 50 , 70 , 90, where k is the number of patterns per class.
In order to compare all the approaches, we also had to find a way of setting minimum support similarly for all of them. For most datasets, we used the following strategy: parameter data that would be used as minimum support. For example, if the training set has 100 instances of 5 classes, but the smallest class has 10 instances, then with  X  = 0 . 5, the minimum support would be set to 5 for all methods.

It turned out that for some datasets (ASL-GT, Context, Skating, and Unix), this approach did not work X  X attern mining was taking too long for all the approaches. For these datasets, we changed the way minimum support was computed: fraction  X  of the whole training set size was used. BIDE-C cannot be used in such situations, but the other three methods ran successfully. In these cases,  X  is equivalent to  X  , minimum support. For Context and Skating,  X  = 0 . 1 , 0 . 15 , 0 . 2.

The way we set minimum support ensures that all four methods are exploring the same pattern space and could conceivably produce exactly the same set of patterns, i.e., a set of closed frequent patterns for minimum support derived using  X  . It follows that the set of patterns found by BIDE is going to be a superset of sets of patterns found by the other approaches for the same value of  X  .

For SMBT experiments, we kept the same values of  X  . However, we had to additionally specify values for minimum leaf size z and leaf purity p . We experimented with p = 0 . 8 , 0 . 9 and z = 1 , 3 , 5.

Classification was performed in MATLAB, using LIBLINEAR [ 14 ] (a fast implementa-tion of linear SVM) with options  X -B 1 -S 5 X , i.e., L1-regularized L2-loss support vector cross-validation on the training set. 6.3 Experimental results Here, we describe the highlights of our evaluation. More details are presented in the  X  X ppen-dix. X  4 Comparison of two BIDE-DC variants: The two variants, BIDE-DC-1R and BIDE-DC-CR, will produce identical sets of patterns, so the only meaningful comparison between them is in speed. Not surprisingly, using a single run (BIDE-DC-1R) is always significantly faster than doing separate runs (BIDE-DC-CR), despite some additional overhead and deeper search required for the former. (These results are shown in  X  X ppendix X ). Thus, in the rest of the paper, we will use only BIDE-DC-1R and, for simplicity, will refer to it as BIDE-DC. Effect of k : We experimented with k = 10 , 20 , 30 , 40 , 50 , 70 , 90, as mentioned in Sect. 6.2 . Performance of BIDE-DC seems to be less sensitive to value of k and of  X  than that of BIDE-D, likely because it distributes patterns evenly across classes, while BIDE-D may suffer from redundant patterns that are all predictive for the same class. Higher values of k lead to better results where the differences are observed. Thus in the rest of the experiments, we will keep k fixed at 90. (The plots showing accuracy versus  X  for different values of k are shown in  X  X ppendix X ).

Effects of purity and leaf size: SMBT and SMBT-FS are not particularly sensitive to choices of minimum leaf size and purity (see  X  X ppendix X ). Thus we focus on results with leaf size 5 and purity of 0.9.

Accuracy comparisons: We start by comparing the accuracies of the most similar approaches: BIDE versus BIDE-C, BIDE-D versus BIDE-DC, and SMBT versus SMBT-FS. We then compare performance of  X  X inners X  to each other. The  X  X ppendix X  shows complete results and discusses them in more detail.
 BIDE leads to better results than BIDE-C on ASL-BU and Blocks, and worse ones on Auslan2 dataset. On the other datasets, however, BIDE-C is difficult to apply due to the need to set minimum support  X  separately for each class. Thus, BIDE is to be generally preferred between these two (Fig. 1 a).
BIDE-DC outperforms BIDE-D on ASL-BU, Context, Pioneer, and WW3D while having comparable accuracy on the other datasets. (Fig. 1 b).
 SMBT-FS noticeably outperforms SMBT on ASL-BU and Unix, is slightly worse on Auslan2, Context, and WW3D, and is comparable on the other datasets. As can be seen in  X  X ppendix, X  SMBT-FS also performs better against other methods (i.e., SMBT does better on datasets where both methods perform poorly). Thus, SMBT-FS is a better method. We now compare the  X  X inners X  from each pair. Comparison between BIDE-DC and SMBT-FS favors the former, which performs much better on most datasets, only slightly worse on Skating and Auslan2 (Fig. 2 b). SMBT-FS also is clearly worse than BIDE, again excepting Skating and Auslan2 (Fig. 3 a). BIDE-DC and BIDE are closely matched (Fig. 3 b), with the differences in accuracy less than 2% in either direction in almost all the cases.
We can thus conclude that BIDE-DC is the best and the most stable of the direct methods in terms of accuracy and gives performance comparable to using all the patterns mined in an unsupervised fashion, i.e., BIDE. SMBT-FS can occasionally perform somewhat better than BIDE-DC, but also frequently produces much worse results.
 Number of patterns and mining speed: Having compared the accuracies of different approaches, we turn our attention to the number of patterns mined and computational efficiency. Figure 4 a shows, on log scale, the ratio of patterns produced by BIDE to that produced by BIDE-DC. It is obvious that on most datasets, BIDE-DC uses just a small fraction of frequent closed patterns that BIDE generates. Figure 4 b shows reduction in pattern mining time from using BIDE-DC, compared to using BIDE. BIDE-DC is faster, sometimes by a lot, except on ASL-GT and Auslan2 datasets. Note that these are the same datasets where the number of patterns produced by BIDE-DC is comparable or same as that produced by BIDE. The explanation for these results is that on some datasets, for certain values of  X  few patterns exist, and BIDE finds them all faster than BIDE-DC due to smaller overhead. However, when the number of potential patterns is high, BIDE-DC is faster.

Finally, we demonstrate that using only the top discriminative features leads to improved training time for a classifier, such as SVM. Figure 5 a compares training times with patterns produced by BIDE and BIDE-DC, with latter consistently faster except on ASL-GT and Auslan, as discussed.

We also note that SMBT-FS can sometime produce a lot fewer patterns and be several times faster in mining time and training time (Fig. 5 b) than BIDE-DC, but again, the cost in terms of accuracy can be high. Detailed results are presented in the  X  X ppendix. X  7 Conclusions We have described a direct sequential pattern mining approach, BIDE-Discriminative, and how it can be utilized for discriminative sequential pattern mining in multi-class problems. We have evaluated approaches for discriminative sequential pattern mining in multi-class prob-lems (BIDE-D, BIDE-DC, SMBT, and SMBT-FS) against unsupervised approaches (BIDE and BIDE-C). Our experiments suggest that BIDE-DC is usually the best option, as it effi-ciently generates a small number of predictive patterns leading to comparable classification performance while potentially saving the user order of magnitude in memory and noticeable amount of time both during the pattern mining stage and when training a classifier. The slight advantage in accuracy obtained by BIDE-DC over BIDE-D is likely due to former extracting fewer redundant patterns.
SMBT and SMBT-FS can sometimes match accuracy of BIDE-DC with a lot fewer pat-terns, but can also result in significantly worse performance. Also, while in some cases they were faster than the other methods, in others the reverse was true. This unpredictability makes caution against these approaches, though in some application domains where very small models are required, the benefits may outweigh the risks.

Our evaluation was performed on a collection of nine real-world datasets and is the first evaluation of sequential discriminative pattern mining methods.
 References
