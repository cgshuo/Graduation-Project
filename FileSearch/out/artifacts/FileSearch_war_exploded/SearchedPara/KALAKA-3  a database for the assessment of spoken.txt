 Luis Javier Rodr X   X  guez-Fuentes 1  X  Mikel Penagarikano 1  X  Amparo Varona 1  X  Mireia Diez 1  X  Germa  X  n Bordel 1 Abstract KALAKA-3 is a speech database specifically designed for the devel-opment and evaluation of Spoken Language Recognition (SLR) systems. The database provides TV broadcast speech for training, and audio data extracted from YouTube videos for tuning and testing. The database was created to support the Albayzin 2012 Language Recognition Evaluation (LRE), which featured two lan-guage recognition tasks, both dealing with European languages. The first one involved six target languages (Basque, Catalan, English, Galician, Portuguese and Spanish) for which there was plenty of training data, whereas the second one involved four target languages (French, German, Greek and Italian) for which no training data was provided. This second task tried to simulate the use case of low resource languages. Two separate sets of YouTube audio files were provided to test the performance of language recognition systems on both tasks. To allow open-set tests, these datasets included speech in 11 additional (Out-Of-Set) European lan-guages. In this paper, we first discuss the design issues considered when creating the database and describe the data collection procedure. Then, we present the results attained in the Albayzin 2012 LRE, along with the performance of state-of-the-art systems on the four evaluation tracks defined on the database. Both series of results demonstrate the usefulness of KALAKA-3 as a challenging benchmark for the advancement of SLR technology. As far as we know, this is the first database specifically designed to benchmark SLR technology on YouTube audios.
 Keywords Spoken language recognition YouTube audio Broadcast speech European languages Low-resource languages Spoken Language Recognition (SLR) is the task of recognizing by computational means the language spoken in an utterance. SLR has been typically used as an auxiliary module in many applications, such as multilingual conversational systems (Zue and Glass 2000 ), spoken language translation (Waibel et al. 2000 ), multilin-gual speech recognition (Ma et al. 2002 ), spoken document retrieval (Bertoldi and Federico 2003 ), etc. State-of-the-art SLR systems combine two main complemen-tary approaches based on low-level acoustic models and high-level phonotactic models, respectively. Acoustic systems take information from the spectral characteristics of the audio signal, whereas phonotactic systems extract information from phonetic sequences/lattices produced by phone decoders. A complete survey of state-of-the-art technologies, the most common benchmarks and metrics can be found in (Li et al. 2013 ).
 The development of SLR technology has been largely supported by NIST Language Recognition Evaluations (LRE). 1 As a result, the datasets produced and distributed for such evaluations have become standard benchmarks to check the usefulness of new approaches. NIST LRE datasets consist of narrow-band (8 kHz) conversational telephone speech and include a relatively high number of target languages (e.g. 24 in NIST 2011 LRE). NIST evaluations have fostered a symbiotic relationship between the research community, that provides increasingly powerful algorithms, and the government agencies, that continuously support the production of data. This fruitful collaboration has focused on the large-scale verification of telephone conversations in a number of target languages interesting for defense/ intelligence applications. But there has been a lack of resources to objectively assess SLR technology on other types of speech (e.g. speech produced by multiple speakers in different and changing environments), on other sets of languages (e.g. European languages) or applications (e.g. indexing of the spoken language in spoken documents).

With the aim of filling this gap in the assessment of SLR technology, in 2008 we started organizing a new series of SLR evaluations: the Albayzin LRE, supported by the Spanish Thematic Network on Speech Technologies 2 and the ISCA Special Interest Group on Iberian Languages (SIG-IL). Three editions have been carried out so far. In 2008 and 2010, a NIST-style verification task was proposed (i.e. systems should decide whether or not a target language was spoken in a test utterance), with the same protocols, performance measures, file formats, etc. as NIST LRE, but using wide-band TV broadcast speech, possibly produced by multiple speakers and under diverse environments.
The Albayzin 2008 LRE (Rodriguez-Fuentes et al. 2010a ) used the four official languages in Spain (Basque, Catalan, Galician and Spanish) as target languages, and four European languages (English, French, German and Portuguese) as Out-Of-Set (OOS) languages for open-set tests. Speech segments were extracted from wide-band stereo TV broadcast recordings and stored as single-channel 16 kHz 16-bit PCM-encoded WAV files. Despite the high quality of speech recordings, a relatively high confusion among target languages (specially between Galician and Spanish) was found. Note that most speakers of Basque, Catalan and Galician also speak Spanish in their daily lives (Spanish being even the mother language for some of them), making their phoneme inventories, pronunciations, etc. quite close to each other, which would partly explain the high confusion among these languages.
The Albayzin 2010 LRE (Rodriguez-Fuentes et al. 2011 ) considered six target languages: Basque, Catalan, English, Galician, Portuguese and Spanish, for which an increased amount of training data was provided, and four OOS languages (Arabic, French, German and Romanian) for open-set tests. Besides studio-quality (clean) TV broadcast speech, a second type of recordings was used, which included background noise/music and/or overlapping conversations. Thus, two different tasks were defined, the first one on clean speech and the second one on a mixture of clean and noisy speech. We found that the average performance on the full set of target languages was better than the average performance on the four official languages in Spain, meaning that error rates for Portuguese and English were lower than the average. This is not surprising, since English and Portuguese utterances come from speakers that do not speak Spanish in their daily lives and thus we may reasonably expect them to be more easily distinguishable from the other four languages. The average performance on the four official languages in Spain was better than that attained in 2008, in part due to improvements in technology but also to the availability of more training data. Finally, as could be expected, performance degraded remarkably when switching from clean speech to a mixture of clean and noisy speech.

Based on these findings, we concluded that language recognition technology should deal with acoustic variability (channel, noise, music, overlapping speakers, etc.) which is inherent to some media (such as the videos posted by people in the Internet), and data availability constraints which may seriously limit the performance of state-of-the-art systems [for a more detailed study, see (Ro-driguez-Fuentes et al. 2012c )]. Therefore, the Albayzin 2012 LRE (Rodriguez-Fuentes et al. 2013 ) was designed with the aim of testing the performance of state-of-the-art language recognition systems on unconstrained speech, possibly in a low-resource scenario (i.e. with just a reduced amount of hand-checked data available for development), which is the case of many languages in the world. These new conditions matched two pre-requisites that, from our point of view, are essential to any technology benchmark: (1) the task must be of practical interest (in this case, indexing multimedia contents with the spoken language); and (2) the task must be challenging (i.e. difficult) enough in order to foster technological improvements. With these goals in mind, KALAKA-3, the database created to support the Albayzin 2012 LRE, was built by recycling TV broadcast speech (both clean and noisy) from previous evaluations and by collecting new unconstrained speech signals from YouTube videos. TV broadcast speech signals were used for training, whereas audio tracks extracted from YouTube videos were used for tuning and testing.

Two different tasks were defined: (1) Plenty-of-Training , which involved six target languages (Basque, Catalan, English, Galician, Portuguese and Spanish) for which a large amount of hand-checked training data was provided; and (2) Empty-Training , which involved four target languages (French, German, Greek and Italian), for which no training data was provided, trying to simulate the conditions of a low-resource scenario. Our choice of target languages was largely motivated by our competence as auditors to reliably label speech files with the spoken language. It is noteworthy that the four target languages used in the Empty-Training task are not intrinsically low-resource, since there is plenty of speech data available for them, from different sources. However, participants in the Albayzin 2012 LRE were constrained (in both tasks) to train models based exclusively on the speech data provided by the organizers.

Two disjoint sets of YouTube audio files were provided for tuning and testing system performance, respectively. To allow open-set tests, these datasets were augmented with speech files including nominally 11 OOS languages: Bulgarian, Czech, Croatian, Finnish, Hungarian, Polish, Romanian, Russian, Serbian, Slovak and Ukrainian. With nominally we mean that these files were collected from a language-specific list of YouTube videos but checked only to not contain any target language , being labelled generically as OOS. During the evaluation, language labels (needed to compute performance) were released only for the tuning set. Language labels for the test set were released just after the publication of system results.
The main features of the database are outlined in Sect. 2 , including a breakdown of the training dataset. Details about the design of the development and evaluation datasets are given in Sect. 3 . Section 4 describes the collection procedure and the validation criteria applied to select YouTube audios. Section 5 presents a summary of the evaluation conditions, the performance metric and the results attained in the Albayzin 2012 LRE, along with the performance attained by state-of-the-art SLR systems developed at our site. Finally, conclusions and future work are outlined in Sect. 6 . KALAKA-3 consists of three subsets: training, development and evaluation. The training dataset comes entirely from KALAKA-2 (Rodriguez-Fuentes et al. 2012b ), the database created to support the Albayzin 2010 LRE, which was an extension of KALAKA (Rodriguez-Fuentes et al. 2010b ), the database that supported the Albayzin 2008 LRE. The training dataset consists of TV broadcast recordings (stored as single-channel 16 kHz 16-bit PCM encoded WAV files), including both planned and spontaneous speech in diverse environment conditions (excluding telephone-channel speech) and spoken by multiple speakers. Though not strictly checked, the number of speakers in each broadcast recording ranged typically from one to ten. Since tens of broadcasts were recorded per target language, the training dataset features a sizeable amount of speakers. Two disjoint training subsets have been defined, consisting of clean speech (more than 90 h) and noisy speech (more than 23 h), respectively (see Table 1 ).

Noisy-speech segments may include noisy and/or overlapped speech, maybe with short fragments of clean speech. Different and variable types of noise may appear: street, music, cocktail party, laughs, clapping, etc. Most speech overlaps appear in hot spots of informal debates in late night shows, magazines, etc. which, on the other hand, usually feature clean-channel and quiet-background (studio) conditions. In all cases, each training segment contains speech in a single language. The development and evaluation datasets have been specifically collected for KALAKA-3, and consist of unconstrained YouTube audio signals (originally in different formats and qualities, but all of them stored as single-channel 16 kHz 16-bit PCM encoded WAV files), with the only requirement that a single language is spoken in them. Note that, besides speech produced by possibly multiple speakers, any other sound (music, noise, etc.) could appear in YouTube audios, which makes the task specially challenging. The design of these datasets, the collection procedure and the validation criteria are described in Sects. 3 and 4 .

In summary, the training dataset amounts to around 113 h of speech (80 % clean, 20 % noisy), with nearly 19 h on average for each one of the 6 target languages considered in the Plenty-of-Training task. The development and evaluation datasets have the same size (each containing more than 2000 YouTube audios) and structure, but a different distribution of OOS languages, to avoid that systems overfit to reject specific OOS languages. The whole database amounts to around 200 h of audio and is distributed as a set of downloadable tarballs, after direct request to the authors. As noted above, the training data was entirely imported from KALAKA-2, so efforts focused on collecting audio from YouTube videos for the development and evaluation datasets. Since language recognition systems were expected to be tuned on the developmet dataset, we kept in mind that the evaluation dataset should be as independent as possible from the development dataset in order to avoid a biased benchmark. This was partially addressed by collecting videos from different YouTube categories, meaning that different topics, different speakers and even different recording conditions appear in both datasets. In previous Albayzin LREs, system performance was measured on three different subsets of speech signals, with nominal durations of 30, 10 and 3 s. In the Albayzin 2012 LRE, these nominal duration subsets were not considered anymore. However, to keep things reasonably bounded, YouTube audios were constrained to be between 30 and 120 s long, with at least 5 s of speech. Also, in order to keep consistency in the database, YouTube audios containing telephone-channel speech were discarded. The goal was to collect 300 YouTube audios for each target language (150 for development and 150 for evaluation) and around 100 YouTube audios for each OOS language (with different distributions in the development and evaluation datasets). In this way, each dataset would consist of around 2000 YouTube audios, which was considered enough for benchmarking language recognition technology. 4.1 Building lists of YouTube videos After a preliminary study for Spanish, based on a small set of keywords taken from the aspell 3 dictionary and considering different YouTube video categories, we chose the six categories most likely to contain speech: Education , News , Entertainment , Howto , Nonprofit and Technology . Then, a large list of YouTube videos was created for each category and each of the 21 languages considered in the database, using the YouTube API 4 to search for language-specific common words in the title, description and other metadata (tags) associated to each video. Also, in order to avoid (as far as possible) that the same speaker appeared multiple times in the audios, each YouTube user was allowed to contribute a single video.

The words used for searching video metadata were taken from the aspell dictionary of each language, with the following criteria:  X  Word inflections and verb tenses were not considered (only canonical forms  X  Words with less than 6 characters were filtered out. This way, we tried to discard  X  For each language, 2000 words were randomly chosen among those fulfilling the  X  For any given language, those words that appeared in the aspell dictionaries of  X  Only 1000 words were retained per language.
 Two additional criteria were applied to rank the videos in the list:  X  The first and most important criterion was the existence of a Creative Commons  X  The second criterion was the geographical location tag associated to some
As a result of applying the above described criteria, we found ourselves with 21 languages 6 categories  X  126 long lists (10 6  X  60 for target languages ? 11 6  X  66 for OOS languages) of ranked YouTube videos to validate. It must be noted that our efforts for collecting videos with a Creative Commons (CC) license had little success. In the resulting lists, only a small fraction of the videos had a CC license. 4.2 Validating YouTube videos Each list consisted of a spreadsheet with links to YouTube source pages, which was scrolled through by a human auditor to listen to and look at the YouTube videos. The goal was to validate 55 videos per list for target languages, and 17 videos per list for OOS languages. These numbers are slightly higher than those needed, to account for errors during or after downloading.

Let us consider a spreadsheet corresponding to a target language t . A video was validated if and only if: (1) There was sufficient amount of speech (around 5 s) to allow for the (2) Only the target language t was spoken (other languages not being allowed), no (3) The environment conditions and the recording quality were good enough for (4) There was no telephone-channel speech in the video.
 All these criteria were applied subjectively by the auditors (meaning that e.g. no objective measure of intelligibility was applied). In the case of OOS languages, the second condition was relaxed, so that there could be speech in several languages, provided that none of them was a target language. In other words, an audio file was labelled as OOS if and only if the auditor was sure that it didn X  X  contain any target language. In case of doubt, the audio file was discarded. Note again that target languages were chosen based primarily on the competence of auditors to recognize them. Auditors might fail to distinguish OOS languages among each other, but were able to reliably detect the presence of any target language.

The validation task was carried out by five auditors (the authors of this paper) and took more than 2 months. A breakdown of the audited and validated videos per language and category is shown in Table 3 . For some target languages (such as Basque, Galician and specially Portuguese), the validation task required the audition of a large number of videos. In the case of Portuguese, few videos were geographically located and most of them contained Brazilian Portuguese. Unfor-tunately, these videos had to be discarded in order to keep consistency with the training materials, which include only European Portuguese. In the case of Basque and Galician, many videos did not contain speech and there was a lack of materials for some YouTube categories (e.g. Howto and Nonprofit ), which was compensated by including more videos from other categories. The most difficult category was Entertainment , for which 738 videos were validated out of 5342 videos audited (meaning a success rate of only 13.6 %). Finally, auditors did not verify the spoken language in audio files corresponding to OOS languages, but just the absence of target languages (which was enough to label them as OOS). Thus, OOS languages in Table 3 are just nominal (the same stands for Table 4 ).
 4.3 Fetching and converting YouTube audios The validated YouTube videos were automatically downloaded by processing the spreadsheets and applying the youtube-dl 5 tool. Then, the ffmpeg 6 tool was used to extract the audio layer from the videos and the SoX 7 tool was applied to get single-channel 16 kHz 16-bit PCM encoded WAV audio files. Since YouTube contents evolve dynamically (many videos are available only for some months or even weeks before the owner removes them), we made a local copy of all the videos, audios and metadata downloaded from YouTube, strictly for backup purposes. Thus, the collection of videos used for this benchmark is just a representative sample of YouTube contents at a specific time, and there is no guarantee that the particular videos drawn from the repository are still available. The database does not provide any information about the videos, but just the audio and the identity of the spoken language, which is specified in the ground truth files used to measure system performance.
 4.4 Collection of YouTube audios As a result of the above described procedure, 4168 YouTube audios were validated out of 21,860 audited videos: 2059 audios were posted for development (extracted from the News , Education and Howto categories), whereas 2109 audios were posted for evaluation (extracted from the Entertainment , Nonprofit and Tech categories), with at least 150 audios validated per target language in each dataset. In order to avoid that SLR systems overfit to reject specific OOS languages, we designed the development and evaluation datasets with different distributions of OOS languages, as follows:  X  All the audios nominally containing speech in Czech, Croatian, Polish and  X  All the audios nominally containing speech in Bulgarian, Finnish, Slovak and  X  The audios nominally containing speech in Hungarian, Russian and Ukrainian A breakdown of the development and evaluation datasets is shown in Table 4 .Some of the validated videos were not available at the time of downloading (typically because the owner removed them). Audio conversion errors were also encountered. Thus, numbers in Table 4 are slightly smaller than those presented in Table 3 (e.g. for Catalan and German in the development set). 5.1 Albayzin 2012 LRE 5.1.1 Evaluation conditions As noted above, the Albayzin 2012 LRE defined two tasks: Plenty-of-Training , involving 6 target languages for which plenty of training data was provided; and Empty-Training , involving 4 target languages for which no training data was provided. The second task was introduced with the aim of measuring system performance in the use case of a low-resource scenario. In both tasks, participants were allowed to use only the training and development data provided by the organizers. Besides, the evaluation considered two conditions (open-set and closed-set) depending on whether or not audio files labelled as OOS were included in the test set. Therefore, four different tracks were defined: Plenty-Closed (PC), Plenty-Open (PO), Empty-Closed (EC) and Empty-Open (EO). 5.1.2 Performance metric A new metric was introduced in this evaluation, based on a calibration-sensitive, multi-class cross-entropy criterion, which does not require making hard decisions but instead measures the information provided by a spoken language recognition system through a set of log-likelihoods. The performance metric, called actual relative confusion ( F act ), represents the factor by which the system changes (hopefully, reduces) the prior confusion (that corresponding to a non-informative system). Informative systems will attain relative confusions between 0 and 1 (being 0 for a perfect system and 1 for a non-informative system). Under this metric, the task is defined as follows: given a test audio and assuming N target languages, the system must provide N  X  1 scores, one per target language plus an additional score for OOS languages, which are interpreted as log-likelihoods. The score for OOS languages is ignored in the closed-set condition. Note that participants could separately optimize and submit different systems for the closed-set and open-set conditions. This new metric was developed in collaboration with Niko Bru  X  mmer, from Agnitio South Africa, and can be computed by means of a freely available toolkit. 8 For a more complete and motivated explanation of F act , see (Rodriguez-Fuentes et al. 2013 ) and (Rodriguez-Fuentes et al. 2012a ). 5.1.3 SLR technology Most of the systems submitted to the Albayzin 2012 LRE followed state-of-the-art approaches, including the Total Variability Factor Analysis (iVector) approach (Dehak et al. 2011 ), followed either by Support Vector Machines (SVM) or by linear generative classifiers (Martinez et al. 2011 ), and the Parallel Phone Recognizer followed by SVM (PPR-SVM) approach (Li et al. 2007 ; Richardson and Campbell 2008 ), based on high-performance phone decoders (Matejka et al. 2005 ). In particular, iVector systems were trained on different sets of features, including Shifted Delta Cepstrum (SDC) (Torres-Carrasquillo et al. 2002 ), trigrams of posteriorgram counts (D X  X aro et al. 2012 ) and prosodic features (Mart X   X  nez et al. 2012 ). The primary systems were built by fusing at the score level various independent subsystems (developed with different features and/or modeling approaches), commonly by applying the FoCal toolkit (Bru  X  mmer and van Leeuwen 2006 ; Bru  X  mmer 2008 ). Another important issue for having success in this evaluation was Speech Activity Detection (SAD). Note that YouTube audios, besides speech and silent segments, may contain any sequence of sounds, sometimes mixed: noise, music, background voices, etc. so the task of extracting relevant speech segments was not easy. Participants in the Albayzin 2012 LRE were allowed to use any data sources and/or subsystems to perform SAD. 5.1.4 Results Table 5 presents the F act values for all the primary systems submitted to the evaluation: those submitted on time, those submitted after the deadline (late) and those submitted after the keyfile was released (postkey), typically to fix bugs. Best overall results in each track are marked in boldface. Since teams names and affiliations were not allowed to be disclosed in communications or papers describing the evaluation, we just provide a team identifier. Table 5 also provides a summary of the subsystems built by each team, by listing the modeling approaches applied to each type of features (acoustic, phonotactic or prosodic, if used): iVector-LR (iVectors with Logistic Regression), iVector-G (iVectors with linear generative models, typically Gaussian), SVM (Support Vector Machines), iVector-LDA (iVectors with Linear Discriminant Analysis), iVector-SVM (iVectors with SVM), ngram-LM ( n -gram based language models), cosine (cosine distance), GMM-MMI (Gaussian Mixture Models with Maximum Mutual Information training), GMM-SVM (Gaussian Mixture Model supervectors followed by SVM), MCSK (Multiple Coordinate Sequence Kernel, similar to GMM-SVM, but using first and second order Baum-Welch statistics as supervectors), Fisher-SVM (similar to GMM-SVM but applying a Fishervoice transformation to the features).
The most competitive system in the Plenty-of-Training task (1-late) yielded only slightly better performance on the PC track ( F act  X  0 : 068) than on the PO track ( F act  X  0 : 077). Moreover, the best system in the Empty-Training task yielded worse performance in the EC track than in the EO track ( F act  X  0 : 142 and F act  X  0 : 130, respectively). Therefore, either system scores were (on average) better for OOS languages than for target languages, or a more robust calibration of system scores was attained when including OOS utterances. Probably, most systems were able to easily discriminate OOS languages (most of them Slavic) from target languages (most of them Romance). From this point of view, a better choice of OOS languages could have been done, with them being closer to target languages, in order to make the open-set condition more challenging than it eventually proved to be.
In the Empty-Training task, we expected that using Plenty-of-Training models to get a set of reference scores, along with a backend (trained on the development data available for target languages) to map reference to target scores, would help overcome the lack of training data. This was the approach applied by some teams (e.g. 2-postkey and 4-postkey), but a large degradation in performance with regard to the Plenty-of-Training task was observed, presenting the lack of training data as a truly challenging condition.

Some systems (1-late, 1-postkey and 6-late) tried to alleviate the lack of training data by using part of the development data to train models for the target languages. Considering the results attained by these systems (remarkably those of 1-postkey, which is the best system in EC/EO), it seems clear that having a certain amount (even a small amount) of training data for target languages greatly helps to attain good performance. Note, however, that the SLR approach plays a key role for high performance: besides the large difference between 1-postkey and 6-late, it is also remarkable that 6-late obtained the same or even worse performance in EC/EO than 4-postkey, which did effectively apply the restriction of zero training data for target languages. In any case, there is still room for improving SLR technology when little training data is available for target languages: F act of the best system in EC/EO is twice that of the best system in PC/PO. 5.1.5 Confusion among languages In this Section, we will briefly analyze the confusion among languages, using the best systems submitted to the four tracks of Albayzin 2012 LRE (see Table 5 ). To that end, log-likelihoods are first converted to log-likelihood ratios (LLR), then NIST-style trials (involving a test audio and a target language) are considered and decisions made according to a threshold h (a trial is decided False if the corresponding LLR is lower than h , and True otherwise), so that miss and false alarm error rates can be computed for each target language. The LLR threshold is set to h  X  0, which corresponds to the prior and costs typically used in NIST evaluations: P target  X  0 : 5 and C miss  X  C fa  X  1.

Here, the term confusion refers to the situation where a bad decision is made, not to the metric F act . Given a subset of test audio files S l , known to contain the computed as the fraction of files of S l for which the system decision is True . Similarly, when l is used as target language, the miss error probability P miss  X  l  X  is computed as the fraction of files of S l for which the system decision is False .
Figures 2 and 3 show P miss  X  l  X  and P fa  X  l ; t  X  for the best systems in the PC, PO, EC and EO tracks. Each column corresponds to a target language l , with P miss  X  l  X  in the bottom panel and P fa  X  l ; t  X  values stacked in the top panel, including the average P fa as a special narrow box. The analyzed systems come all from the same team, but an optimized (postkey) version has been used for the Empty-Training task, which explains the good results attained when compared to those of the Plenty-of-Training task. Also, depending on the closed-set/open-set condition, different datasets were used to calibrate scores. Strictly speaking, confusion is only related to P fa , not to P miss , so our analyses will focus on P fa values. Note, however, that both types of errors are implicitly related through the chosen threshold h .

In the Plenty-of-Training task, closed-set condition, English shows (as could be expected) the lowest P fa , followed by Portuguese and Basque (both being confused mostly with Catalan and Galician). As it was already found when performing the same analysis for KALAKA-2 (Rodriguez-Fuentes et al. 2012c ), relatively high P fa values are found among the four languages spoken in Spain (Basque, Catalan, Galician and Spanish), Spanish and Galician being (by far) the most confusable languages. Besides their common origins (except for Basque), a strong argument to explain this high confusion is that all the involved speakers speak Spanish in their daily lives, in many cases as mother language. Though Basque has unknown origins (probably, it was spoken in the same region from ancient times), it shares some vocabulary, phonetic and phonological features with the surrounding languages. In any case, its syntax and the most common words have nothing to do with other languages and may partly explain the lower confusion values for Basque. The same trends are observed in the open-set condition, with generally higher confusion values (except for Portuguese) and English and Portuguese presenting much higher P fa values for OOS than for target languages.

In the Empty-Training task, closed-set condition, the confusion among target languages is not much higher than in the Plenty-of-Training task, which supports the decision of using part of the development data to train models for target languages. Results are specially good in the open-set condition (EO track), even better than in the PO track. In this regard, we must remind that an optimized version of the SLR system has been applied in the Empty-Training task. Some readers may find these results contradictory with the F act performance presented in Table 5 (where F act is twice for EO than for PO). To explain this, it must be noted that F act evaluates the information provided by system scores, not the detection decisions based on them. This means that the scores of a system A may provide much more information than those of another system B, but detection performance ( P miss and P fa ) at a particular operation point may be better for system B.

The lowest average P fa in the Empty-Training task is found for Greek (third column, mostly confused with Italian). Note, however, that other target languages (specially Italian) do confuse with Greek, which probably means that Greek utterances feature smaller variability and higher internal consistency than the average for target languages in this task. The highest confusion in the EC track is found for Italian (confused mostly with Greek). In the EO track, all the P fa values decrease, specially for Italian, and French yields the highest confusion. Overall, Greek and Italian account for most of the confusion. Interestingly, OOS languages are also mostly confused with Greek and Italian. 5.2 Evaluation based on in-house SLR systems To further validate KALAKA-3 and to fairly compare SLR performance on the four tracks of Albayzin 2012 LRE using the same set of systems (same approaches, same configuration, etc.), we have carried out a series of experiments based on five state-of-the-art systems (two acoustic ? three phonotactic) developed at our site, and on different fusions of them. 5.2.1 Acoustic systems The acoustic systems follow the Total Variability Factor Analysis ( iVector ) approach as described in Dehak et al. ( 2011 ) and Martinez et al. ( 2011 ). Two different feature sets were considered under the iVector approach: (1) the traditional Mel-Filter Cepstral Coefficient / Shifted Delta Cepstrum (MFCC/SDC) features (Torres-Carrasquillo et al. 2002 ), under a 7-2-3-7 configuration; and (2) the recently introduced log-likelihood ratios of phone posterior probabilities, hereafter called Phone Log-Likelihood Ratios (PLLR) (Diez et al. 2012 ), based on the posteriors provided by the open-software Temporal Patterns Neural Network (TRAPs/NN) phone decoder for Hungarian developed by the Brno University of Technology (BUT) (Schwarz 2008 ).

In both cases, Speech Activity Detection (SAD) was performed by removing the feature vectors for which the highest PLLR value was that of the phonetic unit representing non-speech events (silent pauses, short noises, etc.), using the BUT phone decoder for Hungarian. Though this SAD scheme has worked fairly good for detecting speech in telephone audio, in this case we deal with unconstrained speech, and SAD could be failing due to e.g. the presence of music or background conversations. A more sophisticated audio classification scheme should be applied instead to avoid these issues.

A gender-independent 1024-mixture UBM was estimated by Maximum Likeli-hood on the training dataset. The total variability matrix T was estimated according to the procedure defined in Dehak et al. ( 2011 ), but using only data from target languages, as in Martinez et al. ( 2011 ). A generative modeling approach was applied in the iVector feature space, the set of iVectors of each language being modeled by a single Gaussian distribution, as in Martinez et al. ( 2011 ). 5.2.2 Phonotactic systems First, note that phonotactic models are based on segmental information (e.g. phone n -gram counts), not frame-level information, thus a frame-level SAD such as the one applied to acoustic models (which splits out non-speech frames in a discrete fashion) is not suitable, because it may seriously distort segmental information. Therefore, we decided not to apply any SAD in this case, relying on the phone decoder to handle non-speech events. Though a sizeable amount of mistakes could be expected when dealing with noisy speech or noisy/music segments, phonotactic models could also collect relevant information for the SLR task.

The phonotactic systems developed for these experiments followed a phone-lattice Support Vector Machine (SVM) approach (Penagarikano et al. 2011b , c ). Given an input signal, one of the BUT TRAPs/NN phone decoders for Czech (CZ), Hungarian (HU) and Russian (RU) (Schwarz 2008 ) was applied to produce a sequence of frame-level phone posteriors. This sequence was used to produce phone lattices by means of HTK (Young et al. 2006 ) along with the BUT recipe, on which expected counts of phone n-grams were computed using the lattice-tool of SRILM (Stolcke 2002 ). Finally, a SVM classifier was applied, SVM vectors consisting of counts of features representing the phonotactics of an input utterance. In this work, phone n -grams up to n  X  3 were used, weighted as in Richardson and Campbell ( 2008 ). A sparse representation was used, which involved only the most frequent features according to a greedy (and quite efficient) feature selection algorithm (Penagarikano et al. 2011a ). L2-regularized L1-loss support vector classification was applied, by means of LIBLINEAR (Fan et al. 2008 ), whose source code was slightly modified to get regression values. 5.2.3 Backend and fusion When processing an input utterance, our SLR systems provide a score for each target language. A Gaussian backend was estimated, based on the scores obtained for the development set, and applied to the scores obtained for the evaluation set, in order to get log-likelihoods (one per target language). Log-likelihoods were then calibrated and fused according to a discriminative linear model which minimized the so called C llr function on the development set, by means of logistic regression, as explained in (Bru  X  mmer and van Leeuwen 2006 ). To alleviate the lack of training data in the Empty-Training condition, one half of the development data was used for training models of target languages and the other half for estimating the backend and fusion parameters. 5.2.4 Results Table 6 shows the performance of the above described systems and their fusion in the four tracks of the Albayzin 2012 LRE. Besides F act , Equal Error Rate (EER) values are also shown, for the reader to compare to other results in the literature and to better evaluate the difficulty of the proposed tasks. 9 Performance is pretty good in the Empty-Training task when compared to results in Table 5 , specially for the acoustic systems. Moreover, the fusion of all systems provides the best reported performance in the EC track ( F act  X  0 : 104) and very competitive performance in the EO track ( F act  X  0 : 169, with only 1-postkey outperforming it). This remarkable performance can be partly explained by the use of half of the development data to train specific models for target languages. Note that, though this use was not explicitly forbidden in the Albayzin LRE Plan (Rodriguez-Fuentes et al. 2012a ), the development dataset was designed only for tuning systems, not for training models. Things were arranged this way to force Albayzin 2012 LRE participants to explore alternative ways of performing the task in a low-resource scenario, where no training data was available, i.e. without using specific models for target languages.
In the Plenty-of-Training condition, the performance of our systems is not far from the best results in Table 5 , specially in the case of acoustic systems. The fusion of all systems provides competitive performance in the PC track, but there is still room for improvement in the PO track. After a preliminary study of the obtained results, we found a sizeable number of SAD errors which made our acoustic systems to produce bad scores. In the case of phonotactic systems, we did not even apply any SAD, so we reasonably hope to get improved performance by using a better (segmental-level) SAD. In this paper, we have described KALAKA-3, the database created to support the Albayzin 2012 Language Recognition Evaluation. This evaluation was targeted at exploring the performance of state-of-the-art language recognition systems on unconstrained speech, even in a low-resource scenario where little training data is available for target languages. The datasets used for tuning and testing systems consist of YouTube audios, which, as far as we know, are used for the first time to benchmark spoken language recognition technology.

Besides describing the collection procedure and the criteria applied to filter and validate YouTube videos, the paper has also evaluated the database, by presenting a summary of the results attained in the Albayzin 2012 Language Recognition Evaluation, along with a brief analysis of the confusion among target languages. Results attained by two acoustic iVector systems and three phonotactic phone-lattice-SVM systems recently developed by our research group have been also presented to further validate the database. We are currently working on the development of a more robust SAD module, that we expect to lead to improved performance.

Considering the results attained by state-of-the-art SLR systems on NIST LRE datasets (typically, between 2 and 5 % Equal Error Rate on 30-s segments) (Martin et al. 2014 ) and the results reported in this paper, we think that KALAKA-3 can be consistently used as an alternative and challenging SLR benchmark to support further technology advancements. In fact, KALAKA-3 has been already used for the development of new SLR approaches in D X  X aro et al. ( 2013 ), D X  X aro et al. ( 2014a ), D X  X aro et al. ( 2014b ). Hitherto, the database has been distributed as a set of downloadable tarballs after direct request to the authors. We are currently working towards licensing and distribution of the development and evaluation datasets of KALAKA-3 through the Linguistic Data Consortium, taking advantage of their agreement with Google for distributing and licensing materials extracted from YouTube videos (either with or without a CC license).
 References
