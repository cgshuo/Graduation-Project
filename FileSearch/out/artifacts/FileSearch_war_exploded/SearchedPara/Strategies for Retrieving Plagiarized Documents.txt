 For the identification of plagiarized passages in large document collections we present retrieval strategies which rely on stochastic sampling and chunk indexes. Using the entire Wikipedia corpus we compile n -gram indexes and compare them to a new kind of finger-print index in a plagiarism analysis use case. Our index provides an analysis speed-up by factor 1.5 and is an order of magnitude smaller, while being equivalent in terms of precision and recall. Categories and Subject Descriptors : H.3.3 [Information Stor-age and Retrieval]: Information Search and Retrieval X  Retrieval models, search process ; H.3.1 [Information Storage and Retrieval]: Content Analysis and Indexing X  Abstracting methods, Indexing methods General Terms : Theory, Performance Keywords : plagiarism analysis, hash-based indexing, fuzzy-finger-printing
In the generic plagiarism analysis situation we ask the following question:  X  X id the author of a document d q commit a plagiarism offense? X 
Approaches for computer-based plagiarism analysis break down this question into manageable parts:  X  Given a collection D of documents,
The respective algorithms require the collection D stored in a preprocessed form, typically as a tailored chunk index D . A chunk index is an inverted file whose vocabulary (the left-hand side of the mapping) contains larger portions of the document, such as n -grams, shingles, or other multi-term features. Based on such an in-dex, we propose to organize the retrieval of plagiarized documents as a three-stage process (cf. Figure 1): 1. Heuristic Retrieval. Samples of the suspicious document d 2. Detailed Analysis. The possibly plagiarized passages in each 3. Knowledge-based Post-processing. It is analyzed whether
In this paper we concentrate on the first step: We present a new kind of chunk index, based on so-called fuzzy-fingerprints, and compare it to a standard n -gram index with respect to storage re-quirements and retrieval efficiency. In our analysis we have indexed the entire Wikipedia corpus 1 , for which we achieve a storage reduc-tion by a factor &gt;10 and a retrieval speed-up by factor 1.5. Related Work. Research on automated plagiarism analysis focuses mainly on Step 2 in Figure 1, using technologies from the field of near-duplicate detection [1, 5]. New similarity hashing techniques from [4, 6] have not yet been considered for plagiarism analysis, despite their promising retrieval properties. Contributions related to tailored indexes for near-duplicate detection come from [1, 3]. The former introduce an index based on the so-called SPEX algo-rithm which allows for the identification of duplicate chunks within a closed document collection but cannot be employed to open re-trieval situations. The latter develop a variant of an inverted file index where duplicate text-passages are indexed only once, achiev-ing size reductions of 30% .
In the plagiarism analysis use case, a comparison of a document d against a collection D is a problem of inherently quadratic run-time at the passage level: n -gram by n -gram of d q must be looked-up in an inverted file index D of D . A small value for n leads to a large set of candidate documents implying a high post-processing effort, a large value for n minimizes the chance of detecting a pla-giarized passage that has been slightly modified. Experience has shown that useful values for n are between 3 and 5 [5]. A Wikipedia snapshot from November 4th, 2006 was indexed. Figure 2: Only a portion  X  of all chunks identifies a plagiarized passage p q of size l q . The figure shows the computation of  X  . Lower Bounds for Sample Extraction. Given an upper bound for the plagiarized portion p of d q , a lower bound for the number of n -grams to be extracted can be stated such that plagiarized passages are covered with high probability.

Suppose that a chunk is drawn uniformly at random from d q let X be the random variable that has value 1 if the chunk belongs to a plagiarized passage, and 0 otherwise. Then X has a Bernoulli distribution with parameter p . If the experiment is repeated r times with X 1 ,...,X r denoting the respective outcomes, the variable S r = r i =1 X i has a binomial distribution, and the probability that S r takes value k is P ( S = k )= r k p k (1  X  p ) r  X  k determine a value for r such that the equation P ( S r &gt;k holds for a given k at a desired confidence threshold  X  : Remarks. (1) k defines the number of plagiarized passages that are discovered in d q . (2) In general holds r  X  k p since the expected value E ( S r )= rp . (3) The last implication results from the Cher-noff Inequality applied to the binomial distribution; it is used to derive a closed form for a lower bound of r .
 Chunking with Fuzzy-Fingerprints. The chunk size can be sig-nificantly increased, if a fuzzy match instead of an exact match is applied in the heuristic retrieval step. In this connection we pick up the concept of fuzzy-fingerprinting [6]: the vector space represen-tation of a text passage is  X  X ondensed X  toward a small set of prefix classes, where a prefix class comprises all terms with the same pre-fix. The observed prefix class frequencies are normalized and com-pared with their expected values from the British National Corpus. The resulting exact deviations are fuzzified with different fuzzifica-tion schemes and encoded as numbers. In this way, chunks of size l , with l  X  [40 , 200] , are encoded as two or three 8-byte numbers.
In our use case we assume for a suspicious paragraph p q and its plagiarized counterpart p x a similarity of  X  ( p q ,p x )  X  0 the vector space model. Thus, to identify this similarity with two chunks c q  X  p q and c x  X  p x , the overlap of c q and c at least 0 . 8 , which in turn means, that only a portion  X  of all chunks that intersect with p q can be considered. Figure 2 illustrates the computation of  X  ; when working with fuzzy-fingerprints the value p in Inequality (1) and (2) must be multiplied by  X  .
For analysis purposes we have compiled three chunk indexes containing the 1.5 million Wikipedia articles. Two of the indexes, D 3-gram and D 4-gram store all 3 -and 4 -grams respectively; the third
Table 1: Characteristics of the Wikipedia chunk indexes. index, D FF , stores all fuzzy-fingerprints that were computed from chunks of size l =40 having a 50% offset.
 Index Data Structure. Since the Wikipedia collection is known in advance we designed a space efficient inverted file data structure based on minimal perfect hashing. The data structure implements a hash table T with |T| storage positions. A particular hash function h : U  X  X T| is used to map the universe of chunks, U , perfectly onto a minimum number of storage positions, say, |T| = | U be constructed in O ( | U | ) time and space and requires additional storage of size 4 . 6  X | U | bytes [2]. The overall sizes of the compiled indexes are shown in Table 1. Note that D FF needs only a small fraction of the storage of the n -gram indexes. Also note that postlist compression based on a gap statistic evaluation cannot significantly change these relations.
 Runtime Analysis. To compare the runtime performance we ana-lyzed custom-built documents that contained plagiarized passages from Wikipedia articles. The documents X  lengths ranged from to 300 pages, the plagiarized portion p ranged from 0 . 1% to As determined by Equation 1 for  X  =0 . 9 and k =1 , between r = 150 ... 3500 chunks (size l =40 )and 4 -grams were ran-domly extracted from the test documents and looked-up in the in-dexes D FF and D 4-gram respectively. For each document the aver-age number of result documents per query was recorded, whereas common 4 -grams (occurring in more than 10 documents) were dis-carded. The left plot in figure 3 shows the averaged number of returned documents depending on the plagiarized portion p :there-trieval based on D FF outperforms the 4 -gram index by factor 1.5. Note that the result set size determines the true plagiarism analysis effort, since each result document must be loaded and compared in detail. The right plot shows precision and recall at certain similarity thresholds for fuzzy-fingerprinting. Note that at similarities above 0 . 8 we achieve a reasonable recall performance. Figure 3: Runtime performance of the chunk indexes measured in the number of retrieval results to be post-processed.
