 Malcolm Strens mjstrens@QinetiQ.com Many tasks in machine learning can be converted into a sampling problem with a target density  X  ( w ) of the form: Typically w is a real-valued vector, for example the weights of a neural network or parameters in a model. The exponent f ( w ) takes on various meanings depend-ing on the task; it is variously termed the error func-tion , fitness function , objective function , expected re-turn , energy or log likelihood depending on the setting. Nevertheless, it is often formed from a sum over a set of instances (weighted by some constant K ): This is the special additive structure exploited by our algorithm. For convenience we will also define p ( w )  X  exp( f ( w )) for the unnormalized target density. Policy search is the process of optimizing a control pol-icy using repeated simulation runs ( X  X rials X ). Let the policy be defined by a set of parameters w ; for example the gains in a conventional control law, or the weights of a neural network controller. The outcome of each trial depends on both the state in which the simulation is started, and stochasticity in the simulator itself. A single scalar performance measure called the return is assumed to be obtained at the end of each trial. For any given policy, many trials may be required to ob-tain an accurate estimate of the expected return (the objective for optimization).
 A scenario is a particular choice for the initial state simulation (Ng &amp; Jordan, 2000). Given the choice of w and a scenario (indexed by i ), a simulation trial is deterministic (repeatable) and yields a return f i ( w ). A deterministic objective function for policy search can be formed by averaging the empirical returns for a given policy over a large set of scenarios of size N . Hence f ( w ) may take the form of equation (1) with K = 1 /N .
 Using this estimate for policy evaluation, we can query p ( w )  X  exp( f ( w )) at arbitrary policies w , and a sam-pling method such as Markov Chain Monte Carlo (MCMC) can be applied to explore this density. To achieve optimization rather than sampling, the shape of the target density can be gradually  X  X harpened X  during MCMC sampling. This simulated annealing process (Kirkpatrick et al., 1983), subject to strict con-ditions, causes the sampling chain to become trapped at w  X  , a global maximum of p ( w ). The sharpening is achieved by setting K = 1 / ( NT ) where the temper-ature T can be gradually reduced to zero during the sampling process.
 Unfortunately, the number of scenarios N represents a very difficult trade-off between accurate policy eval-uation and the number of MCMC steps that can be achieved in a given amount of computation time. We will propose a new algorithm that examines only O (log N ) scenarios to achieve a similar policy improve-ment as would be achieved using the full N scenarios in conventional MCMC. It does this by making use of sequences of steps using inaccurate policy evaluations (few scenarios) to obtain proposed moves (changes to w ) that are then accepted or rejected using more ac-curate policy evaluations (more scenarios). Markov Chain Monte Carlo is a sequential form of im-portance sampling. It provides a simple and practical way to generate samples from a probability density  X  ( w ) that is not available in analytical form, but can be queried for any policy-state 2 w (up to some un-known normalizing constant). The technique works by noting that we can construct a Markov chain that has  X  ( w ) as its invariant distribution, and simulating this chain to obtain a sequence of samples. Consecu-tive samples from the chain are highly correlated, but their distribution converges to  X  ( w ). 2.1. Metropolis Hastings Rule Suppose the current policy-state is w t at time step t , and that the unnormalized target density p (  X  ) can be queried. A proposal w 0 is generated according to some proposal density P ( w 0 | w t ), and p ( w 0 ) is evalu-ated. The Metropolis Hastings rule (MHR) (Hastings, 1970) can then be applied to stochastically accept or reject the proposal as the new policy-state of the chain: where u is drawn uniformly in the range [0 , 1]. 2.2. MCMC Algorithm Notation is now introduced to work with subsets of the complete set of N scenarios. The objective function for subset j (of size n ) is defined as: where j  X  N/n . For a simple MCMC implementation we are concerned only with the special case f N, 1 ( w ) which is identical to f ( w ); the more general form will be needed later for defining a hierarchy. Input : w in Current policy-state Input : n Number of scenarios forming the sum Input : j Counter Output : w out Proposed new policy-state w 0  X  w in + N (0 ,  X  2 ) if accept(  X f ) then else end A single MCMC step is defined by algorithm 1. Line 1 perturbs the input policy-state ( w in ) by Gaussian noise (s.d.  X  ) to obtain a proposal w 0 . This specific choice of the proposal density is symmetrical, and so the inequality used in the MHR reduces to the simple Metropolis rule (Metropolis et al., 1953): Substituting the quantity of interest, we have: Writing  X f  X  f n,j ( w 0 )  X  f n,j ( w ), we implement the Metropolis rule by defining the stochastic function accept (  X f ) to draw a new value of u uniformly from the range [0 , 1], then test the condition u &lt; exp(  X f ) (returning true or false accordingly). mcmc step can be called repeatedly with n = N and j = 1 to obtain an unbiased chain of samples from the target density: w out from one call is used as w in for the next. Note that f n,j (  X  ) is assumed to be available from a procedure call. Each step in this chain requires N trials. Our hierarchical algorithm will achieve O ( N ) such primitive (  X  -sized) steps using only O( N log N ) trials. (The complexity analysis assumes some non-zero lower bound on achieved acceptance rates.) 2.3. Conditions for Unbiased Sampling The proposal density and acceptance rule together determine the transition function P T ( w 0 | w ) of the Markov chain. The acceptance rule ensures that  X  is a fixed point of the transition function: However this property is not sufficient to ensure that samples from the chain will converge to accurately ap-proximate  X  . We also require that every policy-state be reachable in a finite number of steps from every other policy-state (ergodicity) and there to be no con-straint on the time phase at which a policy-state can be reached (aperiodicity). Many proposal densities, in-cluding additive Gaussian noise, are adequate to meet these conditions. Asymmetric proposal densities can also be used if the full MHR is applied. Bounded do-mains can be accounted for by rejecting proposals that break the hard constraints. 2.4. Hierarchical formulation of MHR The Metropolis Hastings rule can be implemented by calling accept (  X f  X   X  ) where the additional  X  X astings correction X  term  X   X  log P ( w 0 | w )  X  log P ( w | w 0 ) ac-counts for asymmetry in the proposal density. In our hierarchical formulation, a lower level sampling chain will be the source of this proposal density. There may have been a series of steps in obtaining w 0 from w , and so  X  becomes a sum of (logged probability ratio) terms over the lower level steps. The detailed balance property of each lower-level application of the MHR ensures that these terms (denoted  X  X  in algorithm 2) are already known. At the lowest level, the simple Metropolis rule can be applied. Therefore unbiased (and aperiodic) sampling at all levels can be proved by depth-first induction on the hierarchy. Proving er-godicity requires additional (weak) assumptions on the nature of the target density at each level. 2.5. Parallel Tempering Evolutionary MCMC methods run multiple chains in parallel, and allow them to interact to obtain better samples (Liang &amp; Wong, 2001). Parallel tempering (Liu, 2001) is a special case in which the chains are operated at several different temperatures; the lowest temperature chain corresponds to the target density of interest while the high temperature chains aid ex-ploration. Our hierarchical sampler will also exploit this benefit by allowing temperature to differ between levels. We introduce an algorithm called hierarchical impor-tance with nested training samples (HINTS) that ex-ploits the additive form of the objective function. It is applicable whenever the individual terms f i ( w ) are faster to compute than the complete average f ( w ) and will be most useful for a large number N of terms, for example N &gt; 16. 3.1. Defining the Hierarchy A tree or hierarchy defines how the complete set of scenarios (size N ) is broken down into smaller subsets. Levels in the hierarchy are indexed by l with l = 0 for leaves and l = L for the root. Let m 0 be the size of the smallest subsets (at the leaves). Let the branching factors be m l ( l &gt; 1). The number of scenarios n l considered at level l is therefore: The branching factors must be chosen so that n L = N ; i.e. the complete set is considered at the root node. For example, figure 1 shows a HINTS tree with L = 2, m 0 = 1 and m 1 = m 2 = 12. Therefore N = n 2 = 144. Nodes in the figure are labelled with their associated scenario subsets.
 Associated with each node in the hierarchy is the cor-responding approximation for the objective function. For node j at level l this is given by f n MCMC notation, or f l j ( w ) for compactness. Thus the complete objective function f ( w )  X  f N, 1 ( w )  X  f L 1 is associated with the root node, whereas averages of only m 0 terms are found at each leaf node. Also each where child( l, j, k )  X  m l ( j  X  1) + k is the index of the k th child. 3.2. Illustration of HINTS Operation HINTS uses the observation that average returns for subsets of scenarios can provide useful information about the full objective f ( w ) (which depends on all N scenarios). In particular, a sequence of moves is taken at some level in the hierarchy to propose a move at a higher level. This allows many small steps to be combined before a costly policy evaluation with N sce-narios takes place at the root. Before formalizing this process we give an illustrative example. Consider a simple task that involves aiming (of a bow and arrow, for example) to hit a target. Suppose this aiming  X  X olicy X  is specified as a 2-element vector w defining a vertical and horizontal displacement from some origin. Let the true optimal policy be w  X  , but as-sume that the intended aim is corrupted by noise ( e.g. wind turbulence). Specifying a set of scenarios means obtaining a sample set {  X  i } of size N from the noise distribution. The miss distance for policy w in sce-nario i is therefore  X  i  X || w +  X  i  X  w  X  || . In this illustra-tion one simulation  X  X rial X  corresponds to a single miss distance calculation, but in genuine applications each trial involves a costly simulation run. The optimiza-tion objective is to minimize the squared miss distance, which can be achieved by defining f i ( w ) = (  X   X  2 i ). We assume that N = 128 scenarios would be adequate to obtain a good approximation for expected return. A sampling hierarchy is defined with m 0 = 1 and m l = 2 for 1  X  l  X  7. Therefore we have a binary tree with N = n 7 = 128 leaf nodes. The hierarchy is operated at very low temperature to obtain optimization rather than sampling ( T l = (8  X  l ) / 50).
 Figure 2a marks the optimal policy w  X  with a small cir-cle. Gray-levels are used to show the probability den-sity exp( f 1 , 1 ( w )) corresponding to the first leaf node in the hierarchy. This density is obtained using only a single scenario and therefore exhibits a large bias (its mode is displaced significantly from the target cen-ter). Therefore a sample drawn from this density does not usually provide a good estimate for w  X  . Figures 2b and 2c show the probability densities exp( f 8 , 1 ( w )) and exp( f 128 , 1 ( w )) corresponding to N = 8 and N = 128 scenarios respectively. These are used at levels l = 3 and l = 7 (root node) respectively. By increasing N , the bias and variance are both reduced. Any sample at N = 128 is a good estimate for w  X  .
 Figure 2d shows a MCMC sampling chain for the den-sity shown in 2c. The initial policy-state w is shown by a cross and is at a displacement of (-4,-5) from w  X  . Solid and empty markers indicate MCMC proposals that are accepted and rejected, respectively. Each pro-posal is a displacement of 0.25 units in a random direc-tion, and is accepted or rejected using policy evalua-tions with all 128 scenarios. Note that the policy-state of the chain makes regular but slow progress towards ally, the chain would reach the small high-density area around w  X  and optimization would have been achieved. Figure 2e shows the sampling progress of the HINTS algorithm using a total budget of only 1024 trials. In this period the root node changes its value only once. The size of the circular markers is indicative of the level in the hierarchy (and hence the number of sce-narios used for each policy evaluation). Most moves are made with small numbers of scenarios; the full set of scenarios is only considered at the start and finish locations. Although there is an occasional low-level move in the wrong direction, these are always rejected at some level in the hierarchy.
 Inspecting the policy-states visited at level 4 in the hierarchy (figure 2f), note that most proposals are in an appropriate direction because they have been gen-erated from sequences of accepted moves at the lower levels. (Only one such proposal is rejected.) Figure 2g shows the progress made at the root node in the hier-archy after two such periods (2048 trials). For output purposes, the policy-state at the root of the tree is the only one of interest; the lower levels are merely pro-viding proposals. Much more progress has been made towards the optimization objective compared with the Our aim is to generate a sequence of samples from the unnormalized target density p ( w )  X  exp( f L 1 ( w )), associated with the root node. The HINTS algorithm has a simple recursive description. To make a move at node j of level l , a sequence of m l lower-level moves are first combined to form a proposal. The proposal is then accepted or rejected using the MHR with target density exp( f l j ( w )). Therefore execution proceeds in a depth-first left-to-right ordering.
 For example, in figure 1 the execution order is indi-cated by arrows. 12 steps are taken at leaf nodes (using one scenario each) to form a proposal. This proposal is accepted/rejected by policy evaluation using 12 sce-narios in the middle layer of the tree. This process is repeated 12 times to form a proposal for the root node of the tree, where it is then accepted/rejected using all 144 scenarios. In this process, the policy-state at the root could have moved by the equivalent of up to 144 primitive steps.
 Algorithm 2 implements the main recursive pro-cedure ( w out ,  X  X  out )  X  hints move( w in , l, j ) and can be seen as a direct substitute for w out  X  Input : w in Current policy-state Input : l Level in tree (0 = leaf) Input : j Counter Output : w out Proposed new policy-state Output :  X  X  out Hastings correction if ( l = 0) then 2 w out  X  mcmc step( w in , m 0 , j ) else 5  X   X  0 6 for k = 1 : m l do 7 ( w 0 ,  X  X  )  X  hints move( w 0 , l  X  1 , child( l, j, k )) 8  X   X   X  +  X  X  end
Algorithm 2: ( w out ,  X  X  out )  X  hints move( w in , l, j ) mcmc step( w in , n l , j ). It returns a new policy-state w out which may be the same as or different to the in-put policy-state w in , according to whether lower level proposals have been accepted.
 If a leaf node is encountered (line 1), a simple MCMC step is taken using m 0 scenarios for policy evaluation. Otherwise, a proposal is obtained using a sequence of m l moves at a lower level in the hierarchy. This is achieved by a sequence of recursive calls (line 7) that each update w 0 . If m 0 = 1, these recursive calls will terminate at exactly n leaf nodes, and so the proposal will be made up of n primitive  X  -sized moves com-bined. Line 10 accepts or reject the proposal according to the MHR. The  X  X astings correction X   X  accumulated (in line 8) from lower-level transitions is applied to ac-count for asymmetry in the proposal. See section 2.4 for a discussion of this quantity. 4.1. Efficient Implementation An efficient implementation re-uses evaluations of f ( w ) when the same values of i and w occur more than once. For example f l j ( w in ) on line 9 will nor-mally be known from the output of the preceding call when operating at the top level ( l = L ). Secondly, the lower level evaluation f l  X  1 (  X  ) ( w ) made in the recursive Finally, whenever there is a rejection, there is an op-portunity to re-use a previous evaluation. These effi-ciency savings can be achieved by passing lists of evalu-ation results into and out of the hints move procedure, or passing evaluations of f n,j ( w ) through a caching mechanism. The O ( n log n ) computational complex-ity is unchanged, but the constant of proportionality is improved. Here, we evaluate HINTS with a  X  X hip landing X  policy search problem that is simple to describe and difficult to solve. The problem is described in terms of the altitude (and descent rate) of an air vehicle that is re-quired to land on a ship X  X  deck. The height of the deck changes with time depending on sea turbulence, and is modelled as a damped harmonic oscillator. The ve-hicle has only one action: selecting between 2 different levels of vertical thrust. 5.1. Dynamics The vehicle X  X  physical state is ( y,  X  y ). At each time step the controller must select a thrust acceleration a  X  { X  0 . 5 , 0 . 5 } . The vehicle X  X  state is updated after each time step  X t by applying the midpoint method to its dynamics, defined by  X  y = a , subject to the limit  X  y &lt; 1 (maximum climb rate). The deck X  X  state is obtained by applying the midpoint method to its second order dynamics: where the turbulence acceleration  X  is drawn indepen-dently at each time step from a zero-mean Normal dis-tribution with standard deviation 0 . 25 / stant  X  determines the resonant frequency and was chosen to be  X  2 / 25. The constant  X  determines the amount of damping and was chosen to be 1 / 10. 5.2. Performance Measure and Returns When the vehicle hits the deck ( y  X  z ), the difference |  X  y  X   X  z | between their velocities determines success or failure. If the difference is less than 1, then the trial is deemed a success; otherwise it is a failure. The trial is also a failure if the maximum duration (400 s) is exceeded without the vehicle hitting the deck. The initial physical state is always ( y,  X  y ) = (100 ,  X  1) and ( z,  X  z ) = (0 , 0), and so the only source of random-ness in trial returns is the turbulence. Therefore each scenario i will be fully defined by a single random num-ber seed, yielding a deterministic return f i ( w ) for con-troller parameters w .
 Success rate will be the measure of performance, but this does not provide a useful objective function for learning because it does not indicate the degree of fail-ure and hence the direction of policy improvement. Therefore the return used in learning is given at trial end time ( t ) by: unless t &gt; 400 in which case f i ( w ) = 0. This assigns large returns for closely matching the velocity of the vehicle to the velocity of the deck, and for completing the trial as soon as possible. Return is always in the range [0 , 1]. 5.3. Controller Parameterization The optimization problem is expressed as a search through a parameterized family of controllers. The controller is a nonlinear mapping from the state to the control action. A scaled state vector x is formed from ( z , y/ 10,  X  z ,  X  y ). The first layer of our controller ap-plies a linear transformation to obtain the hidden unit activation vector x 0 : where A is a 2-by-4 matrix of weights and b is a 2-element bias vector. The action is then determined according to the test: Together ( A, b, c ) parameterize this nonlinear function. These can be concatenated into a weight vector w of size 11. 5.4. Example Figure 3 shows the heights of vehicle and deck during a successful trial (using the optimized controller). Fig-ure 4 shows the rate of height change (  X  y and  X  z ) over the same trial. Note that the vehicle initially acceler-ates to increase its descent rate, then decelerates to a low rate of descent. A period follows when the con-troller roughly matches the vehicle X  X  velocity to that of the deck. This ensures that the terminal constraint |  X  y  X   X  z | &lt; 1 is met even though the deck is rising very rapidly at the end of the trial. 5.5. Simulated Annealing Baseline A simple approach to this task is simulated anneal-ing (SA) with a fixed N . For small values of N we expect fast learning but convergence to a sub-optimal policy, because the policy is  X  X verfitted X  to the partic-ular N scenarios that are chosen. Conversely, for large N , learning will be slow but there will be less bias in the final result. Figure 5 shows the success rate for SA with N  X  { 4 , 16 , 64 } , evaluated using a test-set of 1000 unseen scenarios, and averaged over 80 runs. The temperature was reduced linearly from 0.05 to 0. (This initial temperature was obtained by trial-and-error.) The N = 4 case converges to a sub-optimal solution, whereas N = 16 appears to be adequate. Therefore SA with 16 scenarios will form a baseline against which to compare HINTS. 5.6. HINTS Performance HINTS is applied to this optimization task by setting the temperature of the root node to 0, but operating lower levels of the hierarchy at non-zero temperature to enable exploration. The temperature of level l was chosen to be ( L  X  l ) / (10 L ). Unlike simulated anneal-ing, there is no change in these temperatures during learning. Essentially, the root node is acting as an optimizer whereas the lower levels of the tree are in-creasingly random to enable exploration.
 Figure 6 shows the result of applying HINTS with bi-nary trees having either N = 256 or N = 4096 leaves ( m 0 = 1; m l = 2 ( l &gt; 0); n l = 2 l ). There is no limit on the size of N that could be used (subject to the total experiment duration). Error bars show standard devi-ations, not standard errors. HINTS with N = 256 is trials onward. Furthermore, this has been achieved with a much higher value of N , and so the final result will have less bias.
 For N = 4096, the root node first changes its state af-jumps to a good solution. The root node then changes trials is exhausted. There is no apparent improvement over N = 256, suggesting that no more than 256 sce-narios are required to eliminate most bias. HINTS is an algorithm for sampling from a target function where the log probability is a sum or ex-pectation. This additive structure is very common: HINTS is applicable to many noisy optimization and inference tasks in signal processing, data fusion and machine learning. The use of HINTS for Bayesian in-ference will be addressed in a separate paper. This paper has focussed on policy search, where the high computational costs of simulation trials mean that a sophisticated approach is required.
 The method was shown to be several times faster than simulated annealing (and has less terminal bias) in a policy search task. Much greater benefits would come in applications that have larger variation in returns between scenarios and hence require larger values of N . HINTS also has the advantage of being a statisti-cally stationary process that can provide output at any time; there is no need for a cooling schedule. Although the chosen task was fully observable, policy search is also effective in partially observable domains. It is also feasible to use the hierarchical sampling within population-based approaches such as particle filtering for regression (Vermaak et al., 2004) and evolutionary MCMC.
 Many simulation models have parameters that control the trade-off between accuracy and computation time. This could be exploited by a  X  X ariable fidelity X  opti-mizer that uses higher accuracy simulation at the root of the tree than at the leaves. This would allow larger numbers of scenarios to be used at leaf nodes. HINTS as described, does not make use of gradi-ent information that may be available. For exam-ple, the controller used in our evaluation was differ-entiable with respect to its weights. This property is exploited by policy gradient ascent methods (Sut-ton et al., 2000). The Hamiltonian MCMC algorithm (Neal, 1993) uses gradient information to determine proposal directions. HINTS could also exploit gradient information (calculated for individual training exam-ples) to obtain its primitive proposals. The resulting algorithm might have the efficiency of policy gradient methods, while offering global optimization.
 This research was funded by the UK Ministry of Defence Corporate Research Programme in Energy, Guidance &amp; Control. I acknowledge numerous helpful comments from Graham Watson, Drew Bagnell, Nick Everett, Simon Maskell and the reviewers.
 Andrieu, C., de Freitas, N., Doucet, A., &amp; Jordan,
M. (2003). An introduction to mcmc for machine learning.
 Hastings, W. K. (1970). Monte carlo sampling meth-ods using markov chains and their applications. Biometrika , 97 X 109.
 Kirkpatrick, S., Gelatt, C. D., &amp; Vecchi, M. P. (1983).
Optimization by simulated annealing. Science , 220, 4598 , 671 X 680.
 Liang, F., &amp; Wong, W. H. (2001). Real-parameter evo-lutionary Monte Carlo with applications to Bayesian mixture models. Journal of the American Statistical Association , 96 , 653.
 Liu, J. S. (2001). Monte carlo strategies in scientific computing . Springer Verlag.
 Metropolis, N., Rosenbluth, A. W., Rosenbluth, M. N.,
Teller, A. H., &amp; Teller, E. (1953). Equation of state calculations by fast computing machines. Journal of Chemical Physics , 21 , 1087 X 1092.
 Neal, R. M. (1993). Probabilistic inference using
Markov chain Monte Carlo methods (Technical Re-port CRG-TR-93-1). University of Toronto.
 Ng, A. Y., &amp; Jordan, M. (2000). PEGASUS:A policy search method for large MDPs and POMDPs. Pro-ceedings of the 16th Conference on Uncertainty in Artificial Intelligence (UAI-00) (pp. 406 X 415). San Francisco, CA: Morgan Kaufmann Publishers.
 Sutton, R., McAllester, D., Singh, S., &amp; Mansour, Y. (2000). Policy gradient methods for reinforcement learning with function approximation. Advances in
Neural Information processing Systems 12 (Proceed-ings of the 1999 Conference) (pp. 1057 X 1063). MIT Press.
 Vermaak, J., Godsill, S. J., &amp; Doucet, A. (2004). Se-quential bayesian kernel regression. Advances in
Neural Information Processing Systems 16 (Proceed-
