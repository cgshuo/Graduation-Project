 Statistical properties are commonly used to de-scribe entities, e.g. population for countries, net value for companies, points scored for ath-letes, etc. Claims about such properties are very common in news articles and social media, how-ever they can be erroneous, either due to au-thor error or negligence at the time of writing or because they eventually become out of date. While manual verification (also referred to as fact-checking) is conducted by journalists in news or-ganizations and dedicated websites such as www. emergent.info , the volume of the claims calls for automated approaches, which is one of the main objectives of computational journalism (Co-hen et al., 2011; Flew et al., 2012).

In this paper we develop a baseline approach to identify and verify simple claims about statistical
Figure 1: Claim identification and verification. properties against a database. The task is illus-trated in Figure 1. Given a sentence, we first iden-tify whether it contains a claim about a property we are interested in ( population in the exam-ple), which entity it is about and the value claimed (Lesotho and 2,000,000 respectively). We then proceed to verify the value claimed in text for the property of this entity against the value known in a knowledge base such as Freebase and return a score reflecting the accuracy of the claim (abso-lute percentage error in the example).

Claim identification is essentially an instance of information extraction. While it would be possi-ble to develop supervised models, this would re-quire expensive manual data annotation for each property of interest. Instead, we follow the dis-tant supervision paradigm (Craven and Kumlien, 1999; Mintz et al., 2009) using supervision ob-tained by combining triples from a knowledge base and raw text. However, statistical properties are more challenging in applying the distant super-vision assumption than relations between named entities due to the fact that the numerical values are often approximated in text, as in the example of Figure 1. Consequently, linking the values men-tioned in text with those in the knowledge base is not trivial and thus it is not straightforward to gen-erate training instances for the property of interest.
To address this issue, we propose a distantly supervised claim identification approach that re-lies on approximate instead of exact matching between values in text and the knowledge base. In experiments on 16 statistical properties about countries from Freebase we show that our ap-proach identifies simple statistical claims with 60% precision, while it is able to verify these claims without requiring any explicit supervision for this task. In developing our approach, we also evaluate it as a statistical property extrac-tor achieving 0.11 mean absolute percentage er-ror. The code and the datasets developed are pub-licly available from https://github.com/ uclmr/simpleNumericalFactChecker . Our approach to claim identification relies on dis-covering textual patterns between an entity and a numerical value used to express the property of in-terest. For example, the first, second and fourth patterns in Table 1 express the population prop-erty, and we would like our approach to select them to identify claims about this property.
During training, we assume as input a set of entity-value pairs from the knowledge base for the property of interest (top-right part of Table 1) and a set of textual patterns (bottom-left part), each associated with entity-value pairs (bottom-right part). The patterns and the entity-value pairs associated with them are obtained by processing raw text, which we discuss in Section 3.

The key difficulty compared to other applica-tions of distant supervision is that numerical val-ues are often approximated in text. Thus, instead of looking for patterns that report the exact val-ues for each entity, we develop an approach for finding the patterns that predict the values well. Intuitively, the algorithm ranks all text patterns ac-cording to how well they predict the values for the property at question and then greedily selects them till the accuracy of the aggregate predictions by the selected paterns stop improving. To compare the predicted entity-value pairs  X  EV against the prop-erty entity-value pairs EV prop we use the mean ab-solute percentage error (MAPE):
MAPE ( EV prop ,  X  EV ) = Note that only the values predicted for entities in both EV prop and  X  EV (denoted by E 0 ) are taken into account in this equation, thus in calculat-ing MAPE for the pattern  X  X  has inhabitants X  from Table 1 against the entity-values for popu-lation only the two values present are considered. MAPE is commonly used in measuring forecast-ing accuracy of algorithms in finance (Hyndman and Koehler, 2006). Unlike mean absolute error or mean squared error it adjusts the errors accord-ing to the magnitude of the correct value.

Initially (line 1) the algorithm decides on a de-fault value ( v def ) to return for the property at ques-tion among three options: the mean of the training values, their median or zero. The criterion for the choice is which one results in a better MAPE score on the training data. We refer to this prediction as the InformedGuess . This default value is used when predicting (lines 16-29) in case there are no values for an entity in the patterns selected, e.g. if only the pattern  X  X  has inhabitants X  is selected and the prediction for Iceland is requested.
Following this, the patterns are ranked accord-ing to the MAPE score of their entity values with respect to the entity values of the property at ques-tion (lines 2-4). We then iterate over the patterns in the order they were ranked. For every pattern, we add it to the set of patterns used in predicting (lines 8-9), and evaluate the resulting predictions using MAPE with respect to the training values (line 10). If MAPE is increased (predictions become worse), then we remove the newly added pattern from the set and stop. Otherwise, we continue with the next pattern in the queue.

In experiments with this algorithm we found that while it often identified useful patterns, some-times it was misled by patterns that had very few entities in common with the property and the values of those entities happen to be similar to those of the property. For example, the pattern  X  tourists visited X X  in Figure 1 has only one entity-value pair ( X  X rance:68,000,000 X ) and the value is very close to the population value for  X  X rance X . To ameliorate this issue, we adjusted the MAPE scores used in the ranking step (line 4) according to the number of values used in the calculation us-ing the following formula: where N is the number of values used in calcu-lating MAPE in Equation 1 and c is a parame-ter that regulates the adjustment. Lower c puts more importance on the number of values used, Algorithm 1: Claim identification algorithm Input : Entity-values for property
Output : Selected patterns P sel v def = InformedGuess ( EV prop ) priorityQueue Q =  X  foreach pattern p  X  P do 4 push ( Q , ( p , MAPE ( EV prop , EV p ))) mp = MAPE ( EV prop , predict( E, P sel ) ) while Q 6 =  X  do 8 pattern p = pop ( Q ) 10 mp 0 = MAPE ( EV prop , predict 11 if mp 0 &gt; mp then function predict (entities E , patterns 18 foreach e  X  E do 19 sum = 0 20 count = 0 21 foreach p  X  P sel do 22 if ( e, v )  X  EV p then 23 sum + = p { e } 24 count + = 1 25 if count &gt; 0 then thus leading the algorithm to choosing patterns as-sessed with more values, and thus more reliably. To evaluate the claim identification approach de-veloped we compiled a dataset of statistical prop-of all instances of the statistical region entity type with all their properties with their most recent val-ues, keeping only those were from 2010 onwards. From those we selected the 16 properties listed in Table 2, each property having values for 150-175 regions (mostly countries).

To collect texts from which the text patterns be-tween entities and numerical values will be ex-tracted we downloaded documents from the web. In particular, for each region combined with each property we formed a query consisting of the two and submitted it to Bing via its Search API. Fol-lowing this we obtained the top 50 results for each query, downloaded the HTML pages correspond-ing to each result and extracted their textual con-tent with BoilerPipe (Kohlsch  X  utter et al., 2010). We then processed the texts using the Stanford CoreNLP toolkit (Manning et al., 2014) and from each sentence we extracted textual patterns be-tween all the named entities recognized as loca-tions and all the numerical values. Two kinds of patterns were extracted for each location and nu-merical value: surface patterns (as the ones shown in Table 1) and lexicalized dependency paths.
This pattern extraction process resulted in a large set of triples consisting of a region, a pattern and a value. Different sentences might result in triples containing the same region and textual pat-tern but different value. Such variation can arise due to either the approximations of values in text or due to the pattern being highly ambiguous, e.g.  X  X  is  X . We distinguish between the two by re-quiring each region-pattern combination to have appeared at least twice and its values to have stan-dard deviation less than 0.1. In this case, then the region-pattern value is set to the mean of the val-ues it is encountered with, otherwise is removed. We first evaluate our approach as a statistical prop-erty extractor for two reasons. First, while our main goal is to develop a claim identification ap-proach, there is no data for this task to evaluate, thus making development difficult. On the other hand, we can evaluate statistical property extrac-tion in a straightforward way, thus facilitating de-velopment and parameter tuning. Second, the al-gorithm described learns such an extractor, thus it is of interest to know its performance.

We split the values collected from Freebase into 2/3 for training and 1/3 for testing, ensuring that all regions are present in both datasets. The ac-curacy is evaluated using MAPE. When using ad-justed MAPE we set the parameter c for each prop-erty using 4-fold cross-validation.

The performance of Algorithm 1 using the unadjusted MAPE was 0.72 averaged over all properties. Using the adjusted version this was greatly improved to 0.49. We also evaluated the InformedGuess prediction which returns the same value for all regions (it chooses the value that performs best among the mean, the median and 0), and its overall MAPE was 0.79. Recalling that Algorithm 1 returns the InformedGuess in case no pattern is found for an entity, we also evaluate the performance without returning a value for such entities, thus ignoring them in the evalua-tion. In that case the performance with unadjusted MAPE improves to 0.17 but 10% coverage, while with adjusted MAPE it improves to 0.11 with 43% coverage. Best performances were achieved for relations such as population which have a wide range of values that is well separated from the rest, while percentage rates were usually harder for the opposite reason. Thus we conclude that the algo-rithm with adjusted MAPE selects better patterns for each property that are encountered more fre-quently, which is important for the main goal of this paper, claim identification. We now evaluate our approach on claim identifica-tion. For each property, we run Algorithm 1 using adjusted MAPE and the parameter c as chosen in the experiments of the previous section to select gdp nominal per capita 415 0.20 patterns expressing it. We then process all texts and if a sentence contains one of the selected pat-terns between an entity and a value, it is returned for manual inspection as shown in Figure 1.
The claims returned were labeled by the authors of the paper as correctly or incorrectly identified according to the following guidelines. A claim is extracted correctly only if both the entity and the value are extracted correctly and the sentence ex-presses the property at question. E.g. a claim iden-tified in a sentence containing a country and its correct GDP growth rate without stating it as such (the same percentage rate can be true for multiple statistical properties) is considered incorrect. Fur-thermore, we considered claims referring to past measurements (e.g. results of a past census) to be correctly identified.
 Results for each property are shown in Table. 2. Overall precision was 60% over 7,092 statements, and it varied substantially across properties. Per-fect precision was found for claims of renew-able freshwater for which one textual pattern was responsible for all the claims identified and it was correct. On the other hand, the zero precision for claims of internet user % was due to identifying correctly sentences listing countries and their re-spective values for this property but not identify-ing the country-value pairs correctly. More repre-sentative of properties with precise claim identifi-cation was population , for which the relatively few errors were due to the patterns learned not being able to distinguish between different types of pop-ulation e.g. general vs working population. On the other hand, the claims for gni per capita had low accuracy because they were confused with those of gdp nominal per capita , as their values tend to be relatively close. The claims identified and an-notated manually are attached to our submission. Finally, some errors are due to the algorithm being constrained to extract a claim considering only the text pattern between the entity and the value, thus ignoring parts of the sentence that might be rele-vant. For example, the pattern  X  X he population of X is  X  is generally reliable, but in the sentence  X  X he population of Tajikistan is 90 % Muslim X  it extracts a claim incorrectly.

The verification stage of the simple claims we extract is rather simple; we just score the claims according to the absolute percentage error of the value claimed in text with respect to the value in known in Freebase. In the process of la-beling the claims identified we did a qualita-tive analysis of the claims with high error. We found cases where our algorithm flagged cases of out of date estimates of populations used, e.g. the webpage http://www.economywatch. the population of Bolivia is 9 million, while it is estimated to be above 10 million. As explained, we tackle claim identification as an instance of information extraction, and propose a baseline able to perform both tasks. However, it is important to distinguish between them. In claim identification we are interested in all claims about a property, even inaccurate ones; in information extraction on the other hand, and especially its for-mulation as knowledge base population, we are in-terested in the accurate claims only, since extract-ing inaccurate ones will lead to erroneous informa-tion added to the knowledge base. The difference between the two tasks is captured by the verifica-tion task. In this paper our main goals are identifi-cation and verification, but we train our approach on information extraction, relying on the assump-tion that most claims made in the texts retrieved via the web search engine are accurate.

In related work, Nakashole and Mitchell (2014) developed an approach to verify subject-verb-object triples against a knowledge base, taking into account the objectivity of the language used in the sources stating the triple. Our approach is ag-nostic to the syntactic form of the claims, thus it can identify claims expressed in greater linguis-tic variety. Ciampaglia et al. (2015) fact-checked subject-predicate-object triples against a knowl-edge graph constructed from DBpedia, but they considered only the paths between the subject and the predicate in their algorithm thus ignoring the predicate itself. Dong et al. (2015) established the trustworthiness of a web source by comparing the subject-predicate-object triples extracted from it to the Knowledge Vault built by Google, but did not focus on claim identification and verification. Adar et al. (2009) developed an approach to detect inconsistencies between versions of Wikipedia in different languages, but they focused on manually extracted infoboxes. Finally, Vlachos and Riedel (2014) compiled a dataset of claims fact-checked by journalists, but the claims are much more com-plex than the ones we considered in this paper.
Other work that discussed the extraction of statistical properties includes the approaches of Hoffmann et al. (2010) and Intxaurrondo et al. (2015), both employing approximate matching to deal with the approximation of numerical values in text. In order to learn their model, Hoffmann et al. (2010) take advantage of the structure of the articles in Wikipedia developing a classifier that identifies the schema followed by each arti-cle, which is not straightforward to extend to texts beyond this source. Intxaurrondo et al. (2015) on the other hand focus on tweets and make the as-sumption that the entity discussed in each tweet is determined in advance, thus the extractor needs only to associate a numerical value with the prop-erty of interest, i.e. the task is reduced from triple extraction to labeling values. In this paper we developed a distantly supervised approach for identification and verification of sim-ple statistical claims. We evaluated both as statis-tical property extractor and as a claim identifier on 16 relations from Freebase. In future work we aim to improve our approach by taking into account continuous representations of the words in the pat-terns and to extend it to more complex claims, e.g. claims about change in financial indicators.
