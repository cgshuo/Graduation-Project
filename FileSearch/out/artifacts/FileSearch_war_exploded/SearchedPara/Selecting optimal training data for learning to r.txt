 1. Introduction
Learning to rank has been gaining more and more attention recently in the research communities of information retrieval and machine learning. So far most research efforts have been made on the development of new algorithms ( Burges et al., always true, especially when considering the following facts.
 short and ambiguous, while topical categories in classification can be more rigorously defined. Besides, in ranking human tators might not agree with each other on the label of the same query-document pair ( Bailey et al., 2008; Harman, 1996;
Voorhees, 2000 ). Since human annotation is costly, one can usually not afford to have several annotators to make multiple judgments. As a result, the judged data may be biased and is unreliable.
 Second, due to the high cost of labeling, it is in general impossible to label all the documents associated with a query.
Usually only a proportion of the documents are labeled and the rest are regarded irrelevant ( Lee et al., 2002; Zobel, 1998 ). However, it is highly possible that some unlabeled documents are relevant. For example, the pooling strategy at  X  the Million Query Track of TREC 2007 and 2008 is aimed at comparing different participating systems ( Allan et al., 2008, systems may not be labeled. If we construct training data from that data source, then the irrelevant labels might be unreliable.
 might hurt the accuracy of the learning algorithms. For example, Ranking SVMs and other SVMs based ranking methods are proach is to select and utilize a subset of the original training data, which is of high-quality. This is exactly the motivation of this paper.

As far as we know, there was little work on training data selection for ranking in the literature. In contrast, there have been many attempts ( Brodley and Friedl, 1999; Tomek, 1976; Valizadegan and Tan, 2007; Wilson, 1972 ) on the same issue for classification. While one may argue that the extension of the existing methods can solve the problem for ranking, we point out that such an extension would not work well, mainly due to the significant differences between ranking and classification.

To tackle the challenge, we propose designing a specific training data selection technique for ranking. In particular, we
Basically the quantity measures whether the document pairs associated with each query have consistent directions in the see that the quantity considers both the ordinal relationship between documents of a query and the hierarchical structure on queries and documents in the training data, which are both unique properties of ranking.

Given the concept of PPC, we formulate the task of training data selection as an optimization problem. The objective is to maximize the PPC of the selected subset of training documents, and at the same time maximizing the size of the subset. Di-stage, we try to select a subset of document pairs that can maximize PPC. Although this is still a 0 X 1 integer programming problem, we show that the problem can be efficiently solved by projecting the variables to the linear space spanned by the eigenvectors of the similarity matrix between document pairs, and maximizing a lower bound of the original objective. At we solve an optimization problem, whose objective is to minimize the difference between the document pairs selected at the first stage and those given by document selection. Again, although the problem is a 0 X 1 integer programming problem, we can convert it to an eigen-decomposition problem by performing an equivalent transformation and relaxing the integer con-cient manner.

We have tested the effectiveness of our proposed method on both LETOR benchmark datasets ( Liu et al., 2007 ) and a large-scale dataset from a commercial web search engine. The experimental results have shown that our approach can sig-nificantly improve the test performance of the ranking models learned from the selected training data. We have also eval-uated the training documents discarded by our method and found that those are really low-quality training data. The result indicates that our method indeed has the ability of optimizing the quality of training data.
 To sum up, the contributions of this work lie in the following aspects.
 We have pointed out the difference between training data selection for ranking and that for classification. We have proposed using PPC to measure the quality of a training data collection for ranking.

We have proposed selecting high-quality training data for ranking by maximizing the PPC of the selected subset, and con-verting the problem into an optimization problem with two stages. We have proposed efficient solutions for both stages.
We have empirically verified that by selecting and utilizing an optimal subset of training data, the performance of learn-ing to rank algorithms can be significantly improved. 2. Previous work
Ranking in search is a task as follows. Given a query, the documents related to the query are retrieved from the document repository and sorted according to their relevance to the query using a ranking model. The top ranked documents in the sorted list are presented to users as the answer to the query. Learning to rank is an approach to automatically construct learning to rank, several major approaches have been proposed. These include the pointwise approach ( Li et al., 2007; ( Cao et al., 2007 ).

As far as we know, most previous work on learning to rank focused on the development of novel learning algorithms, and alho et al. (2008) propose using a sigmoid loss function to replace the hinge loss in RankSVM in order to resist noises. ument pairs which are mistakenly ranked and therefore reduces the effectiveness of the learning process. In this regard, a more general solution for improving training data quality is needed.
 disagree with those of its neighbors, and propose not selecting such instances. Some algorithms are based on ensemble label of an instance, then the instance should be discarded. A problem with the above algorithms is that they determine whether an instance should be selected or not independently, and therefore the removal of a low-quality instance will optimum can hardly be achieved.
 One may argue that we can solve the problem of training data selection for ranking by extending the above algorithms. ranking cares about the ordinal relationship between documents while classification focuses on prediction of the labels of the data type is simpler.

In this paper, we will propose a novel approach, which considers the unique properties of ranking and therefore can do a better job than the above algorithms in the scenario of learning to rank. 3. Pairwise preference consistency cally a reasonable measure of data quality in ranking should have the following properties. (1) It should follow some sound assumptions used by the quality measures in classification. For example, it is assumed (2) It should reflect some unique properties of ranking. First, it should take into account the ordinal relations between
There might be multiple measures that can satisfy the above requirements. In this section, we propose a measure called pairwise preference consistency (PPC) as an example. Its definition is given as below, where S is the training data collection; Q is the query set in S ; X and the i th document is more relevant than the j th document; r associated with query q ; it is defined as r q i ; j  X  r q the pair; Sim (.) is a similarity function, a simple yet effective example of it is the inner product. is in accordance with the assumptions in previous work. Second, the measure is defined with respect to document pairs, and therefore can emphasize the ordinal relationship between documents. Third, PPC can be decomposed into two parts, the consistency within each query, and the consistency between different queries, which corresponds to the two terms in the right-hand side of Eq. (1) . For ease of reference, we call these two terms Within Query Consistency (WQC) and Cross Query
Consistency (CQC) respectively. Note that the CQC measures the similarity between any two documents pairs in any two que-ries. This helps us to identify queries which are consistent within itself, while are quite different from other ones. that PPC is a reasonable measure of data quality in ranking. 4. Data selection for ranking We formulate the problem of training data selection as maximizing the PPC of the selected subset of documents.
We use variable a q i to indicate whether the i th document associated with query q is selected or not (i.e., if a document is selected, and otherwise not selected). Only when both documents associated with a query are selected, can the the inner product as the similarity function for ease of discussion):
Then the task is to find optimal indicator variables a q i unique properties of ranking can be considered. Second, it is possible that we can achieve the globally optimal selection, that the majority of the training data is of high quality, and detects noisy documents according to whether the document pairs might be quite different from other ones. In such cases, removing this document will increase the PPC value.
A straightforward solution to the above problem, however, would be very hard, mainly because it is an integer program-reduce the computational complexity and get the original optimization problem efficiently solved. 4.1. Selecting document pairs
In this subsection, we describe how to select the optimal subset of document pairs that has the maximum PPC. 4.1.1. Problem formulation
Given a training data collection S , we use variable x q i ; j can be written as below, where x is a vector of dimension p ( p  X  P q 2 Q N q is the total number of document pairs); its element is x R is a f p matrix ( f is the number of features) with each column representing 1 ries sequentially for the construction of x and R .
 potentially hurt the generalization ability of learning algorithms. Considering this, we propose maximizing the number of selected document pairs at the same time. As a result, we obtain the following optimization problem, where parameter l is a tradeoff coefficient.
 lowing aspects. (i) This is a 0 X 1 integer programming problem, which is known to be NP-hard. To tackle the challenge, we propose an efficient solution, as shown below.
 4.1.2. Efficient solution
We propose the following strategies to reduce the complexity of solving Problem (4) , (i) Conducting eigen-decomposition on R T R . (ii) Projecting variable x to the linear space spanned by the eigenvectors, so as to transform the original problem to its (iii) Solving the equivalent problem by maximizing a lower bound of its objective.
 First we conduct eigen-decomposition on R T R . Since the dimension of R rectly performing the decomposition is almost infeasible. Alternatively, we conduct eigen-decomposition on RR efficient manner. The following proposition shows the relationship between the eigenvectors of R proof of the proposition can be found in Appendix A.
 Proposition 1. Suppose the eigenvalues and their corresponding eigenvectors of RR eigenvalues of R T R are r 2 1 , ... ; r 2 f , and 0, ... ,0. Suppose the eigenvectors of R
Second, we can obtain the following theorem, which gives an equivalent form of Problem (4) . The proof of the theorem can be found in Appendix A.

Theorem 1. Problem ( 4 ) is equivalent to the following problem, Third, according to the Cauchy-Schwarz inequality, we have
On this basis, we choose to optimize the following lower bound of the objective function in Problem (6) ,
Furthermore, Problem (8) is equivalent to Problem (9) ,
Actually such a bound optimization technique has been widely used in the literature to tackle intractable optimization problems ( Rustagi, 1994 ). By using it, we can significantly reduce the computational complexity. It takes O (2 solve Problem (4) , while O ( pf 2 + f 3 ) for eigen decomposition and O ( pf ) to solve Problem (8) . 4.2. Selecting documents
After obtaining the optimal selection of document pairs, we perform document selection by regarding the document pair selection as a given condition. 4.2.1. Problem formulation the following optimization problem, where n q denotes the total number of documents associated with query q ; a q  X  X  a and P q i ; j  X  0 if the pair is not selected. P q i ; j  X  1 not considered in the training process.
Note that we can optimize Problem (10) for each query separately, since different queries do not have interactions in the is known to be NP-hard. A common way to solve such a problem is to relax the 0 X 1 constraints. However, the relaxed prob-equivalent problem, and then relax the constraints afterwards. In this way, the global optimum of the relaxed problem can be easily obtained. 4.2.2. Efficient solution
For ease of discussion, we omit the superscript q when there is no confusion. Basically we have the following theorem, which gives an equivalent transformation of Problem (10) . The proof of the theorem can be found in Appendix A.
Theorem 2. Problem ( 10 ) is equivalent to Problem ( 11 ) with a where T  X  ZZe e T Z 0 ; Z  X  E 2 P, and E is an n n matrix with each element equal to 1.

Further relaxing the integer constraints to b 2 [ 1,1] n +1 vector corresponding to the smallest eigenvalue of matrix T ( Pothen et al., 1990 ). smallest eigenvalue of T , denoted as b  X  , we can get the solution of Problem (10) as a , we select the i th document if a i P 0 : 5.

One may be concerned with the accuracy of the relaxation we have used above. Actually, in Problem (11) , we try to max-imize b T b at the same time as minimizing b T T b . Maximizing b accuracy can be high.
 In our approach, we only need to find the smallest eigenvalue of matrix T , and then find the corresponding eigenvector.
The computational complexity is O ( n 2 ). This is much more efficient than O (2 solution. 5. Experiments 5.1. Experimental settings 5.1.1. Datasets
We used the TD2003 and TD2004 datasets in LETOR ( Liu et al., 2007 ) and a real dataset obtained from a commercial Web search engine (denoted as REAL) for our experiments.
 TD2003 and TD2004 contain 50 and 75 queries respectively. For each query, there are about 1000 associated documents. sets. Therefore, we regard the validation and test sets as having high-quality, and regard the training set as having low-quality. We repeated the addition of noise to the training set for ten times at each noise level, and report the average performance of the learned ranking models on the test set.
 fully labeled and confirmed by multiple annotators; in contrast, the training set is labeled by only one annotator. We 5.1.2. Parameter tuning
We used Ranking SVMs ( Joachims, 2002 ) as the learning algorithm. We trained Ranking SVMs with different values of the parameter l . We tried l 2 {1 e 5, 5 e 5,8 e 5,1 e 4,5 e 4, 8 e 4, 0.001, 0.005, 0.008, 0.01, 0.05, 0.08}. Again, the validation set was used to find the best value of the parameter. 5.1.3. Evaluation measure We used NDCG ( J X rvelin and Kek X l X inen, 2002 ) as an evaluation measure. NDCG@ k is defined as below, evant and r ( j ) = 0 otherwise), and Z k is chosen so that NDCG at each position of the perfect list equals one. 5.1.4. Baselines We denote our proposed approach as OptPPC. We compared OptPPC with the following baselines, Origin : This approach uses the original training set without performing data selection.

Weight KNN(WKnn) : In this approach, the label of a document is compared to the prediction given by its k nearest neigh-( Valizadegan and Tan, 2007 ). We implemented two versions of WKnn, WKnnQ and WKnnC. In WKnnQ, only documents associated with the same query are considered neighbors, while in WKnnC the neighbors are defined on the entire col-lection. Another extension of WKnn is to view a document pair as an instance, and use the idea of WKnn to select pairs, this can be used for pairwise algorithms. We denote this approach as PKnn. We also implemented two versions of PKnn, PKnnQ and PKnnC.

Majority SVM(MSVM) : In this approach, we first partition the training set into 10-folds and use 9-folds to train an SVM mented two versions of MSVM, MSVMQ and MSVMC. In MSVMQ we build the classifiers for each query using only the documents associated with it; while in MSVMC we build the classifiers using documents in the entire collection. Similarly to WKnn, we also extended MSVM to its pairwise version, and implemented two versions, PSVMQ and PSVMC. 5.2. Experimental results The experimental results on TD2003 and TD2004 are listed in Table C.1 , and those on REAL are presented in Table C.2 . From these results we have the following observations.

First, OptPPC significantly outperforms Origin. For example, OptPPC beats Origin on TD2003 by 9 points at NDCG@10, when the noise level is 0.3. OptPPC also outperforms Origin on REAL and the improvement is statistically significant ( p -value = 3.7e 5, 5.3e 8 at NDCG@5 and NDCG@10 respectively). We hypothesize that the improvements mainly come from the increase of PPC. Take the first fold in TD2003 as an example. We computed the PPC at the noise level of 0.05 (see Table C.3 ), and found that the PPC (also WQC and CQC) of the selected subset of documents is much larger than that of the entire dataset. We further conducted some experiments on random sampling, to verify the advantage of our proposed optimization based approach. Again, taking the first fold in TD2003 and the noise level of 0.05 as an example, we randomly selected a subset of training data with the same number of documents as selected by OptPPC, and used it for training. We repeated the above steps for ten times; and find that the average test performance of the learned ranking models underper-forms the model trained with all the documents by 1.3 point at NDCG@10. This shows that randomly reducing the size of the instances using our proposed approach.

Second, both WKnnC and WKnnQ perform much worse than Origin on TD2003 and TD2004. The performance decrease is larger than 10 NDCG points in many situations. WKnnQ performs similar to Origin on REAL (We did not report WKnnC on
REAL, because the time complexity of computing the similarity matrix on REAL is too high). The poor performances may be due to two reasons. First, these methods have not considered the ordinal relationship between documents, which is, how-ever, the key in ranking. As a result, it is not guaranteed that we can get a large WQC. Second, and more importantly, we found that for most documents, the majority of their nearest neighbors come from the same query as they are associated that it is associated with. In one aspect, this explains why WKnnQ and WKnnC perform similarly. In another aspect, since queries can hardly be considered. This can be verified by Table C.3: after data selection using WKnnQ and WKnnC, WQC drops by about 80%, and CQC decreases by more than 95%. It is obvious that one will encounter challenges when learning the ranking model using such inconsistent training data, and it is not a surprise to observe the performance drop.
Third, the performance of MSVMQ is also much worse than that of Origin. The explanation can be very similar to that regarding WKnnQ: (1) the classifiers do not consider the ordinal relationship between documents and thus will lead to poor WQC; (2) the classifiers are learned using documents associated with the same query, and therefore will lead to poor CQC. The explanation can be verified by Table C.3 . Furthermore, as one may notice, we have not reported the performance of space, which makes the learning process unconverged. For example, we plot the document distribution of query 9 and query 25 in TD2003 (each document is placed in a 2-dimensional space using PCA) in Fig. C.2 . The red triangles, and green squares denote relevant documents of query 9, relevant documents of query 25, irrelevant documents guish them when we put multiple queries together. This observation is somehow reasonable: relevant documents of a rare query might not be as  X  X  X elevant X  X  as relevant documents of a popular query, and sometimes they may even look like irrele-vant documents of a popular query. This is also in accordance with some previous studies ( Geng et al., 2008 ).
Fourth, one may notice that we did not report PKnnQ and PKnnC. That is because that the time complexity of such an extension is too high. For example, in TD2003, there are about 1000 documents per query. In the worst case, there are 250,000 document pairs per query. Therefore, one needs to compute a similarity matrix of size 250,000 250,000 in PKnnQ, and a similarity matrix of size (250,000 30) (250,000 30) in PKnnC. This is clearly infeasible.

Fifth, we observe that PSVMC and PSVMQ can slightly outperform Origin. However, the improvement is not significant, learning the classifiers, PSVMQ only considers documents associated with the same query and the Cross Query Consistency cannot be guaranteed. For instance, in Table C.3 , the CQC of selected subset of PSVMQ is much smaller than that of
PSVMC. 5.3. Result analysis
To further understand our proposed approach and have a conclusion on what kind of documents are removed, we have performed some analysis on the removed instances in REAL.

Specifically, we randomly sampled 300 documents that are not selected by OptPPC, and asked three human annotators to  X  X  www.harcourt.com/about/news/2006.html  X  X  with respect to the query. The document talks about news for harcourt educa-whether the document should be judged as relevant or not. This is actually not a rare example; in many cases, the document end of the page. In this regard, it seems to be relevant to the query.

To conclude, our analysis results show that for most of the documents removed by our proposed approach, either their original labels are incorrect, or the human annotators cannot determine their labels. In other words, such documents will bring negative effects to the training process, and it is better not to include them in the training data. 6. Conclusion and future work proach on various datasets.

As future work, we plan to further investigate the following problems. (1) We will investigate other uses of the proposed concept of pairwise preference consistency. For example, we will check (2) We will study other measures of the quality of training data in ranking. This is because some learning to rank algo-Appendix A. Proof of Proposition 1 Proof. Suppose the singular value decomposition of R T is R Besides, since
Therefore, m 1 , ... , m f are the eigenvectors of RR T , u eigenvalues of R T R . That is, r 2 1 ; ... ; r 2 f are the eigenvalues of R Besides, since the rank of R T R is f , 0 is an eigenvalue of R Appendix B. Proof of Theorem 1
Proof. Since u 1 , ... , u p is a basis of R p (here R is the space of real numbers), for any x 2 R that x  X  P p i  X  1 t i u i . Then we have Since u j ( j =1, ... , p ) are eigenvectors of R T R , we get R R T Ru j  X  r 2 j u T i u j  X  0  X  i ; j  X  1 ; ... ; p ; i  X  j  X  . Therefore, Since R T Ru i =0, i = f +1, ... , p , we have
Since R T Ru i  X  r 2 i u i , we have u T i R T Ru i  X  r 2
Combining x  X  P i t i u i ; u T i u j  X  0  X  i ; j  X  1 ; ... ; p ; i  X  j  X  and k u
Now Problem (4) can be rewritten as: Appendix C. Proof of Theorem 2
Proof
Setting a  X  1 2  X  a 0  X  e  X  and Q = E 2 P , we get: Setting we get See Figs. C.1 and C.2 and Tables C.1, C.2, C.3 .
 References
