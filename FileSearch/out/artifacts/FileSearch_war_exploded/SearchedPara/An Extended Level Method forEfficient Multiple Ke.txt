 { zlxu, king, lyu } @cse.cuhk.edu.hk measure among data. A generic approach to learning a kernel f unction is known as multiple kernel bination of base kernel functions which maximizes a general ized performance measure. Previous kernel functions, and as a result to improve the performance .
 nel function, several measures have been studied for multip le kernel learning, including maximum The multiple kernel learning problem was first formulated as a semi-definite programming (SDP) problem by [5]. An SMO-like algorithm was proposed in [2] in o rder to solve medium-scale prob-objective function and updates the kernel weights by solvin g a corresponding linear programming problem. Although the SILP approach can be employed for larg e scale MKL problems, it often suf-fers from slow convergence. One shortcoming of the SILP meth od is that it updates kernel weights solely based on the cutting plane model. Given that a cutting plane model usually differs signif-icantly from the original objective function when the solut ion is far away from the points where the cutting plane model is constructed, the optimal solutio n to the cutting plane model could be significantly off target. In [10], the authors addressed the MKL problems by a simple Subgradient Descent (SD) method. However, since the SD method is memoryl ess, it does not utilize the gradi-search.
 To further improve the computational efficiency of MKL, we ex tended the level method [6], which objective function using the solutions to the intermediate SVM problems. A new solution for kernel results with eight UCI datasets show that the extended level method is able to greatly improve the efficiency of multiple kernel learning in comparison with th e SILP method and the SD method. experimental results by comparing both the effectiveness a nd the efficiency of the extended level method with the corresponding measures of SILP and SD. We con clude this work in section 5. Let X = ( x dimensional space. We further denote by y = ( y for the data points in X . We employ the maximum margin classification error, an objec tive used in SVM, as the generalized performance measure. Following [ 5], the problem of multiple kernel learning for classification in the primal form is defined as fo llows: where P = { p  X  R m : p  X  e = 1 , 0  X  p  X  1 } and Q = {  X   X  R n :  X   X  y = 0 , 0  X   X   X  C } respectively. Here, e is a vector of all ones, C is the trade-off parameter in SVM, { K a convex-concave problem. It is important to note that the bl ock-minimization formulation of MKL presented in [10, 2] is equivalent to (1).
 A straightforward approach toward solving the convex-conc ave problem in (1) is to transform it into a Semi-definite Programming (SDP) or a Quadratically Co nstrained Quadratic Programming (QCQP) [5, 2]. However, given their computational complexi ty, they cannot be applied to large-scale MKL problems. Recently, Semi-infinite Linear Program ming (SILP) [12] and Subgradient Descent (SD) [10] have been applied to handle large-scale MK L problems. We summarize them in the case of ambiguity.
 As indicated in Algorithm 1, both methods divide the MKL prob lem into two cycles: the inner cycle solves a standard SVM problem to update  X  , and the outer cycle updates the kernel weight vector Algorithm 1 A general framework for solving MKL 1: Initialize p 0 = e /m and i = 0 2: repeat 3: Solve the dual of SVM with kernel K = P m 4: Update kernel weights by p i +1 = arg min {  X  i ( p ;  X  ) : p  X  X } 5: Update i = i + 1 and calculate stopping criterion  X  i 6: until  X  i  X   X  p . They differ in the 4 th step in Algorithm 1: the SILP method updates p by solving a cutting plane model, while the SD method updates p using the subgradient of the current solution. More specifically,  X  i ( p ;  X  ) for SILP and SD are defined as follows: where  X   X  2 [(  X  i  X  y ) to p at ( p i ,  X  i ) . Comparing the two methods, we observe prevent the updated solution from being too far from the curr ent solution. to convex-concave problems and its application to MKL. 3.1 Introduction to the Level Method The level method [6] is from the family of bundle methods, whi ch have recently been employed to lower bound f serving a similar purpose to the regularization term in SD, p revents the new solution x i +1 from L compared with x 0 . 3.2 Extension of the Level Method to MKL We now extend the level method, which was originally designe d for optimizing non-smooth func-ing to van Neuman Lemma, for any optimal solution ( p  X  ,  X   X  ) we have This observation motivates us to design an MKL algorithm whi ch iteratively updates both the lower first construct the cutting plane model. Let { p j } i i iterations. Let  X  j = arg max We construct a cutting plane model g i ( p ) as follows: We have the following proposition for the cutting plane mode l g i ( x ) Proposition 1. For any p  X  X  , we have (a) g i +1 ( p )  X  g i ( p ) , and (b) g i ( p )  X  max two quantities f i and f i as follows: The following theorem shows that { f j } i bounds for f ( p  X  ,  X   X  ) .
 Theorem 1. We have the following properties for { f j } i (b) f 1  X  f 2  X  . . .  X  f i , and (c) f 1  X  f 2  X  . . .  X  f i .
 Proof. First, since g i ( p )  X  max Second, since f ( p j ,  X  j ) = max Combining the above results, we have (a) in the theorem. It is easy to verify (b) and (c). We furthermore define the gap  X  i as The following corollary indicates that the gap  X  i can be used to measure the sub-optimality for solution p i and  X  i .
 problem: later. We summarize the steps of the extended level method in Algorithm 2.
 Algorithm 2 The Level Method for Multiple Kernel Learning 1: Initialize p 0 = e /m and i = 0 2: repeat 3: Solve the dual problem of SVM with K = P m 4: Construct the cutting plane model g i ( p ) in (5) 5: Calculate the lower bound f i and the upper bound f i in (6), and the gap  X  i in (3.2) 6: Compute the projection of p i onto the level set L i by solving the optimization problem in (8) 7: Update i = i + 1 8: until  X  i  X   X  monotonically decreases through iterations. The followin g theorem shows the convergence rate of the level method when applied to multiple kernel learning.
 Theorem 3. To obtain a solution p that satisfies the stopping criterion, i.e., | max f ( p  X  ,  X   X  ) | X   X , the maximum number of iterations N that the level method requires is bounded operator  X  Due to space limitation, the proof of Theorem 3 can be found in the long version of this paper. details can be found in [8, 6]. We conduct experiments to evaluate the efficiency of the prop osed algorithm for MKL in constrast with SILP and SD, the two state-of-the-art algorithms for MK L. 4.1 Experimental Setup We follow the settings in [10] to construct the base kernel ma trices, i.e., Table 1: The performance comparison of three MKL algorithms . Here n and m denote the size of training samples and the number of kernels, respectively.
 Each base kernel matrix is normalized to unit trace. The expe riments are conducted on a PC with 3.2GHz CPU and 2GB memory. According to the above scheme of co nstructing base kernel matri-ces, we select a batch of UCI data sets, with the cardinality a nd dimension allowed by the memory remaining data are used for testing. The training data are no rmalized to have zero mean and unit max to  X  accelerates the projection when the solution is close to the optimal one. We use the SimpleMKL toolbox [10] to implement the SILP and SD methods. The linear programming in the SILP method and the auxiliary subproblems in the level method are solved using a general optimization toolbox MOSEK ( http://www.mosek.com ). The toolbox for the level method can be downloaded from http://www.cse.cuhk.edu.hk/  X zlxu/toolbox/level_mkl. html . 4.2 Experimental Results number of kernels selected. From Table 1, we observe that all algorithms achieve almost the same are essentially trying to solve the same optimization probl em. Regarding the computational effi-ciency, we observe that the time cost of the SILP approach is t he highest among all the three MKL algorithms. For datasets  X  X ono X  and  X  X onar X , the SILP metho d consumes more than 30 times the most efficient among three methods in comparison. To obtain a better picture of the computational observe that the level method saves 91 . 9% of computational time on average when compared with the SILP method, and 70 . 3% of computational time when compared with the SD method. In order to see more details of each optimization algorithm, we plot the logarithm values of the MKL objective function to base 10 against time in Figure 1. Due to space limitation, we randoml y number of function evaluations are consumed in order to comp ute the optimal stepsize via a line requires solving an SVM problem. As an example, we found that for dataset  X  X ono X , although SD and the level method require similar numbers of iterations, SD calls the SVM solver 1231 times on instability of SILP is further confirmed by the examination o f kernel weights, as shown below. kernel weights for datasets  X  X ono X ,  X  X reast X , and  X  X ima X  in Figure 2. We observe that the values of p computed by the SILP method are the most unstable due to oscil lation of the solutions to the cutting plane models. Although the unstable-solution prob lem is to some degree improved by the for the proposed level method, the values of p change smoothly through iterations. We believe that method can be more efficient than the SILP and the SD methods.
 Only parts of the evolution curves are plotted for SILP due to their long tails. problem. In particular, the level method overcomes the draw backs of both the SILP method and the SD method for MKL. Unlike the SD method that only utilizes the gradient information of the iterations; meanwhile, unlike the SILP method that updates the solution only based on the cutting is the employment of the projection step that guarantees find ing an updated solution that, on the computational time of MKL over both the SD method and the SILP method. For future work, we to other tasks, such as one-class classification, multi-cla ss classification, and regression.  X  X ima X  computed by the three MKL algorithms
