 We propose a new multi-view clustering method which uses clustering results obtained on each view as a voting pattern in order to construct a new set of multi-view clusters. Our experiments on a multilingual corpus of documents show that performance increases significantly over simple concate-nation and another multi-view clustering technique. H.3.3 [ Information Search and Retrieval ]: Clustering; I.5.3 [ Clustering ]: Algorithms Algorithms, Experimentation Multilingual document clustering, Multi-view learning, PLSA
Much data is now available in multiple representations, or views , for example multimedia content or web pages trans-lated into several languages. Multi-view learning is a princi-pled approach to handle these kinds of documents using the relations between the multiple views. The key is to lever-age each view X  X  characteristics in order to do better than simply concatenating views. Recently, multi-view cluster-ing methods have been proposed that address this situation and have been shown to improve over traditional single-view clustering. [2] proposed an extension of k-means and EM for a dataset with two views, and [5] presented a late fusion approach which re-estimates the relationship between doc-uments from single-view clustering results. [3] and [6] show that dimensionality reduction via canonical correlation be-tween views gives better results for document clustering than via principal components analysis or random projections. This paper introduces a novel multi-view clustering method. Our approach consists of two steps. First we find robust top-ics for each view using the PLSA approach. The topic pattern over the multiple views defines cluster signatures for each document. We use those to prime a second-stage clustering process over all the views. Experiments carried out on a large five language corpus of Reuters documents show that we consistently improve over competing techniques. where each version or view d v ,v  X  X  1 ,...,V } provides a rep-resentation of document d in a different language, with fea-ture space X v . Our algorithm operates in two steps.
At the first stage of our multilingual clustering, we ap-ply Probabilistic Latent Semantic Analysis ( PLSA ) [7] inde-pendently over each of the V languages, constraining each model to have the same number of unobserved topics. For every view v , the probability that document d v arises from topic z  X  Z is given by p ( z | d v ), estimated by PLSA .Docu-ments are then assigned to each topic using the maximum posterior probability. We hence obtain a set of V estimated topics ( z 1 d ,...,z V d )foreachdocument d ,whichwecallthe voting pattern in the following. Each z v d indicates the es-timated topic index of d on the v th viewaccordingtothe view-specific PLSA model.
Once a voting pattern is obtained for each multilingual document, we attempt to group documents such that in each group, documents share similar voting patterns. As documents belonging to each of these groups received by definition similar votes from the view-specific PLSA models, the voting pattern representing each of these groups is called the cluster signature . We keep the C largest groups with the most documents as initial clusters. Documents that have voting patterns with at least V  X  1incommonwitha clus-ter signature are pre-assigned to that cluster. The remaining documents have voting patterns different from any of the se-lected cluster signatures . They are matched to one of these C groups by applying a PLSA model on the concatenated document features.

The parameters of the final PLSA model are first initialized using the documents that have been pre-assigned to the se-lected cluster signatures . For these documents p ( c | d )has abinaryvalueequalto1if d belongs to cluster c and 0 otherwise. For the remaining d ocuments, posteriors are es-timated at each iteration as in the traditional E-step .Inthe M-step , after updating model parameters, we keep the val-ues of p ( c | d ) fixed for the pre-assigned documents. After convergence, documents are assigned to the clusters using the posteriors p ( c | d ). Note that any generative model giv-ing p ( c | d ) may be employed instead of PLSA ,suchasLatent Dirichlet Allocation [4].
We perform experiments on a publicly available multilin-gual multi-view text categorization corpus extracted from the Reuters RCV1/RCV2 corpus [1]. 1 This corpus con-tains more than 110K documents from 5 different languages, ( English , German , French , Italian , Spanish ), distributed over 6 classes. The multilingual collection is originally a comparable corpus as it covers the same subset of topics in all languages. In order to produce multiple views for docu-ments, each original document extracted from the Reuters corpus was translated in all other languages using a phrase-based statistical machine translation system. The indexed translations are part of the corpus distribution.
Experiments are repeated 10 times on the whole dataset, using different random initializations of the PLSA models. The number of topics in each single-view PLSA model as well as the number of clusters C are fixed to 6, the num-ber of classes in the collection. We used the micro-averaged precision ( micro-AvgPre )aswellastheNormalizedMutual Information ( NMI ) to measure clustering results [8]. In or-der to use these evaluation measures, the predicted label for each cluster is the label of the most dominant class in that cluster. The reported performance is averaged over the 10 different runs. To validate our approach we compare our algorithm (denoted by voted-PLSA in the following) with a PLSA model operating over the concatenated feature rep-resentations of documents ( conc-PLSA ) and the late fusion approach ( Fusion-LM ) for multi-view clustering [5].
First, we are interested in the clustering results after the first step of our algorithm on the C = 6 largest clusters con-taining each the same voting pattern documents. Table 1 shows the micro-AvgPre performance of the clustering re-sults per language as well as the percentage of documents being grouped with our voting strategy. We observe that partitions formed using the votes of single-view models con-tain more than half of the documents in the collection and that these groups are highly homogeneous with an average precision of 0 . 76.
 Table 1: Proportion of pre-assigned documents and average precision on those, obtained from the first stage single-view PLSA models.
 Table 2 summarizes results obtained by conc-PLSA , Fusion-LM and voted-PLSA averaged over five languages and 10 dif-http://multilingreuters.iit.nrc.ca/ Table 2: micro-AvgPre and NMI of different clustering techniques averaged over 10 initialization sets and 5 languages.
 ferent initializations. We use bold face to indicate the high-est performance rates, and the symbol  X  indicates that per-formance is significantly worse than the best result, accord-ing to a Wilcoxon rank sum test used at a p-value threshold of 0 . 05. Note that in our approach, the second stage multi-view clustering model relies on a PLSA on the concatenated views, just as in conc-PLSA . This suggests that the difference of2to3pointsin micro-AvgPre and NMI (respectively) be-tween voted-PLSA and conc-PLSA shows the real impact of the first stage voting process. In addition, both voted-PLSA and conc-PLSA perform at least as well as Fusion-LM .
We presented a multi-view clustering approach for multi-lingual document clustering. The proposed approach is an incremental algorithm which first groups documents hav-ing the same voting patterns assigned by view-specific PLSA models. Working in the concatenated feature spaces the remaining unclustered documents are then assigned to the groups using a constrained PLSA model. Our results have brought to light the positive impact of the first stage of our approach which can be viewed as a voting mechanism over different views. The effect of the length of these voting patterns and the number of latent variables in view-specific PLSA models are interesting avenues for future research. This work was supported in part by the IST Program of the EC, under the PASCAL2 Network of Excellence.

