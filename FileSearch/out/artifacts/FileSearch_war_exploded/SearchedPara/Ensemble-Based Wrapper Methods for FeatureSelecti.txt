 Feature selection is a critical procedure for high-dimensional data classification. The benefits of feature selection are seve ral-fold and dependent on the applica-tions. For creating classification models, feature selection can often improve pre-dictive accuracy and comprehensibility [1]. For many bioinformatics applications, feature selection is a critical procedure for identifying important biomarkers [2].
The techniques for feature selection a re commonly classified as filter approach, wrapper approach, and embedded approach. Filter approach and embedded ap-proach are relatively computationally efficient and are commonly applied as a fast feature ranking procedure [3]. In contrast, wrapper approach evaluates fea-tures by performing internal classification with a given inductive algorithm [4]. Therefore, they are much more computatio n intensive. Nevertheless, wrapper ap-proach remains attractive for two reasons. Firstly, wrapper approach evaluates features iteratively with respect to an inductive algorithm. Therefore, features selected by wrapper approach are more likely to suit the inductive algorithm, and therefore, yield high classification accuracy [4]. Secondly, wrapper approach evaluates features jointly and are effectiv e in capturing intrinsic relationships such as interactions among multiple features [5].

Learning from imbalanced data is an important problem in many data mining applications. Such a case arises when samples from one class significantly out-number those from the other class. Imbalanced data are common in text mining [6] and bioinformatics where the minority class often represents the rare cases. It is well known that many classification algorithms are sensitive to the imbalanced class distribution [7]. Therefore, many strategies have been proposed to deal with class imbalance learning. Generally, they fall into two categories: cost-sensitive learning and data sampling [8]. With cost-sensitive learning, a given algorithm will receive a higher penalty when a mis take is made on the minority class than on the majority class. The advantage of cost-sensitive learning is that it does not modify the class distribution. Howe ver, an accurate cost -metric needs to be specified beforehand. As for data samplin g, the learning instances in the major-ity class and minority class are manipulated in certain way so as to balance the class distribution. The downside is that sampling strategies may introduce noise or remove useful information while modifying class distribution.

The challenges of feature selection and imbalanced data classification meet when the dataset to be analysed is of both high-dimensionality and highly im-balanced class distribution [9]. In such a scenario, if wrapper approach is adopted for feature selection, the inductive algorithm may introduce significant bias be-cause the merit of the feature subset is evaluated based on the performance of the inductive algorithm. Therefore, if the inductive algorithm favours a single class, the features selected will also bias to this class while being less useful to the adverse class.

In this study, we propose an ensemble-based wrapper approach for feature selection from highly imbalanced datasets. The proposed algorithm retains the advantages of wrapper feature selection while also maximises data usage and reduce feature selection bias simultaneously by training multiple base classifiers with balanced sample subsets. A hybrid multiple sampling procedure is employed to create balanced sample subsets. Together we introduce a unified framework that incorporates ensemble feature selection and multiple sampling in a mutually beneficial manner.

The paper is organised as follows. In Section 2, we outline the proposed frame-work and describe each component in details. Section 3 describes the experimen-tal procedure. Results are presented i n Section 4 and Section 5 concludes the paper. Wrapper algorithms, in general, consist of three main components [10]: (1) a search algorithm, (2) a fitness function, and (3) an inductive algorithm. The proposed system adheres to this structure. In this section, we outline the system and describe each component. 2.1 System Overview A schematic representation of the proposed ensemble-based wrapper approach is shown in Figure 1. The imbalanced training dataset is balanced by a hybrid sam-pling approach (which will be explained in Section 2.3). Such a hybrid sampling procedure is applied multiple times producing multiple sets of balanced training data each of which is used to train a base classifier. The base classifiers trained on the balanced datasets are subsequently applied to classify an imbalanced test dataset. The classification distributions of each sample in the test dataset are normalised and combined, and the area under ROC curve (AUC) is calculated as the fitness indices for feature selectio n. The wrapper procedure terminates when it reaches a predefined number of iteration or a desired number of features is selected (i.e. greedy search), and the final feature subsets are used for further validation.
 2.2 Search Algorithm There are several popular search strategies, including hill climbing algorithms best exemplified by forward selection and backward elimination [11,12] and evo-lutionary algorithms such as genetic algorithm [13] and particle swarm optimi-sation [14].

In this study, we apply two search algorithms. The first one is a hill climb-ing algorithm that starts with an empty set and greedily selects a feature at a time that maximises the given fitness function. This is a typical greedy forward selection approach and at each step the best feature f  X  is determined by: where S is the set that contains the features selected so far and f is a feature under evaluation according to a fitness function.

The second search algorithm is a simple elitism genetic algorithm. The feature size is pre-specified and the algorithm selects the best feature set that maximises the given fitness function through genetic operations such as crossover and mu-tation. Here each feature in the best set S  X  is determined simultaneously: where p is the population size of the genetic algorithm.

The above two typical yet simple wrapper procedures offer a transparent way to compare different inductive components. 2.3 Hybrid Sampling from Imbalanced Data Sampling is a popular approach to balance the dataset with imbalanced class distribution. The simplest methods are random under-sampling and random over-sampling [15]. The random under-sampling method balances the dataset by randomly removing samples in the majority class. On the contrary, the ran-dom over-sampling method balances the dataset by sampling from the minority class with/without replacement and reattaching them to the dataset. A more sophisticated approach is to synthesise  X  X ew X  samples from the minority class (known as SMOTE) [16]. Several studies also found that better results can be achieved by increasing minority samples a nd decreasing majority samples simul-taneously [17,18].

Here we apply our own hybrid approach in which the dataset (denoted as D ) is balanced by increasing minority class with SMOTE and decreasing majority class with random under-sampling as follows: where I maj , I min , N maj and N min are the majority samples, minority samples, and their sample sizes, respectively. Random ( . ) randomly selects from I maj a subset of samples I R and SMOTE ( . ) creates synthetic samples I S using I min . The balanced dataset D  X  retains the original minority samples and introduces 1 / 2  X  N min synthetic minority samples. The majority samples I R are reduced to match the new set of minority samples in D  X  and result in a class ratio of 1. 2.4 Ensemble Learning The classic idea of ensemble is to generate multiple datasets using a sampling method such as bootstrap, and train a set of homogeneous learning algorithms which classify new instances in a consensus manner [19]. This idea has been extended both to imbalanced data classification [20] and feature filtering [21]. However, no work has been done to unify them as a single procedure which may be mutually beneficial.

Here, we extend the idea of ensemble to feature selection in a wrapper manner and provide a unified framework that inco rporates ensemble fe ature selection as well as multiple sampling. Specifically, given a training dataset constrained by a set of features S , suppose we apply the above hybrid sampling procedure L times, each time producing a balanced sampling dataset D  X  S i ( i =1 ...L ), and each sampling dataset is used to tr ain a base classifier denoted as h i . Then, the ensemble classification distribution y of each test sample x is computed as follows: where Prob ( h i ( x ) , D  X  S i ) is a probability vector computed by using an ensemble of L base classifiers ( h i ), each is trained on a balanced sampling set D  X  S i selected by the feature set S . Therefore, both feature set information and data sampling information are incorporated in an ensemble framework. 2.5 Fitness Function An inductive algorithm (classifier) is commonly used to generate fitness indices in wrapper algorithms. It is well known that the overall accuracy as a metric is biased when the class distribution is imbalanced in the data. A more reliable way to compute the fitness of a feature set in such a case is to use the area under the ROC curve (AUC). AUC is a numeric value summarising the trade-off between the true positive rate and the false positive rate across the entire sample classification distribution of a dataset.

When using a single inductive algorithm, the AUC value is directly calculated by sorting the classification probability of each sample, calculating trade-off value of the true positive rate and false positive rate at each classification threshold, and calculating the area under the trade-off values. As to the ensemble classi-fier, classification distribution of each sample is combined and normalised across all base classifiers. Then, the same procedure as those for a single inductive algorithm is applied to calculate the AUC value.

Accordingly, we define the fitness of a feature subset as follows: where x 1 is the first sample in the test dataset and m is the total sample size. Function AUC ( . ) calculates the AUC value.
 2.6 Main Algorithm of Ensemble-Based Wrapper Approach Algorithm 1 represents the core of the ensemble-based wrapper approach in pseudo-code: Algorithm 1. Ensemble Component
The ensemble component is independent from the search algorithm. It is flex-ible and can be reused in different wrapper algorithms. In this section, we summarise the datasets used for evaluation and detail the algorithms and parameter settings. Following that, the performance evaluation is described. 3.1 Datasets and Data Partitioning We used 5 datasets with high-dimensionality and highly imbalanced class distri-bution. Table 1 summarises the datasets.

Specifically, fbis, re0, and oh5 are text mining datasets extracted by Han and Karypis [22]. The ALL (acute lymphoblastic leukemia) dataset is from a leukemia study [23], and the oil dataset is from study [24].

For datasets with multiple classes, we reserved the class with the small-est number of samples as the minority class and combined the other classes as the majority class. To make the problem computationally less demanding, for datasets with very high dimensions, we applied a  X  2 filtering to reduce the feature size to 500.
 The datasets are partitioned using the double-level cross-validation strategy. That is for each dataset, we partitioned it using a 2-fold stratified cross-validation to obtain the training and evaluation sets. For the training set, it is further par-titioned using a 5-fold stratified cross-validation to obtain the internal training and internal testing sets for feature sel ection. The evaluation set is reserved from the feature selection procedure and is only used for evaluating the usefulness of the selected features after the feature selection procedure. 3.2 Algorithms and Parameter Settings For the greedy forward feature selection a lgorithm, we specified it to search 20 steps in which 1 to 20 features are select ed one after an other. As for the genetic algorithm, we set both the population size and the termination generation to 20. The crossover probability and the mutation probability are 0.7 and 0.1, respectively. The  X  X hromosome X  is coded as a string of feature indexes, and the chromosome size of 1 to 20 are tested which corresponds to the feature subset size of 1 to 20. Different from the greedy forward feature selection algorithm which builds the feature subset on previously selected features, the genetic algorithm tests different size of feature subsets separately.

The decision tree algorithm (J48) is used for induction in our wrapper al-gorithms. In ensemble learning, the decision tree algorithm is prevailingly used as the base classifier because it is relatively fast to train and unstable to small changes in the data [25]. These are the important merits to our wrapper algo-rithms since we need to evaluate features using multiple classifiers in an efficient manner. Yet, it is widely known that the decision tree algorithm is sensitive to the imbalance of the data class distribution [26]. Hence, it is of both theoretical and practical interests to use decision tr ee in our experimental settings. For the ensemble wrapper, we used the ensemble size of 20. That is 20 different sampling dataset are produced in each iteration and 20 decision tree cla ssifiers are trained on these sampling dataset and then used for feature selection.

To evaluate the selected features, we use d 6 different classification algorithms, including random forest (RF), nearest neighbour with k =3 (3-NN), nearest neighbour with k =7 (7-NN), logistic regression (LogReg), multiple layer percep-tron (MLP), and alternating decision tree (ADTree). The rationale is that if the wrapper algorithm is able to select useful features, the selected features should be able to improve the classification result regardless what type of classification algorithm is used. Therefore, evaluating a wide range of different classifiers can better reflect the genuine usefulness of the selected features. 3.3 Performance Evaluation In this study, we focus on comparing wrapper algorithms with ensemble-based imbalanced sampling and classification component to wrapper algorithms with a single inductive algorithm. We refer to the first approach as the ensemble ap-proach and the latter as the single approach. To summarise the performance results, the AUC values obtained from ea ch classifier using features selected by ensemble approach and single approach are compared. If the ensemble ap-proach yields a higher AUC value compared to the single approach, we label it as  X  X nsemble win X . Similarly, if the ensemble approach yields a lower AUC value compared to the single approach, we label it as  X  X ingle win X . When the AUC values from these two approaches are equal, we obtain a  X  X ie X . The comparison is conducted from feature size 1 to 20.

In addition, the Friedman test [27] is applied to evaluate the performance of each classifier. The confidence of 95% is used under the null hypothesis that the performance of each classifier is not significantly different by using the features selected by the ensemble approach and the single approach. The null hypothesis is rejected if there are significant perf ormance difference when using features selected by ensemble approach as to single approach. AUC comparison of ensemble wrapper and single wrapper using greedy forward feature selection with fbis and re0 dataset are plotted in Figure 2 and Figure 3, respectively. As can be seen, the ensemb le wrapper approach exhibited a better performance compared to the single wrapper approach. We summarise results in Figures 2 and 3 and the rest of the comparison across using feature sets with size from 1 to 20 in Table 2 and Table 3 (see 3.3 for details of the summarisation method). Specifically, Table 2 shows the comparison of the ensemble approach and the single approach using greedy forward selection algorithm, and Table 3 shows the comparison using genetic algorithm. It is clear that across all datasets most classifiers achieves better classificat ions using features selected by ensemble approach than those selected by single approach. This implies that the ensemble approach is more robust to high-dimensionality and highly imbalanced class distribution. Hence, the features select ed by the ensemble approach are likely to be more useful to both the majority class and the minority class.

The greedy forward selection appears to be more sensitive to ensemble com-ponent. In most cases, the improvements are significant. In comparison, genetic algorithm based selection is less sensitive to the ensemble component, and most improvements are moderate. This may attr ibuted to their different feature se-lection styles. That is, greedy forward selection builds the feature subset on previously selected features. Therefore , if a good feature is selected, it will con-tinually be used in later iterations. Whereas, the genetic algorithm tries different size of feature subsets separately, and for each run the initiation, crossover, and mutation operations introduces randomn ess to the selection procedure. It follows that the greedy forward selection approach is likely to aggregate the effect of the ensemble through iterations, while the genetic algorithm approach may reduce the effect of the ensemble due to its stochastic behaviour.

It is interesting to see that different classification algorithms performed dif-ferently even with the same set of feat ures. For the extreme case, in Table 2 the classification results of 7-NN on oh5 dataset and 3-NN and 7-NN on oil dataset contradict to the rest of the classi fiers. Even for classifiers with similar comparison results, each of them may still behave differently throughout the feature subset size of 1 to 20. For example, in Figure 2, RF shows an increasing trend when more features are added. However, LogReg and ADTree indicate a decreasing trend when more features are included, whereas the performance of MLP increases first and then decreases. Note that the same sets of features and the same evaluation dataset are used for each classification algorithm. There-fore, it is clear that using a single classification algorithm for results evaluation is insufficient. Instead, multiple classification algorithms should be evaluated in order to reflect the general usefulness of the selected features. In this study, we proposed an ensemble approach that incorporate feature selec-tion and imbalanced data sampling in a wrapper framework. Using two search algorithms and several high-dimensional and highly imbalanced datasets, we demonstrated that features selected by the ensemble-based wrapper approach are more useful than the traditional approach (i.e. using single inductive algo-rithm) in terms of feature selection and imbalance learning. This implies that the traditional approach that uses a single inductive algorithm for feature evalu-ation may perform suboptimally when the dataset is of both high-dimensionality and highly imbalanced class distribution. By designing a multiple sampling and an ensemble feature evaluation components, we can correct the undesirable bias and identify more useful features and/or feature subsets.

