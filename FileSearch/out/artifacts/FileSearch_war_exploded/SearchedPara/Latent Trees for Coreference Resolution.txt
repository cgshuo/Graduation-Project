 Instituto Federal de Goi  X  as Brazilian Research Lab IBM Research PUC-Rio cause it incorporates a meaningful hidden structure. Additionally, using an automatic feature induction method, we can efficiently build enhanced nonlinear models using linear model learn-ing algorithms. We present empirical results that highlight the contribution of each modeling technique used in the proposed system. Empirical evaluation is performed on the multilingual unrestricted coreference CoNLL-2012 Shared Task data sets, which comprise three languages: pronoun lists. A previous version of this system was submitted to the CoNLL-2012 Shared Task closed track, achieving an official score of 58 . 69 , the best among the competitors. The unique nested mentions for the Chinese language. By including such arcs, the score increases by almost 3 . 5% error reduction, and is the best performing system for each of the three languages. 1. Introduction
Mentions are textual references to real-world entities or events. In a given document, mentions that refer to the same entity are called coreferring mentions and form a men-tion cluster . Coreference resolution is the task of identifying the mention clusters in a document and has been a core research topic in natural language processing. It has wide applications in question answering, machine translation, automatic summarization, and information extraction. Coreference resolution systems have been evaluated for several decades, beginning with MUC-6 (Sundheim and Grishman 1995). Following those evaluation efforts, the CoNLL-2011 Shared Task (Pradhan et al. 2011) has been dedicated to the modeling of unrestricted coreference resolution for English text. The
CoNLL-2012 Shared Task (Pradhan et al. 2012) extends the task to a multilingual scope, considering three languages: Arabic, Chinese, and English.
 coreference resolution task consists of identifying the non-singleton mention clusters in a document. This task is usually split into three subtasks: mention detection, mention clustering, and singleton elimination. In Figure 1, we present an illustrative example.
First, ten mentions are detected and shown in bold. They are sequentially tagged with the numbers 1, 2, ... , 10. Next, four mention clusters are identified by tagging each men-c = { 4, 5, 6 } , d = { 10 } are the four mention clusters. Finally, clusters that contain only one mention are ignored, such as the one with Iran as its unique mention. In this example, we ignore some noun phrases in the mention detection subtask to simplify the illustration.
 subtask, several specific heuristics have been proposed, enabling the construction of high-recall mention detectors. The second subtask is harder, as it requires a complex output. Most of the current effort toward solving the coreference resolution task is focused on the mention clustering subtask.
 on two key modeling techniques: latent coreference trees and entropy-guided feature induction. Our approach is based on a graph whose nodes are the mentions in the given document. The arcs of this graph link mention pairs that are coreferent candidates. The resulting structure predictor has similar steps for training and testing.
 1. Mention Detection  X  where we build a graph node for each mention by 2. Candidate Pair Generation  X  where we add a directed arc for each candidate 802 3. Basic Feature Setting  X  where we set basic features that indicate whether an 4. Context Feature Induction  X  where we conjoin basic features to generate 5. Coreference Tree Learning  X  where we learn how to extract the trees that feature induction . Using this mechanism, we automatically generate several feature templates that capture coreference-specific local context knowledge. Furthermore, this feature induction mechanism extends the structured perceptron framework by providing an efficient general method to build strong nonlinear classifiers. three further steps: 4. Context Feature Setting  X  where we set the values of the additional induced 5. Coreference Tree Prediction  X  where we apply the Chu-Liu-Edmonds 6. Coreference Cluster Extraction  X  where we extract the clusters of coreferring to find the maximum score coreference trees. This process is efficiently performed by the Chu-Liu-Edmonds algorithm (Chu and Liu 1965; Edmonds 1967). A tree score is simply the sum of its arc scores, which are given by a weighted sum of the arc features.
The feature weights are learned during training in the Coreference Tree Learning step, which is based on the structured perceptron algorithm. Because coreference trees are not given in the training data, we assume that these structures are latent and use the latent structured perceptron (Sun et al. 2009; Yu and Joachims 2009) as the learning algorithm. In fact, we use a large margin extension of this algorithm (Fernandes and Brefeld 2011). is a set of coreference trees. Each coreference tree corresponds to a cluster of coreferring mentions.
 multiple metrics have been proposed for evaluating coreference performance. No single metric is clearly superior to the others. We follow the CoNLL-2012 Shared Task evalua-tion scheme, adopting the unweighted average of the MUC, B also use the multilingual data sets provided in the CoNLL-2012 Shared Task to assess our system. The official ranking for this task is given by the mean of the system scores on three languages: Arabic, Chinese, and English.
 such as language-dependent static pronoun lists. Our system does not consider verbs when creating candidate mentions. Therefore, it does not resolve coreferences involving events. We participated in the CoNLL-2012 Shared Task with a previous version of our system (Fernandes, dos Santos, and Milidi  X  u 2012). The system submitted to this task achieved scores of 54 . 22, 58 . 49, and 63 . 37 on the Arabic, Chinese, and English test sets, respectively. Its official score is thus 58 . 69, which is the best among the competitors.
Later, we extended this system by including candidate arcs linking nested mentions for the Chinese language. This version shows an official score of 60 . 15, corresponding to a 1 . 46 point absolute improvement or a 3 . 5% error reduction. As far as we know, this is currently the best performing system on the CoNLL-2012 Shared Task Arabic, Chinese, and English test sets.
 work. In Section 3, we detail our approach to the mention detection subtask. In Section 4, we describe coreference trees, a key element in solving the mention clustering subtask in our system. We also examine the coreference tree prediction problem. In Section 5, we detail the entropy-guided feature induction method and its application to coreference resolution. In Section 6, we describe the large margin latent structured perceptron that we use to learn the coreference trees. In Section 7, we present our experimental setting.
The experimental findings are provided in Section 8. Finally, in Section 9, we present our concluding remarks. 2. Related Work
Over the last two decades, many different machine learning X  X ased approaches to co-reference resolution have been proposed. Most of them use supervised learning and divide the task into two phases: the detection of potential mentions and the linking of mentions to form coreference chains, that is, mention clustering. In Ng (2010), the author presents a detailed review of supervised approaches to coreference resolution. 1995) and the Automated Content Extraction (ACE) corpus (Doddington et al. 2004) use gold mention boundaries and, hence, do not address the mention detection task (Culotta, Wick, and Mccallum 2007; Finkel and Manning 2008; Poon and Domingos 2008; Haghighi and Klein 2009). Most of the works that perform mention detection use a set of heuristics. The common approach consists of extracting all noun phrases from the parse tree and considering them to be candidate mentions (Soon, Ng, and Lim 2001; dos Santos and Carvalho 2011; Haghighi and Klein 2010; Stoyanov et al. 2010; Chang et al. 2011; Bansal and Klein 2012; Lee et al. 2013; Sapena, Padr  X  o, and Turmo 2013). A few works approach the mention detection task by training classifiers (Bengtson and not a suitable approach because singleton mentions are not annotated in this corpus.
Systems trained and tested with these data sets frequently privilege mention recall
Therefore, these systems usually extract noun phrases that are not identified in the parse tree. This strategy increases consistency with the corpus annotation (Chang et al. 2011; Sapena, Padr  X  o, and Turmo 2011; Bj  X  orkelund and Farkas 2012; Lee et al. 2013). The system proposed in this work uses a rule-based approach to mention detection, which is similar to previous work with the CoNLL data sets (dos Santos and Carvalho 2011). pairwise classification task (McCarthy and Lehnert 1995; Soon, Ng, and Lim 2001; Ng and Cardie 2002; Ponzetto and Strube 2006; Bengtson and Roth 2008; Ng 2009; Stoyanov et al. 2009; Bj  X  orkelund and Nugues 2011; Uryupina et al. 2011; Uryupina, Moschitti, and 804
Poesio 2012). In this strategy, a preprocessing step is used to generate pairs of candidate coreferring mentions. In the training phase, it is necessary to generate positive and negative examples of coreferring pairs. For this purpose, the approach proposed by Soon, Ng, and Lim (2001) is normally used. For instance, in the CoNLL 2011 Shared
Task (Pradhan et al. 2011), 11 of the 18 machine learning X  X ased participant systems used Soon, Ng, and Lim X  X  approach or a variation of it. In the classification phase, each candidate pair of mentions is classified as coreferring or not, using the classifier learned from the annotated corpus. When using the mention-pair approach, a linking step is necessary to remove inconsistencies that would result from the pairwise classifications and to construct a partition on the set of mentions. An aggressive strategy for this last step consists of merging each mention with all preceding mentions that are classified as coreferent with it (McCarthy and Lehnert 1995; dos Santos and Carvalho 2011). More sophisticated strategies, such as inference methods, have also been proposed (Bengtson and Roth 2008; Chang et al. 2011). One of the main disadvantages of the mention-pair classification approach is the lack of global information. Moreover, the lack of information on the clusters already formed can lead to contradictory links.
 tion. Entity-mention models address the lack of information about the clusters already formed by seeking to classify whether a mention is coreferent with a preceding cluster (Luo et al. 2004; McCallum and Wellner 2005; Culotta, Wick, and Mccallum 2007; Yang et al. 2008). While entity-mention models have the advantage of allowing the creation of cluster-level features, they fail to identify the most probable candidate antecedent, just as the mention-pair classification approach does. Ranking models address this issue by directly comparing different candidate antecedents for the mention part of the training criterion (Yang et al. 2003; Yang, Su, and Tan 2008; Denis and Baldridge 2008). However, ranking models are unable to exploit cluster-level features. An approach that combines the advantages of entity-mention and ranking models was proposed by Rahman and
Ng (2009). Their method, the cluster-ranking model, ranks preceding clusters rather than candidate antecedents, thereby enabling the use of cluster-level features. have also been proposed. Cai and Strube (2010), Cai, Mujdricza-Maydt, and Strube (2011) and Sapena, Padr  X  o, and Turmo (2013) present systems that implement global de-cision via hypergraph partitioning. Whereas Cai and Strube (2010) and Cai, Mujdricza-
Maydt, and Strube (2011) use a spectral clustering algorithm to perform hypergraph partitioning, Sapena, Padr  X  o, and Turmo (2013) use relaxation labeling for the resolution process. There are also approaches that perform global inference using integer linear programming to enforce consistency on the extracted coreference chains (Denis and Baldridge 2007; Klenner 2007; Finkel and Manning 2008).
 resolution as a correlation clustering problem. The former use structural SVMs as the learning algorithm, and the latter use Collins X  structured perceptron. Our system is also based on the structured perceptron; however, we use a large margin extension of this algorithm and use latent trees to represent each coreferring cluster. Yu and
Joachims (2009) propose structural SVMs with latent variables and apply this approach to coreference resolution. They compare their results with those of Finley and Joachims (2005) and show that the latent trees significantly improve performance. The main between coreferring mentions are considered to compute the solution score. However, many pairs of coreferring mentions do not have a direct relation. For example, in
Figure 1, mentions 2 and 9 are coreferent pronouns, but they are in neither anaphoric nor cataphoric relation. Mention 1 is an antecedent of mention 2; mention 8 is an antecedent of mention 9; and mentions 1 and 8 are obviously coreferent. Hence, these two pronouns can be predicted as coreferent only by transition between other direct references. By using trees, the natural hierarchy of coreference dependencies is better modeled.
 it is also based on latent trees. However, they use undirected trees to represent clusters, whereas we use directed trees. This difference is most likely not very significant, but we think that most coreference dependencies are directed relations. Using the example from Figure 1 again, we can see that mention 2 is a reference to mention 1 and not the other way around. By using directed trees, we model coreference dependencies, such as the dependencies between nouns and pronouns referring to the same entity. Other differences between Yu and Joachims (2009) and our work are that they do not use an artificial root node and, moreover, that their empirical evaluation is based on the
MUC-6 task (Sundheim and Grishman 1995), whereas ours is based on the CoNLL-2012 Shared Task. The CoNLL-2012 Shared Task considers multilingual unrestricted coreference resolution. It defines a more general task than the one proposed by the
MUC-6, as the annotated mentions in OntoNotes cover entities and events not limited to noun phrases or a limited set of entity types (Pradhan et al. 2011).
 induction to coreference resolution, which we experimentally demonstrate as highly effective. There are some on-line methods (della Pietra, della Pietra, and Lafferty 1997;
McCallum 2003) that perform feature induction within the learning algorithm. These methods choose among candidate features by evaluating their impact on performance along with the learning process. Our method is off-line . In this case, feature induction is performed by an efficient procedure before the training step. Off-line methods have two main advantages over on-line ones: (i) they are usually faster, as inducing features dur-ing training requires substantially more learning iterations; and (ii) there is not a need to modify the learning algorithm, so we can use it as is. The proposed method extends the automatic template generation strategy proposed in Entropy-Guided Transformation
Learning (dos Santos and Milidi  X  u 2009) to structure learning. This strategy has been previously applied to the English unrestricted coreference resolution task (dos Santos and Carvalho 2011). However, this previous work is based on a completely different task modeling and a different learning algorithm. 3. Mention Detection
The first subtask in coreference resolution, mention detection, is formally solved by an auxiliary predictor A given by where m m m is the list of detected mentions in the given document d d d . Singleton mentions are not annotated in the CoNLL-2012 data sets. Hence, a specific noun phrase may be annotated as a mention in one document but not in another document due to the absence of coreferring mentions in the latter. Therefore, machine learning X  X ased mention detectors are difficult to build in this case.
 implement heuristics that have been proven effective in previous works (dos Santos and 806
Carvalho 2011). For a given document d d d , this predictor generates a list m m m of candidate mentions by including the following items: 1. noun phrases from the provided parse tree; 2. pronouns, even when they appear inside larger noun phrases; 3. named entities in the categories Person (PERSON), Organization (ORG), 4. and possessive marks, to better align with the CoNLL-2012 annotated use of parse tree information and that privilege recall over precision. Note that rules aligning the set of detected mentions more fully with the CoNLL-2012 annotated mentions. Note also that all but the fourth rule are language independent. The fourth rule is used for the English language only. This set of rules can be further improved by including rules that contain language-specific knowledge. For instance, Lee et al. It seems that .

However, the current version of our system does not consider verbs when creating candidate mentions and therefore does not resolve coreferences involving events. 4. Mention Clustering
In the mention clustering subtask, we must find a structure predictor F that assigns to its input x x x a high-quality mention clustering  X  y y y given by set.
 correct input X  X utput pairs. More specifically, the structured perceptron algorithm learns the weight vector w w w of a parameterized predictor given by
The prediction  X  y y y is the solution of an optimization problem and is called the prediction problem . The objective function of this problem is given by s , and it scores the candidate clusterings for the given document.
 a directed tree whose nodes are the coreferring mentions in a cluster and whose arcs represent strong coreference relations between mentions. To illustrate this concept, we use the labeled document in Figure 1. In Figure 2, we present a plausible coreference tree for the { 1, 2, 8, 9 } mention cluster.
 they are simply auxiliary structures for the clustering task. Nevertheless, we have two arguments to justify the application of these structures. First, the concept is linguisti-cally plausible because there is a dependency relation between coreferring mentions.
Based on the aforementioned example, it may be observed that mention 8 ( North Ko-2 than to mention 1. Hence, the coreference trees should capture this dependency structure. Second, the concept is algorithmically plausible because most hierarchical clustering methods incrementally link an unclustered mention to a previous cluster. strength of the coreference relation between two mentions. This scoring function on the arcs guides the construction of the coreference trees with the largest aggregate scores.
 (2009). However, Yu and Joachims use undirected spanning trees instead of directed ones. We think that directed trees are more appropriate for use in coreference resolution to represent dependencies between mentions. Recalling the example from Figure 2, it is clear that mention 2 ( its ) is a reference to mention 1 ( North Korea ), not the other way around. Hence, there is indeed a direction related to the dependency between a pair of coreferring mentions.
 coreference trees, one tree for each cluster. Hence, for the sake of simplicity, we link the root node of every coreference tree to an artificial root node, obtaining the document tree . In Figure 3, we depict a document tree for the current example.
 a mention clustering as follows: 1. remove the artificial root node and all its outgoing arcs, splitting the 2. from each tree in the resulting forest, output its node set as a mention its North Korea's 808 mention clustering subtask. Formally, we decompose the original predictor into two predictors, as follows: is defined as as the joint feature vector representation of x x x and document tree h h h . over its features. To understand how this predictor works, we must define the set H ( x x x ) arg max combinatorial problem.
 candidate pairs graph . Therefore, we must describe how to build the nodes and the connects a pair of mentions in x x x . Ideally, we would consider the complete graph for each document. However, because the number of mentions may be large and because most mention pairs are not coreferent, we filter the arcs simply by using sieves from the method proposed by Lee et al. (2013) for English coreference resolution. To further reduce the total number of arcs, we only consider forward arcs, which means that a j in the document text. In this way, we avoid including many unnecessary arcs in G ( x x x ), which speeds up the training. Each forward arc that passes through any of the used of a candidate arc as a candidate pair . Because we heuristically filter arcs out of G ( x x x ), y y y is impossible. However, such cases are rare, given that the recall of the used sieves is approximately 90%. To complete G ( x x x ), we add an artificial root node and connect it to each mention in x x x . These extra arcs are the artificial arcs , and under the current setting, they also point to the first mention in a cluster. Hence, we can learn how to use them to control the number of mention clusters.
 is linearly decomposed as (  X  to identify whether i and j are coreferent or not. To define these features, we utilize the entropy-guided feature induction method described in the next section.
 proaches for dependency parsing (McDonald, Crammer, and Pereira 2005; Fernandes and Milidi  X  u 2012). Thus, the arg max combinatorial problem reduces to a maximum branching problem, which can be efficiently solved using the Chu-Liu-Edmonds algo-rithm (Chu and Liu 1965; Edmonds 1967). 5. Entropy-Guided Feature Induction
The CoNLL-2012 Shared Task data sets include features that are either naturally present in documents, such as words, or automatically generated by external systems, such as POS tags and named entities information. We call this information provided by the data sets basic features . At the same time, most structure learning algorithms are based on linear models, as such algorithms have strong theoretical guarantees regarding their prediction performance and, moreover, are computationally efficient.
However, linear models using basic features alone do not capture enough information to effectively represent coreference dependencies. Conjoining basic features to derive new features is a common way to introduce nonlinear contextual patterns into linear models.
 basic feature conjunctions, which we call feature templates . These templates are later used in the structured modeling described in the previous sections.
 linked by an arc are coreferring. To solve this auxiliary binary classification problem, we derive a new set of data sets from the original one. Using all arcs in each of the an example of such a data set for the document in Figure 1. This example includes the following features: i-head is the head word of mention i ; i-pos is the head POS mention j ; sameNE indicates whether both mentions have the same named entity type; and dist is the number of mentions in the document text that occur between mentions i and j . 810 problem by conjoining basic features that are highly discriminative. This method is based on the conditional entropy of arc label c ( e ) given the basic features  X   X   X  ( e ).
The decision variable indicates whether the arc links two coreferring mentions. We use the arcs of all training examples; that is, for each training document, we generate an example for each candidate arc. Thus, the learned decision tree (DT) predicts whether the mentions linked by an arc are coreferring. In Figure 4, we present a decision tree learned from an arc data set. Each internal node in the DT corresponds to a feature; each leaf node has a label value (0 or 1, in the binary case); and each arc is labeled with a value of the source node feature. Note that in this sample DT, we suppress several possible feature values to simplify the presentation.
 most informative feature (Quinlan 1992; Su and Zhang 2006). The Gain Ratio is simply a normalized version of the information gain measure. Hence, these algorithms provide a quick way to obtain entropy-guided feature selection. We propose a new automatic feature generation method for structure learning algorithms. The key idea is to use decision tree induction to conjoin the basic features. One of the most frequently used algorithms for DT induction is C4.5 (Quinlan 1992). We use Quinlan X  X  C4.5 system to obtain the required entropy-guided selected features.
 partial paths starting at the root node. For each path, a template is created by conjoining all of its node features. Because we aim to generate feature templates  X  X onjunctions of basic features that do not include feature values X  X e ignore the feature values and the decision variable values in the DT. Thus, we do not use arc labels or leaf nodes. Figure 5 illustrates our method. The tree in the left side of this figure is the skeleton obtained from the decision tree in Figure 4 by discarding the aforementioned pieces of information.
Its nodes are basic features with high discriminative power. The generated templates the features in each path from the root node to every other internal node in the given decision tree. Additionally, we eliminate duplicate templates.
 step, our method generates feature templates with high discriminative power based on entropy. This method can provide a very large number of templates. Hence, to limit the maximum template length, we use C4.5 pruned trees and limit the maximum template length when traversing the DT. This parameter is clearly task-dependent and must be calibrated by cross-validation or on a development set.
 that occur in the structured data set D . For each template, we generate many binary (  X  j-pos sameNE is given by
Note that this feature captures a context that is not used by the DT in Figure 4. Indeed, we eliminate the DT feature values when generating the templates and then instantiate these templates based on every context that occurs in a training example.
 indicate the derived features that are active in arc e . These active derived features depend on the values of the basic features  X   X   X  ( e ). The number of derived features M is very large because several combinations of basic feature values arise for each template. However, the number of active derived features in a specific arc is much smaller. In fact, for each arc, there is one active derived feature for each template; all others are set to zero. running the DT algorithm. For such features, each integer value is considered a different category. This approach works better than slicing the value range of these features, as the integer features considered in our system have quite narrow ranges. 812 6. Large Margin Latent Tree Learning In our learning set-up for the mention clustering subtask, we are given a training set mention clustering y y y . Beforehand, we also generate the candidate pairs graph G ( x x x ) for each example, as described in Section 4. The generation of these graphs for the CoNLL-2012 data sets is detailed in Section 7.2. Observe that document trees are not given in D .
Thus, we assume that these structures are latent and use the latent structured perceptron algorithm (Sun et al. 2009; Yu and Joachims 2009; Fernandes and Brefeld 2011) to train our models.
 using a specialization of the document tree predictor, the constrained tree predictor
F H ( x x x ).
 by removing intercluster arcs regarding the correct clustering y y y from the candidate pairs one arc that connects the artificial node to the first mention of the cluster. Hence, the constrained document tree can only include arcs between mentions that lie in the same cluster plus one arc from the artificial root node to each cluster. We use the constrained document tree  X  h  X  h  X  h as the ground truth in the current perceptron iteration. such cases are rare, given that the recall of the used sieves is approximately 90%. More-clustering given the candidate arcs.
 margin version of the document tree predictor. This modified predictor uses a large loss function into the prediction problem. As is usual in such methods, the large margin training improves the quality of the learned model. In Section 8, we present experimen-given by where ` r is a non-negative loss function that measures how much a document tree h h h iteration. The loss function measures the impurity in the predicted document tree, and
C is a loss parameter tuned on the development set. In our model, we use a loss function that simply counts how many arcs are different in a given tree h h h compared to the current iteration ground truth  X  h  X  h  X  h , that is,
This large margin trick is only applied during training because its purpose is to learn a model that separates the ground truth tree from any alternative tree by a distance proportional to the loss of the alternative tree. The greater the loss ` current iteration.
 arc sets and, due to the loss function, different arc costs. Nevertheless, the optimization algorithm is identical in both cases because the loss function can be factored along the original arc weights. Then, we use the Chu-Liu-Edmonds algorithm to predict the optimum tree on the graph with modified arc weights. Different loss functions can be used in the large margin predictor. However, if the loss function does not factor along arcs, the prediction problem will be different, and, consequently, another optimization algorithm will be required. For instance, to use the CoNLL metrics as loss functions may be practically infeasible. These metrics are based on very strong, global dependencies along the output structure, and the resulting optimization problem will thus be much harder. Our simple loss function relies on the latent trees to be effective and computa-tionally efficient.
 that is, ceptron algorithm for the mention clustering subtask. Similar to its univariate counter-part (Rosenblatt 1957), the averaged large margin latent structured perceptron is an 814 on-line algorithm that iterates through the training set. For each training instance, it uses the given input and the current model estimate to perform three major steps: (i) a an output prediction  X  h  X  h  X  h ; and (iii) a model update based on the difference between the predicted output features and the current iteration ground truth features. We use the averaged structured perceptron, as suggested by Collins (2002), because it provides a more robust model.
 assist in the target task, which is clustering. Thereafter, for the prediction of an unseen graph by applying the Chu-Liu-Edmonds algorithm. Its output, in turn, is fed to F to finally give the predicted clusters. 7. Empirical Evaluation Setting
We evaluate our system using the data sets from the CoNLL-2012 Shared Task. In this adopted in the experiments for different steps of our system, including candidate pair generation, basic feature setting, and entropy-guided feature induction. We also describe the minor adaptations that are necessary for each language. Finally, we present the evaluation metrics used to assess our system. 7.1 Data Sets
The CoNLL-2012 Shared Task is dedicated to the modeling of unrestricted coreference resolution in three languages: Arabic, Chinese, and English. Its data sets are provided by the OntoNotes project (Weischedel et al. 2011). In addition to coreference information, the shared task data sets contain various annotation layers, namely, part-of-speech (POS) tags, syntactic parses, word senses, named entities (NE), and semantic roles (SRL).
The task consists of the automatic identification of coreferring mentions of entities and events given the predicted information on other layers.
 and test. We report our system performances on the development and test sets. The development results are obtained using systems that have been trained only on the training sets. However, the test set results are obtained by training on a larger data set that is obtained by concatenating the training and development sets. During training, we use the gold standard input features, which produce better performing models than do those trained on the automatic values. We believe that the use of gold values during training avoids the additional noise introduced by automatic features. However, during evaluation, we always use the automatic values provided in the data sets. 7.2 Candidate Pair Generation
The input for the prediction problem is the candidate pairs graph G ( x ). Recall that for a consider the complete graph for each document, where every mention pair would be an option for building the document tree. However, for a document with many mentions, easily identified as unnecessary or even incorrect. Thus, we filter the mention pairs that are less likely to be coreferent out of the candidate pairs graph.

Lee et al. (2013) to English coreference resolution. Lee et al. propose a list of handcrafted rules that are sequentially applied to mention pairs to iteratively merge mentions into entity clusters. These rules are termed sieves because they filter the correct mention pairs. In Lee et al. X  X  system, sieves are ordered from higher to lower precision. However, important. The objective here is to build a small set of candidate arcs that show good recall. Additionally, we do not want sieves that are strongly language dependent, as our target is multilingual coreference resolution. Hence, we select the most general of Lee et al. X  X  sieves. The resulting sieves can also be directly applied to the Arabic and
Chinese data sets provided in the CoNLL-2012 Shared Task. Therefore, given a mention one of the following conditions holds: 1. Distance  X  the number of mentions between i and j is not greater than a 2. Named Entity Alias  X  i and j are named entities of the same kind, plus: 3. Head Word Match  X  the head word of i matches the head word of j . 4. Shallow Discourse  X  shallow discourse attributes match for both mentions. 5. Pronouns  X  j is a pronoun, and i has the same gender, number, speaker, and 6. Pronouns and NE  X  j is a pronoun, and i is a compatible pronoun or named versions of the ones proposed by Lee et al. (2013). Sieve 1 is introduced by us to raise recall while avoiding strongly language-dependent sieves. 7.3 Basic Feature Setting coreference strength of the mention pair connected by the arc. These features provide lexical, syntactic, semantic, and positional information. One of the semantic features is the prediction of the baseline system proposed by dos Santos and Carvalho (2011), which classifies the candidate arc as either coreferent or not. The other features have 816 been adapted from previously proposed features (Ng and Cardie 2002; Sapena, Padr  X  o, and Turmo 2010; dos Santos and Carvalho 2011).
 identifies each feature. The Type column indicates the value type of each feature, for how many basic features correspond to each description.
 (2002) binding constraints and maximal NP, this type of feature can be used in our system. An arc that activates one or more negative features is highly unlikely to be in the solution. In other words, if a mention pair has the value yes for one or more negative features, the two mentions are unlikely to be coreferent. When using negative features, the structured perceptron will most likely learn negative weights for these features.
Consequently, arcs that activate negative features have a lower chance of inclusion in the solution because these arcs tend to decrease the coreference tree weight. 7.4 Language Specifics
Our system can be easily adapted to different languages given the basic features. In our experiments, only minor changes are needed to train and apply the system to three different languages. The adaptations are due to (i) the lack of input features for some languages; (ii) the different POS tagsets across data sets; and (iii) the creation of static lists of language-specific pronouns. The necessary adaptations are restricted to only two preprocessing steps: basic features and coreference arcs generation.
 or Chinese data sets. The Arabic data set does not contain named entity, semantic role labeling, and speaker annotations. Therefore, for Arabic, we do not derive the following basic features: Sy9, Se3, Se4, Se5, Se6, Se7, and P3. For Chinese, information related to named entities is not provided. Thus, we do not derive the following basic features: Se3,
Se4, and P3. Additionally, the Chinese data set uses a different POS tagset. Hence, some mappings are used during the basic feature derivation stage.
 candidate pair generation step. For Chinese, we do not use Sieve 5, and, for Arabic, we only use Sieves 1, 3, and 6. Sieve 6 is not used for the English data set because it is a specialization of Sieve 5. The first sieve threshold is 4 for Arabic and Chinese and 8 for
English. These values have been chosen to optimize recall in the development sets and simultaneously fit the training data sets in memory.
 of language-specific pronouns. In our experiments, we use the POS tagging information and the gold entity clusters to automatically extract these pronoun lists from the training data.
 mentions. Although such mentions are never coreferent in Arabic or English, the Chi-nese data sets include many nested coreferring mentions. Hence, in the latest version of our system, we include such arcs for the Chinese language. 7.5 EFI
Using EFI, we can automatically generate feature templates without human effort, which allows us to easily experiment with different template sets. In our experiments, we use 70 basic features and apply EFI to generate feature templates from them. Only 58 basic features appear at least once in the template set of any of the three languages.
In decreasing frequency order, the ten most frequent basic features in the templates are Se1, L13, L3, L1 j , Se7, Sy3 j , L1 i , Se4, Sy12 i , and Sy3 to the root in the EFI auxiliary decision tree. When following our template generation method, they are the most discriminative features. Twelve of the 70 basic features do not appear in any template, namely, Sy4, Sy7 j , Sy9, Se5, and Se6. Thus, these features 818 are not used when training with the structured perceptron algorithm. Note that features Sy4 and Sy7 j carry some information that is already present in other features, such as
Sy10 and L12, respectively. Features Sy9, Se5, and Se6 are all derived from the SRL basic features. Therefore, when following our template generation strategy, SRL information contributes little to the coreference resolution task.

EFI to the training corpus of that language. However, merging different template sets usually produces better results, even when merging template sets of different lan-guages. For instance, for the Chinese and Arabic languages, merging their templates sets with a template set generated for the English language produces an improvement of approximately 2% on the MUC F-score. We believe this behavior to be due to the greedy nature of the decision tree induction algorithm used in EFI and the variation automatically generated feature templates are not optimal.
 merging the output of two independent EFI executions. These two runs are based on two different training data sets comprising (a) the mention pairs produced by Sieves 2 X  5; and (b) the mention pairs produced by all sieves. For Chinese and Arabic, the template sets automatically generated using the corresponding training data sets are merged with template set (a) generated for the English language. The final set for Chinese comprises 197 templates, and the final set for Arabic comprises 223 templates.
 basic features, the size of the training set, and the settings of the decision tree algorithm.
The number of feature templates has a direct impact on memory use, as increasing the number of templates increases the number of binary contextual features instantiated.
Hence, our main restriction when automatically creating and merging feature template sets is memory. Using the template sets previously mentioned, we are able to train our English system, which corresponds to the largest data set, using approximately 48 GB
RAM. For the Arabic and Chinese systems, less than 20 GB RAM is used during training. 7.6 Evaluation Metrics information is highly faceted, and the value of each facet varies substantially from one application to another. Thus, when reporting and comparing coreference system performances, it is very difficult to define one metric that fits all purposes. Therefore, we follow the methodology proposed in the CoNLL-2012 Shared Task to assess our systems, as it combines three of the most popular metrics. The metrics used are the link-based MUC metric (Vilain et al. 1995), the mention-based B 1998), and the entity-based CEAF e metric (Luo 2005). All these metrics are based on precision and recall measures, which are combined to produce an F-score value. The mean F-score of these three metrics gives a unique score for each language. Additionally, when appropriate, the official CoNLL-2012 Shared Task score is reported, which is the average of the F-scores for all languages. We denote this metric as the CoNLL score . methodologies, such as the ones used in the MUC and ACE evaluations, consider approximate matching of mention spans. However, the CoNLL-2012 Shared Task eval-uation considers only exact span matching.
 to keep our results comparable with most of the results reported in the literature. It is important to note that, in early 2014, a committee of researchers revised some of the evaluation metrics and released a new version of these scripts. changes the absolute scores, the ranking of the top performing systems, including ours, remain the same. 8. Empirical Results In this section, we present nine sets of empirical findings on the CoNLL-2012 Shared
Task data sets. In Section 8.1, we demonstrate our system X  X  overall quality and compare it with state-of-the-art systems. In Section 8.2, we assess the mention detection step. Sec-tion 8.3 details the results of the candidate pair generation step. In Section 8.4, we assess EFI, showing that this method significantly improves the resulting system performance.
In Section 8.5, we present experimental evidence that latent trees improve the prediction performance of our system compared to simple static structures. In Section 8.6, we show that the root loss value also significantly improves our system performances. Section 8.7 presents results that show the contribution of the large margin trick. In Section 8.8, we show that by enhancing our Chinese modeling with nested mentions, we achieve state-of-the-art performance for this language. Finally, in Section 8.9, we present a detailed analysis of the different types of errors in our system output.
 cessor and 48 GB RAM. We report the processing times for the English data sets. For the other languages, these times are proportionally lower. First, regarding training, the feature induction procedure takes 27.4 minutes; loading the training data set and gener-ating features from the learned templates take 7.5 minutes; and the training algorithm takes 57.5 minutes. The test procedure on the development set takes 6 seconds to load the data set, 45 seconds to load the model and to generate features from templates, and 30 seconds to predict the output structures for all documents. Most of these procedures could be further optimized in terms of both execution time and memory use.
 8.1 million parameters; the Chinese model has approximately 9.7 million parameters; and the English model has approximately 15.5 million parameters. 8.1 State-of-the-Art Systems
In Table 3, we present the per-language CoNLL scores of the best performing systems on the CoNLL-2012 Closed Track Shared Task test sets. The first row of this table corresponds to the last version of our system, and the second row corresponds to our official entry in the CoNLL-2012 Shared Task. The difference between these two versions is the inclusion of candidate arcs linking nested mentions for the Chinese language. By including such arcs, the score increases by almost 4.5 points for that language. The last two rows of this table correspond to the competitors that are ranked second (Bj  X  orkelund and Farkas 2012) and third (Chen and Ng 2012) in the shared task. Our system obtains the best score for each language.
 are similar. However, the performances on the Arabic language are much lower. Given the smaller size of the Arabic training corpus, this reduction is expected. 820
Shared Task test set are presented in Table 4, where we report the recall (R), precision (P), and F-score (F) for the three metrics considered in the CoNLL-2012 Shared Task.
Additionally, in the final column of this table, we present the mean F-score over the three metrics, which gives a unique per-language score. In these results, we see that the runner-up system outperforms our system by approximately 1.4 points on both the
MUC and B 3 F-scores. However, our system outperforms theirs on CEAF 5 points. In the end, our system achieves a mean score that is 0.67 point higher than Bj  X  orkelund and Farkas X  system. The third-ranked system (Uryupina, Moschitti, and
Poesio 2012) scores much lower, being competitive only on the B achieves a substantially higher recall.
 are presented in Table 5. For this language, our system is competitive on all metrics and particularly on MUC, where our system outperforms all competitors by more than 2.5 points of F-score. Note that this result is achieved by considering the nested mentions.
 the English language. Our system outperforms all others by more than 2 points on the mean score. Moreover, it achieves the best F-scores on the three metrics used, being outperformed only by Martschat et al. X  X  system on the B 3 values. These achievements are sound, especially considering the large research effort applied to the English language. 8.2 Mention Detection
The first step of our coreference resolution system is mention detection, where we seek to detect all possible mentions in the given document. In this preliminary step, recall is much more important than precision because missing mentions correspond to unre-coverable errors for the subsequent steps. Moreover, in the CoNLL-2012 unrestricted coreference resolution task, singleton mention clusters are treated as precision errors.
Thus, the performance of our mention detector must be checked only on the set of non-singleton mentions.

Hence, for each model, we assess mention detection twice: before and after the men-tion clustering step. The first evaluation is performed immediately after the mention detection step but before the mention clustering step, when our system has neither identified nor excluded singleton mentions. The second evaluation is performed on the final output of our system, when clusters comprising only one mention have been deleted. In Table 7, we present these performances on the development sets of the three languages. The Total column indicates how many mentions are annotated in the gold standard data set, which comprises non-singletons only. The Extracted column indicates the number of mentions detected by our system, and the Correct column indicates how many of them are correct. The columns R, P, and F correspond, respectively, to the recall, precision, and F-score of mentions. As in the CoNLL-2012 Shared Task, a mention is considered correct only when its exact span has been detected.
 822 for all languages: our system extracts many singleton mentions because it privileges recall over precision. Conversely, it correctly detects approximately 90% of the non-singleton mentions for all languages. After the mention clustering step, the mention detection precision increases because many correctly identified singletons are excluded.
Of course, some non-singleton mentions are wrongly identified as singletons in the mention clustering step, and there is a consequent drop in recall. 8.3 Candidate Pair Generation The candidate pair generation step is responsible for creating the candidate pairs graph
G ( x x x ) given the set of detected mentions. Our approach to this subtask is based on the sieves proposed by Lee et al. (2013). The purpose of using sieves in this step is twofold.
First, when generating G ( x x x ), we want to include only the arcs that link mentions that are likely to be coreferent to avoid dealing with a dense graph in the subsequent steps, which involve arc feature generation and solving maximum branching problems on
G ( x x x ). Second, we do not want to generate too many precision errors for the subsequent steps by linking mentions that are not likely to be coreferent. Conversely, the sieves can-not be too restrictive, or they would miss too many intracluster arcs and put coreferent mentions in different connected components. If two coreferent nodes m to different connected components in G ( x x x ), the coreference tree predictor is unable to generate a tree on G ( x x x ) containing the coreferent nodes m trade-off between the restrictiveness of the sieves and the coreference resolution recall. corresponds to the upper bound for our coreference resolution system when this graph is given as input to the tree predictor.
 the MUC recall of the candidate arcs for the Arabic, Chinese, and English development and the overlap among them. It is important to note that, for each document in the development set, one distinct candidate pairs graph is generated. Hence, in Tables 8 X  10, we show an overall assessment of the graphs generated for all documents in the respective development sets. The first row of each table corresponds to the case where all graphs contain all possible candidate pairs, which means filtering no arcs out. The recall errors in this case are due to missing mentions, which means that these errors occur in the mention detection step. In addition to recall, for each sieve set configuration, we report the recall percentage (% Recall), which gives the percentage of the maximum recall (first row) achieved by the configuration. The tables also present the number of generated candidate arcs and the percentage of arcs in relation to the maximum number of arcs. Note that for the Chinese language, using only 6.1% of the possible arcs, we achieve 92.2% of the maximum recall. For the English language, we achieve 93.7% of the maximum recall using only 10.6% of the possible arcs. These results demonstrate that our sieve sets for these two languages achieve a good trade-off between the restric-tiveness of sieves and the coreference resolution recall. For the Arabic language, there is still considerable room for improvement in the aforementioned trade-off. 8.4 EFI
We use entropy-guided feature induction to automatically generate nonlinear features by conjoining the 70 basic features. In this section, we compare our EFI-based system with systems trained using basic features alone. It is important to note that these 70 basic features include several complex features. Some of these features are even conjunctions of simpler basic features, and others provide complex task dependent information, such as head words and agreement on number and gender. These 70 basic features were manually generated by domain experts and encode valuable coreference information. alone (upper half) and the performance of our EFI-based system (lower half). We observe that the CoNLL score improves impressively, by 8.54 points, when using EFI 824 to generate new features. Moreover, EFI consistently outperforms the baseline on all languages for all metrics.

This process is expensive and highly language-dependent. To assess EFI X  X  inductive power, we perform another experiment. We randomly remove 30% of the original basic features, retaining only 49 of the 70 basic features. Next, we evaluate the performance impact of this feature reduction on the English development set. Again, we evaluate two systems: one that uses only basic features and another that uses EFI to generate features.
We repeat this procedure 20 times and average the results. In Table 12, we present the averaged results for the English development set. The final column of this table presents the standard errors of the three-metric mean. We can see that EFI improves the mean score by almost 6 points.
 different template sets, as described in Section 7.5. For this experiment, to simplify the comparison, we use only the template set derived using all English sieves. 8.5 Latent Trees
One key modeling aspect of our method consists of representing coreference clusters coreference clusters is linguistically and computationally plausible. Here, we present experimental evidence that latent trees are better than a simple static structure in terms of prediction performance. Our findings are aligned with the results reported by Yu and Joachims (2009), who compare undirected latent trees to correlation clustering. simple approach to generate static representations of the coreference clusters in the training set. For a given coreference cluster, we generate a chain of mentions by con-necting each mention to the previous mention in the same cluster, as in Soon, Ng, and
Lim (2001). After generating a chain for each cluster in a document, the first mention the perceptron algorithm learns to predict chains of coreferring mentions instead of general trees. However, for some clusters, due to missing arcs that are filtered out in the candidate pair generation step, it is impossible to generate a chain. In these cases, we connect a mention to the closest previous mention such that the two mentions are in the same cluster, and there is an arc connecting them in the training set. In this experiment, we do not need to use the Latent Structured Perceptron training algorithm. Instead, we use the ordinary Structured Perceptron.
 opment sets: the system trained with static chains to represent coreference clusters and our system based on latent trees. We see that latent trees improve the CoNLL score by more than 1 point. Additionally, improvements are observed on all languages for almost all metrics, except for Arabic MUC. Among the three languages, English presents the largest improvement, more than 2 points, when latent trees are used. In contrast, the impact of latent trees for the Arabic corpus is marginal.
 latent structures present another benefit. Latent structures simplify modeling be-cause they avoid the need for a heuristic to derive static structures. Latent struc-tures are automatically derived and evolved during training, guided by the annotated clusters. 8.6 Root Loss Value
Just as some coreference metrics can be more important than others for certain ap-plications, precision and recall have different values for applications. Specifically, for the CoNLL score X  X hich is based on the F  X  = 1 score X  X he balance between precision and recall is important. For this reason, we introduced an important parameter in our system: the root loss value . This parameter specifies a different loss function value for 826 outgoing arcs in the artificial root node. Observe that, in a document tree, each arc from the root node corresponds to a cluster. The effect of a root loss value larger than one parameter can be used to adjust the balance between precision and recall.
 velopment sets when we set this parameter to one, which is equivalent to not using this parameter at all. We observe that in this case, the differences between the recall and precision values are very high on all metrics and languages, lowering the F-score values. Using the development sets for tuning, we set the root loss value to 6, 2, and present the performances using these settings. We observe that this parameter sub-stantially improves the balance between precision and recall, consequently increasing the F-score values. Its effect is accentuated on Arabic and Chinese because the unbal-ancing issue is worse on these languages. The increase in the CoNLL score is nearly 4 points. 8.7 Large Margin and Averaging
In Figure 7, we show the impact on the MUC F-score provided by the large margin trick using our choice for the loss function and the averaged perceptron. Throughout the training procedure, we report the per-epoch performances of the current model on the English development set during the first 50 epochs. Each epoch corresponds margin averaged perceptron with the parameter C equal to 2,000, which is used to train our best model. Second, we simplify by using the averaged perceptron with no margin . Using the margin trick, we observe a consistent improvement in all epochs, and in the final model, we obtain a significant improvement of more than 1%. We also can see that when not using the averaging procedure, the algorithm eventually achieves the optimum performance, as occurs after the 25th epoch. However, without averaging, the performance varies considerably from one epoch to another, which can be very harmful. Very similar behavior is observed for the Arabic and Chinese languages.
 the number of epochs used to train all models used in this work.
 8.8 Chinese Nested Mentions
Nested noun phrases such as (( the happy boy ) from Brazil ) are very common. Consid-ering either all nested noun phrases as coreferring mentions or only the outer noun phrases as mentions is an annotation design choice. In the CoNLL-2012 data sets, for both English and Arabic, only the outer noun phrases are considered as mentions. How-ever, in the Chinese newswire documents, nested mentions are annotated as coreferring (Chen and Ng 2012). Thus, in this work, we evaluate the effect of using arcs linking nested mentions for the Chinese language.
 is 0.41% in the training, 0.34% in the development, and 0.45% in the test subsets of the
Chinese data set. These percentages are calculated over the candidate arcs only (i.e., the arcs selected by the Chinese sieves). Thus, the computational impact of adding these arcs is negligible.
 provement provided by nested mentions. For that purpose, we select all correct arcs from the candidate ones and compute the MUC recall of these arcs. When nested mentions are not included in the candidate arcs, we obtain a MUC recall of 71.29%. However, when nested mentions are included, we obtain a MUC recall of 77.58%, which is a significant increase in the performance upper bound. These values are computed using the development set.
 system performance. The first row shows the results when arcs linking nested men-tions are included, and the second row shows the results when they are ignored. 828
Considering these arcs increases our system score on the Chinese language by almost 4 points. 8.9 Error Analysis
In this section, we provide statistics regarding the most common errors within the outputs of our structure predictor for the development set of each language. Three generation, and mention clustering. The errors in each step propagate to the next steps. three key steps. Remember that during mention detection and candidate pair genera-tion, recall is much more important than precision. Hence, we can observe that recall is indeed kept much higher than precision during these steps for all languages. For the Arabic and Chinese languages, the drops in recall are larger than are the drops for English in both preliminary steps. This behavior is because of missing features and sieves for these two languages and also because most of the features and sieves were originally proposed for the English language. In particular, the Arabic candidate pair generation step generates too many recall errors because it is based on only three sieves.
 for most of the precision errors during the mention detection step. Our system does not handle coreferring mentions of events, and it thus does not consider verbs when creating candidate mentions, which is responsible for many recall errors in this step. generated parse trees. This type of error is more frequent in the Arabic and Chinese corpora.
 cision, this step is able to discard a large portion of the candidate arcs, as depicted in Section 8.3.
 coreference tree predictor. Now, let us examine the errors generated by this predictor.
For this purpose, we compare each predicted document tree intracluster arcs and some additional artificial root arcs. For each incorrectly predicted arc ( i , j )  X   X  h  X  h  X  h that links non-coreferent mentions i and j , let ( i tree is computed on the graph generated in the candidate pair generation step, which already includes recall errors. Thus, the frequencies reported here are computed over the remaining errors.
 stand for singular proper noun, NN for singular noun, NNS for plural noun, NT for temporal noun, PRP for personal pronoun, PRP$ for possessive pronoun, PN for pronoun, and Root for the artificial root node, which means that the corresponding node is the root of its coreference subtree.
 language, 37.7% for Chinese, and 31.9% for English. The pleonastic it is a particularly problematic singleton in the English language. Indeed, for English, such cases roughly correspond to 14.2% of all singleton errors. In Table 18, we present the most frequent head POS tags for these singleton errors in each language development set. For Arabic and English, the order among these POS tags is very similar. The most frequent errors are pronouns, followed by proper nouns. However, in the English language, the propor-tion of pronouns is much higher than the proportion of proper nouns, most likely due 830 to the pleonastic it cases. Surprisingly, in the Chinese language, most singleton errors are nouns. 9. Conclusion
In this article, we describe a new machine learning system for multilingual unrestricted coreference resolution, based on two key modeling techniques: latent coreference trees and entropy-guided feature induction. We use the large margin structured perceptron as training algorithm. According to our experiments, latent trees are powerful enough to model the complexity of coreference structures in a document while rendering the learning problem computationally feasible. Our empirical findings also show that EFI enables our system to learn effective nonlinear classifiers while using a linear training algorithm.
 three languages: Arabic, Chinese, and English. To cope with this multilingual task, our system needs only minor adaptations due to certain language-dependent aspects of the used data sets. As far as we know, our system presents the best known performance on the three languages.
 individual parts of our system, providing important insights to researchers interested in coreference resolution. These results also highlight interesting aspects of our approach that can be explored for further improvements.
 local information is clearly not sufficient to model all dependencies involved in coref-erence resolution. It is necessary to consider more complex contextual features. Hence, in future work, we plan to include higher-order features and cluster-sensitive features.
Higher-order tree-based features have been successfully applied to dependency parsers (McDonald and Pereira 2006; Koo et al. 2010) and can be used to extend our corefer-ence system. Cluster-sensitive features can further extend our modeling by considering features that are more strongly adherent to the coreference task. However, as far as we know, there is as yet no structure learning system that considers such features. Thus, we must design new prediction algorithms to cope with this complex context.
 Acknowledgments References 832 834
