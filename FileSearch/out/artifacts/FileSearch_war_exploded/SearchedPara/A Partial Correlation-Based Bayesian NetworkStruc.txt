 Learning the structure of Bayesian network from dataset D is useful, unfortu-nately, it is an NP-hard problem [2]. Consequently, many heuristic techniques have been proposed. One of the most basic search algorithms is a local greedy hill-climbing search over all DAG structures. The size of the search space of greedy search is super exponential to the number of variables. One of the ap-proaches uses constraints placed on the s earch to improve efficiency of the search, such as the K2 algorithm [3], the SC algorithm [4], the MMHC algorithm [15], the L1MB algorithm [7].

One drawback of the K2 algorithm is that it requires a total variable order-ing. The SC algorithm first introduces local learning idea and proposes two-phase framework including a Restrict step and a Search step. In the Restrict step, the SC algorithm uses mutual information to find a set of potential neighbors for each node and achieves fast learning by r estricting the search space. One draw-back of the SC algorithm is that it only allows a variable to have a maximum up to k parents. However a common param eter k for all nodes will have to sac-rifice either efficiency or quality of reconstruction [15]. The MMHC algorithm uses the max-min parents-children (MMPC) algorithm to identify a set of po-tential neighbors [15]. Experiments show that the MMHC algorithm has quite high accuracy, one drawback of which is that it needs conditional independency tests on exponentially large conditioning sets. The L1MB algorithm introduces L1 techniques to learn DAG structure and uses the LARS algorithm to find a set of potential neighbors [7]. The L1MB algorithm has good time performance. However, the L1MB algorithm can des cribe the correlation between a set of variables and a variable, not the correla tion between two variables. Experiments show that the L1MB algorithm has low accuracy.

In fact, many algorithms, such as the K2, SC, PC [13], TPDA [1], MMHC, can be implemented efficiently with discrete variables, and are not applicable to the continuous variables straightforwardly. The L1MB algorithm has been designed for continuous variables. Howev er, its accuracy is not very high.

Partial correlation method can revea l the true correlation between two vari-ables by eliminating the influences of other correlative variables[16]. It has been successfully applied to many fields such as medicine [8], economics [14], and geol-ogy [16]. In causal discovery, it has been used (as transformed by Fisher X  X  z [12] ) as a continuous replacement for CI tests in PC algorithm. Pellet et al. introduced partial-correlation-based CI test into causation discovery with the assumption that data follows multivariate Gaussian distribution for continuous variables [9]. However, when the data doesn X  X  follow multivariate Gaussian distribution, can partial correlation be CI test?
Our first contribution is that we give the proof that partial correlation can be used as the criterion of CI test under linear simultaneous equation model (SEM), which includes multivariate Gaussian distribution as a special case. Our second contribution is that we propose an effective algorithm, called PCB (Par-tial Correlation-Based), which combines ideas from local learning with partial correlation techniques in an effective way. PCB algorithm works on the continu-ous variable setting with the assumption data generated by SEM. Computational complexity of PCB is O (3 mn 2 + n 3 )(nisthenumberofvariablesandmisthe number of cases). Advantages of PCB are that PCB has quite good time perfor-mance and quite high accuracy. The time complexity of our PCB is polynomially bounded by the number of variables. The third advantage of PCB algorithm is that PCB algorithm uses a relevance threshold to evaluate the correlation to al-leviate the drawback of SC algorithm (common parameter k for all nodes), and we also find the best relevance threshold b y a series of extensive experiments. Empirical results show that PCB outperforms the above existing algorithms in both accuracy and time performance.
 The remainder of the paper is structured as follows. In section 2, we present PCB algorithm and give computational complexity analysis. Some empirical results are shown and discussed in section 3. Finally, we conclude our work and address some issues about future work in section 4.
 PCB(Partial Correlation-Based) includes two steps: the Restrict step and the Search step. 2.1 Restrict Step The restrict step is analogous to the pruning step of the SC algorithm, the MMHC algorithm, the L1MB algorithm. In this paper, partial correlation is used to identify the candidate neighbors. To a certain extent, there is a corre-lation between each two variables, but the correlation is affected by the other correlative variables. Simple correlation method does not consider the influences, so it cannot reveal the true correlation between two var iables. Partial correla-tion can eliminate the influences of other correlative variables and reveal the true correlation between two variables. A larger magnitude of partial correlation coefficient means a closer correlation [Xu et al., 2007]. So partial correlation is used to select the potential neighbors. Before we give our algorithm, we give some definitions and theorems.
 Definition 2.1 [9] (SEM). A SEM (structural equation model) is a set of equa-tions describing the value of each variable X i in X as a function f X i of its parents Pa ( X i ) and a random disturbance term u X i : In our paper, without loss of generality, we simplify the function as linear, so we multiply a weight set W T X Here W X i and Pa ( X i ) are vectors, and W T X can obtain the equation(2): equation(2) is a special case of the general SEM described by equa tion(1). Distur-bances u X i are continuous random variables. Specially, when all u X i are Gaus-sian random variables, X subjects to multivariate Gaussian distribution. Then, partial correlation is a valid CI measure [9]. However,we want to deal with a more general case, when u X i are continuous but from arbitrary distribution. Can partial correlation be a valid CI measure? Definition 2.2 [9] (Conditional independence). In a variable set X ,two random variables X i , X j  X  X are conditionally independent given Z  X  X \ { Definition 2.3 [9] (d-separation). In a DAG G , two nodes X i , X j are d-separated by Z  X  X \{ X i ,X j } , if and only if every path from X i to X j is blocked by Z , denoted as Dsep ( X i ,X j | Z ). A path is blocked if at least one diverging or serially conn ected node in Z or if at least one converging node and all its descendants are not in Z. If X and Y are not d-separated by Z ,theyare d-connected,denoted as Dcon ( X i ,X j | Z ).
 Theorem 2.1. [12] In a SEM with uncorrelated errors (that means for two ran-dom variables X i ,X j  X  X , u X i and u X j are uncorrelated), Z  X  X \{ X i ,X j } , a partial correlation  X  ( X i ,X j | Z ) is entailed to be zero if and only if X i and X j are d-separated given Z .
 Definition 2.4 [10] (Perfect map). If the Causal Markov and Faithfulness con-ditions hold together, A DAG G is a directed perfect map of a joint probability distribution P ( X ), and there is bijection between d-separation in G and condi-tional independence in P : Definition 2.5 [5] (Linear Correlation). In a variable set X , the linear corre-lation coefficient  X  X i X j between two random variables X i , X j  X  X , provides the most commonly used measure to assess th e strength of the linear relationship between X i and X j is defined by where  X  X i X j ,denotes the covariance between X i and X j ,and  X  X i and  X  X j denote the standard deviation of X i and X j respectively.  X  x i x j is estimated by Here, m is the number of instances. x ki means k -th realization (or case) of X i , Definition 2.6 [10] (Partial correlation). In a variable set X , the partial correlation between tw o random variables X i ,X j  X  X ,given Z  X  X \{ X i ,X j } , the least-squares linear regression of X i on Z and of X j on Z , respectively. Partial correlation can be computed efficiently without having to solve the regression problem by inverting the correlation matrix R of the X .With R  X  1 =( r ij ), here R  X  1 is the inverse matrix of R ,wehave: In this case, we can compute all partial correlations with a single matrix inver-sion. This is an approach we use in our algorithm.
 Theorem 2.2. In a SEM with uncorrelated errors, when data is generated by the SEM no matter what distribution disturbances subject to, we can use partial correlation as the criterion of CI test.
 Prove: From Theorem 2.1 ,  X  X i ,X j  X  X ,  X  Z  X  X \{ X i ,X j } , the partial corre-lation  X  ( X i ,X j | Z ) is entailed to be zero if and only if X i and X j are d-separated given Z ;From Definition 2.4 there is bijection bet ween d-separation in G and correlation  X  ( X i ,X j | Z ) is entailed to be zero if and only if X i and X j are condi-tionally independent given Z . So we can use partial correlation as the criterion of CI test in a SEM with uncorrelated errors.
 Definition 2.7 (Strong relevance).  X  X i ,X j  X  X ,  X  Z  X  X \{ X i ,X j } , X i is strongly relevant to X j if the partial correlation  X  ( X i ,X j | Z ) &gt; = threshold . Definition 2.8(Weak relevance).  X  X i ,X j  X  X ,  X  Z  X  X \{ X i ,X j } , X i is weekly relevant to X j if the partial correlation  X  ( X i ,X j | Z ) &lt; = threshold . The outline of the Restrict step is shown in Fig.1. Input of the step is threshold k and a dataset D = { x 1 ,  X  X  X  , x m } of instances of X ,whereeach x i is a complete assignment to the variables X 1 ,  X  X  X  ,X n in Val ( X 1 ,  X  X  X  ,X n ). Each column of the dataset represents one variable. Output of the step is a set of potential neighbors PN ( X j )ofeach X j and the matrix of potential neighbors PNM .If PNM ( i, j ) is 1, that means X i is X j  X  X  potential neighbor. Otherwise, if PNM ( i, j )is0, X i isn X  X  X j  X  X  potential neighbor. Initially, PN ( X j )(potential neighbors of each variable X j ) is empty, all elements of PNM are set to 0 (step 1 ). Then we select a set of potential neighbors for each variable and obtain the final matrix of potential neighbors (from step 2 to step 9). For each pair of variables X i and X as X j  X  X  potential neighbor and set the value of PNM ( i, j ) to 1, otherwise set the value of PNM ( i, j )to0.Infact,  X  ( X i ,X j | Z )( i&lt;j )equals  X  ( X j ,X i | Z ), however, if there is strong correlation between them, we only set PNM ( i, j )to 1. PNM is upper triangular matrix and on the diagonal elements are 0. Because, Search step includes reverse-edge operator, by performing greedy hill-climbing search, the step can orient the edges properly. 2.2 Search Step After Restrict step, we find the candidate neighbors of each variable, then per-form a greedy hill-climbing search. We assume that we have fully observed (com-plete) data, which are continuous, that our goal is to find the DAG G that minimizes the MDL cost, MDL is defined as Themethodisusedin[7].Where m isthenumberofdatacases, n is the number of variables, Pa ( X i ) are the parents of node i in G , NLL ( i, Pa ( X i ) , X  ) is the negative loglikelihood of node i with parents Pa ( X i ) and parameters  X  , and  X   X  mle i = argmin  X  NLL ( i, Pa ( X i ) , X  ) is the maximum likelihood estimate of i  X  X  parameters. The term |  X   X  i | isthenumberoffreeparametersintheCPD (conditional probability distributions) for node i . For linear regression, |  X   X  i | = | Pa ( X i ) | , the number of parents.

Search step performs a greedy hill-climbing search to obtain the final DAG. We follow the L1MB implementation (also to allow for a fair comparison). The search begins with an empty graph. The basic heuristic search procedure we use is a greedy hill-climbing that considers local moves in the form of edge addition, edge deletion, and edge reversal. At each iteration, the procedure examines the change in the score for each possible move, and applies the one that leads to the biggest decrease in MDL score. The se iterations are repeate d until convergence. The important difference from st andard greedy search is that the search is constrained to only consider adding an edge if it was discovered by PCB in the Restrict step. The operator remove-edge just can be used to remove the edge added in the graph actually. Maybe the orientation of some edge is not right, if reversing the edge can lead to decrease in MDL score, t he operator reverse-edge should be used to reverse the edge. We terminate the procedure after some fixed number of changes failed to result in an improvement over the best score so far. After termination, the procedure returns the best scoring structure it encountered. 2.3 Time Complexity of PCB Algorithm A dataset with n variables and m cases is considered. For comparison with L1MB, we only consider time complexity of Rest rict step. Time complexity of Restrict step is O (3 mn 2 + n 3 ).

Computations of PCB algorithm mainly exist in calculating the correlation coefficient matrix R and calculating the inverse matrix of R .Matrix( n  X  n ) multiplication algorithm needs n 2 vector inner products and computational com-plexity of vector( n ) inner product is O ( n ), so computational complexity of ma-trix multiplication algorithm is O ( n 3 ) at most. From Definition 2.5 ,weknow that calculating the correlation coeffici ent of two variables needs 3 vector in-ner products, and the correlation coefficient matrix has n 2 elements, calculating the correlation coeffici ent matrix requires 3 n 2 inner products, for m cases, com-putational complexity of vector( m ) inner product is O ( m ), thus computational complexity of calculating the correlation coefficient matrix R is O (3 mn 2 ). We know that the computation of the inverse matrix and matrix multiplication are equal, so computational complexity of calculating the inverse matrix of R ( n  X  n ) is at most O ( n 3 ). We can get the conclusion that the total time complexity of Restrict step is O (3 mn 2 + n 3 ). 3.1 Networks, Datasets and Measures of Performance The experiments were conducted on a computer with Windows XP, Inter(R) 2.6GHz CPU and 2GB memory. All together 8 networks are selected from Bayes net repository (BNR) 1 except factors network. Factors network is syn-thetic. The networks, including their number of nodes and edges are shown as follows: 1.alarm (37/46), 2.barley (48/84), 3.carpo (61/74), 4.factors(27/68), 5.hailfinder(56/66), 6.insurance (27/52) , 7.mildew (35/46), 8.water (32/66).
Datasets used in our experiment are generated by SEM. We adopt the follow-ing two kinds of SEMs. The weights are generated randomly, generally, randomly distributed uniformly [9] or distributed normally[7]. We sampled the weights from  X  1+ N (0 , 1) / 4. Datasets sampled from SEM (1) belong t o multivariate Gaussian distribution and are continuous data. Datasets sampled from SEM (2) don X  X  belong to mul-tivariate Gaussian distribution and are also continuous data.

We employ two metrics to compare the algorithms: run time and structural errors. Structural errors include all of the errors including missing edges, extra edges, missing orientation and wrong orientation. The number of structural er-rors means the number of incorrect edge s in the estimated model compared to the true model[7]. 3.2 Experimental Results and Analyses We firstly evaluate the performance of PCB algorithm under the above two cases, with different sample sizes, thresholds and networks. Fig.2 shows the re-sults under SEM (2). X axis denotes networks. Y axis denotes the number of structural errors. The results of SEM (1) are omitted because of space. From Fig.2, we can see that threshold has a great effect on the performance of PCB algorithm. The results of different SEMs are similar. When the dataset size is the SEMs. For time performance, PCB (0.1) wins 5, ties 2, and loses 1 under SEM (1), wins 5, ties 3, and loses 0 under SEM (2). Under the two SEMs, the results are similar. For DAG, potential neighbors of each variable are all the other variables. In the Search step, because we set the maximum number of iteration to 2500, maybe 2500 is too small, the Search step may terminate before finding the best DAG, so the structural errors are more. Without the pruning step, time performance of DAG algorithm is worse than PCB (0.1), the reason is as follows: Search step examines the change in the score for each possible move. For without pruning, the number of potential neighbors for each variable is large, the number within consideration is also large, so the cost of search step is higher. (2) PCB (0.1) algorithm vs SC(5) and SC(10). PCB algorithm achieves both better time performance and higher accuracy almost on all the networks un-der all the SEMs. SC algorithm needs specify the maximum fan in advance, however some nodes in the true structure may have much higher connectivity than others, so a common parameter for all nodes is not reasonable. In addition, based on correlation coefficient SC algor ithm of DAGLearn software selects the top k (maximum fan) candidate neighbors and doesn X  X  consider symmetry of correlation coefficient, and this will lead to redundant information of potential neighbors, and will sacrifice either effi ciency or performance. PCB algorithm has not the above problems. From the section 2.3 we know that computational complexity of calculating the co rrelation coefficient matrix is O (3 mn 2 ); in order to select the top k candidate neighbors, we must sort each row of correlation coefficient matrix, comput ational complexity is n 3 , so the total complexity is O (3 mn 2 + n 3 ), which is equal to PCB restrict step ( O (3 mn 2 + n 3 )). However, total time performance of SC algorithm is worse than that of PCB (0.1). Due to the unreasonable selection of potential neighbors and redundant information of potential neighbors, the cost of the search step will be increased. So SC algo-rithm has worse time per formance and accuracy. (3) PCB (0.1) algorithm vs L1MB. PCB algorithm achieves both better time performance and higher accuracy on all the networks under all the SEMs.
L1MB algorithm adopts LARS algorithm to select potential neighbors. For a variable, L1MB selects the set of variabl es that have the best predictive accuracy as a whole, and L1MB evaluates the effects of a set of variables, not a single vari-able. Using the method to select potentia l neighbors has some shortcomings. The method can describe the correlation be tween a set of variables and a variable, not the correlation between two variables. There maybe exist some variables, which do not belong to the selected set of potential neighbors, but have strong relevance with the target variable. How ever, Partial correlation method can re-veal the true correlation between two variables by eliminating the influences of other correlative variables. PCB algorithm is based on partial correlation to se-lect potential neighbors and evaluates the effect of a single variable. So PCB algorithm is more reasonable, and experimental results also indicate that PCB algorithm has fewer structural errors.

PCB (0.1) has better time performance than L1MB. From section 3.4, we know that time complexity of PCB is O (3 mn 2 + n 3 )( n is the number of variables and m is the number of cases ). For L1MB, time complexity of computing the L1-regularization path is O ( mn 2 ) in the Gaussian case (SEM (1) and SEM (2))[7]. In addition, L1MB also includes computing the Maximum Likelihood parameters for all non-zero sets of var iables encountered along this path and selecting the set of variables that achieved the highest MDL score. So L1MB has worse time performance than PCB(0.1) under all the SEMs. (4) PCB (0.1) algorithm vs PC(0.05) algorithm. PCB (0.1) algorithm achieves both better time performance and higher accuracy on all the networks under all the SEMs. PC (0.05) algorithm has been designed for discrete variables, or imposes restrictions on which variables may be continuous. PC first identifies the skeleton of a Bayesian network and then orients the edges. However, PC algorithm may fail to orient some edges, and in our experiments, we take the edges as wrong. So PC algorithm has more structural errors. PC algorithm needs O ( n k +2 )CItests, k is the maximum degree of any node in the true structure[13]. Time complexity of CI test at least is O ( m ). So time complexity of PC algorithm is O ( mn k +2 ). PC algorithm has an exponential time complexity in the worst case. Time complexity of PCB (0.1) is O (3 mn 2 + n 3 ). Obviously, PC algorithm has worse time performance. (5) PCB(0.1) algorithm vs TPDA(0.05) algorithm. PCB algorithm achieves both better time performance and higher accuracy on all the networks. TPDA has been designed for discrete variables , or imposes restrictions on which vari-ables may be continuous. So TPDA algorithm has more structural errors. TPDA requires at most O ( n 4 ) CI tests to discover the edges. In some special case, TPDA requires only O ( n 2 ) CI tests[1]. Time complexity of CI test at least is O(m). So time complexity of TPDA is O ( mn 4 )or O ( mn 2 ). Compared with PC algorithm, TPDA algorithm has better time performance, however compared with PCB algorithm O ( mn 2 ), time complexity of TPDA algorithm is still high. So PCB (0.1) has better time performance than TPDA (0.05). The contributions of this paper are two-fold. (1)We prove that partial correlation can be used as CI test under SEM, which includes multivariate Gaussian distribution as a special case. We redefine the strong relevance and weak relevance. Based on a series of experiments, we find the best relevance threshold. (2) We propose PCB algorithm, and theoretical analysis and empirical results show that PCB algorithm performs better than the other existing algorithms on both accuracy and run time.

We are seeking a way of automatically determining the best threshold. We will also extend our algorithm to higher dimension and larger datasets. The research has been supported by 973 Program of China under award 2009CB 326203, the National Natural Science F oundation of Chin a under a ward 61073193 and 61070131. The authors are very grateful to the anonymous reviewers for their constructive comments and suggestions that have led to an improved version of this paper.

