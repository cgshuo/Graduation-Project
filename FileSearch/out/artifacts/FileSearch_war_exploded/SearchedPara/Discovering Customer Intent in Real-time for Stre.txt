 Businesses require the contact center agents to meet pre-specified customer satisfaction levels while keeping the cost of operations low or meeting sales targets, objectives that end up being comple-mentary and difficult to achieve in real-time. In this paper, we de-scribe a speech enabled real-time conversation management system that tracks customer-agent conversations to detect user intent (e.g. gathering information, likely to buy, etc.) that can help agents to then decide the best sequence of actions for that call. We present an entropy based decision support system that parses a text stream generated in real-time during a audio conversation and identifies the first instance at which the intent becomes distinct enough for the agent to then take subsequent actions. We provide evaluation results displaying the efficiency and effectiveness of our system. H.3.3 [ Information Search and Retrieval ]: Search process, Se-lection Process Algorithms, Design, Performance Real-time, Contact Center Conver sation, Customer Intent Recog-nition
Customer contact centers (also known as call centers) were started as a low-cost alternative to maintaining physical centers for serving existing customers of a business. However, with a view to increasing profits and to make maximum utilization of the agents time, other activities like lead generation, taking orders, etc. were slowly added to the tasks performed. In this paper, we present, AgentAid -a real-time audio conversation management system that will streamline service desk conv ersations by automatically recog-nizing customer intent regarding a service and immediately prompt the agent with relevant information. AgentAid is designed to help service desk agents in solving the customer X  X  information need while ensuring operational effectiveness. AgentAid achieves this by prompting the agent, at appropriate time periods during the call, with information which is useful for successfully closing the call. This information could be in the form of queries which the agent can ask the customer to gain a better understanding of the cus-tomer X  X  problem or product/service recommendations for effective cross-sell/up-sell. Two key aspects of the AgentAid system are a) The information which is displayed to the agent is generated in real time, i.e., as the agent-customer conversation is in progress b) The system, based on the conversation context, automatically generates this information without requiring any input from the agent. The AgentAid system consists of two main modules namely the Deci-sion Module and the Query Builder .

The Decision Module X  X  aim is to identify the customer X  X  intent in real-time while an audio conversation between a customer and contact center agent is taking place. To do this, we start by con-verting the on-going call audio to text using an Automatic Speech Recognizer(ASR). We then segregate the streaming text into dis-tinct portions and assign a set of predefined categories that reflect the intent of the customer. This is depicted in Figure 1 where the Decision Module has assigned call segment d 0 to d 1 to the  X  X ar unavailability issue X  category and d 2 to d 3 to the  X  X igh rates X  cat-egory. The critical task for Decision Module is to identify intent points i.e. earliest point in the streaming conversation where the customer X  X  intent becomes evident with reasonable degree of con-fidence. Once this is done and the call segment assigned a category (intent category) relevant information is fetched from the backend database using the Query Builder and presented to the agent. For instance, the moment the Decision Module detects (step 1 in Fig-ure 1) that a particular call segment is of type  X  X ar unavailability issue X  it invokes the Query Builder (step 2 in Figure 1) and passes the relevant call context (i.e. portion of the ASR that was catego-rized as  X  X ar unavail ability issue X  t 25 to t 42 ). The Query Builder uses this call context to automatically build a query for the relevant backend (step 3 in Figure 1) database (e.g. database containing current fleet inventory at the location). The result of the query is then displayed to the agent in realtime (step 4 in Figure 1). In this paper, we describe in detail the Decision Module component of the AgentAid system. The Query Builder module of our system is explained in our previous work [21].

The intent identification is done by using a classifier that not only identifies customer X  X  intent but also discovers the intent points -points at which intent changes. Typically classifiers make decisions after the entire document. However, a call center conversation can be visualized as a continuously evolving document that is stream-ing into the system. Herein lies the first challenge -The text stream is non-stationary i.e. distribution of features change with time and therefore the system must determine whether it has enough infor-mation to make the decision that an intent point has been reached and the call segment can be labeled with a pre-specified intent cat-egory. Determining the customer X  X  intent early in the call can help the agent take appropriate action (e.g. if the system prompts the agent that the customer X  X  intent is to shop around for a good price it might be helpful to provide some kind of price comparison or offers upfront). Even if we can only narrow down to a broad in-tent category e.g. interested in opening a checking account and not to the particular type, a student X  X  checking ; the quicker mapping of information could help reduce the average time for the handling the call. The second challenge arises from the fact that the interactions between customer and agent is over a real-time audio call and there-fore the customer expects satisfactory answers and suggestions in real-time. The third challenge is with respect to handling the high noise in audio to text translations done by most ASRs.

The remainder of the paper is organized as follows. In the next section, Section 2, we list related research efforts. In Section 3, we formulate the problem for efficiently finding customer intent in realtime conversation. We then show how we can assign intent categories and find intent points in Section 4. Section 5 presents detailed evaluation results over both synthetic and real-life data. We summarize our contributions in Section 6.
There exists prior work on building classifiers for call classifica-tion [16] and routing customer calls based on the customer response to an open ended system prompts such as, "Welcome to xxx, How may I help you?" [12]. In call classification and routing the com-plete call (document) is collected before a decision is made. In the case of online data streams where the individual class distribution does not remain stationary incremental learning is used [9]. The approach usually taken to capture non-stationarity is to take a time window or weigh the data depending on age or relevance. The size of the window tries to balance the adaptivity and generalization of the classifier [19]. The concept-adapting very fast decision tree learner [8] applies the very fast decision tree learner [4] to build the model incrementally using a sliding window of fixed size. Tech-niques to dynamically decide the window horizon to incorporate the relevance of the data stream have been proposed in [2]. Support Vector Machines (SVMs) have been used with a dynamic window size in [10]. The Incremental On Line Information Network [3] dy-namically adjusts the window size and training frequency based on statistical measures.

In above methods the solution is to learn a classifier that for-gets its past and learns the new distribution based on the changed concepts. This model is not suited in the call center conversation model where the document is evolving but a clear intent has not been established. Evolving the current state of classifier as more data streams in becomes necessary. To the best of our knowledge we are not aware of any prior work that looks at issues related to automatically finding intent points in real time conversations.
The primary challenge faced by AgentAid is to identify a par-ticular conversational segment and classify it in realtime. Often the boundaries of these segments are fuzzy. For any real time call analysis, determining these boundaries quickly and effectively and making a classification decision is very important. The difficulty is compounded by the fact that segment boundaries are not available even at training time and often vary from call to call. In practice class labels for these particular segments are available only at the call level (i.e. entire document level) during training time. These labels are usually generated by contact center quality analysts as a post operational quality check exercise. The analysts after hearing portions of the call provide labeling at the call level only. In or-der to provide real time assistance to the agent we have to identify call segments and classify them as quickly as possible with high confidence. In the following section we describe an entropy based approach that identifies call segment boundaries (intent points) at which customer X  X  intent changes even when no segment boundary information is available at training time. We further propose to use the intent points as a feedback loop to retrain the classifier on par-tial call segments so as to improve the accuracy of the classifier. The Problem: In our system conversations of contact center agents are passed in real time through an ASR system which outputs streaming text corresponding to a transcription of the conversation. At the end of every speaker utterance the ASR outputs the word uttered by the speaker [6]. Hence, at a given time t i the transcript of the conversation till t i is available. When such text is passed to a classifier at the end of every word the classifier potentially has a new decision. It can alter the estimated confidence about the customer X  X  intent at t i  X  1 or it can detect a completely new in-tent. We assume that words in a conversation w 1 ...w n are gen-erated at time stamps t 1 ...t n . Our classifier makes the decision based on the conditional probability model p ( C x /a 1 ,a where  X  A = a 1 ...a n are features derived from the incoming words w 1 ...w n in the conversation and C x takes on a value from the set of class labels C . It is important to note that the features sary for classification, could all have been collected before the call reaches its end. In effect this means that even a partial call may be sufficient for the classifier to make a confident decision. Below we first define an intent point and then formally state the problem.
Intent Point: is the time instance t when the classifier has the least uncertainty U ( s t ) about the intent category to assign to the incoming conversation s t = w 1 ,w 2 ,...,w t . Any decision made before or after this point has non-decreasing uncertainty.
Problem Definition: In text classification for (streaming) con-versational text the intent iden tification cannot wait till the com-plete document has been seen. Hence, in conversational text the classifier needs to take two decisions: (1) the classification deci-sion to determine the customer X  X  intent in the conversation (2) the point in time where to take the classification decision (determining the intent point).
In information theory, the information entropy or Shannon en-tropy of a random variable X is the measure of uncertainty associ-ated with X . The information entropy H ( X ) of a discrete random variable X , that can take values in the set { x 1 ,x 2 ,...,x fined to be where, I ( X ) is the self information of X , which is itself a ran-dom variable and p ( x i ) is the probability mass function of X .
Let C x be the class predicted for an instance x by a probabilistic classifier such as logistic regression [13]. C x takes on a value from the set of class labels C . The classifier associates a distribution with C x . We use entropy of the class random variable C measure the uncertainty U ( x ) associated with the classification of x .
 where, p ( C x ) is the posterior probability of class C x stance x as predicted by the probabilistic classifier. A high value of U ( x ) implies that the classifier is uncertain about the classification of instance x .

We make use of this uncertainty measure for determining the text segment that must be chosen from the test document to determine its class. To make computations feasible, we restrict the choice of text segments to multiples of units that are sentences. Further, we restrict the choice of segments to those that start at the beginning of the conversation. We refer to the segment comprising the first k sentences from a conversation S as s k . For instance, given the conversation in Figure 1 we consider the following as candidate segments
Given a test conversation S consisting of N sentences, we con-sider two methods for determining its true class C ( S ) on online streaming text
GlobalEntropy: This method is applicable when the entire con-versation is made available before-hand, and the task is to deter-mine segment that best determines the class label for the conversa-tion. The best segment is determined by computing the uncertainty U (
S i ) of every segment s i , 1  X  i  X  N and finding the segment that has the lowest uncertainty.
 where, C ( S, GlobalEntropy ) is the class determined for the test instance S using the GlobalEntropy method and C ( s k class determined for segment s k using the probabilistic classifier that is being used. Further we define the quantity Global Test Ratio for a call as the ratio of k N .

LocalEntropy: This method is applicable when the conversation is made available in a streaming manner and the task is to classify the conversation as quickly as possible. This method computes the uncertainty for the segment that has been seen so far and as soon as the uncertainty increases, it determines the class label based on the segment till the previous time instance.
 where, C ( S, LocalEntropy ) is the class determined for the test instance S using the LocalEntropy method. Further we define the quantity Local Test Ratio for a call as the ratio of k
With streaming text the classifier incrementally updates its deci-sions. At every segment, the classifier has a decision, p an associated Entropy value, U ( x ) . At some point into the stream the classifier has to decide on the  X  X rue" class. The relative entropy over the stream determines the point at which to make this decision. While we have presented our explanation based on a segment level stream it also holds for a word stream. It is possible that the Loca-lEntropy based segment detection may get stuck in a local minima, which can be determined by checking whether the LocalEntropy again dips after n words. If it does not dip we declare the decision at minima point. If a new minima is encountered in the look ahead zone we update the minima point and again look ahead.
We have so far taken the approach of building a classifier on the complete call (i.e. document) and then using the entropy to make decisions on partial documents. Below we consider an alternate model where the classifier learnt using partial data itself with the hope that it will perform better. Basically, we argue that depending on the class labels different portions of the call may be important. To classify whether the agent opens the call in a proper way, a clas-sifier built on just the first portion of the call is sufficient. However, for separating calls where credit card details are asked versus those where the customer agrees to pay on delivery, the middle segment of the call may be important. So the same call may need to be classified into these different classes, where different classifiers can come into play on different segments of the call.
The text stream for a conversation is non-stationary therefore features for specific tasks are limited to portions of the call. So for example, in most calls the features relating to greeting will occur at the beginning of the call and features relating to types of payment may appear in another portion of the call. The classifier makes a decision based on the conditional model p ( C x /a 1 ,...,a
Learning a classifier on streaming text would mean that the clas-sifier is updated with each new feature that appears. Earlier we hypothesized that a classifier built using the whole call reaches a  X  X rue" decision after seeing just a fraction of the call. This implies that the features needed for the classifier have all been collected at this point in the call which could be a fraction of the complete call. Given a collection T consisting of m conversations, we use the fol-lowing method for determining the segment s k that should be used for training the classifier:
TrainingCutOffPoint: The cut off segment is determined by com-puting the Global Test Ratio , G , for each call in the collection T . Basedonthisratioweselect s k = G  X  N from each call to train the classifier.
The ability of our solutio n to suggest entitie s of interest is there-fore dependent on how well it can handle the noise in the input. Generally, ASR systems are known to have 30-40% error in tran-scription of service desk conversations. ASRs have three com-ponents -An acoustic model that works at the level of phonemic sounds (sub word), a lexical model that defines the vocabulary and models how each word is formed from the phonetic sounds com-prising it and a language model that organizes the words to model the structure of the language. A speech signal is converted into a sequence of words based on a search that is constrained using these three components [17]. ASR systems typically use a lexicon of over 60,000 words and the models are trained using a few hundred hours of speech data. Also the language models are modified to take into account the domain specific features [18]. For example, in calls relating to an IT help desk, there will be a lot of computer specific words, and these words are given higher weights.
Since, AgentAid is only focussing on the noun phrases, only er-rors introduced in the noun phrase transcription would affect the system. Moreover, as explained above, a domain specific vocab-ulary or controlled vocabulary can be used to tune the language model of the ASR to perform better for words appearing in the given vocabulary. Since, we are only interested in noun-phrases appearing in the transactional database of a particular business, we can build a business specific controlled vocabulary and tune the ASR systems to reduce error. Such vocabularies can be modified in time to reflect popular terms based on call transcription logs.
An added advantage of using a controlled vocabulary is the abil-ity to overcome spelling errors in the transcript. Spelling errors can be reduced by consulting the controlled vocabulary and checking for similarity between the transcribed term and any element of the vocabulary. In our implementation, we use a graded Levenshtein distance 1 measure to map terms in transcript to those in the vocab-ulary. The grading is dependent on the length of the word.
In the following, we present results of evaluations done to as-sess the Decision Module X  X  effectiveness in detecting intent points during real-time audio conversations. Below we first describe the dataset used for evaluation and then present results of various eval-uations done using it.
We obtained automatic transcriptions of contact center dialogs from a car rental process using an ASR.The speech recognition system was trained on 40 hours of data comprising of calls sam-pled at 8KHz. The resultant transcriptions had a word error rate of about 40%. We used 527 calls from the car rental process for our experiments. These calls were labeled according to two criteria -1) based on the reason for non-booking by a customer and 2) based on reason for call or call-opening . Non-booking here relates to a call where the customer didn X  X  rent a car. A total of 9 labels such as shopping around, high rates etc. were assigned by contact center quality analyst as reasons fo r non booking. We call this the non-booking data-set. The second criteria was a binary labeling based on whether call was made for booking or to find out the rates. We refer to this as the call-opening data-set.

We also used a publicly available data-set, the 20NewsGroups data-set to simulate a real-time stream and test our methods in a non car rental setting. We chose two classes, talk.politics.mideast and talk.politics. guns , and picked all the postings within these classes that had approximately 30 to 50 sentences. This resulted in 285 files being used for  X  X ideast X  class and 327 files for  X  X uns X  class. We appended 65 to 85 sentences to these postings by randomly picking these sentences from the remaining 18 classes of 20news-group data. The sentences for the postings are treated as being gen-erated in a streaming manner. Construction of dataset in this way results in the decision boundary of the  X  X ideast X  and  X  X uns X  classes to be between 0.26 (30/115) and 0.43 (50/115) of the normalized duration of the call.
We conducted classification experiments for the tasks of call opening intent point identification using the call-opening data-set and non-booking intent point identification using the non-booking data-set. The objective was to evaluate how the different methods proposed in the paper perform for the intent point detection and intent classification task i.e. categorizing the intent behind non-booking (high rates, una vailability etc) and for the call-opening in-
The Levenshtein distance between two strings is given by the min-imum number of operations needed to transform one string into the other, where an operation is an insertion, deletion, or substitution of a single character. tent detection i.e. whether the customer X  X  intent is to do a booking or find out rates. We trained a logistic regression classifier complete call transcripts of all the 527 calls for each task. Dur-ing the testing phase instead of classifying the complete call we classified the call incrementally where each unit of increment was a sentence as described in Section 4. For each such incremental segment, s 1 ...s k , we computed the entropy.
 Table 1: Accuracies for classifiers trained on different fractions of the call for call-opening intent classification
In Tables 1, 2 and 3 we show the various accuracies for classi-fiers that have been trained on fractions of the call for call-opening intent classification, non-booking intent classification and 20news-group classification respectively. Baseline accuracy represents the case when the testing is done on the complete call. Global accu-racy represents the accuracy when the segment is selected based on GlobalEntropy. The average global test ratio represents the frac-tion of the call where the global minima of the entropy lies on an average for the whole set. Local accuracy is what is actually used in real-time classification of conversational text. It is the accuracy when the segment is selected based on LocalEntropy. The average local test ratio is the fraction of the call one needs to see before making a classification decision. The best accuracy is the maxi-mum classification accuracy for different segment lengths. The best accuracy is arrived at by computing the classification scores for dif-ferent segment lengths and selecting that segment which gives the highest accuracy over the whole collection. The best accuracy is the upper bound that can be obtained for the given task using a given classifier. In all the tables we see that the best accuracy peaks when the classifier is trained on a fraction of the call. Table 2: Accuracies for classifiers trained on different fractions of the call for non-booking intent classification
We also compared the entropy based intent point selection with the human decision making process. Given the streaming text out-put from the conversation, a human listener would perform the
We chose logistic regression as a representative since it performs very well in practice and has been shown to be closely related to SVM [20] and AdaBoost [5].
 Table 3: Accuracies for classifiers trained on different fractions of 20 NewsGroups Data Table 4: Comparing Intent Point Determination by Humans and Entropy Technique classification after she has gathered enough evidence. For the call-opening intent classification task this decision point is at 0.25 and for non-booking intent classification task it is at 0.5. The human reaches close to 100% classification accuracy at this intent point. Also after this the classification accuracy remains at 100%. This is because once the human believes she has all the evidence required to reach a decision she does not change her mind. However, as shown in the results earlier our classifier achieves a high classifi-cation accuracy close to the intent point but after this intent point the accuracy actually goes down. In Table 4 we show the compari-son between the intent point for classification done by a human and that determined by our method. Several observations can be made, namely -1) A human classifier needs to see less of the call to make a final decision regarding the intent (i.e. the human intent point is earlier than that of our method). 2) For our method the classi-fication accuracy is highest near the intent point and lesser before and after the intent point and 3) For a human classifier the classi-fication accuracy reaches 100% at the intent point and remains at 100% even as more of the conversation is seen.
The accuracy of a transcript is characterized by its word error rate (WER) . WER is measured as S + D + I N  X  100 where N is the total number of words in the test set, and S , I and D are the total number of substitution, insertion, and deletion errors respectively. We also defined two other metrics, Keyword Error Rate (KER) and Controlled Keyword Error Rate (CKER) .BothKERandCKERare estimated similar to WER but differ over the set of words used for the measurement. KER is measured using all noun phrases appear-ing in the test set while CKER is measured using the noun phrases that appear in the controlled vocabulary. For our evaluation we ran-domly selected 501 call transcripts from the initial set of 527 call transcripts obtained from the car rental process. We looked at the Yahoo Auto Database 3 to determine a possible set of 300 words that could appear in a controlled vocabulary about cars.
We then measured, WER, KER and CKER using the automat-ically and manually generated transcripts. In addition, we also measured WER X  where all stopwords were removed and CKER+ where we considered terms with a graded Levenshtein distance of from the controlled vocabulary as similar to term in the vocabulary. http://autos.yahoo.com Figure 2 compares the various error measurements for all the calls in our dataset. As expected the WER rate is around 30-35% for all the calls. However, CKER performs much better (25-30% er-ror) than all the above and validates the assertion that ASR systems when tuned to domain specific words do improve the transcription accuracy.
In this paper we identify the need for identifying customer in-tent during a real-time call center conversation. We then presented details of the AgentAid conversation management system which prompts agents, at appropriate points during a real-time audio call, with information which is useful for successfully closing the call. Evaluation results demonstrate the ability of our system to effi-ciently and effectively identify customer intent, extract relevant in-formation from backend database and prompt the agent about next steps. The results show our approach is able to overcome the er-rors introduced by Automatic Sp eech Recognition systems and is able to perform under strict response time constraints. To the best of our knowledge, AgentAid is the only conversation management system currently available for helping an agent during a real-time conversation. It can be implemented with minimal domain-specific setup and used by an agent with minimal training leading to re-duced overall agent training costs. [1] S. Agarwal, S. Godbole, D. Punjani, S. Roy. 2007. How Much [2] C. Aggarwal, J. Han, J. Wang, P. S. Yu. 2004. On Demand [3] L. Cohen, G. Avrahami, M. Last, A. Kandel, O. Kipersztok. [4] P. Domingos, G. Hulten. 2000. Mining High-Speed Data [5] Y. Freund, R. E. Schapire. 1996. Experiments in new boosting [6] J. R. Glass, T. J. Hazen, I. L. Hetherington. 1999. Real-time [7] M. A. Hearst. 1994. Multi-paragraph segmentation of [8] G. Hulten, L. Spencer, P. Domingos. 2001. Mining [9] I. Katakis, G. Tsoumakas, I. Vlahavas. 2006. Dynamic feature [10] R. Klinkenberg, T. Joachims. 2000.Detecting concept drift [11] K. Kummamuru, Deepak P., S. Roy, L. V. Subramaniam. [12] H.-K. J. Kuo, C.-H. Lee. 2003. Discriminative training of [13] N. Landwehr, M. Hall, F. Eibe. 2003. Logistic Model Trees. [14] Y. Park. 2007. Automatic call section segmentation for [15] H. Takeuchi, L. V. Subramaniam, T. Nasukawa, S. Roy. [16] M. Tang, B. Pellom, K. Hacioglu. 2003. Calltype [17] M. Padmanabhan, G. Saon, J. Huang, B. Kingsbury, and [18] H. Chevalier, C. Ingold, C. Kunz, C. Moore, C. Roven, [19] G. Widmer, M. Kubat. 1996. Learning in the presence of [20] V. N. Vapnik. 1995. The nature of statistical learning theory . [21] U. Nambiar, H. Gupta, R. Balakrishnan and M. Mohania. [22] T. Mitchell. 1997 Machine Learning. McGraw Hill .
