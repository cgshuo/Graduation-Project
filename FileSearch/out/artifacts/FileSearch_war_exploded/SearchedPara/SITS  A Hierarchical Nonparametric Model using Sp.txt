 Conversation, interactive discussion between two or more people, is one of the most essential and com-mon forms of communication. Whether in an in-formal situation or in more formal settings such as a political debate or business meeting, a conversa-tion is often not about just one thing: topics evolve and are replaced as the conversation unfolds. Dis-covering this hidden structure in conversations is a key problem for conversational assistants (Tur et al., 2010) and tools that summarize (Murray et al., 2005) and display (Ehlen et al., 2007) conversational data. Topic segmentation also can illuminate individuals X  agendas (Boydstun et al., 2011), patterns of agree-ment and disagreement (Hawes et al., 2009; Abbott et al., 2011), and relationships among conversational participants (Ireland et al., 2011).

One of the most natural ways to capture conversa-tional structure is topic segmentation (Reynar, 1998; Purver, 2011). Topic segmentation approaches range from simple heuristic methods based on lexical simi-larity (Morris and Hirst, 1991; Hearst, 1997) to more intricate generative models and supervised meth-ods (Georgescul et al., 2006; Purver et al., 2006; Gruber et al., 2007; Eisenstein and Barzilay, 2008), which have been shown to outperform the established heuristics.

However, previous computational work on con-versational structure, particularly in topic discovery and topic segmentation, focuses primarily on con-tent, ignoring the speakers. We argue that, because conversation is a social process, we can understand conversational phenomena better by explicitly model-ing behaviors of conversational participants. In Sec-tion 2, we incorporate participant identity in a new model we call Speaker Identity for Topic Segmen-tation (SITS), which discovers topical structure in conversation while jointly incorporating a participant-level social component. Specifically, we explicitly model an individual X  X  tendency to introduce a topic. After outlining inference in Section 3 and introducing data in Section 4, we use SITS to improve state-of-the-art-topic segmentation and topic identification models in Section 5. In addition, in Section 6, we also show that the per-speaker model is able to dis-cover individuals who shape and influence the course of a conversation. Finally, we discuss related work and conclude the paper in Section 7. Data Properties We are interested in turn-taking, multiparty discussion. This is a broad category, in-cluding political debates, business meetings, and on-line chats. More formally, such datasets contain C conversations. A conversation c has T c turns, each of which is a maximal uninterrupted utterance by one speaker. 1 In each turn t  X  [1 ,T c ] , a speaker a c,t utters N words { w c,t,n } . Each word is from a vocabulary of size V , and there are M distinct speakers. Modeling Approaches The key insight of topic segmentation is that segments evince lexical cohe-sion (Galley et al., 2003; Olney and Cai, 2005). Words within a segment will look more like their neighbors than other words. This insight has been used to tune supervised methods (Hsueh et al., 2006) and inspire unsupervised models of lexical cohesion using bags of words (Purver et al., 2006) and lan-guage models (Eisenstein and Barzilay, 2008). We too take the unsupervised statistical approach. It requires few resources and is applicable in many domains without extensive training. Like previ-ous approaches, we consider each turn to be a bag of words generated from an admixture of topics. Topics X  X fter the topic modeling literature (Blei and Lafferty, 2009) X  X re multinomial distributions over terms. These topics are part of a generative model posited to have produced a corpus.

However, topic models alone cannot model the dy-namics of a conversation. Topic models typically do not model the temporal dynamics of individual docu-ments, and those that do (Wang et al., 2008; Gerrish and Blei, 2010) are designed for larger documents and are not applicable here because they assume that most topics appear in every time slice.

Instead, we endow each turn with a binary latent variable l c,t , called the topic shift . This latent variable signifies whether the speaker changed the topic of the conversation. To capture the topic-controlling behav-ior of the speakers across different conversations, we further associate each speaker m with a latent topic shift tendency ,  X  m . Informally, this variable is in-tended to capture the propensity of a speaker to effect a topic shift. Formally, it represents the probability that the speaker m will change the topic (distribution) of a conversation.

We take a Bayesian nonparametric ap-proach (M  X  uller and Quintana, 2004). Unlike parametric models, which a priori fix the number of topics, nonparametric models use a flexible number of topics to better represent data. Nonparametric distributions such as the Dirichlet process (Ferguson, 1973) share statistical strength among conversations using a hierarchical model, such as the hierarchical Dirichlet process (HDP) (Teh et al., 2006). 2.1 Generative Process In this section, we develop SITS, a generative model of multiparty discourse that jointly discovers topics and speaker-specific topic shifts from an unannotated corpus (Figure 1a). As in the hierarchical Dirichlet process (Teh et al., 2006), we allow an unbounded number of topics to be shared among the turns of the corpus. Topics are drawn from a base distribution H over multinomial distributions over the vocabu-lary, a finite Dirichlet with symmetric prior  X  . Unlike the HDP, where every document (here, every turn) draws a new multinomial distribution from a Dirich-let process, the social and temporal dynamics of a conversation, as specified by the binary topic shift indicator l c,t , determine when new draws happen.
The full generative process is as follows:
The hierarchy of Dirichlet processes allows sta-tistical strength to be shared across contexts; within a conversation and across conversations. The per-speaker topic shift tendency  X  m allows speaker iden-tity to influence the evolution of topics.

To make notation concrete and aligned with the topic segmentation, we introduce notation for seg-ments in a conversation. A segment s of conver-sation c is a sequence of turns [  X , X  0 ] such that l l c,t = 0 , G c,t is the same as G c,t  X  1 and all topics (i.e. multinomial distributions over words) {  X  c,t,n } that generate words in turn t and the topics {  X  c,t  X  1 ,n } that generate words in turn t  X  1 come from the same distribution. Thus all topics used in a segment s are drawn from a single distribution, G c,s ,
For notational convenience, S c denotes the num-ber of segments in conversation c , and s t denotes the segment index of turn t . We emphasize that all segment-related notations are derived from the poste-rior over the topic shifts l and not part of the model itself.
 Parametric Version SITS is a generalization of a parametric model (Figure 1b) where each turn has a multinomial distribution over K topics. In the parametric case, the number of topics K is fixed. Each topic, as before, is a multinomial distribution  X  1 ... X  K . In the parametric case, each turn t in con-versation c has an explicit multinomial distribution over K topics  X  c,t , identical for turns within a seg-ment. A new topic distribution  X  is drawn from a Dirichlet distribution parameterized by  X  when the topic shift indicator l is 1 .

The parametric version does not share strength within or across conversations, unlike SITS. When applied on a single conversation without speaker iden-tity (all speakers are identical) it is equivalent to (Purver et al., 2006). In our experiments (Section 5), we compare against both. To find the latent variables that best explain observed data, we use Gibbs sampling, a widely used Markov chain Monte Carlo inference technique (Neal, 2000; Resnik and Hardisty, 2010). The state space is latent variables for topic indices assigned to all tokens z = { z c,t,n } and topic shifts assigned to turns l = { l c,t } . We marginalize over all other latent variables. Here, we only present the conditional sampling equations; for more details, see our supplement. 2 3.1 Sampling Topic Assignments To sample z c,t,n , the index of the shared topic as-signed to token n of turn t in conversation c , we need to sample the path assigning each word token to a segment-specific topic, each segment-specific topic to a conversational topic and each conversational topic to a shared topic. For efficiency, we make use of the minimal path assumption (Wallach, 2008) to generate these assignments. 3 Under the minimal path assumption, an observation is assumed to have been generated by using a new distribution if and only if there is no existing distribution with the same value.
We use N c,s,k to denote the number of tokens in segment s in conversation c assigned topic k ; N c,k denotes the total number of segment-specific top-ics in conversation c assigned topic k and N k de-notes the number of conversational topics assigned topic k . TW k,w denotes the number of times the shared topic k is assigned to word w in the vocab-ulary. Marginal counts are represented with  X  and  X  represents all hyperparameters. The conditional distribution for z c,t,n is P ( z c,t,n = k | w c,t,n =
Here V is the size of the vocabulary, K is the current number of shared topics and the superscript  X  c,t,n denotes counts without considering w c,t,n . In Equa-tion 2, the first factor is proportional to the probability of sampling a path according to the minimal path as-sumption; the second factor is proportional to the likelihood of observing w given the sampled topic. Since an uninformed prior is used, when a new topic is sampled, all tokens are equiprobable. 3.2 Sampling Topic Shifts Sampling the topic shift variable l c,t requires us to consider merging or splitting segments. We use k c,t to denote the shared topic indices of all tokens in turn t of conversation c ; S a c,t ,x to denote the num-ber of times speaker a c,t is assigned the topic shift with value x  X  X  0 , 1 } ; J x c,s to denote the number of topics in segment s of conversation c if l c,t = x and N c,s,j to denote the number of tokens assigned to the segment-specific topic j when l c,t = x . 4 Again, the superscript  X  c,t is used to denote exclusion of turn t of conversation c in the corresponding counts.
Recall that the topic shift is a binary variable. We use 0 to represent the case that the topic distribution is identical to the previous turn. We sample this assignment P ( l c,t = 0 | l  X  c,t , w , k , a ,  X  )  X 
In Equation 3, the first factor is proportional to the probability of assigning a topic shift of value 0 to speaker a c,t and the second factor is proportional to the joint probability of all topics in segment s t of conversation c when l c,t = 0 .

The other alternative is for the topic shift to be 1 , which represents the introduction of a new distri-bution over topics inside an existing segment. We sample this as P ( l c,t = 1 | l  X  c,t , w , k , a ,  X  )  X 
As above, the first factor in Equation 4 is propor-tional to the probability of assigning a topic shift of value 1 to speaker a c,t ; the second factor in the big bracket is proportional to the joint distribution of the topics in segments s t  X  1 and s t . In this case l c,t = 1 means splitting the current segment, which results in two joint probabilities for two segments. This section introduces the three corpora we use. We preprocess the data to remove stopwords and remove turns containing fewer than five tokens.
 The ICSI Meeting Corpus: The ICSI Meeting Corpus (Janin et al., 2003) is 75 transcribed meetings. For evaluation, we used a standard set of reference segmentations (Galley et al., 2003) of 25 meetings. Segmentations are binary , i.e., each point of the doc-ument is either a segment boundary or not, and on average each meeting has 8 segment boundaries. Af-ter preprocessing, there are 60 unique speakers and the vocabulary contains 3346 non-stopword tokens. The 2008 Presidential Election Debates Our sec-ond dataset contains three annotated presidential de-bates (Boydstun et al., 2011) between Barack Obama and John McCain and a vice presidential debate be-tween Joe Biden and Sarah Palin. Each turn is one of two types: questions ( Q ) from the moderator or responses ( R ) from a candidate. Each clause in a turn is coded with a Question Topic ( T Q ) and a Re-sponse Topic ( T R ). Thus, a turn has a list of T Q  X  X  and T
R  X  X  both of length equal to the number of clauses in the turn. Topics are from the Policy Agendas Topics Community Development (14), Government Operations (20). Codebook, a manual inventory of 19 major topics and 225 subtopics. 5 Table 1 shows an example anno-tation.

To get reference segmentations, we assign each turn a real value from 0 to 1 indicating how much a turn changes the topic. For a question-typed turn, the score is the fraction of clause topics not appearing in the previous turn; for response-typed turns, the score is the fraction of clause topics that do not appear in the corresponding question. This results in a set of non-binary reference segmentations. For evaluation metrics that require binary segmentations, we create a binary segmentation by setting a turn as a segment boundary if the computed score is 1 . This threshold is chosen to include only true segment boundaries. CNN X  X  Crossfire Crossfire was a weekly U.S. tele-vision  X  X alking heads X  program engineered to incite heated arguments (hence the name). Each episode features two recurring hosts, two guests, and clips from the week X  X  news. Our Crossfire dataset con-tains 1134 transcribed episodes aired between 2000 and 2004. 6 There are 2567 unique speakers. Unlike the previous two datasets, Crossfire does not have explicit topic segmentations, so we use it to explore speaker-specific characteristics (Section 6). In this section, we examine how well SITS can repli-cate annotations of when new topics are introduced. We discuss metrics for evaluating an algorithm X  X  seg-mentation against a gold annotation, describe our experimental setup, and report those results. Evaluation Metrics To evaluate segmentations, we use P k (Beeferman et al., 1999) and WindowDiff (WD) (Pevzner and Hearst, 2002). Both metrics mea-sure the probability that two points in a document will be incorrectly separated by a segment boundary. Both techniques consider all spans of length k in the document and count whether the two endpoints of the window are (im)properly segmented against the gold segmentation.

However, these metrics have drawbacks. First, they require both hypothesized and reference seg-mentations to be binary. Many algorithms (e.g., prob-abilistic approaches) give non-binary segmentations where candidate boundaries have real-valued scores (e.g., probability or confidence). Thus, evaluation requires arbitrary thresholding to binarize soft scores. To be fair, thresholds are set so the number of seg-ments are equal to a predefined value (Purver et al., 2006; Galley et al., 2003).
 To overcome these limitations, we also use Earth Mover X  X  Distance (EMD) (Rubner et al., 2000), a metric that measures the distance between two distri-butions. The EMD is the minimal cost to transform one distribution into the other. Each segmentation can be considered a multi-dimensional distribution where each candidate boundary is a dimension. In EMD, a distance function across features allows par-tial credit for  X  X ear miss X  segment boundaries. In addition, because EMD operates on distributions, we can compute the distance between non-binary hy-pothesized segmentations with binary or real-valued reference segmentations. We use the FastEMD im-plementation (Pele and Werman, 2009).
 Experimental Methods We applied the following methods to discover topic segmentations in a docu-ment: Parameter Settings and Implementations In our experiment, all parameters of TextTiling are the same as in (Hearst, 1997). For statistical models, Gibbs sampling with 10 randomly initialized chains is used. Initial hyperparameter values are sampled from U (0 , 1) to favor sparsity; statistics are collected after 500 burn-in iterations with a lag of 25 itera-tions over a total of 5000 iterations; and slice sam-pling (Neal, 2003) optimizes hyperparameters. Results and Analysis Table 2 shows the perfor-mance of various models on the topic segmentation problem, using the ICSI corpus and the 2008 debates.
Consistent with previous results, probabilistic models outperform TextTiling. In addition, among the probabilistic models, the models that had access to speaker information consistently segment better than those lacking such information, supporting our assertion that there is benefit to modeling conversa-tion as a social process. Furthermore, NP-SITS out-performs NP-HMM in both experiments, suggesting that using a distribution over topics to turns is bet-ter than using a single topic. This is consistent with parametric results reported in (Purver et al., 2006).
The contribution of speaker identity seems more valuable in the debate setting. Debates are character-ized by strong rewards for setting the agenda; dodg-ing a question or moving the debate toward an oppo-nent X  X  weakness can be useful strategies (Boydstun et al., 2011). In contrast, meetings (particularly low-stakes ICSI meetings) are characterized by pragmatic rather than strategic topic shifts. Second, agenda-setting roles are clearer in formal debates; a modera-tor is tasked with setting the agenda and ensuring the conversation does not wander too much.

The nonparametric model does best on the smaller debate dataset. We suspect that an evaluation that directly accessed the topic quality, either via predic-tion (Teh et al., 2006) or interpretability (Chang et al., 2009) would favor the nonparametric model more. In this section, we focus on the ability of SITS to capture speaker-level attributes. Recall that SITS associates with each speaker a topic shift tendency  X  that represents the probability of asserting a new topic in the conversation. While topic segmentation is a well studied problem, there are no established quantitative measurements of an individual X  X  ability to control a conversation. To evaluate whether the tendency is capturing meaningful characteristics of speakers, we compare our inferred tendencies against insights from political science. 2008 Elections To obtain a posterior estimate of  X  (Figure 3) we create 10 chains with hyperparameters sampled from the uniform distribution U (0 , 1) and averaged  X  over 10 chains (as described in Section 5). In these debates, Ifill is the moderator of the debate between Biden and Palin; Brokaw, Lehrer and Schief-fer are the three moderators of three debates between Obama and McCain. Here  X  X uestion X  denotes ques-tions from audiences in  X  X own hall X  debate. The role of this  X  X peaker X  can be considered equivalent to the debate moderator.

The topic shift tendencies of moderators are much higher than for candidates. In the three de-bates between Obama and McCain, the moderators X  Brokaw, Lehrer and Schieffer X  X ave significantly higher scores than both candidates. This is a useful reality check, since in a debate the moderators are the ones asking questions and literally controlling the topical focus. Interestingly, in the vice-presidential debate, the score of moderator Ifill is only slightly higher than those of Palin and Biden; this is consis-tent with media commentary characterizing her as a Table 2: Results on the topic segmentation task. Lower is better. The parameter k is the window size of the metrics P k and WindowDiff chosen to replicate previous results.
Table 3: Topic shift tendency  X  of speakers in the 2008 Presidential Election Debates (larger means greater tendency) weak moderator. 7 Similarly, the  X  X uestion X  speaker had a relatively high variance, consistent with an amalgamation of many distinct speakers.

These topic shift tendencies suggest that all can-didates manage to succeed at some points in setting and controlling the debate topics. Our model gives Obama a slightly higher score than McCain, consis-tent with social science claims (Boydstun et al., 2011) that Obama had the lead in setting the agenda over McCain. Table 4 shows of SITS-detected topic shifts. Crossfire Crossfire, unlike the debates, has many speakers. This allows us to examine more closely what we can learn about speakers X  topic shift ten-dency. We verified that SITS can segment topics, and assuming that changing the topic is useful for a speaker, how can we characterize who does so effec-tively? We examine the relationship between topic shift tendency, social roles, and political ideology.
To focus on frequent speakers, we filter out speak-ers with fewer than 30 turns. Most speakers have relatively small  X  , with the mode around 0 . 3 . There are, however, speakers with very high topic shift tendencies. Table 5 shows the speakers having the highest values according to SITS.

We find that there are three general patterns for who influences the course of a conversation in Cross-fire . First, there are structural  X  X peakers X  the show uses to frame and propose new topics. These are audience questions, news clips (e.g. many of Gore X  X  and Bush X  X  turns from 2000), and voice overs. That SITS is able to recover these is reassuring. Second, the stable of regular hosts receives high topic shift tendencies, which is reasonable given their experi-ence with the format and ostensible moderation roles (in practice they also stoke lively discussion).
The remaining class is more interesting. The re-maining non-hosts with high topic shift tendency are relative moderates on the political spectrum: This suggests that, despite Crossfire  X  X  tendency to create highly partisan debates, those who are able to work across the political spectrum may best be able to influence the topic under discussion in highly po-larized contexts. Table 4 shows detected topic shifts from these speakers; two of these examples (McCain and Whitman) show disagreement of Republicans with President Bush. In the other, Kasich is defend-ing a Republican plan (school vouchers) popular with traditional Democratic constituencies. In the realm of statistical models, a number of tech-niques incorporate social connections and identity to explain content in social networks (Chang and Blei, examples of those with high topic shift tendency  X  . Table 5: Top speakers by topic shift tendencies. We mark hosts (  X  ) and  X  X peakers X  who often (but not al-ways) appeared in clips (  X  ). Apart from those groups, speakers with the highest tendency were political moderates. 2009) and scientific corpora (Rosen-Zvi et al., 2004). However, these models ignore the temporal evolution of content, treating documents as static.

Models that do investigate the evolution of topics over time typically ignore the identify of the speaker. For example: models having sticky topics over n-grams (Johnson, 2010), sticky HDP-HMM (Fox et al., 2008); models that are an amalgam of sequential models and topic models (Griffiths et al., 2005; Wal-lach, 2006; Gruber et al., 2007; Ahmed and Xing, 2008; Boyd-Graber and Blei, 2008; Du et al., 2010); or explicit models of time or other relevant features as a distinct latent variable (Wang and McCallum, 2006; Eisenstein et al., 2010).

In contrast, SITS jointly models topic and individ-uals X  tendency to control a conversation. Not only does SITS outperform other models using standard computational linguistics baselines, but it also pro-poses intriguing hypotheses for social scientists.
Associating each speaker with a scalar that mod-els their tendency to change the topic does improve performance on standard tasks, but it X  X  inadequate to fully describe an individual. Modeling individuals X  perspective (Paul and Girju, 2010),  X  X ide X  (Thomas et al., 2006), or personal preferences for topics (Grim-mer, 2009) would enrich the model and better illumi-nate the interaction of influence and topic.
Statistical analysis of political discourse can help discover patterns that political scientists, who often work via a  X  X lose reading, X  might otherwise miss. We plan to work with social scientists to validate our implicit hypothesis that our topic shift tendency correlates well with intuitive measures of  X  X nfluence. X  This research was funded in part by the Army Re-search Laboratory through ARL Cooperative Agree-ment W911NF-09-2-0072 and by the Office of the Director of National Intelligence (ODNI), Intelli-gence Advanced Research Projects Activity (IARPA), through the Army Research Laboratory. Jordan Boyd-Graber and Philip Resnik are also supported by US National Science Foundation Grant NSF grant #1018625. Any opinions, findings, conclusions, or recommendations expressed are the authors X  and do not necessarily reflect those of the sponsors.
