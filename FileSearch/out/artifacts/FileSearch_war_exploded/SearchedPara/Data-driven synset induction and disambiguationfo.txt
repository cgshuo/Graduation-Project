 Abstract Automatic methods for wordnet development in languages other than English generally exploit information found in Princeton WordNet (PWN) and translations extracted from parallel corpora. A common approach consists in pre-serving the structure of PWN and transferring its content in new languages using alignments, possibly combined with information extracted from multilingual semantic resources. Even if the role of PWN remains central in this process, these automatic methods offer an alternative to the manual elaboration of new wordnets. However, their limited coverage has a strong impact on that of the resulting resources. Following this line of research, we apply a cross-lingual word sense disambiguation method to wordnet development. Our approach exploits the output of a data-driven sense induction method that generates sense clusters in new lan-guages, similar to wordnet synsets, by identifying word senses and relations in parallel corpora. We apply our cross-lingual word sense disambiguation method to the task of enriching a French wordnet resource, the WOLF, and show how it can be efficiently used for increasing its coverage. Although our experiments involve the English X  X rench language pair, the proposed methodology is general enough to be applied to the development of wordnet resources in other languages for which parallel corpora are available. Finally, we show how the disambiguation output can serve to reduce the granularity of new wordnets and the degree of polysemy present in PWN.
 Keywords Cross-lingual word sense disambiguation Word sense induction Sense clustering Parallel corpora WordNet 1 Introduction The growing need for lexical and semantic knowledge in natural language processing (NLP) applications has steered several initiatives for resource develop-ment in recent years. A common trend has been to develop multilingual resources based on Princeton WordNet (PWN) (Fellbaum 1998 ): the structure of PWN is generally preserved and its contents are translated in new languages (Vossen 1998 ; Pianta et al. 2002 ;Tufis  X  et al. 2004 ). The advantages of this approach, which explain its wide adoption, are that it avoids the time-consuming manual elaboration of the semantic hierarchy in new languages and allows the alignment of the resulting wordnets, a feature particularly useful for multilingual NLP. Its main limitation is the strong bias imposed by PWN on the content and structure of the newly built wordnets. The structure of PWN is preserved in the target language and its content is transferred based on an assumption of language independence of concepts and semantic relations. However, concepts present in PWN might not be present in the target language, in which case the corresponding synsets in the new wordnet cannot be filled. This issue becomes more important in case of fine-grained sense distinctions in PWN, where finding target language counterparts becomes difficult or impossible. Combined to limitations due to the quantity of information available in the bilingual dictionaries used for manually translating PWN synsets into new languages, these factors have a strong impact on the coverage of the resulting wordnets which is generally much smaller than that of PWN.

In an attempt to address these weaknesses, several automatic wordnet develop-ment methods have been proposed that exploit information found in parallel corpora. These methods permit to acquire semantic information from texts and to circumvent, in this way, the need for pre-defined resources. Moreover, by exploiting alignment information, these methods offer an alternative to the manual filling of wordnets: translations are extracted from parallel corpora and are automatically integrated in the wordnet hierarchy. Nevertheless, the success and coverage of these methods highly depends on the nature of the parallel corpora and on the way the extracted information is used for wordnet filling. Coverage becomes an issue especially when automatically acquired translations are used to fill PWN-based resources, as rare senses (present in PWN) might not be found in the parallel corpora. Additionally, automatic methods often fail to retrieve semantic information as fine-grained as the one found in PWN, leaving numerous synsets empty. As a consequence, the resulting wordnet resources are rather sparse and methods for increasing their coverage are needed.

Following this line of research, we propose a novel automatic approach to wordnet development. We demonstrate how a cross-lingual word sense disambig-uation (WSD) method can be applied to the wordnet development task for creating new resources or for enriching existing ones. WSD is the task of automatically identifying the meaning of words in context (Navigli 2009 ) while its cross-lingual variant predicts semantically correct translations (Apidianaki 2009 ; Lefever and Hoste 2010 ). In this work, we apply a cross-lingual WSD method (Apidianaki 2009 ) to the enrichment of an automatically built wordnet for French, the WOLF (Sagot and Fis  X  er 2008 ). The disambiguation method exploits the output of a cross-lingual word sense induction (WSI) method which identifies word senses and their relations in parallel corpora. The WSI method generates clusters of semantically related words in the new language (French) similar to wordnet synsets, which are integrated in the WOLF hierarchy by the cross-lingual WSD method.

The paper is organized as follows. Section 2 presents related work on wordnet development in languages other than English. We first describe the classical approach to cross-lingual transfer of WordNet information based on pre-defined lexical and semantic resources. Then we move to a number of automatic methods which combine existing resources with information extracted from corpora and describe the method that initially served to build WOLF, the French resource that we aim to enrich. Finally, we present a number of purely data-driven semantic analysis methods and explain our choice to apply a WSD method to the wordnet development task. Section 3 presents our data-driven synset induction method as well as the disambiguation method that serves to integrate the newly acquired synsets in the French resource. In Sect. 4 , we present the results of a manual evaluation intended to estimate the quality of the clustering and the correctness of the new WOLF entries. The main findings of this study are summed up in the last section where we also present some avenues worth pursuing in future work, before concluding. 2 Cross-lingual approaches to wordnet development 2.1 Transfer of WordNet information to new languages Multilingual wordnet development has always strongly relied on PWN (Fellbaum 1998 ). Large-scale projects aiming the creation of wordnets in languages other than English, such as EuroWordNet, BalkaNet and MultiWordNet (Vossen 1998 ; Pianta et al. 2002 ;Tufis  X  et al. 2004 ), have adopted a translation-driven approach: the structure of PWN was preserved while its contents were imported in the newly built resources by applying various translation-based methods. The main advantage of this approach, also called the expand model , is that it permits to avoid the time-consuming and expensive manual elaboration of the semantic hierarchy in new languages. An additional advantage is that the newly built wordnet is automatically aligned to PWN and to other wordnets built following the same principle. The resulting resources are thus interesting for contrastive semantic analysis and can be particularly useful in multilingual NLP tasks, such as Multilingual Information Retrieval.

Despite its strengths, the translation approach to wordnet development also presents a number of drawbacks. One of them is the strong bias imposed by PWN on the content and structure of the new wordnets. The structure of PWN is generally preserved and its content is transferred in the new languages based on the assumption that concepts and semantic relations between them are X  X t least to a large extent X  X anguage independent. This assumption is not theoretically valid and has important practical implications during the compilation of new wordnets. Several senses present in PWN have no target language counterpart, a problem that becomes more apparent in the case of fine-grained WordNet senses. As a consequence, a varying number of target language synsets may be left unfilled, depending on the language, and this sparseness limits the usefulness of the newly built resource in NLP applications.

Other issues posed by the translation approach are its heavy reliance on external lexico-semantic resources and the manual work needed for transfer. In EuroWord-Net, BalkaNet and MultiWordNet, PWN literals were mainly translated by human lexicographers using external resources such as dictionaries, thesaurus and taxonomies. 1 Apart from limiting the approach to specific language pairs, the reliance on pre-defined resources introduces a new bias as their coverage has a strong impact on the one of the newly built wordnets. 2.2 Automatic cross-lingual information transfer 2.2.1 Combining lexicographic resources and parallel corpora In spite of the theoretical and practical drawbacks inherent to the translation approach, new wordnets are still heavily based on PWN. The methods used for transferring information into new languages have however evolved towards becoming more or less automatic, limiting the cost of the manual methods employed before. Moreover, these methods often exploit lexico-semantic informa-tion extracted from monolingual or multilingual corpora, instead of solely relying on pre-defined semantic resources. For instance, the French hierarchy WOLF (Sagot and Fis  X  er 2008 ), that we intend to enrich in this work, was automatically built by combining information from several multilingual resources (the EUROVOC thesaurus and Wikipedia-related resources) with information extracted from a multilingual parallel corpus. Another PWN-based resource for French, the JAWS network, was compiled by combining a bilingual dictionary and syntactic information acquired from corpora for disambiguating polysemous nouns and correctly integrating them in the hierarchy (Mouton and de Chalendar 2010 ).
The multilingual semantic network BabelNet goes a step further by jointly exploiting PWN, Wikipedia and the output of statistical Machine Translation systems (Navigli and Ponzetto 2010 ). In BabelNet, PWN and Wikipedia are combined by automatically mapping WordNet senses and Wikipages and comple-menting their respective concept inventories. Multilingual lexicalizations of the concepts are acquired from the human-generated translations provided in Wikipedia and by using a statistical Machine Translation system to translate occurrences of the concepts within sense-tagged corpora. The resulting multilingual network has a wide coverage, as it contains 9 million entries (concepts and named entities) in 50 languages. 2 In a different setting, aiming the semantic annotation of new languages, Diab and Resnik ( 2002 ) combine translation information from a parallel corpus with semantic information in PWN. The possible semantic tags provided in PWN for the English translations of a foreign word are found, and the one characterizing the whole set of translations is selected and used as the foreign word X  X  sense tag.

All these approaches successfuly combine information in PWN and other lexical and semantic resources with information learned from corpora. In the next section, we provide more information on the WOLF resource that we intend to enrich, the way it was compiled, its content and its coverage. 2.2.2 The WOLF resource WOLF (Sagot and Fis  X  er 2008 ) is a freely available wordnet for French. Its first version (WOLF 0.1.4) was created on the basis of PWN (version 2.0) by following the expand model for wordnet development and relying on both existing resources and parallel corpora. In this section, we briefly sketch the approach that was used for building WOLF version 0.1.4. A more detailed discussion on the construction of the resource and its evaluation can be found in earlier publications (Sagot and Fis  X  er 2008 ).

To fill the WOLF, monosemous literals in the PWN were automatically translated using a bilingual French X  X nglish lexicon built from various multilingual resources. In particular, data was extracted from Wikipedia using inter-wiki links, from the English and French Wiktionary, the Wikispecies encyclopedia of living beings and the EUROVOC thesaurus ( http://europa.eu/eurovoc ).

Polysemous PWN literals were handled by an alignment approach (cf. Sect. 2.3 ) based on the multilingual parallel corpus SEE-ERA.NET (Steinberger et al. 2006 ), which is composed of the English, French, Romanian, Czech and Bulgarian parts of the JRC-Acquis corpus. The corpus was lemmatized, part of speech (POS) tagged and word aligned, and bilingual lexicons were automatically built including the translations of English words in different languages. These lexicons were then combined into various multilingual lexicons (3-lingual to 5-lingual) and a synset id was assigned to each lexicon entry by gathering all possible ids for this entry in all languages from the corresponding BalkaNet wordnets, which share the same inventory of synset ids. The underlying assumption being that it is unlikely that the same polysemy occurs in different languages, the intersection of the possible senses was expected to output only the correct synset. In this way, the ids shared by all non-French lexicon entries were assigned to their French translation. For example, among the various French X  X zech X  X ulgarian X  X nglish alignments involving the French word droit and the English word law , the alignment droit-pr X vo --law was found 56 times. The only synset that contained pr X vo in the Czech wordnet, 05791721-n. Therefore, droit could be added in the WOLF in this synset.

The synsets obtained for monosemous and polysemous literals by these two approaches were merged. The resulting network preserves the hierarchy and structure of PWN 2.0 and contains the definitions and usage examples provided in PWN for each synset. As information was not found for all PWN synsets by the employed automatic methods, the version 0.1.6 of WOLF which is used in our experiments is rather sparse. 3 In total, it contains 32,351 non-empty synsets including 37,991 unique literals (vs. 115,424 synsets with 145,627 literals in PWN 2.0). These synsets are filled with 34,827 unique French noun literals, 1,521 adjectives, 979 verbs and 664 adverbs.

The work presented in this paper is aimed at enriching this resource and increasing its coverage. However, in spite of the focus on this particular resource, the proposed methodology is general enough to be applied to the development of new wordnet resources in other languages. Before presenting our method in more detail, we will refer to a number of purely data-driven semantic analysis works developed in a multilingual setting and will explain the reasons for choosing a cross-lingual WSD method for this task. 2.3 Data-driven approaches to wordnet development 2.3.1 Corpus-based semantic analysis The methods presented in this section do not use PWN but rely only on information coming from parallel corpora for building semantic resources. The basic assumption underlying these methods is that the translations of words in real texts offer insights into their semantics (Resnik and Yarowsky 1999 ). This strong assumption, which has been widely exploited in works on cross-lingual semantic analysis (Ng and Chan 2007 ; Apidianaki 2008 ; Bannard and Callison-Burch 2005 ) and the integration of semantics in Machine Translation (Carpuat and Wu 2007 ; Chan et al. 2007 ), has also been shown to be useful for creating semantic resources in new languages.

The first purely automatic method for semantic resource creation based on parallel corpora was the Semantic Mirrors method (Dyvik 1998 , 2005 ), which discovers word senses by treating each language in a bilingual parallel corpus as the mirror of the other. Concepts and semantic relations are discovered by going back and forth between the two sides of the parallel corpus on the basis of alignment links. The extracted relations permit to organize the concepts retained in the new language in a complex lexico-semantic network similar to PWN. However, the structure of the obtained network is different than the one of PWN and the resource is not aligned to other wordnets. Translation information is also used by Ide et al. ( 2002 ) for inducing word senses from a multilingual parallel corpus. The translations of source (English) words in six languages found in the corpus serve as features for building translation vectors. The vectors are then clustered according to their similarity and the obtained clusters describe the English words X  senses. In the same vein, van der Plas and Tiedemann ( 2006 ) propose an alignment approach to synonym extraction. Translation vectors are built from the alignments of source (Dutch) words in ten languages found in a multilingual parallel corpus and the similarity of the vectors of different source words reveals their semantic proximity.
The cross-lingual method of Apidianaki ( 2008 ) combines translation and distributional information found in a bilingual parallel corpus for WSI. The translations of a source word are represented by weighted feature vectors built from the corresponding source contexts. The distributional vectors serve to cluster the translations according to their similarity and the obtained translation clusters describe the senses of source words in the corpus. The cross-lingual WSD method that we use in this work exploits the translation clusters built by applying this WSI method to an English X  X rench (EN X  X R) parallel corpus. The automatically acquired French sense clusters constitute the new synsets to be integrated into WOLF by the WSD method. The vectors that serve for translation clustering are exploited during disambiguation for finding the most adequate anchor points for the new clusters in the hierarchy. The main advantages of the proposed method are that it is fully automatic, it requires no manual translation and the semantic information is directly derived from a publicly available corpus (Europarl) which was not used for the initial construction of WOLF. The method is still dependent on the structure of PWN, however it offers an alternative way for automatically creating new wordnets or adding synsets to already existing ones while preserving the alignment of the resources.

In the next section we explain this methodological choice, as well as the advantages of using a cross-lingual rather than a monolingual method for filling wordnets in new languages. 2.3.2 Adapting a cross-lingual WSD method to wordnet development Filling empty synsets in a wordnet can be divided into two subtasks: (a) creating new clusters of synonyms (synsets), and (b) defining the place where the synsets should be located in the hierarchy. New synonym clusters can be acquired automatically by a WSI method. For the second subtask, a word sense disambig-uation method is needed.

For enriching WOLF, one option would be to acquire new synsets from monolingual French corpora and integrate them in the hierarchy. Monolingual WSI methods generally discover senses by clustering word usages on the basis of distributional information which can subsequently be used for disambiguation (Manandhar et al. 2010 ). If sense induction was performed in French, then the disambiguation method would need to exploit information in WOLF to identify where the new synsets should be placed in the hierarchy. However, as WOLF contains a high number of empty synsets, its sparsity would have a negative impact on disambiguation.

Given that WOLF has the same structure as PWN (version 2.0), an alternative to using a monolingual disambiguation method is to exploit information in the English WordNet for disambiguating the new French synsets. In this case, the new French synsets would be included in the hierarchy by means of a cross-lingual WSD classifier, based on information found in the English WordNet. The cross-lingual WSD method proposed by Apidianaki ( 2009 ) is well adapted to the task at hand for several reasons. First, it exploits the results of a WSI method that generates synset-like clusters of the translations of words in a parallel corpus (Apidianaki 2008 ). The translations are grouped together according to their semantic similarity, calculated on the basis of source language distributional information. More precisely, the translations are characterized by source language feature vectors whose similarity serves to group the translations into clusters. When applied to the EN X  X R language pair, the method clusters the French translations of English words by comparing the corresponding English context feature vectors. The obtained clusters of translations describe the senses of the English words in the corpus and contain semantically close words in French, similar to wordnet synsets. These automatically built French clusters constitute the synsets to be included in the resource.

The second reason that makes this cross-lingual WSD method well suited for this task is that the proposed WSD classifier selects French clusters for filling the empty synsets based on source language (English) information. This is due to the nature of the output of the WSI method: the generated translation clusters are characterized by weighted English feature vectors that can be used for assessing the similarity between a cluster and a synset. During disambiguation, the comparison of the vectors to information extracted from WordNet would serve to identify the most adequate synset for each French cluster. The WSI and disambiguation methods employed in this study are presented in detail in the next section. 3 Data-driven synset induction and disambiguation 3.1 Synset acquisition through word sense induction 3.1.1 Training Our WSI method is trained on the sentence aligned EN X  X R part of the Europarl corpus (release v6) (Koehn 2005 ). Prior to word alignment, we apply standard pre-processing steps: the corpus is tokenized and lowercased, and imbalanced sentence pairs, which are hard to word align, are omitted. 4 Both sides of the corpus are then lemmatized and part-of-speech (POS) tagged using the TreeTagger (Schmid 1994 ), and the corpus is aligned at the level of word types using GIZA ?? (Och and Ney 2003 ) in both directions. Two bilingual lexicons are extracted from the alignment results, one for each translation direction (EN X  X R/FR X  X N). To discard noisy alignments, the translations are filtered on the basis of their alignment score (threshold: 0.01) and according to their POS, keeping for each word only high confidence translations pertaining to the same grammatical category (i.e. we retain the noun translations of nouns, the verb translations of verbs, etc.). This eliminates erroneous alignment correspondences due to translation divergences. Finally, an intersection filter discards any translation correspondences not found in both lexicons. The translations used for clustering are the ones that translate a source word ( w ) more than ten times in the training corpus. This threshold, which was experimentally shown to perform well, leaves out some translations of the source words but has a double merit: it reduces data sparseness issues that pose problems during clustering and eliminates erroneous translations which may be present in the lexicons because of spurious alignments. 3.1.2 Semantic similarity calculation For each translation of a source word w , we extract the content words that co-occur with w in the corresponding source sentences of the parallel corpus (i.e. words that occur in the same sentence as w whenever it is translated by that translation). The retained source language words constitute the features of the vector built for each translation. For instance, four vectors are built for the translations retained from the training corpus for the English noun stage : stade , phase ,  X tape and sc X ne . The features of each vector are the content words that cooccur with stage in the source side of the aligned sentences where it is translated by each French translation, as shown in Table 1 .
 A similarity score is computed for each pair of translations using the weighted Jaccard (WJ) measure (Grefenstette 1994 ). The input of the similarity calculation consists of the co-occurrence counts of the source language features retained for the translations of the source word. The score assigned to a pair of translations indicates their degree of similarity and is computed as follows.
 Let F be the total number of features retained from the contexts of w and let N 2 F be the number of features retained for each translation T i . Each feature F j (1 B j B N ) receives a total weight with the translation [ w  X  F j ; T i  X  ], defined as the product of the feature X  X  global weight [ gw  X  F j  X  ] and its local weight with that translation [ lw  X  F j ; T i  X  ]. The global weight of a feature F j is a function of the number n of translations to which F j is related and of the probability ( p ij ) that F j co-occurs with instances of w translated by each of the translations: Each p ij is computed as the ratio of the co-occurrence counts of F j with w when translated as T i to the total number of features ( N ) seen with this translation. On the other hand, the local weight between feature F j and translation T i ( lw  X  F j ; T i  X  ) directly depends on the number of times they occur together: The intuition underlying this weighting scheme is that if an interesting semantic relation exists between a feature F j and a translation T i of w , then we expect the probability ( p ij ) of the feature F j occuring in the contexts where w is translated by this translation to be higher than if they were independent. In other words, a feature gets a high total weight with a translation when it appears frequently in the cor-responding source contexts and rarely in the contexts of the other translations of w . Recall now that the total weight of a feature with a translation is defined as follows: The WJ similarity of two translations T m and T n is then calculated using the total weight of the features that occur with each translation: Translation pairs with a score above a threshold are considered as semantically related. The threshold is defined locally for each source word using the dynamic thresholding procedure proposed by Apidianaki and He ( 2010 ). The threshold for a word w is initially set to the mean of the scores (above 0) of its translation pairs. The set of translation pairs of w is then divided into two sets ( G 1 and G 2) according to whether they exceed or are inferior to the threshold. The average of scores of the translation pairs in each set is computed ( m 1 and m 2) and constitutes the new threshold which serves to re-partition the translation pairs into two sets. The procedure is repeated until convergence.

The similarity calculation output and the similarity threshold are exploited by the clustering algorithm which groups closely related translations into sense clusters , describing the senses of the source language words. The clusters generated for the noun stage , for example, describe its two senses in the training corpus: { stade , phase ,  X tape } and { sc X ne } (i.e., the  X  X  X hase X  X  sense and the  X  X  X latform X  X  sense). The clustering procedure is detailed in the next section. 3.1.3 Semantic clustering The semantic clustering algorithm used in our experiments groups the translations into clusters by exploiting the results of the similarity calculation described in the previous section (Apidianaki 2008 ; Apidianaki and He 2010 ). The input of the algorithm for a source word w consists in: (a) the list of w  X  X  translations; (b) their similarity scores, and (c) the similarity threshold. The clustering is performed in two steps. First, each pair of translations with a similarity score above the threshold is considered as semantically close and forms an initial cluster (C). These two-element clusters are derived directly from the similarity table. During the second step, they may be enriched by additional translations by a recursive function which takes as input the cluster C and the list of translations of w , and outputs C eventually enriched by other translations. A new translation is included in a cluster if it is exceeds the threshold). The clustering stops when all the translations of w are included in some cluster and all their relations have been checked. In graph theory terms, the final clusters are characterized by global connectivity given that all their elements are linked between them by strong relations. The translations having no strong relations to any other translation of w are included in separate one-element clusters.

Using this cross-lingual WSI method, two sense cluster inventories are created from our training data: an EN X  X R inventory, where the senses of English words are described by clusters of their French translations, and a FR X  X N inventory, where the senses of French words are described by clusters of their English translations. The sense clusters group semantically similar words in the target language and could be compared to wordnet synsets. In Table 2 , we present some examples of English and French entries of different POS and degrees of polysemy. The EN verb accommodate , for instance, has four translations ( adapter, r X pondre, satisfaire, accueillir ) which are grouped in three sense clusters: { adapter, r X pondre } ( X  X  X dapt X  X  sense), { satisfaire, r X pondre } ( X  X  X atisfy X  X ) and { accueillir } ( X  X  X ut up X  X  sense). The first two clusters overlap (they both contain the French verb r X pondre ), which means that the described senses are probably related. The cluster overlaps could actually serve as clues to their merge, if coarser-grained sense descriptions were needed. However, as a translation might be found in the intersection of two clusters because of being ambiguous between the two senses, a merge would be more reliable if the intersection contained more than one element.

Here, the sense clusters are used for filling French synsets corresponding to PWN synsets, which are characterized by fine granularity. As wordnet synsets might in general contain the same literals, the cluster overlaps pose no problem in this context. Consequently, clusters are not merged but used as proposed by the WSI method. 3.2 Sense clusters integration into WOLF The automatically built EN X  X R inventory contains entries for English words of different parts of speech. In this first experiment, we focus on word meanings that correspond to empty synsets in WOLF. In future work, we intend to further enrich non empty synsets (i.e. synsets that already contain one or more French literals) with additional information found in the sense clusters.

We use the cross-lingual WSD method proposed by Apidianaki ( 2009 ) which exploits the output of the WSI method presented in the previous section. In a monolingual data-driven WSD task, the clusters obtained for a word by clustering its instances in a monolingual corpus would constitute its candidate senses from which the most adequate one would have to be selected for new instances of the word in context. This selection would be performed by comparing the cluster vectors to information in the new context.

In the current setting, the goal of the WSD method is to assign French clusters to empty synsets in WOLF using English feature vectors. So, the information exploited for WSD consists in the words found in the corresponding English synsets (in PWN) and their related synsets, their definitions and usage examples. Given that information in the vectors built from the training corpus is lemmatized, the information retained from PWN is lemmatized as well (Schmid 1994 ) and gathered in a bag of words. The adequacy of a cluster ( C ) for filling a given synset ( S )is estimated by comparing the vectors of the clustered translations to the information retained from PWN for the synset. If common features (CFs) are found with just one cluster, this cluster is selected. Otherwise, each  X  X luster-synset X  association is assigned a score corresponding to the mean of the weights of the CFs with the clustered translations [weights assigned to each feature during WSI (cf. Sect. 3.1 )]. In formula 6 , CF j j CF j j  X  1 is the set of CFs between the cluster and the synset, and N CF is the number of translations T i in the cluster characterized by a feature CF (i.e. translations having the CF in their vector). The cluster that receives the highest score is selected and assigned to the empty synset.
 For instance, the empty synset  X  X dd#a#2 X  (definition:  X  X  X ot easily explained X  X ; usage:  X  X  X t is odd that his name is never mentioned X  X ), is correctly filled by the French cluster { curieux , bizarre }. The other clusters available for odd , which do not fit this synset and get lower scores, are: { contradictoire , singulier , bizarre } and { curieux ,  X trange }. More examples of synsets filled by the WSD method are shown in Table 3 . We provide the PWN id of the empty synsets in WOLF, the English headword, the literals in the corresponding PWN synsets, as well as their definitions and usage examples. 5 , 6 The French literals in the sense cluster most strongly associated with a PWN synset, which are used to fill the corresponding synset in WOLF, are given in the last column of Table 3 .
 The process of selecting the French synset that best suits a cluster on the basis of English contextual information is illustrated with the example given in Table 4 .It details the case of the English adjective peaceful which belongs to three synsets in PWN, all empty in WOLF. The WSD method has to fill one of these synsets with the French cluster { paisible , pacifique }. Each of the PWN synsets for peaceful is shown in Table 4 (including literals, definition and usage examples) together with some of the related synsets that are used to build the corresponding bags of words. The features that characterize, at the same time, the cluster vector and one of the synsets are shown in boldface. The bag of words representing the synset ENG20-01686906-a is the closest to that of the vector of the French cluster, and the synset also gets the highest score during WSD. Therefore, paisible and pacifique are added to synset ENG20-01686906-a in the WOLF. 4 Evaluation results Our approch fills 3,904 previously empty synsets in WOLF: 2,333 nominal, 576 verbal, 709 adjectival and 286 adverbial synsets. Given that no gold standard is available for this task X  X hich would permit to perform an automatic evaluation X  we have manually examined 10 % of the synsets filled for each POS to evaluate the quality of the proposed clusters and the correctness of their assignment to some synset in WOLF according to the following criteria:  X  a cluster is considered as a good quality one if it groups words that share the  X  the assignment of a cluster to a synset is considered as correct if its contents A cluster can be correctly assigned to a synset only if it is of good quality according to the first evaluation criterion. Consequently, all WSD assignments involving noisy clusters are considered as wrong assignments. We consider as noisy the clusters that contain one or more translations that are not semantically close to the others, even if the rest of the translations in the cluster are synonymous.

Both aspects have been evaluated by two annotators. The inter-annotator agreement was measured at j = 0.67 for cluster quality and 0.59 for the WSD results, which is conventionally interpreted as  X  X  X ood X  X  agreement (Cohen 1960 ). The lower agreement obtained for disambiguation is unsurprising, given the closeness of the considered synsets which correspond to fine-grained sense distinctions in PWN. As has been shown by Erk and McCarthy ( 2009 ), multiple WordNet senses might apply to an occurrence of a polysemous word in context, an argument in favor of graded sense assignments (Jurgens and Klapaftis 2013 ). Here, we are looking for the best-fitting WordNet sense for a cluster of translations (not for words in context) but the same effect of varying sense applicability occurs in this setting.

The evaluation results are presented in Table 5 . According to the results obtained for all POS, the clusters group semantically similar words in 75.5 % of the cases. Significant variations are however observed for different POS. The first row of the table contains the percentage of good quality clusters in the test set. The second row of the table shows the percentage of the clusters that were correctly assigned to WOLF synsets by the WSD method. Given that according to our evaluation criteria only good clusters can be correctly integrated into WOLF, we calculate the (conditional) accuracy of the WSD method by reference to the number of good clusters. This accuracy for WSD insertions on all POS is 67 %, which is very encouraging. 7 Table 5 shows that for WSD, as is the case for WSI, the results vary from one POS to another. We also provide overall accuracy results for the whole task, i.e. the proportion of automatically obtained clusters that are both good clusters (grouping words that share the same meaning) and assigned to the correct synset. This overall accuracy is computed directly as the WSI accuracy times the WSD conditional accuracy (recall that the latter was computed only on good clusters). In a setting where a manually compiled bilingual dictionary would be available and the candidate senses would not contain any noise, the focus would be put only on the accuracy of the WSD method.

The divergences observed between different parts of speech are due to the restrictive cluster quality criterion according to which one incorrect word in an otherwise correct cluster turns the whole cluster into an incorrect one. This strict criterion unfairly penalizes and rejects interesting although noisy clusters. We notice that this constraint has a strong impact during evaluation especially on clusters with many translations, like the verb clusters. We plan to proceed to a more detailed and flexible evaluation in order to more accurately estimate the actual merit of the clustering method, which will also imply devising methods for cleaning noisy clusters. In a semi-automatic setting, the manual cleaning of the noisy clusters by a lexicographer would considerably improve their integration in the French resource.
We should highlight the difficulty of the disambiguation task as the WSD method is asked to fill synsets that were left empty by the methods initially employed for creating WOLF. These empty synsets often correspond to rare senses in PWN that may not exist in the training corpus, or to senses for which little information is available in PWN. The role of the training corpus is particularly important in data-driven WSI and disambiguation. Given that the training corpus used in our experiments contains parliamentary proceedings, the derived senses are not always adequate for filling a general language resource like WOLF, as senses represented by empty synsets might not be present in the corpus.

In order to more fairly estimate the performance of the WSD method in this setting, we also tested it on the whole resource. In this case, the method was asked to select the most appropriate synset for each cluster from all synsets in WOLF (not only the empty ones). In this setting, the WSD method reaches a much higher performance of 80.13 %, as shown in Table 6 , which shows that it is particularly well adapted to the wordnet development task. Table 6 contains detailed results for words of different parts of speech. 5 Discussion and perspectives 5.1 Analysis of the errors in the clustering output From a close examination of the clustering results, two main sources of errors were identified. In some cases, the noise found in the clusters is due to alignment errors that were not detected and eliminated by the filters that served to clean the lexicons (cf. Sect. 3.1 ). In other cases, the noise is introduced during clustering. The error analysis indicates some cases of problematic clustering that fall into the second category: (a) cases where multiword units were not considered during word alignment. This (b) clustering of topically related but not synonymous words, as in the cluster (c) clustering of antonymous but distributionally similar words, as in the case of 5.2 Generating coarser wordnets using cross-lingual WSD A common criticism of PWN is the high granularity of the proposed semantic descriptions which, combined to the great number and similarity of the senses, might hamper the efficient use of this resource for WSD (Edmonds and Kilgarriff 2002 ; Ng et al. 2003 ). Fine sense distinctions increase the processing complexity and the risk of information loss when a forced choice among closely related senses has to be made without considering their relations (Dolan 1994 ). As pointed out by Ide and Wilks ( 2006 ), this fine granularity is not even necessary for efficient WSD in NLP applications where disambiguation, when needed, mostly involves homonym-level distinctions. In the rare cases where finer-grained distinctions are needed, they should be handled by more robust types of processing. As discussed earlier in this paper, an additional problem posed by the high granularity of PWN during the building of new wordnets is that it is difficult to find correspondences for fine-grained senses in the new languages. This results in resources much sparser than PWN, containing numerous empty synsets. Furthermore, the difficulty to establish correspondences between fine-grained senses and their translations makes difficult the exploitation of the resources in multilingual applications (Specia et al. 2006 ).

The granularity of wordnet-like resources can be reduced by identifying the similarity of the proposed senses. 8 Several attempts have been made for reducing the polysemy of words in PWN and the granularity of their senses. Peters et al. ( 1998 ) perform automatic sense clustering of nouns and verbs using the relations defined in WordNet (sisters, autohyponyms, twins and cousins). Mihalcea and Moldovan ( 2002 ) apply three principles from the lexical semantics litterature (Cruse 1986 ) to measuring the ambiguity level between PWN synsets of all parts of speech. The proposed rules take into account the overlaps between the elements in different synsets and their relations to other synsets in the hierarchy (hypernyms, antonyms and pertainyms). Synsets whose similarity is high enough according to these rules are collapsed together into one. Furthermore, the polysemy of WordNet is reduced based on the frequency of senses and the probability of a synset occurring in texts (as measured on SemCor, a corpus sense-tagged with PWN synsets). Synsets with very low probability of occurrence are dropped and the number of senses of polysemous words is reduced. So, the semantic principles result in collapsed synsets, while the probabilistic principles determine which synsets can be discarded. The application of these two types of rules results in a reduction in the number of synsets and, consequently, in the number of word senses.

The output of the cross-lingual WSD method presented in this paper could also serve to reduce the granularity of the new wordnet resource and, consequently, the degree of polysemy in PWN. This can be done by collapsing highly similar synsets together in the new wordnets and identifying the relations between the corresponding PWN synsets. As explained in the previous sections, the enrichment of WOLF synsets with new literals was performed by finding the most adequate synset for each translation cluster . The contents of the cluster were then used to fill the selected synset. It would however also be possible to proceed the other way around, i.e. to seek the most adequate cluster for each synset . In this way, the same cluster could be associated to different PWN synsets and this would serve as a clue for measuring the similarity of the synsets and merging them. The examples presented in Table 7 illustrate cases of one-to-many associations established between clusters and PWN/WOLF synsets, where the clusters given in the last column are associated to several empty synsets of the source word. For instance, the third cluster of noun waste :{ perte, gaspillage } (other clusters for the word: { ordure }, { d X chet, gaspillage }) is associated by the WSD method to three empty synsets of waste in WOLF, whose glosses are given in the third column of Table 7 . Each cluster-synset association is weighted during disambiguation (cf. Sect. 3.2 ) and the scores in column 4 show the strength of the association; the higher the score, the stronger the association between the cluster and the synset. In the same way, the cluster { perturbation, trouble } is assigned to four empty synsets of the word disturbance , while both empty synsets of birthday are assigned the cluster { anniversaire }.

An alternative way to establish one-to-many correspondences would be to retain for each cluster not just the synset that gets the best association score during WSD but more than one high scored synsets. In this case, a careful study of the proposed associations could help to define a threshold that would reflect good assignments. The establishment of one-to-many correspondences can serve to fill multiple WOLF synsets at once or to merge them into one, before assigning the contents of the cluster. Furthermore, these  X  X luster-synset X  associations can serve to merge the corresponding PWN synsets, to reduce the granularity of the resource and facilitate the establishment of correspondences with words in the target language. This information seems thus to be particularly relevant for creating coarser-grained resources. Nevertheless, although successful in several cases, the output of this process should be treated with caution as noisy associations might lead to erroneous merges. In future work, we intend to explore ways for identifying strong correspondences and ruling out erroneous ones but until then, the method would be better suited to a semi-supervised setting where the proposed associations could be manually validated by a lexicographer. 6 Conclusion We have proposed a novel approach to developing and enriching wordnet resources in languages other than English. We have shown how a cross-lingual word sense disambiguation method can be used to assign content to otherwise empty synsets in newly built wordnet resources. Our WSD method exploits the results of a data-driven sense induction method which discovers the senses of English words by grouping their French translations into sense clusters. The obtained clusters are integrated by the WSD method into the French wordnet resource WOLF based on information found in PWN, to which the WOLF is aligned. The results indicate that the proposed methods are particularly useful for building wordnets in new languages. Moreover, given that wordnet resources in languages other than English are often aligned to PWN, the cross-lingual WSD method can be used to enrich these resources and increase their coverage.

Our work shows that WSI and disambiguation methods can efficiently collaborate for enriching lexico-semantic resources by mapping senses automati-cally extracted from parallel data to a manually developed sense inventory like PWN. It is therefore a valuable and complementary alternative to more common approaches that leverage multilingual lexicons extracted from dictionaries. Still, if such lexicons are available they could replace the sense induction output and be directly exploited by the disambiguation method for enriching wordnet resources or creating new ones.

Based on these encouraging results, we aim at extending the use of the proposed cross-lingual WSD method for enriching non-empty WOLF synsets with additional literals. This is because some synsets are incomplete and lack some literals which could be retrieved from corpora by the methods introduced in this paper. As explained above, the quality of the automatically acquired synsets and the performance of the disambiguation method strongly depend on the parallel corpora used for training. The present study was carried out using information extracted from the Europarl corpus. We would like to include more diverse training corpora in order to obtain richer semantic representations and further extend WOLF X  X  coverage. Last but not least, we intend to explore methods for discarding synsets corresponding to rare senses in PWN or to senses that have no counterpart in French, and for merging semantically close synsets in order to reduce the granularity and the sparseness of the French resource. The output of the disambiguation method would be particularly useful to this aim given that the method can establish one-to-many correspondences between clusters and synsets, highlighting in this way their semantic similarity that can serve to their grouping.
 References
 Abstract Automatic methods for wordnet development in languages other than English generally exploit information found in Princeton WordNet (PWN) and translations extracted from parallel corpora. A common approach consists in pre-serving the structure of PWN and transferring its content in new languages using alignments, possibly combined with information extracted from multilingual semantic resources. Even if the role of PWN remains central in this process, these automatic methods offer an alternative to the manual elaboration of new wordnets. However, their limited coverage has a strong impact on that of the resulting resources. Following this line of research, we apply a cross-lingual word sense disambiguation method to wordnet development. Our approach exploits the output of a data-driven sense induction method that generates sense clusters in new lan-guages, similar to wordnet synsets, by identifying word senses and relations in parallel corpora. We apply our cross-lingual word sense disambiguation method to the task of enriching a French wordnet resource, the WOLF, and show how it can be efficiently used for increasing its coverage. Although our experiments involve the English X  X rench language pair, the proposed methodology is general enough to be applied to the development of wordnet resources in other languages for which parallel corpora are available. Finally, we show how the disambiguation output can serve to reduce the granularity of new wordnets and the degree of polysemy present in PWN.
 Keywords Cross-lingual word sense disambiguation Word sense induction Sense clustering Parallel corpora WordNet 1 Introduction The growing need for lexical and semantic knowledge in natural language processing (NLP) applications has steered several initiatives for resource develop-ment in recent years. A common trend has been to develop multilingual resources based on Princeton WordNet (PWN) (Fellbaum 1998 ): the structure of PWN is generally preserved and its contents are translated in new languages (Vossen 1998 ; Pianta et al. 2002 ;Tufis  X  et al. 2004 ). The advantages of this approach, which explain its wide adoption, are that it avoids the time-consuming manual elaboration of the semantic hierarchy in new languages and allows the alignment of the resulting wordnets, a feature particularly useful for multilingual NLP. Its main limitation is the strong bias imposed by PWN on the content and structure of the newly built wordnets. The structure of PWN is preserved in the target language and its content is transferred based on an assumption of language independence of concepts and semantic relations. However, concepts present in PWN might not be present in the target language, in which case the corresponding synsets in the new wordnet cannot be filled. This issue becomes more important in case of fine-grained sense distinctions in PWN, where finding target language counterparts becomes difficult or impossible. Combined to limitations due to the quantity of information available in the bilingual dictionaries used for manually translating PWN synsets into new languages, these factors have a strong impact on the coverage of the resulting wordnets which is generally much smaller than that of PWN.

In an attempt to address these weaknesses, several automatic wordnet develop-ment methods have been proposed that exploit information found in parallel corpora. These methods permit to acquire semantic information from texts and to circumvent, in this way, the need for pre-defined resources. Moreover, by exploiting alignment information, these methods offer an alternative to the manual filling of wordnets: translations are extracted from parallel corpora and are automatically integrated in the wordnet hierarchy. Nevertheless, the success and coverage of these methods highly depends on the nature of the parallel corpora and on the way the extracted information is used for wordnet filling. Coverage becomes an issue especially when automatically acquired translations are used to fill PWN-based resources, as rare senses (present in PWN) might not be found in the parallel corpora. Additionally, automatic methods often fail to retrieve semantic information as fine-grained as the one found in PWN, leaving numerous synsets empty. As a consequence, the resulting wordnet resources are rather sparse and methods for increasing their coverage are needed.

Following this line of research, we propose a novel automatic approach to wordnet development. We demonstrate how a cross-lingual word sense disambig-uation (WSD) method can be applied to the wordnet development task for creating new resources or for enriching existing ones. WSD is the task of automatically identifying the meaning of words in context (Navigli 2009 ) while its cross-lingual variant predicts semantically correct translations (Apidianaki 2009 ; Lefever and Hoste 2010 ). In this work, we apply a cross-lingual WSD method (Apidianaki 2009 ) to the enrichment of an automatically built wordnet for French, the WOLF (Sagot and Fis  X  er 2008 ). The disambiguation method exploits the output of a cross-lingual word sense induction (WSI) method which identifies word senses and their relations in parallel corpora. The WSI method generates clusters of semantically related words in the new language (French) similar to wordnet synsets, which are integrated in the WOLF hierarchy by the cross-lingual WSD method.

The paper is organized as follows. Section 2 presents related work on wordnet development in languages other than English. We first describe the classical approach to cross-lingual transfer of WordNet information based on pre-defined lexical and semantic resources. Then we move to a number of automatic methods which combine existing resources with information extracted from corpora and describe the method that initially served to build WOLF, the French resource that we aim to enrich. Finally, we present a number of purely data-driven semantic analysis methods and explain our choice to apply a WSD method to the wordnet development task. Section 3 presents our data-driven synset induction method as well as the disambiguation method that serves to integrate the newly acquired synsets in the French resource. In Sect. 4 , we present the results of a manual evaluation intended to estimate the quality of the clustering and the correctness of the new WOLF entries. The main findings of this study are summed up in the last section where we also present some avenues worth pursuing in future work, before concluding. 2 Cross-lingual approaches to wordnet development 2.1 Transfer of WordNet information to new languages Multilingual wordnet development has always strongly relied on PWN (Fellbaum 1998 ). Large-scale projects aiming the creation of wordnets in languages other than English, such as EuroWordNet, BalkaNet and MultiWordNet (Vossen 1998 ; Pianta et al. 2002 ;Tufis  X  et al. 2004 ), have adopted a translation-driven approach: the structure of PWN was preserved while its contents were imported in the newly built resources by applying various translation-based methods. The main advantage of this approach, also called the expand model , is that it permits to avoid the time-consuming and expensive manual elaboration of the semantic hierarchy in new languages. An additional advantage is that the newly built wordnet is automatically aligned to PWN and to other wordnets built following the same principle. The resulting resources are thus interesting for contrastive semantic analysis and can be particularly useful in multilingual NLP tasks, such as Multilingual Information Retrieval.

Despite its strengths, the translation approach to wordnet development also presents a number of drawbacks. One of them is the strong bias imposed by PWN on the content and structure of the new wordnets. The structure of PWN is generally preserved and its content is transferred in the new languages based on the assumption that concepts and semantic relations between them are X  X t least to a large extent X  X anguage independent. This assumption is not theoretically valid and has important practical implications during the compilation of new wordnets. Several senses present in PWN have no target language counterpart, a problem that becomes more apparent in the case of fine-grained WordNet senses. As a consequence, a varying number of target language synsets may be left unfilled, depending on the language, and this sparseness limits the usefulness of the newly built resource in NLP applications.

Other issues posed by the translation approach are its heavy reliance on external lexico-semantic resources and the manual work needed for transfer. In EuroWord-Net, BalkaNet and MultiWordNet, PWN literals were mainly translated by human lexicographers using external resources such as dictionaries, thesaurus and taxonomies. 1 Apart from limiting the approach to specific language pairs, the reliance on pre-defined resources introduces a new bias as their coverage has a strong impact on the one of the newly built wordnets. 2.2 Automatic cross-lingual information transfer 2.2.1 Combining lexicographic resources and parallel corpora In spite of the theoretical and practical drawbacks inherent to the translation approach, new wordnets are still heavily based on PWN. The methods used for transferring information into new languages have however evolved towards becoming more or less automatic, limiting the cost of the manual methods employed before. Moreover, these methods often exploit lexico-semantic informa-tion extracted from monolingual or multilingual corpora, instead of solely relying on pre-defined semantic resources. For instance, the French hierarchy WOLF (Sagot and Fis  X  er 2008 ), that we intend to enrich in this work, was automatically built by combining information from several multilingual resources (the EUROVOC thesaurus and Wikipedia-related resources) with information extracted from a multilingual parallel corpus. Another PWN-based resource for French, the JAWS network, was compiled by combining a bilingual dictionary and syntactic information acquired from corpora for disambiguating polysemous nouns and correctly integrating them in the hierarchy (Mouton and de Chalendar 2010 ).
The multilingual semantic network BabelNet goes a step further by jointly exploiting PWN, Wikipedia and the output of statistical Machine Translation systems (Navigli and Ponzetto 2010 ). In BabelNet, PWN and Wikipedia are combined by automatically mapping WordNet senses and Wikipages and comple-menting their respective concept inventories. Multilingual lexicalizations of the concepts are acquired from the human-generated translations provided in Wikipedia and by using a statistical Machine Translation system to translate occurrences of the concepts within sense-tagged corpora. The resulting multilingual network has a wide coverage, as it contains 9 million entries (concepts and named entities) in 50 languages. 2 In a different setting, aiming the semantic annotation of new languages, Diab and Resnik ( 2002 ) combine translation information from a parallel corpus with semantic information in PWN. The possible semantic tags provided in PWN for the English translations of a foreign word are found, and the one characterizing the whole set of translations is selected and used as the foreign word X  X  sense tag.

All these approaches successfuly combine information in PWN and other lexical and semantic resources with information learned from corpora. In the next section, we provide more information on the WOLF resource that we intend to enrich, the way it was compiled, its content and its coverage. 2.2.2 The WOLF resource WOLF (Sagot and Fis  X  er 2008 ) is a freely available wordnet for French. Its first version (WOLF 0.1.4) was created on the basis of PWN (version 2.0) by following the expand model for wordnet development and relying on both existing resources and parallel corpora. In this section, we briefly sketch the approach that was used for building WOLF version 0.1.4. A more detailed discussion on the construction of the resource and its evaluation can be found in earlier publications (Sagot and Fis  X  er 2008 ).

To fill the WOLF, monosemous literals in the PWN were automatically translated using a bilingual French X  X nglish lexicon built from various multilingual resources. In particular, data was extracted from Wikipedia using inter-wiki links, from the English and French Wiktionary, the Wikispecies encyclopedia of living beings and the EUROVOC thesaurus ( http://europa.eu/eurovoc ).

Polysemous PWN literals were handled by an alignment approach (cf. Sect. 2.3 ) based on the multilingual parallel corpus SEE-ERA.NET (Steinberger et al. 2006 ), which is composed of the English, French, Romanian, Czech and Bulgarian parts of the JRC-Acquis corpus. The corpus was lemmatized, part of speech (POS) tagged and word aligned, and bilingual lexicons were automatically built including the translations of English words in different languages. These lexicons were then combined into various multilingual lexicons (3-lingual to 5-lingual) and a synset id was assigned to each lexicon entry by gathering all possible ids for this entry in all languages from the corresponding BalkaNet wordnets, which share the same inventory of synset ids. The underlying assumption being that it is unlikely that the same polysemy occurs in different languages, the intersection of the possible senses was expected to output only the correct synset. In this way, the ids shared by all non-French lexicon entries were assigned to their French translation. For example, among the various French X  X zech X  X ulgarian X  X nglish alignments involving the French word droit and the English word law , the alignment droit-pr X vo --law was found 56 times. The only synset that contained pr X vo in the Czech wordnet, 05791721-n. Therefore, droit could be added in the WOLF in this synset.

The synsets obtained for monosemous and polysemous literals by these two approaches were merged. The resulting network preserves the hierarchy and structure of PWN 2.0 and contains the definitions and usage examples provided in PWN for each synset. As information was not found for all PWN synsets by the employed automatic methods, the version 0.1.6 of WOLF which is used in our experiments is rather sparse. 3 In total, it contains 32,351 non-empty synsets including 37,991 unique literals (vs. 115,424 synsets with 145,627 literals in PWN 2.0). These synsets are filled with 34,827 unique French noun literals, 1,521 adjectives, 979 verbs and 664 adverbs.

The work presented in this paper is aimed at enriching this resource and increasing its coverage. However, in spite of the focus on this particular resource, the proposed methodology is general enough to be applied to the development of new wordnet resources in other languages. Before presenting our method in more detail, we will refer to a number of purely data-driven semantic analysis works developed in a multilingual setting and will explain the reasons for choosing a cross-lingual WSD method for this task. 2.3 Data-driven approaches to wordnet development 2.3.1 Corpus-based semantic analysis The methods presented in this section do not use PWN but rely only on information coming from parallel corpora for building semantic resources. The basic assumption underlying these methods is that the translations of words in real texts offer insights into their semantics (Resnik and Yarowsky 1999 ). This strong assumption, which has been widely exploited in works on cross-lingual semantic analysis (Ng and Chan 2007 ; Apidianaki 2008 ; Bannard and Callison-Burch 2005 ) and the integration of semantics in Machine Translation (Carpuat and Wu 2007 ; Chan et al. 2007 ), has also been shown to be useful for creating semantic resources in new languages.

The first purely automatic method for semantic resource creation based on parallel corpora was the Semantic Mirrors method (Dyvik 1998 , 2005 ), which discovers word senses by treating each language in a bilingual parallel corpus as the mirror of the other. Concepts and semantic relations are discovered by going back and forth between the two sides of the parallel corpus on the basis of alignment links. The extracted relations permit to organize the concepts retained in the new language in a complex lexico-semantic network similar to PWN. However, the structure of the obtained network is different than the one of PWN and the resource is not aligned to other wordnets. Translation information is also used by Ide et al. ( 2002 ) for inducing word senses from a multilingual parallel corpus. The translations of source (English) words in six languages found in the corpus serve as features for building translation vectors. The vectors are then clustered according to their similarity and the obtained clusters describe the English words X  senses. In the same vein, van der Plas and Tiedemann ( 2006 ) propose an alignment approach to synonym extraction. Translation vectors are built from the alignments of source (Dutch) words in ten languages found in a multilingual parallel corpus and the similarity of the vectors of different source words reveals their semantic proximity.
The cross-lingual method of Apidianaki ( 2008 ) combines translation and distributional information found in a bilingual parallel corpus for WSI. The translations of a source word are represented by weighted feature vectors built from the corresponding source contexts. The distributional vectors serve to cluster the translations according to their similarity and the obtained translation clusters describe the senses of source words in the corpus. The cross-lingual WSD method that we use in this work exploits the translation clusters built by applying this WSI method to an English X  X rench (EN X  X R) parallel corpus. The automatically acquired French sense clusters constitute the new synsets to be integrated into WOLF by the WSD method. The vectors that serve for translation clustering are exploited during disambiguation for finding the most adequate anchor points for the new clusters in the hierarchy. The main advantages of the proposed method are that it is fully automatic, it requires no manual translation and the semantic information is directly derived from a publicly available corpus (Europarl) which was not used for the initial construction of WOLF. The method is still dependent on the structure of PWN, however it offers an alternative way for automatically creating new wordnets or adding synsets to already existing ones while preserving the alignment of the resources.

In the next section we explain this methodological choice, as well as the advantages of using a cross-lingual rather than a monolingual method for filling wordnets in new languages. 2.3.2 Adapting a cross-lingual WSD method to wordnet development Filling empty synsets in a wordnet can be divided into two subtasks: (a) creating new clusters of synonyms (synsets), and (b) defining the place where the synsets should be located in the hierarchy. New synonym clusters can be acquired automatically by a WSI method. For the second subtask, a word sense disambig-uation method is needed.

For enriching WOLF, one option would be to acquire new synsets from monolingual French corpora and integrate them in the hierarchy. Monolingual WSI methods generally discover senses by clustering word usages on the basis of distributional information which can subsequently be used for disambiguation (Manandhar et al. 2010 ). If sense induction was performed in French, then the disambiguation method would need to exploit information in WOLF to identify where the new synsets should be placed in the hierarchy. However, as WOLF contains a high number of empty synsets, its sparsity would have a negative impact on disambiguation.

Given that WOLF has the same structure as PWN (version 2.0), an alternative to using a monolingual disambiguation method is to exploit information in the English WordNet for disambiguating the new French synsets. In this case, the new French synsets would be included in the hierarchy by means of a cross-lingual WSD classifier, based on information found in the English WordNet. The cross-lingual WSD method proposed by Apidianaki ( 2009 ) is well adapted to the task at hand for several reasons. First, it exploits the results of a WSI method that generates synset-like clusters of the translations of words in a parallel corpus (Apidianaki 2008 ). The translations are grouped together according to their semantic similarity, calculated on the basis of source language distributional information. More precisely, the translations are characterized by source language feature vectors whose similarity serves to group the translations into clusters. When applied to the EN X  X R language pair, the method clusters the French translations of English words by comparing the corresponding English context feature vectors. The obtained clusters of translations describe the senses of the English words in the corpus and contain semantically close words in French, similar to wordnet synsets. These automatically built French clusters constitute the synsets to be included in the resource.

The second reason that makes this cross-lingual WSD method well suited for this task is that the proposed WSD classifier selects French clusters for filling the empty synsets based on source language (English) information. This is due to the nature of the output of the WSI method: the generated translation clusters are characterized by weighted English feature vectors that can be used for assessing the similarity between a cluster and a synset. During disambiguation, the comparison of the vectors to information extracted from WordNet would serve to identify the most adequate synset for each French cluster. The WSI and disambiguation methods employed in this study are presented in detail in the next section. 3 Data-driven synset induction and disambiguation 3.1 Synset acquisition through word sense induction 3.1.1 Training Our WSI method is trained on the sentence aligned EN X  X R part of the Europarl corpus (release v6) (Koehn 2005 ). Prior to word alignment, we apply standard pre-processing steps: the corpus is tokenized and lowercased, and imbalanced sentence pairs, which are hard to word align, are omitted. 4 Both sides of the corpus are then lemmatized and part-of-speech (POS) tagged using the TreeTagger (Schmid 1994 ), and the corpus is aligned at the level of word types using GIZA ?? (Och and Ney 2003 ) in both directions. Two bilingual lexicons are extracted from the alignment results, one for each translation direction (EN X  X R/FR X  X N). To discard noisy alignments, the translations are filtered on the basis of their alignment score (threshold: 0.01) and according to their POS, keeping for each word only high confidence translations pertaining to the same grammatical category (i.e. we retain the noun translations of nouns, the verb translations of verbs, etc.). This eliminates erroneous alignment correspondences due to translation divergences. Finally, an intersection filter discards any translation correspondences not found in both lexicons. The translations used for clustering are the ones that translate a source word ( w ) more than ten times in the training corpus. This threshold, which was experimentally shown to perform well, leaves out some translations of the source words but has a double merit: it reduces data sparseness issues that pose problems during clustering and eliminates erroneous translations which may be present in the lexicons because of spurious alignments. 3.1.2 Semantic similarity calculation For each translation of a source word w , we extract the content words that co-occur with w in the corresponding source sentences of the parallel corpus (i.e. words that occur in the same sentence as w whenever it is translated by that translation). The retained source language words constitute the features of the vector built for each translation. For instance, four vectors are built for the translations retained from the training corpus for the English noun stage : stade , phase ,  X tape and sc X ne . The features of each vector are the content words that cooccur with stage in the source side of the aligned sentences where it is translated by each French translation, as shown in Table 1 .
 A similarity score is computed for each pair of translations using the weighted Jaccard (WJ) measure (Grefenstette 1994 ). The input of the similarity calculation consists of the co-occurrence counts of the source language features retained for the translations of the source word. The score assigned to a pair of translations indicates their degree of similarity and is computed as follows.
 Let F be the total number of features retained from the contexts of w and let N 2 F be the number of features retained for each translation T i . Each feature F j (1 B j B N ) receives a total weight with the translation [ w  X  F j ; T i  X  ], defined as the product of the feature X  X  global weight [ gw  X  F j  X  ] and its local weight with that translation [ lw  X  F j ; T i  X  ]. The global weight of a feature F j is a function of the number n of translations to which F j is related and of the probability ( p ij ) that F j co-occurs with instances of w translated by each of the translations: Each p ij is computed as the ratio of the co-occurrence counts of F j with w when translated as T i to the total number of features ( N ) seen with this translation. On the other hand, the local weight between feature F j and translation T i ( lw  X  F j ; T i  X  ) directly depends on the number of times they occur together: The intuition underlying this weighting scheme is that if an interesting semantic relation exists between a feature F j and a translation T i of w , then we expect the probability ( p ij ) of the feature F j occuring in the contexts where w is translated by this translation to be higher than if they were independent. In other words, a feature gets a high total weight with a translation when it appears frequently in the cor-responding source contexts and rarely in the contexts of the other translations of w . Recall now that the total weight of a feature with a translation is defined as follows: The WJ similarity of two translations T m and T n is then calculated using the total weight of the features that occur with each translation: Translation pairs with a score above a threshold are considered as semantically related. The threshold is defined locally for each source word using the dynamic thresholding procedure proposed by Apidianaki and He ( 2010 ). The threshold for a word w is initially set to the mean of the scores (above 0) of its translation pairs. The set of translation pairs of w is then divided into two sets ( G 1 and G 2) according to whether they exceed or are inferior to the threshold. The average of scores of the translation pairs in each set is computed ( m 1 and m 2) and constitutes the new threshold which serves to re-partition the translation pairs into two sets. The procedure is repeated until convergence.

The similarity calculation output and the similarity threshold are exploited by the clustering algorithm which groups closely related translations into sense clusters , describing the senses of the source language words. The clusters generated for the noun stage , for example, describe its two senses in the training corpus: { stade , phase ,  X tape } and { sc X ne } (i.e., the  X  X  X hase X  X  sense and the  X  X  X latform X  X  sense). The clustering procedure is detailed in the next section. 3.1.3 Semantic clustering The semantic clustering algorithm used in our experiments groups the translations into clusters by exploiting the results of the similarity calculation described in the previous section (Apidianaki 2008 ; Apidianaki and He 2010 ). The input of the algorithm for a source word w consists in: (a) the list of w  X  X  translations; (b) their similarity scores, and (c) the similarity threshold. The clustering is performed in two steps. First, each pair of translations with a similarity score above the threshold is considered as semantically close and forms an initial cluster (C). These two-element clusters are derived directly from the similarity table. During the second step, they may be enriched by additional translations by a recursive function which takes as input the cluster C and the list of translations of w , and outputs C eventually enriched by other translations. A new translation is included in a cluster if it is exceeds the threshold). The clustering stops when all the translations of w are included in some cluster and all their relations have been checked. In graph theory terms, the final clusters are characterized by global connectivity given that all their elements are linked between them by strong relations. The translations having no strong relations to any other translation of w are included in separate one-element clusters.

Using this cross-lingual WSI method, two sense cluster inventories are created from our training data: an EN X  X R inventory, where the senses of English words are described by clusters of their French translations, and a FR X  X N inventory, where the senses of French words are described by clusters of their English translations. The sense clusters group semantically similar words in the target language and could be compared to wordnet synsets. In Table 2 , we present some examples of English and French entries of different POS and degrees of polysemy. The EN verb accommodate , for instance, has four translations ( adapter, r X pondre, satisfaire, accueillir ) which are grouped in three sense clusters: { adapter, r X pondre } ( X  X  X dapt X  X  sense), { satisfaire, r X pondre } ( X  X  X atisfy X  X ) and { accueillir } ( X  X  X ut up X  X  sense). The first two clusters overlap (they both contain the French verb r X pondre ), which means that the described senses are probably related. The cluster overlaps could actually serve as clues to their merge, if coarser-grained sense descriptions were needed. However, as a translation might be found in the intersection of two clusters because of being ambiguous between the two senses, a merge would be more reliable if the intersection contained more than one element.

Here, the sense clusters are used for filling French synsets corresponding to PWN synsets, which are characterized by fine granularity. As wordnet synsets might in general contain the same literals, the cluster overlaps pose no problem in this context. Consequently, clusters are not merged but used as proposed by the WSI method. 3.2 Sense clusters integration into WOLF The automatically built EN X  X R inventory contains entries for English words of different parts of speech. In this first experiment, we focus on word meanings that correspond to empty synsets in WOLF. In future work, we intend to further enrich non empty synsets (i.e. synsets that already contain one or more French literals) with additional information found in the sense clusters.

We use the cross-lingual WSD method proposed by Apidianaki ( 2009 ) which exploits the output of the WSI method presented in the previous section. In a monolingual data-driven WSD task, the clusters obtained for a word by clustering its instances in a monolingual corpus would constitute its candidate senses from which the most adequate one would have to be selected for new instances of the word in context. This selection would be performed by comparing the cluster vectors to information in the new context.

In the current setting, the goal of the WSD method is to assign French clusters to empty synsets in WOLF using English feature vectors. So, the information exploited for WSD consists in the words found in the corresponding English synsets (in PWN) and their related synsets, their definitions and usage examples. Given that information in the vectors built from the training corpus is lemmatized, the information retained from PWN is lemmatized as well (Schmid 1994 ) and gathered in a bag of words. The adequacy of a cluster ( C ) for filling a given synset ( S )is estimated by comparing the vectors of the clustered translations to the information retained from PWN for the synset. If common features (CFs) are found with just one cluster, this cluster is selected. Otherwise, each  X  X luster-synset X  association is assigned a score corresponding to the mean of the weights of the CFs with the clustered translations [weights assigned to each feature during WSI (cf. Sect. 3.1 )]. In formula 6 , CF j j CF j j  X  1 is the set of CFs between the cluster and the synset, and N CF is the number of translations T i in the cluster characterized by a feature CF (i.e. translations having the CF in their vector). The cluster that receives the highest score is selected and assigned to the empty synset.
 For instance, the empty synset  X  X dd#a#2 X  (definition:  X  X  X ot easily explained X  X ; usage:  X  X  X t is odd that his name is never mentioned X  X ), is correctly filled by the French cluster { curieux , bizarre }. The other clusters available for odd , which do not fit this synset and get lower scores, are: { contradictoire , singulier , bizarre } and { curieux ,  X trange }. More examples of synsets filled by the WSD method are shown in Table 3 . We provide the PWN id of the empty synsets in WOLF, the English headword, the literals in the corresponding PWN synsets, as well as their definitions and usage examples. 5 , 6 The French literals in the sense cluster most strongly associated with a PWN synset, which are used to fill the corresponding synset in WOLF, are given in the last column of Table 3 .
 The process of selecting the French synset that best suits a cluster on the basis of English contextual information is illustrated with the example given in Table 4 .It details the case of the English adjective peaceful which belongs to three synsets in PWN, all empty in WOLF. The WSD method has to fill one of these synsets with the French cluster { paisible , pacifique }. Each of the PWN synsets for peaceful is shown in Table 4 (including literals, definition and usage examples) together with some of the related synsets that are used to build the corresponding bags of words. The features that characterize, at the same time, the cluster vector and one of the synsets are shown in boldface. The bag of words representing the synset ENG20-01686906-a is the closest to that of the vector of the French cluster, and the synset also gets the highest score during WSD. Therefore, paisible and pacifique are added to synset ENG20-01686906-a in the WOLF. 4 Evaluation results Our approch fills 3,904 previously empty synsets in WOLF: 2,333 nominal, 576 verbal, 709 adjectival and 286 adverbial synsets. Given that no gold standard is available for this task X  X hich would permit to perform an automatic evaluation X  we have manually examined 10 % of the synsets filled for each POS to evaluate the quality of the proposed clusters and the correctness of their assignment to some synset in WOLF according to the following criteria:  X  a cluster is considered as a good quality one if it groups words that share the  X  the assignment of a cluster to a synset is considered as correct if its contents A cluster can be correctly assigned to a synset only if it is of good quality according to the first evaluation criterion. Consequently, all WSD assignments involving noisy clusters are considered as wrong assignments. We consider as noisy the clusters that contain one or more translations that are not semantically close to the others, even if the rest of the translations in the cluster are synonymous.

Both aspects have been evaluated by two annotators. The inter-annotator agreement was measured at j = 0.67 for cluster quality and 0.59 for the WSD results, which is conventionally interpreted as  X  X  X ood X  X  agreement (Cohen 1960 ). The lower agreement obtained for disambiguation is unsurprising, given the closeness of the considered synsets which correspond to fine-grained sense distinctions in PWN. As has been shown by Erk and McCarthy ( 2009 ), multiple WordNet senses might apply to an occurrence of a polysemous word in context, an argument in favor of graded sense assignments (Jurgens and Klapaftis 2013 ). Here, we are looking for the best-fitting WordNet sense for a cluster of translations (not for words in context) but the same effect of varying sense applicability occurs in this setting.

The evaluation results are presented in Table 5 . According to the results obtained for all POS, the clusters group semantically similar words in 75.5 % of the cases. Significant variations are however observed for different POS. The first row of the table contains the percentage of good quality clusters in the test set. The second row of the table shows the percentage of the clusters that were correctly assigned to WOLF synsets by the WSD method. Given that according to our evaluation criteria only good clusters can be correctly integrated into WOLF, we calculate the (conditional) accuracy of the WSD method by reference to the number of good clusters. This accuracy for WSD insertions on all POS is 67 %, which is very encouraging. 7 Table 5 shows that for WSD, as is the case for WSI, the results vary from one POS to another. We also provide overall accuracy results for the whole task, i.e. the proportion of automatically obtained clusters that are both good clusters (grouping words that share the same meaning) and assigned to the correct synset. This overall accuracy is computed directly as the WSI accuracy times the WSD conditional accuracy (recall that the latter was computed only on good clusters). In a setting where a manually compiled bilingual dictionary would be available and the candidate senses would not contain any noise, the focus would be put only on the accuracy of the WSD method.

The divergences observed between different parts of speech are due to the restrictive cluster quality criterion according to which one incorrect word in an otherwise correct cluster turns the whole cluster into an incorrect one. This strict criterion unfairly penalizes and rejects interesting although noisy clusters. We notice that this constraint has a strong impact during evaluation especially on clusters with many translations, like the verb clusters. We plan to proceed to a more detailed and flexible evaluation in order to more accurately estimate the actual merit of the clustering method, which will also imply devising methods for cleaning noisy clusters. In a semi-automatic setting, the manual cleaning of the noisy clusters by a lexicographer would considerably improve their integration in the French resource.
We should highlight the difficulty of the disambiguation task as the WSD method is asked to fill synsets that were left empty by the methods initially employed for creating WOLF. These empty synsets often correspond to rare senses in PWN that may not exist in the training corpus, or to senses for which little information is available in PWN. The role of the training corpus is particularly important in data-driven WSI and disambiguation. Given that the training corpus used in our experiments contains parliamentary proceedings, the derived senses are not always adequate for filling a general language resource like WOLF, as senses represented by empty synsets might not be present in the corpus.

In order to more fairly estimate the performance of the WSD method in this setting, we also tested it on the whole resource. In this case, the method was asked to select the most appropriate synset for each cluster from all synsets in WOLF (not only the empty ones). In this setting, the WSD method reaches a much higher performance of 80.13 %, as shown in Table 6 , which shows that it is particularly well adapted to the wordnet development task. Table 6 contains detailed results for words of different parts of speech. 5 Discussion and perspectives 5.1 Analysis of the errors in the clustering output From a close examination of the clustering results, two main sources of errors were identified. In some cases, the noise found in the clusters is due to alignment errors that were not detected and eliminated by the filters that served to clean the lexicons (cf. Sect. 3.1 ). In other cases, the noise is introduced during clustering. The error analysis indicates some cases of problematic clustering that fall into the second category: (a) cases where multiword units were not considered during word alignment. This (b) clustering of topically related but not synonymous words, as in the cluster (c) clustering of antonymous but distributionally similar words, as in the case of 5.2 Generating coarser wordnets using cross-lingual WSD A common criticism of PWN is the high granularity of the proposed semantic descriptions which, combined to the great number and similarity of the senses, might hamper the efficient use of this resource for WSD (Edmonds and Kilgarriff 2002 ; Ng et al. 2003 ). Fine sense distinctions increase the processing complexity and the risk of information loss when a forced choice among closely related senses has to be made without considering their relations (Dolan 1994 ). As pointed out by Ide and Wilks ( 2006 ), this fine granularity is not even necessary for efficient WSD in NLP applications where disambiguation, when needed, mostly involves homonym-level distinctions. In the rare cases where finer-grained distinctions are needed, they should be handled by more robust types of processing. As discussed earlier in this paper, an additional problem posed by the high granularity of PWN during the building of new wordnets is that it is difficult to find correspondences for fine-grained senses in the new languages. This results in resources much sparser than PWN, containing numerous empty synsets. Furthermore, the difficulty to establish correspondences between fine-grained senses and their translations makes difficult the exploitation of the resources in multilingual applications (Specia et al. 2006 ).

The granularity of wordnet-like resources can be reduced by identifying the similarity of the proposed senses. 8 Several attempts have been made for reducing the polysemy of words in PWN and the granularity of their senses. Peters et al. ( 1998 ) perform automatic sense clustering of nouns and verbs using the relations defined in WordNet (sisters, autohyponyms, twins and cousins). Mihalcea and Moldovan ( 2002 ) apply three principles from the lexical semantics litterature (Cruse 1986 ) to measuring the ambiguity level between PWN synsets of all parts of speech. The proposed rules take into account the overlaps between the elements in different synsets and their relations to other synsets in the hierarchy (hypernyms, antonyms and pertainyms). Synsets whose similarity is high enough according to these rules are collapsed together into one. Furthermore, the polysemy of WordNet is reduced based on the frequency of senses and the probability of a synset occurring in texts (as measured on SemCor, a corpus sense-tagged with PWN synsets). Synsets with very low probability of occurrence are dropped and the number of senses of polysemous words is reduced. So, the semantic principles result in collapsed synsets, while the probabilistic principles determine which synsets can be discarded. The application of these two types of rules results in a reduction in the number of synsets and, consequently, in the number of word senses.

The output of the cross-lingual WSD method presented in this paper could also serve to reduce the granularity of the new wordnet resource and, consequently, the degree of polysemy in PWN. This can be done by collapsing highly similar synsets together in the new wordnets and identifying the relations between the corresponding PWN synsets. As explained in the previous sections, the enrichment of WOLF synsets with new literals was performed by finding the most adequate synset for each translation cluster . The contents of the cluster were then used to fill the selected synset. It would however also be possible to proceed the other way around, i.e. to seek the most adequate cluster for each synset . In this way, the same cluster could be associated to different PWN synsets and this would serve as a clue for measuring the similarity of the synsets and merging them. The examples presented in Table 7 illustrate cases of one-to-many associations established between clusters and PWN/WOLF synsets, where the clusters given in the last column are associated to several empty synsets of the source word. For instance, the third cluster of noun waste :{ perte, gaspillage } (other clusters for the word: { ordure }, { d X chet, gaspillage }) is associated by the WSD method to three empty synsets of waste in WOLF, whose glosses are given in the third column of Table 7 . Each cluster-synset association is weighted during disambiguation (cf. Sect. 3.2 ) and the scores in column 4 show the strength of the association; the higher the score, the stronger the association between the cluster and the synset. In the same way, the cluster { perturbation, trouble } is assigned to four empty synsets of the word disturbance , while both empty synsets of birthday are assigned the cluster { anniversaire }.

An alternative way to establish one-to-many correspondences would be to retain for each cluster not just the synset that gets the best association score during WSD but more than one high scored synsets. In this case, a careful study of the proposed associations could help to define a threshold that would reflect good assignments. The establishment of one-to-many correspondences can serve to fill multiple WOLF synsets at once or to merge them into one, before assigning the contents of the cluster. Furthermore, these  X  X luster-synset X  associations can serve to merge the corresponding PWN synsets, to reduce the granularity of the resource and facilitate the establishment of correspondences with words in the target language. This information seems thus to be particularly relevant for creating coarser-grained resources. Nevertheless, although successful in several cases, the output of this process should be treated with caution as noisy associations might lead to erroneous merges. In future work, we intend to explore ways for identifying strong correspondences and ruling out erroneous ones but until then, the method would be better suited to a semi-supervised setting where the proposed associations could be manually validated by a lexicographer. 6 Conclusion We have proposed a novel approach to developing and enriching wordnet resources in languages other than English. We have shown how a cross-lingual word sense disambiguation method can be used to assign content to otherwise empty synsets in newly built wordnet resources. Our WSD method exploits the results of a data-driven sense induction method which discovers the senses of English words by grouping their French translations into sense clusters. The obtained clusters are integrated by the WSD method into the French wordnet resource WOLF based on information found in PWN, to which the WOLF is aligned. The results indicate that the proposed methods are particularly useful for building wordnets in new languages. Moreover, given that wordnet resources in languages other than English are often aligned to PWN, the cross-lingual WSD method can be used to enrich these resources and increase their coverage.

Our work shows that WSI and disambiguation methods can efficiently collaborate for enriching lexico-semantic resources by mapping senses automati-cally extracted from parallel data to a manually developed sense inventory like PWN. It is therefore a valuable and complementary alternative to more common approaches that leverage multilingual lexicons extracted from dictionaries. Still, if such lexicons are available they could replace the sense induction output and be directly exploited by the disambiguation method for enriching wordnet resources or creating new ones.

Based on these encouraging results, we aim at extending the use of the proposed cross-lingual WSD method for enriching non-empty WOLF synsets with additional literals. This is because some synsets are incomplete and lack some literals which could be retrieved from corpora by the methods introduced in this paper. As explained above, the quality of the automatically acquired synsets and the performance of the disambiguation method strongly depend on the parallel corpora used for training. The present study was carried out using information extracted from the Europarl corpus. We would like to include more diverse training corpora in order to obtain richer semantic representations and further extend WOLF X  X  coverage. Last but not least, we intend to explore methods for discarding synsets corresponding to rare senses in PWN or to senses that have no counterpart in French, and for merging semantically close synsets in order to reduce the granularity and the sparseness of the French resource. The output of the disambiguation method would be particularly useful to this aim given that the method can establish one-to-many correspondences between clusters and synsets, highlighting in this way their semantic similarity that can serve to their grouping.
 References
