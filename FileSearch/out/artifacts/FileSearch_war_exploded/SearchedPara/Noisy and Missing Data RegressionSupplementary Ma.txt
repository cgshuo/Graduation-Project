 4 provides additional numerical simulations. and ( 1 n  X  2 e , 1 n  X  2 e ) , respectively. We assume they are independent of each other. estimator:  X  min ( A ) and  X  max ( A ) denote the smallest and largest eigenvalue of the matrix A ): Then with high probability, the estimator above satisfies: given in (1), our proposed estimator for  X   X  naturally becomes:  X   X   X   X   X   X  k  X  k 2 2 . The r.h.s. is upper-bounded by 2  X   X   X   X   X   X   X  (  X   X  ,  X   X  ) depends on the assumption of what is known.
 Additive Noise For additive noise, the models we use are as follows. Corollary 4 (Knowledge of  X  w ) . Suppose n &amp; (1+  X  2 w ) 2  X   X   X  = Z &gt; y , satisfies Remark. The bound is linear for  X  w large, but it does not vanish when  X  w and  X  e are zero. the estimator built using  X   X  = Z &gt; UU &gt; Z , and  X   X  = Z &gt; UU &gt; y , satisfies strength of the Instrumental Variable).
 Missing Data Remark. Note that as with the previous results, the dependence on k  X   X  k 2 is given explicitly. results will also be used in the proof of the high-dimensional results in the next section. We state some supporting concentration results, and postpone their proofs to the appendix. log k , then Then for any fixed vectors v 1 , v 2 , we have In particular, if n &amp; log p  X  log m  X  log k , we have w.h.p. then for any  X  1 , we have Substituting Z = X + W into the definition of  X   X  and  X   X  , we obtain Using Lemma 9, we have w.h.p.
 It follows that The corollary then follows by applying Theorem 3. In this case, we have By Lemma 9, we have w.h.p.
 So  X   X   X   X   X   X   X   X  min ( X  x ) . The corollary then follows by applying Theorem 3. First observe that  X  We bound each term. The result follows from applying Theorem 3.  X  v , we have magnitude of the i -th term of (  X   X   X   X  x )  X   X  is Note that we use M i  X  to denote the i th row of the matrix M .
 Combining pieces, we have The corollary follows by applying Theorem 3. all the results. The following four theorems correspond to Theorem 3, 5, 7, 4 in the main paper, respectively. support of  X   X  with high probability, provided for all i  X  supp (  X   X  ) .
 satisfies: support of  X   X  provided for all i  X  supp (  X   X  ) . Moreover, the output of estimator (2) with knowledge of  X  satisfies then M 0  X  1 . lemma.
 Lemma 16. Under the assumptions of Theorem 12, w.h.p.  X  I 1  X  I  X  , I c 1 = I  X   X  I 1 , Proof. By Lemma 11 and a union bound, we have w.h.p.  X  I 1  X  I  X  ,  X  min X &gt; I c we have Again by Lemma 11,  X  min Z &gt; I with probability at least 1  X  exp cn 1 (1+  X  2 . It follows that q third term is bounded w.h.p. by 4 which is greater than 1 which is smaller than 1 . Using a union bound shows this holds for all i  X  I  X  c .
 Theorem 13 follows immediately from Theorem 12, and Corollary 4 and 5. i  X  ( I  X  ) c .
 Consider the first term. We have  X  min ( Z &gt; I  X  So the first term is at least 1  X   X  By Corollary 10 and a union bound, we obtain which is smaller than 1  X   X  8 (1  X   X  )  X   X  I By Corollary 10 and a union bound, it follows that Z &gt; I So by independence of Z i and X I  X  and Corollary 10, we obtain which is small than 1  X   X  of bounding the minimax error to a hypothesis testing problem over P . In particular, we have we have bounded by Fano X  X  inequality: I ( y ; B | Z ) = I ( y ; B | Z ) .
  X  can be computed explicitly. This is done in [5], which gives provided It remains to that show Eq.(5) holds by choosing the appropriate P , M and  X  . parameter, and noise intensity/erasure probability.
 we focus on verifying the scaling with the other parameters such as k, X  w , X  and k  X   X  k . random  X  1 vector; note that k  X   X  k = predicts that, with fixed n , the error scales proportional to (  X  w +  X  2 w ) k  X   X  k in Figure 1 (b).
 E being i.i.d. standard Gaussian variables; in this case we have  X  1 ( X  UX )  X   X  k ( X  UX ) =  X ( again match well our simulation results shown in Figure 2 (a) and (b).  X   X  w  X  0 . The crossover occurs roughly at  X  w = 1 .
 zero when  X   X  0 . If we plot the error versus the control parameter k align. It would be interesting in the future to tighten our bound to match this scaling. projected gradient method in all cases.
 description of the algorithm, supp-OMP has exactly the same running time as standard OMP. average over 200 trials. lines and align. Each point is an average over 100 trials. 100 trials. is an average over 200 trials.  X   X  cases considered. Each point is an average over 200 trials.
 supp-OMP than the projected gradient method.
 the results under the following choice of covariance matrix of X : use of in Section 2 and 3. We repeat the statements of the results below for convenience.  X  y , 1 n  X  2 y . Then for any fixed vector v 1 , v 2 , we have In particular, if n &amp; log p  X  log m, log k , we have w.h.p. is plotted against the control parameter k considered. Each point is an average over 50 trials.
 terms gives with probability at most exp  X  cn min t 2 ,t .
 R n , then for any  X  1 , we have  X  2 x k v k 2 2 . Applying the last lemma with t = k n  X  2 x ,  X  1 to the first term, we obtain The corollary follows. then |A A Becaue 3 X ( v 1 ) ,u ( v 1 )  X  v 1 , and 3 X ( v 2 ) ,u ( v 2 )  X  v 2 , it follows that hence sup v [10] B. Yu. Assouad, Fano, and Le Cam. Festschrift for Lucien Le Cam , pages 423 X 435, 1997.
