 John Shawe-Taylor JST @ CS . RHUL . AC . UK The set covering machine (SCM) has recently been proposed by Marchand and Shawe-Taylor (2001; 2002) as an alternative to the support vector machine (SVM) when the objective is to obtain a sparse classi-fier with good generalization. Given a feature space, the SCM tries to find the smallest conjunction (or dis-junction) of features that gives a small training er-ror. In contrast, the SVM tries to find the maximum soft-margin separating hyperplane on all the features. Hence, the two learning machines are fundamentally different in what they are trying to achieve on the training data.
 The learning algorithm for SCM generalizes the two-step algorithm of Valiant (1984) and Haussler (1988) for learning conjunctions (and disjunctions) of Boolean attributes to allow features that are con-structed from the data and to allow a trade-off be-tween accuracy and complexity. For the set of fea-tures known as data-dependent balls , Marchand and Shawe-Taylor (2001; 2002) have shown that good generalization is expected when a SCM with a small number of balls and errors can be found on the train-ing data. Furthermore, on some  X  X atural X  data sets, they have found that the SCM achieves a much higher level of sparsity than the SVM with roughly the same generalization error.
 In this paper, we introduce a new set of features for the SCM that we call data-dependent half-spaces . Since our goal is to construct sparse classifiers, we want to avoid using O ( d ) examples to construct each half-space in a d -dimensional input space (like many com-putational geometric algorithms). Rather, we want to use O (1) examples for each half-space. In fact, we will see that by using only three examples per half-space, we need very few of these half-spaces to achieve a generalization as good (and sometimes bet-ter) as the SVM on many  X  X atural X  data sets. More-over, the level of sparsity achieved by the SCM is al-ways substantially superior (sometimes by a factor of at least 50) than the one achieved by the SVM. Finally, by extending the sample compression tech-nique of Littlestone and Warmuth (1986), we bound the generalization error of the SCM with data-dependent half-spaces in terms of the number of errors and the number of half-spaces it achieves on the train-ing data. We will then show that, on some  X  X atural X  data sets, our bound is as effective as 10-fold cross-validation in its ability to select a good SCM model. We provide here a short description of the Set Cov-ering Machine (SCM), more details are provided in Marchand and Shawe-Taylor (2002).
 Let x denote an arbitrary n -dimensional vector of the input space X which could be arbitrary subsets of &lt; which the training set S = P  X  N consists of a set P of positive training examples and a set N of neg-ative training examples. We define a feature as an arbitrary Boolean-valued function that maps X onto h ( x ) and any training set S , the learning algorithm returns a small subset R  X  H of features. Given that subset R , and an arbitrary input vector x , the output f ( x ) of the SCM is defined to be: To discuss both the conjunction and the disjunction cases simultaneously, let us use P to denote set P in the conjunction case but set N in the disjunction case. Similarly, N denotes set N in the conjunction case but denotes set P in the disjunction case. It then fol-lows that f makes no error with P if and only if each h i  X  R makes no error with P . Moreover, if Q i de-notes the subset of examples of N on which feature h i makes no errors, then f makes no error on N if and only if by Haussler (1988), the problem of finding the small-est set R for which f makes no training errors is just the problem of finding the smallest collection of Q i s that covers all N (where each corresponding h i makes no error on P ). This is the well-known Minimum Set Cover Problem (Garey &amp; Johnson, 1979). The inter-esting fact is that, although it is NP -complete to find the smallest cover, the set covering greedy algorithm will always find a cover of size at most z ln( |N| ) when the smallest cover that exists is of size z (Chv  X  atal, 1979; Kearns &amp; Vazirani, 1994). Moreover this al-gorithm is very simple to implement and just consists of the following steps: first choose the set Q i which covers the largest number of elements in N , remove from N and each Q j the elements that are in Q i , then repeat this process of finding the set Q k of largest car-dinality and updating N and each Q j until there are no more elements in N .
 The SCM built on the features found by the set cov-ering greedy algorithm will make no training errors only when there exists a subset E  X  H of features on which a conjunction (or a disjunction) makes zero training error. However, this constraint is not really required in practice since we do want to permit the user of a learning algorithm to control the tradeoff be-tween the accuracy achieved on the training data and the complexity (here the size) of the classifier. Indeed, a small SCM which makes a few errors on the training set might give better generalization than a larger SCM (with more features) which makes zero training errors. One way to include this flexibility into the SCM is to stop the set covering greedy algorithm when there re-mains a few more training examples to be covered. In this case, the SCM will contain fewer features and will make errors on those training examples that are not covered. But these examples all belong to N and, in general, we do need to be able to make errors on training examples of both classes. Hence, early stop-ping is generally not sufficient and, in addition, we need to consider features that also make some errors with P provided that many more examples in N can be covered. Hence, for a feature h , let us denote by Q h the set of examples in N covered by feature h and by R h the set of examples in P for which h makes an error on. Given that each example in P misclassi-fied by h should decrease by some fixed penalty p its  X  X mportance X , we define the usefulness U h of feature h by: Hence, we modify the set covering greedy algorithm in the following way. Instead of using the feature that covers the largest number of examples in N , we use the feature h  X  X  that has the highest usefulness value U h . We removed from N and each Q g (for g 6 = h ) the elements that are in Q h and we removed from each R g (for g 6 = h ) the elements that are in R h . Note that we update each such set R g because a feature g that makes an error on an example in P does not increase the error of the machine if another feature h is already making an error on that example. We repeat this pro-cess of finding the feature h of largest usefulness U h and updating N , and each Q g and R g , until only a few elements remain in N (early stopping the greedy). Here is a formal description of our learning algorithm. The penalty p and the early stopping point s are the two model-selection parameters that give the user the ability to control the proper tradeoff between the train-ing accuracy and the size of the function. Their val-ues could be determined either by using k-fold cross-validation, or by computing our bound (see section 4) on the generalization error based on what has been achieved on the training data. Note that our learning algorithm reduces to the two-step algorithm of Valiant (1984) and Haussler (1988) when both s and p are in-finite and when the set of features consists of the set of input attributes and their negations.
 Input: A machine type T (which is either  X  X onjunc-tion X  or  X  X isjunction X ), a set P of positive training examples, a set N of negative training examples, a penalty value p , a stopping point s , and a set H = { h i ( x ) } |H| i =1 of Boolean-valued features. Output: A conjunction (or disjunction) f ( x ) of a subset R X  X  of features.
 Initialization: R =  X  . 1. If ( T =  X  X onjunction X ) let P  X  P and N  X  N . 2. For each h i  X  H , let Q i be the subset of N cov-3. Let h k be a feature with the largest value of 4. Let R X  X  X  { h k } . Let N  X  X   X  Q k and let 5. For all i do: Q i  X  Q i  X  Q k and R i  X  R i  X  R k . 6. If ( N =  X  or |R| X  s ) then go to step 7 (no more 7. Return f ( x ) where: With the use of kernels, each input vector x is implic-itly mapped into a high-dimensional vector  X  ( x ) such that  X  ( x )  X   X  ( x 0 ) = k ( x , x 0 ) (the kernel trick). We consider the case where each feature is a half-space constructed from a set of 3 points {  X  a ,  X  b ,  X  c } where  X  a is the image of a positive example x a ,  X  b is the im-age of a negative example x b , and  X  c is the image of a P -example x c . The weight vector w of such an half-space h c t is identified by t def = w  X   X  c  X   X  , where  X  is a small positive real number in the case of a conjunction but a small negative number in the case of a disjunction. Hence where When the penalty parameter p is set to  X  , Build-SCM tries to cover with half-spaces the examples of N without making any error on the examples of P . In that case,  X  c is the image of the example x c  X  P that gives the smallest value of w  X   X  ( x c ) in the case of a conjunction (but the largest value of w  X   X  ( x c ) in the case of a disjunction). Note that, in contrast with data-dependent balls (Marchand &amp; Shawe-Taylor, 2002), we are not guaranteed to always be able to cover all N with such half-spaces. When training a SCM with finite p , any x c  X  P might give the best threshold for a given ( x a , x b ) pair. Hence, to find the half-space that maximizes U h , we need to compute U h for every triple ( x a , x b , x c ) .
 Note that this set of features (in the linear kernel case and Revow (1996) for decision tree learning but no analysis of their learning method has been given. First note that we cannot use the  X  X tandard X  VC theory to bound the generalization error of SCMs with data-dependent half-spaces because this set of functions is defined only after obtaining the training data. In con-trast, the VC dimension is a property of a function class defined on some input domain without reference to the data. Hence, we propose another approach. Since our learning algorithm tries to build a SCM with the smallest number of data-dependent half-spaces, we seek a bound that depends on this number and, consequently, on the number of examples that are used in the final classifier (the hypothesis). We can thus think of our learning algorithm as compressing the training set into a small subset of examples that we call the compression set . It was shown by Littlestone and Warmuth (1986) and Floyd and Warmuth (1995) that we can bound the generalization error of the hy-pothesis f if we can always reconstruct f from the compression set. Hence, the only requirement is the existence of such a reconstruction function and its only purpose is to permit the exact identification of the hypothesis from the compression set and, possi-bly, additional bits of information. Not surprisingly, the bound on the generalization error raises rapidly in terms of these additional bits of information. So we must make minimal usage of them.
 We now describe our reconstruction function and the additional information that it needs to assure, in all cases, the proper reconstruction of the hypothesis from a compression set. As we will see, our proposed scheme works in all cases provided that the learning algorithm returns a hypothesis that always correctly classifies the compression set (but not necessarily all of the training set). Hence, we need to add this con-straint in BuildSCM 1 for our bound to be valid but, in practice, we have not seen any significant perfor-mance variation introduced by this constraint. Given a compression set (returned by the learning al-gorithm), we first partition it into three disjoint sub-sets  X  a ,  X  b ,  X  c that consists of the examples of type x , x b , and x c that we have described in section 3. Now, from these sets, we must construct the weight vectors. Recall that each weight vector w is speci-fied by a pair ( x a , x b ) . Hence, for each x a we must specify the different points x b  X   X  b that are used to form a weight vector with x a . Although each point can participate in more than one weight vector, each pair ( x a , x b )  X   X  a  X   X  b can provide at most one weight vector under the constraint that the compres-sion set is always correctly classified by the hypoth-esis. Hence, the identification of weight vectors re-quires at most  X  a  X  b bits of information (where  X  a = |  X  a | and  X  b = |  X  b | ). However, it is more economi-cal to provide instead log the number r of weight vectors and then log bits to specify which group of r pairs ( x a , x b ) is cho-sen among the set of all possible groups of r pairs taken from  X  a  X   X  b . To find the threshold t for each w , we choose the example x  X   X  c  X   X  a that gives the smallest value of w  X   X  ( x ) in the case of a con-junction. In the case of a disjunction, we choose the example x  X   X  c  X   X  b that gives the largest value of w  X   X  ( x ) . This is the only choice that assures that the compression set is always correctly classified by the resulting classifier. Note that we adopt the convention that each point in the compression set is specified only once (without repetitions) and, consequently, a point of  X  a or  X  b can also be used to identify the threshold. In summary, we can always reconstruct the hypothe-sis from the compression set when we partition it into the subsets  X  a ,  X  b ,  X  c defined above and provide, in addition, log weight vectors from  X  a  X   X  b . This is all that is re-quired for the next theorem.
 Theorem 1 Let S = P  X  N be a training set of pos-itive and negative examples of size m = m p + m n . Let A be the learning algorithm BuildSCM that uses data-dependent half-spaces for its set of features with the constraint that the returned function A ( S ) always correctly classifies every example in the compression set. Suppose that A ( S ) contains r half-spaces, and makes k p training errors on P , k n training errors on N (with k = k p + k n ), and has a compression set  X  =  X  a  X   X  b  X   X  c (as defined above) of size  X  =  X  a +  X  b +  X  c . With probability 1  X   X  over all random training sets S of size m , the generalization error er( A ( S )) of A ( S ) is bounded by er( A ( S ))  X  1  X  exp where and where Proof Let X be the set of training sets of size m . Let us first bound the probability given that m ( S ) is fixed to some value m where For this, denote by E p the subset of P on which A ( S ) makes an error and similarly for E n . Let I be the mes-sage of information bits needed to specify the weight vectors (as described above) for a given  X  a and  X  b . Now define P 0 for some fixed set of disjoint subsets { S i } 5 and some fixed information message I 0 . Since B  X  is the number of different ways of choosing the differ-ent compression subsets and set of error points in a training set of fixed m , we have: where the first two factors come from the additional information that is needed to specify the weight vec-tors. Note that the hypothesis f def = A ( S ) is fixed in P quired information bits are given). To bound P 0 make the standard assumption that each example x is independently and identically generated according to some fixed but unknown distribution. Let p be the probability of obtaining a positive example, let  X  be the probability that the fixed hypothesis f makes an error on a positive example, and let  X  be the prob-ability that f makes an error on a negative exam-ple. Let t p def =  X  a +  X  c + k p for the conjunction case (and t p def =  X  a + k p for the disjunction case). Simi-larly, let t n def =  X  b + k n for the conjunction case (and t =  X  b +  X  c + k n for the disjunction case). We then have: Consequently: The theorem is obtained by bounding this last expres-sion by the proposed value for  X   X  ( m ) and solving for  X  since, in that case, we satisfy the requirement that: P where the sums are over all possible realizations of m for a fixed m p and m n . With the proposed value for  X   X  ( m ) , the last equality follows from the fact that P In order to obtain the tightest possible bound, note that we have generalized the approach of Littlestone and Warmuth by partitioning the compression set into three different subsets and by taking into account the number of positive and negative examples actually ob-served in the training set.
 Basically, our bound states that good generalization is expected when we can find a small SCM that makes few training errors. It may seem complicated but the important feature is that it depends only on what the hypothesis has achieved on the training data. Hence, we could use it as a guide for choosing the model se-lection parameters s and p of algorithm BuildSCM since we can compute its value immediately after training. We have compared the practical performance of the SCM with the Support Vector Machine (SVM) equipped with a Gaussian kernel (also called the Ra-dial Basis Function kernel) of variance 1 / X  . We have used the SVM program distributed by the Royal Hol-loway University of London (Saunders et al., 1998). The data sets used and the results obtained are re-ported in table 1. All these data sets where obtained from the machine learning repository at UCI, except the Glass data set which was obtained from Rob Holte, now at the University of Alberta. For each data set, we have removed all examples that contained attributes with unknown values (this has reduced substantially the  X  X otes X  data set) and we have removed examples with contradictory labels (this occurred only for a few examples in the Haberman data set). The remaining number of examples for each data set is reported in ta-ble 1. No other preprocessing of the data (such as scal-ing) was performed. For all these data sets, we have used the 10-fold cross validation error as an estimate of the generalization error. The values reported are expressed as the total number of errors ( i.e. the sum of errors over all testing sets). We have ensured that each training set and each testing set, used in the 10-fold cross validation process, was the same for each learning machine ( i.e. each machine was trained on the same training sets and tested on the same testing sets).
 The results reported for the SVM are only those ob-tained for the best values of the kernel parameter  X  and the soft margin parameter C found among an ex-haustive list of many values. The  X  X ize X  column refers to the average number of support vectors contained in SVM machines obtained from the 10 different training sets of 10-fold cross-validation.
 We have reported the results for the SCM with data-dependent balls (Marchand &amp; Shawe-Taylor, 2002) (with the L 2 metric) and the SCM with data-dependent half-spaces (with a linear kernel). In both cases the T column refers to type of the best machine found: c for conjunction, and d for disjunction. The p column refers the best value found for the penalty pa-rameter, and the s column refers the the best stopping point in terms of the number of features ( i.e. balls and half-spaces respectively). Again, only the values that gave the smallest 10-fold cross-validation error are re-ported. We have also reported, in the  X  X ound X  col-umn, the bound on the generalization error obtained by computing the r.h.s. of the inequality of Theo-rem 1 (with  X  = . 05 ), for each of the 10 different training sets involved in 10-fold cross validation, and multiplying that bound with the size of each testing sets. We see that, although the bound is not tight, it is nevertheless non-trivial. This is to be contrasted with the VC dimension bounds which cannot even be ap-plied for our case since the set of functions supported by the SCM depends on the training data. Further-more, if we exclude the BreastW data set, we can see in the  X  X atio X  column of table 1 that the ratio of the Name #exs  X  C size errors T p s errors T p s errors bound ratio BreastW 683 0.005 2 58 19 c 1.8 2 15 c 1.0 1 18 103 5.72 Votes 52 0.05 15 18 3 d 0.9 1 6 c 0.8 1 6 20 3.33 Pima 768 0.002 1 526 203 c 1.1 3 189 c 1.5 3 175 607 3.47 Haberman 294 0.01 0.6 146 71 c 1.4 1 71 d 0.7 1 68 209 3.07 Bupa 345 0.002 0.2 266 107 d 2.8 9 107 c 1.4 1 103 297 2.88 Glass 163 0.8 2 92 34 c 0.85 4 33 c 1.05 3 39 113 2.90
Credit 653 0.0006 32 423 190 d 1.2 4 194 d 1.2 3 148 513 3.47 bound to the generalization error is remarkably stable even across different learning tasks, suggesting that the bound may indeed work well as a model selection criterion.
 The most striking feature in table 1 is the level of sparsity achieved by the SCM in comparison with the SVM. This difference is huge. In particular, the SCMs with half-spaces never contained more than 3 half-spaces ( i.e. a compression set of at most 9 points). Compared with the SVM, the SCM with half-spaces is more than 50 times sparser than the SVM on the Pima, Bupa, and Credit data sets! The other important fea-ture is that SCMs with half-spaces often provide better generalization than SCMs with balls and SVMs. The difference is substantial on the Credit data set. Hence it is quite clear that data-dependent half-spaces pro-vides an alternative to data-dependent balls for the set of features used by the SCM. Although it is within tion time since triples of points needs to be examined to find a half-space but only pairs of points need to be considered for balls.
 We now investigate the extent to which our bound can perform model-selection. More specifically, we want to answer the following question. Given a set of SCMs obtained from BuildSCM for various values of the model-selection parameters p and s , is our bound on the generalization error, evaluated on the training set, effective at selecting the SCM that will give the best generalization? Note that the results reported in table 1 are, in fact, the 10-fold cross validation estimate of the general-BreastW c 1.2 23 4.7 1.8 25 4.4 Votes c 1.1 9 3.1 1.0 6 2.8 Pima c 4.3 191 9.2 3.8 181 11 Haberman d 1.7 73 5.0 3.8 74 3.8 Bupa c 2.3 118 6.0 2.5 115 8.2 Glass c 2.6 50 7.9 2.5 49 7.3
Credit d 3 157 17 3.5 162 17 ization error that is achieved by the model selection strategy that correctly guesses the best values for and s . This model-selection strategy is, in that sense, optimal (but not realizable). Hence, we will refer to the score obtained in table 1 as those obtained by the optimal model-selection strategy.
 The results for the model-selection strategy based on our bound are reported in table 2. Here we have used our bound to select the best SCM among those ob-tained for various penalty values among a list of fif-teen penalty values (that always contained the optimal value) and for all possible sizes s . Also shown in these tables, are the results obtained for the 10-fold cross validation model selection method. This latter method is perhaps the most widely used X  X ere, it consists of using 10-fold cross validation to find the best stop-ping point s and the best penalty value p on a given training set and then use these best values on the full training set to find the best SCM. Both model selec-tion methods were tested by 10-fold cross validation. Finally, in addition to the error and size (as in the pre-vious tables), we have also reported a rough estimate of the standard deviation of the error. This estimate was obtained in the following way. We first compute the standard deviation of the generalization error (per example) over the 10 different testing sets and then divide by iid random variables, each with variance  X  2 , is  X  2 /n ). Finally we multiply this estimate by the number of ex-amples in the data set. From the results of table 2, we see that model selection by using our bound is gener-ally as effective as using 10-fold cross validation (but takes substantially less computation time). We have introduced a new set of features for the SCM, called data-dependent half-spaces, and have shown that it can provide a favorable alternative to data-dependent balls on some  X  X atural X  data sets. Com-pared with SVMs, our learning algorithm for SCMs with half-spaces produces substantially sparser clas-sifiers (often by a factor of 50) with comparable, and sometimes better, generalization.
 By extending the sample compression technique of Littlestone and Warmuth (1986), we have bound the generalization error of the SCM with data-dependent half-spaces in terms of the number of errors and the number of half-spaces it achieves on the training data. Our bound indicates that good generalization error is expected whenever a SCM, with a small number of half-spaces, makes few training errors. Furthermore, on some  X  X atural X  data sets, we have seen that our bound is generally as effective as 10-fold cross valida-tion at selecting a good SCM model. Note, however, that our bound applies only to the case of symmetri-cal loss. Hence, the next important step is to general-ize our bound to the case of asymmetrical loss (which frequently occurs in practice) and investigate its effec-tiveness at performing model selection.
 Work supported by NSERC grant OGP0122405 and, in part, under the KerMIT project, No. IST-2001-25431.
 Chv  X  atal, V. (1979). A greedy heuristic for the set covering problem. Mathematics of Operations Re-search , 4 , 233 X 235.
 Floyd, S., &amp; Warmuth, M. (1995). Sample compres-sion, learnability, and the Vapnik-Chervonenkis di-mension. Machine Learning , 21 , 269 X 304.
 Garey, M. R., &amp; Johnson, D. S. (1979). Comput-ers and intractability, a guide to the theory of np-completeness . New York, NY: Freeman.
 Haussler, D. (1988). Quantifying inductive bias: AI learning algorithms and Valiant X  X  learning frame-work. Artificial Intelligence , 36 , 177 X 221. Hinton, G., &amp; Revow, M. (1996). Using pairs of data-points to define splits for decision trees. Advances in Neural Information Processing Systems 8 , 507 X  513.
 Kearns, M. J., &amp; Vazirani, U. V. (1994). An introduc-tion to computational learning theory . Cambridge, Massachusetts: MIT Press.
 Littlestone, N., &amp; Warmuth, M. (1986). Relating data compression and learnability (Technical Re-port). University of California Santa Cruz, Santa Cruz, CA.
 Marchand, M., &amp; Shawe-Taylor, J. (2001). Learn-ing with the set covering machine. Proceedings of the Eighteenth International Conference on Ma-chine Learning (ICML 2001) , 345 X 352.
 Marchand, M., &amp; Shawe-Taylor, J. (2002). The set covering machine. Journal of Machine Learning Reasearch , 3 , 723 X 746.
 Saunders, C., Stitson, O., Weston, J., Bottou, L.,
Schoelkopf, B., &amp; Smola, A. (1998). Support vec-tor machine reference manual (Technical Report CSD-TR-98-03). Department of Computer Science, Royal Holloway, University of London, London, UK.
 Valiant, L. G. (1984). A theory of the learnable. Com-munications of the Association of Computing Ma-
