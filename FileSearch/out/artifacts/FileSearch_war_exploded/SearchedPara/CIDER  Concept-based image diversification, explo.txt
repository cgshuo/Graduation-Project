 1. Introduction
Over the course of the last decade, image search has become a popular sub-class of Web search tasks. However, despite its increasing popularity, the primary techniques used for Web image retrieval have not changed significantly, remaining firmly entrenched within the foundations of text retrieval. Textual queries are matched to the textual information associated with images on the Web, images are ranked based on relevance scores, and paged grids of images are presented for the searcher to browse. Since this process is not all that different from Web document retrieval, it suffers from the same problem of search-ers providing short and ambiguous queries ( Andr X , Cutrell, Tan, &amp; Smith, 2009 ). The outcome of this ambiguity is the mixing of images from seemingly irrelevant interpretations of the query with those from the searcher X  X  intended meaning. Further-more, if the search engine promotes an interpretation of the query that does not match the searcher X  X  intent, there may be very few relevant images within the search results set. These problems, coupled with the inability to easily manipulate or explore within the image set, results in the search tasks becoming tedious, frustrating, and unpredictable.

In this paper, we outline a new approach to Web image search that extracts the various conceptual aspects of a query, automatically performs query expansion to retrieve a diverse range of images, organizes these images based on conceptual and visual information, presents a hierarchy of the extracted concepts, and allows the searcher to actively explore within the image search results to focus on concepts and images that match their intention for the query. We call this approach CIDER  X  (concept-based image diversification, exploration, and retrieval). Such an approach moves beyond the traditional informa-tion retrieval focus of matching queries to images, instead providing a Web Information Retrieval Support Systems (WIRSS) that aims to enhance the human element of Web search ( Hoeber, 2008; Yao &amp; Yao, 2003 ). The goal is to allow searchers to take an active role within the search process, which is particularly valuable for search tasks that are ambiguous or explor-atory in nature ( Hoeber, 2008 ). By diversifying the image set and then organizing both the images and their associated con-cepts, the searcher can identify regions of the image space and specific concepts that match what they are seeking. In contrast to scrolling within a grid-based organization of images, this approach has the ability to improve searchers X  perfor-mance by taking advantage of human decision-making processes during the evaluation and exploration phases of the search task.

In previous work, we have explained how our approach automatically discovers related concepts found in Wikipedia per-taining to an image search query, and retrieves images through a query expansion process ( Hoque, Strong, Hoeber, &amp; Gong, 2011b; Hoque, Hoeber, Strong, &amp; Gong, 2011c ). This results in a diversification of the image search results, covering a broad range of concepts when the query is ambiguous. The images within the search results set are organized based on their visual and conceptual similarity using a hierarchical multi-resolution Self-Organizing Map (SOM) ( Strong, Hoque, Gong, &amp; Hoeber, 2010; Strong &amp; Gong, 2011 ). In addition, the names of the concepts themselves are organized in a hierarchical manner. A set of novel interactive visualization features have been designed to take advantage of the conceptual information obtained through the query expansion process. In particular, concept-based focusing and filtering allow semantically similar images to be highlighted, and zooming and panning supports exploration within the image search space ( Hoque, Hoeber, et al., 2011c ).
We have also studied the trade-offs between the degree to which diversification is being promoted and the precision of the image set with respect to each of the interpretations of the query. These experiments were conducted with a set of que-ries that ranged from ambiguous to specific. The evaluations showed that a highly ambiguous query can benefit from a high degree of diversification, whereas more specific queries can be harmed by too much diversification. As a result, we derived a simple formula that increases the degree of diversification as the degree of ambiguity of the query increases ( Hoque, Hoeber, &amp; Gong, 2011a; Hoque, Hoeber, &amp; Gong, 2012 ).

In this paper, we are interested in observing the performance of CIDER through a preliminary user study. Our goal is to analyze whether the system can assist searchers in finding images more efficiently, and improve their perceptions regarding the difficulty of the search tasks, satisfaction, knowledge gained, ease of use, usefulness, and preferences for an image retrie-val interface. The automatic diversification of the image search results is coupled with a high degree of interactive control that allows the searchers to take advantage of their intelligence and decision-making abilities. This is in contrast to the pas-sive process of letting the search engine choose the interpretation of the query it thinks the searcher wants, which is com-mon in public image search engines.

The remainder of this paper is organized as follows: an overview of the previous work on image search results diversification and interactive support for search results exploration is provided in Section 2 .InSection 3 , an outline of the design and core features of CIDER is given. Examples of using CIDER to interactively explore the image search results are provided in Section 4 .
The evaluation methods and results from the preliminary evaluation are provided in Section 5 . The paper concludes with a dis-cussion on the lessons learned from this research in Section 6 , followed by conclusions and future work in Section 7 . 2. Related work 2.1. Query expansion and search results diversification
In the literature of document search techniques, most of the previous approaches to search results diversification can be proaches perform direct comparison between the retrieved documents, under the assumption that similar documents will cover similar aspects. An alternative approach to diversification is to explicitly utilize different aspects associated with a query by directly modelling these aspects.

Query expansion is an explicit way of diversifying the image search results through adding a number of meaningful terms to an initial query. Although query expansion has been studied in only a few instances within the context of Web image retrieval, those that have explored such techniques have shown them to be promising ( Hoque, Strong, et al., 2011b ; Myoupo, Popescu, refinement, it can be beneficial for image search. This is especially true for searchers conducting discovery tasks where their information needs are not clear, their queries are ambiguous, and many different images may be viewed as relevant.
However, one of the challenges associated with query expansion is to find an appropriate source of knowledge required for the expansion process. It has been noted that many image search queries are associated with conceptual domains that ing a suitable knowledge base that has sufficient coverage of a realistic conceptual domain is very important. Wikipedia is a good candidate for such a knowledge base since it includes a large number of articles describing people, places, landmarks, animals, and plants.

The second challenge is to design efficient and effective algorithms that can process such semi-structured knowledge to derive and rank the meaningful terms to be used in the query expansion process. A useful approach to this problem is to measure the semantic relatedness between the original query and each of the concepts derived from that query. A number of different methods have been devised to use Wikipedia for this purpose, including WikiRelate! ( Strube &amp; Ponzetto, 2006 ),
Explicit Semantic Analysis (ESA) ( Gabrilovich &amp; Markovitch, 2007 ), and Wikipedia Link-based Measure (WLM) ( Milne &amp; Wit-ten, 2008 ). Due to the computationally efficiency and accuracy of WLM, we use this approach in our work.

The third issue to resolve is to decide whether to make the query expansion process interactive or automatic. While inter-active query expansion has been extensively studied for the document-centric information retrieval ( Fonseca, Golgher, P X s-well studied in the domain of image retrieval. The difficulty in applying interactive query expansion in image retrieval is that it forces the searcher to continue to think about how to describe their image needs in text, when their primary intention is look at the images. We have instead chosen to automatically expand the query using the most similar concepts to the ori-ginal query extracted from Wikipedia. In doing so, the query expansion and image search result diversification processes is seamless to the searcher, allowing them to delve into the exploration of the images without delay. 2.2. Interactive image search results exploration
Among the popular modern search engines, the presentation of image search results has changed very little over the last decade: a paged grid layout is used to organize images based on their relevance rank. While such grid interfaces are easy to use, they provide limited ability to manipulate and explore the search results. Further, simply ordering by rank does not pro-mote diversity within the search results set. Rather, similar images from the most common interpretation of the query are is less common. Considering the limitations of traditional approaches, a number of organization and visualization techniques have been proposed including clustering and tree-based visualizations, Focus + Context, three dimensional visualizations, and similarity-based image browsing.

Clustering techniques have shown some promise in promoting diversity in the search results because of their discrimi-vide more meaningful representations of image search results ( Rodden, Basalaj, Sinclair, &amp; Wood, 2001 ). In Google Image
Swirl, a large set of hierarchically clustered images are visually organized using a radial layout. Each layer of the tree is ar-clustering algorithms, however, share some common challenges including determining a suitable similarity measure, the efficiency of the clustering algorithms, determining an appropriate number of clusters to use, and deciding how to order or organize the clusters.

Over the last few years, faceted search emerged as a promising approach to image searching. Many of the research works in this direction are exploring how useful facets can be generated and how they can be presented for exploring image search results. ImageSieve extracts named entities from the descriptions of the retrieved images and use them to organize a faceted browsing interface, which then helps users to make sense of and further explore the retrieved images ( Lin, Ahn, Brusilovsky,
He, &amp; Real, 2010 ). The researchers from Yahoo! describe a system that enables faceted exploration of Web image search re-semi-structured information sources to extract objects and facets. Then the facets are ranked based on a statistical analysis of image search query logs and the tagging behavior of users annotating photos in Flickr. However, the system allows the searcher to browse only one level of facets, which may not be sufficient enough for the queries when the searcher starts with queries that are too ambiguous.

Focus + Context techniques provide both overview and detail in o ne unified interface by spatia lly distorting a data represen-tation, giving more room to designated points of interest and decreasing the space given to regions away from those points &amp; Ma, 2006 )). Despite the fact that they may be useful in providing th eoverviewofalargeimageco llection, these techniques generally suffer from problems associated with the irregular size of the images and some interaction difficulties.
A variety of three dimensional visualization approaches have also proposed, such as ImageFlow ( Jampani, Ramos, &amp; Druc-ker, 2010 ). This approach presents image results on a canvas where semantic features (e.g., relevance, related queries) are mapped to the canvas X  spatial dimensions to allow seamless navigation through the semantic space. While such 3D visual-izations can be aesthetically pleasing, occlusion and irregular size of the images often cause important concerns.
Similarity-Based Image Browsing (SBIB) is an approach that takes advantage of the fundamental aspects of content-based image retrieval, but eliminates the need for the searcher to identify a set of relevant images a priori . Images are organized based solely on their features, allowing searchers to explore the collection even if they do not have a clearly defined infor-mation need ( Heesch, 2008 ). The challenge of SBIB is to arrange images based on visual similarities in such a way as to sup-port the browsing and exploration experience. While a number of different approaches have been proposed in the literature use a different method that employs a hierarchical multi-resolution extension to SOMs. This approach provides both an orga-nizing structure for the images and a measure of importance that can be used to determine which images to display when to be very useful and easy to use ( Strong, Hoeber, &amp; Gong, 2010 ). 2.3. Evaluation methodology
For evaluating Web search interfaces, conducting user studies is not only regarded as an accepted practice but it is one of the more valuable ways in which researchers can evaluate the potential benefits that their work might have for real users. It provides researchers with a means to verify and validate design assumptions, confirm or reject hypotheses, and make com-parisons between different systems and techniques ( Hoeber, 2009 ). Evaluating Web search interfaces is challenging, mainly due to the knowledge-centric nature of the domain. The issue of evaluation is further complicated when the Web search interface includes highly visual and interactive elements. Methods for evaluating information visualization systems have re-cently become a rather active research topic ( Carpendale, 2008; Hoeber, 2009 ).

Laboratory studies are the most common methods used for evaluating image search interfaces. However, designing a user study for image search interfaces is inherently difficult because of the exploratory nature of the search activities, where the users X  image needs are ill-defined. The main challenge here is to clearly and precisely define the image search task and the conditions under which the task is considered completed. While a common procedure for conducting user studies is to as-sign tasks to the participants and measure the time it takes to complete the tasks and the number of errors made, such mea-sures alone may not provide a clear picture of how much knowledge is gained through the search activities. Qualitative methodologies are often used in conjunction with quantitative measures by conducting interviews or administering questionnaires.

Rodden et al. (2001) evaluated the effectiveness of browsing image collections organized by similarity. They performed two experiments in a laboratory setting. The first experiment compared caption-based vs. visual feature-based image orga-nization, and the second one compared visual feature-based organization vs. random arrangement. In addition to recording time to task completion data, the study also collected qualitative measures including usefulness and satisfaction.
Strong et al. (2010) evaluated two different layout mechanisms for a similarity-based image organization system in com-parison to a traditional grid based layout. The study was conducted in a laboratory setting using a 3 3 (interface x search task) between-subjects design. Each participant used each interface only once, and conducted each search task only once.
Time to task completion and accuracy were measured, along with subjective reactions regarding to the usefulness and ease of use of the interface.

Zhang et al. (2006) compared two novel image organization methods in comparison to Google X  X  grid-based view, using two image search tasks. The primary data collected was qualitative in nature, focusing on general impressions of the search opinions on specific features of the interface.

The common themes among these studies are their controlled nature (wherein the participants are assigned specific im-age retrieval tasks), and the inclusion of qualitative measures of the opinions of the searchers in addition to the quantitative measures of time to task completion and error rates. Although measures such as search effort ( Wang, Jing, He, Du, &amp; Zhang, 2007 ) and search efficiency ( Porta, 2006 ) may appear to also be useful, they may not be particularly useful for measuring the success and usefulness of exploratory search interfaces such as CIDER. 3. CIDER 3.1. Diversification via concept-based query expansion
Within the CIDER system, image search results are explicitly diversified using a concept-based query expansion technique based on information derived from Wikipedia. For the short and ambiguous queries that are common in image search, query expansion attempts to capture the various aspects of the query. The benefits of query expansion are twofold: (1) it allows retrieved images to cover a wide spectrum within the concept space and (2) it tags each retrieved image with a concept that facilitates concept-based organization. The detailed procedure of this approach is described in ( Hoque, Hoeber, et al., 2011c ). Here it is briefly explained for the completeness of the paper.

The process of this query expansion method is as follows. At first, a user-supplied query Q is matched to a Wikipedia-de-rived knowledge base by selecting the best matching article (referred as the home article) using Wikipedia X  X  search feature.
In the case where the query is ambiguous and Wikipedia suggests multiple interpretations (senses), multiple home articles are chosen. The candidate concepts for the query are extracted from the in-going links (articles having links to a home arti-cle), out-going links (articles to which a home article links), and the links derived from the captions associated with any images on the home article. The end result of this process is the selection of a set of home article(s) { h of given query Q ), along with a list of all the candidate concepts C These concepts provide the basis for the automatic query expansion process.

Due to the rich and interconnected nature of Wikipedia, the number of concepts obtained in the process described above may become very large. Thus, a filtering step is necessary to ensure that the chosen concepts remain focused on the topic of the query. Here, our objective is to select the top-N concepts from among all the candidate articles. To achieve this objective, a semantic relatedness measure based on WLM ( Milne &amp; Witten, 2008 ) is used to evaluate all candidate articles, and the top-
N concepts with the highest relatedness measures to their home article are selected. These concepts are used as the source for the query expansion. The value of N serves here as a diversification parameter. How it affects the diversification and pre-cision of search results are discuss in ( Hoque, Hoeber, &amp; Gong, 2011a ).

In order to ensure that the expanded queries remain focused on the topic of the query itself, the original query Q is pre-pended to each of the top-N related concepts { c r j 0 6
N expanded queries. All queries are then sent to the Google AJAX Search for retrieving the desired number of images. 3.2. Image organization
The process of concept-based query expansion provides the search engine with a better chance to satisfy the searchers X  intentions. However, it also introduces new challenges, such as how to present the broad and diverse range of images so that the searchers can focus on the specific aspects of the query in which they are interested. A na X ve approach would be to use a traditional paged grid layout of the images, ordered by the rank in the search results list, and perhaps the semantic related-ness between the top-N concepts and the home articles. However, such an approach may not be effective in supporting im-age search tasks, as the rationale for the organization of the images may not be obvious. Since the expanded query produces a broad range of images representing the various interpretations of the query, it is beneficial to provide an interface that can allow the searcher to easily find the aspect of their query that matches their search intentions. A visual method for organiz-ing the images retrieved is well suited to this task.

Our approach arranges images based on their conceptual and visual similarity on a 2D virtual canvas using a hierarchical version of Self-Organizing Map (SOM) ( Kohonen, 1995 ). In this approach, all the images in the search results set are repre-sented using feature vectors. For each image, a conceptual feature vector C ( I ) is created using the concept tag { c training is complete, each image is mapped to the best matching unit (BMU) in the SOM, which provides a mapping from images to locations on a 2D canvas ( Strong et al., 2010 ).

At this stage of the organization, each image has a position on a grid that makes sense based on its visual and conceptual similarity to its neighbors. However, very often there will be more images retrieved than can be shown at a reasonable size within the available display space. To address this problem, a display priority is assigned to all images such that those images that are more representative of their specific region in the image space are given a higher display priority ( Strong &amp; Gong, 2008; Strong &amp; Gong, 2011 ). When the screen space is insufficient, only images with high display priority are shown. The end result is a hierarchical organization of images that allows the searcher to zoom into a region of interest, displaying the images with lower display priorities as sufficient space is made available. A more detailed outline of the approach, algo-rithms, and analysis of such an approach to image search results organization is provided in ( Hoque, Hoeber, et al., 2011 ). 3.3. Concept hierarchy generation
In addition to aiding in the arrangement of images from the search results, CIDER also uses the top-N related concepts from which the expanded queries were derived to support focusing and filtering operations. This set of concepts is mapped to an ontology using DBPedia ( Bizer et al., 2009 ). According to the mapped ontology, concepts having the same type are grouped together under their corresponding type (e.g., people, location, company, automobile, etc.). The organization of con-cepts is displayed to the searcher in a hierarchical manner. Within the concept hierarchy, the query itself represents the root node, while each of the interpretations of the query are placed as children of the root node. At the next level, concepts are grouped by their types under the interpretation to which they belong. Images retrieved for a particular concept are con-nected as the children of that concept. All of the images available in the image space are represented as terminal leaves in the concept hierarchy ( Hoque, Strong, et al., 2011 ).

Fig. 1 shows a screenshot of CIDER for a sample ambiguous query. One can readily see the four different interpretations of the query within the concept hierarchy. In addition, it is clear that the images for these different interpretations are also grouped together. How a searcher can interactively use this interface to fulfill their image seeking goals is outlined in the section that follows. 3.4. Support for exploration and retrieval
The searcher can use the hierarchy of concepts for both focusing and filtering operations. By selecting any of the concepts in concept hierarchy, the images that were retrieved as a result of that concept are pulled to the front of the display (tem-porarily increasing their display priority within the image organization process); the remaining images are dimmed giving the focused images more visual prominence. Similarly, by clicking on an interpretation, all the images that were connected to the concepts under that interpretations are highlighted. In addition, the searcher can use checkboxes associated with each node in the concept hierarchy to filter the search results, removing the associated images from the display. This feature al-lows the searcher to quickly scan the names of the concepts, removing those that are obviously not relevant to their meaning for the query. Together, these two features can allow the searcher to quickly inspect a particular concept (clicking on it will focus the display on the images that were retrieved as a result of this concept), and decide whether to keep it as part of the search results set or remove it via the filtering operation. This interactivity allows searchers to easily explore the search re-sults, based on the Wikipedia concepts that generated the expanded queries.
Since the images comprising the search results are organized based on both conceptual and visual similarity, a logical method for exploring within this image space is through pan and zoom operations ( Strong &amp; Gong, 2008; Strong &amp; Gong, 2011 ). In the organization, each image has a position on a grid that makes sense based on its similarity to its neighbors.
As the searcher zooms into an area of interest and more space is created between the images that are visible, additional images that were previously hidden are shown. At the same time, images that are distant from the focal point of the zoom operation are pushed out of the viewport. This interactive feature supports the searchers to explore other images that are semantically and/or visually similar to the region of focus, but were previously hidden due to space constraints of the display. 4. Example
In order to illustrate the benefits the system provides to the users as they seek to interactively explore the image search results, an example is provided here and a video is posted online. the intention to find images related to a VW Beetle car. Because of the ambiguity in the provided query, it is automatically ex-panded and a broad range of search results are retrieved.

The image search results are organized and presented to the searcher, as shown in Fig. 2 a. Here, the concept hierarchy presents an overview of the top-N related concepts as a tree. The image space presents the search results based on their con-ceptual and visual similarity, resulting in images from similar concepts being grouped together (e.g., images from  X  X W Beetle cars are located in the top-right side of the image space) while at the same time grouping visually similar images (e.g., the white VW Beetle cars are placed near one another).

The searcher can quickly browse the images by using the concept-based focusing operation. Fig. 2 b shows the results of such a focusing operation as the searcher selects the interpretation  X  X  X olkswagen Beetle X  X  from the concept hierarchy. This causes images related to VW Beetle cars to be pulled to the front of the display, and other images to be dimmed so that the searcher can easily find the area where the VW Beetle images are located.

As the searcher performs the focusing operation, images from other interpretations get dimmed but still remain within the image space. The searcher can filter them from the display by unchecking the checkboxes in the concept hierarchy. Fig. 2 c shows the results of the concept-based filtering operation, as the searcher unchecked the concepts from the interpretation  X  X  X eetle (insect) X  X . As a result of the filtering operation, all the images from this interpretation are removed from the display, leaving gaps in the image organization structure. This demonstrates the ability of the organization method to group conceptually similar images.

At this point, the searcher has found an area of interest within the image space (through conceptual focusing operation) and eliminated the images that were irrelevant (through the filtering operation). Now, the searcher can zoom into the area of VW Beetle images, and pan around different areas of canvas to examine the images of interest at the highest zoom level.
Fig. 2 d shows such a screenshot, where the searcher has reached the bottom level zoom, and the canvas contains the images that are primarily of VW Beetle cars. At this point, as well as at any other point during the search process, searchers can examine individual images that seem relevant to their image needs.

At any time it is also possible to zoom back out to the initial display once again see the overview of the image space. The searcher may also continue to manipulate the concept-based filter and focus controls to further explore within the image search results. 5. Preliminary evaluation 5.1. Purpose
The key features of CIDER include automatically expanding the original query using related concepts to retrieve a broad range of images, organizing the images and associated concepts, and supporting the searcher in actively exploring within the image space. Since this approach moves beyond the traditional image search paradigm, it leads to a fundamental research question: does the approach for diversifying and organizing the image search results improve searcher performance and sub-jective opinions regarding their image search tasks?
Evaluating highly interactive systems such as this can be very challenging due to the large number of experimental con-ditions that may be manipulated ( White, Muresan, &amp; Marchionini, 2006 ). Rather than evaluating each feature in isolation, a more realistic approach for this preliminary evaluation is to study the system as a whole. As such, a user study was con-ducted in a controlled laboratory setting to evaluate the complete feature set of CIDER in comparison to the traditional ap-proach to image search.

While a common procedure for conducting user studies is to assign tasks to the participants and measure the time it takes to complete the tasks and the number of errors made, such measures alone may not provide a clear picture of how much knowledge is gained through the search activities. Hence, in this preliminary evaluation, we also collected the participants X  qualitative feedback in order to measure their perceptions of the amount of knowledge gained through the search process, the overall usefulness of the approach and satisfaction in completing the task, and the ease of use of the software. The par-ticipants X  opinions on the usefulness of specific features of the systems being studied were also recorded, as were their open-ended comments and our observations of their use of the system. 5.2. Method
Even though we considered this a preliminary evaluation, we took care in designing the study to allow for the compar-isons among the collected data. As such, the study was designed as a 2 2 (interface task complexity) mixed design. Each participant used each of the two interfaces (CIDER and Google Image Search) twice, but performed each of the four search task only once. The image retrieval tasks were designed at two levels of task complexity: simple tasks ( S ) and complex tasks ( C ).

In order to simplify the study design, participants performed a complete set of tasks (a training task, a simple task, and a complex task) using Google Image Search first, followed by a second set of tasks using CIDER. This simplification was based on the assumption that participants in the study would already be familiar with interfaces such as Google Image Search that follow a scrollable grid-based approach to organizing the images. This assumption was validated in the pre-study question-naire (see Section 5.4 ). Assigning the simple tasks first allowed the searchers to gain some experience using the interfaces before being exposed to the more difficult task. In order to address potential learning effects regarding the specific tasks, they were varied using participant group assignment.

To reflect the exploratory nature of the image search activities, tasks were designed based on a set of four ambiguous que-ries (see Table 1 ), but with rather specific information needs. To avoid the effect of variation in ambiguity of the query, each was chosen such that they had a similar level of ambiguity (i.e., three primary interpretations as provided by Wikipedia).
However, task complexity was varied for the different queries. For the queries  X  X  X eetle X  X  (S1) and  X  X  X ivoli X  X  (S2), the tasks were chosen such that the desired images were found among the top ranked images provided by Google Image Search. For the queries  X  X  X aguar X  X  (C1) and  X  X  X uji X  X  (C2), the tasks were more difficult to complete because the relevant images were scattered throughout the search results. Although these tasks do not represent the breadth of image search tasks that real-world searchers may wish to solve, they do allow us to make some comparisons of searcher performance in these preliminary evaluations.

For CIDER, the concept-based query expansion process was performed, 300 images were retrieved for each query, orga-nized based on conceptual and visual features, and presented to the searcher. The diversity within the search results required the participants to interactively explore within the image space in order to find relevant images. For Google Image Search, 300 images were retrieved for each query, and were presented in a traditional grid-based interface. This organization al-lowed participants to scroll and page through the image grid in order to find the relevant images. 5.3. Procedure
At the beginning of the user study, pre-study questionnaires were administered to determine prior experience with image search, educational background, and computer use characteristics. Participants were then given a brief training task with the
Google Image Search interface, outlining the basic features of the interface and the methods for identifying relevant images (i.e., verbal identification). They were allowed to take as much time as they needed, but were required to find five images with respect to the query. The relevance of identified images as well as the time taken to find five images was measured. This procedure was repeated using CIDER.

After each task, questionnaires were administered to obtain the participants X  subjective opinions regarding the assigned interface and task, including their perceptions of task difficulty, satisfaction, and knowledge gain. After completing all tasks, post-study questionnaires were administered based on the Technology Acceptance Model (TAM) ( Davis, 1989 ), capturing subjective reactions regarding the usefulness and ease of use of both interfaces. In addition, these questionnaires also mea-sured the participants X  impressions of specific interface features and their overall preference for an image search interface.
An open-ended question asking for additional comments was also provided, and observations regarding the participants X  use of the systems were logged. 5.4. Participants
Sixteen participants were recruited from first and second year undergraduate computer science and engineering courses at our university to participate in this study. As expected all participants in the study were frequent computer users. How-ever, their responses regarding how often they performing image search tasks on the Web ranged from infrequent (1 X 5 searches per week) to very frequent (over 15 searches per week), with the median at a moderate frequency (6 X 10 searches per week). While most indicated that they normally search for named entities (e.g., people, location/landmarks, specific ob-jects, etc.), some also expressed interests in searching for general objects and themes. All participants reported being familiar with the traditional grid-based image search interface (e.g., Google Image Search).

From this data, we conclude that the participants in this study constitute a relatively coherent group with a moderate level of expertise in performing image search tasks. While there was some variability regarding how frequently they search for images on the Web, we believe this variability is representative of real-world image search users. Furthermore, even for participants who search for images infrequently, it is still an activity that they performed on a weekly basis. As such, we can assume that all the participants are equally adept at using existing image search interfaces and familiar with the general task of finding images that match some specified need. 5.5. Evaluation results
In the course of this user study, a number of specific measurements were taken in order to gauge participant performance, along with their subjective opinions and impressions of the interfaces used. In the sections that follow, the results from each of these measures are discussed in detail. Where applicable, significance of statistical analysis of the data is highlighted with a bold font. 5.5.1. Time to task completion
The average times required to complete the four tasks using the two interfaces are illustrated in Fig. 3 . The results for the simple tasks were mixed, with participants completing task S1 faster using Google Image Search, and completing task S2 faster using CIDER. Participants completed both complex tasks faster using CIDER. Table 2 provides the results of a statistical
CIDER was found to be faster, the results were statistically significant. For the one task in which Google Image search was found to be faster, this difference is not statistically significant.

This result illustrates that when searchers are faced with a difficult image retrieval task, the interactive features of CIDER can help the searchers to find relevant images faster than simply browsing through a ranked grid of images. In particular, the overhead of interacting with the interface resulted in a significant time savings by allowing the searcher to identify and focus on a region within the image space that contained relevant images. This also held for one of the simple tasks even though the desired images where ranked highly in the Google Image Search interface. For the other simple tasks, the participants were able to visually browse the search results in the Google Image Search interface and quickly find a set of relevant images. 5.5.2. Accuracy
During each task, participants were asked to find five relevant images according to the information needs that were as-signed to them. After the participants completed the tasks, the images selected by the participants were carefully inspected to verify their relevance. Two participants abandoned the tasks when using Google Image Search, as they were not interested in moving to the next pages once they could not find relevant images among the first couple of pages. As such, these two records were treated as outliers and removed in this analysis. Table 3 provides the average accuracy for each interface-task pair, as well a statistical analysis. While participants were marginally more accurate in finding relevant images using CIDER, the results were not statistically significant.

These findings indicate that there may not be much difference between CIDER and the scrollable grid-based interface used by Google Image Search in terms of the participants X  abilities to decide the relevance of individual images. Both inter-faces showed individual images at the same resolution, providing an equivalent level of support for relevance judgement once candidate images were identified for evaluation. 5.5.3. Perceived difficulty
After each task was completed, participants were asked to indicate how difficult they found the task. The goal was to com-pare how this subjective measure varies for the very same task but using different interfaces. The average responses to this question are reported in Fig. 4 . For simple tasks, the results were mixed: for task S1, participants who used Google Image
Search reported the task to be easier than those who used CIDER; the results were reversed for task S2. For the two complex tasks, participants consistently reported that the tasks were simpler when using CIDER. Table 4 provides the results of a for all tasks except for S1.

We anticipated that the participants would perceive the simple tasks to be more difficult when using CIDER, based on the overhead of using the interactive features to manipulate and explore the image space. For complex tasks, we expected the opposite: that participants would find the complex tasks easier when using CIDER, due to their ability to interactively focus on a region of the image space that includes relevant images. As with the time to task completion analysis, the results were mixed for the simple tasks, but consistent with the our expectations for the complex tasks. 5.5.4. Perceived satisfaction
Upon identifying a set of five relevant images for each search task, participants were asked to rate their satisfaction with the using the assigned interface to complete the task. The average responses to this question are reported in Fig. 5 , and the results of the statistical analysis are summarized in Table 5 . Consistent with the participants X  perceptions of difficulty, their perceptions of satisfaction were mixed for the simple tasks, but consistently positive for the complex tasks after having used CIDER.

Of note is the substantial disparity in the participants X  perceptions of satisfaction between the two interfaces for the com-plex tasks. Although we anticipated a difference in the data, we did not expect such a wide range between the responses. This result provides strong positive evidence of the value of the visual organization of the images and the highly interactive fea-tures of CIDER in comparison to the simple scrollable grid-based interface of Google Image Search, especially for image search tasks that cannot be solved simply by scanning the top images in the display. 5.5.5. Perceived knowledge gain
The amount of knowledge the participants are gaining as they perform their assigned search activities is very difficult to estimate ( Hoeber, 2009 ). While it may be possible to quiz the participants at the end of the session, separating the ability of the participant to learn about the topic from the support the search interface provides may not be possible. As such, a simple approach was followed to measure perceived knowledge gain, wherein after each task was completed, the participants rated how much knowledge they thought they had gained about the given search topic. As can be seen from Fig. 6 , the results were in favor of CIDER across all tasks. As shown in Table 6 , the difference in the participants X  perceptions of knowledge gain be-tween the two interfaces is statistically significant.

We attribute this positive result in favor of CIDER to the presentation of the concepts extracted from Wikipedia regarding the search topic. By presenting this information to the searcher, and allowing them to use it to both filter unwanted images and focus on specific images within the search results, the participants were able to easily discover interesting features of their search query that would have been difficult for them to articulate based on their prior knowledge. 5.5.6. Ease of use and usefulness
After all of the search tasks were completed, a post-task questionnaire was administered to gauge the participants X  overall perceptions of ease of use and usefulness. Multiple questions for each of these usability goals were asked, following the TAM instrument. The results were aggregated, and are presented in Fig. 7 . The statistical analysis of these results are reported in Table 7 .

Although the average responses regarding the ease of use were superior for CIDER, this difference was not statistically significant. However, a positive outcome that we can draw from this is that even though the interactive features of CIDER are certainly more complex than Google Image Search, the participants did not report them as being any more difficult to use than the simple and familiar interactions of scrolling and scanning a grid of images.

In terms of usefulness, the participants reacted more positively to CIDER than Google Image Search; this finding was found to be statistically significant. Given that for three of the four tasks, participants were faster in completing the tasks using CIDER, perceived these tasks to be less difficult, expressed a greater level of satisfaction, and indicated a higher degree of knowledge gain, it is not surprising that they also indicated that CIDER was more useful for their image search tasks. This result provides support for the validity of the other findings. 5.6. Opinions regarding specific features
The post-study questionnaire also included a number of questions regarding the value of specific features of both CIDER and Google Image Search. The results of these responses are provided in Fig. 8 . Although conducting a direct feature-by-fea-ture comparison or statistical analysis on this data is not feasible, we can readily see that the responses regarding specific features of Google Image Search were distributed over the entire range and neutrally biased. In contrast, the responses regarding the specific features of CIDER were ranged from neutral to strongly agree, and were primarily in the agree to strongly agree range. 5.6.1. Preference
At the conclusion of the study, the participants were asked to indicate their preference for an image search interface. Fif-teen of the 16 participants indicated their preference for CIDER (93%) over Google Image Search. A Wilcoxon signed rank test ticipants received in having the search results dynamically diversified, and using the interactive features of CIDER to explore the image space. 5.7. Open-ended feedback and observations At the end of the study, the participants were allowed to freely comment about their experiences with CIDER and Google
Image Search. While most provided positive feedback in favor of CIDER, some commented on their difficulties in understand-ing the association between concepts and images. In the current prototype, these elements are displayed in two separate visual components: the concept hierarchy and the image space. Making the connection between a specific concept and its associated images requires the searcher to select the concept to bring the images to the foreground in the image space. In some cases, we observed participants focusing on many different concepts as they explored the search results, which re-quired that they keep track of the concept-image associations on their own. Although we made a design decision to not explicitly link the concepts and images within CIDER, it may be beneficial to explore some of the options for allowing the searchers to more readily observe these relationships.

When the participants were using Google Image Search, we observed that they very quickly became reluctant to examine the images sequentially in the traditional grid-based interface. Their frustration at not being able to find relevant images among the top few became clear from their body language. In these cases, some participants scanned the images and se-lected ones based solely on their visual appearance, while others randomly selected images in the hopes of getting lucky. In a few cases, this frustration at not being able to find what they were seeking resulted in an abandonment of the task.
By contrast, when these participants were using CIDER, they were able to take advantage of the clustering of related images to limit the range of their visual inspection and search to a much smaller set of images. As a result, they expressed a higher level of confidence in their results.

Some participants also asked if they could refine the query, rather than examine the search results generated by the query provided. This occurred for both interfaces. Rather than exploring the search results, these requests indicated that these par-ticipants would have preferred to simply modify the query in order to create a more accurate description of the images they were asked to find. While both interfaces allow for query refinement, we restricted the use of this feature in order to keep the study focused on the browsing and exploration tools provided by the two interfaces. The reluctance to examine the search results when the solution is not found immediately is common in Web search ( Jansen &amp; Spink, 2006 ), and apparently extends to image search as well.

One of the challenges that a small number of participants faced when using CIDER was their inexperience with the fea-tures of the interface. While they were provided with a brief introduction to the key interface elements, and were permitted to use the interface during the training task, some faced difficulties in using the features effectively when performing the first image space), whereas concept-based focusing and filtering might have been a more effective search strategy. This inexpe-rience can explain the reason for the time to task completion being higher for the simple tasks (which were performed first) and lower for the complex tasks. After completing the simpler tasks, the participants became more comfortable and familiar with the features, and were able to use the interface more efficiently for the complex tasks. In future studies, a more exten-sive training phase may alleviate this learning effect, and will provide more accurate data regarding time to task completion. 6. Discussion
In the course of the design, development, and study of CIDER, we have learned a number of important lessons regarding the diversification, presentation, and exploration of image search results. In addition, we have also learned a number of use-ful lessons with respect to the preliminary evaluation which will improve the reliability of future evaluations of this ap-proach and others like it. These lessons are discussed below.

The traditional approach to image retrieval relies on the searcher to provide an accurate query, and for the image search engine to correctly interpret the meaning of the query. However, as with document searching on the Web, people have dif-ficulty providing specific and accurate queries, and commonly use short and ambiguous ones. As a result, the searcher X  X  meaning for the query may not match the images provided by the search engine. CIDER is designed to address this situation by automatically diversifying the search results via automatic query expansion, organizing the images visually and concep-tually, and providing an interactive interface to support the exploration among the images and associated concepts. How-ever, diversification is not a magic solution to image search problems in all circumstances.

One of the main drawbacks to performing search result diversification is that it can introduce potentially irrelevant images within the search results. In previous work, we have shown how the precision for the most common interpretation of an ambiguous query will be reduced, while the precision for the less common interpretations will be increased ( Hoque et al., 2012 ). To mitigate the possible negative consequences of over-diversification, we have devised a method for automatically adjusting the degree of diversification based on the ambiguity of the query. That is, if a query is highly ambiguous, then CIDER will diversify the search results to a large degree in order to get an even coverage of the interpreta-tions of the query in the search results; if a query is not ambiguous, then CIDER will diversify the search results to a low degree in order to avoid introducing unintended interpretations of the query. Even so, if the searcher is looking for images that match the search engine X  X  interpretation of the query, then diversification of the search results will be a hinderance.
However, if there is a mismatch between the searcher X  X  intent for the query and search engine X  X  interpretation, then diver-sification can be very beneficial.

In order to counter-act the potential for non-relevant images being included in the search results, CIDER incorporates a hybrid concept-visual organization of the images that supports zooming within regions of interest, and interactive features to allow the searcher to filter and focus the search results based on conceptual information. Using these features to explore the search results, the precision for a particular sense of the query may be improved as irrelevant images are moved out of the field of view during a zoom operation, or are filtered by removing concepts that are uninteresting to the searcher. An empirical evaluation of the changes in the precision of the search results based on using these features has previously been conducted, showing their value when good choices are made ( Hoque, Hoeber, et al., 2011c ). Without such features, merging the diversified set of images could be challenging, and it may be difficult for searchers to make sense of the search results when the query is ambiguous. As such, we consider this a fundamentally important element of the system.

One of the goals of the preliminary evaluations discussed in Section 5 was to examine whether participants were indeed able to make good choices when using CIDER to explore the diversified set of image search results. Although the complexity of the search tasks was varied, all of the queries for examining this question were chosen to be ambiguous since such queries are common in Web image search ( Andr X  et al., 2009; Jansen et al., 2003 ). For complex search tasks that required the search-er to delve deeper into the search results set, CIDER allowed the participants to complete the tasks quicker, more easily, and with a higher degree of satisfaction and knowledge gain. For the simple tasks, the results were mixed. We attribute this to the difference in the prior knowledge of the participants on these two tasks. Task S1 (Beetle) could be solved based on com-mon knowledge, whereas Task S2 (Tivoli) required more specialized knowledge. Of interest here is the fact that the measures for Task S2 were similar to that of the complex tasks. Although this result suggests that CIDER may be a useful tool for a searcher to learn about an unfamiliar topic, further study is required.

It should come as no surprise that CIDER performed well in these preliminary evaluations, given the fact that the diver-sification method and the interactive features were designed to support searching for ambiguous topics. The comments that some of the participants made after the completion of the tasks highlighted the experimental findings in the value of the approach. For example, one participant commented:  X  X  X  find it really easy to get pictures that I wanted ... When I was looking for the Volkswagen car, I clicked on the Volkswagen concept and immediately all the Volkswagen car pictures were high-lighted ... It was interesting to see all the Volkswagen cars are together. X  X  Another participant stated that the  X  X  X ooming fea-ture was really cool X  X , and further noted  X  X  X  really want to have it instead of scrolled pages X  X .

Even though the results of these preliminary evaluations were positive, there are areas in which improvements can be made to CIDER. The success of this approach depends to a great extent on the ability of the system to match the searcher X  X  query to the knowledge base derived from Wikipedia. If there is no Wikipedia article that matches the query, then the diver-sification process is not possible, the concept hierarchy will be empty, and the image search results will be organized based only on visual similarity. Fortunately, the types of topics upon which people search for images matches well with the con-cepts that are contained in Wikipedia (e.g., people, places, things). However, this is not a guarantee. Further work on devel-oping a comprehensive knowledge base, perhaps with searcher-supplied information for when no match is possible, could increase the robustness of this approach to image query diversification.

As noted previously, making a more explicit match between the concepts and their associated images within the interface may also be beneficial. However, the challenge is to design additional interface features such that these relationships are apparent, but do not become the prominent interface feature. It is important to keep in mind that the primary goal of the searcher is to find relevant images, which means that the images themselves need to remain the primary interface features.
There is also an issue related to the real-time performance of the current prototype implementation. Since CIDER is imple-mented as a search interface using Google AJAX Search as the image provider, it must retrieve all of the images and calculate the hybrid feature vectors required by the image organization process before any images are shown to the searcher. Further-more, the hierarchical multi-resolution SOM used in this organization process is not able to organize the images in real-time.
For the purposes of the preliminary user studies, the search results were retrieved and the SOM calculated in an off-line man-ner. However, once this was done, the interface supported real-time zooming, focusing, and filtering interactions. These per-formance issues could be addressed by tightly integrating CIDER with an existing image retrieval engine, such that the visual feature vector information is calculated during the Web crawling process and stored with the images. It may also be possible to improve the efficiency of the image organization process via pre-processing measures, algorithmic improvements, and performing the calculations using GPUs.

The main lessons learned with respect to conducting the preliminary user evaluations was the need to provide sufficient training for the participants and taking into consideration the prior knowledge of the participants on the assigned search tasks. As previously noted, for a small number of participants, the training session did not give them enough time to become comfortable with using the interactive features of CIDER. As a result, they did not take full advantage of the benefits of the system for the first few tasks. There may have also been some differences in the participants X  prior knowledge on the simple tasks that affected their performance. For future evaluations, it will be beneficial to carefully observe the participant X  X  use of the system during the training, ensuring that each participant is able to make use of the full range of features before pro-ceeding. In addition, controlling for the participants X  prior knowledge will allow for a more reliable comparison of the results.
Since the preliminary user study was conducted in a laboratory setting, where the participants had limited opportunities to get familiar with CIDER, this limited their ability to become skilfull with the software. Furthermore, the search tasks were the same for all participants in order to allow for comparisons. A more realistic evaluation could allow for the participants to choose their own image search tasks, and to conduct their searches over an extended period of time. Studying CIDER in such a longitudinal setting may provide valuable insights regarding the learnability, usability, and utility of the interface by cap-turing specific search and interaction operations, along with the participants ongoing impressions of the system during real-world use as they become experts in the features of the software ( Hoeber, 2009 ). 7. Conclusions and future work
In this paper, we have outlined the key features of CIDER, designed to support the human element of image retrieval in situations where a searcher is unable to provide a clear and concise description of their needs. Starting with an ambiguous query, CIDER automatically expands the query resulting in a diversification of the images retrieved. The concepts selected for the query expansion are provided to the searcher using a hierarchal representation, supporting filtering and focusing oper-ations. The images themselves are organized within a zoomable image space, such that conceptually and visually similar images are placed near one another. Such an organization allows the searcher to visually identify regions that contain poten-tially relevant images, and zoom into these regions to find similar images.
 A preliminary user study was conducted to determine the potential benefits of the visual and interactive features of CI-
DER, in comparison to Google Image Search, a baseline grid-based image retrieval interface. The key finding in this study was that participants were able to often find a set of relevant images faster, with higher degrees of satisfaction and perceptions of knowledge gain, and lower degrees of perceived task difficulty. In addition, CIDER was reported to be as easy to use as Google
Image Search, but at the same time, more useful. All but one participant indicated that they preferred using CIDER. These results provide strong evidence in favor of the support CIDER provides to the task of finding images when a searcher is chal-lenged by the requirement of textually describing their needs. Further laboratory evaluations with a broader range of queries that vary over ambiguity, complexity, and prior knowledge, and using a larger pool of participants, are in the planning stages, as is a longitudinal evaluation to study how searchers are able to employ the features of CIDER for their real-world image retrieval tasks.

Although CIDER was developed for image retrieval in the general domain of Web, the approaches may be extended to image retrieval and browsing in other contexts. For example, the approach could be used to support finding images within a personal image library, or exploring images within an extensive image repository such as the collections used within advertising agencies. Furthermore, the approach could also be applied to multimedia retrieval, whereby images are used as surrogates for more complex multimedia such as entire videos or video segments. A key requirement for applying this approach to other such domains is the need for conceptual information associated with each image. Such information might be derived from metadata associated with the images, including tags, timestamps, and locations. Further work on improving the performance of the approach will benefit not only the current application of the approach, but also these vertical domains.
 References
