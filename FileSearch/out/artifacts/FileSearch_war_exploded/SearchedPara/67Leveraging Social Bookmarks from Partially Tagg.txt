 The World Wide Web contains a wealth of information in amounts so enormous that it may seem daunting at first to be able to mine any useful information one is looking for. Fortunately, Web mining techniques such as clustering help to organize the Web content into appropriate subject-based categories so that their efficient search and retrieval becomes manageable.

Traditional Web page clustering typically uses only the page content information (usually, just the page text) in an appropriate feature vector representation such as Bag of Words, TF-IDF, etc., and then applies standard clustering algorithms (e.g., K-means algorithm [McQueen 1967], spectral clustering [von Luxburg 2007], etc.). Another approach somewhat related to clustering is to mine topic information from documents collections (e.g., Latent Dirichlet Allocation [Blei et al. 2003]), which can be seen as clustering words occurring in each document (instead of clustering documents directly).

On the one hand, the proliferation of the World Wide Web presents ever increas-ing challenges for the search engines to cope with task of mining the humongous wealth of available information on the web nowadays. On the other hand, the increas-ing amounts of user-generated content nowadays nicely complements this information and can help in an effective mining of the data present on the Web. For example, users can provide captions for images on the internet, provide tags to Web pages and other media content they regularly browse on the internet, etc. Therefore such user-generated content can provide useful information in various form such as meta-data, or in more explicit ways such as tags.

User specified social tags , in particular, have proven to be extremely effective in browsing, organizing, and indexing of Web pages. Various social bookmarking Web sites such as StumbleUpon and Delicious allow users to tag Web pages with keywords or short text snippets that can provide a description of the Web pages. Users can col-laboratively tag Web pages, and this has made organizing, sharing, navigating, and retrieving Web content much easier than ever before. In this work, we aim to exploit the tag information for a Web-mining task, namely Web page clustering.

Since user-provided tags can often provide high-level, contexual information for Web pages, we want to exploit them by treating the tag information as an alternate view of the data. Motivated by the success of multiview learning algorithms [Ando and Zhang 2007; Bickel and Scheffer 2004; Blum and Mitchell 1998; Brefeld and Scheffer 2004; Kakade and Foster 2007; Muslea et al. 2002] in various machine learning tasks, we use two views of the data (page-text and social tags) to extract highly discriminative fea-tures and perform clustering using these features. The feature extraction amounts to performing clustering in a lower dimensional subspace which is also effective in deal-ing with the problem of overfitting when we only have a small number of documents having a very large number of features. In particular, we use a regularized variant of the Kernel Canonical Correlation Analysis [Gestel et al. 2001; Hardoon et al. 2004; Hotelling 1936] (KCCA) algorithm to learn this subspace. KCCA (and Canonical Corre-lation Analysis (CCA) in general) has received tremendous attention due to its ability for effectively extracting useful features from heterogeneous or parallel data sources, such as images and text [Socher and Fei-Fei 2010], or features and labels (supervised dimensionality reduction [Ji et al. 2008; Rai and Daum  X  e III 2009]). Therefore, such an approach is expected to be useful for extracting useful features in the case of Web page clustering as well since such data natually comes with multiple views (page-text and social tags in our case).

One problem with the most existing multiview learning algorithms is that they re-however not always be the case. For example, in the context of social bookmarking datasets, user tags may be available only for a small subset of Web pages. One way to apply multiview learning algorithms in this setting would be to first try to predict (using some classification algorithm) the set of social tags for each non-tagged Web page. This can however be very expensive since the set of possible tags ( X  X abels X ) can be really large (equal to the tag vocabulary size).

This limitation makes it necessary to develop multiview algorithms that can work even with incomplete view information. With this motivation, we also present an ex-tension of our kernel CCA based approach approach to deal with missing views. Our approaches to deal with missing views is based on the fact that the similarity between a pair of examples should be the same across all the views. In particular, we show how the kernel CCA based approach to multiview clustering [Chaudhuri et al. 2009] can be used in situations when only one view (the primary view) is complete whereas the other view(s) could potentially be incomplete , that is, features from such view(s) are available only for a small number of examples. Our approach does not require computing the explicit features in the incomplete views (e.g., we do not require the tags to be predicted for the nontagged Web pages). In particular, we take the kernel variant of CCA [Hardoon et al. 2004], which works on the kernel matrices defined over each view, and propose a way to construct the full kernel matrix corresponding to the incomplete view, given the other complete view. This is followed by applying the kernel CCA-based multiview clustering algorithm. Our presentation is based on the kernel CCA-based multiview clustering but our approach can also be applied to other kernel-based multiview clustering algorithms [de Sa 2005].

The rest of the article is organized as follows. In Section 2, we describe the general framework we are considering in this article. Section 3 briefly describes multiview learning algorithms. Section 3.1 and Section 3.2 describe CCA and kernel CCA re-spectively. Section 4 describes our approach for dealing with the incomplete views in the kernel CCA setting. Our results are described in Section 5. We discuss related work in Section 6. In Section 7, we briefly describe some possible future work, includ-ing an active learning [Settles 2009] extension that can help in choosing which Web pages to get the social tags for, if we can get the social tags for only a small number of Web pages. We conclude with Section 8. Our problem setting consists of a collection of Web pages where each page also has a set of user-specified tags (e.g., from social bookmarking Web sites such as Delicious or StumbleUpon). The goal is to obtain a clustering of the Web pages into semantically relevant categories. To assess the relevance and coherency of the discovered clusters, one can use hierarchical Web directories such as the Open Directory Project (ODP) as the gold standard. Web directories such as ODP are widely acceptable gold standards because they usually provide an agreed-upon clustering of Web pages by human users, and have been used for evaluations in various recent works [Lu et al. 2009; Ramage et al. 2009].

In this article, we study vector space models for clustering in which each document (a Web page) is represented using a feature vector derived from the page-text (and, if available, other contextual information, such as tags, which we consider in this article). The K -means algorithm is a popular vector space model for flat-clustering that works iteratively by assigning each data point to its nearest cluster center, recomputing the cluster centers, and repeating the process until convergence. In this article, we use the K -means algorithm for our evaluations. Our approach, however, is applicable to any vector space clustering algorithm.

Formally, for our clustering task, we are given a collection of N Web pages, with each Web page consisting of a bag of words from a word vocabulary W , and a bag of tags from a tag vocabulary T . The goal is to cluster the Web pages in K clusters where is expected to have very little overlap with the word vocabulary since the social tags assigned to a Web page are usually words conveying contexual and semantic information about the Web page. Therefore, in most cases, the words used for social tags are not part of the Web pages.

There are a number of ways in which the vector space algorithms such as K -means can exploit the tag information to improve clustering of Web pages. Some of the com-mon choices are Ramage et al. [2009] and Lu et al. [2009]. (1) Words Only. Discard the tag information (use only bag of words in page-text). (2) Tags Only. Discard the word information (use only bag of tags). (3) Words + Tags. Form a combined bag of both words and tags, and use it to derive (4) Word Vector + Tag Vector. Form two separate feature vectors (e.g., in bag of words
It turns out [Lu et al. 2009; Ramage et al. 2009] that the concatenation of word and tag feature vectors (4) outperforms approaches that use feature vectors derived from the word (1) vocabulary, the tag vocabulary (2), or vocabulary derived from a union of words and tags (3).

However, the concatenation approach inflates the feature vector size of each docu-ment, and therefore the approach tends to not do well if the number of Web pages is small as compared to the feature dimensionality [Kriegel et al. 2009]. The reason can be attributed to the fact that clustering, and density estimation in general, can yield poor parameter estimates if the number of features far exceeds the number of data points. Furthermore, one would expect that there would be a significant correlation between the words and the tags for a given Web page and the concatenation based ap-proach fails to exploit this correlation. Also, the relative importance of features in the tags and words views of the concatenated vector can be different which may require an explicit weighting of features in the two views [Ramage et al. 2009].

A number of efficient clustering algorithms deal with high data dimensionality by first projecting the high-dimensional data onto a lower-dimensional subspace, and then performing clustering in that subspace. The projection step is usually performed us-ing standard dimensionality reduction techniques such as principal component analy-sis [Vempala and Wang 2002] (PCA), or random projections [Dasgupta 1999]. However, PCA or random projections preserve only the data variances or pairwise distances and fail to take advantage of multiple views of the data (if such information is available). Also note that even if PCA is performed on the joint words + tags vector, it would only maximize the variances of word and tag feature spaces individually, without capturing their correlations. In multiview learning, the features can be split into two subsets such that each subset alone is sufficient for learning. By exploiting both views of the data, multiview learn-ing can result in improved performance on various learning tasks, both supervised and unsupervised [Ando and Zhang 2007; Bickel and Scheffer 2004; Brefeld and Scheffer 2004; Foster et al. 2008; Kakade and Foster 2007; Muslea et al. 2002]. Multiview approaches help supervised learning algorithms by being able to leverage unlabeled data [Blum and Mitchell 1998], whereas, for unsupervised learning algorithms, multi-ple views of the data can often help in extracting better features [Foster et al. 2008].
Canonical Correlation Analysis [Hotelling 1936] (CCA) is an unsupervised feature extraction technique for finding dependencies between two (or more) views of the data by maximizing the correlations between the views in a shared subspace. This property makes CCA a suitable choice for multiview learning algorithms. In our settings, the two views are words in the page-text, and the set of tags for each Web page. CCA is then applied as a projection technique to extract features from Web page data, with projection direction guided by the tag information. Final clustering is then performed using the features extracted by CCA. Canonical Correlation Analysis (CCA) is a technique for modeling the relationships between two (or more) set of variables, as shown in Figure 1. CCA computes a low-dimensional shared embedding of both sets of variables such that the correlations among the variables between the two sets is maximized in the embedded space. CCA has been applied with great success in the past on a variety of learning problems dealing with multimodal data [Hardoon and Shawe-Taylor 2003; Hardoon et al. 2004; Rustandi et al. 2009].

More formally, given a pair of datasets X  X  R D 1  X  N and Y  X  R D 2  X  N , CCA seeks corresponding examples in the two datasets are maximally correlated in the projected space. The correlation coefficient between the two datasets in the embedded space is given by
Since the correlation is not affected by rescaling of the projections w x and w y ,CCA is posed as a constrained optimization problem. subject to:
It can be shown [Hardoon et al. 2004] that the given formulation is equivalent to solving the following generalized eigen-value problem: where xx and yy denotes the covariances of data samples X =[ x 1 ,..., x n ]and Y = [ y Canonical Correlation Analysis is a linear feature extraction algorithm. Many real-world datasets, however, exhibit nonlinearities, and therefore a linear projection may not be able to capture the properties of the data. Kernel methods [Shawe-Taylor and Cristianini 2004] give us a way to deal with the nonlinearities by mapping the data to a higher (potentially infinite) dimensional space and then applying linear methods in that space (e.g., Support Vector Machines [Burges 1998] for classification, Kernel Principal Component Analysis [Sch  X  olkopf et al. 1998] for dimensionality reduction). The attractiveness of kernel methods is attributed to the fact that this mapping need not be computed explicitly, via the technique called the kernel trick [Shawe-Taylor and Cristianini 2004].

The kernel variant of CCA (called Kernel Canonical Correlation Analysis -KCCA) can be thought of as first (implicitly) mapping each D dimensional data point x to a higher dimensional space F defined by a mapping  X  whose range is in an inner product space (possibly infinite dimensional), followed by applying linear CCA in the feature space F .

To get the kernel formulation of CCA, we switch to the dual representation [Hardoon et al. 2004] by expressing the projection directions in Equation (1) as w x = X  X  and w y = Y given by: Now using the fact that K x = X T X and K y = Y T Y are the kernel matrices for X and Y , kernel CCA amounts to solving the following problem: subject to the following constraints  X  T K 2 x  X  =1and  X  T K 2 y  X  =1.

KCCA works by using the kernel matrices K x and K y of the examples in the two views X and Y of the data. This is in contrast with linear CCA which works by doing an eigen-decomposition of the covariance matrix. The eigenvalue problem for kernel CCA is given by:
For the case of linear Kernel, KCCA reduces to the standard CCA. However, working under the kernel formalism has the additional advantage of being computationally efficient if the number of features greatly exceeds the number of examples because KCCA works on N  X  N kernel matrices, whereas CCA works on D  X  D covariance matrices. The former would be much more efficient than the latter if D N ,whichis usually the case with document clustering where the vocabulary size often far exceeds the number of documents.
 To avoid overfitting and trivial solutions (nonrelevant solutions), CCA literature [Hardoon et al. 2004; Shawe-Taylor and Cristianini 2004] suggests regularizing the projection directions w x and w y by penalizing them using Partial Least Squares (PLS) which basically means that their high weights are penalized. This is achieved by adding regularization terms corresponding to w x and w y in the denominator of Equa-tion (4).
Since this equation is invariant to scaling of  X  and  X  , we impose the following con-straints on the denominator terms: Kernel CCA relies on the decomposition of kernel matrices which can be an expensive operation as the number of examples grows. To deal with this, one can use Incom-plete Cholesky Decomposition [Bach and Jordan 2003] (ICD). We, on the other hand, use Partial Gram-Schmidt Orthogonalization (PGSO) as suggested in Hardoon et al. [2004]. Incomplete Cholesky method can be seen as a dual implementation of PGSO. The advantage of PGSO over ICD is that the former does not require permutations of rows and columns unlike the latter. One shortcoming of both CCA and KCCA is that they assume that features across all views are available for each example. This may, however, not be the case with many multiview datasets. For example, not all Web pages in a corpus might be tagged by users. Likewise, not all Web pages can be expected to have hyperlinks pointing towards them. Therefore, although one view (i.e., page-text) would be available for all the Web pages, the other view might be available only for a small number of Web pages. To apply multiview clustering on such datasets, one needs a way to deal with the lack of data in the incomplete view(s). In this section, we present an approach to address this shortcoming for KCCA. The problem for standard CCA can also be dealt with by using KCCA with a linear kernel. Also, our approach is not limited to kernel CCA based multiview clustering. It can also be used for other kernel based multiview clustering algorithms such as the multiview spectral clustering [de Sa 2005].
Note that KCCA works by first constructing the kernel matrix for each view of the data. For simplicity, let us denote the two views by X and Y . Generalization to more than two views with one complete and remaining incomplete views can be done in a likewise manner. Let us assume that view X is complete whereas view Y is incomplete, that is, the features for this view are available for only a subset of the total examples. To formalize, we denote the set of Web pages with features present in both the views X Web pages with features present only in view X (i.e., unpaired or missing )as M = { x the examples using features from view X . The corresponding graph Laplacian defined as L x = D x  X  K x , where D x is the diagonal matrix consisting of the row sums of K x along its diagonals.

Likewise, for view Y , we denote the kernel matrix by K y . However, since features for view Y are only available for a small number of examples, only an c  X  c subblock of the full kernel matrix K y will be available for this view (see Figure 2). In order to apply kernel CCA, one must first construct the full kernel matrix K y . Using the ideas from Laplacian regularization, this can be achieved by solving the following optimization problem for kernel matrix completion:
This objective optimizes w.r.t. K y the alignment between K x and K y , given the known part of K y . Here tr denotes the matrix trace. Note that although the multi-view assumption requires the views to be conditionally independent, since both views are just expressing different representations of the same object, both kernel matrices K x and K y are still expected to have a high degree of alignment between them.
The positive semi-definite constraint on the kernel matrix K y makes it a semi-definite program (SDP) [Boyd and Vandenberghe 2004], which can be solved using the existing SDP solvers. One problem with the SDP-based solvers is their lack of scalabil-ity to a large number of examples. Although the scalability can still be dealt with using first-order solvers such as SDPNAL [Zhao et al. 2010], assessing convergence can be an issue with such approaches. In this article, we take a different approach and, due to the special problem structure (i.e., upper left subblock of K y being known), we can in fact obtain a closed-form solution for K y . Furthermore, our approach is much less com-putationally intensive than having to solve an SDP since, as we will show, it requires only a couple of matrix multiplication and inverses. A similar approach was proposed in Carreira-Perpinan and Lu [2007] to compute the embedding of out-of-sample data-points in Laplacian eigenmaps.
 matrix for the set of examples with view Y available.

Since K y is a positive semidefinite matrix, we can express it as AA T where A is a matrix of reals. Further, let us write A as A = A c A
Using these, we can rewrite Equation (2) as follows:
Simplifying the equation, and using the fact that A c is a constant (since A c A T c = K cc y , a constant), gives:
Using the matrix trace property tr ( X )= tr ( X T ), one can see that the equation re-duces to:
Again, using the fact A c A T c = K cc y , we write the equation as: min
Taking the derivative w.r.t. A m and setting it to zero gives: Using K y = AA T gives us the closed-form expression for K y : Finally, substituting back for A c A T c = K cc y gives:
Having obtained the full kernel matrix K y for all c + m examples on view Y , we can now apply kernel CCA on the two kernel matrices K x and K y , and use the extracted features in any off-the-shelf clustering algorithm such as k -means. For our experiments, we compare our CCA-based approach against a number of base-lines, and show that accounting for the correlations between tags and words helps in extracting better features which lead to improved clustering performance. The K -means algorithm is chosen as the base clustering algorithm for all the approaches considered in the article. Any other vector-space clustering algorithm can also be used however. Since K -means is sensitive to initialization, we repeated each experiment 20 times and have reported the average scores with standard deviations. Section 5.2 describes the experiments with the fully tagged corpus and Section 5.3 describes the experiments with the partially tagged corpus.
 Our dataset consists of a collection of 2000 tagged Web pages that we use for our Web page clustering task. All Web pages in our collection were downloaded from URLs that are present in both the Open Directory Project (ODP) Web directory (so that their ground-truth clustering are available) and Delicious social bookmarking Web site (so that their tag information is available). The Delicious dataset of tags is available at http://kmi.tugraz.at/staff/markus/datasets/ .

Each Web page that we crawled and downloaded was tagged by a number of users on Delicious. Therefore, for each Web page, we combine the tags assigned to it by all users who tagged that Web page.

After stemming and stop-word removal, we had a page text vocabulary of 70168 unique words and a tag vocabulary (set of all unique tags) of 4328 unique tags. These are essentially the sizes for the page-text based and tag based feature vectors respec-tively. We used the bag-of-words representation for the feature vectors. Our approach can however also be applied with other feature representations such as the term-frequency/inverse-document-frequency (TF/IDF). Our first set of experiments are with a fully tagged corpus. To assess the efficacy of the inclusion of tag information for Web page clustering, we compare the following approaches in our experiments. (1) Word feature vector only. For this, we only consider the words appearing in the (2) Tag feature vector only. For this, we only consider the tags associated with each (3) Word feature vector + Tag feature vector. For this, we created an augmented feature (4) Kernel PCA on words + tags feature vector. For this, we apply Kernel PCA on the (5) Kernel CCA on words and tags feature vectors. For this, we treat features derived In addition, we also experimented with Kernel PCA separately on word features and tag features, and found the performance in both cases to be lower than Kernel PCA on the joint vector. Therefore we skip those results from the presentation, and only report the results of Kernel PCA on the joint words + tags vector.

In our experiments with Kernel PCA and Kernel CCA, we have used linear, poly-nomial, and Gaussian (RBF) kernels. The hyperparameter for Gaussian kernel (the kernel width parameter) is set to the median pair-wise distance between examples. We note that it is also possible to learn a suitable kernel from the data [Weinberger et al. 2004] but that is not our focus in this article.

We performed experiments both with full data available, and also with varying amount of data. In particular, the latter experiment was conducted to assess the per-formance of various approaches when the number of Web pages is small but the feature vector associated with each Web page is high dimensional. The number of projection directions for PCA and CCA are kept sufficiently large; as the feature vector size is much larger than the number of Web pages, we simply set the number of projection directions equal to the number of Web pages so that it is reasonably large. 5.2.1. Full Data. In our first experiment, we run all the algorithms on the entire collection of the tagged Web pages. Our results on the full data are shown in Table I. As the results in the table indicate, inclusion of tag information in any form seems to improve the performance as compared to the case when only words from page-text are used. This is evidenced by the better results of words + tags as compared to words only and tags only (which has also been shown in some other recent works [Lu et al. 2009; Ramage et al. 2009]).

Among the kernel-based approaches, Kernel PCA on words + tags performs mostly comparably with raw words + tags (although it did better for the Polynomial Kernel case). Finally, we observe that the Kernel CCA-based approach does best overall, sug-gesting that taking into account the correlations between tags and words indeed leads to an improved performance. Among the kernel based approaches, the polynomial kernel (with degree 2) performed the best in all cases. 5.2.2. Varying Data Amount. In our second experiment, we looked at how the various approaches perform when the number of Web pages is small. For this experiment, we gradually vary the number of Web pages from 100 to 600 and monitor the F-scores reported by all the approaches. The results are shown in Figure 3.

As we can see in Figure 3 (left) that words only, tags only, and words + tags based approaches perform poorly when the number of Web pages is small. Also, notice that words + tag performs worse than words only when the number of Web pages is very small, possibly due to poor parameter estimation for high dimensional yet small sam-ple size. The words + tags based approach does however begin to outperform the words only and tags only approaches as the number of Web pages increases. On the other hand, we observe that both PCA-and CCA-based approaches consistently perform better than the other 3 baselines, with CCA the best overall.
 Figure 3 (right) compares both kernel based feature extraction approaches; Kernel PCA and Kernel CCA for 2 choices of kernels, polynomial and Gaussian. Compared with the linear feature extraction (Figure 3, top), we see that the kernel based approaches yield better F-scores, with the Kernel CCA better than Kernel PCA. The better performance of Kernel CCA over Kernel PCA can be attributed to the fact that although Kernel PCA performs a joint projection of words + tags feature vector, it maximizes the variances of the word feature vector and the tag feature vector individually. On the other hand, the Kernel CCA-based approach maximizes their correlations, resulting in the better performance. To simulate the partially tagged corpus setting, we provide our algorithm the tag fea-tures for only a small fraction of Web pages in the corpus. For the remaining Web pages, we only use the page-text based features. We call Web pages with both page-text and tag information available as paired , and Web pages with only page-text infor-mation available as nonpaired . In our experiment, we vary the fraction of paired Web pages from 10% to 60%.

We compare our kernel-completion-followed-by-KCCA based approach with two baselines. Our first baseline is KCCA with full view information, that is, all the Web pages are paired with their corresponding tag information. Our second base is an incomplete view setting like ours: KCCA on paired Web pages but Kernel PCA on nonpaired Web pages (since only a single view, page-text, is available for nonpaired Web pages). The k -means algorithm was used as the base clustering algorithm in our approach as well as the other baselines. However, any other clustering algorithm can be used as well. Since k -means might be sensitive to initializations, we run it 20 times and report the mean and standard deviations. Gaussian kernel was used for all the kernel computations and the width parameter was set to the median pairwise distance between the examples. For the evaluation of clustering performance, we used the av-erage cluster entropy , which is based on the impurity of a cluster given the true classes i ,and N be the total number of examples, then the average cluster entropy is defined The performance of our approach and the other baselines is shown in Figure 4 (left). As we can see, our approach with incomplete view with just about 50% to 60% paired Web pages achieves comparable results to the fully paired KCCA case. With fractions higher than that, we observed the performance to wiggle around and stay roughly the same (moderately better or moderately worse) to the fully paired case. On the other hand, it significantly outperforms the other baseline that uses KCCA on the paired Web pages and kernel PCA on the nonpaired Web pages. The inferior performance of the KCCA+KPCA approach can be attributed to the fact that only a small subset of the Web pages use the tag information for the low-dimensional projection.

We also experimented to see how good is the reconstructed tag view kernel matrix with respected to the ground truth kernel matrix of tag features. To do this, we vary the fraction of paired examples as did in the previous experiment and plot the align-ment of matrices in both views. As we can see from Figure 4 (right), the alignment gets better as the fraction of paired examples increases, and with about 55 X 60% paired ex-amples, the alignment is almost as high as the alignment obtained on the ground truth kernel matrix.

Note that merely having a high alignment between K x and K y does not ensure that the multiview clustering performance will be good. If the number of examples in view Y is very small, then the optimization could give a kernel matrix K y that may be very similar to K x and it may not give any useful information from view Y . Therefore, one needs a sufficient number of examples from view Y so that the obtained kernel matrix K y actually is a good representative of the similarities between examples in view shown in our experiments, about 50% to 60% tagged Web pages gave close to optimal performance. Another thing to note here is that the reconstructed kernel matrix K y in our approach depends on the kernel K cc y constructed using tagged set of Web pages so the reconstruction accuracy (and hence the clustering performance) depends on how expect the reconstructed K y to be sufficiently close to the optimal kernel matrix in view Y . 5.3.1. A Tag-Prediction Based Baseline for Partially Tagged Corpus. Another way to deal with the case when the tags are available for only a small number of Web pages is to use the tagged Web pages for predicting the tags for the rest of them (akin to the framework proposed by Hardoon et al. [2006] which automatically annotates images using annotations for similar images). Under this approach, one can perform a latent semantic analysis or CCA to discover a semantic subspace of Web pages having tag information available. After that, each nontagged Web page can be projected onto this subspace and can be assigned the same tags as that of the tagged Web page closest to it in the semantic subspace. We note here that although the similarities among documents can be compared in the original feature space, a closeness measure in the semantic subspace is a better measure of similarity between two documents, because we would be measuring thematic similarities in this subspace. Once we do this for all nontagged Web pages, we will have full information (i.e., tags with page-text for all Web pages) to apply the CCA-based approach we proposed in this article. In our exper-iments, we found that this baseline did roughly similar to the KCCA+KPCA baseline. A number of techniques have been proposed in the past to improve information re-trieval tasks using auxiliary sources of information, for instance, anchor text for Web search [Eiron and McCurley 2003], interconnectivity of Web pages [Cohn and Hofmann 2001], captions for image retrieval [Blei and Jordan 2003], etc. Other recent works on exploiting social annotations, in particular, to improve various Web mining tasks include annotation based approaches to Web search [Bao et al. 2007], Web page classi-fication [Zubiaga et al. 2009], and information retrieval in general [Zhou et al. 2008]. Similar in spirit to our work, using tag information for Web page clustering has ear-lier been proposed in Ramage et al. [2009] and Lu et al. [2009] using a concatenation of word and tag feature vectors. Ramage et al. [2009] also proposed a probabilistic generative model based on an extension of the Latent Dirichlet Allocation [Blei et al. 2003]. Their model is essentially the same as the conditionally independent LDA (CI-LDA) which assumes separate sets of topics for words and tags. This assump-tion tends to loosen the coupling/correlations between the word topics and the tag topics [Newman et al. 2006]. Another issue is that exact inference in such models is intractable, and therefore approximations are needed, which require using Markov Chain Monte Carlo, or variational methods. In contrast, our CCA based approach re-duces to solving an eigenvalue problem that can be solved efficiently using existing eigensolvers. Another benefit of using the kernel variant of CCA we use in this article is that the complexity of solving the eigenvalue problem depends on the number of Web pages rather than the vocabulary size which would be especially advantageous when the number of Web pages is small as compared to the vocabulary size.

Among other works that use CCA, Chaudhuri et al. [2009] used the CCA-based approach for audio-visual speaker clustering and hierarchical Wikipedia document clustering by category, and showed that CCA-based approach outperforms PCA based clustering approaches. In another work, Blaschko and Lampert [2008] use CCA for clustering images using the associated text as a second view. Both of these works assume that the views are complete, unlike the setting we considered in this article. There are a number of possible extensions of our work. One direction in the partially tagged corpus setting would be to identify which Web pages one should get tags for so as to have the best performance on the learning task at hand (e.g., clustering with par-tially tagged corpus). Active Learning [Settles 2009] could be useful in such a setting. Here we briefly describe an active learning based approach to accomplish this. In the partially tagged case, we have a set of tagged Web pages and rest of the Web pages are untagged. If there is a budget on the set of tagged Web pages, one should get those Web pages tagged that are the most informative about the rest of the corpus (especially about the Web pages that are not tagged). The partially tagged corpus setting is like a transduction/semisupervised learning setting where we want to learn with both labeled and unlabeled data. Let us denote by T the binary matrix indicating with T ij = 1 if a tagged Web page i has been assigned the tag j . Given T ,justasin Section 4 where we compute A m , we can use the following equation to predict the tags for the untagged Web pages (assuming that the tags for the untagged Web pages come from the same tag vocabulary): where L x is the graph Laplacian of the data using the page-text view (Section 4). Using the tag predictions U on the untagged Web pages, we can compute the estimated risk (expected tag prediction error on the untagged Web pages) on the untagged Web pages as is done in Zhu et al. [2003]. In our active learning extension, as we choose a new Web page to tag, a new row is added to the matrix T , and we get a new estimate U for the tag predictions of the untagged Web pages. As suggested in Zhu et al. [2003], the chosen Web page should be such that it minimizes the estimated risk on the untagged set of Web pages. This can be our criteria at each step to select Web pages that we should get tags for. We leave the further details for future work. In this article, we used a kernel matrix completion approach. There exist a number of other possibilities which are worth investigating in our setting. We describe some of these here briefly. The first approach (Section 7.2.1) uses a semisupervised version of CCA which can extract features using both tagged and nontagged Web pages, or can use a combination of CCA and LSA on the tagged and nontagged Web pages re-spectively. The second approach (Section 7.2.2) is based on first predicting the tags for nontagged Web pages using any of the several methods described, and then applying the Kernel CCA based clustering approach we have proposed in this article. 7.2.1. Semisupervised Projections. It is possible to apply the CCA-based approach in a semi-supervised fashion using both tagged and nontagged Web pages. For exam-ple, one can take a probabilistic approach to CCA [Rai and Daum  X  e III 2009] and treat the missing tags for nontagged Web pages as latent variables. In the nonprobabilistic setting, one can use the semisupervised variants of CCA [Blaschko et al. 2008; Kim and Pavlovic 2009] which do not require full information from both the views. Alter-natively, a somewhat similar way of accomplishing this would be to write a combined eigenvalue problem with one part of it being CCA on the tagged Web pages, and the other being LSA on the nontagged Web pages. 7.2.2. Predicting Tags for Nontagged Web Pages. A number of approaches have also been proposed in the recent past that autopredict tags [Brooks and Montanez 2006] and such approaches can be also used for predicting tags for nontagged Web pages. Another rather na  X   X ve option could be to use the tagged corpus of Web pages to train several pre-diction models, one for predicting each tag, and then use these models to predict the tags for nontagged Web pages. A problem with such an approach is the large number of tags which leads to scalability issues. Furthermore, tags can potentially come from an open-vocabulary and be sparse [Law et al. 2010]. Another issue could be synonymy where two different tags may have the same meaning. To address these issues in the context of music clip tag prediction, Law et al. [2010] proposed a framework that orga-nizes tags into semantically meaningful classes using topic models, and then predicts these classes given a nontagged piece of music. Such an approach can be useful for Web page tag prediction as well. Finally, not all tags are meaningful for a given Web page. Some spurious tags can ham-per the discriminative power of the more relevant ones. One can filter such spurious tags before using them [Suchanek et al. 2008]. This roughly amounts to doing fea-ture selection but here the feature selection for tags can benefit from the other sources of information (such as how many users applied a particular tag to some document). Incorporating such information can lead to identifying the tags that are most discrim-inative, and hence is expected to lead to even better performance. User generated content can be a very rich source of useful information for Web mining and information retrieval on the Web. Intelligent ways of harnessing this rich source of information can greatly benefit the existing Web-mining algorithms. Often the use-fulness of user-generated content is because it is small but structured (e.g., tags), in addition to being semantically precise, which can nicely complement the huge but un-structured information (e.g., page-text). As we have seen in this article, tag informa-tion can be exploited in numerous ways to improve Web page clustering, both when tags for available for all Web pages as well as in the case when the tag information is available only for a small subset of Web pages. Although we have presented results for Web page clustering, due to the discriminative information provided by the tags, the features extracted by our CCA-based approach can also be useful for Web page classi-fication. In this article we have considered the case when tags are the auxiliary source of information; the proposed approaches can also be useful for harnessing the benefits of other type of metadata generated by users on the Web.

Finally, future work will also investigate how considering metadata such as tags associated with document can help in domains other than the Web. For example, in Medical Informatics, clustering patient records can be a difficult problem since these records often tend to be highly unstructured and noisy. However, often these records are marked with very specific tags which can be exploited in a manner similar to what we have presented in this article. Also, the multiview clustering with incomplete views has natural applications in clustering with multilingual data (for cross-lingual information retrieval). Since machine translation is a hard problem and can also be error-prone, one could consider obtaining good translations for a small fraction of the documents in the corpus and then apply our multiview clustering algorithm with in-complete views.

