 650 Harry Rd, San Jose, CA 95120 decision in hindsight.
 exponential size in the problem parameters) and the cost functions are non-linear. are the celebrated polynomial time algorithms for submodular function minimization [7]. P ( S ) = is easily seen to be submodular as well.
 subsets.
 depends only polynomially on n ? problems, with regret which is bounded by a polynomial in n  X  the underlying dimension  X  and domized algorithms with expected regret O ( n analysis. We make crucial use of a continuous extension of a submodular function known as the style of Zinkevich [15].
 characteristic vector in { 0 , 1 } n , i.e.  X  S ( i ) = 1 if i  X  S , and 0 otherwise. elements i  X  E , we have an oracle that returns the value of f at any given set S  X  [ n ] .
 of Iwata [6] and Iwata-Orlin [8] which runs in time  X  O ( n 4 EO + n 5 ) . Online Submodular Minimization. In the Online Submodular Minimization problem, over a S decision maker is defined to be randomness in the algorithm.
 settings of the problem: randomized algorithm that attains the following regret bound: Furthermore, Regret T  X  O (( n + ized algorithm that attains the following regret bound: Furthermore, Regret T  X  O ( nT 2 / 3 Both of the theorems above hold against both oblivious as well as adaptive adversaries. of [ n ] : Now let x  X  K . There is a unique chain A 0  X  A 2  X   X  X  X  A p such that x can be expressed as a convex combination x = other words, we have x = E  X  [  X  S Pr simply by sorting the coordinates of x .
 Now, we are ready to define 1 the Lov  X  asz extension  X  f : combination x = extension  X  f at x is defined to be We will also need the notion of a maximal chain associated to a point x  X  K in order to define subgradients of the Lov  X  asz extension: Definition 3. Let x  X  X  , and let A 0  X  A 2  X  X  X  X  A p be the unique chain such that x = where  X  i &gt; 0 and in the B j chain.
 chapter IV.
 Proposition 3. The following properties of the Lov  X  asz extension  X  f : K X  R hold: which attain the same regret bound of O ( n an analytical algorithm based on (sub)gradient descent on the Lov  X  asz extension. 3.1 A Combinatorial Algorithm the full information Online Submodular Minimization setting: Algorithm 1 Submodular Follow-The-Perturbed-Leader 1: Input: parameter  X  &gt; 0 . 3: for t = 1 to T do 5: end for Define  X  t : 2 [ n ]  X  R as  X  t ( S ) =  X  is poly-time solvable given oracle access to  X  t .
 regret bound of Theorem 1: Theorem 4. Algorithm 1 run with parameter  X  = 1 / proved in Theorem 1.1 of [9], which bounds the regret as follows: regardless of when R is chosen.
 To bound this, we need the following lemma: Lemma 5.
 Proof. First, we note the following simple union bound: randomness in choosing r j for all j 6 = i . Define R 0 : 2 [ n ]  X  R as R 0 ( S ) = ( S ) =  X  t ( S ) . Let ( A ) , we have j 6 = i , and conclude that of i yields the required bound on Pr [ S t 6 = S t +1 ] . Continuing the proof, we have The last inequality follows from Lemma 5. Now, we have R ( S  X  )  X  R ( S 1 )  X  2 n/ X  , and so since  X  = 1 / 3.2 An Analytical Algorithm R n  X  X  defined by x =  X  K ( y ) is defined by Algorithm 2 Submodular Subgradient Descent 1: Input: parameter  X  &gt; 0 . Let x 1  X  X  be an arbitrary initial point. 2: for t = 1 to T do 4: Find a maximal chain associated with x t ,  X  = B 0  X  B 1  X  B 2  X   X  X  X  B n = [ n ] , and use 5: Update: set x t +1 =  X  K ( x t  X   X g t ) . 6: end for Zinkevich X  X  analysis of Online Gradient Descent to vector-valued random variables whose expec-this section, but it will be useful in the next section): Then the expected regret of playing x 1 , x 2 , . . . , x T is bounded by Since this Lemma follows rather easily from [15], we omit the proof in this extended abstract. We can now prove the following regret bound: Theorem 7. Algorithm 2, run with parameter  X  = 1 / Furthermore, with probability at least 1  X   X  , Regret T  X  (3 n + on the regret. Here, we use the bound k  X  g t k 2 = k g t k 2  X  4 n . E [ Regret T ] = Since  X  = 1 / also get that with probability at least 1  X   X  , which implies the high probability regret bound. We now present an algorithm for the Bandit Online Submodular Minimization problem. The algo-algorithm).
 Algorithm 3 Bandit Submodular Subgradient Descent 1: Input: parameters  X ,  X  &gt; 0 . Let x 1  X  X  be arbitrary. 2: for t = 1 to T do 3: Find a maximal chain associated with x t ,  X  = B 0  X  B 1  X  B 2  X   X  X  X  B n = [ n ] , and let 4: Choose the set S t as follows: 5: If S t = B 0 , then set  X  g t =  X  1  X  6: Update: set x t +1 =  X  K ( x t  X   X   X  g t ) . 7: end for X variance) of X t conditioned on all the randomness chosen by the algorithm until round t . actual loss functions. Lemma 8. For all t , we have E [ f ( S t )]  X  E [  X  f t ( x t )] + 2  X . Proof. From Definition 2 we have that  X  f ( x t ) = P i  X  i f ( B i ) , and hence: randomness chosen in the first t  X  1 rounds.
 Furthermore, we can bound the norm of this estimator as follows: We can now remove the conditioning, and conclude that E [ k  X  g t k 2 ]  X  16 n 2  X  . Theorem 9. Algorithm 3, run with parameters  X  = n bound: Proof. We bound the expected regret as follows: The bound is now obtained for  X  = n 4.1 High probability bounds on the regret gives high probability bounds against an adaptive adversary.
 Theorem 10. With probability 1  X  4  X  , Algorithm 3, run with parameters  X  = n achieves the following regret bound: The proof of this theorem is deferred to the full version of this paper. possible to attain O ( [1] A. D. Flaxman, A. T. Kalai, and H. B. McMahan, Online convex optimization in the bandit [2] Satoru Fujishige, Submodular functions and optimization , Elsevier, 2005. [8] Satoru Iwata and James B. Orlin, A simple combinatorial algorithm for submodular function [12] James B. Orlin, A faster strongly polynomial time algorithm for submodular function mini-
