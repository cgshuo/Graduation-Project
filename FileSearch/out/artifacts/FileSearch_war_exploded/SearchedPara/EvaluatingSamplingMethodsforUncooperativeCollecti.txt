 Many server selection metho ds suitable for distribut ed infor-mation retrieval applications rely, in the absence of cooper-ation, on the availability of unbiased sampl es of documents from the constituen t collections. We describ e a number of sampl ing methods which depend only on the norm al query-response mechanism of the applicable searc h facilities. We evaluate these methods on a number of collections typical of a personal metasearc h appli cation. Results demonstrate that biases exist for all methods, parti cularly toward longer documen ts, and that in some cases these biases can be re-duced but not eliminated by choice of paramet ers.
We also introduce a new sampling technique, \multiple queries", which produces samples of similar quality to the best current technique s but with signi can tly reduced cost. H.3.4 [ Informa tion Storage and Retrieval ]: Systems and Software| perform anc e evaluation Experimentation, Meas urem ent Distri buted informat ion retrieval, random sampl ing
Simultaneous search of documents from several indep en-dent collections presents challenges. In many cases, it is not possible to create a single central index: access restri c-tions, privacy concerns, or technical obstacl es may mean the only interface to a collection is through a searc h engine. Distri buted inform ation retrieval (DIR) 1 aims to provide a
DIR is also referred to as \meta search" or \federated search".
 Cop yright 2007 ACM 978-1-59593-597-7/07/0007 ... $ 5.00. uni ed search service spanni ng the combined collections of these indep enden t searc h engines. Figure 1 illustrates one application: a personal DIR tool capabl e of searching all a user's digital collections including email, calendars , and corporate database s.

Besides providing a singl e entry point to multiple collec-tions, and therefore enabling greater coverage than any sin-gle search system, such a tool would have several advan-tages. It can scale to large sizes with low cost, and using local search engines to process queries for their own col-lections allows collection-sp eci c optim isations such as the-sauri. Further, there may be signi ca nt advantages to users including a need to learn only one search interface, reduced cogni tive load, and a reduced chanc e of certain errors.
In the most general case, we assume that search engines do not cooperate with any DIR framework and provide only the most minimal interface: they simply accept a query and produce a set of documen t identi ers as a result . In par-ticular, they do not make available statistics such as the number of documents indexed, term frequenci es, or weights; nor do they expose any informat ion on their internal work-ings. This informa tion is however neede d for funda mental DIR tasks such as estimating overlap between collections [3], selecting a collection to answer a query [11], and merging result sets from multiple collections [6], and there fore tech-niques have been developed for estimati ng collection statis-tics based on sampl es of documents from consti tuent servers.
Thes e estimation technique s are impro ved by or explicitly rely upon random (unbiased) samples. Overlap estimates [3], as well as the standa rd capture-reca pture [9] and sample-resample [14] technique s for estimating collection sizes re-quire random samples as input ; biased sampl es lead to sys-tematic error in predicting overlap and systematic undere sti-mates of collection size. The multiple capture-recapt ure and capture history techniques [13] also assume random samples, although estimates from these techniques are adjus ted to account for sample quality, as does the random documents method [4].

Early evidence also sugges ts that biased sampl es, and hence biased term statistics and size estimates, have a neg-ative impact on standard server selection algorithms includ-ing CORI [5], ReDDE [14], and KL-divergence [15].
Obtai ning a random sampl e from an uncooperativ e searc h engine is a non-trivial task. With a limited interface, it is not possible to enumerate documents in the collection or to retrieve them according to some identi er; therefore we can-not simply take a uniform sampl e. Further, charact eristic s of the search engine itself which may improve performance for typical uses are likely to make sampl ing less convenien t. For example, certai n documen ts may be more likely to be re-turned since they are considered in some sense more impor-tant, or more useful; this introduces strong bias. Similarly, result lists may be arbitarily trunca ted, or near-dupl icates remo ved, to save work on the server or to presen t a more useful list to a client. Any random sampl ing technique will have to work in this environm ent.

Ideall y, for DIR we would like a sampling technique which produces samples with as little bias as possibl e; which re-quires as little run-time or pre-pro cessing resources as pos-sible; and which works over a wide range of collections and search engines with as little prior knowledge as possible. In parti cular, we do not want to rely on parti culars of docu-ment format such as the availability of hyperlinks.
In this work we consider sampl ing techniques against these criteria. A number of candi date techniques have been de-veloped for sampl ing from the Web, some of which are more generall y appli cable, and we consider the runtime cost and perform ance of these. To the best of our knowledge this is the rst statistical evaluation of \random" document sam-ples from these techniques . We also consider a new metho d, \multiple queries", which provides sampl es of similar qual-ity to existing metho ds with much reduced cost. Finally, we consider the e ects of the parameters available in each technique.
In the discus sion which follows we use notation following that of Bar-Y ossef and Gurevich [2]: D is the set of all doc-umen ts available through a search engine, d an individual documen t from D , and N = jDj the number of documents a search engine provides access to. x repres ents the mean of some value x .

For each query q sent to a search engine, res ( q ) denot es the results returned. This result set may be constrained by a limit k , imposed either by the search engine itself or by our samplers ; if j res ( q ) j k we say that q \over ows", and if j res ( q ) j = 0 we say q \under o ws" 2 .
Bar-Yossef and Gurevich refer to over ow if j res ( q ) j &gt; k .
The problem of sampling from an unco operative collection is very similar to that of sampling from the Web, and several algorit hms have been introduced for the latter. These are summ arised in Table 1. \General " methods do not rely on link structures between documen ts, and are applicabl e across a wide variety of doc-umen t types.
 Bharat and Broder [3] introduced a simple technique for sampl ing random pages from Web search engines. The tech-nique does not however rely on any parti cular characteri stic of the Web or of Web pages and is applicable to a variety of collections. The algorit hm is extremely simple: a single query is construct ed and sent to a searc h engine, and a sam-ple documen t is chosen at random from the set of matches returned.

The appli cation to Web searc h engines, which typically return a smal l number of results regardless of the number of possible matches, prevents us from deali ng with large re-sult sets. As a work-around, Bharat and Broder generated queries which they expected to return between one and 100 documen ts, although if a query matched more than 100 doc-umen ts they used only the top 100 returned. Query term s came from a lexicon built during an earlier crawl, and queries were either four-term disjunct s or two-term conjuncts with term s chosen for their frequency . (In a later paper [7], Gulli and Signorini used this method with slight modi cati ons to estimate the size of publi c search engines and hence the publi c Web. Query terms again came from an earlier crawl, but Gulli and Signorini used single-term queries in more than 75 languages.)
Although working with an unco ntrolled, dynamic corpus mean t Bharat and Broder were not able to investigate the quali ty of the sampl er, they identi ed six sources of bias. Two of these are relev ant to the DIR case. \Query bias" is the bias towards longer, content-rich documen ts which are more likely to match the queries used. \Ranking bias" is the result of searc h engines ranking documen ts and a sampler not seeing those past rank k . By choosing queries with a smal ler number of result s, they note that it is possible to eliminate ranking bias at the expense of increasing query bias. Query bias has been noted in other work [1, 2] and is con rm ed by our results below.

In our implem entation of the singl e queries sampler, we are able to eliminate ranking bias by ignoring any queries that unde r ow or over ow and only choosing from result sets where 1 j res ( q ) j &lt; k .
 Reject ion sampl ing is a Monte Carlo method which can be used to simulate sampl ing accordi ng to one distribut ion (for exampl e, the uniform distri bution) when it is only fea-sible to draw samples with some other distri bution p (such as that resulti ng from query or ranking biases). Bar-Y ossef We prefer the formulati on here as, without extra inform a-tion from a search engine, it is not general ly possible to tell whether a result set is bounded by k or by the number of matches. The two de nitions are interchangabl e by incre-menting or decrementing k .
 hyperlink graph and are examined in our experimen ts. and Gurevic h introduced an application of this technique to the probl em of sampling Web pages [2]. The algorithm con-sists of an inner round of rejecti on sampl ing, which chooses a query accordi ng to the size of its result set; and an outer round, which chooses a documen t from the result set of this query.

Pool-based sampling requires as input a \query pool" P , a set of queries drawn from all possibl e queries a search en-gine accepts . Ideall y queries in P have high recall (meaning that taken together, they cover a large prop ortion of the documen ts in D ), and simultaneously a low probabi lity of under o w or over ow. We use P + to denot e the subset of P which neither under o ws nor over ows.

We also use ma tch P ( d ) to denot e the queries which a documen t d matches, from amongst a pool of queries P . ma tch P ( d ) can be calculat ed from the text of a documen t | for exampl e, if P is the set of all 3-gram queries, it suc es to extract text from d and enumerate all 3-term phrases in the document.

The inner loop uses rejecti on sampling to choose a query q from P + with a distri butio n based on j res ( q ) j . A query is chosen uniforml y from the pool and forwarded to the search engine; if it neither under o ws nor over ows we accept it with probabi lity j res ( q ) j =k and return to the outer loop
In the outer loop, a candi date document is chosen uni-form ly from res ( q ). At this point the probabi lity of choos-ing a document d as a candidat e is prop ortional to the probabi lity that d matches the query ; in other words to j ma tch P + ( d ) j . A second round of rejecti on sampling there-fore returns d as a sampl e with proba bility 1 = j ma tch and otherwise iterates select ing another query and docu-ment. (Note that although documents are presen ted as can-didates with a distributi on based on j ma tch P + ( d ) j , they are selected as sampl es based on 1 = j ma tch P ( d ) j . This intro-duces an error the size of which depends on the di erence between P and P +, which is hard to determine without detai led knowledge of how queries and documents are pro-cessed at the search engine.)
Experiments to charac terise possible pools P used text from a crawl of web pages in the Open Direct ory Project (ODP ) [10]. Bar-Yossef and Gurevich used a crawl of a sub-
The original descript ion of the algorithm leaves the pa-rameters C and ^ ( q ), the envelope constan t and the unnor-malised sampl e distribut ion, unsp eci ed. In our implemen-tation we let ( q ) be uniform, use an unnormal ised version ^ ( q ) = 1 for all q , and set C = k= ^ ( q ) = k . set of the ODP as a source of terms and considered single term s, 3-, 5-, and 7-term phrases for recall, under o w, over-ow, and other prop erties; 5-term phras es were considered the best tradeo in this instance, and we use 5-term phrases in our initial experiments below.
 A further variant on sampling through random walks was in-troduced at the same time as pool-based sampl ing [2]. This varian t uses the Metrop olis-Hasti ngs algorithm , which car-ries out a random walk with each step chosen accordi ng to a \prop osal functi on", and before each step empl oys an ac-ceptanc e/reject ion procedure to determine whether or not the step will be taken. If the prop osal funct ion satis es certai n simple criteria, a walk with the Metrop olis-Hasti ngs algorit hm will eventually converge on a desired distribution. The param eter B , the burn-i n period, determines the num-ber of (possible) steps in the walk: as with other random walk methods, this can only be set empi rically without prior knowledge of the collection.

Bar-Y ossef and Gurevich adapt this algorithm to collec-tions witho ut a hyperlink struct ure by de ning a graph such that two documents are joined by an edge i they both match at least one query: i.e. an edge exists between two documen ts x and y i ma tch P + ( x ) \ ma tch P + ( y ) 6 = ; . Given a document d , the sampler then proceeds by choos-ing a query uniform ly from ma tch P + ( d ) (whic h determ ines a subset of edges from the curren t documen t); choosing a documen t d 0 unifo rmly from the results of this query (whic h determi nes an individual edge); and then choosing to follow norm alises for the number of queries d 0 matches). After B iteratio ns, the curre nt document is returned as the sampl e.
Evaluation experiments compared this rando m walk with the pool based and single query sampl ers on a testbed of 2.4 million documen ts from the ODP. With both the pool based and random walk samplers , \little or no" bias was seen due to documen t size, and no \signi can t bias" due to the static documen t rank used by their search sytem. As with other methods, to the best of our knowledge no quantitative tests for bias have been performed.

Unlike other variants on random walks, a random walk on ma tch P + does not require an explicit hyperlink struc-ture and is appropriat e to a wide range of collections. We therefore include this sampl er in our experiments below.
A further set of random -walk sampling metho ds assume documen ts are linked in a graph, such as a web graph. No-table among these are the pagera nk-s ample method of Henzi nger et al. [8], the WebWalker algorit hm of Bar-Y ossef et al. [1] and the directed-sample and un dir ected-sam-ple methods of Rusmev ichientong et al. [12].

Thes e methods are not generally applicable in the DIR case, since not all constit uent corpora will have such a struc-ture. Many componen t collections typical of personal DIR, such as email, databases, calendars and catalogues are not hyperlinked. According ly, we have excluded these metho ds from our experiments.
The multiple queries sampl er is a straightforward exten-sion of the single queries sampler. To reduce query bias, we run several queries with a large cuto k ; we then choose any number of documents from the union of all result sets. (Note that if a document is returned in reponse to more than one query, we record it only once. This has a similar e ect to adjusti ng for visit freque ncy in pagera nk-sample .) We choose queries from a pool de ned indep enden tly of the col-lection, with as high a recal l as possible, and as with other sampl ers ignore any which under-or over ow.

The high values of k (10,000 in our initial experiments) sugges t a large amoun t of network trac; however since we do not need to download the text of documen ts, and can sampl e many documents in a single run, this trac does not seem excessive. Further, although we rely on searc h engines providing large result sets we do not rely on them making documen t text available. Experiments varying k and queries used are described in Section 6.2 below.
A desideratum for a working DIR system is that sampl ers should require as few resources as possible. The most sig-ni cant resource is comm unicat ion with the searc h engines we are sampling, and in this section we consider the cost for each document sampled in the number of queries issued and in the number of documen ts downloaded and parsed.
The pagera nk-s ample , WebWalker, and (un )di rected-sample algorit hms rely on explicit hyperlinks between doc-umen ts and are not applicabl e to most collections likely to be used in DIR. In the remai nder of this paper we there fore consider only the single queries, pool based, random walk, and multiple queries sampl ers.

For each of the following analyses a rst step is to deter-mine how many queries success fully compl ete (i.e. neither under o w nor over ow). Following Bar-Yossef and Gurevich we refer to the \validity density" of a documen t, vd ensity ( d ): of those queries d matches which return 1 : : : k 1 results. Similarly, we use vd ensity ( P ) to represen t the prop ortion of queries in a pool P which neither under o w nor over ow: vd ensity ( P ) = jP + j = jPj .
In its original form, the cost of the singl e queries sampler is a single query per document sampled for any collection and any source of queries . In the implementation used here, we rst must nd a query which neither under o ws nor over-ows; the number of queries we will issue is geom etrically distri buted with p 1 = vd ensity ( P ), so we expect to issue 1 = vd ensity ( P ) queries per documen t sampl ed.
In both implemen tations, there are no documents down-loade d or parsed.
We consider the inner round of rejecti on sampl ing (to nd a query) and the outer round (to nd a documen t) seper-ately.

In the inner round, the number of iterations needed to rst nd a valid query and then select it is geom etrically distri buted with p 1 = j res ( q ) j =k vd ensity ( P ); so the ex-pected number of queries issue d before one is selected is E c = k= j res ( q ) j 1 = vd ensity ( P ) .

In the outer round, having select ed a query a document from the result set is downloaded and considered for the nal sample. The chance of sampl ing a document after each iteratio n is 1 = j ma tch P ( d ) j , so the number of queries selected before a documen t is selected is geomet rically distributed with E s = j ma tch P ( d ) j .

The expected number of queries executed per document sampl ed is therefore
We also expect to downloa d and parse j ma tch P ( d ) j doc-umen ts per sample.
As with the singl e queries sampler above, the expected number of queries we must issue before nding one which neither under o ws nor over ows is 1 = vd ensity ( P ). After each such query is found we download and parse a docu-ment, and we repeat the process B times (recal l that B is the burn-i n time of the random walk). We can there fore expect to need B= vd ensity ( P ) queries and B downloads per documen t sampled, although with more knowledge of the query pool and collection it is possible to collect second and subsequent documen ts more cheaply by continuing the random walk.

Since the cost depends on B , a param eter of our choosing, we can choose to impro ve runtime at the expense of the random ness of our sample.
The cost of the multiple queries sampler is determined by the number of queries per sample s q , the number of documen ts per sampl e s d , and the pool validity density vd ensity ( P ). Since we need s q queries which neither un-der o w nor over ow, from which we select s d documen ts, we expect ( s q =s d ) = (1 = vd ensity ( P )) queries per documen t sampl ed and no downloads.
We have carried out a number of experiments to explore two questions: rst, do the sampl ing techniques described above provide unbiased samples acros s a range of collec-tions? Secondl y, what e ect do the available param eters have on performance?
Collection Docs Range Mean Std dev Topics Table 2: Summary statistics of collectio ns used in our experiments.
Our six collections, summari sed in Table 2, represen t a range of sizes (over three orders of magnit ude), data types, and topic skew which are likely to be characteristi c of per-sonal DIR and personal informa tion managemen t (PIM) ap-plications.

The \calendar" collection contains 1049 documents (ap-pointmen ts) from a calendar appli cation, spanning about two years. Documents are typically sentence fragm ents, only a few term s long, and the terms used in each document have little overlap with others .

The \zsh-l ist" and \procmail" collections repre sent the archives of two public mailing lists, both on narro w techni-cal topics. \Emai l" is a third emai l collection, this time of documen ts from a personal email archive with much broader topics. \WSJ" (from TREC CD 1) collects several years of con-tents of the Wall Stree t Journal, includi ng articles and let-ters. It covers a broad range of topics, and document length varies widely .
 The largest collection, \.GOV" (from the TREC Web Track), is a 1.25M page parti al crawl of Web hosts in the .gov top-level domain (US governm ent agenci es). As with the WSJ collection, documen t size, style, and topics vary considerabl y.
In the rst instance, we have run each sampl er with pa-rameters set as original ly describ ed. For the singl e queries sampl er, samples were taken with k = 100 and with queries from a 1% subset of term s in each collection. (If all queries were exhausted in the course of a long run, two-and nall y three-t erm disjunct s were used.)
The pool-base d sampler was run with a limit k = 5, and a query pool of a 1% subset of 5-grams from each collec-tion (for .GOV, a 0.1% subset was taken as the collection is large). The rando m walk sampler used the same parameters, although we were able to use a pool of all possible 5-gram s since in this instance queries do not need to be enumerated in advance.

The multiple queries sampler used single-term queries, with term s chosen indep endently of the collection from a list of common English words. We used 100 queries per sample and sampl ed twenty documen ts at a time with a result limit k = 10,000.

The estimated cost of each sampl er with these settings is summari sed in Table 3. The random walk sampl er is signi can tly more expensiv e than others; however this cost scales with B , the burn-i n time. We experiment with lower
Collection q d q d q d q d Table 3: Estimated cost in queries (\q") and docu-ment downlo ads (\d"), per document samp led, for each samp ler. burn-i n times in Section 6.2 below. Costs for the pool-based and multiple queries sampler are similar, althoug h the mul-tiple queries sampler generall y requires fewer interacti ons and never requires documen t text.
Two simple tests were used to indicate bias in samples. \T", number of times seen If from a collection of size N we draw i indep enden t samples, each of size n , and each sample is drawn random ly, then the probabil ity of see-ing a documen t t times in the i sampl es follows a binomi al distri bution: Pr(see n t times) = i t ( n N ) t ( N n N ) a 2 test to compa re this expected distribut ion with ob-served frequencies from large numbers of sampl es.
Failure of this test would most likely indicate that some individual documents are being sampled too frequently and others too infrequen tly | in other words that there is a bias towards some subset of the documen ts. \S", size distribution Test \S" considers one likely source of bias. Earlier work has sugges ted that the single query sampler, in particular, strongly favours longer docu-ments. This could be due to two factors: a ranking bias, if search engines prom ote longer documen ts, or a query bias, since longer documen ts are more likely to contain our chosen search terms. The rst factor is controlled in our samplers, but we may still expect to see e ects of the second.
To investiga te, we divided each collection into deciles ac-cordi ng to documen t lengt h, and recorded the prop ortion of documents sampled from each decile. A random sample should have documen ts from all deciles equally repres ented. As for test \T", we use a 2 test ( df = 9) for comparisons. Failure of this test indicates a bias towards documents of parti cular lengths; in practi ce this tends to be towards longer documen ts.
Informed by the results of our initial tests, a series of experiments investigated the e ects of the available param -eters for each sampler. These are described in more detai l in Secti on 6.2 below.
Result s showed sampl ers perform ed well with only some collections, and that all were subject to some degree of query bias.

Altering query pools or other parameters did not provide improvemen ts.
 Table 4: Email samp les for test \T". i = 30 samp les of n = 20 documents each where possible; i = 600 and n = 1 otherwise. N = 24 ; 974 documents.
 Table 5: 2 results ( p values) for test \T". Signi -cant deviatio ns from randomn ess (i.e. proba ble non-uniform samples) are underlined.
Test \T", number of times seen Table 4 summ arises the number of times each document in the emai l collection was returned by each sampler, and the theoreti cal distri -bution described in Secti on 5.3. p is the chanc e of seeing a distri bution this far from the expected if we had a truly uniform sample ( 2 test, df = 2 with t 2 coun ted as one category). We use the emai l collection for illustration; similar trends were seen with the other collections tested.
In this exampl e, the single queries sampler has returned fewer documen ts than we would expect: 434 unique doc-umen ts are returned whereas we would expect 593 from a truly random sample. Further, those documen ts which are sampl ed are returned too frequently: 63 documents are seen twice (we expect only seven), and two documen ts are seen eight times each. Together these observations suggest a bias towards a subset of the collection. (We examine one form of this bias below.)
Table 5 summari ses test \T" across each collection and sampl er. In every case of test \T" failing, the sampler has sampl ed too few documents too freque ntly, suggesti ng a bias towards some part of the collection. This bias is most ex-trem e with the calendar collection, on which every sampler failed: the random walk sampl er returned one documen t 24 times and the pool based sampler returned seven distinct documen ts more than 15 times each.

We surmi se the failures on the calendar collection are due to the unusual combination of document length and docu-ment language. Since documen ts are very short, the graph on ma tch P + used by the rando m walk sampl er is much Figure 2: Email samples by size decil e for test \S". A uniform sample would have 10% of sampled doc-uments in each decile.
 Table 6: 2 results ( p values) for test \S". Signi -cant deviatio ns from randomn ess (i.e. proba ble non-uniform samples) are underlined. spars er and there is less opportuni ty to move from the ini-tial documen t. For other samplers, we would expect to see strong bias towards the small fraction of documents match-ing the query pool since the terms used in each document vary considerably .

Test \S", size distribution Figure 2 illustrates the distri bution of documen ts sampled by each algori thm from the email collection. A truly random sample would include around 10% from each decil e; instead a bias is appare nt in all sampl ers towards larger documents. Table 6 has 2 results ( df = 9) for all collections and samplers .

In the great majority of cases, a failed test is due to a bias towards longer documents. This is likely due to query bias; a longer document is more likely to match any given query, and by ignori ng over owing queries the samplers may be e ecti vely coun teract ing any length normal isation at the search engine.

As may be expected, the single queries sampl er perform s very poorly on this test, failing with every collection with a very large bias towards longer documents. The pool based sampl er also fails in all six cases, which we expect is due to the size of the query pool: a uniform subsample of 1% of 5-grams would cover a small prop ortion of documents and these documents would tend to be longer. If this conjecture is correct, a larger pool (althoug h more dicult to construct) would result in less bias. We explored this possibility by varying query pools in experimen ts describ ed below.
The random walk sampl er fails in several cases again with a bias towards longer documents. It is not clear why this should be the case, since the sampler explicitly controls for the number of queries a documen t matches; however di er-ences in comput ing ma tch P ( d ) at the searc h engine and at our sampler may give rise to a bias of this nature. Since we must agree with an unkno wn search engine with each step of query processing, including tokenisati on, stemm ing and stopping , and parsing queries, errors of this kind are very likely and may explain the observ ed bias.

The larger number of documen ts considered by the muti-ple queries sampl er provides some improvement on the single queries sampl er, and for the zsh-list and procmai l collections suces to overcom e query bias. In other cases a signi can t bias to longer documen ts rema ins, although we note this sampl er perform s no worse than the random walk sampler and with signi can tly lower runtime.
The biases describ ed above may be due in part to the par-ticular parameters chosen for each sampler. For each sam-pler, we varied the available parameters hoping to generate improvements.

Single queries The single queries sampl er proved par-ticularly susceptible to query bias. Since we ignore all queries which over ow, varying k will change the subset of queries used and may a ect the quality of the resulting sample; so may varying the source of queries .

We ran tests \T" and \S" as above for the single queries sampl er with k = 100, 500, and 10,000; and with queries cho-sen from a subsample of term s index ed, from a subsample of 5-grams, and from a collection-independen t set of common Engli sh words. In each case the sampl er performed poorly, again returning too few unique documen ts and with a strong bias towards longer documents. We conclude that the single queries sampler is unlikely to avoid bias for any combinat ion of parameters.

Pool based In our initial experiments we observ ed a consisten t bias in the pool based sampler towards longer doc-umen ts, and surmi sed that this was due to our smal l query pool (1% of 5-grams for most collections). With a larger pool of 10% of 5-gram s, results impro ved for both tests and most collections, which is consistent with this conject ure.
Whil e a larger pool impro ves result s, we generated pools by pre-processing the collections used. It is not clear how a working system might generate pools except from document text; since any bias in the pool will result in biased samples, documen ts for the pool should be returned by an unbiased sampl er we are faced with a dicul t bootstrapping problem .
Further experiments varied k , using the original pool of 1% of all index ed 5-gram s. We observed improvements over k = 5 with k = 50, and further impro vemen ts with k = 500 across most collections. These sampl ers however have signi can tly increased runtime, as k increases much faster than j res ( q ) j .

Rand om walk The quality of the samples generated by the random walk sampler seem to depend strongl y on B , the burn-i n period of the walk. Our initial experiments used B = 1000, as originally speci ed, and followup experiments used values of B = 100 or 10. With smaller values of B we saw a mark ed increase in bias; this bias was more appar-ent with larger corpora. The choice of seed for the walk is strongl y e ect ed by query bias, so this result can be seen as a straigh tforw ard result of the sampl er taking fewer steps from this initial biased choice. With some knowledge of the size of the corpus and the connect ivity of the ma tch P graph it would be possibl e to choose a minimal useful B , but it is not clear how this could be derived without prior knowledge.
Multiple queries As for the single queries sampl er, the multiple queries sampl er is sensitive to the value of k . Samples were taken with k set to 10,000, 1000, and 100; in general, results were somewhat worse with k = 1000 and deteriora ted markedly with k = 100. This is expected: as k decreas es, only queries which are more speci c will not over ow. This leads to a smaller number of documents from which to take a nal sampl e.

This obser vation may also explain variation seen with dif-feren t query sources. With collection-sp eci c query pools of 1% of term s, the multiple queries sampl er showed more bias. We conclude that for our data | documen ts of varying size and format but all in Engli sh and most with full sen-tences | a list of comm on words is both more convenient and produces better sampl es.
Several key metho ds for DIR rely, explicitly or implic-itly, on an unbiased sample of documen ts from constit uent collections. Techniques for generati ng these samples, given only a query interface and an otherw ise unco operati ve search engine, must contend with biases due to searc h engine op-timisations (\ranking bias") and documen t content (\query bias"). It appears possible to elimina te ranking bias but query bias is persistent across a variety of samplers and col-lections.

We have described seven sampl ers. Of these , four are applicabl e to documen t types without hyperlink structures and were tested across six collections repres enting a range of sizes and document types. No sampl er perform ed well across all collections.

The singl e queries sampler is very badly e ected by query bias, and consisten tly prefers longer documents across all collections tested. The pool based sampl er as initially de-scrib ed perform s poorly, but depends greatly on the choice of pool | with a larger sample of document text the sampl es generated are of higher quality. It is not clear, however, how a real-world system can generate such a query pool without prior knowledge of collection contents.

The random walk sampler performs better than most al-ternati ves, but has a much higher cost as measured in inter-actions with the search engine. It also requires some knowl-edge of how queries are processed at the searc h engine; any errors in assumptions here are re ect ed in biased samples. Our multiple queries sampler can produce sampl es of compa-rable quality, with fewer interact ions and no prior knowledge of the collection being sampled, but requires search engines to support large result sets.

Future work is needed in two areas. It remains to de-velop a sampling technique which is applicable across a range of collections and which requires little prior knowledge of collection contents. Further work also remai ns to quantify the impact impro ved samples have on standard metho ds for DIR. [1] Z. Bar-Y ossef, A. Berg, S. Chien, J. Fackcharoenphol, [2] Z. Bar-Y ossef and M. Gurevich. Random sampling [3] K. Bharat and A. Broder. A technique for measuring [4] A. Broder, M. Fontura, V. Josi vs ki, R. Kumar, [5] J. P. Callan, Z. Lu, and W. B. Croft. Searc hing [6] N. Crasw ell, D. Hawking, and P. Thistlewaite. [7] A. Gulli and A. Signorini . The indexable web is more [8] M. R. Henzinger, A. Heydon, M. Mitz enmacher, and [9] K.-L. Liu, A. Santoso, C. Yu, W. Meng, and [10] Open directory project. http://dmoz.org/ . [11] A. L. Powell, J. C. French, J. Callan, M. Connel l, and [12] P. Rusmevichientong, D. M. Pennock, S. Lawrence, [13] M. Shok ouhi, J. Zobel, F. Scholer, and S. M. M. [14] L. Si and J. Callan. Relev ant documen t distribut ion [15] J. Xu and W. B. Croft. Cluster-based langua ge
