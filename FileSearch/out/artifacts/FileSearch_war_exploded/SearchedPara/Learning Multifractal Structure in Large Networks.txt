 Using random graphs to model networks has a rich history. In this paper, we analyze and improve the multifractal network genera-tors (MFNG) introduced by Palla et al . We provide a new result on the probability of subgraphs existing in graphs generated with MFNG. This allows us to quickly compute moments of an impor-tant set of graph properties, such as the expected number of edges, stars, and cliques for graphs generated using MFNG. Specifically, we show how to compute these moments in time complexity inde-pendent of the size of the graph and the number of recursive levels in the generative model. We leverage this theory to propose a new method of moments algorithm for fitting MFNG to large networks. Empirically, this new approach effectively simulates properties of several social and information networks. In terms of matching sub-graph counts, our method outperforms similar algorithms used with the Stochastic Kronecker Graph model. Furthermore, we present a fast approximation algorithm to generate graph instances follow-ing the multifractal structure. The approximation scheme is an im-provement over previous methods, which ran in time complexity quadratic in the number of vertices. Combined, our method of mo-ments and fast sampling scheme provide the first scalable frame-work for effectively modeling large networks with MFNG.
 H.2.8 [ Database Applications ]: Data mining Algorithms, Theory graph mining; real-world networks; multifractal; method of mo-ments; graph sampling; stochastic kronecker graph; random graphs
Generative random graph models with recursive or hierarchical structure are successful in simulating large-scale networks [5, 15]. The recursive structure produces graphs with heavy-tailed degree distribution and high clustering coefficient. Random samples from recursive models are used to test algorithms, benchmark computer performance [13], anonymize data, and to understand the structure of networks.

A relatively new model is the multifractal network generators (MFNG, [14]). However, there are two issues that are barriers to making MFNG a practical model for large-scale networks. First, results for fitting MFNG models to graphs have been extremely limited. Current procedures can only match a single graph property, such as the number of nodes with degree d . Second, to our knowl-edge, all MFNG sampling techniques are O ( | V | 2 ) algorithms, where V is the vertex set. This makes the generation of large graphs in-feasible.

In this paper, we address both barriers and demonstrate that MFNG can be a better alternative to the more popular stochastic Kronecker graphs. In Section 3, we show how to compute several key prop-erties of MFNG (e.g., expected number of edges, triangles, stars, etc.) with computational complexity independent of | V | and the recursion depth. This result lets us develop an extremely efficient method of moments algorithm to fit networks to MFNG. We test our new method of moments algorithm on synthetic data and large social and information networks. In Section 5, we provide a heuris-tic fast approximate sampling scheme to randomly sample MFNG with complexity O ( | E | log | V | ) , where E is the edge set of the network. In Section 6.1, we show that our algorithm can identify model parameters in synthetic graphs sampled from MFNG, and in Section 6.2, we see that our algorithm can match the number of edges, wedges, triangles, 4 -cliques, 3 -stars, and 4 -stars in large networks. Our contributions are summarized as follows:
Popular recursive and hierarchical models include Stochastic Kro-necker Graphs (SKG, [5]), Block Two-Level Erd  X  os-R X nyi (BTER, [15]), and Random Typing Generator (RTG, [1]). An older, popu-lar model is the recursive matrix (R-MAT, [2]), which is a specific instance of an SKG model with a 2  X  2 generator matrix.

SKG is popular for several reasons including capturing degree distributions, clustering coefficients, and diameter in large networks. There are several methods for fitting SKG parameters to simulate a target network, including maximum likelihood estimation (the KronFit algorithm, [5, 6]) and the method of moments [3]. Max-imum likelihood estimation is also used for the Multiplicative At-tribute Graph model [4], and a simulated method of moments is used for mixed Kronecker product graph models [11, 12]. Finally, SKG produces graph samples in time complexity O ( | E | log( | V | )) rather than O ( | V | 2 ) . On the other hand, SKG is constrained by a rather strong assumption on the relationship between the number of recursion levels and the number of nodes in the graph. Specifically, the number of recursive levels is d log( | V | ) e .

MFNG decouples the relationship between the recursion depth and the number of nodes and also naturally handles graphs where | V | is not a power of two. While there are ad-hoc methods for SKG when | V | is not a power of two, all analyses in the literature make the assumption. We do not assume that | V | is a power of two in our analysis in Section 3. Furthermore, the variable interval lengths in MFNG allow for more flexibility than is offered by the SKG framework. These reasons make MFNG more flexible as a generator for graphs.
MFNG is a recursive generative model based on a generating measure, W k . The measure W k consists of an m -vector of lengths ` with P m i =1 ` i = 1 and a symmetric m  X  m probability matrix P . The subscript k is the number of recursive levels, which we will subsequently explain. In this paper, we refer to the m indices of ` as categories . Also, since the measure is completely characterized by P , ` , and k , we write W k ( P ,` ) to explicitly describe the full measure.
 An undirected graph G = ( V,E ) is distributed according to W k ( P ,` ) if it is generated by the following procedure: 1. Partition [0 , 1] into m subintervals of length ` i , i = 1 ,...,m . 2. Sample N points uniformly from [0 , 1] and create the nodes 3. For every pair of nodes x i and x j identified by the k -tuple of
While the generation is intricate, MFNG admit a geometric in-terpretation. Consider first the partition of the unit square into m 2 rectangles according to the lengths ` . The rectangle in posi-tion ( q,s ) has side lengths ` q and ` s , 1  X  q,s  X  m . The point ( x ,x j )  X  [0 , 1]  X  [0 , 1] lands in the unit square, inside some rect-angle R with side lengths ` i 1 and ` j 1 . The edge  X  X urvives X  the first round with probability p i 1 ,j 1 . In the next round, we recursively partition R according to the lengths ` . The relative positions of x and x j land the point in a new rectangle with side lengths ` ` . The edge survives the second round with probability p i The process is repeated k times and is illustrated in Figure 1. If an edge survives all k levels, then it is added to the graph. x
Figure 1: MFNG X  X  recursive edge generation with m = k = 3 .
The original work on MFNG [14] shows how to compute the ex-pected feature counts for graph properties by examining the entire expanded measure W k ( P ,` ) . In other words, to count the features, the entire probability matrix of size m k  X  m k is formed. However, in some cases m k is of the order of O ( | V | ) (see the examples in Section 6.2). Clearly, computing and storing O ( | V | 2 ) probabilities is infeasible for large networks. Thus, current methods for count-ing and fitting features are intolerably expensive. Theorem 2 shows that we can count many of the same features by only looking at the probability matrix P a constant number of times (independent of | V | ). Hence, we are able to scale these computations to graphs with a large number of nodes.
We start with a lemma that shows how to decompose a generat-ing measure W k with k recursive levels in k measures with depth one. This will make it easier to count subgraphs in Theorem 2.
L EMMA 1. Consider generating measures W 1 ( P ,` ) and W which are parameterized by the same probability matrix P and lengths ` but different recursion depths. Let graphs H 1 ,...,H W 1 ( P ,` ) be independently drawn, and also denote H i = ( V,E with nodes labelled arbitrarily. Then the intersection graph G = ( V,  X  k i =1 E i ) = ( V,E G )  X  X  k ( P ,` ) .

P ROOF . We prove the lemma by conditioning on the categories to which the nodes belong (recall that a category is the set of inter-vals that a node falls into at each level of the recursion). Each node u  X  V is identified with some real number in [0 , 1] . The proba-bility that the k -tuple of categories corresponding to u is c ( u ) = ( c ,...,c k ) in any graph H  X  X  k ( P ,` ) is simply Q k r =1 independence of the H i , the probability that the same node u is in the same categories c 1 ,...,c k in the graphs H 1 ,...,H tively, is also Q k r =1 ` c r . Note that
P (( u,v )  X  E G | c ( u ) = ( c u 1 ,...,c u k ) ,c ( v ) = ( c =
P ( u,v )  X  X  X  k i =1 E i | c ( u ) = ( c u 1 ,...,c u k ) ,c ( v ) = ( c =
Y =
Y In the first equality, we use the definition of E and third equalities, we use the independence of the H i ; and in the final equality, we use the definition of W 1 . However, for any graph Figure 2: Illustration of Lemma 1. If three graphs H 1 ,H are generated from W 1 ( P ,` ) , then their intersection G follows the distribution of W 3 ( P ,` ) .
 G 0  X  X  k ( P ,` ) , we have =
Figure 2 illustrates Lemma 1. Our main result is a straightfor-ward consequence of this lemma.

T HEOREM 2. Let W 1 ( P ,` ) and W k ( P ,` ) be generating mea-sures defined by the same probabilities P and lengths ` but with different recursion depths. Consider k multifractal graphs H ( V,E i ) generated independently from W 1 ( P ,` ) and a multifractal graph G = ( V,E G ) generated from W k ( P ,` ) . For any event A on G that can be written as A = { S  X  E G } , where S  X  { ( i,j ) : i,j  X  X  1 ,...,n } ,i &lt; j } ,
In other words, the probability that a subset of the edges exists if the graph is drawn from W k is the k -th power of the probability that these edges exist if the graph is drawn from W 1 . The condition that A can be written as A = { S  X  E } is subtle. It states that Theorem 2 holds if we can specify a subset of the edges that must be present in the graph. We can be indifferent about certain edges, but we cannot specify that an edge is not present in the graph.
We can now easily compute the moments of subgraph counts, such as the number of edges, triangles, and larger cliques in MFNG. The following corollary shows how to use Theorem 2 for these cal-culations. for graphs generated by MFNG.

C OROLLARY 3. The expected number of edges | E | in a graph sampled from MFNG is
P ROOF . Let u and v , u 6 = v , be two random nodes of G . Let A denote the event A = { ( u,v )  X  E } , and we define A i to denote the analogous event restricted to H i in the multifractal generator. By Theorem 2, we have that Now we can restrict ourselves to A 1 ,
P ( A 1 ) = X We conclude that The expected number of edges is then given by
C OROLLARY 4. Graphs sampled from MFNG also have the following moments. The expected number of d -stars 1 S d is: In particular, the expected number of wedges ( 2 -stars) is The variance  X  E = Var ( | E | ) of the number of edges is  X  where s is the same as in Corollary 3.
 The expected number of t -cliques 2 C t is where In particular, the expected number of triangles ( 3 -cliques) is:
A d -star is a graph with d + 1 vertices and d edges that connect the first node to all other vertices.
A t -clique is a graph with t vertices where every possible edge between the vertices exists. Finally, the expected number of nodes with degree d , E d E P ROOF . The proofs follow the same patterns as of the proof of Corollary 3. We include the proofs in supplementary material on-line 3 .

These are some examples of properties for which we can com-pute the exact expectation. However, we can also compute useful approximations. For a given measure W k , we could empirically compute the value of E [ C t ] for each t until we find 1 &gt; E [ C t  X  +1 ] , which is a good estimator of the expected maxi-mum clique size.

Finally, we note that there are graph properties which will cer-tainly not translate to this theoretical framework. Let  X  ( G ) be the chromatic number of G , i.e., the smallest number of colors needed to color the vertices such that vertices connected by an edge are not the same color. Suppose we want to compute P (  X  ( G ) &lt; 10) . If the theorem is used directly, then the result is P (  X  ( G ) &lt; 10) = P (  X  ( H 1 ) &lt; 10) k . But P (  X  ( G ) &lt; 10)  X  P (  X  ( H taking the intersection of graphs can only reduce the chromatic number. In this case, P (  X  ( G ) &lt; 10) cannot be written as an event on the subset of the edges of the graph. Hence, the assumptions of the theorem are violated.
Now we change gears and look at how we can use the theory laid out above to fit multifractal measures to real networks. Given a graph G , we are interested in finding a probability matrix P , a set of lengths ` , and a recursion depth k , such that graphs generated from the measure W k ( P ,` ) are similar to G . The theoretical results in Section 3 make it simple to compute moments for MFNG, so a method of moments is natural. In particular, given a set of desired features counts f i (such as number of edges, 2 -stars, and triangles), we seek to solve the following optimization problem: Here, F i denotes the actual count of feature f i in the MFNG.
If certain features are more important to fit, then we can weight the terms in the objective function, but for simplicity of our nu-merical experiments, we only use an unweighted objective in this paper. Similar objective functions were proposed for SKG [3] and for mixed Kronecker product graph models [12]. In Section 6, we see that the simple objective function works well on synthetic and real data sets. We want to model real world networks accurately and efficiently. Theorem 2 shows that, given a generating measure W k ( P ,` ) , we http://stanford.edu/~arbenson/mfng.html can quickly compute moments of several (local) feature counts. However, (global) graph properties such as degree distribution and clustering coefficient are not covered by our theoretical results. Therefore, we use local feature counts, such as number of d -stars and t -cliques 4 , as a proxy. If the number of d -stars and t -cliques are similar, then we expect the degree distribution and clustering to be similar as well. For example, the global clustering coeffi-cient is three times the ratio of the number of triangles ( 3 -cliques) to the number of wedges ( 2 -stars) in the graph. In Section 6.2, we show that matching star and clique subgraph counts in social and information networks leads to a generating measure that produces graphs with a similar degree distribution.
Optimization problem (11) is not trivial to solve, as there are many local minima and some of them turn out to be very poor. On the other hand, if we are given the feature counts of a graph and fix k , running a standard optimization solver such as fmincon in Matlab, we find a critical point quickly: we only have to fit m m +1 variables, where, typically, m is two or three. Thus, we solve the optimization problem with many random restarts and use the best result. For each random restart, we first choose a random k and then solve the optimization problem with k fixed. We demonstrate that this crude method works on several practical examples (see Section 6). More sophisticated methods are beyond the scope of this work. We also point out that the bottleneck of the estimation is performing the feature counts, not solving the above optimization problem (despite the many restarts).
In this section, we discuss a heuristic method for generating sample graphs following the multifractal measure that is effective when the graph to be generated is sparse, i.e. has relatively few edges. This is important because the naive sampling method takes O ( | V | 2 ) time X  X t considers the edge for every pair of nodes in the graph. The fast heuristic algorithm is inspired by the  X  X all-dropping X  scheme for SKG (see Section 3.6 of [5]). Unlike the SKG case, however, our algorithm is merely a heuristic due to the stochastic nature of the location of the nodes. The speed-up is ob-tained by fixing the number of edges in advance and only consid-ering O ( | E | ) pairs of vertices. We will demonstrate that our sam-pling algorithm runs in time O ( | E | log( | V | )) . The pseudo-code is given in Algorithm 1. In the next sections, we give the details of the algorithm and briefly discuss the performance.
In order to avoid looping over all pairs of nodes, we fix the num-ber of edges. The number of edges is sampled from a normal random variable with mean E [ | E | ] and variance  X  E , as provided by Corollaries 3 and 4. Since the number of edges is a sum of Bernoulli trials, this is well approximated using a Gaussian random variable.

Once the number of edges in the graph is fixed, we start adding edges. Because node locations are random (i.e., every node has a random category), it is nontrivial to select a candidate edge. This contrasts with SKG, where the edge probabilities for a given node is deterministic. The algorithm selects node categories level by level, for each edge. To select categories, we sample an index ( c,c matrix Q :
From now on we implicitly mean counting subgraphs if we say counting d -stars or t -cliques. The sampling is done proportional to the entries in Q . The matrix Q reflects the relative probability mass corresponding to an edge falling into those categories. In other words, it is the probability of selecting the categories c and c 0 at a given level and the edge sur-viving the level. The category sampling is performed k times, one for each level of recursion. This gives two k -tuples of categories: c = ( c 1 ,...,c k ) and c 0 = ( c 0 1 ,...,c 0 k ) .

Now we want to add an edge between nodes u and u 0 that have the categories c and c 0 . However, we have to be careful about the number of nodes that have the same category. We can think of the category pair ( c,c 0 ) as a box B on the generating measure. Consider two boxes B 1 and B 2 and suppose that both have the same area in the unit square, and the probability between potential boxes in B 1 and B 2 is the same.

A simple example is the following case: The edge probabilities in any two boxes B 1 and B 2 in the measure are the same, and the probabilities of selecting either box (from sampling the Q matrix) are the same. However, because of the randomness categories for nodes, there may be 10 node pairs in B and only one node pair in B 2 . If we simply pick a node pair at random from a box, the probability of connecting the node pair in B 2 is much higher than in B 1 .

To overcome this discrepancy, we take into account the differ-ence between the expected number of nodes pairs in a box and the actual number of node pairs in a box. Note that the joint distribu-tion of nodes is Multinomial ( n ; l 1 ,l 2 ,...,l m k ) where l the length of interval i (after recursive expansion). Let p the edge probability in the box corresponding to the category pair ( c,c 0 ) . Let the box X  X  sides have lengths l and l 0 . Using standard properties of the Multinomial distribution, the expected number of nodes in a box, n c,c 0 , is: Finally, we sample where V c = { v  X  V | category of v is c } . We then add e to the box ( c,c 0 ) . Thus, if there are more node pairs in a box than expected, we add more edges to the box.

There are a couple of details we have swept under the rug. First, we haven X  X  discussed what to do if the box ( c,c 0 ) is empty. In this case, we simply re-sample c and c 0 . In practice, this does not oc-cur too frequently. Second, we have introduced some dependence between edges, and MFNG samples edges independently. For this reason, we use the accuracy factor  X  . By increasing  X  , the sampling takes longer, but there is less dependency between edges.
The speedup achieved by this fast approximation algorithm re-ally depends on the type of graph. We trade an O ( | V | 2 for an algorithm that takes O ( | E | log | V | ) time if there are no re-jected tries due to empty boxes, edges that are already present, etc. In the case that the graph is sparse and k &lt;  X  log m n , this is fine. However, for denser graphs, this fast method will actually turn out to be slower. To arrive at a complexity of O ( | E | log | V | ) we note that it takes O ( | V | log | V | ) time to compute the categories for each Algorithm 1 Fast approximate sampling algorithm 1: Input: Generating measure W k ( P ,` ) , accuracy factor  X  2: Output: Graph G with distribution approximately W k ( P ,` ) . 3: Add | V | nodes by uniformly sampling on [0 , 1] and assigning 4: Set V c = { v  X  V | category of v is c } for each category c . 5: Fix number of candidate edges | E | = bEc , where E  X  6: Compute Q , where Q ij = p ij ` i ` j for 1  X  i,j,  X  m 8: while e global &lt; | E | do 9: for h = 1 to k do 10: Pick category c h ,c 0 h independently and with probability 11: end for 12: c = ( c 1 ,...,c k ) , c 0 = ( c 0 1 ,...,c 0 k ) . 13: Set l , l 0 to lengths of interval corresponding to c , c 14: if | V c ||| V c 0 |6 = 0 then 15: if c = c 0 then 16: n c,c 0 = | V | ( | V | l 2  X  l 2 + l ) 17: else 18: n c,c 0 = | V | ( | V | X  1) ll 0 19: end if 20: Draw e to add  X  Poisson ( n c,c 0 / (  X  | V c ||| V c 21: Set k = 0 22: Set e local = 0 23: while e local &lt; e to add and k &lt; max k do 24: Pick u  X  V c and v  X  V c 0 uniform at random. 25: if ( u,v ) /  X  E and u 6 = v then 26: Add ( u,v ) to E 28: end if 29: Set k = k + 1 30: end while 32: end if 33: end while 34: Return G = ( V,E ) u  X  V . Then, assuming that the number of retries is small, the while loop of Algorithm 1 is executed O ( | E | ) times, each taking O ( k ) = O (log | V | ) steps. Therefore, in total, the algorithm has complexity O ( | E | log | V | ) .
In the next sections, we demonstrate the effectiveness of our ap-proach to modeling networks. First, we show that our method is able to recover the multifractal structure if we generate synthetic graphs following the MFNG paradigm. Thereafter, we consider several real-world networks and compare the performance of our method to alternative methods that use the SKG framework.
Before turning to real networks, it is important to see if our method of moments algorithm recovers the structure of graphs that are actually generated by MFNG with some measure W k . In other words, can our method of moments identify graphs generated from our model? There are two success metrics for recovery of the gen-erating measure. First, we want the method of moments to recover a measure similar to W k . Second, even if we cannot recover the distributions.
 Figure 3: d -stars and t -cliques features that are counted in the ex-periments in Section 6. measure, we want a measure that has similar feature counts. Our experiments in this section show that we can be successful in both metrics. If we can recover a measure with similar moments, then the new measure will be a useful model for the old one.

Our basic setup is as follows: 1. Construct a measure W k ( P ,` ) and generate a single graph 2. Run the method of moment algorithm from Section 4 with
We use two different measures W k for testing. The first is equiv-alent to an Erd  X  os-R X nyi random graph. This is modeled by a gener-ating measure W k ( P ,` ) where every entry of P is identical. In this case, MFNG is an Erd  X  os-R X nyi generative model with edge proba-bility P k 11 , independent of ` . Table 1 shows the retrieved measure  X  W  X  P and  X  ` are quite different than P and ` ,  X  W  X  k ( measure close to an Erd  X  os-R X nyi random graph model. The reason is that the length vector ` is heavily skewed to the second com-ponent ( ` 2  X  0 . 94 ). In expectation, 0 . 94  X  k  X  0 . 53 of the nodes correspond to the same category. These nodes are all connected with probability 0 . 6829  X  k  X  0 . 022 , which is nearly the same as the edge probability in the original Erd  X  os-R X nyi measure. Figure 4 shows the histograms of the features that were used in the method of moments algorithms (as well as the clustering coefficient). The green histogram is the data for graphs sampled from W k ( P ,` ) , the red histogram is the same data for graphs sampled from  X  and the blue line is the feature count in the original graph G used as input to the method of moments. There is remarkable overlap be-tween the empirical distribution of the features for  X  W the distribution of the features for the original measure.
For a second experiment, we used an original measure W k ( P ,` ) that did not possess the uniform generative structure of Erd  X  os-R X nyi random graphs. Table 2 shows the retrieved measure and the origi-nal measure. In this case, the method of moments identified a simi-lar generative measure. The parameters  X  k ,  X  P , and  X  similar to k , P , and ` . Figure 5 shows the distribution of the fea-tures in graphs sampled from the two measures. Again, there is rather significant overlap in the empirical distributions.
Finally, we compare the degree distributions of the original and retrieved measures in Figure 6. The degree distributions are nearly identical.

These results show that the method of moments algorithm de-scribed in Section 4 can successfully identify MFNG instances us-ing a single sample.
We now show how the method of moments from Section 4 per-forms when fitting to the following four real-world networks to MFNG: 1. The Gnutella graph is a network of host computers sharing 2. The Citation network is from a set of high energy physics 3. The Twitter network is a combination of several ego net-4. The Facebook network is a combination of several ego net-All data sets are from the SNAP collection. We use the optimization procedure described in Section 4 with 2,000 random restarts. The features we use (the f i in Section 4) are number of edges, wedges ( S For each network, we use m = 2 , 3 and k = d log m ( | V | ) e . With these values of m we are able to effectively fit the networks to MFNG. We do not believe that larger values of m are useful: we would need to estimate too many parameters and we lose a lot in in-terpretability of results. While k can be arbitrary, a smaller value of k leads to many nodes belonging to the same categories and hence having the same statistical properties. In large graphs, this causes a  X  X lumping X  of properties such as degree distribution near a small set of discrete values. While smaller k may be satisfactory for test-ing algorithms, keeping k near log m ( | V | ) produces more realistic graphs. In an additional set of experiments, we only fit the number of edges, wedges, and triangles. We also compare against KronFit and the SKG method of moments [3].

The results are summarized in Table 3. In addition, the online material lists all recovered parameters. Overall, for both m = 2 and m = 3 , the method of moments can effectively match most feature counts. The number of 4 -stars ( S 4 ) was the most difficult parameter to fit. We see that when only fitting the number of edges, wedges, and triangles, the other feature moments can be signifi-cantly different from the original graph. In particular, the number of 4 -cliques tends to be severely under-or over-estimated. Although KronFit does not explicitly try to fit moments, the results show that it severely underestimate several feature counts. The method of moments approach to SKG can fit three of the features, which is consistent with results on other networks [3].

As mentioned in Section 4.1, the clustering coefficient is three times the ratio of the number of triangles ( 3 -cliques) to the number of wedges ( 2 -stars) in the graph. The results of Table 3 show that retrieved measures produce almost identical distributions. the method of moments can match both the number of triangles and the number of wedges in expectation. This does not make any guar-antees about the ratio of these random variables, but the synthetic experiments (Section 6.1) demonstrated that their variances are not too large. Therefore, the expectation of the ratio is near the ratio of the expectations, and we approximately match the global clustering coefficient.

Figure 7 shows the degree distributions for the original networks and a sample from the corresponding MFNG, using the fast sam-pling algorithm. We see that, even though we only fit feature mo-ments, the global degree distribution is similar to the real network. However, the MFNG degree distributions experience oscillations, especially in the case when m = 2 . This is a well-known issue in SKG [16], and we address this issue in Section 6.3. Finally, note that we only plot the degree distribution for a single MFNG sample. The reason is that the samples tend to have quite similar degree dis-tributions. This lack of variance has been observed for SKG [10], and addressing this issue for MFNG is an area of future work.
Table 4 shows the diameter, effective diameter, and average node eccentricity for the original networks and sampled MFNG networks. Effective diameter is the 90-th percentile of the linearly interpo-
Netw ork Diameter Eff. Diameter Avg. Eccentricity Gnutella 11 / 13 / 12 6.73 / 5.66 / 5.09 8.94 / 6.05 / 4.27 Citation 13 / 15 / 21 4.99 / 5.62 / 6.56 9.15 / 7.02 / 12.17 Twitter 7 / 11 / 21 4.52 / 4.47 / 6.47 5.92 / 5.57 / 10.94 Facebook 8 / 10 / 14 4.76 / 3.98 / 4.90 6.35 / 5.91 / 8.40 T able 4: Diameter, effective diameter and average node eccentric-ity for the original networks and sampled MFNG networks. For MFNG, each value is the median of five samples from the method described in Section 5. For diameter and effective diameter, 20% of the nodes were used to approximate the property. lated distribution of shortest path lengths [5]. The values between the original network and the MFNG samples are similar.
Figure 7 shows that the graphs generated with MFNG experience oscillations in the degree distribution. The oscillations for the de-gree distribution are a well-known issue in SKG [16]. Seshadhri et the true feature count. S d is the number of d -stars in the graph, and C is the number of t -cliques in the graph. A value of 1.00 means that the Algorithm 2 Noisy MFNG ( m = 2 ) 1: Input: 2  X  2 probability matrix P , lengths vector ` , number of 2: Output: noisy MFNG matrix G 3: for i = 1 to k do 4: Sample  X  i  X  Uniform [  X  b,b ] . 6: P ( i ) = min(max( P ( i ) , 0) , 1) entry-wise 7: Sample H i  X  X  1 ( P ( i ) , l ) 8: end for al . present a  X  X oisy SKG X  model that perturbs the initiator matrix at each recursive level, which dampens the oscillations. Inspired by their work, we present a similar  X  X oisy MFNG" in this section.
We first note that Figure 7 shows that using m = 3 results in less severe oscillations in the degree distribution. The intuition be-hind this is that more categories get mixed at each recursive level, producing a larger variety of edge probabilities. For m = 2 , we propose a Noisy MFNG model, which is described in Algorithm 2. The idea is to perturb the probability matrix slightly at each level. In the context of Lemma 1, this means that the noisy MFNG graph is the intersection of several graphs generated from slightly differ-ent probability matrices. The fast generation method still works in this case X  X  different matrix at each level determines the categories instead of one single matrix. The probability perturbations are anal-ogous to those performed by Seshadhri et al . We test Noisy MFNG on the citation and Twitter networks, and the results are in Fig-ure 8. The graphs are sampled using the fast sampling algorithm. We see that increasing the noise significantly dampens the degree distribution. However, the far end of the tail still experiences some oscillations.
We have shown that the multifractal graph paradigm is well suited to model and capture the properties of real-world networks by build-ing on the work of Palla et al . [14] and incorporating several ideas from SKG. The foundation of our theoretical work is Theorem 2, which has opened the door to quick evaluation of the expected value of a number of important counts of subgraphs, such as d -stars and t -cliques. Combined with standard optimization routines, we are Figure 8: Degree distributions for fitting the citation and Twitter networks to Noisy MFNG with varying degrees of noise. We see that adding noise dampens the oscillations in the degree distribu-tions. At the far end of the tail, it is still difficult to control the degree distribution. The graph samples were generated with the fast sampling algorithm in Section 5. able to fit large graphs fast and accurately. Our method of moments algorithm identifies synthetically generated MFNG and also pro-duces close fits for real-world networks. It is quite amazing how fitting a few  X  X ocal X  properties leads to a generator that fits the over-all structure of graphs well.

This would not be too useful if we were not able to also gener-ate multifractal graphs of the same scale. For this, we presented a fast heuristic approximation algorithm that generates such graphs in O ( | E | log | V | ) complexity, rather than the naive O ( | V | rithm. Since many real-world networks are sparse, this is a signifi-cant improvement.
Future work includes the development of approximation formu-las for the moments of global properties like graph diameter and a more tailored approach in the optimization routines for the fitting. A pressing issue is the theory behind the fast generation method. While the generation tends to produce similar graphs to the naive generation in practice, we want to prove that the approximation is good. Furthermore, it is possible to improve the generation fur-ther by considering a parallel implementation. Lastly, it would be interesting to do a theoretical analysis of the oscillatory degree dis-tribution, similar in spirit to [16].
We thank David Gleich and Victor Minden for helpful discus-sions. Austin R. Benson is supported by an Office of Technology Licensing Stanford Graduate Fellowship. Carlos Riquelme is sup-ported by a DARPA grant research assistantship under the supervi-sion of Prof. Ramesh Johari. Sven Schmit is supported by a Prins Bernhard Cultuurfonds Fellowship. [1] L. Akoglu and C. Faloutsos. RTG: a recursive realistic graph [2] D. Chakrabarti, Y. Zhan, and C. Faloutsos. R-MAT: A [3] D. F. Gleich and A. B. Owen. Moment-based estimation of [4] M. Kim and J. Leskovec. Multiplicative attribute graph [5] J. Leskovec, D. Chakrabarti, J. Kleinberg, C. Faloutsos, and [6] J. Leskovec and C. Faloutsos. Scalable modeling of real [7] J. Leskovec, J. Kleinberg, and C. Faloutsos. Graphs over [8] J. Leskovec, J. Kleinberg, and C. Faloutsos. Graph evolution: [9] J. McAuley and J. Leskovec. Learning to discover social [10] S. Moreno, S. Kirshner, J. Neville, and S. Vishwanathan. [11] S. Moreno and J. Neville. Network hypothesis testing using [12] S. I. Moreno, J. Neville, and S. Kirshner. Learning mixed [13] R. C. Murphy, K. B. Wheeler, B. W. Barrett, and J. A. Ang. [14] G. Palla, L. Lov X sz, and T. Vicsek. Multifractal network [15] C. Seshadhri, T. G. Kolda, and A. Pinar. Community [16] C. Seshadhri, A. Pinar, and T. G. Kolda. An in-depth analysis
