 In the last few years, many CQA systems have been launched, including Ya-hoo! Answers, BuyAns, Live QnA. Since th eir inception, CQA sites have rapidly gained popularity. Hundreds of millions of answers have already been posted for tens of millions of questions in Yahoo! Answers. When a user chooses to ask a new question in a CQA service, the user would have to wait for a long time be-fore getting answers. If the CQA service could automatically search and display relevant pre-existing questions and thei r answers, it will reduce the waiting time and improve the user satisfaction. Hence, it X  X  important the search service offers relevant results efficiently.

Instead of inputting just keywords or so, users form questions using natural language. Many previous approaches introduce natural language models to find similar questions, such as translation model [1], MDL tree [3], syntactic tree [4] and so on [5]. They focus on the text contents, weakening the structure infor-mation of forums. Some researchers ha ve utilized some features of CQA sites. Sun et al. [6] did the question retrieval based on user ratings. The category information of questions is proven to be positive information for question re-trieval [7]. It can prune the search space and improve the efficiency and the effectiveness. However, the utilization of features is not enough.

In this paper, we propose a retrieval model that utilizes more features of CQA sites, including question, description, a nswer, category and users X  posted ques-tions. The model can be divided into two parts. First, the category classification of a query is predetermined according to the asker X  X  posted questions. Second, we make use of  X  X ependency syntactic tree X  to find similar questions within the predetermined categories. Many previous approaches introduce natural language models to find similar questions. Xue et al. [1] proposed a retrieval model that combined a translation-based language model for the question part with a query likelihood approach for the answer part. Joen et al. [2] discussed methods for question retrieval that were based on using the similarity between answers in the archive to estimate probabilities for a translation-based retrieval model. Duan et al. [3] proposed to use the MDL-based tree cut model for identifying question topic and ques-tion focus automatically. Wang et al. [4] tackled the similar question matching problem based on syntactic tree structure. The model did not rely on training.
Some researchers have solved some problems in the CQA by exploring the features of CQA sites. Sun et al. [6] ut ilized the question star feature to rec-ommend new question to the askers who posted similar questions. Liu et al. [9] developed a variety of content, structure, and community-focused features for user satisfaction prediction. Jeon et al. [10] used textual and non-textual features that were commonly recorded by web ser vices to improve the search quality.
Closest to our work, Cao et al. presented a new approach [8] to exploit category information of questions for improving the performance of question retrieval. In the paper [7], they used a category language model to smooth a question language model, and integrated the classification scores returned by a classifier built with historical question data into language models. Our work differs from it on the classification and the retrieval model. Our classification is based on the users X  posted questions, not only the category. The similar questions are retrieved with the  X  X eywords X  extracted from  X  X ependency syntactic tree X . 3.1 Category Probability Table Construction Given a new question q , the probability of the question belonging to a category c for the asker u is defined as p ( c | q,u ). A question can be represented by a set of words. Then the computation for p ( c | q,u ) depends on the computation for p ( c | w,u ). After tokenization, stop words filtering and stemming, p ( c | w,u )is estimated as follows: where freq ( w,c | u ) represents the frequency of the word w in the category c with the user u ,and con ( w,c | u ) is the correlation of the word w to the category c for the user u .Thet are estimated as follows: where n ( q | c, w, u ) denotes the number of the user u  X  X  posted questions which contain the word w in the category c ,and n ( q | c, u )isthenumberoftheuser u  X  X  posted questions in the category c . where n ( c | w,u ) denotes the number of categories where the word w occurs for the user u . The lower the value, the more correlated between the word and the category. 3.2 Question Classification We apply the max algorithm to compute the top-k ranked results for a new question query. The benefit o f the max algorithm is that p ( c | q,u ) depends on the frequent words in the specific category, ignoring the effect of noise words. Given a query q containing l words and the asker u , the probability of the question q belonging to the category c can be obtained by: where p ( c | w,u ) is the probability of the word w belonging to the category c with the user u Dependency syntactic tree analysis is a research focus in the field of Natural Language Processing. A new query is represented by a  X  X ependency syntactic tree X  with the Minipar [11]. In the tree, th e connection between the nodes reflects the dependency between them. The cate gory attribute of node reflects POS (part of speech).
 Definition 1 (Keywords). keywords consists of two parts. One is single noun  X  X inglekey X  except stop words. The other is double words  X  X ikeys X  ( bikeys 1 bikeys 2 ) from  X  X ependency syntactic tree X . The word bikeys 1 has dependency relationship with the word bikeys 2 . The POS of them is limited to noun, verb, adjective and adverb. In addition, the keywords are processed with stemming. The candidate questions are obtained by searching the questions that have at least one the same keyword of the query q in the field of question/description/ answer within the top-k similar categories. The method utilizes all the informa-tion of a question. Though a question has no literal similarity with the query, if the answer field has keywords, it will be a candidate question.

The candidate questions are ranked by taking the addition across all the weight of the keywords in the question: where n ( q | c ) denotes the number of questions in the category c ,and n ( q | w,c )is the number of questions which contain the keyword k in the category c . 5.1 Experimental Setup Dataset. Our data was based on a snapshot of Yahoo! Answers, containing 4,773,563 questions. It was crawled in the late 2012. There are 26 categories at the first level and 1263 categories at the leaf level. Each question belongs to a unique leaf-level category. We rando mly selected 100 users whose activities are public in each top-level category, 2600 users in total. Then we collected the questions that have been posted by the 100 users as the users X  posted questions set. 300 questions are randomly selected from the questions set as the test data set, the rest of questions set is as the training data for question classification. We used the Lucene to preprocess the text, including tokenization, stop word filtering and stemming.

We remove the stop words. For each model, the top 20 retrieval results are kept. We put all the results from different models for one query question to-gether for annotation. Thus annotators do not know which results are from which model. Annotators are asked to label each returned question with  X  X el-evant X  or  X  X rrelevant X . Two annotators are involved in the annotation process. If conflicts happen, a third person will make judgment for the final result. We eliminate the query questions that do not have relevant questions. Finally we get 246 queries that have relevant questions.
 Evaluation Metrics. We evaluate the performance of our approaches using Mean Average Precision (MAP), Mean reciprocal rank (MRR) and Precision@n.
MAP rewards approaches returning relevant questions earlier, and also em-phases the rank in returned lists. MRR gives us an idea of how far down we must look in the ranked list in order to find a relevant question. Precision@n is the fraction of the Top-n questions retrieved that ar e relevant. The evaluation was based on the top 20 results from all approaches.
 Methods Compared. We compare with the Translation Model (TR) that have been used for question retrieval in the paper [2], category based retrieval [7] (CR). We also report the results of the Vector Space Model(VSM), the Okapi Model and the Language Model (LM).
 5.2 Experimental Results Each question belongs to a unique leaf-level category by the classification al-gorithm. Question search can benefit from classification if the correct category is contained in the Top-n returned categories. In order to see if the correct category is contained in the Top-n returned categories, we compute the percent-age of test query questions whose correct categories are contained in the Top-n categories returned by the classification algorithm. The paper [7] called the per-centage  X  X uccess@n. X  It is shown in Figure 1(a). We can see that the accuracy of question classification in the Top-10 categories returned by the classification algorithm of our method  X  X &amp;MR X  reaches 80%. There are 1263 leaf-level cate-gories. It greatly prunes the search space.

Figure 1(b) shows the performance of question classification with varying number of history data. The abscissa sh ows the number of users X  posted ques-tions for training. Then we use users X  latest 10 questions to test. History ques-tions have nearly no effect in the situa tion when the correct category is at the first rank. However, it becomes helpful when the correct ca tegory in the top-3/5/10 categories returned by the classific ation algorithm, and the  X  X uccess@n X  increases with the more posted questi ons as the training set. The percentage may reach 80% when the number of history data is 60.

As shown in Table 1, our approach  X  X &amp;MR X  significantly outperforms other approaches, and performs slightly worse than CR on the metric of P@n. The relevant questions returned by C&amp;MR have high rank. The number of returned relevant questions is a little less than that of CR. This is because the keywords extracted from  X  X ependency syntactic tree X  search the match in the three fea-tures: question, description and answer. The match in the three features has the same weight. The relevant questions with more keywords match will get high rank score. It rewards the highly similar questions. The relevant question which is only similar in the question feature will get low rank score. In this paper, we propose a retrieval model that utilizes more features of CQA sites, including question, description, a nswer, category and users X  posted ques-tions. The model is divided into question classification and question retrieval. First, the category classification of a qu ery are predetermined according to the asker X  X  posted questions. Second,  X  X ependency syntactic tree X  is made use of to find similar questions within the predetermined categories. We have demon-strated the effectiveness of our appr oach in large-scale experiments Acknowledgments. The work described in this paper was fully supported by the National Natural Science Fo undation of China under Grant No. 61170184.
