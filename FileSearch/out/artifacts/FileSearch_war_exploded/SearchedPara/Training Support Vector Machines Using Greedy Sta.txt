 Hard margin support vector machines have a risk of getting overfitting in the presence of the noise [1]. To deal with this problem, soft margin SVMs [2] introduce the capac-is a highly effective mechanism for avoiding overfitting, which leads to good gener-soft margin SVMs: the problem  X  . Probably, the most famous method among them is sequential minimi-zation optimization algorithm (SMO), which is proposed by Platt [4] and further im-development of SVMs. space by algorithm itself. In summary, the proposed algorithms possess the following two attractive properties:  X  No extra capacity control parameter is required. The Wolfe dual of hard margin SVMs can be regarded as a loss function induced by reproducing kernel Hilbert space norm. This allows us to approximate it using greedy algorithm. Due to the room limitation, overfitting due to no capacity control term. Here, we will deal with the two problems following. For 1, 2, ml = L , and then each basis function is used once at most. 
For SVMs, w takes the form ,0 y  X   X  X   X  . Using the loss function (1) we have Note that the first two terms of (4) can be ignored. Define the gradient vector We can reformulate (4) as have the subproblem solution, i.e. Combining (7) and (9), we get In GS-SVMs, each basis function corresponds to a specified training sample and vice versa. Hence, if the basis function decreasing. Hence we will terminate the algorithm if the above condition is satisfied. lowing equations Thus the greedy stagewise algorithm for SVMs (GS-SVMs) can be described as memory requirement of GS-SVMs is only ( ) Ol . 
K  X  = X  X  X  xx x x . Following [4], we compare the number of kernel evaluations of GS-SVMs and SMO, which is an effective measure of the algorithm X  X  speed. For the sake of fair comparison, we use the same data sets and kernel parame-ter as in [4]. Note that the number of kernel evaluations of SMO in Table 1. denotes the average number under the different capacity control parameters. Problems Size  X  Dim SMO-1 SMO-2 GS-SVMs 
From Table 1, we can see that GS-SVMs obtain the speedup range from 10 to 30 compare it with hard magin and Soft margin SVMs on the fifteen benchmark data used to extend binary classifiers to multi-class classifiers. 
On each data set, ten-fold cross validation is run. The average accuracy of ten-fold ment setup is the following: (a) For soft margin SVMs, Kernel width and capacity control parameter are chosen SVMs on Glass and worse than SVMs on Liver. As for the remaining data sets, GS-SVMs and SVMs obtain the similar performance. Hence we have the conclusion that GS-SVMs are significantly better in speed than SMO and comparable in performance with SMO. Problems Size Dim Class GS-SVMs HM-SVMs SVMs Ionosphere 351 34 2 94.00 94.00 94.02 This paper proposes a greedy stagewise algorithm, named GS-SVMs to deal with the of GS-SVMs. 
