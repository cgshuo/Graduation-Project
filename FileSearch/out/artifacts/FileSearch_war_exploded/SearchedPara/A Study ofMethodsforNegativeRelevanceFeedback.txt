 Ne gati ve rele vance feedback is a special case of rele vance feed-back where we do not have any positi ve example; this often hap-pens when the topic is dif cult and the search results are poor . Al-though in principle any standard rele vance feedback technique can be applied to negati ve rele vance feedback, it may not perform well due to the lack of positi ve examples. In this paper , we conduct a systematic study of methods for negati ve rele vance feedback. We compare a set of representati ve negati ve feedback methods, cov-ering vector -space models and language models, as well as sev-eral special heuristics for negati ve feedback. Ev aluating negati ve feedback methods requires a test set with suf cient dif cult topics, but there are not man y naturally dif cult topics in the existing test collections. We use two sampling strate gies to adapt a test collec-tion with easy topics to evaluate negati ve feedback. Experiment re-sults on several TREC collections sho w that language model based negati ve feedback methods are generally more effecti ve than those based on vector -space models, and using multiple negati ve mod-els is an effecti ve heuristic for negati ve feedback. Our results also sho w that it is feasible to adapt test collections with easy topics for evaluating negati ve feedback methods through sampling.
 Categories and Subject Descriptors: H.3.3 [Information Search and Retrie val]: Retrie val models General Terms: Algorithms Keyw ords: Ne gati ve feedback, dif cult topics, language models, vector space models
No retrie val model is able to return satisf actory results for every query . Indeed, a query might be so dif cult that a lar ge number of top-rank ed documents are non-rele vant. In such a case, a user would have to either reformulate the query or go far down on the rank ed list to examine more documents. Thus studying how to im-pro ve search results for such dif cult topics is both theoretically interesting and practically important.

A commonly used strate gy to impro ve search accurac y is through feedback techniques, such as rele vance feedback [12, 8], pseudo-rele vance feedback [1, 20], and implicit feedback [14]. In the case of a dif cult topic, we lik ely will have only negati ve (i.e., non-rele vant) examples, raising the important question of how to per -form rele vance feedback with only negati ve examples. We refer to this problem as negative feedbac k . Ideally , if we can perform ef-fecti ve negati ve feedback, when the user could not nd any rele vant document on the rst page of search results, we would be able to impro ve the ranking of unseen results in the next a few pages.
Ho we ver, whether such negati ve feedback can indeed impro ve retrie val accurac y is still lar gely an open question. Indeed, the ef-fecti veness of current feedback methods often rely on rele vant doc-uments; negati ve information, such as non-rele vant documents, is mostly ignored in past work [4, 8].

On the surf ace, any standard rele vance feedback technique can be applied to negati ve rele vance feedback. Ho we ver, our recent work [19] has sho wn that special care and special heuristics are needed to achie ve effecti ve negati ve feedback. Specically , in this work, we have sho wn that some language model-based feedback methods, although quite effecti ve for exploiting positi ve feedback information, cannot naturally handle negati ve feedback, thus sev-eral methods were proposed to perform negati ve feedback in lan-guage modeling frame work. Ho we ver, this study is neither com-prehensi ve nor conclusi ve for several reasons: (1) It is only limited to language models; vector space models have not been evaluated. (2) The results are evaluated over only one collection. (3) The lack of systematic experiment design and result analysis mak es it hard to kno w the adv antages or disadv antages of dif ferent methods.
In this paper , we conduct a more systematic study of dif ferent methods for negati ve rele vance feedback. Our study is on two representati ve retrie val models: vector space models and language models. We rst cate gorize negati ve feedback techniques into sev-eral general strate gies: single query model, single positi ve model with single negati ve query model, and single positi ve model with multiple negati ve query models. Follo wing these strate gies, we then develop a set of representati ve retrie val methods for both re-trie val models. Systematic comparison and analysis are conducted on two lar ge representati ve TREC data sets. Ideally , test sets with suf cient naturally dif cult topics are required to evaluate these negati ve feedback methods, but there are not man y naturally dif-cult topics in the existing TREC data collections. To overcome this dif culty , we use two sampling strate gies to adapt a test collec-tion with easy topics to evaluate negati ve feedback. The basic idea of our sampling methods is to simulate dif cult queries from easy ones through deleting a set of rele vant documents so that the results become poor . The effecti veness of these sampling methods is also veried on the TREC data sets.
 Our systematic study leads to several interesting conclusions. We nd that language model-based negati ve feedback methods are generally more effecti ve and rob ust than those based on vector space models possibly due to more accurate learning of negati ve models. While cluster hypothesis [7] generally holds for rele vant documents, our results sho w that negati ve documents do not clus-ter together . Thus adapting standard rele vance feedback to learn a single query model is not optimal for negati ve feedback, and using multiple negati ve models is more effecti ve than a single negati ve model since negati ve documents may distract in dif ferent ways. Our results also sho w that it is feasible to adapt test collections with easy topics (through sampling) to evaluate negati ve feedback methods.

The rest of the paper is organized as follo ws. In Section 2, we re-vie w the related work. In Section 3, we describe our problem setup and dif ferent techniques for negati ve feedback. We describe our sampling methods to simulate dif cult topics by adapting easy ones in Section 4. Experiments are analyzed and discussed in Section 5. We conclude this paper and discuss our future work in Section 6.
The study of dif cult queries has attracted much attention re-cently , partly due to the launching of the ROB UST track in the TREC conference, which aims at studying the rob ustness of a re-trie val model and developing effecti ve methods for dif cult queries [18, 17]. Ho we ver, the most effecti ve methods developed by the participants of the ROB UST track tend to rely on external resources (notably the Web) to perform query expansion, which has in some sense bypassed the dif culty of the problem as in reality , there is often no such external resource to exploit, or otherwise, the user would have directly gone to the external resource to nd informa-tion. Indeed, the Web resource would not help impro ve search ac-curac y for dif cult topics on the Web itself. In our work, we aim at exploiting negati ve feedback information in the tar get collection from which we want to retrie ve information.

There has been some work on understanding why a query is dif -cult [6, 3, 2], on identifying dif cult queries [17], and on predicting query performance [21]. But none of this work has addressed the important question of how to impro ve search accurac y for dif cult queries.

Feedback techniques have been extensi vely studied and mostly sho wn to be effecti ve to impro ve retrie val accurac y [12, 10, 1, 13, 5, 20, 22, 14]. In general, most feedback techniques rely on posi-tive documents  X  documents that are explicitly judged as rele vant or implicitly assumed to be rele vant  X  to pro vide useful related terms for query expansion. In contrast, negati ve (i.e., non-rele vant) docu-ments have not been found to be very useful. In general, exploiting non-rele vant information is lar gely une xplored; query zone [16] ap-pears to be the only major heuristic proposed to effecti vely exploit non-rele vant information in document routing tasks. It sho wed that using non-rele vant documents which are close to the original queries is more effecti ve than using all non-rele vant documents in the whole collection. Ho we ver, this problem was studied for doc-ument routing tasks and a lot of rele vant documents are used. Our problem setting is quite dif ferent in that we only have non-rele vant documents for feedback, and we start with non-rele vant documents close to a query to study how to use this negati ve information opti-mally in ad hoc retrie val.

Our recent work [19] is the rst study on the problem of negati ve feedback in language models. It sho ws that special techniques are needed to handle negati ve feedback. Our current work can be re-garded as an extension of this pre vious work to include additional retrie val models, additional heuristics, and making more conclu-sive ndings. The main dif ferences between our current work and the pre vious work [19] include: (1) We extend pre vious study and propose several gener al negati ve feedback strate gies that can be ap-plied to both vector space and language models. (2) We study two sampling methods to construct lar ger collections to evaluate nega-tive feedback methods. (3) Our experiments are more systematic and conducted over more collections.
We formulate the problem of negati ve feedback in a similar way as presented in [19]. Given a query Q and a document collection a retrie val system returns a rank ed list of documents L the i -th rank ed document in the rank ed list. We assume that so dif cult that all the top f rank ed documents (seen so far by a user) are non-rele vant. The goal is to study how to use these nega-tive examples, i.e., N = f L 1 ; :::; L f g , to rerank the next documents in the original rank ed list: U = f L f +1 ; :::; L set f = 10 to simulate that the rst page of search results are irrel-evant, and set r = 1000 . We use the follo wing notations in the rest of the paper: S ( Q; D ) is the rele vance score of document D for query c ( w; D ) is the count of word w in document D . c ( w; Q ) is the count of word w in query Q . jC j is the total number of documents in the collection C df ( w ) is the document frequenc y of word w . j D j is the length of document D . avdl is the average document length.
 N is the set of negati ve feedback documents.
 U is the set of unseen documents to be rerank ed.
Since negati ve feedback can be regarded as a special case of rel-evance feedback where no positi ve example is available, our rst general strate gy is simply to apply any existing feedback methods (e.g., Rocchio [12]) to use only non-rele vant examples. We call this strate gy query modication because most existing feedback meth-ods would achie ve feedback through modifying the representation of a query based on rele vant and non-rele vant feedback documents. In effect, the y often introduce additional terms to expand a query and assign more weight to a term with more occurrences in rele vant documents and less weight or negati ve weight to a term with more occurrences in non-rele vant documents.

Some existing feedback methods, such as Rocchio method [12], already have a component for using negati ve information, so the y can be directly applied to negati ve feedback. Ho we ver, other meth-ods, such as model-based feedback methods in language modeling approaches [22], can not naturally support negati ve feedback, thus extension has to be made to mak e them work for negati ve feedback [19]. Later we will further discuss this.

Note that with this strate gy, we generally end up with one sin-gle query model/representation which combines both positi ve in-formation from the original query and negati ve information from the feedback documents.
The query modication strate gy mix es both positi ve and nega-tive information together in a single query model. Sometimes it is not natural to mix these two kinds of information as in the case of using generati ve models for feedback [22]. A more exible alter -nati ve strate gy is to maintain a positi ve query representation and a negati ve query representation separ ately , and combine the scores of a document w.r.t. both representations. We call this state gy scor e combination .

With this strate gy, negati ve examples can be used to learn a neg-ative query representation which can then be used to score a doc-ument based on the lik elihood that the document is a distracting non-rele vant document; such a score can then be used to adjust the positi ve rele vance score between the original query and the corre-sponding document.

Intuiti vely , a document with higher rele vance score to the nega-tive query representation can be assumed to be less rele vant, thus the nal score of this document can be computed as where Q neg is a negati ve query representation and is a parameter to control the inuence of negati ve feedback. When = 0 , we do not perform negati ve feedback, and the ranking would be the same as the original ranking according to query Q . A lar ger value of causes more penalization of documents similar to the negati ve query representation.

Equation (1) sho ws that either a high score of S ( Q; D ) score of S ( Q neg ; D ) would result in a high score of S This means that the proposed score combination may favor non-rele vant documents if the y have lower similarity to the negati ve model; this is risk y because the negati ve query representation is only reliable for ltering out highly similar documents. Thus a more reasonable approach would be to only penalize documents which are most similar to the negati ve query model and avoid af-fecting the rele vance scores of other documents. To achie ve this goal, instead of penalizing all the documents in U , we need to pe-nalize only a subset of documents that are most similar to the nega-tive query . We propose to use the follo wing two heuristics to select documents for penalization (i.e., adjusting their scores using Equa-tion (1)): Heuristic 1 (Local Neighborhood): Rank all the documents in by the negati ve query and penalize the top documents.
 Heuristic 2 (Global Neighborhood): Rank all the documents in C by the negati ve query . Select, from the top documents of this rank ed list, those documents in U to penalize.

In both cases, is a parameter to control the number of doc-uments to be penalized and would be empirically set. The two heuristics essentially dif fer in how this value affects the num-ber of documents in U to be penalized. In Heuristic 1, the actual number of documents in U to be penalized is x ed, i.e., , but in Heuristic 2, it is dynamic and could be smaller than , because the top documents most similar to the negati ve query are generally not all in the set of U . If we are to set to a constant for all queries, intuiti vely Heuristic 2 can be more rob ust than Heuristics 1, which is conrmed in our experiments.

Ho w do we compute the negati ve query repsentation Q n eg the score S ( Q neg ; D ) ? A simple strate gy is to combine all the negati ve information from N to form a single negati ve query rep-resentation, which would be referred to as  X Single Ne gati ve Model X  ( SingleNe g ). Ho we ver, unlik e positi ve information, negati ve infor -mation might be quite diverse. Thus, it is more desirable to capture negati ve information with more than one negati ve query model. Formally , let Q i neg , where 1 i k , be k negati ve query models, we may compute S ( Q neg ; D ) as follo ws: where F is an aggre gation function to combine the set of We call this method  X Multiple Ne gati ve Models X  ( MultiNe g ).
We have discussed two general strate gies with some variations for negati ve feedback, which can be summarized as follo ws: (1) SingleQuery : query modication strate gy; (2) SingleNeg : score combination with a single negati ve query model; (3) MultiNeg : score combination with multiple negati ve query models. For both SingleNe g and MultiNe g models, we can use either of the two heuris-tics proposed in the pre vious subsection to penalize documents se-lecti vely . In the next subsection, we discuss some specic ways of implementing these general strate gies in both vector space models and language models:
In vector space models, documents and queries are represented as vectors in a high-dimensional space spanned by terms. The weight of a term w in document D can be computed in man y dif-ferent ways and typically a similar measure such as dot product is used to score documents[15].

In our experiments, we use the follo wing BM25 weight [11]: where k 1 and b are parameters. The weight of a query term is set to the raw term frequenc y, i.e., c ( w; Q ) . We compute the rele vance score using the dot product: S ( Q; D ) = ! Q ! D where ! represent document vector and query vector , respecti vely .
The Rocchio method [12] is a commonly used feedback method in vector space models. The idea is to update a query vector with both rele vant and non-rele vant documents. When only non-rele vant documents N are available, the Rocchio method can be written as This gives us an updated query vector ! Q new , which can be used to rerank documents in U .
SingleNe g adjusts the original rele vance score of a document with a single negati ve query . We compute the negati ve query as the center of negati ve documents, i.e., ! Q neg = 1 Using Equation (1), the combined score of a document D is It is straightforw ard to verify that Equation (3) and (4) are equi v-alent. Ho we ver, SingleNe g has the adv antage of allo wing us to penalize negati ve documents selecti vely using either of the two heuristics presented earlier .
MultiNe g adjusts the original rele vance score of a document with multiple negati ve queries which can be obtained, e.g., through clus-tering. In our experiments, we tak e each negati ve document as a negati ve query and use max as our aggre gation function. Intu-itively , max allo ws us to penalize any document that is close to at least one negati ve document. Thus This score is then combined with S ( Q; D ) to rerank the documents in
U . Again, we have two variants of this method corresponding to applying the two heuristics discussed abo ve.
KL-di vergence retrie val model [9] is one of the most effecti ve re-trie val models in the language modeling frame work. The rele vance score is computed based on the negati ve KL-di vergence between query model Q and document model D where V is the set of words in our vocab ulary . The document model D needs to be smoothed and an effecti ve method is Dirich-the collection language model and is a smoothing parameter .
Unlik e vector space models, it is not natural to directly modify a query model using negati ve information in language model since no term can have a negati ve probability . In our recent work [19], several methods have been proposed for negati ve feedback in the language model frame work. We adopt the methods there and com-bine them with the two heuristics discussed earlier for document penalization.
SingleNe g adjusts the original rele vance score of a document with a single negati ve model. Let Q be the estimated query model for query Q and D be the estimated document model for docu-ment D . Let N be a negati ve topic model estimated based on the negati ve feedback documents N = f L 1 ; :::; L f g . In SingleNe g method, the new score of document D is computed as Note that we only penalize documents selected by either of the two heuristics.

We now discuss how to estimate negati ve model N given a set of non-rele vant documents N = f L 1 ; :::; L f g . We use the same estimation method as discussed in [19]. In particular , we assume that all non-rele vant documents are generated from a mixture of a unigram language model N (to generate non-rele vant information) and a background language model (to generate common words). Thus, the log-lik elihood of the sample N is where is a mixture parameter which controls the weight of the background model and the background model is estimated with ments), a standard EM algorithm can then be used to estimate pa-rameters p ( w j N ) . The result of the EM algorithm gives a discrim-inati ve negati ve model N which eliminates background noise.
SingleQuery method is to update original query with negati ve information. Since every term has a non-ne gati ve probablity in a query model, there is no natural way to update original queries with negati ve information. Ho we ver, given Equation (5), a Single-Query method can be deri ved after applying algebra transformation and ignoring constants that do not affect document ranking in the follo wing way The abo ve equation sho ws that the weight of term w is [ p ( w j probability in the negati ve topic model N . In this way, in some sense is the language modeling version of Rocchio. For consistence with vector space model, we use to replace and use [ p ( w j Q ) p ( w j N )] as the updated query model. For this query model, we use the equation abo ve to rerank all the documents in
U . Note that for SingleQuery method, we can not apply the two penalization heuristics.
MultiNe g adjusts the original rele vance scores with multiple neg-ative models. We use the same EM algorithm as SingleNe g to esti-mate a negati ve model i for each indi vidual negati ve document in N . We then obtain f negati ve models and combine them as
S combined ( Q; D ) = S ( Q; D ) S ( Q neg ; D )
In order to evaluate the effecti veness of negati ve feedback meth-ods, it is necessary to have test collections with suf cient dif cult topics. Ho we ver, TREC collections do not have man y naturally dif-cult queries. In this section, we describe two sampling strate gies to construct simulated test collections by con verting easy topics to dif cult topics.

In our problem formulation, a query is considered to be dif cult if none of the top 10 documents retrie ved by a retrie val model is rele vant. Thus, in order to con vert an easy query to a dif cult one, our main idea of sampling methods is to delete some rele vant doc-uments of an easy query and assume these documents do not exist in the collection so that all top 10 documents are non-rele vant. We now discuss two dif ferent ways to delete rele vant documents:
Minimum Deletion Method: Given a query and a rank ed doc-ument list for the query , we keep deleting the top rank ed rele vant document until none of the top 10 rank ed documents of the list is rele vant. We assume that the deleted rele vant documents do not exist in the collection.

Random Deletion Method: Given a query and all of its rel-evant documents, we randomly delete a rele vant document each time until none of the top 10 documents of the rank ed list is rele-vant. Again, we assume that the deleted documents do not exist in the collection.

In both methods, we keep deleting rele vant documents until none of top 10 rank ed documents is rele vant. Note that the constructed collections are dependent on retrie val models. After deletion, we obtain a new rank ed list whose top 10 documents are irrele vant for a query . We then use these 10 irrele vant documents for negati ve feedback to rerank the next 1000 documents in this new rank ed list.
To evaluate the effecti veness of negati ve feedback techniques, we construct our test collections based on two representati ve TREC data sets: ROB UST track and Web track data sets.
Our rst data set is from the ROB UST track of TREC 2004. It has about 528,000 news articles [17]. On average, each document has 467 terms. We use all the 249 queries as our base query set. This data set is denoted by  X R OB UST . X 
The second data set is the GO V data set used in the Web track of TREC 2003 and 2004. It is about 18 GB in size and contains 1 ; 247 ; 753 Web pages cra wled from the  X .go v X  domain in 2002. On average, each document has 1,094 terms. In our experiment, we only use the content of the pages for retrie val. There are 3 types of queries used in Web track: homepage nding, named page nding, and topic distillation. We use the queries with topic distillation type in both Web track 2003 and 2004. In total, we have 125 queries in our base set (50 from Web track 2003 and 75 from Web track 2004). We denote this data set by  X GO V. X 
For both data sets, preprocessing involv es only stemming but without remo ving any stopw ord. Since our goal is to study dif-cult queries, we construct dif ferent types of query sets from our base sets as follo ws.
The rst type of query set consists of those naturally dif cult queries. In this paper , we say that a query is a naturally dif cult query if its P@10=0, given a retrie val model.

For both language models (LM) and vector space models (VSM), we use their standard ranking functions to select their naturally dif-cult queries respecti vely . We rst optimize the parameters of for LM and k 1 and b for the VSM using the base set of queries on each data set. The optimal parameters are sho wn in Table 1. All these parameters are x ed in all the follo wing experiments. Using the optimal parameter setting, we then select those queries whose P@10=0 as our naturally dif cult queries. The row of QS0 in Ta-ble 2 sho ws the number of queries in this type of query sets.
Since there are not man y naturally dif cult queries, we further used the two deletion-based sampling methods to construct simu-lated dif cult queries from easy ones. In our experiments, we use two types of easy queries. The rst type consists of those queries whose P@10 satisfy 0 : 1 P@10 0 : 2 (QS12 in Table 2) and the second consists of those queries whose P@10 satisfy 0 : 4 0 : 6 (QS46 in Table 2). Again, all these queries are selected for the two retrie val models on the two data sets respecti vely .
The last type of query sets is the ALL query sets which are the union of the three types of query sets. Table 2 gives a summary of all the query sets used in our experiments.
Our experiment setup follo ws Section 3 to rerank the next un-seen 1000 documents. We use two sets of performance measures: (1) Mean Average Precision (MAP) and Geometric Mean Average Precision (GMAP), which serv e as good measures of the overall ranking accurac y. (2) Mean Reciprocal Rank (MRR) and Precision at 10 documents (P@10), which reect the utility for users who only read the very top rank ed documents. Figur e 1: The perf ormance of Rocchio feedback under differ -ent parameters.
In this section, we use the Rocchio method in VSM to sho w that existing rele vance feedback techniques do not work well if we only have negati ve information. The standard Rocchio method updates a query as where R is the set of rele vant feedback documents. We use the query sets of  X ALL  X  type. For any query , we rst obtain its original ranking list. Starting from the top of the ranking list, we search downw ard until we arri ve at a cutting point , before which we just nd 10 irrele vant documents. All the documents before the cut-ting points, including both rele vant and non-rele vant documents, are used in the Rocchio feedback. The updated query vectors are then used to rerank the next 1000 documents starting from the cut-ting points. In our experiments, we set = 1 : 0 and vary and . The results are sho wn in Figure 1. From this gure, we can see that if we have rele vant information, i.e., &gt; 0 ues can be impro ved dramatically . Ho we ver, when we do not have any rele vant information, i.e., = 0 , negati ve information always hurts MAP . This means that the existing rele vance feedback tech-niques are not effecti ve if only negati ve information is available for feedback, although the y are very effecti ve for positi ve feedback. This also sho ws that special techniques are needed for negati ve rel-evance feedback.
Using naturally dif cult query sets QS0, in this section, we study the effect of dif ferent negati ve feedback techniques. For both LM and VSM on the two data sets, we sho w their performance of the original ranking (OriginalRank) and the 5 negati ve feedback meth-ods: SingleQuery means the SingleQuery strate gy; SingleNe g1 and SingleNe g2 are the SingleNe g strate gy plus Heuristic 1 and Heuris-tic 2 respecti vely; MultiNe g1 and MultiNe g2 are the MultiNe g strate gy plus Heuristic 1 and Heuristic 2 respecti vely . We vary the parameters for each method: from 0.01 to 1 for SingleQuery , from 0.1 to 0.9 and from 50 to 1000 for SingleNe g and MultiNe g methods. In Table 3, we compare the optimal per -formance (selected according to GMAP measure) of all methods. From this table, we have the follo wing observ ations: (1) LM approaches usually work better than VSM approaches. On the ROB UST data, LM can impro ve the MAP from 0.0293 to 0.0363, 23.8% relati ve impro vement, but VSM can only impro ve from 0.0223 to 0.233, 4.4% relati ve impro vement. On the GO V data, LM approaches can signicantly impro ve over both Original-Rank and SingleQuery approaches, but VSM approaches can not consistently give impro vements. Table 4: Similarity between POS, NEG, MNEG lear ned from Gr oup1 and rele vant/irr ele vant documents in Gr oup2. (2) For LM approaches, MultiNe g always works better than Sin-gleQuery and SingleNe g. This sho ws that irrele vant documents may distract in dif ferent ways and do not form a coherent clus-ter . To verify this, we use the cutting point dened in Section 5.2.1 to form two groups of documents for each query: all documents before the cutting point form Group1 and the next 50 documents after the cutting point form Group2. We learn a positi ve (denoted as POS) and a negati ve language model (denoted as NEG) using the rele vant and non-rele vant documents in Group1. Using the exponential transform of negati ve KL-di vergence as the similarity measure, we calculate the average similarity between POS/NEG and rele vant/irrele vant documents of Group2. The average values over all queries are sho wn in Table 4. We can see that POS has a notably higher similarity to rele vant documents than to irrele vant documents in Group2, but NEG does not have a notably higher sim-ilarity to irrele vant than rele vant documents. In this table, we also sho w the results of multiple negati ve models (denoted as MNEG). Clearly , MNEG can distinguish between rele vant and irrele vant documents better than NEG, conrming that negati ve documents are more diverse and MultiNe g is more appropriate for negati ve feedback. (3) The results of VSM are mix ed, and MultiNe g can not yield notable impro vement on the GO V data. One possible reason is that the negati ve query vector generated using one single document in MultiNe g tends to over-emphasize rare terms due their high IDF values. In Table 5, we sho w two documents G37-07-3260432 and G43-41-3966440 in the GO V data set and their high-weight terms in the extracted negati ve query vectors. It is clear that VSM is bi-ased towards those rare words such as  X xxxxx X  and  X 4294 X , which mak es the computed negati ve vectors less powerful to push down those similar irrele vant documents. For LM, the extracted terms are much better . This means that LM is more powerful to pick up more meaningful terms from negati ve documents and thus works better on GO V data.

This may also explain why SingleNe g in VSM is generally more effecti ve than MultiNe g on the GO V data set: SingleNe g uses mul-tiple negati ve documents to compute the negati ve models. While the rare words may bias the negati ve model computed from a sin-gle negati ve document in MultiNe g, their weights are small in Sin-gleNe g since the y are not common in all the negati ve documents.
We have proposed two deletion-based sampling methods to mak e an easy query articially dif cult. In this section, we sho w the re-trie val results on simulated dif cult queries using ALL query sets. We only sho w the GMAP values in Table 6. For Random Deletion, we run it 10 times and the average performance values are reported here. In this table, we sho w the results of both deletion methods on both retrie val models and both data sets. Since Random Dele-tion deletes more rele vant documents for each query than Minimum Deletion, it is expected that its overall performance is much lower than that of Minimum Deletion. The relati ve performance of dif-ferent negati ve feedback methods is, howe ver, similar to what we observ ed on the naturally dif cult query sets, further conrming that the effecti veness of LM approaches and the MultiNe g strate gy.
To see whether a simulated test set generated using our sampling methods is as good as a test set with naturally dif cult queries for evaluating negati ve feedback, we use both to rank dif ferent negati ve feedback methods based on their retrie val accurac y (e.g., MAP) and compare the two rankings; if the y are highly correlated, it would indicate that the simulated test set can approximate a  X natural X  data set well.

Formally , assume that we have n negati ve retrie val functions. We can rank them based on their performance on the  X gold standard X  set (i.e., the naturally dif cult queries). This would be our  X gold standard ranking.  X  Similarly , we can use the simulated dif cult queries to rank all these retrie val functions. We then compute the correlation between these two ranking lists based on Kendall' s rank coef cient. Given two ranking lists r 1 and r 2 of n functions, the coef cient is dened as The range of the coef cient is between 1 and 1 . When ( r 0 , r 1 and r 2 are positi vely correlated. The lar ger the value, the higher the correlation. ( r 1 ; r 2 ) = 1 if r 1 and r 2 are exactly the same.
We construct the simulated dif cult queries using both Minimum and Random Deletion methods. Again we run the Random method 10 times and uses the average values to rank retrie val functions.
Our retrie val functions are from the 5 methods. For each method, we vary its parameter setting in a certain range. Each parameter setting will give us a dif ferent retrie val function. In total we have 110 retrie val functions.

Table 7 sho ws the Kendall' s correlation coef cients between the naturally dif cult queries QS0 and the simulated dif cult queries on ALL query sets using the two deletion methods. From this ta-ble, we can see that both deletion methods are positi vely correlated with the naturally dif cult queries. This conrm that our two dele-tion methods are reasonable to con vert an easy query to a dif cult one. Ov erall, Random Deletion is better than Minimum Deletion. Comparing two measures GMAP and MAP , we can see that the simulated dif cult queries are more consistent with the naturally dif cult queries on the GMAP measure. This indicates that GMAP is more appropriate as a measure on the simulated dif cult queries than MAP . Indeed, GMAP has been used in ROB UST track to eval-uate dif cult queries and this sho ws the reasonableness of our dele-tion methods to simulate dif cult queries.
In this section, we study the parameter sensiti vity . Due to space limit, we only sho w the results on ROB UST data set with the natu-rally dif cult query set QS0; other results are similar .
Figure 2 sho ws the impact of on the SingleQuery method for both LM and VSM. We can see that SingleQuery can not effecti vely use the negati ve feedback information and it is quite sensiti ve if is lar ger . Figure 3 sho ws the impact of score combination param-eter where we set = 200 . All methods have the same level of sensiti vities to value. Figure 4 sho ws the impact of the penaliza-tion scope parameter . It can be seen that SingleNe g1 and Multi-Ne g1 are very sensiti ve to this parameter , while SingleNe g2 and MultiNe g2 are more rob ust. These results conrm that Heuristic 2 is more stable than Heuristic 1 in general. Ev entually , when is lar ge enough, the performance of SingleNe g2 and MultiNe g2 will drop as we penalize more documents which are not very similar to negati ve models. Finally , we study the impact of the number of feedback documents in MultiNe g. We set f = 10 but we only use a subset of these 10 documents in negati ve feedback. The result is in Figure 5 and it sho ws that we can get more impro vement accord-ing to both MAP and GMAP if we use more documents in negati ve feedback. This means that our method can help more when a user accumulates more negati ve information.
Ne gati ve feedback is very important because it can help a user when search results are very poor . In this paper , we conducted a Figur e 5: Impact of the number of feedback documents in MultiNeg . systematic study of negati ve rele vance feedback techniques. We proposed a set of general strate gies for negati ve feedback and com-pared their instantiations in both vector space model and language modeling frame work. We also proposed two heuristics to increase the rob ustness of using negati ve feedback information. Experiment results sho w that modeling multiple negati ve models is more effec-tive than a single negati ve model and language model approaches are more effecti ve than vector space model approaches. Studying negati ve feedback needs a test set with suf cient dif cult queries. We further proposed two sampling methods to simulate dif cult queries using easy ones. Our experiments sho w that both sampling methods are effecti ve.

This work inspires several future directions. First, we can study a more principled way to model multiple negati ve models and use these multiple negati ve models to conduct constrained query ex-pansion, for example, avoiding terms which are in negati ve models. Second, we are interested in a learning frame work which can uti-lize both a little positi ve information (original queries) and a certain amount of negati ve information to learn a ranking function to help dif cult queries. Third, queries are dif cult due to dif ferent rea-sons. Identifying these reasons and customizing negati ve feedback strate gies would be much worth studying.
We thank the anon ymous revie wers for their valuable sugges-tions. This work is in part supported by the National Science Foun-dation under award numbers IIS-0347933 and IIS-0713581. [1] R. Attar and A. S. Fraenk el. Local feedback in full-te xt [2] C. Buckle y. Why current IR engines fail. In SIGIR'04 , pages [3] D. Carmel, E. Yom-T ov, A. Darlo w, and D. Pelle g. What [4] M. Dunlop. The effect of accessing non-matching documents [5] D. A. Ev ans and R. G. Lef ferts. Design and evaluation of the [6] D. Harman and C. Buckle y. SIGIR 2004 Workshop: RIA and [7] M. A. Hearst and J. O. Pedersen. Ree xamining the cluster [8] M. Iw ayama. Rele vance feedback with a small number of [9] J. D. Laf ferty and C. Zhai. Document language models, [10] S. Robertson and K. Sparck Jones. Rele vance weighting of [11] S. E. Robertson, S. Walk er, S. Jones, [12] J. J. Rocchio. Rele vance feedback in information retrie val. In [13] G. Salton and C. Buckle y. Impro ving retrie val performance [14] X. Shen, B. Tan, and C. Zhai. Conte xt-sensiti ve information [15] A. Singhal. Modern information retrie val: A brief overvie w. [16] A. Singhal, M. Mitra, and C. Buckle y. Learning routing [17] E. M. Voorhees. Draft: Ov ervie w of the trec 2005 rob ust [18] E. M. Voorhees. Ov ervie w of the trec 2004 rob ust retrie val [19] X. Wang, H. Fang, and C. Zhai. Impro ve retrie val accurac y [20] J. Xu and W. Croft. Query expansion using local and global [21] E. Yom-T ov, S. Fine, D. Carmel, and A. Darlo w. Learning to [22] C. Zhai and J. Laf ferty . Model-based feedback in the [23] C. Zhai and J. Laf ferty . A study of smoothing methods for
