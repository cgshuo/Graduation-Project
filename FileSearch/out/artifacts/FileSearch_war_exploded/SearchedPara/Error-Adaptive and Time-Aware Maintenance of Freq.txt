 With the emergence of data stream applications, data mining for data streams has attracted great attention from both researchers and practitioners. Among the mining tasks for stream data, maintaining frequency counts over data streams is a basic mining problem with a wide range of applications, such as web advertisement fraud detection and network flow identification[1][2]. A number of algorithms have been introduction to these algorithms is given in reference [2]. Most of these algorithms are designed to maintain a set of approximate frequency counts satisfying an error requirement within a theoretical memory bound, and they are mostly false-positive oriented. Usually the error bound is given by an end user. To satisfy this error bound, different algorithms use different methods to consume as less memory as possible. Among these algorithms, an algorithm called space-saving [2] uses an integrated analysis and experimental results show that this method achieves a better performance in terms of accuracy and memory usage compared to other algorithms, such as GroupTest [3], FREQUENT [4], CountSketch [7] and Probabilistic-InPlace [4]. However, after studying these existing algorithms, we have following observations:  X 
Timestamp information is ignored at processing each data arrival. Stream data are temporally ordered, fast changing, massive, and potentially infinite sequences of people are usually interested in recent changes of the data streams. However, as far as we know, all of these existing algorithms for approximating the frequency counts do not take this kind of information into account.  X 
Precision and recall may not be enough to measure the performance of an algorithm. Many algorithms use precision and recall as important measures to judge if an algorithm is good. However, precision and recall depend on minimum minsup or low k value in a skewed data stream, they are usually 1. From these two maintaining the frequency counts as a whole. For example, if we use 10000 counters to monitor frequency counts of items over a data stream with length of 100,000 and 10000 distinct items. More than 50% of the frequency counts maintained by space-saving are 1, and in the meantime they also have the highest estimation error among all the counts maintained, while the exact answer tells us precision and recall. 
In this paper we focus on addressing these two issues described above. We propose made in this paper:  X 
We propose to make use of time dimension information when designing the pruning strategy. In order to do that, we propose a method, called fractionization , to compress timestamp of each arrival for an item into existing count and error data. We also propose several methods to utilize this information to achieve better pruning result.  X 
We propose to use the sum of maintained error and the error of estimation error as well as the sum of all errors to measure the quality of mining algorithm. In order to improve the quality of mining results in terms of these measurements, we adaptively so that the error bound can be achieved and in the meantime a low maintained error can also be achieved.  X 
We develop and implement an algorithm named Adaptive to use error-adaptive pruning technique to maintain frequency counts over data streams. Comprehensive experimental studies indicate that this algorithm can achieve better performance.  X 
We design and implement two algorithms, SSTime and AdaTime, to extend the existing space-saving algorithm and the new algorithm Adaptive by taking the time dimension into consideration. Experimental results show that time information is effective in terms of improving the mining quality. 
The remainder of the paper is organized as follows. Section 2 describes how to keep and use time dimension, and give the description of algorithm SSTime . Section 3 describes the error-adaptive pruning technique, and presents two new algorithms measures for performance study and presents experimental results, and section 5 concludes the paper. summarizing the dynamic data stream. 2.1 Problem Definition among the k highest frequencies, where k is specified by user. 
For a data stream application, the exact solution of finding all of the frequent items or finding all of the top-k frequent items is usually impractical due to time and space limitation. Therefore, the problem becomes finding an approximate set of frequent items and top-k items. To solve this problem, except for the parameter minsup and k , an error rate,  X  , is also given by a user. With the relaxation of the original problem, the task of mining frequent items becomes finding all of frequent items whose estimated and their true counts is at most  X  N . Similarly, the task of finding top-k frequent items difference between the estimated counts and their true counts is also at most  X  N. 2.2 Fractionization: A Method to Keep Time Information Existing algorithms for mining frequent items in data streams can be categorized into two kinds of techniques: counter-based and sketch-based . Counter-based method use method. 
Due to the space limitation and the big size of the stream, usually only a subset of memory to keep frequency counts, then at any point of time, only m distinct items are monitoring. 
Almost all of the counter-based algorithms use the following method to maintain item X  X  frequency. If the newcome item is currently monitored, its frequency is increased. Otherwise, an item currently monitored is pruned to make room for the new item. Although these existing algorithms are different from each other in terms of applications, items in data stream are changing as time changes. For example, old frequent items may become infrequent as time goes on. Therefore, a straightforward way to use time information is that whenever pruning is required, among candidates, we choose old one instead of recent one to prune. But how can we judge which one is older than others? 
The answer to this question depends on how time information for every arrival of items is recorded. If we have enough space, it is easy to record time information. But in order to achieve high accuracy, we need to use as less memory as possible for each counter. Therefore, how to put time information into existing information that a counter keeps is important. 
Suppose for each counter we maintain three pieces of information for an item: key , method to save time information of each item arrival is called Fractionization , which means that we first transform the information of each item into a decimal fraction, and then save it as a decimal part of existing triple element such as error. In this case, we use float rather than integer to represent it. However, even by this way, we still cannot record every occurrence of an item. In order to save space, we sum all of time information of its occurrence, and then save it as a subpart of the error element. 
Now the problem becomes how to express the time information of an occurrence of an item. There are many ways to do that. A simple one is that we use the timestamp of the first item in the stream is 1, and second is 2, and so on. In this way, since the length of stream increases continually, the sum of timestamp may become Taking natural logarithm as an example, in order to transform the sum of timestamp should be greater than one. As a result, if we use natural logarithm computation, the time stamp of the first item in the stream could be 3. 
In sum, we could use the following formula (1) to record the time information of a monitored item ( e i , count i , error i ): 
Besides the linear sum of the timestamp information of an item X  X  each monitored arrival, we can also record the square sum of the timestamp information by a similar way as shown in formula (2). Here, we put the time information in the item element, occurrence order. For example, the timestamp of the first item in a stream is ln (3). 2.3 Algorithm: SSTime To show the effectiveness of using item X  X  time information, we integrate the time keeping and using method with the space-saving algorithm [2]. The algorithm called SSTime is outlined in Fig.1. This algorithm is similar to space-saving . There are two differences between them. record more information about time. When an item is pruned from the memory, i.e., it We can also record this information in the item that replaces it. The second difference is that when choosing the pruning item, SSTime takes time into consideration. Among all of items with the same ( count + error ), where error means the integer part of the item is old is not an easy job. In this algorithm, we use a straightforward method. The shown in formula (3). We can also use some complex method, which will be discussed in the next section. 
With the information maintained by this algorithm, at any point of time, a query method to fulfill these two kinds of queries is the same as given in space-saving , and two algorithms which will be described in the following sections. 3.1 Error-Adaptive Pruning Method As discussed in section 1, using the pruning method proposed in space-saving , most of the frequency counts maintained in memory have only one guaranteed frequency count, whereas they have the highest estimated error. In other words, most of them are this, we propose a new pruning method, called error-adaptive . 
The pruning method used in space-saving is that whenever an existing monitored method is that among these candidate items, some have very high guaranteed counts, and others have only one guaranteed count. Treating them equally during pruning will recall and precision. This method is shown in Definition 2. point, a new coming item in data stream cannot find a counter to monitor its frequency count. Let the current length of the stream is N, then this pruning point is called pruning point N. Definition 2. (error-adaptive pruning method) Suppose user-specified error rate is  X  , monitored item e i . The error-adaptive pruning method selects all of items e j satisfying both of the following conditions as candidate items: Using error-adaptive pruning method, we have the following lemmas. following equation (4) holds. monitored, it will replace one existing item. The counter for the existing item will be used to monitor the new arrival item. This counter X  X  original count and error will be new arrival is also recorded. Hence, at any time point, the summation of any counter X  X  count and error equals the number of item arrivals currently in data stream. Lemma 2. At any pruning point N , there is always at least one candidate item that can be found to prune. satisfying ( count + error )  X  N / m. The proof is by contradiction. Assume every item Lemma 1. Lemma 3. Using error-adaptive pruning method, the frequency count estimation error rate for any item is not greater than  X  .
 Proof. Items can be classified into two categories: items that are monitored currently, error is zero, which is obviously less than  X  . If it is monitored at the pruning point N For those not monitored, we regard its frequency count zero. Suppose it is last pruned at the pruning point N , then according to definition 2, before its pruning, the sum of than N  X  . Therefore, the lemma also holds for this case.

Using this error-adaptive pruning method for mining task given in section 2.1, the algorithm space-saving . In space-saving , at every pruning point, the error for the new coming item is overestimated as the minimum estimated count, which is min( count + error ). By our method, the error estimated is no less than min( count + error ), so it is also an overestimation. Therefore, there is only false positive among output frequency count. This is also demonstrated by comprehensive experimental study results. 
Based on this error-adaptive pruning method, we propose two algorithms, Adaptive and AdaTime , for finding frequent items and top-k frequent items. 3.2 Algorithm: Adaptive Adative is the algorithm we design for finding frequent items and top-k frequent items based on error-adaptive pruning method. It is depicted in Fig.2. 
In this algorithm we do not consider time information. Based on user-specified error rate  X  , we use m (=1/  X  ) counters to monitor items in stream D . When a new item arrives in the stream, if it is currently monitored, its count is increased by one (lines 5-6). If it is a pruning candidate, we delete it from the candidate set (line 7). If it is not Getcandidate (), is called to select candidate items from all of counters based on error-adaptive pruning method described in Definition 2 (lines 13-14). Then, one candidate item is randomly picked to prune and make its counter available to the new item (line from the candidates to prune instead of selecting pruning item from all of the counters each of them satisfies ( count + error )  X  N  X  . Suppose the current pruning point is M, 16-18). 
The function GetCandidate ( m , n ) is called to find all of the candidate items from m counters at pruning point n . This is done by traversing from counters with the value. Therefore, when traversing buckets from the one with the lowest estimated count, once this value is greater than n  X  , we could stop further traverse. 3.3 Algorithm: AdaTime To show the effect of the time informatio n to the error-adaptive pruning method, we propose another algorithm, AdaTime , which is outlined in Fig. 3. 
The major difference between algorithms Adaptive and AdaTime is shown in lines Here we introduce another way. Suppose the timestamp for the n th arrival is ln ( n+2 ), method introduced in section 2 to do that. In line 15, instead of randomly picking one which item is older, we can use the linear sum of the timestamps and square sum of new coming item. Due to the space limitation, we do not give the further detail of this method. The larger the distance is, the older the item is. Similar to line 7, in line 18, at the pruning point, time information is also recorded. 4.1 Measures In order to evaluate performance of an algorithm completely, besides the measures such as recall, precision, space, and time, we propose three other measures to evaluate the effectiveness of various pruning method. The second is the average absolute error of maintained counts , or mError in short, as shown in formula (5). 
The third is the average absolute error of maintained error , or eError in short, as shown in formula (6). 
We have implemented the three algorithms proposed in this paper in C language and run them on a Pentium IV 2GHz IBM Thinkpad laptop with 1.5G memory running Window 2003 Server system. For algorithm SSTime and AdaTime , when we implement them, we have tried several different methods to record and use time method as shown in Fig. 1. 
We use synthetic data generated by following a Zipf -like distribution [8]. 4.2 Varying the Data Skew In this set of experiments, we change the skew factor of the data stream, and measure the recall, precision, aError , mError , eError , and time. We fix the number of distinct 0.0001. We compare the performance of our algorithms with s pace-saving which proves to have better performance than other algorithms in [2], and is implemented to our best knowledge. Since we use the data structure as used in space-saving , the space used by our algorithms is similar to space-saving . We vary the skew factor from 0.5 to 2, and the results are shown in Fig. 4 and 5. AdaTime produce better error results than space-saving and SSTime . Furthermore, although it is hard to see from these figures, algorithm AdaTime is slightly better than Adaptive, and SSTime is slightly better than AdaTime . time information and selecting candidate based on time information take more time, it is not difficult to understand this result. The reason why SSTime is much slower than others is that at each pruning point, every item with the min( count + error ) is needed to scan and compare. 4.3 Varying the Query Parameters length of stream to be 10,000,000, the error rate to be 0.0001, and skew factor to be 1. We change two parameters, minsup and k , to see the recall, precision. Since this data set is one of those used in section 4.2, the other measures for this data set remain the same as given above. The results are depicted in Fig. 6. 
One can see from Fig. 6 (a) and (b), for low minsup , Adaptive and AdaTime have better recall and precision than space-saving and SSTime , whereas AdapTime is better than Adaptive and SSTime is a little better than space-saving . As the top-k query, the results for recall are the same as precision, so we do not put the figure here. Fig. 6(c) and (b). We study the problem of maintaining frequency counts for items over data streams in this paper. We propose to use time information when pruning items, and give a fractionization method to represent and record the time information without spending much space. We also propose a new pruning method, error-adaptive pruning , to improve maintenance accuracy as a whole. Using these two methods, we design and implement three algorithms, Adaptive , AdaTime, and SSTime , and conduct comprehensive experiments. Our experimental results show that time information can improve the maintenance accuracy, but needs more runtime. Our results also indicate that the new pruning method is effective for improving accuracy as a whole. 
