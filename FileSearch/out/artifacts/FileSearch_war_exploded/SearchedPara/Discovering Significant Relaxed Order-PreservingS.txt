 Mining order-preserving submatrix (OPSM) patterns has re-ceived much attention from researchers, since in many sci-entific applications, such as those involving gene expression data, it is natural to express the data in a matrix and also important to find the order-preserving submatrix patterns. However, most current work assumes the noise-free OPSM model and thus is not practical in many real situations when sample contamination exists.
 In this paper, we propose a relaxed OPSM model called ROPSM. The ROPSM model supports mining more rea-sonable noise-corrupted OPSM patterns than another well-known model called AOPC (approximate order-preserving cluster). While OPSM mining is known to be an NP-hard problem, mining ROPSM patterns is even a harder prob-lem. We propose a novel method called ROPSM-Growth to mine ROPSM patterns. Specifically, two pattern growing strategies, such as column-centric strategy and row-centric strategy, are presented, which are effective to grow the seed OPSMs into significant ROPSMs. An effective median-rank based method is also developed to discover the underlying true order of conditions involved in an ROPSM pattern. Our experiments on a biological dataset show that the ROPSM model better captures the characteristics of noise in gene expression data matrix compared to the AOPC model. Im-portantly, we find that our approach is able to detect more quality biologically significant patterns with comparable ef-ficiency with the counterparts of AOPC. Specifically, at least 26 . 6% (75 out of 282) of the patterns mined by our approach are strongly associated with more than 10 gene categories (high biological significance), which is 3 times better than that obtained from using the AOPC approach.
 H.2.8 [ Database Management ]: Database Applications X  data mining Algorithm, Experimentation, Performance biclustering, order-preserving submatrices, backbone order, relaxed order-preserving submatrics
The advent of DNA microarray technologies has greatly advanced the studies of gene expression. Gene expression data are usually arranged in a matrix, with each row cor-responding to one gene, each column to one condition, and each entry in the matrix representing the expression level of a gene under a specific condition. In the area of gene expres-sion analysis, an important research problem is to discover submatrix patterns in the gene expression matrix. We follow the convention in [3] and call this problem biclustering and the discovered patterns biclusters . A bicluster consists of a set of genes that have similar expression levels under a set of conditions. Based on different similarity functions, different bicluster models can be formulated. One typical example is the order-preserving submatrix model (or the OPSM for short), proposed by Ben-Dor et al. [1].
 Table 1: An example of order-preserving submatrix
An order-preserving submatrix consists of a subset of genes and a subset of experimental conditions such that the ex-pression levels of every gene induce the same linear order of the conditions. This linear order of the conditions repre-sents the consensus trend that the expression levels of all the genes in the subset follow. Considering the gene-condition matrix shown in Table 1, there is a submatrix ( P, Q )with bold font. For any gene g i in P , we order its expression levels under conditions in Q in ascending order and then re-place the values by their corresponding condition labels. We find that all the genes in P induce an identical linear order on Q , i.e., [ t 4 t 1 t 2 t 3 ]. Thus, the submatrix ( P, Q ) is an order-preserving submatrix.
While it has been shown in extensive research work that the adoption of the OPSM model promotes the detection of important biological associations in gene expression analysis [1, 12, 8, 7, 10], researchers also find that it may not be realistic to assume that, as required by the OPSM model, all the rows in the submatrix induce the same linear orders in applications. The reason is that, due to the instrumental limitations or measurement errors, it is inevitable that the data in gene expression matrix are corrupted by noise [1, 12]. In this case, the strict OPSM model prohibits the discovery of larger (usually more significant) but noise-contaminated OPSMs. Therefore, it is necessary to relax the strict OPSM model in order to allow more significant order-preserving submatrices to be discovered.
 Recently, Zhang et al. [12] proposed a relaxation to the OPSM model with the general idea as follows: a pre-specified fraction of rows in the bicluster are required to induce the same linear order of columns, and this linear order is called the core order of the bicluster, while every other row is only required to induce a similar order with the core order. Such a bicluster is called an approximate order preserving cluster (or AOPC for short). The core order is used to capture the consensus trend that all the rows in the AOPC follow. This relaxation has enabled the discovery of more biologically sig-nificant biclusters.
 In this paper, we propose a new bicluster model as follows. We assume there exactly exists a backbone order associated with a bicluster and require that all the rows in the biclus-ter be similar enough to the backbone order. In contrast to the AOPC model, our model removes the fraction re-quirement that a pre-specified fraction of the rows involved in an AOPC are required to induce exactly the same or-der. In fact, our relaxation is motivated by the observation that two valid AOPCs can be merged into a bigger bicluster which is biologically more significant, yet the bigger biclus-ter is not a valid AOPC pattern because it fails to satisfy the fraction requirement, as demonstrated by the example in Section 2.3. We call the proposed model the relaxed order-preserving submatrix (ROPSM for short). Our empirical studies show that our ROPSM model is more effective in finding significant biclusters including many that are missed by the AOPC model.

As it is already known that mining strict OPSMs is an NP-hard problem [1], mining ROPSMs, which is a more general problem, is even more difficult to resolve. There are also other challenges as follows. First, the OPSMs patterns hold some nice properties like anti-monotonicity, which can re-duce the search space when the OPSMs are mined. However such a property is no longer held by ROPSM patterns. Sec-ond, due to the presence of noise, it is very difficult to iden-tify the backbone order of the ROPSM patterns. The AOPC mining method assumes that the core order of an AOPC pat-tern is maintained by a fraction of rows, and is passed on through the merging process. However, this method cannot be used for mining ROPSM patterns, since we cannot expect that any one of the rows in the ROPSM should retain the backbone order of the pattern. Thus, we adopt a new strat-egy for mining ROPSM patterns, which takes seed patterns as input and expands them until maximal ROPSM patterns are reached.

In summary, the main contributions of this paper are as follows. 1. We propose a new relaxation model, called ROPSM , 2. We propose an ROPSM mining method, which starts 3. Extensive empirical studies have been conducted us-The organization of the rest of this paper is as follows. In Section 2, we first introduce some notations that are used throughout the paper, and then propose our new ROPSM model. Following that, a comparison with an existing noisy OPSM model is presented. In Section 3, an ROPSM mining method is introduced. Experiments on real gene expression datasets are presented in Section 4. In Section 5, related works are discussed. Finally, we conclude the paper in Sec-tion 6.
In this section, we first introduce some notations that are used throughout the paper. Then, we give formal defini-tions of our ROPSM model. A comparison with another relaxed OPSM model is also presented to further illustrate the modeling capabilities of ROPSM.
We denote a gene expression matrix with n genes and m conditions as M ( G, T ), where the gene set is given by G = { g 1 ,...,g n } and the condition set is given by T = { t 1 ,...,t m } . Given P  X  G and Q  X  T , M ( P, Q )iscalled a submatrix of M ( G, T ). We may use a lighter notation ( P, Q ) in subsequent discussion to mean M ( P, Q ) whenever no ambiguity arises.

Given a gene g i  X  G and a set of conditions Q , we order g  X  X  expression values under conditions in Q in ascending order and then replace the values by their corresponding condition labels. The resultant linear order of condition la-on Q , e.g., o ( g 1 ) Q =[ t 4 t 1 t 2 t 3 ] in Table 1. We denote by O ( P,Q ) the set of all o ( g i ) Q for g i  X  P and call O induced order set of the submatrix ( P, Q ). In particular, if all the orders in O ( P,Q ) are identical, we call the submatrix ( P, Q ) an order-preserving submatrix of M . For example, the submatrix ( { g 1 ,g 2 ,g 3 } , { t 1 ,t 2 ,t 3 ,t 4 } tern of the matrix shown in Table 1.
In this subsection, we define our ROPSM model, which tries to capture the characteristics of noisy OPSM patterns.
We adopt a similarity function defined based on the con-cept of longest common subsequence (or simply LCS), and the similarity function is formally defined as follows.
Definition 2.1. the LCS similarity. Given two orders o and o 2 , the LCS similarity between o 1 and o 2 , denoted as d
LCS ( o 1 ,o 2 ) , is defined as where | LCS ( o 1 ,o 2 ) | is the length of the longest common sub-sequence between o 1 and o 2 ,and T ( o i ) is the set of items involved in o i .

For example, given two orders o 1 =[ t 1 t 2 t 3 t 4 ] and o =[ t 2 t 1 t 3 t 4 ], the longest common subsequences between o 1 and o 2 are [ t 1 t 3 t 4 ] and [ t 2 t 3 t 4 both of which have the length of 3. Therefore, the LCS similarity between o 1 and o 2 is 3 4 . The LCS similarity and its variants are widely used in molecular biology [2], and can be computed by using dynamic programming in O ( n 2 ) time.
We now define our relaxed OPSM model, called ROPSM , as follows.

Definition 2.2. Relaxed Order-Preserving SubMa-trix (ROPSM) . Given a similarity threshold  X  , a subma-trix ( P, Q ) is an ROPSM if there exists a linear order  X  Q such that the LCS similarity between  X  Q and the induced order of every row in P is larger than or equal to  X  . The linear order  X  Q is called the backbone order of the ROPSM ( P, Q ) .

Since an ROPSM ( P, Q ) is always associated with a back-bone order  X  Q ,wealsodenotetheROPSMas( P, Q :  X  Q ). Consider the matrix shown in Table 1 again. Suppose that the entry ( g 2 ,t 4 ) in the matrix is corrupted and its value changes from 6 to 10, and assume that the similarity thresh-old  X  is set to be 0.75. Although the submatrix ( { g 1 ,g { t 1 ,t 2 ,t 3 ,t 4 } ) is not an OPSM, it is an ROPSM satisfying  X  , with [ t 4 t 1 t 2 t 3 ] as the backbone order. Note that OPSM is a special case of ROPSM, i.e., ROPSM satisfying the threshold  X  =1.

An ROPSM ( P, Q :  X  Q ) is said to be maximal if there does not exist any other ROPSM ( P ,Q :  X  Q ) such that P  X  P , Q  X  Q ,and  X  Q is a subsequence of  X  Q .
 Having defined the ROPSM model, we then formulate the ROPSM mining problem as follows:
Definition 2.3. ROPSM mining problem . Given a data matrix M and a similarity threshold  X  , the goal is to mine from M the maximal ROPSMs that satisfy the thresh-old  X  .
The AOPC model was proposed in [12]. A submatrix ( P, Q ) is called an AOPC if the following two requirements are satisfied. First, at least  X  s  X | P | rows induce an identical linear order of Q . This order is called the core order of the AOPC and is denoted as  X  Q . Second, the linear orders induced from the other (1  X   X  s )  X | P | rows share with  X  a longest common subsequence with the length of at least  X   X  X  Q | .

While both the ROPSM model and the AOPC model adopt a similarity threshold, i.e.,  X  and  X  c , to restrict the amount of noise existing in the pattern, our ROPSM model does not assume the core order and removes the first require-ment of the AOPC model. Such relaxation is not trivial, in the sense that more significant biclusters, which are not discovered based on the AOPC model, now becomes valid ROPSMs. This point was testified in our experiments, and can be seen in the following example illustrated in Table 2.
Since the similar dataset is used, we implement the AOPC mining method and mine the AOPCs by setting  X  s and  X  c the same values as used in [12], i.e.,  X  s =0 . 2 and  X  c 0 . 6. Among the set of AOPCs that are generated, there are two AOPCs, respectively denoted as M 1 ( P 1 ,Q 1 ) and M 2 ( P 2 ,Q 2 ), whose information is listed in Table 2. These two AOPCs contain the same set of columns, i.e., Q 1 = Q , and the core orders of the two AOPCs, i.e.,  X  Q 1 and  X 
Q 2 , are also the same. In AOPC M 1 , 30 out of 121 rows induce identical orders as the core order, while 30 out of 98 rows in AOPC M 2 induce identical orders as the core order. We call this portion of rows maintaining rows ( main rows for short). Then, we try to merge these two AOPCs into a bigger submatrix, denoted as M ( P, Q ), such that P = P 1  X  P 2 and Q = Q 1  X  Q 2 . The merged submatrix M contains 7 columns with the same core order as  X  Q 1 and  X 
Q 2 , and 159 rows with 30 maintaining rows. M is not a valid AOPC because the fraction of maintaining rows, i.e., 30 / 159, is smaller than the pre-specified threshold  X  s However, when we check the biological significance of the two AOPCs and the merged submatrix M using the widely-used p -value measure [10, 6], the two AOPCs have 12 and 8 strongly associated gene categories (with p -value  X  10 [12]) respectively, and have the smallest p -value of 10 and 10  X  15 with their most strongly associated categories. In contrast, the biological significance of the merged submatrix M is strongly associated with 15 categories and has the smallest p -value of 10  X  24 with the most strongly associated category, which means that the merged submatrix is more biologically significant than the two AOPCs. 1
Under the ROPSM model, the merged submatrix M is an ROPSM pattern that satisfies the similarity threshold  X  =0 . 6, although it is not a valid AOPC. In this section, we propose an efficient algorithm called OPSM-Growth for mining ROPSM patterns.
 Notably, given a data matrix M ( G, T ), for a set of columns Q with Q  X  T and a linear order  X  Q , we can simply check every row in G to see if it supports  X  Q or not. A row in G is said to support  X  Q if its induced order on Q has the LCS similarity with  X  Q no smaller than  X  . Let P be the set of rows that supports  X  Q . We can see that P can be determin-istically identified and that ( P, Q ) is an ROPSM with the backbone order  X  Q . Thus, it appears that a straightforward way of mining ROPSM patterns is to exhaustively search all possible linear orders of combinations of columns in T .How-ever, as discussed in [1], the cost of such an exhaustive search
Detailed information about this example is available at http://www.cse.ust.hk/~fang/aopc-example.html . is prohibitively expensive and is infeasible when the number of columns in T is larger than or equal to 4. On the other hand, we should avoid searching some linear orders which may finally lead to insignificant patterns . While we aim to discover patterns that show strong biological significance, a consensus is that, considering the size of the patterns, for a fixed number of columns (or rows), a larger number of rows (or columns) usually leads to more significance [7]. There-fore, when mining ROPSM patterns, we focus on searching some linear orders that likely lead to patterns with larger size.

The backbone orders of OPSM patterns are good candi-dates for such linear orders. Given an OPSM ( P, Q :  X  Q and the similarity threshold  X  , we can always expand the OPSM by ( 1  X   X  1) | Q | columns, denoted as the column set  X  Q , and get a submatrix ( P, Q  X   X  Q ). We then get a lin-ear order of ( Q  X   X  Q ), denoted as  X  Q  X   X  Q , such that  X  is a subsequence of  X  Q  X   X  Q . Since the induced order of ev-ery row in the submatrix ( P, Q  X   X  Q ) shares with  X  Q  X   X  longest common subsequence with the length of at least | Q the LCS similarity between them is at least  X  . Therefore, the submatix ( P, Q  X   X  Q ) is an ROPSM with  X  Q  X   X  Q as the backbone order.

On the other hand, intuitively, a significant ROPSM very likely contains at least one order-preserving submatrix. Con-sidering a noise-contaminated data matrix, it is reasonable to assume the probability that an entry in the matrix is corrupted by noise is less than 50%. Otherwise, we can-not distinguish whether the mined patterns really show the association among the involved genes or they are simply formed because of noise. Moreover, noise does not always affect the formation of OPSM patterns. For example, in Table 1, even if the entry ( g 2 ,t 4 ) changes to 7 due to some noise, the induced order of row g 2 on { t 1 ,t 2 ,t 3 ,t the same, i.e., [ t 4 t 1 t 2 t 3 ]. Thus, the submatrix ( { fore, only the noise that causes the entries of the matrix deviating from respecting the original induced order of the corresponding row may finally affect the discovery of OPSM patterns. The probability that such noise exists, however, should be even smaller. Thus, it is reasonable to assume that a significant ROPSM pattern may still contain a non-contaminated order-preserving submatrix .

Motivated by the above ideas, we propose an ROPSM mining method as follows: we first mine a set of OPSMs that satisfy some size thresholds; then we take those OPSMs as seeds and expand them until maximal ROPSMs patterns are reached. We adopt an existing OPSM mining method, called the OPC-Tree [8], to mine seed OPSMs. The OPC-Tree method was proposed to exhaustively mine all OPSMs that satisfy two size thresholds r min and c min , which means that the OPSMs contain at least r min rows and at least c min columns. The general idea of the OPC-Tree method is that, for ev-ery induced linear order of columns, its subsequences that may lead to a valid OPSM are enumerated and organized in a compact prefix-sharing tree structure. Then, the size thresholds are used to further prune the tree. Finally, a lin-ear scan over the tree is conducted to output all OPSMs that satisfy the thresholds.

Here, we adapt this method to mining all maximal OPSMs that satisfy the two size thresholds r min and c min . Note that, although our ROPSM mining method also takes OPSMs as an input, which seems to be similar to the AOPC mining method, the resultant ROPSMs mined by our method are hardly influenced by the settings of the thresholds r min c min , which however have a big effect on the performance of the AOPC mining method.

The AOPC mining method takes the set of OPSMs as an input, and merges all possible pairs of OPSMs that may re-sult in a valid AOPC pattern. Therefore, those rows that appear in some final AOPC patterns should definitely ap-pear in some initial OPSMs. However, it may happen that the setting of the thresholds r min and c min exclude the dis-covery of some OPSM patterns, which contain correlated rows to a certain significant AOPC pattern. These rows will fail to appear in the AOPC pattern as the final result. To avoid missing promising rows of significant patterns, we search for ROPSMs by expansion with the remaining part of the matrix being probed in an efficient way.
In this subsection, we introduce the OPSM-Growth algo-rithm, which grows the seed OPSMs into ROPSM patterns. Given an ROPSM (initially an OPSM), the OPSM-Growth algorithm can expand it column wise and row wise. We re-spectively implement the columm-wise and row-wise expan-sion as Col-Expand and Row-Expand procedures. The Col-Expand procedure always takes the best column (in terms of some criterion) to expand the current ROPSM (or OPSM) pattern. The Row-Expand procedure simply scans the re-maining rows that are not included in the current pattern, and expand the pattern by those rows which have high enough LCS similarity with its backbone order. When the current pattern is expanded by a new column, the backbone order of the pattern needs to be updated accordingly, while the backbone order does not change during row-wise expansion.
We can grow an OPSM pattern by calling the Col-Expand and Row-Expand procedures in different order, which leads to different pattern growing strategies. Also, different meth-ods can be adopted to update the backbone order during column-wise expansion. Next, we introduce the techniques adopted by OPSM-Growth algorithm that handles pattern growing and backbone order updating.
Combining the Col-Expand and Row-Expand procedures in different order leads to different pattern growing strate-gies. Here we discuss two representative strategies: column-centric strategy and row-centric strategy . a. Column-centric Strategy b. Row-centric Strategy } )
Note that both the column-centric strategy and the row-centric strategy guarantee that the resultant ROPSMs are maximal.
In the OPSM-Growth algorithm, when a new column is taken to expand the current pattern, the backbone order of the current pattern should be updated accordingly. We propose a median-rank based method for updating the back-bone order, and call the method MedRank Updating .
The concept of median rank is effective for reducing the effect of noise on rankings, which is adopted in [4, 5] for producing the consensus (linear or bucket) order of a set of permutations. We define a median-rank based score, called MedScore (denoted as S m ), and determine the backbone or-der of a pattern according to the MedScores of its columns. The definition of MedScore is as follows.
 Definition 3.1. Median Rank Based Score (MedScore) Given an input matrix M ( G, T ) , and an ROPSM pattern ( P, Q ) , the MedScore of a column t  X  Q , denoted as S m is computed as: where o ( g i ) T is the induced order of row g i on the column set T , r ( o ( g i ) T ,t ) is the rank of column t in o ( g i median() returns the median value of the set of ranks.
We use the following example to further illustrate the computation of MedScore . Suppose there is an input ma-trix ( G, T ) with G = { g 1 ,...,g 5 } and T = { t 1 ,...,t its induced order set is listed in the second column of Ta-ble 3. Given a pattern ( P, Q ) with P = { g 1 ,g 2 ,g 3 Q = { t 1 ,t 2 ,t 3 ,t 4 } , we compute the MedScore of column t based on ( P, Q ). We first get t 2  X  X  ranks in order o ( o
T , and o the MedScore of t 2 , i.e., S m ( t 2 ), is the median of the four ranks which is 2.5. The function median () returns the aver-age of the two median ranks when the number of ranks are even.
 Table 3: Illustration of MedScore computation g g g g g
Having computed the MedScores of all the columns in Q , the columns are ordered in increasing order of their Med-Score , and the resultant order of the columns is the MedRank backbone order of the above pattern ( P, Q ). For exam-ple, the MedRank backbone order of the pattern ( P, Q )is [ t (1) t 2 (2 . 5) t 3 (4) t 4 (6)], where the number in the bracket is the MedScore of the column. ( P, Q )isanROPSM pattern satisfying  X  =0 . 75. We expect that the MedRank backbone order could well tolerate the influence of noise and express the underlying true order of the columns in the pat-tern.

Note that, the MedScore of columns are computed based on their ranks in the induced orders on T instead of the ranks in the induced orders on Q . In this way, the gaps between the ranks of columns in the induced orders on T can be kept, so that the ordering relationship among columns may be more distinguishable.

Let us consider the example in Table 3 again. The ranks of column t 3 in the induced orders on T are respectively 5, 5, 3, 2, and thus its MedScore is S m ( t 3 ) = 4. Since S m ( t 2 ) is smaller than S m ( t 3 ), t 2 is ranked higher than t in the MedRank backbone order. The ordering relationship between t 2 and t 3 in the backbone order intuitively well respects the truth that t 2 has high ranks in the majority of the induced orders of ( G, T ) while t 3 gets comparatively low ranks. However, when we check the ranks of t 2 and t 3 in the induced orders on Q as listed in the third column of Table 3, both t 2 and t 3 have ranks of 2 , 2 , 3 , 3 and thus they have the same median rank of 2.5. The ordering relationship between t and t 3 cannot be determined in this way.

In addition to the advantage that computing the MedScore using the ranks in induced orders on T can better keep the ordering relationship between columns, another benefit of computing MedScore in this way is that, those ranks are fixed throughout the expansion process. Thus, an inverted index can be built for fetching the ranks of items efficiently.
Algorithm 1 shows the details of the MedRank-Updating method. Given an ROPSM ( P, Q :  X  Q ) and a new column Algorithm 1: MedRank-Updating
Input : ROPSM ( P, Q :  X  Q ) with  X  Q =[ t 1  X  X  X  t | Q | ];
Output :  X  Q  X  X  t } -the MedRank backbone order of
Variable : O ( P,T ) -induced orders from ( P, T ) 1. V 2. S m ( t ) equals to the median of ranks in V ( P,T ) ( t ); 3.  X 
S m ( t i )  X  S m ( t ) with 1  X  i  X  k and S m ( t i ) &gt;S k +1  X  i  X | Q | ; Algorithm 2: ROW-MED-Growth Input :aset U of OPSMs; the similarity threshold  X  Output :aset V of ROPSMs
Variable :  X   X  -current smallest LCS similarity 1. for ( P, Q :  X  Q )  X  U do 2. if ( P, Q ) is a submatrix of an ROPSM in V then 3. discard ( P, Q :  X  Q ); 4. else 5. while Col-Expand(( P, Q :  X  Q ),  X  ,  X   X  )= succeed 6. Row-Expand(( P, Q :  X  Q ),  X   X  ); 7. end 8. end 9. end t ,the MedScore of t , i.e., S m ( t ) is firstly computed (Lines 1 and 2). Then, the column t is inserted into  X  Q such that the MedScore s of all the columns before t are no larger than S m ( t ) (Line 3).
 Time Complexity. The time complexity of the MedRank-Updating method is analyzed as follows. Suppose the final ROPSM pattens contain at most k rows and s columns, each call of the MedRank-Updating method costs O ( k log k ) time, which is actually the cost of computing the MedScore of newly added columns.
Having illustrated the pattern growing strategies and the backbone updating method, we now introduce the OPSM-Growth algorithm. Taking different growing strategies, we devise two variants of the OPSM-Growth algorithm, called COL-MED-Growth and ROW-MED-Growth . The ROW-MED-Growth method adopts the row-centric pattern growing strat-egy, and its details are shown in Algorithm 2. The two pro-cedures Col-Expand and Row-Expand are shown in Algorithm 3 and 4, respectively.
 First, let us see the procedure Col-Expand (Algorithm 3). Suppose ( P, Q :  X  Q ) is the current ROPSM pattern. Every remaining column t j in ( T  X  Q ) is taken to update the cur-rent backbone order  X  Q (Line 2). Then, both the smallest LCS similarity  X  between the updated backbone order and induced orders and the sum of the LCS similarities S LCS are recorded (Lines 3 and 4). Among all the columns in ( T  X  Q ), column t k which has the largest  X  value is picked; if there are ties, the column with larger S LCS wins (Line 6). We set  X   X  as the  X  value of column t k (Line 7). If  X   X  is no Algorithm 3: Col-Expand
Input : an ROPSM ( P, Q :  X  Q ); the similarity threshold
Output : succeed if ( P, Q ) is updated by a new column;
Variable : O ( P,Q ) -the induced order set of ( P, Q ) 1. for t j  X  T  X  Q do 2.  X  Q  X  X  t j } = MedRank-Updating(( P, Q :  X  Q ) ,t j ); 3.  X  ( Q, t j ) = min { d LCS (  X  Q  X  X  t j } ,o i ) ,  X  o 4. S LCS ( Q, t j )= 5. end 6. Pick t k such that  X  ( Q, t k ) = max {  X  ( Q, t j ) , ties are broken with larger S LCS ; 7.  X   X  =  X  ( Q, t 8. if  X   X   X   X  then 9. Update  X  Q to  X  Q  X  X  t k } ; Q  X  X  t k } X  Q ; Algorithm 4: Row-Expand
Input : an ROPSM ( P, Q :  X  Q ); current smallest LCS
Variable : o ( g j ) Q -the inducd orders of row g j on Q 1.  X  P =  X  ; 2. for row g j in G  X  P do 4.  X  P  X  X  g j } X   X  P ; 5. end 6. end 7. P  X   X  P  X  P ; smaller than the similarity threshold  X  , column t k is chosen to update the current pattern ( P, Q ) as well as the back-bone order, and the Col-Expand procedure returns succeed ; otherwise, it returns fail (Lines 8 to 11).

The Row-Expand procedure (Algorithm 4) takes the cur-rent ROPSM pattern ( P, Q :  X  Q ) and the current smallest LCS similarity  X   X  , which is passed from Col-Expand , as input parameters. It conducts a linear scan of the remaining rows in the input matrix. Those rows whose induced order has the LCS similarity with  X  Q no smaller than  X   X  are chosen to expand the current ROPSM pattern (Lines 2 to 7).
We now introduce the ROW-MED-Growth algorithm as shown in Algorithm 2. For every seed OPSM ( P, Q :  X  Q ), we first detect whether it is a submatrix of some ROPSM pattern that is mined. If it is, we discard this OPSM and continue with a new OPSM (Lines 2 to 4). We call this step early pruning , which will be explained in more detail later. Then, an OPSM that passes the early pruning is expanded with the row-centric growing strategy being adopted. That is, Col-Expand and Row-Expand are alternately called until a maximal ROPSM is reached (Lines 5 to 7). Finally the newly generated ROPSM is added to V (Line 10).

We omit the details of the algorithm COL-MED-Growth , i.e., the other variant of OPSM-Growth . Generally speaking, the COL-MED-Growth algorithm takes the column-centric pattern growing strategy. That is, for every seed OPSM that passes the early pruning step, the Col-Expand procedure is repetitively called until no more columns can be added. Finally, one round of Row-Expand is performed.
 Early Pruning. We observe that, no matter which vari-ant of the OPSM-Growth algorithm is taken, it is possible that similar OPSMs may finally grow into the same ROPSM pattern. In order to avoid useless expansion that may pro-duce an ROPSM already found, we adopt an early pruning step as follows. Before expanding an OPSM, we first check whether it is a submatrix of some mined ROPSM. The early pruning step is quite efficient, since we only need to check if the backbone order of a given OPSM is a subsequence of the backbone order of some ROPSM. If it is, the OPSM is surely the submatrix of the ROPSM.

It might be argued that some seed OPSMs that will finally lead to new ROPSM patterns may also get pruned during the early pruning step. However, such new ROPSM pat-terns will be very similar to some other ROPSM patterns, which may, for example, catch similar functional associa-tions among a similar set of genes. Considering real appli-cations, such similar patterns may not be very informative even they are significant. Therefore, we still prefer to adopt the strict early pruning step in order to avoid generating many similar patterns.
 Time Complexity. Suppose the input matrix contains n rows and m columns, and the mined ROPSM patterns contain at most p rows and q columns. The running time of the Col-Expand procedure is O ( m ( q log q + q 2 )+ m ) where O ( q log q ) is spent updating the MedRank backbone order and O ( q 2 ) is spent computing the LCS similarity between orders. The running time of the Row-Expand procedure is O ( nq 2 ). Then the time complexity of the ROW-MED-Growth method is O ( p ( m + n ) q 2 ). The COL-MED-Growth method calls the procedure Col-Expand p times and calls Row-Expand only once, and therefore its time complexity is O (( pm + n ) q 2 ).
 Table 4: Data matrix ( G, T ) with G = { g 1 ,g 2 ,g 3 ,g and T = { t 1 ,t 2 ,t 3 ,t 4 ,t 5 }
Next, we further illustrate the ROW-MED-Growth algo-rithm in the following example. Suppose there is an input matrix ( G, T ) as shown in Table 4, and the induced orders of the rows are listed in the last column of the table. The similarity threshold  X  is 0.75. We start with a seed OPSM ( P, Q ) where P = { g 2 ,g 3 ,g 4 } and Q = { t 1 ,t 2 ,t tial backbone order is  X  Q =[ t 1 (2) t 2 (4) t 3 (5)] where the numbers in the bracket are the MedScore s of the columns. First, ( P, Q ) are expanded column-wise. For the remain-ing two columns t 4 and t 5 , we find that expanding ( P, Q ) with either one will lead to the  X   X  value of 0.75. How-ever, when t 4 is taken for expansion, the sum of LCS simi-larities (i.e., S LCS value) is larger. Therefore, the OPSM is expanded with column t 4 , i.e., Q = Q  X  X  t 4 } . Since the MedScore of t 4 is 1, the backbone order is updated to  X  Q =[ t 4 (1) t 1 (2) t 2 (4) t 3 (5)]. Then, the current ROPSM is expanded row-wise. We find that the induced order of g 1 on Q , i.e., o ( g 1 ) Q ,is[ t 4 t 2 t 3 t 1 the LCS similarity between o ( g 1 ) Q and  X  Q is 0.75 which is no smaller than  X   X  . Thus, the current ROPSM is expanded by row g 1 , and turns out to be ( { g 1 ,g 2 ,g 3 ,g 4 } , { We try to expand the ROPSM column-wise again, and find that the remaining column t 5 cannot be taken for expan-sion. We therefore stop with the final ROPSM pattern as ( {
In this section, we study the performance of the two vari-ants of our ROPSM mining algorithm, i.e., COL-MED-Growth and ROW-MED-Growth , through a series of experiments. We also compare them with the AOPC mining algorithm in terms of the biological significance of the mined patterns. All the experiments are conducted on a Macbook Pro with 2.53GHZ CPU and 4G memory.

The AOPC mining method takes a set of OPSMs as an input, and merges all possible pairs of OPSMs that may re-sult in a valid AOPC pattern. The validity of the AOPCs is evaluated using the two thresholds  X  s and  X  c . We respec-tively set  X  s and  X  c to be 0.2 and 0.6, which are the same as those adopted by the experiments in [12]. To improve the efficiency of the AOPC mining method, the input OPSM set can be divided into groups. The AOPC mining algorithm runs on every group, and the resultant AOPCs produced by each running are gradually combined. Referring to [12], we run the AOPC mining method by respectively setting the initial number of groups to be 1 and 64, and we denote the AOPC mining method with these two different settings respectively as AOPC-1 and AOPC-64.

The dataset we use is a real gene expression data set  X  the yeast cell cycle data from [11], which contains the expression levels of 771 regulated genes across 18 time points. We use the tool Gene Ontology Term Finder (GO-TermFinder) 2 to validate the biological significance of the mined ROPSMs. The GO-TermFinder tool computes the p -values between the mined patterns and known gene categories. A smaller p -value indicates a stronger association between the patterns and the category. In the experiments, for a particular pat-tern (ROPSM, AOPC, and OPSM), we only count those categories the pattern strongly associates with. We took the same p -value threshold, i.e., 10  X  9 , for strong associa-tion as the experiments in [12]. That is, a pattern is said to be strongly associated with a known gene category if the p -value between the pattern and the category is less than 10 We first adopt the OPC-Tree method to mine all maximal OPSMs satisfying the thresholds l min = 60 and s min =5. There are totally 595 OPSMs having been mined, which form the input set of OPSMs for both the OPSM-Growth algorithm and the AOPC mining method.
In the first set of experiments, we study the effectiveness of the COL-MED-Growth and ROW-MED-Growth methods as the similarity threshold  X  changes. Intuitively the  X  value should be larger than 0.5, and thus we respectively set  X  to be 0 . 6, 0 . 7, and 0 . 8. http://search.cpan.org/dist/GO-TermFinder/ OPSM-Growth algorithm Figure 1(a) shows the running time of the two methods, Figure 1(b) shows the number of ROPSM patterns being mined, and Figure 1(c) shows the amount of time that is spent in finding a single pattern, which we call unit time . We can see from Figure 1(a) that, the running time of both methods increases as  X  becomes smaller. The reason is that when more noise is allowed to exist in the ROPSM patterns, more time is needed for each seed OPSM to grow into a bigger ROPSM. On the other hand, when  X  is smaller, a seed OPSM tends to grow into an ROPSM with larger size, which is accordingly more likely to cover other seed OPSMs and prune them during the early pruning step. Therefore, less ROPSMs are finally mined when  X  is smaller, as shown in Figure 1(b).
 Comparing these two methods, we can see that COL-MED-Growth is more efficient in terms of the running time, and the advantage is more significant when  X  is small. The rea-son is that the row-centric growing strategy needs to scan the remaining rows (in Row-Expand ) repetitively during the expansion. Although one scan only takes linear time, it is still time-consuming since the number of rows contained in the gene expression matrix is much larger than the number of columns. Considering the number of mined ROPSMs, we can see that ROW-MED-Growth mines slightly more pat-terns. This is because the column-centric expansion tends to mine patterns with more columns, and thus the seed OPSMs are more likely to be pruned by those previously mined ROPSM patterns. Therefore, less ROPSMs can be finally mined by COL-MED-Growth .

The efficiency and scalability of the COL-MED-Growth are more apparent in Figure 1(c), which shows the unit time of the two methods under differnt  X  values. When  X  is 0.6, the COL-MED-Growth method on average spends about 0.53 seconds to find one ROPSM pattern, while the ROW-MED-Growth method needs 1.18 seconds to find one ROPSM pattern, which is more than two times longer.
We conduct the second set of experiments to compare the biological significance of the ROPSMs that are mined by the ROW-MED-Growth method and the COL-MED-Growth method. The experiment results are shown in Figure 2, where the x-axis of the two histograms are the number of cat-egories that an ROPSM strongly associates with, and each bar corresponds to a combination of the ROPSM mining method and the  X  value. For each method and under ev-ery  X  value, we respectively count the number of ROPSMs that strongly associate with more than a certain number of categories (e.g., 12 , 11 ,... ). The histogram in (a) shows the statistics of the number of significant ROPSMs, and the histogram in (b) shows the statistics of the fraction of sig-nificant ROPSMs.

If we consider the ROPSMs that strongly associate with more than 10 categories, we can see that both methods mine a larger percentage (67 . 9% and 70 . 0%, respectively) of such ROPSMs when  X  is 0.6, which means that setting  X  to be 0.6 better captures the characteristics of noisy OPSM patterns in this data set. However, the total number of ROPSMs mined when  X  is 0.6 is smaller, as already illustrated in the previous subsection. Both methods mine the most absolute number (98 and 102, respectively) of such ROPSMs when  X  is 0.7.
 When comparing the COL-MED-Growth method and the ROW-MED-Growth method, we find that, generally, ROW-MED-Growth mines a larger percentage of the significant pat-terns when  X  is 0.6; while COL-MED-Growth mines more per-centage of significant patterns when  X  is 0.8. The results im-ply that the column-centric expansion performs better when less noise is allowed to exist in the ROPSM patterns while the row-centric expansion performs better when more noise is allowed.
The last set of experiments is conducted to compare the biological significance of the patterns (ROPSM, AOPC, or OPSM) respectively mined by our OPSM-Growth algorithm, the AOPC mining method, and the OPC-Tree method. For the AOPCs, we list the statistical records of AOPCs mined by both AOPC-1 and AOPC-64. The AOPC-64 method runs much faster than AOPC-1, although several significant patterns that strongly associate with more than 12 cate-gories fail to be mined.

Note that our OPSM-Growth algorithm and the AOPC mining method aim at mining different kinds of noisy OPSM patterns, i.e., ROPSMs and AOPCs. To make the results of these two algorithms comparable in some sense, we impose a possible requirement, that is, the amount of time spent in mining one pattern should be roughly the same. Accord-ing to this requirement, we choose the COL-MED-Growth method for comparison with  X  set to 0.8. The experiment results are listed in Table 5.

The COL-MED-Growth method spends 5.78 seconds in find-ing 282 ROPSMs, and thus mining a single ROPSM pattern costs 0.0205 second. In comparison, the AOPC-64 method only spends 0.0167 seconds in finding an AOPC. Although the unit time cost for finding an ROPSM is slightly longer than the unit time cost for finding an AOPC, more number of significant patterns have been found by our COL-MED-Growth method. For example, considering the patterns that strongly associate with more than 10 gene categories, 75 out of 282 (26.60%) ROPSMs mined by COL-MED-Growth are such patterns. In contrast, neither AOPC-1 nor AOPC-64 mines more than 20 (or more than 8.56%) AOPCs that ful-fill such level of significance. Actually, more significant num-ber of patterns are mined by our COL-MED-Growth methods considering any level of significance. It should also be noted that COL-MED-Growth with  X  to be 0.8 is never the best set-ting for mining significant patterns. If we consider the trade-off between the unit time and the quality of the mined pat-tern, COL-MED-Growth with  X  set to be 0.7 spends 0.0733 seconds in finding one ROPSM which is about 4.3 times longer than the unit time cost by AOPC-64. However, the number of significant ROPSM patterns surely exceeds 4.4 times that of significant AOPC patterns at the same level, especially when those patterns strongly associate with 9 or more categories are considered.

The last column of Table 5 lists the statistical records of the OPSMs that are taken as the input of our OPSM-Growth algorithm. We can see that even less number (or percentage) of OPSMs, i.e., 16 (or 2.69%) strongly associate with more than 10 categories. However, starting from these OPSM patterns with much lower level of significance, our OPSM-Growth algorithm can mine considerably more signif-icant patterns.
The concept of biclustering was first introduced to dis-cover submatrix patterns (biclusters) in gene expression data matrix by Chen and Church [3]. Madeira et al. [9] clas-sify four major types of biclusters: constant-value bicluster, constant-row (or -column) bicluster, coherent-value biclus-ter, and coherent-evolution bicluster. The order-preserving submatrix (OPSM) is a typical type of coherent-evolution bicluster, which was defined by Ben-Dor et al. [1].
While the OPSM model is shown to be biologically sig-nificant, mining OPSM patterns is very challenging and the problem is NP-hard. Therefore, Ben-Dor et al. first pro-posed a model-based method that keeps a limited number of partial models which are smaller OPSMs for possible fur-ther expansion, and then expand them into full OPSMs. Their method however is designed to identify only a single pattern, and the significance of the pattern is very sensitive to the selection of partial models. Liu et al. later proposed an OPSM mining method that mines multiple OPSMs at the same time. Their method adopts a tree structure that organizes all the candidate OPSMs, with some pruning tech-niques being also applied. However, the computation cost is still large, especially when the number of columns increases. Gao et al. X  X  KiWi framework [7] also adopts a tree struc-ture to store all necessary information for searching twig OPSMs, which is characterized by containing a large num-ber of columns and a few rows.

Ben-Dor et al. also noticed that the OPSM model is too strict with the presence of noise, and thus they developed a probabilistic noise model under the assumption of uni-formly random noises. Zhang et al. later proposed an in-tuitive relaxation called the approximate order-preserving cluster (AOPC) [12]. The AOPC model only requires a pre-specified fraction of rows in the bicluster to induce the same linear order of columns, while the remaining rows only need to be similar enough. Based on the AOPC model, they also proposed an AOPC mining method that merges pairs of smaller AOPCs (initially OPSMs) into bigger ones in a greedy manner. Our ROPSM model further relaxes the AOPC model and demonstrates much better effectiveness in mining quality patterns.

Chen and Church X  X  algorithm (CC X  X  algorithm for short) [3], which similarly adopts column-wise and row-wise expan-sion or deletion, is designed for finding approximate constant-value and constant-row (or column) biclusters. CC X  X  algo-rithm adopts a two-phase strategy. First rows and columns are iteratively removed from the original input matrix until a valid pattern is reached. The pattern discovered in the first phase can be regarded as a seed, and it is then iteratively expanded by previously removed columns and rows until a maximal valid pattern is reached. This two-phase method, however, is very time-consuming. Besides, if more patterns are needed, those previously discovered patterns are masked by random values to avoid finding identical patterns, which accordingly influences the quality of the following discov-ered patterns. In comparison, we take maximal OPSMs as seeds, which is shown to be more likely to obtain significant patterns.
In this paper, we study the problem of mining noisy OPSM patterns and develop a new relaxed OPSM model called the ROPSM model. In contrast to the strict OPSM model, ROPSM only requires that each gene in the bicluster in-duces a linear order that is sufficiently similar with respect to the backbone order of the biclusters. Thus, our proposed model is able to better capture the biological fact that corre-lated genes usually induce similar but not exactly the same orders on the same set of conditions.
 We propose a new method called ROPSM-Growth to mine ROPSM patterns, which uses an effective median rank based method to generate (or approximate) the backbone order and hence find the ROPSMs. Our experiment on a biolog-ical dataset shows that the ROPSM model better captures the characteristics of noise in gene expression data matrix compared to the AOPC model. Importantly, compared with the more effective version of AOPC, our ROPSM mining method mines more quality biologically significant patterns and needs far less time to process the mining than its coun-terpart. When compared with the more efficient version of AOPC, we need comparable mining time but gain far more significant patterns. Thus, our OPSM-Growth method achieves a better balance between the efficiency in process-ing the mining and the quality of the mined patterns.
This work is partially supported by China NSF Grant 60970043 and HKUST RGC Grant 618509. We would like to thank KDD reviewers for giving us insightful comments. [1] A. Ben-Dor, B. Chor, R. Karp, and Z. Yakhini. [2] L. Bergroth, H. Hakonen, and T. Raita. A survey of [3] Y. Cheng and G. M. Church. Biclustering of [4] R. Fagin, R. Kumar, and D. Sivakumar. Efficient [5] J. Feng, Q. Fang, and W. Ng. Discovering bucket [6] R. A. Fisher. On the interpretation of X 2 from [7] B. J. Gao, O. L. Griffith, M. Ester, and S. J. M. [8] J. Liu and W. Wang. Op-cluster: Clustering by [9] S. C. Madeira and A. L. Oliveira. Biclustering [10] A. Preli  X  c, S. Bleuler, P. Zimmermann, A. Wille, and [11] P. T. Spellman, G. Sherlock, M. Q. Zhang, and et al. [12] M. Zhang, W. Wang, and J. Liu. Mining approximate
