 There is a growing body of experimental evidence consistent with the idea that animals are some-how able to represent, manipulate and, ultimately, make decisions based on, probability distribu-tions. While still unproven, this idea has obvious appeal to theorists as a principled way in which to understand neural computation. A key question is how such Bayesian computations could be per-formed by neural networks. Several authors have proposed models addressing aspects of this issue [15, 10, 9, 19, 2, 3, 16, 4, 11, 18, 17, 7, 6, 8], but as yet, there is no conclusive experimental evidence in favour of any one and the question remains open.
 Here we focus on the problem of tracking a randomly moving, one-dimensional stimulus in a noisy environment. We develop a neural network whose dynamics can be shown to approximate those of a one-dimensional Kalman filter, the Bayesian model when all the distributions are Gaussian. Where the approximation breaks down, for large prediction errors, the network performs something akin to outlier or change detection and this  X  X ailure X  suggests ways in which the network can be extended to deal with more complex, non-Gaussian distributions over multiple dimensions.
 Our approach rests on the modification of the line attractor network of Zhang [26]. In particular, we make three changes to Zhang X  X  network, modifying the activation rule, the weights and the inputs in such a way that the network X  X  dynamics map exactly onto those of the Kalman filter when the prediction error is small. Crucially, these modifications result in a network that is no longer a line attractor and thus no longer suffers from many of the limitations of these networks. For clarity of exposition and to define notation, we briefly review the equations behind the one-dimensional Kalman filter. In particular, we focus on tracking the true location of an object, x ( t ) , mean Gaussian random noise with standard deviation  X  z ( t ) , and a model of its dynamics, x ( t + 1) = Kalman filter X  X  estimate of the position,  X  x ( t ) , can be computed via the following three equations dates the model X  X  uncertainty,  X   X  x ( t + 1) , in its estimate; and equation 3 updates the model X  X  estimate The network is a modification of Zhang X  X  line attractor model of head direction cells [26]. We use rate neurons and describe the state of the network at time t with the membrane potential vector, u ( t ) , where each component of u ( t ) denotes the membrane potential of a single neuron. In discrete time, the update equation is then where w scales the strength of the weights, J is the connectivity matrix, f [  X  ] is the activation rule that maps membrane potential onto firing rate, and I ( t + 1) is the input at time t + 1 . As in [26], we set J = J sym +  X  ( t ) J asym such that the the connections are made up of a mixture of symmetric, J sym , and asymmetric components, J asym (defined as spatial derivative of J sym ), with mixing strength  X  ( t ) that can vary over time. Although the results presented here do not depend strongly on the exact forms of J sym and J asym , for concreteness we use the following expressions where N is the number of neurons in the network and  X  w , K w and c are constants that determine the width and excitatory and inhibitory connection strengths respectively.
 To approximate the Kalman filter, the activation function must implement divisive inhibition [14, 13] where [ u ] + denotes recitification of u ;  X  determines the strength of the divisive feedback and S determines the gain when there is no previous activity in the network.
 When w = 1 ,  X  ( t ) = 0 and I ( t ) = 0 , the network is a line attractor over a wide range of K w ,  X  w , c , S and  X  , having a continuum of fixed points (as N  X   X  ). Each fixed point has the same shape, taking the form of a smooth membrane potential profile, U ( x ) = J sym f [ U ( x )] , centered at location, x , in the network.
 When  X  ( t ) 6 = 0 , the bump of activity can be made to move over time (without losing its shape) [26] and hence, so long as  X  ( t ) = v ( t ) , implement the prediction step of the Kalman filter (equation this configuration, the network can already implement the first step of the Kalman filter through its recurrent connectivity. The next two steps, equations 2 and 3, however, remain inaccessible as the network has no way of encoding uncertainty and it is unclear how it will deal with external inputs. In this section we outline how the neural network dynamics can be mapped onto those of a Kalman filter. In the interests of space we focus only on the main points of the derivation, leaving the full working to the supplementary material. Our approach is to analyze the network in terms of U , which, for clarity, we define here to be the fixed point membrane potential profile of the network when w = 1 ,  X  ( t ) = 0 , I ( t ) = 0 , S = S 0 and  X  =  X  0 . Thus, the results described here are independent of the exact form of U so long as it is a smooth, non-uniform profile over the network.
 We begin by making the assumption that both the input, I ( t ) , and the network membrane potential, u ( t ) , take the form of scaled versions U , with the former encoding the noisy observations, z ( t ) , and the latter encoding the network X  X  estimate of position,  X  x ( t ) , i.e., Substituting this ansatz for membrane potential into the left hand side of equation 4 gives and into the right hand side of equation 4 gives For the ansatz to be self-consistent we require that RHS can be written in the same form as LHS . We now show that this is the case.
 As in the previous section, the recurrent input, implements the prediction step of the Kalman filter, which, after a little algebra (see supplementary material), allows us to write With the variable C defined as where I = P i [ U i ( X  x ( t ))] + .
 If we now suppose that the prediction error [ z ( t + 1)  X   X  x ( t + 1)] is small, then we can linearize around the prediction,  X  x ( t + 1) , to get (see supplementary material) which is of the same form as equation 8 and thus the ansatz holds. More specifically, equating terms in equations 8 and 12, we can write down expressions for  X  ( t + 1) and  X  x ( t + 1) which, if we define w such that are identical to equations 2 and 3 so long as Thus the network dynamics, when the prediction error is small, map directly onto the Kalman filter equations. This is our main result. Figure 1: Comparison of noiseless network dynamics with dynamics of the Kalman Filter for small prediction errors. 4.1 Implications Reciprocal code for uncertainty in input and estimate Equation 16a provides a link between the strength of activity in the network and the overall uncertainty in the estimate of the Kalman filter,  X   X  ( t ) , with uncertainty decreasing as the activity increases. A similar relation is also implied for the uncertainty in the observations,  X  z ( t ) , where equation 16b suggests that this should be reflected in the magnitude of the input, A ( t ) . Interestingly, such a scaling, without a corresponding narrowing of tuning curves, is seen in the brain [20, 5, 2].
 Code for velocity signal As with Zhang X  X  line attractor network [26], the mean of the velocity signal, v ( t ) is encoded into the recurrent connections of the network, with the degree of asymmetry in the weights,  X  ( t ) , proportional to the speed. Such hard coding of the velocity signal represents a limitation of the model, as we would like to be able to deal with arbitrary, time varying speeds. However, this kind of change could be implemented by pre-synaptic inhibition [24] or by using a  X  X ouble-ring X  network similar to [25].
 Equation 16c implies that the variance of the velocity signal,  X  v ( t ) , is encoded in the strength of the divisive feedback,  X  (assuming constant S ). This is very different from Zhang X  X  model, that has no concept of uncertainty and is also very different from the traditional view of divisive inhibition that sees it as a mechanism for gain control [14, 13].
 The network is no longer a line attractor This can be seen by considering the fixed point values equation 13 gives values for these fixed points as This second solution is exactly zero when w satisfies equation 15, hence the network only has one fixed point corresponding to the all zero state and is not a line attractor. This is a key result as it removes all of the constraints required for line attractor dynamics such as infinite precision in the weights and lack of noise in the network and thus the network is much more biologically plausible. 4.2 An example In figure 1 we demonstrate the ability of the network to approximate the dynamics of a one-dimensional Kalman filter. The input, shown in figure 1A, is a noiseless bump of current centered the random walk dynamics,  X  v ( t ) = 0 . 2 . In accordance with equation 16b, the height of each bump is scaled by 1 / X  z ( t ) 2 .
 In figure 1B we plot the output activity of the network over time. Darker shades correspond to higher firing rates. We assume that the network gets the correct velocity signal, i.e.  X  ( t ) = v ( t ) and  X  is set such that equation 16c holds. The other parameters are set to K w = 1 ,  X  w = 0 . 2 , c = 0 . 05 , S = S 0 = 1 and  X  0 = 1 which gives I = 5 . 47 . As can be seen from the plot, the amount of activity in the network steadily grows from zero over time to an asymptotic value, corresponding to the network X  X  increasing certainty in its predictions. The position of the bump of activity in the network is also much less jittery than the input bump, reflecting a certain amount of smoothing. In figure 1C we compare the positions of the input bumps (gray dots) with the position of the network bump (black line) and the output of the equivalent Kalman filter (red line). The network clearly tracks the Kalman filter estimate extremely well. The same is true for the network X  X  estimate of the uncertainty, computed as 1 / p  X  ( t ) and shown as the black line in figure 1D, which tracks the Kalman filter uncertainty (red line) almost exactly. We now consider the effect of noise on the ability of the network to implement a Kalman filter. In particular we consider noise in the input signal, which for this simple one layer network is equivalent to having noise in the update equation. For brevity, we only present the main results along with the results of simulations, leaving more detailed analysis to the supplementary material.
 Specifically, we consider input signals where the only source of noise is in the input current i.e. there is no additional jitter in the position of the bump as there was in the noiseless case, thus we write where ( t ) is some noise vector. The main effect of the noise is that it perturbs the effective position of the input bump. This can be modeled by extracting the maximum likelihood estimate of the input position given the noisy input and then using this position as the input to the equivalent Kalman filter. Because of the noise, this extracted position is not, in general, the same as the noiseless input position and for zero mean Gaussian noise with covariance  X  , the variance of the perturbation,  X  z ( t ) , is approximately given by Now, for the network to approximate a Kalman filter, equation 16b must hold which means that we require the magnitude of the covariance matrix to scale in proportion to the strength of the input that is found all over the brain.
 In figure 2 we demonstrate the ability of the network to approximate a Kalman filter. In panel A we show the input current which is a moving bump of activity corrupted by independent Gaussian noise of standard deviation  X  noise = 0 . 23 , or about two thirds of the maximum height of the fixed point bump, U . This is a high noise setting and it is hard to see the bump location by eye. The network dramatically cleans up this input signal (figure 2B) and the output activity, although still noisy, reflects the position of the underlying stimulus much more faithfully than the input. (Note that the colour scales in A and B are different).
 In panel C we compare the position of the output bump in the network (black line) with that of the equivalent Kalman filter. To do this we first fit the noisy input bump at each time to obtain input positions z ( t ) shown as gray dots. Then using  X  z = 2 . 23 computed using equation 19 we can compute the estimates of the equivalent Kalman filter (thick red line). which closely match those of the network (black line). Similarly, there is good agreement between the two estimates of the uncertainty,  X   X  x ( t ) , panel D (black line -network, red line -Kalman filter). 5.1 Performance of the network as a function of noise magnitude The noise not only affects the position of the input bump but also, in a slightly more subtle manner, causes a gradual decline in the ability of the network to emulate a Kalman filter. The reason for this (outlined in more detail in the supplementary material) is that the output bump scale factor, where we plot the steady state value of  X  (for constant input strength, A ( t ) ) as a function of  X  noise . The average results of simulations on 100 neurons are shown as the red dots, while the black line represents the results of the theory in the supplementary material.
 The reason for the decline in  X  as  X  noise goes up is that, because of the rectifying non-linearity in the activation rule, increasing  X  noise increases the amount of noisy activity in the network. Because of inhibition (both divisive and subtractive) in the network, this  X  X oisy activity X  competes with the bump activity and decreases it -thus reducing  X  .
 This decrease in  X  results in a change in the Kalman gain of the network, by equation 14, making it different from that of the equivalent Kalman filter, thus degrading the network X  X  performance. We quantify this difference in figure 3B where we plot the root mean squared error (in units of neural position) between the network and the equivalent Kalman filter as a function of  X  noise . As before, the results of simulations are shown as red dots and the theory (outlined in the supplementary material) is the black line. To give some sense for the scale on this plot, the horizontal blue line corresponds to the maximum height of the (noise free) input bump. Thus we may conclude that the performance of the network and the theory are robust up to fairly large values of  X  noise . We now consider the dynamics of the network when the prediction error is large. By large we mean that the prediction error is greater than the width of the bump of activity in the network. Such a big discrepancy could be caused by an outlier or a changepoint, i.e. a sustained large and abrupt change in the input position at a random time. In the interests of space we focus only on the latter case and such an input, with a changepoint at t = 50 , is shown in figure 4A.
 In figure 4B we show the network X  X  response to this stimulus. As before, prior to the change, there is a single bump of activity whose position approximates that of a Kalman filter. However, after the changepoint, the network maintains two bumps of activity for several time steps. One at the original position, that shrinks over time and essentially predicts where the input would be if the change had not occurred, and a second, that grows over time, at the location of the input after the changepoint. Thus in the period immediately after the changepoint, the network can be thought of as encoding two separate and competing hypotheses about the position of the stimulus, one corresponding to the case where no change has occurred, and the other, the case where a change occurred at t = 50 . In figure 4C we compare the position of the bump(s) in the network (black dots whose size reflects the size of each bump) to the output from the Kalman filter (red line). Before the changepoint, the two agree well, but after the change, the Kalman filter becomes suboptimal, taking a long time to move to the new position. The network, however, by maintaining two hypotheses reacts much better. Finally, in figure 4D we plot the scale factor,  X  i ( t ) , of each bump as computed from the simulations (black dots) and from the approximate analytic solution described in the supplementary material (red line for bump at 30 , blue line for bump at 80 ). As can be seen, there is good agreement between theory and simulation, with the largest discrepancy occurring for small values of the scale factor. Thus, when confronted with a changepoint, the network no longer approximates a Kalman filter and instead maintains two competing hypotheses in a way that is qualitatively similar to that of the run-length distribution in [1]. This is an extremely interesting result and hints at ways in which more complex distributions may be encoded in these type of networks. 7.1 Relation to previous work Of the papers mentioned in the introduction, two are of particular relevance to the current work. attractors. Although this work, at first glance, seems similar to what is presented here, there are several major differences, the main one being that our network is not a line attractor at all, while the results in [8] rely on this property. Also, in [8], the Kalman gain is changed manually, where as in our case it adjusts automatically (equations 13 and 14), and the form of non-linearity is different. Probabilistic population coding [16, 4] is more closely related to model presented here. Combined with divisive normalization, these networks can implement a Kalman filter exactly, while the model presented here can  X  X nly X  approximate one. While this may seem like a limitation of our network, we see it as an advantage as the breakdown of the approximation leads to a more robust response to outliers and changepoints than a pure Kalman filter. 7.2 Extension beyond one-dimensional Gaussians A major limitation of the current model is that it only applies to one-dimensional Gaussian tracking -clearly an unreasonable restriction for the brain. One possible way around this limitation is hinted at by the response of the network in the changepoint case where we saw two, largely independent bumps of activity in the network. This ability to encode multiple  X  X articles X  in the network may allow networks of this kind to implement something like the dynamics of a particle filter [12] that can approximate the inference process for non-linear and non-Gaussian systems. Such a possibility is an intriguing idea for future work. 7.3 Experimental predictions The model makes at least two easily testable predictions about the response of head direction cells Assuming that all bearing cues can indeed be eliminated, by setting A ( t ) = 0 in equation 13, we expect the activity of the neurons to fall off as 1 /t and that the shape of the tuning curves will remain approximately constant. Note that this prediction is vastly different from the behaviour of a line attractor, where we would not expect the level of activity to fall off at all in the dark. Another, slightly more ambitious experiment would involve perturbing the reliability of one of the landmark cues. In particular, one could imagine a training phase, where the position of one landmark is jittered over time, such that each time the rat encounters it it is at a slightly different heading. In the test case, all other, reliable, landmark cues would be removed and the response of head direction cells measured in response to presentation of the unreliable cue alone. The prediction of the model is that this would reduce the strength of the input, A , which in turn reduces the level of activity in the head direction cells,  X  . In particular, if  X  z is the jitter of the unreliable landmark, then we expect  X  to scale as 1 / X  2 z . This prediction is very different from that of a line attractor which would predict a constant level of activity regardless of the reliability of the landmark cues. In this paper we have introduced a novel neural network model whose dynamics map directly onto those of a one-dimensional Kalman filter when the prediction error is small. This property is robust to noise and when the prediction error is large, such as for changepoints, the output of the network diverges from that of the Kalman filter, but in a way that is both interesting and useful. Finally, the model makes two easily testable experimental predictions about head direction cells.
 Acknowledgements We would like to thank the anonymous reviewers for their very helpful comments on this work.
