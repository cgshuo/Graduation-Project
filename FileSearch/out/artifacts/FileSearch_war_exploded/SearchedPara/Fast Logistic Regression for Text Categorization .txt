 A common representation used in text categorization is the bag of words model (aka. unigram model). Learning with this particular representation involves typically some pre-processing, e.g. stopwords-removal, stemming. This results in one explicit tokenization of the corpus. In this work, we introduce a logistic regression approach where learning in-volves automatic tokenization. This allows us to weaken the a-priori required knowledge about the corpus and results in a tokenization with variable-length (word or character) n-grams as basic tokens. We accomplish this by solving lo-gistic regression using gradient ascent in the space of all n-grams. We show that this can be done very efficiently using a branch and bound approach which chooses the maximum gradient ascent direction projected onto a single dimension (i.e., candidate feature). Although the space is very large, our method allows us to investigate variable-length n-gram learning. We demonstrate the efficiency of our approach compared to state-of-the-art classifiers used for text catego-rization such as cyclic coordinate descent logistic regression and support vector machines.
 H.3 [ Information Storage and Retrieval ]: Content Anal-ysis and Indexing; I.2.6 [ Artificial Intelligence ]: Learning Algorithms, Performance, Experimentation Fast Logistic Regression, N-gram Learning, Text Catego-rization, Variable-Length N-grams
The standard bag of words representation is widely used in text categorization as an explicit tokenization of the training text, before employing learning algorithms. Typically, some language dependent pre-processing is employed, such as stop words removal or stemming. Furthermore, a feature selec-tion step [32] is often crucial for computational efficiency and generalization. Such feature-engineering often requires detailed knowledge about the language of the text to be cat-egorized. In practice, this results in a lot of tuning of the classifiers in order to find the right unigram features.
However, there are important text classification tasks for which the initial unigram bag-of-words representation does not capture the rich facets of the problem, even if the learner itself is very powerful [13, 18, 23, 27, 34].

Examples are: email categorization, sentiment polarity mining in product or movie reviews, subjectivity versus ob-jectivity mining of given texts, authorship attribution, user classification in social networks, and others. For instance, opinion mining often needs to consider entire phrases such as  X  . . . [This president did] not meet our expectations . . .  X , and classification in social communities may want to consider ti-tles of music songs that a person likes, which are usually phrases.

For the above applications, more complex features are needed, like word n-grams or even natural-language parse trees resulting in an even more involved tuning procedure. Kudo et al. [22] observed that the performance with n-grams did not differ much from the quality achievable by using deep NLP (natural-language processing) techniques, which would be orders of magnitude more expensive anyway.
 In this paper, we focus on n-gram sequences as features. We consider both word-level n-grams to capture phrases, and character-level n-grams to capture morphological vari-ation (stemming, transcription from non-Latin alphabets, misspelling, etc.).

Introducing n-grams as features of a classification model confronts the learner with a combinatorial-explosion prob-lem and a quality-efficiency trade-off. Simply including all n-grams up to some maximum length, say 3 or 4, leads to extremely high-dimensional feature spaces. Although many learners can cope reasonably well with large but sparse in-put spaces (e.g., [9, 17, 35]), their learning cost is at least linear in the number of features that are present in the train-ing data. Here, high-accuracy classification implies high training cost; conversely, a conservatively bounded set of n-grams like 2-grams often leads to merely mediocre classi-fication quality. An alternative approach is to pre-process text corpora to identify interesting n-grams by various forms of co-occurrence statistics or frequent-itemset mining [3]. However, this kind of feature engineering also entails high training-time costs, which would prevent it from being used in environments that require frequent re-training (e.g., in spam mail detection). This may be mitigated, to some ex-tent, by active learning (e.g., [20]), but this in turn puts the burden on the users by requiring a potentially large amount of human attention.

The bottom line of these considerations is that all prior methods are strongly limited in their ability to reconcile ex-pressive n-gram-aware feature spaces with fast training pro-cedures. This paper presents a new learning algorithm that is able to work with the entire space of (unbounded) n-grams as features, but automatically selects a compact set of most valuable n-grams for its final model.
Our solution, coined SLR (for Structured Logistic Regres-sion ), incorporates the best n-gram features, for variable-length n , into the feature space while staying highly effi-cient in its training procedure. To this end, we develop a coordinate-wise gradient ascent technique for maximizing the logistic regression likelihood of the training data. Our method exploits the inherent structure of the n-gram fea-ture space in order to automatically provide a compact set of highly discriminative n-gram features. Instead of com-puting the gradient value at each coordinate (dimension) corresponding to a possible n-gram feature, we search for the n-gram feature which gives the highest value of the gra-dient in a given iteration. The vector found this way is non-orthogonal to the full gradient vector, thus guarantee-ing that it is a good direction to follow in order to maximize the objective function.

To determine the feature with the best gradient value as fast as possible, we derive a theoretical bound which quanti-fies the X  X oodness X  X f the gradient for each n-gram candidate given its length-( n  X  1) prefix. This way we can timely decide whether it is worthwhile advancing the search in a particu-lar part of the search space. The effect is that we can prune large parts of the search space, resulting in a practically vi-able method even for large n . The result of our learning algorithm is a sparse linear model learned in the space of all possible n-grams in the training data.

We present experiments that compare our SLR method against the state-of-the-art classifiers BBR (a logistic regres-sion method) [9] and SV M perf [17]. These opponents are widely viewed as the best known methods for text classifi-cation, with fast training procedures. We study a variety of configurations for three different real-life datasets: the op-ponents can employ n-grams, with different choices of maxi-mum n , and are tuned for each setting. The F1 measure for our method is comparable to that of the best opponent. In terms of training run-time, SLR is more than one order of magnitude faster than its opponents.

To the best of our knowledge, SLR is the first method that can incorporate variable-length n-grams into the learning of advanced text classifiers, without any noticeable penalty on the size of the feature space and computational cost of the training.
Recent advances in efficient, regularized learning algo-rithms, such as SVM [17] and sparse logistic regression [9] have reduced the need for explicitly modifying the input fea-ture space, by better coping with large feature spaces and still providing very good predictive models. These methods still scale linearly with the feature space size and therefore are usually employed with the unigram bag of words rep-resentation, rather than the much richer feature space of all (word or character) n-grams in the training text. As a side effect of this efficiency aspect, most text categorization approaches fix the basic token of the text representation at the word level, rather than at the character level. This has the effect of potentially losing some of the robustness of the learned predictive models, since the character-level tokenization may better capture several facets of language use. For example, learning with variable or unrestricted-length character n-grams could better capture spelling mis-takes, spam characteristics (punctuation, etc.) or sub-words (implicit stemming) and phrasal features. Furthermore, the sometimes difficult problem of defining word-like segments in Asian language text could be avoided. Other benefits of using variable-length character n-grams could come from the more robust statistics captured by substrings of the text.
Some existing learning approaches can work with charac-ter sequences rather than bag of words, for example Markov chain models [8, 28], which are generative approaches, or SVM with string kernel [29], a discriminative approach. Markov chain models can be in fixed order/memory or vari-able order/memory [25, 34]. The Markov chain models in fixed order n are usually called n-gram language models [10, 28]. Recently, [27] tried character-level n-gram mod-eling for text classification, but in order to achieve decent performance one needs to choose an appropriate order n and employ good smoothing techniques [27, 34]. Markov chain models in variable order adjust the memory length according to the context, hence they are much more flex-ible than fixed order Markov chain models. The amnesic probabilistic automata (aka PST -prediction suffix trees) [5], text compression [2] methods such as PPM (prediction by partial matching) and PPM* [4] belong to the family of variable order Markov models. However, previous work has repeatedly shown that generative approaches are gener-ally outperformed by discriminative approaches (e.g. SVM) for word-based text categorization [6, 16, 31, 34]. For string-based (e.g. character-level n-gram) categorization, the num-ber of distinct substrings in a large corpus becomes pro-hibitively large, thus preventing the straightforward appli-cation of most discriminative approaches. SVM with string kernel is a discriminative approach that can perform string-based text categorization. However, SVM with string kernel has not become as popular as the word-based kernel SVM for text classification tasks, due to efficiency and classifi-cation performance reasons [24, 34]. Recent work [34] has advocated the usage of an efficient feature selection step for selecting a subset of character-level n-gram features based on a suffix tree algorithm, followed by learning an SVM clas-sifier. This again disconnects the feature selection step from the actual learning algorithm, which is undesirable (the com-bined process of feature selection followed by a learning al-gorithm has no clear statistical foundation [9]) and could be avoided by employing efficient classifiers that can do the feature selection on-the-fly as part of the learning process.
For maximum likelihood logistic regression, the most com-mon optimization approach in statistical software is the mul-tidimensional Newton-Raphson method and its variants [26]. Newton algorithms have the advantage of converging in very few iterations. For high-dimensional problems such as text categorization, however, Newton algorithms have the seri-ous disadvantage of requiring O ( d 2 ) memory, where d is the number of model parameters. A variety of alternate optimization approaches have therefore been explored for maximum likelihood logistic regression, and for regularized (Maximum A Posteriori) logistic regression. Some of these algorithms, such as limited memory BFGS [26], conjugate gradient [26], and hybrids of conjugate gradient with other methods [19], compute the gradient of the objective function at each step. This requires only O ( d ) memory (in addition to the data itself). Efron et al. [7] describe a new class of  X  X east angle X  algorithms for lasso linear regression and re-lated models. Other methods solve a series of partial opti-mization problems. Some of these methods use the subprob-lems to maintain an evolving approximation to the gradient of the full objective [26], which still requires O ( d ) memory. Others use each subproblem only to make progress on the overall objective, using only constant memory beyond that for the parameter vector. The one dimensional subproblems may be based on processing one parameter at a time, as in iterative scaling [15], and cyclic coordinate descent [30, 35]. Some of these algorithms have already shown promise on text categorization or other language processing tasks. One of the methods we compare with, Bayesian Logistic Regres-sion (BBR) [9], is an efficient implementation of regularized cyclic coordinate descent logistic regression. Let D = { ( x 1 , y 1 ) , ( x 2 , y 2 ) , . . . , ( x N , y ing set. Let d be the number of distinct n-grams in the fea-ture space. The training samples are represented as binary vectors x i = ( x i 1 , . . . x ij , . . . x id ) T , x ij y  X  X  0 , 1 } are class labels encoding membership (1) or non-membership (0) of the training samples in the category. We focus here on binary classification; multi-class classification is treated as several binary classification problems. Let X be the set of all samples x . Let  X  = (  X  1 , . . . ,  X  j be a parameter vector. Under the logistic regression model, the probability of a sample belonging to class  X  X ne X  is ([11]): { 0 , 1 } from the given training set D such that given a sam-ple x  X  X , we can predict a class label y  X  X  0 , 1 } . Learning such a mapping for logistic regression is equivalent to finding the parameter vector  X  , that maximizes the log-likelihood of the training set. The log-likelihood of the training set for logistic regression is ([11]): We take an optimization approach for solving this problem. Next, we show a procedure for maximizing l (  X  ), based on a coordinate-wise gradient ascent in the space of all n-grams in the training set.
Recall from Section 2 that all prior methods for logistic regression need at least O ( d ) memory where d is the size of the overall feature space. With u distinct unigrams, there are O ( u n ) potential n -grams in the training set, thus for large u and n , this would incur a very high cost.
In this section we present a new approach that in prac-tice requires o ( d ) memory, for solving logistic regression in the large space of all n-gram sequences in the training text. This becomes possible because we do not need to explicitly store all the distinct features, which would already use O ( d ) memory. Our algorithm is based on a branch-and-bound approach which chooses the maximum gradient ascent di-rection projected onto a single dimension (i.e., candidate feature).

Using equation (1), the gradient of l with respect to a coordinate value  X  j evaluated at a given parameter vector  X  is: Let j be a coordinate corresponding to a given n-gram se-quence s j , and j 0 be a coordinate corresponding to a super sequence of s j , s j 0 , i.e. s j is a prefix of s j 0 . We write s to mean x ij 6 = 0.

The following theorem, inspired by work on boosting [22], gives a convenient way of computing an upper bound on the gradient value for any super sequence s j 0  X  s j .
Theorem 1. For any s j 0  X  s j and y  X  X  0 , 1 } , the ab-solute value of the gradient of l (  X  ) with respect to  X  bounded by  X  (  X  j ) , where  X  (  X  j ) = max { X
Proof. We split the analysis into two subproblems, the first concerning the  X  X ositive X  class, and the second concern-ing the  X  X egative X  class. First we prove the bound for the positive class: The last inequality holds due to the fact that { i | y i = 1 , s x } X  X  i | y i = 1 , s j  X  x i } , for any s j 0  X  s j .
Similarly , we can show for the negative class that Thus we have:
The absolute value of the gradient of l (  X  ) at coordinate j corresponding to n-gram sequence s j 0 is thus bounded by  X  (  X  j ) :  X  X 
The theorem essentially states that at a given coordinate, i.e. n-gram sequence, we can decide whether the gradient of l (  X  ) can be improved by further extending this sequence. This facilitates casting the learning process as a search for the coordinate with best gradient value, rather than comput-ing the full gradient vector, in each optimization iteration. This process searches the entire space of all possible sub-sequences of the text, and guarantees to find the globally optimal feature (i.e. coordinate) in terms of the gradient value. We sketch the implementation of our algorithm in the next subsection.

Once we find the feature with the best gradient value, we adjust the value of the weight vector: and repeat the search for the coordinate with maximum (ab-solute) gradient value. This essentially produces one candi-date feature per iteration. is known in the literature as the step length , and is usually estimated via line search al-gorithms [26]. The iterations typically start at  X  old = 0 ([11]). Note that, since each restricted gradient direction is not conjugate to the previous ones, the chosen feature is not necessarily distinct from the already selected features. This is a common property of gradient methods and carries over to our greedy gradient ascent method. The outcome of this iterative process is a very sparse weight vector  X  , which is a linear model learned in the space of all n-gram sequences.
A high level overview of our gradient-based search algo-rithm is shown in Algorithm 1. This algorithm returns the best (in terms of gradient value) n-gram feature and it is called repeatedly up to the number of desired optimization iterations. The gradient value in iteration i is always com-puted at the parameter vector estimated in iteration i  X  1, thus the selection of a new feature depends on the set of pre-viously chosen features. The main parameter of our method is the number of optimization iterations, which directly in-fluences the size of the final model. We estimate this pa-rameter by thresholding the aggregated change in score pre-dictions (details in Section 4.3). We also implement a line search algorithm for estimating the step length in the di-rection with best gradient value. We currently use a binary search algorithm for estimating the step length [26]. Al-though the search space is very large, the pruning bound proposed in this paper effectively prunes the search. We have empirically observed that the pruning condition pre-sented in Theorem 1 prunes more than 90% of the search space.
 Algorithm 1 Find best n-gram feature 2: Output: Optimal feature (e.g. with best gradient value) 3: begin 4: global  X  , best feature 5:  X  = 0 //suboptimal value of gradient 6: //for each single unigram 8: end 9: return best feature 10: end 1: function grow sequence (s) 3: if abs ( gradient ( s )) &gt;  X  then 4: best feature = s //suboptimal solution 5: end 8: end 9: end
In this section we compare our learning technique to lo-gistic regression and support vector machines.

For logistic regression we use the open source implementa-tion by Genkin et al. [9] which we denote by BBR. This is a recent implementation of regularized logistic regression that was particularly designed to simultaneously select variables and perform learning. BBR is able to handle a large set of features via regularized cyclic coordinate gradient descent. See [9] for details.
 For support vector machines we used the latest open source SV M perf solver by Joachims et al. [17] which is especially tuned for linear problems.
To study the effect of using variable-length n-gram se-quences as basic features, we vary the maximum (word or character) n-gram length and train all methods in the space of all sequences up to a fixed length. For example, if we fix the n-gram size to n = 3, this means we train in the space of all n-grams up to trigrams, e.g. all the unigrams, bigrams and trigrams.

For BBR and SVM, we first use a state-of-the-art pattern mining tool [1, 21, 33] in order to produce all the n-grams up to a given size (e.g., up to 5-grams), and then learn the two classifiers in this space. Thus the feature space is the same for all methods, but for BBR and SVM we need to generate the space explicitly using a pattern mining tool while SLR searches the entire space automatically and incrementally adds only those features that contribute to a good classifier.
We evaluate all methods with respect to training run-times and micro and macro-averaged F1 measure [31], with word level and character level n-grams. We show that our method can benefit from using arbitrary length n-grams, which is reflected in the F1 measure, and that due to our pruning bound we are much faster than the other methods.
We study three different applications 1 of text categoriza-tion that could benefit from learning variable-length n-grams. The first application is movie genre classification . We take a subset of movies from IMDB, which have a plot sec-tion, i.e. one or two paragraphs that describe the movie. IMDB contains information about the genre of each movie in the database. The classification task is to learn the genre of the movie from the short plot description associated with each movie. We take a subset of movies classified to either of the genres Crime or Drama. We select these two genres because they are close in terms of topicality, thus the classi-fication task is harder than when learning to classify movies belonging to orthogonal genres, e.g. Crime versus Comedy. The dataset contains a total of 7,440 documents with 3,720 documents for each genre. There are 63,623 distinct word-unigrams ( IMDB dataset ).

The second application we study is book reviews clas-sification by genre . We work with a dataset of editorial reviews of books from Amazon. The collection was first used in [14]. The editorial reviews are grouped into three genres: Biology, Mathematics and Physics. There are 5,634 reviews, with reviews of books about Biology and Mathematics hav-ing roughly 2,200 documents each, and those about Physics having about 1,300 documents. There are 55,764 distinct unigrams in this collection ( AMAZON dataset ).
 The third application we analyze is topic detection for Chinese text . We considered the TREC-5 People X  X  Daily News dataset investigated in [12]. The dataset contains 6 classes: (1) Politics, Law and Society; (2) Literature and Arts; (3) Education, Science and Culture; (4) Sports; (5) Theory and Academy and (6) Economics, and it is split into training and test. Each class has 500 training documents and 100 test documents. The training set contains 4,961 distinct characters ( CHINESE dataset ).
 No pre-processing of the first two datasets was carried out. For the Chinese dataset we removed the SGML tags. For the case of multiple classes, we convert the classification task into several binary classification tasks in the one-versus-all manner.
We first compare the training running times for all meth-ods, for each dataset and experiment, with a fixed parameter set. We also show various statistics on the models learned by each method. For reporting micro/macro-averaged F1 we also tune the most important (in terms of influence on classification performance) parameter for each method.
Setting the number of iterations. For our method the number of iterations directly influences the size of the
All datasets are available at: final model, since in each iteration we select a candidate feature to be included in the final model. In order to set this parameter, we consider the aggregated change in the score predictions, and if this is not above a fixed threshold, we stop the iterations. The threshold is set to 0.005. This is fixed across all datasets and experiments, for measuring running times. For tuning the number of iterations, we run cross-validation for values between 200 and 1,000 with steps of 100, and choose the best parameter. We set the final classification threshold to the value that minimizes training error. The data representation for our method is the original text, interpreted as a word-level or character-level n-gram sequence.
Setting the regularization parameter. For bayesian logistic regression [9] the regularization parameter is the most important (i.e. the prior variance for parameter val-ues). This is initially set to the value recommended in [9], i.e. the ratio of the number of distinct features to the average euclidean norm of documents in the dataset. For tuning this parameter, we use the autosearch option, which automati-cally searches for the best cross-validated hyperparameter value. The other parameters are set as: -p 1 -t 1. The -p 1 parameter selects the Laplace prior distribution on the model parameters. We choose this prior due to the resulting sparse models. The Laplace (aka. lasso) logistic regression is also much faster and more accurate than the Gaussian (aka. ridge) logistic regression. The -t 1 parameter sets the final classification threshold to the value that minimizes the number of training errors. All other parameters are used with their default values. The data representation for BBR is sparse vectors, i.e. id and value for non-zero features.
Setting the soft-margin C parameter. For SV M perf [17] the soft-margin C parameter is the most important. [17] observed that any parameter value between 100 and 500 would be good across datasets. After some initial trials we set C = 100, for the fixed parameter experiments. For the tuning setting, we chose the C value which performed best in cross-validation on the training set, from several values selected from the range 100 to 500. All other parameters are kept with their default values (linear kernel is a default setting). The data representation for SVM is also sparse vectors.
The experiments were run on a Linux machine with 1GByte memory and 2.8GHz Intel CPU. We show micro/macro-averaged F1 [31] as global measure of quality for each dataset and classifier.
The IMDB dataset is not explicitly split into training and test, thus we run 5-fold cross validation and report the micro/macro-averaged F1. In Table 1 we report the training running time for all methods. We observe that our method is much faster than both BBR and SVM. For word n-grams, our running time stays almost constant with increasing n-gram length (0.35 minutes even with unrestricted length), while BBR goes from 0.3 minutes for unigrams to 4 minutes for (up to) 5-grams. SVM shows a similar trend as BBR, its running time increases from 0.3 minutes for unigrams to 15 minutes for 5-grams. The time reported is an average running time per cross-validation split averaged across top-ics. The trend for char-level n-grams is similar, our method stays almost constant at 1.7 minutes for 3-grams and 2.4 minutes for unrestricted n-gram length. BBR and SVM go from 2.7 and 2.5 minutes respectively for character 3-grams, to 8.4 and 50 minutes respectively for 5-grams. Thus, SLR is orders of magnitude faster than the state-of-the-art meth-ods SVM and BBR. This difference in running time is due to the way our technique deals with large feature spaces, by its branch-and-bound search strategy. We also notice that SLR selects much fewer features in the final model, as compared to BBR. For word n-grams, out of 190 iter-ations for unrestricted n-gram size, it selects 135 distinct features in the final model. BBR runs for 374 iterations in the space of 5-grams, and selects 4,000 distinct features in the final model. For char n-grams, our model runs for 620 iterations and selects 420 distinct features, while BBR runs for 370 iterations for 5-grams and selects 4,000 features. In terms of micro/macro-averaged F1 (Table 1, Table 4), we observe that our method is as good as the state-of-the-art regarding generalization ability, while being orders of mag-nitude faster. For the case of fixed parameters, our method is 3 to 5% better than both BBR and SVM. In the case of tuned parameters, for word n-grams our method achieves 74.04% macro-averaged F1, while BBR achieves 73.94% and SVM 71.24%. Regarding performance with char n-grams, our method achieves the best macro-averaged F1 (74.88%), while BBR achieves 74.72% and SVM 70.43%. In Table 5 we show top 5 word-level and char-level n-gram features for the positive (Crime) and the negative (Drama) class. We can observe the following effects comparing the top word-level n-grams with the char n-grams in Table 5. The top-5 word-level features are highly discriminative unigrams, for both the positive and the negative class. The character-level n-grams extract characteristic substrings (word stems, syllables, etc.) of words and provide robustness to morpho-logical variation of wordings and misspellings. For example, the n-gram urde a potential substring of murder, murderer, murdering , is selected as a positive feature for the Crime class (962 times in Crime vs 303 times in Drama). Note that urde rather than murde is selected because adding the m does not increase (in this particular case) the discrimina-tion power of this feature. Other examples are the prefix lov , from love, loving, loveable , a feature much more frequent in the Drama movie plots (925 times), than the Crime plots (470 times). Similarly, substrings such as choo from school, schools, schooling, etc. are chosen as characteristics of the Drama class (300 times in Drama, 110 times in Crime). The macro-averaged F1 for the word n-gram and the char n-gram models is comparable, e.g. 74.04% versus 74.88% both achieved with SLR. Similarly, we run 5-fold cross validation on the AMA-ZON dataset and report the micro/macro-averaged F1 re-sults (averaged over cross-validation splits). In Table 2 we present training running times on this dataset for all meth-ods. Again, SLR is much faster than BBR and SVM, with highest running time of 0.2 minutes for unrestricted word n-grams, and 1.5 minutes for unrestricted char n-grams. BBR takes 5.4 minutes for learning a word-level 5-gram model and 9.6 minutes for learning a 5-gram char model. SVM takes somewhat longer with 24 minutes average runtime for word-level 5-gram model and 55 minutes for char-level 5-gram model. In terms of classification quality, for the fixed pa-rameters setting (Table 1) SLR is better by 2-3% than either BBR or SVM. In the tuned setting (Table 4), SLR is compa-rable to BBR (word-level: 83.16% versus 83.44%; char-level: 83.01% versus 82.97%) and SVM (word-level: 80.68%; char-level: 80.52%).

In Table 5 we show top-5 positive and negative n-grams for this dataset. We observe the same effect of implicit stem-ming for the character n-grams as in IMDB. For example, our model selects features such as bio , instead of biology , or mat instead of mathematics . These features carry already enough information for discriminating the given topics, thus there is no need for selecting the entire word. Note the fea-tures such as olo which seem unexpected at a first glance. The reason for selecting olo is that it is contained in words such as biology, biologist, biological, ecology, etc. , thus it is by itself already good for discriminating between Biology and Mathematics-Physics. This sort of features may seem prone to overfitting, but our learning method is robust enough to decide on inclusion of only highly discriminative features. Examples of syllable extraction are features like ics instead of physics, mathematics, statistics , etc. Misspellings, such as physics versus pyhsics , can influence the features much less with this kind of representation. Char n-grams could also potentially capture other effects of language use, such as re-named entities, e.g. Alon Halevy, A. Halewi, A. Halevy , slang, e.g. Eire for Ireland , and abbreviations, e.g. math instead of mathematics .
This dataset is split into training and test, thus we train all models on the training set and report micro/macro-averaged F1 on the test set. The training set contains 4,961 distinct characters, and about 6,000,000 char n-grams up to size 5. Table 3 shows training run-times for this dataset. SLR takes 0.5 minutes for learning a model in the space of char-level n-grams of unrestricted length. BBR needs 11 minutes, while SVM needs 5 minutes for learning 5-gram models. These running times do not include the time required for pattern generation for BBR and SVM (9 extra minutes). Regarding prediction quality, SLR achieves 79.75% macro-averaged F1 which compares well to the 80.66% achieved by SVM and 79.63% achieved by BBR.

Tables 1 to 4 show micro/macro-averaged F1 behavior for all methods and corpora for varying maximum n-gram length n . For word n-grams, we note that a higher order n improves the quality of the models in the case of fixed parameters, but not in the case of tuned parameters. For char n-grams, we observe that all methods benefit from us-ing higher order n-gram features, also in the tuned setting. Overall, char n-gram models seem to be at least as good and often better than word n-gram models. Furthermore, the accuracy of SLR models with fixed parameter is close to that of tuned SLR models, thus reducing the need for care-ful tuning of the number of iterations. Although the space of variable length n-grams is very large, our method can ef-ficiently learn accurate models, thus avoiding the need for additional pre-processing, such as feature selection or word-segmentation.
In this paper we present a coordinate-wise gradient ascent technique for learning logistic regression in the space of all (word or character) n-gram sequences in the training data. Our method exploits the inherent structure of the n-gram feature space in order to automatically provide a compact set of highly discriminative n-gram features.

We propose a theoretical bound which quantifies the X  X ood-ness X  of the gradient for each n-gram candidate given its length-( n  X  1) prefix. We show that due to the proposed bound, we can efficiently work with variable-length n-gram features both at the word-level and the character-level.
We present experiments that compare our SLR method against the state-of-the-art classifiers BBR (a logistic regres-sion method) [9] and SV M perf [17]. We show that while SLR generalizes as good as the state-of-the-art methods, it is more than one order of magnitude faster than its oppo-nents.

With the method presented in this paper we study the problem of learning the tokenization of the input text, rather than explicitly fixing it in advance (as in the bag of words model). The tokens learned by SLR can be arbitrary sized, rather than restricted to a hypothesized  X  X ood X  size. This opens interesting research directions, such as supervised en-tity extraction and unsupervised or semi-supervised text clustering. Furthermore, our technique can be applied to other domains such as gene sequence classification, where mining variable-length sequences is of particular importance.
Open source code for SLR is available on-line at: http://www.mpi-inf.mpg.de/  X  ifrim/slr . We thank the reviewers for their helpful comments and Fernando Pereira for suggesting the relevant work on pre-diction suffix trees [5]. [1] K. Abe, S. Kawasoe, T. Asai, H. Arimura, and [2] T. C. Bell, J. G. Cleary, and I. H. Witten. Text [3] H. Cheng, X. Yan, J. Han, and C.-W. Hsu.
 [4] J. G. Cleary and W. J. Teahan. Unbounded length [5] O. Dekel, S. Shalev-Shwartz, and Y. Singer. The [6] S. Dumais, J. Platt, D. Heckerman, and M. Sahami. [7] B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani. [8] E. Frank, C. Chui, and I. H. Witten. Text [9] A. Genkin, D. Lewis, and D. Madigan. Large-scale [10] J. Goodman. A bit of progress in language modeling. [11] T. Hastie, R. Tibshirani, and J. Friedman. The [12] J. He, A.-H. Tan, and C.-L. Tan. On machine learning [13] D. Holmes and R. Forsyth. The Federalist revisited: [14] G. Ifrim and G. Weikum. Transductive learning for [15] R. Jin, R. Yan, J. Zhang, and A. Hauptmann. A faster [16] T. Joachims. Text categorization with Support Vector [17] T. Joachims. Training linear SVMs in linear time. In [18] B. Kessler, G. Nunberg, and H. Schtze. Automatic [19] P. Komarek and A. Moore. Fast robust logistic [20] A. C. K  X  onig and E. Brill. Reducing the human [21] T. Kudo. An implementation of freqt (frequent tree [22] T. Kudo and Y. Matsumoto. A boosting algorithm for [23] Y.-B. Lee and S. H. Myaeng. Text genre classification [24] H. Lodhi, C. Saunders, J. Shawe-Taylor, [25] C. Manning and H. Sch  X  utze. Foundations of Statistical [26] J. Nocedal and S. Wright. Numerical Optimization . [27] F. Peng, D. Schuurmans, and S. Wang. Augmenting [28] R. Rosenfeld. Two decades of statistical language [29] B. Scholkopf and A. J. Smola. Learning with Kernels . [30] S. K. Shevade and S. S. Keerthi. A simple and efficient [31] Y. Yang and X. Liu. A re-examination of text [32] Y. Yang and J. O. Pedersen. A comparative study on [33] M. Zaki. Efficiently mining frequent trees in a forest. [34] D. Zhang and W. S. Lee. Extracting [35] T. Zhang and F. Oles. Text categorization based on
