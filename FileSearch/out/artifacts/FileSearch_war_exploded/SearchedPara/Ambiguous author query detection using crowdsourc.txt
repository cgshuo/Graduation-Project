 1. Introduction
Bibliometric methods measure the impact of papers, researchers, journals, and even organizations. Many measures based on citation data have been proposed to estimate quantitatively the impact of an author. Examples include the H-index to-date citation data is critical for an accurate assessment of author impact.

The publications of an author are typically identified by the author X  X  name. However, we cannot always correctly match publications with authors because names can be ambiguous. There are two types of name ambiguity: multiple name vari-pact analysis noisy and therefore complicates efforts to measure impact.
 detect ambiguous names at query time.

Prior work in name disambiguation is based on supervised or unsupervised machine learning algorithms that partition a analysis tools, which require query-time detection of ambiguous names. When an ambiguous name is detected in this  X  setting the tool should prompt the user to refine the query, e.g., by adding keywords to make the query more specific. We by querying a digital library , decide if the author names of these publications match the name in the query . 1.1. Contributions and outline
After background on related work in Section 2 and description of data crowdsourced through Scholarometer in Section 3 , we present the proposed approach in Section 4 , which includes the following contributions: A heuristic based on name variations and citations (Section 4.1 ).

A two-step method to capture the consistency between coauthor, title, and venue metadata across publications (Section 4.2 ).
 An algorithm to measure the coherence between the topics associated with title and venue metadata (Section 4.3.1 ).
An algorithm to measure the consistency between topics associated with publication metadata with the addition of crowdsourced discipline annotations for authors (Section 4.3.2 ).

In Section 5 we evaluate these features in a supervised learning setting and show the effectiveness of combining them with each other. We also show that our approach outperforms a baseline derived from Microsoft Academic Search. 2. Related work average of 1.71 distinct individuals per author name in Korean publications. Our own analysis based on Scholarometer data suggests that 25% of the queried author names are ambiguous.

The literature on disambiguation is mainly categorized into supervised and unsupervised learning approaches. Super-curation. Recently, Levin, Krawzyk, Bethard, and Jurafsky (2012) proposed a self-supervised algorithm for author disambig-author name disambiguation approach with self-training capabilities.
 Unsupervised approaches ( Bhattacharya and Getoor, 2007; Cota et al., 2007; Han, Xu, et al., 2005; Han, Zha, et al., 2005; different features of the data, such as coauthors, venues, titles, and affiliations.

Recently, Microsoft Academic Search ( academic.research.microsoft.com ) and Google Scholar ( scholar.google.com ) have introduced hybrid approaches that combine automatic clustering with manual curation, a crowdsourcing approach to name disambiguation. This approach is not applicable in our setting because the Scholarometer system does not require authors to create profiles.

The approach taken by the Scholarometer system for disambiguation is based on supervised learning, but defines the problem in a different way. Given a set of papers for a given author name, the task is to determine whether the name is ambiguous, i.e., corresponds to multiple authors. Our first attempt to deal with ambiguous author names deployed a simple mentioned in Section 4.3.1 . With increased popularity the number of authors in Scholarometer has grown significantly, revealing many undetected ambiguous names. This has motivated us to explore more features for better detection of ambig-uous names. 3. Crowdsourced data In this section we outline the data acquisition in the Scholarometer system and the statistics of the crowdsourced data. Further details can be found elsewhere ( Kaur et al., 2012 ).
 3.1. Data acquisition
Crowdsourcing is an approach to harness knowledge from a community via Web platforms in order to solve practical problems. Scholarometer applies crowdsourcing to scholarly annotations. Users provide disciplinary annotations in ex-
As a browser extension, Scholarometer accepts queries about authors, which must include discipline annotations. The polluting databases. We also apply manual and automatic data cleaning techniques to deal with noisy annotations (tags). Another important issue is the ambiguity of author names, which is the topic of this paper. 3.2. Data analysis
The Scholarometer system was first released in November 2009. At the time of writing, the database has collected about of authors. The Scholarometer database was initially dominated by computing-related disciplines due to the publicity re-www.scholarometer.indiana.edu/explore.html ). 4. Ambiguous name detection
Our algorithm extracts features from all publications retrieved for the queried author name and performs binary classi-classes of features of the publications of an author. 4.1. Name variations and citations
Typical author names have two or three variations. To extract name variations for a queried author, we compute a heu-ristic similarity measure between the author names from the retrieved publications and the queried name. All the names with similarity above an empirical threshold are considered as variations and ranked by the citations counts of the corre-on impact measures. If the top name variations account for a low fraction of the citations, it is reasonable to assume the contrary.
 variation ( CNV n ). 4.2. Metadata consistency papers in similar journals or conferences. The metadata associated with these publications by the same author should be consistent; inconsistencies between publication patterns are evidence of an ambiguous author name.
We explore three common publication attributes: coauthors, title, and venue. For each attribute we compute the average similarity over pairs of publications. Some preprocessing, including stopword removal and stemming, takes place before computing any similarity.

Two kinds of similarity methods are used: cosine similarity and overlap coefficient. Cosine similarity is defined as: overlap coefficient is defined as: where x and y are two sets of binary features.
 publications are computationally costly for a query-time algorithm. To alleviate these problems, we propose a two-stage strategy: common, they belong to the same author ( Cota et al., 2007 ). This algorithm, though simple, groups some publications together with high confidence. In every cluster, the titles and venues are merged together. duces the computational cost of the similarity computation, even though these features are still expensive to compute for query-time detection. Therefore, in Section 5.2 we propose a sampling strategy to limit the number of cluster comparisons. 4.3. Topic consistency
We leverage the discipline tags crowdsourced from the users of the Scholarometer system ( Fig. 1 ) to capture topical con-sistency between authors and publications. Each discipline can be represented by a vector of authors who have been tagged these two disciplines. As an illustration, we list eight pairs of highly related disciplines in Table 1 . 4.3.1. Publication topic consistency
To capture the possibility that publications in different disciplines may be from the same author in spite of inconsistent and Topic 1 and Topic 2 are highly related, we may infer that the publications are consistent and the author is not ambig-uous. This is the basic intuition for a feature that we call publication topic consistency ( PTC ).
We use a subset of the crowdsourced tags from a controlled vocabulary, namely the JCR categories. These 242 preexisting Index from Thomson Reuters X  Web of Science .

For every author all the publication titles and venues are merged together into a set of keywords P and mapped to pre-belongs to d as: contain w , which is used to measure the generality of that word.
 sum over all profile topics. Formally: 4.3.2. Author-publication topic consistency
Suppose that the majority of the publications of an author have high metadata and publication topic consistency, but the we may infer that the author name is ambiguous.
 as the similarity between the publications profile and the crowdsourced discipline tags: disciplines that contribute to the numerator in Eq. (4) .

The number of votes together with the number of tags are used to determine heuristically which tags are reliable for each author ( Kaur et al., 2012 ). 5. Evaluation formed other methods in most cases. For each combination of features, we report on three performance measures: accuracy (Acc), area under ROC curve (AUC), and F 1 . Average values of these measures are obtained by performing 10-fold cross-validation.

To train and test our classifier, we manually labeled 500 author names. The names were selected among the top authors (ranked by H-index) from each of the top 100 disciplines (ranked by number of authors) in the Scholarometer database. Four affiliations, and crowdsourced tags were inspected to obtain the ambiguity labels. We randomly selected 250 authors and among the 500 author names, 283 were labeled  X  X  X ot ambiguous X  X  and 217  X  X  X mbiguous. X  X  5.1. Name variations and citations
Based on the citations-per-name-variation heuristic, we compare the accuracy of the classifier based on the percentage of that using the top three name variations results in better detection of ambiguous names.

We notice that when there are more than three name variations, CNV 3 is a good feature, which can be seen from the upper ing ambiguity. Based on the accuracy, we select CNV 3 . 5.2. Metadata consistency
To compute the metadata features efficiently, we exploit estimated knowledge of author impact. We only consider the top h (author X  X  H-index) publications, as they are the ones that affect the impact computation. While this choice precludes us the large amount of noise present in the tail of the publications returned by Google Scholar.

Fig. 5 shows the accuracy achieved with metadata features computed over samples of publication/cluster pairs of differ-ent sizes. A sample of 200 pairs provides an adequate balance between efficiency and accuracy. We use this sample size to produce the results shown in Tables 4 and 5 .
 with different groups of people at different times, so the average coauthor similarity may be low even for unambiguous authors. Table 5 shows the results of our two-stage strategy. From both tables, we see that the best results are obtained by combining the two metadata consistency features with coauthor clustering. 5.3. Topic consistency and summary
Table 6 shows that PTC achieves relatively high accuracy as a single feature, demonstrating that it is reasonable to con-rations. APTC is not as good as PTC ; inconsistencies between publication topics and author tags may be caused by poor tag choices rather than ambiguous names. In combination with all the other four features, however, APTC does provide a slight advantage ( Table 7 ).

Fig. 6 and Table 7 summarize the performance of five single features and their combination. The performance of the best achieved with the combined feature CNV 3 + PTC ( Sun et al., 2011 ).

The above results, while promising, are based on a relatively small hand-curated dataset of 500 authors. To evaluate our 92.3%.
 5.4. Comparison with a baseline
For better assessment it is desirable to compare our approach with a suitable baseline. However, we are not aware of other methods in the literature that perform ambiguous author name detection at query time as we have defined the task here.

In search of a baseline for our evaluation, let us consider Microsoft Academic Search ( MAS ). MAS provides name disambig-uation by clustering the papers associated with an author name into profiles. To derive a suitable baseline from MAS, we derived from the state of the art.

A number of measures must be taken to ensure that our comparison is not biased against the baseline and in favor of our of these authors, we query MAS to make sure that the publications in MAS and Google Scholar (the Scholarometer data and 79 not ambiguous according to our definition.

For the sake of obtaining a baseline ambiguity detector, we ignore the profile details and focus solely on the number of trained on the 361 excluded authors and then tested on the same set of 139 authors as MAS.

The confusion matrices generated by our detector and the MAS baseline are shown in Table 8 . MAS has a high false neg-of an author into small coherent groups so that an unambiguous author name may have several profiles. In the context of and G-index values reported by the two systems for 49 highly cited computer scientists with unambiguous names in
Fig. 7 . While the values are correlated, MAS tends to report lower impact measures. For the same reason, MAS has no false positives, while our system misses an ambiguous author, overestimating impact of six authors.

Overall, the accuracy of our detector is 71% vs. 64% for the baseline. Note that our system X  X  accuracy is lower than re-proach achieves better performance for ambiguous author name detection in the context of citation analysis. 6. Conclusions
We investigated the detection of ambiguous crowdsourced names in a social citation analysis system. Three classes of features were explored, extending previous work. The first is a heuristic based on the percentage of citation accrued by the top name variations for an author. The second is based on consistency of metadata, including titles and venues across coauthor clusters. The third utilizes crowdsourced data to detect ambiguity at the topic level. Our experiments show that approach compares favorably with a baseline derived from Microsoft Academic Search when testing on a subset of authors.
We have implemented the proposed method into the latest version of the Scholarometer system, and the detector is suf-compute the features and run the detection algorithm once the data is obtained from Google Scholar.
In the future we plan to enhance the approach by exploring additional features, such as publication years, to further im-prove the accuracy of ambiguous author detection. Furthermore, for undetected ambiguous names, we will explore the use of more traditional name disambiguation algorithms to partition publications into coherent clusters, combined with crowd-sourcing techniques to let users select, merge, and/or split profiles for matching queried authors. Acknowledgments
The work presented in this paper was performed while Xiaoling Sun and Lino Possamai were visiting the Center for Com-plex Networks and Systems Research ( cnets.indiana.edu ) at the Indiana University School of Informatics and Computing.
We are deeply grateful to Diep Thi Hoang, without whom Scholarometer would not exist, for her continuous support. Thanks works and agents Network ( cnets.indiana.edu/groups/nan ) for helpful suggestions and discussions. We acknowledge of Padua, the University of Bologna, the Lilly Endowment, and NSF (Award IIS-0811994) for funding the computing infra-structure that hosts the Scholarometer service.
 References
