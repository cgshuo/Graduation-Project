 This paper introduces the notion of intelligent SSDs. First, we present the design considerations of intelligent SSDs, and then ex-amine their potential bene fi ts under various settings in data mining applications.
 H.2.8 [ Database Management ]: Database Applications X  Data min-ing Intelligent SSD, big data mining, in storage processing
Recently, big data analysis becomes more important to extract interesting patterns from a huge amount of data. Owing to the high I/O bandwidth and low access latency, solid state drives (SSDs) have been rapidly replacing hard disk drives (HDDs) [1].
SSD manufacturers try to overcome the physical limits of fl ash memory and to provide a higher I/O bandwidth for fast data access. However, such efforts incur a new performance bottleneck in data-intensive applications for big data analysis [2].

In HDD environment, low I/O bandwidth of HDDs is a main performance bottleneck in data-intensive applications since HDDs read data by physically rotating a magnetic disk .However,SSDs employ no mechanical devices and provide high internal I/O band-width thanks to the multi-way and multi-channel interleaving. As a result, in SSD environment, the I/O bandwidth of host interface becomes a new bottleneck in data-intensive applications [2].
In this paper, in order to solve the performance bottleneck of the host interface, we propose an approach called intelligent SSD (iSSD) in which a large volume of data are directly processed by  X  Corresponding author processing units inside SSD and only a small resulting data is trans-ferred to the host. With this approach, the host interface is no longer the performance bottleneck in data-intensive applications.
The internal I/O bandwidth in SSDs can easily scale up by in-creasing the multi-way and multi-channel interleaving [3]. So, the additional increase of the internal I/O bandwidth can be fully uti-lized in data processing. This allows us to enjoy the signi fi cant per-formance improvement in data-intensive applications. Moreover, as the data is processed in the place much closer to the data source itself, we can save both of the data transfer cost and energy [4].
Data mining is a typical example of data-intensive applications and has the following characteristics: (1) It frequently accesses the stored data during its execution; (2) It conducts relatively simple operations repeatedly such as scanning and fi ltering on the accessed data [5]. In this paper, we present design considerations and the potential bene fi ts of the iSSD in terms of data mining applications.
Figure 1 shows the overall architecture of the SSD [3]. Typically, the SSD contains 8  X  32 channels . In each channel, there are (1) 8  X  16 NAND fl ash memory cells for storing data, (2) a fl ash mem-ory controller (FMC) for managing all the fl ash memory cells in the channel, and (3) DRAM for reading/writing data from/to fl ash memory cells. All the channels are managed by fi rmware called fl ash translation layer (FTL) . Embedded CPUs (core CPUs) exe-cute the FTL and DRAM saves FTL metadata in the SSD.
 Table 1 shows the I/O bandwidth of each part [3]. The internal I/O bandwidth of the SSD with 8-way and 16-channel is 6.4GB/s. This indicates that the internal bandwidth of the SSD is bound to that of host interface (PCIe x16: 6GB/s). Internal bandwidth of an SSD 8-way, 16-channel: 6.4GB/s
In order to fully exploit the internal I/O bandwidth for data pro-cessing, suf fi cient processing power is required inside the iSSD 1 ; Otherwise, it could become a new performance bottleneck. We could add a cheap (while having relatively low performance) pro-cessor called channel CPU to each channel inside the iSSD.
The advantages with this approach are as follows: (1) Data can be processed over all channel CPUs in parallel . The size of the data processed in a channel CPU is in inverse proportion to the number of channels in the iSSD. Therefore, each cheap channel CPU suf fi -ciently handles its data if we increase the number of channels inside the iSSD; (2) Adding a channel into the iSSD provides not only the expansion of the internal bandwidth but also the improvement of the inside processing power, thereby not burdening both the host interface and the host CPU; (3) A number of cheap channel CPUs are better than a single or a few high performance core CPUs inside an iSSD in the thermal aspect [7].

There have been several researches on processing data in stor-age devices [4], mainly focusing on HDDs. Since the HDD has a limitation to the expansion of the internal I/O bandwidth due to its physical characteristics (i.e., very high seek time and rotational latency), their approaches have not been practically adopted.
On the contrast, since the SSD employs no mechanical devices and provides high scalability of in ternal I/O bandwidth , the iSSD could be a good solution for dealing with large-scaled data mining applications. To support our opinion, recently several studies have explored the feasibility of in storage processing on SSDs [2].
In this section, we analyze the execution costs of data mining applications. We formulate two cost models: Conventional data processing based on the host CPU (In-Host Processing, IHP) and data processing in the iSSD (In-Storage Processing, ISP). Figure 2 illustrates the execution steps of a data mining application in the IHP and the ISP, respectively.

The execution costs of data mining applications are mainly com-posed of two parts: (1) data accessing and (2) data processing. Through analyzing these two costs, we identify the potential and new bottleneck of the ISP compared to the IHP.
We begin by de fi ning some preliminaries to describe the cost to transfer d bytes in each path, similar to the derivation in [3].  X  Cell  X  channel: Equation 1 represents the cost to read d from fl ash memory cell to channel memory. Here, S page is the size of a single NAND fl ash memory page, N way is the number of ways in a channel, t cell  X  read is the time to read 1 page from fl ash memory cell, and t bus is the elapsed time to load a page from a ject storage device [6] is needed for processing data inside the iSSD. However, such interface is orthogonal to our contributions, and thus is not mentioned further. fl ash memory bus after the busy phase for a page read.  X  Channel  X  core: For ef fi ciency, data transfer between chan-nel memory and core memory employs the direct memory access (DMA) operation and the pipeline strategy [3]. Equation 2 repre-sents the cost to read d bytes from channel memory to core memory. Here, t core  X  write is the time to write d bytes to core memory.  X  Core  X  host: Similar to t ch  X  core , data transfer between core memory and host memory also employs the DMA operation and the pipeline strategy. Equation 3 represents the cost to read from core memory to host memory. Here, t host  X  write is the time to write d bytes to host memory through the host interface and t core  X  read is the time to read d bytes from core memory. The data access costs, derived from above preliminaries, in the IHP and in the ISP are as follows. Here, N ch is the number of channels in the iSSD.
As shown in Figure 2, the data transfer path of the ISP is shorter than that of the IHP. Moreover, in the ISP, data does not move through the host interface which is a new bottleneck in iSSDs. So, the data access cost in the ISP is much lower than that of the IHP.
We formulate the data processing cost of a target data mining application by a unit of a function . This is because, even with the functions belonging to one data mining application, their execution times show huge difference depending on their characteristics.
The data processing costs of the target data mining application in the IHP and in the ISP are formulated as follows. Note that, in the ISP, data mining applications are performed over all channel CPUs in parallel . Here, | F | is the number of functions in the target performing function f k in the host CPU (in the channel CPU).
The data processing cost directly depends on aCPUrate of the place where the function is performed. So, the data processing cost in the ISP is much high compared with that of the IHP, even if the iSSD performs data mining applications with multiple channel CPUs in parallel.

In the ISP, some data mining applications require global merge that aggregates all local results produced by channel CPUs. The global merge adversely affects the performance in the ISP because it cannot be performed over channel CPUs in parallel. We assume that the global merge is conducted in the core CPU since it ef-fi ciently communicates with every channel CPU and has perfor-mance higher than channel CPUs.

The global merge cost in the ISP is as follows. Here, t m the time for performing global merge for function f k in the core CPU and R ( f k ) is the local result size of function f k
The global merge cost is in fl uenced by (1) the size of a local result from each channel CPU, (2) the number of channels in the iSSD, and (3) the complexity of the global merge operation: the global merge cost increases when the size of a local result, the number of channels, and the complexity of the global merge oper-ation increases.
Equations 4 and 5 represent the total execution costs of the tar-get data mining application in the IHP and in the ISP, respectively. With these equations, we can quantify the degree of the perfor-mance improvement in the ISP over the IHP. Here, N ( f k ) is input data size for function f k .
We fi rst validate the cost models through three well-known data mining applications of k -means [5], PageRank [8], and Apriori [5]. Note that, Apriori requires the global merge to aggregate all the local results, each of which is the local frequency of a candidate itemset obtained by each channel CPU, in order to get its global frequency [5].

Table 2 represents the descrip tions of primitive functions in each data mining application. Here, N is a total data size. To extract the number of cycles per instruction (CPI) and the number of instruc-tions in each data mining application, we executed the application
We can estimate the total execution cost of each data mining application in the IHP (ISP) to substitute the descriptions shown in Table 2 into Equation 4 (5). For the data access cost, we used the function in the data processing cost, we used Equation 6. 2 http://software.intel.com/en-us/intel-vtune-ampli fi er-xe/ / fl ash-ssd/catalogue
For validating the correctness of our cost models, we compared the execution time of k -means, PageRank, and Apriori estimated by our IHP model with that obtained by executing them in a real machine. The results indicate a fairly high accuracy of 95  X  99% with our models.
Experiment 1. ISP with different iSSD settings: We changed the number of channels in the iSSD as 32, 64, 128, and 256, while setting the rates of a core CPU a nd a channel CPU as 400MHz and 200MHz, respectively. As shown in Figure 3(a), the performance of the ISP in all cases dramatically increases as the number of chan-nels increases. In particular, it increases almost linearly with the number of channels for k -means and PageRank which do not have the stage of global merge.

We changed the channel CPU rates as 200MHz to 800MHz in step of 200MHz. We fi xed the rate of a core CPU and the number of channels as 400MHz and 32, respectively. In Figure 3(b), we observe linear performance improvement with channel CPU rates in k -means and PageRank. However, in case of Apriori, the perfor-mance improves much less due to its global merge. Figure 3: Performance of the ISP with different iSSD settings.
Experiment 2. Comparison of ISP with IHP: For showing the potential bene fi ts of the iSSD quantitatively, we compared the per-formance of IHP and ISP in k -means, PageRank, and Apriori. For the ISP, we considered two iSSD architectures: iSSD_C (typical setting in the current SSD) with a core CPU of 400MHz, 32 chan-nels, and their CPUs of 200MHz and iSSD_F (setting in the future iSSD) with a core CPU of 800MHz, 256 channels, and their CPUs of 800MHz. For the IHP, we employed a host CPU of 2.5GHz, and also deployed both HDD and SSD for examining performance change with different storage media.

In Figure 4, the ISP in both iSSD architectures is shown to signif-icantly improve the performance in all cases. Even with the current settings (iSSD_C), the ISP outperforms the IHP about 2  X  3 times. With the future settings (iSSD_F), the ISP dramatically improves the performance of the IHP up to 83 times.

In case of Apriori, the ISP(iSSD_F) shows insigni fi cant perfor-mance improvement compared with the ISP(iSSD_C) due to its global merge. Such fi ndings are vividly illustrated in Figure 5.
Figure 5 shows the breakdown result of the ISP cost. The global merge costs of Apriori in both ISP(iSSD_C) and ISP(iSSD_F) are almost same. This is because as the number of channels increases, the number of local results to be merged in the core CPU also in-creases. So, in Apriori, the data processing in the core CPU be-comes new performance bottleneck. As a possible solution to solve the performance bottleneck in the core CPU, we can consider the method to conduct the global merge in the host CPU which has much higher performance than the core CPU.

In this paper, we introduced the iSSD as a turbo for big data mining. We fi rst presented design considerations for the iSSD. The iSSD should have (1) enlarging internal I/O bandwidth and (2) suf-fi cient inside processing power. We then formulated the execu-tion costs of data mining applications for predicting their perfor-mance in the iSSD. In the ISP, the execution cost is composed of (1) data access cost, (2) data processing cost for accessed data, and (3) global merge cost for aggregating all local results in core CPUs. Finally, we validated the cost models and showed the potential of the ISP through a series of experiments. The results show that our iSSD-based processing achieves speed up by up to 83 times com-pared with current host-CPU based processing.
This work was fi nancially supported by (1) Semiconductor In-dustry Collaborative Project between Hanyang University and Sam-sung Electronics Co. Ltd, (2) the National Research Foundation of Korea (NRF) grant funded by the Korea government (MEST) (No. NRF-2012047724), and (3) Seoul Creative Human Development Program (HM120006). [1] D. Bae, J. Chang, and S. Kim,  X  X n Ef fi cient Method for [2] J. Do et al.,  X  X uery Processing on Smart SSDs: [3] S. Kim et al.,  X  X ast, Energy Ef fi cient Scan Inside Flash [4] E. Riedel, G. Gibson, and C. Faloutsos,  X  X ctive Storage For [5] J. Han and M. Kamber, Data Mining: Concepts and [6] D. Du et al.,  X  X xperiences Building an Object-Based System [7] V. Reddi et al.,  X  X eb Search using Small Cores: Quantifying [8] L. Page et al., The PageRank Citation Ranking: Bringing
