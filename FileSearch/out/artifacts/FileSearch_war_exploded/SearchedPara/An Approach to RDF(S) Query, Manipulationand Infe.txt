 The semantic web is emerging as the nex t generation web, where web contents could be understood by machines. RDF(S) is a W3C standard for the formal-ization of information on web resources, laying one of the foundations of the semantic web. Thus effective and efficient management of RDF(S) data is a must, including storage, query, manipulation and inference.
 to support the management of RDF(S) data in large volumes. Therefore, in this paper we propose an approach to the storage, query, manipulation and inference of RDF(S) on top of the relational database, which is the primary choice to manage large-volume data in practice.
 inferred data, we store all the triples that are deduced from the RDF(S) closure computing, instead of calculating them on the fly. This is extremely imperative if a large data set is presented or the query is quite complex. One consequence of this decision is that we have to do inference on each manipulation, so as to maintain the consistence and compl eteness of the inferred data. Two modes of inference, i.e. batch mode and incremental mode, are therefore developed to reduce the maintenance cost according to different manipulation scenarios. In addition, we introduce the Original Semantics Assumption and accordingly cir-cumvent mutual dependency loops contained in the RDF(S) entailment rules while performing inference. The database schemas in our approach are elabo-rately designed so that not only query but also inference is well supported. We also introduce a language called RQML, which provides powerful and flexible query ability as well as manipulation ability. Finally, we report our performance evaluation, which shows that in a typica l scenario the inference time complexity is around O ( n 2 ), and the query time complexity is around O ( n ). ogy engineering environment called Orient [1], which has been released at the IBM AlphaWorks website 1 .
 schemas we adopted to store RDF(S) data. Section 3 discusses our inference algorithm on databases, including the batch and incremental mode. Section 4 introduces RQML, our RDF(S) query and manipulation language. The perfor-mance of our approach on query and inference is evaluated in Section 5. Finally we discuss the related work in Section 6 and conclude the paper in Section 7. In order to support efficient RDF(S) query and manipulation processing as well as RDF(S) inference (i.e. RDF(S) closure calculation), we carefully designed a relational database schema to store RDF(S) ontology. We followed the following principles in the design of the storage schema:  X  For high-performance query, manipulation and inference, we trade storage  X  Although the storage schema should be able to handle arbitrary RDF(S) Table.1 lists the database tables in the current storage schema design. The design of the tables are mainly influenced by and closely related to the RDF(S) closure computing algorithm, which is introduced in detail in the next section. bles. Typical queries about class hierarchy, property hierarchy/domain/range and instance type can be answered quickly by directly querying the corre-sponding table. In addition to these tables for certain RDF schema triples, the RDFLiteralInteger , RDFLiteralFloat , RDFLiteralBoolean and RDFLiteral String tables are created to hold common data type literals and leverage na-tive SQL data type comparison and calculation, which is required to support some kinds of queries. We also borrow Jena2 X  X  Property Tables design [2] in the RDFUserProp and RDFProp* tables to speed up the queries on certain user specified properties.
 contain a flag column which can take one of the following values: EXPLICIT, DERIVED and SUSPENDED .Thevalue EXPLICIT and DERIVED indicate whether the statement is explicitly declared or is derived by inference. The value SUSPENDED is a temporary value which will be used in the process of inference. For example, a statement like (?x, rdf:type, ?y) exists both in the RDFType table and RDFStatement table. In addition, as mentioned above, we store derived statements together with ex plicitly declared statements in the tables. This can also be seen as a kind of redundancy. However, this redundancy greatly facilitates and speeds up query processing.  X  As derived statements are stored together with the explicitly declared state- X  Since the data contained in each table are complete by itself, the minimum  X  For most simple queries, only a single statement table is involved. As a Reasoning on existing knowledge to discover implicit information is an important process on the Semantic Web. Common queries, such as  X  X hat are the (direct and indirect) sub-classes and instances of an exiting class X  and  X  X hich instances have a certain relationship with a given instance X , may all involve inference. semantic level, that is they want to query the derived data together with the explicitly declared ones. Currently in most RDF(S) management systems, the inference is not performed until it is imperative to answer a query. However, answering queries in this way might be time-consuming, especially when the data amount is large or the query is complex, w hich is the case in practice. Therefore, we choose to achieve better query performance at the cost of larger storage size. In other words, we choose to compute the complete RDF(S) closure and store all the derived RDF(S) statements in the database together with the explicit RDF(S) statements. When the knowledge base is modified, inference would be performed to maintain the consistency and completeness of the inferred data. ing rdf1, rdfs2  X  rdfs11, rdfs13 (as defined in the RDF Semantics docu-ment[3]).Wecallthesetwelverules the rule set and the involved five RDF(S) properties ( rdfs:domain , rdfs:range , rdfs:type , rdfs:subPropertyOf and rdfs:subClassOf ) the reserved property vocabularies . This rule set is selected based on the entailment rules X  importance and usage in common RDF ontology engineering scenarios.
 modes are provided, i.e. the batch mode and the incremental mode. In the batch mode, RDF(S) closure is not computed until the system is told to do so. This mode is suitable for batch update to the RDF data, since batch update may lead to a large amount of closure computation that would cost a long time. We design an optimized algorithm for this mode to compute the RDF(S) closure. In section 3.1, we will give the detailed description of this algorithm. to the RDF data. The algorithm for the incremental mode is more like a forward-chaining closure computing method. However, since the forward-chaining closure computing method does not support retra ctions, we design an special algorithm for this purpose. We will give the detailed description in section 3.2. 3.1 Inference in the Batch Mode A brute force method of calculating the closure is to repetitively grow the RDF knowledge base according to the rule set until a fix point is reached. This straight-forward method, however, is very inefficient, especially when it is performed on a relational database. By analyzing the calculation process, we found that the inefficiency of the process mainly ste ms from the following problems: the predicates X  derivation relationships caused by the rule set (Fig.1). Edge labels are names of the rules that cause the derivation relation. Two set of derivation relations are omitted from the graph: actually represent the problem P1. Without considering self loops, it is now clear from Fig.1 that rdfs6, rdfs8, rdfs10 and R2 create big and complex loops in the derivation graph, which is just the cause of P2.
 we first attack the problem P2. We find that, if some dependency relationships could be removed, then the graph could become a directed acyclic graph and the vertices (rules) could be topologically so rted, so the repetitive calculation in P2 can be reduced to just one pass. For this purpose, we now introduce the OSA (Original Semantics Assumption).
 Original Semantics Assumption. Let A be the set of RDF(S) axiomatic triples defined in the RDF Semantics document [3]. Let T be the closure of A un-der the rule set. Let  X  be the closure of the current RDF knowledge base under the rule set and R X   X  be the set of all the triples whose subject is in the set { rdfs:domain, rdfs:range, rdfs:type, rdfs:subPropertyOf, rdfs:subClassOf } . The original semantics assumption supposes that R X  X  . strengthen the semantics of the five re served RDF(S) property vocabularies. They still keep their original meaning defined by the axiomatic facts. In most RDF(S) ontology engineering scenarios, this assumption is quite natural and can be satisfied because most applications does not require the change of the original RDF(S) semantics.
 become trivial. We can apply them only once after the rdf:type closure is obtained. We can be sure that no more triples can be obtained from the results of them. That is, they can not create lo ops in the derivation graph. Now let X  X  look at the rdfs8 rule: Under the OSA, rdfs:subClassOf can not be other property X  X  sub-property and rdfs:Resource can not be other class X  X  sub-cla ss. Therefore, this rule can be applied only once after the rdf:type closure is obtained. We can be sure that no more triples can be derived from the result of it. Hence this rule can not create loops in the derivation graph either. We can now safely remove the back lines caused by rdfs6, rdfs8, and rdfs10 from Fig.1.
 cannot be the sub property of o ther predicates. Hence the rdfs7 rule can only create derivation relation from  X  X ther predicates X  to the five reserved vocabu-laries in Fig.1. If R2 is now drawn on Fig.1, the only loop it can create is the one between the  X  X ther predicates X  and the rdfs:subPropertyOf . However, this loop can be broken because the rdfs:subPropertyOf closure actually can be independently computed (step 2-4 of the algorithm described below). our algorithm can thus compute the entire closure in one pass.
 any resources or literals, with the restrictions that the predicates of a triple can only be URI references and the subj ects of a triple cannot be literals. which can be entailed by the rule set have been added to the database. We have also verified the correctness of our algorithm using W3C RDF test cases. sitive closure on rdfs:subPropertyOf and rdfs:subClassOf , actually a Floyd [4] like algorithm is used for better efficiency. Take rdfs:subPropertyOf for ex-ample. For each property ?y ,therule X  (?x, rdfs:subPropertyOf, ?y) , (?y, rdfs:subPropertyOf, ?z)  X  (?x, rdfs:subPropertyOf, ?z)  X  is applied by table join to insert entailed triples into database in batches. This implementation has better performance than the two intuitive implementations as follows. As to our Floyd like algorithm, the new triples are added to the database mainly in batches, while the self join of the RDFSubClass table is much more constrained, thus achieving better performance.
 employ table join and batch insertion SQL command. 3.2 Inference in the Incremental Mode In this section, we will describe the algorithm used in the incremental mode. In the incremental mode, every modification to the RDF(S) data will trigger computing the changes of the RDF(S) closure. The modification to the RDF(S) data can be regarded as appending some RDF triples and/or removing some RDF triples. We will discuss these two kinds of modification respectively. chaining closure computing is performed. The key point is that, according to the characteristic of the triple being appended, only a few rules in the rule set which may be relevant to the triple will be triggered. For example, if the predicate of the triple is not one of the RDF(S) reserved property vocabularies, only rdf1, rdfs4a, rdfs4b and rdfs6 will be triggered.
 the consistency of the RDF(S) data is not as easy as when inserting triples. Since some triples derived from the removed triples may also be derived from some remaining triples, they could not be simply removed. However, examining one by one whether they can be derived from remaining triples is greatly time consuming. So we propose an algorithm which first removes all the suspect triples and then perform an incremental batch closure computing.
 removed from the database immediately. Simultaneously, these triples are added to a queue for further processing.
 evant to the triple are checked to see if there are DERIVED triples in the database that could be derived from the triple being processed. 3 If such triples exist, they are marked as SUSPENDED and are added to the queue. This process continues until the queue becomes empty. Actually, it is a Breadth-First Search. tables which contain SUSPENDED triples are marked as DIRTY .
 used in the batch mode, is performed. The difference is that, if a table is not marked as DIRTY , the steps that add triples to that table will be skipped. In order for both users and programs to query and manipulate RDF data in an uniform manner, here we define a declarative RDF Query and Manipulation Language (RQML). RQML is designed based on several previous RDF query languages such as RQL[5], RDQL[2] and SeRQL[6].
 RDQL, while at the same time borrow features like path expression from SeRQL. In addition, as literal comparison and calculation are frequently used in prac-tice, RQML provides direct support of t hese features on several widely used XML literals. Such support is not available in most of the previous RDF query languages.
 URI references or literals. Further, the implementation of RQML queries allows the query results to be read in a streaming fashion. Therefore with all these features, the use of RQML in practice can be quite scalable.
 language, RQML includes some necessary manipulation commands like INSERT, DELETE and UPDATE.
 In this section we report the results of t he experiments performed to empirically evaluate the performance of query and inference of our approach. The inference performance test is first presented followed by the query performance test. called  X  X 57 X  that consists of only rdfs:subClassOf and rdfs:subPropertyOf relation triples that construct a class hierarchy tree and a property hierarchy tree. Both the two trees have a maximum height of 7 and a constant fan-out of 5. Another data set is  X  X N X , which is the RDF representation of WordNet 5 .All experiments are performed on a PC with one Pentium-4 2.4GHz CPU and 1GB memory running Windows XP Pro, using J2SDK 1.4.1 and Eclipse-SDK-2.1.1 connecting to a local machine DB2 UDB V8.1 Workgroup Server. The inference time is measured as the time cost to perform a full RDF(S) closure computing in the batch mode, starting from the database state of containing only the explicit triples. The query time is measured as the total time consumed from the sending of the query to finish fetching all the results from the database.
 the time complexity is the calculation of the transitive closure of sub properties and sub classes. The T57 data set is specifically designed to measure the empir-ical time complexity of it. By growing the two trees in T57 via adding one level of height each time, a series of growing d ata set for inference is got. The result is shown in Fig.2. The T57-1 line shows th e relation between the inference time and the number of explicit triples. The T57-2 line shows the relation between the inference time and the number of triples after inference.
 with a very large set of instances and instance relations. The instance data in the four WordNet RDF files are sampled at the same speed. The number of sampled triples are multiplied by 5 each time and the triples from the four files are put together to get a series of growin g data set for inference. The result is also shown in Fig.2. The WN-1 line shows the relation between the inference time and the number of explicit triples and the WN-2 line shows the relation between the inference time and the number of triples after inference. when the number of triples are large ( &gt; 1000). Linear regression analysis of the last four points at the end of each T57 line shows that the slopes are 1.87 and 1.75 for T57-1 and T57-2 respectively. This indicates approximately O ( n 1 . 87 )and O ( n 1 . 75 ) time complexity. In theory, calculating transitive closure using Floyd al-gorithm has worst-case time complexity O ( n 3 ). When the algorithm is performed on a database, many factors of the RDBMS may further affect the performance. Combining the experiment result, we tend to empirically predicate that, when performed on a largely tree hierarchy ontology like T57 on a relational database, the time complexity of our batch inference algorithm is around O ( n 2 ). of inferred triples and inference time are much lower than T57 It, however, shows the same trend of linear relation in Fig.2. Linear regression analysis of the last four data points at the end of each WN line indicates approximately O ( n 0 . 92 )and O ( n 0 . 93 ) time complexity. Similarly, we empi rically expect that, when performed on a largely instance data ontology like WordNet on a relational database, the time complexity of our batch inference algorithm is around O ( n ). inference is in linear proportion to that of the explicit triples. If an RDF(S) on-tology satisfies this property and the OSA, with characteristics between T57 and WN, we empirically estimate that its inference time complexity on a relational database is likely between O ( n )and O ( n 2 ). The n here can represent either the number of the explicit triples or the number of the triples after inference. with derived triples. We used the following four queries to test sub-class query, simple query, query with join and query involving literals: Q1 is performed on the T57 data set to obtain all subclasses of a given class. For each T57 data sample, and for each height of the tree hierarchy in that sample, Q1 is executed once using a class in that height. The query time of Q1 is then obtained as the average of the execution times. Q2, Q3 and Q4 are performed on the WordNet inferred data sets. The query time is averaged over 1000 query executions by randomly selecting a Word Net constant to replace the random constant in the above queries. The result is shown in Fig.3.
 pecially Q1. Linear regression analysis of the four lines shows approximately the worst-case time complexity of querying on one database table with indices is O ( n log n ). The actual query time also depends on the size of the result set. This test shows that the query time has a strong tendency of linear time O ( n ) complexity and the query is executed quite speedy. There exist some other RDF(S) management systems such as Jena2 [2] and the Sesame system [6]. They also support storage and management of the RDF data on a relational database, but they only treat database as an alternative storage method. When answering queries, performing the inference on the fly is time consuming. In contrast, our approach performs efficient inference on database and stores all the derived triples to optimize the online query answering response. algorithm in [7]. The main difference is that, in [7], after marking some of the triples as SUSPENDED , these triples are examined whether they can be entailed from explicit triples. However, as this examining process requires accessing the database very frequently and fragmentally, such an approach is not as efficient as it seems to be. On the other hand, although our algorithm may first remove some triples and then insert them back to the database, these removal and insertion operations are executed in a batch manner (that is, use less SQL commands to circumvent the problem P3 discussed in Sec.3). Thus, our algorithm could run faster than the one proposed by [7] in most cases.
 generic rule-based forward-chaining algorithm, RETE is very efficient. However, as the RETE algorithm will consume lot s of temporary memory in the inference procedure, it is not suitable for processing large-volume RDF data on databases. In this paper, we present our approach on query, manipulation and inference of RDF data on relational databases. Different from most of the previous systems, the design of our approach aims at highly efficient query and inference with large-volume RDF data on relational databases. For query, we chose to store all the derived triples in database as well as the explicit ones. For inference, we carefully designed our algorithms for two inference modes, i.e. the batch mode and the incremental mode. We also defined a powerful RDF(S) query and manipulation language RQML, and presented the evaluation result of our approach.
