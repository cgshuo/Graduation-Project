 Categories and Subject Descriptors: H.4.m [ Information Sys-tems ]: Miscellaneous General Terms: Algorithms, Measurement Keywords: Web Dynamics, Web Archive Search, Web Graph, PageRank
The World Wide Web is increasingly becoming the key source of information pertaining not only to business and entertainment but also to a spectrum of sciences, culture, and politics. However, the Web has an even greater source of information within it  X  evo-lutionary history of its structure and content. It not only captures the evolution of digital content but embodies the near-term history of our society, economy, and science. Although efforts such as the Internet Archive [1] are archiving a large fraction of the Web, there is a serious lack of tools that are designed for the effective search over these Web archives.

Time travel queries are aimed at supporting the evolu-tionary (temporal) analysis over Web archives extending the power of Web search-engines. Specifically, a time travel query Q is defined as a pair  X  Q ir ,Q tc  X  , where Q ir is the IR-style keyword query and Q tc is the target tempo-ral context. For example, consider the following time travel query which asks for pages concerning Olympics Games 2004, Q =  X  Q ir : {  X  X lympic X ,  X  X ames X  } ,Q tc : 15 /July/ 2004  X  . It is required that the Q ir be evaluated and ranked based on the state of the archived collection as of the time instance Q tc .

Effective results for such time travel queries consist of a list of pages that are ranked based on a combination of their content rele-vance with regard to the query terms and a query-independent mea-sure reflecting their authority . Due to the high dynamics of the Web, current authority scores do not accurately reflect historical authority of Web pages. In this work, we therefore focus on recon-structing historical PageRank scores, a popular authority measure. The reconstructed scores can then be combined with traditional measures of content relevance such as tf  X  idf or OKAPI BM25 to obtain the final scores that determine the ranking of Web pages.
We first introduce a novel normalization scheme for PageRank scores that enables their comparison across instances of the Web graph at different times. Building on a time-series representation of these normalized scores, we propose a compact Rank Synopses structure that allows efficient reconstruction of historical PageRank scores on Web archives.
PageRank is a well known link-based ranking technique, widely adopted both in practice and research. Given a directed graph G ( V,E ) representing the link graph of the Web, the following for-which is again the score assigned to a node without incoming edges. We use this refined lower bound for normalizing the PageRank scores  X  for a node v its normalized PageRank score is defined as
The proposed normalization eliminates the dependence on the size of the graph with very little additional computational cost. For the earlier example, the normalized PageRank scores of the gray and the white nodes do not change as can be seen from the table in Figure 1. Further details of the normalization technique have been omitted here due to space limitations.
At each observation of an evolving Web graph, G , one can compute PageRank scores for all nodes in the graph. For a given time series of such PageRank scores of a Web page, approximation given by, Elements ( [ s i ,e i ] ,  X  i ) of  X  contain a set of parameters  X  i of the linear function that is used to approximate the time series on the time interval [ s i ,e i ] and are referred to as segments in the remain-der. The segments cover the whole time period of the time series, i.e., and time intervals of subsequent segments have overlapping right and left boundaries, i.e.,
Our goal is to construct a rank synopses having a minimum num-ber of linear segments while retaining a guarantee on the approx-imation error per observation. This approximation error per seg-ment is defined as the maximal relative error made on an observa-tion within the segment, i.e., A tunable parameter  X  is used as a threshold for the approximation error thus controlling the quality of the synopses fit.
An optimal rank synopsis can be computed using a dynamic pro-gramming algorithm having overall O ( n 4 ) time complexity, while a close-to-optimal rank synopsis can be generated using a greedy heuristic that reduces the time complexity to O ( n 2 ) [5]. Further-more, close-to-optimal rank synopses can be maintained incremen-tally as new observations of the evolving Web graph become avail-able.
Although we used a variety of datasets for our analysis, in this paper we report results over the evolving graph obtained through the revision history of the English Wikipedia encyclopedia [2]. This dataset contains the editing history of Wikipedia spanning the time window from January 2001 to December 2005 (the time of our download). From this rich dataset we extracted a graph whose nodes correspond to articles and edges correspond to their interconnecting hyperlinks. This graph has 1,618,650 nodes and 58,845,136 edges in total. We took 60 monthly snapshots of this graph and using the popular value = 0 . 15 as our random jump probability, we precomputed PageRank scores for each month.
Kendall X  X   X  is used in our experiments to compare rankings. We employ the implementation provided by Boldi et al. [3] to compute Kendall X  X   X  values reported in the experimental results. As per the definition that they have used, these scores are in the range [  X  1 , 1] , with 1 (  X  1 ) indicating a perfect agreement (disagreement) of the two compared permutations.

