 1. Introduction The wider availability of electronic resources through the Internet and more specifically the World Wide
Web has made possible the investigation of empirical regularities in the properties of full text electronic documents, their surrogates, and how they are used. This sub-area of informetrics, known by names such as cybermetrics or webometrics, has garnered growing research interest since the mid 1990s (Almind &amp; Ingwersen, 1997). Data captured by Web crawlers and usage characteristics recorded by search ser-vices have provided large and readily analyzable data sets, making informetric study of electronic infor-mation production processes (Egghe &amp; Rousseau, 1990) feasible. In fact, interest in these regularities has extended beyond researchers involved in informetrics, to computer, physical and cognitive scientists for whom these regularities may inform system design and usage behavior. However, beyond reports of data sets following inverse power laws (Broder et al., 2000; Huberman, 2001; Nielsen, 1997a, 1997b), long known by informetricians to apply to print and bibliographic database sources, little in-depth study has been undertaken to rigorously model data sets to a range of distribution families beyond simple Zipf or related power law models using more than visual inspection of plotted data sets to determine goodness-of-fit. Many aspects of regularities in Internet/Web content, indexing, retrieval, and use await more detailed study, including document field attributes, indexing characteristics, index and document growth, domain characteristics, user querying characteristics, browsing patterns, and resource utilization (Wolfram, 2003).

The present study investigates the frequency distribution of image tags within Web pages. The use of image tags provides an indication of the image richness of a set of Web documents. The patterns of image usage within Web pages have implications for both efficient document transfer and user interaction. Al-though the use of graphics is commonly used to attract user attention, the frequency of graphics on a given page can have important implications for page content download time and text-based browsers used by disabled users who cannot interpret graphical content. Image frequency also provides an indication of how many server requests must be made to completely access a document, assuming images are not cached locally (Note: Although the current version of the hypertext transfer protocol [HTTP 1.1] used to transfer
Web content permits persistent and more efficient Web server connections for the transmission of multiple inline documents, such as images, without having to establish a new connection for each request, server requests must still be made for each image within a given connection). A shallow frequency distribution of images per document, indicating a higher proportion of documents with a larger number of images, can point to a space and, hence, a bandwidth intensive collection. Conversely, a relatively steep distribution reveals a more conservative use of images per document. A related area of investigation, the distribution of specific image occurrences within or across documents, also represents a topic awaiting investigation.
Specific instances of image types within individual documents or across a document set could be investi-gated in the same way that text types (i.e., different words) and tokens (i.e., occurrences of specific words) are currently studied, where file names correspond to image types, and instances of a given image represent tokens. Such analyses would reveal patterns of duplication in image usage, for example in the repetitive use of icons as images. Identification of image types could be problematic, as images representing different types may share a common name. The use of image size information could be useful in distinguishing the different types with common names. The distribution of image type frequency across a document set would be useful for determining redundancy in image content across a website. Because images can be quite large, having multiple copies of images across a site can increase disk space requirements. Repetition may also have implications for image representation integrity. As an example, on academic or corporate websites, there may be an expectation for every page to use the same banner image. With multiple copies of the image scattered throughout a server, if the image needs to be modified, changes would need to be reflected on every instance of the image. If the distribution of image occurrence reveals a high degree of redundancy, images could be accessed from a centralized location.

In the present study, the investigators explore the distribution of the use of image tags in top level websites across five top level Internet domains. The distribution of image tags on pages within individual websites for these domains is also explored. The primary research questions guiding the present research are:  X  Is the average number of image tags on top level pages the same for the five domain types?  X  Does the distribution of image tags on top level pages vary by top level domain (TLD)?  X  How can the distribution of image tags within top level Web pages and across Web pages within a given website best be modeled?
Findings of the study will provide a better understanding of observed image usage on Web pages, which may have implications for Web page usability by people with disabilities, and for the downloading of Web pages. Also, this study contributes to informetric modeling in electronic environments by investigating additional distributions beyond traditional power models (Huberman, 2001; Nielsen, 1997a, 1997b), to determine the applicability of different theoretical distributions for use in modeling Internet/Web regu-larities. The fact that one can observe a zero class in some Web phenomena, whereas traditional mathe-matical models used in informetrics based on inverse power forms are not capable of modeling processes with a zero class without adding a shift component or eliminating the zero class from the data set, provides additional motivation to investigate a wider range of models that can accommodate a zero class. 2. Previous research
Informetric studies of Web characteristics can be divided into two broad categories: (1) system studies based on regularities in domain/document content and their implications for efficient storage and retrieval, and; (2) user studies (e.g., query and session analysis, resource utilization). Literature discussed here focuses on the former, where processes studied are at the domain, document, or sub-document level.

Although much more recent in origin than other areas of informetric research, cybermetric studies investigating empirical regularities of Web content and use have evolved rapidly over the past decade.
Studies have focused primarily on Web size and growth (Albert &amp; Barabasi, 2000; Albert, Jeong, &amp; Bar-abasi, 1999; Huberman &amp; Adamic, 1999) and linkage characteristics (Broder et al., 2000; Pennock, Flake,
Lawrence, Glover, &amp; Giles, 2002). The broader appeal of these studies is also apparent. Web search service evaluation sites such as Search Engine World ( http://www.searchengineworld.com ) publish statistical findings of different search engines, including the high-level domain frequency distribution for Google ( http://www.google.com ). Recent monographic works such as those by Chakrabarti (2003) and Huberman (2001) address a number of issues in patterns observed with domain analysis, but primarily from a Web mining, as opposed to informetric, perspective.

The distributed nature of domain level data can present data collection challenges not found in site-specific data analysis, where a complete data set is available. This is comparable to more traditional print-based informetric studies where investigators are dealing with a potentially incomplete data set because of the lack of availability or awareness of additional relevant sources. A domain-level study by Rousseau (1997) investigated the frequency distribution of TLDs for websites indexed by the AltaVista search service that deal with informetrics, scientometrics, or bibliometrics. The TLD affiliation of the resulting 343 documents was tabulated into a domain distribution. The observed distribution was found to fit a Lotka model.

Adamic and Huberman (2001) investigated the distributions of several domain-level data aspects for websites, including the number of pages indexed per website using data from infoseek.org and archive.org.
By plotting the number of pages (or text files) per site against the proportion of sites containing that number of pages using a logarithmic transformation on each axis, they were able to fit an inverse power law distribution  X  1 = x b  X  , although the b values of the parameters for these data sets were not reported. The slightly convex plot of the logarithmically data, as opposed to a straight line observed for data following power laws, would indicate that a more general model is needed to adequately fit the site size data. The use of theoretical distributions used in informetric research merit investigation for what appear to be an inverse relationship with a long tail. The authors noted that most websites consist of a small number of pages whereas very few consist of millions of pages. The definition of what constitutes a website was not provided, but a completely inverse relationship based on the number of Web pages would mean the most frequent number of pages per site is one. This would seem small for an Internet domain, but perhaps not for the number of top level Web pages per site. If one could demonstrate the applicability of a Lotka/Zipf model for domain distributions, the lack of fit to these models within a search service index could indicate the lack of comprehensive indexing by the system. Further research is needed to better define the characteristics of the distribution of domains over the Web or within search service indexes. Adamic and Huberman also examined the download times for the main pages of a randomly selected number of websites. They found the distribution to be heavy-tailed and fit it to a lognormal model.

Document content studies that examine Web page or site attributes have primarily dealt with inlink (i.e., hypertext links pointing to a given Web page or site) and outlink characteristics (i.e., hypertext links originating from given Web page or site). Albert and Barabasi (2000) studied linkage models of evolving networks and demonstrated two potential topologies and their influence on prediction models. The dis-tribution of connections was shown to follow a form of power law. Dorogovtsev, Mendes, and Samukhin (2000) and Levene, Fenner, Loizou, and Wheeldon (2002) proposed generalizations to Albert and Bara-basi  X  s model in subsequent studies. Broder et al. (2000) conducted a large-scale study of graph structure on the Web examining the distribution of inlinks and outlinks. The authors concluded that the frequency distributions of links follow power laws based on visual inspection of resulting logarithmically scaled plots.
No additional goodness-of-fit testing was performed; however the authors recognized distinctions in out-comes resulting from size-frequency forms of power laws and rank-frequency Zipf models. Pennock et al. (2002), in a study of inlink distributions for subsets of the Web, demonstrated that a power law does not apply on smaller scales, represented by different types of Web pages such as company and university Web pages. Unlike power law distributions, where an inverse relationship exists between the number of links and their frequency across pages, a unimodal distribution was observed with modes occurring at greater than the minimal linkage values. Similarly, Adamic and Huberman (2001) fitted the distribution of inlinks and outlinks for a set of websites, finding that both fitted an inverse power law for sites containing at least several dozen inlinks or outlinks. The relationship broke down for smaller numbers, where the proportion of sites with a smaller number of inlinks and outlinks was overestimated. Rousseau (1997) also investigated the distribution of inlinks to a set of websites dealing with informetrics. The initial frequency distribution could not be initially modeled to any classical informetric distribution. By assuming an implicit link from each site to itself, in essence removing the class of zero links, the observed distribution could be fitted to a
Lotka model (or size-frequency form of a Zipf model). He noted that more than 70% of the sites did not receive any inlinks from outside sources.

The study of Web document attributes may extend to image usage. Extraction of image attribute data may be studied in document collections formatted with markup languages such as HTML, SGML or
XML. At least two approaches to the study of image attribute regularities are possible: their presence in documents and their properties. First, the frequency of occurrence of images within documents as an attribute of the documents themselves can be studied in the same way that outlinks in hypertext documents are studied. Woodruff, Aoki, Brewer, Hauthier, and Rowe (1996) investigated the average frequency of use of different markup language tags within Web documents, but did not report on the frequency distributions of occurrence of these tags.

Theoretical distributions used in existing studies of Web content characteristics to date have been limited to primarily Zipf and Lotka models. Their applicability for document characteristics other than inlink/ outlink and indexing environments has not been tested. More sophisticated distributions may model a wider range of observed behaviors, and, therefore, could be more appropriate for fitting purposes. Many distributions have been proposed or tested for different informetric phenomena. For example, Ajiferuke (1991) investigated co-authorship distributions for data sets from a range of disciplines and tested the goodness of fit of 16 different mathematical distributions for goodness of fit to the data sets collected over six time periods. Based on initial assumptions about the research process, a shifted waring distribution to model the data was proposed. After testing different models, the author concluded that a two-parameter inverse Gaussian X  X oisson (IGP) distribution was most generally appropriate to model patterns of co-authorship. A three-parameter generalized form of the IGP, called the GIGP was proposed by Sichel (1985, 1992). He demonstrated the applicability of the distribution for modeling several types of informetric data, although the tractability of the distribution remains challenging. Nelson (1989) relied on theoretical models to fit term frequency data from several database indexes. In addition to the GIGP and a three-parameter
Mandelbrot X  X ipf model, he also relied on a generalized waring distribution (Irwin, 1975), with each dis-tribution demonstrating a best fit for one or more data sets. The GIGP was also successfully modeled to frequency distributions of music representation data (Nelson &amp; Downie, 2002).

Despite the recent research interest in domain and document content studies, rigorous testing of different theoretical models to different types of cybermetric data sets has not been widely undertaken. A better understanding of the use and distribution of image tags within Web pages has implications for the number of requests made for images from a Web server. The relative richness or scarcity of images across Web pages may also impact usability by users who cannot interact with or benefit from non-textual page content. 3. Methodology 3.1. Data collection
Resource discovery remains an issue on the Web, even with powerful spidering software. To collect data for the study, methods of identifying Web pages and extracting image content were first developed. Five
TLDs were initially selected (.edu, .com, .gov, .net, .org) for comparison. These five domains are well-established and provide a general sense of the topical nature or purpose of the websites they each represent, unlike national TLDs, which can encompass any type of website, thus making it difficult to categorize websites by the type of content they provide without investigating each site. One shortcoming of this ap-proach is that it may introduce an American bias for some domains, most notably for the .edu domain, which represents US colleges and universities only.
 Image tag usage data were collected for top level pages of randomly selected URLs within each domain.
Top level pages were selected for each domain, representing level 2 domains (L2Ds) within each TLD (e.g., www.L2D.TLD ), because they represent the primary entry point to a website  X  s content. Top level URLs for the domains were identified in two ways: through publicly available lists and from search engine results. A comprehensive list of URLs within the .edu domain ( http://www.clas.ufl.edu/CLAS/american-universi-ties.html ) was parsed for the individual sites. For the remaining domains, top level URLs were collected by conducting a broad level search with a domain limiter using AltaVista. Because the maximum number of returned sites by AltaVista is 1000, queries were modified with different rankings specified to include new
URLs within the top 1000 returned to increase the sample size. Lists were combined with duplicates and references to internal-level URLs removed. Although AltaVista returns results based on a relevance ranking, the sample can still be said to be unbiased, as the measurement of interest in the study is the number of image tags within pages, not the relevance ranking of results. These data sets served as the basis for the image tag extraction. A Java application was written to return the page content and to count the number of  X  IMG  X  tags appearing on the page. Not all URLs returned from the searches were reachable by the routine. URLs that were not longer valid or unavailable were ignored, resulting in samples of varying sizes for each of the TLDs.
When fitting models using samples from a large population, the sample sizes need not be a large percentage of the overall population. Notwithstanding, samples must be large enough to ensure a high level of statistical power and to reduce the risk of making a Type II error X  X  X n this case, erroneously concluding that a theo-retical model does not adequately fit the observed data. For a large population (i.e., hundreds of thousands or larger), which is observed for the number of L2Ds within the .com, .net, and .org TLDs, a sample size of at least 800 is adequate to ensure sufficient statistical power at a significance level of 0.05 (Murphy &amp; Myors, 1998, Chapter 3). See Table 1 for the number of URLs selected for each TLD.

Individual Web pages may be image-rich or image-sparse for several reasons, making it possible to count images in several ways. For example, pages containing JavaScripts with image rollovers display more than one image. These could be counted or treated as single images. For this study, each image file must be requested from the server and is, therefore, counted separately. Also, pages based on frames technically contain only frame references to additional pages that may contain images. In the case of frame-based top level pages, the Java routine followed each frame to tally the  X  IMG  X  tags across the pages referenced by the frames.

In order to examine the distribution of images on a website, four websites of varying sizes were purposely selected for each TLD investigated. For each selected website, the number of  X  IMG  X  tags appearing on each of its pages was counted using a routine developed in Perl. Only html page types (e.g. .html, .asp, .php, .cgi, and .cfm) were examined, whereas .txt, .pdf, .exe, and zip files were ignored. It should also be noted that in some cases the routine could not reach selected pages on a website. These pages were, therefore, ignored.
Image counts were tallied to form observed image frequency distributions and served as the data sets to which theoretical models were fitted. 3.2. Theoretical models tested
As noted above, many researchers have used forms of the inverse power law to model the character-istics of Web phenomena. Two approaches may be used in data representation and modeling. Some researchers have used a size-frequency form (e.g., Lotka  X  s law), whereas others have used a rank-frequency form (e.g., Zipf  X  s law) of the inverse power law. The authors of the present study employ the size-frequency form of the data sets for modeling purposes. Data conversion is otherwise needed for rank-frequency forms along with a method for dealing with the numerous ties that occur in informetric data sets.
Also, an inverse power model normally has a minimum class value of one. Because a Web page may conceivably contain no images, a modified form of the power law that recognizes a zero class (Rousseau, 1997), was adopted.

In addition to the modified power law, other theoretical distributions that have been used successfully in modeling informetric characteristics that exhibit inverse patterns were tested. These include the size-fre-quency form of the Mandelbrot (Mandelbrot, 1954), generalized inverse Gaussian X  X oisson (Burrell &amp;
Fenton, 1993; Sichel, 1985, 1992), generalized waring (Irwin, 1975), and generalized negative binomial distributions (Famoye, 1997). The forms of the distributions tested are as follows: (i) Modified power law (MPL) (ii) Mandelbrot (MDB) (iii) Generalized waring (GW) (iv) Generalized inverse Gaussian X  X oisson (GIGP) (v) Generalized negative binomial (GNB)
For the first four models, the parameters were estimated using the minimum chi-square method em-ployed by Nelson and Downie (2002). In the case of the generalized negative binomial distribution, the most common methods of estimating the parameters are the maximum likelihood method, three moments methods, first two moments and proportion of zeros, minimum chi-square method, and first two moments and ratio of first two frequencies. According to Famoye (1997), for a finite sample size, the first two moments and proportion of zeros method is recommended because the estimates have very small biases and variances. This method was adopted for the current study, but the three moments method was used in a few cases where the estimates could not be computed.

The two primary methods used to measure the goodness-of-fit between observed and fitted theoretical distributions are the chi-square test  X  v 2  X  and the Kolmogorov X  X mirnov (K X  X ) test. The authors have adopted the chi-square test for two reasons. First, the K X  X  test was initially intended to be used only where the hypothesized distribution is completely specified (Conover, 1980), but, in situations where parameters are to be estimated, the test has to be modified, and different critical values obtained for each hypothesized distribution. For discrete distributions, one often has to use Monte Carlo simulation (or bootstrapping) in order to obtain these critical values. Second, whereas the K X  X  test is argued to be more powerful than the chi-square test for continuous distributions or when the sample size is small, the chi-square test is as powerful as the K X  X  for a discrete hypothesized distribution and a large empirical data (Famoye, 1999; Sichel, 1992). 4. Findings 4.1. Comparison of the number of images across domain types Basic descriptive measures of the number of images on top level pages for each TLD appear in Table 2. Given the size of the Web, the samples collected for the .com, .org and .net domains may appear small when compared to the total number of hosts within each of these TLDs. However, the number L2Ds within each
TLD is much smaller than the total number of hosts. Also, for the .edu and .gov domains, the total number of L2Ds is relatively small to begin with. The samples collected from these domains respectively represent more than 25% and 50% of the total number of L2Ds based on the total hosts per domain reported by the Internet Software Consortium ( http://www.isc.org/ds/ ) as of January 2003.

For each domain, the median is more appropriate than the mean as a measure of central tendency due to the skewness of the distribution of images. A Kruskal X  X allis test (the lack of homogeneity of the variances makes it inappropriate to use the analysis of variance test) shows that there is a significant difference among the mean ranks of images for the five domains ( v 2  X  79 : 686, d.f.  X  4, and p  X  0 : 000).

A multiple comparison test shows that the mean rank of the number of images on top level pages of .com or .edu sites is significantly higher than the mean rank for .net, .gov or .org sites. This indicates that .com and .edu sites tend to have more images on their top level pages than .net and .org sites. There is, however, no significant difference between .com (or .edu) and .gov TLDs. 4.2. Modeling of distribution of images on top level pages for a domain type
The distribution of images on top level pages for each domain type is approximately reverse J-shaped with zero as the mode. Fig. 1 displays the observed distribution for the .org data set.

The five models were fitted to the five data sets. Overall, the GIGP performed best for all data sets, providing a good fit for four data sets at p  X  0 : 05. The GNB provided a reasonable fit to only one data set, whereas the MPL, MDB and GW could not adequately model any of the data sets (Table 3). A summary of the fitted model characteristics for the GIGP distribution appears in Table 4. 4.3. Modeling of distribution of images on pages within a website
Four sites from each TLD were randomly selected, resulting in 2 0data sets. The number of pages on the selected websites ranges from 101 to 1494. The distributions of images on pages of websites vary from being approximately bimodal (see for example Fig. 2) to being approximately multimodal (see for example Fig. 3). The five models were fitted to the 2 0data sets, but none provided any good fit (see Table 5).
It is not surprising that none of the models provide a reasonably good fit to the data sets because these models are unimodal distributions, whereas the data sets are bimodal or multimodal. In a situation such as this, where the frequency distribution is non-homogeneous, a popular option is to try a mixture of uni-modal distributions. A mixture of distributions is a superimposition of distributions with different func-tional forms (e.g. Poisson and negative binomial distributions) or different parameters (e.g. two Poisson distributions with different values of k ), in specified proportions (Johnson, Kotz, &amp; Kemp, 1993). For example, Baayen (2001) fitted a mixture model to the word frequency distribution of Turkish text on archaeology. The mixture model has a lognormal distribution as the base component with a mixing parameter of p  X  0 : 2 (i.e. 20% of the tokens are allocated to the base component) and a GIGP distribution as the top component. Barford and Crovella (1998) also fitted a lognormal distribution to the main body of the distribution of Web document size for different document types (i.e., html files, image files, audio files, and video files with size being measured in bytes) with a mixing parameter p  X  0 : 93, and a Pareto distri-bution fitted to the  X  X  X eavily-tailed X  X  base component.

Hence, to get reasonably good fits, our bimodal data sets would have to be partitioned into two com-ponents. For example, the data set for www.cerious.com was partitioned into a main component with 428 tokens (i.e. a mixing parameter of p  X  0 : 98) and a top component with 7 tokens. In this case, the mixing proportion for the top component is too small for any meaningful model fitting; therefore we only at-tempted to fit a model to the main component. The distribution for the main component is approximately reverse J-shaped (Fig. 4), and the generalized negative binomial model provided a good fit to the data set (Table 6).

Data sets that are multimodal must be partitioned into three or more components. For example, the data set for www.baaqmd.gov was partitioned into three components with mixing parameters p p  X  0 : 27, whereas the data set for www.rutgers.edu was partitioned into four components with mixing parameters p 1  X  0 : 20, p 2  X  0 : 22, and p 3  X  0 : 44. A mixture of three generalized negative binomial distri-butions with different parameters provided a reasonably good fit to the www.baaqmd.gov data set (Table 7). A mixture of four generalized negative binomial distributions with different parameters provided a reasonably good fit to the www.rutgers.edu data set (see Table 8). 5. Discussion The finding that .com and .edu sites possess a significantly higher average number of image tags than
Web pages associated with the .net and .org domains supports the notion that image-rich sites are used to attract user attention. To attract and keep users on their sites, commercial and educational websites will rely on more images to create visual stimulation or to encapsulate ideas/products. The .net and .org do-mains, on the other hand, are less likely to have many images on their opening pages, suggesting that these sites do not rely as heavily on images to promote their sites. Although following roughly a reverse J-shape, the relatively shallow distributions observed (i.e., a small percentage of sites are associated with the lower number of images per page, with a gradual decrease in frequency with an increase in the number of images per page) is atypical of other classic informetric phenomena consisting of a large number of rare events (Baayen, 2001).

The multimodal distributions observed for the within-site data sets reveals a less regular pattern of occurrence of image tags across Web pages. The multimodal data may be influenced by several factors having to do with sampling or the nature of image tag usage. The smaller the website with respect to the number of pages it contains, the more likely it is that the observed distribution will be less regular. Also, repeated use of the same specific images across all pages or for a subset of pages that contain logos, for example, or are used to standardize page content will impact the distribution by skewing the average number of image tags used per file. The repeated use of specific images across pages can be demonstrated by an example. To informally investigate this property, the specific URLs for image tags appearing on 146 pages of the website www.discountpetsdrugs.com were collected and tallied (Table 9). The vast majority of distinct images, that is different image URLs, appear only one time. However, when one cumulates the occurrence of image tokens that occur more than once, these account for approximately 57% of all image tags (474 of 828).

Several of the distinct images, in addition to appearing on more than one page also appear multiple times on individual pages. From a space efficiency and content integrity perspective, it makes sense to have only a single copy of a given image across a website and have it referenced absolutely by each page incorporating the image, instead of duplicating the image across directories. Once a multiply-occurring image is requested from the Web server, it should not be necessary to repeat this for additional occurrences of the same page or other pages containing the same image, as these should be available from the browser  X  s cache if images were referenced from a centralized location. So, in this example, with 373 distinct images and 828 tokens, less than half of the image tags encountered should result in a file request to the server, if already cached.
Limitations that arise from the data collection for this type of study are apparent. The authors have not examined the sizes of images, only their frequency, which impacts the number of requests made to Web servers but will have less to do with the time required to download images. The number of image tags used per Web page may also be influenced by its storage size or length, where one may argue that larger pages with more content may be more likely to contain a greater number of images. On the other hand, one cannot equate the amount of content a Web page contains based on its size as there is the potential for large quantities of hidden formatting tags and scripts that do not add to the overall content of a page but do add to its storage requirements. There is also the potential for the use of duplicate images within a given page, for example, to serve as graphical bullets for a series of points, where local cache would be referenced instead of a request being made to the Web server. The determination of image token equivalence may be based on the same name used, file size, and creation date. However, different copies of the same image using a different name or URL will result in a new image type being tabulated instead of an added token for an existing image. Additional study of these image characteristics that account for image token equivalence is needed. 6. Conclusions
In this study the authors have examined the distribution of image tag usage on top level Web pages across five TLDs and the distribution of image tags within individual websites. Although following an inverse relationship between number of images and Web pages, the observed distributions are more var-iable than classic informetric data sets, making fitting the observed data sets to theoretical models more challenging. For the distribution of top level Web pages, a GIGP provided the best fits of the five models tested, providing non-significant departures from the observed data sets for four of the five data sets at the 0.01 significance level. The fitting of image tag distributions on pages within individual websites proved to be even more challenging due to the multimodal nature of some of the data sets. A mixture of distributions has the potential to improve the fits to such data sets, and this was illustrated with three of the data sets for which we were able to obtain reasonably good fits with a mixture of generalized negative binomial dis-tributions. It is apparent that not all observed regularities in Web-based documents can be fitted with simple inverse power distributions.

With knowledge of the distribution of image tags within Web pages, one may estimate the number of requests that need to be made to a Web server to download the complete content of Web pages, or the likelihood that a specific image is already located in a local cache. Future research should further examine the distribution of image tag usage within other sites, both for small websites and larger ones. With an increase in the number of TLDs, additional domains based on organization type X  X  X s opposed to geo-graphic domains that have no common attributes other than geography X  X  X ould also be investigated. The frequency distribution of specific images within and across pages of a given website also merits further investigation to permit the modeling of browser requests for specific images.
 Acknowledgements The authors would like to thank Karl Fast, a doctoral student at the Faculty of Information and Media
Studies, University of Western Ontario, for developing the Perl routine used in collecting within-site image tag data.
 References
