 University of Maryland, College Park
The combination of recent developments in question-answering research and the availability of medical domain provides a unique opportunity to explore complex question answering in the domain of clinical medicine. This article presents a system designed to satisfy the information needs of physicians practicing evidence-based medicine. We have developed a series of knowl-edge extractors, which employ a combination of knowledge-based and statistical techniques, for structured representations of information needs, in accordance with the principles of evidence-can bring relevant abstracts into higher ranking positions, and from these abstracts generate responses that directly answer physicians X  questions. We describe three separate evaluations: one focused on the accuracy of the knowledge extractors, one conceptualized as a document reranking of real-world clinical questions show that our approach significantly outperforms the already competitive PubMed baseline. 1. Introduction
Recently, the focus of question-answering research has shifted away from simple fact-based questions that can be answered with relatively little linguistic knowledge to  X  X arder X  questions that require fine-grained text analysis, reasoning capabilities, and the ability to synthesize information from multiple sources. General purpose reasoning on anything other than superficial lexical relations is exceedingly difficult because there is a vast amount of world knowledge that must be encoded, either manually or auto-matically, to overcome the brittleness often associated with long chains of evidence. This situation poses a serious bottleneck to  X  X dvanced X  question-answering systems. How-ever, the availability of existing knowledge sources and ontologies in certain domains provides exciting opportunities to experiment with knowledge-rich approaches. How might one go about leveraging these resources effectively? How might one integrate statistical techniques to overcome the brittleness often associated with knowledge-based approaches? cusing on the information needs of physicians in clinical settings. This domain is well-suited for exploring the posed research questions for several reasons. First, substantial understanding of the domain has already been codified in the Unified
Medical Language System (UMLS) (Lindberg, Humphreys, and McCray 1993). Sec-ond, software for utilizing this ontology already exists: MetaMap (Aronson 2001) identifies concepts in free text, and SemRep (Rindflesch and Fiszman 2003) extracts relations between the concepts. Both systems utilize and propagate semantic infor-mation from UMLS knowledge sources: the Metathesaurus, the Semantic Network, and the SPECIALIST lexicon. The 2004 version of the UMLS Metathesaurus (used in this work) contains information about over 1 million biomedical concepts and 5 million concept names from more than 100 controlled vocabularies. The Seman-tic Network provides a consistent categorization of all concepts represented in the
UMLS Metathesaurus. Third, the paradigm of evidence-based medicine (Sackett et al. 2000) provides a task-based model of the clinical information-seeking process. The
PICO framework (Richardson et al. 1995) for capturing well-formulated clinical queries (described in Section 2) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system. The conflu-ence of these many factors makes clinical question answering a very exciting area of research.
 service has been well studied and documented (Covell, Uman, and Manning 1985;
Gorman, Ash, and Wykoff 1994; Ely et al. 1999, 2005). MEDLINE, the authoritative repository of abstracts from the medical and biomedical primary literature maintained by the National Library of Medicine, provides the clinically relevant sources for answer-ing physicians X  questions, and is commonly used in that capacity (Cogdill and Moore 1997; De Groote and Dorsch 2003). However, studies have shown that existing systems for searching MEDLINE (such as PubMed, the search service provided by the National
Library of Medicine) are often inadequate and unable to supply clinically relevant answers in a timely manner (Gorman, Ash, and Wykoff 1994; Chambliss and Conley 1996). Furthermore, it is clear that traditional document retrieval technology applied to MEDLINE abstracts is insufficient for satisfactory information access; research and experience point to the need for systems that automatically analyze text and return only the relevant information, appropriately summarizing and fusing segments from multiple texts. Not only is clinical question answering interesting from a research perspective, it also represents a potentially high-impact, real-world application of lan-guage processing and information retrieval technology X  X etter information systems to provide decision support for physicians have the potential to improve the quality of health care.
 cine (EBM), a widely accepted paradigm for medical practice that stresses the impor-tance of evidence from patient-centered clinical research in the health care process.
EBM prescribes an approach to structuring clinical information needs and identi-fies elements (for example, the problem at hand and the interventions under con-sideration) that factor into the assessment of clinically relevant studies for medical practice. The foundation of our question-answering strategy is built on knowledge extractors that automatically identify these elements in MEDLINE abstracts. Using these knowledge extractors, we have developed algorithms for scoring the relevance 64 of MEDLINE citations in accordance with the principles of EBM. Our scorer is em-ployed to rerank citations retrieved by the PubMed search engine, with the goal of bringing as many topically relevant abstracts to higher ranking positions as possible. sponses that directly address physicians X  information needs. We evaluated our system with a collection of real-world clinical questions and demonstrate that our combined knowledge-based and statistical approach delivers significantly better document re-trieval and question-answering performance, compared to systems used by physicians today.
 an overview of evidence-based medicine and its basic principles. Section 3 provides an overview of MEDLINE, the bibliographic database used by our system, and PubMed, the public gateway for accessing this database. Section 4 describes our system architec-ture and outlines our conception of clinical question answering as  X  X emantic unifica-tion X  between query frames and knowledge frames derived from MEDLINE citations.
The knowledge extractors that underlie our approach are described in Section 5, along with intrinsic evaluations of each component. In Section 6, we detail an algorithm for scoring the relevance of MEDLINE citations with respect to structured query represen-tations. This scoring algorithm captures the principles of EBM and uses the results of the knowledge extractors as basic features. To evaluate the performance of this citation scoring algorithm, we have gathered a corpus of real-world clinical questions. Section 7 presents results from a document reranking experiment where our EBM scores were used to rerank citations retrieved by PubMed. Section 8 provides additional details on attempts to optimize the performance of our EBM citation scoring algorithm. Answer generation, based on reranked results, is described in Section 9. Answers from our system were manually assessed by two physicians; results are presented in Section 10.
Related work is discussed in Section 11, followed by future work in Section 12. Finally, we conclude in Section 13. 2. The Framework of Evidence-Based Medicine
Evidence-based medicine (EBM) is a widely accepted paradigm for medical practice centered clinical research such as reports from randomized controlled trials, in mak-ing decisions about patient care. Naturally, such evidence, as reported in the primary medical literature, must be suitably integrated with the physician X  X  own expertise and patient-specific factors. It is argued by many that practicing medicine in this manner leads to better patient outcomes and higher quality health care. The goal of our work is to develop question-answering techniques that complement this paradigm of medical practice.
 for codifying the knowledge involved in answering clinical questions. These three complementary facets are outlined below.
 (arranged roughly in order of prevalence):
Therapy: Selecting treatments to offer a patient, taking into account effectiveness, risk, Diagnosis: This encompasses two primary types: Etiology/Harm: Identifying factors that cause a disease or condition in a patient.
Prognosis: Estimating a patient X  X  likely course over time and anticipating likely to note that they exist independently of information needs, namely, searching is not necessarily implicated in any of these activities. We are, however, interested in situations where questions arise during one of these clinical tasks X  X nly then does the physician engage in information-seeking behavior. These activities translate into natural  X  X earch tasks. X  For therapy, the search task is usually therapy selection (for example, determining which course of action is the best treatment for a disease) or prevention (for example, selecting preemptive measures with respect to a particular disease). For diagnosis, ing multiple hypotheses regarding what disease a patient has; in diagnostic methods
For etiology, cause determination is the search task, and for prognosis, patient outcome prediction .
 sively studied by the Hedges Project at the McMaster University (Haynes et al. 1994;
Wilczynski, McKibbon, and Haynes 2001). The results of this research are implemented in the PubMed Clinical Queries tools, which can be used to retrieve task-specific cita-tions (more about this in the next section).
 ture of a well-built clinical question. The following four components have been iden-tified as the key elements of a question related to patient care (Richardson et al. 1995): for Patient/Problem, Intervention, Comparison, and Outcome.
 presented in the study, that is, how much confidence should a physician have in the results? Several taxonomies for appraising the strength of evidence based on the type and quality of the study have been developed. We chose the Strength of Recommenda-tions Taxonomy (SORT) as the basis for determining the potential upper bound on the 66 quality of evidence, due to its emphasis on the use of patient-oriented outcomes and its attempt to unify other existing taxonomies (Ebell et al. 2004). There are three levels of recommendations according to SORT: medicine must be sensitive to the multifaceted considerations that go into evaluating an abstract X  X  relevance to a clinical information need. It is exactly these three comple-mentary facets that we attempt to encode in a question-answering system for clinical decision support. 3. MEDLINE and PubMed
MEDLINE is a large bibliographic database maintained by the U.S. National Library of Medicine (NLM). This database is viewed by medical professionals, biomedical researchers, and many other users as the authoritative source of clinical evidence, and hence we have adopted it as the target corpus for our clinical question-answering sys-tem. MEDLINE contains over 15 million references to articles from approximately 4,800 journals in 30 languages, dating back to the 1960s. In 2004, over 571,000 new citations were added to the database, and it continues to grow at a steady pace. The subject scope of MEDLINE is biomedicine and health, broadly defined to encompass those areas of the life sciences, behavioral sciences, chemical sciences, and bioengineering needed by health professionals and others engaged in basic research and clinical care, public health, health policy development, or related educational activities. MEDLINE including aspects of biology, environmental science, marine biology, plant and animal science, as well as biophysics and chemistry. 1 name of the authors, name of the publication, publication type, date of publication, language, and so on. Of the entries added over the last decade or so, approximately 76% have English abstracts written by the authors of the articles X  X hese texts provide the source for answers extracted by our system.
 tant of these is the controlled vocabulary terms assigned by human indexers. NLM X  X  controlled vocabulary thesaurus, Medical Subject Headings (MeSH), imately 23,000 descriptors arranged in a hierarchical structure and more than 151,000
Supplementary Concept Records (additional chemical substance names) within a separate thesaurus. Indexing is performed by approximately 100 indexers with at least bachelor X  X  degrees in life sciences and formal training in indexing provided by NLM. Since mid-2002, the Library has been employing software that automatically suggests
MeSH headings based on content (Aronson et al. 2004). Nevertheless, the indexing process remains firmly human-centered.
 acetaminophen X  might have the following MeSH headings associated with it: up to three subheadings may be assigned, as indicated by the slash notation. In this example, a trained user could interpret from the MeSH terms that the article is about drug therapy for fever and the therapeutic use of ibuprofen and acetaminophen. An asterisk placed next to a MeSH heading indicates that the human indexer interprets the term to be the main focus of the article. Multiple MeSH terms can be notated in this manner.
 tional Library of Medicine X  X  gateway, or through third-party organizations that license
MEDLINE from NLM. PubMed is a sophisticated boolean search engine that allows users to query not only on abstract text, but also on metadata fields such as MeSH terms.
In addition, PubMed provides a number of pre-defined  X  X earch templates X  called Clin-ical Queries (Haynes et al. 1994; Wilczynski, McKibbon, and Haynes 2001) that allow users to narrow the scope of retrieved articles. These filters are implemented as fixed boolean query fragments (containing restrictions on MeSH terms, for example) that are appended to the original user query. Our experiments involve the use of PubMed to retrieve an initial set of candidate citations for subsequent processing. 4. System Architecture
We view clinical question answering as  X  X emantic unification X  between information needs expressed in a PICO-based frame and corresponding structures automatically extracted from MEDLINE citations. In accordance with the principles of EBM, this matching process should be sensitive to the nature of the clinical task and the strength of evidence of retrieved abstracts.
 The information need might be formally encoded in the following manner: 68 of the clinical question. After processing MEDLINE citations, automatically extracting
PICO elements from the abstracts, and semantically matching these elements with the query, a system might produce the following answer: oriented clinical finding X  X or example, the relative efficacy of two drugs. Thus, out-comes can serve as the basis for good answers and an entry point into the full text. The system should automatically evaluate the strength of evidence of the citations supplying the answer, but the decision to adopt the recommendations as suggested ultimately rests with the physician.
 include a natural language question or a structured PICO query frame. We advocate the latter. With a frame-based query interface, the physician shoulders the burden of translating an information need into a frame-based representation, but this provides several advantages. Most importantly, formal representations force physicians to  X  X hink through X  their questions, ensuring that relevant elements are captured. Poorly formu-lated queries have been identified by Ely et al. (2005) as one of the obstacles to finding answers to clinical questions. Because well-formed questions should have concretely instantiated PICO slots, a frame representation clearly lets the physician see missing elements. In addition, a structured query representation obviates the need for linguistic analysis of a natural language question, where ambiguities may negatively impact overall performance. We discuss alternative interfaces in Section 12.
 tion with those derived from MEDLINE citations (taking into consideration other EBM-relevant factors). However, we do not have access to the computational resources nec-essary to apply knowledge extractors to the 15 million plus citations in the MEDLINE database and directly index their results. As an alternative, we rely on PubMed to standard pipeline architecture commonly employed in other question-answering sys-tems (Voorhees and Tice 1999; Hirschman and Gaizauskas 2001).
 sible for converting a clinical question (in the form of a query frame) into a PubMed
Section 6). PubMed returns an initial list of MEDLINE citations, which is then analyzed by our knowledge extractors (see Section 5). The input to the semantic matcher, which implements our EBM citation scoring algorithm, is the query frame and annotated
MEDLINE citations. The module outputs a ranked list of citations that have been scored in accordance with the principles of EBM (see Section 6). Finally, the answer generator takes these citations and extracts appropriate answers (see Section 9).
 matching suggests the need for a number of capabilities, which correspond to the bold outlined boxes in Figure 1: knowledge extraction, semantic matching for scoring citations, and answer generation. We have realized all three capabilities in an imple-mented clinical question-answering system and conducted three separate evaluations formulator, although see discussion in Section 12. Overall, results indicate that our implemented system significantly outperforms the PubMed baseline. 5. Knowledge Extraction for Evidence-Based Medicine
The automatic extraction of PICO elements from MEDLINE citations represents a key capability integral to clinical question answering. This section, which elaborates on preliminary results reported in Demner-Fushman and Lin (2005), describes extraction algorithms for population, problems, interventions, outcomes, and the strength of evi-dence. For an example of a completely annotated abstract, see Figure 2. Each individual
PICO extractor takes as input the abstract text of a MEDLINE citation and identifies the relevant elements: Outcomes are complete sentences, while population, problems, and interventions are short noun phrases.
 for identifying segments of text that correspond to concepts in the UMLS Metathe-saurus. Many of our algorithms operate at the level of coarser-grained semantic types called Semantic Groups (McCray, Burgun, and Bodenreider 2001), which capture higher-level generalizations about entities (e.g., C HEMICALS feature we take advantage of (when present) is explicit section markers present in some abstracts. These so-called structured abstracts were recommended by the Ad Hoc Work-ing Group for Critical Appraisal of the Medical Literature (1987) to help humans assess the reliability and content of a publication and to facilitate the indexing and retrieval processes. These abstracts loosely adhere to the introduction, methods, results, and conclusions format common in scientific writing, and delineate a study using explicitly marked sections with variations of the above headings. Although many core clinical headings. Even when present, the headings are not organized in a manner focused on patient care. In addition, abstracts of much high-quality work remain unstructured. For these reasons, explicit section markers are not entirely reliable indicators for the various semantic elements we seek to extract, but must be considered along with other sources of evidence.
 corpus of MEDLINE abstracts, created through an effort led by the first author at the National Library of Medicine (Demner-Fushman et al. 2006). As will be described herein, the population, problem, and the intervention extractors are based largely on recognition of semantic types and a few manually constructed rules; the outcome extrac-70 tor, in contrast, is implemented as an ensemble of classifiers trained using supervised machine learning techniques (Demner-Fushman et al. 2006). These two very different approaches can be attributed to differences in the nature of the frame elements: Whereas problems and interventions can be directly mapped to UMLS concepts, and populations easily mapped to patterns that include UMLS concepts, outcome statements follow no predictable pattern. The initial goal of the annotation effort was to identify outcome statements in abstract text. A physician, two registered nurses, and an engineering researcher manually identified sentences that describe outcomes in 633 MEDLINE abstracts; a post hoc analysis demonstrates good agreement (  X  = 0 . 77). The annotated abstracts were retrieved using PubMed and attempted to model different user behaviors ranging from naive to expert (where advanced search features were employed). With the exception of 50 citations retrieved to answer a question about childhood immunization, the rest of the results were retrieved by querying on a disease, for example, diabetes. Of the 633 citations, 100 abstracts were also fully annotated with population, problems, and interventions. These 100 abstracts were set aside as a held-out test set. Of the remaining citations, 275 were used for training and rule derivation, as described in the following sections.
 practical to annotate PICO entities at the phrase level due to significant unresolvable disagreement and interannotator reliability issues. Consider the following segment: intervention. However, they could not agree on the exact phrasal boundaries of each element, and more importantly, general guidelines for ensuring consistent annotations.
For example, should the whole clause starting with adult men and women be marked as population, or should type 2 Diabetes Mellitus ( type 2 DM ) be marked up only as the problem? How should we indicate that the cholesterol levels description belongs to 151 subjects of the study, and so forth? This issue becomes important for evaluation because there is a mismatch between annotated ground truth and the output of our knowledge extractors, as we will discuss.
 component evaluations that assess their accuracy. This section is organized such that the description of each extractor and its evaluation are paired together. Results are reported in terms of the percentage of correctly identified instances, percentage of instances for which the extractor had no answer, and percentage of incorrectly identified instances.
The baselines and gold standards for each extractor vary, and will be described in-dividually. The goal of these component evaluations is a general characterization of performance, as we focused the majority of our efforts on the two other evaluations. 5.1 Population Extractor
The PICO framework makes no distinction between the population and the problem, which is rooted in the concept of the population in clinical studies, as exemplified by text such as POPULATION: Fifty-five postmenopausal women with a urodynamic diagnosis of genuine urinary stress incontinence. Although this fragment simultaneously describes the population (of which a particular patient can be viewed as a sample therefrom) and the problem, we chose to separate the extraction of the two elements because they are not always specified together in abstracts (issues with respect to exact boundaries men-tioned previously notwithstanding). Furthermore, many clinical questions ask about a particular problem without specifying a population.
 of manually crafted rules that codify the following assumptions: patterns: position in the abstract and its position in the clause from which it was extracted. If a number is followed by a measure, for example, year or percent , the number is discarded, and pattern matching continues. After the entire abstract is processed in this manner, the match with the highest confidence value is retained as the population description. 5.2 Evaluation of Population Extractor
Ninety of the 100 fully annotated abstracts in our collection were agreed upon by the annotators as being clinical in nature, and were used as test data for our population extractor. Because these abstracts were not examined in the process of developing the extractor rules, they can be viewed as a blind held-out test set. The output of our popu-72 lation extractor was judged to be correct if it occurred in a sentence that was annotated as containing the population in the gold standard. Note that this evaluation presents an upper bound on the performance of the population extractor, whose outputs are noun phrases. We adopted such a lenient evaluation setup because of the boundary issues previously discussed, and also to forestall potential difficulties with scoring partially overlapping string matches.
 stract. We considered the baseline correct if any one of the sentences were annotated as containing the population in the gold standard (an even more lenient criterion).
This baseline was motivated by the observation that the aim and methods sections of structured abstracts are likely to contain the population information X  X or structured ab-stracts, explicit headings provide structural cues; for unstructured abstracts, positional information serves as a surrogate.
 analysis revealed three sources of error: First, not all population descriptions contain a number explicitly, for example, The medical charts of all patients who were treated with populations are population groups, as for example in All primary care trusts in England.
Finally, tagging and chunking errors propagate to the semantic type assignment level and affect the quality of MetaMap output. For example, consider the following sentence: erroneous chunking: other extractors as well. For example, lead was mistagged as a noun in the phrase
Echocardiographic findings lead to the right diagnosis, which caused MetaMap to identify the word as a P HARMACOLOGICAL S UBSTANCE (lead is sometimes used as a homeo-pathic preparation). 5.3 Problem Extractor
The problem extractor relies on the recognition of concepts belonging to the UMLS semantic group D ISORDER . In short, it returns a ranked list of all such concepts within a given span of text. We evaluate the performance of this simple heuristic on segments of the abstract varying in length: abstract title only, abstract title and first two sentences, and entire abstract text. Concepts in the title, in the introduction section of structured abstracts, or in the first two sentences in unstructured abstracts, are given higher confi-dence values due to their discourse prominence. Finally, the highest-scoring problem is designated as the primary problem in order to differentiate it from co-occurring conditions identified in the abstract. 5.4 Evaluation of Problem Extractor
Although our problem extractor returns a list of clinical problems, we only evalu-ate performance on identification of the primary problem. For some abstracts, MeSH headings can be used as ground truth, because one of the human indexers X  tasks in assigning terms is to identify the main topic of the article (sometimes a disorder). For this evaluation, we randomly selected 50 abstracts with disorders indexed as the main topic from abstracts retrieved using PubMed on the five clinical questions described in Sneiderman et al. (2005).
 only, the title and first two sentences, and the entire abstract. These results are shown in
Table 2. Here, a problem was considered correctly identified only if it shared the same concept ID as the ground truth problem (from the MeSH heading). The performance of our best variant (abstract title and first two sentences) approaches the upper bound on
MetaMap performance X  X hich is limited by human agreement on the identification of semantic concepts in medical texts, as established in Pratt and Yetisgen-Yildiz (2003).
MetaMap performance, the error rate could be further reduced by more sophisticated recognition of implicitly stated problems. For example, with respect to a question about immunization in children, an abstract about the measles-mumps-rubella vaccination never mentioned the disease without the word vaccination; hence, no concept of the type D ISEASE OR S YNDROME was identified. 5.5 Intervention Extractor
The intervention extractor identifies both the intervention and comparison elements in a PICO frame; processing of these two frame elements can be collapsed because they belong to the same semantic group. In many abstracts, it is unclear which intervention is the primary one and which are the comparisons, and hence our extractor simply returns a list of all interventions under study.

UMLS Semantic Network relations associated with each clinical task. Restrictions on the semantic types allowed in these relations prescribe the set of possible clinical in-terventions. For therapy these relations include treats , prevents ,and carries out ; diagnoses 74 for diagnosis; causes and result of for etiology; and prevents for prognosis. At present, the identification of nine semantic types, for example, D IAGNOSTIC D
RUG ,andH EALTH C ARE A CTIVITY , serves as the foundation for our intervention extraction algorithm.
 abstracts, concepts of the relevant semantic type are given additional weight if they appear in the title, aims, and methods sections. In unstructured abstracts, concepts towards the beginning of the abstract text are favored. Finally, the intervention extractor takes into account the presence of certain cue phrases that describe the aim and/or methods of the study, such as This study examines or This paper describes. 5.6 Evaluation of Intervention Extractor
The intervention extractor was evaluated in the same manner as the population extrac-contained human-annotated interventions served as ground truth. The output of our annotated as containing the intervention in the gold standard. As with the evaluation of the population extractor, this represents an upper bound on performance. Results are shown in Table 3.
 serum levels of anti-HBsAg and presence of autoantibodies (ANA, ENA) were evaluated, serum is recognized as a T ISSUE , levels as I NTELLECTUAL and ANA as I MMUNOLOGIC F ACTORS . In this case, however, autoantibodies should be considered a L ABORATORY OR T EST R ESULT . 3 In other cases, extraction errors were caused by summary sentences that were very similar to intervention statements, glycaemic control, and lipids in patients with Type 2 diabetes. For this particular abstract, the correct interventions are contained in the sentence Patients with Type 2 diabetes glibenclamide (initially 1.75 mg QD, n = 109) as monotherapy. 5.7 Outcome Extractor
We approached outcome extraction as a classification problem at the sentence level, that is, the outcome extractor assigns a probability of being an outcome to each sentence in an abstract. Our preliminary work has led to a strategy based on an ensemble of classifiers, which includes a rule-based classifier, a unigram  X  X ag of words X  classifier, an n -gram classifier, a position classifier, an abstract length classifier, and a semantic classifier. With the exception of the rule-based classifier, all classifiers were trained on the 275 citations from the annotated collection of abstracts described previously. effort, by a registered nurse with 20 years of clinical experience. This classifier estimates the likelihood that a sentence states an outcome based on cue phrases such as signif-icantly greater, well tolerated, and adverse events. The likelihood of a sentence being an outcome as indicated by cue phrases is the ratio of the cumulative score for recognized phrases to the maximum possible score. For example, the sentence The dropout rate due to adverse events was 12.4% in the moxonidine and 9.8% in the nitrendipine group is segmented into eight phrases by MetaMap, which sets the maximum score to 8. The two phrases dropout rate and adverse events contribute one point each to the cumulative score, which results in a likelihood estimate of 0.25 for this sentence.
 the API provided by the MALLET toolkit. 4 This classifier outputs the probability of a class assignment.
 ent set of features. We first identified the most informative unigrams and bigrams using the information gain measure (Yang and Pedersen 1997), and then selected only the positive outcome predictors using odds ratio (Mladenic and Grobelnik 1999). Disease-specific terms, such as rheumatoid arthritis, were then manually removed. Finally, the list of features was revised by the registered nurse who participated in the annotation effort. This classifier also outputs the probability of a class assignment.
 an outcome based on its position in the abstract (for structured abstracts, with respect to the results or conclusions sections; for unstructured abstracts, with respect to the end of the abstract).
 that an abstract of a given length (in the number of sentences) contains an outcome statement. For example, the probability that an abstract four sentences long contains an outcome statement is 0 . 25, and the probability of finding an outcome in a ten sentence X  long abstract is 0 . 92. This feature turns out to be useful because the average length of abstracts with and without outcome statements differs: 11 . 7 sentences for the former, 7 . 95 sentences for the latter.
 of UMLS concepts belonging to semantic groups highly associated with outcomes given a boost if the concept has already been identified as the primary problem or an intervention.
 interpolation scheme: on ad hoc weight selection based on intuition. The second involved a more principled method using confidence values generated by the base classifiers and least squares lin-76 ear regression adapted for classification (Ting and Witten 1999), which can be described by the following equation: k (for classifiers that do not return actual probabilities, we normalized the scores and by n classifiers are combined using the coefficients (  X  termined in the training stage as follows: Probabilities predicted by base classifiers for each sentence are represented in an N  X  M matrix A, where M is the number of class assignments for each sentence is stored in a vector b , and weights are found by computing the vector  X  that minimizes || A  X   X  b || . The solution can be found using singular value decomposition, as provided in the JAMA basic linear algebra package released by NIST. 5 5.8 Evaluation of Outcome Extractor
Because outcome statements were annotated in each of the 633 citations in our collec-tion, it was possible to evaluate the outcome extractor on a broader set of abstracts. From those not used in training the outcome classifiers, 153 citations pertaining to therapy were selected. Of these, 143 contained outcome statements and were used as the blind held-out test set. In addition, outcome statements in abstracts pertaining to diagnosis (57), prognosis (111), and etiology (37) were also used.
 fidence. Based on the observation that human annotators typically mark two to three sentences in each abstract as outcomes, we evaluated the performance of our extractor at cutoffs of two and three sentences. These results are shown in Table 4: The columns marked AH2 and AH3 show performance of the weighted linear interpolation approach with ad hoc weight assignment at two-and three-sentence cutoffs, respectively; the columns marked LR2 and LR3 show performance of the least squares linear regression model at the same cutoffs. In the evaluation, our outcome extractor was considered correct if the returned sentences intersected with sentences judged as outcomes by our human annotators. Although this is a lenient criterion, it does roughly capture the performance of our knowledge extractor. Because outcome statements are typically found in the conclusion of a structured abstract (or near the end of the abstract in the case of unstructured abstracts), we compared our answer extractor to the baseline of Table 4).
 at the two-sentence cutoff, for the most part. Bigger improvements, however, can be seen at the three-sentence cutoff level. It is evident that the assignment of weights in our ad hoc model is primarily geared towards therapy questions, perhaps overly so.
Better overall performance is obtained with the least squares linear regression model. sentence boundary identification, chunking errors, and word sense ambiguity in the
Metathesaurus. 5.9 Determining the Strength of Evidence
The strength of evidence is a classification scheme that helps physicians assess the quality of a particular citation for clinical purposes. Metadata associated with most
MEDLINE citations (MeSH terms) are extensively used to determine the strength of evidence and in our EBM citation scoring algorithm (Section 6).
 identified using the Publication Type (a metadata field) and MeSH terms pertaining to the type of the clinical study. Table 5 shows our mapping from publication type and MeSH headings to evidence grades based on principles defined in the Strength of Recommendations Taxonomy (Ebell et al. 2004). 5.10 Sample Output A complete example of our knowledge extractors working in unison is shown in
Figure 2, which contains an abstract retrieved in response to the following question:  X  X n children with an acute febrile illness, what is the efficacy of single-medication therapy with acetaminophen or ibuprofen in reducing fever? X  (Kauffman, Sawyer, and Scheinbaum 1992). Febrile illness is the only concept mapped to D is identified as the primary problem. 37 otherwise healthy children aged 2 to 12 years is correctly identified as the population. Acetaminophen, ibuprofen, and placebo are correctly 78 Antipyretic efficacy of ibuprofen vs acetaminophen Kauffman RE, Sawyer LA, Scheinbaum ML Am J Dis Child. 1992 May;146(5):622-5
OBJECTIVE X  X o compare the antipyretic efficacy of ibuprofen, placebo, and acetaminophen. DESIGN X  X ouble-dummy, double-blind, randomized, placebo-controlled trial. SETTING X  X mergency department and inpatient units of a large, metropolitan, university-based, children X  X  hospital in Michigan. PARTICIPANTS X  febrile  X  X  X  X  X  X  X  X  X  X  illness Problem . INTERVENTIONS X  X ach child was randomly assigned to receive
Oral temperature was measured before dosing, 30 minutes after dosing, and hourly thereafter for 8 hours after the dose. Patients were monitored for ad-verse effects during the study and 24 hours after administration of the as-Publication Type: Clinical Trial, Randomized Controlled Trial PMID: 1621668 Strength of Evidence: grade A extracted as the interventions under study. The three outcome sentences are correctly classified; the short sentence concerning adverse effects was ranked lower than the other three sentences and hence below the cutoff. The study design, from metadata associated with the citation, allows our strength of evidence extractor to classify this article as grade A. 6. Operationalizing Evidence-Based Medicine
In our view of clinical question answering, the knowledge extractors just described sup-ply the features on which semantic matching occurs. This section describes an algorithm that, when presented with a structured representation of an information need and a
MEDLINE citation, automatically computes a topical relevance score in accordance with the principles of EBM.
 necessary to possess a corpus of clinical questions on which to experiment. Because no such test collection exists, we had to first manually create one. Fortunately, collections of clinical questions (representing real-world information needs of physicians), are available on-line. From two sources, the Journal of Family Practice
Exchange, 7 we gathered 50 clinical questions, which capture a realistic sampling of the scenarios that a clinical question-answering system would be confronted with. These questions were minimally modified from their original form as downloaded from the
World Wide Web. In a few cases, a single question actually consisted of several smaller questions; such clusters were simplified by removing questions more peripheral to the central clinical problem. All questions were manually classified into one of the four clinical tasks; the distribution of the questions roughly follows the prevalence of each task type as observed in natural settings, noted by Ely et al. (1999). The final step in the preparation process was manual translation of the natural language questions into PICO query frames.
 verification purposes. The breakdown of these questions into the four clinical tasks and the development/test split is shown in Table 6. An example of each question type from our development set is presented here, along with its query frame: 80 for a question-answering system. Instead, a structured PICO-based representation cap-tures physicians X  information needs in a more perspicuous manner X  X rimarily because clinicians are trained to analyze clinical situations with this framework.
 into population, primary problem, and co-occurring problems in the query representa-tion. The justification for this will become apparent when we present our algorithm for scoring MEDLINE citations, as each of these three facets must be treated differently.
Note that many elements are specified only to the extent that they were explicit in a population, that element will be empty. Finally, outcomes are not directly encoded in the query representation because they are implicit most of the time; for example, in Does quinine reduce leg cramps for young athletes?, the desired outcome, naturally, is to reduce the occurrence and severity of leg cramps. Nevertheless, outcome identification is an important component of the citation scoring algorithm, as we shall see later.
Evidence-based medicine outlines the need to consider three different facets (see Sec-tion 2), which we operationalize in the following manner: contributions from matching PICO structures, the strength of evidence of the citation, tasks). In what follows, we describe each of these contributions in detail.
 enter into consideration when a physician examines a MEDLINE citation. Although the assignment of numeric scores is based on intuition and may seem ad hoc in many cases, evaluation results in the next section demonstrate the effectiveness of our algorithm.
This issue will be taken up further in Section 8. 6.1 Scores Based on PICO Elements The score of an abstract based on extracted PICO elements, S ual components according to the following formula: problem in the query frame and the primary problem in the abstract (i.e., the highest-scoring problem identified by the problem extractor). A score of 1 is given if the prob-lems match exactly based on their unique UMLS concept ID as provided by MetaMap.
Matching based on concept IDs has the advantage that it abstracts away from termino-logical variation; in essence, MetaMap performs terminological normalization. Failing an exact match of concept IDs, a partial string match is given a score of 0 . 5. If the primary problem in the query has no overlap with the primary problem from the abstract, a score of  X  1 is given. Finally, if our problem extractor could not identify a problem (but the query frame does contain a problem), a score of  X  0 . 5 is given.
 and cause determination search tasks because knowledge of the problems is typically incomplete in these scenarios. Therefore, physicians would normally be interested in any problems mentioned in the abstracts in addition to the primary problem specified in the query frame. As an example, consider the question What is the differential diagnosis of chronic diarrhea in immunocompetent patients? Although chronic diarrhea is the primary problem, citations that discuss additional related disorders should be favored over those points, and disorders mentioned anywhere else receive one point (in addition to the match score based on the primary problem, as discussed).
 sure the overlap between query frame elements and corresponding elements extracted from abstracts. A point is given to each matching intervention and matching population.
For example, finding the population group children from a query frame in the abstract increments the match score; the remaining words in the abstract population are ignored.
Thus, if the query frame contains a population element and an intervention element, the score for an abstract that contains the same UMLS concepts in the corresponding slots is incremented by two.
 outcome sentence (we employed the outcome extractor based on the linear regression model for our experiments). As outcomes are rarely explicitly specified in the original question, we decided to omit them in the query representation. Our citation scoring algorithm simply considers the inherent quality of the outcome statements in an ab-stract, independent of the query. This is justified because, given a match on the primary problem, all clinical outcomes are likely to be of interest to the physician. 6.2 Scores Based on Strength of Evidence
The relevance score component based on the strength of evidence is calculated in the following manner:
Medical Association (JAMA) get a score of 0 . 6for S journal study type, S study , clinical trials, such as randomized controlled trials, receive a score of 0 . 5; observational studies, for example, case reports, 0 . 3; all non-clinical publications,  X  1 . 5; and 0 otherwise. The study type is directly encoded in the Publication Type field of a MEDLINE citation.
 between the date of the search and the date of publication. 6.3 Scores Based on Specific Tasks
The final component of our EBM score is based on task-specific considerations, as reflected in manually assigned MeSH terms. For search tasks falling into each clinical task, we gathered a list of terms that are positive and negative indicators of relevance. 82 The task score, S task , is given by: indicator for that particular task type, or a negative score if the term is a negative indi-cator for the clinical task. Note that although our current system uses MeSH headings assigned by human indexers, manually assigned terms can be replaced with automatic processing if needed (Aronson et al. 2004).
 is a set of negative indicators common to all tasks; these were extracted from the set of genomics articles provided for the secondary task in the TREC 2004 genomics track evaluation (Hersh, Bhupatiraju, and Corley 2004); examples include genetics and cell physiology. The positive and negative weights assigned to each term heuristically encode the relative importance of different MeSH headings and are derived from the Clinical
Queries filters in PubMed, from the JAMA EBM tutorial series on critical appraisal of medical literature, from MeSH scope notes, and based on a physician X  X  understanding of the domain (the first author).
 Indicators for Therapy Tasks. Positive indicators for therapy were derived from the
PubMed X  X  Clinical Queries filters; examples include drug administration routes and any of its children in the MeSH hierarchy. A score of  X  1 is given if the MeSH descriptor or qualifier is marked as the main theme of the article (indicated via the star notation search task of prevention , three additional headings are considered positive indicators: prevention and control, prevention measures, and prophylaxis.

Indicators for Diagnosis Tasks. Positive indicators for therapy are also used as negative indicators for diagnosis because the relevant studies are usually disjoint; it is highly unlikely that the same clinical trial will study both diagnostic methods and treatment methods. The MeSH term diagnosis and any of its children are considered positive indicators. As with therapy questions, terms marked as the major theme get a score of  X  1 . 0, and  X  0 . 5 otherwise. This general assignment of indicator terms allows a system to differentiate between questions such as Does a Short Symptom Checklist accurately diagnose ADHD? and What is the most effective treatment for ADHD in children?, which might retrieve very similar sets of citations.
 Indicators for Prognosis Tasks. Positive indicators for prognosis include the following
MeSH terms: survival analysis, disease-free survival, treatment outcome, health status, preva-marked as the major theme, a score of + 2 is given; + 1 otherwise. There are no negative indicators, other than those common to all tasks previously described.
 Indicators for Etiology Tasks. Negative indicators for etiology include therapy-oriented + 0 . 1. The following MeSH terms are considered highly indicative of citations rele-vant to etiology: population at risk, risk factors, etiology, causality, and physiopathology. If one of these terms is marked as the major theme, a score of + 2 is given; otherwise, a score of + 1 is given. 7. Evaluation of Citation Scoring
The previous section describes a relevance-scoring algorithm for MEDLINE citations that attempts to capture the principles of EBM. In this section, we present an evaluation of this algorithm.
 edge structures derived from MEDLINE citations. However, knowledge extraction on such large scales is impractical given our computational resources, so we opted for an IR-based pipeline approach. Under this strategy, an existing search engine would be employed to generate a candidate list of citations to be rescored, according to our algorithm. PubMed is a logical choice for gathering this initial list of citations because it represents one of the most widely used tools employed by physicians and other health professionals today. The system supports boolean operators and sorts results chronologically, most recent citations first.
 for our citation scoring algorithm X  X s a document reranking task. Given an initial hit list, can our algorithm automatically re-sort the results such that relevant documents are brought to higher ranks? Not only is such a task intuitive to understand, this conceptualization also lends itself to an evaluation based on widely accepted practices in information retrieval.
 fetch an initial set of hits. These queries took advantage of existing advanced search features to simulate the types of results that would be currently available to a knowl-edgeable physician. Specifically, widely accepted tools for narrowing down PubMed search results such as Clinical Queries were employed whenever appropriate.
 analgesic rebound headaches? The search started with the initial terms  X  X nalgesic rebound headache X  with a  X  X arrow therapy filter. X  In PubMed, this query is: in abstract text and MeSH headings. We always restrict searches to articles that have abstracts, are published in English, and are assigned the MeSH term humans (as opposed to say, experiments on animals) X  X hese are all strategies commonly used by clinicians. panded with the term side effects to emphasize the aspect of the problem requiring an intervention. The final query for the question became: 84 manually for every question in our collection; she verified that each hit list contained at least some relevant documents and that the results were as good as could be reasonably achieved. The process of generating queries averaged about 40 minutes per question. citations were gathered because some queries returned fewer than 50 hits. The process of generating a  X  X ood X  PubMed query is not a trivial task, which we have side-stepped in this work by placing a human in the loop. We return to this issue in Section 12. the first author. It is important to note that relevance assessment in the clinical domain requires significant medical knowledge (in short, a medical degree). After careful con-sideration, we decided to assess only topical relevance, with the understanding that the applicability of information from a specific citation in real-world settings depends on a variety of other factors (see Section 10 for further discussion). Each citation was assigned one of four labels: issues when comparing different systems. In total, the relevance assessment process took approximately 100 hours, or about an average of 2 hours per question.
 we implemented the scoring algorithm. The development questions were also used to tune the weight for combining scores from the term-based scorer and EBM-based scorer; by simply trying all possible values, we settled on a  X  of 0 . 8, that is, 80% weight to the
EBM score, and 20% weight to the term-based score. As we shall see later, it is unclear which metric should be optimized. The test questions were hidden during the system development phase and served as a blind held-out test set for assessing the generality of our algorithm.
 using our relevance judgments: mine the statistical significance of the results. This test is commonly used in information retrieval research because it makes minimal assumptions about the underlying distrib-ution of differences. For each evaluation metric, significance at the 1% level is indicated by either or , depending on the direction of change; significance at the 5% level is indicated by or , depending on the direction of change. Differences that are not statistically significant are marked with the symbol  X  .
 documents marked  X  X ontains answer X  and  X  X elevant X  were given credit; these results are shown in Table 7 (for the development set) and Table 8 (for the blind held-out test set). Across all questions, both the EBM-based reranker and combination reranker sig-nificantly outperform the PubMed baseline on all metrics. In many cases, the differences are particularly noteworthy X  X or example, our EBM citation scoring algorithm more than doubles the baseline in terms of MAP and P10 on the test set. There are enough therapy questions to achieve statistical significance in the task-specific results; however, not statistically significant. Results also show that the simple term-based reranker out-performs the PubMed baseline, demonstrating the importance of recognizing outcome statements in MEDLINE abstracts.
 rerankers statistically significant? Results of Wilcoxon signed-rank tests are shown in
Table 11. Both the EBM and combination rerankers significantly outperform the term-based reranker (at the 1% level, on all metrics, on both development and test set), with 86 the exception of MRR on the development set. However, for all metrics, on both the development set and test set, there is no significant difference between the EBM and combination reranker (which combines both term-based and EBM-based evidence). In the parameter tuning process, we could not find a weight where performance across all measures was higher; in the end, we settled on what we felt was a reasonable weight that improved P10 and MRR on the development set.
 credit; these results are shown in Table 9 (for the development set) and Table 10 gains were achieved under the strict scoring criteria for our EBM and combination rerankers. Results of Wilcoxon signed-rank tests on the term-based, EBM, and com-bination rerankers are also shown in Table 11 for the strict scoring condition. In most cases, combining term scoring with EBM scoring does not help. In almost all cases, the EBM and combination reranker perform significantly better than the term-based reranker.
 mance? We shall return to this issue in Sections 9 and 10, which describe and evaluate the answer generation module, respectively. In the next section, we describe more detailed experiments with our EBM citation scoring algorithm. 88 8. Optimizing Citation Scoring
A potential, and certainly valid, criticism of our EBM citation scoring algorithm is its ad hoc nature. Weights for various features were assigned based on intuition, reflecting our understanding of the domain and our knowledge about the principles of evidence-based medicine. Parameters were fine-tuned during the system implementation process by actively working with the development set; however, this was not done in any systematic fashion. Nevertheless, results on the blind held-out test set confirm the generality of our citation scoring algorithm. the first materialization of a new capability to be rather ad hoc in its implementation.
This is a reflection of an initial attempt to understand both the problem and solution spaces. Subsequent systems, with a better understanding of the possible technical ap-proaches and their limitations, are then able to implement a more principled solution.
Because our clinical question-answering system is the first of its type that we are aware of, in terms of both depth and scope, it is inevitable that our algorithms suffer from some of these limitations. Similarly, our collection of clinical questions is the first test collection of its type that we are aware of. Typically, construction of formal models is only made possible by the existence of test collections. We hope that our work sheds new insight on question answering in the clinical domain and paves the way for future work. 90 generative) model. Most methods for training such models require independently and identically distributed samples from the underlying distribution X  X hich is certainly not the case with our test collection. Moreover, the event space of queries and documents is extremely large or even infinite, depending on how it is defined. Our training data, assumed to be samples from this underlying distribution, is extremely small compared to the event space, and hence it is unlikely that popular methods (e.g., maximum likelihood estimates) would yield an accurate characterization of the true distribution. of maximum likelihood techniques X  X hich do not maximize the correct objective function. Maximizing the likelihood of generating the training data does not mean that the evaluation metric under consideration (e.g., mean average precision) is also maximized X  X his phenomenon is known as metric divergence.
 in our system. This section describes a few experiments aimed at this goal.
 each representing a facet of evidence-based medicine. This structure naturally suggests a modification to Equation (3) that weights each score component differently: questions, we exhaustively searched through the entire parameter space, in increments of hundredths, and determined the optimal settings to be  X  was found to slightly improve all metrics). The performance surface for mean average on the development set. Numeric results are shown in Table 12. It can be seen that increase in any of the metrics. Furthermore, these gains do not carry over to the blind held-out test set. We also tried optimizing the  X   X  X  on all questions in the development significant.
 task. This explains why optimizing across all question types at the same time did not improve performance. On the other hand, there are too few questions of any particular type to represent an accurate sampling of all possible questions. This is why parameter tuning on therapy questions did not significantly alter performance. These work. 92 ad hoc weights is S task , defined in Equation (7) and repeated here:
Because these weights were heuristically assigned, it would be worthwhile to examine the impact they have on performance. As a variant, we modified  X  ( t ) so that all MeSH terms were mapped to  X  1; in other words, we did not encode granular levels of  X  X oodness. X  These results are shown in Table 8. Although performance dropped across all metrics, none of the differences were statistically significant except for P10 on the test set.
 parameter settings on abstract reranking performance. As can be seen from the results, our algorithm is relatively invariant with respect to the choice of parameters, con-firming that our primary contribution is the EBM-based approach to clinical question answering, and that our performance gains cannot be simply attributed to a fortunate choice of parameters. 9. From Scoring Citations to Answering Questions
The aim of question-answering technology is to move from the  X  X it list X  paradigm of information retrieval, where users receive a list of potentially relevant documents that they must then browse through, to a mode of interaction where users directly receive responses that satisfy their information needs. In our current architecture, fetching a higher-quality ranked list is a step towards generating responsive answers.
 in their study of real-world physicians, is that they focus on bottom-line clinical advice X  X nformation that physicians can directly act on. Ideally, answers should in-tegrate information from multiple clinical studies, pointing out both similarities and rive at the same conclusion X  X t need not be repeated unless the physician wishes to  X  X rill down X ; the system should reconcile contradictions, for example, if two abstracts disagree on a particular treatment because they studied different patient populations.
We have noted that many of these desiderata make complex question answering quite similar to multi-document summarization (Lin and Demner-Fushman 2005b), but these features are also beyond the capabilities of current summarization systems.
 analysis that is beyond the current state of the art, even with the aid of existing medical ontologies. For example, even the seemingly straightforward task of identifying simi-larities and differences in outcome statements is rendered exceedingly complex by the tremendous amount of background medical knowledge that must be brought to bear results; the closest analogous task in computational linguistics X  X edundancy detection for multi-document summarization X  X eems easy by comparison. Furthermore, it is unclear if textual strings make  X  X ood answers. X  Perhaps a graphical rendering of the semantic predicates present in relevant abstracts might more effectively convey the desired information; see, for example, Fiszman, Rindflesch, and Kilicoglu (2004). Per-haps some variation of multi-level bulleted lists, appropriately integrated with interface elements for expanding and hiding items, might provide physicians a better overview of the information landscape; see, for example, Demner-Fushman and Lin (2006). approach to answer generation. For each abstract in our reranked list of citations, our system produces an answer by combining the title of the abstract and the top three outcome sentences (in the order they appeared in the abstract). We employed the outcome scores generated by the regression model. No attempt was made to synthesize information from multiple citations. A formal evaluation of this simple approach to answer generation is presented in the next section. 10. Evaluation of Clinical Answers
Evaluation of answers within a clinical setting involves a complex decision that must not only take into account topical relevance (i.e.,  X  X oes the answer address the infor-mation need? X ), but also situational relevance (e.g., Saracevic 1975, Barry and Schamber 94 1998). The latter factor includes many issues such as the strength of evidence, recency of results, and reputation of the journal. Clinicians need to carefully consider all these elements before acting on any information for the purposes of patient care. Within the framework of evidence-based medicine, the physician is the final arbiter of how clinical answers are integrated into the broader activities of medical care, but this complicates any attempt to evaluate answers generated by our system.
 evaluation of topical relevance X  X ssessors were only presented with answer strings, generated in the manner described in the previous section. Metadata that would con-tribute to judgments about situational relevance, such as the strength of evidence, names of the authors and the journal, and so on, were purposefully suppressed. Our evaluation compared the top five answers generated from the original PubMed hit list and the top five answers generated from our reranked list of citations. Answers were prepared for all 24 questions in our development set.

National Library of Medicine to evaluate the textual answers. Our instructions clearly stated that only topical relevance was to be assessed. We asked the physicians to provide three-valued judgments: the question of  X  X hat exactly is an answer to a clinical question? X  Informally, an-swers marked with a plus can be considered  X  X ctionable X  clinical advice. Answers marked with a check provide relevant information that may influence the physician X  X  actions.
 from both systems were presented in a randomized order without any indication of which system the response came from (duplicates were suppressed). A paper printout, containing each question followed by the blinded answers, was presented to each assessor. We then coded the relevance judgments in a plain text file manually. During this entire time, the key that maps answers to systems was kept in a separate file and hidden from everyone, including the authors. All scores were computed automatically without human intervention.
 condition (Table 15), only  X  X lus X  judgments were considered good; under the lenient condition (Table 16), both  X  X lus X  and  X  X heck X  judgments were considered good. As can be seen, our EBM algorithm significantly outperforms the baseline under both the strict and lenient conditions, according to both assessors. On average, the length of answers generated from the original PubMed list of citations was 90 words; answers generated from the reranked list of citations averaged 87 words. Answers from both sources were significantly shorter than the abstracts from which they were extracted (250 word average for original PubMed results and 270 word average for reranked results). the following question: The following is an example of a response that received a  X  X lus X  judgment: analgesics are most likely to cause the problem, and gives a direct guideline for preven-tive treatment.
 96 headaches, neither prevention nor treatment is explicitly mentioned. For these reasons this response was marked as potentially leading to an answer, but not as containing one. is capable of supplying clinically relevant responses to physicians. Compared to
PubMed, which does not take into account the principles of evidence-based medicine, our question-answering system represents a leap forward in information access capabilities. 8 11. Related Work and Discussion
Clinical question answering is an emerging area of research that has only recently begun to receive serious attention. As a result, there exist relatively few points of comparison to our own work, as the research space is sparsely populated. In this section, however, we will attempt to draw connections to other clinical information systems (although not necessarily for question answering) and related domain-specific question-answering systems. For an overview of systems designed to answer open-domain factoid ques-tions, the TREC QA track overview papers are a good place to start (Voorhees and
Tice 1999). In addition, there has been much work on the application of linguistic and semantic knowledge to information retrieval; see Lin and Demner-Fushman (2006a) for a brief overview.
 evidence-based medicine is not new. Based on analyses of 4,000 MEDLINE citations,
Mendonc  X a and Cimino (2001) have studied MeSH terms associated with the four basic clinical tasks of therapy, diagnosis, prognosis, and etiology. The goal was to auto-Project (Haynes et al. 1994; Wilczynski, McKibbon, and Haynes 2001). Cimino and
Mendonc  X a reported good performance for etiology, diagnosis, and in particular therapy, but not prognosis. Although originally developed as a tool to assist in query formu-lation, Booth (2000) pointed out that PICO frames can be employed to structure IR results for improving precision. PICO-based querying in information retrieval is merely an instance of faceted querying, which has been widely used by librarians since the introduction of automated retrieval systems (e.g., Meadow et al. 1989). The work of
Hearst (1996) demonstrates that faceted queries can be converted into simple filtering constraints to boost precision.
 sources has been demonstrated by Niu and Hirst (2004). Their study also illustrates the importance of semantic classes and relations. However, extraction of outcome state-ments from secondary sources (meta-analyses, in this case) differs from extraction of outcomes from MEDLINE citations because secondary sources represent knowledge that has already been distilled by humans (which may limit its scope). Because sec-ondary sources are often more consistently organized, it is possible to depend on certain surface cues for reliable extraction (which is not possible for MEDLINE ab-stracts in general). Our study tackles outcome identification in primary medical sources and demonstrates that respectable performance is possible with a feature-combination approach. abstracts for non-clinical purposes. For example, McKnight and Srinivasan (2003) de-scribe a machine learning approach to automatically label sentences as belonging to introduction, methods, results, or conclusion using structured abstracts as training data weighting of automatically labeled sections can lead to improved retrieval performance.
Note, however, that such labels are orthogonal to PICO frame elements, and hence are not directly relevant to knowledge extraction for clinical question answering. In a similar vein, Light, Qiu, and Srinivasan (2004) report on the identification of speculative statements in MEDLINE abstracts, but once again, this work is not directly applicable to clinical question answering.
 plementary approach to addressing clinical information needs. The PERSIVAL project, the most comprehensive study of such techniques applied on medical texts to date, leverages patient records to generate personalized summaries in response to physicians X  queries (McKeown, Elhadad, and Hatzivassiloglou 2003; Elhadad et al. 2005). Although the principles of evidence-based medicine. Patient information is no doubt important to answering clinical questions, and our work could certainly benefit from experiences gained in the PERSIVAL project.
 answering has been explored by a variety of researchers (e.g., Jacquemart and
Zweigenbaum 2003, Rinaldi et al. 2004), and was also the focus of recent workshops on question answering in restricted domains at ACL 2004 and AAAI 2005. Our work contributes to this ongoing discourse by demonstrating a specific application in the domain of clinical medicine.
 problem. Although it is clear that measures designed for open-domain factoid questions are not appropriate, the community has not agreed on a methodology that will allow meaningful comparisons of results from different systems. In Sections 9 and 10, we have discussed many of these issues. Recently, there is a growing consensus that an evaluation methodology based on the notion of  X  X nformation nuggets X  may provide an appropriate framework for assessing the quality of answers to complex questions.
Nugget F-score has been employed as a metric in the TREC question-answering track since 2003, to evaluate so-called definition and  X  X ther X  questions (Voorhees 2003). A number of studies (e.g., Hildebrandt, Katz, and Lin 2004) have pointed out shortcom-ings of the original nugget scoring model, although a number of these issues have been recently addressed (Lin and Demner-Fushman 2005a, 2006b). However, adaptation of the nugget evaluation methodology to a domain as specific as clinical medicine is an endeavor that has yet to be undertaken. 12. Future Work
The design and implementation of our current system leaves many open avenues for future exploration, one of which concerns our assumptions about the query interface.
Previously, a user study (Lin et al. 2003) has shown that people are reluctant to type full natural language questions, even after being told that they were using a question-answering system and that typing complete questions would result in better perform-ance. We have argued that a query interface based on structured PICO frames will yield better-formulated queries, although it is unclear whether physicians would invest 98 the upfront effort necessary to accomplish this. Issuing extremely short queries appears to be an ingrained habit of information seekers today, and the dominance of World Wide
Web searches reinforce this behavior. Given these trends, physicians may actually prefer the rapid back-and-forth interaction style that comes with short queries. We believe that if systems can produce noticeably better results with richer queries, users will make more of an effort to formulate them. This, however, presents a chicken-and-egg problem: One possible solution is to develop models that can automatically fill query frames given a couple of keywords X  X his would serve to kick-start the query generation process.
 study was performed with high-quality manually crafted queries (that were part of the test collection). Although this was intended to demonstrate the performance of our
EBM citation scoring algorithm with respect to a strong baseline, it also means that we have omitted a component in the automatic question-answering process. Translating a clinical question into a good PubMed query is not a trivial task X  X n our experiments, it required an experienced searcher approximately 40 minutes on average per question.
However, it is important to note that query formulation in the clinical domain is not a problem limited to question-answering systems, but one that users of all retrieval systems must contend with.
 there is an infinite variety of clinical questions, the number of query types is bounded and far smaller in number; see Huang, Lin, and Demner-Fushman (2006) for an analysis.
In a query interface based on PICO frames, it is possible to identify a number of proto-typical query frames. From these prototypes, one can generate query templates that ab-stract over the actual slot fillers X  X his is the idea behind Clinical Queries. Although this method will probably not retrieve citations as high in quality as custom-crafted queries, there is reason to believe that as long as a reasonable set of citations is retrieved, our sys-tem will be able to extract relevant answers (given the high accuracy of our knowledge extractors and citation scoring algorithm). The second approach to tackling this problem is to bypass PubMed altogether and index MEDLINE with another search engine.
Due to the rapidly changing nature of the entire MEDLINE database, experiments for practical purposes would most likely be conducted on a static subset of the collection, for example, the ten-year portion created for the TREC 2004 genomics track (Hersh,
Bhupatiraju, and Corley 2004). Recent results from TREC have demonstrated that high performance ad hoc retrieval is possible in the genomics domain (Hersh et al. 2005), and it is not a stretch to imagine adopting these technologies for clinical tasks. Using a separate search engine would provide other benefits as well: Greater control over the document retrieval process would allow one to examine the effects of different indexing schemes, different query operators, and techniques such as query expansion; see, for example, Aronson, Rindflesch, and Browne (1994). Finally, yet another way to solve the document retrieval problem is to eliminate that stage completely. Recall that our two-stage architecture was a practical expediency, because we did not have access to the computing resources necessary to pre-extract PICO elements from the entire
MEDLINE database and directly index the results. Given access to more resources, a system could index identified PICO elements and directly match queries against a knowledge store.
 we would have to first define what a good answer should be. We have empirically verified that an extractive approach based on outcome sentences is actually quite satisfactory, but our algorithm does not currently integrate evidence from multiple abstracts; although see Demner-Fushman and Lin (2006). Furthermore, the current an-swer generator does not handle complex issues such as contradictory and inconsistent statements. To address these very difficult challenges, finer-grained semantic analysis of medical texts is required. 13. Conclusion
Our experiments in clinical question answering provide some answers to the broader research question regarding the role of knowledge-based and statistical techniques in advanced question answering. This work demonstrates that the two approaches are complementary and can be seamlessly integrated into algorithms that draw from the best of both worlds. Explicitly coded semantic knowledge, in the form of UMLS, and software for leveraging this resource X  X or example, MetaMap X  X ombine to simplify many knowledge extraction tasks that would be far more difficult otherwise. The re-spectable performance of our population, problem, and intervention extractors, all of which use relatively simple rules, provides evidence that complex clinical problems can be tackled by appropriate use of ontological knowledge. Explicitly coded semantic knowledge is less helpful for outcome identification due to the large variety of possi-ble  X  X utcomes; X  nevertheless, knowledge-rich features can be combined with simple, statistically derived features to build a good outcome classifier. Overall, this work demonstrates that the application of a semantic domain model yields clinical question answering capabilities that significantly outperform presently available technology, especially when coupled with traditional statistical methods (classification, evidence combination, etc.).
 that assists physicians in the patient care process. Our work demonstrates that the principles of evidence-based medicine can be computationally captured and imple-mented in a system, and although we are still far from operational deployment, these positive results are certainly encouraging. Information systems in support of the clinical decision-making process have the potential to improve the quality of health care, which is a worthy goal indeed.
 Acknowledgments 100 102
