 Top-K ranking query in uncertain databases aims to find the top-K tuples according to a ranking function. The in-terplay between score and uncertainty makes top-K rank-ing in uncertain databases an intriguing issue, leading to rich query semantics. Recently, a unified ranking framework based on parameterized ranking functions (PRFs) is formu-lated, which generalizes many previously proposed ranking semantics. Under the PRFs based ranking framework, effi-cient pruning approach for Top-K ranking on dataset with tuple uncertainty has been well studied in the literature. However, this cannot be applied to top-K ranking on dataset with value uncertainty (described through attribute-level uncertain data model), which are often natural and useful in analyzing uncertain data in many applications. This pa-per aims to develop efficient pruning techniques for top-ranking on dataset with value uncertainty under the PRFs based ranking framework, which has not been well studied in the literature. We present the mathematics of deriving the pruning techniques and the corresponding algorithms. The experimental results on both real and synthetic data demonstrate the effectiveness and efficiency of the proposed pruning techniques.
 H.2.8 [ DATABASE MANAGEMENT ]: Database Ap-plications uncertain data, Top-K ranking, pruning
There has been a great deal of research interest in uncer-tain databases [10, 8, 1] due to the large amount of uncertain data emerging from a variety of application domains, such as information extraction and integration, market predic-tion, data mining and machine learning, and sensor network. Figure 1: Table M containing 3 tuples, each of which describes a movie with an uncertain rating A line of research work has focused on ranking uncertain data, with different rank semantics proposed in the litera-ture, such as tuple probability based rank [9], U-Top and U-Rank [11], probabilistic threshold top-K [4], expected rank [5], and typical top-K queries [3]. Recently, a unified approach to top-K ranking in uncertain databases, based on parameterized ranking functions (PRFs), is formulated [7], which generalizes many of the previously proposed ranking functions. The PRFs based ranking framework first maps each tuple in the uncertain database to a value (called PRF value in the following), and then sorts the tuples according to their PRF values. Computing the PRF value for each tuple in the database needs a quadratic time cost, which is too ex-pensive. Wang et al. [13] further proposed efficient pruning techniques to improve the performance, which mainly aims for the tuple-level uncertain data model. The tuple-level uncertain data model is widely adopted to represent object uncertainty, where each tuple is considered as a description of an object, and has an additional attribute describing the probability that the corresponding object exists.
Parallel to the tuple-level uncertain data model, the attribute-level uncertain data model is widely used to represent value uncertainty where the uncertain attributes of the correspond-ing object are described as probability distributions. The value uncertainty arises naturally from many practical ap-plications such as sensor readings [2, 6], and spatial objects with fuzzy locations [12]. Developing efficient pruning tech-niques for the unified PRFs based ranking framework on dataset with value uncertainty is the aim of this paper.
Next, we illustrate a movie rating example, where the value uncertainty naturally arises.

Example 1. In Amazon, each customer can rate a movie from 1 star to 5 star. Thus, each movie X  X  rating can be de-scribed as a discrete probability distribution, with each prob-ability computed as the proportion of the customers giving the corresponding rating. Figure 1 illustrates a table con-taining 3 tuples, each of which describes a movie with an uncertain rating. For example, the first tuple describes a movie whose rating can take 3 star with probability 0.9, and Figure 2: Table M under the x-tuple model corre-sponding to Table M in Figure 1 under the attribute-level model. The generating rules are t 1  X  t 2 , t 3  X  t t  X  t 6  X  t 7 . 4 star with probability 0.1, meaning that 90% of its rating customers give a 3 star, and 10% of its rating customers give a 4 star. Note that we will continue to use this sim-ple example for illustrating some basic computing methods in this paper, and in realistic applications there may be a largeamountofmovieseachofwhichmayhaveallthefive possible ratings.
 Suppose that the user wants to find the top-2 movies in Example 1. In the PRFs based ranking framework, a pa-rameterized ranking function is used to map each tuple to its PRF value. Then, the three tuples are sorted according to their PRF values, and the 2 movies with the largest PRF values are returned to the user. For the uncertain table M in Figure 1, the PRF value for each tuple t (denoted as  X  is computed as the weighted sum of the probabilities that t is ranked at the first, second, and third place (denoted as P ( r ( t )=1), P ( r ( t )=2),and P ( r ( t ) = 3) respectively). That is,  X  ( t )=  X  1 P ( r ( t )=1)+  X  2 P ( r ( t )=2)+  X  3 P ( r ( where  X  1 ,  X  2 ,  X  3 are non-increasing real numbers. Note that the PRFs based ranking framework generalizes a vari-ety of different ranking functions through appropriate weight parameter choices. For example, when  X  1 =1,  X  2 =0,  X  3 = 0, all the tuples are ranked according to the prob-abilities that they have the largest ratings; when  X  1 =1,  X  2 =1,  X  3 = 0, all the tuples are ranked according to the probabilities that they are ranked at the top-2 places; when  X  =3  X  i (1  X  i  X  3), the PRF values will produce the same ranking as the expected rank semantic [5]. Note that the expected score semantic, which ranks the movies by their expected ratings in Example 1, cannot be simulated using the PRFs based ranking framework. As illustrated in [5], the expected score semantic is usually rejected due to its sensi-tivity to the score values. In realistic applications, replacing expected score semantic with expected rank semantic has the effect of moderating the influence of outliers.
The PRFs based top-K ranking under the attribute-level uncertain data model can be implemented by transforming the corresponding uncertain table into an equivalent uncer-tain table with x-tuple model, and then using the ranking techniques developed for the tuple-level uncertain model [7]. The corresponding algorithm is called Model Transforming Based Algorithm in the following. Figure 2 shows the table M under the x-tuple model transformed from the table M under the attribute-level model in Figure 1. The first tu-ple in M is transformed to two tuples in M , corresponding to the two possible ratings for Movie1, and a corresponding generating rule is produced to represent the mutual exclu-sive relation of the two tuples in M . The corresponding tuples and generating rules in M for the other two tuples in
M are produced similarly. The PRF value for a tuple in M can be computed by first computing the PRF val-ues of its corresponding tuples in M and adding them all. After computing the PRF value for each tuple in M ,the K tuples with the largest PRF values are returned as the answer of the top-K ranking query. We cannot apply the efficient pruning techniques under the x-tuple model as pre-sented in [13] to the attribute-level model, since the efficient pruning techniques under the x-tuple model aims to find the top-K tuples with the largest PRF values without comput-ing the exact PRF values of a large portion of the tuples in the uncertain database, while computing the PRF value of a tuple in a table under the attribute-level uncertain model needs all the PRF values of the corresponding tuples in the transformed table under the x-tuple model.

In this paper, we present a Tuple Insertion Based Algo-rithm for computing top-K ranking query on uncertain data with value uncertainty under the PRFs based ranking frame-work, where each tuple is considered as a whole unit instead of split tuples under the x-tuple model. Suppose that there are N tuples in the table under the attribute-level model, and each tuple X  X  score has an average number of M possi-ble alternatives. In the Model Transforming Based Algo-rithm, the time cost of computing the PRF value for each tuple under the original attribute-level model is O ( M 3 N 2 while the time cost of the Tuple Insertion Based Algorithm is
O ( MN 2 ). Based on the Tuple Insertion Based Algorithm, we present our pruning algorithm for computing the top-K ranking query, which avoids the computation of the exact PRF values of some tuples which are guaranteed not to be in the top-K list. We also show the mathematics of de-riving the pruning algorithm. Our experimental results on both real data and synthetic data show orders of magnitude speedup over algorithms without pruning.

This paper is organized as follows. In Section 2, we review the attribute-level uncertain data model and the PRFs based ranking framework. In Section 3, we present an equivalent PRF value computing method, which forms the basis of our pruning algorithm. In Section 4, we present the mathemati-cal derivation for our pruning techniques and corresponding pruning algorithms. Section 5 presents our experimental re-sults. Section 6 concludes the paper.
In this section, we review the attribute-level uncertain data model for representing value uncertainty and the PRFs based ranking framework.
Without loss of generality, we assume the uncertain database contains simply one table T with N tuples t 1 ,t 2 ,...,t The commonly adopted attribute-level uncertain data model is used to represent the value uncertainty. Each tuple t in
T has one score attribute s i whose value is uncertain. s is viewed as a discrete random variable described as a pmf (probability mass function), { s i 1 ,p i 1 , s i 2 ,p s s i with probabilities s , ... , s in i the n i alternatives of score s i . The different Figure 3: A Table following the attribute-level un-certain data model random variables corresponding to different tuple scores are assumed to be independent. A table following this model is illustrated in Figure 3.
Now, we review the top-K ranking query under the PRFs based ranking framework.

In the PRFs based ranking framework, a parameterized ranking function(PRF),  X  : T  X  R is defined to be where t is a tuple in T , r ( t ) is the rank of t ,and P is the probability that tuple t is ranked at the position  X  , ... ,  X  N are monotonically non-increasing real numbers which weight the corresponding ranking positions of tuple Note that from the definition in [7], the PRF function and all the weights  X  i can take complex numbers. In our work, we constrain the corresponding domains to real numbers, which represent the majority of applications.
 For each tuple t ,  X  ( t ) is called its PRF value. A top-ranking query on table T first computes all its tuples X  PRF values, and then returns the K tuples with the largest PRF values.

The PRFs based ranking framework can be interpreted under the prevalent possible world semantics [10]. The set of all possible worlds is denoted by PW = { pw 1 ,pw 2 ,...,pw Each possible world pw i is formed by taking one alternative for each tuple X  X  uncertain score and has a probability P ( computed as the multiplication of the corresponding alter-natives X  probabilities. Each possible world corresponds to a single deterministic database instance. For each tuple t its rank in each possible world pw i , denoted as r pw i ( be determined according to all the alternative scores in pw The probability that tuple t is ranked at the i th position can be computed as: Then, all the tuple X  X  PRF values can be computed from Equation (2), and the top-K tuples with the largest PRF values can be determined.

Note that for a tuple t and a possible world pw , if there are x tuples whose scores are larger than t  X  X  score in pw define the rank of t as x + 1. For example, suppose that there are 6 tuples in a possible world with scores 4, 3, 3, 2, 2, 1, then, their ranks are 1, 2, 2, 4, 4, 6 respectively.
Example 2. Figure 4 illustrates all the possible worlds for the uncertain table in Figure 1. Tuple t 1 is ranked in the first place in possible worlds pw 1 , pw 2 , pw 3 , pw pw 9 . Hence, P ( r ( t 1 )=1)= P ( pw 1 )+ P ( pw 2 )+ P ( pw P ( pw 7 )+ P ( pw 8 )+ P ( pw 9 )=0 . 6 .Similarly,wecancompute P ( r ( t 1 )=2)=0 . 4 , P ( r ( t 1 )=3)=0 . 0 . Iftheweightsare Figure 4: All the possible worlds for the uncertain table in Figure 1 set as  X  1 =1 ,  X  2 = 1 2 ,  X  3 = 1 3 , then,  X  ( t 1 )=  X  with the largest PRF values are t 1 and t 2 .

Since there can be exponentially many combinations of tu-ples each generating a distinct possible world, the computing method of enumerating all the possible worlds is infeasible. In [7], a Model Transforming Based Method is proposed. To compute all the tuples X  PRF values, T can be first trans-formed to an equivalent table T with x-tuple model, where each tuple t i with score { s i 1 ,p i 1 , s i 2 ,p i 2 ,..., is transformed to n i tuples in T , with each tuple correspond-tion rule is added for T , representing the exclusive relation of the tuples originating from t i . Then, the PRF values of the resulting tuples in T arecomputed,andthePRFval-ues of the original tuples in T are computed by adding the PRF values of their alternatives in T . For each tuple in the time cost for computing its PRF value is quadratic to the number of tuples in T [7]. Suppose that there are N tuples in the table under the attribute-level model, and each tuple has an average number of M possible alternatives. In the Model Transforming Based Algorithm, the time cost of computing the PRF value for each tuple under the original attribute-level model is O ( M 3 N 2 ), since for each original tuple there are an average number of M tuples under the x-tuple model and computing the PRF value for each of them needs a time cost of O (( MN ) 2 ). In this section, we present an equivalent Tuple Insertion Based Method for computing the PRF values, which forms the basis of our pruning algorithm.
 For tuple t in T , to compute  X  ( t ), we start with a tuple set S which only contains t .Then,in S , t is ranked at the first place with probability 1.0, and the PRF value of t ,when only the tuples in S is considered, is  X  (1) ( t )=  X  1 P 1) =  X  1 . Here, we use  X  ( j ) ( t ) to denote the PRF value of t when only the j elements in S are considered. Then, a new tuple in T is added to S ,and  X  (2) ( t ) is computed. This process continues until all the N elements are added to S and  X  ( N ) ( t ) is computed. At last,  X  ( t )=  X  ( N ) ( following, for convenience of description, we use t 1 to denote t ,and t i to denote the i th added tuple from T .
Suppose that t 1 , t 2 , ... , t j has been added to S .Weuse  X  uv to denote, when only considering the j tuples in S ,the probability that t 1  X  X  score takes value s 1 u and t 1 is ranked at the v th position. When t j +1 is added to S ,if t j +1 will remain unchanged; otherwise, t 1  X  X  ranking position will increase by 1.

Hence,  X  ( j +1) uv can be computed as Combining the initial conditions that  X  (1) u 1 = p 1 u , deductively.
 Then, P ( r ( t 1 )= i ) can be computed as and  X  ( t 1 ) can be computed from Equation (2).

Example 3. We compute the PRF values for the tuples of the uncertain table in Figure 1 by using the Tuple Inser-tion Based Computing Method. For tuple t 1 ,whenonly t 1 when considering the only one tuple in S , the probability that t takes the value s 11 =3 and is ranked at the first place is 0.9 and the probability that t 1 takes the value s 12 =4 and is ranked at the first place is 0.1. After t 2 is added to S only considering the two tuples in S ,wehave Similarly, after t 3 is added to S , according to Equation (4), we have  X  (3) 11 ( t 1 )=0 . 54 ,  X  (3) 12 ( t 1 )=0 . 36 ,  X  21 ( t 1 )=0 . 06 ,  X  and if  X  1 ,  X  2 ,  X  3 are set to 1 , 1  X  ( t 2 )=0 . 7 and  X  ( t 3 )=0 . 39 canbecomputedsimilarly,and we get the same results as in Example 2.

The Tuple Insertion Based Computing Method conforms to the possible world semantics. The proof is omitted here due to the space constraint. For each tuple t whose score has M alternatives, the time cost of computing the PRF value for t is O ( MN 2 ), since there are M alternatives and for each alternative the time cost of computing  X  ( N ) uv according to Equation (4) is O ( N 2 ).
The basic algorithm for computing a top-K ranking query on table T = { t 1 ,t 2 ,...,t N } is first computing  X  ( ... ,  X  ( t N ), selecting the K largest, and then returning the corresponding tuples.

The skeleton of our pruning algorithm is first selecting K tuples, and computing their PRF values. For each other tuple t in T , we compute an upper bound for the PRF value of t . If the upper bound is less than the K th smallest PRF value retrieved so far, then, the computing of the exact PRF value for t is avoided and the corresponding time cost is saved. Otherwise, the exact PRF value for t is computed and the top K list is updated. The pruning algorithm is based on the Tuple Insertion Based Computing Method described in Section 3, and we call it Upper Bound Estimation Based Pruning Algorithm in the following.

The key point is to decide an upper bound for t  X  X  PRF value, without computing its exact value. The main idea comes from the following three heuristics.
 Heuristic 1: As described in the Tuple Insertion Based Computing Method, we also use t 1 to denote the new tuple from T . In the Tuple Insertion Based Method, after t 1 , ... , t j have been added to S , when only considering the j tuples in S ,wehave and  X  ( j ) ( t 1 ) can be computed as Then, we have
Proposition 1.  X  (1) ( t 1 )  X   X  (2) ( t 1 )  X  ...  X   X  ( N )
Proof. We need to prove  X  ( j ) ( t 1 )  X   X  ( j +1) ( t 1 j  X  N  X  1.

In the Tuple Insertion Based Computing Method, after t j +1 is added to S ,wehave In the last step, can be obtained by recombining the sum items and using
Continuing, from and we get Based on Proposition 1, for each new tuple t in T ,wecan i (1  X  i  X  N  X  1),  X  ( i ) ( t )islessthanthe K th smallest PRF value retrieved so far, then, t must not be in the top-K and the computing of the exact PRF value for t is avoided; otherwise,  X  ( t )=  X  ( N ) ( t ).

Example 4. We continue with the uncertain table in Fig-ure 1. The goal is to find the top-2 tuples with the largest PRF values. Suppose that the weights are set as  X  1 =1 ,  X  2 = 1 2 ,  X  3 = 1 3 and we have computed  X  ( t 1 )=0 . 8 and  X  ( t 2 )=0 . 7 by using the Tuple Insertion Based Computing Method. Now, consider t 3 .Wecompute  X  (1) ( t 3 ) ,  X  (2) and  X  (3) ( t 3 ) successively. We start with a set S which only contains t 3 , and then add t 1 , t 2 in order. When S only con-tains t 3 ,wehave  X  (1) ( t 3 ) = 1 . 0 .After t 2 is added to we have  X  (2) ( t 3 )=0 . 545 , computed according to the Tuple Insertion Based Algorithm. Since the second smallest PRF clude that t 3 mustnotbeinthetop-2listandthefurther computation is avoided.

Heuristic 2: For a tuple t , if there are n L tuples whose pmf support (the support of a probability mass function f probability mass function its pmf support) lies to the left of of t  X  X  pmf support, and n R tuples whose pmf support lies to the right of t  X  X  pmf support, then all the possible ranking places for t are { i | n R &lt;i  X  n  X  n L } .Weuse T to denote the tuples whose pmf support overlap with t  X  X  pmf support. Then, when computing the PRF value for t , only the tuples in
T need to be considered. We only need to set the weight corresponding to the i th place ( n R &lt;i  X  n  X  n L )in w ( n
R + i )th ranking place in T , and use the same computing process as on T .

Heuristic 3: We can sort all the tuples in T according to their left end point of pmf support, and denote the K th smallest left end point of pmf support as  X  . For each other tuple t , if its right end point of pmf support is less than  X  , then t must not be in the top-K , and can be neglected. Thus, the time costs for computing the PRF values of all the tuples whose right end points of pmf supports are less than  X  are saved.
To evaluate the performance of our pruning algorithm, we implemented the Model Transforming Based Algorithm, the Tuple Insertion Based Algorithm, and the Upper Bound Estimation Based Pruning Algorithm. Our algorithms were implemented in GNU C++. We conducted a systematic em-pirical study using both real datasets and synthetic datasets. All the experiments were run on a quad-core 2.2GHZ PC with 2GB RAM, running Linux operating system.

We have verified that all the algorithms produce the same top-K answer list on both the real datasets and the synthetic datasets. This observation empirically verifies the correct-ness of the Tuple Insertion Based Algorithm and the Upper Bound Estimation Based Pruning Algorithm. In the follow-ing, we focus on the performance study.

For the parameterized ranking function in Equation (2), we experimented with three forms of weights: (1) randomly generated weights, reciprocal weights, and probabilistic thresh-old top-K weights. For the randomly generated weights, we generated N random numbers and sorted them into a de-scending order and  X  i is set to the i th number; for the recip-rocalweights,weset  X  i = 1 i ; for the probabilistic threshold top-K weights, we set  X  1 =1, ... ,  X  K =1,  X  K +1 =0, ...  X 
N = 0. In our experiments, we found that the performance of our pruning algorithm for the randomly generated weights is similar to that using the reciprocal weights, and is worse than the probabilistic top-K weights. In the following, we only report the results for the randomly generated weights.
To investigate the performance of our pruning algorithm in real applications, we use the MovieLens Data 1 ,which contains the following three datasets with three different scales: (1) MovieLens 100k (consisting of 100,000 ratings from 943 users on 1682 movies), (2) MovieLens 1M (consist-ing of 1,000,209 ratings from 6,040 users on 3,706 movies) and (3) MovieLens 10M (consisting of 10,000,054 ratings from 71,567 users on 10,677 movies). All the ratings are made on a 5-star scale. The ratings of the first two data sets have whole-star increments, producing 5 different scales. The ratings of the third dataset have half-star increments, producing 10 different scales. Based on the three datasets, we produced three uncertain tables: Movie1, Movie2 and Movie3 with 1682 tuples, 3706 tuples, 10677 tuples respec-tively. Each tuple represents a movie and has an uncertain score attribute representing the movie X  X  rating. Each score attribute has 5 possible ratings and corresponding proba-bilities for tables Movie1 and Movie2, and 10 possible rat-ings and corresponding probabilities for table Movie3. The probability that a movie takes a rating is computed as the proportion of the users who give the corresponding rating to the movie.  X 
Figure 5 compares the running times of the Model Trans-forming Based Algorithm, the Tuple Insertion Based Algo-rithm and the Upper Bound Estimation Based Pruning Al-gorithm for the top-K ranking query on the three uncertain tables Movie1, Movie2 and Movie3. We set K = 10. The http://www.grouplens.org Figure 5: Running times of the algorithms for top-K ranking query on uncertain tables Movie1, Movie2, and Movie3 Figure 6: Running times of the algorithms for top-K ranking query on Movie3 with respect to different number of K running time of the Tuple Insertion Based Algorithm is al-ways less than the Model Transforming Based Algorithm, and the Upper Bound Estimation Based Pruning Algorithm outperforms the Tuple Insertion Based Algorithm by 2, 3 and 5 orders of magnitude for the three uncertain tables respectively.

Figure 6 shows the running times of the Model Transform-ing Based Algorithm, the Tuple Insertion Based Algorithm and the Upper Bound Estimation Based Pruning Algorithm for the top-K ranking query on table Movie3 with respect to different number of K . The time costs of all the algorithms are linear to the number of K .

We also run the corresponding algorithms on various syn-thetic data sets with different score distributions and prob-ability distributions. We investigated the running times of the algorithms for the top-K queries with respect to dif-ferent number of tuples, different number of alternatives in each tuple X  X  score and different number of K , and arrived the similar conclusion that the Tuple Insertion Based Algorithm outperforms the Model Transforming Based Algorithm, and the Upper Bound Estimation Based Pruning Algorithm out-performs the Tuple Insertion Based Algorithm by up to 5 orders of magnitude
In this paper, we present a pruning algorithm for top-K ranking on dataset with value uncertainty. Experiments on both real data and synthetic data demonstrate the efficiency of the algorithm. We have made a simple assumption that each tuple has only one score attribute represented as a pmf . This can be easily extended to the general case where a ranking function of multiple score attributes is used since the pmf of the ranking function can be derived from the pmf s of the corresponding score attributes and then the pruning algorithm can be applied. Our future work will be applying our algorithm to real world applications. The work is supported by Tsinghua University Initiative Sci-entific Research Program, Tsinghua-Tencent Joint Labora-tory for Internet Innovation Technology, National Natural Science Foundation of China (60773156, 61073004), Chi-nese Major State Basic Research Development 973 Program (2011CB302203-2), and Important National Science &amp; Tech-nology Specific Program (2011ZX01042-001-002-2). [1] C. C. Aggarwal and P. S. Yu. A survey of uncertain [2] A. Deshpande, C. Guestrin, S. R. Madden, J. M. [3] T. Ge, S. Zdonik, and S. Madden. Top-k queries on [4] M. Hua, J. Pei, W. Zhang, and X. Lin. Ranking [5] J. Jestes, G. Cormode, F. Li, and K. Yi. Semantics of [6] B. Kanagal and A. Deshpande. Online filtering, [7] J. Li, B. Saha, and A. Deshpande. A unified approach [8] J.Pei,M.Hua,Y.Tao,andX.Lin.Queryanswering [9] C. R  X e, N. Dalvi, and D. Suciu. Efficient top-k query [10] A. D. Sarma, O. Benjelloun, A. Halevy, and J. Widom. [11] M. A. Soliman, I. F. Ilyas, and K. C. Chang. Top-k [12] Y. Tao, R. Cheng, X. Xiao, W. K. Ngai, B. Kao, and [13] C. Wang, L. Y. Yuan, H.-H. You, and O. R. Zaiane.
