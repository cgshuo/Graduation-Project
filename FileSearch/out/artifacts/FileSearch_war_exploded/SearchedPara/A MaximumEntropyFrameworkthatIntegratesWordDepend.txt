 Fuliang.weng@rt c.bosch.com Automatic reading comprehension (RC) systems can analyze a given passage and generate/e xtract answers in response to questions about the pas-sage. The RC passages are often constrained in their lengths and the tar get answer sentence usu-ally occurs only once (or very few times). This dif ferentiates the RC task from other tasks such as open-domain question answering (QA) in the Text Retrie val Conference (Light et al., 2001). In order to generate/e xtract a specic precise answer to a given question from a short passage,  X deep X  linguis-tic analysis of sentences in a passage is needed.
Pre vious efforts in RC often use the bag-of-w ords (BO W) approach as the baseline, which is further augmented with techniques such as shallo w syn-tactic analysis, the use of named entities (NE) and pronoun references. For example, Hirschman et al. (1999) have augmented the BO W approach with stemming, NE recognition, NE ltering, se-mantic class identication and pronoun resolution to achie ve 36% HumSent 1 accurac y in the Reme-dia test set. Based on these technologies, Rilof f and Thelen (2000) impro ved the HumSent accurac y to 40% by applying a set of heuristic rules that as-sign handcrafted weights to matching words and NE. Charniak et al. (2000) used additional strate gies for dif ferent question types to achie ve 41%. An exam-ple strate gy for why questions is that if the rst word of the matching sentence is  X this,  X   X that,  X   X these X  or  X those,  X  the system should select the pre vious sen-tence as an answer . Light et al. (2001) also intro-duced an approach to estimate the performance up-per bound of the BO W approach. When we apply the same approach to the Remedia test set, we ob-tained the upper bound of 48.3% HumSent accurac y. The state-of-art performance reached 42% with an-swer patterns deri ved from web (Du et al., 2005).
This paper investigates the possibility of enhanc-ing RC performance by applying  X deep X  linguistic analysis for every sentence in the passage. We refer to the use of two types of features, namely word dependencies and grammatical relations, that are inte grated in a maximum entrop y frame work. Word dependencies refer to the headw ord depen-dencies in lexicalized syntactic parse trees, together with part-of-speech (POS) information. Grammat-ical relations (GR) refer to linkages such as sub-ject, object, modier , etc. The ME frame work has sho wn its effecti veness in solving QA tasks (It-tycheriah et al., 1994). In comparison with pre vi-ous approaches mentioned earlier , the current ap-proach involv es richer syntactic information that cover longer -distance relationships. We used the Remedia corpus (Hirschman et al., 1999) and ChungHw a corpus (Xu and Meng, 2005) in our experiments. The Remedia corpus contains 55 training stories and 60 testing stories (about 20K words). Each story contains 20 sentences on aver-age and is accompanied by ve types of questions: who, what, when, wher e and why . The ChungHw a corpus contains 50 training stories and 50 test stories (about 18K words). Each story contains 9 sentences and is accompanied by four questions on average. Both the Remedia and ChungHw a corpora contain the annotation of NE, anaphor referents and answer sentences. Suppose a story S contains n sentences, C 0 , . . . , C the objecti ve of an RC system can be described as: Let  X  x  X  be the question (Q) and  X  y  X  be the answer sentence C computed by the ME method (Zhou et al., 2003): factor , f f ; f f . For a given question Q , the C probability is selected. If multiple sentences have the maximum probability , the one that occurs the earliest in the passage is returned. We used the selecti ve gain computation (SGC) algorithm (Zhou et al., 2003) to select features and estimate parameters for its fast performance.
 Figure 1. The lexicalized syntactic parse trees of a question and a candidate answer sentence. A feature in the ME approach typically has binary values: f wise f of  X deep X  linguistic features to be inte grated in the ME frame work in two subsections. 4.1 POS Tags of Matching Words and Consider the follo wing question Q and sentence C , Q: Who wr ote the  X Pledg e of Alle giance X  C: The pledg e was written by Frances Bellamy . The set of words and POS tags 2 are: Q: { write/VB, pledg e/NN, alle giance/NNP } C: { write/VB, pledg e/NN, by/IN, Frances/NNP , Two matching words between Q and C (i.e.  X  write  X  and  X  pledg e  X ) acti vate two POS tag features:
We extracted dependencies from lexicalized syntactic parse trees, which can be obtained accord-ing to the head-rules in (Collins, 1999) (e.g. see Figure 1). In a lexicalized syntactic parse tree, a dependenc y can be dened as: where hc is the headw ord of the child node, hp is the headw ord of the parent node ( hc 6 = hp ), hr is the headw ord of the root node. Sample Figure 2. The dependenc y trees produced by MINI-PAR for a question and a candidate answer sentence. dependencies in C (see Figure 1) are: The dependenc y features are represented by the combined POS tags of the modiers and headw ords of (identical) matching dependencies 3 . A matching dependenc y between Q and C , &lt; pledg e  X  write &gt; acti vates a dependenc y feature: f In total, we obtained 169 and 180 word dependenc y features from the Remedia and ChungHw a training sets respecti vely . 4.2 Matching Grammatical Relationships (GR) We extracted grammatical relationships from the de-pendenc y trees produced by MINIP AR (Lin, 1998), which covers 79% of the dependenc y relationships in the SUSANNE corpus with 89% precision 4 . IN a MINIP AR dependenc y relationship: CA TE1 and CA TE2 represent such grammatical cat-egories as nouns, verbs, adjecti ves, etc.; RELA-TION represents the grammatical relationships such as subject, objects, modiers, etc. 5 Figure 2 sho ws dependenc y trees of Q and C produced by MINI-PAR. Sample grammatical relationships in C are pledg e N:det:Det the , and write V:by-subj:Pr ep by. GR features are extracted from identical matching relationships between questions and candidate sen-tences. The only identical matching relationship be-tween Q and C ,  X  write V:obj:N pledg e  X  acti vates a grammatical relationship feature: f total, we extracted 44 and 45 GR features from the Remedia and ChungHw a training sets respecti vely . We selected the features used in Quarc (Rilof f and Thelen, 2000) to establish the reference performance level. In our experiments, the 24 rules in Quarc are transferred 6 to ME features:  X If contains(Q, { start, begin } ) and contains(S, { start, begin, since , year } ) Then Score(S)+=20 X   X  f contains  X start X  or  X be gin X  and C contains  X start,  X   X be gin,  X   X since X  or  X year X  ; f
In addition to the Quarc features, we resolv ed ve pronouns ( he, him, his, she and her ) in the stories based on the annotation in the corpora. The result of using Quarc features in the ME frame work is 38.3% HumSent accurac y on the Remedia test set. This is lower than the result (40%) obtained by our re-implementation of Quarc that uses handcrafted scores. A possible explanation is that handcrafted scores are more reliable than ME, since humans can generalize the score even for sparse data. Therefore, we rened our reference performance level by combining the ME models (MEM) and handcrafted models (HCM). Suppose the score of a question-answer pair is scor e ( Q, C probability that C We combined the probabilities from MEM and HCM in the follo wing manner: To obtain the optimal  X  , we partitioned the training set into four bins. The ME models are trained on three dif ferent bins; the optimal  X  is determined on the other bins. By trying dif ferent bins com-binations and dif ferent  X  such that 0 &lt;  X  &lt; 1 with interv al 0.1, we obtained the average optimal  X  = 0 . 15 and 0 . 9 from the Remedia and ChungHw a training sets respecti vely 7 . Our baseline used the combined ME models and handcrafted models to achie ve 40.3% and 70.6% HumSent accurac y in the Remedia and ChungHw a test sets respecti vely .
We set up our experiments such that the linguistic features are applied incrementally -(i) First , we use only POS tags of matching words among questions and candidate answer sentences. (ii) Then we add POS tags of the matching dependencies. (iii) We ap-ply only GR features from MINIP AR. (iv) All fea-tures are used. These four feature sets are denoted as  X +wp,  X   X +wp+dp,  X   X +mini X  and  X +wp+dp+mini X  respecti vely . The results are sho wn in Figure 3 for the Remedia and ChungHw a test sets.

With the signicance level 0.05, the pairwise t -test (for every question) to the statistical signicance of the impro vements sho ws that the p-v alue is 0 . 009 and 0 . 025 for the Remedia and ChungHw a test sets respecti vely . The  X deep X  syntactic features signif-icantly impro ve the performance over the baseline system on the Remedia and ChungHw a test sets 8 . Figure 3. Baseline and proposed feature results on the Remedia and ChungHw a test sets. This paper proposes the inte gration of two types of  X deep X  linguistic features, namely word dependen-cies and grammatical relations, in a ME frame work to handle the RC task. Our system leverages linguistic information such as POS, word depen-dencies and grammatical relationships in order to extract the appropriate answer sentence for a given question from all available sentences in the passage. Our system achie ves 44.7% and 73.2% HumSent accurac y on the Remedia and ChungHw a test sets respecti vely . This sho ws a statistically signicant impro vement over the reference performance levels, 40.3% and 70.6% on the same test sets.
 Ackno wledgements This work is done during the rst author' s internship at RTC Bosch Corp. The work is also afliated with the CUHK Shun Hing Institute of Adv anced Engi-neering and partially supported by CUHK4237/03E from RGC of HKSAR Go vernment.

