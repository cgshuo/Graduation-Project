 Centre for Systems and Synthetic Biology, and Department of Computer Science, Royal Holloway, University of London, Egham, UK Departamento de Ciencias de la Computaci X n e Inteligencia Artificial, E.T.S.I. Inform X tica y de Telecomunicaci X n, Universidad de Granada, Granada, Spain 1. Introduction
In this work we present a novel solution for the multilabel categorization problem. In this kind of problems, a subset of categories (instead of just one) is assigned to each instance. Multilabel classifica-tion problems arise, in a natural way, in information processing, concretely on the subfield of automatic document categorization [23]. Due to their nature, an important number of text corpora are of a multi-label kind. For example, news articles can often belong to more than one category (this is the case of the Reuters-21578 [12] and the RCV1 [13] collections, which are composed of articles from the Reuters agency). In other domains, multiple labels are assigned as metadata which give a better description of the documents (this occurs, for example, in scientific papers and legal documents, which have associ-ated keywords from a controlled vocabulary, like the Mathematics Subject Classification or the MeSH Thesaurus in one case, and the Eurovoc thesaurus in the other case [4]). On the other hand, multilabel instances occur very commonly in the Internet: in many blog applications, blog posts for example, can be categorized with an arbitrary number of labels. Furthermore, in collaborative environments (like folk-sonomies, [31]) where users can add tags, multilabel is an ordinary process. Due to this fact, sometimes the word  X  X ag X  or  X  X abel X  is used instead of  X  X ategory X , they are all synonyms.
 sis of musical emotions [26,33], scene or image categorization [7,24], protein and gene function predic-tion [18,30] or medical diagnosis [25].

Although each instance has a given set of associated labels, and they are assigned as a whole, the normal approach to solve this problem is just ignore this fact and concentrate in obtaining good solutions not). Here, we propose a solution which takes into account the inter-category dependence, trying to find natural associations in order to improve the final categorization results.

The content of this paper is organized as follows: first of all (in Section 1.1) we recall the well-known problem of multilabel supervised categorization, reviewing some previous works in this area (Section 1.2) together with a brief explanation (Section 1.3) of what is the semantic of adding multiple labels to an instance instead of one. Based on probabilistic foundations, our approach is presented in Section 2, which results in two different models. An extensive experimentation with test collections coming from the text categorization field will be carried out in Section 3 to prove the validity of our proposal. Finally, in the light of the results previously obtained, several conclusions and future works will be pointed out in Section 4. 1.1. Multilabel supervised categorization
The problem of supervised multilabel classification (see, for instance [27]) deals with supervised learning, where the associated labels can be a set of undetermined size. Formally, it can be stated as a multilabel classifier means inferring a function f : X X  X  X  2 C \ X  , in other words, a function able to assign non-empty subsets of labels to any unlabeled instance.

In order to cope with this exponential output space, the classical approach consists in dividing the problem into |C| binary independent problems, and therefore learning |C| binary classifiers f i : X X  X  X  { c i , c i } (which decide whether the category is assigned or not to the instance). Although this is a naive This approach is called the binary relevance method [9,27,36], and it is often criticized for ignoring the existing correlations among labels.

Given the fact that many of the multilabel problems come from multi-tagged collections (i.e., collec-some of these tags are associated not only because of the content, but also due to the presence/absence of another tag. We explain examples of this phenomenon in Section 1.3. Roughly speaking, the main those results given a model previously trained on the label assignment vectors, which explicitly cap-tures relationships among categories and therefore improves classification results. As it is shown, our approach utilizes a simple but powerful independence assumption which results in a model that is more complex than the binary relevance method, but still is cheap enough.
 1.2. Related work
There is more than one hundred references partially related with multilabel categorization, most of them in the last years 1 . At first sight, the two possible solutions to this problem basically consist of transforming it to a single-label one, or adapting a learning procedure to work with these multiple labels at the same time. This taxonomy is given in [27], naming the former solutions problem transformation methods and algorithm adaptation the latter ones. Because our contribution adapts a learning procedure to multilabel learning, we review here only approaches of the second kind.

Almost all the typical classification algorithms have a multilabel version. For example, an adaptation of the entropy formula of the C4.5 tree learning algorithm has been proposed in [3] for multilabel clas-sification. In the lazy algorithms field, variations of the k -NN algorithm have been presented for this kind of problems [14,36]. Also, in [2] a k -NN is combined with a logistic regression classifier (in a different way that we do) to cope with multiple labels. Of course, variations on the SVM algorithm are shown in [9], where both intra-class dependencies and an improvement of the definition of margin for multilabel classification are used to build a new model.

On the other hand, there are methods dealing with multilabel classification within the probabilistic framework and therefore closer to our approach. In [17], a generative model is trained using training data, and completed with the EM algorithm, and computed the most probable vector of categories that should be assigned to the document. A subset of Reuters-21578 is used for experimentation, and noticeable improvements are shown.

A generative model is also presented in [29]. Here, the main assumption is that words in documents belonging to several categories can be characterized as a mixture of characteristic words related to each of the categories, being this assumption confirmed with experimentation. Both first (PMM1) and second (PMM2) order models are built, and learning algorit hms (using a MAP estimation) are proposed for both alternatives. Experiments are carried out with webpages gathered from the yahoo.com server. Presented results are good, and improve other methods as SVM, naive Bayes and k -NN.

More recently [5] proposed a novel method, called probabilistic classifier chains (generalizing the classifier chains) which exploits label dependence, showing that the method outperforms others in terms of loss functions. This is claimed via an extensive experimentation with artificial and real datasets. 1.3. The semantic of assigning multiple labels
In this section we try to enumerate three clear examples showing possible reasons for a manual indexer to assign multiple labels. Although we do not pretend to be exhaustive, we think that the three presented examples are common and may occur easily in multilabel problems: 1. Mixture of topics . An instance matches all the abstract description of several categories. This is 2. Contextualization . Some tags are added in order to fix the context in which other label is used. For 3. Non overlapping labels . Some subsets of labels do not admit instances belonging to all of them.
The only  X  X ure X  multilabel phenomenon is the first one. The second and the third denote that the occurrence of labels is not only based on the content of the instance, but also in the occurrence (or not) to say, the occurrence of a certain subset of labels increases the likelihood of other being added. On the other hand, in the third case, a song labeled with  X  X aroque X  (at a certain degree) and  X  X eggae X  (in a lower degree) may be detected by a classifier as an  X  X nomaly X , and be labeled only with  X  X aroque X  (given that the system has previously learned that the label  X  X aroque X  gives information about the low likelihood of also using the label  X  X eggae X  given that  X  X aroque X  is being used).

Although the first phenomenon can be initially captured by different binary classifiers, the second and the third can be tackled by looking at the labels of a training set, 2 and not only to the content of the category, will be a combination of the result of the binary classifier with the evidence given in the other categories by the other binary classifiers, taking into account the existing relationships among categories captured in the training set. In fact, this issue of label dependence has been recently shown to be crucial in multilabel learning [6]. All of this will be modeled in a probabilistic framework, which will give us a rich language to describe this procedure. 2. A probabilistic model for multilabel classification
Let x i  X  X  be an instance. Suppose we have a set of p categories C = { c 1 ,...,c p } . For every category c j , we define a binary random variable C j = { c j , c j } . Let us assume that we have p probabilistic binary classifiers, based only on the content of the instances (the features describing them). Thus, the subindex j indicates that this distribution is different and independent for every category). Assuming we have a perfect knowledge of the underlying probability distribution, these classifiers define p labeling  X  X lassify x i as c j if p j ( c j | x i ) &gt; 0 . 5  X .
 which represents the labeling of an instance with the other p  X  1 classifiers. We shall note for l j a j -th one). Thus, every component l jk of the vector l j is binary, and corresponds to the variable C k if k&lt;j and to C k
Our aim is to give a model which describes the probability of a class given knowledge of both the the probability p j ( c j | x i , l j ) . 2.1. The proposed model
Our model will start making a reasonable simplifying assumption ( general naive Bayes assumption 3 ): the other categories for this instance, l j , are independent, that is
From the point of view of a certain category c j , if its value is known, this equation assumes that the x good classification performance. Nevertheless, it should be stressed that our assumption does not mean independence between labels at all, but independence between labels and content given other label. As
Then, using Bayes X  theorem with this assumption, we compute the desired probability:
The first term is a proportionality factor which does not depend on the category. Therefore we get the expression, which leads us to the final formula: 1 estimated, through a learning process, from the labels of the training data, where every instance has as features some binary values telling if the instance belongs or not to any of the p  X  1 categories (all categories except j ). So, in our model, we need to train p binary classifiers from the content of the instances, and p binary classifiers from the labels assigned to these instances.

A last point should be clarified in the model. Given an instance x i to classify, we easily compute, for a need to know the true assignments of the labels which are not c j . In our model, we shall make a second assumption, approximating l j for l j , which is computed as follows: Where  X  k is a threshold function (  X  k ( z )= c k if z&lt; 0.5,  X  k ( z )= c k otherwise). In other words, Therefore p j ( c j | l j ) will be approximated by p j ( c j | l j ) . 2.2. An improved version of the model
It should be noticed that the model summed up in Eq. (2) does not really need the assumption that the random vectors L j are made of binary variables. In fact, the only required binary variables are the C j ones. Thus, the same equation can be applied equally if the variables in L j are continuous.
Therefore, we can rewrite an approximation of l j different than the one given in Eq. (3) by removing the threshold function:
This means that the components of the vector l j are values in [0,1] which represent our degree of belief in these labels being assigned to the instance. Note that, in order to use this extended version of (in the previous version we could assume the inputs were binary, and so the classifier). 2.3. The algorithm
We present finally the proposed algorithm for multilabel classification. We need, as input data, the different from c j : p j ( c j | l j ) . 2. For every category c j ,j  X  X  1 ,...,p } ,
The whole process is summed up in Fig. 1. Note that the computational load with respect to the binary relevance method is very low. Initially, apart from the binary classifiers based on content (which work on an input space containing, say, m features), one label classifier needs to be trained for each class. However, the label classifiers usually will be very cheap to train, as they work on an input space possessing p  X  1 features (and usually p m ). For the classification part, the binary relevance method as the number of features is low), and some additional computations needs to be performed, summarized in Eq. ( 2 ). As it can be seen, neither much additional memory space, nor computational power is needed to perform this approach. Therefore, its scalability should not be an issue.
 We have shown a new method to deal with relationships among labels, using probabilistic classifiers. This is why it is presented as a  X  X ethodology X . For a concrete problem, two decisions should be made about which underlying models should be used: first for the content classifiers, and second for the label classifiers. This choice relies heavily on the kind of problem selected, and is suitable of previous expe-rimentation to find a working set of methods. Also one may note that both choices are independent (so, each one with two possi bilities for computing the vectors l j ).
 3. Experimentation
We shall expose in this section an experimentation to test the validity of our approach, where some combinations of classifiers will be selected following a certain criterion. Of course, we do not aim being exhaustive and giving a long list of experiments. We are aware that many different collections and combinations of classifiers could be selected, so we tried to make a good experimentation by selecting a restricted but representative set of classifiers. 3.1. Corpora
Three different document categorization corpora have been used for experimentation. We describe them now, together with the preprocessing procedure used to obtain the term vectors.

First, Reuters-21578 (see [12], for instance) is a collection of 21578 news articles. We have used the most famous split (the one named ModApte), which divides the set of documents into a training and a test set, and categories only assigned to documents in the test set are removed (then resulting only 90 of them).

The Ohsumed collection [10] is a set of 348566 references from MEDLINE, an online medical infor-mation database. For every record the assigned MeSH terms (categories) are given. Because the number which are the root of some categories in a hierarchy. Documents which do not belong to that subtree of categories, are discarded, and the resulting corpus is called Ohsumed-23. This is the methodology followed in [11].

Finally, the RCV1 corpus is a relatively more recent corpus (see [13]), also based on Reuters news stories. It contains many documents ( 806791 for the final version, RCV1-v2), where the documents are preprocessed, with stopwords removed, and terms already stemmed. The number of categories named  X  X opic codes X  4 is 103 (after the split). An standard split is a lso provided, called LYRL2004, which gives a training set with over 23000 documents, and a test set with 781000. We have removed two categories which appear only in the training set (reducing the number to 101).

In the first two cases, the stopwords list used consists of 571 stopwords of the SMART retrieval system [22]. Also, the English stemming algorithm of the Snowball package [20] was applied to resulting words. In the Reuters-21578 also XML marks were removed.

In order to reduce the size of the lexicon, terms occurring in less than three documents were removed in Reuters-21578 and Ohsumed-23 (following the guidelines of [11]), and in less than five documents in RCV1, as done in [13]. All document preprocessing stage was made with DauroLab [21]. 3.2. Evaluation measures In this subsection we discuss the different evaluation measures we have used for our experimentation. Following the taxonomy considered in [15,28], we have selected label-based measures, example-based and ranking measures . We shall present the chosen measures along with a brief explanation of them, and a discussion of which performance aspects are considered for each one. 3.2.1. Example-based measures
They are measures specifically designed for multilabel problems. That is, they try to account for the Of those available, we have selected two: Hamming loss and subset 0/1 loss . Note that, as both measures are losses, the lower the value they have, the better the classification is.

Hamming loss computes the normalized Hamming distance between the set of assigned labels and the set of true labels, the Hamming loss will be equal to: where  X  stands for the symmetric difference of two sets and p is the number of labels. The measure ranges between 0 and 1 and will be averaged through all the elements of the test set to obtain its final value.

Subset 0/1 loss, in contrast, is a generalization of the 0/1 loss for multilabel problems where the set of labels is considered as a whole. For the previous case, the loss function is equal to: through all the test set. 3.2.2. Label-based measures
These are classical measures for binary classification problems. We have made in all cases hard cate-gorization (assigning or not every label) and then, we have used suitable measures for this task. Being where TP j , FP j and FN j stand for  X  X rue positives X ,  X  X alse positives X  and  X  X alse negatives X  of the j -th category. We have then selected the F 1 -measure adapted for categorization (the harmonic mean between precision and recall), in its macro and micro averaged versions (denoted by MF 1 and  X F 1 , respectively). See [15,23] for more details.

All the probabilistic classifiers have a natural threshold in 0 . 5 . We have not made any threshold tuning because it was not the aim of this paper. Anyway, it could be made independently, likely improving the results (see the discussion in Section 4). 3.2.3. Ranking measures
We have implemented one ranking measure, the one error , which evaluates how many times the label takes partially into account the ranking to perform the evaluation. For real-world use cases, in multilabel classification, where predicted labels are suggested to a human indexer using a ranking procedure, it would be very important that the labels at the top of the list were relevant. 3.3. Basic probabilistic classifiers
In order to have, first a baseline, and secondly a basic content classifier to be used afterwards for our model, we have considered three different classifiers, widely used in the literature. One which usually selected the multinomial naive Bayes, in its binary version [16]. For the second case, a k -NN classifier has been used, normalizing the output in order to obtain a value in [0 , 1] which can be interpreted as a probability. Finally, a linear SVM, with probabilistic output, 5 as performed by Platt X  X  algorithm [19] was chosen. We recall that the flexibility of this procedure is very high because any probabilistic classifier could be used instead of these three.

For the k -NN classifier, the best performing value of k was selected, based on previous experimen-tation existing in other works. Thus, we chose k =30 for Reuters and k =45 for Ohsumed-23 (as set in [11]). For RCV1, a value of k = 100 was used [13]. Also, following those references, we have performed feature selection in Reuters (1000 features selected by information gain, using the  X  X um X  combination [23]), and in RCV1 (8000 features selected by  X  2 , using the  X  X ax X  combination). No fea-ture selection was performed in Ohsumed-23 as noted in [11].

The implementation used for the algorithms was that contained in the DauroLab [21] package, except for the SVM where libsvm [1] was chosen. 3.4. Label classifiers
Here we show the two alternatives that we have selected as label classifiers. 3.4.1. Logistic regression
On the first hand, and based on previous experimentations (not shown here), we have chosen a logis-tic regression-based classifier to model the posterior probability distribution of the category given the correct labels of all the other categories, namely: where w 0 is a real scalar and w a p  X  1 dimensional real vector, and  X   X   X  is the usual dot product. The parameters of this model are learned to maximize a certain criterion (generally a maximum likelihood approach).
 and L j (in which we are not interested). Finally, it is very simple, fast to learn, and works reasonably well in almost all the environments.

In order to have accurate estimates, and because the dimensionality of the problem p is high, the method selected to learn the weights is a Bayesian logistic regression, concretely the one proposed in [8], with Gaussian priors. The implementation used here is the one included with the Weka package, with the default parameters (see [32] for details).

One of the flexibilities of any logistic regression classifier is that it can model distributions with real inputs. As we have two approximations for the input vector in this classifier, we can propose different models, which will be called M1 (corresponding to Eq. (3)) and M2 (for Eq. (4)). It seems reasonable that, if the content classifiers perform well, the model M2 will be more accurate than the model M1. We shall discuss this point afterwards. 3.4.2. Linear support vector machine
As a complement to the previous one, we have added the results obtained by using a linear support vector machine with probabilistic outputs (fitted using Platt X  X  algorithm [19]). Thus, the input to the SVM the real-valued outputs of the linear classifier will be therefore transformed to probabilities with the model learned in the training data by the mentioned algorithm.

In order to show the validity of our approach we have used a linear kernel (i.e., the simplest SVM) without any further modification to the default parameters to those included in the canonical imple-mentation of the Weka [32] SMO (Sequential Minimal Optimization) class. It is well known that linear SVMs tend to perform reasonably well in classification tasks, although they are generally outperformed by their kernelized counterparts. We have selected the simplest model to illustrate that we can improve in some measures the baseline of the binary classifiers without too much effort. 3.5. Results 3.5.1. Experiments with label-based measures
We present the results for Reuters-21578 (in Table 1), Ohsumed-23 (in Table 2) and RCV1 (in Table 3). NB denotes the multinomial naive Bayes, k -NN the k nearest neighbors classifier, and SVM the linear support vector machine with probabilistic outputs. The terms  X  X LR X  or  X  X MO X  refers to the label classifiers used, if any (Bayesian Logistic Regression, and linear Support Vector Machine, respectively). We have used the term SMO to distinguish this label classifier from the SVM in the content classifier.
A classifier + M1 or M2 denotes our proposal with the binary or real inputs presented to the corre-comparison purposes.

We give the micro and macro F 1 values (  X F 1 and MF 1 , respectively), and the percentage of improve-ment (positive or negative) over the baseline, denoted by  X  .

In all the cases our proposal with the M2 version of the algorithm improved at least one of both measures of the baseline (i.e.  X  &gt; 0 ), of which, most of them had the two measures improved. The results were noticeable, even reaching a gain of 72% in the case of the macro F 1 measure with respect to the baseline (which is a remarkable achievement). On the other hand, the M1 version presented not so systematic improvements.

Moreover, we have run statistical significance tests for every couple of classifiers (a basic probabilistic classifier, and our proposal, either with the M1 or the M2 configuration). For the micro measure, a micro sign s-test was performed, and for the macro measure, we chose the macro S-test. Both are presented in [35] and constitute the nowadays standard for comparing this kind of experiments. Nevertheless it true positives and true negatives (as shown in Section 3.2, F 1 only considers true positives). Also, note that the macro S-test does not take into account the amount of improvement, but the number of categories where the measure is improved, leading sometimes to counter-intuitive results (with a non-significant high improvement in macro due to the improvement in only very few categories).

In the test the first system, A , will be the best performing one, in terms of micro or macro F 1 measure, being B the second. The null hypothesis is that A is similar to B . On the other hand, the alternative hypothesis says that A is better than B .

If the p-value is less than 0 . 01 , we show in the tables the sign (resp. ) to denote that our baseline indicate the same fact if the p-value is between 0.01 and 0.05. With the sign  X  we point out no significant difference between the two systems. Table 4 displays a summary of the results, showing the number of times that each one of our models is better, significantly better, worse and significantly worse than the baseline.

Once shown the results, we may state that the methodology presented is useful for improving classi-fication results in a multilabel environment. Concretely, in the presented tables, the following facts can be found:  X  The use of this technique clearly improves the classification results of the baseline. For the macro  X  Comparing both approaches, the model M1 (binarized) performs, in general, worse than the contin- X  In general, we can say that this methodology seems to benefit classification of less populated cate- X  The improvement for the micro measure in the RCV1 corpus were quite small, but it should be noted 3.5.2. Experiments with example-based and ranking measures
We present the results for Reuters-21578 (in Table 5), Ohsumed-23 (in Table 6) and RCV1 (in Table 7). The rest of the notation is similar to the one used in the previous tables. We have added the symbol  X  to the values which are better or equal than the corresponding baseline, and  X  to those which are worse.
The results show different patterns for each one of the measures. One error behaves really good, showing a very good performance in all three collections, improving the baseline both in the M1 and M2 versions for almost all the cases. For the rest of measures, they tend to work well with naive Bayes (perhaps the weakest of the three classifiers). Subset 0/1 loss is improved in all cases in Ohsumed-23 but only in 5 and in 3 cases in Reuters and RCV1, respectively. It seems that, for most of the cases, M2 is better than M1 for this measure, and BLR better than SMO.

Hamming loss shows a good performance for Ohsumed-23, where all the M2 versions of our classifier outperform the baselines. In Reuters, however, only 5 out of 12 cases a performance improvement was shown. This fact is also present in RCV1, where only for naive Bayes and BLR an improvement is obtained. Again, M2 presents a better performance than M1 in general. The fact that the two example-based measures do not show great improvements (and even decrease their performance) can be explained with the fact that our method tends to be designed to produce great increments in macro-measures (i.e. per category) and these measures are averaged for each instance (that is, they are close to the micro-ones), except of the fact that Hamming loss accounts for the number of false positives and negatives, regardless of how many other labels have been predicted well. A summary of these results is shown in Fig. 8, where the number of times that each one of our models is better for each measure than the corresponding baseline is shown. 4. Conclusions and future works
In this paper we have proposed a quite general methodology to better manage multilabel classification problems. It is based on explicitly taking into account the dependences among labels. The proposed method can use as the starting point any set of binary classifiers (one for each label, which are trained from the content of the instances in the training set) able to produce a probabilistic output. This infor-mation is merged in a principled way with the information generated by another set of binary classifiers, which in this case are trained using only the information about the labels assigned to the instances in the training set (capturing the dependences among labels). These label-based classifiers, like the content-based ones, can be built using a variety of learning methods, provided that their output can be interpreted probabilistically.

We have carried out experiments with three well-known multilabel document collections, using three very different content-based baseline classifiers (naive Bayes, k-NN and SVM) and two label-based classifiers (logistic regression and SVM). The experiments confirm that our methodology often tends to improve the results obtained by the baseline classifiers.

The combination of our methods (which so far use a natural threshold of 0.5) with any of the well-known thresholding techniques [34] is an open question. The first and obvious question is: what happens if a thresholding algorithm is applied after our method (instead of just binarizing the final probability output)? At a first glance it should improve the results, but how? Is this combination worthwhile? Also, a more sophisticated variant of the M1 method, with a better thresholding function  X  (using learned values for the thresholds, not just 0.5) can be studied. Or even, a combination of both proposals. As future work we would like to study the synergies between our proposal for improving multilabel classification and some thresholding techniques.

Another open question is the following: using the same approach, a different independence assumption than the one given in Eq. (1) could be given, leading to other models. In particular, we would like to explore methods where the independence assumption is relaxed, because we think that a complete independence may be unrealistic in some categories.

Finally, we would like to use this methodology in a different environment. We have selected document categorization for validating this model because it is a natural form of multilabeled instances. In the future, we would like to work with different datasets as, for example, musical patterns, protein data or social network data, which we think are also suitable for this method.
 Acknowledgement
This study has been jointly supported by the Spanish research programme Consolider Ingenio 2010 and the Consejer X a de Innovaci X n, Ciencia y Empresa de la Junta de Andaluc X a, under the projects MIPRCV (CSD2007-00018) and P09-TIC-4526, respectively.
 References
