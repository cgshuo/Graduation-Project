 We present an algorithm to extract an high-quality approx-imation of the (top-k ) Frequent itemsets (FIs) from ran-dom samples of a transactional dataset. With high prob-ability the approximation is a superset of the FIs, and no itemset with frequency much lower than the threshold is in-cluded in it. The algorithm employs progressive sampling, with a stopping condition based on bounds to the empirical Rademacher average, a key concept from statistical learning theory. The computation of the bounds uses characteris-tic quantities that can be obtained efficiently with a sin-gle scan of the sample. Therefore, evaluating the stopping condition is fast, and does not require an expensive mining of each sample. Our experimental evaluation confirms the practicality of our approach on real datasets, outperforming approaches based on one-shot static sampling.
 H.2.8 [ Database Management ]: Database Applications X  Data mining Algorithms, Theory, Performance, Experimentation Frequent Itemsets; Pattern Mining; Rademacher Averages; Sampling; Statistical Learning Theory
The task of Frequent Itemsets (FIs) mining is to extract all sets of items that appear in at least a fraction  X  of a transac-tional dataset D , or the k most frequent set of items [2]. It is a fundamental primitive of knowledge discovery and is use-ful, among the others, for market basket analysis, inference, classification, and network management [13]. Exact algo-rithms to mine FIs have since long been available but their c  X  practicality is hindered by the need to scan the dataset mul-tiple times [1, 12]. When the dataset is too large to fit into main memory, as it is the case for many modern datasets, the running time of exact FIs mining algorithms may be too high to be practical. A natural way to reduce the depen-dency on the dataset size is to only analyze a small random sample of the dataset that can reside in main memory. The collection of FIs obtained from the sample will be an approx-imation to the exact collection, due to the fact that only a subset of the dataset is analyzed. Approximate collections of FIs are nevertheless acceptable in most cases due to the exploratory nature of the FIs mining step in the knowledge discovery process. There is an intrinsic trade-off between the size of the sample (number of transactions in the sam-ple) and the accuracy of the estimation, but a loose analysis of this trade-off may result in sample sizes much larger than what is needed to obtain and approximation with the desired level of accuracy and confidence. It is therefore necessary, although challenging, to develop algorithms that leverage on tight bounds to the trade-off between sample size and accuracy in order to fully exploit the power of sampling. Contributions. In this work we study the trade-off be-tween approximation quality and sample size using concepts and results from statistical learning theory [29]. We present a randomized algorithm to mine a high-quality approximation of the collection of FIs w.r.t. a minimum frequency thresh-old  X  (and of the top-k most frequent itemsets) from random samples of the dataset D . With probability at least 1  X   X  , for some user-specified  X   X  (0 , 1) , the returned approximation is a superset of the exact collection of FIs and no itemset in the approximation may have frequency less than  X   X   X  , for some user-specified  X   X  (0 , 1) . Moreover, the estima-tion of the frequency of all itemsets in the output is within  X / 2 of their exact value. The algorithm uses progressive sampling , i.e., it starts from a small sample and enlarges it until a suitable stopping condition is verified, meaning that an high-quality approximation can be obtained from the sample. The stopping condition is based on bounds to the empirical Rademacher average of the problem at hand, a key concept from statistical learning theory [5]. In partic-ular we prove that we can bound the empirical Rademacher average and therefore the maximum deviation between the frequency of an itemset in the dataset and the frequency of that itemset in the sample using a function of the sample size, of  X  , and of a partitioning of the set of Closed Itemsets (CIs) [19] in the sample. We also give a bound, which is of independent interest, to the number of CIs in the sam-ple. To our knowledge this is the first algorithm that uses bounds on the empirical Rademacher average in the domain of pattern mining, and one of the first to adapt these highly theoretical concepts to develop an efficient algorithm for an important practical task. We conducted an extensive ex-perimental evaluation to test our algorithm and assess its performances in terms of the quality of the returned col-lection of itemsets and of the runtime, comparing it with standard baselines.
 Outline. We start by reviewing related works in Sect. 2. We then formalize the problem of FIs mining and formally define the concept of approximation in Sect. 1. Our algorithm and its analysis are presented in Sect. 4. The goals, methodology, and results of the experimental evaluation can be found in Sect. 5. Finally, we draw some conclusions and suggest some future directions in Sect. 6.
The idea of using random samples to speed up the extrac-tion of FIs has been studied since shortly after the first effi-cient exact algorithms had been presented [28]. Many works focused on deriving bounds for the size of a single sample to obtain high-quality approximation. Riondato and Upfal [24] present what is currently the best available bound. We refer the interested reader to their extensive discussion of previous results on fixed sample sizes and we focus here on the works that examined progressive sampling [7, 8, 14, 18, 20, 21, 26].
The use of progressive sampling, in contrast with a sam-ple of fixed size, can contribute to an even greater speed up of the extraction of FIs, especially when combined with an appropriate schedule and starting sampling size [3, 14, 21]. Developing a stopping rule that allows to obtain approxi-mations of guaranteed quality is a challenging task. Chen et al. [7], Parthasarathy [18], and Chuang et al. [8] pro-pose progressive-sampling-based algorithms that use heuris-tics based on self-similarity or the frequency of single items to determine the stopping sampling size. Because of the use of heuristics, these approaches offer no guarantee on the quality of the obtained collection. In contrast, our algori-thm returns, with high probability, a collection of itemsets with strong approximation guarantees.

Pietracaprina et al. [20] and Scheffer and Wrobel [26] fo-cuses on extracting the top-k most frequent itemsets us-ing progressive sampling. The stopping condition suggested by Scheffer and Wrobel [26] employs progressive filtering of the set of candidate FIs based on Chernoff bounds until only k itemsets are left, but offers no guarantee on whether the returned collection contains any of the actual top-k FIs, rather a much weaker guarantee is offered. Our algorithm instead guarantees that the returned collection of itemsets is a superset of the top-k FIs. The algorithm by Pietracaprina et al. [20] uses a stopping condition based on the frequency of all itemsets in the sample. The analysis is based on tra-ditional Chernoff and union bounds and limited to itemsets up to a fixed length. Our algorithm does not suffer from this limitation and our analysis uses powerful deviation bounds based on Rademacher averages.

Another major point of difference between our work and the ones previously presented is the fact that checking the stopping condition of our algorithm does not require to run an exact FI mining algorithm on the sample. As a con-sequence, our stopping condition is much more efficient to evaluate, resulting in lower running time.

We use bounds to the Rademacher averages [4, 16], an important concept from statistical learning theory. We only introduce the necessary notation and results, and we refer the reader to the book by Shalev-Shwartz and Ben-David [27] for an in-depth presentation of these topics.
To the best of our knowledge, the only previous use of bounds or estimates of the Rademacher averages in a pro-gressive sampling setting is the work by Elomaa and K X  X ri X i-nen [9] on learning two-level decision trees, whose settings and problem are very different from the ones we study.
Let I be a set of items with an arbitrary fixed total order  X  &lt;  X . A transaction is a subset of I , and a transactional dataset is a collection of transactions. An itemset is a set of items that appear together in a transaction (i.e., a subset of a transaction). Given an itemset A and a transaction  X  s.t. A  X   X  , we say that A appears or is contained in  X  and that  X  contains A . The support set T D ( A ) of A in D is the subset of transactions in D that contain the itemset A , and the frequency of itemset A in dataset D is the fraction of transactions of D that contain A : Given a frequency threshold  X   X  (0 , 1] , the set FI ( D , I , X  ) of Frequent Itemsets (FIs) in D w.r.t.  X  is the collection of all itemsets with frequency at least  X  in D : Similarly, let f ( k ) D be the maximum frequency such that at least k itemsets have frequency at least f ( k ) D in D , then the set of the top-k FIs is Note that | TOPK ( D , I ,k ) | X  k .
 Goal. We aim at approximating the collection of (top-k ) FIs by mining (i.e., extracting the FIs from) random samples of D (i.e., random collections of transactions from D ). Definition 1. For  X , X   X  (0 , 1) , a (  X , X  ) -approximation of FI ( D , I , X  ) is a collection C = { ( A,f A ) : A  X  X  ,f such that, with probability at least 1  X   X  : 1. for any ( A,f D ( A ))  X  FI ( D , I , X  ) there is a pair ( A,f 2. for any ( A,f A )  X  X  , it holds f D ( A )  X   X   X   X  ; and 3. for any ( A,f A )  X  X  , it holds | f D ( A )  X  f A | X   X / 2 . of TOPK ( D , I ,k ) .
We want to compute an (  X , X  ) -approximation to FI ( D , I , X  ) (or to TOPK ( D , I ,k ) ) from random samples of D of progres-sively increasing size (i.e., through progressive sampling ). In the rest of this section we focus on FI ( D , I , X  ) , while the case for top-k FIs is presented in Sect. 4.5.

The basic steps of the iterative progressive sampling pro-cess are: 1. at iteration i , create a random sample S i of some pre-2. check a stopping condition to determine if an (  X , X  ) -3. if the stopping condition is satisfied, return the collec-
In order to obtain an algorithm from this high-level de-scription, it is necessary to formally specify the following components: 1. a sampling schedule ( |S i | ) i  X  1 of sample sizes. 2. a stopping condition involving the sample S i , and an 3. a revised minimum frequency threshold  X  .

Any non-decreasing sequence ( |S i | ) i  X  1 can act as a sample schedule, giving complete freedom to the algorithm designer and the user in this sense. In Sect. 4.6 we show how to compute the next sample size using information from the current sample.

The choice of  X  and of the stopping condition are inter-twined. We choose  X  =  X   X   X / 2 , as motivated by the following lemma, and this choice defines rigorous requirements for the stopping condition.
 Lemma 1. Let S be a sample of D , and consider the event If then the collection FI ( S , I , X   X   X / 2) is a (  X , X  ) -approximation to FI ( D , I , X  ) .

Proof. Assume that the event E S in (1) is verified, which happens by hypothesis with probability at least 1  X   X  . Then hence B  X  FI ( S , I , X   X   X / 2) , as required by property 1 from Def. 1. Let C be any itemset with f D ( C ) &lt;  X   X   X  . We have the condition specified by property 2 from Def. 1. Property 3 from Def. 1 follows from the fact that the event E S is verified.
 This lemma gives the intuition behind the stopping condi-tion of our algorithm: we can stop when (2) holds for the sample S under consideration, as we can then use  X  =  X   X   X / 2 to extract FI ( D , I , X  ) , which is an (  X , X  ) -approximation to FI ( D , I , X  ) .

The rest of this section is devoted to formalize this condi-tion and derive a procedure to check whether (2) holds for a sample S .

Checking whether (2) holds is equivalent to checking whether, with probability at least 1  X   X  , hence we focus on bounding this quantity.
For each itemset A  X  I , we define the indicator function  X 
A : 2 I  X  X  0 , 1 } as: When B is a transaction,  X  A ( B ) = 1 if the itemset A appears in the transaction B . Hence, we have and analogously for the frequency f S ( A ) of A in a sample S .

Assume that the sample S has size |S| = n . For each  X   X  X  , 1  X  i  X  n , let  X  i be a Rademacher random variable, i.e., a random variable taking value  X  1 or 1 , each with prob-ability 1 / 2 . The random variables  X  i are independent. The (sample) conditional Rademacher average is the quantity where E  X  denotes the expectation taken only w.r.t. the ran-dom variables  X  i , 1  X  i  X  n (i.e., conditionally on the sam-ple) [5, 16]. An important result from statistical learning theory bounds the supremum of the deviations with the con-ditional Rademacher average.

Theorem 1 (Thm. 3.2 [5]). With probability at least 1  X   X  ,
Note that the bound in the above theorem depends only on properties of the sample.

Computing R S directly is not easy. It would first require to mine all itemsets from S (i.e., extracting FI ( S , I , 1 / |S| ) , which is excessively expensive, and then to find the expec-tation over the  X  i variables. Given that no analytical meth-ods are currently available to compute this expectation in general, this second step would require an expensive Monte-Carlo simulation [5]. Nevertheless a different result from statistical learning theory allows us to bound R S using com-binatorial properties of the sample. For any itemset A  X  X  , let v S ( A ) be the n -dimensional vector and let V S = { v S ( A ) ,A  X  X } . Since V S is a set rather than a bag , we have | V S | X  2 |I| (and potentially | V S | 2 Theorem 2 (Massart X  X  Lemma, Thm. 3.3 [5]).
 where k X k denotes the Euclidean norm.

Although the above is the form in which the Theorem is usually stated, a careful reading of its proof allows us to state the following stronger version.

Theorem 3. Let w : R +  X  R + be the function Then
Proof. As in the proof for [5, Thm. 3.3] we can use the independence of the  X  i  X  X  and the Hoeffding X  X  inequality to show that, for any s &gt; 0 and for any itemset A  X  X  , we have We can use this inequality to write We can now take the logarithm on both sides and divide by s (which is strictly positive) and we obtain w . Since the above inequalities are true for any s &gt; 0 , we can choose the one that minimizes the r.h.s. to obtain the thesis.
As we show later, computing the function w is too ex-pensive for our purposes as it requires the computation of the set V S , therefore, in the following, we develop an upper bound to w that is easy and fast to compute.
We now show a connection between the set V S and the collection of Closed Itemsets [19].

We recall that a Closed Itemset (CI) is an itemset A  X  X  such that none of its proper supersets has the same fre-quency of A (i.e., there is no B ) A s.t. f S ( B ) = f S Let CI ( S ) be the set of CIs in the sample.

Lemma 2. The set V S contains all and only the vectors v ( A ) for all A  X  CI ( S ) : To prove Lemma 2, we need the following result.

Lemma 3. Let S  X  S . There is at most one CI A in S whose support set in S is T S ( A ) = S .

Proof. Suppose that there could be more than one CI in S with support set S , for example, w.l.o.g., two itemsets C and D . Then the support set of C  X  D in S would be exactly S , so C and D can not be closed, as there is a superset of them with the same support set. We reached a contradiction, so the thesis is true.
 We can now prove Lemma 2.

Proof of Lemma 2. Let A be a CI in S , and let S A be the set of subsets of A with the same frequency in S as A : The elements of S A are the itemsets that appear in all and only the transactions of S where A appears. This means that, for all B  X  S A , v S ( B ) = v S ( A ) . To conclude the proof it is sufficient to show that there can not be two CIs C and D in S s.t. v S ( C ) = v S ( D ) . This is an immediate consequence of Lemma 3 and the proof is complete.

This result explains why computing the function w from (3) is expensive: we would need to extract the set CI ( S ) of all CIs in the sample (i.e., mine the sample at frequency 1 / |S| ). In the following we develop an upper bound to w that can be computed efficiently with a single scan of the sample.
In this section we show how to efficiently bound the con-ditional Rademacher average R S . To do so, we define a function  X  w which is an upper bound to w from (3) in every point of R + . The advantage of  X  w is that it can be com-puted using just the frequencies in the sample of the items in I and some additional information that can be obtained with a single scan of the sample. To define  X  w we need a partitioning of CI ( S ) that we now introduce.

Assume to sort the items in I S in increasing order by their frequency in S , ties broken arbitrarily (e.g., according to the order &lt; on I ). Let &lt; i denote the resulting ordering. Given an item a , assume to sort the transactions of T S ( { a } ) in increasing order by the number of items they contain that come after a in the ordering &lt; i , ties broken arbitrarily (e.g., using unique transaction identifiers). Let &lt; a denote the resulting ordering.

Let C 1 = CI ( S )  X  X  S and C 2+ be the subset of CI ( S ) con-taining only the CIs of size at least two. We partition C as follows. Let A  X  C 2+ and let a  X  A be the item in A that comes before any other item in A according to the order  X  &lt; i  X . Let  X  be transaction containing A that comes before any other transaction containing A in the order  X  &lt; Clearly a  X   X  . We assign A to the set C a, X  .

Consider now a transaction  X   X  T S ( { a } ) , and assume that it contains exactly k a, X  items that come after a in the order-ing &lt; i . In the ordering &lt; a , the transaction  X  comes 1. before all transactions with more than k a, X  items that 2. before zero or more of the transactions with exactly For each r  X  1 , let g a,r be the number of transactions in T ( { a } ) containing exactly r items that come after a in the ordering &lt; i . Let  X  a = max { r : g a,r &gt; 0 } and let The value  X  a is the maximum r such that there exists at least one transaction in T S ( { a } ) containing exactly r items that come after a in the order &lt; i . Each value h a,r is the number of transactions in T S ( { a } ) that contain at least r items that come after a in the order &lt; i .

Now, assume that  X  is the ` a, X  -th transaction in the or-dering &lt; a that contains exactly k a, X  items that come after a in the ordering &lt; i . In other words, if we consider only the transactions containing exactly k a, X  items that come after a in the ordering &lt; i , then  X  is the ` a, X  -th of such transactions in the ordering &lt; a . We have the following result on the size
Lemma 4. We have
Proof. The quantity 2 k a, X  is the number of subsets B of  X  such that B = { a } X  C where C is any non-empty subset of  X  containing only items that come after a in the order &lt; Since C a, X  contains only itemsets that appear in  X  and are in the form of B , then |C a, X  | X  2 k a, X  .

Consider now an itemset A  X  C a, X  . Apart from  X  , A can only appear in transactions  X  0  X  T S ( { a } ) such that  X  &lt; as A = { a } X  C , for C as above. This is true for any itemset A  X  C a, X  . Let T denote the set of such transactions, then |T| = h a,k a, X   X  ` a, X  . From Lemma 3 we have that there is at most one CI for each set D = {  X  } X  F of transactions, From Lemma 4 and the fact that we have the following result on the number of Closed Item-sets in S , which is of independent interest.

Corollary 1. | CI ( S ) | X |I S | + X
The following lemma puts together the above results to obtain an upper bound to R S .

Lemma 5. Let  X  w : R +  X  R + be the function  X  w ( s ) = 1 Then
Proof. Consider the function w from (3). From the definition of Euclidean norm, we have that, for any A  X  I , k v S ( A ) k = p nf S ( A ) . Using this fact and combining Lemma 2 and the equality from (4), we can rewrite w as w ( s ) = 1 We now show that w ( s )  X   X  w ( s ) for any s  X  R + . The thesis will then follow from Thm. 3.
 First of all, since C 1  X  X  S , we have Then, for any a  X  X  S , where we used Lemma 4 to bound the size of C a, X  and the fact that for any A  X  C a, X  , f S ( A )  X  f S ( a ) , given the anti-monotonicity property of the frequency.

Finally, we can rewrite the right-hand side of this last equation as By combining these equations we have that w ( s )  X   X  w ( s ) for any s  X  R + , and the thesis follows from Thm. 3.

We are now ready to formally state our stopping con-dition that guarantees that an (  X , X  ) -approximation can be computed when the condition is satisfied.

Theorem 4 (Stopping condition). Let i be the min-imum index for which it holds that Proof. The proof follows by combining Lemma 1, Thm. 1, Thm. 3, and Lemma 5.
We now discuss how it is possible to check the stopping condition with a single scan of the sample. In particular, it is possible to obtain the expression for  X  w with a single scan, then its minimum of  X  w can be found by computing the value s which minimizes  X  w .
 Computing  X  w . To compute the expression for  X  w we only need the quantities g a,k and h a,r for any a  X  X  S and for all r , 1  X  r  X   X  a . These can be computed with a single scan of the sample. Indeed, the order &lt; i can be obtained from the frequencies of the items in the sample, which we assumed to have been computed during the sample creation. Then, it is sufficient to look at each transaction  X  once, sort its items according to the order &lt; i and, for any item a  X   X  , increment g a,k a, X  by one and increase by one all counters h a,r for 1  X  r  X  k a, X  .
 Minimizing  X  w . The function  X  w has first and second deriva-tives w.r.t. s everywhere in R + and it is convex , so it has a global minimum which can be found efficiently using a non-linear optimization solver like NLopt [15].

Algorithm 1 presents the pseudocode of our progressive sampling algorithm to compute an (  X , X  ) -approximation to FI ( D , I , X  ) . The function random_sample ( D ,m ) returns m transactions sampled at random with replacement from D .
Only minor modifications are needed to obtain an algori-thm for computing (  X , X  ) -approximations to the set of top-k FIs. The main differences from the algorithm presented in the previous section are: 1. a stricter stopping condition; and 2. the need to run an exact mining algorithm on the fi-nal sample twice , one to find the top-k -th highest frequency f
S in the sample and one to extract the approximation at a lowered frequency threshold that depends on f ( k ) S .
Theorem 5. Let i be the minimum index for which it holds that quent itemset in S i . Then FI ( S i , I ,f ( k ) S approximation to TOPK ( D , I ,k ) .

The proof leverages on Thm. 4, following the same steps as the proof for [24, Lemma 5.3].
Algorithm 1: Progressive sampling algorithm input : a dataset D built on alphabet I , parameters output : An (  X , X  ) -approximation to FI ( D , I , X  ) i  X  0 , S 0  X  X  X  , |S 0 | X  0 repeat until  X   X   X / 2 return FI ( S i , I , X   X   X / 2)
Any non-decreasing sequence ( |S i | ) i  X  1 can act as a sam-pling schedule and Provost et al. [21] showed that a geo-metric sampling schedule (i.e., a schedule where S i =  X  i for some constant  X  ) is asymptotically optimal when check-ing the stopping condition takes time O ( |S i | ) . Nevertheless, even such a schedule requires the user to specify two param-eters: an initial sample size S 0 , and a  X  X rowth rate X   X  &gt; 1 .
In our case it is possible to avoid forcing the choices of these parameters to the user, and instead allow the algori-thm to select an initial sample size and then choose succes-sive sample sizes based on the quality of the current sample. This has the net result of removing two parameters from the algorithm.
 possible to choose the initial sample size wisely so that the algorithm does not waste time in creating and analyzing samples that are just too small for the stopping condition to be satisfied (exceeding in the other direction, i.e., hav-ing an initial sample size that is a bit too large, is not a significant issue). In our case it is possible to compute the  X  X ecessary X  initial sample size S  X  0 , i.e., the minimum sample size which makes it possible for the stopping condition to be satisfied. In other words, for sample size smaller than S is deterministically impossible that the stopping condition is satisfied, and therefore it is useless to create and analyze samples smaller than S  X  0 .
 Lemma 6. Let The stopping condition (5) from Thm. 4 can not be satisfied on samples with size smaller than S  X  0 .

Proof. Assume that there exists a sample S of size smaller than S  X  0 for which the stopping condition (5) in Thm. 4 can be satisfied. For such a sample, we have From this and the fact that  X  w ( s )  X  0 , we have that the stop-ping condition can not be satisfied, so we reached a contra-diction and the thesis holds.
 the information obtained from mining the current sample to compute a meaningful sample size for the next iteration.
Assume to be at iteration i  X  0 of the algorithm, and let  X  i be the value of the l.h.s. of (5) at the end of the current iteration, and let |S i | be the size of the sample used in iteration i . Then at the next iteration i + 1 we use a sample of size |S i +1 | , with The intuition behind the above formula is that  X  i is, through Thm. 1, an upper bound to the maximum deviation be-tween the frequency in S i of any itemset and the frequency of that itemset in the original dataset. There is a necessary quadratic dependency between this measure and the sample size [17], hence we can use  X  i and |S i | to compute a sample size |S i +1 | for which, everything else unvaried , the error al-lowed in a sample of that size (i.e., the l.h.s. of (5)) would be at most  X / 2 , as required by the stopping condition of our algorithm.

Although the method we just presented does not give any guarantee on the optimality of the schedule, our experimen-tal evaluation results (Sect. 5) show that is highly effective and results in a faster execution of the algorithm than us-ing a geometric sample schedule, thanks to the fact that intermediate sample sizes that are probably not sufficient for computing an (  X , X  ) -approximation are skipped.
To the best of our knowledge, our algorithm improves over all progressive-sampling approaches previously presented in the literature [7, 8, 14, 18, 20, 21, 26] because it does not require the execution of any expensive Frequent Itemsets mining algorithm on each sample to check the stopping con-dition. Indeed the computation of the stopping condition only requires one scan of the sample. More straightforward stopping conditions with the same requirements are possi-ble: we explored a number of them, both empirically and theoretically, and found them substantially looser (i.e., sat-isfied only at larger sample size) than the one presented in this work. We plan to include a presentation of these al-ternative sub-par stopping conditions, and the comparison of their performances with the one from our algorithm in the extended version of this work. It is indeed necessary to strike a delicate balance between the speed of checking the stopping condition and its strictness (i.e., how early it be-comes satisfied), otherwise the advantages of using sampling rather than analyzing the entire dataset are lost.
As the stopping condition does not depend in any way on  X  , this parameter can be fixed at a later stage. This is again a consequence of the fact that we do not need to run a mining algorithm on the sample to check the stopping condition.

We remark that the dependency on 1 / X  2 of the sample size can not be improved, as shown by Liberty et al. [17].
A variant of the approach presented in previous sections can be used in a static-sampling fashion. Consider the fol-lowing scenario: rather than having access to the entire dataset D and being able to sample from it as much as de-sired, we are given a single random sample S of the dataset of some size n , a fixed parameter  X   X  (0 , 1) , and a mini-mum frequency threshold  X   X  (0 , 1] . The task requires to compute an (  X , X  ) -approximation to FI ( D , I , X  ) for the best possible  X  . No access to the dataset is possible and no other information about the dataset is available. This scenario is realistic and actually common, as it may be easy to create (and maintain) one single random sample of the dataset of a specific size while the dataset is created, while obtaining access to the entire dataset may not be feasible. Previous approaches like those presented by Riondato and Upfal [24] and Chakaravarthy et al. [6] rely on characteristic quantities of the dataset (e.g., the d-index [24], or the longest transac-tion in the dataset [6]) to compute a single sample size that allows to obtain the desired quality guarantees. Computing such quantities require scanning the entire dataset. Not only this may be extremely expensive for modern datasets, but it is not even possible in the scenario we just described. These approaches would then be useless in this scenario, as they have no information on the characteristic quantities they need. On the other hand, our approach only uses sample-dependent quantities (namely, the distribution of the sample frequencies of single items and related quantities), and can therefore compute the best (i.e., smallest)  X  obtainable from the given sample S . Indeed, it follows from Thm. 4 that such value is
Similarly, to approximate TOPK ( D , I ,k ) : from Thm. 5 we get
Even if we relax the scenario and assume that the algori-thm by Riondato and Upfal [24] (currently the best available for static sampling) has knowledge of an upper bound to the d-index of the dataset, the results of our experimental eval-uation (Sect. 5) show that the value for  X  computed by our approach using (8) is consistently (although not always) bet-ter (i.e., smaller) than the one computed by the algorithm in [24].

Riondato et al. [22] presented PARMA, a MapReduce al-gorithm for mining approximate collections of FIs. The variant presented in this section can be used in PARMA to obtain even higher-quality approximation or even smaller samples.
We evaluate the performances of our algorithm by assess-ing the accuracy of the returned collection of FIs and by evaluating the algorithm runtime, comparing it with the time needed to extract the exact collection of FIs and to extract an approximate collection with the same guarantees using the algorithm by Riondato and Upfal [24] (from now on denoted as VC , as it is based on VC-dimension). We choose to compare to this static sampling approach rather than to other existing progressive sampling approaches due to the fact that no existing progressive sampling approach offers the same guarantees of our algorithm. Due to space limitations, we only report here a subset of the results. More results are available in the Appendix of the extended ver-sion [23].
 tation by Grahne and Zhu [11] for the mining step. Our implementation is publicly available at http://cs.brown. edu/~matteo/radeprogrfi.tar.bz2 . We use NLopt [15] to compute the minimum of  X  w for the stopping condition (Thm. 4). We run the experiments on a machine with a quad-core AMD Phenom TM II X4 955 processor and 16GB of RAM, running GNU/Linux 3.2.0. We used datasets from the FIMI X 03 repository ( http://fimi.ua.ac.be/data/ ) [10]. The characteristics of the datasets are reported in Table 1. Each dataset is replicated a number of times (between 200 and 1000) w.r.t. its FIMI X 03 version, so that its size is repre-sentative of modern datasets and the real-life distributions of the frequencies of the itemsets and of the transaction length are preserved. The d-bound d is a quantity used by VC to compute the sample size n as It is, informally, the maximum index d for which the dataset contains at least d different transactions of length at least d [24, Sect. 4.1], and can be computed with a scan of the whole dataset.

In all our experiments we fixed  X  = 0 . 1 . The initial sam-ple sizes are computed according to (6). Except when oth-erwise specified, we used the  X  X utomatic X  sampling schedule described in Sect. 4.6, i.e., we used (7) to compute the size of the sample to analyze at the next iteration. The value run our algorithm five times for each combination of pa-rameters, in order to evaluate fluctuations in accuracy and running time due to the randomized nature of our algori-thm. Unless otherwise specified, the reported quantities are the averages over the runs.
 terms of the recall, precision, and error in frequency estima-tion for the output collection.
 Recall. The first result of our experimental evaluation is that in all the hundreds of runs of our algorithm, for all datasets and combinations of parameters, the returned collection of itemsets always has the three properties from Def. 1, not just with probability 1  X   X  . This holds in par-ticular for the first property (all itemsets in FI ( D , I , X  ) are in the output collection), which means that the recall of our algorithm is always 100%.
 Precision. As for the precision of our algorithm, i.e., the ratio between the size of FI ( D , I , X  ) and the output size, we remark that our algorithm gives no guarantee in this sense, as any itemsets with frequency in [  X   X   X , X  ) may be in the output collection. Hence the precision depends on the dis-tribution of the dataset frequencies in this interval. In our experiments, it varied between 15% and 92%, depending on the parameters and on the dataset. We also measure the fraction of the itemsets with frequencies in [  X   X   X , X  ) that are included in the output. This quantity ranges from 49% to a vanishingly small quantity when  X   X   X  (indeed in this case any itemset appearing in the dataset may be included in the output). We report the behavior of this quantity for various values of  X  in Figure 1, for the connect dataset at line), the number of possible  X  X alse Positives X  (i.e., the num-ber of itemsets with frequencies in [  X   X   X , X  ) in the dataset), the average number of FP in the output collection, and the ratio between this latter quantity to the former (aligned to the right vertical axis). We can see that the number of in-cluded FP grows slower than the number of possible FP, and therefore the ratio goes down. These False Positives are the price to pay when mining a sample of the dataset, and, by setting  X  , the user understands that such False Positives are possible. In any case, our algorithm still returns a rela-tively compact collection of itemsets, rather than including any itemset that could theoretically be included (i.e., all the itemsets with frequencies in [  X   X   X , X  ) ). Indeed the collection can still be used, in all cases, as a set of candidates from which to compute efficiently the exact collection of FIs with a single linear scan of the dataset. The cost of this operation is almost always negligible. We remark once more that no itemset with frequency less than  X   X   X  was ever included in the output nor any itemset with frequency at least  X  was ever missed.
 Frequency Estimation. For every itemset A in the output collection, we measure the absolute frequency error err S | f
S ( A )  X  f D ( A ) | , where S is the last sample analyzed. The third property from Def. 1 requires err S ( A ) to be at most  X / 2 . Figure 2 shows the behavior of err S on the retail dataset, with  X  = 0 . 015 . The behavior for other datasets and combination of parameters was similar and can be found in the extended online version [23]. We can see that the ab-solute error is almost an order of magnitude less than  X / 2 , both for the average and for the maximum error, and that it is very concentrated around the average. These low numbers are not due to the fact that many itemsets in the output col-lection have a low frequency in the dataset: we also measure the relative frequency error, defined as 100 err S ( A ) /f and we report it in Figure 2, aligned to the right vertical axis. As we can see, this quantity was always less than 1.4%. In the future, we plan to develop an algorithm that gives guarantee on the relative frequency error, rather than on the absolute error. absolute freq. error relative freq. error Discussion. The results of the accuracy experiments allow us to state that the algorithm performs in practice even better than what the theory guarantees. This is due to the fact that the theoretical analysis uses upper bounds that are developed for the worst case which almost never corresponds to naturally-arising datasets. The results also suggest that there is room for further improvements in the derivation of these bounds and their use in pattern mining.
 mining algorithm based on progressive can be faster than one based on static sampling as it avoids the need to com-pute (or assume as known) characteristic quantities of the dataset which would require access to the entire dataset, and it can use properties of the sample to stop at smaller sample sizes. We compare the running time of our algori-thm to that of VC , to that of an exact algorithm for mining FI ( D , I , X  ) from the whole dataset [11], and to the running time of our algorithm using a geometric sampling schedule |S | =  X  i |S 0 | for different values of  X  (in these cases, the initial sample size |S 0 | was still computed using (6) as in all our experiments). The results are reported for BMS-POS,  X  = 0 . 015 in Figure 3. Results for other datasets are sim-ilar and are not reported due to space limitations, but can be found in the extended version [23]. From the plot, it is possible to appreciate that our algorithm vastly outper-forms the exact algorithm and also VC . While the first fact should be expected, the latter is due to VC having to scan the dataset in order to compute the d-bound, which can be relative expensive compared to our algorithm which needs no such computation. Moreover, as we discuss later, the sample size computed by VC is in most cases larger than the final sample size used by our algorithm. We also report the running time for our algorithm using a geometric sample parameter  X  . This allows us to evaluate the performances of the  X  X utomatic X  sampling schedule described in Sect. 4.6. We can see that the automatic sampling schedule is more efficient as it allows our algorithm to run faster than with a geometric sample schedule by avoiding the creation and analysis of samples whose size is probably not sufficient for the stopping condition to be satisfied, based on information obtained from the current sample. In almost all the runs of our algorithm, for all combinations of parameters and datasets, our algorithm stops after only two iterations (the only exception (3 iterations) happens for larger values of on the kosarak dataset). This means that the information ob-tained at the minimum reasonable sample size (as computed by (6)) is extremely useful to compute a sufficient sample size using (7). Instead, the runs using the geometric sample schedule stops after a variable number of iterations, which was not possible to predict in advance, and does not behave monotonically, as can be seen from Fig. 3. Hence, the use of the automatic sampling schedule is highly recommended, as it allows faster or comparable execution times and the re-moval of the parameter  X  ), whose impact on the algorithm performances may not be clear a priori to the user.
We also analyze the breakdown of the runtime of our algo-rithm, splitting it between time needed to sample the trans-action, time needed to evaluate the stopping condition, and time needed to perform the mining of the sample after the stopping condition is satisfied. The results are reported for the pumsb_star dataset,  X  = 0 . 32 in Figure 4. We can see that the runtime decreases as  X  grows. This is due to the sampling time and the mining time decreasing because the algorithm stops at smaller samples for larger values of  X  . The fact that the mining time decreases as  X  increases is particu-larly interesting: the lowered frequency threshold  X   X   X / 2 at which the final sample is mined is smaller for larger values of  X  and, on a sample of the same size, it would imply longer mining time than for lower values of  X  . Instead the time saved due to the smaller sample dominates the impact of the lower threshold. It is also clear that at small values of  X  , the sampling time accounts for the majority of the running time. As expected, the sampling time depends quadratically on 1 / X  while the time needed to evaluate the stopping con-dition is almost constant. This suggests that it is indeed important to achieve a delicate balance between the cost of evaluating the stopping condition and the possibility that it is satisfied at smaller sample sizes. This was indeed one of our main guiding principles when designing our stopping condition. runtime (ms) Figure 4: Breakdown of runtime for pumsb_star,  X  = 0 . 32 . variant presented in Sect. 4.8 could outperform VC . We com-pared for a given sample size n , the value for  X  obtained using (8) on a sample of size n , to the value  X  VC obtained using VC for the same sample size, which is where d is the d-bound of the dataset (values in Table 1). In Figure 5 we show the results for the datasets kosarak and accidents. It is possible to see that  X  VC is smaller than the one computed by our method at smaller sample size on the accidents dataset, but the  X  computed using (8) decreases faster as n grows and becomes smaller than  X  VC at larger but reasonable sample sizes. On the other hand, on kosarak our method vastly outperforms VC , with a  X  that is half the one computed by VC . The datasets BMS-POS, pumsb-star, and retail showed results similar to those for kosarak, while the comparison for the dataset connect was similar to that on accidents. Looking at the characteristics of the datasets connect and accidents we noticed that they have a smaller number of items, a smaller d-bound, and more items with very high frequency than the other datasets. Of these characteristics, the last two are intuitively the ones with major impact on the results we see: a low d-bound results in a smaller  X  VC , while high-frequency items will have a high frequency also in the sample, resulting in higher values for w ( s ) , which depends on the items frequencies, and therefore in a higher  X  . We are currently investigating how to improve our stopping condition in these cases.

We remark again that VC requires access to the entire dataset in order to compute d , which makes it unusable in some situation, as mentioned in Sect. 4.8. Moreover, com-puting d , as we showed when presenting the runtime results, can be extremely expensive, and the loss in terms of the ac-curacy parameter  X  may be traded off by the gain in speed. The comparison is therefore a slightly unfair to our meth-ods, given that VC is allowed to obtain crucial information by performing additional computation on the entire dataset. For these reasons, we consider our method more flexible and more powerful than VC .
We present an algorithm for extracting a high-quality ap-proximation of the collection of FIs with probabilistic guar-antees. The algorithm employs progressive sampling with a stopping condition that relies on bounding the conditional Rademacher average of the problem using easy-to-compute characteristic quantities of the sample. The stopping con-dition can therefore be evaluated very efficiently without the need to perform an expensive in-depth mining of the frequent itemsets in the sample at each step. To our knowl-edge this is the first work that uses Rademacher averages in a knowledge discovery setting. The experimental results confirm that the algorithm is extremely successful at stop-ping fast at early iterations, and allows to extract very high-quality approximation of the collection of FIs. Among the possible directions for future work, it would be particularly interesting to better study the trade-off between the compu-tational complexity of the stopping condition and its ability to stop at small sample sizes. We are currently investigat-ing algorithms that give relative/multiplicative approxima-tion guarantees, and extensions of our work to additional significance measures different from frequency [25, 26, 30]. This work was supported by NSF grant IIS-1247581 and NIH grant R01-CA180776. [1] R. Agrawal and R. Srikant. Fast algorithms for mining [2] R. Agrawal, T. Imieli X ski, and A. Swami. Mining asso-[3] B. Gu, B. Liu, F. Hu, and H. Liu. Efficiently determin-[4] P. L. Bartlett, S. Boucheron, and G. Lugosi. Model [5] S. Boucheron, O. Bousquet, and G. Lugosi. Theory [6] V. T. Chakaravarthy, V. Pandit, and Y. Sabharwal. [7] B. Chen, P. Haas, and P. Scheuermann. A new two-[8] K.-T. Chuang, M.-S. Chen, and W.-C. Yang. Progres-[9] T. Elomaa and M. K X  X ri X inen. Progressive Rademacher [10] B. Goethals and M. J. Zaki. Advances in frequent [11] G. Grahne and J. Zhu. Efficiently using prefix-trees in [12] J. Han, J. Pei, and Y. Yin. Mining frequent patterns [13] J. Han, H. Cheng, D. Xin, and X. Yan. Frequent pat-[14] G. H. John and P. Langley. Static versus dynamic sam-[15] S. G. Johnson. The NLopt nonlinear-optimization pack-[16] V. Koltchinskii. Rademacher penalties and structural [17] E. Liberty, M. Mitzenmacher, J. Thaler, and J. Ullman. [18] S. Parthasarathy. Efficient progressive sampling for as-[19] N. Pasquier, Y. Bastide, R. Taouil, and L. Lakhal. Dis-[20] A. Pietracaprina, M. Riondato, E. Upfal, and [21] F. Provost, D. Jensen, and T. Oates. Efficient progres-[22] M. Riondato, J. A. DeBrabant, R. Fonseca, and E. Up-[23] M. Riondato and E. Upfal. Mining frequent itemsets [24] M. Riondato and E. Upfal. Efficient discovery of associ-[25] M. Riondato and F. Vandin. Finding the true frequent [26] T. Scheffer and S. Wrobel. Finding the most interest-[27] S. Shalev-Shwartz and S. Ben-David. Understanding [28] H. Toivonen. Sampling large databases for association [29] V. N. Vapnik. The Nature of Statistical Learning The-[30] G. I. Webb. Discovering significant patterns. Mach.
