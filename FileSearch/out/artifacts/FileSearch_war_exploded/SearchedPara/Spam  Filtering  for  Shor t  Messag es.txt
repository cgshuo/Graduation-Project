 Weconsiderthepr oblem ofcontent-basedspam filtering for short text messages that arise in three contexts: mo-bile (SMS) communication, blog comments, and email sum-mary information such as might be displayed by a low-bandwidth client. Short messages often consist of only a few words, and therefore present a challenge to traditional bag-of-words based spam filters. Using three corpora of short messages and message fields derived from real SMS, blog, and spam messages, we evaluate feature-based and compression-model-based spam filters. We observe that bag-of-words filters can be improved substantially using different features, while compression-model filters perform quite well as-is. We conclude that content filtering for short messages is surprisingly effective.
 H.3.3 [ Information Search and Retrieval ]: Information Search and Retrieval X  information filtering ; H.3.4 [ Infor-mation Search and Retrieval ]: Systems and Software X  performance evaluation (efficiency and effectiveness) Performance, Experimentation, Security, Standardization SMS, blog, spam, email, filtering, classification
While the purpose of IR is to deliver data to the user that meets his or her information need, the purpose of spam is the opposite: to deliver data to the user contrary his or her need. The purpose of a spam filter is to mitigate the effect of spam, thus improving delivery of relevant data to the user. Although public interest and much research has, to date, been directed primarily at the scourge of email spam [12], manyothermediaarenowafflicted.Spamisnowprevalent in instant messages, mobile (SMS) messages, web logs and bulletin boards. Often these messages are very short. All the spam needs to deliver its payload is to communicate a web URL, telephone number, stock market symbol, or some odd word (e.g. Gouranga ) that can be found with a search engine.

We are concerned here with content filtering for short mes-sages and short message fragments that range in length from just a few characters (perhaps one word, such as  X  X K X , or a short phrase) to several hundred characters. Content fil-tering has been shown to be remarkably effective for whole email messages which are typically an order of magnitude larger; our purpose is to examine the transferability of suc-cessful email filtering techniques to very short messages. The fundamental uncertainty underlying this question is X  X re there enough features in short spam messages to distinguish them from non-spam messages? X 
The following sections describe the compilation of three independent corpora consisting of mobile (SMS) messages, blog comments and email message fragments. Experiments are conducted to evaluate the effectiveness of state-of-the-art email spam filters, without modification, on the corpora. Further experiments are conducted to investigate the effect of lexical feature expansion  X  pre-processing the messages to include tokens based on juxtaposed or near-juxtaposed words and characters  X  on feature-based filters.
A number of situations arise in which messages to be fil-tered are short by nature, or their first bytes must be taken to perform filtering. We consider several of such situations: In all these situations, either the messages are short in na-ture, or the part used for filtering is much shorter than the average email message. It is worth considering which of the currently available content-based filtering techniques may transfer from full-length or long messages to short messages, which may include specific slang, new features like short phone numbers, and in which the feature space is intuitively larger and sparser than in longer messages.
The problem of spam filtering may be viewed as text clas-sification, typically modeled as supervised learning task in which a binary classifier is induced on a set of labelled train-ing messages and then used to predict the class of each of a set of unlabeled test messages [25]. Many approaches fur-ther abstract each message to a bag of words or a vector of features derived from the message. Cormack and Lynam [6], and Cormack and Bratko [10] provide a comparative review of available email spam filter methods, published re-sults and evaluation methods. The TREC spam filter eval-uation tracks [8, 5] represent the most comprehensive email spam filter evaluations to date. From these results we ob-serve: The problem of spam classification for short messages has been considered by Healy et al. [15], who compare Knn, SVM and Naive Bayes classifiers on two private corpora con-sisting of SMS messages and hotel comment forms. Messages were represented as bag-of-words augmented by some statis-tical features (e.g. the frequency of upper case letters in the text). They conclude that SVM and Naive Bayes substan-tially outperformed Knn, contrary to their previous results for full email messages. Other published results, in contrast, show SVM and Naive Bayes to outperform Knn on full email messages [7] as well. Mishne and Carmel [20] created a cor-pus of 1024 blog comment messages and use the content of the original blog posting to predict which of the comments is spam. We used these messages to form our blog corpus; while the results may be compared we note that Mishne and Carmel derive their classifier f rom extrinsic data (the blog posting) while we use the comments themselves.

More generally, the problem of short text classification has been considered by Zelikovitz et al. [27] who use ex-trinsic information and transductive learning to bear. Many approaches to spam filtering also use extrinsic information [13]; their consideration is beyond the scope of this research.
Two of us (G  X  omez Hidalgo et al. [11]) previously reported the effects of feature engineering on the application of stan-dard classifiers to SMS messages. Preliminary results of the work detailed here have been the subject of a poster presen-tation [4]. The current presentation adds extensive results and analysis on a substantially enlarged SMS corpus, as well as new email and blog corpora which we make available for comparative evaluation.
We selected for evaluation five spam filtering approaches that compare favourably with others in the literature:
Because many of the messages to be classified are only a few words long, we had reason to believe that feature-based classifiers  X  that is, all except DMC  X  would suffer due to insufficient input. We conjectured that the same sort of patterns harnessed by DMC  X  that is, inter-word and intra-word dependencies  X  might be harnessed by the feature-based methods (see, for example, Sculley [23]). Fur-thermore, since the messages were short, there was little risk of exceeding time or memory limits by an aggressive expan-sion of the number of features.

Prior to our experiments, we chose the following features for our expanded feature set: In evaluating the SVM and LR classifiers, it was a simple matter to substitute the expanded feature set in place of the word-based set. Altering the feature sets for Bogofilter and OSBF-Lua was more problematic, as these filters take text (email messages) as input and we were loathe to modify the filters themselves. In the case of Bogofilter, we could verify using dump utilities that its feature recognition method was word-based as described above. It was therefore a simple manner to generate text as a sequence of nonsense words, with each word corresponding to a feature in our expanded set. These Bogofilter scanned as words and therefore used as features in its learning algorithm.

We were not sure at all whether or not the same technique would be effective for OSBF-Lua, which uses word-based features but also its own OSB features. Nevertheless, we applied OSBF-Lua to exactly the same sequence of nonsense text that was given as input to Bogofilter.

DMC does not use features and therefore it was not eval-uated on the expanded feature set. In total, we applied nine combinations of filter and feature selection to each of the test collections: 1. Bogofilter applied to the raw text so as to use its own 2. Bogofilter applied to synthetic text so as to use the 3. DMC applied to the raw text. 4. Logistic regression applied to the word-based binary 5. Logistic regression applied to the expanded feature set. 6. OSBF-Lua applied to the raw text so as to use its own 7. OSBF-Lua applied to the synthetic text. 8. SVM applied to the word-based binary feature set. 9. SVM applied to the expanded feature set.
In this work, we have used an expanded SMS Spam Test collection from our previous work, an email spam test col-lection from the TREC Spam Filtering Track, and a Blog Comment Spam corpus, all of them described in the follow-ing sections.
We have built a collection of English SMS messages, in-cluding 1002 legitimate messages randomly extracted from the NUS SMS Corpus and the Jon Stevenson Corpus, and 322 SMS spam messages collected from the Grumbletext mo-bile spam site. The average number of words per message is 15.72, and the average length of a word is 4.44 characters long.

This collection is based on one used in a previous work [11], augmented with 290 new spam messages. These col-lections are far bigger than those employed in the related work [15]. This collection is available on request and will be published in due course.
For the Blog Comment Spam experiments, we have used the corpus due to Mishne and Carmel [20](ref) 1 .Thiscor-pus was built by collecting 50 random blog posts, along with the 1024 comments posted to them; all pages containing a mix of spam and ham (legitimate) comments. The duplicate and near-duplicate comments have been removed, being the number of comments per post between 3 and 96. The au-thors have manually classified the comments finding 332 to be legitimate comments, some of them containing links to related pages and some containing no links; and the other 692 comments being link-spam comments. We split the mes-sages in the corpus into three subcollections:
Available at http://ilps.science.uva.nl/Resources/blogspam/.
In these experiments, we have also used the Spam Corpus provided for the TREC 2005 Spam Filtering Track. As the messages in this corpus are in chronological order, and that this order affects filtering performance [7], we extracted ten chronological splits from the corpus. Each split consists of 1500 consecutive messages in the corpus, with the first 1000 used as the training set and the following 500 used as the test set. These splits model real spam filter usage, in which the classifier must be constructed from messages seen before those to be classified.

Instead of using the full text of the messages, and address-ing the scenarios described in the motivation, we have built several test collections by using only selected fields of the messages. The rationale for these choices is to examine the performance of filters on a set of natural prefixes such as might be seen by a router or shown in a quarantine summary or on a low-bandwidth device.

For comparison, we also ran three of the filters  X  those previously configured for TREC evaluation  X  on the full text of the same email messages.
We used the TREC Spam Filter Evaluation Toolkit 2 and the associated evaluation methodology [6]. However, the the toolkit assumes that messages will be evaluated on-line in chronological order, an assumption that cannot be met in our experiments. First, the SMS and Blog collections are undated, so it is impossible to know the correct chrono-logical order. Second, these corpora are quite small  X  of the order of 1000 messages  X  and so it would be difficult to achieve statistically precise results from a single on-line evaluation. Therefore, for these corpora, use the  X  X elayed feedback X  facility of the toolkit to perform 10-fold cross val-idation (cf. [7]). On the other hand, the email collection is chronologically ordered and contains ample (92,100) ex-amples. Yet SVM and LR classifiers require adaptation for use in an on-line setting [14, 24], which we wished to avoid in the interests of a standard comparison. Furthermore, we wished the results to be comparable to those for SMS and Blog messages. So we chose ten different chronological splits of the corpus. Our chronological splits each consisted of 1000 training messages and 500 test messages, with no message appearing in two or more splits. In each split all training messages predated all test messages.

The TREC toolkit performs ROC analysis on the classi-fication results and computes several summary statistics as well as an ROC curve which indicates the tradeoff that may be achieved by trading off false positives against false neg-atives. Although no scalar value completely characterizes
Available at http://plg.uwaterloo.ca/  X trlynam/spamjig/. the performance of a spam filter, due to space constraints we choose one  X  ROC area under the curve (AUC), the pri-mary measure used at TREC. Following TREC, we report 1-AUC as a percentage (i.e. AUC = 0.982 is reported as 1-AUC(%) = 1.8. AUC values with 95% confidence intervals are reported 3 as calculated by the toolkit. Space precludes us from presenting ROC curves for all evaluation runs; se-lected examples are presented to demonstrate general agree-ment between AUC and filter performance throughout its operating range.
Tables 1 through 4 report 1-AUC(%) summary results for all combinations of spam filter, feature engineering and mes-sage collection. (Recall that smaller numbers indicate better performance.) Table 5 shows, for comparison with the state of the art, show the result of running the three on-line fil-ters on the entire text of each message. Figures 1 show 2 the ROC curves corresponding to one column selected from each of these tables.

On the SMS corpus (table 1, figure 1, left panel) us-ing default features, SVM (0.0806) slightly outperforms LR (0.1221) and DMC (0.1488), and substantially outperforms Bogofilter (0.4686) and OSBF-Lua (1.0910). Feature expan-sion dramatically improves Bogofilter (0.0722) and substan-tially improves SVM (0.0502), LR (0.0806) and OSBF-Lua (0.4187), although OSBF-Lua remains uncompetitive. DMC is not subject to feature expansion.

Results on the Blog corpus (table 2; figure 1 right panel) show substantial improvement due to feature expansion for all methods on all subcollections. Except for OSBF-Lua X  X  default configuration, all methods did a better job of classi-fying the header information than the text of the message. This result is surprising as the header information is short, but perhaps structured in such a way that it is more dif-ficult to disguise its  X  X pamminess. X  Most filters performed substantially better on head+text than on either separately, notable exceptions being SVM and LR with expanded fea-tures. One possible explanation is that SVM and LR are discriminative rather than generative filters, and therefore less able to harness heterogeneous indicators of spamminess. Overall, LR is the best performer on the individual fields while Bogofilter is best for the combination.

The TREC corpus (tables 3 and 4; figure 2, left panel) demonstrates once again that feature expansion improves all filters on all subcollections. DMC, however, shows the best performance on the From and Subject fields, as well as both combinations, From+Subject+To and From+Subject+To+ prefix. On the To field, DMC falls to fourth place after OSBF-Lua, SVM and LR. All filters show dramatically bet-ter performance on the combined fields than on the individ-ual fields themselves. It is perhaps not surprising that the 1-AUC numbers are quite high for these very terse header fields; more surprising is that the combination of these fields yields a classifier that outperforms the same method applied to 200 bytes of the message text.

The results for OSBF-Lua and Bogofilter on the full email messages (table 5; figure 2, right panel) are much better than those for the fragments and combination of fragments in our short corpora; more particularly so when compared to their default feature configuration. DMC also shows improved
To the same ridiculously large number of significant figures. performance on the full messages, but insubstantially (and certainly not significantly) so. This observation is consis-tent with the fact that DMC, in its default configuration, uses only the first 2500 bytes of the raw message! Over-all, the results  X  even for the full text of email messages  X  are substantially inferior to those reported elsewhere for a superset of this data [7]. The most likely difference is in training set size; our corpus has a training set size of 1000 messages while the full corpus has over 90,000.
Short messages contain an insufficient number of words to properly support bag of words or word bigram based spam classifiers. Their performance is improved markedy by ex-panding the set of features to include orthogonal sparse word bigrams and also to include character bigrams and trigrams. DMC  X  a compression-model-based classifier  X  does not rely on explicit featurization and performs well on short messages and message fragments. Overall, performance on selected fields or prefixes of messages is quite good; further analy-sis is required to determine to which aspects of the various messages the classifiers are sensitive. We have no reason to believe we have yet found the optimal set of features, or the optimal method of combining the results of multiple classi-fiers on multiple fields of the message. The results of Bratko et al. [3] or Lynam et al. [19] may be useful in this regard.
