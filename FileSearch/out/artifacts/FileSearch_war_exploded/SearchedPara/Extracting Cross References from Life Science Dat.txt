 Scholars in life sciences have to process huge amounts of data in a disciplined and efficient way. These data are spread among thou-sands of databases which overlap in content but differ substantially with respect to interface, formats and data structure. Search en-structured sources but fall short of providing a relevance ranking of the results that reflects the needs of life science scholars. One such need is to acquire insights to cross-references among entities in the databases, whereby search hits with many cross-references are ex-pected to be more informative than those with few cross-references. In this work, we investigate to what extend this expectation holds. We propose BioXRef , a method that extracts cross-references from multiple life science databases by combining targeted crawling, pointer chasing, sampling and information extraction. We study the retrieval quality of our method and the relationship between manually crafted relevance ranking and relevance ranking based on cross-references, and report on first, promising results. H.2 [ Database Management ]: Data Mining; J.3 [ LIFE AND MEDICAL SCIENCES ]: Biology and genetics Theorie Ranking, search engines, outgoing cross references, life sciences
As David Roos remarks  X  X e are swimming in a rapidly rising sea of data X  and there is no way to  X  X eep from drowning X  [ ? ]. For-tunately, this pessimistic view has not become reality; people are comfortably using search engines to acquire speedily and precisely the information they need. However, general-purpose search en-gine solutions do not transfer to domain-specific search as in the life sciences, where not only the appearance of keywords matters but also the objects referenced in a document. We propose the search method BioXRef that assists search in life science databases by ranking search hits on the basis of the objects they reference.
Search in life science databases exhibits some fundamental dif-ferences from conventional search. First, links play a central role: all articles on this entity -though articles just mentioning this en-tity may be irrelevant. Second, life science databases are orga-nized in different ways  X  usually around specific entity types (e.g. metabolomics). It is rather difficult to collect information on an entity if the database is organized across entities of a different type.
Third, and very importantly, the exploitation of links across enti-ties in different databases is impeded by the lack of an agreed-upon convention for referencing data records.

For this reason scholars cannot take full advantage of cross ref-erences when they retrieve information on some entity (enzyme, gene, protein etc). Systems like GoPubMed [ ? ] or ProMiner [ ? ] use text mining methods to extract entities and other forms of rel-evant information (including types of interaction among entities, like "activation") in specific databases. Search engines like "Essie" [ ? ] rank search results on the relevance of specific database fields and word combinations but these must have been defined in ad-vance. Weighting and ranking search results on the amount (or importance) of cross references in each result is still an open issue.
In this study, we propose BioXRef , an algorithm that extracts cross references from multiple life science databases and thus sets the basis for an enhanced relevance ranking in search engines over biomedical data. Our method is backing the search engine LAILAPS [ ? ] by taking a set of database entries as input, analyzing the entries and their attributes and identifying potential cross refer-ences in the same and in other databases. We describe BioXRef in section ?? after discussion related work in section ?? . In section ?? , we evaluate BioXRef on the search hits for a specific query, re-port on precision and recall, and concrete references retrieved for randomly selected hits. The last section concludes the study with a brief summary and outlook. "Finding information in the WWW is not much of a challenge. Just head for Google or Entrez and get the related web page or database entry." This is being said among biologists who search in-formation about a certain object [ ? ]. However, issues like finding reliable information about the function of a protein, or identifying the protein that is involved in a certain activity of the cell cycle, are much more challenging tasks. One has to consider around 1,100 life science databases and billions of database records [ ? ] ods developed to rank natural language text or WWW-sites are not performing well on life science content and databases [ ? ]. For ex-ample, the Google top-hit for " arginase " is a Wikipedia page.
Scholars of different disciplines apply different criteria to decide about the relevance of a database entry to a scientific task; a dentist has other relevance criteria than a plant biologist. Criteria like who published in which journal, for which organism, evidence scores, surrounding keywords etc. are of major importance and are used for query refinement. For some disciplines there exist even complete search guides, e.g. for dentists [ ? ].

Some search algorithms over scientific databases use Term Fre-quency -Inverse Document Frequency (TF-IDF) for ranking. Apache-Lucene 2 is a popular implementation of this concept and is fre-quently used in bioinformatics as in LuceGene from the GMOD project [ ? ] which is used for the EBI  X  X oogle X  like search frontend EBeye. The TF-IDF approach works well but misses the semantic context between the database entries and the query. Another ap-proach is probabilistic relevancy ranking [ ? ] which shows promis-ing results when combined to a user feedback system [ ? ].
After choosing a ranking algorithm for a search engine, the next task is to define possible ranking criteria. Conventional search en-gines use several ranking criteria. Andrade and Silva consider the similarity between the result entry and the search query itself as a top-ranking criterion [ ? ]. The importance of linkage in ranking has been put forward by PageRank, its variations and ranking exten-sions [ ? ] which now constitute a mature field.

In their work [ ? ] Lars J. Jansen et al. not only show the im-portance of text retrieval and information retrieval methods in life sciences, they also point out that such methods can be used for cre-ating novel hypotheses by combining information of several papers. By further elaborating this thought we can suppose that combina-tions and linkages of several papers, websites or database entries are important.

Another approach is coming from Dennis Quan. He reinforces the statement that the retrieval of relevant information is the most important aspect of research, also in life sciences, and that infor-mation is often spread across multiple systems. Moreover Quan also pointed out that there are differences in  X  X ata formats, naming schemes and network protocols X  [ ? ]. For solving this problem he created a drug discovery dashboard called BioDash [ ? ]. This ap-proach mainly uses the linkage between different aspects and prop-erties. Thus it is likely that references between entries could also be found by some algorithm.
 There already are several approaches for solving this problem. Up to now cross references were detected by just picking the at-tribute named  X  X ross references X  or equals. In our context this is not possible, because the knowledge of semantics of particular database attributes, as known from Semantic Web approaches like RDF, is not a-priory given. An intuitive approach is to manually maintain a list of cross referencing attributes like BridgeDB This link-database is a valuable asset but such an effort does not scale well to the number of databases to be linked. Hence, it leads to the question whether an automated approach is a feasible alterna-tive of complement . BioXRef uses BridgeDB to identify links and use regular expressions for different databases.
A list of life science databases and search engines can be found in http://en.wikipedia.org/wiki/List_of_biological_databases http://lucene.apache.org http://www.bridgedb.org/
The core idea of our method BioXRef is that a document returned by a search for a specific entity (enzyme, protein, species etc.) is more informative if it contains many cross-references to other en-tities. Whilst the nature of each such cross-reference cannot be efficiently investigated by a search engine, the number of cross-references can be used as informativeness indicator when ranking the search results. Hence BioXRef scans each database entry to identify attributes that are potentially containing cross-references, uses sampling and thresholding to extract the most promising can-didates and then detects the identifiers (IDs) of referenced entities within these attributes. Thus, BioXRef associates each database with a lookup table of cross references.

BioXRef is part of the LAILAPS [ ? ] framework which we de-scribe briefly below, before presenting the components of BioXRef in subsection ?? .
The intention of the LAILAPS search engine is to find relevant data in non-integrated life science databases. LAILAPS loads and stores life science databases in an entity-attribute-value (EAV) [ ? ] adapted database schema and computes an inverse text index using Apache-Lucene. In the publicly available systems (see Table ?? ), we provide protein synonyms extracted from UNIPROT/SWISSPROT [ ? ] and a list of 46 keywords that are used as one feature of the fea-ture model depicted in Table ?? .
 Search Portal Records Data Domain UniProt 1,215,553 protein annotation
IPK Databases [ ? ]
DAWIS-M.D. [ ? ]
The features shown on Table ?? reflect the aspects deemed im-portant in general for search in life sciences. However, LAILAPS is a personalized search engine: a user may provide further synonyms and relevance-influencing keywords and LAILAPS trains a person-alized neural network on the relevance ratings of each user for a training set of database entries (won with user-specific queries). For each entry we presented the 9-dimensional feature vectors to the user as input and the manually curated relevance scores consti-tute the network X  X  output layer [ ? ].

To check whether the indegree and outdegree of a database en-try are appropriate as additional relevance features, we detect links from one entry to others and incorporate them to the ranking system of the search engine with BioXRef . We show that this increases the performance in relevance prediction.
BioXRef is depicted in Algorithm ?? . It takes as input the LAILAPS database L ( L actually consists of several databases), two sets of regular expressions R 1 and R 2 , and a sampling rate S . R 1 is the set of regular expressions for the databases from which we col-lect attributes; R 2 contains regular expressions for a larger set of databases, in which we search for cross references. The specifi-cation of these databases and the parameter S are described later. Algorithm 1: BioXRef algorithm Data : A set of entries from several databases, L A set of n regular expressions used for attribute search, R 1 A set of m regular expressions used for cross reference detection, R 2 1 begin 2 T =  X  ; attributeList =  X  ; 3 for (each database D in L ) do 5 add to attributeList a new Map of attributes of D ; 6 for (each entry e  X  D and D  X  L ) do 8 attrList = attributeList .getValue( database ); 9 for (each attribute a  X  attrList ) do 11 IDList := getCrossReferences( aContent,R 2 ); 12 for (each element x  X  IDList ) do if ( x 6 X  T ) then add ( x,T ) ; feature description
F 1 : attribute in which attribute the query term was found
F 2 : database in which database the found entry is included
F 3 : fre-quency
F 4 : cooccur-rence
F 5 : keyword gives information, if good or bad keyword are
F 6 : organism to which organism the database entry relates to
F 7 : sequence length
F 8 : text posi-tion
F 9 : synonym gives information if the hit was produced by an BioXRef returns a lookup table T : for each input database entry, T contains the identifiers of potential cross references.

For each database entry from L , BioXRef first extracts attributes from the database D where the entry came from (Line 4). The at-tributes expected to deliver many cross references are identified and considered further (Line 5, Lines 7ff). The list of cross references contributed by each attribute (Line 11) are placed in a lookup table (Line 12). The identification of attributes and the counting of ex-pected cross references is based on regular expressions which we have collected and used heuristically, as explained in sequel.
When working with BridgeDB 4 , we identified a list containing names of databases and their URLs, regular expressions for the formation of IDs and similar useful data. Such a regular expres-sion can be used for pattern matching inside a database or across http://www.bridgedb.org/ databases. The idea is that if there is a match within an attribute of a search hit (a single entry returned by the search engine) then this match is likely to be a cross reference to another database entry. We have built a list of 184 regular expressions.
In life sciences, we must deal with the interplay between re-call and execution time. Ideally, all cross references should be found. However, searching with 184 regular expressions incurs a high overhead. Hence, we used KEGG "Link-only Databases" as basis, and identified databases with at least one million links. The output set consisted of DB attr = { EMBL, GenBank, RefSeq, Uniprot, EGenes } ; we have set the input parameter R 1 to the set of regular expressions for these five databases. BioXRef (Algorithm ?? : Line 3) collects candidate attributes from these databases by invoking Algorithm ?? : getAttributesFromSource().
 Algorithm 2: getAttributesFromSource Data : A database D A set of n regular expressions, R 1 Sample size S used to compute a sample from D A threshold that each attribute has to reach, threshold
Result : A list of attributes whose amount of cross references 1 begin 2 ResultList =  X  ; D 0 = sample of D having size S 3 for (each attribute a of D ) do 4 aID = primary key of a ; CRef = 0 5 for ( regexp  X  R ) do 6 if ( matches ( regexp,a ) then CRef++ 7 x.attributeID = aID 8 x.crossRefereneAmount = CRef 9 if (x.crossRefereneAmount &gt; threshold ) then 10 add ( x , ResultList ) 11 return ResultList http://www.genome.jp/dbget/linkdb.html
This subroutine identifies all attributes of each database (which contributes entries to our input database entries) and uses the set of regular expressions R 1 to locate and count potential cross refer-ences for each attribute (Lines 5-8). If this count exceeds a thresh-old, the attribute and its anticipated number of cross references is added to the output (Lines 9-10). BioXRef uses the attributes and their indicatory counts to collect cross references from an expanded set of databases (cf. Algorithm ?? : Line 9), as explained next.
We have widened the search scope by identifying those KEGG databases with which the initial set DB attr is linked. We came up with a set DB cross of 18 databases. For this superset of DB we specified the set of regular expressions R 2 (cf. Table ?? ) which we use for cross reference detection.

The matching of identifiers from the detection of cross references is undertaken by Algorithm ?? : getCrossReferences(). Matching is possible per se, because the necessary regular expressions have already been identified (originally from BridgeDB) and associated with different databases.
 Algorithm 3: getCrossReferences Data : string ST representing the content of current attribute, A set R 2 of m regular expressions.

Result : A list containing the regular expression and the 1 begin 2 IDList =  X  3 substringArray = components of ST , split at 4 for (each regexp  X  R 2 ) do 5 for (each s  X  substringArray ) do 6 if ( matches ( regexp,s ) then 7 crossReferenceArray = [ regexp,s ] 8 add ( crossReferenceArray , IDList ) 9 return IDList ;
The complexity of detecting attributes with many cross refer-ences is O ( n 2 ) , where n is the number of database entries. To keep execution time low, BioXRef samples over the input set of database entries D with sampling rate S , ans applies a lower boundary  X  on the number of expected cross references that an attribute must have to be further considered.

The sampling parameter S is used as follows. We process the database entries in D to extract a first set of attributes (Line 3 of BioXRef ), on the basis of which we collect cross references (Line 9 of BioXRef ). Since the entries in D have some commonalities (in semantics and in the databases they appear in), how many en-tries from D should we process (in Line 4) to identify all relevant cross references to the entries in D ? (in Line 11). We have exper-imented with different percentages S over D . The results are on Table ?? : we see that a small sample suffices to detect all 11 rel-evant attributes, although the number of processed attributes drops with the sample size. Even with 0.001 of the original set we iden-tify 10 out of 11 attributes, hence we chose S = 0 . 01 .
Finally, we applied a threshold  X  , on the number of potential cross references that an attribute from the sample must have to be considered further (Line 9 of BioXRef ). To compute this  X  , we use Table 4: Impact of sample size (percentage of D ) on the number of processed attributes and relevant attributes
Sample size Processed attributes Relevant attributes 0.1 49 11 0.01 39 11 0.001 25 10 the number of entries in the database D , the sample percentage S over D , the number of distinct attributes appearing in D (let A this set), and the cardinality of the set R 1 of regular expressions: where c is a tuning parameter that helps personalizing the thresh-LAILAPS as too small or too large for her personal interests.
The reader may have noticed that the number of regular expres-sions is small. This is not always feasible. An altenrnative is to use a seed of identifiers instead of a seed of databases without associ-ating these identifiers to a certain database. Then only the number of found foreign IDs is stored in a lookup table. Hence for a search query only one SQL-query is needed per result entry to figure the number of cross references that should be considered for ranking.
We evaluated BioXRef for the detection and collection of cross references on a test set that contained result entries for the query  X  X arotene 7,8-desaturase X . We have chosen this query, because the search hits for it have already been inspected by users who provided relevance feedback. With their feedback, we organized the entries in the search output into three classes on "relevance": "high", "mid-dle", "low". For creating our lookup table containing detected cross references we implemented a JAVA program and used an Oracle database as storage backend. Our test database D had 15,017,563 entries and | A D | = 63 attributes. We considered 5 regular expres-sions and set S = 0 . 01 . For threshold  X  , we run preliminary tests to reach a proper number of attributes and set c = 0 . 02 .
The most important aspect of rating an algorithm is its predic-tive performance. We selected three result entries of our test set randomly and inspected them to compute the true positive (TP), false positive (FP) and false negative (FN) values. In figure ?? we illustrate the evaluation of UniProt entry 6B1_AGRT4.

In Table ?? we show the absolute TP, FP and FN values together with the derived precision and recall. Best results are achieved for 6B1_AGRT4 where both precision and recall were more than 75%, while the results for ANGL5_HUMAN are comparably good. For AGRP_HUMAN both measures return lower values, although still higher than 50%. We found that some false positives were caused by regular expressions that matched publication year or mol weight. Since both pieces of information are of quite simple nature, such false positives can be easily dealt with.

Some regular expressions are similar, resulting in the same hit (positive) or miss (negative) being returned and counted more than once. We have permitted this, since it is not a priori clear whether such duplicates are more likely to be true or false positives.
We next checked for an association between the number of cross references an entry has, i.e. its outdegree, and the relevance of the entry as perceived by the human users. To this purpose, we used Table 5: Performance results for 3 randomly selected records Entry name TP FP FN Precision Recall
ANGL5_HUMAN 41 22 17 0.65 0.71 6B1_AGRT4 13 4 4 0.76 0.76
AGRP_HUMAN 54 40 33 0.57 0.62 a manually ranked set of 63 entries: they consist of search query results, ranking positions and relevance class specified by the user. Figure 2: Relationship between outdegree of database entries (y-axis) and relevance class
In the boxplot of Figure ?? , we juxtapose relevance class and outdegrees of the entries. The black bar in the middle of each box in Figure ?? presents the median of the class. The boxes represent the inner 50% of the data of the relevance class. As one can see, the medians  X  x of each relevance class have very similar values for the number of cross references. Except of the outlier in class "high", the outdegrees of the entries in the three classes show very similar distribution (cf. median, minimum, maximum values In Figure ?? ).
Figure ?? indicates that the number of references emanating from a database entry does not influence the relevance class of the entry. However, one must take into account that the sample contains only 63 entries, all of which were relevant to some extent; a random selection of entries might have shown a different distribution of outdegrees among the relevance classes. Second, prior knowledge about the number of cross references might have influenced the perception of the evaluators on the relevance of the entries. Fur-ther, the importance associated with some cross reference depends also on the user X  X  expertise. Hence, we plan to study whether user feedback on relevance changes when cross references are used for scoring or not.

BioXRef is intended for a search engine (LAILAPS), so it is proper to investigate performance limitations. Finally we consider the execution time of BioXRef . Algorithm ?? contains a loop per database and one per entry but both loops can benefit from multi-threaded computation. On the other hand, searching for cross ref-erences of each entry in each database may be less efficient than crawling each database once for all entries. Hence, we intend to run experiments that test the efficiency of the method when the number of databases to be accessed per entry is changed when the num-ber of cross references per entry in each database varies and as the size of the input dataset D increases. Such an experiment requires synthetic data, i.e. an appropriate data generator, which also takes the response time of individual databases into account. This was beyond the scope of this first proof-of-concept study.
We have presented BioXRef , a method that collects cross ref-erences of biomedical database objects with the intention to rank search hits with the number of cross references per hit. Our algo-rithm combines automated data crawling and information extrac-tion without human intervention, with knowledge from existing life science databases, knowledge about links and regular expressions.
We have tested BioXRef on a test set of search query results that were manually categorized on relevance. The first results show that the algorithm performs well, achieving reasonable recall and pre-cision. However, more tests are needed to shed light on the im-pact of different object types (e.g. protein, gene, enzyme) that are stored differently in each database. A thorough performance evalu-ation with proper synthetic data is also planed for future work, most likely including the development of an appropriate data generator.
Future issues include alternative realizations, namely as part of a search engine, whereupon multi-threading looks promising, and as offline back-end that collects cross references for future ranking, whereupon bundling of accesses to each database may be needed. Finally, we want to investigate the correlation between cross-refe-rence-based ranking and perceived relevance of each search result, by providing the new type of ranking as alternative to conventional ranking in an experiment with several users.
