 The skyline operator returns records in a dataset that pro-vide optimal trade-offs of multiple dimensions. It is an ex-pensive operator whose query performance can greatly bene-fit from materialization. However, a skyline can be executed over any subspace of dimensions, and the materialization of all subspace skylines, called the skycube, dramatically mul-tiplies data size. Existing methods for skycube compression sacrifice too much query performance; so, we present a novel hashing-and bitstring-based compressed data structure that supports orders of magnitude faster query performance. H.2.4 [ Database Management ]: Systems X  query process-ing skycube;compression;hashmap;data structure
The skyline operator [1] selects from a database all tu-ples that are not clearly less interesting than any others. For example, Table 1 lists some top movies from IMDB. Whether one is interested in movies that are newer, higher-rated, or higher-grossing, or any combination of these at-tributes, Titanic is still less interesting than Avatar : the latter has higher values on every attribute than the former. By contrast, The Shawshank Redemption is older and lower-grossing than Avatar , but still interesting for its high rating.
The skyline includes all data points that are strictly higher on at least one attribute or equal on every attribute, when compared to all other points (like The Shawshank Redemp-tion but not Titanic ). These are the most interesting points. Subspace skylines Often, it is advantageous for a user to pose a skyline query on only the few attributes that are rel-evant to him/her: a typical moviegoer is unconcerned with a movie X  X  sales figures, so is better served by the skyline on just the year and rating attributes. On the other hand, a studio accountant may have a very different perspective.
A subspace skyline query [6,9,10] returns the skyline com-puted over a subset of attributes specified by the user, per-sonalizing the result. However, it is nearly as expensive to compute skylines in arbitrary subspaces as the full dimen-sionality, a cost amplified when users pose a series of queries in different subspaces (such as in exploratory scenarios [2]). Skycube To offer the best possible response time for a subspace skyline query, one solution is to precompute the answer. To do so for every possible subspace skyline is to construct the skycube [6,11], a set of 2 d  X  1 subspace skylines. However, storage of the skycube is quite large. Although compressed skycube data structures exist [8, 10], query per-formance on the state-of-the-art structure is inadequate.
Therefore, we introduce Hashcube to compress a skycube with bitstrings and hash maps. It achieves an order of mag-nitude compression over the default structure, while provid-ing query performance 1000  X  faster than state-of-the-art.
We assume a table P of n records, each described by d ordinal attributes. We denote the i  X  X h record by p the j  X  X h attribute of p i by p i [ j ]. Our approach is based on bitstrings (fixed-length sequences of binary values). 1 We de-note the j  X  X h bit of a bitstring B i by B i [ j ] and the substring of B i from bit j to k , inclusive, by B i [ j,k ]. Additionally, a subspace s is represented by a bitstring of length d , where s [ i ] = 1 iff the subspace includes the i  X  X h dimension.
In this paper, we propose a compact data structure to rapidly answer skyline queries [1] over arbitrary subsets of attributes, which relies on a notion called dominance [1]: Definition 1 (subspace dominance ( p,q,s )) . Given points p,q  X  P and a bitstring s of length d , let EQ , GT also be
Bitstrings and integers here mean both an integer value and the bitstring representing that value (e.g., 7 and 1110). Table 2: Table 1 movies and their domspaces vector (using subspace order  X  Y,R,YR,S,YS,RS,YRS  X  and big-endian). bitstrings of length d , where: Then, p dominates q in subspace s , denoted p s q , iff:
If all data values are unique, known as Distinct Value Con-dition [7], EQ fades from Definition 1. A subspace skyline [6] is the subset of points not dominated in the subspace: Definition 2 (subspace skyline ( P,s )) . Given set of records P and a bitstring s of length d , the subspace skyline of P is:
If s = 2 d  X  1, Definition 2 produces the full skyline . The skycube [6, 11] is the set of subspace skylines (each called a cuboid [11]) for all non-zero bitstrings of length d .
Finally, we define for our data structure a mapping be-tween points and subspace skylines (examples in Table 2): Definition 3 (domspaces vector of p i ) . Point p i  X  X  domspaces vector , denoted DOM i , is a bitstring of length 2 d  X  1 where:
In other words, a domspaces vector records the subspaces in which point p i is dominated ( not in the skyline).
The objective in this paper is to store a compact rep-resentation of all cuboids that can support more efficient subspace skyline queries than state-of-the-art algorithms. Skycube algorithms B  X  orzs  X  onyi et al. [1] introduced the skyline with external-memory algorithms block-nested-loops (BNL) and divide-and-conquer (DC). Then, Sort-First Sky-line (SFS) [3] improves BNL, pre-sorting the data so points will first be compared to those more likely to dominate them. Object-based Space Partitioning (OSP) [12] improves DC by recursively partitioning points based on existing sky-line points, rather than a grid. BSkyTree [4] improves OSP by optimally choosing the points with which to partition P .
The skycube was introduced independently by Yuan et al. [11] and Pei et al. [6], with adaptations of the DC [1] and SFS [3] skyline algorithms, respectively. More recently, QSkyCube [5] adapted the BSkyTree algorithm [4]. These algorithms compute cuboids one-by-one, using the corre-sponding skyline algorithm. Based on results reported in [4, 5], BSkyTree and QSkyCube are state-of-the-art. Skycube data structures The default skycube struc-ture is the lattice , used in QSkyCube [5]. It is an array of 2  X  1 vectors, and the i  X  X h vector contains all points in the i  X  X h cuboid. Naturally, this has optimal performance: one retrieves the proper vector from the array and then reports all points lying therein. However, it is maximal in terms of Figure 1: The Hashcube, built from Table 2 with | w i | = 4. space: each point is duplicated for every cuboid it is in, times for points with maximal values on some attribute.
Two smaller data structures have been proposed. The closed skycube [8] defines equivalence classes over subspaces and avoids duplicating points within an equivalence class.
The more recent compressed skycube [10] defines minimal subspaces skyline points and constructs a bipartite member-ship graph between points and minimum subspaces. Thus a point is not duplicated for any subspaces between its mini-mal subspaces and the full skyline. In the absence of Distinct Value Condition, it introduces overhead to rederive any par-ticular cuboid, because false positives must be verified with dominance tests in all subspaces of the query subspace.
Here, we introduce the Hashcube, obtaining up to | w | -fold compression (for | w | , the number of bits in each logical word) and state-of-the-art query performance. We illustrate a Hashcube in Figure 1, using the data from Tables 1 and 2. The high-level idea is to split the domspaces vectors for each point into words of length | w | (4 in examples, 32 in experiments), and to index the points by their resultant substrings using hash maps. Since the domspaces vector has length 2 d , each point will be indexed  X  2 d  X  lg | w | times. The substrings are the keys for the hash maps. More precisely, if containing at least one zero, and k = max(1 , 2 d  X  lg | w | Definition 4 (Hashcube ( P )) . A Hashcube on P is a set of k hash maps, h 0 ,...,h k  X  1 , each mapping from valid bitstrings in  X  to subsets of P , h j :  X   X  X  ( P ), where:
That is, each hash map corresponds to a group of | w | cuboids. Points are binned according to the combination of those cuboids in which they appear. For example, in Fig-ure 1, w 1 corresponds to subspaces { Year,Sales } , { Year, Rat-ing } , and { Year,Rating,Sales } , respectively. Both Avatar and The Avengers are binned to 0, since they appear in all three cuboids. Although The Godfather appears in the last two cuboids, it does not appear in { Year,Sales } : it has a dif-ferent combination, namely 1, and maps to that bin instead.
Compression for a Hashcube depends on the number of clear bits in the substring of a domspaces vector, up to | w | . Note, first, that a point is only ever indexed by a hash map if it has a zero bit, i.e., if it appears it at least one of the Algorithm 1 Querying the Hashcube Input : Hashcube; query subspace , B ; word length , | w | Output : The skyline of subspace B 1: Let j = B/ | w | 2: Let mask = (1 ( B % | w | )) 3: for all active hash keys k i of h j do 4: if ( k i &amp; mask ) == 0 then 5: Output all pid in h j ( k i ) corresponding | w | cuboids. If so, it must also be indexed for that cuboid by the lattice. Conversely, a point is only indexed once by each hash map, no matter how many of the | w | cuboids in which it appears; the lattice may index the point | w | times. Further compression comes by not storing unused hash keys and by points mapping to identical bins.
Notice from Definition 3 that the j  X  X h cuboid consists of all points p i for which DOM i [ j ] = 0. So, for the Hashcube, the query operation is to concatenate all vectors of point ids for which that bit is not set. Because Definition 4 treats each group of | w | bits/cuboids independently of the rest, the query can be resolved with just one of the 2 d  X  lg | w | maps. Algorithm 1 describes the query operation: first the relevant hash map is determined, and then all  X  2 | w | active hash keys for that hash map are iterated. For those that have the relevant bit clear, the entire vector of point ids is output. No point will be output twice, because each point id is stored at most once per hash map. The iteration of active hash keys is the primary source of overhead relative to the lattice, a cost of at most 2  X  2 | w | binary/logical operations. The cost of querying the data structure is also very low. Lines 1  X  2 require constant computations. We then read up to 2 | w | active hash keys, perform two operations, and (possibly) output some unique point ids (if the condition on Line 4 evaluates true). So, if there are m point ids to output, then the cost of querying the Hashcube is O (2 | w | + m ).
We compare Hashcube ( | w | = 32) to the compressed sky-cube (CSC) [10], the lattice, and computation from scratch using the BSkyTree [4] skyline algorithm. (Note that larger | w | improves compression; smaller | w | improves query time.) We implement (code available 3 ) the data structures and query algorithms in C++ . 4 The lattice is built as an array of vectors of point ids. CSC is strongly implemented, evi-denced by the faster performance than reported in [10] (al-beit on newer hardware). The implementation of BSkyTree was provided by the authors, but adapted to handle sub-space queries. We use an Intel Core i7-2700 machine with four 3.4 GHz cores and 16 GB of memory, running Linux (kernel version 3.5.0).

We evaluate the data structures in terms of space and query time. We measure space by counting 32-bit point ids and hash keys used, a more robust measure than physical disk space because of external libraries (e.g., std::map ).
The Hashcube also requires outputting up to 2 | w | X  1 sepa-rate lists, rather than one, long contiguous one. source at: http://cs.au.dk/research/research-areas/ data-intensive-systems/repository/ . compiled using g++ (4.7.2) with the -O3 optimization flag
We measure query time by dividing the total time to se-quentially query every subspace by 2 d  X  1. In contrast to uni-formly sampling subspaces with replacement as in [10], this better estimates expected performance: the worst cases (( d -1)-dimensional subspaces) are otherwise unlikely included. Output to an array in memory, but not init time, is included.
We evaluate how the structures scale with respect to both d and | P | on anti/correlated data, generated as in [1]. We adopt default values, d = 12 and | P | = 500K, from [4]. Experiment Results Overall, CSC achieves the most compression. Figure 2 shows that all data structures scale well with | P | in terms of size, since the size of each cuboid grows sub-linearly with | P | . That the CSC has a worse com-pression rate on anti-correlated data is intuitive, because the minimum subspaces for each point are larger. In Figure 3, see that CSC X  X  compression relative to the lattice increases with d , because there are longer paths between minimum subspaces and the full skyline. Hashcube is generally closer to CSC than to the lattice. Relative to the lattice, it obtains  X  10  X  compression and permits storing in the same amount of space 2 X 4 more dimensions (4 X 16  X  more cuboids). The same trends exist for physical space (not shown). We use standard libraries, rather than more space-efficient, custom-built containers. Still, for d = 12 and | P | = 500K, HashCube achieves a compression ratio (in bytes) of 7 . 9  X  (compared to 13 . 8  X  ). The same ratios apply for CSC. Figures 4 and 5 report average query performance. The Hashcube performs very strongly, closely following the op-timal performance of the lattice, typically 1000  X  10000  X  faster than CSC. The iteration of all hash keys only takes 5  X  10  X  as long as the direct lookup in the lattice. Further, on anticorrelated data, Hashcube converges towards the lat-tice with increasing | P | (Figure 4). By contrast, CSC is rather slow, beaten in most instances by simply computing the skyline from scratch with BSkyTree. CSC outperforms BSkyTree only on small, correlated instances of &lt; 200  X  s.
The poor query performance of CSC results from dom-inance tests required to reconstruct each cuboid. As d in-creases, exponentially more subspaces of a query space must be examined for false positives. With respect to | P | , trends match the size plots. The correlation is expected: for each subspace, the number of dominance tests is quadratic in the number of points for which that is the minimum subspace. It should also be noted that the variance of query times for HashCube is small, i.e., never exceeds 1ms, while CSC typ-ically spends minutes on high-dimensional queries. This is a result of the split into several bitstrings of size | w | , which limits the number of hash keys for each query, while CSC needs to iterate the data points in all subspaces of the chosen dimensions and needs to perform dominance checks.

Hashcube is efficient to query, typically 1000  X  10000  X  faster than CSC and computing from scratch with BSkyTree. The iteration of all hash keys only slows Hashcube 5  X  10  X  relative to the lattice. Further, on anticorrelated data, Hashcube converges towards the lattice with increasing | P | . The cost of outputting longer contiguous vectors is neglible; so, the increased input size only slows the data structure if new points associate with as-yet-unused hash keys. With re-spect to d , the curve follows that of the lattice quite closely.
We call particular attention to Figure 5, because it ex-presses very well the balance that Hashcube obtains. We are unable to finish the plot for both the lattice and CSC, but for opposite reasons. The lattice does not fit in 16 GB Figure 2: Size of the data structures w.r.t. to n ( d =12). F igure 3: Size of the data structures w.r.t. to d ( n =500K). F igure 4: Data structure query time w.r.t. to n ( d =12). F igure 5: Data structure query time w.r.t. to d ( n =500K). of memory; so, we cannot query it fairly. On the other hand, CSC achieves good compression, but has prohibitive query time ( &gt; 48 hrs total). Hashcube is very efficient in both re-spects and supports this 16-dimensional, anticorrelated case. It compresses well and still, across all tested combinations of | P | and d , can be queried on average in less than 200  X  s.
We introduced a compressed skycube based on bitstrings, the Hashcube. Relative to the lattice, it achieves  X  10  X  com-pression. Relative to the state-of-the-art compressed sky-cube, queries are  X  1000  X  faster. Further, we showed that, while the compressed skycube is updatable, it is outper-formed by skyline computation from scratch. Thus, updat-ing skycubes is still a challenging open problem. For future work, we believe equivalence class ideas from [8] and/or more sophisticated cuboid grouping choices can be integrated into the Hashcube to further improve compression. Small, aux-illiary structures may help handle some update types.
This research was supported in part by the Danish Coun-cil for Strategic Research, grant 10-092316. We thank the BSkyTree authors [4] for their skyline implementation.
