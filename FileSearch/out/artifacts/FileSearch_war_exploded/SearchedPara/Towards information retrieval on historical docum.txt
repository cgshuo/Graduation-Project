 ORIGINAL PAPER Annette Gotscharek  X  Ulrich Reffle  X  Christoph Ringlstetter  X  Klaus U. Schulz  X  Andreas Neumann Abstract Due to the large number of spelling variants found in historical texts, standard methods of Information Retrieval (IR) fail to produce satisfactory results on historical doc-ument collections. In order to improve recall for search engines, modern words used in queries have to be asso-ciated with corresponding historical variants found in the documents. In the literature, the use of (1) special matching procedures and (2) lexica for historical language have been suggested as two alternative ways to solve this problem. In the first part of the paper, we show how the construction of matching procedures and lexica may benefit from each other, leading the way to a combination of both approaches. A tool is presented where matching rules and a historical lexicon are built in an interleaved way based on corpus analysis. In the second part of the paper, we ask if matching procedures alone suffice to lift IR on historical texts to a satisfactory level. Since historical language changes over centuries, it is not simple to obtain an answer. We present experiments where the performance of matching procedures in text collec-tions from four centuries is studied. After classifying missed vocabulary, we measure precision and recall of the matching procedure for each period. Results indicate that for earlier periods, matching procedures alone do not lead to satisfac-tory results. We then describe experiments where the gain for recall obtained from historical lexica of distinct sizes is estimated.
 Keywords Historical spelling variants  X  Electronic lexica Information retrieval 1 Introduction In present days, still, millions of historical books and doc-uments are hidden in libraries, often only accessible to a small group of experts. Libraries which want to present their treasures to the public have realized that it does not suf-fice to create digital images. In fact, searching for interest-ing documents is only possible once texts are available in symbolic form. A recent study of the Australian National Library [ 9 ] found the numbers of visitors to increase by fac-tor ten when collections provide a searchable fulltext index. Following these insights large projects have been initialized where optical character recognition (OCR) is applied to his-torical documents [ 16 , 4 ]. Though todays OCR software has severe deficiencies on historical texts, the situation will prob-ably improve over the next years.

Still, it would be naive to expect that problems are solved as soon as OCR works in an acceptable way. Even if histor-ical texts are recognized in a perfectly accurate way, Infor-mation Retrieval (IR) will suffer from the effect that due to missing normalization of orthography historical documents typically are full of spelling variants. A non-expert user X  who of course uses modern keywords in his queries X  X ill miss a large amount of relevant documents in the answer set, being unaware of all the variants how words are written in historical texts.
In the IR literature, two solutions have been suggested to cope with this problem [ 8 , 2 ]. The first approach is to use some form of approximate matching procedure to associ-ate modern keywords with words that are likely to represent corresponding historical spelling variants [ 2 , 3 ]. The second approach is based on special lexica that assign to each histor-ical spelling variant the lemma of the corresponding modern word. In this way, each link between a modern keyword and a historical spelling variant is manually checked. Despite some isolated evaluation work in the mentioned references, little empirical evidence has been collected that would help to judge the strengths and shortcomings of either approach. For obtaining a clearer view, three problems are discussed in this paper: (1) It is far from clear how much time is needed for the con-struction of a historical lexicon. Existing experiences from the work on modern lexica are misleading. When building lexica for modern language, word lists that X  X y and large X  cover the relevant vocabulary are easily obtained from Inter-net resources. In contrast, it is a difficult task to collect a corpus of proofread historical documents that offers a good basis for building a historical lexicon. Furthermore, his-torical lexica to be built for Information Retrieval applica-tions have to assign modern lemmas to historical spellings. Taking this task into account, how can we minimize the man-ual effort for building the entries of the lexicon? Which part of the workload cannot be avoided? (2) As to the use of matching procedures, correctness and completeness of this approach need to be estimated on a firm empirical basis. Correctness (precision) measures the percentage of correct links between historical spellings and modern spellings among all such links produced by the matching procedure. Completeness (recall) addresses the question which amount of the vocabulary of a historical text can be correctly traced back to a modern spelling using the matching procedure. Estimates for correctness and complete-ness are seriously complicated by the fact that distinct results have to be expected for historical corpora from distinct tem-poral periods. (3) Possibilities need to be discussed how to combine both approaches, obtaining an optimal basis for Information Retrieval on historical document collections. Furthermore, an interesting related question is how matching approaches can benefit from lexicon building and vice versa.

To provide a common ground for discussing (1 X 3), we first describe matching based approaches and lexica for IR on historical documents in Sect. 2 . The first core part of the paper is then devoted to Problem (1), the efficient con-struction of historical lexica. Section 3 gives an overview on the collection of historical proofread corpora as a basis for lexicon building work. In Sect. 4 , we describe a web tool for corpus-based and collaborative construction of historical lexica. The tool tries to minimize the manual work of the lexicographer when building entries of the lexicon. A cen-tral insight, relevant also for the combination of lexica and matching procedures (Problem (3) mentioned above), is the following. We found that appropriate matching procedures support efficient lexicon construction. From our point of view, lexicon construction and the design of matching proce-dures should be interleaved, with benefits for both. Using the tool, a historical lexicon with approximately 10,000 entries has been built. On this basis, we have a much clearer view on the effort needed to build a historical lexicon.
The second part of the paper is devoted to Problems (2) and (3) mentioned above. We present several evaluation exper-iments based on samples with materials from four centu-ries (sixteenth, seventeenth, eighteenth, nineteenth century) to explore the reach of the different approaches. In Sect. 5 , we first measure the amount of vocabulary of each century that can be covered using a pure matching approach and a lexicon of modern language as a basis. We then analyze and classify the remaining vocabulary. This helps to judge which kinds of lexica may help to enable access to a larger amount of the vocabulary.

In Sect. 6 , we consider the algorithmic problem of find-ing all historical variants of a modern word. Correctness and completeness of a pure matching approach are analyzed for the four periods of time. Our results show that a pure match-ing approach does not lead to satisfactory results for earlier periods, in particular for German sixteenth-and seventeenth-century document collections.

In Sect. 7 , we estimate the improvement of recall that is obtained from historical lexica of distinct sizes in experi-ments with the aforementioned samples from four centuries. A last series of experiments illuminate the benefit obtained from using refined pattern sets for the matching approach. Refined pattern sets for four centuries were derived from the historical lexicon built so far, which points to Problem (3) above.

After discussion of related work in Sect. 8 , we finish with a brief conclusion in Sect. 9 . Here, we briefly come back to Problem (3). An earlier version of this paper has been pub-lished in [ 6 ]. 2 Matching procedures and lexica for IR on historical documents To create a basis for later discussions, we first give a brief introduction to matching procedures and special lexica for historical language variants, also pointing to limitations. 2.1 Matching procedures Looking at the large number of spelling variants in old texts, it is unclear if a static lexicon of words collected from a historical corpus is able to cover a sufficient amount of his-torical spelling variants in new and previously unseen histor-ical documents. In the literature, other resources have been suggested (cf. Sect. 8 ). Many historical spelling variants can be traced back to a corresponding modern word by applying characteristic patterns. By a pattern, we mean an edit oper-ation such as  X  X   X  th X  that locally explains the difference between modern and historical spellings. Patterns may come with a probability or an edit weight. In this view, the prob-lem of finding the modern pendant for a historical word X  or the dual problem of finding historical spelling variants for a given modern word X  X ppears as a special approximate matching problem. The central challenge is the definition of a word neighborhood that captures most of the real correspon-dences between modern and historical wordforms (complete-ness, recall) and minimizes the number of wrong associations (correctness, precision) at the same time.

From a more technical point of view, there are distinct ways how special approximate matching procedures can help to improve Information Retrieval on historical document col-lections. If a lexicon is available that maps modern lemmas to modern full forms, the matching procedure can be used in online mode : in this scenario, the user asks a modern lemma w . In a first step, we derive all inflected forms w of w using the modern lexicon. In a second step, the match-ing tool is used to collect all words w from the index that can be matched to some w . At indexing time, no lexicon is needed. In offline mode , words w of the document col-lection are first mapped to corresponding modern inflected forms w and then lemmatized using the lexicon. The index only lists modern lemmas w , with pointers to all occurrences of the form w . 2.1.1 Limitations Matching rules in general will not fully capture variations caused by historical morphology. Furthermore, for some his-torical words, there exists a modern word with the same (or a similar) meaning; however, the surface form is completely distinct. Hence, we cannot expect that matching procedures give full access to all historical words. Matching procedures can of course introduce wrong associations. An interesting special case are situations where a modern word w in the text is erroneously interpreted as a historical variant of another modern word. An example we found in our experiments is the modern word  X  X tatt X  (English:  X  X nstead of X ) which was inter-preted as a variant of  X  X tadt X  (English  X  X own, city X ). Note that in other cases the latter interpretation might be the correct one. Another source of errors is any automated form of lem-matization. In general, the mapping from inflected forms to lemmas is ambiguous. 2.2 Lexica for IR on historical documents In what follows, by a lexicon for historical language we mean a collection of wordforms that represent spelling vari-ants occurring in proofread historical documents. Each entry is manually checked, and with each entry we store at least one attestation , an occurrence in the corpus showing that the spelling variant has really been used. For Information Retrieval applications, the lexicon is enriched. To each entry w we manually assign the corresponding modern lemma(s) w . The advantage of the lexicon approach is twofold. First, if we ignore the minor effect that the mapping from histor-ical words to modern lemmas is sometimes ambiguous, the association between historical words and modern lemmas, in difference to an automatic matching procedure, is manu-ally verified and safe. Second, in the lexicon we can explic-itly store associations that are not covered by a matching approach.

A lexicon of this form can be used to realize mappings from historical words to modern lemmas or vice versa. In an IR scenario, lexica can be used in online mode at query time or in offline mode at indexing time. 2.2.1 Limitations The construction of a lexicon of the form described above is time consuming. It is not always trivial to find the cor-rect lemma for a given word, and in some cases there simply is no corresponding modern word. We also found situations where the meaning of the modern word had changed. Thus, the idea that each historical spelling variant can be  X  X rans-lated X  into an exact modern pendant is an oversimplification. The vocabulary covered by a lexicon  X  X nchored X  in a his-torical corpus is limited. While lexica help to avoid wrong associations, ambiguities remain an issue. 3 Collecting and preparing a corpus basis for lexicon building The creation of a good corpus for building historical lexica is a difficult task. So far, most  X  X istorical corpora X  found in the web are just large collections of document images. For lexicon construction, we need proofread (i.e., ground truth) symbolic texts. Depending on the language, the problem of collecting/constructing such a corpus of sufficient size might be simple, difficult, or practically impossible. For German, appropriate sources are distributed over various places, often only known to a small and special scientific community. In this section, we first describe the corpus initially used for our lexicon building work. We then explain why and how the corpus was extended for improved lexicon building. 3.1 First corpus The first corpus we composed for lexicon building cov-ers documents from several centuries from four differ-ent sources: the collection  X  X istorisches Korpus X  from the  X  X nstitut f X r Deutsche Sprache X  IDS with 408 ground truth texts from the year 1700 to 1918; the  X  X onner Fr X hneu-hochdeutschkorpus X  consisting of 40 sources from 1350 to 1700 ordered by language regions. Each text contains approx-imately 30 pages; the  X  X erManC Corpus X  with 50 German newspaper texts from 1650 to 1800 where each text repre-sents a sample of 2,000 words; a sample of 53 twice proof-read German texts from 1504 to 1904 found on Wikisource, a collaborative initiative for the keying of important historic texts. The complete corpus contains 2,693,966 tokens (words in running text) and 288,709 types (unique words).
When evaluating matching procedures, we found that for texts from the sixteenth and seventeenth century a pure matching-based approach hardly leads to satisfactory com-pleteness and correctness for assigning modern pendants to historical spelling variants. These experiments are described later in the paper. We then wanted to illuminate the gain that can be obtained from historical lexica. At this point, it turned out that the First Corpus was far from optimal, since it con-tained only a small number of texts from early periods. In the light of our experiments, a historical lexicon should have a good coverage especially for the vocabulary of the six-teenth and seventeenth century. As a consequence of these insights, we decided to put additional work in improving the corpus basis for lexicon building, extending the set of early texts. 3.2 Additional corpus of early new high German In a direct collaboration with the world X  X  biggest owner of German books of the period 1500 X 1650, the Bavarian State Library, we composed a corpus for Early New High German with approximately 500,000 tokens. We first man-ually selected a large collection of images from appropriate books and documents. From the images, ground truth texts were prepared by service providers. With the additional texts, a more balanced corpus for the four centuries under inves-tigation was at our disposal. The additional corpus contains 76,546 non-modern types. We would like to emphasize the following: both the selection of appropriate images and the preparation of keying instructions for the service provider turned out to be time-consuming tasks. For example, one problem to be solved was the keying of special symbols or letters which appear in historical documents.

As a matter of fact, all tests described later in this paper were made on corpora (introduced in Sect. 5 ) which are dis-joint from the above two corpora used for lexicon building. 4 Creating lexica and matching procedures in an interleaved way This section is split in three parts. We first explain the main ideas and principles guiding our corpus-based construction of lexica for historical language. We then describe a web tool for collaborative construction of historical lexica. Details are given which might help other groups to build up a lexicon for historical language in a similar way. In our tool, efficient lexicon construction is supported by a matching procedure. The rule set is extended as a byproduct. In the third subsec-tion, we expand this perspective and outline the idea of a knowledge base where the tuning of a matching procedure and the construction of a lexicon for historical language are fully interleaved. 4.1 Corpus-based lexicon construction Given a large historical corpus, we ignore all words found in a modern lexicon of the language, as well as special mod-ern vocabulary such as names, geographic expressions, etc. We then analyze the set of remaining words by frequency. The frequency-based construction ensures that the lexicon soon receives a reasonable recall in the sense that for a large amount of historical variants in the text the link to the respec-tive modern words is available.

In order to minimize the workload of the lexicographers, we employ a number of advanced NLP techniques. The ulti-mate goal is the following ideal division of work: it is the role of the machine to produce meaningful suggestions of what to include into the lexicon. The lexicographers then just con-firm or reject the suggestions. In reality, also more difficult cases occur where more complex action and real input of lexicographers is needed. In what follows, we describe the resources that are used to come close to this goal. 4.1.1 Word lists for modern vocabulary In order to separate between modern words and historical spellings, a collection of word lists for modern vocabulary is used. We use special lists for names and geographic expres-sions, as well as a large list D G that covers modern German standard vocabulary. This list extends CISLEX (see below) by important compound expressions. 4.1.2 List of patterns As we explained above, many historical spelling variants can be traced back to a set of  X  X atterns X  that locally explain the difference between modern and historical spelling. Promi-nent patterns for German are t  X  th and ei  X  ey, as exempli-fied by the pairs (of the form  X  X odern  X  old X ) Hut  X  Huth (English  X  X at X ) or ein  X  eyn (English  X  X  X ). Based on corpus inspection and as a side result of lexicon construction, we collected a list L of 140 patterns for German. 4.1.3 Matching modulo patterns We implemented a tool for matching modulo patterns. The tool uses the word list D G and the list L of patterns as back-ground resources. Given an input word w occurring in the historical corpus, all words w in D G are computed where w can be obtained from w by applying one or several patterns. The output list is ranked, preferring candidates w where a small number of pattern applications are needed to rewrite w into w . With each suggestion w , the tool also outputs the set of patterns that are used to rewrite w into w . Our tool is implemented as a finite-state device. The lexicon D G is repre-sented as a deterministic finite-state automaton. For traversal of the automaton, a special procedure has been implemented that takes pattern variation into account, using the list L . 4.1.4 Lemmatizing modern wordforms As a result of the above process, we receive one or several modern wordforms which correspond to a given historical token. It remains to assign the correct lemma(s) and part-of-speech (lexical category) to the wordform(s). Our analysis module has direct access to CISLEX [ 7 , 12 ], a large morpho-logical lexicon for modern German. Each entry represents a wordform with associated information. The information attached to a full form includes the underlying lemma, part-of-speech (POS) category, morphological analysis. For each lemma, the inflectional class and possible Fugenformen 1 for building compound expressions are stored as well. CISLEX can be used in two directions. Given a full form, we immedi-ately receive the lemma(s) plus morphological information. Conversely, we can ask for the full paradigm (set of inflec-tional variants) of a given lemma. 4.1.5 Analyzing compound expressions German texts generally contain a large number of com-pounds. Since an infinite number of compound terms can be built, it is not possible to store all compounds in a con-ventional lexicon. Using information on Fugenformen in CISLEX, we have created an analyzer that checks if an unknown word can be interpreted as a modern compound expression. These words are removed from the frequency list presented to the lexicographer. 4.2 A web-tool for collaborative construction of lexica for In order to support collaborative lexicon construction, we use a web-based tool. The main modules of the web-tool, which build up on the resources described above, are the following. 4.2.1 Analyzer module Given a historical string observed in the corpus, the analyzer module first suggests corresponding modern full forms based on matching. Each such interpretation v comes with the set of patterns that were applied. Second, for a given modern full form v , the analyzer computes the lemma(s) X  X ncluding part-of-speech info X  X hich may underly the full form v . 4.2.2 Database for historical lexicon The confirmed entries for the historical lexicon are stored in a special database DB . Standard entries of the database consist of the historical string as found in the corpus, the correspond-ing modern lemma, the part-of-speech category, pointers to concordances 2 in the historical corpus which serve as attes-tations for the given interpretation, and the name of the per-son who created the entry. Note, a historical string can be associated with several entries of the database. The database also contains  X  X on-standard X  entries such as named entities, abbreviations, and historical words that do not have a corre-sponding modern lemma. 4.2.3 Managing module The managing component realizes the communication between the distinct modules and resources, determines the exact form of the visualization of each view, and includes a user administration. It is responsible for all details of the work flow and guarantees that no conflicts arise when several lexicographers simultaneously work on the vocabulary of the corpus. 4.2.4 Graphical user interface The main functionality of the graphical user interface is to support lexicographers in adding new entries to the database DB for the historical lexicon. The views offered for this pur-pose and the steps for creating new entries are explained below. Other views of the GUI show the lexical database, the set of patterns, the list of texts of the background corpus, or they display a given text with additional information on the status of each token (s.b.). 4.2.5 Creating new entries For the creation of new lexicon entries, two modes exist, a corpus and a document mode. 4.2.6 Corpus mode In order to show the work that is left, the GUI presents two word lists (1) the list of unknown words in the corpus that can be derived via patterns from modern words, and (2) the list of other unknown words of the corpus, both ordered by frequency. Figure 1 shows list 1 (2) on the left (right) side. If the user selects an entry w of the left list, she is taken to a new screen that visualizes the possible interpretations of w .Byan interpretation , we mean a pattern based derivation of w from a valid modern full form (cf. Fig. 2 ). The user now confirms or rejects the proposed interpretations. For each confirmed interpretation, the possible linguistic readings in terms of the corresponding lemma(s) have to be determined. Readings based on the modern lexicon are suggested by the system. 3 Each reading is confirmed or rejected. Before the lexicog-rapher may confirm a reading, she has to select at least one attestation, i.e., a concordance where the reading in question is the correct one. To this end, all concordances are shown graphically (cf. Fig. 3 ).

For every confirmed reading, a separate lexicon entry is created that includes the associated attestations. If a pro-cessed string has other than pattern-based mappings to a mod-ern word form or lacks a modern explanation, it is included into one of the following special sublexica : historic words without a modern equivalent; historic abbreviations; historic wordforms which lack a simple transition pattern; named entities; missing words of the modern lexicon (cf. Fig. 2 , lower part).

Entries of the right frequency list are more complicated with respect to the cognitive load of the lexicographer. The lexicographer can assign these entries to the special lexica mentioned above. If the lexicographer sees a derivation from a modern word using a new pattern p she can suggest to add p to the list of patterns. In the current version, there is no automated update mechanisms for the list of patterns and the matching procedure. 4.2.7 Document mode The lexicographer may also decide to work on a specific text. On the basis of the current lexicon and the matching rules, the text is visualized with all words marked according to their lexical explanation (cf. Fig. 4 ). Additional information is provided through mouse events. We distinguish modern words, checked entries of the lexicon for historical word-forms, entries of the left (right) frequency list shown in the corpus mode, and non-explained strings. If a string is acti-vated, the sequential processing is the same as for the corpus mode.

The system is web based and collaborative. Both issues are of great importance for the project. Since the involved lexicographers do not work at the same location, flexibility concerning their individual workplaces is needed. Since the professional abilities of the contributors and the complexity of certain lexicon entries differ significantly, a workflow was created that leaves more challenging entries in the frequency list to trained historical linguists, whereas other lexicogra-phers deal with the simple cases. Our current lexicon for his-torical language contains 9,753 historical wordforms with 10,298 readings. Not counting maintenance and necessary adjustments of the tool 10 person months have been spent on lexicon building. 4.3 Future perspective: interleaved construction From our present perspective, corpus, matching rules, and lexicon should be considered as a joint knowledge base. Given a set of patterns, we may use old wordforms and cor-responding modern wordforms stored in the lexicon and in addition the corpus for deriving meaningful probabilities or edit weights for the patterns. To make this more clear, let p denote a pattern. A pair of the form (modern wordform w associated historical wordform w ) from the lexicon is called a p -pair if p is used for deriving w from w . Count-ing the number of occurrences of historical word forms from p -pairs in the corpus (i.e., situations where p was applied) and counting the number of modern or pattern derivable tokens in the corpus where the left-hand side of p is not modified ( p was not applied) gives a basis for estimat-ing the probability for pattern p .Asamatteroffact, frequency-based lexicon construction also helps to find new relevant patterns. In this sense, lexicon and corpus provide empirical evidence for patterns (rules) and help to finetune approximate matching. Conversely, we have seen above how refined matching procedures help to speed up lexicon construction. Summing up, this shows that refinement of matching procedures and lexicon construction can be directly interleaved in a kind of bootstrapping procedure. 5 Analyzing matching procedures on the vocabulary of four centuries In Sect. 2 , we pointed to possible limitations of the matching procedure and suggested a combined approach for enabling IR on historical documents. Here, we ask how serious these limitations are in practice. The question is difficult since the answer depends on the period of time when the documents in the collection were born. To obtain an empirical basis for a proper judgment, we created an evaluation corpus with his-torical documents from the Bavarian State Library from the sixteenth, seventeenth, eighteenth, and nineteenth century. The corpus, which is disjoint from the development corpora described in Sect. 3 , has a total number of 31,745 tokens.
We applied our module for matching modulo patterns (cf. Sect. 4 ) to each token of the corpus. Through manual inspection, we decided in each case if the pattern-based matcher offers the correct interpretation of a token ( positive case ). Included are situations where we have an exact match and a token represents a word of the modern lexicon. The modern lexicon used here also contains a large amount of compounds. It is important to note that in the negative case our pattern based approach X  X ith the given lexical resources for modern words X  X oes not give any correct interpretation; in other words, the correct interpretation was  X  X issed X  by our matching approach. Each missed interpretation was further classified to see why it was missed. We used the following classes: 1. missed simple word, with subclasses (1a) missing entry 2. missed compound, with subclasses (2a) missing mod-3. missing geographic name, with subclasses (3a) missing 4. missing person name, 5. other (Latin, abbreviation, garbage,...)
Note that for coping with tokens of categories (a) and (b)wewouldhaveto extend our resources for modern language . For enabling access to tokens of the form (c) on the other hand, we need to build a historical lexicon . Since for Information Retrieval content words are more relevant, we built a list of (modern as well as historical) stopwords, which are then excluded from IR evaluation. Table 1 presents the results. The most important insights are made more trans-parent and extended in Figs. 5 and 6 :
Grey bars in Fig. 5 show the percentage of missed tokens of the form (c) for which we need a historical lexicon (all stopwords excluded from the calculations). In the nineteenth-and eighteenth-century subcorpora, the percentage is small, which shows that the matching-based approach works well. Though lexica can help to make new words accessible, the benefit is restricted. In the seventeenth and sixteenth century subcorpora, the percentage becomes significant. Here a pure matching approach misses a large part of the vocabulary, and special lexica become more important. For example, almost 20% of the non-stopwords in the sixteenth century are missed by the matching approach. For comparison, the numbers with stopwords included are visualized by the black bars.
In a similar way, Fig. 6 shows in how far gaps in the mod-ern language resources limit the performance of the match-ing approach. The grey bars show the percentage of missed tokens of category (a) and (b) X  X hese tokens could not be interpreted correctly because the necessary modern words are missing in the modern dictionary. Same as for vocabulary of type (c), the effect is stronger if stopwords are excluded from the calculation. In the nineteenth century, more than 4% of the tokens can not be interpreted correctly due to missing modern vocabulary X  X able 1 shows that here missing com-pounds and geographic names play a dominant role. 6 Precision and recall of matching procedures To evaluate the IR accessibility of historical documents for different lexical settings, we measured precision (correct-ness) and recall (completeness) of our matching approach on the corpus described in the previous section. It should be mentioned that there are distinct options to formally define precision and recall. We comment on this below.

The basic intuition for our definition is as follows. The user specifies a modern full form w (the query). Based on the matching approach, the system delivers variants w of w found in the corpus (the answers). Some variants are cor-rect, others are incorrect, the system does not find all correct variants. To minimize ambiguities, we refined the matching approach using the following modern wins strategy :ifatoken w of the text according to our modern language resources can be interpreted as a modern word, all alternative interpreta-tions as a historical variant are canceled. In what follows, each link from a wordform in the text to a modern word proposed by the matching procedure is called a proposed match .From a query perspective, a match is either correct or incorrect. Definition 1 Precision is defined as the total number of pro-posed correct matches divided by the total number of pro-posed matches.

The definition amounts to a form of micro evaluation. The experiment described in the previous section is used to count the number of proposed correct matches. Exactly the events characterized as  X  X ositive cases X  in Sect. 5 are proposed cor-rect matches. The modern wins strategy is an obvious way to improve precision. However, it also gives rise to a special class of errors. By a historical false friend error , we mean a wordform of historical language that accidently coincides with a modern wordform, but has a distinct meaning. An example is the spelling variant  X  X tatt X  of  X  X tadt X  (town, city) which is read as the modern word  X  X tatt X  (instead of). In order to enable an automated evaluation, the definition of recall is simplified.
 Definition 2 Recall is the total number of proposed correct matches divided by the total number of relevant tokens of the vocabulary.  X  X elevant X  are those tokens we want to have access to in an IR scenario. The experiment described in the previ-ous section is used to count the number of relevant tokens. Exactly the tokens of category  X  X ther X  were treated as irrel-evant. Note that a precise calculation of recall would need to connect each modern query term with exactly all its variants in a historical document base, which is infeasible in particular for very old texts.

The results for precision, recall, and the rate of false friends are summarized in Table 2 . As in the previous section, for the evaluation we used two modes. The first mode exactly follows the above description, looking at all relevant tokens. In the second mode, modern and historical stopwords were completely ignored in all calculations. 6.1 Insights The number of false friends grows when going back in time (8.47% for sixteenth century, 6.92% for seventeenth century; here and in the following numbers are given without stop-words). Because of that, the modern wins strategy turns out to be dangerous for very early periods. For the nineteenth century, it appears to be  X  X afe. X  For the eighteenth and nine-teenth century, the precision of the pattern-based approach lies within the high 90 X  X  (in accordance with literature [ 3 ]). For the seventeenth (sixteenth) century, on the other hand, precision goes down to 76.49% (62.09%). Here, a special lexicon for historical language will help to eliminate wrong matches. Recall values for the eighteenth and nineteenth cen-tury are acceptable (89.96 and 90.26%). For the sixteenth and the seventeenth century, recall values of 78.69 and 76.15% show that a large number of words can only be explained by a historical lexicon. If we abstain from the modern wins strat-egy , the precision lowers to 43.11% for the sixteenth, 48.52% for the seventeenth, 51.35% for the 18th and 56.24% for the nineteenth century. Summing up, as a rule of the thumb our pure matching approach seems acceptable for the nineteenth and eighteenth century, but too weak to obtain satisfactory results for the sixteenth and seventeenth century. 6.2 Comments Evaluations of the above form are not straightforward as dis-tinct options for experimental settings and definitions exist. One issue often ignored in IR, is the depth of linguistic analy-sis: lemmatization, for example, introduces new ambiguities. To simplify the evaluation, we did not look at lemmatization in our experiments. When counting wordforms missed by the matching approach, we implicitly assumed that queries have to be covered by the modern lexicon. With a string-based indexing scheme, problems related to missed modern wordforms (cf. Fig. 6 ) are limited to morphological variants. 7 Gains of lexicon work At the present point of time, we just started to redirect our focus for lexicon building, integrating the new corpus mate-rial available (cf. Sect. 3 ). Yet it is possible to estimate the gain in terms of recall that can be obtained from future lexi-con building work. To this end, we assume that the historical lexicon is built by selecting the top k most frequent entries of non-modern words in the enriched corpus, achieving a good tradeoff between resources spent and recall gained. 4 For recall, we can estimate the effects of a supplementary historical lexicon on the data annotated for IR evaluation (cf. Sect. 6 ). Before finishing lexicon work, precision cannot be estimated reasonably, because we do not know the number of ambiguities that even persist for the application of an explicit lexicon, i.e. cases where the historical word points to differ-ent modern word forms. 7.1 Improving recall/completeness As it is seen in our classification of vocabulary missed by the matching approach (cf. Table 1 ), the words of the class 1c  X  X istorical word X  represent the most important source for missing recall of the matching approach. For these words, the matching approach cannot offer a correct link between their modern and historic surface forms. Hence, they remain  X  X nvisible X  when using a pure matching approach, assuming that users only use modern words as queries. Envisaging a historical lexicon where this relation is explicitly coded by a lexicographer, these words become visible and  X  X R accessi-ble. X  In our experimental setting, we ask the following:
We present two answers to this question. The results when using the Additional Corpus of Early New High German for lexicon building can be found in Table 3 . For the sake of comparison, corresponding results for the First Corpus are shown in Table 4 . 7.1.1 Insights When focusing on texts from the sixteenth century, lexi-con building based on the Additional Corpus is preferable. Even with a small lexicon of 5,000 words, a large percentage (33 . 11%) of the vocabulary missed by the matching approach becomes  X  X R accessible. X  Using the First Corpus instead we would need to collect nearly 20 , 000 words to reach the same estimated recall (cf. Fig. 7 ).

On the other hand, regardless of the corpus that is used it seems difficult to push recall beyond a certain limit: even with a lexicon of 50,000 words we only cover 50 . 66% of the missed vocabulary. We currently do not have a final explana-tion for this phenomenon. One reason is that the Additional Corpus is not large enough. In our work, we found that lexi-con building with words that have a corpus frequency of three or less leads to negligible additional recall in most cases. This limit is reached with approximately 30,000 entries processed through the frequency-based approach. A second problem is the extremely high variance of the orthography due to miss-ing standardization in the Early New High German period.
For the texts from the seventeenth century, no strong pref-erence exists as to the choice of the corpus for lexicon build-ing. When using a large lexicon, now 61 . 41% of the missed vocabulary is covered. For the eighteenth and nineteenth century, not surprisingly, the use of the First Corpus leads to better results. From these numbers, it becomes obvious that the selection of a proper historical corpus for lexicon building is decisive for gains that can be expected from lex-icon work.

Let us stress again that the numbers in Tables 3 and 4 relate to the missed vocabulary of type 1c in the respective evaluation sets of the four centuries. The amount of missed vocabulary itself is large (small) for early (late) centuries (cf. Table 1 ). The gain of a supplementary historical lexicon with 20,000 entries related to all words of the evaluation sets -excluding stopwords -can be found in Fig. 8 . The gains for the sixteenth century (5.41%) and the seventeenth century (2.74%) are much higher than for the 18th (1.20%) and the nineteenth century (0.1%) data sets. 7.2 Using lexicon information to improve matching As a last point in our discussion, we now look at possible ways to improve the matching-based approach by using statistics obtained from lexicon building for calculating improved pat-tern sets. Recall that for lexicon construction, we use the matching procedure to generate candidates for lexicon entries for the unknown words of the corpus. In our corpora, each text is annotated with the year of its publication. Hence, as a side result of lexicon building we obtain a statistics showing which patterns actually occur in texts from distinct centu-ries. This information can then be used to adapt the matching approach to a certain time period, using restricted pattern sets. Following this idea, we analyzed our explicit lexicon con-structed so far with approximately 10,000 re-writable words and derived refined pattern sets for four centuries.
In Table 5 , we provide numbers on recall, precision, and the F -measure reached on our evaluation set. Evaluations are based on matching computed with the complete pattern set and with a pattern set adapted to the respective histor-ical period. As a matter of fact, the choice of a pattern set does not affect the trivial treatment of modern words by the matching approach. To obtain a better picture of the role of the pattern set, in the experiments only non-modern words were considered. When using this restricted focus, refined pattern sets lead to a clear loss of recall. However, the gain of precision is much stronger, which means that F -score was improved in a significant way in all cases. 8 Related Work Work of enabling IR on historical texts [ 3 ] is related to cross-language IR, which has influenced some approaches [ 11 ]. A research project at the University of Duisburg-Essen is cen-tered around the development of a rule-based search engine for historical texts [ 1 , 13 , 14 ], automated rule derivation from training examples, development and test of special distance measures for historical spelling variants [ 10 ] and the gen-eration of historical spellings from modern words [ 2 , 3 ]. A variant detector for the analysis of English historical texts is described in [ 15 ]. In [ 5 ], a Historical Dictionary of Brazil-ian Portuguese (HDBP) is described. The dictionary is based on a corpus of 3,000 texts with 7.5 million words from the sixteenth to the nineteenth century. Lexicon entries are gener-ated by a cluster algorithm that connects pattern-based vari-ants. Entries are coded generically as a masculine singular noun, disregarding the real POS category; the canonical form is simply the most frequent term in the cluster. The true POS as well as the lemmatized form and additional morpholog-ical information are intended to be included at a later stage of the project manually. For German with many variants on orthographical as well as on inflectional level during the cen-turies, this would be impossible without having the potential readings with their lemmatized form already preprocessed and presented to the lexicographer. In many cases, context is needed to decide whether a string is a valid historical variant of a certain lemma and to annotate additional morphological information. 9 Conclusion In this paper, we discussed appropriate resources for enabling IR on historical document collections. The discussion was focused on three points, (1) efficient corpus-based construc-tion of historical lexica and the effort needed, (2) strengths and limitations of matching approaches for German docu-ments from distinct temporal periods, and (3) ways to inter-leave matching procedures and special lexica.

As to (1), we described the collection of a proofread corpus for lexicon building and a toolbox for web-based and collab-orative construction of lexica for historical language. Based on these resources, we built a historical lexicon (ongoing work) with approximately 10,000 entries; estimates on the effort were given. We emphasized that the design of match-ing procedures and the construction of lexica for historical language should be interleaved, with clear benefits for both sides (cf. Problem 3)).

As to (2), the essential insight is that a pure matching approach leads to acceptable values of correctness/precision and completeness/recall for nineteenth (and, in a weaker sense, eighteenth) century collections. Corpus-based esti-mates show that special lexica for historical language can only offer a small additional benefit for these periods. For the sixteenth (and, in a weaker sense, seventeenth) century, a pure matching approach is seriously limited, both in terms of recall and precision. We showed that the additional use of histori-cal lexica of modest size leads to a significant improvement of recall in particular for texts from the sixteenth (and sev-enteenth) century (Problem (3)). As a matter of fact, refined matching procedures could lead to a modified picture. Still, from our analysis we expect that the above general perspec-tive remains valid.

As to (3), we found that efficient lexicon construction may benefit from matching procedures. We also saw that statis-tics obtained from corpus-based lexicon construction may be used to adapt pattern sets to distinct centuries. The use of refined pattern sets for the matching approach improves pre-cision and F -score when treating texts from specific periods. From our point, an approach combining lexica and matching procedures is also optimal in the context of IR on historical document collections where we want to link queries contain-ing modern words to historical spellings found in the corpus. We may first use lexica for linking a modern lemma (the query) in a safe way to historical spelling variants of the document basis. If the user has a preference on high recall, the matching approach can be used to compute in a second round an additional set of historical wordforms that are likely to represent valid hits. In this sense, the combined approach gives maximal flexibility.

We would like to point out that lexicon construction using proofread historical texts is just one possible way to obtain create historical lexica. The best way to proceed strongly depends on the available resources. An interesting point for future work is lexicon construction using OCRed texts as a basis. In the EU project IMPACT, we currently discuss with our partners the design of lexicon building tools that deal with OCRed texts. Here, the interface needs to offer mechanisms to efficiently check the correct spelling of an OCRed word. References
