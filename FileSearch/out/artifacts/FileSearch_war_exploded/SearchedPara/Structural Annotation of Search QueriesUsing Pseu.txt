 Marking up queries with annotations such as part-of-speech tags, capitalization, and segmentation, is an important pa rt of many approaches to query processing and understand-ing. Due to their brevity and idiosyncratic structure, sear ch queries pose a challenge to existing annotation tools that a re commonly trained on full-length documents. To address this challenge, we view the query as an explicit representation o f a latent information need, which allows us to use pseudo-relevance feedback, and to leverage additional informatio n from the document corpus, in order to improve the quality of query annotation.

Automatic mark-up of textual corpora with structural an-notations is a common practice in natural language process-ing (NLP) applications, but is done less often in informa-tion retrieval (IR). Accordingly, in this paper we focus on the structural annotation of user search queries. There are several key differences between search queries and the cor-pora usually used in NLP (e.g., news articles or web pages). As previous research shows, these differences severely limi t the applicability of standard NLP techniques for annotat-ing query corpora [1, 3, 15]. The most salient difference is length, since search queries are very short. Due to their Table 1: Annotating search query where is the closest brevity, the queries often cannot be divided into sub-parts , and do not provide enough context for accurate annotations to be made using the standard NLP tools, which are trained on more syntactically coherent textual units.

However, despite their brevity, queries do differ in both length and grammatical structure [2]. Some queries are key-word concatenations, while others are semi-complete phras es. It is essential for the search engine to correctly interpret the query structure, since it influences both the way the user interacts with the search engine [2] and the way the re-trieval should be performed [9]. However, even sentence-li ke queries are often hard to parse and annotate, as they tend to lack prepositions, proper punctuation, or capitalizati on, since users (often correctly) assume that these features ar e disregarded by the retrieval system.

Table 1 presents a simple annotation scheme, exempli-fied using a web search query. In this scheme, each query is marked-up using three annotation sequences : capitalization, POS tags, and segmentation indicators. While this type of simple annotation can be done with a very high accuracy for standard document corpora, it is quite challenging to perform well on queries [1, 3, 11]. Hence, we propose a probabilistic approach that relies on the latent user infor-mation need , rather than the query itself. This allows us to use pseudo relevance feedback (PRF) and leverage the doc-ument corpus to improve the quality of query annotation.
The rest of the paper is organized as follows. Sec. 2 details the related work. In Sec. 3 we introduce the query annota-tion framework. Sec. 4 describes several practical applica -tions of this framework. Sec. 5 presents the experimental results, and Sec. 6 concludes the paper.
In recent years, structural annotation of search queries has been receiving an increasing attention as an important step toward better query processing and understanding. The literature on query annotation includes query segmentatio n [3, 12, 9, 20], part-of-speech and semantic tagging [1, 17], named-entity recognition [8, 15, 19, 18], abbreviation dis -ambiguation [21] and stopword detection [14, 11].
In this paper, we advocate the use of pseudo-relevance feedback for query annotation. Pseudo-relevance feedback has proven to be a successful technique for query expansion [6, 13]. Recently, it has also been shown to be effective for query classification [5], query translation [10] and spelli ng corrections [7].
An explicit representation of the user X  X  latent informatio n need I is a search query Q , which is a sequence of n terms q , . . . , q n . Given a query Q , our task is to annotate it with the appropriate structure or a set of such structures. We denote an arbitrary structural annotation for Q as  X  Q .
In this paper, we consider shallow structural annotations that can be written as a sequence of annotation symbols  X 
Q = [  X  1 , . . . ,  X  n ]. In other words, each symbol in the an-notation sequence corresponds to a single query term.
We define the optimal structural annotation  X   X  Q for a query Q as the annotation which has the highest probability given the latent information need I underlying the query: Clearly, the quality of  X   X  Q will depend on the way the condi-tional probability p ( X  Q |I ) is estimated. Next, we describe two ways of estimating this conditional probability.
The most straightforward way to estimate the conditional probability in Eq. 1 is using the explicit representation of the information need I , i.e. the query terms. That is, we can approximate the conditional probability by
In practice, to make the resulting estimation feasible, we take a bag-of-words approach, and assume independence be-tween both the query terms and the corresponding annota-tion symbols, Thus, we can rewrite Eq. 1 as:
Given a short, often ungrammatical query, it is hard to accurately estimate the conditional probability in Eq. 1 us -ing the query terms alone. For instance, a keyword query hawaiian falls , which refers to a location, will be inaccu-rately interpreted by a standard POS tagger as a noun-verb pair. On the other hand, given a sentence from a corpus that is relevant to the query such as  X  X awaiian Falls is a family-friendly waterpark X  , the word  X  X alls X  is correctly identified by a standard POS tagger as a proper noun.

Accordingly, we are interested in bootstrapping the docu-ment corpus, in order to better approximate the latent infor -mation need I . To this end, we propose employing pseudo-relevance feedback  X  a method that has a long record of success in IR for tasks such as query expansion [6, 13].
In the most general form, given the set of all retrievable instances (whole documents or their sub-parts) in the doc-ument corpus C , and assuming that  X  Q and I are indepen-dent given a retrieved instance r , we can derive
Since for most instances the conditional probability p ( r |I ) is vanishingly small, we can closely approximate the above by considering only a set of instances R , retrieved at top-k positions in response to Q . This yields Intuitively, the equation above models the information nee d I as a mixture of top-k retrieved instances, where each in-stance is weighted by its relevance to the information need.
Furthermore, to make the estimation of the conditional probability p ( X  Q | r ) feasible, we assume that the symbols  X  in the annotation sequence are independent, given an in-stance r . Note, that this assumption differs from the in-dependence assumption in Eq. 2, since here the annotation symbols are not independent given the information need I .
We are now ready to derive the new estimate for Eq. 1, using pseudo-relevance feedback
Generally, an estimate of p (  X  i | r ) in this paper will be a smoothed estimator of the form where  X  is a constant 1 . This smoothed estimator com-bines two maximum-likelihood estimators, one based on a retrieved instance r and one based on some large text cor-pus C . It could be useful in cases when the annotation of the query terms in the top retrieved documents differs sig-nificantly from their annotation in the entire collection.
In this section we discuss the application of the framework presented in Sec. 3 for practical annotation tasks. For each of these tasks, we define the form that each annotation symbol  X  in the annotation sequence  X  Q can take, and the way the Eq. 3, respectively) are estimated. The capitalization annotation is defined as a sequence C
Q = [ c 1 , . . . , c n ] over the query terms [ q 1 , . . . , q each annotation in the sequence is defined, for simplicity, a s a binary trait c i  X  { C, L } (capitalized or lowercased) task is thus to estimate the conditional probability of a cap -italization sequence given an information need  X  p ( C Q |I ).
There are two ways to estimate the capitalization of query terms. First, following Eq. 2, we can use only the query terms. In this case, to estimate the conditional probabilit y p ( c i | q i ), we use a maximum likelihood estimator based on the statistics of a large text corpus C where x  X  { C, L } . Second, we can estimate the capital-ization using pseudo-relevance feedback. In this case, to estimate p ( c i | r ), we use a smoothed estimate p ( c i = x | r ) ,  X  Eq. 5 is a mixture model, which combines the estimate of the capitalization of the query terms in each of the top retrieve d instances with the background model of its capitalization i n the entire collection.
For this annotation task, we define a simple part-of-speech tagging scheme, consisting of only three tags: nouns, verbs and all other parts-of-speech. While simple, this scheme is quite useful for short queries, where even such simple disti nc-tion can be challenging to make. Accordingly the tagging annotation is defined as a sequence T Q = [ t 1 , . . . , t the query terms, and each annotation in the sequence is de-fined, as a ternary trait t i  X  { NN, V B, X } (noun, verb or other). As in the previous case, there are two ways to estimate POS tagging. The first, following Eq. 2, is to use only the query terms. In this case, to find the optimal tagging T
Q , we can simply run an existing POS tagger the query, and annotate each term using the tagger output (collapsing the actual output tags into the three categorie s above).

The second way to estimate POS tagging is using pseudo-relevance feedback. Here, to estimate p ( t w | r ), we can use a smoothed estimate p ( t i = x | r ) ,  X 
Essentially, the estimate in Eq. 6 leverages the POS tag-ging of the top-retrieved instances to enhance the initial P OS tagging of query terms alone.
Segmentation is defined as a sequence of annotations S Q = [ s , . . . , s n ], where each annotation s i is based on a decision of whether to create a segmentation between terms ( q i  X  1 Following previous work on query segmentation [3] we define each annotation decision as s i  X  { B, I } (beginning of the phrase or inside the phrase).
In unsupervised query segmentation, it is common that some term association measure such as mutual information or likelihood ratio is used to determine whether there is a break between two adjacent query terms [3, 12, 20]. Simi-larly to this prior work, we determine the probability of a segmentation decision { B, I } between two terms ( q i  X  1 based on the strength of association between them in some text x . Hence  X  p ( s i = B | q i , x ) = and occurring together in a text x , and  X  is a constant threshold.
The estimate of a probability of an annotation sequence now depends on the way we select the text x , over which the associations between the query terms is computed. There are two possible ways to select x . First, we can simply use the collection C and get that which is similar to the way the unsupervised segmentation of queries is done in the previous work [3, 12, 20].
Alternatively, we can employ pseudo-relevance feedback, using the co-occurrences of query terms in the top retrieved instances. Accordingly, to estimate p ( s i | r ), we use a smoothed estimate 4 p ( s i = x | r ) =  X   X  p ( s i = x | q i , r ) + (1  X   X  ) X  p ( s
For evaluating the performance of our query annotation methods, we sample 250 queries from a search log of a com-mercial search engine 5 , and annotate all of them with three annotations: capitalization , POS tags , and segmentation , ac-cording to the description of these annotations in Sec. 4. In this set of 250 queries, there are 96 verbal phrases, 93 ques-tions and 61 keyword queries 6 .

For the PRF-based estimates, we use each of these queries to retrieve 50 pages per query using Bing API and index them using Indri 7 . The resulting set of documents consti-tutes the document corpus from which we retrieve sentences Table 3: Breakdown of query annotation results by for constructing set R (see Eq. 3) for the pseudo-relevance feedback estimates. We use Google n-grams [4], as a back-ground corpus C .
In order to test the effectiveness of the models described in Sec. 4, we compare two annotation methods: a-QRY and a-PRF . Method a-QRY is based on  X   X  ( QRY ) Q estimator (Eq. 2). Method a-PRF is based on the  X   X  ( P RF ) Q estimator (Eq. 3).

For reporting the performance of our methods we use two measures. The first measure treats the annotation decision for each symbol  X  i as a classification. In case of capitaliza-tion and segmentation annotations these decisions are bina ry and we report F1. In case of POS tagging, the decisions are ternary, and hence we report the classification accuracy. We also report an additional measure, MQA (mean query ac-curacy), which treats the accuracy of the annotation on a query-by-query basis.

In Table 2, we see that a-PRF outperforms a-QRY for all annotation types, using both performance measures. The improvements are as high as 10.9% for F1 (capitalization), and as high as 5.7% for MQA (segmentation), and, in all cases, are statistically significant. These results verify the performance contributions stemming from using the pseudo-relevance feedback for query annotation.

Table 3 presents a breakdown of the performance of our methods by query type. We note that the contribution of different methods varies significantly across query types . For instance, a-PRF has a highly positive impact on POS-tagging estimates for keyword queries. Using a-PRF on key-word queries results in 12% improvement over the a-QRY baseline, compared to just 4% improvement for all queries.
In this paper, we have investigated two methods for an-notating search queries with capitalization, POS tags and segmentation mark-up. First, we examined a query-based method that takes into the account only the query terms and their collection statistics. Second, we proposed a pseudo-relevance feedback based method that takes into the account the top-retrieved instances with response to the query, as well as the query itself.

Our experimental findings over a range of queries from a web search log unequivocally point to the superiority of the PRF-based annotation method over the query-based one. We are encouraged by the success of our PRF-based query annotation technique, and intend to pursue the investigati on of its utility for IR applications.
This work was supported in part by the Center for In-telligent Information Retrieval and in part by ARRA NSF IIS-9014442. Any opinions, findings and conclusions or rec-ommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor.
