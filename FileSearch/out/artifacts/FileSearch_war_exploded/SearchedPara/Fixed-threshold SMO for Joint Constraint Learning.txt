 In this paper, we describe a fixed-threshold sequential minimal optimization (FSMO) for a joint constraint learning algorithm of structural classification SVM problems. Because FSMO uses the b =0, FSMO breaks down the quadratic programming (QP) problems of structural SVM into a series of smallest QP problems, each involving only one variable. By using only one variable, FSMO is advantageous in that each QP sub-problem does not need subset selection.
 H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing Keywords Structural SVM, joint constraint learning algorithm, fixed-threshold SMO. machines (SVMs) [1][2][3][4]. SVMs have empirically been shown to give good generalizati on performance on a wide variety taxonomic text classification [3], learning to rank [6], and s poken language understanding [9]. The formulation of SVM is base d on a two-class problem; hence discriminative approach to parsing inspired by the large-margin criterion underlying SVM in which the loss function is factorized analogous to the decoding process [7]. Tsochantaridis et al. proposed large-margin models based on SVMs for structured classification problem (structura l SVM) in general and apply sequence alignment [3]. Y. Yue et al. used structural SVM to globally optimize mean average precision (MAP) [6]. While conventional training met hods for standard SVM, in particular decomposition methods like SVM-Light [4] and SMO [2], handle problems with a large number of features N quite use inefficient or even intractable on large datasets. SVM solver (i.e. SVM-light) for solving dual form of structural SVM, despite the fact that structural SVM has b =0 [4]. Recently, Joachims proposed a joint constraint learning algorithm alternative formulation of the SVM optimization problem that formulas. In this paper, we describe a fixed-threshold sequential minimal optimization (FSMO) for the joint constraint learning algorithm of structural SVM problems. FSMO is conceptually simple, easy to implement, and faster compared to the standard SVM training algorithms for the joint constraint algorithm of structural SVM problems. FSMO uses the fact that the formulation of the joint constraint algorithm has no bias (i.e., the threshold b is held fixed at zero). Therefore, FSMO breaks the QPs of structural SVM into involving only one variable, FSMO is advantageous in that each QP sub-problem does not n eed subset selection. of structural SVM. For a given training set ( x 1 , y 1 ) ,..., ( x n , y n ) with x following optimization problem (A non-zero b can easily be modeled by adding an additional feature of constant value to each x ) [8]. min Joachims proposed an alterna tive formulation of the SVM optimization problem to predict st ructured outputs and to optimize Precision/Recall Break-Even Point [8]. The following is a specialization of this formulation for the case of error rate. min (c formulation corresponds to the sum of a subset of constraints from equivalent [8]. Using a Lagrangian, OP2 can be converted into a dual form which dependent on a set of Lagrangian multipliers  X  , In this section, we describe FSMO for solving structural SVM. algorithm. The extremum of the object function L of OP3 is at Let s new + = c c  X   X  and following update equation from (1): The update equation forces the output of the structural SVM to be constraint C Because structural SVM has no bias (i.e. b =0) in OP2, OP3 does Therefore, FSMO can optimize only one Lagrange multiplier at a time. We implemented the structural SVM using FSMO (in C++) for solving QP problems. For comparison, we also run the SVM-Struct [3] that uses SVM-light for solving structural SVM and SVM-Perf that uses SVM-light for solving OP2 [8]. We use for news20 data set. and news20 data set. In figure 1, FSMO is faster than both SVM-Struct and SVM-Perf. This paper presents FSMO for th e joint constraint algorithm of structural SVM problems. FSMO is conceptually simple, easy to implement, and faster compared to the standard SVM training algorithms for the joint constraint algorithm. Because FSMO uses smallest QP problems, each invol ving only one variable. By using only one variable, FSMO is advantageous in that each QP sub-problem does not need subset selection. [1] V. Vapnik, Statistical Learning theory , Wiley, New York, [2] J. Platt,  X  X equential Minimal Optimization: A Fast [3] I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun, [4] T. Joachims,  X  X  Statistical Learning Model of Text [5] D. Sculley and G. M. Wachman,  X  X elaxed Online SVMs for [6] Y. Yue, T. Finley, F. Radlinsk i, and T. Joachims,  X  X  Support [7] B. Taskar, C. Guestrin, and D. Koller,  X  X ax margin markov [8] T. Joachims,  X  X raining Linear SVMs in Linear Time, X  [9] C. Lee, J. Eun, M. Jeong, G. Lee, Y. Hwang, and M. Jang, 
