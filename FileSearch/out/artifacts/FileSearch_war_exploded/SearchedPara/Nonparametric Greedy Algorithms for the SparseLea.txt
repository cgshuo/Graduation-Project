 The linear model is a mainstay of statistical inference. At present, there are two major approaches to fit sparse linear models: convex regularization and greedy pursuit . The convex regularization approach regularizes the model by adding a sparsity constraint, leading to methods like LASSO [19, 7] or the Dantzig selector [6]. The greedy pursuit approach regularizes the model by iteratively selecting the current optimal approximation according to some criteria, leading to methods like the matching pursuit [14] or orthogonal matching pursuit (OMP) [20].
 Substantial progress has been made recently on applying the convex regularization idea to fit sparse additive models. For splines, Lin and Zhang [12] propose a method called COSSO, which uses the sum of reproducing kernel Hilbert space norms as a sparsity inducing penalty, and can simul-taneously conduct estimation and variable selection; Ravikumar et al. [17, 16] develop a method called SpAM. The population version of SpAM can be viewed as a least squares problem penalized by the sum of L 2 ( P ) -norms; Meier et al. [15] develop a similar method using a different sparsity-smoothness penalty, which guarantees the solution to be a spline. All these methods can be viewed as different nonparametric variants of LASSO. They have similar drawbacks: (i) it is hard to extend them to handle general multivariate regression where the mean functions are no longer additive; (ii) due to the large bias induced by the regularization penalty, the model estimation is suboptimal. One way to avoid this is to resort to two-stage procedures as in [13], but the method becomes less robust due to the inclusion of an extra tuning parameter in the first stage.
 In contrast to the convex regularization methods, the greedy pursuit approaches do not suffer from such problems. Instead of trying to formulate the whole learning task to be a global convex opti-mization, the greedy pursuit approaches adopt iterative algorithms with a local view. During each iteration, only a small number of variables are actually involved in the model fitting so that the whole inference only involves low dimensional models. Thus they naturally extend to the general multi-variate regression and do not induce large estimation bias, which makes them especially suitable for high dimensional nonparametric inference. However, the greedy pursuit approaches do not attract as much attention as the convex regularization approaches in the nonparametric literature. For additive models, the only work we know of are the sparse boosting [4] and multivariate adaptive regression splines (MARS) [9]. These methods mainly target on additive models or lower-order functional ANOVA models, but without much theoretical analysis. For general multivariate regression, the only available method we are aware of is rodeo [11]. However, rodeo requires the total number of variables to be no larger than a double-logarithmic of the data sample size, and does not explicitly identify relevant variables.
 In this paper, we propose two new greedy algorithms for sparse nonparametric learning in high di-mensions. By extending the idea of the orthogonal matching pursuit to nonparametric settings, the main contributions of our work include: (i) we formulate two greedy nonparametric algorithms: additive forward regression (AFR) for sparse additive models and generalized forward regression (GFR) for general multivariate regression models. Both of them can simultaneously conduct esti-mation and variable selection in high dimensions. (ii) We present theoretical results for AFR using specific smoothers. (iii) We report thorough numerical results on both simulated and real-world competitors, including LASSO, SpAM, and an adaptive parametric forward-backward algorithm called Foba [22].
 formulation and notations. In Section 3 we present the AFR algorithm, in section 4, we present the GFR algorithm. Some theoretical results are given in Section 5. In Section 6 we present numerical results on both simulated and real datasets, followed by a concluding section at the end. We begin by introducing some notations. Assuming n data points ( X i ,Y i ) n i =1 are observed from a high dimensional regression model where X i = ( X i 1 ,...,X i p ) T  X  R p is a p -dimensional design point, m : R p  X  R is an unknown smooth mean function. Here we assume m lies in a p -dimensional second order Sobolev ball with finite radius. In the sequel, we denote the response vector ( Y 1 ,...,Y n ) T by Y and the vector ( X 1 j ,...,X n j ) T by X j for 1  X  j  X  p .
 We assume m is functional sparse , i.e. there exists an index set S  X  X  1 ,...,p } , such that where | S | = r p and x S denotes the sub-vector of x with elements indexed by S .
 Sometimes, the function m can be assumed to have more structures to obtain a better estimation result. The most popular one is additivity assumption [10]. In this case, m decomposes into the sum of r univariate functions { m j } j  X  S : where each component function m j is assumed to lie in a second order Sobolev ball with finite radius so that each element in the space is smooth enough. For the sake of identifiability, we also assume E m j ( X j ) = 0 for j = 1 ,...,p , where the expectation is taken with respect to the marginal distribution of X j .
 Given the models in (2) or (3), we have two tasks: function estimation and variable selection . For the first task, we try to find an estimate is some function norm. For the second task, we try to find an estimate b S , which is an index set of variables, such that P b S = S  X  1 as n goes to infinity. if the true index set for the relevant variables is known, the backfitting algorithm can be directly applied to estimate equations in a function space. In particular, we denote the estimates on the j th variable X j to be b m vector R j = Y  X   X   X  P k 6 = j S j : R n  X  R n is a smoothing matrix, which only depends on X 1 ,...,X n but not on Y . Once b m j is updated, the algorithm holds it fixed and repeats this process by cycling through each variable until convergence. Under mild conditions on the smoothing matrices S 1 ,..., S p , the backfitting algorithm is a first order algorithm that guarantees to converge [5] and achieves the minimax rate of convergence as if only estimating a univariate function. However, for sparse learning problems, since the true index set is unknown, the backfitting algorithm no longer works due to the uncontrolled estimation variance.
 By extending the idea of the orthogonal matching pursuit to sparse additive models, we design a forward greedy algorithm called the additive forward regression (AFR), which only involves a few variables in each iteration. Under this framework, we only need to conduct the backfitting algorithm on a small set of variables. Thus the variance can be well controlled. The algorithm is described in Figure 1, where we use  X  X  ,  X  X  n to denote the inner product of two vectors.
 The algorithm uses an active set A to index the variables included in the model during each iteration and then performs a full optimization over all  X  X ctive X  variables via the backfitting algorithm. The main advantage of this algorithm is that during each iteration, the model inference is conducted in low dimensions and thus avoids the curse of dimensionality. The stopping criterion is controlled by a predefined parameter  X  which is equivalent to the regularization tuning parameter in convex regularization methods. Other stopping criteria, such as the maximum number of steps, can also be adopted. In practice, we always recommend to use data-dependent technique, such as cross-validation, to automatically tune this parameter.
 Moreover, the smoothing matrix S j can be fairly general, e.g. univariate local linear smoothers as described below, kernel smoothers or spline smoothers [21], etc. model to be additive. In this case, to find a good estimate To estimate the general multivariate mean function m ( x ) , one of the most popular methods is the local linear regression: given an evaluation point x = ( x 1 ,...,x p ) T , the estimate solution where K (  X  ) is a one dimensional kernel function and the kernel weight function in (4) is taken as a product kernel with the diagonal bandwidth matrix H 1 / 2 = diag { h 1 ,...,h p } . Such a problem can be re-casted as a standard weighted least squares regression. Therefore a closed-form solution to the the local linear estimate can be explicitly given by where e 1 = (1 , 0 ,..., 0) T is the first canonical vector in R p +1 and
W x = diag Here, S x is the local linear smoothing matrix. Note that if we constrain  X  x = 0 , then the local linear has been characterized in [8]: | p &gt; 10 .
 To handle the large p case, we again extend the idea of the orthogonal matching pursuit to this setting. For an index subset A  X  { 1 ,...,p } and the evaluation point x , the local linear smoother restricted on A is denoted as S ( A ) and where W ( A ) x is a diagonal matrix whose diagonal entries are the product of univariate kernels over the set A and X ( A ) x is a submatrix of X x that only contains the columns indexed by A . Given these definitions, the generalized forward regression (GFR) algorithm is described in Figure 2. Similar to AFR, GFR also uses an active set A to index the variables included in the model. Such mechanism allows all the statistical inference to be conducted only in low-dimensional spaces. The GFR algorithm using the multivariate local linear smoother can be computationally heavy for very high dimensional problems. However, GFR is a generic framework and can be equipped with arbitrary multivariate smoothers, e.g. kernel/Nearest Neighbor/spline smoothers. These smoothers lead to much better scalability. The only reason we use the local linear smoother as an illustrative example in this paper is due to its popularity and potential advantage on correcting the boundary bias. In this section, we provide the theoretical properties of the additive forward regression estimates using the spline smoother. Due to the asymptotic equivalence of the spline smoother and the local linear smoother [18], we deduce that these results should also hold for the local linear smoother. to implement AFR algorithm, the resulting estimator is consistent with a certain rate. When the underlying true component functions do not go to zeroes too fast, we also achieve variable selection consistency. Our analysis relies heavily on [3]. A similar analysis has also been reported in the technical report version of [16].
 Theorem 1. Assuming there exists some  X  &gt; 0 which can be arbitrarily large, such that p = O ( n  X  ) . For  X  j  X  { 1 ,...,p } , we assume m j lies in a second-order Sobolev ball with finite radius, and m =  X  + P p j =1 m j . For the additive forward regression algorithm using the spline smoother with a truncation rate at n 1 / 4 , after ( n/ log n ) 1 / 2 steps, we obtain that Furthermore, if we also assume min j  X  S k m j k =  X  log n n to infinity. Here, b S is the index set for nonzero component functions in The rate for k minimax rate O ( n  X  4 / 5 ) . This is mainly an artifact of our analysis instead of a drawback of the additive forward regression algorithm. In fact, if we perform a basis expansion for each component function to first cast the problem to be a finite dimensional linear model with group structure, under some more stringent smallest eigenvalue conditions on the augmented design as in [23], we can show that AFR using spline smoothers can actually achieves the minimax rate O ( n  X  4 / 5 ) up to a logarithmic factor. A detailed treatment will be reported in a follow up paper.
 Sketch of Proof : We first describe an algorithm called group orthogonal greedy algorithm (GOGA), which solves a noiseless function approximation problem in a direct-sum Hilbert space. AFR can then be viewed as an empirical realization of such an  X  X deal X  algorithm.
 GOGA is a group extension of the orthogonal greedy algorithm (OGA) in [3]. For j = 1 ,...,p , let H j be a Hilbert space of continuous functions with a Hamel basis D j . Then for a function m in the direct-sum Hilbert space H = H 1 + H 2 + ... + H p , we want to approximate m using the union of many truncated bases D = D 0 1  X  ...  X  X  0 p , where for all j , D 0 j  X  X  j .
 We equip an inner product  X  X  ,  X  X  on H :  X  f,g  X  H ,  X  f,g  X  = R f ( X ) g ( X ) dP X where P X is the marginal distribution for X . Let k X k be the norm induced by the inner product  X  X  ,  X  X  on H . GOGA begins by setting m (0) = 0 , and then recursively defines the approximant m ( k ) based on m ( k  X  1) to be the projection of r ( k  X  1) onto the truncated basis D 0 j , i.e. f ( k ) j = arg min g  X  X  0 onto the additive function space generated by A ( k ) = D 0 j (1) +  X  X  X  + D 0 j ( k ) : AFR using regression splines is exactly GOGA when there is no noise. For noisy samples, we replace the unknown function m by its n -dimensional output vector Y , and replace the inner product  X  X  ,  X  X  by  X  X  ,  X  X  n , which is defined as  X  f,g  X  n = 1 residual vector onto each dictionary D 0 j is replaced by the corresponding nonparametric smoothers. Considering any function m  X  H , we proceed in the same way as in [3], but replacing the OGA arguments in their analysis by those of GOGA. The desired results of the theorem follow from a simple argument on bounding the random random covering number of spline spaces. In this section, we present numerical results for AFR and GFR applied to both synthetic and real data. The main conclusion is that, in many cases, their performance on both function estimation and variable selection can clearly outperform those of LASSO, Foba, and SpAM. For all the re-ported experiments, we use local linear smoothers to implement AFR and GFR. The results for other smoothers, such as smoothing splines, are similar. Note that different bandwidth parameters will have big effects on the performances of local linear smoothers. Our experiments simply use the plug-in bandwidths according to [8] and set the bandwidth for each variable to be the same. For AFR, the bandwidth h is set to be 1 . 06 n  X  1 / 5 and for GFR, the bandwidth is varying over each iteration such that h = 1 . 06 n  X  1 / (4+ |A| ) , where |A| is the size of the current active set. For an estimate error (MSE), which is defined as MSE( we do not know the true function m ( x ) , we approximate the mean squared error using 5-fold cross-validation scores. 6.1 The Synthetic Data For the synthetic data experiments, we consider the compound symmetry covariance structure of the design matrix X  X  R n  X  p with n = 400 and p = 20 . Each dimension X j is generated according to where W 1 ,...,W p and U are i.i.d. sampled from Uniform(0,1). Therefore the correlation between X j and X k is t 2 / (1 + t 2 ) for j 6 = k . We assume the true regression functions have r = 4 relevant variables: To evaluate the variable selection performance of different methods, we generate 50 designs and 50 trials for each design. For each trial, we run the greedy forward algorithm r steps. If all the relevant variables are included in, the variable selection task for this trial is said to be successful. We report between covariates by varying the values of t .
 We adopt some synthetic examples as in [12] and define the following four functions: g 1 ( x ) = x , g 0 . 3 sin 2 (2  X x ) + 0 . 4 cos 3 (2  X x ) + 0 . 5 sin 3 (2  X x ) .
 The following four regression models are studied. The first model is linear; the second is additive; the third and forth are more complicated nonlinear models with at least two way interactions: Compared with LASSO, Foba, and SpAM, the estimation performance using MSE as evaluation criterion is presented in Figure 3. And Table 1 shows the rate of success for variable selection of these models with different correlations controlled by t .
 From Figure 3, we see that AFR and GFR methods provide very good estimates for the underlying true regression functions as compared to others. Firstly, LASSO and SpAM perform very poorly when the selected model is very sparse. This is because they are convex regularization based ap-proaches: to obtain a very sparse model, they induce very large estimation bias. On the other hand, the greedy pursuit based methods like Foba, AFR and GFR do not suffer from such a problem. Sec-ondly, when the true model is linear, all methods perform similarly. For the nonlinear true regression
Figure 3: Performance of the different algorithms on synthetic data: MSE versus sparsity level function, AFR, GFR and SpAM outperform LASSO and Foba. It is expectable since LASSO and Foba are based on linear assumptions. Furthermore, we notice that when the true model is addi-tive (Model 2) or nearly additive (Model 4), AFR performs the best. However, for the non-additive general multivariate regression function (Model 3), GFR performs the best. For all examples, when more and more irrelevant variables are included in the model, SpAM has a better generalization performance due to the regularization effect.
 The variable selection performances of different methods in Table 1 are very similar to their estima-tion performances. We observe that, when correlation parameter t becomes larger, the performances of all methods decrease. But SpAM is most sensitive to the correlation increase. In all models, the performance of SpAM can decrease more than 70% for the larger t ; in contrast, AFR and GFR are more robust to the increased correlation between different covariates. Another interesting observa-tion is on model 4. From the previous discussion, on this model, AFR achieves a better estimation performance. However, when comparing the variable selection performance, GFR is the best. This suggests that for nonparametric inference, the goals of estimation consistency and variable selection consistency might not be always coherent. Some tradeoffs might be needed to balance them. 6.2 The real data In this subsection, we compare five methods on three real datasets: Boston Housing , AutoMPG , and Ionosphere data set 1 . Boston Housing contains 556 data points, with 13 features; AutoMPG 392 data points (we delete those with missing values), with 7 features and Ionosphere 351 data points, with 34 features and the binary output. We treat Ionosphere as a regression problem although the response is binary. We run 10 times 5-fold cross validation on each dataset and plot the mean and standard deviation of MSE versus different sparsity levels in Figure 4.
 Figure 4: Performance of the different algorithms on real datasets: CV error versus sparsity level From Figure 4, since all the error bars are tiny, we deem all the results significant. On the Boston Housing and AutoMPG datasets, the generalization performances of AFR and GFR are clearly bet-performance of the greedy methods are much better than the convex regularization methods due to the much less bias being induced. On the Ionosphere data, we only need to run GFR up to 15 selected variables, since the generalization performance with 15 variables is already worse than the null model due to the curse of dimensionality. Both AFR and GFR on this dataset achieve the best performances when there are no more than 10 variables included; while SpAM achieves the best CV score with 25 variables. However, this is not to say that the true model is not sparse. The main reason that SpAM can achieve good generalization performance when many variables included is due to its regularization effect. We think the true model should be sparse but not additive. Similar trend among different methods has also appeared in Model 4 of previous synthetic datasets. We presented two new greedy algorithms for nonparametric regression with either additive mean functions or general multivariate regression functions. Both methods utilize the iterative forward stepwise strategy, which guarantees the model inference is always conducted in low dimensions in each iteration. These algorithms are very easy to implement and have good empirical performance on both simulated and real datasets.
 One thing worthy to note is: people sometimes criticize the forward greedy algorithms since they can never have the chance to correct the errors made in the early steps. This is especially true for high dimensional linear models, which motivates the outcome of some adaptive forward-backward procedures such as Foba [22]. We addressed a similar question: Whether a forward-backward pro-forward-backward procedures using the same way as in [22]. We conducted a comparative study to see whether the backward steps help or not. However, the backward step happens very rarely and the empirical performance is almost the same as the purely forward algorithm. This is very different from the linear model cases, where the backward step can be crucial. In summary, in the nonpara-metric settings, the backward ingredients will cost much more computational efforts with very tiny performance improvement. We will investigate more on this phenomenon in the near future. A very recent research strand is to learn nonlinear models by the multiple kernel learning machinery [1, 2], another future work is to compare our methods with the multiple kernel learning approach from both theoretical and computational perspectives.
 We thank John Lafferty, Larry Wasserman, Pradeep Ravikumar, and Jamie Carbonell for very help-ful discussions on this work. This research was supported in part by NSF grant CCF-0625879 and a Google Fellowship to Han Liu. [1] Francis Bach. Consistency of the group lasso and multiple kernel learning. Journal of Machine [2] Francis Bach. Exploring large feature spaces with hierarchical multiple kernel learning. In [3] Andrew R. Barron, Albert Cohen, Wolfgang Dahmen, and Ronald A. DeVore. Approximation [4] Peter B  X  uhlmann and Bin Yu. Sparse boosting. Journal of Machine Learning Research , 7:1001 X  [5] Andreas Buja, Trevor Hastie, and Robert Tibshirani. Linear smoothers and additive models. [6] Emmanuel Candes and Terence Tao. The dantzig selector: statistical estimation when p is [7] Scott Shaobing Chen, David L. Donoho, and Michael A. Saunders. Atomic decomposition by [8] Jianqing Fan and Ir ` ene Gijbels. Local polynomial modelling and its applications . Chapman [9] Jerome H. Friedman. Multivariate adaptive regression splines. The Annals of Statistics , 19:1 X  [10] Trevor Hastie and Robert Tibshirani. Generalized additive models . Chapman &amp; Hall Ltd., [11] John Lafferty and Larry Wasserman. Rodeo: Sparse, greedy nonparametric regression. The [12] Yi Lin and Hao Helen Zhang. Component selection and smoothing in multivariate nonpara-[13] Han Liu and Jian Zhang. On the estimation consistency of the group lasso and its applications. [14] S. Mallat and Z. Zhang. Matching pursuit with time-frequency dictionaries. IEEE Transactions [15] Lukas Meier, Sara van de Geer, and Peter B  X  uhlmann. High-dimensional additive modelling. [16] Pradeep Ravikumar, John Lafferty, Han Liu, and Larry Wasserman. Sparse additive models. [17] Pradeep Ravikumar, Han Liu, John Lafferty, and Larry Wasserman. Spam: Sparse additive [18] B. W. Silverman. Spline smoothing: The equivalent variable kernel method. The Annals of [19] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal [20] Joel A. Tropp. Greed is good: Algorithmic results for sparse approximation. IEEE Trans. [21] Grace Wahba. Spline models for observational data . SIAM [Society for Industrial and Applied [22] Tong Zhang. Adaptive forward-backward greedy algorithm for learning sparse representations. [23] Tong Zhang. On the consistency of feature selection usinggreedy least squares regression.
