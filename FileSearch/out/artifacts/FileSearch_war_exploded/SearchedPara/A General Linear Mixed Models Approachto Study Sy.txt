 Topic variance has a greater effect on performances than system variance but it cannot be controlled by system devel-opers who can only try to cope with it. On the other hand, system variance is important on its own, since it is what system developers may affect directly by changing system components and it determines the differences among sys-tems. In this paper, we face the problem of studying system variance in order to better understand how much system components contribute to overall performances. To this end, we propose a methodology based on General Linear Mixed Model (GLMM) to develop statistical models able to isolate system variance, component effects as well as their interac-tion by relying on a Grid of Points (GoP) containing all the combinations of analysed components. We apply the pro-posed methodology to the analysis of TREC Ad-hoc data in order to show how it works and discuss some interesting outcomes of this new kind of analysis. Finally, we extend the analysis to different evaluation measures, showing how they impact on the sources of variance.
The experimental results analysis is a core activity in In-formation Retrieval (IR) aimed at, firstly, understanding and improving system performances and, secondly, assess-ing our own experimental methods, such as robustness of experimental collection or properties of the evaluation mea-sures. When it comes to explaining system performances and differences between algorithms, it is commonly under-stood [10, 17, 23] that system performances can be broken down to a reasonable approximation as system performances = topic effect + system effect+ even though it is not always possible to estimate these effects separately, especially the interaction one.
It is well-known that topic variability is greater than sys-tem variability [23, 26]. Therefore, a lot of effort has been put in better understanding this source of variance [17] as well as in making IR systems more robust to it, e.g. [25, 28], basically trying to improve on the interaction effect. Nev-ertheless, with respect to an IR system, topic variance is a kind of  X  X xternal source X  of variation, which cannot be con-trolled by developers, but can only be taken into account to better deal with it.

On the other hand, system variance is a kind of  X  X nternal source X  of variation, since it is originated by the choice of sys-tem components, may be directly affected by developers by working on them, and represents the intrinsic differences be-tween algorithms. Its importance is witnessed by the wealth of research on how to compare systems performances in a re-liable and robust way [1, 2, 4, 9, 20 X 23, 27].

However, a limitation of the current experimental method-ology is that it allows us to evaluate IR systems only as a kind of  X  X lack-boxes X , without an understanding of how their different components interact with each other and contribute to the overall performances. In other terms, we consider sys-tem variance as a single monolithic contribution and we can-not break it down into the smaller pieces (the components) constituting an IR system.

In order to estimate the effects of the different compo-nents of an IR system, we develop a methodology, based on General Linear Mixed Model (GLMM) and ANalysis Of VAriance (ANOVA) [13, 18], which makes us of a Grid of Points (GoP) containing all the possible combinations of in-spected components. The proposed methodology allows us to break down the system effect into the contributions of stops lists, stemmers or n -grams and IR models, as well as to study their interaction.

We experimented on standard Text REtrieval Conference (TREC) Ad-hoc collections and produced a GoP by using the Terrier 1 open source IR system [12]. This gave us a very controlled experimental setting, which allowed us to system-atically fit our General Linear Model (GLM) and break down the system variance. Note that such a controlled experimen-tal setting is typically not available in evaluation campaigns, such as TREC, where participating systems do not consti-tute a systematic sampling of all the possible combinations of components and often are not even described in such a detail to know exactly what components have been used.
We applied the proposed methodology to TREC 5, 6, 7, and 8 Ad-hoc collections and we employed different mea-sures  X  AP, Precision at 10, RBP, nDCG@20, and ERR@20. This setup allows us not only to highlight how components contribute to the overall system variance but also to gain insights on how different evaluation measures impact on sys-tem and component variances.

The paper is organized as follows: Section 2 presents related work; Section 3 introduces our methodology; Sec-tion 4 experiments the proposed methodology; and, Sec-tion 5 draws conclusions and discusses future work.
The impossibility of testing a single component by set-ting it aside from the complete IR system is a long-standing and well-known problem in IR experimentation, as early re-marked by [16]. Component-based evaluation methodolo-gies [6 X 8] have tried to tackle this issue by providing tech-nical solutions for mixing different components without the need of building a whole IR system. However, even if these approaches allowed researchers to focus on the components of their own interest, they have not delivered yet estimates of the performance figures of each component.

The decomposition of performances into system and topic effects has been exploited by [1, 23] to analyze TREC data; [4] proposed model-based inference, using linear models and ANOVA, as an approach to multiple comparisons; [10] used multivariate linear models to compare non-deterministic IR systems among them and with deterministic ones. In all these cases, the goal is a more accurate comparison among systems rather than an analysis and breakdown of system variance per se. [17] applied GLMM to the study of per-topic variance by using simulated data to generate more replicates for each (topic, system) pair in order to estimate also the topic/system interaction effect; however, they did not use real data nor did focus on breaking down the system effect.
The idea of creating all the possible combinations of com-ponents has been proposed by [7], who noted that a sys-tematic series of experiments on standard collections would have created a GoP, where (ideally) all the combinations of retrieval methods and components are represented, allowing us to gain more insights about the effectiveness of the differ-ent components and their interaction; this would have called also for the identification of suitable baselines with respect to which all the comparisons have to be made. Even though [7] introduced the idea of a GoP and how it could have been central to the decomposition of system component perfor-mances, they did not come up with an full-fledged method-ology for analyzing such data and breaking down component performances, which is the contribution of the present work instead.

More recently, the proliferation of open source IR sys-tems [24] has greatly ameliorated the situation, allowing re-searchers to run systematic experiments more easily. This led the community to further investigate what reproducible baselines are [5, 11] and the  X  X pen-Source Information Re-trieval Reproducibility Challenge X  provided several of these baselines, putting some points in the ideal GoP mentioned above. We move a step forward with respect to [11] since we propose an actual methodology for exploiting such GoPs to decompose system performances and we rely on a much finer-grained grid, in terms of number of components and IR models experimented.
The goal of the proposed methodology is to decompose the effects of different components on the overall system perfor-mances. In particular, we are interested in investigating the effects of the following components: stop lists; Lexical Unit Generator (LUG) , namely stemmers or n -grams; IR models, such as the vector space or the probabilistic model.
We create a Grid of Points (GoP) on a standard exper-imental collection by running all the IR systems resulting from all the possible combinations of the considered com-ponents (stop list, LUG, IR model); we consider stemmers and n -grams as alternative LUG components, thus we do not consider IR systems using both stemmer and n -grams.
Given a performance measure, such as Average Precision (AP) , we produce a matrix Y , as the one shown in Figure 1, where each cell Y ij represents a measurement on topic t of the system s j . Note that the column average  X  i.e.,  X  is the performance mean over all topics for a given system, e.g. Mean Average Precision (MAP) ; the row average  X  i.e.,  X  i  X   X  is the performance mean over all systems for a given topic.
 A GLMM explains the variation of a dependent variable Y ( X  X ata X ) in terms of a controlled variation of independent variables ( X  X odel X ) in addition to a residual uncontrolled variation ( X  X rror X ).

The term  X  X eneral X  refers to the ability to accommodate distinctions on quantitative variables representing continu-ous measures (as in regression analysis) and categorical dis-tinctions representing groups or experimental conditions (as in ANOVA). In our case, we deal with categorical indepen-dent variables, as for example different types of stemmers, which constitute the levels of such categorical variable. The term  X  X inear X  indicates that the  X  X odel X  is expressed as a linear combination of factors, where the factors can be single independent variables or their combinations. In our case, we are interested both in single independent variables, i.e. the main effects of the different components alone, and their combinations, i.e. the interaction effects between compo-nents. The term  X  X ixed X  refers to the fact that some in-dependent variables are considered fixed effects  X  i.e. they have precisely defined levels, and inferences about its effect apply only to those levels  X  and some others are considered random effects  X  i.e. they describe a randomly and inde-pendently drawn set of levels that represent variation in a clearly defined wider population; a random factor is indi-cated by adding a single quote as superscript to the variable name. In our case, the different kinds of systems and com-ponents are fixed effects while topics are random effects.
The experimental design determines how you compute the model and how you estimate its parameters. In particular, it is possible to have independent measures designs where different subjects participate to different experimental con-ditions (factors) or repeated measures designs, where each subject participates to all the experimental conditions (fac-tors). In our case systems and their components are the experimental conditions (factors) while topics are the sub-jects and, since each topic is processed by each system, we have a repeated measure design.

One advantage of repeated measures designs is a reduction in error variance due to the greater similarity of the scores provided by the same subjects; in this way, variability in individual differences between subjects is removed from the error. Basically, a repeated measure design increases the statistical power for a fixed number of subjects or, in other terms, it allows us to reach a desired level of power with less subjects than those required in the independent measures design.

A final distinction is between crossed/factorial designs, where every level of one factor is measured in combination with every level of the other factors, and nested designs, where levels of a factor are grouped within each level of an-other nesting factor. In our case, we have a crossed/factorial design because in the generated GoP we experiment each possible combination of components. Figure 1: Single factor repeated measures design.

This design is the one typically used when ANOVA is ap-plied to the analysis of the system performances in a track of an evaluation campaign, as in [1, 23], where the subjects are the topics and the factors are the system runs. Basically, in this context ANOVA is used to determine which experi-mental condition dependent variable score means differ, i.e. which systems are significantly different from others.
In our case, we are interested also in a second aspect, i.e. to determine what proportion of variation in the depen-dent variable can be attributed to differences between spe-cific experimental groups or conditions, as defined by the independent variables. This turns into determining which proportion of variation is due to the topics and which one to the systems.
The full GLMM model for the one-way ANOVA with re-peated measures is: where: Y ij is the score of the i -th subject (topic) in the j -th factor (system);  X   X  X  is the grand mean;  X  i is the effect of the i -th subject  X  i =  X  i  X   X   X   X  X  where  X  i  X  is the mean of the i -th subject;  X  j is the effect of the j -th factor  X  j =  X  where  X   X  j is the mean of the j -th factor;  X  ij is the error committed by the model in predicting the score of the i -th subject in the j -th factor. It consists of a term (  X  X  ) which is the interaction between the i -th subject and the j -th factor 2 ; and, a term ij which is any additional error due to uncontrolled sources of variance.
We have the following estimators for the parameters of the model above:
We can write the model of equation (3.1) introducing the estimated parameters as Y ij =  X   X   X  X  +  X   X  i +  X   X  j +  X   X  ij which leads to the following decomposition of the effects
Y | {z }
From equation (3.2), we can compute the sum of squares (SS), degrees of freedom (DF), and mean squares (MS) as follows:
In order to determine if the factor effect is statistically significant, we compute the F statistics defined as: and compare it with the distribution F ( df the null hypothesis H 0 that there are not significant dif-ferences in order to estimate the probability (p-value) that F fact has been observed by chance. We can set a significance level  X  (typically  X  = 0 . 05) and, if p-value &lt;  X  , the factor effect is considered statistically significant.

As introduced above, we are not only interested in de-termining whether the factor effect is significant but also which proportion of the variance is due to it, that is we need to estimate its effect-size measure or Strength of As-sociation (SOA) . The SOA is a  X  X tandardized index and es-timates a parameter that is independent of sample size and quantifies the magnitude of the difference between popula-tions or the relationship between explanatory and response variables X  [15]. We use the  X   X  2  X  fact  X  SOA: which is an unbiased estimator of the variance components associated with the sources of variation in the design.
The common rule of thumb [14] when classifying  X   X  2  X  fact  X  effect size is: 0.14 and above is a large effect, 0.06 X 0.14 is a medium effect, and 0.01 X 0.06 is a small effect.  X   X  values could happen to be negative and in such cases they are considered as zero.

When you conduct experiments, two types of error may happen. A Type 1 error occurs when a true null hypothesis is rejected and the significance level  X  is the probability of committing a Type 1 error. A Type 2 error occurs when a false null hypothesis is accepted and it is concerned with the capability of the conducted experiment to actually de-tect the effect under examination. Type 2 errors are often overlooked because if they occur, although a real effect is missed, no misdirection occurs and further experimentation is very likely to reveal the effect.

The power is the probability of correctly rejecting a false null hypothesis when an experimental hypothesis is true where  X  (typically  X  = 0 . 2) is the Type 2 error rate.
To determine the power of an experiment, we compute the effect size parameter: and we compare it with its tabulated values for a given Type 1 error rate  X  to determine  X  .
While single factor designs manipulate a single variable, factorial designs take into account two or more factors as well as their interaction. As an example a two factors re-peated measure design can be defined extending the design described above, where we manipulated one factor (A), by adding an additional factor (B) and the interaction between them (AB).

We can therefore define a three factors design where we manipulate factors A, B and C which correspond to the stop lists, the LUG and the IR models respectively; with this design we can also study the interaction between component pairs as well as the third order interaction between them.
In Figure 2 we can see a table which extends to three factors the design presented in Figure 1 for a single fac-tor. We can see that the systems are now decomposed into three main constituents: (i) factor A (stop lists) with p levels where, for instance, A 1 corresponds to the absence of a stop list, A 2 to the indri stop list, A 3 to the terrier stop list and so on; (ii) factor B (LUG) with q levels where B 1 corresponds to the absence of a LUG, B 2 to the Porter stemmer, B 3 to the Krovetz stemmer and so on; (iii) factor C (IR models) with r levels where C 1 corresponds to BM25, C 2 to TF*IDF and so on. We call this design a p  X  q  X  r factorial design. Each cell of the table in Figure 2, say Y npqr , reports the mea-surement (e.g., AP) on topic T 0 n , for the system composed by the stop list A p , the LUG B q and IR model C r .
The full GLMM model for the described factorial ANOVA for repeated measures with three fixed factors ( A , B , C ) and a random factor ( T 0 ) is: where: Y ijkl is the score of the i -th subject in the j -th, k -th, and l -th factors;  X   X  X  X  X  is the grand mean;  X  i is the ef-fect of the i -th subject  X  i =  X  i  X  X  X   X   X   X  X  X  X  where  X  mean of the i -th subject;  X  j =  X   X  j  X  X   X   X   X  X  X  X  is the effect of the j -th factor, where  X   X  j  X  X  is the mean of the j -th factor;  X  k =  X   X  X  k  X   X   X   X  X  X  X  is the effect of the k -th factor, where  X  is the mean of the k -th factor; and,  X  l =  X   X  X  X  l  X   X   X  X  X  X  effect of the l -th factor where  X   X  X  X  l is the mean of the l -th factor;  X  ijkl is the error committed by the model in predict-ing the score of the i -th subject in the three factors j,k,l . It consists of all the interaction terms between the random subjects and the fixed factors, such as (  X  X  ) ij , (  X  X  ) so on, plus the error ijkl which is any additional error due to uncontrolled sources of variance. As in the single factor design to calculate interaction effects with the subjects, you need to have replicates; when there is only one score per subject per factor the factor ijkl cannot be separated from the interaction effects with the random subjects. B X X
X
The estimators of the main effects can be derived by exten-sion from those of the single factor design; for instance, the and its estimator is  X   X  k =  X   X   X  X  k  X   X   X   X   X  X  X  X  .
The estimators of the interaction factors are calculated as follows, let us consider (  X  X  ) jk :  X   X 
Similarly, we calculate the estimators for all the other in-teraction factors  X  i.e. c  X  X  jl and c  X  X  kl ; d  X  X  X  jkl is calculated by extending equation (3.7):
In this design the error  X  ijkl = Y ijkl  X   X  Y ijkl contains the variance not explained by the main and interaction effects discussed above and it is composed by all the interactions of the subjects  X  j with the other factors in the model in addition to the uncontrolled sources of variance.

The sum of squares, mean squares and degrees of freedom of the main effects can be derived by extension form those of the one factor design. As an example, the degrees of freedom of factor A are p  X  1 and its sum of squares is:
As an example of the computations for the interaction terms, we consider the term A  X  B whose degrees of freedom are ( p  X  1)( q  X  1) and whose sum of squares is:
As in the single factor design case, the mean squares of a factor (both main and interaction) are calculated by divid-ing its sum of squares by its degrees of freedom, the F-test is calculated with equation (3.3), the SOA measure with equation (3.4), and the power with equation (3.5).
We considered three main components of an IR system: stop list, LUG and IR model. We selected a set of alternative implementations of each component and by using the Terrier open source system we created a run for each system defined by combining the available components in all possible ways. The components we selected are: stop list: nostop, indri, lucene, smart, terrier; LUG: nolug, weak Porter, Porter, Krovetz, Lovins, 4grams, model: BB2, BM25, DFRBM25, DFRee, DLH, DLH13,
Note that the stemmers and n -grams of the LUG com-ponent are used as alternatives, this means that we end up with two distinct groups of runs, one using the stemmers and one using the n -grams; the nolug component is common to both these groups. The group using the stemmers defines a 5  X  5  X  16 factorial design with a grid of points consisting of 400 runs; the group using the n -grams defines a 5  X  3  X  16 factorial design with a grid of points consisting of 240 runs. Table 1: Single factor, ANOVA table for TREC 08 (stemmer group) using AP.

We conducted single factor and three-factors ANOVA tests for both the groups on TREC 05, 06, and 08 collections, and by employing the following five measures: AP, P@10, nDCG@20, RBP and ERR@20. All the test collections are composed by 50 different topics and have binary relevance judgments; the corpus of TREC 05 is the TIPSTER disk 2 and 4 counting 525K documents, the corpus of TREC 06 is TIPSTER disk 4 and 5 counting 556K documents and the corpus of TREC 07 and 08 is the TIPSTER disk 4 and disk 5 (minus Congressional Record) counting 528K documents.
To ease reproducibility, the code for running the experi-ments is available at: http://gridofpoints.dei.unipd.it/.
We conducted 40 single factor ANOVA tests (4 collections  X  5 measures  X  2 run groups), so for space reasons we cannot report all the result; as an example, Table 1 reports the synthesis data of the ANOVA test for TREC 08 using the stemmer group of runs measured with AP.

From the sum of squares (SS) and the mean squares (MS), we can see that topics explain a large portion of the total variance. Nonetheless, the effect of the IR systems is statis-tically significant (p-value 0). We can also see that the sum of squares of the error is not negligible since it contains both the variance of the unexplained topics/systems interaction effect and the the other uncontrolled sources of variance. From this table we can calculate the statistical power of the experiment, which is 1 with a Type 1 error probabil-ity  X  = 0 . 05, indicating that we are observing effects in a reliable way.

Table 2 reports the  X   X  2  X  system  X  SOA measure and the p-value of the ANOVA test for the single factor models on all the test collections for all the considered evaluation mea-sures. The  X  X UG X  column indicates the runs group we are considering (stemmers or n -grams). This table shows that despite the high variance of the topics, the system effect sizes are generally large and this is consistent across all the collections and measures. Moreover, system effect sizes of stemmer runs group systems are large ( &gt; 0 . 14) for all the collections and measures with the solely exception of AP for TREC 05. Whereas, for the n -grams runs group we can see that the system effect sizes are consistently smaller than those of the stemmer group; this, supports the obser-vation that  X  X or English, n -grams indexing has no strong impact X  [3].

Table 2 shows that measures impact on the amount of variance explained by the system effect. Generally, system effect sizes are higher when nDCG@20 is used, followed by RBP, P@10, AP and ERR@20. This could be related to two characteristics of the measures: their discriminative power and their user model. Indeed, if a measure is less discrimi-native than another one, it could be able to grasp less vari-ance in the system effect; on the other hand, different user models mean looking at (very) different angles of system performances and this can change the explained variance.
To explore a bit this hypothesis, in Table 3 we report the discriminative power of the five considered measures over the test collections calculated by employing the paired bootstrap test defined in [19]. We can see that there is some agree-ment between the system effect sizes for a measure and its discriminative power; for instance, ERR@20 explains less system variance than the other measures and this can be explained by its discriminative power which is the lowest amongst all measures; similarly, RBP and nDCG@20 have both comparable discriminative power and close system ef-fect sizes. The main exception is AP which typically has the highest discriminative power but the smallest system effect size; this could be due to the user model behind AP, which is quite different from the one of the other measures and may counterbalance the higher discriminative power leading to a final lower system effect size.
In Table 4 we report the ANOVA table of a three factors test for the stemmer group of runs on TREC 08 measured with AP.

We can see that the sum of squares of the topics is the same as the one determined with the single factor design, as well as the error and the total sum of squares. The main dif-ference with the one factor design is that the variance of the systems is now decomposed into three main effects (stop list, stemmer and IR model) and four interaction effects. In this case all the main effects are statistically significant meaning that they have a role in explaining systems variance; in par-ticular, the stop list explains more variance than the model and the stemmer is the component with the lowest impact in this design. Amongst the interaction effects, only the sto-plist*model effect is significant explaining a tangible portion of the systems variance. The statistical power for the main effects is 0 . 97 for the stop list, 0 . 66 for the stemmer and 0 . 99 for the model with a Type 1 error probability  X  = 0 . 05.
Table 5 reports the estimated  X  2 SoA for all the main and interaction effects and the p-values for all the ANOVA three-way tests we conducted; from this this table we can see that main and interaction effect sizes are consistent across the different collections.

Analyzing the main effect sizes reported in Table 5 we can see that for the stemmer group of runs the stop list has always a higher  X   X  2 than the IR model and the stemmer and, with the solely exceptions of TREC 05 for AP and ERR@20, the stop list has a medium effect size. Whereas, n -grams tend to reduce the stop list effect and to increase the IR model one; this can be also seen from the n -grams*model interaction effect which is small but statistically significant, differently from the stemmer*model effect which is never significant.

These observations cast a light on the importance of lin-guistic pre-processing and linguistic resources, given that the role of the stop list is significant in an IR system as well as choosing between stemmers or n -grams. We can further an-alyze these aspects by looking at Figure 3; the plot on the left reports the main effects for the TREC 08 stemmer group case and we show the marginal means (response means) de-scribed in Section 3.2 for the effect under investigation on the y-axis and the various components on the x-axis.
From the first plot we see that the presence or absence of a stop list affects the system performances because the line connecting  X  X o stop X  and  X  X ndri X  is not horizontal, whereas Table 3: Discriminative power of the evaluation measures on TREC 05, TREC 06, TREC 07 and TREC 08 for the stemmers and n -grams groups.
 Table 4: Three factor, ANOVA table for TREC 08 (stemmer group) using AP.
 the lines connecting the different stop lists have much lower slope. In particular, we see that the choice of the stop list does not make a big difference with respect to use or not use a stop list; this can be further explored looking at the Tukey HSD test plot on the upper-right corner of the figure (in blue the selected component; in grey the components in the same group, i.e. not significantly different; in red the components in a different group, i.e. significantly different), where we can see that there are no significant differences between the  X  X ndri X ,  X  X mart X  and  X  X errier X  stop lists, whereas the  X  X ucene X  stop list (which is composed by 15 words) is significantly different from the other three.

The main effect of the stemmer is always significant even though its size is quite small; nevertheless, the central plot of Figure 3 shows that there is a tangible difference between systems using or not using a stemmer. This can be seen also from the Tukey HSD test plot on the right; in particular, we can observe that there is no significant difference between the Porter and the Krovetz stemmer which are the stemmers with the highest impact on variance followed by the weak Porter and the Lovins ones.

Lastly, the plot on the right of Figure 3 reports the main effects of the IR models: they behave differently, as shown by several lines with high slopes, but the corresponding Tukey HSD shows that a many models are not significantly differ-ent one from the other. This can explain why the IR models effects are statistically significant but their effect sizes are not large.

For all the collections, consistently across the measures and both for the stemmer and the n -grams group, the higher effect size is reported by the stop list*model interaction effect which is always of medium or large size. This effect shows us that the variance of the systems is explained for the bigger part by the stop list and the model components. For the stemmer group of TREC 08, this can be seen in the plots on the upper-right and lower-left corners of Figure 4 where the lines of the interaction between the stop lists and the models intersect quite often. Indeed, the interaction plots show how the relationship between one factor and a response depends on the value of the second factor. These plots display means for the levels of one factor on the x-axis and a separate line for each level of another factor; if two lines (or segments) are parallel then no interaction occurs, if the lines are not parallel then an interaction occurs and the more nonparallel the lines are, the greater the strength of the interaction.
The stop list*stemmer interaction effects are always not significant as we can see from the p-values of Table 5 and the interaction plots in the upper-left part of Figure 4 where the line segments are parallel. A very similar trend can be observed for the stemmer*model interaction effect.

It is interesting to note that the second order interactions for the n -grams group are all statistically significant and that, in particular, we can see that n -grams, differently than the stemmers, have a bigger effect on the stop list than on the IR model.

We observe that different measures see the stop lists in a comparable way in terms of effect size and this is consistent with what we have seen in the one factor analysis. This is valid also for the stemmer, with the exception of ERR@20 for which it has an almost negligible effect size even though it is statistically significant. In Table 5 we can see that AP and ERR@20 weight the effects in a similar way as it happened in the single factor analysis reported in Table 2. For the n -grams group all the measures are comparable and ERR@20 is not as low as it happens for the stemmers.
Lastly, we can see that the third order interaction are never significant.
In this paper we faced the issue of how system variance contributes to the overall performances and how to break it down into some of the main components constituting an IR system. To this end, we developed an analysis method-ology consisting of two elements: a Grid of Points (GoP) created on standard experimental collections, where all the combinations of system components under examination are considered; and, a GLMM model to decompose the contri-bution of these components to the overall system variance, paired with some graphical tools for easily assessing the main and interaction effects.  X  effect sizes are in bold; not significant effects are highlighted.
We conducted a thorough experimentation on TREC col-lections and used different evaluation measures to show how the proposed approach works and to gain insights on the con-sidered components, i.e. stop lists, stemmers and n -grams, and IR models.

We found that the most prominent effects are those of stop lists and IR models, as well as their interactions, while stemmers and n -grams play a smaller role. Moreover, we have seen that stemmers produce more variation on system performances than n -grams. Overall, this highlights impor-tance of linguistic resources.

Finally, measures explain system and component effects differently one from the other and not all the measures seem to be suitable for all the cases as it happens for ERR@20 which almost does not detect the stemmer effect. These insights can be useful to understand where to invest effort and resources for improving components, since they give us an idea of the actual impact of a family of components on the overall performances.

As far as future work is concerned, we plan to extend the proposed methodology in order to be able to capture also interaction between topics/systems and topics/components. Indeed, to estimate interaction effects, more replicates would be needed for each (topic, system) pair, as [17] simulated, and they are not possible in the present settings, since run-ning more than once the same system on the same topics produces exactly the same results.

Moreover, we plan to further investigate the impact of the measures on the determination of effect sizes. A possible approach could be to conduct a four-factor analysis, using measures as additional factor. However, even if the measure scores are normalized in the range [0 , 1], they do not mean the exactly the same thing, i.e. AP = 0 . 20 is not exactly the same as ERR = 0 . 20 because of their different user mod-els. A possibility for smoothing these differences and make the scores more directly comparable could be to normalize them by the maximum value achieved on the dataset, thus reasoning in term of ratios.

Lastly, an open challenge is how to run this kind of analy-sis on the systems which participated to past TREC editions. A first obstacle is that often there is no precise description of all the components used in these systems and so their metadata should be enriched in the way we suggested in [5]. A second obstacle is that the GoP would be very sparse and many combinations would be missing; therefore, we would need to rely on unbalanced GLMM and, probably, to con-sider the components as random factors.
