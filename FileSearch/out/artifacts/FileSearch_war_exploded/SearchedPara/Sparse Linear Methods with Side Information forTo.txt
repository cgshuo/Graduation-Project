 The increasing amount of side information associated with the items in E-commerce applications has provided a very rich source of information that, once properly exploited and incorporated, can significantly improve the performance of the conventional recommender systems. This paper focuses on developing effective algorithms that utilize item side in-formation for top-N recommender systems. A set of sparse linear methods with side information ( SSLIM ) is proposed, which involve a regularized optimization process to learn a sparse aggregation coefficient matrix based on both user-item purchase profiles and item side information. This ag-gregation coefficient matrix is used within an item-based rec-ommendation framework to generate recommendations for the users. Our experimental results demonstrate that SSLIM outperforms other methods in effectively utilizing side infor-mation and achieving performance improvement.
 H.4.m [ Information Systems ]: Miscellaneous; J.7 [ Computer Applications ]: Computers in other systems X  Consumer products Recommender system, Sparse Linear Methods, Side infor-mation
Top-N recommender systems have been widely used in E-commerce applications to recommend ranked lists of items so as to help the users in identifying the items that best fit their personal tastes. Over the years, various algorithms for top-N recommendation have been developed [12]. The con-ventional top-N recommendation algorithms primarily fo-cus on utilizing user-item purchase profiles to generate rec-ommendations. Such algorithms can be categorized into two classes: collaborative filtering methods and model-based methods. Collaborative filtering methods typically build neighborhood for each user/item by considering the similar-ities of the purchase patterns among users/items from their profiles, and then recommend new items from the neigh-borhood. Model-based methods learn to explain the user-item purchase patterns using a specific model. For instance, the most popular matrix factorization ( MF ) methods present users and items in a common latent space such that the user-item purchase patterns can be explained by the user-item similarities in the space. Recently, a sparse linear method ( SLIM ) [10] has been developed that leverages the advan-tages of the above two classes of methods and achieves both better prediction accuracy and run-time performance than the state-of-the-art methods.

With the increased availability of additional information associated with the items (e.g., product reviews, movie plots, etc), referred to as side information , there is a greater in-terest in taking advantage of such information to improve the quality of conventional top-N recommender systems. As a result, a number of approaches have been developed from Machine Learning (ML) and Information Retrieval (IR) com-munities for incorporating side information. Such approaches include hybrid methods [5], matrix/tensor factorization [14, 8], and other regression methods [1].

In this paper, we propose a set of S parse L inear M ethods that utilize the item S ide information ( SSLIM ) for top-N recommendation. These methods include collective SLIM ( cSLIM ), relaxed collective SLIM ( rcSLIM ), side information induced SLIM ( fSLIM ) and side information induced double SLIM ( f2SLIM ). The key idea behind these methods is to learn linear models that are constrained and/or informed by the relations between the item side information and the user-item purchase profiles so as to achieve better recom-mendation performance. We conduct a comprehensive set of experiments on various datasets from different real ap-plications. The results show that SSLIM produces better recommendations than the state-of-the-art methods.

The rest of this paper is organized as follows. In Section 2, a brief review on related work is presented. In Section 3, the definitions and notations are provided. In Section 4 and Section 5, the methods are described. In Section 6, the ma-terials used for experiments are presented. In Section 7, the results are presented. Finally in Section 8 are the discussions and conclusions.
Various methods have been developed to incorporate side information in recommender systems. Most of these meth-ods have been developed in the context of the rating predic-tion problem, whereas the top-N recommendation problem has received less attention. In the rest of this section we review some of the best performing schemes for both the rating prediction and top-N recommendation problems.
The first category of these methods is based on latent fac-tor models. In [14], Singh et al proposed the collective ma-trix factorization method for both rating prediction and top-N recommendation, which collectively factorizes user-item purchase profile matrix and item-feature content matrix into a common latent space such that the two types of informa-tion are leveraged via common the item factors. Agarwal et al [1] proposed regression-based latent factor models for rating prediction, which use features for factor estimation. In their method, the user and item latent factors are es-timated through independent regression on user and item features, and the recommendation is calculated from a mul-tiplicative function on user and item factors. Yang et al [16] developed a joint friendship and interest propagation model for top-N recommendation, in which the user-item interest network and the user-user friendship network (side infor-mation on users) are jointly modeled through latent user and item factors. User factors are shared by the interest network approximation component and the friendship net-work approximation component so as to enable information propagation. They demonstrated the their model is a gener-alization of Singh et al [14], Koren [9] and Agarwal et al [1].
Methods using tensor factorization ( TF ) have also gained popularity. Karatzoglou et al [8] considered the user-item-feature relation as a tensor, and they proposed to use regu-larized TF to model the relations between the three sets of entities for rating prediction. TF can be considered as a gen-eralization of MF , in which the relations among all the modes (i.e., users, items and features) are jointly learned. Rendle et al [11] also treated user-item-feature as a tensor, and they factorized all pairwise interactions in the tensor (i.e, items vs users, items vs context features, users vs context features) rather than the entire tensor for rating prediction.
Another category of algorithms that utilize side infor-mation is based on networks. Gunawardana et al [6] pro-posed unified Boltzmann machines for top-N prediction, in which user-item profile information and side information are treated as homogeneous features, and interaction weights be-tween such features and user actions are learned in a coher-ent manner so as to reflect how well such features can predict user actions. Campos et al [3] combined content-based and collaborative-filtering based recommendations with Bayesian networks, which are composed of item nodes, user nodes and item feature nodes. During predictions, content information is propagated from purchased items to non-purchased items via feature nodes, and purchase information is propagated from other users to the user of concern via item nodes. Then the two parts are combined to make recommendations.
In this paper, the symbols u , t , and f ( | f | = l ) will be used to denote the users, items and item side information vectors, respectively. Individual users and items will be denoted us-ing different subscripts (i.e., u i , t j ). The side information vector for item t j will be denoted by f j . The size of user set and item set are denoted by m and n , respectively.
The user-item purchase profile is represented by a binary m  X  n matrix M , in which the ( i,j ) entry (denoted by m is 1 if user u i has ever purchased item t j , otherwise it is marked as 0. The i -th row of M , denoted by m T i , represents the purchase profile of user u i on all items. The j -th column of M , denoted by m j , represents the purchase profile of item t j from all users. The side information on all items is represented by an l  X  n matrix F . The j -th column of F represents the side information vector of item t j (i.e., f
All vectors (e.g., m T i and f j ) are represented by bold lower-case letters and all matrices (e.g., M and F ) are represented by upper-case letters. Row vectors are represented by having the transpose supscript T , otherwise by default they are col-umn vectors. Approximation relation is denoted using  X  and approximation value is denoted by heading a  X  head. The matrix/vector notations are used instead of user/item/side information if no ambiguity is raised.
In this paper, we focus on incorporating item side informa-tion within the Sparse LInear Method ( SLIM ) that we pro-posed previously [10]. In SLIM , the recommendation score on an urn-purchased item t j of a user u i (denoted by  X  m is calculated as a sparse aggregation of the items that have been purchased by u i , that is, where m ij = 0 and s j is a size-n sparse vector of aggregation coefficients. Thus, the model can be presented as where S is an n  X  n sparse matrix of aggregation coefficients, whose j -th column is s j as in Equation 1, and each row  X  m  X  M = MS represents the recommendation scores on all items for user u i . The top-N recommendations for u i are obtained by sorting u i  X  X  non-purchased items based on their scores in  X  m i in decreasing order and recommending the top-N items.
The SLIM method views the purchase activity of user u i on item t j in M (i.e., m ij ) as the ground-truth item rec-ommendation score. It learns the n  X  n sparse matrix S in Equation 2 as the minimizer for the following regularized optimization problem: where k S k 1 = P n i =1 P n j =1 | s ij | is the entry-wise ` S , and k X k F is the matrix Frobenius norm. In Equation 3, MS is the estimated matrix of recommendation scores (i.e.,  X  M ) by the sparse linear method as in Equation 2. The non-negativity constraint is applied on S such that the learned S corresponds to positive aggregations over items. The con-straint diag( S ) = 0 is also applied so as to avoid trivial solutions (i.e., the optimal S is an identity matrix such that an item always recommends itself). In addition, the con-straint diag( S ) = 0 ensures that m ij is not used to compute  X  m ij . In order to learn a sparse S , SLIM introduces the ` norm of S as a regularizer in Equation 3. It is well known that ` 1 -norm regularization introduces sparsity into the so-lutions [15]. The matrix S learned by SLIM is referred to as SLIM  X  X  aggregation coefficient matrix. Extensive experi-ments in [10] have shown that SLIM outperforms the state-of-the-art top-N recommendation methods.
SLIM provides a general framework in which only the ag-gregation coefficient matrix S is needed for efficient top-N recommendations, and this matrix is learned from the user-item purchase profiles. In this section, we present four differ-ent extensions of SLIM that are designed to incorporate side information about the items in order to further improve the quality of the recommendations.
The first approach assumes that there exist correlations between users X  co-purchase behaviors on two items and the similarity of the two items X  intrinsic properties encoded in their side information. In order to enforce such correlations, this approach imposes the additional requirement that both the user-item purchase profile matrix M and the item side information matrix F should be reproduced by the same sparse linear aggregation. That is, in addition to satisfying M  X  MS , the coefficient matrix S should also satisfy This is achieved by learning the aggregation coefficient ma-trix S as the minimizer to the following optimization prob-lem: where k F  X  F S k 2 F measures how well the aggregation coeffi-cient matrix S fits the side information. The parameter  X  is used to control the relative importance of the user-item purchase information M and the item side information F when they are used to learn S . Note that in this method, the side information is actually used to regularize the orig-inal SLIM method (i.e., via adding the regularization term k F  X  F S k 2 F into Equation 3). The recommendations are generated in exactly the same way as in SLIM . That is, the recommendation score for user u i on item t j is calculated as  X  m ij = m T i s j . Since the matrix S is learned from both M and F collectively by using F to regularize the original SLIM method, this approach is referred to as c ollective SLIM and denoted by cSLIM .

The solution to the optimization problem in Equation 5 is identical to the solution of an optimization problem in the same form as in Equation 3 with M in Equation 3 replaced by M 0 = [ M,
The second approach also tries to reproduce the item side information using a sparse linear method as in cSLIM , but it uses an alternative approach for achieving this. Specifically, it uses an aggregation coefficient matrix Q to reproduce F as where Q is not necessarily identical to S as in Equation 2. Thus, this method is a relaxation from cSLIM . However, the two aggregation coefficient matrices S and Q are tied by requiring that Q should not be significantly different from S (i.e., S  X  Q ). The matrix S and the matrix Q in Equation 6 are learned as the minimizers of the following optimization problem: where the parameter  X  1 controls how much S and Q are al-lowed to be different from each other. Similar to cSLIM , this method regularizes the original SLIM using item side infor-and  X  1 2 k S  X  Q k 2 F and the recommendations are generated in the same way as in SLIM . Since this method is a relaxation from cSLIM , it is refereed to as r elaxed c ollective SLIM and denoted by rcSLIM .

The optimization problem in Equation 7 can be solved via an approach alternating on solving S and Q . In each iteration, one variable is fixed and the problem becomes a regularized optimization problem with respect to the other variable, and it can be solved using a similar approach of stacking matrices as in Section 5.1. The solution of cSLIM is used as the initial value of S .
An alternative way to learn the aggregation coefficient matrix S of SLIM is to represent S as a function in the item feature space and thus it captures the feature-based relations of the items. One option of achieving this is to use the item-item similarity matrix calculated as FF T , that is, the ag-gregation coefficient from one item to another is calculated as the dot-product of their feature vectors (i.e., item-item feature similarity). However, in this way, the aggregation coefficient matrix is not customized to the user-item pur-chase profiles M at all, and thus a SLIM with such aggrega-tion coefficient matrix can fit M very poorly. Another way is to learn a weighting matrix W such that the aggregation coefficient value s ij can be represented as a linear combina-tion of item t i  X  X  feature f i weighted by item t j  X  X  personalized weighting vector w j over individual item features, that is, s ij = f T i w j and w j is W  X  X  j -th column. In this way, the coefficient matrix S can be represented as a weighted linear combination of the item features F using W , that is,
Such weighting matrix W can be learned as the minimizer of the following optimization problem: where D = diag(diag( F T W )) is a diagonal matrix with the corresponding diagonal values from F T W . D is subtracted from F T W so as to ensure that m ij is not used to compute  X  m ij , and this is equivalent to the constraint diag( S ) = 0 in Equation 3. In this method, the recommendation score for user u i on item t j is calculated as  X  m ij = m T i where w j and d j is the j -th column of W and D , respec-tively. Since this method explicitly specifies the aggregation coefficient matrix S as a function of the item side informa-tion F , it is referred to as side in f ormation induced SLIM and denoted by fSLIM .

The optimal solution to the optimization problem in Equa-tion 9 is W  X  = [ w 1 , w 2 ,  X  X  X  , w j ,  X  X  X  , w n ], where w optimal solution to the following problem: minimize subject to w j  X  0 , where c p = P n k =1 f pk , and F  X  j is a matrix with F  X  X  j -th column set to 0.
SLIM and fSLIM have their own advantages. SLIM learns the aggregation coefficient matrix S purely from purchase profiles such that it better fits the user-item purchase infor-mation. fSLIM forces the aggregation coefficient matrix S to be expressed in the item feature space and therefore it cap-tures useful information from the item features. SLIM and fSLIM can be coupled within one method so as to leverage both their advantages and better learn from purchase pro-files and side information concurrently. One way to combine SLIM and fSLIM is to have the user-item purchase profile M reproduced by both SLIM and fSLIM as where the S and W matrices can be learned as the minimiz-ers of the following optimization problem: minimize subject to S  X  0 ,W  X  0 , In this method, the recommendation score for user u item t j is calculated as  X  m ij = m T i s j + m T i ( F w j and d j is the j -th column of W and D , respectively. This method is a combination of SLIM and fSLIM and thus it is refereed as side in f ormation reduced double SLIM and denoted by f2SLIM .

That the optimal solution of W in the problem in Equa-tion 11 is identical to the first l rows of the optimal solution W 0 to the problem in Equation 9 with F replaced by [ F,I ] where I is an n  X  n identity matrix, and D 0 = diag(( F 0 whereas the optimal S is the last n rows of W 0 .
We evaluated the performance of different methods on the following real datasets: ML100K , NF , CrossRef , Lib , BBY , and Yelp , whose characteristics are summarized in Table 1.
ML100K The ML100K dataset corresponds to movie rat-ings and was obtained from the MovieLens research project. The movie plots were fetched from the IMDb database and the words that appear in at least 5 plots are used as the movie side information.
 NF The NF dataset is a subset extracted from the Netflix Prize dataset. The item side information was generated as in the ML100K dataset. Only the movies that were rated by 10-30 users were selected.

CrossRef The CrossRef dataset was obtained from cross-ref.org, and contains scientific articles and lists of article ci-tations. All the articles (i.e., references) that have DOI links and are cited by at least 50 other articles were first selected. Then the articles which cite more than 3 of such references were selected. In this way, an article-reference dataset is constructed, in which the articles (the references) are anal-ogous to the users (the items). The words in the reference titles are used as side information. The top-N recommenda-tion on CrossRef dataset becomes a task to recommend a reference for a certain article.
 Lib The Lib dataset was obtained from the University of Minnesota libraries, and contains the library users and their viewed articles. From the entire library records, the users who viewed at least 5 different articles and the articles that were viewed by at least 10 users were collectively selected to construct an user-article matrix. The words in the article titles are used as the article side information. The top-N recommendation on Lib is to recommend an article to a user.

BBY The BBY dataset is a subset of the BestBuy user-product rating and review dataset from BestBuy website ( https://developer.bestbuy.com/documentation/archives ). The products that were reviewed by at least 5 users and the users who reviewed at least 2 such products were collectively selected so as to construct the dataset. The side information for each item was the text of all the reviews of that item.
Yelp The Yelp dataset is a subset of the academic version of Yelp user-business rating and review dataset downloaded from Yelp ( http://www.yelp.com/academic dataset ). The users who reviewed at least 3 businesses and the corresponding businesses were selected to construct the dataset. The side information for each item was constructed from the reviews in a way similar to the BBY dataset.
 For the original rating datasets (i.e., ML100K , NF , BBY , Yelp ), the multivariate rating values were converted to 1 X  X .
We applied 5-time Leave-One-Out cross validation to eval-uate the performance of different methods. In each run, each of the datasets is split into a training set and a testing set by randomly selecting one of the non-zero entries of each user and placing it into the testing set. The evaluation is conducted by comparing the size-N (by default N = 10) recommendation list for each user and the item of that user in the testing set.

The recommendation quality is measured by the Hit Rate (ZR) and the Average Reciprocal Hit-Rank (ARHR) [4]. ZR is defined as follows, where #users is the total number of users, and #hits is the number of users whose item in the testing set is rec-ommended (i.e., hit) in the size-N recommendation list. A second measure for evaluation is ARHR, which is defined as follows: where if an item of a user is hit, p is the position of the item in the ranked recommendation list. ARHR is a weighted version of HR and it measures how strongly an item is rec-ommended, in which the weight is the reciprocal of the hit position in the recommendation list.
Besides the learning capability of the SSLIM methods, the representation of the side information can impact the overall performance. Since the side information in our datasets is text (e.g., movie plots, product reviews, etc), we investigated different text representations. In all of these schemes, the text of the side information was preprocessed to eliminate stop words and each word was converted to its stem 1 .
Binary Representation ( F b ) In this scheme, the text of the side information is represented using the bag-of-words model, and the frequency of each word is set to one. The reason for the binarization is that typically the text for an item is short, and there are not many informative words occurring multiple times, and thus a binarized vector is al-most same as the original count vector. In addition, since the user-item profile M is binary, intuitively the item-item coefficient matrix Q learned from a binary feature matrix F should be comparable to the aggregation coefficient matrix S learned from M in terms of the values. In this case, the regularization using Q on S (i.e., the  X  1 2 k S  X  Q k 2 rcSLIM ) can be more effective.

Normalized TFIDF Representation ( F tfidf ) For the meth-ods that directly learn from the item text ( fSLIM and f2SLIM ), it is essential that the text presentation encodes how impor-tant a word is in the text. For this purpose, a normalized TFIDF scheme is adopted. The TFIDF scheme [13] is widely used for weighting words in text mining. After the TFIDF scheme is applied on the feature vectors, the feature vectors are normalized to unit length.

Normalized TFIDF Representation with Feature Se-lection ( F tfidf fs ) Another representation scheme is a modi-fication of F tfidf by using feature selection. For each feature vector, the words were sorted in decreasing order accord-ing to their weights in the TFIDF representation. Then the highest weighted words were selected until cumulatively they contribute to 90% of the vector length. http://glaros.dtc.umn.edu/gkhome/fetch/sw/cluto/doc2mat-1.0.tar.gz
Normalized TFIDF Binary Representation with Fea-ture Selection ( F tfidf fs b ) The last side information repre-sentation scheme is a compromise of F b and F tfidf fs , that is, it converts all the values that are calculated from F tfidf fs binary. This scheme tries to use only the words that are con-sidered as important by F tfidf fs and meanwhile still retain the advantages of the binary representations.
Singh et al [14] proposed the collective matrix factoriza-tion ( CMF ) method for relational learning as follow: where U is an m  X  k user factor from M , W is an l  X  k feature factor from F , and V is an k  X  n item factor which is collectively learned from both M and F . Particularly, k min( m,n,l ). CMF and cSLIM are similar in the sense that a common matrix is learned from both M and F concurrently. However, they are fundamentally different methods. The cSLIM method conforms to linear methods and it models the top-N recommendation process as an aggregation on items. On the contrary, CMF models the recommendation process in a low-dimension latent space.

Thu et al [7] proposed a weighted regularized matrix fac-torization ( WRMF ) method for top-N recommendation, which weights the purchase and nonpurchase activities in M dif-ferently using a weighting matrix C as follows: minimize
Inspired by this weighting method, we combined WRMF with CMF so as to have a collective weighted regularized ma-trix factorization method, denoted by CWRMF , as follows: minimize in which M and F are still collectively factorized but errors from M are weighted differently by C . We use WRMF and CWRMF as the comparison algorithms in the experiments. In addition, we use another two collaborative filtering meth-ods for comparison purposes. The itemkNN method is a widely used item-based collaborative filtering method pro-posed in [4]. The itemSI method is a modification of itemkNN , in which the item similarities are calculated as a linear com-bination of the similarity values calculated from itemkNN and the cosine similarity values calculated from side information weighted by a parameter  X  .
Table 2 presents the detailed results of the SSLIM meth-ods ( cSLIM , rcSLIM , fSLIM and f2SLIM ), the three meth-ods without side information ( itemkNN , WRMF and SLIM ) and another two methods that utilize side information ( itemSI and CWRMF ), with respect to different side information rep-methods itemkNN , WRMF and SLIM , F no is used in Table 2 to denote that side information is not used.

Table 2 shows that SLIM outperforms the other methods that do not utilize side information (i.e., itemkNN and WRMF ) on all the datasets except BBY . For the BBY dataset, WRMF per-forms the best. This conforms to the conclusions as in [10], and thus we use SLIM as the baseline to further evaluate all the methods that utilize side information.

Table 3 summarizes the overall performance of the dif-ferent methods that utilize side information, with respect to SLIM . Irrespective of the feature representation scheme, cSLIM , rcSLIM and f2SLIM perform better than SLIM with average improvement 9.7%, 6.1% and 1.8%, respectively (the last row in Table 3). This demonstrates that side informa-tion contains useful information, and proper incorporation of side information into the recommender systems can bring significant performance improvement.

The methods itemSI , fSLIM and CWRMF perform worse than SLIM . The itemSI method is a trivial extension of itemkNN and it does not involve any learning. fSLIM is a method that learns directly from the side information. The performance of fSLIM indicates that this method may not be able to pick out and highly weight the individual features in the item side information that are most relevant to the rec-ommendations. CWRMF is the worst one and even worse than itemSI . This may be related to the discussion on CMF as in Agarwal et al [2], that is, when the side information is sparse, CMF may not work well.

Comparing the gains that can be obtained by utilizing side information across the different datasets, we see that they are not uniform. For the two movie datasets ( ML100K and NF ), the side information provides minimal benefits, whereas the gains achieved from the other datasets is substantial. We believe that this is due to the fact that the side information used for the movie datasets was quite generic, and does not contain sufficient information.
Table 3 shows that the performance of the side informa-F igure 1: Recommendations for Different N Values tion representation schemes depends on the recommenda-tion methods. For cSLIM , which uses side information for regularization, the binary feature representations ( F F tfidf fs b ) lead to better performance than the multivariate feature representations ( F tfidf and F tfidf fs ). This may be due to the fact that the binary features are treated homoge-neously as the user-item purchase data and thus they can regularize the learning process effectively. However, for the methods fSLIM and f2SLIM , which involve direct learning from side information, F tfidf and F tfidf fs result in better per-formance than the binary ones, since they differentiate the importance of features within the representations. In gen-eral, fSLIM and f2SLIM prefer the feature representations that encode word importance, so even the binary represen-tation F tfidf fs b , which has feature selection applied, also out-performs F b , which does not differentiate features at all.
Figure 1 presents the performance of SLIM and SSLIM methods (except fSLIM since it performs poorly) for top-N recommendation with different values of N . The F tfidf fs b side information representation is used for all the methods. For all the datasets, cSLIM consistently outperforms SLIM and other SSLIM methods on all N values (except N = 25 for dataset ML100K and N = 15 for dataset NF ). The rcSLIM method is the second competitive methods over all N values. The f2SLIM method shows performance that is comparable to SLIM and in some cases (i.e., N = 25 for ML100K , all the N values for BBY ) it outperforms SLIM . It performs consistently worse than SLIM only on Yelp .
To understand how the density of the dataset impacts the gains that can be obtained by utilizing side information, we performed a series of experiments in which we removed some of the user-item purchase profile data as follows. For each dataset, we always keep the testing set and side information unchanged, but randomly select a certain percentage (de-fined as density factor) of non-zero values from each user so as to construct training sets of different information density. Figure 2 presents the results from different recommendation methods on the datasets. The results in these figures are relative to the performance achieved by SLIM at the same density level.

These results show that in general, cSLIM and rcSLIM lead to more significant performance gains when the user-item purchase profiles are sparser. This indicates that when the user-item purchase data is sparse, cSLIM and rcSLIM are more effective in exploiting and incorporating side infor-mation and lead to more accurate top-N recommendations. Note that for the dataset ML100K and NF , performance im-provement from incorporating side information is hardly ob-served. As discussed in Section 7.1, we believe that this is due to the low quality of the side information.
This paper focused on incorporating side information into the sparse linear methods ( SLIM ) for top-N recommender systems. We developed four different approaches that incor-porate side information during the estimation of SLIM  X  X  ag-gregation coefficient matrix. Our experiments showed that the developed methods lead to measurable improvements over the original SLIM methods that relied solely on user-item purchase profiles.
 This work was supported in part by NSF (IIS-0905220, OCI-1048018, and IOS-0820730), the DOE grant USDOE/DE-SC0005013 and the Digital Technology Center at the Uni-versity of Minnesota. Access to research and computing fa-cilities was provided by the Digital Technology Center and the Minnesota Supercomputing Institute. [1] D. Agarwal and B.-C. Chen. Regression-based latent [2] D. Agarwal, B.-C. Chen, and B. Long. Localized [3] L. M. de Campos, J. M. Fern  X andez-Luna, J. F. Huete, [4] M. Deshpande and G. Karypis. Item-based top-n [5] Z. Gantner, L. Drumond, C. Freudenthaler, S. Rendle, [6] A. Gunawardana and C. Meek. A unified approach to [7] Y. Hu, Y. Koren, and C. Volinsky. Collaborative [8] A. Karatzoglou, X. Amatriain, L. Baltrunas, and [9] Y. Koren. Factorization meets the neighborhood: a [10] X. Ning and G. Karypis. Slim: Sparse linear methods [11] S. Rendle, Z. Gantner, C. Freudenthaler, and [12] F. Ricci, L. Rokach, B. Shapira, and P. B. Kantor, [13] G. Salton and M. J. McGill. Introduction to Modern [14] A. P. Singh and G. J. Gordon. Relational learning via [15] R. Tibshirani. Regression shrinkage and selection via [16] S.-H. Yang, B. Long, A. Smola, N. Sadagopan,
