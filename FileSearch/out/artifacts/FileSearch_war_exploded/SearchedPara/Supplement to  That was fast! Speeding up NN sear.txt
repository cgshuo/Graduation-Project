 Emanuele Coviello ecoviell@ucsd.edu Adeel Mumtaz adeelmumtaz@gmail.com Antoni B. Chan abchan@cityu.edu.hk Gert R.G. Lanckriet gert@ece.ucsd.edu of high dimensional distributions X  (Coviello et al., 2013), appearing on the Proceedings of the 30th International Conference on Machine Learning , Atlanta, Georgia, USA, 2013. (JMLR: W&amp;CP volume 28).
 et al., 2013) due to space limitations.
 (Coviello et al., 2013) to prove the monotonicity of ` + m ). In Section 3 we elaborate on the approximation made in our branch and bound algorithm, and on how it affects the decisions of our algorithm. derived from Hershey and Olsen (2007) variational approximation to the KL divergence between mixture models: a mixture with weights  X  j and components B j .
 denominator of the fist term inside the logarithm by exp { D ( M||Q ) } . Similarly, the lower bound to D ( X  ||M ) is found for A =  X  = {  X , M , (1  X   X  ) , Q} and B = { 1 , M} : et al. (2013). for which we have D ( Q||M )  X  0 . 1163 and D ( M||Q )  X  0 . 1251.
 opposite, ` + m (  X  ) = max(0 ,` m (  X  )) is monotonic, as illustrated in Figure 1(b). ` to reduce clutter, we have: (from which follows the convexity). In the derivation of Algorithm 2 in (Coviello et al., 2013), we use in sequence an approxi-mation and a lower bound: In general, when (10) and (11) do not hold as equalities, the algorithm is subject to making two types of incorrect decisions, i.e., exploring parts of the search space that could instead be safely pruned (over-explorative behavior), or pruning away parts that should be explored (under-explorative behavior). Interestingly, we can argue that, instead of making one type of mistake or the other randomly , our algorithm is over-explorative on nodes to which the query is relatively close and under-explorative on nodes to which the query is further away (see (Coviello et al., 2013)).
 Conjecture: If q 0 =  X  0 +  X  0 , for  X  0 small, we have d f ( x  X  , X  )  X  l + m (  X  ). Consequently we have: 1 d (for a small), we have that:  X   X  [0 1]. Since d f ( x  X  , X  )  X  0 we also have d f ( x  X  , X  )  X  ` + m (  X  ). 2013) when the approximations hold as lower bounds, i.e.: NN), we can have the following situations: For the rest of the comparisons (i.e., Steps 5, 6 and 7 of our algorithm), we can have: actually hold as upper bounds, which has the opposite effect of making the the algorithm zero forcing (Nielsen and Nock, 2009), it will have smaller support than the mixture model as upper bounds.
 manifold. Emerging Trends in Visual Computing , pages 75 X 102, 2009.
 E. Coviello, A. Mumtaz, A.B. Chan, and G.R.G. Lanckriet. That was fast! Speeding up
NN search of high dimensional distributions. In Proceedings of the 25th international conference on Machine learning , 2013.
 J.R. Hershey and P.A. Olsen. Approximating the Kullback Leibler divergence between
Gaussian mixture models. In Acoustics, Speech and Signal Processing, 2007. ICASSP 2007. IEEE International Conference on , volume 4. Ieee, 2007. ISBN 1424407273. F. Nielsen and R. Nock. Sided and symmetrized Bregman centroids. IEEE Transactions on IT , 2009.
