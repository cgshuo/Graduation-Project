 This paper presents an approach to combine rank aggrega-tion techniques using a soft computing technique  X  Genetic Programming  X  in order to improve the results in Informa-tion Retrieval tasks. Previous work shows that by combin-ing rank aggregation techniques in an agglomerative way, it is possible to get better results than with individual meth-ods. However, these works either combine only a small set of lists or are performed in a completely ad-hoc way. There-fore, given a set of ranked lists and a set of rank aggregation techniques, we propose to use a supervised genetic program-ming approach to search combinations of them that maxi-mize effectiveness in large search spaces. Experimental re-sults conducted using four datasets with different properties show that our proposed approach reaches top performance in most datasets. Moreover, this cross-dataset performance is not matched by any other baseline among the many we experiment with, some being the state-of-the-art in learning-to-rank and in the supervised rank aggregation tasks. We also show that our proposed framework is very efficient, flex-ible, and scalable.
 H.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithms, Experimentation Rank Aggregation, Soft Computing, Genetic Programming
Rank aggregation is a research line broadly studied in the literature, focused on exploring the complementarity among ranked lists produced by different rankers. The problem of c  X  c  X  rank aggregation refers to how to combine a set of ranked lists so that the final combination is better (more effective) than any individual lists. Rank aggregation has been ap-plied in the last years in several tasks such as document filtering [13], genome database construction [24], informa-tive gene selection [16], metasearch [7, 15, 23], spam web-page detection [7], word association finding [7], and similar-ity search [9].

Rank aggregation techniques may be divided into two main categories: score-based and order-based. In the first group, the aggregation function takes the score informa-tion associated with each lists X  object as input. Examples of methods in this category are the Comb* family of rank aggregation methods (e.g., CombMAX, CombMIN, Comb-SUM, CombANZ, CombMNZ, and CombMED) [25] and the RL-Sim Rank Aggregation [18]. In the second category, only the relative order among the documents is required to ag-gregate the lists. Examples of methods of that category are Borda Count [2], Median Rank Aggregation [9], Markov Chain based rank aggregation [7], Footrule Optimal Aggre-gation [7], and Reciprocal Rank Fusion [3].

Given the large amount of existing techniques, each with different properties, recent work (e.g., [19]) has started to explore the idea of combining the outputs of these meth-ods. For instance, in [19] the authors describe a model to combine in an agglomerative way results of different rank aggregation techniques and show that this model can obtain better results than the individual methods. However, in that and other works, the selection of the rank aggregation techniques was done arbitrarily, in an ad-hoc, not principled way and without exploring the large search space for com-binations of rank aggregation techniques. Moreover, usually only a few techniques are exploited in this combination.
In fact, finding the optimal (Kemeny) combination of ranked lists is an NP-hard problem, even for a small set of lists [8]. Soft computing techniques are well suited for this type of sce-nario. Soft Computing refers to the use of inexact solutions to computationally hard tasks. In this work, we explore a supervised soft computing technique  X  genetic programming  X  to combine a large set of methods. We model a problem as an optimization one, i.e., we try to optimize some met-ric of ranking quality while aggregating the lists produced by the individual methods. Our motivations to use genetic programming are threefold: (1) its solid theoretical back-ground [14]; (2) its powerful global exploration capability in large search spaces, with the capability of finding close-to-optimum solutions in many situations; and (3) a history of
To be more precise, four or more lists. success of applications of this technique in many information retrieval tasks (e.g., [4, 12, 27]) and related areas, getting in some cases comparable or better results than state-of-the-art machine learning techniques.

Our experimental results show that when compared to a large set of baselines (32 in total), including state-of-the-art unsupervised and supervised rank aggregation methods as well as Learning-To-Rank (L2R) baselines, our GP Aggre-gation framework is the only one to achieve top performance (top 1 or top 2) in all tested datasets. Moreover, we show that our method is very efficient and more scalable than sev-eral supervised alternatives. Finally, our solution is a general one which is very flexible; any new rank aggregation method to be developed in the future can be easily incorporated into our framework.

The rest of this paper is organized as follows. Section 2 introduces the problem of rank aggregation and describes briefly some methods of the state of the art. Section 3 covers related work. Section 4 presents principal concepts about ge-netic programming. Section 5 describes the proposed model for rank aggregation. Section 6 presents the experimental protocol, setup and the results, respectively. In section 7, we conclude the paper, presenting future research directions.
The problem of rank aggregation refers to combining a set of preference lists to produce a unique combined list. In the context of information retrieval, the scheme of rank aggrega-tion is illustrated in Figure 1. Each ranker produces a ranked list, ordering the database X  X  elements with respect to the rel-evance for the query. Then, using a rank aggregation func-tion, the lists are combined to produce a final list. More for-mally, let D = { D 1 , D 2 , . . . , D m } be a set of rankers based on which we can compute m ranked lists, R D 1 , R D 2 , . . . , R given a query q . Let R q be the set formed by these ranked lists, i.e., R q = { R D 1 , R D 2 , . . . , R D m } . A rank aggregation function f is used to define a ranked list R q,f for q , such that R q,f = f ( R q ).
 Figure 1: Scheme of rank aggregation in information retrieval.

One of the most cited applications of rank aggregation is meta-search, this is, the problem of creating a search engine that uses the results of a set of search engines to produce better results [7].
 Table 1: Formulas to get the final score of an object, by Fox and Shaw [25], where r ( x ) is the score of x in the ranked list r and R is the set of ranked lists
CombM AX ( x ) max
CombM IN ( x ) min
CombSU M ( x ) P CombM ED ( x ) CombSU M ( x )  X | R | CombAN Z ( x ) CombSU M ( x )  X |{ r | r  X  R and x  X  r }| CombM N Z ( x ) CombSU M ( x )  X |{ r | r  X  R and x  X  r }|
There are many unsupervised methods proposed in the literature. The principal advantage of these methods is their low computational complexity. However, they assume that all ranked lists have the same level of importance, which in practice is not a good idea, because the ranked lists were produced by systems with different levels of precision. Some of these methods are described in the following.

Borda [2] proposed the Borda Count algorithm in the con-text of the social-choice theory. It is an order-based method, where to an element x is assigned a score, in the ranked list r , equal to | r i | X  r i ( x ), where r i ( x ) is the position of the element x in r i . The final score of an element is the sum of the scores obtained in each ranked list.
 Fox and Shaw [25] proposed six score-based techniques: CombMAX, CombMIN, CombSUM, CombMED, CombANZ, and CombMNZ. These methods assign to each object a score using the formulas showed in Table 1. The score of the ob-jects in the ranked list needs to be normalized before using these methods.
 The method RL-Sim Rank Aggregation was proposed by Pedronette and Torres [18]. It is score-based technique, where the final score of an object is given by the product of their scores in each ranked list. This method is inspired by the Na  X   X ve Bayes classifiers, in the sense that it considers that the lists are independent and the score multiplication can be seen like the probability computation of the objects.
Cormack et al. [3] proposed the Reciprocal Rank Fusion algorithm, which is an order-based method that simply as-signs the final score to an object using the formula showed in Equation 1, where C is the set objects, R the set of ranked lists, r ( x ) the position of the object x in the list r , and k is a constant (authors report better results with k = 60):
Fagin et al. [9] proposed the Median Rank Aggregation al-gorithm, which works as follows. It goes through the ranked lists simultaneously and counts the number of occurrences of the objects. The first object that appears in more than half of the lists is taken as the first object of the combined list. Then, the second object that appears in more than half of the lists is taken as the second, and so on, until the k -top objects are found.
As we are exploiting a supervised GP framework, the clos-est methods to ours are learning-to-rank (L2R) approaches and supervised rank aggregation techniques.

L2R methods are usually classified into three categories: pointwise, pairwise, and listwise. Pointwise strategies can be viewed as regression approaches that predict relevance by minimizing a loss function. Differently, pairwise approaches learn, for any given two documents, if one is more relevant than the other. Finally, listwise approaches iteratively op-timize a specialized ranking performance measure, such as Mean Average Precision (MAP). Our approach is more re-lated to listwise approaches as we directly optimize a ranking measure. However, we use an extensive list of L2R methods covering all three categories as baselines in our experiments.
The supervised rank aggregation problem has received considerable attention in recent years and a number of su-pervised approaches have been proposed [17, 20, 26, 28 X 30]. Notably, Volkovs and Zemel [28] have recently shown that by applying SVD factorization to pairwise preference matri-ces effective item features can be extracted. The features transform the problem into a standard L2R one, allowing to apply any of the existing L2R methods to optimize the ag-gregating function for the target metric. While the authors of that work have shown superior empirical effectiveness of this approach to many existing aggregation methods, it also has a major drawback as it requires computing SVD fac-tors at training or test time. For large problems with many documents per query, applying SVD at training or even test time can be prohibitively expensive, limiting the applica-tion of this method. A number of other popular supervised aggregation methods share the same disadvantage and also require applying complex optimization procedures such as semi-definite programming [17].

In [29], on the other hand, the authors address the com-plexity at testing time by developing a flexible Conditional Random Field (CRF) framework for supervised rank aggre-gation. This framework uses preference matrices directly in order to avoid costly optimizations. The authors demon-strate that CRF has superior effectiveness when compared to some supervised alternatives. We use this method as one of our baselines. Although efficient at testing time, this method is very costly at training time, mainly when aggre-gating large ranked lists. Indeed, it took more than a week to run in the biggest dataset we experimented with.
Genetic programming has been previously exploited in [31] for the task of L2R. In that work the authors use ge-netic programming to discover a ranking function composed of a non-linear combination of the feature values of the document-query pairs, using simple arithmetic operations (  X  , + and  X  ) and a set of fixed constants. We include this method as baseline (RankGP).

In [1] the authors proposed an unsupervised rank aggre-gation method using genetic algorithms to search a permu-tation that minimizes the normalized aggregated footrule distance between this permutation and the ranked lists gen-erated by the rankers. An advantage of this method is that it does not require labeled data. An important disadvantage is that if there is a majority of  X  X ad X  rankers, the final ranked list will be very correlated to those. Another significant drawback is that the process of optimization is performed at query time, which, generally, can be very expensive and not really suitable for online (web) search. In contrast, our supervised solution, which is run offline, is used only to dis-cover a close to optimal combination of lists, which is very fast to be applied at query time, as we shall see in our ex-periments.

Several of these methods have explored weighted aggre-gation rules [20, 26] to learn the importance of individual ranked lists before combining them, using a well-explored social choice aggregation rule, such as Borda or Kemeny. The weights are tuned on the training data to reflect each expert  X  X greement X  with the ground truth preferences. We use the two approaches proposed in [26] (SupBordaCount and SupKemeny) as baselines.

Finally, a recent proposal [30] has used linear discriminant analysis (LDA) to train the weights along with the Con-dorcet ranking aggregation algorithm. This method, which learns the weights automatically, can be efficient at both, training and testing time. We also use it as a baseline (Sup-Condorcet).
Genetic Programming (GP) is an evolutionary methodol-ogy introduced by [14]. It is a problem-solving technique based on the principles of biological inheritance and evo-lution of individuals in a population. The search space of a problem, i.e., the space of all possible solutions to the problem, is searched by applying a set of operations that follow the theory of evolution, combining natural selection and genetic operations, to create more diverse and better performing individuals in subsequent generations with the aim of providing a way to find near-optimal solutions for a given task.

GP evolves a number of candidate solutions, called in-dividuals , represented in memory as binary tree structures. Every internal node of the tree is a function and every leaf node, known as terminal, represents either a variable or a constant. The maximum number of available nodes of an individual is determined by the depth of the tree, which is defined before the evolution process begins.

A common GP individual representation is given by a tree, as shown in Figure 2, where the individual represents the function f ( a, b ) = a  X  b + tree ( a and b ) are called terminals and the internal nodes (+ ,  X  ,  X  , and  X  ) are called functions . The functions (usu-ally mathematical operators) are used to combine terminals. Figure 2: Example of a tree representation of an individual.

The evolution process starts with an initial population composed of a set of individuals (the initial population) ran-domly generated. Each individual is evaluated by a fitness function and associated with a fitness value. This fitness function is commonly modeled by a user-defined measure to score the ability of an individual to adapt to the environ-ment (which in most cases correspond to the best solution for a given problem) and it is used to eliminate from the population all unfit individuals, selecting only those closer to the desired goal or those that achieve higher scores. In the case of search systems, this fitness function is the rank-ing quality measure we want to optimize. Individuals evolve generation by generation through genetic operations such as reproduction , crossover , and mutation .

Reproduction is the process that copies the  X  X est X  individ-uals from one generation to the next one, without modifying them. The individuals selected participate in the crossover and selection operations. The crossover operator allows ge-netic content exchange between two other individuals, the parents. In a GP process, two parent trees are selected ac-cording to a matching selection policy. Next, a random sub-tree is selected from each parent. The children trees result from the swap of the selected sub-trees between the parents.
Finally, the mutation operator has the role of keeping a minimum level of individuals diversity in a population. In the mutation operation, a random node in a tree is selected and then replaced by a new randomly created sub-tree.
Thus, at the end of the evolutionary process, a new pop-ulation is created to replace the current one. The fitness value is measured for each new individual, and the pro-cess is repeated over many generations until the termina-tion criterion has been satisfied. This criterion can be a pre-established maximum number of generations or some additional problem-specific success measure to be reached (e.g., an intended value of fitness for a specific individual).
We adopt the GP process suggested in [5]. Its main steps are described in Algorithm 1. Basically, the learning process is an iterative process with two main phases: training (Lines 1 X 10) and validation (Lines 11 X 14). For each phase, a dis-tinct set of queries is selected, which we call the training set and the validation set , respectively. It is important to stress that in this approach the whole GP process is performed of-fline, only once, when processing a training collection. After the process derives a ranking function, this (mathematical) function is applied to combine features at query processing time. For this reason, GP is an extremely low-cost method when considering run-time query processing computational costs.
 1 P  X  Initial random population of individuals; 2 B t  X   X  ; 3 foreach generation g of N g generations do 4 F t  X   X  ; 5 foreach individual i  X  X  do 6 F t  X  X  t  X  X  g, i, fitness ( i, T ) 7 end 8 B t  X  X  t  X  getBestIndividuals ( N b , F t ); 9 P  X  applyGeneticOperations ( P , F t , B t , g ); 10 end 11 B v  X   X  ; 12 foreach individual i  X  X  t do 13 B v  X  X  v  X  X  i, fitness ( i, V ) } ; 14 end 15 BestIndividual  X  applySelectionMethod ( B t , B v );
The process starts with the creation of an initial ran-dom population of individuals (Line 1) that evolves gener-ation by generation using genetic operations (reproduction, crossover, and mutation)  X  Line 9. The process continues until a stopping criterion is met. In the case of Algorithm 1, the criterion is the maximum number of generations of the evolutionary process.

In the training phase, a fitness function is applied to eval-uate all individuals of each generation (Lines 5 X 7), so that only the fittest individuals are selected to continue evolving (Line 8). The fitness of an individual corresponds to the quality of the ranking generated by the individual for each training query.

After the last generation is created, to avoid selecting in-dividuals that work well in the training set but do not gener-alize for different queries (a problem known as over-fitting ), a validation phase is applied. In this phase, the fitness func-tion is also used, but at this time over the validation set of queries and documents (Lines 12 X 14). Individuals that per-form the best in this phase are selected as the final solutions (Line 15). Our proposed model (hereafter called GP-Agg for GP-Based Rank Aggregation ) uses genetic programming to search the best combinations of different rank aggregation tech-niques in an agglomerative way, that is, using the result of one as input of another. The principal reasons to use genetic programming for this task are: (i) the inherent complemen-tarity between the results of different rank aggregation tech-niques studied in [19]; (ii) the large size of the search space for combination functions; and (iii) previous success of using GP in information retrieval.

We use the representation of trees for the GP-Agg indi-viduals, where the terminals are composed of a set of ranked lists R = { R 1 , R 2 , .., R m } generated by a set of base rankers. The functions (internal nodes) are composed of a set of un-supervised rank aggregation techniques F = { f 1 , f 2 , ..., f proposed in the literature. We create randomly the initial population and evolve it through a number of generations. Also, we use the classic genetic operators of reproduction, mutation, and crossover.

Figure 3 illustrates the whole process of learning to ag-gregate rankings. In part A of the figure, we have the set of rankers that are used to create ranked lists, given query doc-uments available in the document collection (data repository indicated by C). The defined ranked lists are then combined by rank aggregation functions f i (see B). The GP-based ap-proach described in Section 4 (see D) is then used to de-fine the best combination function represented as a binary tree (see E). Note that the GP-based aggregation function process uses not only the available rank aggregation func-tions, but also the training and the validation sets (see Al-gorithm 1). Finally, the best individual is used to rank doc-uments in the test set (module indicated as F).

A clear example of an individual is shown in Figure 4, where the individual represents the composed function:  X  f = f lists R 1 and R 2 are combined using the function f 1 the result is combined with R 1 using f 2 , and finally this result is aggregated using the function f 3 with the result of combining the ranked lists R 3 and R 4 through f 1 . Figure 4: Example of an individual in our proposed model for rank aggregation.
We used score and order based methods to combine the ranked lists generated by the base rankers. The list of the methods used in our model are shown in Table 2. We se-lected classical and recent unsupervised methods proposed in the literature.
 Table 2: Set of rank aggregation methods used in the proposed GP-Agg framework.

The following items describe details of the implementation of the proposed model along with the parameter choices se-lected for GP-Agg. In the parameterization process, we start with values used in previously tested successful GP config-urations in related tasks [5, 27], but perform an very com-prehensive parameter search, always using a cross-validation procedure in the training set. 2
The use of a more systematic methodology [11] for defining the values of GP parameters is left as future work.
We use four datasets in our experiments: the informa-tional datasets of Letor 3.0 (TD2003 and TD2004), and the Rank Aggregation datasets of Letor 4.0 (MQ2007-agg and MQ2008-agg). We describe each of these datasets in more details next. These sets of datasets have different properties which help to demonstrate the properties of our proposed framework in different scenarios.
These datasets 3 were chosen because they are publicly available, include several baseline results, and provide evalu-ation tools to ensure accurate comparison between methods. In these and the other datasets, information is represented as (query, document) pairs by means of  X  X eta-level X  features that try to capture important relationships between these two central components of an information retrieval system (a.k.a. query-dependent features (QD)). These features vary from simple strategies such as computing Term Frequency (TF), Inverse Document Frequency (IDF), or the combined TF-IDF metric of the query terms in different parts of the document (e.g., whole document, body, anchor text, title, URL) or by the application of traditional (e.g., BM25) or more recent state-of-the-art rankers such as those based on Language Models [32] or Relevance Propagation Methods (e.g., Hyperlink and Score based Propagation models [22].
Other features that are largely exploited in these datasets are the so-called query-independent features (QI), which do not depend on query terms, but are usually related to hy-perlinked information available in many collections, most notably the Web. Among these features, we may cite Inlink number, PageRank, HITs hub, and authority. Accordingly, each query-document pair ( q , d ) is represented by a set of QD and QI features, usually in the form of a feature vector, and labeled with a numerical score indicating how relevant document d is to query q .
 Among the datasets in this benchmark, The TD2003 and TD2004 datasets comprise informational queries, while the other datasets (NP and HP ones) comprise navigational que-ries, for which there is usually only one  X  X ood answer. X  Rank aggregation techniques are more suitable for informational queries as there are usually several  X  X ood documents X  to ful-fill an information need. http://research.microsoft.com/en-us/um/beijing/ projects/letor/ (As of May 2015).

These datasets are composed of feature vectors for (query, document) pairs, along with a corresponding binary rele-vance judgements (1 = relevant and 0 = irrelevant) indicat-ing whether the document is relevant or not for the query. Particularly in the case of the TD2003 and TD2004 datasets, there are 64 features per pair: 47 query-dependent features and 17 query-independent ones. Each feature can be con-sidered as an independent ranker, whose ranked list will be aggregated by the base methods. These two datasets together comprise around 100 queries and 100.000 labeled documents. Each dataset comes with five precomputed folds with 60/20/20 splits for training/validation/testing. The re-sults shown for each model are the averages of the test set results for the five folds.

Also available on the benchmark X  X  web page, we find 12 different L2R baselines, namely: AdaRank-MAP, AdaRank-NDCG, FRank, ListNet, RankBoost, RankSVM, RankSVM-Struct, RankSVM-Primal, Regression, Regression+L2reg, SVMMAP, and SmoothRank. Aside from FRank and Rank-Boost, all of the algorithms use linear ranking functions.
IN LETOR4.0, there are two datasets especially designed for rank aggregation tasks: MQ2007-agg and MQ2008-agg. MQ2007-agg contains 1692 queries with a total of 69623 doc-uments, while MQ2008-agg contains 784 queries and a total of 15211 documents. Each query contains rankings of the documents under that query. There are 21 features (rankers) in MQ2007-agg and 25 in MQ2008-agg. In addition, in both datasets, to each document is assigned one of three relevance levels (2 = highly relevant, 1 = relevant and 0 = irrelevant). Finally, as before the training, validation, and test sets are pre-computed and results correspond to the average of the five test folds.
The experimental results presented in this section were carried out using a 5-fold leave-one-out cross-validation pro-tocol as discussed before, with best parameters being found in the validation sets and results reported only in the test sets. We used the Normalized Discounted Cumulative Gain NDCG , Precison (at cut-off point 10, for both metrics) as well as Mean Average Precision ( MAP ) metrics to evalu-ate the results, based on the tools provided by the Letor benchmark.
 NDCG for the top k results is defined as: where R ( j, d ) is the score given by assessors to document d for query j (NB: non-binary notion of relevance) and Z k is a normalization factor (perfect ranking at k).

Unlike NDCG, MAP only allows binary (relevant/not rel-evant) document relevance, and is defined in terms of aver-age precision (AP). The average precision (AP) of a query is computed based on the average of precision scores con-sidering each relevant document retrieved. More formally, let q be a query, N r be the number of relevant items in a be a ranked relevance vector to depth d , where r i cates the relevance of the i -th ranked document scored as either 0 (not relevant) or 1 (relevant), the AP is defined as is then computed by averaging AP over all queries. To com-pute P@k and MAP on the MQ datasets the relevance levels are binarised with 2 and 1 converted to 1 and 0 remains un-changed.

As we do have a large number of combinations among base methods, baselines and datasets, a global analysis of the performance of all these combinations is not an easy task. For this, we resort to a performance measure pro-posed in [21], called winning number . This measure tries to assess the most competitive methods among a series of can-didates, given a large series of pre-defined tasks they have to perform. That is, the winning number of a method i in the context of a performance measure M , is given as S index (N datasets considered), i and k are the methods X  in-dex (M methods considered), M i ( j ) is the performance of the i  X  th method on j  X  th dataset in terms of measure M ,
Thus, the larger S i ( M ) is, the better the i  X  th method performs compared to the others.

For all methods we have run ourselves (base aggregation methods, supervised rank aggregation baselines) and for the TD and MQ benchmark baselines for which per-query re-sults were made available, we assess the statistical signifi-cance of the GP-Agg results by means of a per-query paired t-test with 95% confidence. A N symbol indicates that GP-Agg was statistically superior to the alternative, H means the opposite, and  X  means that both methods are statisti-cally tied.

As we have no per-query results in the L2R baseline results in TD2003 and TD2004 for some baselines, as well as for the rank aggregation baselines of the Letor 4.0, and given: 1) the high variability among folds due to both, the small number of queries and highly disparate performances among queries, and 2) the small number of points (only five folds), statisti-cal tests comparing GP-Agg with these baselines cannot be computed reliably. This issue is better discussed in [6]. In this case, for ordering purposes only, we rely on the absolute values of the metrics. In these situations, the absence of sta-tistical results is marked with a  X  , because of the absence of per-query results of a particular baseline in the official benchmarks.
We consider as baselines: 1. In Letor 3.0 datasets: all previously mentioned  X  X ffi-2. In Letor 4.0 datasets: all  X  X fficial baselines X  available 3. In all datasets:
Overall we have an impressive set of 32 different baselines across four different datasets with very different properties.
Results for the GP-Agg framework and all considered base-lines in the TD2003 and TD2004 are shown in Tables 3 and 4, respectively. Results are ordered by NDCG(@10), which we think is the most representative metric for this type of experiments, besides being the most used metric in recent rank aggregation studies (e.g., [29]).

We start by noticing that the performance of the base (unsupervised) methods used by GP-Agg are very far from the performance of GP-Agg itself. In TD2003 the best un-supervised methods are the Comb* family of methods while in TD2004 is RRF. These methods are statistically worse than GP-Agg, with gains for of around 35% and 27% in the respective datasets for GP-Agg. This indicates that in-deed the supervised learning process of GP-Agg can find a proper combination of these methods which maximizes ef-fectiveness.

We can also see in these datasets that, amongst all su-pervised alternatives, GP-Agg stands out with the high-est results for NDCG in TD2003 and the second highest in TD2004. 4 This impressive performance is better cap-tured by its winning numbers in these datasets, shown in Figure 5, 5 comparing GP-Agg with the L2R and supervised rank aggregation methods. In the figure, we can see that amongst all methods, GP-Agg has the best winning num-bers outperforming all other baselines. CRF and ListNet are the most competitive L2R methods under this evalua-tion metric, but they are still far from GP-Agg, mainly when considering P@10.

Moreover, the supervised rank aggregation methods are indeed better than their unsupervised counterparts, but still far from GP-Agg, and still much worse than the L2R ap-proaches, with the exception of CRF, which demonstrated to be competitive in TD2004. In particular, the SupCon-dorcet X  X  effectiveness in these datasets (it does not figure among the top performers) may be explained by the large number of lists to be aggregated. In [30], the authors indeed demonstrate that there is a decline in performance as the numbers of lists grow.
The large number of statistical ties in theses datasets is due to the small number of points (only 50 queries) and large variety in performance among queries.
In the figure, we did not consider statistical ties and con-sidered only the absolute values of the metrics. Figure 5: Winning Numbers for the TD2003 and TD2004 datasets.

Finally, in the TD2003 and TD2004 datasets, the CRF method showed be one of the most competitive baselines. However, this method requires the pre-computation of unary and pairwise potentials (to make the learning process more efficient), which has a complexity of O ( | q | X | d | 2  X |R| ), in which | q | is the number of queries in the training set, | d | is the number of documents per query in each ranked list to be aggregated, and |R| is the number of ranked lists. This pre-computation, in cases when there are larger ranked lists in the training set, becomes more expensive than the training process itself (depending of the training parameters, e.g. the number of iterations). As in these datasets, the number of documents per query in each ranked list to be aggregated | d | is around 1,000, and there is a total of 64 ranked lists, this enormously increase the time complexity.

Similarly to the previous results, in Letor 4.0, the GP-Agg performance is outstanding, statistically tied with CRF in first place in all datasets and under all metrics but MAP in MQ2007-agg in which it lost, but by no more than 2% of difference. Against all other baselines, including the super-vised baselines and the Letor 4.0 benchmark methods, GP-Agg is statistically superior by large margins of difference. These results, for both datasets, are better summarized in the winning numbers shown in Figure 6. 6
Experiments with the CRF baseline showed us scalability issues related to the combination of large lists. This aspect along with the effectiveness results discussed in this and in the previous section, demonstrate that our solution is a very competitive one for several possible rank aggregation scenar-ios.
Since our model combines methods of the literature in an agglomerative way, the time of application of an individual, to a given lists set is equal to the sum of the times required to perform each one of the methods combined. Since it takes only a few milliseconds to run these baselines at test time, this time is very negligible. Moreover, it is possible to run the baselines in parallel to obtain the ranked lists and, by doing so, the time would only correspond to the time to run the slowest base method individually. Similarly, asymptoti-cally, the complexity of an individual would be proportional to the highest complexity among the combined methods.
To keep consistency with the figure for the previous datasets, we again did not consider the statistical ties and use the absolute values of the metrics to calculate the win-ning numbers. However, we should point out that statisti-cal results are much more reliable in these datasets given the large amount of considered points (hundreds) and the smaller variability among queries.
We measure the overall time for training of our model (av-erage of five training processes in the 5-fold cross-validation process) and the average time required to apply the best individual (best discovered function) on the test-set queries (average of 5 test folds), in each one of the datasets. These results are shown in Figures 7 and 8 respectively. For the average query time, we assume that the lists were generated by the rankers and are ready to be combined. Empirical results show a reasonable time for training and a very fast time of application of the best discovered function. In other words, applying the best GP-Agg individual at test time in real situations requires basically no additional costs.
The best individuals found by the GP-Agg process in each dataset are listed below. In this case, we chose the individ-ual from the fold whose effectiveness was the closest to the average of the 5 test folds. This may be considered as the most representative individuals. Figure 6: Winning Points for the MQ2007-agg and MQ2008-agg datasets.
 For these individuals and others of the last generation of GP-Agg process, we observed that ranked lists with better performances are included in most of the individuals. We also observed that methods proposed by Fox and Shaw [25] are used more frequently by individuals with higher fitness of the last generation.
We proposed a soft computing method which, using GP, search for combinations of different rank aggregation tech-niques, aiming at producing a combination which is more ef-fective than any method used in isolation. The main idea of our work consists in exploiting the complementarity of the results produced by different rank aggregation techniques. Experimental results show significant gains over: (i) clas-sic and recent state-of-the-art unsupervised techniques; (ii) state-of-the-art supervised ranking aggregation techniques; (iii) learning-to-ranking strategies present in the Letor bench-marks. Our method also demonstrated to scale better than some state-of-the-art supervised ranking aggregation meth-ods at learning time, while the impact of its application at running (testing) time is negligible. The method is also very flexible and extensible; any new rank aggregation method can be easily incorporated to it. As future work, we intend to exploit the proposed method in other applications such as Figure 8: Average time for a query in test set, in milliseconds. image retrieval and include some of the supervised baselines in our combination to improve results even further. We also want to better investigate issues related to the choice of the fitness function in the overall process. This work was supported by FAPESP, FAPESP-Microsoft Virtual Institute (Proc. 2013/50169-1), FAPEMIG, CNPq, and CAPES.
