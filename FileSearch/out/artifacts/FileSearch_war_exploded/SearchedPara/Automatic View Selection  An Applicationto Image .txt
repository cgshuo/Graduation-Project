 With the advent of the Internet and rapid advances made in storage devices, non-standard data such as images and videos have grown significantly. The process of discovering valuable information from image data is called image mining, which finds its sources in data mining, content-based image retrieval (CBIR), image understanding and computer vision. The tasks of image mining are mostly con-cerned with classification problems such as  X  X abeling X  regions of an image based on presence or absence of some characteristic patterns, and with image retrieval problems where  X  X imilar X  images are identified. In image mining,  X  X raining X  im-ages are used for learning, and results (knowledge obtained from training) are applied to a large number of new images to fulfill the required tasks. data that can be processed by image mining systems.The conventional data for-mat used is the feature-value format (i.e., the tabular format). The attributes (or columns or features) are some characteristics representing an image ob-ject (or instance or rows) having corresponding to those attributes. Features describe the image pixel data. There could be hundreds of different features for an image. These features may include: color (in various channels), texture , etc. Domain experts can usually manually identify a set of relevant features in an ad hoc manner. In general, no single feature can describe an image in its entirety.
 features are useful in an application. Using all available features may negatively affect a mining algorithm X  X  performance (e.g., time and accuracy). In addition, it takes time to extract these features, though many features may be irrelevant or redundant. Furthermore, a large number of features would require a huge number of instances in order to learn properly from data. This is known as the curse of dimensionality [4]. With a limited number of instances but a large number of features, many mining algorithms will suffer from data over-fitting. Hence, it is imperative to choose a set of relevant features (i.e., feature selection [1]). weed called Egeria densa [5] in color infrared (CIR) images. These are available as aerial photographs of various land and water regions over San Joaquin and Sacramento deltas. The weed grows in water bodies and is present in various parts of the image: clusters, patches, or sometimes as a  X  X esh X . When we apply feature selection to this problem using some efficient algorithm [1, 2], we obtain a number of features. One applies a learning algorithm to determine the exact feature-value combinations for determining any particular task. An alternative solution that we propose in this paper is to select a set of feature-value com-binations directly . This combines the effect of feature selection that selects the best features and classification that takes decision based on feature values. This is termed as view selection [6] which is an instantiated feature set. It is appro-priate when there is a specific learning task to be learned. For example, in a set of images it is suitable to select the appropriate views.
 relevant features in a time-consuming process. Automatic view selection aims to find relevant feature-values by learning from training images. Therefore, domain experts are not required to specify relevant features, but only to identify or label instances of interest in an image. The latter is a much easier task than the former for experts, and its results are also more reliable because it directly deals with image classification. However, for k features with binary values, the number of possible views for a given class is 2 k . Because k can be as large as hundreds, enumeration of all views is clearly an insurmountable task. Hence, we need to find an efficient search technique for view selection. In this work we use association rule mining to automatically select the best view. The solution is a combination of top association rules that gives the maximum accuracy. We test our method on Egeria application. Results show that the selected view outperforms other views including the full view that contains all features.
 Here we define a view, and discuss how to learn good views. We give related work on view selection in ht tp://www.nt u.edu.sg/home/asmdash/pakdd expanded.pdf. Views. Given a set of k features f 1 ,f 2 ,f 3 , ..., f k (feature vector fv k )and t possible class labels C 1 , C 2 , ..., C t for an image of t types of objects, the problem of classifying an instance to one of the t classes can be defined as assigning a class with maximum posterior probability, c = arg max i P ( C i | fv k ) where P ( C i | fv ) is the posterior probability of class label C i , given the feature vector fv k . is a value from the domain of feature f j , and each f j corresponds to a feature describing the objects to be detected in an image I .A full view contains all k feature values, i.e., m = k .
 View Selection. In the search for good views, we need to find a suitable per-formance measure that can differentiate good views from the others. The goal of view selection is to find a sufficient and necessary subset of features with suitable values that make good views. The necessity requirement indicates that the subset of features should be as small as possible; and the sufficiency re-quirement suggests that a good view can guarantee the attainment of specified accuracy in image classification/detection. In image mining, positive means the presence of an object of interest, negative otherwise. If an object of interest is to be detected in an image, one of the four possibilities can arise: we may correctly classify/detect the object X  X  presence (True Positive -TP); we may falsely detect the object X  X  presence (False Positive -FP); we may miss the detection of the object which should be present (False Negative -FN); we may correctly detect that the object is absent (True Negative -TN).
 particular object in a two-class classification problem. In building a good clas-sification system, we wish to maximize TP and TN, and minimize FP and FN (errors). Using TP, TN, FP, and FN, we define accuracy A = TP + TN TP + FP + TN + FN . Learning Good Views from Image Data. With the performance measure defined, we can systematically generate views and estimate their accuracy based on training data. We would prefer a view with the smallest number of features. Therefore, we can start evaluating views of 1 feature, increasing the number of features one at a time until all features are considered. However, exhaustive search of all views is impractical when the number of features is moderately large (e.g., k&gt; 20) as the search space grows exponentially. There are many search procedure in the feature selection literature [2] that are not exhaustive yet produce reasonably good feature subsets. In this work we use association rule mining (ARM) for selecting the best view. Association rules are mined according to two measures: Support which measures generality of a rule and Confidence which measures precision of the rule. For a given class C and feature-subset of k features; let fv j be f j = v j . Support and confidence are defined tion rule with high support and confidence is both general and precise. Thus, a good association rule in the pre-specified form of fv 1 ,fv 2 , ..., f v m  X  C can define a good view. In addition, association rules avoid exhaustive search and can be mined with efficient algorithms [3]. Thus, we have successfully mapped the problem of view selection to a problem of ARM in the specific form of fv existing ARM algorithm to achieve automatic view selection. We evaluate our method to detect Egeria weeds in aerial images. In the empirical study, we investigate (1) how to learn good views, and (2) what is the accuracy over testing data. We first describe the application domain used in the experiments, next explain the conventional ways of identifying features and detecting objects of interest, then present results of learning and evaluation. Application -Detecting Egeria. In this application, the problem is to auto-matically identify the Egeria weeds in aerial images of land and water regions over San-Joaquin and Sacremento delta. A set of cover images corresponding to 51 images are provided in which the weeds are manually detected by experts for evaluation purposes. One image is used for training to obtain good views via learning and the remaining 50 are used for testing in experiments.
 30 of size 300  X  300, the remaining 20 of varying sizes 528  X  848  X  312  X  444. The largest image of size 1644  X  1574 was selected by the domain expert to be used as the training image for learning. This large image covers multiple regions of the delta, and is the most comprehensive image that contains a considerable amount of information for defining features to extract Egeria from the image. sion on feature identification and extraction.
 Evaluation Results of Learning. In order to search for the best view, we apply an ARM algorithm to the training data. We find association rules that describe the relationships between feature-values with class label 1 (presence of Egeria), i.e., in the association rule, the antecedent consists of the feature-values and the consequent consists of the class label 1. As we discussed in the previous section, the best view is the rule with the highest support and confidence. Support is the first criterion, i.e., if a rule has higher support than all other rules then it is chosen first. If two rules have equal support then the rule with higher confidence is chosen. The best view found by ARM is: color 1=1  X  color 2=1  X  color 3 =1  X  texture 1b = 0  X  1. This view (R1) has only 4 features: 3 color features (all 3 channels) should be present , and the first channel of the second texture feature should be absent . This view has support=0.751, confidence=0.95. and one texture feature (absence of water body) are sufficient to detect the weed in the training data. The single texture feature has to be 0 in the combination with the color features in order to predict the occurrence of the weed accurately. This is quite different from the scenario where only the three color features are considered in the view, or if the texture feature has a value 1. This view has effectively filtered the possible inclusion of water body in our coverage of the weed. Though 14 features are defined based on the domain, view selection suggests that these 4 features with their proper values should be sufficient for detecting weeds in this image.
 R1 works well. However, as we explained in the previous section, some of the images are dark. R1 is not satisfactory for extracting features from dark images. We observed that the next best view (second best association rule) contains the edge feature (along with other feature-values) which is useful for dark images. Next best view found by ARM is: color 1=1  X  color 2=1  X  color 3=1  X  edge =1  X  texture 2a = 0  X  texture 1a = 0  X  1.
 feature and two texture features. It has a support of 0.61 and a confidence of 0.93. The regions which were misclassified as FP in the image (dark) when we used the first view, are now labelled as TN using this view. We combine the two rules with logical AND (  X  ), i.e., R 1  X  R 2 and then we choose that view which has higher accuracy, i.e., A V = max ( A R 1 ; A R 1  X  R 2 ).
 example, there may be cases where R1 fails to detect correctly (i.e., FN) but R2 detects correctly (i.e., TP). In such cases instead of AND we use OR (  X  )to get the next best view. So, one may have to choose that view which gives the highest accuracy in the training image among the three: R 1, R 1  X  R 2, and R 1  X  R 2, i.e., rithm is used to select the top rules in the Egeria application. 1. Estimate the base accuracy A 0 using a full view. 2. Continue selecting top rules until accuracy of the new view A is larger than Aproper  X  value is dependent on the application. But usually it is small. We design experiments to verify in test images if this view indeed outperforms other views including the full view and if it does so not by chance . So, we conducted three comparative experiments: comparing the above best view (1) with the full view, (2) with random views of four features, and (3) with the best random view. We select random values (0 or 1) for the four randomly selected features. Each image has a manually determined cover image (ground truth) that indicates where Egeria blocks are. For the purpose of the experiments, this cover image is defined as the perfect result achievable for Egeria detection. We use accuracy gain to evaluate the difference between any two views. If the accuracy measures of two views are A and A 0 , Gain is defined as: Gain = A  X  A 0 A accuracy is  X  for best view (using our method): 0.92, for full view: 0.81, (gain for our method = 14%); average of 30 random views: 0.56 (gain = 19%); and for best random view: 0.8 (gain = 14%). For the group of 30 randomly selected views, box plots of accuracy values are shown in Figure 1 (a). These two comparative experiments show that the best view cannot be found by chance.
 Evaluation Results for New Images. The best view is obtained from the training image. We now evaluate its accuracy for the 50 testing images. The performance is evaluated by comparing the accuracy gains of the full View and the selected best view. Though there are some cases where we have negative gains, on average the gain is about 22%. We compare the cover which we ob-tained (using block processing) with the ground truth cover (which is in pixels). Hence there would always be some approximation in our covers -especially in images which have Egeria in thinner patches. These are more or less the images which have negative gains too. The box-plot of the results for the 50 images is shown in Figure 1 (b). Details of the experimental work can be found in http://www.ntu.edu.sg/home/asmdash/pakdd expanded.pdf.
 Conclusion. We define concepts of view and good view, identify the need for automatic view selection, show that a full view can make image mining un-necessarily more complicated and less efficient and effective, then propose and verify a learning approach to automatic view selection. Our system can learn good views from a training image data set and then the views can be applied to unseen images for mass processing of image mining tasks. With this approach, domain experts do not need to provide accurate information about what rele-vant features are, but only need to focus on the tasks they are best at (in this case, identifying if a block is Egeria or not in a training image). The presented approach is independent of type of feature and hence can be applied to many applications to determine good views. A future work is to consider the large number of rules that are usually generated as part of association rule mining. We acknowledge the contributions of Prof Huan Liu and Prof Trishi Foschi.
