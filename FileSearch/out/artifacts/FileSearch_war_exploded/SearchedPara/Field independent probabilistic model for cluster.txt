 1. Introduction
Document clustering uses unsupervised learning algorithms to group documents into different topics, and performs inative) methods and model-based (generative) methods have been two major strategies for learning clusters (Zhong &amp;
Ghosh, 2003 ). The similarity-based methods, such as the classical k -means algorithm, measure the similarity between doc-ument pairs, and group similar documents into the same cluster. In document clustering, each document is usually repre-sented by a vector of weighted selected words according to vector space model, and the similarity between two documents is calculated by Euclidean distance or the cosine of the angle between two corresponding vectors. On the other hand, model-based methods can generate the documents in the same cluster by an identical model, which is usually spec-ified a priori according to the characteristic of the data set. Without explicitly computing similarity between each document pair, the model-based methods usually have a less computational complexity compared to the similarity-based methods.
Various model-based clustering algorithms have already been proposed to tackle the problem of clustering high dimen-sional and very sparse text documents. Multivariate Bernoulli model (BM) (McCallum &amp; Nigam, 1998 ) and multinomial mod-el (MM) ( McCallum &amp; Nigam, 1998 ) are the two most popular models. Zhong and Ghosh (2003) proposed a unified framework for probabilistic model-based clustering, which divides a model-based approach into, a model re-estimation step model-based clustering algorithms (BM, MM and von Miser X  X isher model Banerjee, Dhillon, Ghosh, &amp; Sra, 2003 ) with differ-ent strategies of assigning documents to clusters. They found that MM and von Miser X  X isher outperformed BM in clustering 15 text datasets. In addition, soft and deterministic annealing (DA) based assignments achieved better performance than hard assignment in most of datasets. Meila and Heckerman (2001) also compared soft assignment with hard assignment in multinomial model clustering text documents, and obtained similar results. Rigouste, Capp X , and Yvon (2007) have proposed multinomial mixture models to cluster documents, where each document is generated by a mixture of several MMs.

In this paper, we are particularly interested in probabilistic model-based partitional clustering algorithms. To the best of our knowledge, existing model-based algorithms treat each document as an integrated object. However, the document is usually composed of several distinct fields in reality. One typical example is the multiple-field scientific document consti-tuted by the title, abstract, keywords, main text, and references. Although each field has a common objective of presenting title is usually very short with around ten words summarizing the topic of the document, while the abstract is much longer with about 100 X 200 words briefly describing the motivation of the work, the proposed solution and the experimental result. ument, the title in the news report summarizes the topic, and the body gives a more detailed description. Moreover, the first paragraph of the body most likely outlines what has happened in the news.

Here we propose a new probabilistic model referred to as field independent clustering model (FICM) to explicitly handle that each field of this document is conditionally independent. The generative model for each field can be different according to characteristics of data. FICM outperforms other classical model-based methods for the following reasons. First, it inte-able model for each field, and thus strengthen the performance of FICM. The basis of FICM is the conditional independence assumption of each field. This type of conditional independence has been widely employed and has achieved great successes
BMs are also based on this kind of assumption, where, given the cluster the document belongs to, each word in the document is generated, being conditionally independent. We stress that the corresponding models work very well in practice, although this kind of assumption may not follow reality exactly. This is because that the conditional independence assumption may change the posterior probabilities of each cluster, but the cluster with the maximum posterior probability is often unchanged.

We conducted extensive and thorough experiments by applying FICM to clustering both scientific documents and news document (record) in MEDLINE, many distinct fields are provided, such as title, authors, institution, source, MeSH (Medical Subject Headings) and abstract, among which title, abstract and MeSH are the most informative. MeSH (Nelson, Schopen, ing documents in the MEDLINE database. It includes a set of description terms organized in a hierarchical structure. To make a reliable evaluation, we have built 100 datasets randomly generated from TREC 2004 and 2005 Genomics track, respec-tively, which makes use of 10-years MEDLINE records as the corpus. In addition, we also created 100 news report datasets based on Reuters-21578 to evaluate the performance of FICM.

We first compared the performance of FICM with those of BM and MM in the experiment because of their wide usage. In the simplest case, the direct extension of BM and MM by applying the same model (BM or MM) for all fields in FICM has made a significant improvement over the original models in the majority of cases, being statistically significant in some cases. For example, over the TREC 2004 Genomics data, the direct extension of MM by FICM outperformed MM in 63 out of total 100 datasets, and 15 of them were statistically significant at the 95% confidence level. From this result, we found that the significant improvement in performance comes from the integration of the discriminative ability of each field.
We can then improve the performance of FICM further by assigning a suitable component model to each field. For example, over the TREC 2004 Genomics data, FICM outperformed MM in 98 out of total 100 datasets, and 78 of them were statistically significant at the 95% confidence level by assigning MM to the abstract field and BM to the title and the MeSH fields. We further investigated the component model assignment problem to achieve the best clustering performance, by exploring all eight possible combinations of component assignments in FICM for clustering TREC 2004 and TREC 2005 Genomics data. model X , and experimental results demonstrated the effectiveness of this strategy. Finally, we explore the effect of a method, which we call  X  X ield Weighting X , where a field is weighted more by assuming that the words in the field appear more often. Experimental results showed that weighting the title field moderately further improves the clustering performance of FICM.
The remainder of the paper is organized as follows. In Section 2, we propose the FICM for clustering multiple-field doc-uments. We also discuss the superiority and optimal setting of component model of FICM. Section 3 briefly describes the three data collections, TREC Genomics track 2004, 2005 and Reuters-21578, as well as the procedure of generating 300 test datasets from these three collections. We summarize the evaluation criteria and experimental procedures in Section 4. Sec-tion 5 presents the detailed experimental results, which demonstrates the superiority of FICM and the effectiveness of com-ponent model selection strategy. Section 6 draws conclusions and envisions future works. 2. Field independent clustering model (FICM) In this section, we first briefly introduce the classical BM and MM in turn, and then describe the proposed FICM in detail. In particular, we discuss the superiority as well as the time and space complexities of FICM.
 abstract or MeSH in this work. Let D c be a set of documents only considering field c , and d d . We denote W as the set of words appearing in D ; W c as the set of words appearing in D frequency of word w appearing in document d , and N w ; d wise 0. Let B w ; d c be 1 if N w ; d c &gt; 0, otherwise 0. 2.1. Mixture model for document clustering
All three models, BM, MM and the proposed FICM, belong to one important category of generative models: mixture mod-els. The basic assumption for generative models for document clustering is that each document d in D is independently and the product of the probability of generating every document d in D :
Moreover, the mixture model assumes that, each document is generated by a finite mixture distribution of the form bility of generating d in cluster i , which depends on a parameter vector h of generating document d can also be written as multinomial distribution. Then the basic goal of the mixture model is to use all documents in the dataset to estimate the model parameters, which are usually learned by maximum-likelihood (ML) estimation and the Expectation-Maximization to generate a document, i.e., the specific probability structure of p  X  d j z  X  . 2.2. Multivariate Bernoulli model (BM)
In BM, each document is represented by a binary vector, which denotes the presence or absence of each word in the doc-ument. The basic assumption is that, given the cluster that a document belongs to, the occurrence of each word in the doc-ument is assumed to be conditionally independent. For a document in class z , the probability of word w appearing in the the log-likelihood of generating the whole set of documents D . The ML estimators of this model can be obtained by the following EM algorithm, which repeats the E-and M-steps alternatively until some stopping condition is satisfied. In the
M-step, we employ a Laplacian prior to avoid zero probabilities, which is a form of maximum a posteriori (MAP) parameter estimation ( DeGroot, 1970 ).
 Probabilistic structure:
E-step:
M-step: (with Laplacian smoothing) 2.3. Multinomial model (MM)
In contrast to BM, MM assumes that, given the cluster that a document belongs to, every occurrence of each word in the document is assumed to be conditionally independent. Given a cluster z , it generates each word in a document indepen-steps are shown below.
 Probabilistic structure:
E-step:
M-step: (with Laplacian smoothing) 2.4. Field independent clustering model (FICM)
In spite of different assumptions for document representation and generation, both BM and MM treat the occurrence of each word at the document level rather than the field level. Conversely, the basic idea of FICM is that, given the cluster to which a document belongs, each component field of a document is conditionally independently generated. Although in prac-tice a document may not fully obey this rule, this kind of independence assumption has been widely used successfully in employed in the classical BM and MMs, which assume that, given the cluster to which a document belongs, each word in the document is conditionally independently generated by the underlying probabilistic models. Under the assumption that each field is generated independently, the probability of generating a document d is given as follows: having only two clusters z 1 ; z 2 , the likelihood-ratio LR of assigning d into z formula:
Since p  X  z 1  X  = p  X  z 2  X  is the ratio of the prior distribution of clusters z field. We can see that FICM is more like a framework, whose implementation depends on the probabilistic model used for each field. In this work, the component models are constrained into BM and MM. However, it can be easily extended to other probabilistic models. Let C b be the set of fields modeled by the BM, and C derive the probabilistic structure of FICM as below, and show the E-and M-steps of the EM algorithm to estimate the param-eters of this model. Probabilistic structure:
E-step:
M-step: (with Laplacian smoothing)
In the simplest case, we can use the same probabilistic model for all fields in FICM, such as C as Field Independent Clustering Bernoulli Model (FICBM) or Field Independent Clustering Multinomial Model (FICMM), respectively. 2.5. Superiority of FICM
As shown in Eq. (1), FICM assumes that each field has a certain degree of discriminative ability, and the overall clustering performance could be improved by integrating each field X  X  discriminative information. It is analogous to ensemble learning, which combines a set of individually trained classifiers for improving the overall performance on novel examples ( Polikar, 2006). Ensemble learning has been successfully applied in many classification problems, and recently has also been used for clustering problems (Ghosh, 2002 ). Although both FICM and ensemble learning rely on the component learner (model) to achieve a better performance, the fundamental difference is that FICM directly integrates the discriminative ability of each component in the framework of the generative model for clustering, while ensemble learning computes the final clustering result by aggregating a set of different clustering results, which are obtained by each component learner, respectively.
Dietterich (2000) analyzed the success of ensemble learning from the statistical, computational and representational point of view, and Hansen and Salamon (1990) summarized that the necessary and sufficient condition for the superiority of an think that these two conditions for the component model also hold for FICM for achieving the better clustering performance.
Each component model should accurately describe document topic in order to bring certain benefits to the final clustering result. Additionally, diversity of component models can reduce the bias caused by a single component model, because dif-ferent component models may assign the same document to different clusters. Therefore, we have the basic principle of the component model design in FICM:  X  X  X ssign the best model to each field, and in the meanwhile maintain the diversity of each component model X . 2.6. A  X  X ield Weighting X  extension: Weighting fields differently
To further improve the performance of FICM, we weigh (scale) each field by assuming that the words in each field appear more often. This kind of approach has also been employed by other researchers. Recently Li, Xu, and Zhang (2007) have stud-ied the problem of clustering blog documents of the World Wide Web (WWW). Each blog document consists of three com-ponents: the title, body and comments of the blog pages. They used the k -means clustering algorithm to cluster blog documents with assigning different weights to different components. Their experimental results indicated that assigning a large weight value to the blog comments produced a better clustering solution. In their method, each blog document was still deemed as a vector, and was a weighted sum of all three vectors, representing the title, body and comments, respec-tively. On the other hand, FICM models keeps fields being conditionally independent, and integrates them as a whole in a also applied in FICM.

For a field d c and a cluster z , if the occurrence of each word in d this field by k c , then the probability of generating this weighted field by MM will be p  X  d that field d c happens k c times, instead of only once, then the probability of generating these k too. With this approach, the probability of generating d in FICM will be as follows:
Then we can explore the effect of assigning different k c 2.7. Time and space complexities
Table 1 summarizes the time and space complexities of BM, MM and FICM. The most time-consuming part of BM is com-only need to keep space for p  X  z  X  ; p  X  z j d  X  and p  X  w j z  X  . The space complexity is then upper-bounded by easily see that MM has the same time and space complexities as BM. For FICM, the most time-consuming part is computing is upper-bounded by O  X j C jj D jj W j X  . Although FICM has slightly higher time and space complexities than BM and MM with tering performance. 3. Datasets
To examine the performance of the proposed FICM, we have built evaluation datasets from three collections: (1) TREC Genomics track 2004 and 2005 1 (Hersh et al., 2004; Hersh, Cohen, Bhupatiraju, Johnson, &amp; Hearst, 2005 ); (2) Reuters-21578 news collection. 2
The Genomics track of the TREC 3 conference provides a testbed and benchmark for comparing different information retrie-val systems for biomedical documents. There are totally 4,591,008 documents (MEDLINE records from year 1994 to 2003) in the one document may belong to multiple topics.

In the Genomics track 2004 and Genomics track 2005, we extracted not only the whole text of each record, but also three distinct field, because it usually gives the summary of the news analogous to the abstract in a scientific document. Thus we did not consider those topics associated with only nine or fewer documents, and removed those documents with empty fields or relevant to more than one topic, and finally retained 39 topics (4400 documents) in the Genomics track 2004, 24 topics (2317 documents) in the Genomics track 2005 and 42 topics (8574 documents) in the Reuters-21578, respectively.
In the Reuters-21578, the two largest topics,  X  X  X arn X  and  X  X  X cq X , have respectively 3735 and 2125 documents, which are much larger than the other 40 topics which have at most 355 relevant documents. To avoid the dominance of these two topics in the dataset, we only retained all other 40 topics (2714 documents) for evaluation.

From these filtered topics, we have built two sets of datasets based on the TREC Genomics tracks of 2004 and 2005, and one set of datasets based on the Reuters-21578, which are called as the Genomics2004, Genomics2005 and Reuters1987 col-lections, respectively. For a robust comparison, each collection includes 100 datasets, which were generated from the cor-responding source by randomly choosing three or more (no more than 12) topics. With a specific number of topics, we built 10 different datasets. Compared with other randomly generated document datasets from MEDLINE (Yoo &amp; Hu, 2006), Genomics2004 and Genomics2005 are of high quality, where the topic of each document was assessed manually by the biologists. In the pre-processing step, we removed stop words, carried out case folding and tokenized the documents using the Porter X  X  stemming algorithm ( Porter, 1980). Similar to Zhong and Ghosh (2005) , we eliminated any (stemmed) word that appears in less than three documents. The same procedure was also applied to the words in each field.
Table 2 summarizes the statistical characteristics of Genomics2004, Genomics2005, and Reuters1987, where N number of documents, W is the number of distinct words (tokens), K is the number of classes, N of words in each document, balance is the size ratio of the smallest class to the largest class, N of words in the title (abstract, MeSH) field, and W t  X  W
Each dataset in Genomics2004 and Genomics2005 is named by combining an initial alphabet  X  X  X  X , the year, the number of topics, and the order of the dataset. For example,  X  X  X 200412a X  represents the first dataset with twelve topics generated from the TREC Genomics track 2004. Similarly, each dataset in Reuters1987 is named by combining the initial alphabet  X  X  X  X , the year, the number of topics, and the order of this dataset. Table 2 shows that Genomics2004 varies greatly in some important characteristics: the number of documents varies from 133 to 1960, the number of classes from 3 to 12, the balance from 0.022 to 0.563 and the number of distinct words in each dataset from 879 to 4630. Compared with Genomics2004, Genom-ics2005 is slightly smaller but also has large variations. In contrast, Reuters1987 is the smallest in terms of the average length and the average number of documents in each dataset. The number of documents ranges from 52 to 1232, the number of distinct words from 116 to 1280, and the balance from 0.0282 to 0.619. Overall, the diversity of Genomics2004, Genom-ics2005 and Reuters1987 makes them very suitable for comparing different clustering algorithms. 4. Evaluation criteria and experiment design
In this section, we describe the criterion based on the normalized mutual information to evaluate the proposed FICM. Fur-thermore, we discuss how to compare the performance of different models using our new performance representation form,
S-Pair . 4.1. Normalized mutual information (NMI)
We used external measures as evaluation criteria in document clustering. External measures evaluate the clustering re-sult using the correct (true) class labels of the dataset, which is not provided during the clustering processes. Well-known external measures include purity, average entropy, F-measure and mutual information. Ghosh (2003) compared these exter-nal measures, and showed that mutual information is a superior measure over other external measures. Both purity and average entropy favor large number of clusters, while F-measure is biased towards coarser clusterings. Therefore, we used the normalized mutual information (NMI) to compare the performance of FICM with the other models. Normalized mutual information (NMI) is calculated by the following formula: sample estimate to calculate the NMI, where n is the total number of documents in the whole collection, n the number of documents in cluster l (predicted), and n h ; l value ranges from zero to one, where an NMI value of zero means that the result is equal to random partitioning, and an NMI value close to one means that the result is almost identical to the true class labeling. 4.2. Model comparison: S-Pair
In Genomics2004 and Genomics2005, each document consists of three fields, title, abstract and MeSH, while in Reu-ters1987, each document consists of two fields, title and abstract. When the BM is applied on the documents with title field respectively. When the MM is used, the corresponding models are denoted as M-title, M-abstract, M-mesh and M-whole , respectively. As described in Section 2, FICM is a framework where the specific implementation depends on the component models assigned to each field. Since we focus on two popular probabilistic models, BM and MM, there are 2 implementations of FICM for clustering datasets in Genomics2004 and Genomics2005, and there are 2 mentations of FICM for clustering datasets in Reuters1987. To make a distinction, each specific implementation of FICM is denoted by a combination of an initial alphabet F , the character  X - X  and a number of alphabets, where each representing a for a specific implementation of FICM, FICMM, where all three fields are modeled by MM. Table 3 lists the abbreviations of some models and their corresponding meanings.

To make a fair comparison, the same stop criterion for the EM algorithm was adopted for all the models: the relative change of the maximum log-likelihood in two consecutive iteration is less than 0.001% or the number of iterations of the over, to reduce the possible bias caused by a random initial partition, each experiment was carried out 100 times, and the means, standard deviations and the paired t -test were used to compare different models. The comparison of two different models, h 1 and h 2 , is carried on clustering 100 datasets of Genomics2004, Genomics2005, and Reuters1987. The number of the datasets that h 1 outperforms h 2 is denoted by N h icant improvement at the 95% confidence level is denoted by N h level is denoted by N h 2 &gt; h 1 ;  X  . Thus for comparing h
N &gt; h 2  X  N h 2 &gt; h 1  X  100 because there are 100 datasets in each collection. We can say that h
To show the integrated power of FICM, we divided the datasets in two types in the following manner: We first call the datasets on which FICM outperforms the classical model (BM or MM on the entire text) superior datasets and the other data-neutral datasets, we may say that the performance of FICM can be achieved by the good performance of this component model. To check this hypothesis, we focused on two simplest implementations of FICM: FICBM and FICMM. For example, for FICBM, we used the performance of B-whole (BM over the entire text) as the baseline, and the power of each component model was represented by a relative measure, the ratio of the clustering performance of the component model to the base-line, which we call the relative discriminative ability (RDA) of this field hereafter. That is, for the title field, RDA then compute the total RDA over all fields as RDA  X  RDA title
RDA is given as RDA  X  RDA title  X  RDA abstract 2 . We denote the RDA for superior datasets and neutral datasets by RDA respectively. 5. Experimental results Experimental results include two parts. In the first part, we compared BM and MM with their direct extension, FICBM and
FICMM. We found that FICBM (FICMM) outperformed B-whole ( M-whole ) consistently in all three collections, which demon-strated the effectiveness of FICM, even in the simplest configuration. In the second part, we further examined different com-binations of component models in FICM, and obtained some insights into the component field configurations. 5.1. FICBM and FICMM 5.1.1. Comparison of B-title, B-abstract, B-mesh, B-whole and FICBM
We compared the performance of the BM-based clustering models. In Genomics2004 and Genomics2005, FICBM is de-noted by F-bbb since it consists of three different fields, while in Reuters1987 with two different fields, FICBM is denoted by F-bb . Table 4 illustrates the performance of different models in terms of NMI in Genomics2004, Genomics2005, and Reu-the highest NMI. Experimental results showed that FICBM achieved the highest average NMI (0.756, 0.723 and 0.497) in all tical significance of improvement. The experimental results showed that FICBM outperformed B-whole consistently for all three collections, with S-Pair (+41/80, 0/20) in Genomics2004, S-Pair (+63/95, 1/5) in Genomics2005 and S-Pair (+73/ 89, 1/11) in Reuters1987. 5.1.2. Comparison of M-title, M-abstract, M-mesh, M-whole and FICMM
We compared the performance of MM-based clustering models and FICMM: F-mmm (or F-mm for Reuters1987). Table 6 presents the experimental result of these models on one example dataset and the average of all 100 datasets for Genom-ics2004, Genomics2005 and Reuters1987. We can see that FICMM achieved higher NMI than M-whole in all three collections.
For example, with Genomics2004, FICMM achieved the highest NMI of 0.740, which was followed by M-whole of 0.736. In addition, interestingly, for Genomics2005 and Reuters1987, M-title (using MM on the title only) obtained the highest average
NMI out of all five models, which suggests that only a few distinguished words in the title are effective for document clus-tering. We further compared these models in terms of the significance of improvement by paired t -test. Table 7 gives the results for each collection. We found that FICMM outperformed M-whole consistently in all three collections, with S-Pair (+15/63, 0/37) in Genomics2004, S-Pair (+12/67, 3/33) in Genomics2005 and S-Pair (+69/94, 0/6) in Reuters1987. component model
In the above two rounds of experiments, the direct extension of BM and MM by FICM (FICBM and FICMM) outperformed the corresponding original models ( B-whole and M-whole ) significantly, and the improvement of FICBM over B-whole was especially remarkable. As discussed in Section 4, we can compare RDA we used RDA of the three field case for Genomics2004 and Genomics2005, while that of the only two field case for Reu-ters1987. As shown in Table 8 , the upper three rows are by FICBM and the lower three rows are by FICMM. In all six cases, was the case of FICMM on Genomics2005 with a p -value of 0.43. The reason may be due to the dependence of different fields and the unbalanced contribution of each field in the FICMM. In contrast to BM, MM considers all occurrences of every word, which means that longer fields, such as abstracts, contributed much more than shorter fields, such as titles, in the perfor-mance of FICMM. Over all, we believe that the significant performance improvement of FICBM over BM on the entire text ( B-whole ) and FICMM over MM on the entire text ( M-whole ) comes from the integration of discriminative ability of each component model. 5.2. The combination of different component models in FICM
As discussed in Section 2, to make good use of FICM, the component model should be accurate and diverse, which requires us to assign the best model to each field and to also maintain a diversity of component models. To examine this strategy, we shall explore the effect of different combination of component models in FICM. Since Reuters1987 is too simple with only two fields and four possible combinations in FICM, we focus on exploring different combinations of component models on clustering datasets in the data collections Genomics2004 and Genomics2005, which consists of three fields and eight pos-sible combinations. First, we identified which model is more suitable, BM or MM, for each independent field. Second, we checked if the performance of FICM could be improved by configuring a better model to one field with fixed model settings on the other two fields. Finally, we discussed the optimal configuration of FICM compared with classical BM and MM. 5.2.1. Identifying the best model for each field independently
To identify the best model for each field, it is necessary to know both the important characteristics of each field and the strengths (and weaknesses) of each model. Alternatively, we may carry out some preliminary experiments on a small train-ing dataset to elucidate an appropriate model for each field. In Genomics2004 and Genomics2005, considering the distinct features of the MeSH and title fields, we believe that the best model for MeSH and the title field would be BM and MM, with the following reasons. The MeSH terms are originally organized in a hierarchal structure for indicating the document theme.
Here, both BM and MM treat them flatly without considering this hierarchical information. Some general terms, such as that appear usually are very informative and highly related for expressing the document topic. In this situation, MM, in which the probability for generating each distinct word sums to 1, adds some constraints to those relevant informative words, and would be more appropriate than BM since the latter deals with the presence/absence of each word indepen-dently. To examine these hypotheses, we compared the performance of MM and BM on the title and MeSH fields using a lections. For example, for Genomics2004, B-mesh outperformed M-mesh with S-Pair (+81/94, 5/6), and M-title outperformed B-title with S-Pair (+89/92, 5/8).

For the abstract field, we speculate that BM would be more suitable on small datasets with few topics, while MM outper-forms multivariate models definitely on large datasets, which comes from McCallum and Nigam X  X indings (1998) . They com-pared MM with BM for naive Bayes text classification, and found that BM performed very well on datasets with a limited vocabulary, while MM usually performed better on datasets with a larger vocabulary size. This finding is also consistent with our experiments which compared B-abstract with M-abstract . As illustrated in Fig. 1 , B-abstract usually outperformed M-ab-stract on datasets with small number of topics, while M-abstract outperformed B-abstract on datasets with more topics. For example, for both Genomics2004 and Genomics2005, M-abstract outperformed B-abstract statistically significantly for all 10 datasets with 12 topics. The crossover of B-abstract and M-abstract with similar performance happens when the number of topics in the dataset is around 8. And thus, by dividing the datasets into two groups: K P 8or K &lt; 8, we summarized the comparison of B-abstract and M-abstract in Table 9 .

Over all, in Genomics2004 and Genomics2005, the best model for MeSH and title fields would be BM and MM, respec-tively. And for the abstract field, BM is more suitable than MM when the number of topics in the datasets is small  X  K &lt; 8  X  . Otherwise, MM is more suitable. 5.2.2. Replacing the model setting for only one field in FICM
We compared the performance of different models with paired t -test by changing the model for only one component field the clustering performance.

As shown in Table 10 , changing the component model from MM to BM for the MeSH field improved the clustering per-formance significantly for all combinations. For example, for Genomics2005, F-mmb outperformed F-mmm with S-Pair (+67/ 96, 0/4), and F-bbb outperformed F-bbm with S-Pair (+54/94, 2/6). Moreover, Table 11 shows consistently that replacing
MM with BM for the abstract would increase clustering performance when the number of topics is less than 8. The clustering performance decreased when the number of topics is equal to or larger than 8. For example, for Genomics2004, F-bbb out-performed F-bmb with S-Pair (+34/43, 2/7) when K &lt; 8 as shown in Table 11 , while F-bmb outperformed F-bbb with S-Pair (+37/41, 6/9) when K P 8in Table 12. These experimental results are highly consistent with our hypothesis that applying a better component model will improve the clustering performance of FICM.

Compared with the abstract and MeSH, the effect from changing the model for the title is much weaker and more com-plicated as illustrated in Table 13 . For example, for Genomics2005, F-mbm differed slightly from F-bbm with S-Pair (+5/ 53, 4/47), and F-mbb differed slightly from F-bbb with S-Pair (+1/52, 1/48). This indicates that model selection for the ab-stract or MeSH is more important than model selection for the title with respect to the performance of FICM. A main reason may be that the title is much shorter compared with the other two fields, so that the performance of FICM is dominated by the component model setting in abstract and MeSH. Another reason would be the relative high correlation between the title and the abstract. One popular measure for correlation is the cosine of the angle between two corresponding vectors (cosine similarity). For each dataset in the Genomics2004 and Genomics2005 collections, the cosine similarities between any two fields for both MM and BM are computed, and averaged in Table 14 . We displayed the highest correlated pair in every com-bination of data collection and model in boldface. We see that in all four cases, the title X  X bstract pair always obtained the largest similarity, which was especially significant in MM. For example, in Genomics2004, the average cosine similarity be-tween the title and the abstract in MM was 0.46, while the average cosine similarity between the title and the MeSH was 0.23, and the average cosine similarity between the abstract and the MeSH was 0.25. In this case, assigning different models diversity among component models. For example, as shown in Table 13 , for Genomics2004, F-bmb outperformed F-mmb with S-Pair (+13/76, 3/24), and F-bmm outperformed F-mmm with S-Pair (+15/74, 0/26), and for Genomics2005 F-bmb outperformed F-mmb with S-Pair (+13/68, 0/32), and F-bmm outperformed F-mmm with S-Pair (+35/82, 0/18).
In short, assigning a better model to a field in FICM usually improves the clustering performance, with exception on the title field, which may due to its short length and relative high correlation with the abstract. 5.2.3. Discussion on the best model configuration in FICM
As discussed in Section 2, FICM should work very well if each component model is accurate and diverse. According to this principle, we attempted to determine the best configurations of FICM for clustering datasets in Genomics2004 and Genom-ics2005. The basic idea is to assign the best model to each field with the constraint of keeping the diversity of each compo-nent model. Additionally, for Genomics2004 and Genomics2005, we also found that the effect of model selection on the title field was much weaker than the other two fields. Here, we first compared different configuration of FICM against the two classical models: BM ( B-whole ) and MM ( M-whole ). In this case, when compared with B-whole , the component setting on the abstract field in FICM is fixed to BM, and when compared with M-whole , the component setting on the abstract field in FICM is fixed to MM. In addition, by dividing the datasets into two groups, K P 8 and K &lt; 8, we compared all eight pos-sible configurations in FICM to find the best combination.

In the first case, since the model setting for abstract is already fixed, we only need to set the models for the other two fields: title and MeSH. As we discussed before, the best models for MeSH and title are BM and MM, respectively. Considering the high correlation between title and abstract, we should assign different models to each field. So, we assigned BM to MeSH, and the complement model of abstract to title. Under these settings, we conducted a comparison experiment for BM by all possible three combinations of B-whole , F-bbb and F-mbb . Table 15 shows S-Pairs obtained by these combinations. For MM, a similar comparison experiment was done by using M-whole , F-mmm and F-bmb . Table 16 presented S-Pairs obtained by all three combinations. We found that clustering performance could be improved, especially in the case of MM. For example, for
Genomics2004, F-mmm outperformed M-whole slightly with S-Pair (+15/63, 0/37), while F-bmb outperformed both M-whole and F-mmm remarkably with S-Pairs (+78/98, 0/2) and (+58/93, 0/7), respectively. This result is totally true of Genomics2005.
 In addition, by dividing the datasets into two categories, K P 8 and K &lt; 8, we determined the best configuration of FICM.
In the former case, the best model would be F-bmb , while in the latter case, two models, F-mbb and F-bbb , with very close performances, outperformed all other models significantly. As shown in Table 17 , when K P 8, F-bmb performed slightly bet-ter than F-mmb , which outperformed all other models significantly for both Genomics2004 and Genomics2005. When K &lt; 8,
F-mbb outperformed all other models significantly for both Genomics2004 and Genomics2005, except F-bbb with a perfor-field. Over all, the experimental results further validate the effectiveness of our strategy of assigning the best component model to each field while keeping diversity in FICM to achieve good performance.
 5.3. The effect of field weighting: assigning different weights to different fields in FICM
To further explore the capability of FICM, we can assign different weights to different fields. Here we focus on two best configurations: F-bmb and F-mbb . We use a vector k  X  X  k t k 5, 8 and 10. The experimental results of FICM on F-bmb and F-mbb with field weighting, are shown in Tables 18 and 19 , respectively. For example, in Table 18 , we first present the performance of original F-bmb without field weighting model and the one with field weighting. We highlighted the configuration in boldface that outperformed the original model significantly. In spite of examining the effect of field weighting on two different collections, Genomics2004 and Genom-ics2005, and two different configurations: F-bmb and F-mbb , we were able to observe almost the same tendency from the experiment results. That is, weighting the title field moderately was able to improve the clustering performance signifi-cantly. For example, on the Genomics2005 collection, with  X  k  X  X  3 ; 1 ; 1  X  X  , the average NMI of F-bmb was improved from 0.724 to 0.735 with S-Pair (+39/80, 1/20). The highest improvement was observed when k
NMI of F-bmb was reduced from 0.724 to 0.701 with S-Pair (+0/12, 45/88). Overall, the performance of FICM can be further enhanced when we apply the Field Weighting extension to the suitable fields.
 6. Conclusions
We have presented a probabilistic model, FICM, for clustering multi-field text documents. The advantage of FICM comes from the integration of the discriminative ability of each field and the power of choosing the most suitable generative model for each field. In order to achieve good performance, the component models of FICM should be accurate and diverse. We have experimentally shown that a direct extension of the classical BM and MMs by FICM, FICBM and FICMM, are able to achieve a better performance, and by configuring each field with a suitable model, we obtain much better clustering results. The com-ponent model setting is practical when we have some prior knowledge on the fields, such as BM for MeSH field in our work, which can improve the performance significantly. Alternately, we can carry out some preliminary experiments on the data-set to determine the suitable model for each field. In addition, we introduced a  X  X ield Weighting X  extension, which assigns
Over all, we emphasize that our idea of selecting the best model for each field and of integrating them independently can be applied to other documents with multiple fields, meaning that FICM is capable of improving the performance of clustering documents in other applications.

In our experiments, the number of documents in each dataset ranges from 52 to 1960. We would like to examine the per-formance of FICM on some larger datasets in the future. Techniques for determining the number of topics in the dataset ( Che-ung, 2005 ) is likely to be incorporated into FICM. Finally, we hope that FICM will be applied to another clustering problem where it also has multiple components that can be modeled separately and integrated.
 Acknowledgements The authors would like to thank anonymous reviewers for their helpful comments and advice. Shanfeng Zhu and Hiroshi Mamitsuka have been supported in part by BIRD of Japan Science and Technology Agency (JST), and Shanghai Key Lab of
Intelligent Information Processing, Fudan University. In addition, this project has been partly supported by Startup Fund of Fudan University, the Shanghai Committee of Science and Technology, China (Grant Nos. 08DZ2271800 and 09DZ2272800) and The State Key Lab of Bio-Organic &amp; Natural Products Chemistry, CAS.
 References
