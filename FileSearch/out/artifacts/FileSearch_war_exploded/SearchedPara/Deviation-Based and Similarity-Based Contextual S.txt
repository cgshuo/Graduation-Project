 Context-aware recommender systems (CARS) have been demon-strated to be able to enhance recommendations by adapting users X  preferences to different contextual situations. In recent years, sev-eral CARS algorithms have been developed to incorporated into the recommender systems. For example, differential context modeling (DCM) was modi fi ed based on traditional neighborhood collabora-tive fi ltering (NBCF), context-aware matrix factorization (CAMF) coupled contextual dependency with the matrix factorization tech-nique (MF), and tensor factorization directly models contexts as additional dimensions in the multi-dimensional space, etc. CAMF works well but it is dif fi cult to interpret the latent features in the al-gorithm. DCM is good for explanation but it may only work well on data sets with dense contextual ratings. Recently, we successfully incorporate contexts into Sparse LInear Method (SLIM) and devel-op contextual SLIM (CSLIM) recommendation algorithms which take advantages of both NBCF and MF. CSLIM are demonstrated as more effective and promising context-aware recommenders. In this work, we provide the introduction on the framework of the C-SLIM algorithms, present the current state of the research, and dis-cuss our ongoing future work to develop and improve our CSLIM models for context-aware recommendations.
 H.3.3 [ Information Search and Retrieval ]: Information fi ltering Recommendation; Context; Context-aware recommendation; SLIM; Matrix Factorization In contrast to traditional recommender systems (RS), context-aware recommender systems (CARS) adapt the recommendations to user-s X  speci fi c situations. The fundamental assumption of CARS is that a rating for an item is a function not just of the user and the item but also of the context in which the item is evaluated or used. Us-er X  X  preferences on a given item may vary from context to contex-t. For example, companion is an in fl uential contextual variable in the movie domain; you may make a different decision in selecting movies if you intend to see the movie on a date, than if you plan on watching a movie with children.

Incorporating contexts into recommendation algorithms is an im-portant research problem in CARS. Several CARS algorithms have been developed by incorporating contexts in different ways: Dif-ferential context modeling (DCM) [11] is one approach coupled with the traditional neighborhood-based collaborative fi ltering (N-BCF), where contexts are considered as constraints to be applied to different functional components in NBCF. Extensions of standard matrix factorization (MF) approach have incorporated contextual variables too, such as time-aware MF [3] and context-aware matrix factorization (CAMF) [2]. CAMF works well in context-aware rec-ommendation, but it is dif fi cult to interpret the latent features in the algorithm. DCM is good for explanation but it may only work well on context-aware data sets with dense contextual ratings. Thus, it is important to develop one contextual algorithm taking the advan-tages of both NBCF and MF  X  working well in contextual recom-mendation and also easy to interpret the contextual effects.
Sparse LInear Method (SLIM) is a version of an approach de-signed for top-N recommendation in the traditional RS, where it utilizes the intuition of item-based K -nearest neighbor (ItemKNN) collaborative fi ltering and make use of the learning process in M-F techniques to estimate the coef fi cients between every two items, which is demonstrated as a more effective recommender than the state-of-the-art recommendation algorithms. In this paper, we pro-pose to incorporate contexts into SLIM and develop contextual S-LIM (CSLIM) recommendation algorithms which are demonstrat-ed as more promising models for top-N context-aware recommen-dation compared with the state-of-the-art CARS algorithms. Typi-cally, contextual ratings can be estimated by a deviation-based [15] or similarity-based way. In this paper, we mainly introduce how to utilize those two ways to build the CSLIM algorithms, present the preliminary results and also discuss our ongoing work, as well as the challenges in the future.

The remainder of this paper is organized as follows. Section 2 positions related work in CARS domain. Section 3 introduces the SLIM approach and Section 4 shows our current state of our re-search on developing deviation-based CSLIM algorithms. Sec-tion 5 presents our ongoing work and challenges on a general C-SLIM model, where other possible models, such as similarity-based CSLIM approaches are also introduced and discussed. Finally, con-clusions are made in the Section 6. Traditional recommendation problem can be modeled as a two-dimensional (2D) prediction  X  R : Users  X  Items  X  Ratings , while CARS try to additionally incorporate contexts to estimate user pref-erences using a " multidimensional " rating function  X  R : Users  X  Items  X  Contexts  X  Ratings [1]. In the following section, we main-ly introduce the development of CARS algorithms base on collab-orative fi ltering (CF), including both the traditional neighborhood-based collaborative fi ltering (NBCF) based and matrix factorization (MF) based approaches.

Differential context modeling (DCM) [11] is one approach cou-pled with NBCF, and there are two series of approaches in DCM: d-ifferential context relaxation (DCR) [9, 10] and differential context weighting (DCW) [12]. In DCM, contexts are considered as con-straints being applied to different functional components in NBCF, where DCR uses a relaxation strategy and DCW tries to weight the importance of each context dimension. Those models are good for explanations, where we have demonstrated this bene fi t by explain-ing the emotional effects in recommender systems [13]; however, DCM may only work well in the context-aware data sets with dense contextual ratings. They work well by measuring prediction errors but not that good in ranking metrics (e.g. precision, etc) as shown in our previous work [14]. It is because users may not rate items multiple times in different contextual situations. As a result, it is easy to introduce biases to the DCM models, which further results in the over fi tting problem.

Contexts were also incorporated into the MF-based algorithm-s. Karatzoglou et al. [4] proposed to use tensor factorization (T-F) to integrate contexts. Their approach assumed that contextual variables are independent of other dimensions and contexts can be directly considered as the additional ones in the multi-dimensional rating space, in addition to the traditional user and item dimension-s. But the computation cost in TF is prohibitive increasing expo-nentially as the number of contextual variables increases. Context-aware matrix factorization (CAMF) [2] was then developed by mod-eling contextual dependencies with the user or item dimensions. But CAMF was built based on traditional MF [5], where both user-s and items are represented by factor weights in a matrix. There are also other approaches, such as those based on factorization ma-chines [8]. They work better than DCM but the issue in those MF-based algorithms is that it is dif fi cult to extract patterns from the latent factors to interpret the contextual effects. Sparse linear method (SLIM) is an approach designed for top-recommendations. It improves upon the traditional item-based nearest neighbor (ItemKNN) collaborative fi ltering by learning, di-rectly from the data, a sparse matrix of aggregation coef fi cients that are analogous to the traditional item-item similarities [6].
Assume that there are M users, N items, and let us denote the associated 2-dimensional rating matrix by R .Weuse u i to denote user i and t j to denote the item j .Anentry, R i,j ,inmatrix represents u i  X  X  rating on t j . A row vector in R representing ratings over all items is denoted by R i, : ,and R : ,j denotes a column vector in Matrix R representing ratings on item t j from all users.
In SLIM, the ranking score for user u i on item t j is represented by
S i,j , which is calculated by a sparse aggregation of the ratings on the other items that have been rated by u i . We denote this ap-proach as SLIM-I, since it assigns a N by N non-negative matrix W to represent the aggregation coef fi cients between each two item-s. Accordingly, S i,j can be estimated by Equation 1.

An example of how the SLIM-I approach works is shown in Fig-ure 1. To estimate the ranking score of u 2 on item t 1 , SLIM extracts u  X  X  ratings on the other items, and aggregates the ranking score by multiplying of those ratings by the corresponding coef fi cients be-tween the item t 1 and t h (i.e. t h belongs to one of the other items u has rated) in matrix W . The estimated ranking score will be used to rank the items to fi nally provide top-N recommendations. Similarly, a SLIM-U approach can be easily derived, where calculated by a sparse aggregation of ratings on this speci fi c item but were given by other users, where W is a M by M non-negative matrix representing the coef fi cients between every two users.
Traditional matrix factorization techniques, such as those pro-posed by Koren et al. [5], usually represent users and items by a set of weights on individual latent factors, and both the number of latent factors and training iterations are required to tune the mod-el and fi nd the optimal solution. In contrast to those approaches, the sizes of matrices R and W are fi xed and there is only a single matrix, W , required to be learned in SLIM-I and SLIM-U. W so easy to be interpreted, where the coef fi cients between each two items are analogous to item-item similarities. In this section, we mainly introduce our current work on how to build CSLIM approaches based on contextual rating deviations. Analogously to context-aware matrix factorization techniques [2], we propose to incorporate context into the SLIM approach for top-N recommendations by modeling contextual rating deviations. We refer to this as the Deviation-Based Contextual SLIM (CSLIM) ap-proach [15]. In the following, we use contextual factor to refer to the contextual variables such as "Time", "Location", and "Compan-ion". The term contextual condition refers to a speci fi cvalueina contextual factor. For example, "weekend" and "weekday" are two contextual conditions for the "Time" contextual factor.
Assume there are F contextual factors and L contextual condi-tions in total in a context-aware data set, in addition to the and N items. The primary task in the CSLIM approach is to esti-mate the ranking score S i,j,c for user u i on item t j in contexts tual situation. For example, assume all contextual conditions with L =4 can be represent by { Time= weekend ,Time= weekday , Loca-tion= school , Location= home } . Then, the vector c = &lt;1, 0, 1, 0&gt; indicates that the current contextual situation is { Time= weekend , Location= school } .

In CARS, users X  preferences may vary from context to context for same item. Therefore, it is necessary to make predictions based on rating pro fi les in the same context c . Recall that the estimated ranking score matrix S in SLIM-I is derived based upon the idea in ItemKNN  X  aggregation of users X  ratings on other items. As a result, it is possible to estimate the ranking score S i,j,c aggregation of u i  X  ratings on other items in the same contexts However, a context-aware data set, with multiple ratings for each item, is usually very sparse  X  it is not guaranteed that a user has rated other items in the same context c . In this case, we estimate u  X  X  rating on an item t j in context c (i.e. R i,j,c ) based on the user X  X  non-contextual rating on this item (i.e. R i,j , ratings without con-sidering contexts) and the aggregated contextual rating deviations (CRD, i.e., the rating deviations in different contextual conditions). Accordingly, we can build different CSLIM models based on how to estimate the CRD.

In this section, we present how CSLIM-I models can be built based on how to estimate the CRD. Accordingly, similar ways can be applied to SLIM-U in order to build CSLIM-U models. First, we estimate contextual ratings (i.e. R i,j,c ) based on the non contextual rating R i,j and aggregated CRD. Speci fi cally, we model CRD as the contextual rating deviations on items. In other words, we as-sume there is a rating d eviation for each &lt;item, context condition&gt; pair. Thus, R i,j,c can be estimated as follows: where R is the 2D non-contextual rating matrix. Typically, users may provide non-contextual rati ngs in addition to the ratings for an item in given contexts. But it is not necessary that both non-contextual ratings and contextual ratings are assigned to a same &lt;user, item&gt; entry. The average rating of each user on each item rated within multiple contexts can be added to the rating matrix if there is no knowledge about the non-contextual ratings.
We build a N  X  L CRD matrix D where each row represents an item, and each column represents an individual contextual condi-tion as shown in Figure 2. Thus, D j, : is a row in D representing CRD for the item t j in L different contextual conditions.
We can then use the SLIM-I approach to estimate the ranking score S i,j,c for user u i on item t j in contexts c . This is described in Equation 3, where t h belongs to the set of items for which user u has provided non-contextual ratings (i.e. R i,h &gt; 0). The ranking score is estimated by an aggregation of user X  X  ratings on other items in the same context c . We call this approach CSLIM-I-CI because the CRD are assumed for each &lt;item, context condition&gt; pair, and D is built as a CI matrix (i.e. columns and rows represent contexts and items respectively).

It is important to note that we set h = j to avoid the model learn-ing from u i  X  X  non-contextual rating on t j . Generally, the CSLIM-I-CI model will learn the parameters in D and W based on each entry of known contextual ratings R i,j,c in the training set, and make predictions only relying on three matrices: non-contextual rating matrix R , CRD matrix D , and the aggregation coef fi cien-tmatrix W . Using squared error as the optimization criteria, the loss function with regularization terms can be described in Equa-tion 4, which can be solved by stochastic gradient descent (SGD) approach.  X  and  X  parameters are the learning rates. Both (e.g. W 2 F )and 1 terms (e.g. W 1 ) are included, where the regularization term is usually applied for sparse models.
Minimize D,W
In CSLIM-I-CI, D is modeled as a CI matrix, where the devi-ation is computed for each &lt;item, context condition&gt; pair. The deviations can also be considered for users  X  we assume there is a rating deviation for each &lt;user, context condition&gt; pair, where D results in a CU matrix (i.e. columns denote contexts and rows represent users). The latter mode l is called CSLIM-I-CU. In addi-tion to these two approaches, the rating deviations can also simply be viewed as the deviations only associated with each contextual condition rather than as being paired with users or items. In this case, the CRD can be represented by a L -length vector d instead of a matrix D . We call this approach CSLIM-I-C. Note that the only difference among those three CSLIM-I models is the matrix D , while the matrix W is still the same  X  a N  X  N non-negative matrix of item-item coef fi cients.

Also, contexts can also be introduced to the SLIM-U models. In this case, W is a M  X  M matrix representing user-user coef fi cients. The matrix D can be built using a similar approach as introduced in the CSLIM-I models. In other words, we can build three CSLIM-U models: CSLIM-U-CI, CSLIM-U-CU, CSLIM-U-C.

In a summary, CSLIM-I and CSLIM-U models utilize the intu-itions behind ItemKNN and UserKNN, respectively, with the CRD matrix computed in one of three ways: CRD associated to each individual contextual condition, CRD associated with each &lt;item, contextual condition&gt; pair, or CRD associated with each &lt;user, contextual condition&gt; pair, which results in six CSLIM models.
The complexity of CSLIM models is associated with the num-ber of users or items and the number of contextual conditions. For large-scale recommendations, contexts can be selected in advance based on contextual relevance [7], and feature selection can also be applied to reduce the number of user or item dimensions in compu-tation process, which was explored in the original SLIM algorith-m [6]. The process of this feature selection is necessary and im-portant, especially when it comes to large scale of data sets which result in increased computational cost to learn the large matrix W or D. In CSLIM approaches, there are only two matrices to be learned  X  the CRD matrix D and coef fi cient matrix W , and those two ma-trices can be easily used for explanation, where D interprets how the rating is deviated from non-contextual rating when user is under different contextual conditions, and W measures the coef fi cients between each two items which are analogous to item-item similar-ities in NBCF. We have successfully evaluated the deviation-based CSLIM algo-rithms over fi ve context-aware data sets 1 , where CSLIM approach-es are demonstrated as more promising and effective context-aware recommenders which provide substantial improvements in the top-N recommendation task, compared with the state-of-the-art CARS algorithms, including TF [4], CAMF (i.e. CAMF-CI, CAMF-CU, CAMF-C) [2] and context-aware splitting approaches (CASA) us-ing biased MF as the recommender (i.e. item splitting, user splitting and UI splitting) [14]. Due to limited space, we do not present ex-perimental results here, please refer to our preliminary work on the deviation-based CSLIM algorithms [15]. Recall that the deviation matrix D in the CSLIM algorithms esti-mates the contextual rating deviations from a non-contextual rating one. Actually, the non-contextual rating can be considered as a special contextual rating, where the contextual conditions are al-l EMPTY. Thus, the deviations in D can be viewed as the ones deviated from EMPTY contexts.

In addition to the CSLIM models, a general CSLIM (GCSLIM) approach can be built, where the contextual deviations could be the ones deviated from any contexts other than the EMPTY contexts. In this case, the matrix D is expected to be a CC matrix, where the rows and columns represent the L contextual conditions, and each entry in D represents the rating deviation from a context to another. We can build two GCSLIM models: GCSLIM-I-CC and GCSLIM-U-CC. Take the GCSLIM-I-CC model for example, it is derived from the CSLIM-I model, but user X  X  contextual rating on the items in c -1 (Equation 2) can be estimated by user X  X  rating on the items within other contexts. As shown in Table 1, we have no knowledge values are 0s which represent the EMPTY contexts. Previous C-SLIM models only estimate the contextual rating deviations from c -3 to c -1 , but in GCSLIM models, the D is in shape of a CC matrix and it is able to estimate the R u,i,c -1 from any contextual ratings, D (Weekday, Weekend). Namely, multiple contextual ratings can be used to train the matrix D in the GCSLIM-I-CC model instead of using non-contextual ratings only in the CSLIM models.
Similarly, the CC matrix can be paired with either users or item-s. But it would be expensive to assign a CC matrix to each user or item. A possible solution is to assign a CC matrix to each group of users or items, where users or items can be put into different groups by clustering techniques. Due to that a user may give multiple rat-ings on an item within different contexts, using all contextual rat-ings for training may leave a challenge on computational ef fi ciency. In our future work, we X  X  like to develop strategies to select limited contextual ratings for training, for example, we can simply use the contextual rating where its contexts are the most similar ones to the target contexts. For example, in the table above, c -3 is more simi-lar to c -1 than c -2 , because there are more same bits in the binary contextual vector.
 Currently, those models introduced above are named as Deviation-Based CSLIM approaches. Apparently, the contextual ratings can be derived by contextual similarity other than CRDs. Take the ex-ample shown in Table 1, R u,i,c -1 can be estimated by R Sim (Kids, Partner)  X  Sim (Weekday, Weekend). In other word-s,
D can be replaced by a contextual similarity matrix Sim which measures the similarity between each two contextual conditions. We name those approaches as Similarity-Based CSLIM and we will explore them in future.

Beyond those deviation-based and similarity-based CSLIM ap-proaches, contexts may also be directly incorporated into the coef-fi cient matrix W instead of additionally creating the CRD matrix, which could be a new series of contextual SLIM algorithms. The underlying idea behind is to infer the users X  contextual rating in contexts c by users X  contextual ratings in other contexts (including ratings placed in the EMPTY contexts) multiplying by the coef-fi cients of different contextual conditions. In other words, those models can be directly derived from the original SLIM-I models, where the CRD matrix is not required and there is only one matrix W (size: L  X  L) which measures coef fi cients between every two contextual conditions. Those models can be named as CSLIM-C . Also, the coef fi cient matrix W can be assigned to each user or item too. To reduce the learning complexity, we can assign a W group of users or items. In this paper, we introduce and discuss how to incorporate contexts into the SLIM models to build CSLIM algorithms. More speci fi -cally, we mainly introduce our current state of research on build-ing deviation-based CSLIM approaches and our ongoing work on a more general CSLIM model, as well as the similarity-based C-SLIM algorithms. In addition, CSLIM-C could be a new series of CSLIM approaches which incorporate contexts to the coef fi cient matrix W . Our current experimental results on deviation-based C-SLIM demonstrate that CSLIM algorithms are promising and pro-vide substantial improvement over the state-of-the-art CARS algo-rithms. Furthermore, the matrices D and W in CSLIM are easier for explanations to further interpret the contextual effects. We will continue to work on CSLIM models and explore the proposed new CSLIM approaches in our future work.

