 that patents contain 90%-95% of the information in today's scientific and technological innovations. For any enterprise, patents are the key technical information that must be made public. By analyzing patent data, people can obtain valuable information to avoid wasting money on redundant research and to prevent property rights violations. Before any technical innovations, researchers must thoroughly learn about the existing patents in the targeted domain[1]. patent mining. Patent technology effect matrix analysis has attracted much of different domains differ much in content. To guarantee high recall, we execute the extraction in one domain each time. 
At present, TEC extraction depends on technology effect phrases ( TEP ). In the domain of Chinese patent mining, few algorithms are able to effectively extract TEP's without much human assistance. To ensure high precision and recall, patent mining algorithms invariably rely on manual extraction, rather than traditional keyword extraction algorithms. Most of these algorith ms consider word frequency as a decisive component in determining keywords. However, the uniqueness of patents makes word frequency in patent abstracts fairly low. Therefore, we propose a novel method that is independent of word frequency and that requires little human involvement. 
This paper is organized as follows: Section 2 discusses related works. Section 3 presents our algorithm and its improvements. Section 4 shows our experiment results. Section 5 summarizes this paper and discusses future work. 2.1 Technology Effect Annotation in Patent Data technologies are also developing fast. Current researches mainly focus on the annotation of patent function, technology, and composition parts[3]. In English and Japanese patent processing, large amount of manually annotated patent data have provided good corpora for some machine learning algorithms, such as the rule-based method by Peter Parapaics[5] and many supervised training methods proposed on NTCIR Workshops[4]. Using these algorithms, people have achieved remarkable results in Chen[3] proposes a semi-supervised co-training algorithm to annotate TEC's in the algorithm achieves an F-Measure over 80%. However, in each iteration, manual processing is needed to extract TEP's from newly identified TEC's. Too much human labor prevents this method from practical use. 2.2 Keyword Extraction and Technology Effect Phrase Extraction Many state-of-the-art algorithms are available for extracting keywords from general text. Y. Matsuo[6] et al. propose a domai n-independent keyword extraction algorithm that needs no corpus. The algorithm measur es the distribution of the co-occurrence of frequent terms and other terms to determine the importance of words. It achieves good performance in experiments on general text, but the importance of TEP's is determined by their semantic meanings, rather than how important they seem according to the rules in [6]. Besides, traditional keyword extraction algorithms are dependent on word frequency, but TEP's are hard to distinguish from other phrases in this sense. Ni W.[7] et al. propose an under-sampling approach to extract imbalanced key phrases in general text. The algorithm is based on a training set represented by two views of the samples, i.e. the morphology view and occurrence view of the (non-)key phrases. In TEP extraction, however, the occurrence of TEP's are hard to distinguish from other phrases. Besides, we have tried various approaches to identify TEP's using their morphologies by an LVQ neural network as a classifier but none of them works well. The difficulty lies in isola ting the morphologies belonging only to TEP's. Ying Chen[2] et al. propose a method based on patent structure-grammar-keyword feature to identify the TEP's in algorithm, and the construction of DII demands much processing by patent experts, the algorithm is not successful in reducing human labor. 2.3 Parsing Chinese Patent Data Roger Levy[8] et al. carefully investigate th e errors that Stanford Parser[9] makes in parsing general Chinese text and provide some simple and targeted improvements for each case. These improvements enable the parser to achieve high accuracy in parsing general Chinese text. Unfortunately, after parsing some Chinese patent abstracts with Stanford Parser, we find it almost impossible to distinguish the subtrees containing a identical with other subtrees in both tree st ructure and part-of-speech tagging (See Figure Sciences develops a word-segmenting and part-of-speech tagging software ICTCLAS [10]. The software does well in general Chinese text such as news, magazine articles, and literature works. Though it is now the best Chinese text tokenizer, our experiment shows that it often segments terminologies in patent abstracts into separated parts(See Figure 2). It also wrongly tags a few words in almost every patent abstract. 3.1 Definitions For convenience, we give definitions to some frequently used terms in this paper. Definition 1. Technology Effect Clause (TEC) A technology effect clause (TEC) is a clause describing the technology and effect of a patent. Definition 2. Technology Effect Phrase (TEP) A technology effect phrase (TEP) is a phrase that seldom appears in a non-TEC. Once it appears, the clause containing the phrase must be a TEC. 
There are mainly two forms of TEP's:  X  X erb + noun X  and  X  X djective + noun X . We unify the two forms by  X  X alue + Attribute X . E.g.  X   X  X  X  +  X  X  X   X  (increase + production) and  X   X  +  X  X  X   X  (high + reliability) are effect phrases, and  X   X  X  X  +  X  X  X  X  X   X  (uses + green material) is a technology phrase. Definition 3. Value and Attribute Value is the verb or adjective part in the above two common forms of TEP, and Attribute is the noun part. Definition 4. Domain-Independent Corpus (DIC) A Domain-Independent Corpus (DIC) is a pre-constructed corpus consisting of Values and Attributes shared by most patent domains. Definition 5. Domain-Dependent Corpus (DDC) A Domain-Dependent Corpus (DDC) is a corpus constructed using Values and Attributes in the targeted domain. 3.2 The Basic Idea of the Algorithm We have shown in Section 2.3 that grammar-structure-based methods fail in TEP because patent analyzers know what phrases could be TEP's, yet they hardly parse the text to see which part of the parse tree ma y correspond to a TEP. Therefore, a corpus corpus of TEP's directly is impractical for the following two problems: (We denote the set of patent abstracts by A ) Problem 1. Word redundancy is high in the TEP corpus. E.g. the Value  X   X  X  X   X  (improve) can be matched with many Attributes such as  X   X  X  X   X  (quality),  X   X  X  X   X  (production), and  X   X  X  X   X  (efficiency). Including all these phrases into the corpus seems unnecessary. corpus to extract the patent abstracts. For Problem 1, we partition the corpus into a  X  X alue corpus X  and a  X  X ttribute corpus X . The algorithm first performs a Cartesian product of the Values and the a TEP candidate. 
We address Problem 2 based on a fact we observe in our experiments: the content of patent data in a certain domain is highly domain-dependent. Let B be a subset of A formed by random sampling. The number of elements in B is far less than that of A . This observation is presented as follow: are also frequently mentioned in the set A  X  B , and those technologies and effects in A  X  B but not in B are very rare. 
This observation indicates that we can construct a corpus C B using only the patent abstracts in B . The key is that corpus C B is able to generate new TEP's. There could be many different expressions for one technology or effect. E.g. let there be  X   X  X  X  +  X  X  X   X  potential to generate  X   X  X  X  +  X  X  X  X   X  (improve + stability) and  X   X  X  X  +  X  X  X   X  (increase + efficiency), which are quite likely to appear in set A  X  B . Further observations show that there are domain-independent Values and Attributes. Values such as  X   X  X  X   X (improve) and  X   X  X   X (prevent) and Attributes such as  X   X  Meanwhile, Values like  X   X   X   X (brake) and  X   X   X   X (burn) and Attributes like  X   X  X   X (oil consumption) and  X   X  X  X  X  X   X (mechanical efficiency) are dependent to the domain of diesel engine. To further reduce human labor in corpus construction, we partition both the corpora of Values and Attributes into domain-independent corpus ( DIC , Definition 4) and domain-dependent corpus ( DDC , Definition 5). We have constructed DIC for abstracts. When a patent analyzer wants to extract patent abstracts in a new domain, he only needs to manually extract domain-dependent Values and Attributes (especially Attributes) from a small number of patent abstracts. Then the algorithm will incorporates the DIC and DDC and eliminates duplicate elements during extraction process. We show the details of the partitioning in Figure 3. 3.3 Improving Precision and Recall Though recall of the method in Section 3.2 is acceptable, its precision is only around 50%. This is due to some certain common Chinese language grammar structures. We accordingly propose the following rules to improve precision: Rule 1. If the character  X   X   X  appears in a clause and there is a Value identified right before this  X   X   X , then this Value is considered to be the only Value of this clause. 
The necessity of this rule is substantiated by the abundance of TEP's presented in the form "Value+  X  +Attribute". E.g. in clause  X   X  X  X   X  X  X  X  X  X   X  X  X  X  X  X  X   X  (decrease  X   X  X  X   X  (cost), ignoring the verb  X   X   X   X  (produce). Rule 2. If a Value matches several Attributes in a clause, and none of these Attributes is a substring of another, we check if there is an Attribute right behind a character  X   X   X . If so, we match the Value with only this Attribute. 
This rule is needed for a specific yet common case where an Attribute is embellished by another word before it. E.g. in clause  X   X  X  X   X  X  X  X  X  X  X   X  X  X  X  X  X  X   X ( increase the mileage per unit fuel),  X   X  X  X  X   X ( mileage) rather than  X   X  X  X   X (fuel) is the correct Attribute.
 Rule 3. If a Value matches several Attributes in a clause, and some of these Attributes are substrings of others, only the longest Attribute should be matched. 
This rule is designed to eliminate obvious duplications. Still using the example of  X   X  X  X   X  X  X  X  X  X  X  X  X  X  X  X  X  X  X   X ( increase the mileage per unit fuel), if  X   X  X  X  X  X  X   X  X  X  X  X  X  X  X  X   X ( mileage per unit fuel) appears so frequently that it is included in the DDC of Attribute, it is unnecessary to consider  X   X  X  X  X   X (mileage) as an Attribute.

Summarizing Section 3.2 and 3.3, we now give a formal description of our algorithm in Table 1. A flowchart of the entire method is shown in Figure 4. 
The main step of the algorithm may seem straightforward. However, we have made great efforts trying to come up with algorithms based on supervised learning, referring to specific job of TEP extraction. Because they either show disappointing recall and precision or require too much manual processing. Compared with these methods, our method substantially reduces manual processing while achieving relatively high precision and recall. We also compare our method with two standard keyword extraction algorithms in Section 4.4. Our method achieves obviously higher recall and precision. 3.4 The Number of Manually Extr acted Patent Abstracts In Observation 1 a lower bound M is given for the number of manually extracted patent number. Suppose there are 100 patent abstracts in a domain, a patent analyzer may need to manually extract 20 abstracts to cover the domain-dependent Values and Attributes. But when the number rises to 1000, only about 60 abstracts should be pre-extracted. Since all these patents belong to one domain, the rest 940 patents are all focusing on what the 60 patents are doing. Therefore, the more patents in a domain, the less the portion for manual extraction, and the more cost-effective the method. We give the following function as a reference: patent abstracts in a domain. We use logarithm because we find the increment rate of y Section 4.3. We download over 3000 patent abstracts from the website of State Intellectual Property Office of China. These patents are distributed in 7 domains, each consists of 102, 160, 263, 354, 459, 648, 1042 abstracts respectively. We first make a comparison between show how precision and recall change as the number of manually extracted abstracts grows, and draw some interesting conclusions. Then we provide details on how the parameters in formula (1) are determined. Finally we compare our method with two standard keyword extraction algorithms. 4.1 Precision Increment by the Rules We compare the precision before and after the introduction of each rule in Section 3.3 on the 7 domains respectively. We manually extract 20% abstracts in the first 4 domains and 10% abstracts in the following 3 domains 1 . The results are shown in Figure 5. A significant increase in precision occurs after each rule comes into play. We do not plot changes in recalls in Figure 5 because their decreases are negligible, and these rules are relatively conservative in this sense. 4.2 Precision and Recall To test the overall precision and recall of our method, we run the algorithm on the 7 domains of abstracts respectively. For each domain, we start from zero manual extraction, then each time we manually extract a new 3% abstracts and add the Values and Attributes into DDC. We measure the precision and recall of TEP extraction based on Definition 2. Results are presented in Table 2 and Figure 6. 
In Table 2, the underlined data represent the highest F-Measure achieved in that domain while precision is over 70%. The algorithm achieves F-Measures over 75% in all 7 domains, with recalls over 80% and precisions over 70%. 1
Though precisions are obviously lower than full manual processing, patent retrieval is employed to do the extraction, things are quite on the contrary: Non-TEP's are easy to spot Chinese patent data[3]. With a little human involvement in excluding obvious non-TEP's, our method is able to complete in a few seconds what takes a human several days. 
When the algorithm runs without DDC, the recalls are extremely low. Once a more abstracts are included, the increment rates drop, and the growth basically stops when recalls reach over 80%. This indicates a lower bound for the amount of manual extraction. On the other hand, precisions increase only in the beginning 2 . Then they are on a steady decrease because manual extraction introduces "noises" to DDC. Thus an upper bound of manual extraction amount is expected. This relationship is better seen from the F-Measures. In each column there is a peak for F-Measure. The corresponding M.E. value is what we recommend as the ideal manual extraction amount in each extraction is recommended, and our method is therefore more cost-effective. 2 4.3 Determining the Number of Abstracts to Manually Extract Formula (1) in Section 3.4 helps a patent anal yzer to decide how many patent abstracts to determined. determined as follow: Denote ln ( x ) by t . For each domain in Table 2, take the natural logarithms of the total abstract number as a value of t , take the product of the "ideal Figure 7. 4.4 Comparing with Other Methods We compare our method with two standard keyword extraction algorithms: the TF-IDF algorithm and Matsuo's algorithm[6] for extracting keywords in a single document. We method, and segment all sentences into words by ICTCLAS[10]. Finally we choose the from the merged file by Matsuo's method. The result is shown in Figure 8. Our method outperforms the two algorithms in both precision and recall. The poor performance of the first two algorithms is mainly a result of their inherent ineligibility. Though they have TEP's. The two algorithms are also impaired by the inaccuracy in word-segmentation by ICTCLAS. In contrast, our method is free from such influences of word tokenizer. Technology effect phrase (TEP) extraction is an indispensable part of many patent mining algorithms. To reduce human labor in the extraction, we propose a method based on partitioning corpus to extract TEP's from Chinese patent abstracts. To further increase precision, we propose 3 rules to deal with some specific yet common cases. We also find that the larger the patent domain, the more cost-effective the method. We give a formula function indicates that human workload will be acceptable even when the number of patents is enormous. 
In future, we will try to further increase the recall of our method. According to our experiments, adding more patent abstracts into DDC alone does not help, because the F-Measure will drop fast due to the increasing loss of precision. How to increase recall without sacrificing precision will be our future work. Acknowledgement. This work is supported by the Key Program of National Natural Science Foundation of China (61232002) and the National "863" High-tech Research Development Plan Foundation (2012AA011004). 
