 Query-based multi-document summarization aims to create a short summary given a collection of documents and a query. Most of t he existing methods treat the query as one single sentence and r ank the sentences in the documents based on their similarities w ith the query sentence. However, these methods lack of intensive an aly-sis on the given query which typically consist of several top ic as-pects. In this paper, we propose a topic aspect extraction me thod to discover the aspect words and sentences contained in the q uery narrative texts and the input documents, and then incorpora te these aspect words and sentences into a cross propagation model ba sed on the sentence-term bipartite graph for document summariz ation. Experiments on DUC benchmark data show the effectiveness of our proposed approach on the topic-driven document summari za-tion task.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval; I.2.6 [ Artificial Intelligence ]: Learning Algorithm, Experimentation, Performance Cross Propagation, Topic Aspect Analysis
There are many recent studies on query-based topic-relevan t doc-ument summarization. Generally, the best sentences are con sidered the ones that are most similar to the query and least similar t o the text that is already in the summary. To better understand the infor-mation in documents, learning methods such as clustering or graph analysis are conducted on the document set. For example, a no n-negative matrix factorization (NMF) based query-focused s umma-rization method is proposed in [9], which uses the cosine sim ilarity measure between the expanded query and the semantic feature s ob-tained by NMF to rank sentences; Wan et al. [13] make use of bot h the relationship among all the sentences in the documents an d re-lationship between the given query and the sentences by mani fold ranking; probability models have also been proposed under d iffer-ent assumption on the generation process of the documents an d the queries [2, 6, 11].

Most of these existing methods treat each query as one single unit and rank the sentences in the documents based on the sim-ilarity between each sentence and the topic. However, a quer y usually consist of several topic aspects which can be furthe r ex-tracted and analyzed. A very recent work [12] conducts subto pic analysis for document summarization. In this work, explici t or im-plicit subtopics are discovered using heuristic syntactic rules and term co-occurrence. However, some of the terms in the topics may dominate all the subtopics, thus a more intensive and robust topic analysis method is needed.

In this paper, we first propose a novel topic aspect extractio n method to discover words and sentences that represent each a spect embedded in the queries, and then a cross propagation approa ch on sentence-term bipartite graph is applied to transfer kno wledge between sentence side and word side to finally cluster senten ces in the documents into different groups related the topic aspec ts.
One thing worth mentioning is that this paper is the first work to bring two interrelated but distinct themes from machine l earning for document summarization: semi-supervised learning and learn-ing from labeled features. The goal of the former theme is to l earn from few labeled examples by making use of unlabeled data, wh ile the goal of the latter theme is to utilize the knowledge about term-aspect relationships. Empirical results in this paper demo nstrate that simultaneously integrating both these themes leads to better summarization systems.
Figure 1 presents the framework of our proposed approach. Fi rst of all, the documents are decomposed into sentences and prep ro-cessed into a sentence-term matrix for later use. In the mean time, the query narrative is mapped into aspects using a topic aspe ct ex-traction engine. Then a semi-supervised cross propagation algo-rithm is developed to group sentences into aspect oriented c lusters. Finally, the most important sentences are selected based on an inner cluster sentence selection scheme to form the summary.
Given a set of queries, we define  X  X opic X  and  X  X spect X  as two orthogonal dimensions of queries. The topic defines the pers on/ or-ganization/ location/ thing/ event etc. while the aspect re presents a particular fact related to the topic. Table 1 gives an examp le query from DUC collection, where the query is composed of the title and the narration. In this example, the topic is relate d to the na-Query ID D0601 Title Native American Reservation System -pros and cons
Narrative Discuss conditions on American Indian reservations Topic Native American Reservation System Aspects conditions, benefits, drawbacks, privileges, problems tive American reservation system, and the expected answers should come from the aspects like  X  X onditions X ,  X  X enefits X ,  X  X rawb acks X ,  X  X rivileges X  and  X  X roblems X .

Instead of performing deep grammatical analysis to queries , we extract aspects from a set of queries based on their orthogon ality with the topics. First, the narration of a query is tagged wit h part-of-speech, and ordinary nouns which do not appear in the titl e of the query are selected. Note that we do not use the keywords in title sentences because these title words are likely to be associa ted with the general topic described in the corresponding document c ollec-tion, and they do not discriminate specific aspects under the general topic. We only select ordinary noun words because proper nou ns are often referred to certain entities rather than abstract aspects.
These candidate words in the first step may include aspect wor ds and words highly related to a particular topic. Thus we calcu late the entropy of the topic distribution for each word candidate to measure whether the word should be included as an aspect word. Formal ly, we have Assuming p ( t ) is a uniform distribution, p ( t | w ) can then be calcu-lated as where p ( w | t ) is estimated on the document collections using maxi-mum likelihood estimation. This aspect extraction approac h is par-tially inspired by the method for choosing pivot features in domain adaptation, where the features occur frequently and behave simi-larly in both the source and target domains [1]. Here, our ext racted aspect words can be viewed as pivot features over multiple do mains specified by topics in queries.

Based on the calculated entropy, top k words are selected as as-pect words, where k is set to 30 empirically. Finally, we cluster the extracted aspect words into several aspects using transfer closure by checking whether a pair of words are synonym in WordNet. Once we have the aspect words, we initially select one sentence sh aring the most aspect words for each aspect as the initialization o f the cross propagation algorithm described in the next section.
Once we have the partial label information on words and sen-tences indicating that some words and sentences are related to the aspects, our task is to assign sentences into the aspects, an d then se-lect the most important sentences for each aspect to form the sum-mary.

Assigning sentences into aspects with partial label inform ation can be viewed as a semi-supervised learning problem [15]. Ho w-ever, different from traditional semi-supervised learnin g problems where partial label information is only given on instances, here we have partial label information on both instances (i.e., sen tences) and features (i.e., words). A natural question is how we can u se both the partially labeled features (i.e., words) and insta nces (i.e., sentences) to improve the summarization performance or how we can transfer the knowledge on the word space to the sentence s pace to improve summarization performance [7].

In this paper, we apply a cross propagation algorithm based o n the sentence-term bipartite graph to transfer knowledge be tween word space and sentence space, and obtain the sentence clust ers.
In general, a bipartite graph can be represented by an m  X  n rectangular non-negative matrix (a contingency table) P . There are two types of nodes: x-nodes (each is represented by a column i n the table P , and y-nodes (each is represented by a row in the table P ). P ij denotes the weight between y -node i and x -node j . For ex-ample, in the sentence-term associations, x -nodes represent terms while y -nodes represent sentences, and P ij could be the frequency of term j in sentence i . The sentence-term matrix is:
Among all n x-vertices { x i } n i =1 and m y-vertices { y the bipartite graph, some vertices are already labeled. We fi rst con-sider 2-class problems, where h i =  X  1 for labeled data and h for unlabeled data. Our task is to learn the classification fu nction f = [ f x , f y ] T = [ f ( x 1 ) , . . . , f ( x n ) , f ( y results can be easily extended to multi-class problems such as the sentence clustering problem which is critical to our docume nt sum-marization task.

To be consistent, one requirement is that the error on the lab eled data needs to be minimized while the smoothness of certain qu an-tities (e.g., derivatives) needs to be ensured. Based on the theory of Reproducing Kernel Hilbert Space (RKHS), we can minimize th e following objective function:
J [ f ] = At the large  X  limit, we obtain: where h = [ h x , h y ] T = [ h x 1 , . . . , h x n , h y [3], we use the Laplacian Kernel as K = ( D  X  W )  X  1 + which is based on combinatorial Laplacian D  X  W , where D = diag ( d ) , d = P j W ij , and the positive part () + implies that the zero mode is excluded.

In label propagation for multi-class problem, Eq.(5) can be ex-tended as where H = ( h 1 , . . . , h K ) for K classes. The input H is specified as: if labeled data point x i has a known class label K , H H ik = 0 otherwise. After F is computed, we assign x i to class k = arg max j F ij .

Since it is a bipartite graph, we can calculate the [ D  X  W ]  X  efficiently as following [3]: ( D  X  W )  X  1 + = where Thus we obtain There are two terms contributing to F y k . The first term involving h x is the standard one. The second term involving F y k suggests that the partial labels on column variables propagate to row vari ables. Suppose P is a sentence-term matrix. This shows partial labels on sentences can also affect the labels on words. Similar resul ts apply to column labels F y k . From the derived algorithm, we clearly see that the information on one type of nodes can be propagated to the other type of nodes. Thus we call it cross propagation [3].
After we obtain the sentence clusters, we scan the clusters i n circular order, and select one sentence from a cluster for on e time until the length limitation is met. We start from the largest cluster.
Within a cluster C k , we calculate a combined score for each sen-tence s i Score ( s i ) =  X  where K is the number of sentences in cluster C k . The first term is the average similarity between the sentence and the rest sen tences in the cluster, and Sim ( s i , query ) is the similarity between the sentence and the given query, both of which are calculated us ing cosine similarity.  X  is a weight parameter, and is set to 0.6 empiri-cally.
In this section, we conduct experiments on DUC benchmark 1 data to compare and evaluate individual and consensus summa riza-tion performance.

We use the DUC05 and DUC06 data sets to test our proposed method empirically, both of which are the latest open benchm ark data sets from Document Understanding Conference (DUC) for generic automatic summarization evaluation. Each data set con-sists of 50 topics. The task is to create a summary of no more th an 250 words for each topic to answer the information expressed in the topic statement.

In the experiments, we implement some widely used and sev-eral recently proposed multi-document summarization meth ods as follows:
LSA [5]: conducts latent semantic analysis on terms by sen-tences matrix.

NMF [9]: performs NMF on terms by sentences matrix and ranks the sentences by their weighted scores.

Centroid [10]: applies MEAD algorithm to extract sentences according to the following three parameters: centroid valu e, posi-tional value, and first-sentence overlap.

LexPageRank [4]: first constructs a sentence connectivity graph based on cosine similarity and then selects important sente nces based on the concept of eigenvector centrality.

Seman [14]: performs semantic role analysis and clusters the sentences on sentence-sentence similarity matrix via symm etric non-negative matrix factorization.

MultiMR [13]: uses a manifold-ranking algorithm by consider-ing the within-document sentence relationships.

MultiMR-Topic [12]: analyzes explicit and implicit subtopics for query-relevant document summarization.

The first four methods are the widely used techniques for docu -ment summarization. Seman and MultiMR are two recent resear ch efforts on query-relevant document analysis. All the above methods only use the query as a single sentence and do not perform any f ur-ther analysis. MultiMR-Topic is the only one conducting sub topic analysis on the topic narrative texts. We compare our system , re-ferred as AspectCP , with the above baselines as well as the best team from DUC competition, referred as DUCBest, which decom -poses a complex query into sub-questions using sets of heuri stics. ROUGE-1, ROUGE-2, ROUGE-W [8] are used as the evaluation measures. Table 2 and Table 3 show the ROUGE evaluation results on DUC05 and DUC06 data sets for query-relevant multi-documen t summarization.
 Table 2: Overall performance comparison on DUC05 using ROUGE evaluation methods. http://duc.nist.gov Table 3: Overall performance comparison on DUC06 using ROUGE evaluation methods.

From the comparison results, we have the following observa-tions: (a) The new methods proposed in recent years (e.g., Se man and MultiMR) improve the summarization results much by usin g various advanced techniques such as semantic analysis and m an-ifold ranking. (b) Methods incorporating intensive topic a nalysis (e.g., AspectCP and MultiMR-Topic) outperform previous me th-ods and the results of which are comparable to the best team fr om DUC competition. However the best team of DUC uses textual entailment techniques to identify sentences that share the same se-mantic content, which requires advanced natural language p rocess-ing (NLP). AspectCP and MultiMR-Topic do not heavily rely on NLP, and both of them emphasize on the use of extracted aspect s or subtopics. (c) Our proposed method outperforms MultiMR-To pic, the very recent work in [12], which also performs subtopic an alysis. The main reasons are (1) our topic aspect extraction is more r obust and not based on heuristic rules (2) the cross propagation tr ansfers the knowledge between word space and sentence space and the m u-tual influence between them contributes to the better perfor mance. Figure 2: Results of different number of aspect words on DUC2006 using ROUGE-1.

Number of Aspect Words: We gradually tune the number of se-lected top-ranking aspect words from 10 to 60, and Figure 2 sh ows the results on DUC06 data. Since we have similar results on bo th DUC05 and DUC06 data, due to the space limit, here we only il-lustrate the results on DUC06 data. From the results, we can s ee that if the number of aspect words is too small, some subtopic in-formation may be missed. However too many aspect words may also induce noise and lead to inaccurate analysis.

Inner Cluster Sentence Selection Parameter: Now we gradu-ally tune  X  in Section 2.3 to adjust the weight between the similarity with the topic and the center of the cluster. Figure 3 demonst rates the results.
Figure 3: Results of different  X  on DUC06 using ROUGE-1.
In this paper, we first propose a topic aspect extraction meth od to conduct intensive subtopic discovery in topic-relevant mu lti-document summarization. The extracted aspect words and initial sent ences are then incorporated into a cross propagation framework to trans-fer information between word space and sentence space to ach ieve a better sentence clustering. Finally an inner cluster sent ence se-lection scheme is proposed to form the summary. Experimenta l results on DUC benchmark data demonstrate the effectivenes s of our proposed method.
 This work is supported in part by NSF grants IIS-0549280 and HRD-0833093.
