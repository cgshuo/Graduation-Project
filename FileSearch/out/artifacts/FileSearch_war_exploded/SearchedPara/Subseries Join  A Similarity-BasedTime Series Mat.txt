 Time series are composed of sequences of data items measured at (typically) uniform intervals. Time series arise frequently in many scientific and engineer-ing applications, including finance, medicine, digital audio, and motion capture. Efficiently searching for similarities in a large time series dataset is a challenging problem, especially when partial or subseries matches are needed. Most previous work has focused on either whole match or subseries match problems. Whole matches find a time series similar to a given (query) time series in a dataset consisting of a collection of time series. Such matches are one-to-one. Subseries match, in contrast, finds similar segments (which we will call subseries) in a time series dataset to given a single query time series, and is a one-to-many match. However, when time series are very long and contain complex features, whole matches will find meaningless matches. Subseries matches are more specific but still depend on matching the whole of a specific query time series. This form of match still cannot deal with two timeseries that may have subseries pairs that are similar but are not similar as a whole. To deal with this case, we introduce an operation called subseries join which matches pairs of similar subseries in a set of time series. Subseries join is a symmetric operation and produces many-to-many matches. It is a generalization of both whole match and subseries match. It is potentially useful in many data mining applications such as motif detection and anomaly detection [1]. In this work, we also present a non-uniform indexing method for time series based on intrinsic structure, and a search technique based on this structure that can efficiently compute subseries join. When restricted to matching, our approach can also tolerate both impulsive noise and additive noise better than previous approaches to these problems. In order to define the similarity of time series, we need to define a distance met-ric. The Euclidean distance metric has been used widely in the time series mining area. Various normalizations of the Euclidean distance have also been proposed, such as shifting, scaling, linear trend elimination, noise removal, and moving average filtering. However, Euclidean distance can only be used for measuring the similarity of two time series of the same length. It is also sensitive to noise and time shifting. Dynamic time warping (DTW) [2] defines another distance measure based on dynamic programming. It can detect similar time series even when they are subject to transformations along the time axis. DTW can also tolerate a limited amount of length variation, insertions and deletions, and addi-tive noise. However, DTW often fails when the differences of the lengths of time series are large, it handles changes in length by replication or deletion of samples (which is a poor interpolation method), and it cannot handle impulsive noise. An approximate normalized information distance (ANID) based on Kolmogorov complexity has been studied in [3] but was found to be sensitive to impulsive noise. To overcome such limitations above, we will define a new distance function based on a feature segmentation and continuous feature warping.

Given a long time series and a short time series, most existing subseries match methods first segment the long time series. The common segmentation method is based on sliding windows. There are three major sliding-window based seg-mentation methods: FRM [4], Dual Match [5], and General Match [6]. All these methods are based on uniform segmentation. However, uniform segmentation requires the user to manually select the length of the segments, and is not sensi-tive to the actual behavior of the data. This arbitrary segmentation may cause undesirable division of important features in the data into different segments and involves an assumption of a particular scale for features. Although over-lapping sliding windows can avoid division of features (at least at one scale), it introduces additional costs and a redundant representation.

In this work, we propose an alternative approach that divides time series data into non-uniform segments of arbitrary length based on intrinsic smoothness properties. Pairs of candidate matches are found using a spatial search, and then a dynamic programming approach is used to align candidate subsequences by continuous warping. Subseries join is a symmetric operation that returns all pairs of subseries drawn from two datasets that satisfy a given similarity threshold relative to some metric d and are also of maximal length: Definition 1. Subseries join: Given two sets of time series X and Y ,the subseries join is the set of all pairs ( X i,k ,Y j, ) of subseries X i,k  X  X k for X k  X  X  and Y j,  X  Y for Y  X  X  such that d ( X i,k ,Y j, )  X  , and for which there does not exist any X i,k  X  X i,k and Y j,  X  Y j, where X i,k is longer than X i,k and contains X i,k as a proper subset and where Y j, is longer than Y j, and contains Y or for which d ( X i,k ,Y j, ) &lt; .
 Note that a subseries join computes a subseries match if one of the input datasets X is a singleton set { X } .

We now present our approach to solve the subseries join problem. First, each time series in the dataset is smoothed and segmented by an anisotropic diffu-sion scale-space analysis [7,8]. Anisotro pic diffusion, unlike Gaussian smoothing,  X  X ins X  zero crossings of the second derivative across scales: their positions are invariant, although they are progressively eliminated. We exploit this property to create a strict hierarchy of segments that can be used as an index. Zero cross-ings of the second derivative are present at discontinuities in the data, but not all such zero crossings are useful discontinuities. Therefore, we only segment the data at zero-crossings where the gradient magnitude is also large [9].
Next, to represent features of a segment A of time series X , we use a polyno-mial P ( A, t ) to approximate its shape, but with the parameter space rescaled to the interval [0 , 1]. We can generalize this to an envelope by adding an interval to P ( A, t ) that allows us to compute conservative distance bounds. To evaluate the similarity between two segments A and B , which may be of different lengths |
A | and | B | , we define the (square of the) distance function d as follows: The first term compares shape, the second length; the ratio between these is con-trolled by varying  X  over [0 , 1]. This distance can be computed analytically from the coefficients of the polynomials and the lengths of the segments. In fact, the polynomial coefficients and the segment lengths can be placed in a single vector, and this vector can be linearly transformed so that the ordinary Euclidean dis-tances on the transformed coefficients can be used [10]. We can also transform polynomial envelopes (polynomials plus intervals which can conservatively bound the original data) into axis-aligned line segments (in the higher-dimensional ab-stract feature space) and compute minma x distance between them for conserva-tive distance bounds between functions bounded by the original envelopes. Note that we do not apply the Euclidean distance directly into the original time series data, instead we use the Euclidean distance over the feature representations in a higher-dimensional parameter space. Th is distance function can deal with time series of different lengths easily and interpolation of shape is continuous.
We then build an index by inserting features at all scales of the dataset into an R-tree in the transformed feature space. Every time series in the dataset is represented as a feature sequence. An R-tree join operation [11] can be used to obtain candidate pairs of features whose distance is less than a predefined threshold. Based on the associated leaves of the R-trees, pairs of feature se-quences can be found by counting the number of pairs of matching features from each sequence. If this number is greater than a predefined threshold, these two feature sequences are taken as a pair of candidate matching feature sequences. Finally, we use a sequence alignment algorithm based on dynamic programming [10] to align two candidate matching feature sequences; from this alignment we can compute an overall metric for the entire match by summing the metrics for the matching features. To evaluate effectiveness of our approach (SJ in the following), we consider two baseline approaches: dynamic time warping (DTW) [2] and approximate normal-ized information distance (ANID) [3]. We will limit our comparisons to clustering and matching problems since previous solutions did not include the concept of subseries join.
 All testing time series data are extracted or synthesized from the UCR Time Series Data Mining Archive [12]. A set of 40 time series D were extracted from the UCR Time Series Data Mining Archive. The total number of samples in this dataset is 1 , 091 , 465. The average number of samples per time series is 27 , 287. The dataset is used to generate other synthetic testing datasets in a way that lets us know the correct answers for testing purposes.

We first generate data for a clustering problem. For each time series X  X  D , 50 variations are generated from X by uniformly scaling them by  X  times the original length of X .Given x i  X  X , define a variation x  X  X computed by: where  X  is a random value with  X   X  1.

Given this synthetic construction of testing datasets, the correct clustering and ranking results can be represente d by an evolutionary tree that can be easily computed in advance. The correct clustering result is that each of the 50 synthetic time series should be clustered into the same set with its seed time series. The correct ranking result is that each of the 50 synthetic time series should be ranked according to the distance to its seed time series. The rank is defined as the order in a sequence sorted according to the Euclidean distance metric in one cluster.

Two metrics are used to evaluate the errors of the search results, where e c is called the clustering error and e r is called the rank error . The values of e c and e r of DTW, ANID, and SJ are shown in Figure 1. Given two seed time series and two variations computed from (2), Figure 2 shows the clustering results returned by DTW, ANID and SJ. The results show that our approach produces fewer clustering and ranking errors than DTW and ANID.
To evaluate the robustness of the proposed approach to additive noise, we generated 50 variations for each time series X  X  D by adding a factor  X  of additive noise. Given x i  X  X , define a variation x  X  X computed by: where  X  is a uniform random value over various ranges to be defined. The values of e c and e r forDTW,ANID,andSJareshowninFigure3.Figure4shows clustering results returned by DTW, ANID, and SJ for two seed time series and two variations computed from (5). Our approach again produces fewer clustering and ranking errors than DTW and ANID. The clustering result of SJ is in fact exactly the same as the correct answer, w hile the clustering results of DTW and ANID are quite different. This experiment demonstrates that our approach is more robust to additive noise in time series than either DTW or ANID.
Similarly, we generate 50 synthesized time series for each time series X  X  D by adding impulsive noise to some elements of X . Given some element x j  X  X , where  X  is a random value with  X &gt; 0. The number of such elements is much less than the total number of elements in the time series. The values of e c and e r of DTW, ANID, and SJ are shown in Figure 5. Our approach produces no clustering errors and the percentages of ranking errors are less than 5%. However, the percentages of both clustering errors and ranking errors produced by DTW and ANID are above 40%. Figure 6 shows clustering results returned by SJ, ANID, and DTW, given two seed time series and four variations computed from (6). According to the correct answer, the clustering result of SJ is correct, while the clustering results of DTW and ANID are not. This experiment demonstrates that DTW and ANID cannot tolerate impulsive noise but our approach can tolerate impulsive noise effectively.

The experimental results show that our approach produces fewer clustering errors and rank errors than ANID and DTW no matter whether the data is uniformly scaled, has additive noise, or has impulsive noise. ANID can tolerate more uniform scaling and additive noise than DTW. However, both ANID and DTW fail in the presence of impulsive noise. In all experiments our approach demonstrated a much greater tolerance to uniform scaling, additive noise, and impulsive noise than either DTW or ANID and so we conclude that it is a more robust approach.
 In this work, we proposed a new definition of subseries join that finds similar subseries in two or more time series. We also proposed a method to efficiently compute subseries join based on a hierarchical feature representation and non-uniform segmentation. Experiments have demonstrated the effectiveness and ef-ficiency of our approach by testing on a set of synthetic time series. Compared with previous work, our approach is observed to be relatively immune to changes in length, additive noise, and impulsive noise.

