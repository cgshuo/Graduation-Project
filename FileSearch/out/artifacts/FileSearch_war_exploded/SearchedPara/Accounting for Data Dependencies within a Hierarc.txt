 We propose a hierarchical nonparametric topic model, based on the hierarchical Dirichlet process (HDP), that accounts for dependencies among the data. The HDP mixture mod-els are useful for discovering an unknown semantic struc-ture (i.e., topics) from a set of unstructured data such as a corpus of documents. For simplicity, HDP makes an ex-changeability assumption that any permutation of the data points would result in the same joint probability of the data being generated. This exchangeability assumption poses a problem for some domains where there are clear and strong dependencies among the data. A model that allows for non-exchangeability of data can capture these dependencies and assign higher probabilities to clusters that account for data dependencies, for example, inferring topics that reflect the temporal patterns of the data. Our model incorporates the distance dependent Chinese restaurant process (ddCRP), which clusters data with an inherent bias toward clusters of data points that are near to one another, into a hierar-chical construction analogous to the HDP, and we call this new prior the distance dependent Chinese restaurant fran-chise (ddCRF). When tested with temporal datasets, the ddCRF mixture model shows clear improvements in data fit compared to the HDP in terms of heldout likelihood and complexity. The resulting set of topics shows the sequential emergence and disappearance patterns of topics.
 G.3 [ Probability and Statistics ]: Nonparametric statis-tics; H.3.3 [ Information Search and Retrieval ]: Clus-tering, Text Mining Algorithms, Experimentation Latent Topic Modeling, Bayesian Nonparametric Models
A probabilistic topic model seeks to discover a hidden structure referred to as  X  X opics X  from an unannotated set of data. The assumptions of the LDA (latent Dirichlet allo-cation) family, a popular topic model initially developed for modeling text, is that a topic is represented by a multino-mial over the vocabulary, each document is generated from a set of topics, and each word token in the document is as-signed to a specific topic. This topic modeling problem is an example of the grouped clustering problem where the data are composed of a set of groups, each data point within a group is assigned to a latent cluster, and these latent clusters are shared across the groups. When applied to text, each document corresponds to a group, each word token within the document corresponds to each data point, a topic cor-responds to a latent cluster, and these topics are shared across all the documents in the corpus. The nonparametric extension of the LDA, called the HDP-LDA for Hierarchi-cal Dirichlet process [12], is widely used for the grouped clustering problem, and numerous variants of the HDP have been applied to text modeling [17], sound source modeling [5], activity recognition [6], and computational biology [11]. The nonparametric nature of the HDP means the model can be fitted without specifying the number of clusters a priori because the model infers an appropriate number of clusters.
The HDP assumes that data are exchangeable, and when applied to text, this means that the word tokens, as well as the documents, can be permuted in any way without af-fecting the outcome of the model. For some applications, the exchangeability assumption is appropriate, but in some other applications, it is an overly simplifying assumption. In discovering topics from text, the exchangeability assump-tion would result in topic assignments without considering the ordering of the documents in the corpus. This poses a problem for many corpora such as the corpus of conference proceedings where there are clear temporal patterns of doc-uments and topics. For example, when analyzing conference proceedings,  X  X ollaborative filtering X  came to be a major re-search topic around the year 1999, and so the documents before the year 1999 are unlikely to be generated from that topic. However, a model that assumes exchangeability of data does not distinguish the temporal index of the docu-ments and would allow older documents to be assigned to the new topic.

We propose a new model, the distance dependent Chi-nese restaurant franchise (ddCRF), which takes the concept of the distance dependent Chinese restaurant process (dd-CRP) [1] into a hierarchical construction of the HDP. In the ddCRP, the random assignments of the data points to the clusters depend on the distances between the data points, and this allows the ddCRP to result in a better fit than the Chinese restaurant process (CRP), a metaphor of the Dirichlet process (DP) prior with the usual exchangeabil-ity assumption. By incorporating this idea into the HDP, we are able to construct a new model that performs better on the topic modeling task than the HDP on a sequential dataset. We show this by testing the ddCRF on the task of topic modeling using four datasets of conference proceed-ings. The ddCRF outperforms the HDP on measures of heldout likelihood and complexity, and the ddCRF is also able to discover interesting temporal patterns of topics.
This paper is organized in the following structure. Section 2 describes how we build in data dependence into the CRF to make our model, the ddCRF. Section 3 demonstrates the posterior inference procedure for the ddCRF. Section 4 presents the modeling performance of ddCRF by inferring the topics of four different time-varying corpora of confer-ence proceedings. Finally, Section 5 summarizes this work and discusses directions for future work.
We propose a new model, the distance-dependent Chinese restaurant franchise (ddCRF) by adopting the concept of the distance dependent Chinese restaurant process (ddCRP) into the CRF, to account for dependencies among data. We preserve the notation of ddCRF as described in the original CRF paper [12]. In this section, we show how the ddCRP can be integrated into the CRF.
Blei and Frazier [1] introduced the customer-based dis-tance dependent Chinese restaurant process (ddCRP) where, instead of customers being assigned to tables, they are as-signed to other customers or not assigned to anyone. The probability of a new customer being assigned to other cus-tomers is proportional to the distances between the new customer and the other customers. Explicit table assign-ments do not occur in the customer-based CRP, but the connected components of customers implicitly exhibit a clus-tering property.

This customer-based ddCRP can be reverted to a table-based ddCRP by summing over the distances to each of the customers within the same connected component. Let K be an imaginary number of tables, which would be the same as the number of connected components of customers, and z i denote the index of the imaginary table of the i th customer. Let D denote the set of all distance measurement between customers, d ij denote the distance between customer i and j , and f denote the decay function which takes a distance as its parameter. The probability of each table for the i th customer is specified as follows: In general, the decay function mediates how the distances among customers affect the resulting distribution over par-titions. We consider two decay functions: the exponential decay f ( d ij ) = e  X  d ij /a , and the logistic decay f ( d exp (  X  d ij + a ) / (1 + exp (  X  d ij + a )), where a is a decay pa-rameter.

By setting the right combination of the type of decay func-tion and the distance measure, we obtain the special case of sequential CRPs. When we define the distance measure such that d ij =  X  for those j &gt; i , using either the logistic or the exponential decay function brings f (  X  ) = 0, and this results in a sequential CRP.

The partition probability over customers can be computed in the customer-based ddCRP simply as defined in [1]. In the table-based ddCRP, however, to compute the partition probability over customers, we must consider all combina-tions of table assignments, and the number of combinations increases factorially as the number of customers increases. If we make the assumption of sequential non-exchangeability of data such that the model would be the sequential ddCRP, the partition probability can be computed by where K i is the number of allocated tables until the i th customer sits at a table.

Although this commitment to a table-based ddCRP would mean we cannot take advantage of the sampling efficiency of the customer-based ddCRP [1], we propose the table-based distance dependent CRP and use it for the rest of this paper. We do so because in the hierarchical model presented in the next section, it is relatively easy to implement and compute the conditional posterior probabilities.
The Chinese restaurant franchise is designed as an ap-proach to the problem of model-based clustering of grouped data. The CRF assumes that the data are exchangeable, but this assumption does not take into account inherent de-pendencies among data points in some corpora. In order to capture such dependencies, we can incorporate the key idea of ddCRP, which takes into account the non-exchangeability of data, into the CRF. There are three ways to do that. 1. We can model the first (menu) level CRP as a ddCRP. 2. We can model the second (customer) level CRP as a 3. We can model both first and second levels of CRP as
In the rest of this paper we only consider the first ap-proach. Although we only implement and test the first ap-proach, the other two approaches can be implemented in a straightforward way. For the task of topic modeling of a text corpus, replacing the first level CRP by ddCRP indi-cates that the topic allocation within a document can be influenced by the topics of other documents that are close to the document. Replacing the second level CRP by dd-CRP is also interesting, especially for an application such as object recognition in images where the locations and spatial distances of pixels within one image should be considered [16].

The conditional distribution of the ddCRF follows directly from the conditional distribution of the CRF, only we need to consider the decay function and the distances between data points for the ddCRF. Let  X  k is an atom drawn from based distribution H ,  X  jt is drawn from first level CRP which is a menu of t th table at j th restaurant, and  X  drawn from second level CRP which is a i th customer of j th restaurant. In the case where the first level of CRP is replaced by ddCRP, the conditional distribution of second level  X  ji only depends on the other  X . However, the condi-tional distribution of the first level  X  jt must be computed by considering the distances with other  X , hence we have: The distance d j 0 t 0 ,jt between tables  X  jt and  X  j 0 t carefully defined because it mediates the conditional dis-restaurant to have its own location, or to treat all tables in the same restaurant to share the same location so the dis-tance would be zero between tables at the same restaurant,
In this section, we describe a Gibbs sampling method for the ddCRF mixture model. Several posterior approximation techniques including MCMC [12] and variational inference techniques [13] are introduced for the CRF mixture and its related Bayesian non-parametric mixture models.
Let us recall the variables of interest. x ji is the i th data point in the j th group, t ji is an indicator variable for the table index of x ji , k jt is an indicator variable of the menu index of the t th table at j th restaurant, and n jtk number of data points at t th table in j th restaurant with dish k . We use dot to represent a marginal sum, and a superscript to denote the counts or indicators of variable excluding the one specified by the superscript.

The sampling process is used in order to infer about t and k based on the observed data set x . Before comput-ing the posterior probabilities of these variables, we need to compute the probability of data point x ji given all other variables. We let H denote the prior distribution over prob-ability of data points, and  X  k is drawn from this distribution with probability p (  X  k |  X  ) with hyperparameter  X  , then each data point belonging to this latent cluster has a probability p (  X |  X  k ). Therefore, the conditional density of data x all other variables under mixture component k is computed as: p ( x ji | x  X  ji , t , k ) = p ( x ji , x This equation can be further simplified if we use a conjugate prior H . If we use a dirichlet multinomial conjugacy on H , this equation can be further simplified as follows:
Sampling t Now we show the conditional probability of t ji given other variables by bringing the Chinese restaurant franchise metaphor. The probability of t ji is proportional to the number of customers sitting at the table t times the probability of data point x ji arising from the table. where the probability of the data point x ji drawn from a new table can be calculated by marginalizing over the latent cluster k ,
Sampling k If t ji is drawn from the second term of Equa-tion 2 then we have to draw k jt new for the new table from the following distribution.  X  We use the same computation as in Equation 3 (omitting the common denominator), but we present this again to clarify the sampling procedure. We can also sample k jt by exclud-ing all data items in table jt , and this sampling of new k changes the component membership of all data items in ta-ble jt .
To improve our model, we place a prior for first and second level concentration parameters  X  and  X  , as used in the orig-inal CRF. Sampling  X  is done in the same way as [12], so we discuss here how to sample  X  . We note that the probability of  X  is conditionally independent of customer assignments t given dish assignments k . From this fact, the probability of  X  is where p (  X  ) is the prior on the concentration parameter  X  . As we derived in Equation 1, p ( k |  X  ) can be computed in a sequential setup. Therefore,  X  can be sampled from its posterior distribution. p ( k |  X  ) =  X   X  K where K i is the number of allocated tables until the i th customer in the sequential setup. To sample from the con-tinuous variable we use Griddy-Gibbs method in [8]. This method evaluates the probabilities on a finite set of points, approximates the inverse cdf p (  X  | k ) using these points, and samples from the approximated inverse cdf.
We now describe the experiments to evaluate the perfor-mance of the ddCRF on four different text datasets and show how the ddCRF compares against the HDP. The datasets for the experiments include conferences, SIGIR, SIGMOD, SIGGRAPH abstracts, collected through the ACM digital library, 1 and the NIPS article dataset 2 . These four confer-ences have long histories, their proceedings are published over 20 years, and like many academic publications, their main topics have shifted through time. The detailed statis-tics of the four datasets we used in the experiments are in Table 1.
We modeled these datasets using the ddCRF to capture the topic changes within a conference though time. We re-moved stop words, terms that occurred less than 10 times in NIPS, SIGIR and SIGMOD, and terms that occurred less than 5 times in SIGGRAPH.

We trained the HDP mixture model, the ddCRF mix-ture model, and the latent Dirichlet allocation(LDA) [2] with these datasets and compared their results. Each of the re-sults are averaged over 20 runs. All models used for the evaluation used a symmetric Dirichlet distribution with pa-rameter of 0.5 for the prior H over topic distribution. It is possible to sample the Dirichlet parameter over H , but http://portal.acm.org/ http://www.cs.utoronto.ca/  X  sroweis/nips in that case, the number of topics increases too much [15], constant. The concentration parameters  X  and  X  were given vague gamma priors with the scale parameter of 1 and the shape parameter of 1.
We compare the ddCRF with the HDP using two met-rics, heldout likelihood and complexity. We also compare the discovered topics from the two models for a qualitative analysis.

Heldout Likelihood : Heldout likelihood is widely used in the topic modeling community to compare how well the trained model explains the heldout data (cf. [9, 14, 12]). A better model will give rise to a higher likelihood of heldout documents on average.
 where M train denotes the model already trained by a train-ing data, and W denotes the heldout data. To calculate the heldout likelihood we used the last 10% of documents for testing and 90% of documents for training.

Figure 1 shows the heldout likelihood of the datasets. For all datasets the ddCRF shows better heldout likelihood than the HDP regardless of the decay parameter and the decay function. 4 The results exhibit that the heldout likelihood gets lower when the decay parameter increases on average. Therefore our model inferences more time sensitive topics with lower decay parameters.

Complexity : Bayesian nonparametrics and the related methods are often used as an alternative of the model se-lection and integrate over all complexities of a model. If there are two or more models that produce similar results in terms of heldout likelihood, the less complex model is pre-ferred. To measure the complexity of models, we compute a complexity of each model, as defined in [15]. From the pos-terior topic assignment of the Gibbs sample, we compute the complexity as follows: where K is the number of allocated topics. This measure considers how many topics are used to explain each docu-ment and sum it through the entire corpus. A lower com-plexity indicates that the model uses fewer topics to repre-sent the corpus, and a higher complexity indicates that the model decomposes the data into many dimensions.

Figure 2 shows the model complexities for the four dif-ferent datasets. The average complexities of the ddCRF are better than the HDP in all cases except the SIGMOD dataset.
We also compared the results with LDA, and it showed that ddCRF outperforms LDA with the same number of topics. Figure 1: Heldout-Likelihood. Higher is better. The ddCRF outperforms the HDP regardless of the de-cay function for all datasets.

Emergence and disappearance of topics : One strength of our model is that the model imposes a sequential assump-tion to the first level CRP, and it disallows a word to be assigned to a topic that first appears at a later point in the dataset. Therefore the posterior topic assignment explicitly shows when the topic first appears.

The emergence of topics, combined with the topic trends over time, shows the strength of the ddCRP to model the se-quential non-exchangeability of data. To measure the topic trends over time, we define topic intensity computed from the posterior sample assignment. At each time slice t , the intensity of topic k is computed by the number of terms as-signed to a topic k over the total number of terms at time slice t .

We choose two topics from the SIGIR dataset based on the training results of the ddCRF and look for the most simi-lar topics from the results of the HDP, similarity measured in terms of JS divergence. Figure 3 shows the intensity of those four topics over time. The figures on the left show the topics and their intensities as found by the HDP, and the figures on the right show the topics and their intensities as found by the ddCRF. The topics on the top are about  X  X ollaborative filtering X , and the topics on the bottom are about  X  X pam filtering X . The  X  X pam filtering X  topic, which emerged with the rapid growth of email spams, was found by the ddCRF around the year 2000, but the similar topic found by the HDP seems to be a mix of a topic related to spam filtering and another topic related to news articles. The spam filtering topic in fact became a major topic in the SIGIR conference in 2000, and the HDP did not capture this phenomenon well. Similarly, for the  X  X ollaborative filtering X  topic which became a major research topic in the year 2000, the ddCRF correctly identified the emergence of the topic, whereas the HDP was unable to do so. Figure 2: Model complexity where lower values indi-cate that better (simpler) models were learned. The ddCRF exhibits lower complexity than the HDP for all datasets except for the SIGMOD dataset.

We also found interesting patterns of topic emergences from the SIGMOD corpus as shown in Figure 4. The ddCRF identified the  X  X eb search X  topic emerging around 1994 and the  X  X ml X  related topic emerging around 2000. When we modeled the same dataset using the HDP, however, these two topics seem to mix together into one topic.

While the emergence of new topics is simple to identify, the disappearance of topics cannot be explicitly captured from the posterior assignment. With certain decay function and parameter, however, we can deduce when the topic dis-appeared. An example of a topic that disappears is  X  X ile structure and record X  in the SIGIR corpus, which last ap-peared in the year 1988.
We introduced the distance dependent Chinese restaurant franchise, a hierarchical Bayesian nonparametric model that accounts for dependencies among data. By generalizing the widely used HDP to assume non-exchangeability of data, our model captures temporal patterns in sequential data much better than the existing topic models, HDP and LDA.
One property of the ddCRF that distingishes from the pre-vios attempts is that it can accomodate spatial dependencies in addition to the temporal dependencies. We only consid-ered temporal dependencies here, but modeling spatial de-pendencies will be the most promising future application of our model. Further, applications of the probabilistic topic model are not restricted to analyzing a document corpus. One of the advantages of the probabilistic topic modeling framework is the flexibility to extend the basic LDA and HDP models. These variants are widely applied to diverse fields[10, 4, 3, 7]. Our model can also be applied to various types of data to uncover other meaningful structure from Figure 3: Topic proportions over time identified by the HDP and the ddCRF from the SIGIR corpus.
 The words beneath the x-axis are the top probability words in each of the topics. As the dotted (red) lines in the ddCRF figures show, the ddCRF clearly captures when the topics first emerged. data. More specifically, we plan to explore spatial depen-dencies in modeling of images and spatiotemporal depen-dencies in geo-tagged data (e.g, tweets). Further, we can explore other approximation inference methods such as a variational method. This research was supported by Basic Science Research Program through the National Research Foundation of Ko-rea (NRF) funded by the Ministry of Education, Science and Tehcnology (2010-0025706). [1] D. Blei and P. Frazier. Distance dependent chinese [2] D. Blei, A. Ng, and M. Jordan. Latent dirichlet [3] L. Dietz, S. Bickel, and T. Scheffer. Unsupervised [4] T. L. Griffiths, M. Steyvers, D. M. Blei, and J. B. [5] M. Hoffman, D. Blei, and P. Cook. Finding latent [6] D. Hu, X. Zhang, J. Yin, V. Zheng, and Q. Yang. Figure 4: Topic proportions over time identified from the SIGMOD. As the dotted (red) lines in the figures show, the ddCRF captures the emergences of topics about  X  X eb search X  in 1994 and  X  X ml X  in 2000. [7] C. Lin and Y. He. Joint sentiment/topic model for [8] C. Ritter and M. Tanner. Facilitating the gibbs [9] M. Rosen-Zvi, T. Griffiths, M. Steyvers, and [10] R. Socher, S. Gershman, A. Perotte, and P. Sederberg. [11] K. Sohn and E. Xing. A hierarchical dirichlet process [12] Y. Teh, M. Jordan, M. Beal, and D. Blei. Hierarchical [13] Y. Teh, K. Kurihara, and M. Welling. Collapsed [14] H. Wallach, D. Mimno, and A. McCallum. Rethinking [15] C. Wang and D. Blei. Decoupling sparsity and [16] X. Wang and E. Grimson. Spatial latent dirichlet [17] J. Zhang, Y. Song, C. Zhang, and S. Liu. Evolutionary
